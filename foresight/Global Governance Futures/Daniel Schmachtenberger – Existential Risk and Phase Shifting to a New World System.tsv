start	end	text
0	10360	Hi, and welcome to Imperfect Utopias, based out of the UCL Global Governance Institute.
10360	17400	This is a podcast about the challenges facing humanity and possible global responses.
17400	22840	If you're new to the show and you want to get a list of our favorite books, other resources,
22840	35920	and to pass shows, and to join our community, go to ucl.ac.uk forward slash global dash governance.
35920	40520	We're really delighted to have Daniel Schmacktenberger on the podcast today.
40520	46560	Daniel's a social philosopher and co-founder of the Consilience Project, a non-profit media
46560	52320	organization that aims to capitalize a cultural movement towards higher quality sense-making
52440	54680	and democratic dialogue.
54680	59200	Underpinning much of Daniel's work is the conviction that strengthening individuals'
59200	65840	abilities to handle and filter information is now a civilizational imperative in a context
65840	68120	of existential risk.
68120	72920	I've been following Daniel on the podosphere for some time now, and he's definitely one
72920	79360	of the people I check in with most often when it comes to trying to make sense of what's going on.
79360	84000	Daniel's work has also been a major inspiration for this podcast.
84000	86800	So we're super excited to have you join us today, Daniel.
86800	89440	Thanks so much for making the time.
89440	90440	Happy to be here.
90440	92240	Thanks for inviting me.
92240	96200	And we're going to type in lots we could explore here.
96200	100120	But first, I just ask our pod crew to introduce themselves.
100120	101040	So my name is Sam.
101040	105400	I handle the audio and video and hopefully some of the thinking when out of time.
105440	105960	I'm Zoe.
105960	110520	I help with some of the research and more of the admin and social media side of things.
112800	115880	OK, so Daniel, let's get straight to it.
115880	123720	In a world of, as the UN recently put it, certain near-term, nonlinear change.
123720	132480	How well prepared are we to face some of the existential challenges and not just to say
132480	138320	the natural ones that people might think of like asteroid strikes and those sorts
138320	144760	of risks, which are certainly present, but possibly remote, but perhaps most challenging
144760	145280	of all.
145280	153440	And as we've discussed in some length, the human induced or anthropogenic existential
153440	161360	risks like nuclear, but also biotechnology and, of course, climate change, which arise
161360	167240	out of these kind of complex interactions of human and non-human systems, and which
167240	173160	are, I'd say, already defining our times this decade, this century.
173160	181520	And perhaps put a little bit more meat on the bone as to where we might go with this discussion.
181520	189480	We've talked before about how there is a risk that faced with major disruptions, our societies
189520	197320	could potentially default in the direction of either authoritarian oppression or even
197320	204080	chaos, and that some people do think that this is the direction of travel currently.
204080	212920	So how can we best avoid, what can we do now to avoid that kind of dystopia by default?
213360	221240	As our legacy structures, systems, governance systems, as they falter, and there is that
221240	224400	risk of major events sort of overtaking us.
228240	232640	All right, that's a few good questions.
232640	239800	The first question you asked is, how well prepared are we to deal with the existential
239800	247160	and catastrophic risks that are impending or at least have a non-trivial chance of happening?
247160	252040	And you were mentioning that this is a frame that is recognized by the United Nations now.
252040	258120	If we think of the UN as a starting place to answer this, in terms of the closest thing
258120	265160	to something like global governance or an intergovernmental organization, obviously it
265480	269320	was created after World War II in the recognition that nation states by themselves
270680	273320	weren't an adequate governance system to prevent world war.
274200	281800	And now that we had weapons such that the wars between the major powers could never be fought in
281800	288360	one anymore, we had to figure out a whole new world system to do something different than we
288360	292600	had ever done in the history of kind of like the post-empire world, which was how do you have
292600	298360	the major empires not fight wars? And we don't have a very good historical track record of that,
298360	301960	but then we got to the place where you had weapons where the wars couldn't be won.
301960	309400	And that was a different logic. So I point this out because catastrophic risk that was human-induced
309400	313960	before World War II was different and kind than after World War II because we didn't have any tech
313960	319080	big enough to actually create global catastrophic risk from human action.
319320	324680	That doesn't mean that catastrophic risk is not a part of our history, it was just always local.
325560	333080	And not only was it a part of our history, it was what happened most of the time in the
333080	338680	history of the life cycle of civilizations. So we can see that, you know, if we're studying
338680	343880	the Mayan Empire, the Inc in the Aztec, the Egyptian, the Roman Empire, like one of the first
343880	348520	things that we recognize is that they all don't exist in the forms of their dominance anymore.
348520	355160	They all either had sudden collapses or gradual collapses, but the collapse of civilizations is
356760	360680	the things that we call civilizations is one of the kind of most prominent features
360680	367000	that we can see in history. And so you can see that people faced an existential risk to them
367000	371080	from in the form of a warring army or over consumption of their resources or internal
371080	374520	dissent that was enough that it broke their capacity to continue to coordinate.
374680	381560	It's just those were always local issues, maybe a large locality if it was a large empire.
382120	390360	And those were both for environmental externality reasons, like the first civilizations that
390360	393800	over farmed and created desertification. It was a long time ago, right? That's an
395000	398200	environmental overreach is a multi-thousand of year old problem.
398200	407080	And short-term solutions regarding rivalry that in gender enmity, in the side of the other,
407640	412040	and where they then reverse engineer whatever weapon innovation you had and come back and you
412040	418520	just drive arms races, they're escalating. That's also a very old process. World War II is the
418520	424440	beginning of us getting to the place where the scale of our warfare, and then also shortly
424440	430520	thereafter, the scale of even our environmental externality hit a global catastrophic possibility.
431960	437560	So you see that we created an entire world system following World War II to say, okay, we need to
438760	443640	we now have such incredible power that we can't use in the way that we have previously. We need to
443640	448440	steward that power differently. How do we deal with conflicts without war between the major
448440	454920	superpowers? So the UN was created the World Bank, the IMF, that whole kind of intergovernmental
454920	461080	organizations that would be able to broker nation state interests to have solutions other than war
461080	466280	and the entire set of Bretton Wood agreements, Marshall Plan agreements for how we kind of
466280	473080	rebuild the world where the nations would be so economically interdependent on each other through
473080	479480	trade and globalism that it was more advantageous to them to continue to do trade with each other
479480	484280	than to bomb each other. That was a huge part of it and where we could have so much growth
484280	489080	of the economy that everybody's desire to get more could happen simultaneously without having
489080	493800	to take each other's stuff. The idea that very, very positive sum GDP situations could keep us
493800	501160	from going zero sum conflict oriented. Well, that very positive sum meaning extract resources that
501160	506040	are unrenewable and turn them into trash much faster driving GDP for a very short period of time
506040	511480	also meant we hit planetary boundaries. And so now we're seeing planetary boundaries both on the
511480	517080	side of depletion of unrenewable resources and the waste side, both sides of a unrenewable
517720	523400	linear materials economy on a finite planet, lots of different ones, right? We're not just seeing
523400	528680	too much CO2, but too much plastics and ocean micro plastics and all kinds of things on the
528680	535640	toxicity side and all kinds of things on the overfishing, cutting down to many old growth
535640	545080	forests, soil, micro, diversity loss, microbiological diversity loss, etc. So you can't keep doing the
545080	550440	positive sum thing in that same way that is based on the exponential growth of a linear materials
550440	555560	economy on a finite planet. That's one part of the kind of post-World War II solution
555560	559640	that's kind of run up against an end. The other thing is that that world system
562360	569720	created a lot of fragility, right? Because when you have global supply chains where most
569720	574120	in the of the products that we engage with now no country can make, they're made across six
574120	579480	continents, this computer that we're talking on, this phone in my hand, when you factor all of
579480	585240	the materials processing, the hardware, the software, the satellite infrastructure required
585320	592680	for our communication to be happening. The positive side of getting the world very interconnected
592680	598280	was that we were less oriented to war if we had dependence. The negative side of dependence is
598280	603800	you can get cascading failures, right? If you get failures anywhere, then you can get failures that
603800	610520	start to cascade. And we saw that with COVID, we saw that an issue in one province of China
610520	616360	became a completely global issue affecting almost every sector of the world, that needing to stop
616360	621160	the transmission of the virus in a much more transportation-based world than any previous
621720	626600	plague or pandemic ever happened in, also meant shutting down critical supply chains where
626600	631480	fertilizers and pesticides that were needed for agriculture didn't happen, driving food insecurity
631480	636760	at massive scale, which means that the solution to one problem drove other problems. Second and
636760	645480	third order effects became very problematic. And so the interconnectivity that had advantages
645480	650680	also has these fragility disadvantages, and the interconnectivity also wanted to have maximum
650680	657960	efficiencies, and the efficiencies also drive fragility. We also see that in that World War II
658920	670200	till now-ish kind of Bretton Woods time, we had one catastrophe weapon. And so one catastrophe
670200	675560	weapon could be responded to by that same catastrophe weapon. And so the game theory of it was somewhat
675560	682040	simple. And for the longest time, we only had two superpowers that had it. And as a result,
682040	685640	mutually assured destruction was very effective. You were able to create a kind of forced Nash
685720	692520	equilibrium. And also because it's very, very hard to make nuclear weapons. There's not that many
692520	696280	places that have uranium. Enriching uranium is difficult. You can even see it because of
696280	702360	radioactive tracers from satellites, so it's easy to monitor. And so you could do mutually
702360	705720	assured destruction. Obviously, we're in a situation now where we don't just have two
705720	709480	superpowers that have nukes. We have many countries that have nukes, but we also have
709480	713240	lots of other catastrophe weapons, meaning lots of other weapons that are big enough that they
713240	719400	could cause kind of catastrophic loss of civilization harm. And they aren't hard to make,
719400	725320	and they aren't trackable anymore. It's not hard to make crisper bioweapons or drone-based
727880	733000	infrastructure attack type things. It doesn't even take a nation state to do it, not traceable.
733000	737160	That's a very different situation. So when you have many different catastrophe weapons and you
737160	743000	have many, many different actors that can have them, including a very difficult situation to
743000	747720	be able to monitor which actors, how do you do mutually assured destruction? And so how do you
747720	755320	get the deterrent strategy right? And so what I'm bringing up is that catastrophic risk before
755320	760760	World War II was one phase, all of human history up to that point. Then World War II till now was
760760	766680	kind of one phase, and now we're entering a new phase where the Bretton Woods mutually assured
766680	773000	destruction, IGO, exponential growth of a globalized linear materials economy,
773000	777640	set of solutions doesn't work for the new set of the catastrophic risk landscape that we face.
777640	782120	So we need a totally new set of solutions which will require innovation in our social technologies
782120	788200	of how we coordinate game theoretic type issues. Now, when you say how well prepared are we,
789160	796760	we come back to the UN, we recognize that we have not succeeded in nuclear disarmament.
798520	802920	Even while we kind of claim to succeed in nuclear disarmament in some very limited ways,
802920	807880	we still had arms races of faster delivery mechanisms, hypersonic missiles, whatever,
807880	814920	to try to win first strike and other things like that. We got more countries with nukes rather than
814920	821960	less during that time. We got more other countries that could affect the movement of a new nuclear
821960	828680	weapon through other kinds of geopolitical and less military advance, but engaging the bigger
828680	834120	military type tactics, plausible deniability attacks that get blamed on a larger superpower
834120	841880	and things like that. And during that time, we've also had every new type of advanced technology
841880	846120	create an arms race. There's an arms race on AI autonomous weapons, on
847480	853960	the application of CRISPR technology to bio weapons, cryptographic type weapons for cyber
853960	860840	attacks. And so we have succeeded in preventing no arms races. We have not been able to reverse the
860840	867480	one really critical one. None of the sustainable development goals can really be said to have
867480	874840	been achieved well. So I would say that our global coordination on all of the most critical issues
876520	882760	is inadequate to the timeline and consequentiality of the issues. That seems very, very clear.
883720	891480	And the as exponential tech is advancing, the total number of catastrophic risks and the total
891480	898120	probability of each is increasing. And the capacities that we're utilizing to address them
898120	903640	are not increasing accordingly. So there is a gap that we need to be focused on, which is what you
903640	908440	guys are focused on, which is this kind of global governance topic. We have global issues, not just
908440	914200	local issues. Everybody's scared of global governance, the frame, the term global governance,
914680	919720	or at least global government, for a good reason, which is we have a good
921240	927320	long history of reasons to not trust consolidation of power with no checks and balances.
927320	932760	So nobody wants this kind of massive, unchecked global government. And at the same time,
932760	936520	you have to have governance at the scale that cause an effect is occurring. And if we're having,
937240	945000	if, if nobody can fix climate change on their own, in terms of nation states,
945000	949400	and yet they're all affected by it, and they can't fix overfishing, they can't fix nitrogen,
949400	952600	run off dead zones and oceans and et cetera, there have to be
954760	959240	global coordination solutions. Otherwise, multipolar traps ruin everything, right?
959240	963560	Multipolar trap being some kind of race to the bottom. Arms race is an example, as we've already
963560	969240	mentioned. Tragedy of the commons is another example. But the key to both of them is where the
969240	973880	agent focused on their own short-term well-being does something that advances their short-term
973880	978040	interest, but then makes everybody else have to do the same thing. And where everyone doing it
978600	987000	creates the maximally bad long-term situation. And so if we try to create some treaty around not
987000	991240	overfishing a particular region of the ocean and anybody violates it, then why does it,
991240	994760	if anyone else doesn't violate the treaty, if they can't figure out enforcement,
995480	1000920	then you're just a sucker for holding to the treaty, right? Because all those fish are going
1000920	1003720	to get killed anyways, the ocean's going to get messed up. It's just going to feed another
1003720	1009000	population that's going to grow and have more people to engage in economics and armies. And so,
1009800	1015480	yeah, how do you do enforcement on a nation that has nukes or a nation that has some critical
1015480	1020840	aspect of infrastructure or, you know, or the globalized supply chain? And so enforcement
1020840	1025800	becomes tricky. So then you get these types of things, tragedy of the commons and arms race,
1025800	1030520	multi-polar traps. So you have to figure out how do we solve those coordination issues globally,
1030520	1035240	because we have global issues that can't just keep getting pushed down the road.
1036120	1040120	And yet we want to figure out a solution to do it that isn't a kind of global government that
1040120	1048520	becomes its own catastrophic risk of under the name of some problem that is scary enough,
1048520	1052840	we agree to some totalitarian power structure. And that's the thing you mentioned about order
1052840	1058840	and chaos, is that we can see that the thing we call civilization is a way of having some order,
1058840	1064520	some coordination between lots of people, so that they can do specialization and division
1064520	1069080	of labor, creating a richer world for everybody, and then coordinate all that, they can coordinate
1069080	1074600	their activity for not just those kind of productive purposes, but also protection purposes.
1075560	1079960	So the thing that we call civilization is how we coordinate behavior of lots of people.
1081240	1087240	And that's actually a pretty hard thing to do when you think about people that want different
1087240	1092360	stuff and believe different stuff and aren't necessarily connected to or bonded to each
1092360	1097320	other, like how do you get them to not just do the immediate advantageous thing to them
1097400	1105160	for people that are fundamentally strangers to them. So typically a civilization will try to
1105160	1111160	create order through some kind of imposition, some forced religion, forced patriotism, law,
1111160	1115800	whatever it is, and it can air in the side of an order to have everybody
1116920	1123240	participate with that order becoming increasingly tyrannical, increasingly dictatorial.
1123960	1128920	If it doesn't do that, people end up orienting towards tribalism naturally
1130360	1133800	and fragmenting kind of towards each other and you end up getting the thing failing in the
1133800	1138680	direction of chaos. The only other answer is how do you get order without it being imposed?
1138680	1143720	How do you get emergent order? And this was the kind of idea of democracies and republics and
1143720	1150040	open societies is maybe we could actually get emergent order if we, and it was based on the
1150040	1155480	idea of a culture that invested in the people enough, that the people didn't just believe
1155480	1159400	different things and want different things and be willing to defect into war. You had to actually
1159400	1165400	develop a people that could all come to understand the world similarly. Can everybody understand
1165400	1170360	the philosophy of science well enough that they can all come to understand base objective reality
1170360	1175640	that they share similarly? Can they all have something like Hegelian dialectic capacities
1175640	1179560	where they can notice not just their own values but other people's values and recognize that only
1179560	1184280	solutions that meet everybody's values will end up working? Can they understand things like
1184280	1189240	multi-polar traps well enough to understand that a short-term win of my political party
1189240	1193160	just means that whatever technique we utilize that was effective gets reverse engineered,
1193160	1196840	the other side wins in the next four years and undoes everything that we did for four years and
1196840	1202040	we get nowhere and then dictatorships do much better than us and the society fails? Can people
1202040	1206280	understand those things enough that they don't orient towards the short-termism kinds of things?
1207240	1211000	So this is why the modern democracies emerged out of modernity, emerged out of a
1211000	1215080	philosophic system that said we can come to understand the world and understand each other
1215080	1221080	well enough that we can actually have emergent coordination. Obviously, the world has gotten
1221080	1226280	much more complex during that time and the cultural value of that kind of education has eroded.
1227240	1237080	So here's the way I would frame up the current situation, one way that I'm looking at the current
1237080	1247880	situation. This is a detour but I think it's helpful. I'll go back to World War II and then
1247880	1253000	bridge it to now since that was kind of the beginning of catastrophic level technology.
1253960	1260600	One way of looking at World War II is, and this is not the only way, there's lots of ways,
1260600	1264920	this is a useful way for the construction I'm doing. One way of looking at it is that there were
1264920	1270600	a few social ideologies that were competing for supremacy and what they were competing over was
1270600	1275240	the emergence of a new set of technologies that science made possible that were so much more
1275240	1278920	powerful than the previous technologies that kind of whoever got dominance and then would win.
1279720	1283720	And so the bomb is obviously the center of that but it's not the whole of it,
1283720	1288840	computers, the enigma machine and the whole development of computation, rockets,
1290440	1295800	and chemistry. Chemistry is a part of that, kind of advancing in World War I but then
1295800	1300120	advancing a lot in World War II. Kind of those all came from science getting to the place that we
1300120	1311400	could do atomic physics and physical chemistry well. And the social philosophies we could say
1311400	1319800	are capitalism and liberal democracy. The intersection of theory of markets and something
1319800	1327800	like a democracy or republic. Communism, the Soviets and fascism and a particular kind of ethno
1327800	1333400	centric nation-state fascism. So those were three different types of social systems.
1334440	1340760	And we can see that Germany was actually meaningfully further ahead than the U.S. or the Soviets in
1340760	1347080	certain areas of tech. They got the enigma machine first, they got the V2 first. Those other countries
1347080	1352280	were obviously larger so when they recognized that and fought to catch up they had an advantage in
1352280	1358360	that way. And we can say and there's lots of problems with saying this but for the use of
1358360	1366200	the construction we can say that the U.S. won that competition for that war. Those wars over the
1366200	1371560	new technologies and we did it not through the market running the Manhattan Project but the
1371560	1375480	state running the Manhattan Project. This is actually a very, very important thing to recognize
1375480	1380680	is that the state, the United States recognized that it was an existential risk
1381480	1388680	and you remember it was Einstein and Sillard, I think, the Einstein-Sillard letter that said
1388680	1393560	no, the physics we came up with really does say that Obama is possible and there's a decent chance
1393560	1397960	the Germans know this and they're working on this. We were doing the physics over there together and
1398360	1412840	the idea that states don't innovate and that markets innovate is just not true. Historically,
1413480	1419160	the ability to split an atom, which is in a way the most impressive innovation,
1419720	1424840	was done by the state, not by the market. That was not outsourced or private contracted.
1425240	1434920	And the same with cracking the enigma code and the whole early development of computation that
1434920	1438680	ended up then getting private contract and leading to Silicon Valley was nation-state funded,
1438680	1443240	the Apollo Project. And it kind of stopped with the Apollo Project for some important reasons.
1443240	1448040	But what happened was the United States recognized that the technological advancement
1448680	1453400	was going to determine who had the power to determine the world so much that there was
1453400	1459480	next sense of risk for them that they created a unlimited black budget, brought all the best
1459480	1468120	minds together to drive innovation in technology to be able to make a democratic system stronger.
1470600	1474760	For a bunch of reasons, after that and the decades that followed,
1476280	1479240	more and more of the innovation got outsourced to the private sector
1480120	1484280	and it started to become closer to true that the state wasn't innovating and most of the
1484280	1487080	innovation was happening in the private sector. But the private sector doesn't have the same
1487080	1492760	patriotic interests, it doesn't also have the same people in the private sector aren't voted in,
1492760	1495640	they don't have term limits, there isn't the same jurisprudence applied to them,
1495640	1501560	so they have a different set of agendas, right? And the whole idea of the state, like you can
1501560	1505880	almost think of what the state in a liberal democracy is as like a labor union for the people
1506520	1511000	and as a whole, like a labor union, is how do you unify all the people to have something that
1511000	1516360	is big enough to represent their collective interest so that the large corporations and the
1516360	1520520	major wealth holders within capitalism don't just rule everything like feudalism, which is
1520520	1526840	the thing we were trying to replace before, because it's very clear that if we have a trade system
1527560	1533000	and it's mediated by an abstract system for doing accounting like currency,
1533720	1537000	that pretty soon you'll have a power law distribution of wealth and a few people will
1537000	1540840	own most of the wealth. Some people are better at it and then getting better at it gives you
1540840	1545640	more capacity to keep getting better at it and there's compounding interest, which is an exponential
1545640	1550200	return on owning capital and there's compounding interest on debt and does that thing, right?
1550840	1555160	And we can see the data of that in Piketty's book, but it's also just kind of a natural
1555160	1559400	thing to look at. So the idea was since power law distributions are going to happen,
1559640	1563880	most people are going to have really no power. How do you not have that be oppression? Well,
1563880	1567640	let's have the people all be able to collectively vote where at least the majority of what they
1567640	1573720	care about gets encoded as law. So their values are the basis of the jurisprudence of law. So then
1573720	1581480	rule of law can get enforced by representatives of foreign by the people that are going to be
1581480	1586040	bequeathed with a monopoly of violence so they can actually do enforcement
1587000	1592280	to be able to protect the people in the commons against perverse incentive while letting the
1592280	1596040	market do all the good things that it does. But most of rule of law is actually binding
1596040	1605640	the perverse incentive. So if that only works where the state can check the predatory aspects of
1605640	1610520	markets, if the people are checking the state that it is truly of foreign by the people,
1610520	1614360	there's transparency, everybody's actively engaged, as soon as that stops happening,
1614360	1618520	then the government is just run by people. Those people are economic actors. They're in there for
1618520	1623400	whatever a short period of time and they will be liked about the same whether they do corporate
1623400	1626600	interests or not because nobody's really going to know. And so of course, you end up getting
1626600	1630520	regulatory capture where the market captures the regulatory apparatus and you get crony capitalism
1630520	1637160	and that kind of institutional decay. And as the founding fathers in the US said, and anyone who
1637160	1643000	paid attention, as soon as a couple generations pass and the people forget what it means to fight
1643000	1647640	a revolutionary war and be under oppression, they won't keep investing and being educated enough
1647640	1651160	and actively being engaged in government because they'd rather keep up with the Joneses or party
1651160	1659320	or like some other thing. And so how do you keep the intergenerational transfer of not just the
1659320	1663560	knowledge but the civic virtues necessary to uphold a democracy, which is not a trivial thing.
1663560	1668680	And especially as time goes on and the complexity of the world increases,
1669640	1673720	understanding the issues well enough to really play a role in them and to be able to oversight
1673720	1677400	them and police them gets harder and harder. And so there has to be more and more investment
1677400	1682360	into doing that. So we can see that the people stopped investing in checking the state, the state
1682360	1687320	stopped checking the market, market captured the state, all the innovation got outsourced.
1687320	1693080	And so what we can see today, so we see in that World War II example that the state really pioneered
1693080	1696520	the advancement of all these areas of tech to increase the integrity of the state.
1698920	1705240	There is a jump in technology that is currently happening that is more significant than the
1705240	1713320	World War II jump in technology. And the center of it is AI and computation with AI being the
1713320	1718120	very center, right? It's computation, digital tech, but then the application of AI and digital
1718120	1723640	tech to physical tech as well. So the application of that to biotech and CRISPR kind of stuff and
1723720	1728680	to robotics and robotic automation and the other key areas of computer science from the
1729720	1735960	evolution of the computational basis, quantum computing, photo computing, DNA computing, whatever,
1735960	1746280	and again, the application of that to the material sciences, nanotech, etc. So
1746600	1753880	we're undergoing this huge jump in technology right now that is something like two orders of
1753880	1759080	magnitude more significant than the previous World War II jump was in terms of the total amount of
1759080	1763320	verticality of power and the speed at which it's developing and the number of verticals
1763320	1772280	simultaneously. And the way I see it is that tech will confer so much power that only those
1773000	1776120	who are guiding it will have much of a say in the future.
1778520	1786200	And right now, I only see two types of groups really guiding it meaningfully.
1787720	1792440	Some authoritarian nation states are where the nation state is taking seriously the development
1792440	1797000	of tech in the nation state is investing a very big R&D budget and how to actually increase the
1797000	1801320	integrity of their nation state. And there's a good thing for them to do aligned with whatever
1801320	1806200	their system and their ideologies are. And obviously, China is a prime example here where
1806760	1812120	the application that the government is investing in the development of engineers and in the
1812120	1817880	application of all of those areas of tech to the nature of government itself. And that's everything
1817880	1824680	from their IoT system to their seismic credit system to the transistor development and lithography to
1825640	1830600	the Belt and Road Initiative and getting something like 94% of the world's rare earth
1830600	1836840	minerals in there that are needed for computational substrate and their supply chain to on and on,
1836840	1842680	right? To the creation of their own internet that doesn't have the same problems for their country
1842680	1849560	that the US internet has. So authoritarian nation states are using the exponential tech to become
1849720	1855480	exponentially more effective authoritarian nation states. And the only other kind of org
1855480	1862440	are companies, Western mostly companies. And those companies are supported by a military
1862440	1866360	and capital and infrastructure of the nation state, but they don't are not serving the interests of
1866360	1871720	the nation state other than GDP and jobs and some very short term kind of stuff. And they're
1871720	1878920	becoming exponentially more powerful companies. But you know, Facebook and Google and have more
1878920	1883560	users than China and the US combined have people, right? So these are humongous kinds of things of
1883560	1889000	which there is no precedent for a corporation in history. And Rand never imagined things like this
1889000	1894600	when she was thinking about the symmetry of supply and demand. And she didn't think of things like
1894600	1899640	Metcalfe Dynamics that end up leading to natural monopolies and antitrust law didn't think of
1899640	1904360	that, right? So you end up having Amazon being bigger than all other online stores combined
1904360	1907800	and Google being bigger than all other search engines combined and Facebook being bigger for
1907800	1912120	time on site than all the other social networks, you get a natural power law distribution,
1913480	1918920	not based on government crony capitalism based simply on the nature of network dynamics that
1918920	1924920	once you reach a certain escape velocity, there you're a natural monopoly will start to emerge
1924920	1930600	based on the value of the thing being associated with the second power of the number of users.
1931480	1940600	And so the interesting thing is you see these corporations that are becoming more powerful
1940600	1946680	than nation states in many ways because of the development and direction of the exponential
1946680	1955240	technologies. And as that happens, they are less able to be regulated by the countries while still
1955240	1961400	benefiting from the infrastructure of the countries and simultaneously eroding the
1961400	1967240	integrity of the country. We can see the way that the time on site optimization ad model
1967240	1974120	of Facebook and Google and YouTube have eroded American democracies in specific and Western
1974120	1981160	democracies by doing the time on site optimization appeals to people's cognitive biases and tribalism
1981160	1987000	and limbic hijacks and those types of things. We can see that the kind of consolidation of
1987000	1993160	market function like Amazon that Amazon's growth during COVID matched pretty closely the closure
1993160	1997080	of all small businesses that aren't going to reopen. Well, the American dream without small
1997080	2003400	businesses isn't the thing, right? It's not a thing in the same way. And we see the technological
2003400	2009400	automation of so many jobs impending and not the replacement in the current way that it's trending
2009400	2017000	of a similar American dream kind of sovereignty. So there's kind of a billionaire to centa
2017000	2023320	billionaire class that runs whatever the one big dog on the top of the power law distribution that
2023320	2031240	defines a vertical is and a increasingly less upwardly mobile in terms of real capacity to
2031240	2038440	play those games underclass. And obviously some kind of middle class that is serving
2038440	2045880	the very upper class in that context. So what I see is that that is the movement to a new kind
2045880	2052120	of feudalism, right? A tech feudalism. And it's even interesting, some of those companies,
2052680	2056440	you know, we see this with Tesla, we say with the other ones, some of those companies
2057320	2061800	are getting subsidies, government subsidies. That means they're collecting taxpayer money
2062680	2069720	to do to utilize taxpayer money to do the thing they're doing. But the taxpayers didn't vote on
2069720	2074280	them doing that. They were not elected representatives. They cannot be unelected. And
2074280	2078520	there is no traditional jurisprudence for the guidance of the thing that they're doing.
2078520	2083160	That's something much more like a king than a president, which is why I say kind of an emergent
2083160	2088040	tech feudalism. So what I see is there's one stranger tractor, which is tech feudalism. There's
2088200	2091960	another stranger tractor, which is kind of authoritarian nation states. And anything like
2091960	2097640	an open society where there's participatory governance and jurisprudence that is grounded in
2097640	2102920	the will of the people, there is no system that is based on those ideals that is innovating
2104360	2110120	in exponential tech to make better versions of that social tech. That is the number one
2110120	2114920	imperative of our time, in my opinion. And either we figure that thing out, or those are the only
2115000	2119080	attractors. And the third attractor is that the exponential tech just causes X-risk and we're
2119080	2125320	fucked, right? So you have X-risk feudalism and authoritarianism as the current dominant attractors
2125320	2131720	in the presence of exponential tech. Or there's not 17 sustainable development goals that really
2131720	2135720	matter because we can't fucking achieve any of them without better coordination. There's figuring
2135720	2140760	out coordination that it becomes the central goal of the world, figuring out a kind of coordination
2141080	2146280	that is emergent order, that is neither chaos nor oppression, that is
2147960	2153720	able to utilize the exponential technologies and also to bind and direct them so that they
2153720	2160600	do not either directly or through externality create X-risk, and that they don't create
2160600	2164520	authoritarian systems or kind of feudal systems that erode civil liberties in the process. So we
2164520	2174760	need to have a kind of global innovation zeitgeist of how to apply, develop and apply all the areas
2174760	2180120	of exponential technology to building new social tech that can guide, bind, and direct the exponential
2180120	2185560	tech, prevent X-risk, and do it in a way that is commensurate with what are underlying kind of
2185560	2190040	deepest values for participatory and empowered governance and civics are.
2191000	2197400	Thank you, Daniel. That was a fantastic riff on the opening question and I think really sets the scene
2197400	2202840	and goodness we could go in lots of different directions now. It made me think of, you know,
2202840	2207960	Niall Ferguson's book, The Square and the Tower. Niall Ferguson has said that historians haven't
2207960	2213480	taken network seriously enough and he traces these network dynamics back centuries and actually says
2213480	2218680	they were much more prominent and important. And part of the historical political landscape
2218680	2228760	than we often think. So that was really interesting and I guess what I was thinking we might pick up
2229880	2235080	would be, you said at some point that we often kick these problems down the road and I wonder
2235080	2241240	to what extent we're really coming up against the sort of cognitive limits of humans. Given the
2241240	2245640	rapidity of change, given the challenges that we confront in trying to get our heads around
2245640	2252040	exponential functions. We have this kind of strange parallax right now between continuity
2252680	2258360	and discontinuity. So we have these unique unprecedented challenges, but on the other
2258360	2265880	hand we have these very old forces of zero sum competition, resource wars. Certainly something
2265880	2272520	which I sometimes hear in the academy is this idea that ultimately there's really nothing new under the
2272520	2280040	sun that we can repurpose our existing structures that we do have good enough global governance,
2280040	2285320	if you will. But I wanted to tease out a little bit more this idea of continuity. It seems to me
2285320	2291240	also what we're seeing is kind of a resurgence of understanding that actually we do need to respect
2291240	2296920	the laws of physics, that we need to respect the laws of thermodynamics, that we might actually even
2296920	2303480	have to listen to say E. O. Wilson on the the laws of sociobiology in terms of how do we navigate
2303480	2309720	through a viable path given the current situation we find ourselves in. But on the flip side, we also
2309720	2315000	have a sort of a real lack of radical vision within the half within sort of the corridors of power,
2315000	2320840	even if the UN Secretary General Antonio Guterres is calling for a new international social contract,
2320840	2326120	it doesn't seem to be resonating. And if you go back in the historical record and you look at the
2326680	2331880	debates of the 1950s and the shadow of the bomb and how radical the vision was,
2331880	2336520	of course, what resulted was a compromise. But nevertheless, there were very serious people
2336520	2343960	who were thinking hard about global political federation. What's happened? Why is it, you know,
2343960	2351240	to draw on that famous phrase is easier to imagine the end of the world than the end of capitalism?
2352200	2360200	Why is it so hard for us to to work through the viable path to to address the challenge
2360200	2369080	that you've articulated so clearly? First, on the topic of there's nothing new under the sun and
2369080	2375000	our previous systems of social philosophy and social technology are adequate. I don't think that
2375000	2379080	anyone believes that who's actually studied exponential tech and X risk meaningfully,
2379080	2386040	I have not met them. It doesn't seem like a reasonable position to hold those things together.
2389640	2394760	If you look at just a single category of exponential tech, that idea will change.
2397880	2403480	I'm sure the listeners have all seen this, but like, when you saw the way that AlphaGo beat
2403480	2410680	Stockfish at chess, that was so fucking clear that we're dealing with phenomena that are nothing
2410680	2416520	like any phenomena that the Scottish Enlightenment or the founding fathers or Isaac Newton or Marx
2416520	2423000	or anyone ever had to think about that that the best chess player in the world, which is the
2423000	2426280	cutting edge of there's nothing new under the sun, like chess players got better, but like,
2426280	2432600	people been good at chess for a while, right? It's a slow evolution until AI and Stockfish just
2432600	2437640	devastated the best chess players in the world. We remember seeing Kasparov get beaten, and then
2437640	2441640	Stockfish kept getting better and better with the model of AI or programming all the human games
2441640	2447160	until it was so much better that it stopped even making sense to calibrate it relative to the best
2447160	2452600	humans. And then a breakthrough in AI says, let's do this differently, right? Let's make a type of
2452600	2458680	AI based on rival networks, and we won't actually program any human games into it. We'll just let
2458680	2463080	it play itself a bunch of times and fast forward and see what it learns. And I don't remember
2463080	2469880	exactly, but AlphaGo by Google, I think it trained itself in three hours, just playing itself with
2469880	2475480	no human input of information, just the rules of chess. And then it ended up beating Stockfish.
2475480	2480280	There were a few stales, but it was like 38 to zero in terms of the non stales. And it's like,
2481080	2485400	oh, wow, that's it. And Stockfish was so far beyond humans, and it's like three hours of training. And
2485480	2491480	then that same thing could beat us at go and start to beat us at complex strategic video games.
2492280	2497720	And this is all evolving over the course of almost no period of time, right? This is evolving over
2497720	2505560	the course of months and single digit years. Nothing new under the sun. Nobody can study
2505560	2508600	exponential curves and think that. Now, this brings us to the...
2509320	2520440	It's just such a silly thing to say. When you start looking at scaled species extinction,
2520440	2527880	when you start looking at the Anthropocene as a real thing, where humans are a bigger force
2527880	2532920	than all geologic forces combined in defining the surface of the Earth, like, fuck, it's a different
2533480	2540600	situation than the history of the world was. And like I said, just even starting with the bomb,
2541400	2547160	the world never didn't have the major empire's war. And World War II was like a second ago
2547720	2555480	in historical time, right? And the solution to not use that bomb drove all these other issues. So
2555480	2564520	a lot of our issues are just increases in the severity of the same underlying type of game
2564520	2571480	theoretic dynamics. And so we can say they are continuous with them in type, but there are places
2571480	2578040	where a change of magnitude becomes a change in kind, right? Like as soon as the magnitude gets
2578040	2583320	beyond human information processing capability, it's now a change of kind. As soon as we move
2583320	2586840	from a war that's winnable to a war that's not winnable, even though they're both the logic of
2586840	2591640	war, it's a change of magnitude that becomes a change in kind, right? So there's a lot of places
2591640	2597160	where even the things that are continuous with the past become discontinuous past certain thresholds,
2597160	2601560	meaning that the same types of solutions, the whole class of solutions doesn't apply anymore.
2603560	2608600	Now, that doesn't mean that we throw out everything that we've learned. It means that we have to make
2608600	2612600	sure that we're applying everything that we've learned that is effective, that we aren't making
2612600	2617320	the mistake of not paying attention to the total amount of human thinking and ingenuity
2617320	2622920	that's happened so far, and that the new innovation that we do is commensurate with the smart parts
2622920	2628760	of it. But it happens all the time that we're exploring a search space, and there's a couple
2628760	2633800	branches, and in the immediate term, this branch has more incentive. And so we explore this branch,
2633800	2638200	and then we just forget about this one, and we just keep exploring, and then we hit a cul-de-sac
2638200	2644760	at a certain point. But we have reasons why there's momentum to keep some combination of
2644760	2649800	sunken cost fallacies with the actual belief that this is the only path, this earlier choice,
2649800	2654120	and that we wouldn't go all the way back there to the not even knowing the other branches that were
2655000	2661080	cleaved that we didn't pay attention to, to like perverse institutional incentives of standard
2661080	2665320	models where it's hard to get a research grant to do anything outside of that thing or to get
2665320	2668840	your professor who believes in that thing to change their opinion on it or whatever it is.
2670760	2673800	So there are a bunch of places where we actually have to go back and say, okay,
2676360	2681720	there was an incentive to make faster and faster smaller and smaller computer chips,
2681720	2685720	and there was enough money around that that there were whole other directions in computational
2685720	2692680	substrate that we didn't take that for reasons of manufacturing resilience and a bunch of other
2692680	2697880	things might actually be meaningful and interesting. This is starting to be a real conversation in
2697880	2703080	theoretical physics with string theory, and like maybe we actually need to rewind and try
2703080	2709000	a fundamentally different approach. I think there are places in governance where like we've just
2709000	2720600	accepted, we've just kind of accepted capitalism is the only, in the West, is the only reasonable
2720600	2729800	answer combined with some kind of omen-ish government state. And if you think anything else,
2729800	2735080	you didn't study the history of Mao and Stalin and Paul Potts and whatever, because everything else
2735080	2740440	ends up becoming that kind of dreadful slaughter. That's kind of the dominant narrative where
2741240	2745240	it's worse than going against Christianity or it's similar to going against Christianity
2745240	2748840	in the Dark Ages, right? There's an almost religious tone to it. It's like, well,
2748840	2751640	we could come up with better shit that isn't any of those things, like
2753960	2757960	there's nothing new under the sun, blockchain's new, like the ability to have an uncorruptible
2757960	2763960	ledger where you can have a provenance of data that you can't fuck up. That makes it
2763960	2767240	where you can have a history that can't be corrupted or changed by the winners afterwards.
2767240	2771240	That's kind of new. That's a big deal. Makes it to where you can have a system of justice where
2771240	2776680	you can't actually fuck up the data, right? It means that you can have a system of accounting
2776680	2780520	where let's say the government spending was on a blockchain that was transparently
2780520	2785000	oversighted, there wouldn't be missing money anymore. Right now, there's all these places
2785000	2788680	where the total amount of money going in and the receipts coming out don't add up and there's
2788680	2794840	missing money. It's like, well, that couldn't be. Does that make something new possible?
2794840	2801560	Yeah, totally it makes something new possible. You look at the way that AI can make
2801640	2809560	new sounds. It can do error correction of sound where there is an error or make new sounds or
2809560	2815320	make new faces by doing an averaged composite of all faces that look similarish. You say, well,
2815960	2821480	could people express huge numbers of people, express their sentiments about something and
2821480	2827240	have the AI actually come up with something that is like a weighted average of all of those as a
2827240	2834840	form of proposition creation and then could we use distributed methods of proposition advancement
2834840	2839560	that didn't exist when we had to meet in a town hall and ride a horse from that town hall to
2839560	2843000	the other ones and we haven't innovated the structure of government since we had to ride horses.
2844600	2850200	Like, why do we think this particular thing is the best thing? Well, because the other things,
2850200	2854600	the last time we had that conversation seemed dreadful, at least that was the winning narrative.
2855560	2861480	But totally new things that are not just those previous things are possible.
2864680	2869400	What I would say is someone should not assume that the moment we say maybe there's a problem
2869400	2876680	with capitalism that we're instantly going to turn into Stalinism. But to say, let's make sure
2876680	2879960	we studied that history well enough to know what was wrong with those ideas and we don't do that,
2879960	2886360	yes. But let's also do the critique of the system and not just end with the critique,
2886360	2890840	but take it as a design criteria to say what would a better system look like and have we
2890840	2895880	got all the design criteria? Do we have the critiques of the communist system and the socialist
2895880	2900920	system and the capitalist system simultaneously? And then can we take all those as design criteria
2900920	2905320	and work on a fundamentally better design that might not look like any of those isms that utilizes
2905320	2909080	new technology, which means new possibilities that didn't exist before with new forcing functions
2909080	2915320	that didn't exist before? I think you're also saying, Daniel, that these kinds of challenges
2915320	2921000	do actually have comprehensive solutions. And I think there's quite a lot of people who deep down
2921000	2928520	have very, they doubt that that's actually possible. So whether they should have
2928520	2932840	haven't even tried hard enough to have that doubt mean anything. It's just an emotional default.
2933720	2937880	That was the other question you asked is, are we hitting the limits of cognitive complexity?
2937880	2944200	That is such a shit answer if you haven't actually applied the full limits of human
2944200	2950200	cognitive complexity and seen that we're failing. So we're not even trying. China's trying and they're
2950200	2958040	doing fucking amazing. In the US, we have no high-speed trains. None. None. In the time that
2958040	2961480	they've existed, China's been exporting them all around the fucking world in that same amount of
2961480	2969320	time. But a system that doesn't have term limits and that doesn't have a two-party system where
2969320	2974920	we just use all the energy wasted as heat fighting each other and then whatever you do for four years,
2974920	2979240	the other people undo for four years and nobody invests in anything with longer than four year
2979240	2984280	timelines because it won't get them reelected, that system is just stupid. That's going to fail to a
2984360	2992920	system that can do long-term planning. So if we say, okay, let's imagine just hanging out in the 30s
2994360	2998440	and saying, we got to figure out how to split an atom. No, not just split an atom. We're going to
2998440	3004040	figure out how to split an atom and deliver that as a warhead on a rocket to some other place with
3004040	3008120	some decent precision. In fact, we're going to go beyond that. We're going to use uranium to fission
3008120	3015880	something and split it to then drive nucleons into a fusion. It would be easy to say, well,
3015880	3019720	there's no fucking way. We don't have the cognitive complexity to be able to split atoms. We don't
3019720	3025160	even know what an atom is. But the Manhattan Project was a very serious investment in cognitive
3025160	3029880	complexity and we got everybody there. We got all the best thinkers in the world there. We put the
3029880	3037560	budget on. Are we doing that? Are we even fucking, where we got von Neumann, we got Turing, we got
3037560	3044200	Feynman, we got Oppenheimer, we got all those folks in Bletchley Park and in Los Alamos. Where is
3044200	3049400	the equivalent of that thing outside of very narrow areas of military? Which is why we have a dope
3049400	3052920	military. We have an awesome military, but that's innovation in military. That's not
3052920	3058920	innovation in the social technology of governance itself. We actually have to not just innovate
3059000	3062440	our military, but innovate the social technology of governance for a participatory
3063880	3070760	governance system. And this is why we come back to the, there's this quote that I always forget,
3070760	3075880	so a paraphrase of George Washington's that said something to the effect that the number one aim
3075880	3079800	of the federal government has to be the comprehensive education of every citizen in the science of
3079800	3085320	government. And science of government was the term of art. And I think it's so profound that he did
3085320	3091000	not say the number one aim of the federal government is to protect its borders. And he did not say
3091000	3097160	the name of the federal government is to protect rule of law. Because you can do rule of law
3097160	3102840	effectively with a police state. And you can protect the boundaries fine with a military
3102840	3109880	dictatorship. But they won't be democracies. If it's going to be a democracy, then democratically
3109960	3116040	the people will probably decide to protect their borders and to engage rule of law.
3116600	3121560	But if the number one goal is anything other than the comprehensive education of all citizens,
3121560	3125240	and the education was considered both a cognitive education and a moral education,
3125240	3128600	the way they described it, which is the kind of civic virtues that people are willing to
3128600	3133720	give something for the larger system that they also receive benefit from. And they're actively
3133720	3141720	participatively engaged. So that's the thing we need to be innovating in right now, not just
3141720	3147800	innovating in military while turning it into a some kind of autocratic or kleptocratic system.
3148600	3155560	But how do we apply the new digital and other exponential technologies to be able to both direct
3155560	3161720	the exponential technologies well, so that they don't cause existential risk, and in a way that
3161720	3169000	is aligned with the actual values that we care about as a people. And so then the core question
3169000	3173080	comes, what is a successful civilization? Well, it's one that doesn't fail, but that's not the
3173080	3178440	only criteria. It's one that doesn't fail and that maximizes the possible quality of life for
3178440	3182920	everybody in perpetuity. And then we have to find what is quality of life me, right? So these
3182920	3188440	there's like core existential questions of what is a meaningful human life to be able to design
3188440	3192760	a civilization that is optimizing for that, which is culture, right? Which is why we have to have
3192760	3196920	innovation in culture. Which is why I talk about that there's a cultural renaissance, a cultural
3196920	3202600	enlightenment that is necessary right now as the basis of the creation of these new institutions that
3202600	3206840	can solve the extra problems, because our current problem solving mechanisms can't solve them.
3207400	3211080	Which is why they're not being solved. We have to develop new institutions that are capable of
3211080	3214440	solving these types of problems, these types of complexity. But if those new institutions are
3214440	3218840	created by a few people that get it and impose them on force, then it's some kind of autocracy.
3219720	3225480	So they have to be created by people who want them and are willing to participate with them
3225480	3230600	and capable of participating. That is the cultural enlightenment that has to be the basis of it,
3230600	3236360	which and of course, there's a recursive process of some people engaging in that to then build
3236360	3241720	systems that in turn engage more people in it. So you get a virtuous cycle between cultural evolution
3241720	3249080	and social evolution, employing physical technologies, binding physical technologies,
3249080	3255720	and advancing them for the right purposes. It sounds like we really need a new forms of
3255720	3260280	wisdom education. And obviously I'm glad to say that we've got a Zach Stein coming on the podcast
3260280	3264360	very soon to discuss that very question. And obviously what you're saying, Daniel,
3264360	3268840	big implications for how we think about the university in the current situation.
3269800	3273720	But I'd like to hand over to Sam. I know Sam's got a burning question, so please, Sam.
3275160	3279160	Yeah, hi, Daniel. Yeah, I've got a couple of burning questions, but I'll go with one to start.
3280680	3285080	It seems like with the problems we're facing, they often, as we talked about,
3285080	3289240	they happen at a certain area. So for example, climate change is here already,
3289240	3292680	and it will exponentially grow out. And that's one of the issues that I think we're kind of
3292680	3296760	alluding to, that when there's not the immediate threat of World War II, for example, it's quite
3296760	3304440	hard to galvanize a whole group of people to solve a problem. But do you think that we think about
3304440	3310200	solutions in the same way to the logic of problems in terms of Silicon Valley out,
3310200	3315720	or it will happen in this certain area and slowly filter out? There's that quote,
3315720	3321640	the future is here, it's just not that evenly distributed. And that's quite a worrying logic
3321720	3326120	if we're thinking about the magnitude of exponential risk. And do you think that we're
3326120	3332200	then following the logic that we apply for problems that they happen and exponentially grow out?
3332200	3336040	And is that useful or harmful when we're thinking about solutions that need to
3336760	3341160	really permeate around the whole globe and not leave anyone behind?
3343800	3348440	I'm not sure that I understand the question yet. You were using the example of climate change and
3348440	3352280	saying it's already here, but because it doesn't look like an agent in a way that we
3352920	3357560	evolve to understand as an immediate threat, we don't respond to it appropriately. But that
3357560	3363880	it's already here, it's expanding in a way that maybe we don't respond to appropriately. And
3363880	3368600	you're wondering, is that the case with all of the risks? There's already AI happening that is risky,
3368600	3371400	and we're just not responding to it appropriately? Or was the question different?
3372440	3376680	Yeah, sorry, Daniel. The question was slightly different. So that's how we understand
3376680	3382920	issues like climate change. And we often talk about solutions in a similar way to
3383560	3389400	that issue of climate change, i.e. there'll be an innovation in a certain part of the world.
3389400	3394680	So the solution is already here. And then it slowly permeates out. And then eventually everyone
3394680	3398680	will have it. So you took the example of high speed trains, they are already here. The solution is
3398680	3404120	already here. It's just not that evenly distributed. And do you think that that follows the logic of
3404200	3410280	where we think about things like climate change, where it happens in a certain area and slowly
3410280	3416520	distributes out? Whereas with solutions, they need to be get around everyone very quickly.
3416520	3422440	And they can't work in that logic of slowly from one center expanding out.
3423560	3431240	I understand that. I don't think it's fair to say the solutions are already here and
3431240	3436920	unevenly distributed. It's true for some things. Obviously, we already have a solution to
3438440	3442120	caloric abundance, but it's not evenly distributed because there's extreme poverty.
3442120	3445960	That's an example. And that's one we've lived with for a long time. And we can see that it did not
3445960	3453160	actually pervade out well for certain reasons. To a certain degree and then not beyond.
3453800	3459080	And the same is still true for running water and hygiene and medicine. There's a very unequal
3459080	3464440	distribution of problems we have solved. I would say that many of the most critical
3464440	3469240	issues we need to solve, the solutions don't exist anywhere. It's not true that somewhere
3469240	3478040	has figured them out well. We actually have to do innovation. How do we solve global
3478040	3482600	multipolar trap issues is not solved anywhere. And that's the most central thing we have to figure
3482600	3489720	out. How do we create digital open societies? You can say that it's kind that there are some
3489720	3494680	places that are trying to pioneer like Taiwan and Estonia. That's true. But those are very far from
3494680	3498680	have really got worked out solutions that are adequate to all the other places in scale.
3504120	3509000	I think we have to acknowledge that many of the most critical solutions don't exist at all
3509000	3515640	and need to become the primary focus of innovation. And then where they do
3516200	3520840	start to develop, we have to say what type of governance and incentive landscape would be
3520840	3525480	necessary to get them everywhere they need to be in time. And who would have to be participating
3525480	3530120	to make that happen? And what kind of oversight and enforcement would be necessary to really make
3530120	3538360	it happen? We know in the US, the government making deals with Native Americans and then
3538360	3544840	not keeping them whenever it's inconvenient almost all the time. It's not just about did you
3545560	3549720	say when you developed a new technology that will get it to the world? Is there a method of
3549720	3554520	enforcement that will actually ensure that that occurs and that it occurs within time? That becomes
3554520	3564040	critical. I just had a follow up to that. We talked about how we understand problems and how we
3564040	3570120	understand solutions. Why do you think certain maxims are held in higher esteem than others?
3571000	3574520	In another podcast, you talked about survival of the fittest and how we've almost
3574520	3582200	fetishized that concept above all others. And how can we make sure that other maxims are
3582760	3588760	discussed in a kind of equal or more celebrated light? And is that there a logic that pervades
3588760	3597240	a lot of these more harmful maxims? Yeah, it's apologism. So if I win a war and we kill a bunch
3597240	3603000	of people that we call terrorists or infidels or some bad thing that makes them not human,
3603000	3607800	but what it means is we blew up a lot of civilians and a lot of women and kids and whatever it was.
3608520	3614520	But we got more land and resources and whatever it was out of doing that thing.
3615480	3620520	Survival of the fittest is a nice narrative to say that's how nature works and that's the way
3620520	3627960	that it should be. And it's actually the predators that keep the prey animals from eating themselves
3627960	3632120	into extinction and that drive them to evolve by eating the slow ones so that the good genes
3632120	3637640	kind of inbreed. And most people are like prey animals to the some more predatory humans that
3637640	3644440	cold heard and that kind of drive them who are otherwise kind of lazy eaters. Like that whole
3645160	3654360	ideology is apologism for whoever is winning at an extremely damaging rival risk kind of system.
3657000	3663640	Naive techno-capital optimism is one of the best examples of apologism of this kind where
3664600	3673480	like if you have a theory that criticizes capitalism, nobody who's winning at capitalism
3673480	3679560	who has the money is going to upregulate it. And if you are criticizing tech, nobody that was
3679560	3684040	winning at tech is going to say, yes, I like your idea of why I suck and I'm going to upregulate
3684040	3692360	that. So you realize that for narratives to catch on, somebody has to upregulate them and
3692520	3698760	there's cost associated in doing that and there has to be a motive associated with that cost. So
3700040	3704520	it's not just like the ideas that are the most true and the most beneficial proliferate. The
3704520	3711560	ideas that have the most agentric basis to drive them through the society are a lot of the ones
3711560	3718840	that proliferate. The idea which is often held up sort of as counterposed to survival of the fittest
3718840	3724920	is mutual aid, which is this idea that Peter Kropotkin proposed in the late 19th century and he
3724920	3731560	essentially saw out there in nature. Actually, it wasn't the species that competed most fiercely
3731560	3738360	that survived. It was those that actually cooperated that moved into a kind of a situation of symbiosis
3738360	3748120	if you will. So is that notion of mutual aid, is that a useful reference point for thinking about
3748120	3753720	these vaccines that need to inform how we move forward? How do we actually begin to have meaningful
3753720	3762120	productive conversations within the classroom or within the UN forum or within government
3762840	3769800	corridors of power? How do we begin to chisel away at the memetic sort of structures which
3769800	3781320	seem to reinforce that particular mindset? If we think through the wrong metaphors, we're obviously
3781320	3787720	going to come to the conclusions of those metaphors predisposed, but they're the wrong ones, then
3787720	3794680	there'll be the wrong conclusions. So what kind of animals are humans? Are humans predators?
3794760	3803320	Are we prey? Are we fungus? Are we slime molds? Are we the relationship between trees and animals
3803320	3809000	where we can see gas exchange? There's lots of different biological analogies we can try to use.
3809800	3825400	And none of them apply. So let's say we do the most popular one, which is that we're apex predators.
3828200	3835640	Pick an apex predator, lion, polar bear, and orca. Orca is maybe the best example, the biggest apex
3835640	3844600	predator in the ocean. Compare what an orca does to a school of tuna to what an industrial fishing
3844600	3853800	boat, a commercial fishing boat with a mile long griff net does. The orca misses almost every time,
3855080	3861080	and when it finally catches one, it catches one, right? And as there's less of them, it misses more
3861080	3869240	often. And we can pull up the entire fucking school in a net. We're not apex predators.
3870200	3875480	Apex predators can't do that. If a polar bear decides that it's super pissed off and wants
3875480	3880200	to go on a rampage and destroy as much stuff as it can, like, what's it going to do? And
3881400	3887720	look at human nuclear capability if we were similarly disposed. Like, wait, the idea that we
3887720	3893880	look at, that we don't factor the way that technology means that we are not like the rest of
3893880	3899560	nature. So of course, we need to see in nature, yes, there's some competitive dynamics and some
3899560	3905000	cooperative dynamics. This is true. Where there are competitive dynamics, there are mostly
3905720	3913160	cemeteries of power. The tuna get away as often as the orcas catch them, right? So the slow orcas
3913160	3918680	die, the slow tuna die, the faster of both happen. So the co-selective pressures have them both kind
3918680	3923240	of get better together. And so there's the symmetry of power, right? The orca is not a lot more
3923240	3929960	powerful than tuna in terms of that particular dynamic. And so we can see that if we were to
3929960	3934280	figure out some way to quantify all the interactions that were happening in nature, almost all of them
3934280	3939720	are symbiotic, right, of some kind. Some of them are directly rivalrous and competitive. And sometimes
3939720	3945000	it's kind of both, right? It's a place where the competitiveness at the one-to-one level
3945000	3949960	ends up leading to symbiosis at the species-to-species level. Obviously, both the predator and the
3949960	3954920	prey animal depend on each other. Predator dies, prey animal eats itself to extinction,
3954920	3960920	prey animal dies, predator serves to death. So micro rivalry ends up leading to macro symbiosis
3960920	3965960	because of the symmetry of power thing, right? So we can see that there are certain types of
3965960	3969560	competition, but they're limited, the symmetry is power, and then there's a lot of symbiosis.
3973560	3980680	Well, as soon as humans started making tools, we were able to hunt any species to extinction
3980680	3984520	anywhere and go become the apex predator in any environment and more powerful than the apex predator
3984520	3992760	in any environment. We broke the symmetry, right? We became more lethal predators faster than the
3992760	3998120	environment could evolve to become more resilient to it. As a result, that was the beginning of an
3998120	4002680	extinctionary process that was following an exponential curve that was slow for a long time
4002680	4006040	from stone tools and started to really pick up with agriculture, then really pick up with the
4006040	4012360	industrial revolution is now verticalizing in modern tech world. But stone tools were kind
4012360	4018440	of the beginning of it, right? And the other stone tools and language and that type of coordination
4018440	4029400	that came along with the abstraction capacities. So do humans need to ensure, as the metaphors of
4029400	4034520	nature go, that where we have competition, that it's symmetrical and that it's constrained
4035320	4040200	and that the micro competition really does lead to macro symbiosis? We need to ensure that. This is
4040200	4049080	true. Is the competition between Facebook for your attention and you for your attention symmetrical?
4049960	4053880	No, of course not. Well, you say, well, there's a competition, the competition
4054440	4059480	between supply and demand is symmetrical because there's an equal number of dollars flowing from
4059480	4067080	demand to supply. Bullshit, right? The demand side is not coordinated. The supply side's coordinated.
4067560	4072600	And so even though there's a total symmetry and aggregate, there's not a symmetry of coordinated
4072600	4078120	capacity because it isn't Google against all Google users as a Google user labor union
4078840	4083640	that is also applying similar exponential technologies to bind this thing. It's Google
4083640	4087560	against one person in terms of the person didn't think that they were about to spend the next
4087560	4090920	three hours on YouTube and now they do, which is better for their advertising model, not necessarily
4090920	4097880	for your life. That kind of, and so then you can have supply side driving manufactured demand.
4099240	4104280	Well, now there's not real, market ideology is broken now. That's not a market ideology,
4104280	4109960	was that there was a thing called demand that was foundational, that people wanted real
4109960	4113640	shit that would improve the quality of their life and that created an environmental niche for supply
4114520	4119640	and the rational actors would buy the product or service amongst all of them at the best price
4119640	4123960	that would drive innovation. Well, the moment supply started to get much bigger than demand
4123960	4128920	because of coordination, it realized that it could manufacture supply and the humans weren't
4128920	4134440	all that rational, all the behavioral economics. And now the entire logic of markets is broken,
4135080	4139400	right? Like market theory is broken with manufactured demand and radical asymmetries
4139400	4146200	on the supply side. Okay, that's important to know. And so if you go back to the nature example,
4146200	4150200	where there's competitive forces, do they need to have symmetries in order for them
4150200	4155560	competition to lead to symbiosis as a whole and metastability of the ecosystem? Yes. If you bring
4155560	4161000	something in that is not symbiotic with the rest of it, you get an invasive species that can destroy
4161000	4169080	a whole ecosystem, right? So we should study biology where we're not trying to compare ourselves to Apex
4169080	4173240	predators or slime molds or whatever. We could just study general principles of things like
4173800	4179640	cooperative dynamics and competitive dynamics and metastability. We can kind of get a sense of that.
4179640	4182920	What is needed for metastability? And then say, how does that apply in the human world? But it
4182920	4186760	will be different. It'll be very different. The rest of the animal world is not forecasting the
4186760	4193720	future and making game theoretic decisions based on forecasts of the future. And so this is why,
4193720	4197960	like complexity theory, where we model us as termites is silly, like we don't behave like
4197960	4204040	termites. So it's not that it's useless, but it's profoundly inadequate as a set of metaphors. So
4204040	4211880	we have to recognize our human's part of nature, of course. Is there a distinction between humans
4211880	4216760	and the rest of nature that is fundamental in type? Maybe it was just a change of quantity of
4216760	4221080	neurological complexity that crossed a threshold that became a change of kind, but it is a change
4221080	4228760	of kind. And so we will have to have fundamentally different metaphors for thinking about that,
4228760	4232680	which is why it makes sense to just think about the problem space and make sure that
4232680	4237240	you understand the problem space well and that your solutions are aligned with the problem space.
4239800	4248920	Yeah, yeah. What an exciting research agenda. And of course, an agenda to live by as well and to
4248920	4253560	engage with deeply. And another maxim comes to mind, perhaps, which would be know thyself.
4254600	4261000	It's not just a situation of impersonal inexorable forces bearing down on us, but we're also talking
4261000	4267160	about systems of, I think, human intentionality, which raises the crucial issue which we discuss
4267160	4274600	with Forest Landry in an earlier podcast on how do we make good choices, which perhaps our education
4274600	4279640	systems are not really equipping us with the tools we need to answer that really important
4279640	4285320	question. I know that Zoe's got a question. I want to hand this over to Zoe. So go for it, Zoe.
4286040	4291000	So I kind of building on sort of the meta, we have the wrong metaphors, I guess, we're using
4291000	4297800	the wrong ways of thinking. I kind of wanted to know how do we deal on like, on a societal and
4297800	4301880	a personal level with the amount of cognitive dissonance I think we're existing in, because
4302520	4307880	I think part of the difficulty with coming up with solutions is that some of the challenges
4307880	4312520	are so overwhelming that I feel like majority of people just kind of stick their head in the sand
4312520	4316440	and they're like, no, and so we're existing in like, I feel perpetual cognitive dissonance.
4316440	4322280	And I was kind of wondering what your take on that was and how, yeah, how do you deal with it
4322280	4328680	personally and how does a young person who's trying to sort of move forward in society deal with
4328680	4334600	that as well without, and, you know, exist as a functioning member of society without sacrificing
4335160	4338840	maybe personal ethics and values, even though I kind of, I guess I know that I'm going to have to
4338840	4350600	compromise somewhere down the line.
4359240	4364680	You as an individual probably can't solve those issues,
4365560	4370600	probably not one of them, let alone all of them.
4373480	4380440	And you can't focus on it and really look at it and feel the scope of the current harm and the
4380440	4388680	possible harm and not be able to do anything about it and have continuing to look at it make any sense.
4389240	4402680	So let's say that our social institutions were adequate, as they were at previous points, to deal
4402680	4410040	with whether they were adequate or not depends upon which group you were a part of and which
4410040	4414840	problems you were looking at. But let's just take for a moment that for some things they were adequate.
4414840	4420760	But then if there was a problem you really wanted to solve, you could think about joining
4422280	4430440	the CDC to work on pandemics or joining the military to work on terrorist-mediated
4430440	4433960	ex-risk or joining an intelligence group or whatever it was.
4438360	4443720	If you look around and you see that the scale of the issues requires institutional solutions,
4444680	4450840	whether they're state or network-based decentralized autonomous organization or whatever,
4450840	4455400	but collective intelligence of lots of people, not just a person.
4456680	4461000	And you don't see anyone that is currently doing that, then there isn't something you can join,
4461000	4469160	then what do you do? It's a tricky problem because there's a fairly small number of people that have
4469160	4473320	the right psychological disposition to try to found something of that type.
4475560	4483400	There are a few people who are either going to try to start a new type of company or a new type of
4484280	4487240	non-profit or a new type of social movement or whatever.
4492680	4496200	There's a lot of people that can contribute value to one of those that are probably not
4496200	4500520	going to found it for really not just developmental but typological reasons,
4500520	4502360	different typologies or into different things.
4507160	4510360	So to the degree that there are particular issues you care about
4512280	4516360	and you can find organizations that are doing a pretty good job that you could
4516920	4522120	join or participate with, that's a good answer. It's something of an answer.
4526200	4530520	To the degree that you feel like you have the typological orientation to make a new thing
4530520	4535480	or to be part of make a new thing, to find other people that could co-found some kind of new process,
4535480	4540120	whether it is trying to get an upgrade to an institution with existing government,
4540120	4546280	build a new institution, build Ethereum, some kind of platform for decentralized
4546280	4550360	autonomous organization that maybe will create the future of governance via networks rather than
4550360	4557720	nation-states. Those are all possibilities for, is there some new capacity that I believe
4558440	4560920	is needed that I could help to bring into being?
4564680	4571240	So either you have to join something or you have to make something or you have to join
4571240	4575400	people that are interested where maybe somebody in that scene or some combination of them will
4575400	4586600	be able to make a thing. And it's very hard to know that the thing that you're focused on,
4591080	4595000	even if it's awesome, is not adequate to the scope of issues you're aware of
4595000	4601880	and put all your energy into it and not go nihilist or just anxious all the time.
4605800	4616520	So for a lot of people, I would say they should put their sense-making into things that they feel
4616520	4623000	like they have agency in or could develop agency and that there's some relationship between their sense-making
4623000	4632360	and their agency. So let's say they feel like, okay, well, I don't know how to fix AI, AGI risk issues.
4633240	4639000	Silicon Valley attentionalism issues. But I feel like if I apply my sense-making to the problems
4639000	4643560	in my community, I could actually help improve the quality of life of my community. I could
4643560	4648680	bring warm data labs there and have the people start really getting to know each other in a
4648680	4654200	multi-contextual way much better. I feel that kind of thing. If a lot more people did that,
4654200	4657640	they paid attention to where they could have agency, applied their sense-making there,
4658440	4664360	a lot of problems would get better. And a lot of other people in those communities would evolve
4664360	4669000	to want to do things as well. And some of them would have different aptitudes and people communicating
4669000	4675000	better would have better collective intelligence. And once you solve problems at one scale, you get
4675000	4678440	better at problem solving. You might be like, maybe I can do this for a second community. Oh,
4678440	4682200	I've just figured out a generalized principle. Maybe I can help create a way to do this for
4682200	4688680	communities writ large. All of a sudden, it starts to be able to kind of inductively scale to the
4688680	4697560	scope of the problems. So one thing I would say is like one approach is just try to understand
4697560	4701240	what the world needs without understanding what you can do. Just take you out. Just what does
4701240	4705720	the world need? Because as you come to understand that better, you'll start to have insights of
4705720	4710840	what needs to happen. And then you'll at least be able to parse where are the places doing closest
4710840	4715240	stuff? What is nobody doing? How do I help make that happen? Right? That's one approach.
4716600	4720840	The other approach is what is the stuff I feel like I could do and how do I apply my study to be
4720840	4725640	able to do some of those things where then in the process, I can be increasing my agency
4726520	4731400	to then possibly be able to converge towards doing more stuff. Both of those are valid
4733800	4735480	on their own and in combination.
4735880	4745080	What I would say is that you're increasing your understanding of the world, that you're increasing
4745080	4752200	your sense of your ability to act meaningfully, and that you're increasing both the depth of care
4752200	4758120	and the emotional resilience in the presence of that care simultaneously are things you want
4758120	4762120	to be tending to. There's not one good answer for how to do that, but they're things you want to be
4762120	4771800	tending to. Now, we spoke briefly before the recording started about the very real practical
4771800	4778840	inquiry of what kind of jobs are there in the space. And it's if, let's say, working in existential
4778840	4783080	catastrophic risk are some of the most important areas in the world and pioneering new types of
4783080	4790760	social technologies that both apply and combine physical technologies. If these are some of the
4790760	4796360	most important areas, but there's not really jobs, there's not financial incentive there.
4796360	4801160	And as you're focused on them, there's more emotional difficulty and psychological difficulty
4801160	4807480	associated with looking into the abyss. The incentive landscape is wrong for getting the
4807480	4813720	people engaged in the things that matter. So institutionally, we should try to fix that
4813720	4817880	and say, how do we start to put incentive on the things that matter the most, which the Manhattan
4817880	4823960	Project did, which is why I'm calling for Manhattan Project type things. And in some ways, you can say
4825480	4830920	Ethereum and Holochain and other orgs are trying to do that. So maybe some of Elon's
4830920	4834280	companies, whatever, we're taking on a problem and we're trying to be able to create a lot of
4834280	4837560	jobs and incentive to get people to be able to work on problems that matter.
4837960	4854520	But the other part of that answer I'll just share is for me, a big part of, because I was thinking,
4854520	4857640	like there's a lot more people thinking about X-Rest now, but I was thinking about it from
4857640	4863240	quite young age. I just knew I couldn't focus on anything else and I couldn't focus on anything
4863320	4867080	that wouldn't converge to being adequate. It was okay if what I was working on wasn't
4867080	4871400	adequate. It just had to seem like it was on a path of increasing understanding and capacity that
4871400	4882440	could maybe converge. So I kept, for most of my life, my overhead as close to nothing as I could
4882440	4888040	keep it and figured out things that I could do for work that took the least amount of time possible.
4888120	4895960	So most of my time didn't have any market need on it. Most of my time was self-directed study in
4895960	4900840	these areas because that was the only thing I could actually do and be congruent with myself.
4900840	4905960	So sometimes I did construction to pay the bills, sometimes I did teaching or I became a therapist
4905960	4909640	and did different things, but I kept my bills low enough that it didn't take that many hours.
4909880	4916040	And so most of my time could just be allocated based on my intrinsic
4917400	4921240	orientation of what would be most meaningful, which I highly recommend that path.
4927000	4931080	Brilliant. Well, thank you, Daniel. I think we're rolling to a close.
4932200	4937880	We've covered a lot of ground. It's been really an exciting conversation. I hope we'll have a chance
4937960	4944200	to continue this another time. It does seem that we are in something of quite a sort of
4945240	4950280	incredible moment, possibly a unique moment. We're facing a lot of daunting challenges
4951480	4955800	and we're all trying to grapple with what that means, I guess, for us personally, professionally.
4957080	4964520	For me and in the seminar room, in the university, in society, in my interactions with my loved ones.
4965240	4973080	But I guess it's also, in some ways, it's a time of opportunity as well. It's kind of a
4973080	4977720	cool to adventure, as you sort of said, is there anything more important than really sort of
4978360	4981400	putting your shoulders to the wheel on some of these issues that we've addressed?
4982440	4987640	And yeah, I'd just like to say thank you for all of your work. And I don't know if you have any
4987640	4991400	final closing thoughts, anything that we haven't covered that you'd just like to share with us,
4991400	4997080	to close? It is a thought that comes to mind, kind of following where we just were.
5000280	5009400	One way I think about how to live a meaningful life, a simple but kind of elegant model is
5013320	5018920	we can think about life in terms of the mode of being, the mode of doing, and the mode of becoming.
5019160	5030440	And if you were to describe the mode of being, it is, in the moment, focused on appreciating
5030440	5037480	what already is, appreciating the beauty of life as it is. The mode of doing occurs in time,
5037480	5042680	and it's focused on adding beauty to life. If it's focused on anything else, it's not the
5042680	5047720	mode of doing very well, right? Most people are in the mode of doing, doing shit that if they
5047720	5053000	didn't do it, the world would be better. But the mode of doing that matters for a meaningful life
5053000	5060360	is adding beauty to life and or protecting, serving the beauty that's there. The mode of
5060360	5066680	becoming is increasing your capacity to appreciate life as it is more fully, and to add to beauty
5066680	5073000	more fully, right? Increasing being and becoming and being and doing. So then there's a virtuous
5073000	5079560	cycle between those. But the doing only matters and the becoming only matters because of the
5079560	5085560	intrinsic meaningfulness of being. If ultimately the meaningfulness is grounded in experience,
5085560	5089960	and the fact that experience is just intrinsically beautiful, that taking reality is intrinsically
5089960	5095880	beautiful. So if you, because of the crises, you don't focus on that enough, you'll actually get
5095880	5102040	disconnected from the source of what matters. And then your, your motivational complex will,
5102760	5111800	if I, if I wake up, so like, I wake up, I go sit outside with a cup of coffee and I look at the
5111800	5120040	trees. And I just love watching the trees move in the wind and the clouds in the sky and just
5120040	5125480	like how beautiful this planet is, how much I appreciate it. And there is a fullness in that
5125480	5130600	mode of being that doesn't need anything. So then I'm not motivated based on what's in it for me,
5130600	5135080	because I already feel like I could die right now. And I feel lucky, right? I feel like I have lived
5135080	5142200	a really rich full life. So now it's not what's in it for me. It's not some doing that I have to do.
5142200	5148200	It's that I actually want to protect that beauty. And I want to protect other people's ability to
5148280	5155320	keep experiencing it forever, or at least for a long time, because I can. And because as much as I
5155320	5159560	appreciate it, other people do too, or can and I, that matters to me, right? Like it's intrinsically
5159560	5166120	meaningful. But that's a different come from it. It has a certain anxiety and angst and feverishness
5166120	5173880	that isn't there. And it has a sacredness that is there. And then there's also a courage of like,
5173880	5180600	maybe I fail. I mean, maybe we fail, right? And life has been meaningful each moment. It's not
5180600	5184120	like it wasn't meaningful like it. Okay, maybe the whole thing comes, this whole thing, part of it
5184120	5189160	comes to an end at some point. But I will do what I can to be in service to it. But that service is
5189160	5193640	arising out of seeing it and loving it as is, and then wanting to be of service from there.
5196680	5202520	So I can be in the mode of being just kind of chill and watching TV. I can be in the mode of
5202520	5206120	doing doing a bunch of to do this shit that doesn't really matter. I can be in the mode of
5206120	5213640	becoming trying to get better at doing shit that doesn't matter. I want to think about am I engaging
5213640	5219000	in each of those modes? And am I engaging in them deeply? If I'm in the mode of being, I want to be
5219000	5223160	looking at the sky and I want to be listening to music, I love and be wrapped. I want to be
5223160	5227720	feeling moved by the beauty of life. So why do the mode of being any other way? I want to be with
5227720	5233800	friends that I love where I'm like, yeah, I could die right now full. And in the mode of doing,
5233800	5239960	I want to know that the world would be worse if I didn't do this. Otherwise, I go back to the mode of
5239960	5246440	being, just chill and enjoy it. I want to know that the thing I'm doing adds something of meaning
5246440	5253160	somewhere, right? And the mode of becoming of am I am I developing my ability to appreciate
5253160	5258760	everybody and everything around me? And am I developing my knowledge and agency and capacities
5258760	5266120	to add to it? That's a good framework to think about, you know, when you inventory your day and
5266120	5272840	your week, what being on track means. Wonderful. Well, thank you, Daniel. Thanks so much. And
5273400	5279240	if people want to engage more with you and your work, your website is called Civilization Emerging.
5279800	5284280	Is that correct? Civilization Emerging is just like a personal blog where there's some podcasts
5284280	5289080	and old stuff up there. And you can check it out. And the project that we're focused on that's
5289080	5294040	really just in the earliest beta phase right now. But that is kind of the project where we're
5295800	5301400	trying to bring the information forward that will help decentralize innovation of what the new
5301400	5306360	social technologies that can employ and guide exponential technology are. That project is
5306360	5310680	called the conciliants project conciliants project.org. And that will get increasingly
5310680	5316360	interesting over the next, you know, few months. Yeah. And I'm hoping that we'll have a conversation
5316360	5321400	about how I contribute to that project. Super exciting. And I hope people will go and check out
5321400	5327800	that website. Thank you, Daniel. Look forward to picking this up again at some point. Take care.
5328520	5330200	Thank you. It was good to be with the three of you.
5330840	5342360	Thanks for tuning into Imperfect Utopias. To get access to all of our content and to stay up-to-date
5342360	5352600	with future Zoom calls, workshops and events and more, check us out at ucl.ac.uk forward slash
5352600	5359480	global dash governance. If you like this content, please do leave us a comment and subscribe.
5360200	5367080	Till next time.
