1
00:00:00,000 --> 00:00:10,360
Hi, and welcome to Imperfect Utopias, based out of the UCL Global Governance Institute.

2
00:00:10,360 --> 00:00:17,400
This is a podcast about the challenges facing humanity and possible global responses.

3
00:00:17,400 --> 00:00:22,840
If you're new to the show and you want to get a list of our favorite books, other resources,

4
00:00:22,840 --> 00:00:35,920
and to pass shows, and to join our community, go to ucl.ac.uk forward slash global dash governance.

5
00:00:35,920 --> 00:00:40,520
We're really delighted to have Daniel Schmacktenberger on the podcast today.

6
00:00:40,520 --> 00:00:46,560
Daniel's a social philosopher and co-founder of the Consilience Project, a non-profit media

7
00:00:46,560 --> 00:00:52,320
organization that aims to capitalize a cultural movement towards higher quality sense-making

8
00:00:52,440 --> 00:00:54,680
and democratic dialogue.

9
00:00:54,680 --> 00:00:59,200
Underpinning much of Daniel's work is the conviction that strengthening individuals'

10
00:00:59,200 --> 00:01:05,840
abilities to handle and filter information is now a civilizational imperative in a context

11
00:01:05,840 --> 00:01:08,120
of existential risk.

12
00:01:08,120 --> 00:01:12,920
I've been following Daniel on the podosphere for some time now, and he's definitely one

13
00:01:12,920 --> 00:01:19,360
of the people I check in with most often when it comes to trying to make sense of what's going on.

14
00:01:19,360 --> 00:01:24,000
Daniel's work has also been a major inspiration for this podcast.

15
00:01:24,000 --> 00:01:26,800
So we're super excited to have you join us today, Daniel.

16
00:01:26,800 --> 00:01:29,440
Thanks so much for making the time.

17
00:01:29,440 --> 00:01:30,440
Happy to be here.

18
00:01:30,440 --> 00:01:32,240
Thanks for inviting me.

19
00:01:32,240 --> 00:01:36,200
And we're going to type in lots we could explore here.

20
00:01:36,200 --> 00:01:40,120
But first, I just ask our pod crew to introduce themselves.

21
00:01:40,120 --> 00:01:41,040
So my name is Sam.

22
00:01:41,040 --> 00:01:45,400
I handle the audio and video and hopefully some of the thinking when out of time.

23
00:01:45,440 --> 00:01:45,960
I'm Zoe.

24
00:01:45,960 --> 00:01:50,520
I help with some of the research and more of the admin and social media side of things.

25
00:01:52,800 --> 00:01:55,880
OK, so Daniel, let's get straight to it.

26
00:01:55,880 --> 00:02:03,720
In a world of, as the UN recently put it, certain near-term, nonlinear change.

27
00:02:03,720 --> 00:02:12,480
How well prepared are we to face some of the existential challenges and not just to say

28
00:02:12,480 --> 00:02:18,320
the natural ones that people might think of like asteroid strikes and those sorts

29
00:02:18,320 --> 00:02:24,760
of risks, which are certainly present, but possibly remote, but perhaps most challenging

30
00:02:24,760 --> 00:02:25,280
of all.

31
00:02:25,280 --> 00:02:33,440
And as we've discussed in some length, the human induced or anthropogenic existential

32
00:02:33,440 --> 00:02:41,360
risks like nuclear, but also biotechnology and, of course, climate change, which arise

33
00:02:41,360 --> 00:02:47,240
out of these kind of complex interactions of human and non-human systems, and which

34
00:02:47,240 --> 00:02:53,160
are, I'd say, already defining our times this decade, this century.

35
00:02:53,160 --> 00:03:01,520
And perhaps put a little bit more meat on the bone as to where we might go with this discussion.

36
00:03:01,520 --> 00:03:09,480
We've talked before about how there is a risk that faced with major disruptions, our societies

37
00:03:09,520 --> 00:03:17,320
could potentially default in the direction of either authoritarian oppression or even

38
00:03:17,320 --> 00:03:24,080
chaos, and that some people do think that this is the direction of travel currently.

39
00:03:24,080 --> 00:03:32,920
So how can we best avoid, what can we do now to avoid that kind of dystopia by default?

40
00:03:33,360 --> 00:03:41,240
As our legacy structures, systems, governance systems, as they falter, and there is that

41
00:03:41,240 --> 00:03:44,400
risk of major events sort of overtaking us.

42
00:03:48,240 --> 00:03:52,640
All right, that's a few good questions.

43
00:03:52,640 --> 00:03:59,800
The first question you asked is, how well prepared are we to deal with the existential

44
00:03:59,800 --> 00:04:07,160
and catastrophic risks that are impending or at least have a non-trivial chance of happening?

45
00:04:07,160 --> 00:04:12,040
And you were mentioning that this is a frame that is recognized by the United Nations now.

46
00:04:12,040 --> 00:04:18,120
If we think of the UN as a starting place to answer this, in terms of the closest thing

47
00:04:18,120 --> 00:04:25,160
to something like global governance or an intergovernmental organization, obviously it

48
00:04:25,480 --> 00:04:29,320
was created after World War II in the recognition that nation states by themselves

49
00:04:30,680 --> 00:04:33,320
weren't an adequate governance system to prevent world war.

50
00:04:34,200 --> 00:04:41,800
And now that we had weapons such that the wars between the major powers could never be fought in

51
00:04:41,800 --> 00:04:48,360
one anymore, we had to figure out a whole new world system to do something different than we

52
00:04:48,360 --> 00:04:52,600
had ever done in the history of kind of like the post-empire world, which was how do you have

53
00:04:52,600 --> 00:04:58,360
the major empires not fight wars? And we don't have a very good historical track record of that,

54
00:04:58,360 --> 00:05:01,960
but then we got to the place where you had weapons where the wars couldn't be won.

55
00:05:01,960 --> 00:05:09,400
And that was a different logic. So I point this out because catastrophic risk that was human-induced

56
00:05:09,400 --> 00:05:13,960
before World War II was different and kind than after World War II because we didn't have any tech

57
00:05:13,960 --> 00:05:19,080
big enough to actually create global catastrophic risk from human action.

58
00:05:19,320 --> 00:05:24,680
That doesn't mean that catastrophic risk is not a part of our history, it was just always local.

59
00:05:25,560 --> 00:05:33,080
And not only was it a part of our history, it was what happened most of the time in the

60
00:05:33,080 --> 00:05:38,680
history of the life cycle of civilizations. So we can see that, you know, if we're studying

61
00:05:38,680 --> 00:05:43,880
the Mayan Empire, the Inc in the Aztec, the Egyptian, the Roman Empire, like one of the first

62
00:05:43,880 --> 00:05:48,520
things that we recognize is that they all don't exist in the forms of their dominance anymore.

63
00:05:48,520 --> 00:05:55,160
They all either had sudden collapses or gradual collapses, but the collapse of civilizations is

64
00:05:56,760 --> 00:06:00,680
the things that we call civilizations is one of the kind of most prominent features

65
00:06:00,680 --> 00:06:07,000
that we can see in history. And so you can see that people faced an existential risk to them

66
00:06:07,000 --> 00:06:11,080
from in the form of a warring army or over consumption of their resources or internal

67
00:06:11,080 --> 00:06:14,520
dissent that was enough that it broke their capacity to continue to coordinate.

68
00:06:14,680 --> 00:06:21,560
It's just those were always local issues, maybe a large locality if it was a large empire.

69
00:06:22,120 --> 00:06:30,360
And those were both for environmental externality reasons, like the first civilizations that

70
00:06:30,360 --> 00:06:33,800
over farmed and created desertification. It was a long time ago, right? That's an

71
00:06:35,000 --> 00:06:38,200
environmental overreach is a multi-thousand of year old problem.

72
00:06:38,200 --> 00:06:47,080
And short-term solutions regarding rivalry that in gender enmity, in the side of the other,

73
00:06:47,640 --> 00:06:52,040
and where they then reverse engineer whatever weapon innovation you had and come back and you

74
00:06:52,040 --> 00:06:58,520
just drive arms races, they're escalating. That's also a very old process. World War II is the

75
00:06:58,520 --> 00:07:04,440
beginning of us getting to the place where the scale of our warfare, and then also shortly

76
00:07:04,440 --> 00:07:10,520
thereafter, the scale of even our environmental externality hit a global catastrophic possibility.

77
00:07:11,960 --> 00:07:17,560
So you see that we created an entire world system following World War II to say, okay, we need to

78
00:07:18,760 --> 00:07:23,640
we now have such incredible power that we can't use in the way that we have previously. We need to

79
00:07:23,640 --> 00:07:28,440
steward that power differently. How do we deal with conflicts without war between the major

80
00:07:28,440 --> 00:07:34,920
superpowers? So the UN was created the World Bank, the IMF, that whole kind of intergovernmental

81
00:07:34,920 --> 00:07:41,080
organizations that would be able to broker nation state interests to have solutions other than war

82
00:07:41,080 --> 00:07:46,280
and the entire set of Bretton Wood agreements, Marshall Plan agreements for how we kind of

83
00:07:46,280 --> 00:07:53,080
rebuild the world where the nations would be so economically interdependent on each other through

84
00:07:53,080 --> 00:07:59,480
trade and globalism that it was more advantageous to them to continue to do trade with each other

85
00:07:59,480 --> 00:08:04,280
than to bomb each other. That was a huge part of it and where we could have so much growth

86
00:08:04,280 --> 00:08:09,080
of the economy that everybody's desire to get more could happen simultaneously without having

87
00:08:09,080 --> 00:08:13,800
to take each other's stuff. The idea that very, very positive sum GDP situations could keep us

88
00:08:13,800 --> 00:08:21,160
from going zero sum conflict oriented. Well, that very positive sum meaning extract resources that

89
00:08:21,160 --> 00:08:26,040
are unrenewable and turn them into trash much faster driving GDP for a very short period of time

90
00:08:26,040 --> 00:08:31,480
also meant we hit planetary boundaries. And so now we're seeing planetary boundaries both on the

91
00:08:31,480 --> 00:08:37,080
side of depletion of unrenewable resources and the waste side, both sides of a unrenewable

92
00:08:37,720 --> 00:08:43,400
linear materials economy on a finite planet, lots of different ones, right? We're not just seeing

93
00:08:43,400 --> 00:08:48,680
too much CO2, but too much plastics and ocean micro plastics and all kinds of things on the

94
00:08:48,680 --> 00:08:55,640
toxicity side and all kinds of things on the overfishing, cutting down to many old growth

95
00:08:55,640 --> 00:09:05,080
forests, soil, micro, diversity loss, microbiological diversity loss, etc. So you can't keep doing the

96
00:09:05,080 --> 00:09:10,440
positive sum thing in that same way that is based on the exponential growth of a linear materials

97
00:09:10,440 --> 00:09:15,560
economy on a finite planet. That's one part of the kind of post-World War II solution

98
00:09:15,560 --> 00:09:19,640
that's kind of run up against an end. The other thing is that that world system

99
00:09:22,360 --> 00:09:29,720
created a lot of fragility, right? Because when you have global supply chains where most

100
00:09:29,720 --> 00:09:34,120
in the of the products that we engage with now no country can make, they're made across six

101
00:09:34,120 --> 00:09:39,480
continents, this computer that we're talking on, this phone in my hand, when you factor all of

102
00:09:39,480 --> 00:09:45,240
the materials processing, the hardware, the software, the satellite infrastructure required

103
00:09:45,320 --> 00:09:52,680
for our communication to be happening. The positive side of getting the world very interconnected

104
00:09:52,680 --> 00:09:58,280
was that we were less oriented to war if we had dependence. The negative side of dependence is

105
00:09:58,280 --> 00:10:03,800
you can get cascading failures, right? If you get failures anywhere, then you can get failures that

106
00:10:03,800 --> 00:10:10,520
start to cascade. And we saw that with COVID, we saw that an issue in one province of China

107
00:10:10,520 --> 00:10:16,360
became a completely global issue affecting almost every sector of the world, that needing to stop

108
00:10:16,360 --> 00:10:21,160
the transmission of the virus in a much more transportation-based world than any previous

109
00:10:21,720 --> 00:10:26,600
plague or pandemic ever happened in, also meant shutting down critical supply chains where

110
00:10:26,600 --> 00:10:31,480
fertilizers and pesticides that were needed for agriculture didn't happen, driving food insecurity

111
00:10:31,480 --> 00:10:36,760
at massive scale, which means that the solution to one problem drove other problems. Second and

112
00:10:36,760 --> 00:10:45,480
third order effects became very problematic. And so the interconnectivity that had advantages

113
00:10:45,480 --> 00:10:50,680
also has these fragility disadvantages, and the interconnectivity also wanted to have maximum

114
00:10:50,680 --> 00:10:57,960
efficiencies, and the efficiencies also drive fragility. We also see that in that World War II

115
00:10:58,920 --> 00:11:10,200
till now-ish kind of Bretton Woods time, we had one catastrophe weapon. And so one catastrophe

116
00:11:10,200 --> 00:11:15,560
weapon could be responded to by that same catastrophe weapon. And so the game theory of it was somewhat

117
00:11:15,560 --> 00:11:22,040
simple. And for the longest time, we only had two superpowers that had it. And as a result,

118
00:11:22,040 --> 00:11:25,640
mutually assured destruction was very effective. You were able to create a kind of forced Nash

119
00:11:25,720 --> 00:11:32,520
equilibrium. And also because it's very, very hard to make nuclear weapons. There's not that many

120
00:11:32,520 --> 00:11:36,280
places that have uranium. Enriching uranium is difficult. You can even see it because of

121
00:11:36,280 --> 00:11:42,360
radioactive tracers from satellites, so it's easy to monitor. And so you could do mutually

122
00:11:42,360 --> 00:11:45,720
assured destruction. Obviously, we're in a situation now where we don't just have two

123
00:11:45,720 --> 00:11:49,480
superpowers that have nukes. We have many countries that have nukes, but we also have

124
00:11:49,480 --> 00:11:53,240
lots of other catastrophe weapons, meaning lots of other weapons that are big enough that they

125
00:11:53,240 --> 00:11:59,400
could cause kind of catastrophic loss of civilization harm. And they aren't hard to make,

126
00:11:59,400 --> 00:12:05,320
and they aren't trackable anymore. It's not hard to make crisper bioweapons or drone-based

127
00:12:07,880 --> 00:12:13,000
infrastructure attack type things. It doesn't even take a nation state to do it, not traceable.

128
00:12:13,000 --> 00:12:17,160
That's a very different situation. So when you have many different catastrophe weapons and you

129
00:12:17,160 --> 00:12:23,000
have many, many different actors that can have them, including a very difficult situation to

130
00:12:23,000 --> 00:12:27,720
be able to monitor which actors, how do you do mutually assured destruction? And so how do you

131
00:12:27,720 --> 00:12:35,320
get the deterrent strategy right? And so what I'm bringing up is that catastrophic risk before

132
00:12:35,320 --> 00:12:40,760
World War II was one phase, all of human history up to that point. Then World War II till now was

133
00:12:40,760 --> 00:12:46,680
kind of one phase, and now we're entering a new phase where the Bretton Woods mutually assured

134
00:12:46,680 --> 00:12:53,000
destruction, IGO, exponential growth of a globalized linear materials economy,

135
00:12:53,000 --> 00:12:57,640
set of solutions doesn't work for the new set of the catastrophic risk landscape that we face.

136
00:12:57,640 --> 00:13:02,120
So we need a totally new set of solutions which will require innovation in our social technologies

137
00:13:02,120 --> 00:13:08,200
of how we coordinate game theoretic type issues. Now, when you say how well prepared are we,

138
00:13:09,160 --> 00:13:16,760
we come back to the UN, we recognize that we have not succeeded in nuclear disarmament.

139
00:13:18,520 --> 00:13:22,920
Even while we kind of claim to succeed in nuclear disarmament in some very limited ways,

140
00:13:22,920 --> 00:13:27,880
we still had arms races of faster delivery mechanisms, hypersonic missiles, whatever,

141
00:13:27,880 --> 00:13:34,920
to try to win first strike and other things like that. We got more countries with nukes rather than

142
00:13:34,920 --> 00:13:41,960
less during that time. We got more other countries that could affect the movement of a new nuclear

143
00:13:41,960 --> 00:13:48,680
weapon through other kinds of geopolitical and less military advance, but engaging the bigger

144
00:13:48,680 --> 00:13:54,120
military type tactics, plausible deniability attacks that get blamed on a larger superpower

145
00:13:54,120 --> 00:14:01,880
and things like that. And during that time, we've also had every new type of advanced technology

146
00:14:01,880 --> 00:14:06,120
create an arms race. There's an arms race on AI autonomous weapons, on

147
00:14:07,480 --> 00:14:13,960
the application of CRISPR technology to bio weapons, cryptographic type weapons for cyber

148
00:14:13,960 --> 00:14:20,840
attacks. And so we have succeeded in preventing no arms races. We have not been able to reverse the

149
00:14:20,840 --> 00:14:27,480
one really critical one. None of the sustainable development goals can really be said to have

150
00:14:27,480 --> 00:14:34,840
been achieved well. So I would say that our global coordination on all of the most critical issues

151
00:14:36,520 --> 00:14:42,760
is inadequate to the timeline and consequentiality of the issues. That seems very, very clear.

152
00:14:43,720 --> 00:14:51,480
And the as exponential tech is advancing, the total number of catastrophic risks and the total

153
00:14:51,480 --> 00:14:58,120
probability of each is increasing. And the capacities that we're utilizing to address them

154
00:14:58,120 --> 00:15:03,640
are not increasing accordingly. So there is a gap that we need to be focused on, which is what you

155
00:15:03,640 --> 00:15:08,440
guys are focused on, which is this kind of global governance topic. We have global issues, not just

156
00:15:08,440 --> 00:15:14,200
local issues. Everybody's scared of global governance, the frame, the term global governance,

157
00:15:14,680 --> 00:15:19,720
or at least global government, for a good reason, which is we have a good

158
00:15:21,240 --> 00:15:27,320
long history of reasons to not trust consolidation of power with no checks and balances.

159
00:15:27,320 --> 00:15:32,760
So nobody wants this kind of massive, unchecked global government. And at the same time,

160
00:15:32,760 --> 00:15:36,520
you have to have governance at the scale that cause an effect is occurring. And if we're having,

161
00:15:37,240 --> 00:15:45,000
if, if nobody can fix climate change on their own, in terms of nation states,

162
00:15:45,000 --> 00:15:49,400
and yet they're all affected by it, and they can't fix overfishing, they can't fix nitrogen,

163
00:15:49,400 --> 00:15:52,600
run off dead zones and oceans and et cetera, there have to be

164
00:15:54,760 --> 00:15:59,240
global coordination solutions. Otherwise, multipolar traps ruin everything, right?

165
00:15:59,240 --> 00:16:03,560
Multipolar trap being some kind of race to the bottom. Arms race is an example, as we've already

166
00:16:03,560 --> 00:16:09,240
mentioned. Tragedy of the commons is another example. But the key to both of them is where the

167
00:16:09,240 --> 00:16:13,880
agent focused on their own short-term well-being does something that advances their short-term

168
00:16:13,880 --> 00:16:18,040
interest, but then makes everybody else have to do the same thing. And where everyone doing it

169
00:16:18,600 --> 00:16:27,000
creates the maximally bad long-term situation. And so if we try to create some treaty around not

170
00:16:27,000 --> 00:16:31,240
overfishing a particular region of the ocean and anybody violates it, then why does it,

171
00:16:31,240 --> 00:16:34,760
if anyone else doesn't violate the treaty, if they can't figure out enforcement,

172
00:16:35,480 --> 00:16:40,920
then you're just a sucker for holding to the treaty, right? Because all those fish are going

173
00:16:40,920 --> 00:16:43,720
to get killed anyways, the ocean's going to get messed up. It's just going to feed another

174
00:16:43,720 --> 00:16:49,000
population that's going to grow and have more people to engage in economics and armies. And so,

175
00:16:49,800 --> 00:16:55,480
yeah, how do you do enforcement on a nation that has nukes or a nation that has some critical

176
00:16:55,480 --> 00:17:00,840
aspect of infrastructure or, you know, or the globalized supply chain? And so enforcement

177
00:17:00,840 --> 00:17:05,800
becomes tricky. So then you get these types of things, tragedy of the commons and arms race,

178
00:17:05,800 --> 00:17:10,520
multi-polar traps. So you have to figure out how do we solve those coordination issues globally,

179
00:17:10,520 --> 00:17:15,240
because we have global issues that can't just keep getting pushed down the road.

180
00:17:16,120 --> 00:17:20,120
And yet we want to figure out a solution to do it that isn't a kind of global government that

181
00:17:20,120 --> 00:17:28,520
becomes its own catastrophic risk of under the name of some problem that is scary enough,

182
00:17:28,520 --> 00:17:32,840
we agree to some totalitarian power structure. And that's the thing you mentioned about order

183
00:17:32,840 --> 00:17:38,840
and chaos, is that we can see that the thing we call civilization is a way of having some order,

184
00:17:38,840 --> 00:17:44,520
some coordination between lots of people, so that they can do specialization and division

185
00:17:44,520 --> 00:17:49,080
of labor, creating a richer world for everybody, and then coordinate all that, they can coordinate

186
00:17:49,080 --> 00:17:54,600
their activity for not just those kind of productive purposes, but also protection purposes.

187
00:17:55,560 --> 00:17:59,960
So the thing that we call civilization is how we coordinate behavior of lots of people.

188
00:18:01,240 --> 00:18:07,240
And that's actually a pretty hard thing to do when you think about people that want different

189
00:18:07,240 --> 00:18:12,360
stuff and believe different stuff and aren't necessarily connected to or bonded to each

190
00:18:12,360 --> 00:18:17,320
other, like how do you get them to not just do the immediate advantageous thing to them

191
00:18:17,400 --> 00:18:25,160
for people that are fundamentally strangers to them. So typically a civilization will try to

192
00:18:25,160 --> 00:18:31,160
create order through some kind of imposition, some forced religion, forced patriotism, law,

193
00:18:31,160 --> 00:18:35,800
whatever it is, and it can air in the side of an order to have everybody

194
00:18:36,920 --> 00:18:43,240
participate with that order becoming increasingly tyrannical, increasingly dictatorial.

195
00:18:43,960 --> 00:18:48,920
If it doesn't do that, people end up orienting towards tribalism naturally

196
00:18:50,360 --> 00:18:53,800
and fragmenting kind of towards each other and you end up getting the thing failing in the

197
00:18:53,800 --> 00:18:58,680
direction of chaos. The only other answer is how do you get order without it being imposed?

198
00:18:58,680 --> 00:19:03,720
How do you get emergent order? And this was the kind of idea of democracies and republics and

199
00:19:03,720 --> 00:19:10,040
open societies is maybe we could actually get emergent order if we, and it was based on the

200
00:19:10,040 --> 00:19:15,480
idea of a culture that invested in the people enough, that the people didn't just believe

201
00:19:15,480 --> 00:19:19,400
different things and want different things and be willing to defect into war. You had to actually

202
00:19:19,400 --> 00:19:25,400
develop a people that could all come to understand the world similarly. Can everybody understand

203
00:19:25,400 --> 00:19:30,360
the philosophy of science well enough that they can all come to understand base objective reality

204
00:19:30,360 --> 00:19:35,640
that they share similarly? Can they all have something like Hegelian dialectic capacities

205
00:19:35,640 --> 00:19:39,560
where they can notice not just their own values but other people's values and recognize that only

206
00:19:39,560 --> 00:19:44,280
solutions that meet everybody's values will end up working? Can they understand things like

207
00:19:44,280 --> 00:19:49,240
multi-polar traps well enough to understand that a short-term win of my political party

208
00:19:49,240 --> 00:19:53,160
just means that whatever technique we utilize that was effective gets reverse engineered,

209
00:19:53,160 --> 00:19:56,840
the other side wins in the next four years and undoes everything that we did for four years and

210
00:19:56,840 --> 00:20:02,040
we get nowhere and then dictatorships do much better than us and the society fails? Can people

211
00:20:02,040 --> 00:20:06,280
understand those things enough that they don't orient towards the short-termism kinds of things?

212
00:20:07,240 --> 00:20:11,000
So this is why the modern democracies emerged out of modernity, emerged out of a

213
00:20:11,000 --> 00:20:15,080
philosophic system that said we can come to understand the world and understand each other

214
00:20:15,080 --> 00:20:21,080
well enough that we can actually have emergent coordination. Obviously, the world has gotten

215
00:20:21,080 --> 00:20:26,280
much more complex during that time and the cultural value of that kind of education has eroded.

216
00:20:27,240 --> 00:20:37,080
So here's the way I would frame up the current situation, one way that I'm looking at the current

217
00:20:37,080 --> 00:20:47,880
situation. This is a detour but I think it's helpful. I'll go back to World War II and then

218
00:20:47,880 --> 00:20:53,000
bridge it to now since that was kind of the beginning of catastrophic level technology.

219
00:20:53,960 --> 00:21:00,600
One way of looking at World War II is, and this is not the only way, there's lots of ways,

220
00:21:00,600 --> 00:21:04,920
this is a useful way for the construction I'm doing. One way of looking at it is that there were

221
00:21:04,920 --> 00:21:10,600
a few social ideologies that were competing for supremacy and what they were competing over was

222
00:21:10,600 --> 00:21:15,240
the emergence of a new set of technologies that science made possible that were so much more

223
00:21:15,240 --> 00:21:18,920
powerful than the previous technologies that kind of whoever got dominance and then would win.

224
00:21:19,720 --> 00:21:23,720
And so the bomb is obviously the center of that but it's not the whole of it,

225
00:21:23,720 --> 00:21:28,840
computers, the enigma machine and the whole development of computation, rockets,

226
00:21:30,440 --> 00:21:35,800
and chemistry. Chemistry is a part of that, kind of advancing in World War I but then

227
00:21:35,800 --> 00:21:40,120
advancing a lot in World War II. Kind of those all came from science getting to the place that we

228
00:21:40,120 --> 00:21:51,400
could do atomic physics and physical chemistry well. And the social philosophies we could say

229
00:21:51,400 --> 00:21:59,800
are capitalism and liberal democracy. The intersection of theory of markets and something

230
00:21:59,800 --> 00:22:07,800
like a democracy or republic. Communism, the Soviets and fascism and a particular kind of ethno

231
00:22:07,800 --> 00:22:13,400
centric nation-state fascism. So those were three different types of social systems.

232
00:22:14,440 --> 00:22:20,760
And we can see that Germany was actually meaningfully further ahead than the U.S. or the Soviets in

233
00:22:20,760 --> 00:22:27,080
certain areas of tech. They got the enigma machine first, they got the V2 first. Those other countries

234
00:22:27,080 --> 00:22:32,280
were obviously larger so when they recognized that and fought to catch up they had an advantage in

235
00:22:32,280 --> 00:22:38,360
that way. And we can say and there's lots of problems with saying this but for the use of

236
00:22:38,360 --> 00:22:46,200
the construction we can say that the U.S. won that competition for that war. Those wars over the

237
00:22:46,200 --> 00:22:51,560
new technologies and we did it not through the market running the Manhattan Project but the

238
00:22:51,560 --> 00:22:55,480
state running the Manhattan Project. This is actually a very, very important thing to recognize

239
00:22:55,480 --> 00:23:00,680
is that the state, the United States recognized that it was an existential risk

240
00:23:01,480 --> 00:23:08,680
and you remember it was Einstein and Sillard, I think, the Einstein-Sillard letter that said

241
00:23:08,680 --> 00:23:13,560
no, the physics we came up with really does say that Obama is possible and there's a decent chance

242
00:23:13,560 --> 00:23:17,960
the Germans know this and they're working on this. We were doing the physics over there together and

243
00:23:18,360 --> 00:23:32,840
the idea that states don't innovate and that markets innovate is just not true. Historically,

244
00:23:33,480 --> 00:23:39,160
the ability to split an atom, which is in a way the most impressive innovation,

245
00:23:39,720 --> 00:23:44,840
was done by the state, not by the market. That was not outsourced or private contracted.

246
00:23:45,240 --> 00:23:54,920
And the same with cracking the enigma code and the whole early development of computation that

247
00:23:54,920 --> 00:23:58,680
ended up then getting private contract and leading to Silicon Valley was nation-state funded,

248
00:23:58,680 --> 00:24:03,240
the Apollo Project. And it kind of stopped with the Apollo Project for some important reasons.

249
00:24:03,240 --> 00:24:08,040
But what happened was the United States recognized that the technological advancement

250
00:24:08,680 --> 00:24:13,400
was going to determine who had the power to determine the world so much that there was

251
00:24:13,400 --> 00:24:19,480
next sense of risk for them that they created a unlimited black budget, brought all the best

252
00:24:19,480 --> 00:24:28,120
minds together to drive innovation in technology to be able to make a democratic system stronger.

253
00:24:30,600 --> 00:24:34,760
For a bunch of reasons, after that and the decades that followed,

254
00:24:36,280 --> 00:24:39,240
more and more of the innovation got outsourced to the private sector

255
00:24:40,120 --> 00:24:44,280
and it started to become closer to true that the state wasn't innovating and most of the

256
00:24:44,280 --> 00:24:47,080
innovation was happening in the private sector. But the private sector doesn't have the same

257
00:24:47,080 --> 00:24:52,760
patriotic interests, it doesn't also have the same people in the private sector aren't voted in,

258
00:24:52,760 --> 00:24:55,640
they don't have term limits, there isn't the same jurisprudence applied to them,

259
00:24:55,640 --> 00:25:01,560
so they have a different set of agendas, right? And the whole idea of the state, like you can

260
00:25:01,560 --> 00:25:05,880
almost think of what the state in a liberal democracy is as like a labor union for the people

261
00:25:06,520 --> 00:25:11,000
and as a whole, like a labor union, is how do you unify all the people to have something that

262
00:25:11,000 --> 00:25:16,360
is big enough to represent their collective interest so that the large corporations and the

263
00:25:16,360 --> 00:25:20,520
major wealth holders within capitalism don't just rule everything like feudalism, which is

264
00:25:20,520 --> 00:25:26,840
the thing we were trying to replace before, because it's very clear that if we have a trade system

265
00:25:27,560 --> 00:25:33,000
and it's mediated by an abstract system for doing accounting like currency,

266
00:25:33,720 --> 00:25:37,000
that pretty soon you'll have a power law distribution of wealth and a few people will

267
00:25:37,000 --> 00:25:40,840
own most of the wealth. Some people are better at it and then getting better at it gives you

268
00:25:40,840 --> 00:25:45,640
more capacity to keep getting better at it and there's compounding interest, which is an exponential

269
00:25:45,640 --> 00:25:50,200
return on owning capital and there's compounding interest on debt and does that thing, right?

270
00:25:50,840 --> 00:25:55,160
And we can see the data of that in Piketty's book, but it's also just kind of a natural

271
00:25:55,160 --> 00:25:59,400
thing to look at. So the idea was since power law distributions are going to happen,

272
00:25:59,640 --> 00:26:03,880
most people are going to have really no power. How do you not have that be oppression? Well,

273
00:26:03,880 --> 00:26:07,640
let's have the people all be able to collectively vote where at least the majority of what they

274
00:26:07,640 --> 00:26:13,720
care about gets encoded as law. So their values are the basis of the jurisprudence of law. So then

275
00:26:13,720 --> 00:26:21,480
rule of law can get enforced by representatives of foreign by the people that are going to be

276
00:26:21,480 --> 00:26:26,040
bequeathed with a monopoly of violence so they can actually do enforcement

277
00:26:27,000 --> 00:26:32,280
to be able to protect the people in the commons against perverse incentive while letting the

278
00:26:32,280 --> 00:26:36,040
market do all the good things that it does. But most of rule of law is actually binding

279
00:26:36,040 --> 00:26:45,640
the perverse incentive. So if that only works where the state can check the predatory aspects of

280
00:26:45,640 --> 00:26:50,520
markets, if the people are checking the state that it is truly of foreign by the people,

281
00:26:50,520 --> 00:26:54,360
there's transparency, everybody's actively engaged, as soon as that stops happening,

282
00:26:54,360 --> 00:26:58,520
then the government is just run by people. Those people are economic actors. They're in there for

283
00:26:58,520 --> 00:27:03,400
whatever a short period of time and they will be liked about the same whether they do corporate

284
00:27:03,400 --> 00:27:06,600
interests or not because nobody's really going to know. And so of course, you end up getting

285
00:27:06,600 --> 00:27:10,520
regulatory capture where the market captures the regulatory apparatus and you get crony capitalism

286
00:27:10,520 --> 00:27:17,160
and that kind of institutional decay. And as the founding fathers in the US said, and anyone who

287
00:27:17,160 --> 00:27:23,000
paid attention, as soon as a couple generations pass and the people forget what it means to fight

288
00:27:23,000 --> 00:27:27,640
a revolutionary war and be under oppression, they won't keep investing and being educated enough

289
00:27:27,640 --> 00:27:31,160
and actively being engaged in government because they'd rather keep up with the Joneses or party

290
00:27:31,160 --> 00:27:39,320
or like some other thing. And so how do you keep the intergenerational transfer of not just the

291
00:27:39,320 --> 00:27:43,560
knowledge but the civic virtues necessary to uphold a democracy, which is not a trivial thing.

292
00:27:43,560 --> 00:27:48,680
And especially as time goes on and the complexity of the world increases,

293
00:27:49,640 --> 00:27:53,720
understanding the issues well enough to really play a role in them and to be able to oversight

294
00:27:53,720 --> 00:27:57,400
them and police them gets harder and harder. And so there has to be more and more investment

295
00:27:57,400 --> 00:28:02,360
into doing that. So we can see that the people stopped investing in checking the state, the state

296
00:28:02,360 --> 00:28:07,320
stopped checking the market, market captured the state, all the innovation got outsourced.

297
00:28:07,320 --> 00:28:13,080
And so what we can see today, so we see in that World War II example that the state really pioneered

298
00:28:13,080 --> 00:28:16,520
the advancement of all these areas of tech to increase the integrity of the state.

299
00:28:18,920 --> 00:28:25,240
There is a jump in technology that is currently happening that is more significant than the

300
00:28:25,240 --> 00:28:33,320
World War II jump in technology. And the center of it is AI and computation with AI being the

301
00:28:33,320 --> 00:28:38,120
very center, right? It's computation, digital tech, but then the application of AI and digital

302
00:28:38,120 --> 00:28:43,640
tech to physical tech as well. So the application of that to biotech and CRISPR kind of stuff and

303
00:28:43,720 --> 00:28:48,680
to robotics and robotic automation and the other key areas of computer science from the

304
00:28:49,720 --> 00:28:55,960
evolution of the computational basis, quantum computing, photo computing, DNA computing, whatever,

305
00:28:55,960 --> 00:29:06,280
and again, the application of that to the material sciences, nanotech, etc. So

306
00:29:06,600 --> 00:29:13,880
we're undergoing this huge jump in technology right now that is something like two orders of

307
00:29:13,880 --> 00:29:19,080
magnitude more significant than the previous World War II jump was in terms of the total amount of

308
00:29:19,080 --> 00:29:23,320
verticality of power and the speed at which it's developing and the number of verticals

309
00:29:23,320 --> 00:29:32,280
simultaneously. And the way I see it is that tech will confer so much power that only those

310
00:29:33,000 --> 00:29:36,120
who are guiding it will have much of a say in the future.

311
00:29:38,520 --> 00:29:46,200
And right now, I only see two types of groups really guiding it meaningfully.

312
00:29:47,720 --> 00:29:52,440
Some authoritarian nation states are where the nation state is taking seriously the development

313
00:29:52,440 --> 00:29:57,000
of tech in the nation state is investing a very big R&D budget and how to actually increase the

314
00:29:57,000 --> 00:30:01,320
integrity of their nation state. And there's a good thing for them to do aligned with whatever

315
00:30:01,320 --> 00:30:06,200
their system and their ideologies are. And obviously, China is a prime example here where

316
00:30:06,760 --> 00:30:12,120
the application that the government is investing in the development of engineers and in the

317
00:30:12,120 --> 00:30:17,880
application of all of those areas of tech to the nature of government itself. And that's everything

318
00:30:17,880 --> 00:30:24,680
from their IoT system to their seismic credit system to the transistor development and lithography to

319
00:30:25,640 --> 00:30:30,600
the Belt and Road Initiative and getting something like 94% of the world's rare earth

320
00:30:30,600 --> 00:30:36,840
minerals in there that are needed for computational substrate and their supply chain to on and on,

321
00:30:36,840 --> 00:30:42,680
right? To the creation of their own internet that doesn't have the same problems for their country

322
00:30:42,680 --> 00:30:49,560
that the US internet has. So authoritarian nation states are using the exponential tech to become

323
00:30:49,720 --> 00:30:55,480
exponentially more effective authoritarian nation states. And the only other kind of org

324
00:30:55,480 --> 00:31:02,440
are companies, Western mostly companies. And those companies are supported by a military

325
00:31:02,440 --> 00:31:06,360
and capital and infrastructure of the nation state, but they don't are not serving the interests of

326
00:31:06,360 --> 00:31:11,720
the nation state other than GDP and jobs and some very short term kind of stuff. And they're

327
00:31:11,720 --> 00:31:18,920
becoming exponentially more powerful companies. But you know, Facebook and Google and have more

328
00:31:18,920 --> 00:31:23,560
users than China and the US combined have people, right? So these are humongous kinds of things of

329
00:31:23,560 --> 00:31:29,000
which there is no precedent for a corporation in history. And Rand never imagined things like this

330
00:31:29,000 --> 00:31:34,600
when she was thinking about the symmetry of supply and demand. And she didn't think of things like

331
00:31:34,600 --> 00:31:39,640
Metcalfe Dynamics that end up leading to natural monopolies and antitrust law didn't think of

332
00:31:39,640 --> 00:31:44,360
that, right? So you end up having Amazon being bigger than all other online stores combined

333
00:31:44,360 --> 00:31:47,800
and Google being bigger than all other search engines combined and Facebook being bigger for

334
00:31:47,800 --> 00:31:52,120
time on site than all the other social networks, you get a natural power law distribution,

335
00:31:53,480 --> 00:31:58,920
not based on government crony capitalism based simply on the nature of network dynamics that

336
00:31:58,920 --> 00:32:04,920
once you reach a certain escape velocity, there you're a natural monopoly will start to emerge

337
00:32:04,920 --> 00:32:10,600
based on the value of the thing being associated with the second power of the number of users.

338
00:32:11,480 --> 00:32:20,600
And so the interesting thing is you see these corporations that are becoming more powerful

339
00:32:20,600 --> 00:32:26,680
than nation states in many ways because of the development and direction of the exponential

340
00:32:26,680 --> 00:32:35,240
technologies. And as that happens, they are less able to be regulated by the countries while still

341
00:32:35,240 --> 00:32:41,400
benefiting from the infrastructure of the countries and simultaneously eroding the

342
00:32:41,400 --> 00:32:47,240
integrity of the country. We can see the way that the time on site optimization ad model

343
00:32:47,240 --> 00:32:54,120
of Facebook and Google and YouTube have eroded American democracies in specific and Western

344
00:32:54,120 --> 00:33:01,160
democracies by doing the time on site optimization appeals to people's cognitive biases and tribalism

345
00:33:01,160 --> 00:33:07,000
and limbic hijacks and those types of things. We can see that the kind of consolidation of

346
00:33:07,000 --> 00:33:13,160
market function like Amazon that Amazon's growth during COVID matched pretty closely the closure

347
00:33:13,160 --> 00:33:17,080
of all small businesses that aren't going to reopen. Well, the American dream without small

348
00:33:17,080 --> 00:33:23,400
businesses isn't the thing, right? It's not a thing in the same way. And we see the technological

349
00:33:23,400 --> 00:33:29,400
automation of so many jobs impending and not the replacement in the current way that it's trending

350
00:33:29,400 --> 00:33:37,000
of a similar American dream kind of sovereignty. So there's kind of a billionaire to centa

351
00:33:37,000 --> 00:33:43,320
billionaire class that runs whatever the one big dog on the top of the power law distribution that

352
00:33:43,320 --> 00:33:51,240
defines a vertical is and a increasingly less upwardly mobile in terms of real capacity to

353
00:33:51,240 --> 00:33:58,440
play those games underclass. And obviously some kind of middle class that is serving

354
00:33:58,440 --> 00:34:05,880
the very upper class in that context. So what I see is that that is the movement to a new kind

355
00:34:05,880 --> 00:34:12,120
of feudalism, right? A tech feudalism. And it's even interesting, some of those companies,

356
00:34:12,680 --> 00:34:16,440
you know, we see this with Tesla, we say with the other ones, some of those companies

357
00:34:17,320 --> 00:34:21,800
are getting subsidies, government subsidies. That means they're collecting taxpayer money

358
00:34:22,680 --> 00:34:29,720
to do to utilize taxpayer money to do the thing they're doing. But the taxpayers didn't vote on

359
00:34:29,720 --> 00:34:34,280
them doing that. They were not elected representatives. They cannot be unelected. And

360
00:34:34,280 --> 00:34:38,520
there is no traditional jurisprudence for the guidance of the thing that they're doing.

361
00:34:38,520 --> 00:34:43,160
That's something much more like a king than a president, which is why I say kind of an emergent

362
00:34:43,160 --> 00:34:48,040
tech feudalism. So what I see is there's one stranger tractor, which is tech feudalism. There's

363
00:34:48,200 --> 00:34:51,960
another stranger tractor, which is kind of authoritarian nation states. And anything like

364
00:34:51,960 --> 00:34:57,640
an open society where there's participatory governance and jurisprudence that is grounded in

365
00:34:57,640 --> 00:35:02,920
the will of the people, there is no system that is based on those ideals that is innovating

366
00:35:04,360 --> 00:35:10,120
in exponential tech to make better versions of that social tech. That is the number one

367
00:35:10,120 --> 00:35:14,920
imperative of our time, in my opinion. And either we figure that thing out, or those are the only

368
00:35:15,000 --> 00:35:19,080
attractors. And the third attractor is that the exponential tech just causes X-risk and we're

369
00:35:19,080 --> 00:35:25,320
fucked, right? So you have X-risk feudalism and authoritarianism as the current dominant attractors

370
00:35:25,320 --> 00:35:31,720
in the presence of exponential tech. Or there's not 17 sustainable development goals that really

371
00:35:31,720 --> 00:35:35,720
matter because we can't fucking achieve any of them without better coordination. There's figuring

372
00:35:35,720 --> 00:35:40,760
out coordination that it becomes the central goal of the world, figuring out a kind of coordination

373
00:35:41,080 --> 00:35:46,280
that is emergent order, that is neither chaos nor oppression, that is

374
00:35:47,960 --> 00:35:53,720
able to utilize the exponential technologies and also to bind and direct them so that they

375
00:35:53,720 --> 00:36:00,600
do not either directly or through externality create X-risk, and that they don't create

376
00:36:00,600 --> 00:36:04,520
authoritarian systems or kind of feudal systems that erode civil liberties in the process. So we

377
00:36:04,520 --> 00:36:14,760
need to have a kind of global innovation zeitgeist of how to apply, develop and apply all the areas

378
00:36:14,760 --> 00:36:20,120
of exponential technology to building new social tech that can guide, bind, and direct the exponential

379
00:36:20,120 --> 00:36:25,560
tech, prevent X-risk, and do it in a way that is commensurate with what are underlying kind of

380
00:36:25,560 --> 00:36:30,040
deepest values for participatory and empowered governance and civics are.

381
00:36:31,000 --> 00:36:37,400
Thank you, Daniel. That was a fantastic riff on the opening question and I think really sets the scene

382
00:36:37,400 --> 00:36:42,840
and goodness we could go in lots of different directions now. It made me think of, you know,

383
00:36:42,840 --> 00:36:47,960
Niall Ferguson's book, The Square and the Tower. Niall Ferguson has said that historians haven't

384
00:36:47,960 --> 00:36:53,480
taken network seriously enough and he traces these network dynamics back centuries and actually says

385
00:36:53,480 --> 00:36:58,680
they were much more prominent and important. And part of the historical political landscape

386
00:36:58,680 --> 00:37:08,760
than we often think. So that was really interesting and I guess what I was thinking we might pick up

387
00:37:09,880 --> 00:37:15,080
would be, you said at some point that we often kick these problems down the road and I wonder

388
00:37:15,080 --> 00:37:21,240
to what extent we're really coming up against the sort of cognitive limits of humans. Given the

389
00:37:21,240 --> 00:37:25,640
rapidity of change, given the challenges that we confront in trying to get our heads around

390
00:37:25,640 --> 00:37:32,040
exponential functions. We have this kind of strange parallax right now between continuity

391
00:37:32,680 --> 00:37:38,360
and discontinuity. So we have these unique unprecedented challenges, but on the other

392
00:37:38,360 --> 00:37:45,880
hand we have these very old forces of zero sum competition, resource wars. Certainly something

393
00:37:45,880 --> 00:37:52,520
which I sometimes hear in the academy is this idea that ultimately there's really nothing new under the

394
00:37:52,520 --> 00:38:00,040
sun that we can repurpose our existing structures that we do have good enough global governance,

395
00:38:00,040 --> 00:38:05,320
if you will. But I wanted to tease out a little bit more this idea of continuity. It seems to me

396
00:38:05,320 --> 00:38:11,240
also what we're seeing is kind of a resurgence of understanding that actually we do need to respect

397
00:38:11,240 --> 00:38:16,920
the laws of physics, that we need to respect the laws of thermodynamics, that we might actually even

398
00:38:16,920 --> 00:38:23,480
have to listen to say E. O. Wilson on the the laws of sociobiology in terms of how do we navigate

399
00:38:23,480 --> 00:38:29,720
through a viable path given the current situation we find ourselves in. But on the flip side, we also

400
00:38:29,720 --> 00:38:35,000
have a sort of a real lack of radical vision within the half within sort of the corridors of power,

401
00:38:35,000 --> 00:38:40,840
even if the UN Secretary General Antonio Guterres is calling for a new international social contract,

402
00:38:40,840 --> 00:38:46,120
it doesn't seem to be resonating. And if you go back in the historical record and you look at the

403
00:38:46,680 --> 00:38:51,880
debates of the 1950s and the shadow of the bomb and how radical the vision was,

404
00:38:51,880 --> 00:38:56,520
of course, what resulted was a compromise. But nevertheless, there were very serious people

405
00:38:56,520 --> 00:39:03,960
who were thinking hard about global political federation. What's happened? Why is it, you know,

406
00:39:03,960 --> 00:39:11,240
to draw on that famous phrase is easier to imagine the end of the world than the end of capitalism?

407
00:39:12,200 --> 00:39:20,200
Why is it so hard for us to to work through the viable path to to address the challenge

408
00:39:20,200 --> 00:39:29,080
that you've articulated so clearly? First, on the topic of there's nothing new under the sun and

409
00:39:29,080 --> 00:39:35,000
our previous systems of social philosophy and social technology are adequate. I don't think that

410
00:39:35,000 --> 00:39:39,080
anyone believes that who's actually studied exponential tech and X risk meaningfully,

411
00:39:39,080 --> 00:39:46,040
I have not met them. It doesn't seem like a reasonable position to hold those things together.

412
00:39:49,640 --> 00:39:54,760
If you look at just a single category of exponential tech, that idea will change.

413
00:39:57,880 --> 00:40:03,480
I'm sure the listeners have all seen this, but like, when you saw the way that AlphaGo beat

414
00:40:03,480 --> 00:40:10,680
Stockfish at chess, that was so fucking clear that we're dealing with phenomena that are nothing

415
00:40:10,680 --> 00:40:16,520
like any phenomena that the Scottish Enlightenment or the founding fathers or Isaac Newton or Marx

416
00:40:16,520 --> 00:40:23,000
or anyone ever had to think about that that the best chess player in the world, which is the

417
00:40:23,000 --> 00:40:26,280
cutting edge of there's nothing new under the sun, like chess players got better, but like,

418
00:40:26,280 --> 00:40:32,600
people been good at chess for a while, right? It's a slow evolution until AI and Stockfish just

419
00:40:32,600 --> 00:40:37,640
devastated the best chess players in the world. We remember seeing Kasparov get beaten, and then

420
00:40:37,640 --> 00:40:41,640
Stockfish kept getting better and better with the model of AI or programming all the human games

421
00:40:41,640 --> 00:40:47,160
until it was so much better that it stopped even making sense to calibrate it relative to the best

422
00:40:47,160 --> 00:40:52,600
humans. And then a breakthrough in AI says, let's do this differently, right? Let's make a type of

423
00:40:52,600 --> 00:40:58,680
AI based on rival networks, and we won't actually program any human games into it. We'll just let

424
00:40:58,680 --> 00:41:03,080
it play itself a bunch of times and fast forward and see what it learns. And I don't remember

425
00:41:03,080 --> 00:41:09,880
exactly, but AlphaGo by Google, I think it trained itself in three hours, just playing itself with

426
00:41:09,880 --> 00:41:15,480
no human input of information, just the rules of chess. And then it ended up beating Stockfish.

427
00:41:15,480 --> 00:41:20,280
There were a few stales, but it was like 38 to zero in terms of the non stales. And it's like,

428
00:41:21,080 --> 00:41:25,400
oh, wow, that's it. And Stockfish was so far beyond humans, and it's like three hours of training. And

429
00:41:25,480 --> 00:41:31,480
then that same thing could beat us at go and start to beat us at complex strategic video games.

430
00:41:32,280 --> 00:41:37,720
And this is all evolving over the course of almost no period of time, right? This is evolving over

431
00:41:37,720 --> 00:41:45,560
the course of months and single digit years. Nothing new under the sun. Nobody can study

432
00:41:45,560 --> 00:41:48,600
exponential curves and think that. Now, this brings us to the...

433
00:41:49,320 --> 00:42:00,440
It's just such a silly thing to say. When you start looking at scaled species extinction,

434
00:42:00,440 --> 00:42:07,880
when you start looking at the Anthropocene as a real thing, where humans are a bigger force

435
00:42:07,880 --> 00:42:12,920
than all geologic forces combined in defining the surface of the Earth, like, fuck, it's a different

436
00:42:13,480 --> 00:42:20,600
situation than the history of the world was. And like I said, just even starting with the bomb,

437
00:42:21,400 --> 00:42:27,160
the world never didn't have the major empire's war. And World War II was like a second ago

438
00:42:27,720 --> 00:42:35,480
in historical time, right? And the solution to not use that bomb drove all these other issues. So

439
00:42:35,480 --> 00:42:44,520
a lot of our issues are just increases in the severity of the same underlying type of game

440
00:42:44,520 --> 00:42:51,480
theoretic dynamics. And so we can say they are continuous with them in type, but there are places

441
00:42:51,480 --> 00:42:58,040
where a change of magnitude becomes a change in kind, right? Like as soon as the magnitude gets

442
00:42:58,040 --> 00:43:03,320
beyond human information processing capability, it's now a change of kind. As soon as we move

443
00:43:03,320 --> 00:43:06,840
from a war that's winnable to a war that's not winnable, even though they're both the logic of

444
00:43:06,840 --> 00:43:11,640
war, it's a change of magnitude that becomes a change in kind, right? So there's a lot of places

445
00:43:11,640 --> 00:43:17,160
where even the things that are continuous with the past become discontinuous past certain thresholds,

446
00:43:17,160 --> 00:43:21,560
meaning that the same types of solutions, the whole class of solutions doesn't apply anymore.

447
00:43:23,560 --> 00:43:28,600
Now, that doesn't mean that we throw out everything that we've learned. It means that we have to make

448
00:43:28,600 --> 00:43:32,600
sure that we're applying everything that we've learned that is effective, that we aren't making

449
00:43:32,600 --> 00:43:37,320
the mistake of not paying attention to the total amount of human thinking and ingenuity

450
00:43:37,320 --> 00:43:42,920
that's happened so far, and that the new innovation that we do is commensurate with the smart parts

451
00:43:42,920 --> 00:43:48,760
of it. But it happens all the time that we're exploring a search space, and there's a couple

452
00:43:48,760 --> 00:43:53,800
branches, and in the immediate term, this branch has more incentive. And so we explore this branch,

453
00:43:53,800 --> 00:43:58,200
and then we just forget about this one, and we just keep exploring, and then we hit a cul-de-sac

454
00:43:58,200 --> 00:44:04,760
at a certain point. But we have reasons why there's momentum to keep some combination of

455
00:44:04,760 --> 00:44:09,800
sunken cost fallacies with the actual belief that this is the only path, this earlier choice,

456
00:44:09,800 --> 00:44:14,120
and that we wouldn't go all the way back there to the not even knowing the other branches that were

457
00:44:15,000 --> 00:44:21,080
cleaved that we didn't pay attention to, to like perverse institutional incentives of standard

458
00:44:21,080 --> 00:44:25,320
models where it's hard to get a research grant to do anything outside of that thing or to get

459
00:44:25,320 --> 00:44:28,840
your professor who believes in that thing to change their opinion on it or whatever it is.

460
00:44:30,760 --> 00:44:33,800
So there are a bunch of places where we actually have to go back and say, okay,

461
00:44:36,360 --> 00:44:41,720
there was an incentive to make faster and faster smaller and smaller computer chips,

462
00:44:41,720 --> 00:44:45,720
and there was enough money around that that there were whole other directions in computational

463
00:44:45,720 --> 00:44:52,680
substrate that we didn't take that for reasons of manufacturing resilience and a bunch of other

464
00:44:52,680 --> 00:44:57,880
things might actually be meaningful and interesting. This is starting to be a real conversation in

465
00:44:57,880 --> 00:45:03,080
theoretical physics with string theory, and like maybe we actually need to rewind and try

466
00:45:03,080 --> 00:45:09,000
a fundamentally different approach. I think there are places in governance where like we've just

467
00:45:09,000 --> 00:45:20,600
accepted, we've just kind of accepted capitalism is the only, in the West, is the only reasonable

468
00:45:20,600 --> 00:45:29,800
answer combined with some kind of omen-ish government state. And if you think anything else,

469
00:45:29,800 --> 00:45:35,080
you didn't study the history of Mao and Stalin and Paul Potts and whatever, because everything else

470
00:45:35,080 --> 00:45:40,440
ends up becoming that kind of dreadful slaughter. That's kind of the dominant narrative where

471
00:45:41,240 --> 00:45:45,240
it's worse than going against Christianity or it's similar to going against Christianity

472
00:45:45,240 --> 00:45:48,840
in the Dark Ages, right? There's an almost religious tone to it. It's like, well,

473
00:45:48,840 --> 00:45:51,640
we could come up with better shit that isn't any of those things, like

474
00:45:53,960 --> 00:45:57,960
there's nothing new under the sun, blockchain's new, like the ability to have an uncorruptible

475
00:45:57,960 --> 00:46:03,960
ledger where you can have a provenance of data that you can't fuck up. That makes it

476
00:46:03,960 --> 00:46:07,240
where you can have a history that can't be corrupted or changed by the winners afterwards.

477
00:46:07,240 --> 00:46:11,240
That's kind of new. That's a big deal. Makes it to where you can have a system of justice where

478
00:46:11,240 --> 00:46:16,680
you can't actually fuck up the data, right? It means that you can have a system of accounting

479
00:46:16,680 --> 00:46:20,520
where let's say the government spending was on a blockchain that was transparently

480
00:46:20,520 --> 00:46:25,000
oversighted, there wouldn't be missing money anymore. Right now, there's all these places

481
00:46:25,000 --> 00:46:28,680
where the total amount of money going in and the receipts coming out don't add up and there's

482
00:46:28,680 --> 00:46:34,840
missing money. It's like, well, that couldn't be. Does that make something new possible?

483
00:46:34,840 --> 00:46:41,560
Yeah, totally it makes something new possible. You look at the way that AI can make

484
00:46:41,640 --> 00:46:49,560
new sounds. It can do error correction of sound where there is an error or make new sounds or

485
00:46:49,560 --> 00:46:55,320
make new faces by doing an averaged composite of all faces that look similarish. You say, well,

486
00:46:55,960 --> 00:47:01,480
could people express huge numbers of people, express their sentiments about something and

487
00:47:01,480 --> 00:47:07,240
have the AI actually come up with something that is like a weighted average of all of those as a

488
00:47:07,240 --> 00:47:14,840
form of proposition creation and then could we use distributed methods of proposition advancement

489
00:47:14,840 --> 00:47:19,560
that didn't exist when we had to meet in a town hall and ride a horse from that town hall to

490
00:47:19,560 --> 00:47:23,000
the other ones and we haven't innovated the structure of government since we had to ride horses.

491
00:47:24,600 --> 00:47:30,200
Like, why do we think this particular thing is the best thing? Well, because the other things,

492
00:47:30,200 --> 00:47:34,600
the last time we had that conversation seemed dreadful, at least that was the winning narrative.

493
00:47:35,560 --> 00:47:41,480
But totally new things that are not just those previous things are possible.

494
00:47:44,680 --> 00:47:49,400
What I would say is someone should not assume that the moment we say maybe there's a problem

495
00:47:49,400 --> 00:47:56,680
with capitalism that we're instantly going to turn into Stalinism. But to say, let's make sure

496
00:47:56,680 --> 00:47:59,960
we studied that history well enough to know what was wrong with those ideas and we don't do that,

497
00:47:59,960 --> 00:48:06,360
yes. But let's also do the critique of the system and not just end with the critique,

498
00:48:06,360 --> 00:48:10,840
but take it as a design criteria to say what would a better system look like and have we

499
00:48:10,840 --> 00:48:15,880
got all the design criteria? Do we have the critiques of the communist system and the socialist

500
00:48:15,880 --> 00:48:20,920
system and the capitalist system simultaneously? And then can we take all those as design criteria

501
00:48:20,920 --> 00:48:25,320
and work on a fundamentally better design that might not look like any of those isms that utilizes

502
00:48:25,320 --> 00:48:29,080
new technology, which means new possibilities that didn't exist before with new forcing functions

503
00:48:29,080 --> 00:48:35,320
that didn't exist before? I think you're also saying, Daniel, that these kinds of challenges

504
00:48:35,320 --> 00:48:41,000
do actually have comprehensive solutions. And I think there's quite a lot of people who deep down

505
00:48:41,000 --> 00:48:48,520
have very, they doubt that that's actually possible. So whether they should have

506
00:48:48,520 --> 00:48:52,840
haven't even tried hard enough to have that doubt mean anything. It's just an emotional default.

507
00:48:53,720 --> 00:48:57,880
That was the other question you asked is, are we hitting the limits of cognitive complexity?

508
00:48:57,880 --> 00:49:04,200
That is such a shit answer if you haven't actually applied the full limits of human

509
00:49:04,200 --> 00:49:10,200
cognitive complexity and seen that we're failing. So we're not even trying. China's trying and they're

510
00:49:10,200 --> 00:49:18,040
doing fucking amazing. In the US, we have no high-speed trains. None. None. In the time that

511
00:49:18,040 --> 00:49:21,480
they've existed, China's been exporting them all around the fucking world in that same amount of

512
00:49:21,480 --> 00:49:29,320
time. But a system that doesn't have term limits and that doesn't have a two-party system where

513
00:49:29,320 --> 00:49:34,920
we just use all the energy wasted as heat fighting each other and then whatever you do for four years,

514
00:49:34,920 --> 00:49:39,240
the other people undo for four years and nobody invests in anything with longer than four year

515
00:49:39,240 --> 00:49:44,280
timelines because it won't get them reelected, that system is just stupid. That's going to fail to a

516
00:49:44,360 --> 00:49:52,920
system that can do long-term planning. So if we say, okay, let's imagine just hanging out in the 30s

517
00:49:54,360 --> 00:49:58,440
and saying, we got to figure out how to split an atom. No, not just split an atom. We're going to

518
00:49:58,440 --> 00:50:04,040
figure out how to split an atom and deliver that as a warhead on a rocket to some other place with

519
00:50:04,040 --> 00:50:08,120
some decent precision. In fact, we're going to go beyond that. We're going to use uranium to fission

520
00:50:08,120 --> 00:50:15,880
something and split it to then drive nucleons into a fusion. It would be easy to say, well,

521
00:50:15,880 --> 00:50:19,720
there's no fucking way. We don't have the cognitive complexity to be able to split atoms. We don't

522
00:50:19,720 --> 00:50:25,160
even know what an atom is. But the Manhattan Project was a very serious investment in cognitive

523
00:50:25,160 --> 00:50:29,880
complexity and we got everybody there. We got all the best thinkers in the world there. We put the

524
00:50:29,880 --> 00:50:37,560
budget on. Are we doing that? Are we even fucking, where we got von Neumann, we got Turing, we got

525
00:50:37,560 --> 00:50:44,200
Feynman, we got Oppenheimer, we got all those folks in Bletchley Park and in Los Alamos. Where is

526
00:50:44,200 --> 00:50:49,400
the equivalent of that thing outside of very narrow areas of military? Which is why we have a dope

527
00:50:49,400 --> 00:50:52,920
military. We have an awesome military, but that's innovation in military. That's not

528
00:50:52,920 --> 00:50:58,920
innovation in the social technology of governance itself. We actually have to not just innovate

529
00:50:59,000 --> 00:51:02,440
our military, but innovate the social technology of governance for a participatory

530
00:51:03,880 --> 00:51:10,760
governance system. And this is why we come back to the, there's this quote that I always forget,

531
00:51:10,760 --> 00:51:15,880
so a paraphrase of George Washington's that said something to the effect that the number one aim

532
00:51:15,880 --> 00:51:19,800
of the federal government has to be the comprehensive education of every citizen in the science of

533
00:51:19,800 --> 00:51:25,320
government. And science of government was the term of art. And I think it's so profound that he did

534
00:51:25,320 --> 00:51:31,000
not say the number one aim of the federal government is to protect its borders. And he did not say

535
00:51:31,000 --> 00:51:37,160
the name of the federal government is to protect rule of law. Because you can do rule of law

536
00:51:37,160 --> 00:51:42,840
effectively with a police state. And you can protect the boundaries fine with a military

537
00:51:42,840 --> 00:51:49,880
dictatorship. But they won't be democracies. If it's going to be a democracy, then democratically

538
00:51:49,960 --> 00:51:56,040
the people will probably decide to protect their borders and to engage rule of law.

539
00:51:56,600 --> 00:52:01,560
But if the number one goal is anything other than the comprehensive education of all citizens,

540
00:52:01,560 --> 00:52:05,240
and the education was considered both a cognitive education and a moral education,

541
00:52:05,240 --> 00:52:08,600
the way they described it, which is the kind of civic virtues that people are willing to

542
00:52:08,600 --> 00:52:13,720
give something for the larger system that they also receive benefit from. And they're actively

543
00:52:13,720 --> 00:52:21,720
participatively engaged. So that's the thing we need to be innovating in right now, not just

544
00:52:21,720 --> 00:52:27,800
innovating in military while turning it into a some kind of autocratic or kleptocratic system.

545
00:52:28,600 --> 00:52:35,560
But how do we apply the new digital and other exponential technologies to be able to both direct

546
00:52:35,560 --> 00:52:41,720
the exponential technologies well, so that they don't cause existential risk, and in a way that

547
00:52:41,720 --> 00:52:49,000
is aligned with the actual values that we care about as a people. And so then the core question

548
00:52:49,000 --> 00:52:53,080
comes, what is a successful civilization? Well, it's one that doesn't fail, but that's not the

549
00:52:53,080 --> 00:52:58,440
only criteria. It's one that doesn't fail and that maximizes the possible quality of life for

550
00:52:58,440 --> 00:53:02,920
everybody in perpetuity. And then we have to find what is quality of life me, right? So these

551
00:53:02,920 --> 00:53:08,440
there's like core existential questions of what is a meaningful human life to be able to design

552
00:53:08,440 --> 00:53:12,760
a civilization that is optimizing for that, which is culture, right? Which is why we have to have

553
00:53:12,760 --> 00:53:16,920
innovation in culture. Which is why I talk about that there's a cultural renaissance, a cultural

554
00:53:16,920 --> 00:53:22,600
enlightenment that is necessary right now as the basis of the creation of these new institutions that

555
00:53:22,600 --> 00:53:26,840
can solve the extra problems, because our current problem solving mechanisms can't solve them.

556
00:53:27,400 --> 00:53:31,080
Which is why they're not being solved. We have to develop new institutions that are capable of

557
00:53:31,080 --> 00:53:34,440
solving these types of problems, these types of complexity. But if those new institutions are

558
00:53:34,440 --> 00:53:38,840
created by a few people that get it and impose them on force, then it's some kind of autocracy.

559
00:53:39,720 --> 00:53:45,480
So they have to be created by people who want them and are willing to participate with them

560
00:53:45,480 --> 00:53:50,600
and capable of participating. That is the cultural enlightenment that has to be the basis of it,

561
00:53:50,600 --> 00:53:56,360
which and of course, there's a recursive process of some people engaging in that to then build

562
00:53:56,360 --> 00:54:01,720
systems that in turn engage more people in it. So you get a virtuous cycle between cultural evolution

563
00:54:01,720 --> 00:54:09,080
and social evolution, employing physical technologies, binding physical technologies,

564
00:54:09,080 --> 00:54:15,720
and advancing them for the right purposes. It sounds like we really need a new forms of

565
00:54:15,720 --> 00:54:20,280
wisdom education. And obviously I'm glad to say that we've got a Zach Stein coming on the podcast

566
00:54:20,280 --> 00:54:24,360
very soon to discuss that very question. And obviously what you're saying, Daniel,

567
00:54:24,360 --> 00:54:28,840
big implications for how we think about the university in the current situation.

568
00:54:29,800 --> 00:54:33,720
But I'd like to hand over to Sam. I know Sam's got a burning question, so please, Sam.

569
00:54:35,160 --> 00:54:39,160
Yeah, hi, Daniel. Yeah, I've got a couple of burning questions, but I'll go with one to start.

570
00:54:40,680 --> 00:54:45,080
It seems like with the problems we're facing, they often, as we talked about,

571
00:54:45,080 --> 00:54:49,240
they happen at a certain area. So for example, climate change is here already,

572
00:54:49,240 --> 00:54:52,680
and it will exponentially grow out. And that's one of the issues that I think we're kind of

573
00:54:52,680 --> 00:54:56,760
alluding to, that when there's not the immediate threat of World War II, for example, it's quite

574
00:54:56,760 --> 00:55:04,440
hard to galvanize a whole group of people to solve a problem. But do you think that we think about

575
00:55:04,440 --> 00:55:10,200
solutions in the same way to the logic of problems in terms of Silicon Valley out,

576
00:55:10,200 --> 00:55:15,720
or it will happen in this certain area and slowly filter out? There's that quote,

577
00:55:15,720 --> 00:55:21,640
the future is here, it's just not that evenly distributed. And that's quite a worrying logic

578
00:55:21,720 --> 00:55:26,120
if we're thinking about the magnitude of exponential risk. And do you think that we're

579
00:55:26,120 --> 00:55:32,200
then following the logic that we apply for problems that they happen and exponentially grow out?

580
00:55:32,200 --> 00:55:36,040
And is that useful or harmful when we're thinking about solutions that need to

581
00:55:36,760 --> 00:55:41,160
really permeate around the whole globe and not leave anyone behind?

582
00:55:43,800 --> 00:55:48,440
I'm not sure that I understand the question yet. You were using the example of climate change and

583
00:55:48,440 --> 00:55:52,280
saying it's already here, but because it doesn't look like an agent in a way that we

584
00:55:52,920 --> 00:55:57,560
evolve to understand as an immediate threat, we don't respond to it appropriately. But that

585
00:55:57,560 --> 00:56:03,880
it's already here, it's expanding in a way that maybe we don't respond to appropriately. And

586
00:56:03,880 --> 00:56:08,600
you're wondering, is that the case with all of the risks? There's already AI happening that is risky,

587
00:56:08,600 --> 00:56:11,400
and we're just not responding to it appropriately? Or was the question different?

588
00:56:12,440 --> 00:56:16,680
Yeah, sorry, Daniel. The question was slightly different. So that's how we understand

589
00:56:16,680 --> 00:56:22,920
issues like climate change. And we often talk about solutions in a similar way to

590
00:56:23,560 --> 00:56:29,400
that issue of climate change, i.e. there'll be an innovation in a certain part of the world.

591
00:56:29,400 --> 00:56:34,680
So the solution is already here. And then it slowly permeates out. And then eventually everyone

592
00:56:34,680 --> 00:56:38,680
will have it. So you took the example of high speed trains, they are already here. The solution is

593
00:56:38,680 --> 00:56:44,120
already here. It's just not that evenly distributed. And do you think that that follows the logic of

594
00:56:44,200 --> 00:56:50,280
where we think about things like climate change, where it happens in a certain area and slowly

595
00:56:50,280 --> 00:56:56,520
distributes out? Whereas with solutions, they need to be get around everyone very quickly.

596
00:56:56,520 --> 00:57:02,440
And they can't work in that logic of slowly from one center expanding out.

597
00:57:03,560 --> 00:57:11,240
I understand that. I don't think it's fair to say the solutions are already here and

598
00:57:11,240 --> 00:57:16,920
unevenly distributed. It's true for some things. Obviously, we already have a solution to

599
00:57:18,440 --> 00:57:22,120
caloric abundance, but it's not evenly distributed because there's extreme poverty.

600
00:57:22,120 --> 00:57:25,960
That's an example. And that's one we've lived with for a long time. And we can see that it did not

601
00:57:25,960 --> 00:57:33,160
actually pervade out well for certain reasons. To a certain degree and then not beyond.

602
00:57:33,800 --> 00:57:39,080
And the same is still true for running water and hygiene and medicine. There's a very unequal

603
00:57:39,080 --> 00:57:44,440
distribution of problems we have solved. I would say that many of the most critical

604
00:57:44,440 --> 00:57:49,240
issues we need to solve, the solutions don't exist anywhere. It's not true that somewhere

605
00:57:49,240 --> 00:57:58,040
has figured them out well. We actually have to do innovation. How do we solve global

606
00:57:58,040 --> 00:58:02,600
multipolar trap issues is not solved anywhere. And that's the most central thing we have to figure

607
00:58:02,600 --> 00:58:09,720
out. How do we create digital open societies? You can say that it's kind that there are some

608
00:58:09,720 --> 00:58:14,680
places that are trying to pioneer like Taiwan and Estonia. That's true. But those are very far from

609
00:58:14,680 --> 00:58:18,680
have really got worked out solutions that are adequate to all the other places in scale.

610
00:58:24,120 --> 00:58:29,000
I think we have to acknowledge that many of the most critical solutions don't exist at all

611
00:58:29,000 --> 00:58:35,640
and need to become the primary focus of innovation. And then where they do

612
00:58:36,200 --> 00:58:40,840
start to develop, we have to say what type of governance and incentive landscape would be

613
00:58:40,840 --> 00:58:45,480
necessary to get them everywhere they need to be in time. And who would have to be participating

614
00:58:45,480 --> 00:58:50,120
to make that happen? And what kind of oversight and enforcement would be necessary to really make

615
00:58:50,120 --> 00:58:58,360
it happen? We know in the US, the government making deals with Native Americans and then

616
00:58:58,360 --> 00:59:04,840
not keeping them whenever it's inconvenient almost all the time. It's not just about did you

617
00:59:05,560 --> 00:59:09,720
say when you developed a new technology that will get it to the world? Is there a method of

618
00:59:09,720 --> 00:59:14,520
enforcement that will actually ensure that that occurs and that it occurs within time? That becomes

619
00:59:14,520 --> 00:59:24,040
critical. I just had a follow up to that. We talked about how we understand problems and how we

620
00:59:24,040 --> 00:59:30,120
understand solutions. Why do you think certain maxims are held in higher esteem than others?

621
00:59:31,000 --> 00:59:34,520
In another podcast, you talked about survival of the fittest and how we've almost

622
00:59:34,520 --> 00:59:42,200
fetishized that concept above all others. And how can we make sure that other maxims are

623
00:59:42,760 --> 00:59:48,760
discussed in a kind of equal or more celebrated light? And is that there a logic that pervades

624
00:59:48,760 --> 00:59:57,240
a lot of these more harmful maxims? Yeah, it's apologism. So if I win a war and we kill a bunch

625
00:59:57,240 --> 01:00:03,000
of people that we call terrorists or infidels or some bad thing that makes them not human,

626
01:00:03,000 --> 01:00:07,800
but what it means is we blew up a lot of civilians and a lot of women and kids and whatever it was.

627
01:00:08,520 --> 01:00:14,520
But we got more land and resources and whatever it was out of doing that thing.

628
01:00:15,480 --> 01:00:20,520
Survival of the fittest is a nice narrative to say that's how nature works and that's the way

629
01:00:20,520 --> 01:00:27,960
that it should be. And it's actually the predators that keep the prey animals from eating themselves

630
01:00:27,960 --> 01:00:32,120
into extinction and that drive them to evolve by eating the slow ones so that the good genes

631
01:00:32,120 --> 01:00:37,640
kind of inbreed. And most people are like prey animals to the some more predatory humans that

632
01:00:37,640 --> 01:00:44,440
cold heard and that kind of drive them who are otherwise kind of lazy eaters. Like that whole

633
01:00:45,160 --> 01:00:54,360
ideology is apologism for whoever is winning at an extremely damaging rival risk kind of system.

634
01:00:57,000 --> 01:01:03,640
Naive techno-capital optimism is one of the best examples of apologism of this kind where

635
01:01:04,600 --> 01:01:13,480
like if you have a theory that criticizes capitalism, nobody who's winning at capitalism

636
01:01:13,480 --> 01:01:19,560
who has the money is going to upregulate it. And if you are criticizing tech, nobody that was

637
01:01:19,560 --> 01:01:24,040
winning at tech is going to say, yes, I like your idea of why I suck and I'm going to upregulate

638
01:01:24,040 --> 01:01:32,360
that. So you realize that for narratives to catch on, somebody has to upregulate them and

639
01:01:32,520 --> 01:01:38,760
there's cost associated in doing that and there has to be a motive associated with that cost. So

640
01:01:40,040 --> 01:01:44,520
it's not just like the ideas that are the most true and the most beneficial proliferate. The

641
01:01:44,520 --> 01:01:51,560
ideas that have the most agentric basis to drive them through the society are a lot of the ones

642
01:01:51,560 --> 01:01:58,840
that proliferate. The idea which is often held up sort of as counterposed to survival of the fittest

643
01:01:58,840 --> 01:02:04,920
is mutual aid, which is this idea that Peter Kropotkin proposed in the late 19th century and he

644
01:02:04,920 --> 01:02:11,560
essentially saw out there in nature. Actually, it wasn't the species that competed most fiercely

645
01:02:11,560 --> 01:02:18,360
that survived. It was those that actually cooperated that moved into a kind of a situation of symbiosis

646
01:02:18,360 --> 01:02:28,120
if you will. So is that notion of mutual aid, is that a useful reference point for thinking about

647
01:02:28,120 --> 01:02:33,720
these vaccines that need to inform how we move forward? How do we actually begin to have meaningful

648
01:02:33,720 --> 01:02:42,120
productive conversations within the classroom or within the UN forum or within government

649
01:02:42,840 --> 01:02:49,800
corridors of power? How do we begin to chisel away at the memetic sort of structures which

650
01:02:49,800 --> 01:03:01,320
seem to reinforce that particular mindset? If we think through the wrong metaphors, we're obviously

651
01:03:01,320 --> 01:03:07,720
going to come to the conclusions of those metaphors predisposed, but they're the wrong ones, then

652
01:03:07,720 --> 01:03:14,680
there'll be the wrong conclusions. So what kind of animals are humans? Are humans predators?

653
01:03:14,760 --> 01:03:23,320
Are we prey? Are we fungus? Are we slime molds? Are we the relationship between trees and animals

654
01:03:23,320 --> 01:03:29,000
where we can see gas exchange? There's lots of different biological analogies we can try to use.

655
01:03:29,800 --> 01:03:45,400
And none of them apply. So let's say we do the most popular one, which is that we're apex predators.

656
01:03:48,200 --> 01:03:55,640
Pick an apex predator, lion, polar bear, and orca. Orca is maybe the best example, the biggest apex

657
01:03:55,640 --> 01:04:04,600
predator in the ocean. Compare what an orca does to a school of tuna to what an industrial fishing

658
01:04:04,600 --> 01:04:13,800
boat, a commercial fishing boat with a mile long griff net does. The orca misses almost every time,

659
01:04:15,080 --> 01:04:21,080
and when it finally catches one, it catches one, right? And as there's less of them, it misses more

660
01:04:21,080 --> 01:04:29,240
often. And we can pull up the entire fucking school in a net. We're not apex predators.

661
01:04:30,200 --> 01:04:35,480
Apex predators can't do that. If a polar bear decides that it's super pissed off and wants

662
01:04:35,480 --> 01:04:40,200
to go on a rampage and destroy as much stuff as it can, like, what's it going to do? And

663
01:04:41,400 --> 01:04:47,720
look at human nuclear capability if we were similarly disposed. Like, wait, the idea that we

664
01:04:47,720 --> 01:04:53,880
look at, that we don't factor the way that technology means that we are not like the rest of

665
01:04:53,880 --> 01:04:59,560
nature. So of course, we need to see in nature, yes, there's some competitive dynamics and some

666
01:04:59,560 --> 01:05:05,000
cooperative dynamics. This is true. Where there are competitive dynamics, there are mostly

667
01:05:05,720 --> 01:05:13,160
cemeteries of power. The tuna get away as often as the orcas catch them, right? So the slow orcas

668
01:05:13,160 --> 01:05:18,680
die, the slow tuna die, the faster of both happen. So the co-selective pressures have them both kind

669
01:05:18,680 --> 01:05:23,240
of get better together. And so there's the symmetry of power, right? The orca is not a lot more

670
01:05:23,240 --> 01:05:29,960
powerful than tuna in terms of that particular dynamic. And so we can see that if we were to

671
01:05:29,960 --> 01:05:34,280
figure out some way to quantify all the interactions that were happening in nature, almost all of them

672
01:05:34,280 --> 01:05:39,720
are symbiotic, right, of some kind. Some of them are directly rivalrous and competitive. And sometimes

673
01:05:39,720 --> 01:05:45,000
it's kind of both, right? It's a place where the competitiveness at the one-to-one level

674
01:05:45,000 --> 01:05:49,960
ends up leading to symbiosis at the species-to-species level. Obviously, both the predator and the

675
01:05:49,960 --> 01:05:54,920
prey animal depend on each other. Predator dies, prey animal eats itself to extinction,

676
01:05:54,920 --> 01:06:00,920
prey animal dies, predator serves to death. So micro rivalry ends up leading to macro symbiosis

677
01:06:00,920 --> 01:06:05,960
because of the symmetry of power thing, right? So we can see that there are certain types of

678
01:06:05,960 --> 01:06:09,560
competition, but they're limited, the symmetry is power, and then there's a lot of symbiosis.

679
01:06:13,560 --> 01:06:20,680
Well, as soon as humans started making tools, we were able to hunt any species to extinction

680
01:06:20,680 --> 01:06:24,520
anywhere and go become the apex predator in any environment and more powerful than the apex predator

681
01:06:24,520 --> 01:06:32,760
in any environment. We broke the symmetry, right? We became more lethal predators faster than the

682
01:06:32,760 --> 01:06:38,120
environment could evolve to become more resilient to it. As a result, that was the beginning of an

683
01:06:38,120 --> 01:06:42,680
extinctionary process that was following an exponential curve that was slow for a long time

684
01:06:42,680 --> 01:06:46,040
from stone tools and started to really pick up with agriculture, then really pick up with the

685
01:06:46,040 --> 01:06:52,360
industrial revolution is now verticalizing in modern tech world. But stone tools were kind

686
01:06:52,360 --> 01:06:58,440
of the beginning of it, right? And the other stone tools and language and that type of coordination

687
01:06:58,440 --> 01:07:09,400
that came along with the abstraction capacities. So do humans need to ensure, as the metaphors of

688
01:07:09,400 --> 01:07:14,520
nature go, that where we have competition, that it's symmetrical and that it's constrained

689
01:07:15,320 --> 01:07:20,200
and that the micro competition really does lead to macro symbiosis? We need to ensure that. This is

690
01:07:20,200 --> 01:07:29,080
true. Is the competition between Facebook for your attention and you for your attention symmetrical?

691
01:07:29,960 --> 01:07:33,880
No, of course not. Well, you say, well, there's a competition, the competition

692
01:07:34,440 --> 01:07:39,480
between supply and demand is symmetrical because there's an equal number of dollars flowing from

693
01:07:39,480 --> 01:07:47,080
demand to supply. Bullshit, right? The demand side is not coordinated. The supply side's coordinated.

694
01:07:47,560 --> 01:07:52,600
And so even though there's a total symmetry and aggregate, there's not a symmetry of coordinated

695
01:07:52,600 --> 01:07:58,120
capacity because it isn't Google against all Google users as a Google user labor union

696
01:07:58,840 --> 01:08:03,640
that is also applying similar exponential technologies to bind this thing. It's Google

697
01:08:03,640 --> 01:08:07,560
against one person in terms of the person didn't think that they were about to spend the next

698
01:08:07,560 --> 01:08:10,920
three hours on YouTube and now they do, which is better for their advertising model, not necessarily

699
01:08:10,920 --> 01:08:17,880
for your life. That kind of, and so then you can have supply side driving manufactured demand.

700
01:08:19,240 --> 01:08:24,280
Well, now there's not real, market ideology is broken now. That's not a market ideology,

701
01:08:24,280 --> 01:08:29,960
was that there was a thing called demand that was foundational, that people wanted real

702
01:08:29,960 --> 01:08:33,640
shit that would improve the quality of their life and that created an environmental niche for supply

703
01:08:34,520 --> 01:08:39,640
and the rational actors would buy the product or service amongst all of them at the best price

704
01:08:39,640 --> 01:08:43,960
that would drive innovation. Well, the moment supply started to get much bigger than demand

705
01:08:43,960 --> 01:08:48,920
because of coordination, it realized that it could manufacture supply and the humans weren't

706
01:08:48,920 --> 01:08:54,440
all that rational, all the behavioral economics. And now the entire logic of markets is broken,

707
01:08:55,080 --> 01:08:59,400
right? Like market theory is broken with manufactured demand and radical asymmetries

708
01:08:59,400 --> 01:09:06,200
on the supply side. Okay, that's important to know. And so if you go back to the nature example,

709
01:09:06,200 --> 01:09:10,200
where there's competitive forces, do they need to have symmetries in order for them

710
01:09:10,200 --> 01:09:15,560
competition to lead to symbiosis as a whole and metastability of the ecosystem? Yes. If you bring

711
01:09:15,560 --> 01:09:21,000
something in that is not symbiotic with the rest of it, you get an invasive species that can destroy

712
01:09:21,000 --> 01:09:29,080
a whole ecosystem, right? So we should study biology where we're not trying to compare ourselves to Apex

713
01:09:29,080 --> 01:09:33,240
predators or slime molds or whatever. We could just study general principles of things like

714
01:09:33,800 --> 01:09:39,640
cooperative dynamics and competitive dynamics and metastability. We can kind of get a sense of that.

715
01:09:39,640 --> 01:09:42,920
What is needed for metastability? And then say, how does that apply in the human world? But it

716
01:09:42,920 --> 01:09:46,760
will be different. It'll be very different. The rest of the animal world is not forecasting the

717
01:09:46,760 --> 01:09:53,720
future and making game theoretic decisions based on forecasts of the future. And so this is why,

718
01:09:53,720 --> 01:09:57,960
like complexity theory, where we model us as termites is silly, like we don't behave like

719
01:09:57,960 --> 01:10:04,040
termites. So it's not that it's useless, but it's profoundly inadequate as a set of metaphors. So

720
01:10:04,040 --> 01:10:11,880
we have to recognize our human's part of nature, of course. Is there a distinction between humans

721
01:10:11,880 --> 01:10:16,760
and the rest of nature that is fundamental in type? Maybe it was just a change of quantity of

722
01:10:16,760 --> 01:10:21,080
neurological complexity that crossed a threshold that became a change of kind, but it is a change

723
01:10:21,080 --> 01:10:28,760
of kind. And so we will have to have fundamentally different metaphors for thinking about that,

724
01:10:28,760 --> 01:10:32,680
which is why it makes sense to just think about the problem space and make sure that

725
01:10:32,680 --> 01:10:37,240
you understand the problem space well and that your solutions are aligned with the problem space.

726
01:10:39,800 --> 01:10:48,920
Yeah, yeah. What an exciting research agenda. And of course, an agenda to live by as well and to

727
01:10:48,920 --> 01:10:53,560
engage with deeply. And another maxim comes to mind, perhaps, which would be know thyself.

728
01:10:54,600 --> 01:11:01,000
It's not just a situation of impersonal inexorable forces bearing down on us, but we're also talking

729
01:11:01,000 --> 01:11:07,160
about systems of, I think, human intentionality, which raises the crucial issue which we discuss

730
01:11:07,160 --> 01:11:14,600
with Forest Landry in an earlier podcast on how do we make good choices, which perhaps our education

731
01:11:14,600 --> 01:11:19,640
systems are not really equipping us with the tools we need to answer that really important

732
01:11:19,640 --> 01:11:25,320
question. I know that Zoe's got a question. I want to hand this over to Zoe. So go for it, Zoe.

733
01:11:26,040 --> 01:11:31,000
So I kind of building on sort of the meta, we have the wrong metaphors, I guess, we're using

734
01:11:31,000 --> 01:11:37,800
the wrong ways of thinking. I kind of wanted to know how do we deal on like, on a societal and

735
01:11:37,800 --> 01:11:41,880
a personal level with the amount of cognitive dissonance I think we're existing in, because

736
01:11:42,520 --> 01:11:47,880
I think part of the difficulty with coming up with solutions is that some of the challenges

737
01:11:47,880 --> 01:11:52,520
are so overwhelming that I feel like majority of people just kind of stick their head in the sand

738
01:11:52,520 --> 01:11:56,440
and they're like, no, and so we're existing in like, I feel perpetual cognitive dissonance.

739
01:11:56,440 --> 01:12:02,280
And I was kind of wondering what your take on that was and how, yeah, how do you deal with it

740
01:12:02,280 --> 01:12:08,680
personally and how does a young person who's trying to sort of move forward in society deal with

741
01:12:08,680 --> 01:12:14,600
that as well without, and, you know, exist as a functioning member of society without sacrificing

742
01:12:15,160 --> 01:12:18,840
maybe personal ethics and values, even though I kind of, I guess I know that I'm going to have to

743
01:12:18,840 --> 01:12:30,600
compromise somewhere down the line.

744
01:12:39,240 --> 01:12:44,680
You as an individual probably can't solve those issues,

745
01:12:45,560 --> 01:12:50,600
probably not one of them, let alone all of them.

746
01:12:53,480 --> 01:13:00,440
And you can't focus on it and really look at it and feel the scope of the current harm and the

747
01:13:00,440 --> 01:13:08,680
possible harm and not be able to do anything about it and have continuing to look at it make any sense.

748
01:13:09,240 --> 01:13:22,680
So let's say that our social institutions were adequate, as they were at previous points, to deal

749
01:13:22,680 --> 01:13:30,040
with whether they were adequate or not depends upon which group you were a part of and which

750
01:13:30,040 --> 01:13:34,840
problems you were looking at. But let's just take for a moment that for some things they were adequate.

751
01:13:34,840 --> 01:13:40,760
But then if there was a problem you really wanted to solve, you could think about joining

752
01:13:42,280 --> 01:13:50,440
the CDC to work on pandemics or joining the military to work on terrorist-mediated

753
01:13:50,440 --> 01:13:53,960
ex-risk or joining an intelligence group or whatever it was.

754
01:13:58,360 --> 01:14:03,720
If you look around and you see that the scale of the issues requires institutional solutions,

755
01:14:04,680 --> 01:14:10,840
whether they're state or network-based decentralized autonomous organization or whatever,

756
01:14:10,840 --> 01:14:15,400
but collective intelligence of lots of people, not just a person.

757
01:14:16,680 --> 01:14:21,000
And you don't see anyone that is currently doing that, then there isn't something you can join,

758
01:14:21,000 --> 01:14:29,160
then what do you do? It's a tricky problem because there's a fairly small number of people that have

759
01:14:29,160 --> 01:14:33,320
the right psychological disposition to try to found something of that type.

760
01:14:35,560 --> 01:14:43,400
There are a few people who are either going to try to start a new type of company or a new type of

761
01:14:44,280 --> 01:14:47,240
non-profit or a new type of social movement or whatever.

762
01:14:52,680 --> 01:14:56,200
There's a lot of people that can contribute value to one of those that are probably not

763
01:14:56,200 --> 01:15:00,520
going to found it for really not just developmental but typological reasons,

764
01:15:00,520 --> 01:15:02,360
different typologies or into different things.

765
01:15:07,160 --> 01:15:10,360
So to the degree that there are particular issues you care about

766
01:15:12,280 --> 01:15:16,360
and you can find organizations that are doing a pretty good job that you could

767
01:15:16,920 --> 01:15:22,120
join or participate with, that's a good answer. It's something of an answer.

768
01:15:26,200 --> 01:15:30,520
To the degree that you feel like you have the typological orientation to make a new thing

769
01:15:30,520 --> 01:15:35,480
or to be part of make a new thing, to find other people that could co-found some kind of new process,

770
01:15:35,480 --> 01:15:40,120
whether it is trying to get an upgrade to an institution with existing government,

771
01:15:40,120 --> 01:15:46,280
build a new institution, build Ethereum, some kind of platform for decentralized

772
01:15:46,280 --> 01:15:50,360
autonomous organization that maybe will create the future of governance via networks rather than

773
01:15:50,360 --> 01:15:57,720
nation-states. Those are all possibilities for, is there some new capacity that I believe

774
01:15:58,440 --> 01:16:00,920
is needed that I could help to bring into being?

775
01:16:04,680 --> 01:16:11,240
So either you have to join something or you have to make something or you have to join

776
01:16:11,240 --> 01:16:15,400
people that are interested where maybe somebody in that scene or some combination of them will

777
01:16:15,400 --> 01:16:26,600
be able to make a thing. And it's very hard to know that the thing that you're focused on,

778
01:16:31,080 --> 01:16:35,000
even if it's awesome, is not adequate to the scope of issues you're aware of

779
01:16:35,000 --> 01:16:41,880
and put all your energy into it and not go nihilist or just anxious all the time.

780
01:16:45,800 --> 01:16:56,520
So for a lot of people, I would say they should put their sense-making into things that they feel

781
01:16:56,520 --> 01:17:03,000
like they have agency in or could develop agency and that there's some relationship between their sense-making

782
01:17:03,000 --> 01:17:12,360
and their agency. So let's say they feel like, okay, well, I don't know how to fix AI, AGI risk issues.

783
01:17:13,240 --> 01:17:19,000
Silicon Valley attentionalism issues. But I feel like if I apply my sense-making to the problems

784
01:17:19,000 --> 01:17:23,560
in my community, I could actually help improve the quality of life of my community. I could

785
01:17:23,560 --> 01:17:28,680
bring warm data labs there and have the people start really getting to know each other in a

786
01:17:28,680 --> 01:17:34,200
multi-contextual way much better. I feel that kind of thing. If a lot more people did that,

787
01:17:34,200 --> 01:17:37,640
they paid attention to where they could have agency, applied their sense-making there,

788
01:17:38,440 --> 01:17:44,360
a lot of problems would get better. And a lot of other people in those communities would evolve

789
01:17:44,360 --> 01:17:49,000
to want to do things as well. And some of them would have different aptitudes and people communicating

790
01:17:49,000 --> 01:17:55,000
better would have better collective intelligence. And once you solve problems at one scale, you get

791
01:17:55,000 --> 01:17:58,440
better at problem solving. You might be like, maybe I can do this for a second community. Oh,

792
01:17:58,440 --> 01:18:02,200
I've just figured out a generalized principle. Maybe I can help create a way to do this for

793
01:18:02,200 --> 01:18:08,680
communities writ large. All of a sudden, it starts to be able to kind of inductively scale to the

794
01:18:08,680 --> 01:18:17,560
scope of the problems. So one thing I would say is like one approach is just try to understand

795
01:18:17,560 --> 01:18:21,240
what the world needs without understanding what you can do. Just take you out. Just what does

796
01:18:21,240 --> 01:18:25,720
the world need? Because as you come to understand that better, you'll start to have insights of

797
01:18:25,720 --> 01:18:30,840
what needs to happen. And then you'll at least be able to parse where are the places doing closest

798
01:18:30,840 --> 01:18:35,240
stuff? What is nobody doing? How do I help make that happen? Right? That's one approach.

799
01:18:36,600 --> 01:18:40,840
The other approach is what is the stuff I feel like I could do and how do I apply my study to be

800
01:18:40,840 --> 01:18:45,640
able to do some of those things where then in the process, I can be increasing my agency

801
01:18:46,520 --> 01:18:51,400
to then possibly be able to converge towards doing more stuff. Both of those are valid

802
01:18:53,800 --> 01:18:55,480
on their own and in combination.

803
01:18:55,880 --> 01:19:05,080
What I would say is that you're increasing your understanding of the world, that you're increasing

804
01:19:05,080 --> 01:19:12,200
your sense of your ability to act meaningfully, and that you're increasing both the depth of care

805
01:19:12,200 --> 01:19:18,120
and the emotional resilience in the presence of that care simultaneously are things you want

806
01:19:18,120 --> 01:19:22,120
to be tending to. There's not one good answer for how to do that, but they're things you want to be

807
01:19:22,120 --> 01:19:31,800
tending to. Now, we spoke briefly before the recording started about the very real practical

808
01:19:31,800 --> 01:19:38,840
inquiry of what kind of jobs are there in the space. And it's if, let's say, working in existential

809
01:19:38,840 --> 01:19:43,080
catastrophic risk are some of the most important areas in the world and pioneering new types of

810
01:19:43,080 --> 01:19:50,760
social technologies that both apply and combine physical technologies. If these are some of the

811
01:19:50,760 --> 01:19:56,360
most important areas, but there's not really jobs, there's not financial incentive there.

812
01:19:56,360 --> 01:20:01,160
And as you're focused on them, there's more emotional difficulty and psychological difficulty

813
01:20:01,160 --> 01:20:07,480
associated with looking into the abyss. The incentive landscape is wrong for getting the

814
01:20:07,480 --> 01:20:13,720
people engaged in the things that matter. So institutionally, we should try to fix that

815
01:20:13,720 --> 01:20:17,880
and say, how do we start to put incentive on the things that matter the most, which the Manhattan

816
01:20:17,880 --> 01:20:23,960
Project did, which is why I'm calling for Manhattan Project type things. And in some ways, you can say

817
01:20:25,480 --> 01:20:30,920
Ethereum and Holochain and other orgs are trying to do that. So maybe some of Elon's

818
01:20:30,920 --> 01:20:34,280
companies, whatever, we're taking on a problem and we're trying to be able to create a lot of

819
01:20:34,280 --> 01:20:37,560
jobs and incentive to get people to be able to work on problems that matter.

820
01:20:37,960 --> 01:20:54,520
But the other part of that answer I'll just share is for me, a big part of, because I was thinking,

821
01:20:54,520 --> 01:20:57,640
like there's a lot more people thinking about X-Rest now, but I was thinking about it from

822
01:20:57,640 --> 01:21:03,240
quite young age. I just knew I couldn't focus on anything else and I couldn't focus on anything

823
01:21:03,320 --> 01:21:07,080
that wouldn't converge to being adequate. It was okay if what I was working on wasn't

824
01:21:07,080 --> 01:21:11,400
adequate. It just had to seem like it was on a path of increasing understanding and capacity that

825
01:21:11,400 --> 01:21:22,440
could maybe converge. So I kept, for most of my life, my overhead as close to nothing as I could

826
01:21:22,440 --> 01:21:28,040
keep it and figured out things that I could do for work that took the least amount of time possible.

827
01:21:28,120 --> 01:21:35,960
So most of my time didn't have any market need on it. Most of my time was self-directed study in

828
01:21:35,960 --> 01:21:40,840
these areas because that was the only thing I could actually do and be congruent with myself.

829
01:21:40,840 --> 01:21:45,960
So sometimes I did construction to pay the bills, sometimes I did teaching or I became a therapist

830
01:21:45,960 --> 01:21:49,640
and did different things, but I kept my bills low enough that it didn't take that many hours.

831
01:21:49,880 --> 01:21:56,040
And so most of my time could just be allocated based on my intrinsic

832
01:21:57,400 --> 01:22:01,240
orientation of what would be most meaningful, which I highly recommend that path.

833
01:22:07,000 --> 01:22:11,080
Brilliant. Well, thank you, Daniel. I think we're rolling to a close.

834
01:22:12,200 --> 01:22:17,880
We've covered a lot of ground. It's been really an exciting conversation. I hope we'll have a chance

835
01:22:17,960 --> 01:22:24,200
to continue this another time. It does seem that we are in something of quite a sort of

836
01:22:25,240 --> 01:22:30,280
incredible moment, possibly a unique moment. We're facing a lot of daunting challenges

837
01:22:31,480 --> 01:22:35,800
and we're all trying to grapple with what that means, I guess, for us personally, professionally.

838
01:22:37,080 --> 01:22:44,520
For me and in the seminar room, in the university, in society, in my interactions with my loved ones.

839
01:22:45,240 --> 01:22:53,080
But I guess it's also, in some ways, it's a time of opportunity as well. It's kind of a

840
01:22:53,080 --> 01:22:57,720
cool to adventure, as you sort of said, is there anything more important than really sort of

841
01:22:58,360 --> 01:23:01,400
putting your shoulders to the wheel on some of these issues that we've addressed?

842
01:23:02,440 --> 01:23:07,640
And yeah, I'd just like to say thank you for all of your work. And I don't know if you have any

843
01:23:07,640 --> 01:23:11,400
final closing thoughts, anything that we haven't covered that you'd just like to share with us,

844
01:23:11,400 --> 01:23:17,080
to close? It is a thought that comes to mind, kind of following where we just were.

845
01:23:20,280 --> 01:23:29,400
One way I think about how to live a meaningful life, a simple but kind of elegant model is

846
01:23:33,320 --> 01:23:38,920
we can think about life in terms of the mode of being, the mode of doing, and the mode of becoming.

847
01:23:39,160 --> 01:23:50,440
And if you were to describe the mode of being, it is, in the moment, focused on appreciating

848
01:23:50,440 --> 01:23:57,480
what already is, appreciating the beauty of life as it is. The mode of doing occurs in time,

849
01:23:57,480 --> 01:24:02,680
and it's focused on adding beauty to life. If it's focused on anything else, it's not the

850
01:24:02,680 --> 01:24:07,720
mode of doing very well, right? Most people are in the mode of doing, doing shit that if they

851
01:24:07,720 --> 01:24:13,000
didn't do it, the world would be better. But the mode of doing that matters for a meaningful life

852
01:24:13,000 --> 01:24:20,360
is adding beauty to life and or protecting, serving the beauty that's there. The mode of

853
01:24:20,360 --> 01:24:26,680
becoming is increasing your capacity to appreciate life as it is more fully, and to add to beauty

854
01:24:26,680 --> 01:24:33,000
more fully, right? Increasing being and becoming and being and doing. So then there's a virtuous

855
01:24:33,000 --> 01:24:39,560
cycle between those. But the doing only matters and the becoming only matters because of the

856
01:24:39,560 --> 01:24:45,560
intrinsic meaningfulness of being. If ultimately the meaningfulness is grounded in experience,

857
01:24:45,560 --> 01:24:49,960
and the fact that experience is just intrinsically beautiful, that taking reality is intrinsically

858
01:24:49,960 --> 01:24:55,880
beautiful. So if you, because of the crises, you don't focus on that enough, you'll actually get

859
01:24:55,880 --> 01:25:02,040
disconnected from the source of what matters. And then your, your motivational complex will,

860
01:25:02,760 --> 01:25:11,800
if I, if I wake up, so like, I wake up, I go sit outside with a cup of coffee and I look at the

861
01:25:11,800 --> 01:25:20,040
trees. And I just love watching the trees move in the wind and the clouds in the sky and just

862
01:25:20,040 --> 01:25:25,480
like how beautiful this planet is, how much I appreciate it. And there is a fullness in that

863
01:25:25,480 --> 01:25:30,600
mode of being that doesn't need anything. So then I'm not motivated based on what's in it for me,

864
01:25:30,600 --> 01:25:35,080
because I already feel like I could die right now. And I feel lucky, right? I feel like I have lived

865
01:25:35,080 --> 01:25:42,200
a really rich full life. So now it's not what's in it for me. It's not some doing that I have to do.

866
01:25:42,200 --> 01:25:48,200
It's that I actually want to protect that beauty. And I want to protect other people's ability to

867
01:25:48,280 --> 01:25:55,320
keep experiencing it forever, or at least for a long time, because I can. And because as much as I

868
01:25:55,320 --> 01:25:59,560
appreciate it, other people do too, or can and I, that matters to me, right? Like it's intrinsically

869
01:25:59,560 --> 01:26:06,120
meaningful. But that's a different come from it. It has a certain anxiety and angst and feverishness

870
01:26:06,120 --> 01:26:13,880
that isn't there. And it has a sacredness that is there. And then there's also a courage of like,

871
01:26:13,880 --> 01:26:20,600
maybe I fail. I mean, maybe we fail, right? And life has been meaningful each moment. It's not

872
01:26:20,600 --> 01:26:24,120
like it wasn't meaningful like it. Okay, maybe the whole thing comes, this whole thing, part of it

873
01:26:24,120 --> 01:26:29,160
comes to an end at some point. But I will do what I can to be in service to it. But that service is

874
01:26:29,160 --> 01:26:33,640
arising out of seeing it and loving it as is, and then wanting to be of service from there.

875
01:26:36,680 --> 01:26:42,520
So I can be in the mode of being just kind of chill and watching TV. I can be in the mode of

876
01:26:42,520 --> 01:26:46,120
doing doing a bunch of to do this shit that doesn't really matter. I can be in the mode of

877
01:26:46,120 --> 01:26:53,640
becoming trying to get better at doing shit that doesn't matter. I want to think about am I engaging

878
01:26:53,640 --> 01:26:59,000
in each of those modes? And am I engaging in them deeply? If I'm in the mode of being, I want to be

879
01:26:59,000 --> 01:27:03,160
looking at the sky and I want to be listening to music, I love and be wrapped. I want to be

880
01:27:03,160 --> 01:27:07,720
feeling moved by the beauty of life. So why do the mode of being any other way? I want to be with

881
01:27:07,720 --> 01:27:13,800
friends that I love where I'm like, yeah, I could die right now full. And in the mode of doing,

882
01:27:13,800 --> 01:27:19,960
I want to know that the world would be worse if I didn't do this. Otherwise, I go back to the mode of

883
01:27:19,960 --> 01:27:26,440
being, just chill and enjoy it. I want to know that the thing I'm doing adds something of meaning

884
01:27:26,440 --> 01:27:33,160
somewhere, right? And the mode of becoming of am I am I developing my ability to appreciate

885
01:27:33,160 --> 01:27:38,760
everybody and everything around me? And am I developing my knowledge and agency and capacities

886
01:27:38,760 --> 01:27:46,120
to add to it? That's a good framework to think about, you know, when you inventory your day and

887
01:27:46,120 --> 01:27:52,840
your week, what being on track means. Wonderful. Well, thank you, Daniel. Thanks so much. And

888
01:27:53,400 --> 01:27:59,240
if people want to engage more with you and your work, your website is called Civilization Emerging.

889
01:27:59,800 --> 01:28:04,280
Is that correct? Civilization Emerging is just like a personal blog where there's some podcasts

890
01:28:04,280 --> 01:28:09,080
and old stuff up there. And you can check it out. And the project that we're focused on that's

891
01:28:09,080 --> 01:28:14,040
really just in the earliest beta phase right now. But that is kind of the project where we're

892
01:28:15,800 --> 01:28:21,400
trying to bring the information forward that will help decentralize innovation of what the new

893
01:28:21,400 --> 01:28:26,360
social technologies that can employ and guide exponential technology are. That project is

894
01:28:26,360 --> 01:28:30,680
called the conciliants project conciliants project.org. And that will get increasingly

895
01:28:30,680 --> 01:28:36,360
interesting over the next, you know, few months. Yeah. And I'm hoping that we'll have a conversation

896
01:28:36,360 --> 01:28:41,400
about how I contribute to that project. Super exciting. And I hope people will go and check out

897
01:28:41,400 --> 01:28:47,800
that website. Thank you, Daniel. Look forward to picking this up again at some point. Take care.

898
01:28:48,520 --> 01:28:50,200
Thank you. It was good to be with the three of you.

899
01:28:50,840 --> 01:29:02,360
Thanks for tuning into Imperfect Utopias. To get access to all of our content and to stay up-to-date

900
01:29:02,360 --> 01:29:12,600
with future Zoom calls, workshops and events and more, check us out at ucl.ac.uk forward slash

901
01:29:12,600 --> 01:29:19,480
global dash governance. If you like this content, please do leave us a comment and subscribe.

902
01:29:20,200 --> 01:29:27,080
Till next time.

