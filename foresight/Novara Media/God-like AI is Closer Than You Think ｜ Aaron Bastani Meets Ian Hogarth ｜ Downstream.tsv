start	end	text
0	6240	CEO of OpenAI, he was interviewed about the worst case scenario. This stuff goes badly wrong.
7280	11280	And he had this line where he said, worst case scenarios lights out for humanity. And that was
11280	16640	a statement that Sam Ortonman, who's the CEO of OpenAI, the developers of ChatGPT and GPT4,
16640	21840	made about that existential risk. Now, I don't think he's saying that about next year.
22400	26640	But the real question is, if we build an incredibly powerful intelligence system before
26640	30720	we figure out how to make it safe, I think we should have relatively low confidence it's going
30720	40240	to go well for humanity. Artificial intelligence has been a topic of political conversation
40240	45520	for quite some time. I should know. I partly wrote a book about it. It's called Fully Automated
45520	49760	Luxury Communism. Though I must say, when I wrote that book and when it was published,
49760	54800	many people were skeptical about the rise of the robots and how technological change would
54800	60480	disrupt the economic status quo. Sure, they would say, Aaron, this might be coming in the
60480	67200	2040s or 50s or 60s or maybe when we're all dead in the 2100s. But this is not a problem
67200	73600	for right now here in the 2020s. Yet with the development of ChatGPTO the last 12 months,
73600	78800	people are finally starting to have that conversation. Maybe artificial intelligence
78800	84720	is far more developed than we realize. And with exponential improvements, perhaps it's getting
85120	91840	out of control. Ian Hogarth is a founder and investor in technology companies. He wrote a
91840	98000	brilliant article in the FT Weekend Magazine talking about all of these issues and how,
98000	106080	in fact, we may need political regulation to catch up with technology. Ian, welcome to Downstream.
106080	110720	Thanks for having me. It's our pleasure. We're going to talk about some really big issues which
110720	115280	a lot of our audience may not be that acquainted with artificial intelligence, machine learning,
115280	120480	et cetera. Why do you have authority on these topics and why should they listen to you over the next
120480	127600	hour? I'm not sure I have that much authority. I can tell you my background. I studied machine
127600	132400	learning at university. I did a masters, started off with making a robot and then after that made
132400	137760	a computer vision system. So that's systems where you basically teach a machine to see in some way,
137760	142960	recognize patterns, visual patterns. And the system that I built was one that could replicate
142960	147680	some of the job that a radiologist does when looking at cancer biopsy images. So I sort of
147680	152640	started out with, I suppose, a very engineer's mindset thinking about this. I then built a
152640	158800	software business as a founder and a CEO for a number of years. And then I've kind of been
159600	167120	involved in this since then as really an investor and I guess an academic. So as an investor,
167920	173680	invested at about 50 companies applying AI to different fields, including some of the larger
173680	178320	companies in the field like Anthropic, which is the second most funded AI startup in the world,
179040	184880	and Helsing, which is the leading AI defense company. And then outside of that, I've really been
184880	195200	trying to expand the level of public awareness of what is happening in AI, primarily by writing a
195200	199600	report called the State of AI Report, which I've written for the last five years. It's one of the
199600	203840	most widely read annual reports on everything happening in machine learning over the course of
203840	209920	a year. So I've been sort of trying to just expand the kind of the quality of public information
209920	214400	out there for policymakers or for citizens who are interested to find out more.
214400	218320	That's quite an authority. I mean, I like the humility at the start, but I think you're going
218320	225920	to know what you're talking about. And for people who aren't quite clear about your writing,
225920	229520	I mean, I suppose a quick stop for them would be this recent article you wrote in the FT weekend
229520	232960	magazine. Can you just briefly go over some of the issues you discussed in it?
233600	242400	Yeah, so I wrote that article maybe a month ago now. And the core idea is that there are a small
242400	248400	number of incredibly well-funded private companies, primarily in London and San Francisco,
249280	253600	that are kind of locked into a race, where they're all racing as fast as possible to build
254320	260320	what they describe as AGI. And AGI is basically a kind of godlike AI system that is capable of
260320	266960	doing almost anything a human can do and more. And I felt like that race is now getting quite
266960	272000	out of control. And we need to slow it down. And so I wrote that essay to really just try to
273840	279600	shine a light on some of the things that I learn about as part of my work,
279600	283440	but that are maybe not public domain. So I spend time talking to the people running these
283440	290160	organizations. And I just sort of could see that the level of concern behind closed doors had really
290160	294320	ratcheted up. And it felt like there was a big disconnect between the kind of public discourse
294320	298160	and what people were saying in private. So I wrote the essay really just to try to
298160	302080	close that gap a little bit. That's so interesting. So we're talking really about a machine which is
302080	306160	capable of augmenting its own intelligence. And very quickly, you get a super intelligence,
306160	310400	so to speak, an intelligence that we can't really fathom as human beings. You said that the fears
310400	314640	and the concerns around that had ratcheted up. Over what time frame are we talking here?
315200	321840	So I suppose maybe it's worth zooming out and just talking about progress in the field in general.
321840	330400	So if you sort of look at AI systems over the last decade, they've quite predictably gotten bigger.
330400	334000	And so what we've done is every year, we've been kind of increasing the amount of computing resources
334000	338720	would give the largest AI models. And we've also been giving those systems more data to train on.
338720	343360	And so that's been actually a very consistent exponential curve that's been running now for
343360	348400	over a decade. And there've really been a couple of big kind of moments in time. The first was the
348400	353280	founding of DeepMind, which really just brought a huge amount of ambition and energy to this challenge
353280	357760	of like making these even bigger and more powerful systems. And the second I would say was OpenAI,
357760	362080	which introduced a competitor DeepMind that suddenly meant there was a race. And those
362080	367280	organizations have been racing against each other now for best part of their entire founding history.
368000	373040	And if you look at that, we've gone from kind of, you know, feeding these systems, you know,
373920	379120	some tens of thousands, you know, millions of images to feeding these systems most of the
379120	383440	internet. And we've increased the amount of computing resource we give these kind of most
383440	389440	powerful AI models by a factor of 100 million in just a decade. And so there's been this very,
389440	395120	very, you know, continuous progress in the field. But as with any exponential, it's really only when
395120	399040	you get to the steeper of the curve, you start feeling it. And I think the last couple of years
399040	402960	are kind of busy, where the curve has just suddenly felt a lot steeper and things have
402960	406080	been changing weekly or daily rather than yearly.
406880	411120	Wow, that is really extraordinary. So the word exponential of people who aren't necessarily
411120	416640	familiar with it, my goodness, virtually everybody is in 2023. But this was broadly
416640	421120	integrated within discussions around computer science by Gordon Moore and Moore's law. And
421120	425200	this idea that broadly speaking, computational power would, there's a bunch of ways of sort of
425200	429760	discussing it. But the same amount of power would basically halve and cost every 18 months to two
429760	434880	years. And that has happened for a long time, it's kind of decelerating, it's happened for a very
434880	440240	long time. You were saying with AI, there's a bunch of variables. So it's not just the computational
440240	445440	power, it's also the data that it's feeding on. And the two of these is important, right?
445440	452000	Correct. And so, for your listeners, an exponential, thinking about it simply is,
452000	457040	for example, a system that doubles every year. And so if you play that out over a number of
457040	461520	years, you get a very steep curve, because some property of the system is allowing it to kind
461520	466640	of grow in that way. And the classic we saw was with COVID. But we as humans, I think, are just
466640	472000	really poor at thinking about exponentials. They're not intuitive to us. And so we saw it with
472000	477040	COVID. It's kind of January, people start paying a bit of attention, February things get more serious,
477040	481440	March, some people really start to get, and then we're suddenly locked down. And that's kind of the
481440	486880	nature, I think, of a system where you're having a doubling effect over some period of time. And
486880	493520	that's what's been happening in AI for a decade is just we're now at the kind of February 2020
493520	496720	moment in AI where things are just going super, super quickly.
496720	501360	That's such a powerful analogy. Obviously, you can't go into the nature of private conversations
501360	505360	you've had with people. But when people are putting a date on it, what are they saying
505360	509200	with regards to an AI then? And like I said, it's hard to predict by virtue of exponential growth,
509200	516000	but are they saying in the 2020s, next year? I mean, what's the broad time frame here?
516000	520480	Well, so the first thing I'd say is that the people leading these companies have been thinking
520480	526160	about this problem for a long time. Some of them have been kind of a good example of a Shane leg.
526640	531520	Someone I admire greatly, he's a brilliant computer scientist. He runs DeepMind's AI
531520	535680	alignment efforts, which we'll probably talk about in a bit, what that is as an area.
536560	541520	But Shane, he did his PhD on sort of a computational basis for machine super
541520	545440	intelligence, has made many sort of quite sophisticated predictions over the years around
545440	549520	what it would exactly take in terms of the amount of computing resource and the amount of data before
549520	553280	you would actually get a super intelligent machine that was kind of an artificial general
553280	557680	intelligence. And so the people in this field have been thinking about it a long time,
558320	563680	and I pay the most attention to the people who have been consistently making good predictions
564400	569280	to me behind closed doors about what will happen. And the thing that I've noticed is
569280	572800	it used to be the case that people would say stuff like, you know,
572800	579680	it's possible we might get an AGI, a super intelligent machine in the next 30 years or the
579680	583520	next 20 years, but everyone I think thought the idea of kind of something happening next year
583520	588880	was kind of ridiculous. And now I think if you ask people, you know, let's say, for example,
588880	593600	you know, there was a select committee and the various leaders of these labs, the technical
593600	598320	leaders were asked under oath, what's their probability that we get a super intelligence
598320	602720	next year, it's not going to be 0%. Whereas I think it would have been before, whereas now it
602720	609120	might be, I don't know, 5%. And so you have this kind of shift where I think everybody is starting
609120	613280	to sort of say, actually, we might be closer than we realized, we should start taking it
613280	619120	seriously the possibility that we might be very close. So if a private enterprise develops an AGI
619120	621520	and artificial general intelligence, what happens next, do you think?
623600	630560	I have no idea. I think that the AI alignment community would basically say, you know,
631840	637040	most likely outcome is we're all dead. The CEO of OpenAI, he had this kind of,
637120	641120	it was, you know, interviewed about the worst case scenario, this stuff goes badly wrong.
642160	646000	And he had this line, we said, you know, worst case scenarios, lights out for humanity. And that
646000	651520	was a statement that Sam Ortman, who's the CEO of OpenAI, the developers of ChatGPT and GPT-4,
651520	656720	made about that kind of existential risk. Now, I don't think he's saying that about next year.
657280	661920	But the real question is, if we build an incredibly powerful intelligence system before we figured
661920	666080	out how to make it safe, I think we should have relatively low confidence it's going to go well
666080	673360	for humanity. And there's a kind of very, you know, I guess, sophisticated intellectual argument
673360	678160	about how to think about that. And that's the sort of thing that someone like Eliezer Yudikowski
678160	684960	would write about, where he'll talk through the sort of exact mechanisms, but why a system that's
684960	691600	much more intelligent than humans treats us, you know, treats us badly, you know, primarily by
691600	696960	accident. But I think actually the kind of common sense way of thinking about it makes more sense,
696960	704080	which just says, you know, humans have kind of changed the environment on earth very significantly
704720	708720	as a result of our intelligence relative to other species. And that's had, you know,
708720	712880	significant consequences for some species, and for the biosphere in general. And I think we
712880	717200	should sort of just common sense tells you that something similar might happen if we invent something
717840	718880	more intelligent than us.
720560	725200	Wouldn't the counter argument be, I suppose, that though that we've learned that over time,
725200	730880	we are dependent on the biosphere for our own systems, political, social, economic to sustain.
730880	735600	I suppose as they're not an optimistic account of an AGI, and I think obviously there's a great
736560	742000	deal of thought put behind being skeptical about this stuff. And I'm also skeptical.
742000	745840	But is there not also a world where you have an AGI, which is in some way benevolent,
746480	750480	capable of very long term planning, capable of all some problem solving on a scale that we
750480	758080	can't really comprehend? Yeah. And just to be really clear, I think that is, you know,
758800	763200	that is the happy path we're now on. I think there's basically three paths. There's the,
763920	767680	we have a moratorium that just completely shuts this down. And that could be like,
767680	771520	you know, some of the other moratorium we've had around, you know, genetic engineering,
771600	779120	for example, you know, eugenics. There could be a another path where we develop this kind of
779120	783920	hastily and not thoughtfully and kind of wipe ourselves out in the process. And there's this
783920	787600	third path, which is the one I think we should really all be oriented on, which is we build
787600	793760	systems that massively expand the amount of kind of wisdom in the universe. And we cure diseases
793760	798640	that can't be cured today. We, you know, we have enormous technological abundance. And so there
798640	803360	certainly is an approach where we build EGI and it goes incredibly well for us as a species.
804400	807920	The question is, are we on track to do that or not the way we're doing it today,
807920	811520	with a small number of private companies racing to do it as quickly as possible?
812080	819840	So with regards to chat GPT-4, which was released in March by OpenAI, which is aligned with Microsoft,
821120	826000	how big a jump was that from chat GPT-3, which was obviously the previous version?
826000	836400	So chat GPT when it came out was was arguably a user for interface on top of a very powerful
836400	841040	language model that already existed. And so you already had these amazing language models that
841040	846160	OpenAI had trained and anthropic and Google had trained to do very powerful things. And
846160	851040	what OpenAI did with chat GPT is they basically created a way to interact with it that let that
851040	858080	suddenly opened it up to a lot more people. And so in many ways, it wasn't a sort of research
858080	861600	technological breakthrough. It was actually a user interface breakthrough and said like, here's a
861600	867360	way to use this that suddenly feels a lot more organic and natural to an end user, a consumer
867360	873840	playing around with GPT. The thing they released after that, which was called GPT-4, was a significant
873840	880720	update because the chat GPT was based on what they were calling it GPT-3.5. And that was a big
881360	889040	jump in the underlying model. So for example, GPT-3.5, when you sort of tried to get it to do
889040	894560	the bar exam scored in the bottom 10% of results, whereas GPT-4 scored in the top 10% of results.
894560	898880	So in a single generation of models, you had this massive leap in capabilities where it went from
898880	904240	basically not really being able to be a lawyer to being able to be a lawyer. And so GPT-4 was a
904240	908240	massive step forward for language models in general. And like, you know, one of the most
908240	914240	impressive technological artifacts humanity has ever created. And what was the basis of that jump
914240	918480	from 3.5 to 4? Was it purely because there was more data being fed, or there's been a sudden
918480	925600	boost in computational power? So it's a great question. And we don't really know. So OpenAI
925600	930240	hasn't really explained to us what data they trained it on, the amount of computing resources
930240	936960	they use for it, any algorithmic breakthroughs they made, they have kind of gone from being
938240	942240	open AI, kind of being very open with their research to being much more close with their
942240	946720	research. And they have done it, I believe, for good reason, which is they sort of don't want
946720	952400	to accelerate things any more than they have to by suddenly making it more possible to replicate
952400	955680	this and kind of cause a huge amount of proliferation of this technology.
955680	960320	This is so interesting. So could it be possible then that OpenAI are further down the road to
960320	966800	AGI than we really discuss, we really talk about? But the incentives aren't really there to be quite
966800	971040	public about it, right? The incentives are there to actually be quite private and discreet and not
971040	976240	really convey how close we are to a really transformational technology.
976240	980880	Yeah, the incentives are really challenging. So I'll actually give you a quite a concrete example
980880	986080	that I think brings the race to life. So I'm one of the first investors in this company,
986080	992000	Anthropic. And Anthropic was founded by a group of people who left OpenAI and set up a new AI
992000	998640	startup. And it was the people who did it, who founded it, were the people who led the research
998640	1004000	on GPT2 and then GPT3, so the precursors to these large language models. So they really are the key
1004000	1009600	people from OpenAI who did a lot of the large language models sort of early work. And their new
1009600	1017600	company is very much oriented around a greater emphasis on safety. So they have something like
1017600	1023680	50% of their headcount in 2021 was dedicated to alignment research and safety, which is higher
1023680	1029760	than any of the other labs like DeepMind or OpenAI. And they had a product like ChatGPT
1031040	1036320	about six months prior to OpenAI releasing ChatGPT. And if they'd released it, it would have
1036320	1040800	suddenly put Anthropic on the map in a big way. They would have attracted so much more capital,
1040800	1045280	more attention, and they held it back because they felt like it would just accelerate this race
1045280	1051440	in a counterproductive way. And so if you think about the incentives, it's working against them
1051440	1056880	as a capitalist entity to just hold back stuff, to release less, to create less hype. It's quite
1056880	1062800	challenging. And so recently, they just actually made an announcement maybe two weeks ago where
1062800	1068640	they expanded the context window for the largest language models to 100,000 tokens. And just to
1068640	1073920	explain what that means, it's basically the size of the document that you can feed into a language
1073920	1078880	model and have it work with for you. And so it massively changes what you can do. You can feed
1078880	1085040	like a huge legal document or a massive code base into a GPT-4 like model and get a much more
1085040	1089760	sophisticated response as a result of that. So it's a huge technological breakthrough. It lit
1090560	1095360	the AI research community and start-up community on fire when they did it. And that
1096080	1100080	ultimately attracted more attention to them, probably more capital over time. And so there's
1100080	1104480	these perverse incentives where if you're a startup, you're incentivized to get as much
1104480	1108400	capital and attention as possible so you can go faster. But actually, if everyone does that,
1108400	1114240	then we burn the time we have to make this stuff safe. So it's a very challenging coordination
1114240	1118880	problem where the incentives encourage racing rather than careful, slowed-down coordination.
1119840	1124480	I'm very happy you said that. There's a great quote from Jeffrey Hinton who recently resigned
1124480	1129120	from Google. And he said in an event, I think Google was very responsible to begin with,
1129120	1134960	and this is deep mind. But once OpenAI had built similar things using money from Microsoft and
1134960	1139920	Microsoft decided to put it out there, then Google didn't have much choice. If you're going to live
1139920	1146160	in a capitalist system, you can't stop Google competing with Microsoft. So it almost sounds to
1146160	1152080	me like one of the most powerful things about the market system competition, which can lead to
1152080	1157440	incredible efficiencies, has upside as well as downside. But particularly with regards to AI,
1157440	1163200	this sounds almost like you couldn't build a worse system to potentially accelerate development
1163200	1167360	while also not really addressing things which could go very badly wrong.
1168000	1174560	Yeah, I think that's, it's very challenging because there are areas of AI research where we
1174560	1179680	I think actually capitalist competition is extremely good. So for example, there's 10 start-ups
1179680	1184960	and they're all competing to make AI systems that can take in cancer biopsy images, analyze them
1184960	1189440	really well and improve the lives of patients. I'm not worried about that having a negative
1189440	1193840	consequence on the world. And I think actually the price signals the competition will be really
1193840	1200800	good and it'll ultimately give us all cheaper healthcare, more innovation in the market. So
1201760	1206800	the area of, I guess, narrow AI, where AI is just doing a single task, quite specified,
1206800	1211040	and without these existential considerations, I think this kind of capitalist competition
1211040	1216720	can be great. The problem is if we're trying to apply the same logic to the part of the problem
1216720	1221120	where we're trying to build something smarter than us, there's basically a new species. And that,
1221120	1225600	I think the kind of capitalist market dynamic is not helpful. And I think that, you know,
1225600	1229040	what's great is that the leaders of these organizations, I think in their own ways,
1229040	1234160	they all kind of have done important things to acknowledge this. So, you know, Demis,
1234880	1240640	the CEO of DeepMind is someone I really admire, you know, he's really oriented a lot of DeepMind's
1240640	1245680	efforts towards expanding the scientific commons, you know, things like AlphaFol,
1245680	1249280	which they released for free, and they've really expanded the amount of, that's not a very sort
1249280	1253920	of capitalist maneuver to basically produce this massive breakthrough and then kind of give it away.
1254640	1259840	But I think it hints at how he thinks the economic gains from this should be distributed.
1259840	1266000	Sam Altman, you know, the CEO of OpenAI, he's talked about how he wanted to have the government
1266000	1270240	fund OpenAI early on. So he didn't want to raise money from private investors, just he
1270240	1275600	didn't get the support from the government to do that. He's also, he and his team have explicitly
1275600	1281040	said that if the race becomes too dangerous in their charter, they've said, we will merge and
1281040	1287200	assist another player to change the coordination dynamics for the better. And Anthropik have got
1287200	1291360	a very, you know, very, very thoughtful set of statements they've made about how they want to
1291360	1297280	ultimately be much more cautious as we get closer to this kind of godlike AI. So I think we have
1297280	1301280	actually leadership that is trying hard to do this. It just doesn't really work within the
1301280	1307920	current economic system. And so for example, I, you know, I fought an antitrust case against
1307920	1313680	Ticketmaster in the United States as part of the startup that I built, Songkick. And so, you know,
1313680	1322560	I'm very supportive of the kind of, you know, the work that Lena Khan, or the CMA have been doing to
1322560	1331280	try to sort of decrease concentration in certain markets. But I think in this case, actually
1331280	1335680	sort of antitrust is actually quite harmful because it almost creates, it makes it harder
1335680	1339760	to coordinate. There's less of a safe harbor. And so I think the main thing we need to do is really
1339760	1344240	view these as quite different regimes. There's this kind of narrow AI regime, and there's this
1344240	1349520	trying to build a god regime. And that bit needs a different regulatory approach to that bit.
1350400	1354880	There's a quote from Marx. It's in the Communist Manifesto. I actually, I was reading this the
1354880	1359840	other day. That's why I come on this channel, just to hear about Marx. This is, no, this is
1360480	1365440	Ian, this is terrifying. Now bear in mind, he wrote this 170 years ago. Marx or our Capital Society
1365440	1370000	had quote, conjured up such gigantic means of production in exchange that it was akin to a
1370000	1375440	sorcerer, quote, no longer able to control the powers of the Netherworld, whom he has called
1375440	1382640	up by his spells. I mean, wow, that sounds like capitalist competition, creating something completely
1382640	1388800	beyond motivation and intentionality, and over which has very little oversight. Now, of course,
1388880	1397280	he's talking about, you know, steam power and mills in Manchester and Brussels and
1397280	1402000	Frankfurt in the mid 19th century. But if anything, those words sound more appropriate
1402000	1408560	for AGI in the 21st century. Yeah, I think that one way to sort of frame the capital and kind of
1408560	1418240	labor, the sort of relative power of those two groups is just looking at the size of some of
1418240	1424960	these organizations. So an open AI is a, I think, privately valued at 30 billion US dollars now,
1426000	1429680	you know, significantly changing the world, hundreds of millions of people now using their
1429680	1434800	products. And I think, you know, at the time they released chat, it's probably a few hundred people
1434800	1440880	in terms of the size of the organization and the labor that's directly, you know, benefiting from
1440880	1446560	kind of the work that's being done there. And I think that again, you know, the leaders of these
1446560	1452320	companies are actually thinking hard about this. So Sam Altman, you know, a couple of things I
1452320	1458240	admire that he's done, the first is he was running a very large UBI study in Oakland. And so he was,
1458240	1462320	you know, that was, you know, maybe five, 10 years ago, he was thinking hard about this question of
1462320	1467760	how do you kind of, how do you, if you do have further and further returns to capital, what do
1467760	1473840	you do about kind of that not, that not just massively in increasing inequality. And, you know,
1473840	1477280	he's done this thing called Worldcoin, which is kind of a much more extreme version of that,
1477280	1482080	which is a machine that scans your retina, produces a unique ID for you that would then
1482080	1486320	let you be part of a global UBI scheme. And so, wow. And so there are, you know, they,
1486320	1491360	these kind of people are thinking about these, these kind of the way in which this may fundamentally
1491360	1496960	disrupt some of the ways in that the social contract we currently have that allows capitalism to
1496960	1502000	sort of just continue as it does. But I suppose the concern is you can't be worried, you can't
1502000	1507680	be dependent rather on prevalence and the foresight of certain individuals. You know,
1507680	1511920	there was a great quote a couple of years ago from Mark Cuban. And he was saying,
1511920	1515520	I wouldn't teach my kid to be an accountant. We now know that it was probably quite a good move,
1515520	1519600	because, you know, that's one of the industries, which is very much prone to automation
1519600	1524080	with machine learning. I'd rather they learn philosophy because it will give them insights
1524080	1529360	that are harder to automate, so to speak. And I thought that was interesting. Now, alongside
1529360	1533680	that, he said the world's first trillionaire will be the person who can master widespread
1533680	1538480	commercial applications of AI. And that's the prize on offer, isn't it? I mean, that's the prize
1538480	1542000	on offer. So today we talk about Amazon, which is a trillion dollar company, what used to be,
1542960	1549440	borderline trillion dollar company, Amazon, Microsoft, those kinds of big players. But the
1549440	1555920	truth is the commercial entity, which masters an AGI, and we don't all die, will put all those guys
1555920	1561280	in the dust, won't they? So there are massive incentives for people to pursue this technology
1561280	1565760	without the kinds of caution and intelligence and thoughtfulness that you've talked about with
1565760	1572160	regards to somebody like Sam Altman. Yeah. And I think what's challenging in some ways,
1572160	1577120	we've entered a new phase of this race. So if you look at the leaders of these organizations,
1577120	1582560	the ones who are kind of really at the forefront of the race, whether it's Demis or Sam or Dario
1582640	1588000	Anthropic, I would say that most of them are kind of not particularly motivated by money at this
1588000	1592160	point. They're doing this for some other reason. You know, Sam, you know, recently announced that
1592160	1596720	he actually has no equity in open AI, so doesn't stand to benefit economically from, you know,
1596720	1603200	he won't be the first world's first trillionaire, let's put it that way. And so I think that they've
1603200	1608000	been motivated by other things. And I would say mostly they've been sort of motivated by being
1608000	1612960	the people to do it, to make this thing come alive and, you know, the consequences of that.
1613680	1618000	It's kind of a world historic transition that they want to be a big part of.
1619440	1625520	It's my guess, my hypothesis. But there are now a lot of other people who've kind of suddenly
1625520	1631040	woken up and just seen dollar bills. And those people are just piling on money. They haven't
1631040	1635600	really thought about it. They don't have the same sort of reverence that people like Demis
1635600	1639760	have got for how we should be approaching this moment. And they're actually accelerating the
1639760	1645840	race, but without that same, you know, intrinsic motivation for the thing they're trying to do
1645840	1649600	and much more of a kind of, you know, much more of a desire just to make money.
1649600	1653120	So for people out there who are perhaps skeptical of what I'm saying here about a trillionaire,
1653840	1658720	if you told a 14, 15 year old Ian or a 14, 15 year old Aaron about the same age,
1658720	1663840	that one day there'll be somebody worth 250 billion US dollars, 200 billion US dollars,
1663840	1669040	like Jeff Bezos, we would have thought that's outlandish. Yet he's the guy who starts Amazon,
1669040	1673920	and that's the company which benefits from network effects, you know, ubiquitous mobile
1673920	1680720	internet, and basically building the everything store of e-commerce. And I suppose the question is,
1680720	1688320	could you feasibly see a company like Amazon, which is applying AI to a bunch of industries,
1688320	1693120	which lay off hundreds of thousands of people, just like Amazon have basically shut down hundreds
1693120	1698560	of thousands of local businesses? And that's the outcome we get. Do you think that's a plausible
1698560	1705280	outcome? I think it is possible. And I think the reason for it is that you are, you know,
1705280	1709360	you first of all have a much more globalized economy where things can spread really quickly,
1709360	1714320	across borders in a way they couldn't, especially digital products. Secondly, you have,
1715200	1721200	you know, a lot of these technological, you know, products built on top of prior networks.
1721200	1727120	So for example, chatGPT is the fastest-growing product ever on the internet, but that's partly
1727120	1730880	because we've got Twitter and Facebook and Instagram and Google, and all the ways that
1730880	1736560	information disseminates and spreads faster than it did before. And finally, I think we're starting
1736560	1741760	to tackle some of these incredibly large markets, like, you know, what SpaceX is doing is basically
1741760	1746480	tackling, you know, global internet provision through Starlink, right? And so this is a very big
1746480	1750640	thing. It's not like digging up the street and smashing in loads of fiber. It's something where,
1750640	1755360	you know, the same satellites can basically provide internet access wherever you are in the world
1755360	1762320	without that same degree of physical disruption. And so I do think it's possible. I think a good
1762320	1767760	thought experiment for the kind of company that might be an example of this is the first company
1767760	1774080	to really build incredibly good domestic robots, right? So, you know, I've got a four-year-old,
1774080	1777520	when I'm doing chores around the house, I would sort of say to him, you know, one day,
1777600	1781440	you know, one day, maybe there's going to be a robot doing this because, you know, we didn't
1781440	1784400	used to have dishwashers. We didn't used to have washing machines. We didn't used to have
1784400	1787920	tumble dryers. And now we have all those things. We take them for granted. They're in most homes
1787920	1792160	in the world. What would it be like if we had a robot in our house that just did all the chores
1792160	1796800	that we currently do as kind of around everything else we're doing in our lives, but also that
1796800	1801840	robot could be your plumber when things break, your electrician, your handy person, go to shops
1801840	1806720	for you. So when you think about kind of that sort of disruption, could it be a, you know,
1806720	1810320	10 trillion dollar company where the person founding it is a trillion error, like quite
1810320	1817120	possibly in my view? A quick sort of move away from machine learning software to the kind of
1817120	1820000	robot that you're talking about. How far away do you think that is, by the way, as a sort of
1820000	1825840	household appliance that, you know, middle-class people in the UK or US would be able to buy?
1825840	1830240	Well, so I think it really is fundamentally the same curve we're talking about with AI. So robots
1830240	1834240	basically are machine learning. They're just embodied machine learning where it has a physical
1834240	1839360	presence it's using as well. And we've made, and I've invested in lots of robotics businesses
1839360	1843360	over the years, and it's amazing how rapidly they're progressing. We don't actually see the
1843360	1847840	number of robots that we're using because they're mostly industrial settings. They're like industrial
1847840	1853840	cleaning robots or industrial manufacturing robots or agricultural robots. And so it's happening.
1853840	1859360	And I think that we will start in the next few years to see robots in much more consumer settings,
1859360	1865440	really starting with the rollout of self-driving cars. And so I think we will see humanoid robots
1865440	1871120	that significantly enhance our lives in a domestic setting within the next decade.
1871680	1875920	The next decade? And a lot of that will be knock-on effects from the work we're doing in making
1875920	1881280	these powerful AI systems. So to bring this back to large language models and the sort of things
1881280	1885920	that DeepMind and OpenAI are working on, there was a paper called the tool former paper that came
1886000	1891280	out earlier this year. And it essentially shows that large language models are actually quite good
1891280	1896640	at using tools, which is quite counterintuitive. But to break it down, you take an incredibly large
1896640	1902480	computer, you feed it an enormous amount of text and imagery, and basically get it to get really
1902480	1909440	good at predicting what it's going to see. If you take that same sort of blob of intelligence that's
1909440	1914880	just become smart in some important way, and you give it access to a tool, it is actually quite
1914880	1922160	good at using a tool, whether it's a digital tool or a physical tool. And so large language models
1922160	1926480	actually will have a knock-on effect on real-world robotics. They're not sort of separate industry
1926480	1933760	as a tool. They're very, very related. I like it. And how far are we from machines basically
1933760	1938240	being able to do anything that a human does? So in my mind, I think sort of 25 to 30 years
1938800	1942400	where we have software hardware, which can basically do 95% of the jobs that humans do,
1943120	1948400	at the moment, the big obstacle to that obviously is the fine motor coordination dexterity that
1948400	1953680	things like cleaning or construction relies upon. So it's easier to automate legal services
1953680	1959200	or accountancy than it is, like you say, plumbing. Great example. How far are we from being able to
1959200	1965280	solve the problem of the fine motor coordination? So I guess we don't really know. That kind of
1965280	1969520	goes back to this question of how far away are we from AGI? How far away are we from
1969600	1973920	sort of superintelligent systems? No one knows. All I will say is people have more,
1974480	1978320	they put more probability on it happening soon rather than later. So Jeffrey Hinton,
1978320	1983360	who resigned from Google recently, he said, I used to think this was paraphrasing here,
1983360	1987680	but he said something like, I used to think this was decades away. And now I think it's not inconceivable
1987680	1994720	it happens in the next five years. And he's the godfather of machine learning, the researcher
1994800	2000480	that really kicked off this whole field of large neural networks and really the sort of one of
2000480	2007200	the most important people in the field over the last few decades. And so we don't know,
2007200	2011280	but I think we now need to sort of start to prepare as if we might be closer than
2011280	2014080	people have realized. And the public certainly has realized.
2014080	2017120	I mean, this is a big challenge to our economic system, right? So I mean, I wrote a book about
2017120	2024320	this called Fully Automated Luxury Communism. And the point is if you get increasingly affordable
2024400	2029200	technology, which can do anything that a human can do, whether it's with regards to abstract
2029200	2033520	problem solving or physical capabilities like building a house or cleaning a house or cooking
2033520	2038800	meal, what that does is clearly depress the price of labor to zero. That's what's going to happen.
2038800	2044240	That's a neoclassical understanding of how of how supply and demand would work with the cost of
2044240	2050640	labor of human labor. And yet, if you say that to a lot of people, including politicians, including
2050720	2056480	quote unquote, smart people in legacy media, in the policy world, they think you're crazy.
2056480	2062080	And it's interesting for me, seeing the reception of my book, F.T., oh, this is interesting,
2062080	2067920	very provocative, New York Times and so on. But then in some other places, I won't name the papers,
2067920	2071680	but they were just like, this is ridiculous. They were sort of mocking it. Where do you think
2071680	2075520	this asymmetry comes from where some people take these ideas very seriously and then
2075520	2079600	others just completely disregard them? Is it a lack of information? Do you think it comes from a
2079600	2085600	place of fear? Because realistically, if this is correct, we're going to have to shake things really
2085600	2092880	up a lot, aren't we? I think it's a great question. I guess I've got two hypotheses. The first goes
2092880	2097440	back to the thing we were talking about with exponential change. It's just so hard to think
2097440	2105760	intuitively about exponential progress. And so COVID is sort of fresh in our minds. And I remember
2106320	2113120	sort of January, I was planning a trip to China with a friend. And actually, our trip took us
2113120	2120800	through Wuhan. And he's an expert in kind of biology in various ways. And I spent a bunch of
2120800	2124720	time in China when I was younger studying Mandarin. And so we were talking about this trip and we're
2124720	2128960	talking about COVID. And we sort of started to see the little inklings that COVID was going to
2128960	2135680	potentially go pandemic. And I remember both of us were starting to prepare and
2135680	2140000	get really worried ahead of a lot of the sort of mainstream news. But at the same time,
2140800	2144400	we're both like, well, maybe it'll be done by the time that our trip comes around, right?
2144400	2147600	And it's just a perfect example of like, we thought we were being clever, but actually we
2147600	2152560	seriously still did not really have a good intuition for actually what happens when exponentials
2152560	2158160	really take off. And I think that there's a bit of that where if you're a politician that's just
2158160	2163520	dealing with, you know, you know, day to day issues, and you're then confronted with an exponential
2163520	2172240	change, be it, you know, what we're doing with climate change, or a pandemic, or an accelerating
2172240	2178160	technology like AI, you just don't really have an intuitive way to navigate that. So I think
2178160	2184080	that's part of it. I think the other part of it is just exponentials require, I think, a certain
2184080	2189120	kind of radical thinking, you know, it's sort of like what the I think the UK government did really
2189120	2193760	brilliantly with Cape Bingham and the Vaccine Task Force. I think it was a great example of kind of
2193760	2197680	just incredibly bold, proactive leadership on a serious thing, getting, let's get ahead of it,
2197680	2202560	let's get, let's, let's build that capacity early, let's, let's throw everything at it and prepare
2202560	2206400	as best as possible, because we know this exponential is coming. And that kind of
2207120	2211760	political action and leadership is just, I think, like quite hard to do. And I think
2212480	2216240	Why is it, why is it hard to do? Is it because politicians don't like doing it or because
2216320	2217920	it's just objectively hard to execute?
2217920	2224160	I think it's radical, basically. And I think it's a, it's not business as usual, you know,
2224160	2229280	it's first principles thinking you maybe have to take more risk. And they just haven't been that
2229280	2236560	many examples of political leaders who've kind of acted like that over the last couple of decades
2236560	2243680	in response to technological change. And I remember in 2018, I wrote this essay AI nationalism that
2243680	2248160	really talks about this kind of this fundamental challenge that was coming down the pipe with AI.
2248160	2252080	And I remember meeting with, with kind of MPs from labor MPs from, you know,
2252080	2255840	conservatives laying out these ideas, almost everyone just looked at me like I was completely
2255840	2260320	crazy. Because I was saying things like next time we have a company like DeepMind, we shouldn't
2260320	2265760	let it be acquired because it's too critical to the future of the UK. And then, you know,
2265760	2270400	three or four years later, I saw similar politicians basically saying, oh yeah,
2270400	2274320	now we have to be serious about blocking acquisitions of some, you know, strategically
2274320	2279840	important UK technology companies. And so I think it's just also discomfort with really
2279840	2285120	radical ideas. I think there's much more of a comfort in politics with incrementalism. And
2285120	2287680	that's why often big change happens when you have a crisis.
2288880	2292160	We'll come back to the politics in a moment. I want to ask you, I want to ask you an almost
2292160	2297520	existential question. A super intelligent, artificial general intelligence, would it be
2297520	2303680	conscious? I don't know. I think that there's, I was actually talking about this with
2307600	2312400	someone who specialised in quantum computing yesterday. And their view is actually that
2312400	2318880	like conscious consciousness is a kind of quantum mechanical property, a property of quantum
2318880	2324240	physics. And therefore, it will not be possible to get consciousness on a classical computer.
2325040	2328880	I don't know if that's true or not. But like their claim was basically, we all sort of need
2328880	2334640	quantum computers to actually have conscious AIs. I don't know.
2334640	2337840	What's the argument behind that, by the way? So where does that come from?
2337840	2344880	It comes from quite a, I guess, an esoteric argument about kind of about exactly how we
2344880	2350480	interrelate with the metaverse, which I'm not enough of an expert on any of those topics to
2350480	2358720	really opine. But the way I think about it is, is consciousness and intelligence are probably
2358720	2364160	kind of somewhat orthogonal. You don't necessarily get, you know, get one with the other. And I don't,
2364160	2369920	I think it may have been a, the way it may be an artifact of biological life, which is inherently
2369920	2376000	quantum, rather than necessarily sort of classical computing. So we still could fabricate something
2376000	2378400	close to consciousness, but it would just require quantum.
2378400	2382080	Maybe, or maybe it's not possible. We just don't, we, consciousness is something,
2382080	2385920	as far as I can tell, we don't really understand at all. So we, I think it'd be quite difficult to
2385920	2391200	project how to think about consciousness when it comes to machines. Like, if I'm understanding it
2391200	2400080	correctly, when you, when you get, you know, when you have an operation, you go under anesthetic,
2400080	2404080	you know, the drugs you're being given, essentially just switch off your consciousness.
2404080	2407440	And we don't really understand like why and how that works. And so it's quite hard
2407440	2410960	before we really understand consciousness to really make any claims about how
2410960	2415600	AI is will be conscious. I think that the, but I think we should probably start from the basis
2415600	2419440	that they might not be conscious, but they might still be incredibly intelligent and capable
2419440	2422560	of planning in ways that are threatening to humanity.
2423120	2426960	So we could have an AGI, which is an existential threat to humanity without it being sentient.
2426960	2429040	Yeah. Yeah. Exactly.
2429840	2433920	Interesting. And it could be trying to maximize its own utility. It could be trying
2433920	2438640	to maximize its own interest without really being aware of itself as an entity.
2438640	2443440	Exactly. Yeah. Exactly. I'm thinking about it kind of like the Sorcerer's Apprentice is probably
2443440	2447680	the kind of like a really mainstream example of kind of something where you give it some sort of
2447680	2452480	goal and the optimization of that goal ultimately leads to something that is not what you wanted.
2453200	2456560	And so that's what the field of alignment is about. It's basically about how do you align
2456560	2460480	those goals with our goals as a species? What does it mean to the average person? So we're
2460480	2464640	talking about AI, you know, partly abstract, partly real world. So we're talking about, you
2464640	2468400	know, the potential of trillionaires. We're talking about consciousness. We're talking
2468400	2473600	about politicians asleep at the wheel. But what does it mean to somebody out there who's watching
2474240	2480800	median income, homeowner, mortgage, owner-occupier, if there is an artificial general intelligence
2480800	2485360	that's created to send the next five to 10 years, how will that impact their lives over the next,
2485360	2493840	say, 25 to 30 years? So again, with exponentials, saying anything about, you know, 20 years is really
2493840	2498640	hard. Yeah, let's say it's developed. That's the exponential part. And then let's say there's
2498640	2502960	almost like a cap on AGI. Like it's not really developed after that for regulatory reasons or
2502960	2508960	whatever, political reasons. I'm trying to really ask what are the implications for just everyday
2508960	2512720	people rather than us talking about, you know, like you say, some breakthrough which depends
2512720	2516560	the printing press, the average person in 16th century Europe doesn't really care about the
2516560	2521200	printing press, but then the reformation happens and that is a big deal. So I think that like,
2522560	2528000	I think that synthetic media is a really, really big deal. And I think there is
2528800	2534320	extremely low popular support for it. So, you know, as an example, you know, you can take the
2534320	2540240	sort of generative AI systems, the systems that are creative, capable of generating images or text,
2540320	2544640	and you can use them for harmful purposes. So I'll give you a few examples. The first is,
2545200	2551600	you can take a snippet of someone speaking, and you can then, you know, effectively synthetically
2551600	2556960	clone their voice. And that is now being used by people to do kind of fake kidnappings where you
2556960	2561520	get a snippet of a child's voice, and you basically create a phone call from them calling their parents
2561520	2566320	saying, mom, I've been kidnapped. If you don't do this, then I'm going to, you know, something
2566320	2573760	horrible is going to be done to me. And that happened in, I think, Texas about two months ago.
2573760	2582320	And so that's kind of happening. Another thing that's happening is around synthetic child sexual
2582320	2586800	abuse material. And so the systems you can use to create a kind of funny image of the pope in
2586800	2592080	a puffer jacket, you can also use to create, you know, horrible deep faked porn, including
2592080	2596880	horrible deep faked child sexual abuse material. And that is currently happening. People are using
2596880	2602800	these systems to do that, particularly the kind of open source ones. And the knock on effect is
2602800	2606560	then you've got to, you know, the police are suddenly faced with, you know, is this an image
2606560	2611040	of a child that's actually being harmed or a fake image and having to full of shard their resources
2611040	2616480	between kind of basically fake crimes and real crimes. And so there's a huge number of malicious
2616480	2621040	uses that will bubble up from this very powerful technology where you can clone someone's likeness,
2621040	2627440	someone's image, someone's voice. So I think of that as being like the real structural problem
2627440	2631040	that we're going to encounter. And I think that they're, you know, I was talking to someone there
2631040	2635840	who's been doing focus groups around this. And they were just telling me like, you know, basically
2635840	2640320	the public at large thinks deep fakes should be made illegal already. So there's a kind of, I think,
2641120	2644720	a lot of stuff bubbling under the surface that when it breaks through is going to really,
2645600	2648800	it's going to really have actually quite a populist response to it, because it's just,
2649360	2652000	it seems like something the government should be getting a grip of.
2653040	2656560	And do you think they will? Is there any sort of, you're in this game? Is there any evidence
2656560	2660560	that they will? I think they will actually. Yeah. I think it's just, it passes a kind of
2660560	2664960	common sense test of that that is not a good thing that should be happening. So I think there'll be a
2664960	2670720	genuine motivation to make deep fakes, deep deep fakes without someone's permission illegal.
2671360	2675200	And I think lots of politicians will want to do that. But I think the mechanism for doing that,
2675200	2679040	when you've got significant proliferation of these capabilities into open source is a bit
2679040	2683040	trickier, right? Because the technology is kind of out of the box. And so figuring out how to get
2683040	2690240	it back in, like with the dark web, for example, is just challenging. But I think it will be,
2690240	2692720	I think there'll be broad political support for doing that.
2692720	2696000	You said the common sense thing, but I mean, if you said to somebody 40 years ago that children
2696000	2701920	will be able to access high speed broadband pornography, be able to stream stuff, I mean,
2701920	2704800	I think most people said that obviously should be banned. That's the world we live in.
2706080	2708800	I think we've lived through a strange time, you know, I think that
2710800	2716880	I think about social media a lot and how it's sort of remarkable that we basically left this
2716880	2723680	enormous industry that was so transformational to, to everyday life, to children, to politics,
2723680	2728000	to the general discourse, to basically self regulate, you know, and I think there was
2728000	2733840	actually a really, that was well motivated by a desire not to kind of throttle something with
2733840	2739280	regulation and kind of too much, you know, too early, you know, I think I can kind of,
2739280	2745360	I can see the logic of things like section 320, I think it's called, which is kind of the sort of
2746160	2749760	the mechanism whereby a lot of social media companies have not really
2749760	2755600	have kind of been able to just self regulate. But it feels like we, I think with the benefit
2755600	2760960	of hindsight, you know, it would have been better for regulators to catch up faster and to sort of
2760960	2766320	be a bit more assertive about defining, you know, a smarter way forward. And I think Biden talked
2766320	2772480	about, you know, for example, modifying section 320 recently, and there's been some discussion
2772480	2778160	about this. But I think with AI, you know, I think that the lawmakers need to move faster,
2778160	2782240	because back then, you know, these fledgling startups like YouTube, they had certain sorts of,
2782240	2788080	you know, it's kind of a, you know, innovation was valuable and needed to be sort of encouraged.
2788080	2792560	But this is a different ball game where you've got $20 billion already invested in just a handful
2792560	2797200	of companies, you've got, you know, Microsoft aggressively deploying this as fast as they possibly
2797200	2802800	can. You've got companies like Facebook, open sourcing incredibly powerful models and putting
2802800	2806720	them out there for anyone to sort of expand the modify. And so I don't think I think we're kind
2806720	2812880	of in like the same scenario. It's just now this is not about small startups and fledgling
2812880	2816720	industries. This is about an incredibly powerful tech industry that has just
2817360	2822240	prefers self-regulation to anything else. Do you think social media was a mistake? Because
2822240	2826320	obviously you're involved in the sort of the technological side of things and how, you know,
2826320	2832400	the technological sort of underpinning of global social media, 4G, 5G, mobile internet,
2833200	2838640	high res screens, all these things. And one outgrowth that was social media and Ben Bratton
2838640	2842720	has a really interesting read on this. So he says, we built a global real-time communications
2843520	2848480	computational network, you know, including the kind of exosphere of our planet, which is now
2848480	2853280	caked with satellites. We've built all of this infrastructure so that we can sell out space
2853280	2857440	and be permanently distracted. You know, and I think that's an interesting way of looking at it.
2857520	2862880	And I wonder, we might have something like AI or even an AGI if it's not, you know,
2862880	2865840	deadly. And we just get more of the same, perhaps.
2867280	2873680	So my day job is I'm an investor, and I invest through a fund called Plural. And, you know,
2873680	2880240	it's a European fund focused on accelerating missions that we consider to be of great societal
2880240	2885840	importance, you know, by funding them as startups. And one of the companies I work with is a company
2885840	2892560	called Unitary AI. It's one I'm very proud to be kind of working on. It's a startup that uses AI
2892560	2900000	to understand content and thereby to offer a scalable approach to content moderation
2900000	2905360	and content safety across the internet, content security. So for example, their AI can detect
2905360	2908880	some sort of content that should be illegal or some kind of content that is, you know,
2908880	2912800	causing significant harms and flag it to the platform that's hosting it.
2913520	2919120	And so they're in many ways kind of like a, you know, like a antibiotic to this kind of this,
2919120	2925520	the way in which some of the sort of wild west of content dissemination through social media
2925520	2930640	has kind of played out. So I think there is kind of a, there is a kind of capitalist response
2930640	2934480	coming, but it's hard. It's much easier to make a new social network than it is to make an AI
2934480	2939040	company that's trying to actually solve this problem of kind of how do you scalably tackle
2939040	2946080	the challenges of content? To your question of kind of, you know, if you could kind of go back
2946080	2951920	in time and stop social media from happening, I wouldn't have done that personally. I think
2951920	2958400	it's delivered enormous benefits. I think there is some something amazing about so much connectivity,
2959120	2963120	democratization of media production. I mean, you know, like I, part of the reason I'm sitting here
2963120	2966080	is because I, you know, I learned about you through Twitter and I follow your thinking
2966080	2972160	and I find it interesting. And so I think there is something, something really miraculous about how
2972720	2978320	connected we are now. I just think the problem is we just never really had governments keep up.
2978960	2982800	And I think it's something, I think it's the really the nature of like sort of
2982800	2988160	laissez-faire, neoliberal thinking, having just permeated government over a number of decades
2988160	2993520	where really bold, ambitious projects where the government is a kind of, you know, a real partner
2993520	2997680	to the private sector and actually drives technological change and thinks about regulation
2997680	3001440	in a bold way that embraces exponentials has just been missing.
3001440	3005200	So you think there's neoliberalism, this, because we do live in this really strange moment, right,
3005200	3010480	of like extraordinary technological possibility, like extraordinary profound liquid biopsies that
3010480	3017120	can detect early onset cancer, you know, mapping the human genome, you know, just high res scans of
3017120	3022800	the human brain, just, you know, year on year improving exponentially. And yet we have politicians
3022800	3027680	who say, sorry, we can't address the housing crisis. Sorry, we can't give you affordable
3027680	3032480	health care or free health care. There's a weird disconnect there. It seems almost like the better
3032480	3037440	the technology gets, the more the possibilities, the opportunities, the less capable the state is
3037440	3040720	in addressing those challenges. So you pin that on neoliberalism.
3040720	3044480	Well, I think that particular problem you described, like, it's hard to really
3045200	3051520	know where it started. There's a, there's a kind of an investor I really respect called Matt Clifford.
3051600	3056800	And he has a kind of a thesis on this, which basically says, if you look throughout history,
3056800	3061440	there was always a kind of part of part of society that attracted the most talent.
3061440	3065280	And he's his argument is basically right now, that's the technology industry. And so a lot
3065280	3072400	of talent has kind of gone out of government or out of, you know, the public sector into
3072400	3076000	technology, because the opportunity to change things quickly, make money, you'd be the first
3076000	3080720	trillionaire, whatever, whatever motivates you, right. And so there could be a hollowing out of
3080720	3084800	sort of the capacity of the state to respond. That's one way of thinking about it. I actually,
3084800	3089440	I think of it as being a little bit more ideological. So for me, I think we've just not
3089440	3094400	really had political leadership that sort of sort of said, you know, I'm going to,
3095200	3101040	we're going to transform this country in a way that really embraces all of this. And it's kind of,
3101040	3106160	you know, keeps pace with technological change. And I think you've, you've had examples of that.
3106160	3110080	You know, if you look at Lee Kuan Yew in Singapore, obviously, lots to discuss about
3110080	3114720	Singapore and their politics, but it's very interesting the way that he basically kind of
3114720	3119040	was a founder almost of a country that went from third world to first world in 30 years. And some
3119040	3123760	of the things he did were just very, very bold, ambitious things that ultimately he took a lot
3123760	3127760	of risks and it delivered for the country and for the citizens. And so I think we've lacked that
3127760	3132640	level of boldness. And the reason I sort of cite neoliberalism and laissez-faire economics is because
3132640	3136800	I remember when I was talking to people about that AI nationalism, I was amazing to me that I
3136800	3140800	would be meeting with conservative politicians and they'd be telling me about British Leyland
3140800	3144880	as the reason why you can't nationalize DeepMind. Crazy. And it's just like, what is going on? This
3144880	3150640	is like, you know, it's like, you know, decades later and who really cares about some failed car
3150640	3157040	company at this point? Also, you have Taiwan, which I think Taiwan produces like one sixth of
3157040	3162160	global microprocessors, 90% of the ultra high end ones, you know, the ultra high end ones,
3162160	3165520	which China can't create at the moment. And it's kind of, you know, that's the whole point of this
3166160	3170880	set of sanctions and trade embargoes that the US put on them. That is entirely because of state
3170880	3174160	led innovation by the Taiwanese government. It's a country of what, 25 million people?
3175040	3179840	And in the world where Taiwan is making one sixth of the world microprocessors,
3179840	3183680	we have British politicians saying, sorry, we can't do that because of this thing that happened in
3183680	3189520	1975, whatever. Crazy. Yeah. And I think it's, you know, it's obviously there are some people
3189520	3194880	really trying, but I think the system as a whole is very trapped in an old ideology that sort of
3194880	3199040	doesn't just sort of wants to be quite hands off rather than hands on. And you know, I work with
3199040	3203920	Marianna Mazucato at Institute at UCL. And I really, when I first came across her book,
3203920	3208000	The Entrepreneurial State, it just blew me away because it really, if you've been a founder for
3208000	3212560	most of your kind of working life like I have, it really described and but you really care about
3212560	3217760	the future of the UK, the future of Europe. It really described a different mode of politics,
3217760	3223520	which is this very entrepreneurial kind of founder approach, which says, right, like,
3223520	3227680	the state is going to take a point of view, it's going to have a vision, it's going to pick missions
3227680	3232000	that really matter, it's going to invest very ambitiously to make things happen that wouldn't
3232000	3235840	happen otherwise. And I'll give you an example of something that like right now for me is very
3235840	3242960	inspiring. So within nuclear fusion, you know, you've got a number of different concepts for
3242960	3248640	how to put fusion on the grid. And we've made enormous progress in fusion over the last few
3248640	3252720	decades. And we're now actually I think within touching sites of it happening. And the dominant
3252720	3259440	approach that's really taken us closer to, you know, fusion on the grid is a technology called
3259440	3265120	magnetic confinement fusion, you have very powerful magnets, the confine essentially a sun on earth,
3265120	3269840	and you use that to basically produce a sun on earth and extract energy. And
3270640	3275440	the dominant mode of doing magnetic confinement fusion is something called the tokamak. And the
3275440	3279440	tokamak was a device that's, you know, there's probably $100 billion been invested in tokamaks
3279440	3285280	globally over the last over the last 50 years. And the German government actually took a different
3285280	3290320	point of view. And they said, there is this other device, which has a lot of attractive things about
3290320	3294560	it, that actually, you know, are very hard to do, but we might now have enough computing power to do
3295280	3300640	it. And they've been in quietly investing, you know, large amounts of money, but still quite
3300640	3306000	small for fusion into making this happen. And over the last 20 or so years, they've taken this
3306000	3310240	alternative fusion reactor design, all the way up to the point where we might actually be able to
3310240	3314160	build a power plant, and it's called the stellarator. And the German government has done that kind of
3314160	3318880	almost single handedly, the most advanced stellarator in the world is in North Germany. And it's
3318880	3322640	light years ahead of any other stellarator. And that's the sort of thing that just gives
3322720	3326960	me real goosebumps when I think about what the state can do really, when it really wants to
3326960	3331440	shape markets, you know, if the UK said, right, we're going to take a point of view, fusion is
3331440	3337440	going to happen in the next 20 years. We want to have this country running on fusion in a material
3337440	3340960	way, we want to do a massive industry for this country, we're going to have this incredibly
3340960	3346400	ambitious kind of investment mandate behind that. I think that's the sort of politics that excites
3346400	3352720	me and makes me feel like we would have a politics that matches the exponential change
3352720	3355440	rather than just kind of runs behind it, trying to play catch up.
3355440	3358960	How popular is that kind of stuff in your world? So you mentioned Marianna Mazzicato.
3359600	3367280	I suppose, you know, she proselytises a kind of social democracy with an interventionist state,
3367280	3370080	like you say, with like an emphasis on entrepreneurialism as well. So it's quite
3370080	3376480	a unique blend, although I really find it's a 21st century variant of 20th century social democracy
3376480	3383120	kind of updated for the network society. Are those ideas more popular than one might imagine
3383120	3388240	amongst these kinds of circles? Because tech people are generally open to new ideas or
3388960	3392640	are you something of an outlier in terms of enjoying Marianna's work?
3393440	3400720	I don't really know of an outlier amongst founders. I think that founders are quite specific people
3400720	3405280	and that they're really, they're drawn towards taking risk. They often want to try to make the
3405280	3410720	world better by building some product that they think the world needs. And I think a lot of them
3410720	3414960	are quite radical in their bones, quite risk taking. And I think they're drawn to, they're drawn to
3414960	3419280	interesting ways of thinking about politics often. I would say investors, and you know,
3419360	3427600	I'm kind of technically my day job as investor, tend to be a lot more focused on turning money
3427600	3434480	into money than actually really directing markets. So a little while back, you mentioned quantum
3434480	3439680	computing and the multiverse. And I know that's not the topic of this conversation. And I'm sure
3439680	3442240	you have a great deal of knowledge about it, but you'll be very humble and say I'm not the
3442240	3445680	right person to speak to. But this is really intriguing. And I think your audience would
3445680	3450560	love to hear more about it. What's this relationship between quantum computing and the,
3450560	3457200	the inference that it perhaps gives us a glimpse at the possibility of a multiverse, multiple
3457200	3463360	universes? I think I probably will try to stick to what I know something about within quantum
3463360	3470080	computing. So I chair a company called phasecraft, which is a company founded by a number of
3470800	3476800	UK professors. One of the professors that founded it is the sort of co-chair of the most important
3476800	3480560	conference this year in quantum computing. So they are some of the kind of global leaders in
3480560	3487040	this field. And what they're doing is developing software to run on quantum computers. And so
3487040	3491840	as a result of sort of supporting those founders over the last kind of three and a half years,
3493760	3496960	I've kind of had a bit of a window into what's happening in quantum computing. And it really
3497600	3503360	is quite amazing. So you've got these machines, probably the two kind of most impressive machines
3503360	3510000	in the world. One is built by Google in, down in LA. And the other bit is built in China.
3510000	3516160	And these machines take sort of, they use superconducting materials, trapped in these kind
3516160	3525840	of crazy cages to basically run computing operations that really allow for a much wider range of
3525840	3531760	possibilities than digital computers, which are more sort of deterministic. And so
3532400	3536720	and this is because a digital computer has a zero one binary system and quantum isn't
3536720	3541120	constrained by that. Well, yeah, sort of allow it kind of it remains in superposition. So it's
3541120	3545920	neither one or zero until finally you kind of collapse the superposition. So what that allows
3545920	3553280	you to do is to simulate on a computer things in the real world that are quantum, because we know
3553360	3558960	the real world is quantum, right? That's been established for a long time. And yet when we're
3558960	3564160	interfacing with quantum systems, for example, material science or biology, we still use classical
3564160	3568960	computers to simulate them. And so we're sort of trying to use something deterministic to simulate
3568960	3576240	something quantum. So what's really exciting for me about quantum computing is that you may have
3576240	3582720	a tool that lets us simulate aspects of nature that we have historically not been able to simulate.
3582720	3587360	And so for example, we can suddenly design incredible new materials that we can use to
3587360	3590560	the engineers can use to make things we've not been able to make until now,
3592160	3595600	or we can simulate biology in a way that we currently can't today. And as a result,
3595600	3601760	design amazing new drugs. And so I think that like, the way I think about quantum computers
3601760	3608400	is they are a class of computer that allows us to explore and understand the universe in a way
3608400	3612480	that classical computers don't. And that just feels like it's kind of like the invention of a
3612480	3616800	microscope or something. It's like a really important new tool that gives us visibility
3616800	3621760	into a realm that we currently don't really have computing resources that are appropriate for.
3621760	3624880	You have all these great analogies. I love this. So the idea of a quantum computer is like a
3624880	3630000	microscope. And of course, prior to the microscope, people didn't really understand bacteria, germs,
3630000	3633840	you know, the majority of organisms on the face of the earth, because they weren't visible to the
3633840	3637840	human eye, you're saying something similar could be possible with quantum computing.
3637840	3641440	Yeah. And I think it's really about simulation. And that's the thing that I guess I've got
3641440	3645440	personally really excited about is, you know, you've got some incredible classes of materials
3646240	3649360	that we know are quantum materials, right? The way that superconductive materials,
3649360	3656000	for example, we know a quantum and superconductors are amazing, you know, in that they are from a
3656000	3660160	climate perspective and energy perspective, they're like the material because you have no
3660160	3664560	heat loss to resistance. So you could transmit energy across the UK without losing any,
3664560	3670480	you have new energy storage opportunities, fusion reactors, the type I talked about require
3670480	3675680	very powerful superconducting magnets to work, MRI machines require them. So they're kind of
3675680	3680880	this magical class of materials called quantum materials. And because they are quantum materials,
3680880	3685600	there's only so much we can really understand them by applying classical computing techniques to them,
3685600	3689600	whereas a quantum computer would let you simulate them in a completely different way.
3689600	3693600	And as a result, we might be to discover new materials that we don't have access to today.
3693600	3697520	Before we start this interview, I asked you if you would consider moving to the States,
3697520	3702080	you know, that's kind of a cliche thing, but you know, people generally make their fortune
3702080	3709200	over in the US when it comes to technology businesses. You said no, or you weren't really
3709200	3716160	inclined to. So why do you want to stay here in the UK? I mean, I think the UK's given me kind of,
3716720	3723200	I think, you know, I think I'm quite patriotic. I feel very, you know, very appreciative of
3723200	3728160	what I've been given in terms of the privilege of growing up somewhere with universal health care,
3728960	3734400	with great education, with, you know, freedom of speech, the rule of law, like there's lots of
3734400	3740160	aspects of our society I'm very proud of, proud to be kind of, you know, to proud of and I believe
3740160	3745120	are really important to endure. And as we were talking also earlier, you know, we should recognize
3745120	3750800	these things as fragile, you know, Iran was a democracy once and is not now. And so I feel a
3750800	3757680	duty to give back to the UK, give back to Europe and very specifically the sort of investment
3757680	3762880	fund that my partners and I have set up, it's a we're all former founders, right? That's the
3762880	3768080	first thing about us. And the second thing is, we've set it up to try and have GDP level impact
3768080	3774240	on Europe. And so the idea is to help build some companies using our kind of scar tissue as founders
3774240	3779360	to help another generation of founders who are like us to build companies that can end up being
3779360	3783680	bigger than we've seen in Europe up until now. So I think of Skype, and I think Skype could have
3783680	3788160	been Facebook, I see DeepMind, I think DeepMind could have been Google or OpenAI, or I look at
3788160	3793120	ARM, I think ARM could have been NVIDIA. And so we've got these companies where we never really
3793120	3796960	got them to be what they could have been their full potential to be these like transformational
3796960	3804720	European technology companies. And so our mission as a kind of as a fund is to try to produce a few
3804800	3809440	of these companies that can change whole industries for the better. And by doing so have
3809440	3813920	impacts on European GDP, but also put Europe at the table when it comes to describing what's
3813920	3818000	happening in an important new era of technology. So if we're going to put fusion on the grid
3819440	3826960	in the next, you know, in the 2030s say, and Germany has been the state with the most kind
3826960	3831280	of entrepreneurialism to invest in accelerators well ahead of everyone else in the world, it
3831280	3837200	will be a travesty if the the fusion company that turns that into a into a startup isn't in
3837200	3843440	Europe, in my opinion. And so I do feel I do feel just very embedded in European values. I mean,
3843440	3847120	I lived in China, I lived in America, I've been a lot of admiration for both the societies in
3847120	3851200	different ways. But I guess I'm kind of pretty European to my core.
3852000	3855920	That's interesting. You said patriotic, but then you say European. So can you go into that a bit
3855920	3861040	more? Because in the UK, we've had this, you know, I don't want to sort of tread over old ground,
3861040	3863200	but they're often held in counterpoint to one another.
3863680	3869120	Yeah, and I think that's a bit of a, I don't know, like, I think there's a lot of nuance that's been
3869120	3874800	lost from discussion around Brexit. So a good example is one of the AI companies I work with.
3875680	3880400	The founder chose to actually locate the company in London rather than Silicon Valley,
3880400	3884000	because it was going to be easier to recruit the people who wanted into the company.
3884000	3887920	Right. So we talk about the UK, you know, being a much more, you know, people talk,
3887920	3892080	people caricature it as being more closed post Brexit. But actually, there's been some great
3893200	3897840	research done by John Paul Murdock at the time, the Financial Times that I think sort of shows
3897840	3901440	that's actually not necessarily true in all domains. And I haven't heard that as much as
3902000	3904560	you might think from some of these leading edge technology companies.
3906160	3909040	And so I think there's this kind of, we want to make everything black and white,
3909040	3915760	but actually there are ways in which I think that, you know, a suitably ambitious progressive
3915760	3919600	government that really wants to embrace Brexit and make it work could do really interesting
3919600	3924160	things with it. And so I don't, I'm not like reflex it, even though I voted remain, I'm not
3924160	3928080	reflexively negative about Brexit and sort of, it's just all these people who just basically
3928080	3932320	sit there complaining about it all day long on Twitter, like five, you know, five, you know,
3932320	3938400	and years on what at 20 2016, right? So like seven years on from it, that there's sort of,
3938400	3944000	there's a, there's a sort of, I don't know, it's become a sort of a tribal identity rather than a
3944000	3948640	sort of first principles assessment of what we should actually do as the UK to, to sort of
3949600	3955280	make the UK the best possible place to live. Yeah, David Deutch, who is one of these hugely
3955280	3959040	influential figures in Silicon Valley, British man, you know, he's based, I think in Oxford
3959840	3966320	at the moment, and he advocated leave. And, you know, I don't agree with his arguments,
3966320	3969600	I certainly don't think it was wise to leave the single market in the way that we have done.
3970720	3974480	But this idea that everybody who voted leave is thick and stupid, you know, David Deutch is
3974480	3979120	literally one of the smartest human beings who's ever lived. And, and like you say, there are,
3979120	3983520	there are opportunities, particularly with goal oriented public policy, with new technologies
3983520	3988400	that you probably could do interesting things with in a way that you probably can't, or it's harder
3988400	3993120	to do inside the European Union. But going back to the point about patriotism. So you're
3993120	4001040	patriotic towards the UK, or to Europe, or both? Yeah, I mean, I, I, I guess I, the reason I,
4001680	4007360	I think, I think George Orwell had a lovely sort of take on nationalism versus patriotism.
4007360	4011920	I think he said something like, you know, nationalism is kind of the assertion of your
4011920	4016320	values and other people and pushing your country's values on other people, whereas
4016320	4019600	patriotism is just sort of saying, it's kind of defending your values and just saying like,
4020160	4023520	these are things that are important to me and we want to preserve some of these things. And
4023520	4028880	I, I feel, you know, the, you know, NHS is the good, the kind of classic example is just something
4028880	4035360	that I feel very grateful for. And I feel patriotic about wanting it to, it to continue and to thrive.
4035360	4040240	And, you know, if we're going to apply AI in the NHS, I want it to be done in an incredibly
4040240	4044880	thoughtful way that expands what we've got today rather than anywhere undermines it.
4044880	4048240	So where did you live in China in the US? You said you moved there a few years ago?
4048240	4053600	Yeah, so I lived in, I lived in, I moved to China when I was 18 in 2000, study Mandarin, and then
4054720	4058720	went to university, started a kind of engineering machine learning, and then went back to China
4058720	4068160	in 2005, again, to study machine, to study Mandarin. And then I moved to the US Silicon Valley in 2006.
4068160	4075120	And so, I guess I've, I think I was drawn to both places because there were a lot was changing
4075120	4079280	very rapidly in the world. And it was, I think, somewhere where the pace of change was very
4079280	4085120	exhilarating. But I really feel, I think, the most at home in Europe in terms of kind of the
4085120	4091040	values, you know, I think, and I'd love to see Europe's technology industry and Europe's
4091040	4096320	governments really kind of rise to the moment that is coming in, in, with all this exponential
4096320	4102160	technological change. The final question, because it goes back to the thing you, you've
4102160	4105600	mentioned, you wrote this essay about AI nationalism, there's a great talk on YouTube,
4105600	4109600	by the way, once people have finished watching this and maybe watched another
4109600	4114480	Navarro media video, you do a great talk on AI nationalism. It's from a few years ago.
4115520	4121920	And you, you quote a great book, AI superpowers. And it's the hypothesis of that, I think is a
4121920	4128000	really strong one of the best books I've read in a while. A few years ago, I think Pricewaterhouse
4128000	4133280	Cooper say that between 2015 and 2035, you know, 15, 16 trillion dollars will be added to the global
4133280	4139840	economy by AI, more or less something like that. And about 70% of that goes to basically the US and
4139840	4145600	China. So all of the gains of this new technology, basically are concentrated in these two AI
4145600	4150480	superpowers. Imagine the steam engine, but rather than just Britain benefiting initially,
4150560	4153600	like we get with, you know, the steam revolution, the industrial revolution and then colonialism,
4154160	4159600	AI basically redounds the benefit of Beijing and Washington or Washington slash California.
4160800	4164240	Can you go into this hypothesis? Because it sounds to me, on the one hand, you just said a moment
4164240	4171120	ago, you're patriotic to Europe. Yet this idea of AI nationalism seems to indicate that Europe
4171120	4175520	isn't really at the races. And actually, on the present path, the two places that will benefit
4175520	4183200	from this are China and the US. So I think I think that Europe in general, setting aside the UK is
4183200	4191120	not very significant, unfortunately, within AI right now. If you look at where the most important
4191120	4196240	kind of concentrations of sort of talent and power are within AI and capabilities, it's actually
4196240	4203760	really cities that matter. And it's Toronto, San Francisco, London and Beijing. And London is
4203760	4207440	very much on the map. So London matters more than New York? I would say so. Yeah. I mean,
4207440	4212640	so Demis Osavis, who arguably kicked off the race we've currently got going on with the founding
4212640	4219360	of DeepMind and is an absolutely exceptionally talented person, is now running all of Google's
4219360	4226240	efforts globally from London. You know, the vast majority of the sort of best AI researchers
4226240	4230320	within Google are based in London at DeepMind's office. And so London is incredibly important.
4230320	4235040	It's not just London, not just DeepMind. We also have other organisations of a sort of similar,
4235040	4242240	you know, research stature to DeepMind. So I think that we should not rule the UK out. And
4242240	4248640	actually, I think it's a real opportunity for the UK to play a leadership role within Europe
4248640	4254960	by kind of bringing some of those assets to the table when Europe wants to do stuff in AI regulation.
4255920	4261760	But to go back to your question about kind of this race, this race between different countries,
4262560	4268400	I think it's incredibly nuanced and complicated because there are basically three different
4268400	4273200	levels you need to think about it at. The first is economic. So, you know, a country that has
4273200	4278480	incredibly sophisticated AI companies will probably benefit more economically from it than
4278480	4283040	those that don't. And that's the kind of takers or makers or, you know, the sort of steam analogy
4283040	4290640	you gave. The second, and you know, there is the second is basically military. And so AI will
4290640	4296080	definitely be used by militaries to achieve decisive advantage. And so in that area, again,
4296080	4301760	like a state is incentivised to build out the capacity to, you know, have autonomous autonomous
4303040	4306640	and, you know, even in the Ukraine conflict, you actually see the use of machine learning to
4306640	4312400	actually, you know, provide a decisive edge in certain ways. And the third is this existential
4312400	4317280	thing of like, if we build something smarter, more capable than us, that isn't aligned with our
4317280	4324080	goals, it could wipe us all out. And if you look at the three different levels, you've got the,
4324080	4328560	and you think about the US and China in the way that Kaifu Lee kind of presented that that race,
4329440	4334560	US and China clearly are locked into a battle to compete economically. They're certainly locked
4334560	4338560	in a battle to compete militarily, like a hypersonic missiles or something like that.
4338560	4342320	But this final level, which is like, are they locked in a battle to kind of,
4342320	4347200	you know, ensure that the human species thrives? I think they are actually, you know, they should
4347200	4351200	be on the same page about that, right? So you've got this really challenging problem where you've
4351200	4356240	got two levels of competition, and one level where you desperately need cooperation. And that's why
4356240	4360480	I think that like the really missing piece right now here is kind of international leadership
4360480	4365600	around coordination, and how we approach these most powerful AI systems that could become super
4365600	4370160	intelligent. That's so interesting around AGI. And I can see the argument for cooperation,
4370160	4375840	as you would hope for, say around climate change. But I suppose pulling it back a bit,
4375840	4382880	so going back to the narrow AI things that you spoke about a while back. Clearly,
4382880	4387360	AI is going to be massively disruptive in terms of economics. And I suppose if you look at the
4387360	4392400	last 20, 25 years and Silicon Valley really capturing so much value across the West, you know,
4393040	4396320	I'm from Bournemouth. If you walk down the high street in Bournemouth, loads of businesses have
4396320	4400800	shut down because that value has been captured by a company headquartered in California, right?
4400800	4406320	And that is an incredible concentration of political, economic, and cultural power.
4407520	4411280	And the argument is the same thing will happen with AI with more businesses to a far greater
4411280	4417760	extent. And the concern is that basically the world will be divided into two economic spheres,
4417840	4425200	US, China centric, and the rest of the world will, broadly speaking, just be their satellites.
4425200	4429680	So we don't need to be talking about an AGI and oh, great power confrontation will lead to
4430320	4434880	somebody potentially creating Skynet because they're seeking military superiority. Although
4434880	4440080	that's an interesting debate. But we could be returning to a bipolar world similar to the Cold
4440080	4446000	War, but in many ways far more extensive. Or do you think I'm sort of being a bit sort of
4446000	4451200	hyperbolic here because you have so quickly to go back, Kai-Fu Li talks about AI superpowers.
4451200	4455600	Why would it be China and the US are the superpowers? They have the largest populations,
4456400	4460240	Ergo, the largest amounts of data, and they have the greatest amount of computational power.
4460240	4464320	You might think that, you know, the EU might be a third poll, but it's not really working out
4464320	4471840	here the two. So geopolitically, the geopolitics of AI, I mean, you can see a world, can't you,
4471840	4475840	where there's a great sort of economic dependence, basically a kind of colonialism.
4476560	4482640	So I think, you know, in the AI nationalism essay, I gave an example of a kind of that sort of AI
4482640	4487600	colonialism with a company called Cloudwalk from China that was selling certain technologies to,
4487600	4494240	I think it was Zimbabwe. But I actually don't really view it as much from a kind of how it
4494240	4500320	affects individual citizens at an international level, more national level. You know, I think
4501280	4506480	that kind of haves and have nots within technology are already playing out on a national level.
4506480	4512800	If you look at the, you know, if you look at the deprivation in the Bay Area, right, and you compare
4512800	4518720	the sort of the lot of some of the people, you know, on the streets in the Tenderloin compared
4518720	4524080	to some of the people running these AI companies, you've got far more, you know, the genie coefficient
4524080	4530160	in some ways in sort of some of the US cities or across US states is maybe more extreme than
4530160	4536240	it is between certain nations. And I'm more, I think that's more concerning the the the lot of
4536240	4541120	the average person on Bournemouth versus a lot of the average person in London than it is to me
4541120	4546000	of kind of these these sort of country versus country comparisons. And so I think actually
4546000	4554080	the really challenging political problem is as actually how you kind of how you how you make
4554080	4558800	how you kind of raise the floor more broadly in a world in which there are further and further
4558800	4564080	returns to capital. I think we'll end it there. Ian, this has been a fascinating conversation.
4564080	4574720	Thank you so much for joining us. Thank you for having me.
