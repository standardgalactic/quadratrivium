1
00:00:00,000 --> 00:00:06,240
CEO of OpenAI, he was interviewed about the worst case scenario. This stuff goes badly wrong.

2
00:00:07,280 --> 00:00:11,280
And he had this line where he said, worst case scenarios lights out for humanity. And that was

3
00:00:11,280 --> 00:00:16,640
a statement that Sam Ortonman, who's the CEO of OpenAI, the developers of ChatGPT and GPT4,

4
00:00:16,640 --> 00:00:21,840
made about that existential risk. Now, I don't think he's saying that about next year.

5
00:00:22,400 --> 00:00:26,640
But the real question is, if we build an incredibly powerful intelligence system before

6
00:00:26,640 --> 00:00:30,720
we figure out how to make it safe, I think we should have relatively low confidence it's going

7
00:00:30,720 --> 00:00:40,240
to go well for humanity. Artificial intelligence has been a topic of political conversation

8
00:00:40,240 --> 00:00:45,520
for quite some time. I should know. I partly wrote a book about it. It's called Fully Automated

9
00:00:45,520 --> 00:00:49,760
Luxury Communism. Though I must say, when I wrote that book and when it was published,

10
00:00:49,760 --> 00:00:54,800
many people were skeptical about the rise of the robots and how technological change would

11
00:00:54,800 --> 00:01:00,480
disrupt the economic status quo. Sure, they would say, Aaron, this might be coming in the

12
00:01:00,480 --> 00:01:07,200
2040s or 50s or 60s or maybe when we're all dead in the 2100s. But this is not a problem

13
00:01:07,200 --> 00:01:13,600
for right now here in the 2020s. Yet with the development of ChatGPTO the last 12 months,

14
00:01:13,600 --> 00:01:18,800
people are finally starting to have that conversation. Maybe artificial intelligence

15
00:01:18,800 --> 00:01:24,720
is far more developed than we realize. And with exponential improvements, perhaps it's getting

16
00:01:25,120 --> 00:01:31,840
out of control. Ian Hogarth is a founder and investor in technology companies. He wrote a

17
00:01:31,840 --> 00:01:38,000
brilliant article in the FT Weekend Magazine talking about all of these issues and how,

18
00:01:38,000 --> 00:01:46,080
in fact, we may need political regulation to catch up with technology. Ian, welcome to Downstream.

19
00:01:46,080 --> 00:01:50,720
Thanks for having me. It's our pleasure. We're going to talk about some really big issues which

20
00:01:50,720 --> 00:01:55,280
a lot of our audience may not be that acquainted with artificial intelligence, machine learning,

21
00:01:55,280 --> 00:02:00,480
et cetera. Why do you have authority on these topics and why should they listen to you over the next

22
00:02:00,480 --> 00:02:07,600
hour? I'm not sure I have that much authority. I can tell you my background. I studied machine

23
00:02:07,600 --> 00:02:12,400
learning at university. I did a masters, started off with making a robot and then after that made

24
00:02:12,400 --> 00:02:17,760
a computer vision system. So that's systems where you basically teach a machine to see in some way,

25
00:02:17,760 --> 00:02:22,960
recognize patterns, visual patterns. And the system that I built was one that could replicate

26
00:02:22,960 --> 00:02:27,680
some of the job that a radiologist does when looking at cancer biopsy images. So I sort of

27
00:02:27,680 --> 00:02:32,640
started out with, I suppose, a very engineer's mindset thinking about this. I then built a

28
00:02:32,640 --> 00:02:38,800
software business as a founder and a CEO for a number of years. And then I've kind of been

29
00:02:39,600 --> 00:02:47,120
involved in this since then as really an investor and I guess an academic. So as an investor,

30
00:02:47,920 --> 00:02:53,680
invested at about 50 companies applying AI to different fields, including some of the larger

31
00:02:53,680 --> 00:02:58,320
companies in the field like Anthropic, which is the second most funded AI startup in the world,

32
00:02:59,040 --> 00:03:04,880
and Helsing, which is the leading AI defense company. And then outside of that, I've really been

33
00:03:04,880 --> 00:03:15,200
trying to expand the level of public awareness of what is happening in AI, primarily by writing a

34
00:03:15,200 --> 00:03:19,600
report called the State of AI Report, which I've written for the last five years. It's one of the

35
00:03:19,600 --> 00:03:23,840
most widely read annual reports on everything happening in machine learning over the course of

36
00:03:23,840 --> 00:03:29,920
a year. So I've been sort of trying to just expand the kind of the quality of public information

37
00:03:29,920 --> 00:03:34,400
out there for policymakers or for citizens who are interested to find out more.

38
00:03:34,400 --> 00:03:38,320
That's quite an authority. I mean, I like the humility at the start, but I think you're going

39
00:03:38,320 --> 00:03:45,920
to know what you're talking about. And for people who aren't quite clear about your writing,

40
00:03:45,920 --> 00:03:49,520
I mean, I suppose a quick stop for them would be this recent article you wrote in the FT weekend

41
00:03:49,520 --> 00:03:52,960
magazine. Can you just briefly go over some of the issues you discussed in it?

42
00:03:53,600 --> 00:04:02,400
Yeah, so I wrote that article maybe a month ago now. And the core idea is that there are a small

43
00:04:02,400 --> 00:04:08,400
number of incredibly well-funded private companies, primarily in London and San Francisco,

44
00:04:09,280 --> 00:04:13,600
that are kind of locked into a race, where they're all racing as fast as possible to build

45
00:04:14,320 --> 00:04:20,320
what they describe as AGI. And AGI is basically a kind of godlike AI system that is capable of

46
00:04:20,320 --> 00:04:26,960
doing almost anything a human can do and more. And I felt like that race is now getting quite

47
00:04:26,960 --> 00:04:32,000
out of control. And we need to slow it down. And so I wrote that essay to really just try to

48
00:04:33,840 --> 00:04:39,600
shine a light on some of the things that I learn about as part of my work,

49
00:04:39,600 --> 00:04:43,440
but that are maybe not public domain. So I spend time talking to the people running these

50
00:04:43,440 --> 00:04:50,160
organizations. And I just sort of could see that the level of concern behind closed doors had really

51
00:04:50,160 --> 00:04:54,320
ratcheted up. And it felt like there was a big disconnect between the kind of public discourse

52
00:04:54,320 --> 00:04:58,160
and what people were saying in private. So I wrote the essay really just to try to

53
00:04:58,160 --> 00:05:02,080
close that gap a little bit. That's so interesting. So we're talking really about a machine which is

54
00:05:02,080 --> 00:05:06,160
capable of augmenting its own intelligence. And very quickly, you get a super intelligence,

55
00:05:06,160 --> 00:05:10,400
so to speak, an intelligence that we can't really fathom as human beings. You said that the fears

56
00:05:10,400 --> 00:05:14,640
and the concerns around that had ratcheted up. Over what time frame are we talking here?

57
00:05:15,200 --> 00:05:21,840
So I suppose maybe it's worth zooming out and just talking about progress in the field in general.

58
00:05:21,840 --> 00:05:30,400
So if you sort of look at AI systems over the last decade, they've quite predictably gotten bigger.

59
00:05:30,400 --> 00:05:34,000
And so what we've done is every year, we've been kind of increasing the amount of computing resources

60
00:05:34,000 --> 00:05:38,720
would give the largest AI models. And we've also been giving those systems more data to train on.

61
00:05:38,720 --> 00:05:43,360
And so that's been actually a very consistent exponential curve that's been running now for

62
00:05:43,360 --> 00:05:48,400
over a decade. And there've really been a couple of big kind of moments in time. The first was the

63
00:05:48,400 --> 00:05:53,280
founding of DeepMind, which really just brought a huge amount of ambition and energy to this challenge

64
00:05:53,280 --> 00:05:57,760
of like making these even bigger and more powerful systems. And the second I would say was OpenAI,

65
00:05:57,760 --> 00:06:02,080
which introduced a competitor DeepMind that suddenly meant there was a race. And those

66
00:06:02,080 --> 00:06:07,280
organizations have been racing against each other now for best part of their entire founding history.

67
00:06:08,000 --> 00:06:13,040
And if you look at that, we've gone from kind of, you know, feeding these systems, you know,

68
00:06:13,920 --> 00:06:19,120
some tens of thousands, you know, millions of images to feeding these systems most of the

69
00:06:19,120 --> 00:06:23,440
internet. And we've increased the amount of computing resource we give these kind of most

70
00:06:23,440 --> 00:06:29,440
powerful AI models by a factor of 100 million in just a decade. And so there's been this very,

71
00:06:29,440 --> 00:06:35,120
very, you know, continuous progress in the field. But as with any exponential, it's really only when

72
00:06:35,120 --> 00:06:39,040
you get to the steeper of the curve, you start feeling it. And I think the last couple of years

73
00:06:39,040 --> 00:06:42,960
are kind of busy, where the curve has just suddenly felt a lot steeper and things have

74
00:06:42,960 --> 00:06:46,080
been changing weekly or daily rather than yearly.

75
00:06:46,880 --> 00:06:51,120
Wow, that is really extraordinary. So the word exponential of people who aren't necessarily

76
00:06:51,120 --> 00:06:56,640
familiar with it, my goodness, virtually everybody is in 2023. But this was broadly

77
00:06:56,640 --> 00:07:01,120
integrated within discussions around computer science by Gordon Moore and Moore's law. And

78
00:07:01,120 --> 00:07:05,200
this idea that broadly speaking, computational power would, there's a bunch of ways of sort of

79
00:07:05,200 --> 00:07:09,760
discussing it. But the same amount of power would basically halve and cost every 18 months to two

80
00:07:09,760 --> 00:07:14,880
years. And that has happened for a long time, it's kind of decelerating, it's happened for a very

81
00:07:14,880 --> 00:07:20,240
long time. You were saying with AI, there's a bunch of variables. So it's not just the computational

82
00:07:20,240 --> 00:07:25,440
power, it's also the data that it's feeding on. And the two of these is important, right?

83
00:07:25,440 --> 00:07:32,000
Correct. And so, for your listeners, an exponential, thinking about it simply is,

84
00:07:32,000 --> 00:07:37,040
for example, a system that doubles every year. And so if you play that out over a number of

85
00:07:37,040 --> 00:07:41,520
years, you get a very steep curve, because some property of the system is allowing it to kind

86
00:07:41,520 --> 00:07:46,640
of grow in that way. And the classic we saw was with COVID. But we as humans, I think, are just

87
00:07:46,640 --> 00:07:52,000
really poor at thinking about exponentials. They're not intuitive to us. And so we saw it with

88
00:07:52,000 --> 00:07:57,040
COVID. It's kind of January, people start paying a bit of attention, February things get more serious,

89
00:07:57,040 --> 00:08:01,440
March, some people really start to get, and then we're suddenly locked down. And that's kind of the

90
00:08:01,440 --> 00:08:06,880
nature, I think, of a system where you're having a doubling effect over some period of time. And

91
00:08:06,880 --> 00:08:13,520
that's what's been happening in AI for a decade is just we're now at the kind of February 2020

92
00:08:13,520 --> 00:08:16,720
moment in AI where things are just going super, super quickly.

93
00:08:16,720 --> 00:08:21,360
That's such a powerful analogy. Obviously, you can't go into the nature of private conversations

94
00:08:21,360 --> 00:08:25,360
you've had with people. But when people are putting a date on it, what are they saying

95
00:08:25,360 --> 00:08:29,200
with regards to an AI then? And like I said, it's hard to predict by virtue of exponential growth,

96
00:08:29,200 --> 00:08:36,000
but are they saying in the 2020s, next year? I mean, what's the broad time frame here?

97
00:08:36,000 --> 00:08:40,480
Well, so the first thing I'd say is that the people leading these companies have been thinking

98
00:08:40,480 --> 00:08:46,160
about this problem for a long time. Some of them have been kind of a good example of a Shane leg.

99
00:08:46,640 --> 00:08:51,520
Someone I admire greatly, he's a brilliant computer scientist. He runs DeepMind's AI

100
00:08:51,520 --> 00:08:55,680
alignment efforts, which we'll probably talk about in a bit, what that is as an area.

101
00:08:56,560 --> 00:09:01,520
But Shane, he did his PhD on sort of a computational basis for machine super

102
00:09:01,520 --> 00:09:05,440
intelligence, has made many sort of quite sophisticated predictions over the years around

103
00:09:05,440 --> 00:09:09,520
what it would exactly take in terms of the amount of computing resource and the amount of data before

104
00:09:09,520 --> 00:09:13,280
you would actually get a super intelligent machine that was kind of an artificial general

105
00:09:13,280 --> 00:09:17,680
intelligence. And so the people in this field have been thinking about it a long time,

106
00:09:18,320 --> 00:09:23,680
and I pay the most attention to the people who have been consistently making good predictions

107
00:09:24,400 --> 00:09:29,280
to me behind closed doors about what will happen. And the thing that I've noticed is

108
00:09:29,280 --> 00:09:32,800
it used to be the case that people would say stuff like, you know,

109
00:09:32,800 --> 00:09:39,680
it's possible we might get an AGI, a super intelligent machine in the next 30 years or the

110
00:09:39,680 --> 00:09:43,520
next 20 years, but everyone I think thought the idea of kind of something happening next year

111
00:09:43,520 --> 00:09:48,880
was kind of ridiculous. And now I think if you ask people, you know, let's say, for example,

112
00:09:48,880 --> 00:09:53,600
you know, there was a select committee and the various leaders of these labs, the technical

113
00:09:53,600 --> 00:09:58,320
leaders were asked under oath, what's their probability that we get a super intelligence

114
00:09:58,320 --> 00:10:02,720
next year, it's not going to be 0%. Whereas I think it would have been before, whereas now it

115
00:10:02,720 --> 00:10:09,120
might be, I don't know, 5%. And so you have this kind of shift where I think everybody is starting

116
00:10:09,120 --> 00:10:13,280
to sort of say, actually, we might be closer than we realized, we should start taking it

117
00:10:13,280 --> 00:10:19,120
seriously the possibility that we might be very close. So if a private enterprise develops an AGI

118
00:10:19,120 --> 00:10:21,520
and artificial general intelligence, what happens next, do you think?

119
00:10:23,600 --> 00:10:30,560
I have no idea. I think that the AI alignment community would basically say, you know,

120
00:10:31,840 --> 00:10:37,040
most likely outcome is we're all dead. The CEO of OpenAI, he had this kind of,

121
00:10:37,120 --> 00:10:41,120
it was, you know, interviewed about the worst case scenario, this stuff goes badly wrong.

122
00:10:42,160 --> 00:10:46,000
And he had this line, we said, you know, worst case scenarios, lights out for humanity. And that

123
00:10:46,000 --> 00:10:51,520
was a statement that Sam Ortman, who's the CEO of OpenAI, the developers of ChatGPT and GPT-4,

124
00:10:51,520 --> 00:10:56,720
made about that kind of existential risk. Now, I don't think he's saying that about next year.

125
00:10:57,280 --> 00:11:01,920
But the real question is, if we build an incredibly powerful intelligence system before we figured

126
00:11:01,920 --> 00:11:06,080
out how to make it safe, I think we should have relatively low confidence it's going to go well

127
00:11:06,080 --> 00:11:13,360
for humanity. And there's a kind of very, you know, I guess, sophisticated intellectual argument

128
00:11:13,360 --> 00:11:18,160
about how to think about that. And that's the sort of thing that someone like Eliezer Yudikowski

129
00:11:18,160 --> 00:11:24,960
would write about, where he'll talk through the sort of exact mechanisms, but why a system that's

130
00:11:24,960 --> 00:11:31,600
much more intelligent than humans treats us, you know, treats us badly, you know, primarily by

131
00:11:31,600 --> 00:11:36,960
accident. But I think actually the kind of common sense way of thinking about it makes more sense,

132
00:11:36,960 --> 00:11:44,080
which just says, you know, humans have kind of changed the environment on earth very significantly

133
00:11:44,720 --> 00:11:48,720
as a result of our intelligence relative to other species. And that's had, you know,

134
00:11:48,720 --> 00:11:52,880
significant consequences for some species, and for the biosphere in general. And I think we

135
00:11:52,880 --> 00:11:57,200
should sort of just common sense tells you that something similar might happen if we invent something

136
00:11:57,840 --> 00:11:58,880
more intelligent than us.

137
00:12:00,560 --> 00:12:05,200
Wouldn't the counter argument be, I suppose, that though that we've learned that over time,

138
00:12:05,200 --> 00:12:10,880
we are dependent on the biosphere for our own systems, political, social, economic to sustain.

139
00:12:10,880 --> 00:12:15,600
I suppose as they're not an optimistic account of an AGI, and I think obviously there's a great

140
00:12:16,560 --> 00:12:22,000
deal of thought put behind being skeptical about this stuff. And I'm also skeptical.

141
00:12:22,000 --> 00:12:25,840
But is there not also a world where you have an AGI, which is in some way benevolent,

142
00:12:26,480 --> 00:12:30,480
capable of very long term planning, capable of all some problem solving on a scale that we

143
00:12:30,480 --> 00:12:38,080
can't really comprehend? Yeah. And just to be really clear, I think that is, you know,

144
00:12:38,800 --> 00:12:43,200
that is the happy path we're now on. I think there's basically three paths. There's the,

145
00:12:43,920 --> 00:12:47,680
we have a moratorium that just completely shuts this down. And that could be like,

146
00:12:47,680 --> 00:12:51,520
you know, some of the other moratorium we've had around, you know, genetic engineering,

147
00:12:51,600 --> 00:12:59,120
for example, you know, eugenics. There could be a another path where we develop this kind of

148
00:12:59,120 --> 00:13:03,920
hastily and not thoughtfully and kind of wipe ourselves out in the process. And there's this

149
00:13:03,920 --> 00:13:07,600
third path, which is the one I think we should really all be oriented on, which is we build

150
00:13:07,600 --> 00:13:13,760
systems that massively expand the amount of kind of wisdom in the universe. And we cure diseases

151
00:13:13,760 --> 00:13:18,640
that can't be cured today. We, you know, we have enormous technological abundance. And so there

152
00:13:18,640 --> 00:13:23,360
certainly is an approach where we build EGI and it goes incredibly well for us as a species.

153
00:13:24,400 --> 00:13:27,920
The question is, are we on track to do that or not the way we're doing it today,

154
00:13:27,920 --> 00:13:31,520
with a small number of private companies racing to do it as quickly as possible?

155
00:13:32,080 --> 00:13:39,840
So with regards to chat GPT-4, which was released in March by OpenAI, which is aligned with Microsoft,

156
00:13:41,120 --> 00:13:46,000
how big a jump was that from chat GPT-3, which was obviously the previous version?

157
00:13:46,000 --> 00:13:56,400
So chat GPT when it came out was was arguably a user for interface on top of a very powerful

158
00:13:56,400 --> 00:14:01,040
language model that already existed. And so you already had these amazing language models that

159
00:14:01,040 --> 00:14:06,160
OpenAI had trained and anthropic and Google had trained to do very powerful things. And

160
00:14:06,160 --> 00:14:11,040
what OpenAI did with chat GPT is they basically created a way to interact with it that let that

161
00:14:11,040 --> 00:14:18,080
suddenly opened it up to a lot more people. And so in many ways, it wasn't a sort of research

162
00:14:18,080 --> 00:14:21,600
technological breakthrough. It was actually a user interface breakthrough and said like, here's a

163
00:14:21,600 --> 00:14:27,360
way to use this that suddenly feels a lot more organic and natural to an end user, a consumer

164
00:14:27,360 --> 00:14:33,840
playing around with GPT. The thing they released after that, which was called GPT-4, was a significant

165
00:14:33,840 --> 00:14:40,720
update because the chat GPT was based on what they were calling it GPT-3.5. And that was a big

166
00:14:41,360 --> 00:14:49,040
jump in the underlying model. So for example, GPT-3.5, when you sort of tried to get it to do

167
00:14:49,040 --> 00:14:54,560
the bar exam scored in the bottom 10% of results, whereas GPT-4 scored in the top 10% of results.

168
00:14:54,560 --> 00:14:58,880
So in a single generation of models, you had this massive leap in capabilities where it went from

169
00:14:58,880 --> 00:15:04,240
basically not really being able to be a lawyer to being able to be a lawyer. And so GPT-4 was a

170
00:15:04,240 --> 00:15:08,240
massive step forward for language models in general. And like, you know, one of the most

171
00:15:08,240 --> 00:15:14,240
impressive technological artifacts humanity has ever created. And what was the basis of that jump

172
00:15:14,240 --> 00:15:18,480
from 3.5 to 4? Was it purely because there was more data being fed, or there's been a sudden

173
00:15:18,480 --> 00:15:25,600
boost in computational power? So it's a great question. And we don't really know. So OpenAI

174
00:15:25,600 --> 00:15:30,240
hasn't really explained to us what data they trained it on, the amount of computing resources

175
00:15:30,240 --> 00:15:36,960
they use for it, any algorithmic breakthroughs they made, they have kind of gone from being

176
00:15:38,240 --> 00:15:42,240
open AI, kind of being very open with their research to being much more close with their

177
00:15:42,240 --> 00:15:46,720
research. And they have done it, I believe, for good reason, which is they sort of don't want

178
00:15:46,720 --> 00:15:52,400
to accelerate things any more than they have to by suddenly making it more possible to replicate

179
00:15:52,400 --> 00:15:55,680
this and kind of cause a huge amount of proliferation of this technology.

180
00:15:55,680 --> 00:16:00,320
This is so interesting. So could it be possible then that OpenAI are further down the road to

181
00:16:00,320 --> 00:16:06,800
AGI than we really discuss, we really talk about? But the incentives aren't really there to be quite

182
00:16:06,800 --> 00:16:11,040
public about it, right? The incentives are there to actually be quite private and discreet and not

183
00:16:11,040 --> 00:16:16,240
really convey how close we are to a really transformational technology.

184
00:16:16,240 --> 00:16:20,880
Yeah, the incentives are really challenging. So I'll actually give you a quite a concrete example

185
00:16:20,880 --> 00:16:26,080
that I think brings the race to life. So I'm one of the first investors in this company,

186
00:16:26,080 --> 00:16:32,000
Anthropic. And Anthropic was founded by a group of people who left OpenAI and set up a new AI

187
00:16:32,000 --> 00:16:38,640
startup. And it was the people who did it, who founded it, were the people who led the research

188
00:16:38,640 --> 00:16:44,000
on GPT2 and then GPT3, so the precursors to these large language models. So they really are the key

189
00:16:44,000 --> 00:16:49,600
people from OpenAI who did a lot of the large language models sort of early work. And their new

190
00:16:49,600 --> 00:16:57,600
company is very much oriented around a greater emphasis on safety. So they have something like

191
00:16:57,600 --> 00:17:03,680
50% of their headcount in 2021 was dedicated to alignment research and safety, which is higher

192
00:17:03,680 --> 00:17:09,760
than any of the other labs like DeepMind or OpenAI. And they had a product like ChatGPT

193
00:17:11,040 --> 00:17:16,320
about six months prior to OpenAI releasing ChatGPT. And if they'd released it, it would have

194
00:17:16,320 --> 00:17:20,800
suddenly put Anthropic on the map in a big way. They would have attracted so much more capital,

195
00:17:20,800 --> 00:17:25,280
more attention, and they held it back because they felt like it would just accelerate this race

196
00:17:25,280 --> 00:17:31,440
in a counterproductive way. And so if you think about the incentives, it's working against them

197
00:17:31,440 --> 00:17:36,880
as a capitalist entity to just hold back stuff, to release less, to create less hype. It's quite

198
00:17:36,880 --> 00:17:42,800
challenging. And so recently, they just actually made an announcement maybe two weeks ago where

199
00:17:42,800 --> 00:17:48,640
they expanded the context window for the largest language models to 100,000 tokens. And just to

200
00:17:48,640 --> 00:17:53,920
explain what that means, it's basically the size of the document that you can feed into a language

201
00:17:53,920 --> 00:17:58,880
model and have it work with for you. And so it massively changes what you can do. You can feed

202
00:17:58,880 --> 00:18:05,040
like a huge legal document or a massive code base into a GPT-4 like model and get a much more

203
00:18:05,040 --> 00:18:09,760
sophisticated response as a result of that. So it's a huge technological breakthrough. It lit

204
00:18:10,560 --> 00:18:15,360
the AI research community and start-up community on fire when they did it. And that

205
00:18:16,080 --> 00:18:20,080
ultimately attracted more attention to them, probably more capital over time. And so there's

206
00:18:20,080 --> 00:18:24,480
these perverse incentives where if you're a startup, you're incentivized to get as much

207
00:18:24,480 --> 00:18:28,400
capital and attention as possible so you can go faster. But actually, if everyone does that,

208
00:18:28,400 --> 00:18:34,240
then we burn the time we have to make this stuff safe. So it's a very challenging coordination

209
00:18:34,240 --> 00:18:38,880
problem where the incentives encourage racing rather than careful, slowed-down coordination.

210
00:18:39,840 --> 00:18:44,480
I'm very happy you said that. There's a great quote from Jeffrey Hinton who recently resigned

211
00:18:44,480 --> 00:18:49,120
from Google. And he said in an event, I think Google was very responsible to begin with,

212
00:18:49,120 --> 00:18:54,960
and this is deep mind. But once OpenAI had built similar things using money from Microsoft and

213
00:18:54,960 --> 00:18:59,920
Microsoft decided to put it out there, then Google didn't have much choice. If you're going to live

214
00:18:59,920 --> 00:19:06,160
in a capitalist system, you can't stop Google competing with Microsoft. So it almost sounds to

215
00:19:06,160 --> 00:19:12,080
me like one of the most powerful things about the market system competition, which can lead to

216
00:19:12,080 --> 00:19:17,440
incredible efficiencies, has upside as well as downside. But particularly with regards to AI,

217
00:19:17,440 --> 00:19:23,200
this sounds almost like you couldn't build a worse system to potentially accelerate development

218
00:19:23,200 --> 00:19:27,360
while also not really addressing things which could go very badly wrong.

219
00:19:28,000 --> 00:19:34,560
Yeah, I think that's, it's very challenging because there are areas of AI research where we

220
00:19:34,560 --> 00:19:39,680
I think actually capitalist competition is extremely good. So for example, there's 10 start-ups

221
00:19:39,680 --> 00:19:44,960
and they're all competing to make AI systems that can take in cancer biopsy images, analyze them

222
00:19:44,960 --> 00:19:49,440
really well and improve the lives of patients. I'm not worried about that having a negative

223
00:19:49,440 --> 00:19:53,840
consequence on the world. And I think actually the price signals the competition will be really

224
00:19:53,840 --> 00:20:00,800
good and it'll ultimately give us all cheaper healthcare, more innovation in the market. So

225
00:20:01,760 --> 00:20:06,800
the area of, I guess, narrow AI, where AI is just doing a single task, quite specified,

226
00:20:06,800 --> 00:20:11,040
and without these existential considerations, I think this kind of capitalist competition

227
00:20:11,040 --> 00:20:16,720
can be great. The problem is if we're trying to apply the same logic to the part of the problem

228
00:20:16,720 --> 00:20:21,120
where we're trying to build something smarter than us, there's basically a new species. And that,

229
00:20:21,120 --> 00:20:25,600
I think the kind of capitalist market dynamic is not helpful. And I think that, you know,

230
00:20:25,600 --> 00:20:29,040
what's great is that the leaders of these organizations, I think in their own ways,

231
00:20:29,040 --> 00:20:34,160
they all kind of have done important things to acknowledge this. So, you know, Demis,

232
00:20:34,880 --> 00:20:40,640
the CEO of DeepMind is someone I really admire, you know, he's really oriented a lot of DeepMind's

233
00:20:40,640 --> 00:20:45,680
efforts towards expanding the scientific commons, you know, things like AlphaFol,

234
00:20:45,680 --> 00:20:49,280
which they released for free, and they've really expanded the amount of, that's not a very sort

235
00:20:49,280 --> 00:20:53,920
of capitalist maneuver to basically produce this massive breakthrough and then kind of give it away.

236
00:20:54,640 --> 00:20:59,840
But I think it hints at how he thinks the economic gains from this should be distributed.

237
00:20:59,840 --> 00:21:06,000
Sam Altman, you know, the CEO of OpenAI, he's talked about how he wanted to have the government

238
00:21:06,000 --> 00:21:10,240
fund OpenAI early on. So he didn't want to raise money from private investors, just he

239
00:21:10,240 --> 00:21:15,600
didn't get the support from the government to do that. He's also, he and his team have explicitly

240
00:21:15,600 --> 00:21:21,040
said that if the race becomes too dangerous in their charter, they've said, we will merge and

241
00:21:21,040 --> 00:21:27,200
assist another player to change the coordination dynamics for the better. And Anthropik have got

242
00:21:27,200 --> 00:21:31,360
a very, you know, very, very thoughtful set of statements they've made about how they want to

243
00:21:31,360 --> 00:21:37,280
ultimately be much more cautious as we get closer to this kind of godlike AI. So I think we have

244
00:21:37,280 --> 00:21:41,280
actually leadership that is trying hard to do this. It just doesn't really work within the

245
00:21:41,280 --> 00:21:47,920
current economic system. And so for example, I, you know, I fought an antitrust case against

246
00:21:47,920 --> 00:21:53,680
Ticketmaster in the United States as part of the startup that I built, Songkick. And so, you know,

247
00:21:53,680 --> 00:22:02,560
I'm very supportive of the kind of, you know, the work that Lena Khan, or the CMA have been doing to

248
00:22:02,560 --> 00:22:11,280
try to sort of decrease concentration in certain markets. But I think in this case, actually

249
00:22:11,280 --> 00:22:15,680
sort of antitrust is actually quite harmful because it almost creates, it makes it harder

250
00:22:15,680 --> 00:22:19,760
to coordinate. There's less of a safe harbor. And so I think the main thing we need to do is really

251
00:22:19,760 --> 00:22:24,240
view these as quite different regimes. There's this kind of narrow AI regime, and there's this

252
00:22:24,240 --> 00:22:29,520
trying to build a god regime. And that bit needs a different regulatory approach to that bit.

253
00:22:30,400 --> 00:22:34,880
There's a quote from Marx. It's in the Communist Manifesto. I actually, I was reading this the

254
00:22:34,880 --> 00:22:39,840
other day. That's why I come on this channel, just to hear about Marx. This is, no, this is

255
00:22:40,480 --> 00:22:45,440
Ian, this is terrifying. Now bear in mind, he wrote this 170 years ago. Marx or our Capital Society

256
00:22:45,440 --> 00:22:50,000
had quote, conjured up such gigantic means of production in exchange that it was akin to a

257
00:22:50,000 --> 00:22:55,440
sorcerer, quote, no longer able to control the powers of the Netherworld, whom he has called

258
00:22:55,440 --> 00:23:02,640
up by his spells. I mean, wow, that sounds like capitalist competition, creating something completely

259
00:23:02,640 --> 00:23:08,800
beyond motivation and intentionality, and over which has very little oversight. Now, of course,

260
00:23:08,880 --> 00:23:17,280
he's talking about, you know, steam power and mills in Manchester and Brussels and

261
00:23:17,280 --> 00:23:22,000
Frankfurt in the mid 19th century. But if anything, those words sound more appropriate

262
00:23:22,000 --> 00:23:28,560
for AGI in the 21st century. Yeah, I think that one way to sort of frame the capital and kind of

263
00:23:28,560 --> 00:23:38,240
labor, the sort of relative power of those two groups is just looking at the size of some of

264
00:23:38,240 --> 00:23:44,960
these organizations. So an open AI is a, I think, privately valued at 30 billion US dollars now,

265
00:23:46,000 --> 00:23:49,680
you know, significantly changing the world, hundreds of millions of people now using their

266
00:23:49,680 --> 00:23:54,800
products. And I think, you know, at the time they released chat, it's probably a few hundred people

267
00:23:54,800 --> 00:24:00,880
in terms of the size of the organization and the labor that's directly, you know, benefiting from

268
00:24:00,880 --> 00:24:06,560
kind of the work that's being done there. And I think that again, you know, the leaders of these

269
00:24:06,560 --> 00:24:12,320
companies are actually thinking hard about this. So Sam Altman, you know, a couple of things I

270
00:24:12,320 --> 00:24:18,240
admire that he's done, the first is he was running a very large UBI study in Oakland. And so he was,

271
00:24:18,240 --> 00:24:22,320
you know, that was, you know, maybe five, 10 years ago, he was thinking hard about this question of

272
00:24:22,320 --> 00:24:27,760
how do you kind of, how do you, if you do have further and further returns to capital, what do

273
00:24:27,760 --> 00:24:33,840
you do about kind of that not, that not just massively in increasing inequality. And, you know,

274
00:24:33,840 --> 00:24:37,280
he's done this thing called Worldcoin, which is kind of a much more extreme version of that,

275
00:24:37,280 --> 00:24:42,080
which is a machine that scans your retina, produces a unique ID for you that would then

276
00:24:42,080 --> 00:24:46,320
let you be part of a global UBI scheme. And so, wow. And so there are, you know, they,

277
00:24:46,320 --> 00:24:51,360
these kind of people are thinking about these, these kind of the way in which this may fundamentally

278
00:24:51,360 --> 00:24:56,960
disrupt some of the ways in that the social contract we currently have that allows capitalism to

279
00:24:56,960 --> 00:25:02,000
sort of just continue as it does. But I suppose the concern is you can't be worried, you can't

280
00:25:02,000 --> 00:25:07,680
be dependent rather on prevalence and the foresight of certain individuals. You know,

281
00:25:07,680 --> 00:25:11,920
there was a great quote a couple of years ago from Mark Cuban. And he was saying,

282
00:25:11,920 --> 00:25:15,520
I wouldn't teach my kid to be an accountant. We now know that it was probably quite a good move,

283
00:25:15,520 --> 00:25:19,600
because, you know, that's one of the industries, which is very much prone to automation

284
00:25:19,600 --> 00:25:24,080
with machine learning. I'd rather they learn philosophy because it will give them insights

285
00:25:24,080 --> 00:25:29,360
that are harder to automate, so to speak. And I thought that was interesting. Now, alongside

286
00:25:29,360 --> 00:25:33,680
that, he said the world's first trillionaire will be the person who can master widespread

287
00:25:33,680 --> 00:25:38,480
commercial applications of AI. And that's the prize on offer, isn't it? I mean, that's the prize

288
00:25:38,480 --> 00:25:42,000
on offer. So today we talk about Amazon, which is a trillion dollar company, what used to be,

289
00:25:42,960 --> 00:25:49,440
borderline trillion dollar company, Amazon, Microsoft, those kinds of big players. But the

290
00:25:49,440 --> 00:25:55,920
truth is the commercial entity, which masters an AGI, and we don't all die, will put all those guys

291
00:25:55,920 --> 00:26:01,280
in the dust, won't they? So there are massive incentives for people to pursue this technology

292
00:26:01,280 --> 00:26:05,760
without the kinds of caution and intelligence and thoughtfulness that you've talked about with

293
00:26:05,760 --> 00:26:12,160
regards to somebody like Sam Altman. Yeah. And I think what's challenging in some ways,

294
00:26:12,160 --> 00:26:17,120
we've entered a new phase of this race. So if you look at the leaders of these organizations,

295
00:26:17,120 --> 00:26:22,560
the ones who are kind of really at the forefront of the race, whether it's Demis or Sam or Dario

296
00:26:22,640 --> 00:26:28,000
Anthropic, I would say that most of them are kind of not particularly motivated by money at this

297
00:26:28,000 --> 00:26:32,160
point. They're doing this for some other reason. You know, Sam, you know, recently announced that

298
00:26:32,160 --> 00:26:36,720
he actually has no equity in open AI, so doesn't stand to benefit economically from, you know,

299
00:26:36,720 --> 00:26:43,200
he won't be the first world's first trillionaire, let's put it that way. And so I think that they've

300
00:26:43,200 --> 00:26:48,000
been motivated by other things. And I would say mostly they've been sort of motivated by being

301
00:26:48,000 --> 00:26:52,960
the people to do it, to make this thing come alive and, you know, the consequences of that.

302
00:26:53,680 --> 00:26:58,000
It's kind of a world historic transition that they want to be a big part of.

303
00:26:59,440 --> 00:27:05,520
It's my guess, my hypothesis. But there are now a lot of other people who've kind of suddenly

304
00:27:05,520 --> 00:27:11,040
woken up and just seen dollar bills. And those people are just piling on money. They haven't

305
00:27:11,040 --> 00:27:15,600
really thought about it. They don't have the same sort of reverence that people like Demis

306
00:27:15,600 --> 00:27:19,760
have got for how we should be approaching this moment. And they're actually accelerating the

307
00:27:19,760 --> 00:27:25,840
race, but without that same, you know, intrinsic motivation for the thing they're trying to do

308
00:27:25,840 --> 00:27:29,600
and much more of a kind of, you know, much more of a desire just to make money.

309
00:27:29,600 --> 00:27:33,120
So for people out there who are perhaps skeptical of what I'm saying here about a trillionaire,

310
00:27:33,840 --> 00:27:38,720
if you told a 14, 15 year old Ian or a 14, 15 year old Aaron about the same age,

311
00:27:38,720 --> 00:27:43,840
that one day there'll be somebody worth 250 billion US dollars, 200 billion US dollars,

312
00:27:43,840 --> 00:27:49,040
like Jeff Bezos, we would have thought that's outlandish. Yet he's the guy who starts Amazon,

313
00:27:49,040 --> 00:27:53,920
and that's the company which benefits from network effects, you know, ubiquitous mobile

314
00:27:53,920 --> 00:28:00,720
internet, and basically building the everything store of e-commerce. And I suppose the question is,

315
00:28:00,720 --> 00:28:08,320
could you feasibly see a company like Amazon, which is applying AI to a bunch of industries,

316
00:28:08,320 --> 00:28:13,120
which lay off hundreds of thousands of people, just like Amazon have basically shut down hundreds

317
00:28:13,120 --> 00:28:18,560
of thousands of local businesses? And that's the outcome we get. Do you think that's a plausible

318
00:28:18,560 --> 00:28:25,280
outcome? I think it is possible. And I think the reason for it is that you are, you know,

319
00:28:25,280 --> 00:28:29,360
you first of all have a much more globalized economy where things can spread really quickly,

320
00:28:29,360 --> 00:28:34,320
across borders in a way they couldn't, especially digital products. Secondly, you have,

321
00:28:35,200 --> 00:28:41,200
you know, a lot of these technological, you know, products built on top of prior networks.

322
00:28:41,200 --> 00:28:47,120
So for example, chatGPT is the fastest-growing product ever on the internet, but that's partly

323
00:28:47,120 --> 00:28:50,880
because we've got Twitter and Facebook and Instagram and Google, and all the ways that

324
00:28:50,880 --> 00:28:56,560
information disseminates and spreads faster than it did before. And finally, I think we're starting

325
00:28:56,560 --> 00:29:01,760
to tackle some of these incredibly large markets, like, you know, what SpaceX is doing is basically

326
00:29:01,760 --> 00:29:06,480
tackling, you know, global internet provision through Starlink, right? And so this is a very big

327
00:29:06,480 --> 00:29:10,640
thing. It's not like digging up the street and smashing in loads of fiber. It's something where,

328
00:29:10,640 --> 00:29:15,360
you know, the same satellites can basically provide internet access wherever you are in the world

329
00:29:15,360 --> 00:29:22,320
without that same degree of physical disruption. And so I do think it's possible. I think a good

330
00:29:22,320 --> 00:29:27,760
thought experiment for the kind of company that might be an example of this is the first company

331
00:29:27,760 --> 00:29:34,080
to really build incredibly good domestic robots, right? So, you know, I've got a four-year-old,

332
00:29:34,080 --> 00:29:37,520
when I'm doing chores around the house, I would sort of say to him, you know, one day,

333
00:29:37,600 --> 00:29:41,440
you know, one day, maybe there's going to be a robot doing this because, you know, we didn't

334
00:29:41,440 --> 00:29:44,400
used to have dishwashers. We didn't used to have washing machines. We didn't used to have

335
00:29:44,400 --> 00:29:47,920
tumble dryers. And now we have all those things. We take them for granted. They're in most homes

336
00:29:47,920 --> 00:29:52,160
in the world. What would it be like if we had a robot in our house that just did all the chores

337
00:29:52,160 --> 00:29:56,800
that we currently do as kind of around everything else we're doing in our lives, but also that

338
00:29:56,800 --> 00:30:01,840
robot could be your plumber when things break, your electrician, your handy person, go to shops

339
00:30:01,840 --> 00:30:06,720
for you. So when you think about kind of that sort of disruption, could it be a, you know,

340
00:30:06,720 --> 00:30:10,320
10 trillion dollar company where the person founding it is a trillion error, like quite

341
00:30:10,320 --> 00:30:17,120
possibly in my view? A quick sort of move away from machine learning software to the kind of

342
00:30:17,120 --> 00:30:20,000
robot that you're talking about. How far away do you think that is, by the way, as a sort of

343
00:30:20,000 --> 00:30:25,840
household appliance that, you know, middle-class people in the UK or US would be able to buy?

344
00:30:25,840 --> 00:30:30,240
Well, so I think it really is fundamentally the same curve we're talking about with AI. So robots

345
00:30:30,240 --> 00:30:34,240
basically are machine learning. They're just embodied machine learning where it has a physical

346
00:30:34,240 --> 00:30:39,360
presence it's using as well. And we've made, and I've invested in lots of robotics businesses

347
00:30:39,360 --> 00:30:43,360
over the years, and it's amazing how rapidly they're progressing. We don't actually see the

348
00:30:43,360 --> 00:30:47,840
number of robots that we're using because they're mostly industrial settings. They're like industrial

349
00:30:47,840 --> 00:30:53,840
cleaning robots or industrial manufacturing robots or agricultural robots. And so it's happening.

350
00:30:53,840 --> 00:30:59,360
And I think that we will start in the next few years to see robots in much more consumer settings,

351
00:30:59,360 --> 00:31:05,440
really starting with the rollout of self-driving cars. And so I think we will see humanoid robots

352
00:31:05,440 --> 00:31:11,120
that significantly enhance our lives in a domestic setting within the next decade.

353
00:31:11,680 --> 00:31:15,920
The next decade? And a lot of that will be knock-on effects from the work we're doing in making

354
00:31:15,920 --> 00:31:21,280
these powerful AI systems. So to bring this back to large language models and the sort of things

355
00:31:21,280 --> 00:31:25,920
that DeepMind and OpenAI are working on, there was a paper called the tool former paper that came

356
00:31:26,000 --> 00:31:31,280
out earlier this year. And it essentially shows that large language models are actually quite good

357
00:31:31,280 --> 00:31:36,640
at using tools, which is quite counterintuitive. But to break it down, you take an incredibly large

358
00:31:36,640 --> 00:31:42,480
computer, you feed it an enormous amount of text and imagery, and basically get it to get really

359
00:31:42,480 --> 00:31:49,440
good at predicting what it's going to see. If you take that same sort of blob of intelligence that's

360
00:31:49,440 --> 00:31:54,880
just become smart in some important way, and you give it access to a tool, it is actually quite

361
00:31:54,880 --> 00:32:02,160
good at using a tool, whether it's a digital tool or a physical tool. And so large language models

362
00:32:02,160 --> 00:32:06,480
actually will have a knock-on effect on real-world robotics. They're not sort of separate industry

363
00:32:06,480 --> 00:32:13,760
as a tool. They're very, very related. I like it. And how far are we from machines basically

364
00:32:13,760 --> 00:32:18,240
being able to do anything that a human does? So in my mind, I think sort of 25 to 30 years

365
00:32:18,800 --> 00:32:22,400
where we have software hardware, which can basically do 95% of the jobs that humans do,

366
00:32:23,120 --> 00:32:28,400
at the moment, the big obstacle to that obviously is the fine motor coordination dexterity that

367
00:32:28,400 --> 00:32:33,680
things like cleaning or construction relies upon. So it's easier to automate legal services

368
00:32:33,680 --> 00:32:39,200
or accountancy than it is, like you say, plumbing. Great example. How far are we from being able to

369
00:32:39,200 --> 00:32:45,280
solve the problem of the fine motor coordination? So I guess we don't really know. That kind of

370
00:32:45,280 --> 00:32:49,520
goes back to this question of how far away are we from AGI? How far away are we from

371
00:32:49,600 --> 00:32:53,920
sort of superintelligent systems? No one knows. All I will say is people have more,

372
00:32:54,480 --> 00:32:58,320
they put more probability on it happening soon rather than later. So Jeffrey Hinton,

373
00:32:58,320 --> 00:33:03,360
who resigned from Google recently, he said, I used to think this was paraphrasing here,

374
00:33:03,360 --> 00:33:07,680
but he said something like, I used to think this was decades away. And now I think it's not inconceivable

375
00:33:07,680 --> 00:33:14,720
it happens in the next five years. And he's the godfather of machine learning, the researcher

376
00:33:14,800 --> 00:33:20,480
that really kicked off this whole field of large neural networks and really the sort of one of

377
00:33:20,480 --> 00:33:27,200
the most important people in the field over the last few decades. And so we don't know,

378
00:33:27,200 --> 00:33:31,280
but I think we now need to sort of start to prepare as if we might be closer than

379
00:33:31,280 --> 00:33:34,080
people have realized. And the public certainly has realized.

380
00:33:34,080 --> 00:33:37,120
I mean, this is a big challenge to our economic system, right? So I mean, I wrote a book about

381
00:33:37,120 --> 00:33:44,320
this called Fully Automated Luxury Communism. And the point is if you get increasingly affordable

382
00:33:44,400 --> 00:33:49,200
technology, which can do anything that a human can do, whether it's with regards to abstract

383
00:33:49,200 --> 00:33:53,520
problem solving or physical capabilities like building a house or cleaning a house or cooking

384
00:33:53,520 --> 00:33:58,800
meal, what that does is clearly depress the price of labor to zero. That's what's going to happen.

385
00:33:58,800 --> 00:34:04,240
That's a neoclassical understanding of how of how supply and demand would work with the cost of

386
00:34:04,240 --> 00:34:10,640
labor of human labor. And yet, if you say that to a lot of people, including politicians, including

387
00:34:10,720 --> 00:34:16,480
quote unquote, smart people in legacy media, in the policy world, they think you're crazy.

388
00:34:16,480 --> 00:34:22,080
And it's interesting for me, seeing the reception of my book, F.T., oh, this is interesting,

389
00:34:22,080 --> 00:34:27,920
very provocative, New York Times and so on. But then in some other places, I won't name the papers,

390
00:34:27,920 --> 00:34:31,680
but they were just like, this is ridiculous. They were sort of mocking it. Where do you think

391
00:34:31,680 --> 00:34:35,520
this asymmetry comes from where some people take these ideas very seriously and then

392
00:34:35,520 --> 00:34:39,600
others just completely disregard them? Is it a lack of information? Do you think it comes from a

393
00:34:39,600 --> 00:34:45,600
place of fear? Because realistically, if this is correct, we're going to have to shake things really

394
00:34:45,600 --> 00:34:52,880
up a lot, aren't we? I think it's a great question. I guess I've got two hypotheses. The first goes

395
00:34:52,880 --> 00:34:57,440
back to the thing we were talking about with exponential change. It's just so hard to think

396
00:34:57,440 --> 00:35:05,760
intuitively about exponential progress. And so COVID is sort of fresh in our minds. And I remember

397
00:35:06,320 --> 00:35:13,120
sort of January, I was planning a trip to China with a friend. And actually, our trip took us

398
00:35:13,120 --> 00:35:20,800
through Wuhan. And he's an expert in kind of biology in various ways. And I spent a bunch of

399
00:35:20,800 --> 00:35:24,720
time in China when I was younger studying Mandarin. And so we were talking about this trip and we're

400
00:35:24,720 --> 00:35:28,960
talking about COVID. And we sort of started to see the little inklings that COVID was going to

401
00:35:28,960 --> 00:35:35,680
potentially go pandemic. And I remember both of us were starting to prepare and

402
00:35:35,680 --> 00:35:40,000
get really worried ahead of a lot of the sort of mainstream news. But at the same time,

403
00:35:40,800 --> 00:35:44,400
we're both like, well, maybe it'll be done by the time that our trip comes around, right?

404
00:35:44,400 --> 00:35:47,600
And it's just a perfect example of like, we thought we were being clever, but actually we

405
00:35:47,600 --> 00:35:52,560
seriously still did not really have a good intuition for actually what happens when exponentials

406
00:35:52,560 --> 00:35:58,160
really take off. And I think that there's a bit of that where if you're a politician that's just

407
00:35:58,160 --> 00:36:03,520
dealing with, you know, you know, day to day issues, and you're then confronted with an exponential

408
00:36:03,520 --> 00:36:12,240
change, be it, you know, what we're doing with climate change, or a pandemic, or an accelerating

409
00:36:12,240 --> 00:36:18,160
technology like AI, you just don't really have an intuitive way to navigate that. So I think

410
00:36:18,160 --> 00:36:24,080
that's part of it. I think the other part of it is just exponentials require, I think, a certain

411
00:36:24,080 --> 00:36:29,120
kind of radical thinking, you know, it's sort of like what the I think the UK government did really

412
00:36:29,120 --> 00:36:33,760
brilliantly with Cape Bingham and the Vaccine Task Force. I think it was a great example of kind of

413
00:36:33,760 --> 00:36:37,680
just incredibly bold, proactive leadership on a serious thing, getting, let's get ahead of it,

414
00:36:37,680 --> 00:36:42,560
let's get, let's, let's build that capacity early, let's, let's throw everything at it and prepare

415
00:36:42,560 --> 00:36:46,400
as best as possible, because we know this exponential is coming. And that kind of

416
00:36:47,120 --> 00:36:51,760
political action and leadership is just, I think, like quite hard to do. And I think

417
00:36:52,480 --> 00:36:56,240
Why is it, why is it hard to do? Is it because politicians don't like doing it or because

418
00:36:56,320 --> 00:36:57,920
it's just objectively hard to execute?

419
00:36:57,920 --> 00:37:04,160
I think it's radical, basically. And I think it's a, it's not business as usual, you know,

420
00:37:04,160 --> 00:37:09,280
it's first principles thinking you maybe have to take more risk. And they just haven't been that

421
00:37:09,280 --> 00:37:16,560
many examples of political leaders who've kind of acted like that over the last couple of decades

422
00:37:16,560 --> 00:37:23,680
in response to technological change. And I remember in 2018, I wrote this essay AI nationalism that

423
00:37:23,680 --> 00:37:28,160
really talks about this kind of this fundamental challenge that was coming down the pipe with AI.

424
00:37:28,160 --> 00:37:32,080
And I remember meeting with, with kind of MPs from labor MPs from, you know,

425
00:37:32,080 --> 00:37:35,840
conservatives laying out these ideas, almost everyone just looked at me like I was completely

426
00:37:35,840 --> 00:37:40,320
crazy. Because I was saying things like next time we have a company like DeepMind, we shouldn't

427
00:37:40,320 --> 00:37:45,760
let it be acquired because it's too critical to the future of the UK. And then, you know,

428
00:37:45,760 --> 00:37:50,400
three or four years later, I saw similar politicians basically saying, oh yeah,

429
00:37:50,400 --> 00:37:54,320
now we have to be serious about blocking acquisitions of some, you know, strategically

430
00:37:54,320 --> 00:37:59,840
important UK technology companies. And so I think it's just also discomfort with really

431
00:37:59,840 --> 00:38:05,120
radical ideas. I think there's much more of a comfort in politics with incrementalism. And

432
00:38:05,120 --> 00:38:07,680
that's why often big change happens when you have a crisis.

433
00:38:08,880 --> 00:38:12,160
We'll come back to the politics in a moment. I want to ask you, I want to ask you an almost

434
00:38:12,160 --> 00:38:17,520
existential question. A super intelligent, artificial general intelligence, would it be

435
00:38:17,520 --> 00:38:23,680
conscious? I don't know. I think that there's, I was actually talking about this with

436
00:38:27,600 --> 00:38:32,400
someone who specialised in quantum computing yesterday. And their view is actually that

437
00:38:32,400 --> 00:38:38,880
like conscious consciousness is a kind of quantum mechanical property, a property of quantum

438
00:38:38,880 --> 00:38:44,240
physics. And therefore, it will not be possible to get consciousness on a classical computer.

439
00:38:45,040 --> 00:38:48,880
I don't know if that's true or not. But like their claim was basically, we all sort of need

440
00:38:48,880 --> 00:38:54,640
quantum computers to actually have conscious AIs. I don't know.

441
00:38:54,640 --> 00:38:57,840
What's the argument behind that, by the way? So where does that come from?

442
00:38:57,840 --> 00:39:04,880
It comes from quite a, I guess, an esoteric argument about kind of about exactly how we

443
00:39:04,880 --> 00:39:10,480
interrelate with the metaverse, which I'm not enough of an expert on any of those topics to

444
00:39:10,480 --> 00:39:18,720
really opine. But the way I think about it is, is consciousness and intelligence are probably

445
00:39:18,720 --> 00:39:24,160
kind of somewhat orthogonal. You don't necessarily get, you know, get one with the other. And I don't,

446
00:39:24,160 --> 00:39:29,920
I think it may have been a, the way it may be an artifact of biological life, which is inherently

447
00:39:29,920 --> 00:39:36,000
quantum, rather than necessarily sort of classical computing. So we still could fabricate something

448
00:39:36,000 --> 00:39:38,400
close to consciousness, but it would just require quantum.

449
00:39:38,400 --> 00:39:42,080
Maybe, or maybe it's not possible. We just don't, we, consciousness is something,

450
00:39:42,080 --> 00:39:45,920
as far as I can tell, we don't really understand at all. So we, I think it'd be quite difficult to

451
00:39:45,920 --> 00:39:51,200
project how to think about consciousness when it comes to machines. Like, if I'm understanding it

452
00:39:51,200 --> 00:40:00,080
correctly, when you, when you get, you know, when you have an operation, you go under anesthetic,

453
00:40:00,080 --> 00:40:04,080
you know, the drugs you're being given, essentially just switch off your consciousness.

454
00:40:04,080 --> 00:40:07,440
And we don't really understand like why and how that works. And so it's quite hard

455
00:40:07,440 --> 00:40:10,960
before we really understand consciousness to really make any claims about how

456
00:40:10,960 --> 00:40:15,600
AI is will be conscious. I think that the, but I think we should probably start from the basis

457
00:40:15,600 --> 00:40:19,440
that they might not be conscious, but they might still be incredibly intelligent and capable

458
00:40:19,440 --> 00:40:22,560
of planning in ways that are threatening to humanity.

459
00:40:23,120 --> 00:40:26,960
So we could have an AGI, which is an existential threat to humanity without it being sentient.

460
00:40:26,960 --> 00:40:29,040
Yeah. Yeah. Exactly.

461
00:40:29,840 --> 00:40:33,920
Interesting. And it could be trying to maximize its own utility. It could be trying

462
00:40:33,920 --> 00:40:38,640
to maximize its own interest without really being aware of itself as an entity.

463
00:40:38,640 --> 00:40:43,440
Exactly. Yeah. Exactly. I'm thinking about it kind of like the Sorcerer's Apprentice is probably

464
00:40:43,440 --> 00:40:47,680
the kind of like a really mainstream example of kind of something where you give it some sort of

465
00:40:47,680 --> 00:40:52,480
goal and the optimization of that goal ultimately leads to something that is not what you wanted.

466
00:40:53,200 --> 00:40:56,560
And so that's what the field of alignment is about. It's basically about how do you align

467
00:40:56,560 --> 00:41:00,480
those goals with our goals as a species? What does it mean to the average person? So we're

468
00:41:00,480 --> 00:41:04,640
talking about AI, you know, partly abstract, partly real world. So we're talking about, you

469
00:41:04,640 --> 00:41:08,400
know, the potential of trillionaires. We're talking about consciousness. We're talking

470
00:41:08,400 --> 00:41:13,600
about politicians asleep at the wheel. But what does it mean to somebody out there who's watching

471
00:41:14,240 --> 00:41:20,800
median income, homeowner, mortgage, owner-occupier, if there is an artificial general intelligence

472
00:41:20,800 --> 00:41:25,360
that's created to send the next five to 10 years, how will that impact their lives over the next,

473
00:41:25,360 --> 00:41:33,840
say, 25 to 30 years? So again, with exponentials, saying anything about, you know, 20 years is really

474
00:41:33,840 --> 00:41:38,640
hard. Yeah, let's say it's developed. That's the exponential part. And then let's say there's

475
00:41:38,640 --> 00:41:42,960
almost like a cap on AGI. Like it's not really developed after that for regulatory reasons or

476
00:41:42,960 --> 00:41:48,960
whatever, political reasons. I'm trying to really ask what are the implications for just everyday

477
00:41:48,960 --> 00:41:52,720
people rather than us talking about, you know, like you say, some breakthrough which depends

478
00:41:52,720 --> 00:41:56,560
the printing press, the average person in 16th century Europe doesn't really care about the

479
00:41:56,560 --> 00:42:01,200
printing press, but then the reformation happens and that is a big deal. So I think that like,

480
00:42:02,560 --> 00:42:08,000
I think that synthetic media is a really, really big deal. And I think there is

481
00:42:08,800 --> 00:42:14,320
extremely low popular support for it. So, you know, as an example, you know, you can take the

482
00:42:14,320 --> 00:42:20,240
sort of generative AI systems, the systems that are creative, capable of generating images or text,

483
00:42:20,320 --> 00:42:24,640
and you can use them for harmful purposes. So I'll give you a few examples. The first is,

484
00:42:25,200 --> 00:42:31,600
you can take a snippet of someone speaking, and you can then, you know, effectively synthetically

485
00:42:31,600 --> 00:42:36,960
clone their voice. And that is now being used by people to do kind of fake kidnappings where you

486
00:42:36,960 --> 00:42:41,520
get a snippet of a child's voice, and you basically create a phone call from them calling their parents

487
00:42:41,520 --> 00:42:46,320
saying, mom, I've been kidnapped. If you don't do this, then I'm going to, you know, something

488
00:42:46,320 --> 00:42:53,760
horrible is going to be done to me. And that happened in, I think, Texas about two months ago.

489
00:42:53,760 --> 00:43:02,320
And so that's kind of happening. Another thing that's happening is around synthetic child sexual

490
00:43:02,320 --> 00:43:06,800
abuse material. And so the systems you can use to create a kind of funny image of the pope in

491
00:43:06,800 --> 00:43:12,080
a puffer jacket, you can also use to create, you know, horrible deep faked porn, including

492
00:43:12,080 --> 00:43:16,880
horrible deep faked child sexual abuse material. And that is currently happening. People are using

493
00:43:16,880 --> 00:43:22,800
these systems to do that, particularly the kind of open source ones. And the knock on effect is

494
00:43:22,800 --> 00:43:26,560
then you've got to, you know, the police are suddenly faced with, you know, is this an image

495
00:43:26,560 --> 00:43:31,040
of a child that's actually being harmed or a fake image and having to full of shard their resources

496
00:43:31,040 --> 00:43:36,480
between kind of basically fake crimes and real crimes. And so there's a huge number of malicious

497
00:43:36,480 --> 00:43:41,040
uses that will bubble up from this very powerful technology where you can clone someone's likeness,

498
00:43:41,040 --> 00:43:47,440
someone's image, someone's voice. So I think of that as being like the real structural problem

499
00:43:47,440 --> 00:43:51,040
that we're going to encounter. And I think that they're, you know, I was talking to someone there

500
00:43:51,040 --> 00:43:55,840
who's been doing focus groups around this. And they were just telling me like, you know, basically

501
00:43:55,840 --> 00:44:00,320
the public at large thinks deep fakes should be made illegal already. So there's a kind of, I think,

502
00:44:01,120 --> 00:44:04,720
a lot of stuff bubbling under the surface that when it breaks through is going to really,

503
00:44:05,600 --> 00:44:08,800
it's going to really have actually quite a populist response to it, because it's just,

504
00:44:09,360 --> 00:44:12,000
it seems like something the government should be getting a grip of.

505
00:44:13,040 --> 00:44:16,560
And do you think they will? Is there any sort of, you're in this game? Is there any evidence

506
00:44:16,560 --> 00:44:20,560
that they will? I think they will actually. Yeah. I think it's just, it passes a kind of

507
00:44:20,560 --> 00:44:24,960
common sense test of that that is not a good thing that should be happening. So I think there'll be a

508
00:44:24,960 --> 00:44:30,720
genuine motivation to make deep fakes, deep deep fakes without someone's permission illegal.

509
00:44:31,360 --> 00:44:35,200
And I think lots of politicians will want to do that. But I think the mechanism for doing that,

510
00:44:35,200 --> 00:44:39,040
when you've got significant proliferation of these capabilities into open source is a bit

511
00:44:39,040 --> 00:44:43,040
trickier, right? Because the technology is kind of out of the box. And so figuring out how to get

512
00:44:43,040 --> 00:44:50,240
it back in, like with the dark web, for example, is just challenging. But I think it will be,

513
00:44:50,240 --> 00:44:52,720
I think there'll be broad political support for doing that.

514
00:44:52,720 --> 00:44:56,000
You said the common sense thing, but I mean, if you said to somebody 40 years ago that children

515
00:44:56,000 --> 00:45:01,920
will be able to access high speed broadband pornography, be able to stream stuff, I mean,

516
00:45:01,920 --> 00:45:04,800
I think most people said that obviously should be banned. That's the world we live in.

517
00:45:06,080 --> 00:45:08,800
I think we've lived through a strange time, you know, I think that

518
00:45:10,800 --> 00:45:16,880
I think about social media a lot and how it's sort of remarkable that we basically left this

519
00:45:16,880 --> 00:45:23,680
enormous industry that was so transformational to, to everyday life, to children, to politics,

520
00:45:23,680 --> 00:45:28,000
to the general discourse, to basically self regulate, you know, and I think there was

521
00:45:28,000 --> 00:45:33,840
actually a really, that was well motivated by a desire not to kind of throttle something with

522
00:45:33,840 --> 00:45:39,280
regulation and kind of too much, you know, too early, you know, I think I can kind of,

523
00:45:39,280 --> 00:45:45,360
I can see the logic of things like section 320, I think it's called, which is kind of the sort of

524
00:45:46,160 --> 00:45:49,760
the mechanism whereby a lot of social media companies have not really

525
00:45:49,760 --> 00:45:55,600
have kind of been able to just self regulate. But it feels like we, I think with the benefit

526
00:45:55,600 --> 00:46:00,960
of hindsight, you know, it would have been better for regulators to catch up faster and to sort of

527
00:46:00,960 --> 00:46:06,320
be a bit more assertive about defining, you know, a smarter way forward. And I think Biden talked

528
00:46:06,320 --> 00:46:12,480
about, you know, for example, modifying section 320 recently, and there's been some discussion

529
00:46:12,480 --> 00:46:18,160
about this. But I think with AI, you know, I think that the lawmakers need to move faster,

530
00:46:18,160 --> 00:46:22,240
because back then, you know, these fledgling startups like YouTube, they had certain sorts of,

531
00:46:22,240 --> 00:46:28,080
you know, it's kind of a, you know, innovation was valuable and needed to be sort of encouraged.

532
00:46:28,080 --> 00:46:32,560
But this is a different ball game where you've got $20 billion already invested in just a handful

533
00:46:32,560 --> 00:46:37,200
of companies, you've got, you know, Microsoft aggressively deploying this as fast as they possibly

534
00:46:37,200 --> 00:46:42,800
can. You've got companies like Facebook, open sourcing incredibly powerful models and putting

535
00:46:42,800 --> 00:46:46,720
them out there for anyone to sort of expand the modify. And so I don't think I think we're kind

536
00:46:46,720 --> 00:46:52,880
of in like the same scenario. It's just now this is not about small startups and fledgling

537
00:46:52,880 --> 00:46:56,720
industries. This is about an incredibly powerful tech industry that has just

538
00:46:57,360 --> 00:47:02,240
prefers self-regulation to anything else. Do you think social media was a mistake? Because

539
00:47:02,240 --> 00:47:06,320
obviously you're involved in the sort of the technological side of things and how, you know,

540
00:47:06,320 --> 00:47:12,400
the technological sort of underpinning of global social media, 4G, 5G, mobile internet,

541
00:47:13,200 --> 00:47:18,640
high res screens, all these things. And one outgrowth that was social media and Ben Bratton

542
00:47:18,640 --> 00:47:22,720
has a really interesting read on this. So he says, we built a global real-time communications

543
00:47:23,520 --> 00:47:28,480
computational network, you know, including the kind of exosphere of our planet, which is now

544
00:47:28,480 --> 00:47:33,280
caked with satellites. We've built all of this infrastructure so that we can sell out space

545
00:47:33,280 --> 00:47:37,440
and be permanently distracted. You know, and I think that's an interesting way of looking at it.

546
00:47:37,520 --> 00:47:42,880
And I wonder, we might have something like AI or even an AGI if it's not, you know,

547
00:47:42,880 --> 00:47:45,840
deadly. And we just get more of the same, perhaps.

548
00:47:47,280 --> 00:47:53,680
So my day job is I'm an investor, and I invest through a fund called Plural. And, you know,

549
00:47:53,680 --> 00:48:00,240
it's a European fund focused on accelerating missions that we consider to be of great societal

550
00:48:00,240 --> 00:48:05,840
importance, you know, by funding them as startups. And one of the companies I work with is a company

551
00:48:05,840 --> 00:48:12,560
called Unitary AI. It's one I'm very proud to be kind of working on. It's a startup that uses AI

552
00:48:12,560 --> 00:48:20,000
to understand content and thereby to offer a scalable approach to content moderation

553
00:48:20,000 --> 00:48:25,360
and content safety across the internet, content security. So for example, their AI can detect

554
00:48:25,360 --> 00:48:28,880
some sort of content that should be illegal or some kind of content that is, you know,

555
00:48:28,880 --> 00:48:32,800
causing significant harms and flag it to the platform that's hosting it.

556
00:48:33,520 --> 00:48:39,120
And so they're in many ways kind of like a, you know, like a antibiotic to this kind of this,

557
00:48:39,120 --> 00:48:45,520
the way in which some of the sort of wild west of content dissemination through social media

558
00:48:45,520 --> 00:48:50,640
has kind of played out. So I think there is kind of a, there is a kind of capitalist response

559
00:48:50,640 --> 00:48:54,480
coming, but it's hard. It's much easier to make a new social network than it is to make an AI

560
00:48:54,480 --> 00:48:59,040
company that's trying to actually solve this problem of kind of how do you scalably tackle

561
00:48:59,040 --> 00:49:06,080
the challenges of content? To your question of kind of, you know, if you could kind of go back

562
00:49:06,080 --> 00:49:11,920
in time and stop social media from happening, I wouldn't have done that personally. I think

563
00:49:11,920 --> 00:49:18,400
it's delivered enormous benefits. I think there is some something amazing about so much connectivity,

564
00:49:19,120 --> 00:49:23,120
democratization of media production. I mean, you know, like I, part of the reason I'm sitting here

565
00:49:23,120 --> 00:49:26,080
is because I, you know, I learned about you through Twitter and I follow your thinking

566
00:49:26,080 --> 00:49:32,160
and I find it interesting. And so I think there is something, something really miraculous about how

567
00:49:32,720 --> 00:49:38,320
connected we are now. I just think the problem is we just never really had governments keep up.

568
00:49:38,960 --> 00:49:42,800
And I think it's something, I think it's the really the nature of like sort of

569
00:49:42,800 --> 00:49:48,160
laissez-faire, neoliberal thinking, having just permeated government over a number of decades

570
00:49:48,160 --> 00:49:53,520
where really bold, ambitious projects where the government is a kind of, you know, a real partner

571
00:49:53,520 --> 00:49:57,680
to the private sector and actually drives technological change and thinks about regulation

572
00:49:57,680 --> 00:50:01,440
in a bold way that embraces exponentials has just been missing.

573
00:50:01,440 --> 00:50:05,200
So you think there's neoliberalism, this, because we do live in this really strange moment, right,

574
00:50:05,200 --> 00:50:10,480
of like extraordinary technological possibility, like extraordinary profound liquid biopsies that

575
00:50:10,480 --> 00:50:17,120
can detect early onset cancer, you know, mapping the human genome, you know, just high res scans of

576
00:50:17,120 --> 00:50:22,800
the human brain, just, you know, year on year improving exponentially. And yet we have politicians

577
00:50:22,800 --> 00:50:27,680
who say, sorry, we can't address the housing crisis. Sorry, we can't give you affordable

578
00:50:27,680 --> 00:50:32,480
health care or free health care. There's a weird disconnect there. It seems almost like the better

579
00:50:32,480 --> 00:50:37,440
the technology gets, the more the possibilities, the opportunities, the less capable the state is

580
00:50:37,440 --> 00:50:40,720
in addressing those challenges. So you pin that on neoliberalism.

581
00:50:40,720 --> 00:50:44,480
Well, I think that particular problem you described, like, it's hard to really

582
00:50:45,200 --> 00:50:51,520
know where it started. There's a, there's a kind of an investor I really respect called Matt Clifford.

583
00:50:51,600 --> 00:50:56,800
And he has a kind of a thesis on this, which basically says, if you look throughout history,

584
00:50:56,800 --> 00:51:01,440
there was always a kind of part of part of society that attracted the most talent.

585
00:51:01,440 --> 00:51:05,280
And he's his argument is basically right now, that's the technology industry. And so a lot

586
00:51:05,280 --> 00:51:12,400
of talent has kind of gone out of government or out of, you know, the public sector into

587
00:51:12,400 --> 00:51:16,000
technology, because the opportunity to change things quickly, make money, you'd be the first

588
00:51:16,000 --> 00:51:20,720
trillionaire, whatever, whatever motivates you, right. And so there could be a hollowing out of

589
00:51:20,720 --> 00:51:24,800
sort of the capacity of the state to respond. That's one way of thinking about it. I actually,

590
00:51:24,800 --> 00:51:29,440
I think of it as being a little bit more ideological. So for me, I think we've just not

591
00:51:29,440 --> 00:51:34,400
really had political leadership that sort of sort of said, you know, I'm going to,

592
00:51:35,200 --> 00:51:41,040
we're going to transform this country in a way that really embraces all of this. And it's kind of,

593
00:51:41,040 --> 00:51:46,160
you know, keeps pace with technological change. And I think you've, you've had examples of that.

594
00:51:46,160 --> 00:51:50,080
You know, if you look at Lee Kuan Yew in Singapore, obviously, lots to discuss about

595
00:51:50,080 --> 00:51:54,720
Singapore and their politics, but it's very interesting the way that he basically kind of

596
00:51:54,720 --> 00:51:59,040
was a founder almost of a country that went from third world to first world in 30 years. And some

597
00:51:59,040 --> 00:52:03,760
of the things he did were just very, very bold, ambitious things that ultimately he took a lot

598
00:52:03,760 --> 00:52:07,760
of risks and it delivered for the country and for the citizens. And so I think we've lacked that

599
00:52:07,760 --> 00:52:12,640
level of boldness. And the reason I sort of cite neoliberalism and laissez-faire economics is because

600
00:52:12,640 --> 00:52:16,800
I remember when I was talking to people about that AI nationalism, I was amazing to me that I

601
00:52:16,800 --> 00:52:20,800
would be meeting with conservative politicians and they'd be telling me about British Leyland

602
00:52:20,800 --> 00:52:24,880
as the reason why you can't nationalize DeepMind. Crazy. And it's just like, what is going on? This

603
00:52:24,880 --> 00:52:30,640
is like, you know, it's like, you know, decades later and who really cares about some failed car

604
00:52:30,640 --> 00:52:37,040
company at this point? Also, you have Taiwan, which I think Taiwan produces like one sixth of

605
00:52:37,040 --> 00:52:42,160
global microprocessors, 90% of the ultra high end ones, you know, the ultra high end ones,

606
00:52:42,160 --> 00:52:45,520
which China can't create at the moment. And it's kind of, you know, that's the whole point of this

607
00:52:46,160 --> 00:52:50,880
set of sanctions and trade embargoes that the US put on them. That is entirely because of state

608
00:52:50,880 --> 00:52:54,160
led innovation by the Taiwanese government. It's a country of what, 25 million people?

609
00:52:55,040 --> 00:52:59,840
And in the world where Taiwan is making one sixth of the world microprocessors,

610
00:52:59,840 --> 00:53:03,680
we have British politicians saying, sorry, we can't do that because of this thing that happened in

611
00:53:03,680 --> 00:53:09,520
1975, whatever. Crazy. Yeah. And I think it's, you know, it's obviously there are some people

612
00:53:09,520 --> 00:53:14,880
really trying, but I think the system as a whole is very trapped in an old ideology that sort of

613
00:53:14,880 --> 00:53:19,040
doesn't just sort of wants to be quite hands off rather than hands on. And you know, I work with

614
00:53:19,040 --> 00:53:23,920
Marianna Mazucato at Institute at UCL. And I really, when I first came across her book,

615
00:53:23,920 --> 00:53:28,000
The Entrepreneurial State, it just blew me away because it really, if you've been a founder for

616
00:53:28,000 --> 00:53:32,560
most of your kind of working life like I have, it really described and but you really care about

617
00:53:32,560 --> 00:53:37,760
the future of the UK, the future of Europe. It really described a different mode of politics,

618
00:53:37,760 --> 00:53:43,520
which is this very entrepreneurial kind of founder approach, which says, right, like,

619
00:53:43,520 --> 00:53:47,680
the state is going to take a point of view, it's going to have a vision, it's going to pick missions

620
00:53:47,680 --> 00:53:52,000
that really matter, it's going to invest very ambitiously to make things happen that wouldn't

621
00:53:52,000 --> 00:53:55,840
happen otherwise. And I'll give you an example of something that like right now for me is very

622
00:53:55,840 --> 00:54:02,960
inspiring. So within nuclear fusion, you know, you've got a number of different concepts for

623
00:54:02,960 --> 00:54:08,640
how to put fusion on the grid. And we've made enormous progress in fusion over the last few

624
00:54:08,640 --> 00:54:12,720
decades. And we're now actually I think within touching sites of it happening. And the dominant

625
00:54:12,720 --> 00:54:19,440
approach that's really taken us closer to, you know, fusion on the grid is a technology called

626
00:54:19,440 --> 00:54:25,120
magnetic confinement fusion, you have very powerful magnets, the confine essentially a sun on earth,

627
00:54:25,120 --> 00:54:29,840
and you use that to basically produce a sun on earth and extract energy. And

628
00:54:30,640 --> 00:54:35,440
the dominant mode of doing magnetic confinement fusion is something called the tokamak. And the

629
00:54:35,440 --> 00:54:39,440
tokamak was a device that's, you know, there's probably $100 billion been invested in tokamaks

630
00:54:39,440 --> 00:54:45,280
globally over the last over the last 50 years. And the German government actually took a different

631
00:54:45,280 --> 00:54:50,320
point of view. And they said, there is this other device, which has a lot of attractive things about

632
00:54:50,320 --> 00:54:54,560
it, that actually, you know, are very hard to do, but we might now have enough computing power to do

633
00:54:55,280 --> 00:55:00,640
it. And they've been in quietly investing, you know, large amounts of money, but still quite

634
00:55:00,640 --> 00:55:06,000
small for fusion into making this happen. And over the last 20 or so years, they've taken this

635
00:55:06,000 --> 00:55:10,240
alternative fusion reactor design, all the way up to the point where we might actually be able to

636
00:55:10,240 --> 00:55:14,160
build a power plant, and it's called the stellarator. And the German government has done that kind of

637
00:55:14,160 --> 00:55:18,880
almost single handedly, the most advanced stellarator in the world is in North Germany. And it's

638
00:55:18,880 --> 00:55:22,640
light years ahead of any other stellarator. And that's the sort of thing that just gives

639
00:55:22,720 --> 00:55:26,960
me real goosebumps when I think about what the state can do really, when it really wants to

640
00:55:26,960 --> 00:55:31,440
shape markets, you know, if the UK said, right, we're going to take a point of view, fusion is

641
00:55:31,440 --> 00:55:37,440
going to happen in the next 20 years. We want to have this country running on fusion in a material

642
00:55:37,440 --> 00:55:40,960
way, we want to do a massive industry for this country, we're going to have this incredibly

643
00:55:40,960 --> 00:55:46,400
ambitious kind of investment mandate behind that. I think that's the sort of politics that excites

644
00:55:46,400 --> 00:55:52,720
me and makes me feel like we would have a politics that matches the exponential change

645
00:55:52,720 --> 00:55:55,440
rather than just kind of runs behind it, trying to play catch up.

646
00:55:55,440 --> 00:55:58,960
How popular is that kind of stuff in your world? So you mentioned Marianna Mazzicato.

647
00:55:59,600 --> 00:56:07,280
I suppose, you know, she proselytises a kind of social democracy with an interventionist state,

648
00:56:07,280 --> 00:56:10,080
like you say, with like an emphasis on entrepreneurialism as well. So it's quite

649
00:56:10,080 --> 00:56:16,480
a unique blend, although I really find it's a 21st century variant of 20th century social democracy

650
00:56:16,480 --> 00:56:23,120
kind of updated for the network society. Are those ideas more popular than one might imagine

651
00:56:23,120 --> 00:56:28,240
amongst these kinds of circles? Because tech people are generally open to new ideas or

652
00:56:28,960 --> 00:56:32,640
are you something of an outlier in terms of enjoying Marianna's work?

653
00:56:33,440 --> 00:56:40,720
I don't really know of an outlier amongst founders. I think that founders are quite specific people

654
00:56:40,720 --> 00:56:45,280
and that they're really, they're drawn towards taking risk. They often want to try to make the

655
00:56:45,280 --> 00:56:50,720
world better by building some product that they think the world needs. And I think a lot of them

656
00:56:50,720 --> 00:56:54,960
are quite radical in their bones, quite risk taking. And I think they're drawn to, they're drawn to

657
00:56:54,960 --> 00:56:59,280
interesting ways of thinking about politics often. I would say investors, and you know,

658
00:56:59,360 --> 00:57:07,600
I'm kind of technically my day job as investor, tend to be a lot more focused on turning money

659
00:57:07,600 --> 00:57:14,480
into money than actually really directing markets. So a little while back, you mentioned quantum

660
00:57:14,480 --> 00:57:19,680
computing and the multiverse. And I know that's not the topic of this conversation. And I'm sure

661
00:57:19,680 --> 00:57:22,240
you have a great deal of knowledge about it, but you'll be very humble and say I'm not the

662
00:57:22,240 --> 00:57:25,680
right person to speak to. But this is really intriguing. And I think your audience would

663
00:57:25,680 --> 00:57:30,560
love to hear more about it. What's this relationship between quantum computing and the,

664
00:57:30,560 --> 00:57:37,200
the inference that it perhaps gives us a glimpse at the possibility of a multiverse, multiple

665
00:57:37,200 --> 00:57:43,360
universes? I think I probably will try to stick to what I know something about within quantum

666
00:57:43,360 --> 00:57:50,080
computing. So I chair a company called phasecraft, which is a company founded by a number of

667
00:57:50,800 --> 00:57:56,800
UK professors. One of the professors that founded it is the sort of co-chair of the most important

668
00:57:56,800 --> 00:58:00,560
conference this year in quantum computing. So they are some of the kind of global leaders in

669
00:58:00,560 --> 00:58:07,040
this field. And what they're doing is developing software to run on quantum computers. And so

670
00:58:07,040 --> 00:58:11,840
as a result of sort of supporting those founders over the last kind of three and a half years,

671
00:58:13,760 --> 00:58:16,960
I've kind of had a bit of a window into what's happening in quantum computing. And it really

672
00:58:17,600 --> 00:58:23,360
is quite amazing. So you've got these machines, probably the two kind of most impressive machines

673
00:58:23,360 --> 00:58:30,000
in the world. One is built by Google in, down in LA. And the other bit is built in China.

674
00:58:30,000 --> 00:58:36,160
And these machines take sort of, they use superconducting materials, trapped in these kind

675
00:58:36,160 --> 00:58:45,840
of crazy cages to basically run computing operations that really allow for a much wider range of

676
00:58:45,840 --> 00:58:51,760
possibilities than digital computers, which are more sort of deterministic. And so

677
00:58:52,400 --> 00:58:56,720
and this is because a digital computer has a zero one binary system and quantum isn't

678
00:58:56,720 --> 00:59:01,120
constrained by that. Well, yeah, sort of allow it kind of it remains in superposition. So it's

679
00:59:01,120 --> 00:59:05,920
neither one or zero until finally you kind of collapse the superposition. So what that allows

680
00:59:05,920 --> 00:59:13,280
you to do is to simulate on a computer things in the real world that are quantum, because we know

681
00:59:13,360 --> 00:59:18,960
the real world is quantum, right? That's been established for a long time. And yet when we're

682
00:59:18,960 --> 00:59:24,160
interfacing with quantum systems, for example, material science or biology, we still use classical

683
00:59:24,160 --> 00:59:28,960
computers to simulate them. And so we're sort of trying to use something deterministic to simulate

684
00:59:28,960 --> 00:59:36,240
something quantum. So what's really exciting for me about quantum computing is that you may have

685
00:59:36,240 --> 00:59:42,720
a tool that lets us simulate aspects of nature that we have historically not been able to simulate.

686
00:59:42,720 --> 00:59:47,360
And so for example, we can suddenly design incredible new materials that we can use to

687
00:59:47,360 --> 00:59:50,560
the engineers can use to make things we've not been able to make until now,

688
00:59:52,160 --> 00:59:55,600
or we can simulate biology in a way that we currently can't today. And as a result,

689
00:59:55,600 --> 01:00:01,760
design amazing new drugs. And so I think that like, the way I think about quantum computers

690
01:00:01,760 --> 01:00:08,400
is they are a class of computer that allows us to explore and understand the universe in a way

691
01:00:08,400 --> 01:00:12,480
that classical computers don't. And that just feels like it's kind of like the invention of a

692
01:00:12,480 --> 01:00:16,800
microscope or something. It's like a really important new tool that gives us visibility

693
01:00:16,800 --> 01:00:21,760
into a realm that we currently don't really have computing resources that are appropriate for.

694
01:00:21,760 --> 01:00:24,880
You have all these great analogies. I love this. So the idea of a quantum computer is like a

695
01:00:24,880 --> 01:00:30,000
microscope. And of course, prior to the microscope, people didn't really understand bacteria, germs,

696
01:00:30,000 --> 01:00:33,840
you know, the majority of organisms on the face of the earth, because they weren't visible to the

697
01:00:33,840 --> 01:00:37,840
human eye, you're saying something similar could be possible with quantum computing.

698
01:00:37,840 --> 01:00:41,440
Yeah. And I think it's really about simulation. And that's the thing that I guess I've got

699
01:00:41,440 --> 01:00:45,440
personally really excited about is, you know, you've got some incredible classes of materials

700
01:00:46,240 --> 01:00:49,360
that we know are quantum materials, right? The way that superconductive materials,

701
01:00:49,360 --> 01:00:56,000
for example, we know a quantum and superconductors are amazing, you know, in that they are from a

702
01:00:56,000 --> 01:01:00,160
climate perspective and energy perspective, they're like the material because you have no

703
01:01:00,160 --> 01:01:04,560
heat loss to resistance. So you could transmit energy across the UK without losing any,

704
01:01:04,560 --> 01:01:10,480
you have new energy storage opportunities, fusion reactors, the type I talked about require

705
01:01:10,480 --> 01:01:15,680
very powerful superconducting magnets to work, MRI machines require them. So they're kind of

706
01:01:15,680 --> 01:01:20,880
this magical class of materials called quantum materials. And because they are quantum materials,

707
01:01:20,880 --> 01:01:25,600
there's only so much we can really understand them by applying classical computing techniques to them,

708
01:01:25,600 --> 01:01:29,600
whereas a quantum computer would let you simulate them in a completely different way.

709
01:01:29,600 --> 01:01:33,600
And as a result, we might be to discover new materials that we don't have access to today.

710
01:01:33,600 --> 01:01:37,520
Before we start this interview, I asked you if you would consider moving to the States,

711
01:01:37,520 --> 01:01:42,080
you know, that's kind of a cliche thing, but you know, people generally make their fortune

712
01:01:42,080 --> 01:01:49,200
over in the US when it comes to technology businesses. You said no, or you weren't really

713
01:01:49,200 --> 01:01:56,160
inclined to. So why do you want to stay here in the UK? I mean, I think the UK's given me kind of,

714
01:01:56,720 --> 01:02:03,200
I think, you know, I think I'm quite patriotic. I feel very, you know, very appreciative of

715
01:02:03,200 --> 01:02:08,160
what I've been given in terms of the privilege of growing up somewhere with universal health care,

716
01:02:08,960 --> 01:02:14,400
with great education, with, you know, freedom of speech, the rule of law, like there's lots of

717
01:02:14,400 --> 01:02:20,160
aspects of our society I'm very proud of, proud to be kind of, you know, to proud of and I believe

718
01:02:20,160 --> 01:02:25,120
are really important to endure. And as we were talking also earlier, you know, we should recognize

719
01:02:25,120 --> 01:02:30,800
these things as fragile, you know, Iran was a democracy once and is not now. And so I feel a

720
01:02:30,800 --> 01:02:37,680
duty to give back to the UK, give back to Europe and very specifically the sort of investment

721
01:02:37,680 --> 01:02:42,880
fund that my partners and I have set up, it's a we're all former founders, right? That's the

722
01:02:42,880 --> 01:02:48,080
first thing about us. And the second thing is, we've set it up to try and have GDP level impact

723
01:02:48,080 --> 01:02:54,240
on Europe. And so the idea is to help build some companies using our kind of scar tissue as founders

724
01:02:54,240 --> 01:02:59,360
to help another generation of founders who are like us to build companies that can end up being

725
01:02:59,360 --> 01:03:03,680
bigger than we've seen in Europe up until now. So I think of Skype, and I think Skype could have

726
01:03:03,680 --> 01:03:08,160
been Facebook, I see DeepMind, I think DeepMind could have been Google or OpenAI, or I look at

727
01:03:08,160 --> 01:03:13,120
ARM, I think ARM could have been NVIDIA. And so we've got these companies where we never really

728
01:03:13,120 --> 01:03:16,960
got them to be what they could have been their full potential to be these like transformational

729
01:03:16,960 --> 01:03:24,720
European technology companies. And so our mission as a kind of as a fund is to try to produce a few

730
01:03:24,800 --> 01:03:29,440
of these companies that can change whole industries for the better. And by doing so have

731
01:03:29,440 --> 01:03:33,920
impacts on European GDP, but also put Europe at the table when it comes to describing what's

732
01:03:33,920 --> 01:03:38,000
happening in an important new era of technology. So if we're going to put fusion on the grid

733
01:03:39,440 --> 01:03:46,960
in the next, you know, in the 2030s say, and Germany has been the state with the most kind

734
01:03:46,960 --> 01:03:51,280
of entrepreneurialism to invest in accelerators well ahead of everyone else in the world, it

735
01:03:51,280 --> 01:03:57,200
will be a travesty if the the fusion company that turns that into a into a startup isn't in

736
01:03:57,200 --> 01:04:03,440
Europe, in my opinion. And so I do feel I do feel just very embedded in European values. I mean,

737
01:04:03,440 --> 01:04:07,120
I lived in China, I lived in America, I've been a lot of admiration for both the societies in

738
01:04:07,120 --> 01:04:11,200
different ways. But I guess I'm kind of pretty European to my core.

739
01:04:12,000 --> 01:04:15,920
That's interesting. You said patriotic, but then you say European. So can you go into that a bit

740
01:04:15,920 --> 01:04:21,040
more? Because in the UK, we've had this, you know, I don't want to sort of tread over old ground,

741
01:04:21,040 --> 01:04:23,200
but they're often held in counterpoint to one another.

742
01:04:23,680 --> 01:04:29,120
Yeah, and I think that's a bit of a, I don't know, like, I think there's a lot of nuance that's been

743
01:04:29,120 --> 01:04:34,800
lost from discussion around Brexit. So a good example is one of the AI companies I work with.

744
01:04:35,680 --> 01:04:40,400
The founder chose to actually locate the company in London rather than Silicon Valley,

745
01:04:40,400 --> 01:04:44,000
because it was going to be easier to recruit the people who wanted into the company.

746
01:04:44,000 --> 01:04:47,920
Right. So we talk about the UK, you know, being a much more, you know, people talk,

747
01:04:47,920 --> 01:04:52,080
people caricature it as being more closed post Brexit. But actually, there's been some great

748
01:04:53,200 --> 01:04:57,840
research done by John Paul Murdock at the time, the Financial Times that I think sort of shows

749
01:04:57,840 --> 01:05:01,440
that's actually not necessarily true in all domains. And I haven't heard that as much as

750
01:05:02,000 --> 01:05:04,560
you might think from some of these leading edge technology companies.

751
01:05:06,160 --> 01:05:09,040
And so I think there's this kind of, we want to make everything black and white,

752
01:05:09,040 --> 01:05:15,760
but actually there are ways in which I think that, you know, a suitably ambitious progressive

753
01:05:15,760 --> 01:05:19,600
government that really wants to embrace Brexit and make it work could do really interesting

754
01:05:19,600 --> 01:05:24,160
things with it. And so I don't, I'm not like reflex it, even though I voted remain, I'm not

755
01:05:24,160 --> 01:05:28,080
reflexively negative about Brexit and sort of, it's just all these people who just basically

756
01:05:28,080 --> 01:05:32,320
sit there complaining about it all day long on Twitter, like five, you know, five, you know,

757
01:05:32,320 --> 01:05:38,400
and years on what at 20 2016, right? So like seven years on from it, that there's sort of,

758
01:05:38,400 --> 01:05:44,000
there's a, there's a sort of, I don't know, it's become a sort of a tribal identity rather than a

759
01:05:44,000 --> 01:05:48,640
sort of first principles assessment of what we should actually do as the UK to, to sort of

760
01:05:49,600 --> 01:05:55,280
make the UK the best possible place to live. Yeah, David Deutch, who is one of these hugely

761
01:05:55,280 --> 01:05:59,040
influential figures in Silicon Valley, British man, you know, he's based, I think in Oxford

762
01:05:59,840 --> 01:06:06,320
at the moment, and he advocated leave. And, you know, I don't agree with his arguments,

763
01:06:06,320 --> 01:06:09,600
I certainly don't think it was wise to leave the single market in the way that we have done.

764
01:06:10,720 --> 01:06:14,480
But this idea that everybody who voted leave is thick and stupid, you know, David Deutch is

765
01:06:14,480 --> 01:06:19,120
literally one of the smartest human beings who's ever lived. And, and like you say, there are,

766
01:06:19,120 --> 01:06:23,520
there are opportunities, particularly with goal oriented public policy, with new technologies

767
01:06:23,520 --> 01:06:28,400
that you probably could do interesting things with in a way that you probably can't, or it's harder

768
01:06:28,400 --> 01:06:33,120
to do inside the European Union. But going back to the point about patriotism. So you're

769
01:06:33,120 --> 01:06:41,040
patriotic towards the UK, or to Europe, or both? Yeah, I mean, I, I, I guess I, the reason I,

770
01:06:41,680 --> 01:06:47,360
I think, I think George Orwell had a lovely sort of take on nationalism versus patriotism.

771
01:06:47,360 --> 01:06:51,920
I think he said something like, you know, nationalism is kind of the assertion of your

772
01:06:51,920 --> 01:06:56,320
values and other people and pushing your country's values on other people, whereas

773
01:06:56,320 --> 01:06:59,600
patriotism is just sort of saying, it's kind of defending your values and just saying like,

774
01:07:00,160 --> 01:07:03,520
these are things that are important to me and we want to preserve some of these things. And

775
01:07:03,520 --> 01:07:08,880
I, I feel, you know, the, you know, NHS is the good, the kind of classic example is just something

776
01:07:08,880 --> 01:07:15,360
that I feel very grateful for. And I feel patriotic about wanting it to, it to continue and to thrive.

777
01:07:15,360 --> 01:07:20,240
And, you know, if we're going to apply AI in the NHS, I want it to be done in an incredibly

778
01:07:20,240 --> 01:07:24,880
thoughtful way that expands what we've got today rather than anywhere undermines it.

779
01:07:24,880 --> 01:07:28,240
So where did you live in China in the US? You said you moved there a few years ago?

780
01:07:28,240 --> 01:07:33,600
Yeah, so I lived in, I lived in, I moved to China when I was 18 in 2000, study Mandarin, and then

781
01:07:34,720 --> 01:07:38,720
went to university, started a kind of engineering machine learning, and then went back to China

782
01:07:38,720 --> 01:07:48,160
in 2005, again, to study machine, to study Mandarin. And then I moved to the US Silicon Valley in 2006.

783
01:07:48,160 --> 01:07:55,120
And so, I guess I've, I think I was drawn to both places because there were a lot was changing

784
01:07:55,120 --> 01:07:59,280
very rapidly in the world. And it was, I think, somewhere where the pace of change was very

785
01:07:59,280 --> 01:08:05,120
exhilarating. But I really feel, I think, the most at home in Europe in terms of kind of the

786
01:08:05,120 --> 01:08:11,040
values, you know, I think, and I'd love to see Europe's technology industry and Europe's

787
01:08:11,040 --> 01:08:16,320
governments really kind of rise to the moment that is coming in, in, with all this exponential

788
01:08:16,320 --> 01:08:22,160
technological change. The final question, because it goes back to the thing you, you've

789
01:08:22,160 --> 01:08:25,600
mentioned, you wrote this essay about AI nationalism, there's a great talk on YouTube,

790
01:08:25,600 --> 01:08:29,600
by the way, once people have finished watching this and maybe watched another

791
01:08:29,600 --> 01:08:34,480
Navarro media video, you do a great talk on AI nationalism. It's from a few years ago.

792
01:08:35,520 --> 01:08:41,920
And you, you quote a great book, AI superpowers. And it's the hypothesis of that, I think is a

793
01:08:41,920 --> 01:08:48,000
really strong one of the best books I've read in a while. A few years ago, I think Pricewaterhouse

794
01:08:48,000 --> 01:08:53,280
Cooper say that between 2015 and 2035, you know, 15, 16 trillion dollars will be added to the global

795
01:08:53,280 --> 01:08:59,840
economy by AI, more or less something like that. And about 70% of that goes to basically the US and

796
01:08:59,840 --> 01:09:05,600
China. So all of the gains of this new technology, basically are concentrated in these two AI

797
01:09:05,600 --> 01:09:10,480
superpowers. Imagine the steam engine, but rather than just Britain benefiting initially,

798
01:09:10,560 --> 01:09:13,600
like we get with, you know, the steam revolution, the industrial revolution and then colonialism,

799
01:09:14,160 --> 01:09:19,600
AI basically redounds the benefit of Beijing and Washington or Washington slash California.

800
01:09:20,800 --> 01:09:24,240
Can you go into this hypothesis? Because it sounds to me, on the one hand, you just said a moment

801
01:09:24,240 --> 01:09:31,120
ago, you're patriotic to Europe. Yet this idea of AI nationalism seems to indicate that Europe

802
01:09:31,120 --> 01:09:35,520
isn't really at the races. And actually, on the present path, the two places that will benefit

803
01:09:35,520 --> 01:09:43,200
from this are China and the US. So I think I think that Europe in general, setting aside the UK is

804
01:09:43,200 --> 01:09:51,120
not very significant, unfortunately, within AI right now. If you look at where the most important

805
01:09:51,120 --> 01:09:56,240
kind of concentrations of sort of talent and power are within AI and capabilities, it's actually

806
01:09:56,240 --> 01:10:03,760
really cities that matter. And it's Toronto, San Francisco, London and Beijing. And London is

807
01:10:03,760 --> 01:10:07,440
very much on the map. So London matters more than New York? I would say so. Yeah. I mean,

808
01:10:07,440 --> 01:10:12,640
so Demis Osavis, who arguably kicked off the race we've currently got going on with the founding

809
01:10:12,640 --> 01:10:19,360
of DeepMind and is an absolutely exceptionally talented person, is now running all of Google's

810
01:10:19,360 --> 01:10:26,240
efforts globally from London. You know, the vast majority of the sort of best AI researchers

811
01:10:26,240 --> 01:10:30,320
within Google are based in London at DeepMind's office. And so London is incredibly important.

812
01:10:30,320 --> 01:10:35,040
It's not just London, not just DeepMind. We also have other organisations of a sort of similar,

813
01:10:35,040 --> 01:10:42,240
you know, research stature to DeepMind. So I think that we should not rule the UK out. And

814
01:10:42,240 --> 01:10:48,640
actually, I think it's a real opportunity for the UK to play a leadership role within Europe

815
01:10:48,640 --> 01:10:54,960
by kind of bringing some of those assets to the table when Europe wants to do stuff in AI regulation.

816
01:10:55,920 --> 01:11:01,760
But to go back to your question about kind of this race, this race between different countries,

817
01:11:02,560 --> 01:11:08,400
I think it's incredibly nuanced and complicated because there are basically three different

818
01:11:08,400 --> 01:11:13,200
levels you need to think about it at. The first is economic. So, you know, a country that has

819
01:11:13,200 --> 01:11:18,480
incredibly sophisticated AI companies will probably benefit more economically from it than

820
01:11:18,480 --> 01:11:23,040
those that don't. And that's the kind of takers or makers or, you know, the sort of steam analogy

821
01:11:23,040 --> 01:11:30,640
you gave. The second, and you know, there is the second is basically military. And so AI will

822
01:11:30,640 --> 01:11:36,080
definitely be used by militaries to achieve decisive advantage. And so in that area, again,

823
01:11:36,080 --> 01:11:41,760
like a state is incentivised to build out the capacity to, you know, have autonomous autonomous

824
01:11:43,040 --> 01:11:46,640
and, you know, even in the Ukraine conflict, you actually see the use of machine learning to

825
01:11:46,640 --> 01:11:52,400
actually, you know, provide a decisive edge in certain ways. And the third is this existential

826
01:11:52,400 --> 01:11:57,280
thing of like, if we build something smarter, more capable than us, that isn't aligned with our

827
01:11:57,280 --> 01:12:04,080
goals, it could wipe us all out. And if you look at the three different levels, you've got the,

828
01:12:04,080 --> 01:12:08,560
and you think about the US and China in the way that Kaifu Lee kind of presented that that race,

829
01:12:09,440 --> 01:12:14,560
US and China clearly are locked into a battle to compete economically. They're certainly locked

830
01:12:14,560 --> 01:12:18,560
in a battle to compete militarily, like a hypersonic missiles or something like that.

831
01:12:18,560 --> 01:12:22,320
But this final level, which is like, are they locked in a battle to kind of,

832
01:12:22,320 --> 01:12:27,200
you know, ensure that the human species thrives? I think they are actually, you know, they should

833
01:12:27,200 --> 01:12:31,200
be on the same page about that, right? So you've got this really challenging problem where you've

834
01:12:31,200 --> 01:12:36,240
got two levels of competition, and one level where you desperately need cooperation. And that's why

835
01:12:36,240 --> 01:12:40,480
I think that like the really missing piece right now here is kind of international leadership

836
01:12:40,480 --> 01:12:45,600
around coordination, and how we approach these most powerful AI systems that could become super

837
01:12:45,600 --> 01:12:50,160
intelligent. That's so interesting around AGI. And I can see the argument for cooperation,

838
01:12:50,160 --> 01:12:55,840
as you would hope for, say around climate change. But I suppose pulling it back a bit,

839
01:12:55,840 --> 01:13:02,880
so going back to the narrow AI things that you spoke about a while back. Clearly,

840
01:13:02,880 --> 01:13:07,360
AI is going to be massively disruptive in terms of economics. And I suppose if you look at the

841
01:13:07,360 --> 01:13:12,400
last 20, 25 years and Silicon Valley really capturing so much value across the West, you know,

842
01:13:13,040 --> 01:13:16,320
I'm from Bournemouth. If you walk down the high street in Bournemouth, loads of businesses have

843
01:13:16,320 --> 01:13:20,800
shut down because that value has been captured by a company headquartered in California, right?

844
01:13:20,800 --> 01:13:26,320
And that is an incredible concentration of political, economic, and cultural power.

845
01:13:27,520 --> 01:13:31,280
And the argument is the same thing will happen with AI with more businesses to a far greater

846
01:13:31,280 --> 01:13:37,760
extent. And the concern is that basically the world will be divided into two economic spheres,

847
01:13:37,840 --> 01:13:45,200
US, China centric, and the rest of the world will, broadly speaking, just be their satellites.

848
01:13:45,200 --> 01:13:49,680
So we don't need to be talking about an AGI and oh, great power confrontation will lead to

849
01:13:50,320 --> 01:13:54,880
somebody potentially creating Skynet because they're seeking military superiority. Although

850
01:13:54,880 --> 01:14:00,080
that's an interesting debate. But we could be returning to a bipolar world similar to the Cold

851
01:14:00,080 --> 01:14:06,000
War, but in many ways far more extensive. Or do you think I'm sort of being a bit sort of

852
01:14:06,000 --> 01:14:11,200
hyperbolic here because you have so quickly to go back, Kai-Fu Li talks about AI superpowers.

853
01:14:11,200 --> 01:14:15,600
Why would it be China and the US are the superpowers? They have the largest populations,

854
01:14:16,400 --> 01:14:20,240
Ergo, the largest amounts of data, and they have the greatest amount of computational power.

855
01:14:20,240 --> 01:14:24,320
You might think that, you know, the EU might be a third poll, but it's not really working out

856
01:14:24,320 --> 01:14:31,840
here the two. So geopolitically, the geopolitics of AI, I mean, you can see a world, can't you,

857
01:14:31,840 --> 01:14:35,840
where there's a great sort of economic dependence, basically a kind of colonialism.

858
01:14:36,560 --> 01:14:42,640
So I think, you know, in the AI nationalism essay, I gave an example of a kind of that sort of AI

859
01:14:42,640 --> 01:14:47,600
colonialism with a company called Cloudwalk from China that was selling certain technologies to,

860
01:14:47,600 --> 01:14:54,240
I think it was Zimbabwe. But I actually don't really view it as much from a kind of how it

861
01:14:54,240 --> 01:15:00,320
affects individual citizens at an international level, more national level. You know, I think

862
01:15:01,280 --> 01:15:06,480
that kind of haves and have nots within technology are already playing out on a national level.

863
01:15:06,480 --> 01:15:12,800
If you look at the, you know, if you look at the deprivation in the Bay Area, right, and you compare

864
01:15:12,800 --> 01:15:18,720
the sort of the lot of some of the people, you know, on the streets in the Tenderloin compared

865
01:15:18,720 --> 01:15:24,080
to some of the people running these AI companies, you've got far more, you know, the genie coefficient

866
01:15:24,080 --> 01:15:30,160
in some ways in sort of some of the US cities or across US states is maybe more extreme than

867
01:15:30,160 --> 01:15:36,240
it is between certain nations. And I'm more, I think that's more concerning the the the lot of

868
01:15:36,240 --> 01:15:41,120
the average person on Bournemouth versus a lot of the average person in London than it is to me

869
01:15:41,120 --> 01:15:46,000
of kind of these these sort of country versus country comparisons. And so I think actually

870
01:15:46,000 --> 01:15:54,080
the really challenging political problem is as actually how you kind of how you how you make

871
01:15:54,080 --> 01:15:58,800
how you kind of raise the floor more broadly in a world in which there are further and further

872
01:15:58,800 --> 01:16:04,080
returns to capital. I think we'll end it there. Ian, this has been a fascinating conversation.

873
01:16:04,080 --> 01:16:14,720
Thank you so much for joining us. Thank you for having me.

