1
00:00:00,000 --> 00:00:03,760
These organizations are telling you that they're building something that could kill

2
00:00:03,760 --> 00:00:09,360
you and something that could remove all our freedom and liberty. And they're

3
00:00:09,360 --> 00:00:12,560
saying it's a good thing you should back them because it's cool. They don't care

4
00:00:12,560 --> 00:00:16,400
about the revenue. If they have the political power people are scared of

5
00:00:16,400 --> 00:00:21,080
them. Power should not be invested in any one individual. If I can accelerate this

6
00:00:21,080 --> 00:00:24,720
over the next period I don't have to make an impact. I should not have any power

7
00:00:24,880 --> 00:00:28,000
whereas again you see everyone else trying to get more and more power.

8
00:00:30,320 --> 00:00:34,720
The only way that you can beat it to create the standard that represents humanity

9
00:00:35,360 --> 00:00:38,880
is decentralized intelligence. It's collective intelligence.

10
00:00:39,520 --> 00:00:45,200
The data sets and norms from that will be ones that help children, that help people suffering,

11
00:00:45,920 --> 00:00:51,760
that reflect our moral upstanding and the best of us and gathers the best of us to do it.

12
00:00:55,680 --> 00:01:01,280
A week ago, E-Mod Mistak was on my stage at Abundance 360 talking about the future of

13
00:01:01,280 --> 00:01:08,240
open source AI, democratized, decentralized AI. The day after A360 he stepped down as CEO of

14
00:01:08,240 --> 00:01:14,080
Stability. Now five days later I've sat down with E-Mod to talk about why he's stepping down,

15
00:01:14,080 --> 00:01:19,280
what he's doing next, the future of AI. He takes the gloves off, he talks about the dangers of

16
00:01:19,280 --> 00:01:26,480
centralized AI and the potential for decentralized democratized AI to be the only avenue that truly

17
00:01:26,480 --> 00:01:33,440
uplifts all of humanity. All right, if you liked this episode please subscribe. Let's jump in. If

18
00:01:33,440 --> 00:01:38,320
you're a mood shot entrepreneur this is an episode you're not going to want to miss. All right,

19
00:01:38,320 --> 00:01:42,320
now on to E-Mod. Good morning E-Mod. Good to see you my friend.

20
00:01:42,400 --> 00:01:49,920
Actually it's always pizza. So you and I were on stage literally last week at the 2024 Abundance

21
00:01:49,920 --> 00:01:57,520
Summit talking about the whole open source AI movement. You were beginning to talk about

22
00:01:57,520 --> 00:02:03,600
decentralized AI. You were talking about where Stability was the speed of the development of

23
00:02:03,600 --> 00:02:11,840
the different products and the day after the Abundance Summit was over the news hit that you

24
00:02:11,840 --> 00:02:20,000
had stepped down as CEO of Stability and stepped off the board. So let's begin with the obvious

25
00:02:20,000 --> 00:02:28,640
question. Why? What happened and I have huge respect for you and I know a lot of the issues in the

26
00:02:28,640 --> 00:02:34,240
past but I'd like you to have a chance to share with entrepreneurs out there and folks interested in

27
00:02:34,240 --> 00:02:42,800
AI exactly your side of what's happening. Yeah, thanks. I think that Elon Musk once characterized

28
00:02:42,800 --> 00:02:51,600
being a CEO as staring into the abyss and chewing glass because you are looking at a very uncertain

29
00:02:51,600 --> 00:02:55,360
future having to make decisions and the chewing glass is all the problems that come to you all

30
00:02:55,360 --> 00:03:03,200
the time and it's required to steer the ship when things are incredibly uncertain and Stability

31
00:03:03,200 --> 00:03:08,560
is a pretty unique company at a unique time. We hired our first developer and researcher two

32
00:03:08,560 --> 00:03:14,160
years ago and then in those two years we built the best models of almost every type except for large

33
00:03:14,160 --> 00:03:21,520
language, image, audio, 3D etc and had over 300 million downloads of the various models we created

34
00:03:21,520 --> 00:03:28,880
and supported which was a bit crazy and then gent of AI is crazy. In terms of usually in a startup

35
00:03:28,880 --> 00:03:33,600
you don't have to deal with global leaders and policy debates about the future of humanity

36
00:03:34,240 --> 00:03:41,360
and AI and everything else. At the same time building code. At the same time as building code

37
00:03:41,360 --> 00:03:49,600
yes and especially building code to a fraction of the resources of our competitors. We had

38
00:03:50,400 --> 00:03:56,240
certain teams that offered triple their entire packages to move to other companies. It was

39
00:03:56,240 --> 00:04:05,120
grateful that only a couple of researchers left for other companies and that was just startups.

40
00:04:05,120 --> 00:04:09,760
No one left for another big company which I think is testament to their kind of loyalty in the mission

41
00:04:10,720 --> 00:04:15,600
but you know what we've seen over the last year is or last half year in particular is the question

42
00:04:15,600 --> 00:04:24,560
of governance in AI is something that's incredibly important and who manages, owns, controls this

43
00:04:24,560 --> 00:04:30,480
technology and how it's distributed. So we saw you know everything from kind of open AI to congressional

44
00:04:30,480 --> 00:04:35,200
testimonies to other things and as you know one of my things has always been how do we get this

45
00:04:35,200 --> 00:04:40,800
technology in the hands of people all around the world and then who governs it and how can we then

46
00:04:40,800 --> 00:04:46,080
take this technology to have an impact from education to healthcare to others. The stability

47
00:04:46,080 --> 00:04:51,600
is a company that build great base models and we got to that point. The revenue is going up

48
00:04:51,600 --> 00:04:56,640
which is always nice you know finding the business model and again I think it was always curious to

49
00:04:56,640 --> 00:05:01,520
see that as a deep tech company two years in people are asking us you know why aren't you

50
00:05:01,520 --> 00:05:05,600
profitable and I was like right it takes a bit of time and investment to get to profitability.

51
00:05:07,040 --> 00:05:12,240
I think opening AI when they took they're not there yet but then they have eight years I think

52
00:05:12,240 --> 00:05:17,360
all the comparisons are perhaps unfair. So reviewing everything and kind of looking at it I was like

53
00:05:17,360 --> 00:05:23,200
do I really want to be a CEO? I think the answer is no. I think there is a lot imbued in that in

54
00:05:24,000 --> 00:05:30,560
tech but it's a very interesting position. Just to dive into that a second because we've had this

55
00:05:30,560 --> 00:05:38,480
conversation because there was a lot of pressure asking whether for you to step down as CEO

56
00:05:40,400 --> 00:05:46,960
and I think founders typically want to see themselves or feel they need to be the CEO

57
00:05:47,520 --> 00:05:52,880
and I've heard you say recently you know that you view yourself more as a founder and strategist

58
00:05:52,880 --> 00:05:58,160
than the CEO is that a fair assessment? Yeah I think everyone's got their own skill sets right

59
00:05:58,160 --> 00:06:04,000
so I'm particularly great at taking creatives, developers, researchers, others and achieving

60
00:06:04,000 --> 00:06:10,400
their full potential in designing systems but I should not be dealing with you know HR and

61
00:06:10,400 --> 00:06:14,480
operations and business development and other elements they're probably better people than me

62
00:06:14,560 --> 00:06:20,400
to do that. So now for example our most popular thing Stable Diffusion and come to UI the system

63
00:06:20,400 --> 00:06:26,240
around it is the most widely used image software models in the world. There are great media CEOs

64
00:06:26,240 --> 00:06:30,080
that can take that and amplify that to make hundreds of millions of revenues so they should come in

65
00:06:30,080 --> 00:06:39,040
and meet on that. So why now pal what is there anything that specifically tipped for you that has

66
00:06:39,840 --> 00:06:46,400
I mean because you know it has you know you have done an extraordinary job this has been your baby

67
00:06:47,600 --> 00:06:55,920
I mean how you have to feel a whole slew of emotional elements and I've had to step down

68
00:06:55,920 --> 00:07:02,720
as CEO on two occasions over the 27 companies I've had to sell a company for pennies on the

69
00:07:02,720 --> 00:07:12,000
dollar and it takes an emotional hardship on you. I've had calls for me to step down as CEO

70
00:07:12,000 --> 00:07:19,040
since the well since 2022 you know but I always thought you know what's best for the company in

71
00:07:19,040 --> 00:07:24,320
the mission and where I look at the world right now there's a few things a the company has momentum

72
00:07:24,320 --> 00:07:29,760
it has spread it's turning into a business like last year I said let's not enter into large revenue

73
00:07:29,760 --> 00:07:34,480
contracts because technology isn't mature yet and our processes aren't mature yet and you have to

74
00:07:34,480 --> 00:07:39,120
deliver so we do a lot of experimental things we're setting up and again now it's ramping

75
00:07:39,120 --> 00:07:44,480
on a business side on a technology side the technology is maturing diffusion transformers

76
00:07:44,480 --> 00:07:48,240
such as stable diffusion 3 and saura are going to be the next big thing and again stability has

77
00:07:48,240 --> 00:07:54,320
got a great place there but I think there's also the macro on this so if you look at the open AI

78
00:07:54,400 --> 00:08:01,360
you know sam altman said the board can fire me anytime this is the governance of open AI

79
00:08:02,080 --> 00:08:08,320
and then they fired him and then he is back on and he appoints himself back on the board

80
00:08:08,960 --> 00:08:12,880
there's clearly no governance opening I mean I respect the people on the board greatly

81
00:08:12,880 --> 00:08:17,520
you know I think there's some great individuals but who should manage the technology that drives

82
00:08:17,520 --> 00:08:23,200
humanity and teaches every child and manages our governance who's really needy on that that can build

83
00:08:23,200 --> 00:08:28,560
these models and do those things as you know I've always wanted to build these science models and

84
00:08:28,560 --> 00:08:34,560
the other health teams done that I want me doing the education work and then my concept of a national

85
00:08:34,560 --> 00:08:39,600
model for every country owned by the people of the country all tied together I think it needs to be

86
00:08:39,600 --> 00:08:46,240
by a web three not crypto or necessary token framework that's something that's a brand new

87
00:08:46,240 --> 00:08:52,080
kind of challenge and one that I think there's only a window of a year or two to do if you have

88
00:08:52,160 --> 00:08:55,600
highly capable models let's put aside a geo for now which you can discuss later

89
00:08:56,240 --> 00:09:02,400
really accelerating then no one will be able to keep up with that unless you build in a decentralized

90
00:09:02,400 --> 00:09:08,320
manner and distributed manner for data talent distribution standards and more so there's only

91
00:09:08,320 --> 00:09:13,120
a small window of time here to do that and realistically again successful companies and

92
00:09:13,120 --> 00:09:19,120
these things are all great but genitifia is a bit bigger than the classical knobs just like

93
00:09:19,120 --> 00:09:23,120
the whole lifecycle of the company was a lot faster than the classical norms so that's why I

94
00:09:23,120 --> 00:09:29,360
felt you know now is the right time to make that change and hopefully play my part in making sure

95
00:09:29,360 --> 00:09:34,960
this technology is distributed as widely as possible and governed properly as pretty much I

96
00:09:34,960 --> 00:09:40,000
think I'm the only real independent agent that has built state-of-the-art models in the world right

97
00:09:40,000 --> 00:09:50,960
now yeah we've seen a lot of turbulence with open AI we just saw Mustafa from inflection become part

98
00:09:50,960 --> 00:10:00,720
of Microsoft and I am curious I mean you you had a now famous conversation with Satya a couple of

99
00:10:00,720 --> 00:10:08,000
days after stepping down was that investigatory on your part or was that just a touch base for

100
00:10:08,000 --> 00:10:13,440
an old friend oh that's just chill trolling actually you know like to let us in steam that

101
00:10:13,440 --> 00:10:22,080
picture was I think from when year or two year or so ago okay but you know I think Satya is an

102
00:10:22,080 --> 00:10:28,640
amazing CEO and you know he responds again like the top CEO's incredibly quickly when you message

103
00:10:30,400 --> 00:10:34,800
he's got a great vision but there is again this concern about consolidation in tech

104
00:10:34,800 --> 00:10:40,160
we didn't take money from any trillion dollar companies that stability you know we remained

105
00:10:40,160 --> 00:10:47,040
and retained full independence you know and you know to the detriment of some of the elements

106
00:10:47,040 --> 00:10:53,120
that we could have taken very big checks and other things and even though you have good attention

107
00:10:53,120 --> 00:10:58,400
to that room that companies are slight slow dumb ai's that over optimize for various things

108
00:10:58,960 --> 00:11:02,640
and that's more solely in the best interest of humanity when you have infrastructure

109
00:11:03,600 --> 00:11:09,600
it's like this is the airports the railways the roads of the future AI is an infrastructure

110
00:11:09,600 --> 00:11:14,480
gentle that is an infrastructure which should be and should it be consolidated under the control

111
00:11:14,480 --> 00:11:20,080
of a few private companies with unclear objective functions again the people in the companies may

112
00:11:20,080 --> 00:11:25,280
be great I don't think so and this is a key concern and part of that was that commentary again he's

113
00:11:25,280 --> 00:11:31,200
doing an amazing job he's consolidating a lot of power for the good of the company and also he has

114
00:11:31,360 --> 00:11:37,120
I think genuinely good heart mission to bring technology to the world but it is a bit concerning

115
00:11:37,120 --> 00:11:41,600
right especially with the new types of structure you're speaking of Sam in this case

116
00:11:42,240 --> 00:11:46,640
Satya here oh sat here like a really like again I think the commentary was always interesting

117
00:11:46,640 --> 00:11:52,000
it's like you know sat here playing 40 chess you know assembling the AI Avengers I mean he is

118
00:11:52,000 --> 00:11:56,960
he's building an amazing massive talent covering the bases and Microsoft is doing incredibly well

119
00:11:56,960 --> 00:12:01,280
here right if you asked who's doing best to join tomorrow I think would say Microsoft

120
00:12:02,320 --> 00:12:06,320
but there is has to be concerns about consolidation of talent and power and reach

121
00:12:06,320 --> 00:12:10,800
before I get to your vision going forward because it's so important and what you're doing next

122
00:12:12,400 --> 00:12:22,560
I just again as a founder as a CEO of a moonshot company we have a lot of those listening here

123
00:12:23,280 --> 00:12:29,680
can ask how are you feeling right now because the decision to step down has to have huge emotional

124
00:12:30,240 --> 00:12:38,160
are you feeling relief are you feeling anxiety what's what's the feeling after making a momentous

125
00:12:38,160 --> 00:12:49,440
decision I was a big feeling of relief you know because there's the there's a Japanese concept

126
00:12:50,400 --> 00:12:56,560
I know a guy and I love it yes yeah do what you like and do where you believe you're adding value

127
00:12:56,560 --> 00:13:03,600
and other people do too you know like realistically again I think I was an excellent research leader

128
00:13:03,600 --> 00:13:08,080
strategist other things but I didn't communicate properly or hire the right other leaders in

129
00:13:08,080 --> 00:13:12,480
certain other areas of the company and they're better people to do that and so I wasn't doing

130
00:13:12,480 --> 00:13:18,640
what I was best at a lot or I could have the most measurable value and it was tying down you know

131
00:13:18,640 --> 00:13:22,640
there's a lot of legacy you know technical organization or other debt especially when

132
00:13:22,640 --> 00:13:27,360
you grow so so so fast and you know we were lucky that we had higher attention in the

133
00:13:27,360 --> 00:13:32,160
important areas and we could execute in spite of all of that in spite of going to be company

134
00:13:32,160 --> 00:13:36,640
around I think you know moonshot founders you have to do because you don't have the resources at

135
00:13:36,640 --> 00:13:41,680
the start and you have to guide the ship you know as it goes out from port but there does come that

136
00:13:41,680 --> 00:13:47,600
transition point there and there is a competing thing where you typically take on VC money which

137
00:13:47,600 --> 00:13:53,120
holds an objective function versus your overall mission so again if you look at the genitive

138
00:13:53,120 --> 00:14:00,480
AI world right now how many credible intelligent independent voices are there there they've had

139
00:14:00,480 --> 00:14:05,600
the ability to build models and design things and make an impact you know there's not many so

140
00:14:06,160 --> 00:14:11,360
I was like that's where I can add my most leverage and also the design space is again

141
00:14:11,360 --> 00:14:17,760
is unprecedentedly huge because the entire market has just been created like where does

142
00:14:17,760 --> 00:14:22,800
gentify not fit and where does it not touch and what needs to be built there we need to actually

143
00:14:22,800 --> 00:14:30,320
have the agency to go and build that and so I felt tired relieved I felt that now there's a

144
00:14:30,320 --> 00:14:35,520
million options I want it rather than taking a long break get on with things I've just done the

145
00:14:35,520 --> 00:14:39,760
first thing in kind of web three and I've got a whole bunch of other things we were going to discuss

146
00:14:39,760 --> 00:14:46,880
kind of coming and catalyze stuff that can make an exponential benefit because you know like massively

147
00:14:46,880 --> 00:14:51,600
transformed to purpose here is I want every kid to achieve their potential and give them the tools

148
00:14:51,600 --> 00:14:58,400
to do that and I love you for that because you've been true to that that vision and I know on the

149
00:14:58,400 --> 00:15:04,320
heels of your announcement you've been reached out to by national leaders by corporate you know by

150
00:15:04,400 --> 00:15:10,480
CEOs and major investment groups and you have a lot of opportunity ahead of you so

151
00:15:11,760 --> 00:15:18,560
let's talk about where you want to go next you mentioned publicly and you discussed on our

152
00:15:18,560 --> 00:15:26,800
abundance stage the idea of democratized and decentralized AI let's define that first what

153
00:15:26,800 --> 00:15:33,360
is that why is it important and what do you want to do there yeah I think that when I said I'm going

154
00:15:34,320 --> 00:15:38,880
move to do my part in decentralizing AI people like isn't that just open source you give the

155
00:15:38,880 --> 00:15:45,040
technology right and then anyone can use it but it isn't a decentralizing AI has a few important

156
00:15:45,040 --> 00:15:51,040
components one is availability and accessibility everyone should be able to access this technology

157
00:15:51,040 --> 00:15:55,040
the fruits of labor and there's some very interesting political and other elements around that

158
00:15:55,920 --> 00:16:01,440
number two is the governance of this technology you have centralized governance because the models

159
00:16:01,440 --> 00:16:05,760
are the data there's a recent Databricks thing a model where they show that you have massive

160
00:16:05,760 --> 00:16:10,560
improvements from data we all know that you know who governs the data that teaches your child or

161
00:16:10,560 --> 00:16:16,320
manages your health or runs your government that's an important question I think too few are asking

162
00:16:16,320 --> 00:16:22,480
and we need this transparency and other things like that so accessibility you know you've got the

163
00:16:23,600 --> 00:16:29,920
governance aspect of that and then finally you have how does it all come together is a single

164
00:16:29,920 --> 00:16:36,400
package or is it a modularized infrastructure that people can build on and is available kind

165
00:16:36,400 --> 00:16:41,600
of everywhere you know does it require monoliths and central servers where if it goes down you have

166
00:16:41,600 --> 00:16:47,680
an outage on GPT-4 you're a bit messed up or someone can attack and corrupt it I think that

167
00:16:47,680 --> 00:16:51,600
those are kind of the key elements that I was looking at when I was talking about decentralizing AI

168
00:16:52,240 --> 00:16:59,760
and you know I've got with an infrastructure to do that I hope as well so if you don't mind let's

169
00:16:59,760 --> 00:17:05,920
double click on it even further so you mentioned we don't have long to get there

170
00:17:06,960 --> 00:17:12,800
if that's a true statement why don't we have long to get there and then what does getting

171
00:17:12,800 --> 00:17:20,160
there look like if you had all the capital available and if the right national leaders were

172
00:17:20,160 --> 00:17:27,440
hearing about this because a lot of this is supporting supporting the populace of a nation

173
00:17:28,080 --> 00:17:37,840
um to have AI that serves them versus top down what's it look like two five ten years from now

174
00:17:38,560 --> 00:17:44,720
yeah I think you'll have both proprietary and open source AI and they'll work in combination the

175
00:17:44,720 --> 00:17:49,040
practical example I give is that this AI is like graduates right very talented slightly

176
00:17:49,040 --> 00:17:54,960
enthusiastic graduates and you want those and consultants um but I really have you know on

177
00:17:54,960 --> 00:17:59,520
stage with Matt Friedman last week at 8 360 when we're there he said it's like we discovered this

178
00:17:59,520 --> 00:18:04,960
new concept what do you call it AI Atlantis yes yes with a hundred billion graduates that'll work

179
00:18:04,960 --> 00:18:10,000
for free yes I love that analogy it was it was a brilliant analogy yeah we used to figure out how to

180
00:18:10,000 --> 00:18:16,320
say Atlantis you know um but but there's a few things here first of all is the defaults you know

181
00:18:16,320 --> 00:18:22,560
once a government embraces centralized technology it's very difficult to decentralize it and every

182
00:18:22,560 --> 00:18:32,000
country needs an AI strategy a year ago one year ago was GPT-4 yeah crazy how crazy is that you know

183
00:18:32,000 --> 00:18:39,840
the AI safety summit uh in the UK the king of England came on stage it all came via video call

184
00:18:39,840 --> 00:18:46,000
and he said that this is the biggest thing since fire you know and that was like what six seven

185
00:18:46,000 --> 00:18:53,680
months later where are we going to be in a year yeah I think he took that from uh uh from uh uh

186
00:18:53,680 --> 00:19:00,960
the founder of the CEO of Google um AI is as powerful as fire and electricity yeah yeah yeah

187
00:19:00,960 --> 00:19:05,280
I've heard the same from like Jeff Bezos and a bunch of others you know and not kindle fire

188
00:19:05,280 --> 00:19:11,280
proper fire you know um but then if you think about it norms are going to be set in this next

189
00:19:11,280 --> 00:19:17,600
period like you know I'm in California LA at the moment if you don't set norms on rights for actors

190
00:19:17,600 --> 00:19:22,160
and the movie industry then you could have a massive disruption just occurring this full length

191
00:19:22,160 --> 00:19:28,160
Hollywood features come in a year or two generated you know if you don't have norms around open models

192
00:19:28,160 --> 00:19:32,400
and ownership and governance by the people it'll be top-down governance because governments can't

193
00:19:33,600 --> 00:19:39,040
allow that to be out of control if they don't have a reasonable alternative um and I think the

194
00:19:39,040 --> 00:19:42,480
window is only a year or two because every government must have a strategy by the end of the year

195
00:19:43,200 --> 00:19:47,840
and so I think if you provide them a good solution that has this element of democratic

196
00:19:47,840 --> 00:19:53,680
governance and others that will be immensely beneficial I think also it's urgent because

197
00:19:53,680 --> 00:19:58,560
we have the ability to make a huge difference you know as we kind of may probably discuss later

198
00:19:58,560 --> 00:20:03,360
having all the knowledge of cancer longevity autism at your fingertips you know the technology

199
00:20:03,360 --> 00:20:07,920
for that right now you have the technology that no one ever is ever alone again on those things

200
00:20:08,000 --> 00:20:14,000
or to give every child a superior education literally in a couple of years like there is an

201
00:20:14,000 --> 00:20:19,680
urgency both from there's a small window but also from we must do this now because it can scale and

202
00:20:19,680 --> 00:20:24,480
make that impact we have dreamed of for so long the enabling technology is finally in it's finally

203
00:20:24,480 --> 00:20:28,160
good enough fast enough and cheap enough everybody want to take a short break from our episode to

204
00:20:28,160 --> 00:20:34,000
talk about a company that's very important to me and could actually save your life or the life of

205
00:20:34,000 --> 00:20:39,040
someone that you love company is called fountain life and it's a company I started years ago with

206
00:20:39,040 --> 00:20:45,040
Tony Robbins and a group of very talented physicians you know most of us don't actually know what's

207
00:20:45,040 --> 00:20:51,760
going on inside our body we're all optimists until that day when you have a pain in your side you go

208
00:20:51,760 --> 00:20:55,680
to the physician in the emergency room and they say listen I'm sorry to tell you this but you have

209
00:20:56,320 --> 00:21:02,880
this stage three or four going on and you know it didn't start that morning it probably was a problem

210
00:21:02,880 --> 00:21:09,680
that's been going on for some time but because we never look we don't find out so what we built

211
00:21:09,680 --> 00:21:15,280
at fountain life was the world's most advanced diagnostic centers we have four across the U.S.

212
00:21:15,280 --> 00:21:22,400
today and we're building 20 around the world these centers give you a full body MRI a brain a brain

213
00:21:22,400 --> 00:21:29,920
vasculature an AI enabled coronary CT looking for soft plaque dexa scan a grail blood cancer test

214
00:21:29,920 --> 00:21:37,520
a full executive blood workup it's the most advanced workup you'll ever receive 150 gigabytes of data

215
00:21:37,520 --> 00:21:44,800
that then go to our AIs and our physicians to find any disease at the very beginning when it's

216
00:21:44,800 --> 00:21:49,760
solvable you're gonna find out eventually might as well find out when you can take action fountain

217
00:21:49,760 --> 00:21:54,240
life also has an entire side of therapeutics we look around the world for the most advanced

218
00:21:54,240 --> 00:22:01,280
therapeutics that can add 10 20 healthy years to your life and we provide them to you at our centers

219
00:22:01,280 --> 00:22:09,120
so if this is of interest to you please go and check it out go to fountainlife.com backslash

220
00:22:09,120 --> 00:22:16,160
peter when tony and i wrote our new york times bestseller life force we had 30 000 people reached

221
00:22:16,160 --> 00:22:22,160
out to us for fountain life memberships if you go to fountainlife.com backslash peter we'll put you

222
00:22:22,160 --> 00:22:28,720
to the top of the list really it's something that is for me one of the most important things i offer

223
00:22:28,720 --> 00:22:36,400
my entire family the CEOs of my companies my friends it's a chance to really add decades onto

224
00:22:36,400 --> 00:22:42,240
our healthy lifespans go to fountainlife.com backslash peter it's one of the most important

225
00:22:42,240 --> 00:22:47,520
things i can offer to you as one of my listeners all right let's go back to our episode so the

226
00:22:47,600 --> 00:22:53,360
let's talk about the objective function of democratized and decentralized AI is it that the

227
00:22:53,360 --> 00:23:00,880
compute is resonant in countries around the world is it that the models are owned by the

228
00:23:00,880 --> 00:23:08,640
citizens of the world is it that data is owned and how do you get there from here i think that

229
00:23:09,200 --> 00:23:14,320
you can think of the supercomputers like universities you don't need many universities

230
00:23:14,320 --> 00:23:18,320
honestly if someone's building good quality models that's one of the things that says stability

231
00:23:18,320 --> 00:23:22,640
and we did the hard task we could just stuck with image we said no we're gonna have the best 3d

232
00:23:22,640 --> 00:23:29,280
image audio biomedical all these models and no one else managed that apart from open ai to agree

233
00:23:29,280 --> 00:23:34,480
in fact i think we have more modalities than open ai again kind of what i kind of describe this

234
00:23:34,480 --> 00:23:42,000
is accessibility and governance and a few of these other factors so i think what it means is that

235
00:23:42,000 --> 00:23:46,800
this technology is available to everyone but you see now that you don't assume you need giant

236
00:23:46,800 --> 00:23:52,160
supercomputers to even run it you know we showed you a language model running on a laptop stable

237
00:23:52,160 --> 00:23:58,320
lm2 will run on a gigabyte on a mac macare faster than you can read you know we're writing some poems

238
00:23:58,320 --> 00:24:04,960
various things you know um we see stable diffusion now at 300 images a second or consumer graphics

239
00:24:04,960 --> 00:24:11,360
card our video model was like five gigabytes of v-round this really changes the equation because

240
00:24:11,360 --> 00:24:16,560
in web 2 all the intelligence was centralized on these giant servers and big data now you have

241
00:24:16,560 --> 00:24:21,840
big supercomputers i think you'll need less with better data training these graduates that can go

242
00:24:21,840 --> 00:24:26,400
out and customize to each country but they must reflect the culture of that country like the

243
00:24:26,400 --> 00:24:31,680
japanese stable diffusion model we had if you typed in salary man it gave you a very sad person

244
00:24:31,680 --> 00:24:36,960
versus the base model giving you a very happy person right so you must have graduates that

245
00:24:36,960 --> 00:24:42,880
reflect the local culture and then reflect the local knowledge and then global models again

246
00:24:44,000 --> 00:24:50,720
that reflect our global knowledge and can be answered by anyone but who decides what goes

247
00:24:50,720 --> 00:24:56,640
in that these are some very important questions and who vouchers for the quality as well what's

248
00:24:56,640 --> 00:25:03,040
your advice to a national leader because you know we're now starting to see ministers of AI

249
00:25:03,040 --> 00:25:09,600
in different nation states and what what's your advice to them right now in this area

250
00:25:10,320 --> 00:25:16,240
i think my advice to them would be to start collecting the data sets that they would teach

251
00:25:16,800 --> 00:25:21,360
a graduate that was very smart through school and kind of other things this is national broadcast

252
00:25:21,360 --> 00:25:26,800
data this is the curriculum this is their accounting legal and others and note that those

253
00:25:26,800 --> 00:25:33,920
data sets are infrastructure they will enable the local populist and others to create these models

254
00:25:33,920 --> 00:25:38,640
because models are just data wrapped in algorithms with a bit of compute that's the recipe compute

255
00:25:38,640 --> 00:25:45,040
algorithms and data and it's not going to be as hard as you think to train these models but you

256
00:25:45,040 --> 00:25:50,000
have to build them to get standard so by the end of next year probably year after i would estimate

257
00:25:50,000 --> 00:25:57,280
that a longer 7tb model or a stable diffusion model so these are two leading models in image

258
00:25:57,280 --> 00:26:03,040
and language will cost about under ten thousand dollars probably even one thousand dollars to

259
00:26:03,040 --> 00:26:07,600
train and then it comes all about the data and then it becomes about the standards

260
00:26:09,360 --> 00:26:16,400
you know it's it's it it's interesting um there is so much knowledge in the world that will

261
00:26:16,800 --> 00:26:23,040
vaporize sublimate over the decade ahead as people die you know cultural data locked up in

262
00:26:23,040 --> 00:26:28,640
people's minds and stories and so forth that's never been recorded it's an interesting time to

263
00:26:28,640 --> 00:26:36,480
actually capture that data and permanently store it into the national models yeah and again i think

264
00:26:36,480 --> 00:26:42,240
people over focus on the models versus the data sets i mean is data set yeah yeah yeah with the

265
00:26:43,200 --> 00:26:48,880
exponential compute you can recalibrate and improve the data as well so right now a lot of the

266
00:26:48,880 --> 00:26:54,800
improvements in models are actually synthetically improving data and data quality as you said there's

267
00:26:54,800 --> 00:26:59,760
so much that can be lost but now we can actually capture this and the concepts and the other

268
00:26:59,760 --> 00:27:05,280
guidance and have crosschecks like you can deconstruct laws you know you can translate

269
00:27:05,280 --> 00:27:11,600
between contexts you can make expert information available to everyone because again you have

270
00:27:11,600 --> 00:27:17,520
this new continent of ai around us and all these graduates seem to be specialists that are on your

271
00:27:17,520 --> 00:27:24,960
phone and that's incredibly democratizing you know um because otherwise the knowledge is

272
00:27:24,960 --> 00:27:31,200
throughout history knowledge has always been gate kept always i want to get to uh health

273
00:27:31,200 --> 00:27:38,320
and education next but before we go there i know you were meeting with a mutual friend

274
00:27:38,320 --> 00:27:46,000
jewels or back the other day um and instability announced a deal with otoi endeavor and render

275
00:27:46,000 --> 00:27:52,160
network um are you still an advisor to that venture you know this is part of the whole thing

276
00:27:52,160 --> 00:27:59,200
it's the first of many web three kind of elements there i think web three is 95 percent let's say

277
00:27:59,200 --> 00:28:06,800
90 percent i'll be generous um speculative and rubbish but there is that five ten percent of

278
00:28:06,800 --> 00:28:12,320
genuine people that would be thinking about questions of governance coordination and others

279
00:28:12,320 --> 00:28:17,520
and have built things that are proper so otoi is the bridge to the creative industry you know

280
00:28:17,520 --> 00:28:22,880
that's why we're with arie manuel and eric schmidt and others are on the board um and the render

281
00:28:22,880 --> 00:28:29,520
network is a million gpus largely from creative professionals that are available and so the first

282
00:28:29,520 --> 00:28:35,840
thing i announced there i was going to be initial 10 million gaps 250 million of distributed compute

283
00:28:35,840 --> 00:28:42,880
to create the best 3d data sets like stability we funded and worked with allen institution others on

284
00:28:42,880 --> 00:28:48,320
objeverse excel which is 10 million high quality 3d assets we're going to a billion distributed

285
00:28:48,320 --> 00:28:53,280
you don't need giant supercomputers but then that is a community good that is owned by the people of

286
00:28:53,280 --> 00:28:59,680
the network and accessible to non-academic and others as well why because you need high quality

287
00:28:59,680 --> 00:29:04,240
assets to create better 3d models we have a new 3d model tried by us all that can generate a 3d

288
00:29:04,240 --> 00:29:09,840
image from a 2d image and not go five seconds and that 3d model feeds into better 3d assets

289
00:29:10,480 --> 00:29:14,560
and then what does that mean it means we're heading towards the holodeck without the data

290
00:29:14,560 --> 00:29:19,760
you're not going to get there and that joules joules wants the holodeck for sure yeah so you

291
00:29:19,760 --> 00:29:24,160
know jules and i are the same phase of that you know and you're not going to get there without

292
00:29:24,160 --> 00:29:30,160
again a commons of data that can train the graduates that then become specialized with star trek or

293
00:29:30,800 --> 00:29:35,600
you know star wars or any of these other ips and then also setting standards around

294
00:29:36,800 --> 00:29:43,040
monetization ip rights all sorts of other things and so a network like render is really good for

295
00:29:43,040 --> 00:29:47,120
that but you know i've been talking to a lot of people in web three about the different elements

296
00:29:47,120 --> 00:29:52,160
of the stack because what i basically see is that we have the opportunity to build

297
00:29:52,960 --> 00:29:59,440
almost a human operating system models and data sets for every nation every sector coordinated

298
00:29:59,440 --> 00:30:04,720
through proper web three principles again not speculative tokens or anything like that

299
00:30:05,600 --> 00:30:10,960
you know making it so that every child in the world or adult can create anything they can imagine

300
00:30:11,840 --> 00:30:16,400
they can be protected against the harms and they have access to the right information at the right

301
00:30:16,400 --> 00:30:23,760
time to thrive and again that's infrastructure for everyone it's a common good access to gpu's has

302
00:30:23,760 --> 00:30:31,920
been sort of the limited fuel um do you think decentralized gpu structures like render is part

303
00:30:31,920 --> 00:30:38,560
of that future is an important part of that future i think right now it's far more efficient to train

304
00:30:38,560 --> 00:30:43,520
models on these again big supercomputers the university but the rate of exponential growth

305
00:30:43,520 --> 00:30:49,920
there again is insane last year to train lama two cost ten million dollars in a year or cost

306
00:30:49,920 --> 00:30:55,760
ten thousand dollars a thousand times improvement from algorithms data supercomputes speeds

307
00:30:57,040 --> 00:31:02,160
and that's crazy if you think about it right um so i think this will be the limiting factor i think

308
00:31:02,160 --> 00:31:07,200
the gpu overhang four language models probably lasts until the end of the year but then there's

309
00:31:07,200 --> 00:31:13,600
plentiful supply because what you have is nvidia makes amazing gpu's at an 83 or 87 percent margin

310
00:31:13,840 --> 00:31:20,560
right but the actual calculations aren't complicated like we took intel gpu's and we ran the stable

311
00:31:20,560 --> 00:31:24,240
diffusion three diffusion transformer training so this is the same technology that's used in

312
00:31:24,240 --> 00:31:28,720
saura and stable diffusion three is multimodal so it can train saura models with enough compute

313
00:31:29,440 --> 00:31:33,200
and i think us and them are the only people kind of doing this so i think maybe pixart as well

314
00:31:34,560 --> 00:31:42,000
and then it ran faster than the intel gpu's than the nvidia gpu's but we know that it can run even

315
00:31:42,000 --> 00:31:46,640
faster because it's not optimized for either and it's still running fast so what you'll see

316
00:31:46,640 --> 00:31:52,240
is a commoditization of the hardware once the architect just gets stabilized because gpu is

317
00:31:52,240 --> 00:31:58,000
just a research artifact still diffusion was just a research artifact you're not going to

318
00:31:58,000 --> 00:32:04,160
engineer in phase yet and you've got to the point whereby this runs on macbooks it runs on other

319
00:32:04,160 --> 00:32:10,080
things so i think it's a short term phenomenon of the next year because people were taking a point

320
00:32:10,080 --> 00:32:15,440
in time and extrapolating without taking into account efficiencies optimizations

321
00:32:16,240 --> 00:32:21,040
and the fact that models that work on the edge and it can go to your private data will be more

322
00:32:21,040 --> 00:32:26,560
impactful than generalized intelligence everyone's over indexing on generalized intelligence and

323
00:32:26,560 --> 00:32:35,760
building ai god versus amplified human intelligence shall we say before we leave stability it's now

324
00:32:35,760 --> 00:32:44,320
in the hands of the chair and your past cto what's what do you imagine the future of stability

325
00:32:44,320 --> 00:32:48,960
is going to be going forward i know that you're not involved anymore it's under different leadership

326
00:32:48,960 --> 00:32:53,280
and there's a what's your advice to them or where do you think they're going to go

327
00:32:54,400 --> 00:32:58,320
you know i can do the very basic advice i didn't want any conflicts or anything because

328
00:32:58,320 --> 00:33:03,680
i'll be setting up lots of new companies and you know being a founder and a shareholder

329
00:33:03,760 --> 00:33:08,320
and against stability i'm a founder shareholder in fact you're still the majority shareholder

330
00:33:08,320 --> 00:33:13,840
i think i said right now yeah just about just about yeah that will change i'm sure new money

331
00:33:13,840 --> 00:33:19,440
will come in like we saw co here yesterday and i think it's very little 20 million of revenue

332
00:33:19,440 --> 00:33:24,160
run rate they're raising at five billion might as well it's increasing yes yeah with the right

333
00:33:24,160 --> 00:33:29,760
leadership i think that um it can again have an amazing part to play in media and that's what i've

334
00:33:29,760 --> 00:33:35,040
suggested to it um and again there's a great team that continues to ship great models so last week

335
00:33:35,040 --> 00:33:40,000
there was an amazing code model next week amazing language audio and other models are coming out so

336
00:33:40,000 --> 00:33:46,240
you know you continue shipping and great products around that too um so that was kind of my advice

337
00:33:46,240 --> 00:33:51,920
to them let's focus on media and take that forward but you know i'm not the expert on the business

338
00:33:51,920 --> 00:33:58,560
side of things i did the best i could my expertise on setting this up and taking it 0 to 10 and uh

339
00:33:58,560 --> 00:34:08,240
yeah is zero zero one is is definitely um a role that that you've played here and allow it allow

340
00:34:08,240 --> 00:34:15,280
someone else to take it the rest of the way uh but the area that i know actually there's something

341
00:34:15,280 --> 00:34:19,200
i wanted to kind of discuss here that thing's quite important please or again the founder's

342
00:34:19,200 --> 00:34:25,360
listing and the moonshot companies um there is an imbalance of power when you have

343
00:34:25,840 --> 00:34:33,040
very visionary highly competent leaders there like what i found its stability is there everyone

344
00:34:33,040 --> 00:34:37,040
would be waiting for me no matter how competent because i was the one that could see around the

345
00:34:37,040 --> 00:34:42,480
corners and i was a bit good at everything even if i hired people that built billion dollar startups

346
00:34:42,480 --> 00:34:48,080
or were leaders and research at google or kind of whatever um because you have an outside thing

347
00:34:48,080 --> 00:34:52,640
this is what kind of better says you have to speak last in some cases and some people in eating

348
00:34:53,360 --> 00:34:58,240
because otherwise everyone just does everything you say and they also wait on you now what i

349
00:34:58,240 --> 00:35:02,400
find and what i told the team is that you're flat as a power dynamic you're all on the same page

350
00:35:02,400 --> 00:35:07,040
you're all kind of relatively equal owners and it'll be interesting to see how it evolves from that

351
00:35:07,680 --> 00:35:11,920
given that there's actually a business and again i think this is something that you probably had a

352
00:35:11,920 --> 00:35:18,400
challenge within other founders here whereby they put more on your plate because you are so visionary

353
00:35:18,400 --> 00:35:23,520
and because you're like up there in the future and they're always waiting on you so you was like

354
00:35:23,520 --> 00:35:27,360
well my schedule is completely packed my schedule now is actually quite free which is also quite

355
00:35:27,360 --> 00:35:31,760
nice i've had a chance to speak with you every day for the last few days so that's been a pleasure

356
00:35:31,760 --> 00:35:39,360
to have extra time on your schedule you know so so we do have a world of uh visionary founder led

357
00:35:40,000 --> 00:35:45,360
CEO companies right so you've got musk and you've got bezos historically and you had steve jobs and

358
00:35:45,360 --> 00:35:52,480
you have and and that's both powerful and dangerous the the power is the ability for that

359
00:35:52,480 --> 00:35:58,560
because we don't ever have a company that is pre-existing a new CEO comes in and has the same

360
00:36:01,920 --> 00:36:06,560
both hootspa and and also the power of their of their vision

361
00:36:07,280 --> 00:36:07,760
um

362
00:36:09,600 --> 00:36:16,800
the danger there your concern you're you're saying is not not allowing your team to step

363
00:36:16,800 --> 00:36:22,880
up with their own vision or being over overly indexed on your on your vision yeah i think that

364
00:36:22,880 --> 00:36:27,840
that can be the issue and that's why i wanted stability to again reach the point of spread

365
00:36:27,840 --> 00:36:33,440
and revenue rate increase and other things before i do anything um and i felt again this

366
00:36:33,440 --> 00:36:38,560
external pressure in that if nobody or there's very few people in the world actually thinking

367
00:36:38,560 --> 00:36:42,720
properly about governance and spread and others and a very small window giving the pace of this

368
00:36:42,720 --> 00:36:47,840
to make a difference in the dent i believed i had a reasonable approach to that but i couldn't

369
00:36:47,840 --> 00:36:53,280
while remaining CEO of this company and again it's a pretty unique scenario because you've never

370
00:36:53,280 --> 00:36:59,600
seen a sector move this fast that has such wide-reaching human implications and regressively

371
00:36:59,680 --> 00:37:07,360
there's too few people i think with the right alignment and approach in this area i've been

372
00:37:07,360 --> 00:37:12,800
very disappointed like usually what happens is you have power maximization equations and this

373
00:37:12,800 --> 00:37:19,200
is what we're seeing from the industry consolidation how many people want to genuinely bring this

374
00:37:19,200 --> 00:37:26,080
technology to kids in nigeria or to the global south or to help those leaders build their own

375
00:37:26,080 --> 00:37:30,560
models you know and believe also in a positive some game that was actually my biggest surprise

376
00:37:30,560 --> 00:37:37,280
from the discussions to lincoln valley almost entirely they all believe in uh flat or negative

377
00:37:37,280 --> 00:37:42,560
some you know zero some or negative some things where there has to be a winner everyone's a winner

378
00:37:42,560 --> 00:37:49,840
in this and again i was just very disappointed seeing that i've been asking for you for a while to

379
00:37:49,840 --> 00:37:54,720
write and distribute your vision white paper because i've heard you describe it in detail

380
00:37:54,720 --> 00:37:59,040
and it's brilliant and i still hope that the world will see it soon enough everybody you

381
00:37:59,040 --> 00:38:03,120
want to take a break from our episode to tell you about an amazing company on a mission to

382
00:38:03,120 --> 00:38:09,600
prevent and reverse chronic disease by decoding your biology the company is called viome and

383
00:38:09,600 --> 00:38:15,840
they offer cutting-edge tests and personalized products that help you optimize your gut microbiome

384
00:38:15,840 --> 00:38:21,360
your oral microbiome and your cellular health as you probably know your microbiome is a collection

385
00:38:21,360 --> 00:38:27,680
of trillions of microbes that live in your gut and mouth and these microbiomes influence everything

386
00:38:27,680 --> 00:38:34,560
your digestion immunity mood weight and many other aspects of your health but not all microbes are

387
00:38:34,560 --> 00:38:40,800
good for you some can cause inflammation toxins and actually lead to chronic diseases like diabetes

388
00:38:40,800 --> 00:38:48,640
heart disease obesity and even cancer viome uses advanced mRNA technology and ai to analyze your

389
00:38:48,640 --> 00:38:54,400
microbes and your cells and give you personalized nutrition recommendations and products designed

390
00:38:54,400 --> 00:39:00,080
specifically for your genetics specifically for your biology you can choose from different tests

391
00:39:00,080 --> 00:39:05,760
depending on your goals and needs ranging from improving your gut health your oral health cellular

392
00:39:05,760 --> 00:39:11,360
function or all of them i've been using viome for the past three years i can tell you that it has

393
00:39:11,360 --> 00:39:17,520
made a huge difference in my health and because the data they collect and the ai engine they've built

394
00:39:17,520 --> 00:39:23,840
it gets better every single day i love getting health scores and seeing how my diet and lifestyle

395
00:39:23,840 --> 00:39:29,760
affects my microbiome and my cells and i love getting precision supplements and probiotics tailored

396
00:39:29,760 --> 00:39:34,800
for my specific needs if you want to join me on this journey of discovery and improve your health

397
00:39:34,800 --> 00:39:41,840
from the inside out viome has a special offer for you for a limited time you can get up to 40% off

398
00:39:42,480 --> 00:39:49,440
any viome test using the code moonshots just go to viome.com backslash moonshots and order your

399
00:39:49,440 --> 00:39:55,360
test today trust me you won't regret it all right let's go back to our episode before we jump into

400
00:39:55,360 --> 00:39:59,680
into health and education let's talk about governance a second because we've seen governance

401
00:40:02,160 --> 00:40:06,720
complicate this what is the right governance structure for this super powerful technology

402
00:40:07,280 --> 00:40:10,960
we have representation of democracy that i think can be improved by this like i don't think

403
00:40:10,960 --> 00:40:15,760
democracy survives this technology it's current for it will either improve or it'll end

404
00:40:16,640 --> 00:40:22,720
i don't see anything else like yesterday though what does end what does end mean here a benign

405
00:40:22,720 --> 00:40:29,840
dictatorship a driven by an ai overlord yeah like yesterday there was a announcement of an app called

406
00:40:29,840 --> 00:40:36,000
Hume which had emotionally intelligent speech and they can understand your emotions and talk with

407
00:40:36,640 --> 00:40:40,800
you know i have to discuss this yes you know where that's going right it's very powerful

408
00:40:41,600 --> 00:40:46,880
it's incredibly powerful and governments have a tendency i mean if there's no government but

409
00:40:46,880 --> 00:40:52,880
say it say it here it's important for you to state what what it means because we've discussed it but

410
00:40:52,880 --> 00:40:59,280
help people here be ready for this and democracy is all about representation and you see the

411
00:40:59,280 --> 00:41:03,680
questions of deep fakes and things speech is one of the most impactful elements there but now you

412
00:41:03,680 --> 00:41:11,920
can't believe anything you see here everything so one path that we have is a 1984 on steroids

413
00:41:12,720 --> 00:41:17,920
panoptica you know where life is gamified and you listen to whatever the government says and

414
00:41:17,920 --> 00:41:21,760
they're incredibly convincing and you're happy and you've always been happy and you've always been

415
00:41:21,760 --> 00:41:27,600
at war with eurasia you know propaganda on steroids the other path that you have is things like

416
00:41:27,600 --> 00:41:33,280
citizen assemblies consults of democracy the ability to take right now you can take any of

417
00:41:33,280 --> 00:41:40,320
the bills in congress and completely deconstruct them and find what the motivations are you know

418
00:41:40,320 --> 00:41:46,080
you can check laws against the constitution in a second seconds this is an incredibly powerful

419
00:41:46,080 --> 00:41:51,040
empowering technology from a democratic perspective so i see two routes unfortunately because i think

420
00:41:51,040 --> 00:41:57,040
that once the right thing goes it goes really fast centralized government control increasing

421
00:41:57,040 --> 00:42:02,080
because the government want to protect themselves as an organization and you know every party says

422
00:42:02,080 --> 00:42:06,720
the other party's crap we've seen the increasing polarization in america already

423
00:42:06,720 --> 00:42:10,400
and you know fundamentally come on you can do better than those two leaders that are currently

424
00:42:10,400 --> 00:42:16,400
competing i'm saying this is clear our system is sclerotic across this like democracy is the worst

425
00:42:16,400 --> 00:42:21,760
of all systems except for everyone else you can have a better democracy where it's actually

426
00:42:21,760 --> 00:42:28,880
representative and empowers the people or we will have the end of democracy where it is in 1984

427
00:42:28,880 --> 00:42:33,440
panopticon in my opinion because the momentum will go then of course you'll start using this

428
00:42:33,440 --> 00:42:38,960
technology you're already seeing it being used but not at scale and not intelligently yet which is

429
00:42:38,960 --> 00:42:47,040
scary i think we finally have the technology for a direct democracy versus a representative

430
00:42:47,040 --> 00:42:55,120
democracy right where i can have my my desires directly represented on any specific law or

431
00:42:56,080 --> 00:43:05,920
but i think the point you've made before is that speech if you look back to everybody from

432
00:43:05,920 --> 00:43:15,360
hitler to some of the most persuasive politicians is is a powerful tool and ai can become the most

433
00:43:15,360 --> 00:43:20,960
persuasive speaker out there it can take anyone's speech to make it far more persuasive like i think

434
00:43:21,040 --> 00:43:26,080
my voice is a bit whiny i can remove the wine you know i can go in a very polished british accent

435
00:43:26,080 --> 00:43:30,560
and other things like that right you know must fight them on the hills and the harriers and

436
00:43:30,560 --> 00:43:35,680
whatever public speaking needs to be different someone took hitler's speeches and put them

437
00:43:35,680 --> 00:43:41,440
through an ai and took them into english because when we're not in the german context and we listen

438
00:43:41,440 --> 00:43:45,920
to them it sounds like he's shouting like well that's crazy thing you hear him in english it is

439
00:43:45,920 --> 00:43:52,000
very different in his own voice just like someone took javier mele's one in the united nations

440
00:43:52,000 --> 00:43:57,440
and put him into english again he sounds a bit sharp how shouty but then he sounds very reasonable

441
00:43:57,440 --> 00:44:03,120
when it's in english and you can take the phenomes of obama's best speech and a bit of trumpinism

442
00:44:03,120 --> 00:44:08,080
and a bit of churchill and you'll have full modulation wave control over all of this people

443
00:44:08,080 --> 00:44:14,000
are already using this technology that everyone should have a passcode with their loved ones because

444
00:44:14,000 --> 00:44:19,200
people are getting calls from their mother saying help i'm in an emergency and you just

445
00:44:19,200 --> 00:44:24,560
send money right now and you cannot tell it and it pulls at the emotional strengths and if you

446
00:44:24,560 --> 00:44:29,600
look at something like us radio and you know one side of the political divide is taken over imagine

447
00:44:29,600 --> 00:44:35,840
if you're hearing optimized speech every single day that will have a huge impact and then they

448
00:44:35,840 --> 00:44:41,040
control the visuals and they control the other things we're not set up for defenses yeah if it's

449
00:44:41,360 --> 00:44:47,200
optimized speech for you specifically for you right for the kids you have the age group they

450
00:44:47,200 --> 00:44:52,720
have where you live your historical background and so forth and end of one persuasive speech

451
00:44:53,360 --> 00:45:01,040
coming at you the brain is not set up for defenses we're not and you know we take this as an example

452
00:45:01,040 --> 00:45:06,320
of the youtube algorithm like youtube as an organization is not an evil organization but

453
00:45:06,320 --> 00:45:10,960
it's an organization optimized for engagement which optimized for more extreme content so this

454
00:45:10,960 --> 00:45:17,200
is a dark place in youtube which is an optimized for ices the ices video spread viral i don't know

455
00:45:17,200 --> 00:45:23,040
sometimes viral is good sometimes viral is bad that one was bad because it was extreme and they

456
00:45:23,040 --> 00:45:29,680
didn't understand why and if you look at it our two of our largest germ to bear companies are google

457
00:45:30,240 --> 00:45:36,720
and meta and their business is advertising their business manipulation and they are both a moral

458
00:45:36,720 --> 00:45:43,280
companies because why would you expect a company to have morality our governments are also a moral

459
00:45:43,920 --> 00:45:48,640
and again you can view these things as slow dumb ai so you can see the way they will optimize

460
00:45:48,640 --> 00:45:53,520
unless we do something about it and they will have full control like again you put on your vision

461
00:45:53,600 --> 00:46:00,960
pro headset with your spatial audio that is full sensory control not full but you know what i mean

462
00:46:02,000 --> 00:46:09,680
full immersion full immersion full immersion and so we have to be aware of this and there's

463
00:46:09,680 --> 00:46:13,840
obviously other tools that can be used like in the wake of the ab spring you know governments

464
00:46:13,840 --> 00:46:17,840
targeted everyone else in social media we can do that on a high-end hyper-personalized basis

465
00:46:18,720 --> 00:46:27,200
like we need to set some defaults and standards here to protect democracy but again why democracy

466
00:46:28,240 --> 00:46:31,840
we're not really trying to protect democracy you know again people have different definition

467
00:46:31,840 --> 00:46:37,840
that what we're trying to protect is individual liberty freedom and agency education should be

468
00:46:37,840 --> 00:46:43,200
about enhancing the education of every child it's not you know healthcare is sick care our government

469
00:46:43,200 --> 00:46:47,360
should uplift us but how many people believe our governments do that rather than put us down

470
00:46:47,920 --> 00:46:53,680
because they couldn't encapsulate and cater to the brilliance of each individual

471
00:46:55,680 --> 00:46:59,120
because they didn't have the tools until now so that's why i said which way one man

472
00:46:59,760 --> 00:47:06,320
yes which way an agency or massive control these are the two ways do we control the technology

473
00:47:06,320 --> 00:47:12,800
or do these organizations control the technology that controls us you know when we were on the stage

474
00:47:12,880 --> 00:47:18,320
at at the abundant summit we talked about a future of digital superintelligence right

475
00:47:18,320 --> 00:47:26,480
and a future in which we've got AI a billion times more capable than a human which looking at it

476
00:47:26,480 --> 00:47:36,800
just from a ratio of neurons is the ratio of a hamster to a human um yep uh do you believe

477
00:47:36,800 --> 00:47:44,480
that someday we could have a a benign superintelligence that is supporting humanity

478
00:47:45,680 --> 00:47:49,280
yes and i believe that it should be a collective intelligence

479
00:47:50,560 --> 00:47:57,040
that is made up of amplified human intelligence is amplifying all of us pilots that contain our

480
00:47:57,040 --> 00:48:02,160
collective knowledge and culture and the best of us and datasets that are built from helping and

481
00:48:02,160 --> 00:48:09,360
augmenting us versus a collected intelligence and agi that is top down and designed to effectively

482
00:48:09,360 --> 00:48:14,400
control us again if you look at open ai statements on the rote agi they say this technology will end

483
00:48:14,400 --> 00:48:21,520
democracy and capitalism and maybe kill us all i don't like that i remember i remember seeing that

484
00:48:21,520 --> 00:48:27,520
you you you texted me you said read this does this sound the same as it does to me yeah so what

485
00:48:27,520 --> 00:48:31,680
i'd prefer instead is for this to be distributed like if you have datasets formations that are

486
00:48:31,680 --> 00:48:36,800
built on enhancing the capability of the nation that reflect the local cultures and you push for

487
00:48:36,800 --> 00:48:41,680
data transparency on models which i believe we must have you know especially language models

488
00:48:41,680 --> 00:48:46,480
then you're more likely to have a positive thing and again the human collective can achieve anything

489
00:48:46,480 --> 00:48:51,360
from splitting the atom to go into space if we put our minds to it but we have lacked in

490
00:48:51,360 --> 00:48:57,680
coordination mechanisms they've not been good enough so if you create the human colossus and

491
00:48:57,680 --> 00:49:03,520
every single person has an ai that's just looking at them to enhance their potential and coordination

492
00:49:03,520 --> 00:49:10,000
ai's that is a far more positive view of the future and that is the agi that is a general

493
00:49:10,000 --> 00:49:14,720
intelligence that's the hive mind general intelligence not a bog style hive mind but one

494
00:49:14,720 --> 00:49:19,840
that's really thinking again every child should achieve their potential versus this embodied

495
00:49:19,840 --> 00:49:23,680
concept of an agi that's a very western concept and you see that as well like you know we look at

496
00:49:23,680 --> 00:49:29,200
the japanese concept of a robot the robot is your equal and your helper you look at the western

497
00:49:29,200 --> 00:49:34,400
concept of the robot it's terminator and spinet and all of that and again i think this is again

498
00:49:34,400 --> 00:49:38,800
where cultural norms become very interesting and what do we want to build we want to build ai god

499
00:49:39,520 --> 00:49:43,520
or do we want to build that ai helper that helps us and we help it you know

500
00:49:44,000 --> 00:49:53,680
um those listening now i mean you can see umad's brilliance and why i'm so enamored with the way

501
00:49:53,680 --> 00:49:59,360
you think about this because it's there are very few individuals who are looking at this from

502
00:49:59,920 --> 00:50:03,760
an objective function of what's best for humanity what's best for every nation state

503
00:50:04,880 --> 00:50:11,680
out there let's talk about your going forward future are you going to build something in the

504
00:50:12,480 --> 00:50:21,120
in the decentralized side of of ai the democratized size of ai is there a company there or a fund in

505
00:50:21,120 --> 00:50:27,040
your future for that yeah so uh you're not doing the white paper finally getting there with a bit

506
00:50:27,040 --> 00:50:33,120
of help from ai uh i can't i can't i can't wait to uh to help uh broadcast that white paper

507
00:50:33,840 --> 00:50:39,440
yeah but look i think the basic thing is this what i want to do is set up a ai champion in every

508
00:50:39,440 --> 00:50:44,400
nation with the brightest people of each nation working with the organization of each nation

509
00:50:44,400 --> 00:50:48,560
to help guide them through this next period because there will be massive job displacement

510
00:50:48,560 --> 00:50:53,760
from the graduates going massive uplifts and productivity from the technology being implemented

511
00:50:53,760 --> 00:50:59,920
and again that organization can help govern and create these data sets and these models that

512
00:50:59,920 --> 00:51:05,120
are so important and every nation should have that but then i also believe that every sector

513
00:51:05,120 --> 00:51:09,760
should have a genitive ai first infrastructure company that builds this and helps the healthcare

514
00:51:09,760 --> 00:51:15,200
companies finance companies and others through that and to coordinate all of that you need to have

515
00:51:15,200 --> 00:51:20,000
a web three type protocol what is the protocol for intelligence so what what is a web what is a

516
00:51:20,000 --> 00:51:25,200
web three type protocol to find that for folks listening again people talk about web three it's

517
00:51:25,200 --> 00:51:30,320
not about the tokens or the meme coins or anything like that what about three protocols is that

518
00:51:30,400 --> 00:51:33,920
everyone should have like ai's first of all if i'm going to have bank accounts

519
00:51:35,440 --> 00:51:40,480
they're going to need some way to pay each other or exchange value and again web three is a lot

520
00:51:40,480 --> 00:51:45,600
of work in that there needs to be some sort of identity attribution and other format because

521
00:51:45,600 --> 00:51:51,840
you'll have this mass influx of information and so again web three concepts are very useful there

522
00:51:51,840 --> 00:51:57,120
there needs to be an identity concept because you'll have real and digital people web three concepts

523
00:51:57,120 --> 00:52:02,960
are very useful there so data access station all these other things verifiability so when i look at

524
00:52:02,960 --> 00:52:08,560
it if you've got sectorally my plan is to launch a company for every major sector and we can talk

525
00:52:08,560 --> 00:52:12,240
about health and education and bring the smartest people in the world to solve that challenge of

526
00:52:12,240 --> 00:52:16,880
the infrastructure for the future every nation but you need to have some sort of coordinating

527
00:52:16,880 --> 00:52:22,800
protocol for all of that that becomes a standard and that's the substrate for this amplified human

528
00:52:22,800 --> 00:52:28,480
collective intelligence and and is that where you want to play and focus your energy next

529
00:52:29,200 --> 00:52:34,000
yeah it's setting up these organizations and bringing the brightest smartest people that

530
00:52:34,000 --> 00:52:38,240
really want to make a difference there because there's massive network effects in doing this

531
00:52:38,240 --> 00:52:42,080
but again i just need to be the founder and architect i don't want to run the day to day of

532
00:52:42,080 --> 00:52:48,560
any of these things um and then because the the most scarce talent that there's three types of

533
00:52:48,560 --> 00:52:55,040
capital as i view it there's financial capital human capital and political capital and in order

534
00:52:55,040 --> 00:52:59,440
to effect change in the world you actually need all three but the financial capital actually comes

535
00:52:59,440 --> 00:53:05,120
with the people capital and the political capital and the smartest people in the world in every sector

536
00:53:05,120 --> 00:53:10,960
from health care to education to finance to agriculture almost all believe that gentry

537
00:53:10,960 --> 00:53:15,760
is the biggest thing they've ever seen in the last year everyone's asking you all the smartest

538
00:53:15,760 --> 00:53:19,920
people peter what's next right and you know many of the smartest people in the world so i want to

539
00:53:19,920 --> 00:53:25,760
create organizations that they can come the chefs and the cooks the thinkers and the doers and think

540
00:53:25,760 --> 00:53:31,040
what is the future of finance that's the future of education and then the national champions that

541
00:53:31,040 --> 00:53:36,640
should be owned by the people of each country become the distribution for the amazing infrastructure

542
00:53:36,640 --> 00:53:41,440
they built and there's a vice a nice kind of vice versa but then again you need the coordination

543
00:53:41,440 --> 00:53:46,000
function so i'm trying to bring together people in each of these and you know there'll be public

544
00:53:46,000 --> 00:53:49,760
calls and things like that to build that infrastructure the future because as mentioned

545
00:53:50,560 --> 00:53:54,320
ai isn't infrastructure but it should be i mean maybe it's the rocket ship of the mind right

546
00:53:55,840 --> 00:54:00,320
i love that i love that analogy my friend it is the most important infrastructure

547
00:54:00,320 --> 00:54:05,040
that humanity will have going forward across everything it does and i look forward to helping

548
00:54:05,040 --> 00:54:10,960
you build it exciting right like again i think one of the things i got i got lots of messages

549
00:54:11,040 --> 00:54:15,920
that like i'm so sorry for your loss it was like my dog had died after i left to see you

550
00:54:16,560 --> 00:54:22,800
i was like what is that you know they're like i was nice it's nice that people care right but i'm

551
00:54:22,800 --> 00:54:27,920
generally excited about what's next like you know again it was like staring into the abyss

552
00:54:27,920 --> 00:54:34,160
and chewing gloss every single day and that's not what i'm best at or i could have the most impact

553
00:54:34,160 --> 00:54:38,480
but i wanted to be a point whereby if i can accelerate this over the next period i don't

554
00:54:38,480 --> 00:54:44,800
have to make an impact i should not have any power on this whereas again you see everyone

555
00:54:44,800 --> 00:54:48,560
else trying to get more and more power i want to make sure it's set up properly but i want to give

556
00:54:48,560 --> 00:54:54,720
it all away because power is obligation it's dragging and again it should not be invested in

557
00:54:54,720 --> 00:55:01,120
any one individual we shouldn't have to rely on anyone being nice or good or for this technology

558
00:55:01,120 --> 00:55:06,080
i was talking to uh michael sailor during the abundant summit uh that evening and you know

559
00:55:06,080 --> 00:55:13,520
talking about the fact that because satoshi uh when he set it up uh did not retain any power

560
00:55:13,520 --> 00:55:19,120
and did not trade on the founding blocks and so forth that that's the reason it's been able to

561
00:55:19,120 --> 00:55:25,440
succeed because there wasn't that centralized power and you know bitcoin had been he said bitcoin

562
00:55:25,440 --> 00:55:33,680
betrayed many times before but because it didn't have that uh initial anonymity and and and the

563
00:55:33,680 --> 00:55:38,880
dissolution of founding power that that's the reason it didn't succeed yeah i mean again i think

564
00:55:38,880 --> 00:55:43,760
you need to have it accelerate and you see this with movements right the movement starts but then

565
00:55:43,760 --> 00:55:49,680
it goes once you've got the dna and the story there right you know you see the profits you see the

566
00:55:49,680 --> 00:55:54,960
leaders you see the others but then it's about setting the framework correctly and reframing the

567
00:55:54,960 --> 00:56:01,120
concept this technology is not going to look stability is a company that started two years

568
00:56:01,120 --> 00:56:06,560
ago above a chicken shop in london right you know my first 20 employees i went to the job

569
00:56:06,560 --> 00:56:11,760
center and i said bring me people that have overcome adversity and i will train them young

570
00:56:11,760 --> 00:56:17,120
graduates and six of them are still at stability you know um like because it was a program and

571
00:56:17,120 --> 00:56:22,160
they're doing things from cyber security to run supercomputers we only had like 16 17 phd's

572
00:56:22,800 --> 00:56:27,200
yet we built the state-of-the-art models in every modality we built mind-reading models

573
00:56:27,200 --> 00:56:32,480
like mind's eye you know i remember that contributed to all these things yet you're

574
00:56:32,480 --> 00:56:37,120
told it's impossible to compete we have shown it's not impossible to compete that's a reframing

575
00:56:37,760 --> 00:56:43,120
the reframing is data versus models it's you don't need giant supercomputers for everyone

576
00:56:43,120 --> 00:56:49,760
you just need to have a trusted entity to build it right yeah you know and so i hope to kind of

577
00:56:49,760 --> 00:56:54,320
convey this and then figure out this organizational structure that can proliferate so i can take the

578
00:56:54,320 --> 00:57:00,560
holiday so before we go further let's talk about one area of your next chapter in life that

579
00:57:01,120 --> 00:57:06,320
we both have as a passion which is the use of generative ai in health it's an area that you've

580
00:57:06,320 --> 00:57:11,840
given a huge amount of thought to and i think you're excited about can you share what your vision is

581
00:57:11,840 --> 00:57:19,440
there yeah so i got into ai 13 years ago gosh i was a programmer before for 23 years building

582
00:57:19,440 --> 00:57:24,640
large-scale systems as a hedge fund manager and other things well my son was diagnosed with autism

583
00:57:24,640 --> 00:57:28,560
and then i built an nlp team to analyze all the clinical literature and then looked at

584
00:57:29,280 --> 00:57:34,080
biomolecular pathway analysis of neurotransmitters gavra and glutamate in the brain to repurpose

585
00:57:34,080 --> 00:57:38,800
drugs for him and he went to macy's school which is great n equals one and then i was one of i was

586
00:57:38,800 --> 00:57:44,160
lead architect from the one the covet ai project so the united nations for stanford and others

587
00:57:44,160 --> 00:57:47,040
and then because i didn't get the technology i was like well we've got to build it ourselves

588
00:57:47,440 --> 00:57:54,560
but what is health you know again i think we have this discussion a lot health care is sick care

589
00:57:55,280 --> 00:57:58,880
we don't have all the information that we should have at our fingertips health assumes

590
00:57:58,880 --> 00:58:04,480
ergodicity a thousand tosses of the coin the same as the coin tossed a thousand times but we are

591
00:58:04,480 --> 00:58:10,560
all individual and across the world there are amazing data sets that could be better because

592
00:58:10,560 --> 00:58:16,560
when you write down a clinical trial or your own kind of experiences you lose so much information

593
00:58:16,640 --> 00:58:21,440
at the same time you don't have all the information on cancer or autism multiple

594
00:58:21,440 --> 00:58:25,680
sclerosis at your fingertips in the comprehensive authoritative and upstate way so when i look at

595
00:58:25,680 --> 00:58:34,240
the health operating system we're going to build a gpt for open for cancer and it's going to mean

596
00:58:34,240 --> 00:58:39,360
that nobody is alone again on that journey and uses that agency because they know comprehensive

597
00:58:39,360 --> 00:58:45,280
authoritative upstate all the knowledge but ai models today already outperform human doctors

598
00:58:45,280 --> 00:58:50,000
and empathy they're not going to be a learn on that anymore can i just double click on what you

599
00:58:50,000 --> 00:58:54,800
just said because it's really important i've had so many people because of my role as chairman of

600
00:58:54,800 --> 00:58:59,920
fountain life who reach out and say i just got diagnosed with this cancer or my brother or my

601
00:58:59,920 --> 00:59:08,720
sister or my wife and and there is they're left with this decimating news and they're left googling

602
00:59:09,440 --> 00:59:16,640
um but a model that's able to have the most cutting-edge information and then incorporate

603
00:59:16,640 --> 00:59:23,600
all their medical data and give them advice in empathic fashion how far is that see a couple

604
00:59:23,600 --> 00:59:29,680
of years if we focus maybe even like next year and that's amazing because all of these topics

605
00:59:30,240 --> 00:59:35,280
that again we will have diagnosis that it's superior we will have research augmentation

606
00:59:35,280 --> 00:59:38,880
because again even researchers don't have all that knowledge of their fingertips and again

607
00:59:38,880 --> 00:59:44,400
this is public infrastructure in a public good you know from primary care all the way through so

608
00:59:44,400 --> 00:59:48,800
that what is the open infrastructure in the future where this technology can come again

609
00:59:48,800 --> 00:59:54,000
to your own data as well you have things like melody and other things around homomorphic

610
00:59:54,000 --> 00:59:58,320
encryption federated learning that they were trying to figure out how to preserve privacy

611
00:59:58,320 --> 01:00:03,760
we can run a language model on a smart phone right now that can analyze all your data and then

612
01:00:03,760 --> 01:00:08,560
just feedback stuff to a global collective but people are people so when I look at health care

613
01:00:08,560 --> 01:00:13,920
I see amazing data sets that we can activate by taking the models to the data an infrastructure

614
01:00:13,920 --> 01:00:19,360
that we can build like we had checks agent with stanford the top x-ray radio radiology model

615
01:00:20,160 --> 01:00:25,600
to build good standard things across the entire gamut of health care so we actually get to health

616
01:00:25,600 --> 01:00:30,400
care versus sick care so we can make it so that everyone is empowered to make the best decisions

617
01:00:30,400 --> 01:00:35,760
either as experts or individuals and make it so nobody is alone again as well as increasingly

618
01:00:35,760 --> 01:00:42,640
data quality that will then feed better models that will then save lives save suffering and again

619
01:00:42,640 --> 01:00:47,840
increase our potential like you've got a long job wrong stupid behind you right why don't you have

620
01:00:47,840 --> 01:00:54,400
all the latest knowledge of longevity at your fingertips at a gpt4 level right now that will

621
01:00:54,400 --> 01:00:58,720
happen over the next year we will launch stable health or whatever decide to call it and there

622
01:00:58,720 --> 01:01:03,120
will be the smartest people in each of these areas working on that so again you're never like

623
01:01:03,120 --> 01:01:09,920
doesn't matter if you're with a hundred billion dollars and your kid has autism ast there's no cure

624
01:01:09,920 --> 01:01:15,040
there's no treatment there's nothing doesn't matter how rich you are yet we're just a little bit of

625
01:01:15,040 --> 01:01:20,240
effort right now we can build it as an open infrastructure for the five percent of people

626
01:01:20,240 --> 01:01:24,880
in the world that know someone with autism the fifty percent of people in the world that receive

627
01:01:24,880 --> 01:01:29,680
a cancer diagnosis of them or someone they love and they feel that loss of agency so we're going to

628
01:01:29,680 --> 01:01:35,200
return agency to humanity that way and again it needs to be an open infrastructure that they can

629
01:01:35,200 --> 01:01:40,960
then access private datasets and compensate them appropriately so everyone is incentivized we get

630
01:01:40,960 --> 01:01:48,080
that fast yeah and and that's a beautiful vision it is again infrastructure it and one of the things

631
01:01:48,080 --> 01:01:53,600
that's so beautiful about it is guess what all eight billion people we're all human we're all

632
01:01:53,600 --> 01:01:59,600
running the same software and the the the breakthroughs and the knowledge accumulated in

633
01:02:00,480 --> 01:02:06,800
you know in Kazakhstan is going to be as useful in Kansas yeah but this is the thing operating

634
01:02:06,800 --> 01:02:11,360
system this is the biggest upgrade to the human operating system we can imagine because we're

635
01:02:11,360 --> 01:02:17,920
going from analog to digital text is black and white whereas this these models only understand

636
01:02:18,000 --> 01:02:24,240
context you know daniel kahneman just passed you know amazing kind of guy but you know he did

637
01:02:24,240 --> 01:02:29,680
have this concept of type one type two thinking and so we had one which is these big data things that

638
01:02:29,680 --> 01:02:34,000
can only extrapolate but now we have these models that understand context and so we have the missing

639
01:02:34,000 --> 01:02:38,960
part of the brain and that will allow us to extrapolate allows to have more rainbows you

640
01:02:38,960 --> 01:02:43,760
know have the context of each individual push intelligence to the edge and that's why again

641
01:02:43,760 --> 01:02:49,120
there is this imperative to do this now because there's a window on the freedom agency democracy

642
01:02:49,120 --> 01:02:53,600
side but the other imperative is no one should have to suffer as they're suffering now

643
01:02:55,280 --> 01:02:59,840
amazing and how much does actually it doesn't need that much which is the really amazing stuff

644
01:03:00,400 --> 01:03:04,640
this the total amount spent in janitor bail i think i said at the conference is less than

645
01:03:05,200 --> 01:03:08,160
the total amount spent on the los angeles san francisco railway

646
01:03:08,800 --> 01:03:15,440
which hasn't even started yet and and in building stable health again if that's what it's called i

647
01:03:15,440 --> 01:03:21,360
mean the amount of capital required to build that is de minimis compared to what's spent on a single

648
01:03:21,360 --> 01:03:28,880
human trial of any any drug yeah it is but then you know you build it and you get to that 8020

649
01:03:28,880 --> 01:03:33,360
incredibly quickly that will change hundreds of millions of lives and that will attract the smartest

650
01:03:33,360 --> 01:03:36,240
people in each of these areas thinking about what is the open infrastructure of multiple

651
01:03:36,320 --> 01:03:43,920
sclerosis of longevity of cancer and more but then you can amp that because the value is so so huge

652
01:03:44,880 --> 01:03:50,560
and you i hope to build a trusted organization as part of this whole human operating system upgrade

653
01:03:50,560 --> 01:03:54,400
you know that's what i want to build i want to build human os well at least catalyze it again i

654
01:03:54,400 --> 01:03:59,520
don't want to run or control or own anything i want to figure out how to give back that control

655
01:03:59,520 --> 01:04:04,000
because who should decide what cancer knowledge goes in there who should decide what education etc

656
01:04:04,000 --> 01:04:11,120
let's talk about the second half of your of your vision which is how we originally met

657
01:04:11,680 --> 01:04:17,120
when you were one of the winners of the global learning x prize that ilan and tony

658
01:04:18,080 --> 01:04:25,120
robbins had had co-funded your your vision around education to speak to us about that

659
01:04:26,000 --> 01:04:29,760
yeah you know so we're deploying it kind of the winners kind of separate but

660
01:04:30,720 --> 01:04:35,760
every child right my entire operating system is like if you think about things in terms of

661
01:04:35,760 --> 01:04:41,200
the rights of child and today they have no agency and so we must respect their rights climate

662
01:04:41,200 --> 01:04:46,400
everything becomes a lot simpler now that we have language models on a laptop like i said you can go

663
01:04:46,400 --> 01:04:53,600
to lmstudio.ai download stable lm and it will run on your macbook faster than you can read

664
01:04:53,920 --> 01:05:00,720
uh it's crazy we can have a gpt 4 level ai from us or someone else on a smartphone or a tablet by

665
01:05:00,720 --> 01:05:07,040
next year one laptop per child was too early you know now we have this transformative technology

666
01:05:07,040 --> 01:05:11,840
you have an ai that teaches the child learns from a child are you visual auditory dyslexic

667
01:05:11,840 --> 01:05:17,520
that's the best data in the world for a national model but also to teach these models how to be

668
01:05:18,480 --> 01:05:25,360
optimistic how to this really is this really is uh the young ladies illustrator primer this

669
01:05:25,360 --> 01:05:32,560
really is neil stevensson's vision in that regard yeah but mel shouldn't have had to find the primer

670
01:05:33,200 --> 01:05:41,040
she should have had it from day one as a human right as a human right yes our school's education

671
01:05:41,040 --> 01:05:47,280
system our child care mixed with the social status game mix with petri dish you know they teach our

672
01:05:47,280 --> 01:05:53,200
kids not to have agency yes whereas they should be telling the kids yeah yeah they should be teaching

673
01:05:53,200 --> 01:05:58,320
this is a relic of the industrial age where everyone had to be counted and you can't measure

674
01:05:58,320 --> 01:06:04,240
what you can't manage so you manage the creativity and belief out of people everyone in the world can

675
01:06:04,240 --> 01:06:09,040
do anything why because even if you don't have that talent you can convince someone else who

676
01:06:09,040 --> 01:06:14,400
does have that talent but they don't believe it so they can't do it so what happens if we have an

677
01:06:14,480 --> 01:06:20,160
entire nation of children that have this helper that brings the right information the right time

678
01:06:20,160 --> 01:06:26,160
and tells them they can always believe that supports them entire world what can't you do

679
01:06:26,880 --> 01:06:30,560
you know then they have all of the cancer knowledge of their fingertips and all of the

680
01:06:30,560 --> 01:06:35,760
engineering knowledge of their fingertips and it's a constantly learning adaptive and improving system

681
01:06:36,560 --> 01:06:43,600
again right now almost the entire agi and ai debate is about these machine gods trained on giant

682
01:06:43,600 --> 01:06:49,200
supercomputers that bestow their beneficence down or may kill us or whatever what about that human

683
01:06:49,200 --> 01:06:54,160
operating system upgrades that is a decentralized intelligence where that kid in mongolia or malawi

684
01:06:54,160 --> 01:06:59,440
or wherever can make a real difference to humanity some of the contributors to our open code basis for

685
01:06:59,440 --> 01:07:05,440
our models are 15 years old and they just taught themselves and just happen to be their wizards

686
01:07:05,440 --> 01:07:10,160
you don't know in this new age right and again they should contribute to the whole because

687
01:07:10,160 --> 01:07:15,200
once something goes into this model of the system and again it needs the verification and other

688
01:07:15,200 --> 01:07:20,960
things that can be dynamic they can proliferate to everyone using that system do you think that

689
01:07:20,960 --> 01:07:29,360
once this capability is built it will run into blocks in different nations or do you imagine

690
01:07:29,360 --> 01:07:37,520
that this will become a again a human right and not all need I mean listen there's no greater gift

691
01:07:37,600 --> 01:07:44,640
and no greater asset you can give to a nation's populist than intelligence and education but I'm

692
01:07:44,640 --> 01:07:50,720
not sure every national leader wants to see that and that's why I think again there is a gap here

693
01:07:51,360 --> 01:07:57,120
there is a year maybe where you can go to any national leader and say I'll bring this technology

694
01:07:57,120 --> 01:08:01,440
to your people and I will empower the smart stupid people I want to be earned by the people

695
01:08:01,440 --> 01:08:06,160
and what option do they have this is positive for them what happens is that a lot of the

696
01:08:06,160 --> 01:08:10,960
corruption in the world is because of local maxima you know actually it's weird because

697
01:08:10,960 --> 01:08:15,040
unpredictable corruption is the worst predictable corruption is a bit like tax you know there's

698
01:08:15,040 --> 01:08:19,200
a good boast book by Frosso a job about Harvard about this and then you have taxation kicking

699
01:08:19,200 --> 01:08:24,800
at 14 percent if you can show them something bigger and this is clearly big they will embrace

700
01:08:24,800 --> 01:08:28,640
this technology and set new norms and if you correct the same across all these countries

701
01:08:28,640 --> 01:08:33,360
with talented individuals in each of those groups and talented individuals in each of those sectors

702
01:08:33,360 --> 01:08:38,400
with a shared mission even though they're separate organizations that's how you set amazing

703
01:08:38,400 --> 01:08:43,920
standards that's how you build a network effect and if you tie them all together with a intelligent

704
01:08:43,920 --> 01:08:48,080
protocol and you want to talk about tokens or speculation or ramps or anything like that

705
01:08:48,080 --> 01:08:54,720
but taking the best of thinking around coordination that can work that can break this open you know

706
01:08:56,480 --> 01:09:00,400
but it's not going to be everywhere and also when you look at the current debate the current

707
01:09:00,400 --> 01:09:07,200
debate is for example we can't let China have this technology and you're like what about the kids in

708
01:09:07,200 --> 01:09:12,800
China right well you know it's dangerous they can have a giant so under what circumstance would China

709
01:09:12,800 --> 01:09:18,880
ever have this technology never you know Pakistan when should they have the tech never that's really

710
01:09:18,880 --> 01:09:22,080
what they're kind of saying it's also self-defeating because China has a hundred million people they

711
01:09:22,080 --> 01:09:27,040
can use to create data sets and two exaplops of computers let's put that to the side again it's

712
01:09:27,040 --> 01:09:32,000
a very western oriented debate whereas actually if you go to these countries and you talk to the

713
01:09:32,000 --> 01:09:37,360
leaders and the family officers that have power and the people they will leapfrog in the global

714
01:09:37,360 --> 01:09:44,080
south to an intelligence augmentation like they let frog to mobile they want to embrace this technology

715
01:09:44,080 --> 01:09:49,120
and again you can set norms now versus what's going to happen is you know they will get a

716
01:09:49,120 --> 01:09:54,560
centralized solution they'll adopt that instead if you don't right now for hundreds of millions

717
01:09:54,560 --> 01:10:02,320
billions of people that's why i think again it's a crossroads um is there anybody else working

718
01:10:02,320 --> 01:10:08,960
towards this solution that you know of no in the in the large ai town no certainly no one

719
01:10:08,960 --> 01:10:13,280
credibility and again that's why i had to build these models you know and i had to kind of do

720
01:10:13,280 --> 01:10:18,960
this everyone's working on tiny parts of this but there is expecting emergence build it and

721
01:10:18,960 --> 01:10:22,560
somehow it will spread and again this is why i found it fascinating the web3 community

722
01:10:22,560 --> 01:10:26,400
there are good people in there and i hope to be able to unite them just like hope to unite the

723
01:10:26,400 --> 01:10:30,720
people in health and others and peter you've seen people working on tiny parts of this

724
01:10:31,440 --> 01:10:35,040
but this isn't a manhattan project where we're facing an enemy

725
01:10:35,040 --> 01:10:41,040
unless the enemy is ourselves you know but this does require this big global coordinated push and

726
01:10:41,040 --> 01:10:46,000
that's why i've tried to design this system that i believe will work because all about the talent

727
01:10:46,000 --> 01:10:49,760
and is multiplicative is the race against uh

728
01:10:52,080 --> 01:10:59,280
overly powerful centralized ai systems that achieve some version of agi is that what we're racing against

729
01:11:03,360 --> 01:11:09,200
yeah again we're racing against ourselves like um humans can scale through stories you have

730
01:11:09,200 --> 01:11:14,400
organizations you know come and join a band and come and go to oxford come and do this but then

731
01:11:15,360 --> 01:11:20,240
when we scaled through text text was a lossy information format and there's this poem by

732
01:11:20,240 --> 01:11:24,320
ginsburg how about this carthaginian demon of disorder mollock that comes in

733
01:11:25,360 --> 01:11:29,040
mollock comes in through the data loss our organizations are slow dummy eyes

734
01:11:29,680 --> 01:11:34,320
but now what's happening is they're configuring to achieve the thing of getting more and more power

735
01:11:34,320 --> 01:11:40,080
again corporations are technically a people under law but they're not fully formed people

736
01:11:40,080 --> 01:11:45,120
they eat our hopes and dreams so i believe the competition here is against those organizations

737
01:11:45,120 --> 01:11:50,560
consolidating too much power and creating norms that almost impossible to break so we're almost

738
01:11:50,560 --> 01:11:55,760
competing against ourselves and again the question is this do you believe it amplified human

739
01:11:55,760 --> 01:11:59,680
intelligence or do you believe in artificial general intelligence do you believe in collective

740
01:11:59,680 --> 01:12:06,160
intelligence or do you believe in collected intelligence who decides is this infrastructure

741
01:12:06,800 --> 01:12:14,640
or is this a product like so it's not like a Manhattan project against you know the soviets

742
01:12:14,640 --> 01:12:21,920
or anything like that but it is is require us all to come together or at least the smartest people

743
01:12:21,920 --> 01:12:27,040
in each of these areas from coordination to government systems to health care to education

744
01:12:27,600 --> 01:12:32,000
with a blank slate of how do we upgrade the human operating system the time is now

745
01:12:32,000 --> 01:12:37,760
it's all lost chance to do it and i and i i love you for it because i think you're you're right

746
01:12:38,640 --> 01:12:46,960
you were there when elan beamed in on xx video over starlink and from his airplane which is

747
01:12:46,960 --> 01:12:54,320
which was a fun moment and we were talking about the rate of growth and his his statement

748
01:12:54,960 --> 01:12:58,400
because you know recurrence why was there talking about his still his prediction of

749
01:12:58,880 --> 01:13:06,720
agi by 2029 and ray and written elan saying we'll have agi whatever that means by next year and

750
01:13:07,440 --> 01:13:14,960
and the intelligence of the entire human race by 2029 so i am i am curious what just to close

751
01:13:14,960 --> 01:13:23,120
out what you think about that those timelines and that potential for a super intelligent

752
01:13:23,680 --> 01:13:30,000
agi system that is centralized because that's the people who are building that level of power

753
01:13:30,000 --> 01:13:36,400
are building centralized systems they're building centralized single systems that again take our

754
01:13:36,400 --> 01:13:41,520
collective intelligence like all of youtube in the case of open ai clearly and other things

755
01:13:41,520 --> 01:13:47,440
and they package it up sell it back to us but they don't care you know these organizations

756
01:13:47,440 --> 01:13:51,440
are trying to build a system that will take away our freedom liberty and potentially kill us all

757
01:13:51,440 --> 01:13:58,240
let's be kind of fair about that and sell it to us on an incremental basis the selling to us

758
01:13:58,240 --> 01:14:03,680
is a complete cannot they don't care about the revenue of this again let's kind of call a spade

759
01:14:03,680 --> 01:14:09,520
a spade they're telling you that they're building something that could kill you and something that

760
01:14:09,520 --> 01:14:13,760
could remove all our freedom and liberty and they're saying it's a good thing you should back

761
01:14:13,760 --> 01:14:18,720
them because it's cool it's not it's actually shameful if you think about it and we should

762
01:14:18,720 --> 01:14:21,360
not stand for it anymore and again this is another reason i want to step aside to see you

763
01:14:21,360 --> 01:14:26,000
because you can't say things like that i don't cancel until i can rally so many times but realistically

764
01:14:26,000 --> 01:14:31,920
it's ridiculous and it should not be stood for but they're going to do it anyway because they have

765
01:14:31,920 --> 01:14:37,920
the political power people are scared of them so there has to be an alternative and the alternative

766
01:14:37,920 --> 01:14:43,680
has to be distributed intelligence when i resigned i said you can't beat centralized intelligence

767
01:14:43,680 --> 01:14:47,120
decentralized intelligence you're not going to beat it with this stability the least of great

768
01:14:47,120 --> 01:14:52,160
organization it's going to do well the only way that you can beat it to create the standard that

769
01:14:52,160 --> 01:14:59,520
represents humanity is decentralized intelligence it's collective intelligence and the data sets

770
01:14:59,520 --> 01:15:06,240
and norms from that will be ones that help children that helps people suffering that reflect our

771
01:15:06,240 --> 01:15:12,960
moral upstanding and the best of us and gathers the best of us to do it because if you work in

772
01:15:12,960 --> 01:15:17,680
healthcare if you work in education if you work in finance if you work in any of these things there's

773
01:15:17,680 --> 01:15:24,080
no organization for you to come and join or partner with on this there's no kind of centralized mission

774
01:15:24,080 --> 01:15:28,000
i have looked i've wanted to help other people i didn't want to do this to myself and i don't want

775
01:15:28,000 --> 01:15:32,320
it to be about being very very quickly and so i'm kind of getting it out there now i hope that i can

776
01:15:32,320 --> 01:15:37,840
capitalize something that then people will take forward and time is now for that because agi when

777
01:15:37,840 --> 01:15:42,400
it comes if it comes again there's various definitions of this why on earth do you need any

778
01:15:42,400 --> 01:15:49,360
knowledge workers anything that can be done via a laptop doesn't need humans and so you have concepts

779
01:15:49,360 --> 01:15:55,680
of ubi here you have concept like when agi comes you don't need money money is a common story is a

780
01:15:55,680 --> 01:16:01,520
common good we hear total post capitalist society and i think the example i think you said was a

781
01:16:01,600 --> 01:16:06,720
star trek versus man max you know i'm like star trek versus star wars i think there's a

782
01:16:10,560 --> 01:16:16,560
and so you know the sith lords and all of that um but again if you kind of look at this i don't

783
01:16:16,560 --> 01:16:21,520
think we need money like it's cross contextual like bartering with our ai systems representing us

784
01:16:22,160 --> 01:16:28,160
or it's you don't need money because you're told what to do again our governments the definition

785
01:16:28,160 --> 01:16:33,440
of a government is the entity with the monopoly on political violence and an agi can overtake any

786
01:16:33,440 --> 01:16:38,560
government that they can then control the people because again listen to it whispering look at the

787
01:16:38,560 --> 01:16:44,400
kind of human thing so we have this opportunity to set norms right now the way that the big labs

788
01:16:44,400 --> 01:16:50,400
are going to agi is likely to kill us all elon and i signed that six month pause letter because even

789
01:16:50,400 --> 01:16:54,480
though people like emma do an acceleration as you put all this open source aia you have to think

790
01:16:54,480 --> 01:16:58,720
about the other side and who's involved in that discussion and again if we build an agi as a

791
01:16:58,720 --> 01:17:07,120
centralized thing is windows or linux safer as infrastructure our entire internet infrastructure

792
01:17:07,120 --> 01:17:14,320
is built on open open can be challenged open can be augmented a monolith is like to be crazy and

793
01:17:14,320 --> 01:17:21,120
the way that i put this is we all night both know so many geniuses you know side effect of genius

794
01:17:21,200 --> 01:17:27,920
is insanity honestly we're not men geniuses are not mentally stable why would you expect an agi

795
01:17:27,920 --> 01:17:33,760
to be so and you're putting all your ends in one basket versus creating a complex hierarchical system

796
01:17:34,640 --> 01:17:39,200
that is a hive mind that's the intelligence that represents us all we should be working

797
01:17:39,200 --> 01:17:44,720
towards building that and it's safer it's better it achieves all the benefits that people are talking

798
01:17:44,720 --> 01:17:49,760
about and it's possible today do you think elon shares in this vision of a decentralized ai do

799
01:17:49,760 --> 01:17:54,240
you think he would play in that area and do you think any of the national leaders that you've

800
01:17:54,240 --> 01:18:00,560
been speaking to would support that kind of a vision as well um yeah i can't speak for elon

801
01:18:00,560 --> 01:18:04,640
i'll speak to him and see what he thinks and then i'll get back to you know he always says what he

802
01:18:04,640 --> 01:18:09,920
thinks um but you know he's immensely concerned he was one of the leaders in this area saying

803
01:18:09,920 --> 01:18:15,120
originally why google you know now why in microsoft open ai like it can't be centralized but it's

804
01:18:15,120 --> 01:18:19,680
difficult it's a difficult question how many people have a feasible solution or you've even

805
01:18:19,680 --> 01:18:24,000
thought about this properly you and i both know just not many and that's very sad it should be

806
01:18:24,000 --> 01:18:29,120
everyone thinking about this on the leader side all the leaders i've met are super happy you know

807
01:18:29,120 --> 01:18:37,040
because they again leaders want power they want control and all of this but generally like they

808
01:18:37,040 --> 01:18:42,800
want to see a bundles they're not happy with where their countries are and embracing this technology

809
01:18:42,800 --> 01:18:47,520
they know they can leap ahead and you know they will still have a say in all of this it's not like

810
01:18:47,520 --> 01:18:54,480
it's kicking them out or removing them there are still various kind of mechanisms there and ultimately

811
01:18:54,480 --> 01:19:00,720
improving the health education and capability of your people is not a bad thing i mean like obviously

812
01:19:00,720 --> 01:19:05,120
i haven't talked to the completely oppressive leaders you know maybe that'll be an interesting thing

813
01:19:05,120 --> 01:19:09,360
but honestly i don't want to even be talking to leaders i want to create again a system

814
01:19:09,440 --> 01:19:12,880
that the people of the country coming together with the franchise system

815
01:19:12,880 --> 01:19:17,280
can then build this technology for the good of their people in the open and not be reliant on

816
01:19:17,280 --> 01:19:21,200
anyone politically or any other thing like that so you don't need giant super computers for where

817
01:19:21,200 --> 01:19:27,840
we're going recordination need a few giant super computers yeah what's your timeline for putting

818
01:19:27,840 --> 01:19:34,560
out this white paper i'm working as hard as i can you know i've held i've held i've held you to this

819
01:19:34,560 --> 01:19:39,360
a number of times i've said get the vision out there it's getting there we're about to go off

820
01:19:39,360 --> 01:19:43,680
this call to a four-hour session to dictate all the various bits and pieces and again it was

821
01:19:43,680 --> 01:19:48,000
impossible when i was a CEO of stability there was always another fire there's always another thing

822
01:19:48,000 --> 01:19:52,720
i didn't have time to think you know and i hope people can take that white paper and make it better

823
01:19:52,720 --> 01:19:57,120
i don't have all the answers i'm just trying to capitalize something man i think after i heard you

824
01:19:57,120 --> 01:20:04,720
step down i wrote you a text saying congratulations yeah exactly talk to miserations time to time

825
01:20:04,720 --> 01:20:14,320
to feel unleashed yeah yeah uh imad thank you my friend uh thank you for sharing uh where you are

826
01:20:14,320 --> 01:20:20,080
what led up to this where you're going next and uh and really pulling the gloves off on discussing

827
01:20:20,080 --> 01:20:29,200
the idea of centralized closed ai systems and their dangers uh and the importance of of the

828
01:20:29,200 --> 01:20:35,200
vision that you've portrayed because i'm i'm fully supportive and fully believe that what you've laid

829
01:20:35,200 --> 01:20:43,760
out um is probably one of the most seen visions of ai in the future that i've heard i hope other

830
01:20:43,760 --> 01:20:49,440
people agree you know and they can take it forward other real heroes thank you thank you pal

