WEBVTT

00:00.000 --> 00:02.560
So again, like, if you're an entrepreneur, you'd be an entrepreneur in this.

00:02.560 --> 00:07.920
If you are someone who can communicate, you communicate this to other people and you get paid a million bucks a year as a consultant.

00:07.920 --> 00:09.560
Right. You organize information.

00:09.560 --> 00:14.960
If you're an artist, if you're creative, if you're an artist, you become the most efficient artist in the world when you lean in on this.

00:15.560 --> 00:18.680
You know, like systems can be outcompeted.

00:18.680 --> 00:20.400
It's like the example of the steel mill, right?

00:20.400 --> 00:24.600
There were big vertically integrated steel mills that were outcompeted by lots of little steel mills.

00:24.600 --> 00:26.440
Sure. Micro mills, right?

00:26.440 --> 00:34.480
The big corporations, the big programs, the big things will be outcompeted by just individuals and small groups.

00:34.480 --> 00:36.520
Building on top of this technology can do anything.

00:38.200 --> 00:41.720
And a massive transform to purpose is what you're telling the world.

00:41.720 --> 00:44.920
It's like, this is who I am. This is what I'm going to do.

00:44.920 --> 00:47.080
This is the dent I'm going to make in the universe.

00:50.520 --> 00:52.960
Welcome, everybody. Welcome to Moonshots and Mindsets.

00:53.000 --> 00:57.840
I am here with an old friend and a new friend, Imad Mustak.

00:57.840 --> 01:00.320
Imad, welcome. It's a pleasure to have you here.

01:00.560 --> 01:01.240
Thank you for having me.

01:01.240 --> 01:11.080
We're going to have such a fun, wide-ranging conversation across everything that is of, I think, importance to anyone listening to any entrepreneurs,

01:11.080 --> 01:13.560
any CEOs, any government leaders, any kids.

01:14.080 --> 01:17.160
Everything that you're doing is really transforming the world.

01:17.160 --> 01:18.480
Let me do a proper introduction.

01:18.480 --> 01:26.960
Imad Mustak is the founder and CEO of stability.ai, focused on amplifying humanity's potential through AI.

01:26.960 --> 01:33.920
You probably know stability AI because of its text-to-image model, Stable Diffusion, released in 2022,

01:33.920 --> 01:38.800
which rocked the developed world and I think broke the internet as a good way to describe it.

01:38.800 --> 01:43.200
Previously, Imad has been a hedge fund manager and autism researcher.

01:43.200 --> 01:49.200
Now, he's led multiple technology initiatives across multilateral organizations and governments

01:50.320 --> 01:51.920
and an XPRIZE competitor.

01:51.920 --> 01:58.720
Today, Imad is pursuing an incredible Moonshot, actually a series of Moonshots that can transform many industries.

01:58.720 --> 02:03.200
You know, I started by saying, which industries are you looking to disrupt?

02:03.520 --> 02:07.360
And your answer was, all of them, could do better.

02:08.080 --> 02:10.560
And I think that's not hype.

02:10.560 --> 02:15.280
I think that's actually, from what I understand, what we're going to explore, true.

02:17.280 --> 02:20.800
Scientists and technologists have been talking about AI for decades.

02:22.640 --> 02:24.960
But today is different, isn't it?

02:25.520 --> 02:28.880
Yeah. I mean, we always say this time is different, but this time really is different.

02:29.520 --> 02:33.040
I mean, if you kind of look at it, AI is basically information classification.

02:33.040 --> 02:37.520
And we had classic AI, which was information goes in and then you extrapolate from a dataset,

02:37.520 --> 02:39.760
you create really custom models and it goes out.

02:39.760 --> 02:44.960
Like the big AIs, Internet2 was Google and Facebook taking all that big data and then

02:44.960 --> 02:50.880
targeting PETA with like ads for rockets, you know, and things like that, you know, like rockets.

02:52.720 --> 02:57.680
In 2017, we had a bit of a step change where there was a paper called attention is all you need.

02:58.480 --> 03:01.360
About how to teach an AI to pay attention to the important things.

03:01.360 --> 03:02.160
Fascinating.

03:02.160 --> 03:03.760
And learn principles.

03:03.760 --> 03:05.360
Yeah, principle based analysis.

03:06.240 --> 03:10.880
Yeah. So it's not good old fashioned AI or kind of logic causal based AI, but it's kind of that.

03:10.880 --> 03:16.400
Because you remember, like, who was it by type one, type two thinking?

03:17.200 --> 03:17.760
A con.

03:18.960 --> 03:23.040
So, you know, there's the very logical thing and there's a freaking tiger in that bush over there.

03:24.160 --> 03:25.680
And we didn't have that second part.

03:25.680 --> 03:28.800
We didn't have the ability to kind of just leap to conclusions,

03:28.800 --> 03:30.800
principle based, heuristic based thinking.

03:30.800 --> 03:32.960
You know, this is kind of the mindset thing, right?

03:33.040 --> 03:37.840
Whereby you construct these things and it allows you to go very fast to just amazing conclusions.

03:37.840 --> 03:39.600
That's part of what makes humans humans.

03:39.600 --> 03:40.320
Yes.

03:40.320 --> 03:41.040
It's here now.

03:41.040 --> 03:42.560
It actually works.

03:42.560 --> 03:47.120
Leaping to conclusion, but so it's no longer extrapolation based upon data.

03:47.120 --> 03:53.120
It's actually learning to understand the meaning hidden in the data.

03:53.120 --> 03:53.920
Exactly.

03:53.920 --> 03:59.040
It's kind of semantic understanding, kind of literally, it's kind of called latent spaces,

03:59.040 --> 04:00.720
hidden layers of understanding.

04:00.720 --> 04:03.440
So, like, I'll give you an example.

04:03.440 --> 04:08.160
One of the ways that I kind of entered AI was working with AI to do drug repurposing

04:08.160 --> 04:09.360
for my son who has autism.

04:09.360 --> 04:09.920
Yes.

04:09.920 --> 04:13.680
So, he was two years old at the time and the doctor said nothing could be done.

04:13.680 --> 04:16.000
And I was like, of course, there's something can be done.

04:16.000 --> 04:17.840
Like, let's try and figure out information.

04:17.840 --> 04:20.000
Because all we're looking for is information that needs to come in time.

04:20.000 --> 04:22.400
It's kind of the Claude Shannon theory of information theories.

04:22.400 --> 04:24.880
Information is valuable in as much as it changes the state.

04:24.880 --> 04:28.160
So, how do you find the valuable information amongst all those autism studies

04:28.160 --> 04:29.040
and figure out what's going on?

04:29.040 --> 04:31.760
Because no one can really explain to me what caused it.

04:31.760 --> 04:38.400
So, just one second there, because typically, people are looking for a very clear cut answer

04:38.400 --> 04:40.560
in the data that's obvious.

04:41.120 --> 04:43.040
But there are answers, but it's hidden.

04:43.680 --> 04:44.400
It's hidden.

04:44.400 --> 04:45.120
It's kind of hidden.

04:45.120 --> 04:47.760
And so, you have to look at things from a first principles basis when you can't see

04:47.760 --> 04:49.680
things just from scanning everything.

04:49.680 --> 04:52.240
You have to dig down to another layer, another layer, another layer.

04:52.800 --> 04:58.800
And kind of when I dig down, I kind of used AI to do natural entity recognition

04:59.120 --> 05:01.600
look at all the different compounds, all the different trials that were tried,

05:01.600 --> 05:06.400
built a team, then built a biomolecular pathway analysis model of neurotransmitters

05:06.400 --> 05:06.720
in the brain.

05:06.720 --> 05:07.920
So, it's like just like a common cold.

05:07.920 --> 05:09.760
It seems like something very similar is happening.

05:10.800 --> 05:12.800
It turns out there's two neurotransmitters in the brain.

05:12.800 --> 05:13.920
One of them is GABA.

05:13.920 --> 05:15.840
You pop a valium and chill out.

05:15.840 --> 05:16.480
It chills you out.

05:17.120 --> 05:18.640
And then another one is Glucimate.

05:18.640 --> 05:21.360
And Glucimate kind of makes your brain accelerate.

05:21.360 --> 05:24.240
And so, you know, when you're tapping your leg and you can't focus and concentrate,

05:24.240 --> 05:25.120
that's what happens.

05:25.120 --> 05:28.560
Because humans are really a filtering entity, as it were.

05:28.560 --> 05:31.120
We have so much information in the world is just too much.

05:31.120 --> 05:32.320
So, our brain creates a simulation.

05:32.320 --> 05:34.960
That's why we've got the optic nerve, for example, filled it instantly.

05:36.000 --> 05:39.920
We are kind of constrained a little bit and that filtering allows us to focus.

05:39.920 --> 05:42.800
We all know what it's like when you're tapping your leg because there's too much stuff going on.

05:42.800 --> 05:45.280
So, with kids with ASD, they couldn't filter for various reasons,

05:45.280 --> 05:46.400
but it was different reasons.

05:46.400 --> 05:48.800
Not enough GABA, too much Glutamate.

05:48.800 --> 05:53.040
When you're born, GABA and Glutamate are actually both excitatory and oxytocin flips the switch

05:53.040 --> 05:53.600
on GABA.

05:53.600 --> 05:54.080
Interesting.

05:54.080 --> 05:56.080
So, we started digging down into kind of the things and we're like,

05:56.080 --> 05:58.080
ah, different compounds can affect this in different ways.

05:58.880 --> 06:00.640
But then what was the upshot of that?

06:00.640 --> 06:02.720
And how is this long story coming to a conclusion?

06:03.840 --> 06:06.080
Because there's too much noise, my son couldn't speak,

06:06.080 --> 06:10.800
because he couldn't formulate the hidden layers of meaning and connectivity on concepts.

06:10.800 --> 06:12.240
So, kind of we've got cup here.

06:12.240 --> 06:12.640
Yes.

06:12.640 --> 06:16.000
And you can cup your hands, you have a World Cup, which England hopefully will win,

06:16.000 --> 06:19.680
you know, and you've got various other meanings of the World Cup.

06:19.680 --> 06:22.800
You form this latent space of hidden connective meaning to that.

06:22.800 --> 06:25.920
So, it can be applied in different things, the principle-based analysis.

06:25.920 --> 06:30.080
By reducing the amount of noise, you can then filter and pay attention and build that out.

06:30.080 --> 06:31.920
The AI can now do the same thing.

06:31.920 --> 06:35.520
So, you don't need to have huge petabytes, exabytes of data anymore.

06:35.520 --> 06:38.080
With structured data, it can figure out interconnectivity and connection.

06:38.080 --> 06:43.440
And I want to come back to that because that's the revolution that you're creating right now.

06:44.080 --> 06:50.160
Just, you know, these personalized data sets that are good for you, your country,

06:50.160 --> 06:51.600
your company, yourself.

06:51.600 --> 06:52.640
Your context, exactly.

06:52.640 --> 06:52.720
Yes.

06:52.720 --> 06:55.520
Because humans are heuristic animals.

06:55.520 --> 06:57.600
We learn by principle-based analysis.

06:58.240 --> 07:01.200
And we're animals that are story-based.

07:01.200 --> 07:04.400
So, you have multiple stories that make you up, you know, from your work on the X-Price

07:04.400 --> 07:07.440
to your humanitarian to, you know, bold VC fund and all the things.

07:08.160 --> 07:10.320
We all kind of form connectivity there.

07:10.320 --> 07:12.560
But those stories are very hard to map initially.

07:12.560 --> 07:14.320
Now, we can do it dynamically.

07:14.320 --> 07:18.000
And we can start building tools to re-augment human potential by doing this.

07:18.000 --> 07:19.920
Because we can finally have that assistance.

07:19.920 --> 07:22.640
We're going to go so much, have so much fun on these conversations.

07:22.640 --> 07:28.960
But I want to take us back to some people around the world for the last year or so

07:28.960 --> 07:33.040
have heard of Dolly and Dolly 2.

07:33.040 --> 07:35.040
And here comes stable diffusion.

07:35.680 --> 07:39.520
That is an open source version of Dolly and Dolly 2.

07:39.520 --> 07:46.320
And as I said, broken internet, you sent me an image of the speed of growth on GitHub.

07:46.320 --> 07:46.800
Yeah.

07:46.800 --> 07:49.680
Of people using stable diffusion.

07:49.680 --> 07:52.240
And it was so funny because when you first sent me this image,

07:52.720 --> 07:59.120
here you see on GitHub the users who are on Ethereum over time.

08:00.240 --> 08:06.800
And then there's this line that goes straight up that I thought was one of the axes for the graph.

08:08.080 --> 08:09.120
Like what happened there?

08:09.120 --> 08:11.920
So first of all, what is stable diffusion?

08:11.920 --> 08:14.080
It's one product of your company, right?

08:14.080 --> 08:20.640
Well, stable diffusion is a community effort to build an AI that allows anyone to create anything.

08:20.640 --> 08:25.680
So words go in and then anything you describe comes out, which is a bit insane.

08:26.720 --> 08:31.040
So what we did is, you know, in collaboration with various entities,

08:31.040 --> 08:34.240
which we paid an important part in stable diffusion to is kind of led by us.

08:34.800 --> 08:39.360
We took 100,000 gigabytes of image label pairs, two billion images,

08:39.360 --> 08:43.760
and created a 1.6 gigabyte file that can run offline in your MacBook.

08:43.840 --> 08:52.480
1.6 gigabyte file, which is relatively you can transmit it over the phone network.

08:52.480 --> 08:55.200
You transmit it over the phone network, but you only need to transmit it once.

08:56.080 --> 09:00.320
Everyone having this tiny file that basically compresses the visual information of a snapshot

09:00.320 --> 09:06.880
of the internet can create anything from a high-resolution render of an apartment

09:07.440 --> 09:10.880
to Robert De Niro's Gandalf or anything else.

09:11.520 --> 09:16.000
And they can now do it in a second, something a week.

09:16.000 --> 09:18.400
They can do one of those images in 30 seconds.

09:19.440 --> 09:20.880
You mean one 30th of a second?

09:20.880 --> 09:21.600
One 30th of a second.

09:21.600 --> 09:22.560
We just broke in real time.

09:22.560 --> 09:24.240
We had a 30-time speed up in the last week.

09:24.240 --> 09:28.320
So when you mentioned that, just as we were getting ready for this,

09:28.320 --> 09:33.680
I said, so you're basically able to render a virtualized world in real time.

09:33.920 --> 09:34.240
Yes.

09:34.960 --> 09:40.800
And that's one step away from being able to render a movie in real time.

09:41.760 --> 09:49.280
So are we in the verge of basically, and we're sitting here in Los Angeles,

09:49.280 --> 09:51.360
and you've been in the Hollywood world.

09:51.360 --> 09:56.080
I mean, are we talking about rendering motion pictures by just writing,

09:56.080 --> 10:00.080
reading a script and having some stable diffusion,

10:00.080 --> 10:03.040
three-bottled, create a film in real time?

10:03.040 --> 10:04.160
Well, who needs to write a script?

10:04.160 --> 10:04.960
Okay, I did that.

10:07.760 --> 10:08.640
You just say it.

10:08.640 --> 10:10.240
I want something to make me happy, right?

10:10.240 --> 10:12.080
And then it'll pull together various models,

10:12.080 --> 10:13.280
and it can live-generate a movie.

10:13.280 --> 10:13.760
Yeah.

10:13.760 --> 10:15.040
So we're live-generating.

10:15.040 --> 10:18.160
So Hollywood, so in the next five years,

10:18.160 --> 10:20.800
yes, I think that ready player one awaits this world,

10:20.800 --> 10:23.120
minus the microtransactors of 18 ages will be here.

10:25.440 --> 10:25.680
Great.

10:25.680 --> 10:29.280
Anything you can imagine, iterate it, dynamic,

10:29.280 --> 10:33.520
new forms of communication, new affordances,

10:33.520 --> 10:34.880
like all these clicks and things like that.

10:34.880 --> 10:35.680
You don't need that anymore.

10:37.120 --> 10:39.920
Then this is one of the biggest evolutions in humanity ever,

10:40.560 --> 10:43.120
because the easiest way for us to communicate is what we're doing now

10:43.120 --> 10:44.160
and having a nice chat, right?

10:44.160 --> 10:45.280
Back and forth.

10:45.280 --> 10:46.640
Then the Gutenberg press came,

10:47.360 --> 10:49.840
and suddenly you could communicate through written form,

10:49.840 --> 10:50.720
and it's harder.

10:50.720 --> 10:51.440
Yes.

10:51.440 --> 10:55.360
But still, you see now GPT-3 and other of these AIs are made easier now.

10:55.360 --> 10:57.520
For anyone to do anything like chat, GPT just came out.

10:57.520 --> 10:58.080
It's amazing.

10:58.400 --> 11:00.080
Yes, I saw it from open AI.

11:00.080 --> 11:02.000
Everybody's making extraordinary claims

11:02.000 --> 11:03.440
about what it's going to do to Google.

11:03.440 --> 11:04.000
Exactly.

11:04.000 --> 11:06.720
Well, this is a very interesting thing.

11:07.840 --> 11:09.840
But then visual communication is the hardest,

11:10.880 --> 11:15.520
like you toil to be an artist or a lot of people on this call.

11:15.520 --> 11:17.520
How much have we toiled with PowerPoint?

11:18.240 --> 11:20.640
We're going to rid the world of the tyranny of PowerPoint.

11:20.640 --> 11:23.360
But that's my expertise from a PowerPoint expert.

11:23.360 --> 11:24.400
You're not a PowerPoint expert.

11:24.400 --> 11:25.920
You're a communication expert.

11:25.920 --> 11:29.120
So we remove the barriers whereby any visual medium can be created instantly,

11:29.120 --> 11:31.360
and you just describe to it how it shifts.

11:31.360 --> 11:35.200
And because it's got these latencies, it understands happier or sadder,

11:35.200 --> 11:38.320
or you can say flamboyant, and that's not a word,

11:38.320 --> 11:40.000
but it figures out what that kind of means.

11:40.000 --> 11:42.560
But the ability to rapidly iterate it and say,

11:42.560 --> 11:44.000
okay, I don't like that movie scene.

11:44.000 --> 11:47.600
Make it happier, make it brighter, make it more dramatic.

11:48.160 --> 11:52.480
And then being able to render any form of communications,

11:52.560 --> 11:57.200
like you just said, a movie and have a thousand variants,

11:57.200 --> 11:59.360
I mean, what's going to happen to Hollywood?

11:59.920 --> 12:02.240
I imagine Hollywood will be quite disintermediated.

12:02.240 --> 12:03.040
Yes.

12:03.040 --> 12:04.560
Hollywood in everyone's thing.

12:04.560 --> 12:05.840
I mean, what is Hollywood?

12:05.840 --> 12:09.520
Hollywood emerged because it was far enough away from the east coast

12:09.520 --> 12:10.880
that IP laws didn't apply.

12:12.720 --> 12:17.520
And then it got constructed around here as an entity

12:17.520 --> 12:20.880
that extracted rents from performers, ultimately, and creatives.

12:21.520 --> 12:25.200
Like how many creatives like the Hollywood or the music industry or others?

12:25.200 --> 12:28.000
Not many, because they tend to be treated quite badly.

12:28.000 --> 12:30.000
Some people achieve superstardom.

12:30.000 --> 12:32.720
But a lot of this technology now is just truly democratizing

12:32.720 --> 12:35.680
in that things are going from centralized to the edge,

12:35.680 --> 12:38.240
and we had to create centralized organizations as a people

12:38.240 --> 12:40.880
because we didn't have the information classification

12:40.880 --> 12:42.880
and communication tools to be more dynamic.

12:42.880 --> 12:44.640
It's called the representative democracy.

12:44.640 --> 12:45.840
It's not a true democracy.

12:45.840 --> 12:47.440
It's representative democracy, exactly.

12:47.440 --> 12:50.160
At the same time, everyone speaking at the same time

12:50.160 --> 12:51.360
doesn't make sense.

12:51.360 --> 12:53.920
But what we need, like the most successful organizations

12:53.920 --> 12:55.360
are complex hierarchical systems.

12:56.080 --> 12:59.440
It's groups of people working together loosely bound for a bigger story.

12:59.440 --> 13:00.880
And this is what we can achieve as humanity,

13:00.880 --> 13:03.440
like the human colossus when we all come together to do one big thing.

13:04.000 --> 13:05.680
But it gets blocked by a lot of the stuff

13:05.680 --> 13:07.760
because we're not communicating properly.

13:07.760 --> 13:09.840
Like one of the things I tell my team is that, you know,

13:09.840 --> 13:12.160
roadmaps are not about resources, they're about communication.

13:12.800 --> 13:15.040
If you communicate properly and something's good,

13:15.040 --> 13:16.640
you will always get resources for anything.

13:16.720 --> 13:19.200
It's also about creating a common vision

13:19.200 --> 13:21.520
that Erby's aiming towards in order to get there.

13:22.160 --> 13:24.720
So you're not diversified in a thousand directions.

13:24.720 --> 13:28.480
So like Google is full of amazingly smart people, right?

13:28.480 --> 13:30.640
They did an amazing study called Product Aristotle.

13:30.640 --> 13:30.880
Yes.

13:30.880 --> 13:33.360
Because it's like, why is one smart team better than another?

13:33.360 --> 13:34.400
What was the finding?

13:34.400 --> 13:36.160
The finding was it came up with two things.

13:36.160 --> 13:39.840
Common narrative and ideally something that kind of engages you

13:39.840 --> 13:42.720
and there's a bit of sacrifice, you know, like the...

13:42.720 --> 13:46.000
So this is what Salim and I and Salim is male who you know

13:46.000 --> 13:47.440
well who sends his regards.

13:47.440 --> 13:49.520
Talk about it as a massive transformative purpose,

13:49.520 --> 13:52.320
having a unifying, really compelling,

13:52.320 --> 13:55.040
emotionally charged vision that you're heading towards.

13:55.040 --> 13:57.440
Something deep and something that you've put yourself into as well.

13:57.440 --> 14:00.320
So it becomes part of your story as it were.

14:00.320 --> 14:02.240
And the second part was psychological safety.

14:03.200 --> 14:03.840
Which I think was very...

14:03.840 --> 14:04.800
Amongst the team.

14:04.800 --> 14:06.000
Amongst the team.

14:06.000 --> 14:08.080
The ability to actually express yourself

14:08.080 --> 14:10.400
and talk about something without fear of reproach.

14:10.400 --> 14:12.400
That's what makes the most successful teams.

14:12.400 --> 14:14.880
Because we're too scared often about kind of our status

14:15.120 --> 14:16.960
and worrying people and things like that.

14:16.960 --> 14:19.360
That's obviously on the research side of things primarily.

14:19.360 --> 14:21.360
But I think some of these lessons kind of come because

14:21.360 --> 14:22.960
when you feel comfortable as a community

14:22.960 --> 14:24.960
and you're working towards a massive transformative purpose,

14:25.520 --> 14:26.800
you can do a lot more.

14:26.800 --> 14:28.640
Because when you're falling behind, you can communicate it

14:28.640 --> 14:30.400
and you're not scared of people judging you.

14:30.400 --> 14:33.040
And if you have an idea that's considered, you know,

14:33.040 --> 14:38.400
divergent from the center, you feel open to being able to share it.

14:38.400 --> 14:40.320
In fact, it may well be the right idea.

14:40.320 --> 14:42.720
And I talk about the day before something is truly a breakthrough.

14:42.720 --> 14:43.680
It's a crazy idea.

14:43.760 --> 14:46.400
If you're scared about actually putting forward a crazy idea,

14:46.400 --> 14:47.440
then you're stuck.

14:47.440 --> 14:49.120
Well, again, it comes back to information theory,

14:49.120 --> 14:49.920
Shannon style, right?

14:49.920 --> 14:50.640
Yep.

14:50.640 --> 14:53.520
Information is only valuable in as much as it changes the state.

14:53.520 --> 14:54.080
Yes.

14:54.080 --> 14:55.920
Everyone's on the same page with everything.

14:55.920 --> 14:57.440
Flat equals dead.

14:57.440 --> 14:58.000
Exactly.

14:58.000 --> 15:00.160
And this is a time where we basically have to have

15:00.160 --> 15:01.600
exponential progress.

15:01.600 --> 15:03.200
And now we actually have the tools to help us

15:03.200 --> 15:04.960
to make exponential progress.

15:04.960 --> 15:08.320
Like, you know, Facebook released Galactica recently,

15:08.800 --> 15:12.640
which is their language model trained on 42 million science papers.

15:12.640 --> 15:16.000
They made some claims that were a bit hypey and then people

15:16.000 --> 15:18.800
like, oh, you can use this to create racist science papers.

15:18.800 --> 15:20.560
And so they were forced to take it down.

15:20.560 --> 15:21.920
But I mean, it's an amazing piece of tech

15:21.920 --> 15:23.120
and we're going to help re-release it.

15:24.240 --> 15:25.840
And this is an interesting thing.

15:25.840 --> 15:28.640
You can use it to do things like a null hypothesis creator.

15:28.640 --> 15:31.040
You can use this AI to do all the sorts of things

15:31.040 --> 15:34.240
that enable more divergent original thoughts

15:34.240 --> 15:36.480
or creativity or any of these other things.

15:36.480 --> 15:40.160
Like, classically, I couldn't because it was out of mode.

15:40.880 --> 15:42.240
It was out of the data set.

15:42.720 --> 15:46.640
Of course, if it's not in the data set, then you're screwed.

15:46.640 --> 15:48.400
Whereas now you don't need it to be in the data set.

15:48.400 --> 15:49.600
You can create new data.

15:49.600 --> 15:51.120
Yeah, that's extraordinary.

15:51.120 --> 15:52.880
This episode is brought to you by Levels.

15:52.880 --> 15:56.080
One of the most important things that I do to try and maintain

15:56.080 --> 16:00.720
my peak vitality and longevity is to monitor my blood glucose.

16:00.720 --> 16:03.040
More importantly, the foods that I eat

16:03.040 --> 16:05.840
and how they peak the glucose levels in my blood.

16:05.840 --> 16:08.240
Now, glucose is the fuel that powers your brain.

16:08.240 --> 16:09.520
It's really important.

16:09.520 --> 16:11.520
High, prolonged levels of glucose,

16:11.520 --> 16:14.880
which is called hyperglycemia, leads to everything

16:14.880 --> 16:20.000
from heart disease to Alzheimer's to sexual dysfunction to diabetes.

16:20.000 --> 16:21.280
And it's not good.

16:21.280 --> 16:23.120
The challenges, all of us are different.

16:23.120 --> 16:25.680
All of us respond to different foods in different ways.

16:25.680 --> 16:29.760
Like, for me, if I eat bananas, it spikes my blood glucose.

16:29.760 --> 16:31.680
If I eat grapes, it doesn't.

16:31.680 --> 16:35.840
If I eat bread by itself, I get this prolonged spike

16:35.840 --> 16:37.120
in my blood glucose levels.

16:37.120 --> 16:40.640
But if I dip that bread in olive oil, it blunts it.

16:40.640 --> 16:43.200
And these are things that I've learned from wearing

16:43.200 --> 16:46.400
a continuous glucose monitor and using the Levels app.

16:46.400 --> 16:51.200
So Levels is a company that helps you in analyzing

16:51.200 --> 16:53.120
what's going on in your body.

16:53.120 --> 16:55.520
It's continuous monitoring 24-7.

16:55.520 --> 16:56.880
I wear it all the time.

16:56.880 --> 16:59.840
It really helps me to stay on top of the food I eat,

16:59.840 --> 17:02.080
remain conscious of the food that I eat,

17:02.080 --> 17:04.880
and to understand which foods affect me

17:04.880 --> 17:07.840
based upon my physiology and my genetics.

17:07.840 --> 17:10.960
You know, on this podcast, I only recommend products

17:10.960 --> 17:14.080
and services that I use, that I use not only for myself,

17:14.080 --> 17:15.760
but my friends and my family,

17:15.760 --> 17:18.320
that I think are high quality and safe

17:18.320 --> 17:20.720
and really impact a person's life.

17:20.720 --> 17:21.920
So, check it out.

17:21.920 --> 17:24.800
Levels.link slash Peter.

17:24.800 --> 17:27.520
We give you two additional months of membership.

17:27.520 --> 17:30.640
And it's something that I think everyone should be doing.

17:30.640 --> 17:32.880
Eventually, this stuff is going to be in your body,

17:32.880 --> 17:36.400
on your body, part of our future of medicine today.

17:36.400 --> 17:39.120
It's a product that I think I'm going to be using

17:39.120 --> 17:41.600
for the years ahead and hope you'll consider as well.

17:41.600 --> 17:44.640
You know, when you go to your website,

17:44.640 --> 17:49.600
I love the notion of AI by the people, for the people.

17:50.160 --> 17:53.360
And then you say, stability AI is building open AI tools

17:53.360 --> 17:55.680
that will let us reach our potential.

17:56.400 --> 17:57.200
Let's talk about that.

17:57.200 --> 18:00.160
Because, you know, there's a lot of individuals

18:00.160 --> 18:04.240
that you and I both know that our fearmonger is around AI.

18:04.240 --> 18:05.280
You know, it's the devil.

18:05.280 --> 18:06.400
It's going to destroy us.

18:06.400 --> 18:07.280
It's going to...

18:07.280 --> 18:10.400
Superhuman AI is the end of humanity as we know it.

18:10.400 --> 18:14.240
And I mean, my position is AI is the single most important tool

18:14.240 --> 18:16.720
we're ever creating to solve the world's biggest problems.

18:16.720 --> 18:19.200
And we can't solve our problems from where we were before,

18:19.200 --> 18:21.120
but these are the tools that are going to allow us.

18:21.120 --> 18:22.080
How do you...

18:22.080 --> 18:24.960
So, how do you address the Bill Gates,

18:24.960 --> 18:28.400
the Elon to the world on that side, the fear side?

18:28.400 --> 18:29.440
Well, I think it fares a valid,

18:29.440 --> 18:31.120
because it's the most powerful technology

18:31.120 --> 18:32.960
we've ever created and it comes from us.

18:33.520 --> 18:34.960
But then who is us, right?

18:34.960 --> 18:36.320
Like, if you look our current data sets,

18:36.320 --> 18:37.760
they're massively biased.

18:37.760 --> 18:38.960
They're fixed towards the internet

18:38.960 --> 18:40.240
and they're fixed towards manipulation.

18:40.880 --> 18:43.760
The way I kind of look at AI is that organizations themselves are AI.

18:44.320 --> 18:47.680
They are slow dumb AI that feeds on us and turns us into cogs.

18:47.680 --> 18:49.520
In fact, this is concept of Molok, right?

18:49.520 --> 18:52.320
From the Ginsburg poem, you know, how?

18:52.320 --> 18:54.480
Talking about this Carthaginian demon

18:54.480 --> 18:56.720
that pervades our organizational structures

18:56.720 --> 18:59.760
and turns us into these cogs that feeds on us effectively.

18:59.760 --> 19:02.000
I think this is the first thing that can actually defeat that.

19:02.640 --> 19:04.480
This particular technology that we have today.

19:05.200 --> 19:06.480
Because is the world happy now?

19:07.200 --> 19:09.040
How many people in organizations are happy?

19:09.040 --> 19:10.160
We all know, you know?

19:10.720 --> 19:13.040
We should talk about my idea for a happiness X-Prize,

19:13.040 --> 19:14.080
but that's a different conversation.

19:14.080 --> 19:15.520
Happiness X-Prize, but what is happiness?

19:15.520 --> 19:16.400
Happiness is agency.

19:16.400 --> 19:18.000
Happiness is achieving your potential.

19:18.000 --> 19:20.000
And it's kind of going out there, so we need some help.

19:20.000 --> 19:21.520
But I think a lot of the AI discourse

19:21.520 --> 19:23.920
has been focused on gigantic language models

19:23.920 --> 19:26.080
and fricking supercomputers, which we still have,

19:26.720 --> 19:30.560
to create something that will equal and surpass us.

19:30.560 --> 19:32.320
It's very religious in its way.

19:32.320 --> 19:34.240
And you see parallels to religion across this.

19:34.240 --> 19:34.640
Sure.

19:34.640 --> 19:37.360
In that you have the kind of declaiming people who like for it,

19:37.360 --> 19:39.200
the people who call it heretical.

19:39.200 --> 19:42.080
And then you kind of even have most of ethics in AI.

19:42.080 --> 19:43.360
It's not actually ethics.

19:44.000 --> 19:47.120
Instead, it's ultra-orthodoxy, as it were.

19:47.680 --> 19:48.800
Like in Islamic terms,

19:48.800 --> 19:50.640
everything is haram unless it's declared halal.

19:51.760 --> 19:53.360
You know, classic Jewish,

19:53.360 --> 19:54.720
ultra-orthodox Judaism, etc.

19:54.720 --> 19:55.760
It's all the same thing.

19:55.760 --> 19:57.120
People look at red teaming.

19:57.120 --> 19:59.440
They don't look at green teaming, right?

19:59.440 --> 20:02.160
And they look at this technology as being too powerful.

20:02.160 --> 20:04.400
So just like cryptography, we should keep it from the people.

20:04.400 --> 20:06.640
And we definitely shouldn't give it to emerging markets

20:06.640 --> 20:08.160
and people who aren't smart enough to do it.

20:09.040 --> 20:11.360
And that's very interesting because, again,

20:11.360 --> 20:12.800
if you think about the power of it

20:12.800 --> 20:13.920
and you believe it's powerful,

20:14.640 --> 20:16.640
then the question should be, what is this?

20:16.640 --> 20:18.720
And I believe that this AI is infrastructure.

20:19.680 --> 20:23.600
Clayton Christensen, kind of the departed mental mind,

20:23.600 --> 20:25.840
had an amazing quote, which is that,

20:25.840 --> 20:27.600
infrastructure is the most efficient means

20:27.600 --> 20:29.840
by which society stores and distributes value.

20:30.880 --> 20:32.640
Now, obviously, that's ports and things like that.

20:32.640 --> 20:33.040
Sure.

20:33.040 --> 20:34.400
But it's also information.

20:34.400 --> 20:35.040
Sure.

20:35.040 --> 20:36.240
And I think that's valuable when you

20:36.240 --> 20:38.000
confine it with the Shannon theory.

20:38.000 --> 20:40.000
So this AI is infrastructure

20:40.000 --> 20:41.600
for the next generation of human thought.

20:42.160 --> 20:43.040
And what does that mean?

20:43.040 --> 20:44.400
It should mean that it should be a commons

20:45.120 --> 20:46.720
that is accessible to everyone.

20:47.280 --> 20:52.320
And you made a very definitive decision

20:52.320 --> 20:55.520
to make this an open-source movement here.

20:55.520 --> 20:55.760
Yeah.

20:55.760 --> 20:56.880
Yeah.

20:56.880 --> 20:58.640
Your community's how large now?

20:58.640 --> 21:01.280
I think we've got about 120,000.

21:01.280 --> 21:01.840
Amazing.

21:01.840 --> 21:02.640
In the communities.

21:02.640 --> 21:04.720
And we created communities across verticals.

21:04.720 --> 21:07.120
Classical open-source product related,

21:07.120 --> 21:09.200
where you could have MongoDB or something like that,

21:09.200 --> 21:10.080
and then you've got there.

21:10.080 --> 21:14.720
Whereas we said language, healthcare, BioML, audio.

21:14.720 --> 21:16.400
Let's get all the people who are fantastic

21:16.400 --> 21:17.600
in the private sector, public sector,

21:17.600 --> 21:19.200
actively independent together.

21:19.200 --> 21:20.560
And let's jam on,

21:20.560 --> 21:22.640
how do we build a next generation infrastructure?

21:22.640 --> 21:24.400
The way I put it is, let's go to the future

21:24.400 --> 21:25.520
and bring back AI with us.

21:26.160 --> 21:26.400
Nice.

21:26.400 --> 21:27.920
And we can choose if it's a panopticon

21:27.920 --> 21:30.160
controlled by corporations, like Web2,

21:30.160 --> 21:31.520
or we can choose if it's open,

21:31.520 --> 21:33.120
an infrastructure for humanity.

21:33.120 --> 21:34.160
And if it's infrastructure for humanity,

21:34.160 --> 21:35.440
I think it's an important point,

21:35.440 --> 21:36.400
because I'm just finally saying that.

21:37.040 --> 21:39.520
A lot of the naysayers around AGI and ASI and others,

21:40.800 --> 21:43.280
I would agree with them if it's controlled

21:43.280 --> 21:44.320
by corporations,

21:44.320 --> 21:46.000
which are this type of weird entity

21:46.000 --> 21:46.800
that we've created.

21:47.440 --> 21:49.840
Like YouTube optimized for extremism,

21:49.840 --> 21:50.880
because it was engaging,

21:50.880 --> 21:52.880
so ISIS co-opted those algorithms.

21:52.880 --> 21:54.400
And they adjusted eventually, but it was slow.

21:55.040 --> 21:56.640
If it comes from us and it's biased,

21:56.640 --> 21:57.760
and we've brought in the conversation,

21:57.760 --> 21:59.280
I think AI is more finalist,

21:59.280 --> 22:00.400
likely to kill us all,

22:00.400 --> 22:01.760
if it ever becomes sentient,

22:01.760 --> 22:03.200
which is a big question, you know?

22:04.320 --> 22:05.920
Especially if it's representative.

22:05.920 --> 22:09.680
So I want to come back to this in greater detail later,

22:09.680 --> 22:12.560
but I think we share a common belief

22:12.560 --> 22:14.880
that humanity ultimately is good.

22:15.440 --> 22:16.240
Yes.

22:16.240 --> 22:17.440
At its fundamental level,

22:17.440 --> 22:19.760
because that's a very important distinction.

22:19.760 --> 22:22.000
If you believe that humans,

22:22.000 --> 22:23.440
at their base, are good,

22:23.440 --> 22:25.120
and you're enabling humans

22:25.120 --> 22:26.960
with more and more powerful technology,

22:26.960 --> 22:28.080
they're going to be using that

22:28.080 --> 22:29.360
to make the world a better place

22:29.360 --> 22:32.080
and solve problems on the whole.

22:32.080 --> 22:33.680
On the whole, but it depends on

22:33.680 --> 22:35.600
what the perspective of humanity is.

22:35.600 --> 22:37.280
You know, you've kind of got,

22:37.280 --> 22:38.640
what's it called, the thing when you go to space

22:38.640 --> 22:39.440
and you look at Earth?

22:39.440 --> 22:40.400
Yeah, the overview effect.

22:40.400 --> 22:41.280
The overview effect.

22:41.280 --> 22:41.680
Yeah.

22:41.680 --> 22:43.120
You know, like hopefully more and more people

22:43.120 --> 22:44.400
get to space and have that,

22:44.400 --> 22:45.920
because people are very narrow,

22:45.920 --> 22:47.840
and they view themselves like that.

22:48.480 --> 22:50.640
They're, Rabbi Sacks,

22:50.720 --> 22:51.760
former Chief Rabbi of the UK,

22:51.760 --> 22:52.960
had a very wonderful concept

22:52.960 --> 22:54.000
called altruistic evil.

22:55.120 --> 22:56.160
Those who actually do evil

22:56.160 --> 22:56.960
believe they're doing good.

22:57.680 --> 22:58.160
Interesting.

22:58.800 --> 22:59.600
And you see that.

22:59.600 --> 23:01.120
Like, you know, if you talk to them.

23:01.120 --> 23:02.800
Especially in the religious realm.

23:02.800 --> 23:03.680
Religious or anything like that.

23:03.680 --> 23:05.920
I'm like, Isaiah Berlin's conceptualization,

23:07.040 --> 23:09.920
British philosopher from like the 1940s,

23:09.920 --> 23:12.320
he had a conceptualization of positive liberty

23:12.320 --> 23:13.600
versus negative liberty.

23:13.600 --> 23:15.040
So negative liberty was the freedom

23:15.040 --> 23:16.480
from anyone telling you what to do,

23:16.480 --> 23:17.440
and then to kind of,

23:17.440 --> 23:19.440
laissez-faire capitalism and things like that.

23:19.440 --> 23:22.000
Positive liberty was the ability to believe in an ism.

23:22.000 --> 23:24.240
Something big, like fascism,

23:24.240 --> 23:27.600
communism, capitalism, Islamism, etc.

23:27.600 --> 23:29.120
And people use that as excuses,

23:29.120 --> 23:30.800
being they were doing good to actually do bad.

23:31.360 --> 23:32.800
But then how does this all relate to this?

23:32.800 --> 23:34.560
I believe people inherently want to do good.

23:34.560 --> 23:36.080
It's just that what good is,

23:36.720 --> 23:38.480
can become misdefined and corrupted.

23:39.200 --> 23:40.320
And so my take was,

23:40.320 --> 23:41.760
if we start building infrastructure,

23:41.760 --> 23:43.520
where people can see bigger perspectives,

23:43.520 --> 23:45.200
because they get the information they need,

23:45.200 --> 23:45.920
what does that look like?

23:45.920 --> 23:48.480
What if we mapped all the religious texts in the world,

23:48.480 --> 23:51.200
so that a child in Egypt could see Judaism

23:51.200 --> 23:52.960
from the perspective of a child in Jerusalem?

23:54.080 --> 23:55.120
We have the technology to do that.

23:55.120 --> 23:56.640
What if you could automatically translate

23:57.680 --> 24:01.200
tea-party republicanism into libertarianism,

24:01.200 --> 24:02.160
and have commonality there?

24:02.160 --> 24:03.920
Again, we finally have the technology to do that.

24:04.480 --> 24:05.520
But we didn't before,

24:05.520 --> 24:07.520
so people remain in their huddles,

24:07.520 --> 24:08.640
they look at the other,

24:08.640 --> 24:09.680
and they're encouraged to do so.

24:09.680 --> 24:12.000
We've seen that increase in political polarization.

24:12.000 --> 24:13.200
So society is a tipping point.

24:13.200 --> 24:14.320
And by the way, we've seen,

24:14.320 --> 24:16.160
I mean, that's been what social media

24:16.160 --> 24:18.240
has effectively done so efficiently.

24:18.480 --> 24:21.040
Because it's beneficial for the algorithms,

24:21.040 --> 24:24.160
and for the slow, dumb AIs of the corporations that drive it.

24:25.120 --> 24:28.240
Let's come back to the company and the products,

24:28.240 --> 24:30.880
just to lay out the tapestry here.

24:31.920 --> 24:35.920
Stable diffusion is one product vertical area.

24:35.920 --> 24:37.040
What are the other ones?

24:37.040 --> 24:41.840
So for example, our Luther AI community has GPT Neo and X.

24:41.840 --> 24:44.560
It's the open source version of GPT-3 by OpenAI.

24:45.280 --> 24:47.520
The most popular open language model in the world,

24:47.600 --> 24:49.760
it's been downloaded 25 million times.

24:49.760 --> 24:50.640
Incredible.

24:50.640 --> 24:51.680
And so can you take it?

24:51.680 --> 24:53.120
You customize it for yourself?

24:53.120 --> 24:54.240
No permission needed, right?

24:55.040 --> 24:56.720
Harmony has danced diffusion,

24:56.720 --> 24:58.960
which is the most advanced audio model in the world.

24:59.600 --> 25:00.800
You'll be able to create your music,

25:00.800 --> 25:01.840
so you put your own music in it,

25:01.840 --> 25:02.960
and then you have your own music model

25:02.960 --> 25:04.560
that can create more music of your style.

25:04.560 --> 25:08.080
And you can just basically produce it,

25:08.800 --> 25:10.000
produce your own concerts,

25:10.560 --> 25:12.400
extrapolate in any direction you want.

25:12.400 --> 25:13.280
Exactly.

25:13.280 --> 25:14.480
We have OpenBioML,

25:14.480 --> 25:16.640
where we kind of have OpenFold and LibreFold,

25:16.640 --> 25:20.640
protein folding, DNA diffusion for DNA protein matching,

25:20.640 --> 25:25.680
and OpenBioML for, again, some protein kind of stuff.

25:25.680 --> 25:29.520
So we've spent some time talking about

25:30.400 --> 25:32.960
a passion that I have and you share,

25:32.960 --> 25:36.560
which is the ability to truly transform

25:36.560 --> 25:39.280
the healthcare industry and human longevity,

25:39.280 --> 25:43.520
understanding why we age, maybe why we don't have to.

25:43.520 --> 25:46.080
I think the why is the important part, right?

25:46.080 --> 25:47.200
Most of medicine.

25:47.200 --> 25:49.360
So again, when my son was diagnosed

25:49.360 --> 25:51.360
and scratching a wall to his fingernails bled,

25:52.320 --> 25:53.680
eventually went to mainstream school,

25:53.680 --> 25:55.040
thanks to the interventions.

25:55.040 --> 25:56.480
And I want to come back to those,

25:56.480 --> 25:57.440
for those who are listening,

25:57.440 --> 26:01.840
who have a child with autism or know someone,

26:01.840 --> 26:03.520
I want to come back afterwards.

26:03.520 --> 26:06.320
What were your learnings and what's your advice?

26:06.320 --> 26:07.680
Yeah, there's a lot in this space

26:07.680 --> 26:09.200
that we kind of occupy, of course.

26:09.920 --> 26:13.440
But what I realized is that conventional medicine

26:14.400 --> 26:17.040
and a lot of things view humans as a girdic.

26:17.040 --> 26:18.720
A thousand tosses of a coin is the same

26:18.720 --> 26:20.640
as a thousand toins cost at once.

26:22.320 --> 26:23.360
Coins tossed at once.

26:24.400 --> 26:25.600
But humans are individual.

26:25.600 --> 26:27.520
So for example, a good proportion of humans

26:27.520 --> 26:30.160
have a cytochrome P450 mutation in their liver,

26:30.160 --> 26:33.440
which means they metabolize things like fentanyl

26:33.440 --> 26:34.880
and opioids far quicker.

26:34.880 --> 26:35.360
Yeah.

26:35.360 --> 26:37.760
It's a very simple SMP test, but we don't do it.

26:37.760 --> 26:39.360
So we just prescribe everyone the same thing

26:39.360 --> 26:40.480
and then a bunch get addicted.

26:41.440 --> 26:42.080
You know?

26:42.080 --> 26:44.640
Because we are individualistic kind of creatures.

26:44.640 --> 26:46.160
This is why I went to the first principles

26:46.160 --> 26:47.760
thinking kind of approach on this.

26:47.760 --> 26:49.600
And the question is this personalized medicine thing

26:49.600 --> 26:51.440
has always been kind of out there.

26:51.440 --> 26:52.800
We've not been able to reach it.

26:52.800 --> 26:54.720
Again, I think the technology we have right now

26:54.720 --> 26:56.320
enables personalized medicine.

26:56.320 --> 26:58.880
We've seen things like CRISPR, obviously, and others.

26:59.680 --> 27:02.320
But more than that, it's about data availability

27:02.320 --> 27:03.200
and viability.

27:03.200 --> 27:04.320
But we do need to get to a point

27:04.320 --> 27:06.560
where every child at birth is sequenced

27:06.560 --> 27:09.440
and that is plugged into their personal AI

27:09.440 --> 27:11.760
and they understand exactly how every food, every medicine,

27:12.400 --> 27:16.240
and every aspect of our living affects our physiology.

27:16.240 --> 27:19.600
Again, you go to the future and you bring back the AI with you

27:19.600 --> 27:21.680
and you say, what should it look like?

27:21.680 --> 27:23.520
So you take a whole country and you say,

27:23.520 --> 27:25.440
how do we build an amazing health care system,

27:25.440 --> 27:26.720
education system, et cetera?

27:27.600 --> 27:30.880
Is there any doubt that health care and education

27:30.880 --> 27:33.440
will have AI at the core in 20 years?

27:33.440 --> 27:33.840
Zero.

27:33.840 --> 27:36.320
Well, I think it's not 20 years, it's 10 years.

27:36.320 --> 27:36.640
I know.

27:36.640 --> 27:37.680
But let's just say 20 years.

27:37.680 --> 27:38.720
Fine.

27:38.800 --> 27:40.800
You and me, we're like, now, now, now.

27:40.800 --> 27:41.920
Let's say 20 years, right?

27:42.960 --> 27:46.080
But then is that AI open or closed?

27:47.120 --> 27:47.520
Yeah.

27:47.520 --> 27:49.600
And is it better for it to be open or closed?

27:49.600 --> 27:50.400
There's no question.

27:50.400 --> 27:53.120
Open is fundamentally critical.

27:53.120 --> 27:54.320
Open is fundamentally critical

27:54.320 --> 27:56.320
because then we build, it is infrastructure,

27:56.320 --> 27:57.520
it is valuable.

27:57.520 --> 27:59.520
Like the way that I actually orient my rights,

28:00.240 --> 28:02.560
Vinay Gupta, I was one of the Ethereum guys,

28:02.560 --> 28:04.000
I think you probably know him.

28:04.000 --> 28:05.600
Big thinker, like crazy.

28:06.320 --> 28:08.320
He had a very great conceptualization of rights

28:08.320 --> 28:10.720
which I agree with, which is the rights of children.

28:11.600 --> 28:14.480
And so like effective altruism and all of that,

28:14.480 --> 28:16.000
looking at people a million years in advance,

28:16.000 --> 28:17.200
it's kind of difficult.

28:17.200 --> 28:18.640
And it comes down to utilitarianism,

28:18.640 --> 28:19.520
all sorts of weird stuff.

28:20.080 --> 28:21.440
But the rights of a child.

28:21.440 --> 28:22.560
Rights of a child today.

28:22.560 --> 28:22.880
Yes.

28:23.440 --> 28:24.000
Today.

28:24.000 --> 28:25.440
What does that child have the right to?

28:25.440 --> 28:26.640
Achieving their potential.

28:26.640 --> 28:27.120
Yes.

28:27.120 --> 28:29.040
What infrastructure do we need to give that child

28:29.040 --> 28:33.120
in a refugee camp or in Brooklyn or in Kensington

28:33.120 --> 28:34.640
to help them achieve their potential?

28:34.640 --> 28:36.400
And this goes back to what's on your website

28:36.400 --> 28:41.280
in terms of using AI to help, in this case, a child,

28:41.280 --> 28:44.720
and in the broader case, humanity, achieve its potential.

28:44.720 --> 28:45.200
Achieve it.

28:45.200 --> 28:46.880
I mean, that's everything.

28:46.880 --> 28:47.280
Yeah.

28:47.280 --> 28:48.480
And so for me, it's about...

28:49.440 --> 28:51.600
I've oriented that on agency and happiness.

28:52.240 --> 28:54.640
And it sounds fuzzy, but it's literally,

28:54.640 --> 28:57.040
when you have the tools to be able to do anything,

28:57.040 --> 28:58.400
you know you can do anything.

28:58.400 --> 29:00.000
People underestimate their agency.

29:00.640 --> 29:02.720
Sometimes you've got to go for your shot, as it were.

29:02.720 --> 29:03.360
Yeah.

29:03.360 --> 29:04.400
But again, it's information.

29:04.400 --> 29:06.000
What information can I bring to that person?

29:06.000 --> 29:09.120
What tools can I give them so that they can be creative?

29:09.120 --> 29:11.600
Because we lose that creative spark as we get older, right?

29:11.600 --> 29:11.920
Yeah.

29:11.920 --> 29:13.360
Now it's coming back of it, you know?

29:13.360 --> 29:15.520
What can I give them so they can access the information

29:15.520 --> 29:17.040
they need to be educated?

29:17.040 --> 29:19.440
And what's the optimal way to teach linear algebra?

29:19.440 --> 29:21.120
And by the way, it's all about play.

29:21.120 --> 29:22.160
It's all about play.

29:22.160 --> 29:23.600
It is all about play.

29:23.600 --> 29:27.040
Happiness play, all the neurochemistry of your brain

29:27.040 --> 29:30.000
is maximized for learning, retention, experimentation

29:30.000 --> 29:30.720
around that.

29:30.720 --> 29:31.440
It's flow.

29:31.440 --> 29:32.160
It's flow.

29:32.160 --> 29:34.000
So I used to be a video game investor.

29:34.000 --> 29:34.960
That was my big sector.

29:34.960 --> 29:36.080
It was one of the biggest in the world.

29:36.080 --> 29:38.720
And I used to judge video games by time to fun, flow,

29:38.720 --> 29:39.360
and frustration.

29:40.960 --> 29:42.880
And so if we're building systems for humanity,

29:42.880 --> 29:45.120
we have to look at fun, flow, and frustration.

29:45.920 --> 29:48.480
Because if we can get those, you know when you're just

29:48.480 --> 29:50.320
learning something, like, wow, this is amazing,

29:50.320 --> 29:51.120
and how do you get in there?

29:51.840 --> 29:53.840
It absorbs amazingly quickly.

29:53.840 --> 29:55.680
But our education system is not set up for that.

29:55.680 --> 29:57.040
Our health care system definitely isn't.

29:57.040 --> 29:58.400
And you've got to mess up health care system.

29:58.400 --> 29:58.720
Yeah, I'd say.

29:58.720 --> 29:59.280
Oh, my gosh.

29:59.280 --> 30:01.280
It's maximizing for frustration.

30:01.280 --> 30:03.120
Yeah, that's what I speak about it.

30:03.200 --> 30:06.480
Openly saying my mission is to crumble and destroy

30:06.480 --> 30:07.600
the health care system.

30:07.600 --> 30:11.120
And also education system, which is really sad.

30:11.120 --> 30:12.320
So again, it's very interesting.

30:12.320 --> 30:14.240
You look at the US in inflation numbers.

30:14.880 --> 30:17.040
Education and health care, massive inflation.

30:17.040 --> 30:17.760
Yes.

30:17.760 --> 30:19.200
Everything else, not really.

30:19.200 --> 30:21.760
Yeah, and it's the percentage of the person's income.

30:23.280 --> 30:26.000
And it should be going to zero, right?

30:26.000 --> 30:28.080
The top health care and the top education

30:28.080 --> 30:29.360
should be all AI driven.

30:29.360 --> 30:31.760
It's all basically the cost of electrons.

30:31.760 --> 30:33.920
Life expectancy is falling in the US.

30:33.920 --> 30:36.160
A little bit over the last couple of years due to COVID.

30:36.160 --> 30:36.480
Yeah.

30:36.480 --> 30:37.680
Yeah, but actually, no.

30:37.680 --> 30:38.880
No, before COVID.

30:38.880 --> 30:40.480
Before COVID, it was falling.

30:40.480 --> 30:40.560
Yeah.

30:40.560 --> 30:41.120
I mean, that's the thing.

30:41.120 --> 30:44.400
Whereas, again, it's not complicated,

30:44.400 --> 30:46.320
but it does require coordination.

30:46.320 --> 30:48.480
So the question is, can you create shelling points?

30:48.480 --> 30:52.320
So for me, open source software, this next generation,

30:52.320 --> 30:54.240
this model based one,

30:54.240 --> 30:56.720
whereby stable diffusion is basically a programming primitive.

30:57.440 --> 30:59.600
Just like you have a library to do various things.

30:59.600 --> 31:01.120
So the program is out there.

31:01.120 --> 31:03.280
So in the good old days, when we started programming,

31:03.280 --> 31:04.960
we used to have to code everything by hand.

31:04.960 --> 31:06.240
Now, you've got GitHub and things like that.

31:06.240 --> 31:07.600
It's more like playing with Lego, right?

31:07.600 --> 31:08.000
Yes.

31:08.000 --> 31:09.360
And you're sticking it all together.

31:09.360 --> 31:12.480
It's a new type of thing, a 1.6 GB file that is hashed.

31:13.040 --> 31:15.600
So it can be common across every single computer in the world

31:16.480 --> 31:19.440
that you can call something in and an image comes out.

31:20.080 --> 31:22.000
And you know predictably what that is,

31:22.000 --> 31:22.800
no matter where you are.

31:22.800 --> 31:25.520
And you can take an image and put text on the other way.

31:26.640 --> 31:26.960
Yeah.

31:26.960 --> 31:28.240
That changes the paradigm.

31:28.240 --> 31:30.320
But then what if you have that for language, audio,

31:30.320 --> 31:31.440
all these different modalities?

31:31.440 --> 31:31.760
Okay.

31:31.760 --> 31:34.160
I'm going to go here next then on that,

31:34.160 --> 31:38.000
which is what happens to electoral property rights, IP rights?

31:38.000 --> 31:41.840
Who owns the IP of those images?

31:41.840 --> 31:44.480
Is it the person who puts the prompt in?

31:45.520 --> 31:49.680
Is it, you know, help me understand where that evolves to?

31:49.680 --> 31:50.640
Nobody knows.

31:50.640 --> 31:51.200
Nobody knows.

31:51.200 --> 31:51.360
Okay.

31:51.360 --> 31:52.240
It's a fair answer.

31:52.240 --> 31:52.400
Yeah.

31:52.400 --> 31:53.040
I mean, nobody knows.

31:53.040 --> 31:56.560
I personally think it should belong to the person that prompts it.

31:56.560 --> 31:59.520
Because again, we put this out as a commons for humanity.

32:00.320 --> 32:03.280
Like we did put an ethical use license around it for various reasons

32:03.280 --> 32:05.840
that will be replaced by a purely open source license.

32:06.560 --> 32:09.360
But again, I'm viewing this as building blocks, right?

32:09.360 --> 32:11.280
It's really the person that has the action

32:11.280 --> 32:13.120
because these models do not have agency.

32:13.760 --> 32:15.360
If you're going to breach property right,

32:15.360 --> 32:17.760
you can type in, I want a picture of Mickey Mouse.

32:17.760 --> 32:19.520
And then if you sell that, done.

32:19.520 --> 32:20.560
But it's like photos.

32:20.560 --> 32:21.440
It's like telephone.

32:21.440 --> 32:22.240
It's like internet.

32:22.240 --> 32:23.200
It's like Photoshop.

32:23.920 --> 32:26.320
This is a tool, but it's a tool of a very different type.

32:26.960 --> 32:30.080
Because like, something like Photoshop is a million lines of code.

32:31.280 --> 32:34.160
This is a binary file that can do anything that Photoshop does.

32:34.160 --> 32:35.760
But you still need to act on it.

32:35.760 --> 32:40.080
Last year, there was a patent awarded to an AI in South Africa,

32:40.080 --> 32:40.800
which was interesting.

32:40.800 --> 32:42.400
I think more of a gimmick than anything else.

32:43.200 --> 32:47.440
Do you imagine these models will lead

32:47.440 --> 32:50.400
towards significant intellectual property

32:50.400 --> 32:52.080
on the invention side of the equation?

32:52.080 --> 32:52.640
Oh, 100%.

32:52.640 --> 32:53.280
They already are.

32:53.280 --> 32:54.800
If you look at Google's new TPU chips,

32:54.800 --> 32:56.640
they're partially built by AI that they built.

32:56.640 --> 32:56.960
Sure.

32:56.960 --> 32:57.360
Yes.

32:57.360 --> 32:57.840
I know.

32:57.840 --> 33:03.760
Like we just released OpenLM from our Carpalab,

33:03.760 --> 33:06.960
which is an evolutionary algorithm for code for robots.

33:08.160 --> 33:10.240
So we're trying to optimize robotics via that.

33:10.240 --> 33:12.560
And then when we bring in video, we'll have even better robots.

33:13.120 --> 33:15.200
So are we going to get to a place where I'm saying,

33:15.200 --> 33:19.120
listen, please invent a device that does this for me,

33:19.120 --> 33:20.720
that's under this price point,

33:20.720 --> 33:22.720
that's made of commonly available materials

33:23.280 --> 33:26.000
and constrain it left, right, and center,

33:26.000 --> 33:28.880
and then design it, now go and print it for me

33:28.880 --> 33:30.160
and deliver it for me the next day.

33:30.160 --> 33:33.200
We're going from mine to materialization in one sense.

33:33.200 --> 33:33.760
Yes.

33:33.760 --> 33:34.400
Yeah.

33:34.400 --> 33:36.320
I mean, again, it's kind of the,

33:36.320 --> 33:37.600
was it the Star Trek thing?

33:37.600 --> 33:39.440
Yeah, it's a little bit longer.

33:39.440 --> 33:42.400
Yeah, it's the, oh my God.

33:42.400 --> 33:43.200
We've both forgotten it.

33:43.200 --> 33:43.600
Yeah.

33:43.600 --> 33:45.600
Anyway, you know, when Joe Geordi gets the replicator.

33:45.600 --> 33:46.960
The replicator, exactly.

33:46.960 --> 33:48.880
They're all great to your heart.

33:48.880 --> 33:49.600
Exactly.

33:49.600 --> 33:51.040
But then you can kind of see this already,

33:51.040 --> 33:52.960
because there's an app on the App Store right now.

33:53.600 --> 33:54.640
You pull your Lego out.

33:55.600 --> 33:56.880
And it scans all the Lego.

33:56.880 --> 33:57.440
Oh, nice.

33:57.440 --> 33:58.560
What can you build with it?

33:58.560 --> 33:59.040
Yes.

33:59.040 --> 34:01.120
It comes up with, this is how you build it as well.

34:02.080 --> 34:04.720
So things like you can build birds and cars and things like that.

34:04.720 --> 34:05.200
Awesome.

34:05.200 --> 34:06.000
So you go, da-da-da.

34:07.360 --> 34:08.480
So this is just an extension of that.

34:08.480 --> 34:10.880
Again, they use a transformer based architecture for that.

34:10.880 --> 34:13.680
Well, I'm interested when it'd be fun to say,

34:13.680 --> 34:15.360
okay, create new life forms.

34:16.080 --> 34:17.840
I mean, we're not far from that.

34:17.840 --> 34:20.000
Well, you know, I'm going to leave that to other bioethicists.

34:20.000 --> 34:21.280
I don't want to get stuck in that one.

34:21.280 --> 34:25.520
Wait, wait, there's ethics involved in that?

34:25.520 --> 34:26.880
Okay, exactly.

34:26.880 --> 34:29.040
I mean, the life form thing is very interesting though, right?

34:29.040 --> 34:32.000
Because again, everything is happening all at once.

34:32.800 --> 34:37.520
And so it's not just biology or physics or sociology.

34:37.520 --> 34:39.840
All these models, all these technologies,

34:39.840 --> 34:41.600
all seem to be converging at once.

34:41.600 --> 34:43.120
So you can create anything.

34:43.120 --> 34:44.960
And all the barriers are dropping at once.

34:44.960 --> 34:46.160
And that's complicated.

34:46.160 --> 34:46.640
Yeah.

34:46.640 --> 34:51.760
So the world is, do you think anybody truly understands

34:51.760 --> 34:54.240
how fast the world is about to change?

34:54.800 --> 34:55.120
No.

34:55.120 --> 34:56.720
I mean, like, look at creative industry.

34:56.720 --> 34:59.120
Videogames, $180 billion a year,

34:59.120 --> 35:03.760
like Disney spends $10 billion a year, Amazon $16 billion a year on content.

35:04.400 --> 35:06.880
All of that's going to change in the next couple of years alone,

35:06.880 --> 35:09.760
just from one tiny little two gigabyte file.

35:09.760 --> 35:10.720
Extraordinary.

35:10.720 --> 35:13.200
As well as healthcare, as well as education,

35:13.200 --> 35:15.120
as well as you said, every single industry,

35:15.120 --> 35:18.240
we're going to, so here's another question.

35:18.240 --> 35:18.960
Go ahead, please.

35:18.960 --> 35:22.160
Well, I was going to say industry is about, again, information theory.

35:22.160 --> 35:22.560
Yeah.

35:22.560 --> 35:24.400
That's, most industries are based on that,

35:24.400 --> 35:25.760
especially service-based industries.

35:26.400 --> 35:29.600
And so once you can basically take a system,

35:29.600 --> 35:32.080
you can have human input in the loop to train a system

35:32.080 --> 35:33.920
that's a generalist, to understand principles.

35:34.640 --> 35:36.240
You disrupt just about everything.

35:36.240 --> 35:36.800
Yeah.

35:36.800 --> 35:40.480
I'm going to pause on that because it's probably one of the most important things.

35:40.480 --> 35:44.080
Any entrepreneur, any CEO, any parent, any kid,

35:44.080 --> 35:51.120
anyone needs to understand, we're about to enter a period of hyper-disruption

35:51.120 --> 35:53.520
and growth and hyper-opportunity creation.

35:53.520 --> 35:54.000
Yes.

35:54.000 --> 35:57.760
Like, what happens is that in any area you create,

35:57.760 --> 35:58.800
there are value spikes.

35:58.800 --> 36:00.480
So you can look at it as like a flat area,

36:00.480 --> 36:03.520
and then companies and individuals occupy certain areas.

36:03.520 --> 36:05.440
So they've got like a mix of skills, right?

36:05.440 --> 36:06.640
And that's how you earn your living.

36:07.280 --> 36:09.440
And these are like spikes.

36:09.440 --> 36:10.640
That's all going to get shaken up,

36:10.640 --> 36:12.320
and it's going to be the value is elsewhere.

36:12.880 --> 36:13.040
Yeah.

36:13.040 --> 36:14.400
We don't know where the value will be.

36:14.400 --> 36:15.120
We've got some guesses.

36:15.120 --> 36:17.600
So like in a time when everyone can make anything,

36:17.600 --> 36:18.880
what becomes valuable?

36:18.880 --> 36:19.360
Something.

36:19.920 --> 36:20.160
Yes.

36:20.160 --> 36:21.600
So if a model that can make anything,

36:21.600 --> 36:23.360
that means Disney should have their own models, right?

36:23.920 --> 36:26.160
To create Mickey Mouse's and things like that.

36:26.160 --> 36:28.080
But now they can use that not only internally

36:28.080 --> 36:30.640
to save money on creation, they can use externally.

36:30.640 --> 36:33.600
Why can't we have Mickey Mouse having coffee

36:33.600 --> 36:36.960
with Master Chief at a Starbucks and microtransact pay that?

36:36.960 --> 36:38.880
It's been the promise of the NFTs and things like that.

36:38.880 --> 36:38.960
Yes.

36:38.960 --> 36:41.120
Now with these models, you can do that seamlessly.

36:41.120 --> 36:42.720
Yeah, on demand.

36:43.040 --> 36:44.000
Over and over again.

36:44.000 --> 36:46.640
So one of the realizations for me is that

36:47.440 --> 36:51.440
this open source technology made available to everyone

36:52.320 --> 36:58.160
effectively has the potential to make everyone the equivalent

36:58.160 --> 37:01.120
of millionaires, billionaires, trillionaires

37:01.120 --> 37:04.240
when whatever you want can be manifested, right?

37:04.240 --> 37:06.800
You can have the world's best education for your child,

37:06.800 --> 37:08.560
independent of where you live and what wealth you have.

37:08.560 --> 37:11.600
You can have the best healthcare available.

37:11.600 --> 37:13.840
You can have customized entertainment.

37:15.840 --> 37:19.360
It's, we end up in a world in which the cost of anything

37:19.360 --> 37:24.320
is raw materials in IP, which is going to be disrupted itself,

37:25.360 --> 37:26.640
and electricity.

37:26.640 --> 37:27.680
Yeah, pretty much.

37:27.680 --> 37:31.840
Again, you look at what Elon Musk did with SpaceX, right?

37:31.840 --> 37:32.240
Yeah.

37:32.240 --> 37:35.280
He's like, let's break it down to what is the constituent cost of this?

37:35.280 --> 37:35.760
Yeah.

37:35.760 --> 37:38.000
And basically with the first principle of thinking on rockets.

37:38.000 --> 37:38.720
First principle of thinking.

37:38.720 --> 37:40.880
So what I've been doing is first principle of thinking

37:40.960 --> 37:44.640
on information flow, social theory, and our systems.

37:44.640 --> 37:46.720
It all comes down to just information being in the right place

37:46.720 --> 37:48.640
at the right time to make the maximum impact.

37:48.640 --> 37:50.800
And if we can give that as an open architect to the world,

37:51.360 --> 37:54.880
then we can augment and then replace our existing systems

37:54.880 --> 37:57.040
with better ones because they outcompete.

37:57.680 --> 38:00.960
You go to an African nation and you teach every child

38:00.960 --> 38:03.120
with an AI that teaches them and learns from them.

38:03.120 --> 38:06.480
Within a few years, they will be out competing children

38:06.480 --> 38:08.240
in the top schools in New York.

38:08.240 --> 38:10.480
What if you give them the ability to code the system as well

38:10.480 --> 38:11.040
and improve it?

38:12.000 --> 38:13.120
It becomes very interesting.

38:13.120 --> 38:16.720
The kids are helping train the AI and the AI is helping train the kids.

38:16.720 --> 38:20.400
Yes, but then also the kids can improve the actual code

38:20.400 --> 38:21.200
that they have there.

38:21.920 --> 38:23.600
This is something that we've seen in our things.

38:23.600 --> 38:24.560
A virtuous cycle.

38:24.560 --> 38:25.440
A virtuous cycle.

38:25.440 --> 38:27.760
And you make that open and then you make it transplantable

38:27.760 --> 38:31.360
because what you have then is you have models that are standard,

38:31.360 --> 38:32.400
like a base.

38:32.400 --> 38:35.840
So I like to call Stable Diffusion 1 was a precocious kindergartner.

38:35.840 --> 38:38.320
Then it was a precocious high school, Stable Diffusion 2.

38:38.320 --> 38:40.560
Stable Diffusion 3 looks freaking amazing.

38:40.560 --> 38:42.400
It's going to be like a university level student.

38:42.400 --> 38:45.920
So Stable Diffusion 3, by the way, is the real-time rendering?

38:45.920 --> 38:46.400
No.

38:46.400 --> 38:47.520
That's Stable Diffusion 2.

38:47.520 --> 38:48.080
That's 2.

38:48.080 --> 38:48.560
Okay.

38:48.560 --> 38:49.360
Stable Diffusion 3.

38:49.360 --> 38:51.680
So again, to give you an example of the thing,

38:51.680 --> 38:55.360
when we launched Stable Diffusion 1 on a top-end A100,

38:55.360 --> 38:59.120
which is like a super-souped-up, super-computer chip, right?

38:59.120 --> 38:59.760
We have a lot of those.

38:59.760 --> 39:01.440
We can talk about the supercomputers in a bit.

39:01.440 --> 39:04.880
It took 5.6 seconds on August the 23rd when we launched it.

39:04.880 --> 39:06.000
Do you render a single image?

39:06.080 --> 39:08.400
We render a single image in 5.1.2 by 5.1.2.

39:08.400 --> 39:08.640
Okay.

39:09.520 --> 39:13.200
Today, it takes 0.9 seconds in 7.6.8 by 7.6.8.

39:14.400 --> 39:15.680
We've just sped it up 30 times.

39:16.240 --> 39:16.640
Amazing.

39:17.440 --> 39:18.160
30 times.

39:18.160 --> 39:22.640
And I'm still blown away by the real-time rendering of the world.

39:22.640 --> 39:26.720
Once you get below 200 to 220 milliseconds of response time,

39:26.720 --> 39:29.120
it opens up entirely new URUX.

39:29.120 --> 39:31.360
And we didn't just release Stable Diffusion as an image creator.

39:31.360 --> 39:33.680
We also released an in-painting model.

39:33.680 --> 39:37.920
So you could take, you know, Emma's hat and you could turn it red just by describing it.

39:37.920 --> 39:40.080
We released a depth of image model so you could do transformations.

39:40.080 --> 39:41.440
We released an upscaler.

39:41.440 --> 39:45.360
So you can have a 64x64 image and the AI fills in all the details to take it

39:45.360 --> 39:48.800
to 1.024x1.024, transforms the storage industry from media.

39:49.360 --> 39:49.840
Amazing.

39:49.840 --> 39:51.120
And this is real-time now as well.

39:51.840 --> 39:54.640
So listen, I have to ask you the question, but come back to it later,

39:54.640 --> 39:56.880
which is do you believe we're living in a simulation?

39:58.880 --> 39:59.920
Yes or no?

39:59.920 --> 40:00.320
Yes.

40:00.320 --> 40:01.440
Yeah, as do I.

40:01.440 --> 40:03.280
I think we're living in an nth generation.

40:03.760 --> 40:05.520
Simulation, but that's a different story.

40:05.520 --> 40:07.760
Okay, we'll come back to that for folks who are interested.

40:07.760 --> 40:18.400
But okay, how far out are you able to imagine this world that you're creating the disruption

40:18.400 --> 40:20.080
of industries, the transformations?

40:22.400 --> 40:23.680
How many years out?

40:23.680 --> 40:24.560
I don't know anymore.

40:25.520 --> 40:30.240
Like when I started, so I funded the entire open source art space from January of last year

40:30.240 --> 40:33.600
when it started because you had a generator model and then I released this clip

40:33.600 --> 40:35.120
model which took images to text.

40:35.120 --> 40:37.040
We advanced things back and forth for each other.

40:37.040 --> 40:38.480
We're like, wow, that's the way to do things.

40:39.040 --> 40:41.040
Creators and discriminators as it works.

40:41.040 --> 40:41.920
And it's advanced, advanced.

40:41.920 --> 40:43.280
I was like, this is the next big thing.

40:43.280 --> 40:45.760
Humans can now communicate visually, right?

40:45.760 --> 40:47.440
It's the next big thing since we're going to make press.

40:47.440 --> 40:51.600
And it kind of went as expected, plus minus six months.

40:51.600 --> 40:55.360
I thought we'd get to stable diffusion in Q1, Q2 of next year.

40:55.360 --> 40:56.400
And that was a big massive bit.

40:56.400 --> 40:58.560
I built a gigantic supercomputer and everything.

40:58.560 --> 41:00.800
To get to real time, I think it would take another year or two.

41:01.760 --> 41:03.760
And instead, it took like four weeks.

41:04.720 --> 41:06.960
And again, this is what you mentioned earlier.

41:06.960 --> 41:10.720
The number of GitHub stars now for stable diffusion is more than Ethereum and Bitcoin

41:10.720 --> 41:11.840
and just about everything else.

41:12.800 --> 41:15.200
And that took them 10 years in three months.

41:16.320 --> 41:21.840
Again, it's the notion that people have no idea how fast the world is changing and is accelerating.

41:21.840 --> 41:25.760
And it's what you came back to with the common mission and the energy.

41:26.640 --> 41:30.560
When you sent your first Bitcoin, it was an amazing experience.

41:30.560 --> 41:35.120
They got overtaken by raccoons and divinity and they tried to create an alternative system

41:35.120 --> 41:36.080
outside the existing system.

41:36.080 --> 41:38.720
The interfaces were all the robbery and all the profits were made.

41:39.440 --> 41:43.760
This is something different whereby when we talk to developers and the contributors

41:43.760 --> 41:46.880
that are increasing in the ecosystem, they're so energized.

41:47.520 --> 41:49.200
And this is what drives things forward.

41:49.200 --> 41:52.960
Like when you see teams that do the biggest things, they have the energy.

41:52.960 --> 41:54.000
It's almost palpable, right?

41:54.640 --> 41:56.160
Where it's like that driven thing.

41:56.160 --> 41:58.320
But we've got people from all over the world.

41:58.880 --> 42:02.720
One of our latest developers was an Amazon warehouse worker at the start of this year

42:02.720 --> 42:04.000
who taught himself to code.

42:04.000 --> 42:05.840
And now he's building the most advanced models in the world.

42:05.840 --> 42:07.520
He's got 16-year-olds and 62-year-olds.

42:07.520 --> 42:09.840
And it's a team of how big?

42:09.840 --> 42:11.520
So our team is 137.

42:11.520 --> 42:13.760
But the developers who are creating...

42:13.760 --> 42:14.560
500.

42:14.560 --> 42:16.560
Well, but they're teams of one, right?

42:16.560 --> 42:19.280
Their team and their individuals are able to use this to create...

42:19.280 --> 42:19.600
Oh yeah.

42:19.600 --> 42:22.240
So like if you want to create with this, you can just do it by yourself.

42:22.240 --> 42:25.520
So there's a fantastic Twitter, you can look at levels.io.

42:26.160 --> 42:30.480
And he's like, I'm going to create businesses by myself that are making millions of dollars

42:30.480 --> 42:33.600
of things just because you could took this primitive and he wrapped things around it.

42:33.600 --> 42:35.440
And you're just making money.

42:35.440 --> 42:38.640
Like Avatar me, you can put your own face into the model.

42:38.640 --> 42:41.920
10 images, you can create a Peter Diamandis model and we can put you in space.

42:41.920 --> 42:43.040
In fact, we'll do that a bit later.

42:43.680 --> 42:46.000
So listen, you're listening.

42:46.000 --> 42:47.440
You're a 20-year-old entrepreneur.

42:47.440 --> 42:52.640
You're in college, you're finished, you're skipped college, whatever it is.

42:53.600 --> 42:57.120
What's your advice to that 20-year-old listening right now?

42:57.120 --> 42:58.000
You should drop everything.

42:58.000 --> 43:00.080
You should focus entirely on this.

43:00.080 --> 43:01.440
This is the biggest shift ever.

43:02.320 --> 43:04.560
Self-driving cars, $100 billion went into.

43:04.560 --> 43:06.640
Crypto, hundreds of billions of dollars went into.

43:07.440 --> 43:10.320
$100 billion, a trillion dollars going to go into this sector.

43:10.320 --> 43:11.120
Say that again, how much?

43:11.120 --> 43:13.920
$100 billion in the next few years and then a trillion dollars will go into this sector

43:13.920 --> 43:16.560
because it's so transformative and so few people understand it.

43:17.120 --> 43:21.200
There's never been something a technological advance that will diffuse

43:21.920 --> 43:22.960
as fast as this.

43:22.960 --> 43:29.760
So this is electricity, this is the Gutenberg press times a billion.

43:29.760 --> 43:33.760
This is the Gutenberg press times a billion because it's not just writing.

43:33.760 --> 43:35.280
It's image, 3D.

43:35.280 --> 43:35.760
It's everything.

43:35.760 --> 43:36.240
It's creation.

43:36.240 --> 43:37.120
Protein folding.

43:37.120 --> 43:40.960
It's creativity and extrapolation.

43:42.080 --> 43:44.000
So what does it look like to focus on this?

43:44.000 --> 43:46.960
So again, you say drop everything in focus and I get that.

43:47.840 --> 43:51.840
And part of me is like, maybe I should do that too.

43:53.280 --> 43:54.400
But what does it look like to focus?

43:54.400 --> 43:55.280
What would a person do?

43:55.280 --> 43:57.840
So again, if you're an entrepreneur, you'd be an entrepreneur in this.

43:57.840 --> 44:01.120
If you are someone who can communicate, you communicate this to other people

44:01.120 --> 44:03.120
and you get paid a million bucks a year as a consultant.

44:03.680 --> 44:04.800
You organize information.

44:04.800 --> 44:07.200
If you're an artist, if you're a creative, use a tool.

44:07.200 --> 44:10.160
You become the most efficient artist in the world when you lean in on this.

44:12.400 --> 44:13.920
Systems can be outcompeted.

44:13.920 --> 44:15.600
It's like the example of the steel mill.

44:15.680 --> 44:18.720
There were big vertically integrated steel mills that were outcompeted by

44:18.720 --> 44:20.720
lots of little steel mills, micromills.

44:21.760 --> 44:26.320
The big corporations, the big programs, the big things,

44:26.320 --> 44:29.200
will be outcompeted by just individuals and small groups.

44:29.760 --> 44:31.760
Building on top of this technology can do anything.

44:32.560 --> 44:37.920
And we've seen that over and over again in every converging exponential field.

44:37.920 --> 44:42.000
We've seen entrepreneurs disrupting and the rate of...

44:42.000 --> 44:47.200
I think of this as the asteroid impact that the slow-lumbering dinosaurs all die

44:47.200 --> 44:49.520
and the furry mammals are the ones that rapidly evolve.

44:49.520 --> 44:51.520
Yeah. I mean, let's take a practical example.

44:51.520 --> 44:53.760
And this is what we said a bit earlier about ChatJPT.

44:54.720 --> 44:56.880
Why does anyone need to use Google Image Search in a year?

44:57.680 --> 44:57.920
Yeah.

44:57.920 --> 45:01.520
When you can create any image just by describing it and then iterate it just with your words.

45:01.520 --> 45:02.000
Yes.

45:02.000 --> 45:02.720
Just attribution.

45:02.720 --> 45:02.960
That's it.

45:02.960 --> 45:04.400
But that's an easy lookup table.

45:04.400 --> 45:04.720
Yeah.

45:05.360 --> 45:06.000
All right.

45:06.000 --> 45:07.360
True. Absolutely.

45:08.320 --> 45:09.600
So now let's take it...

45:09.600 --> 45:14.080
Okay. The 20-year-old drop it, start experimenting, start playing, start using.

45:15.840 --> 45:17.680
Now you're running a company.

45:17.680 --> 45:20.480
You're running a $100 million company,

45:21.040 --> 45:23.840
worse off a billion or $10 billion company.

45:23.840 --> 45:25.280
And you see this.

45:25.280 --> 45:28.880
How fearful should you be and what should you be doing?

45:28.880 --> 45:31.360
Again, you should be leaning in to understand this.

45:31.360 --> 45:35.120
The classical innovation process inside a company is very limited.

45:35.120 --> 45:40.400
You should be having a crack team of people who've just given freedom to say,

45:40.400 --> 45:42.560
how can this potentially change my entire company?

45:44.880 --> 45:48.240
But it's hard because you are fighting against the inertia of your company.

45:48.240 --> 45:51.440
You're fighting people being used to certain ways of interacting

45:51.440 --> 45:53.040
or certain ways of distributing stuff.

45:53.760 --> 45:58.480
So this is why I think you have to think and think again from first principles.

45:58.480 --> 46:01.280
If this technology is real-time, fast, across modalities,

46:02.000 --> 46:03.520
and it can look and understand stuff,

46:04.480 --> 46:06.400
let's go forward five, 10 years and work back.

46:07.120 --> 46:09.200
Does my company still exist in this current format?

46:09.200 --> 46:10.160
Can I out-compete that?

46:11.120 --> 46:15.440
If not, how can I fold these into my current processes and procedures, etc.?

46:16.400 --> 46:17.600
Because this is the other thing.

46:18.240 --> 46:22.160
As a true exponential, the AI research actually in this area,

46:22.160 --> 46:24.720
80% of all AI research has become in this area in the last few years.

46:25.440 --> 46:29.440
And it's exponential with a 23-month rate of doubling, a true exponential.

46:29.680 --> 46:31.840
The adoption of this now is also exponential,

46:31.840 --> 46:33.840
because everyone from around the world is using it,

46:33.840 --> 46:35.920
and then each of those introduces another two people,

46:35.920 --> 46:37.600
and it goes four and four and four.

46:37.600 --> 46:41.360
So right now, it's like a wave that's under the surface.

46:41.360 --> 46:42.560
It just hasn't come yet.

46:43.280 --> 46:46.640
Next year is when it cracks, and the year after is when it crashes.

46:51.200 --> 46:57.440
I hear you, and my reaction is, I'm excited as hell about that.

46:58.400 --> 46:58.720
Right?

46:58.720 --> 47:04.560
It's like because it's about transforming the inefficiency of the world today.

47:05.280 --> 47:08.480
It's about taking individuals and empowering them to do,

47:08.480 --> 47:10.800
to giving them agency, like you said.

47:12.080 --> 47:13.600
But this is very interesting.

47:13.600 --> 47:17.200
Inefficiency is where value is created in the current paradigm,

47:18.400 --> 47:20.480
in some ways, or where value is created.

47:20.480 --> 47:22.080
Because efficiencies exist.

47:22.080 --> 47:24.160
Where value is captured by inefficiencies.

47:25.040 --> 47:27.120
So we have to have, we go to the gatekeepers,

47:27.120 --> 47:30.800
and we give them, you pay your lawyers loads of money, right?

47:30.800 --> 47:32.320
Pay your accountants and other things like that,

47:32.320 --> 47:34.320
because there are inefficiencies in the system.

47:34.320 --> 47:36.560
So you pay them to remove the inefficiencies, as it were.

47:37.360 --> 47:40.400
Some of these inefficiencies will no longer remain in the system,

47:40.400 --> 47:42.720
but that means that that value capture will also disappear.

47:43.840 --> 47:47.280
And again, this is why you have to start thinking,

47:47.280 --> 47:50.160
where does the value look like in the new reconfigured landscape?

47:51.120 --> 47:54.720
Do you have an example in an industry that exists today,

47:54.720 --> 47:57.200
and just to make this concrete for someone?

47:57.200 --> 47:59.920
Sure, you don't need lawyers anymore for everything.

48:01.040 --> 48:03.840
I mean, like, you know, you've got donotpay.com,

48:03.840 --> 48:04.960
those are massive things.

48:04.960 --> 48:06.640
They automatically write your tickets for you,

48:06.640 --> 48:07.280
and kind of, yes.

48:08.240 --> 48:10.800
For those who don't know, it's if you got a parking ticket,

48:10.800 --> 48:12.640
and you go to do not pay,

48:12.640 --> 48:16.080
they will figure out the legal loopholes and arguments

48:16.080 --> 48:17.440
to get you off your ticket.

48:17.440 --> 48:19.760
Yeah, but like you still need litigators, right?

48:20.240 --> 48:22.080
At what point can you have a robot litigator?

48:22.080 --> 48:24.480
It's like Phoenix Wright on steroids, right?

48:25.840 --> 48:28.720
You know, if you've got the, again, movie creation thing,

48:29.680 --> 48:32.320
movie creation is just going to transform completely.

48:32.320 --> 48:34.800
Video games, you'll be able to have your own user-generated content.

48:35.440 --> 48:39.040
You know, like, again, people can right now,

48:39.040 --> 48:41.040
use stable diffusion, create any image.

48:41.040 --> 48:43.920
They can go to chat GPT, it's an amazing system by OpenAI,

48:44.480 --> 48:45.600
and they can chat with it, and they're like,

48:45.600 --> 48:48.080
oh, okay, that disrupts a lot of industries as well,

48:48.080 --> 48:49.840
because it knows the different things.

48:49.840 --> 48:51.680
But it's just a model, it's a blob.

48:51.680 --> 48:52.960
It doesn't look on the internet.

48:52.960 --> 48:54.800
What happens when you hook up these models

48:54.800 --> 48:56.880
that are principle-based to the internet?

48:56.880 --> 48:58.640
Stable diffusion plus Google Image Search

48:58.640 --> 48:59.920
is actually incredibly powerful.

49:00.960 --> 49:02.640
But people are just looking at it as stable diffusion.

49:02.640 --> 49:04.640
Stable diffusion as part of a pro-

49:04.640 --> 49:06.240
architecture is incredibly powerful.

49:06.240 --> 49:07.120
People are making movies.

49:07.120 --> 49:09.600
So like if you look at our friends at the Corridor crew,

49:09.600 --> 49:12.240
for example, they did a movie called Spiderman,

49:12.880 --> 49:15.680
Everyone's Home, where they created a custom model

49:15.680 --> 49:18.240
based on the Spider-Verse movie with Mars Morales.

49:19.200 --> 49:21.840
Couple of days, they created a three-minute trailer

49:21.840 --> 49:23.920
that blows away anything big studios can do.

49:24.480 --> 49:26.640
Just a few people, a few thousand bucks.

49:27.360 --> 49:29.760
Let's go to a few general questions on AI.

49:29.760 --> 49:32.720
First of all, you come out of the hedge fund industry.

49:33.760 --> 49:38.560
I have to imagine that the day of the trader

49:39.760 --> 49:42.560
investing on their own without the use of

49:44.160 --> 49:46.640
any of these technologies is long gone.

49:46.640 --> 49:50.560
Does anybody have any advantage on their own?

49:50.560 --> 49:51.360
I mean, I think that's the thing.

49:51.360 --> 49:53.120
What's an edge in trading, right?

49:53.120 --> 49:55.200
And again, it comes down to information theory.

49:55.200 --> 49:59.120
What can move the Apple stock 50% very little information?

49:59.120 --> 50:02.720
What can move it 5% a decent amount of information?

50:02.720 --> 50:04.960
1% quite a lot of information, right?

50:04.960 --> 50:06.480
So we're just looking at the narrative

50:06.480 --> 50:08.000
and the incremental narrative of this,

50:08.000 --> 50:10.240
because again, as humans, we're heuristic creatures.

50:10.240 --> 50:12.480
So like I used to be a specialist in a number of markets,

50:12.480 --> 50:13.280
oil market was one.

50:14.000 --> 50:16.080
An oil barrel is fungible going around the world

50:16.080 --> 50:17.440
because you can just shift it on a ship.

50:17.440 --> 50:17.760
Sure.

50:17.760 --> 50:19.680
But the impact of a Libyan barrel going offline

50:19.680 --> 50:22.560
was the third as impactful as an Oklahoma barrel.

50:22.560 --> 50:23.360
Why?

50:23.360 --> 50:25.120
Because most of the money is in the West

50:25.120 --> 50:26.640
as opposed to near Libya.

50:26.640 --> 50:28.960
So they feel it more and the market reacts more.

50:28.960 --> 50:31.920
The market is a counting mechanism and a voting mechanism.

50:31.920 --> 50:34.160
I think the tools that people will use for investment

50:34.160 --> 50:34.960
are going to change now

50:35.520 --> 50:37.840
because you'll be able to actually visualize stories better

50:37.840 --> 50:39.440
and people will kind of introduce that.

50:39.440 --> 50:42.480
This is kind of why people invest on themes like ESG

50:42.480 --> 50:45.040
and you have these index trackers and all these other things.

50:45.040 --> 50:46.480
So I think that's going to be very interesting.

50:46.480 --> 50:49.760
But then which industries will be disrupted faster than others?

50:49.760 --> 50:50.800
We don't know.

50:50.800 --> 50:53.280
Who will be at the edge and the forefront of this

50:53.280 --> 50:54.480
embracing this technology?

50:55.120 --> 50:56.480
And then who can be left behind?

50:56.480 --> 50:57.600
We don't know.

50:57.600 --> 51:00.640
So I think the entire stock market's going to change and adjust.

51:00.640 --> 51:03.200
It could be a period of supernormal profits

51:03.200 --> 51:05.040
and then outcompetition as well.

51:05.600 --> 51:07.920
But this is against a backdrop of inflation

51:07.920 --> 51:10.720
and recession and all sorts of crazy stuff

51:10.720 --> 51:11.920
all at the same time.

51:11.920 --> 51:15.120
So will you be using this technology

51:15.120 --> 51:22.000
to assist a hedge fund manager out there on investing?

51:22.000 --> 51:23.200
Yeah, I can run to my own hedge fund.

51:24.640 --> 51:27.760
No, look, again, I think this technology would be pervasive

51:27.760 --> 51:29.280
because people again will see the power of this.

51:29.840 --> 51:31.680
And ultimately, like I said,

51:32.400 --> 51:34.000
investing usually comes down to stories.

51:36.080 --> 51:39.280
Influencing human desires and intention.

51:39.280 --> 51:39.760
Exactly.

51:40.000 --> 51:42.000
If you're doing VC to fund management or whatever,

51:42.000 --> 51:43.680
you've just basically got a story.

51:43.680 --> 51:45.680
You can say for all you want that you're trying to be

51:45.680 --> 51:47.840
like quantitative and this and that.

51:49.360 --> 51:51.120
But nobody knows the future.

51:51.120 --> 51:54.320
So you construct a story, but you're saying,

51:54.320 --> 51:56.400
what is the evolution of that story going to be

51:56.400 --> 51:58.480
such that someone will buy this from me at a higher price?

51:59.360 --> 52:00.240
That's all that matters.

52:00.240 --> 52:02.240
And part of it, for entrepreneurs,

52:02.240 --> 52:03.920
one way that I suggest to look at things is,

52:04.640 --> 52:06.000
what's the terminal value?

52:06.000 --> 52:07.520
You get someone to agree to that.

52:07.520 --> 52:08.880
All you're doing when you're raising money

52:08.880 --> 52:11.120
is you're de-risking your path for that terminal value.

52:12.560 --> 52:15.360
And people don't realize that's all of investment.

52:15.360 --> 52:17.040
Yeah, it's a very simplified view of it.

52:17.600 --> 52:21.440
So I'm excited to have you at speaking at abundance360 this year.

52:21.440 --> 52:23.680
Thank you for joining us in March.

52:23.680 --> 52:24.320
Thank you for having me.

52:25.360 --> 52:27.520
And I've dedicated an entire day.

52:27.520 --> 52:30.720
We've added a fourth day of the program focused on AI

52:30.720 --> 52:33.680
because it's just, I think people need to understand

52:33.680 --> 52:37.120
how powerful and disruptive and a view of what's coming.

52:38.000 --> 52:42.160
Another one of our rock star speakers there,

52:42.160 --> 52:44.080
someone who you know, Ray Kurzweil,

52:44.080 --> 52:46.960
and Ray's going to be joining us to speak as well.

52:47.920 --> 52:50.800
And it's been Ray's long held belief

52:50.800 --> 52:54.560
that we're going to reach human level AI of like 2029.

52:56.080 --> 52:57.680
However you want to define that,

52:59.360 --> 53:01.760
what's your thought about that?

53:01.760 --> 53:04.640
Do you think that that is the case?

53:05.680 --> 53:06.960
How do you think about it?

53:07.600 --> 53:08.800
It's singularity, Niere.

53:11.920 --> 53:13.040
Again, how have you defined it?

53:13.040 --> 53:19.600
I think my ideal view is that a future that's an intelligent internet.

53:20.320 --> 53:23.360
Every person, country, culture, and company has their own models

53:23.360 --> 53:27.520
and they're all interacting with each other in an optimal way for humanity.

53:27.520 --> 53:31.200
So again, I kind of go to this thing of Tim Urban and Wait But Why

53:31.200 --> 53:32.640
when he wrote the article about neural link,

53:32.640 --> 53:35.200
had this option, this discussion of the human colossus.

53:36.000 --> 53:42.320
I would like AGI to be something of all of us working for us to make us better.

53:42.320 --> 53:45.840
And I think part of this is why I'm basically pushing open source

53:45.840 --> 53:47.280
and these open source models.

53:47.280 --> 53:49.520
And I've created an organization that goes

53:49.520 --> 53:52.960
and has all these verticals that will then be spin off into independent organizations.

53:52.960 --> 53:55.440
So we can standardize the architecture across that

53:55.440 --> 53:58.160
and it can be an emergent global consciousness as it were.

53:59.040 --> 54:04.320
I think it's important for us to talk about the idea of these models

54:04.320 --> 54:07.840
and these personalized models to understand what that means.

54:07.840 --> 54:14.800
So when you build a massive model that is trained on everything out there,

54:15.360 --> 54:17.200
it's not really necessarily useful.

54:18.080 --> 54:21.040
But you talk about creating models for nations,

54:21.040 --> 54:23.120
for cultures, for companies, for individuals.

54:23.680 --> 54:26.000
Can you just give us the 101 on that

54:26.000 --> 54:28.880
so people understand the power of that and the value of that?

54:28.880 --> 54:31.360
Knowledge evolves at different paces, right?

54:31.360 --> 54:34.960
There's knowledge on how to use a toilet that's been with us for many, many years,

54:34.960 --> 54:38.160
you know, versus knowledge on foundation models, which is just very new.

54:38.160 --> 54:40.720
You kind of see that fashions is like pace layering, as it were.

54:42.160 --> 54:45.920
The thing, the way that models will be built is that you should look at them as like pizza bases.

54:45.920 --> 54:49.600
So we're trying to figure out what an optimal pizza base is for a generalized image model.

54:49.600 --> 54:52.560
Then you've got maybe an Indian model with a bit of culture in there

54:52.560 --> 54:54.880
and then Indian fashion in there.

54:54.880 --> 54:59.440
And then Indian fashion for MAD to try and be fashionable when he's in India.

55:00.080 --> 55:02.080
And so you can kind of look at those as bases.

55:02.080 --> 55:04.080
Because what we do is, again, we flip the paradigm.

55:04.080 --> 55:10.000
So to train a stable diffusion, like we built a 4,800 cluster with our buddies at Amazon.

55:10.720 --> 55:12.560
To put that in context, the fastest supercomputer...

55:12.560 --> 55:14.880
4,800 cluster.

55:14.880 --> 55:15.840
Yeah, it's huge.

55:15.840 --> 55:19.600
Yeah, to put it in context, the fastest supercomputer in the UK, Cambridge 1 is 640.

55:20.160 --> 55:23.920
We've got about 10 times the compute of NASA, roughly speaking.

55:23.920 --> 55:25.360
It's one of the top 10 in the world.

55:25.360 --> 55:27.280
And next year I'll go 10 times bigger.

55:27.280 --> 55:28.480
Yours will go 10 times bigger.

55:29.440 --> 55:30.720
And we make that available to everyone.

55:30.720 --> 55:32.800
And it's on the Amazon cloud?

55:32.800 --> 55:34.080
It's on the Amazon cloud, yeah.

55:34.080 --> 55:36.400
Because it would have been a bit too much for us to build it by ourselves.

55:37.280 --> 55:39.520
So we got them to lean in on that and kind of build this.

55:40.560 --> 55:44.240
Facebook has 16,000, for example, because they're pushing big metaverse.

55:44.240 --> 55:46.880
That's the fastest in the world, if you want to have an idea.

55:46.880 --> 55:51.440
But again, this is an exponential thing whereby we have more than just about every single country.

55:51.440 --> 55:55.920
And we use that to take 100,000 gigabytes of images and compress it into...

55:55.920 --> 55:57.200
100 terabytes.

55:57.200 --> 55:59.440
Yeah, 100 terabytes and compress it down.

55:59.440 --> 56:02.240
So into two, effectively.

56:02.240 --> 56:04.480
So 50,000 to one compression of knowledge.

56:06.480 --> 56:10.000
So when you say that compression, when you're talking about a neural net,

56:12.480 --> 56:17.200
not all of the connections or the pathways in the neural net are useful or valid.

56:18.080 --> 56:21.360
So what you're doing is you're choosing which ones are?

56:21.360 --> 56:23.280
Yeah, because it pays attention to what the most important lines are.

56:23.280 --> 56:25.120
So this is part of the attention attention.

56:25.120 --> 56:27.520
And then it creates these latent spaces that you poke with the prompts,

56:27.520 --> 56:28.800
which are the words that you put in.

56:29.360 --> 56:34.560
So like OpenAI, for example, made GPT-3 was 175 billion parameter model.

56:34.560 --> 56:37.120
The next step after that deep learning, because that's called deep learning,

56:37.120 --> 56:41.280
this thing, because we spend all the energy and all that compute, you don't have to.

56:41.280 --> 56:43.360
You can then take that base, that pizza base.

56:43.360 --> 56:45.360
You can inject some PETA and then you have a PETA model.

56:45.360 --> 56:48.080
It just takes 10 images and then you can put yourself in anything.

56:48.080 --> 56:51.520
Or you can spend 100,000 images and do an even more refined model.

56:51.520 --> 56:52.320
But it forms a base.

56:52.320 --> 56:54.080
It's what's called the foundation model in a way.

56:54.720 --> 56:56.000
But you can even make it even more efficient.

56:56.000 --> 56:58.480
So GPT-3 is 175 billion parameters.

56:58.480 --> 56:59.920
So it's like 80 times bigger.

56:59.920 --> 57:03.440
And they said GPT-4 was going to be like 100 trillion parameters.

57:03.440 --> 57:04.640
Could be, yeah.

57:04.640 --> 57:08.000
I think that chat GPT is actually training GPT-4 right now.

57:08.000 --> 57:08.320
Interesting.

57:08.320 --> 57:09.840
Because what happens is that you have this foundation,

57:10.480 --> 57:16.960
but then in the classical day age of big data, what mattered was who you were.

57:16.960 --> 57:20.880
So they took your data, they built these big models and they targeted you.

57:20.880 --> 57:23.760
What matters now in the day age of big models

57:23.840 --> 57:25.360
is how you use the models.

57:25.360 --> 57:26.720
Because not all those neurons are needed.

57:26.720 --> 57:28.400
We don't need all two billion images, right?

57:28.400 --> 57:28.640
Right.

57:29.280 --> 57:31.600
So GPT-3 was 175 billion.

57:31.600 --> 57:33.040
And they saw how people used it.

57:33.040 --> 57:34.880
And they identified the neurons that lit up.

57:35.600 --> 57:38.000
And they compressed it down to 1.3 billion parameters.

57:38.000 --> 57:40.560
So now when you're using chat GPT, you're training one of those models.

57:41.360 --> 57:43.280
And they're looking at how people are using it to compress it down.

57:43.280 --> 57:44.800
So it's able to usually compress down even more.

57:45.360 --> 57:47.600
But then this is personalized model that's very interesting.

57:47.600 --> 57:50.800
Because you can have almost this standardized base.

57:51.120 --> 57:54.960
And then you can inject your own context into it.

57:54.960 --> 57:59.040
And the context of your culture, your company, your community, and others.

57:59.760 --> 58:01.600
And so that base is manual built.

58:02.320 --> 58:04.240
In that you can extend out the latent spaces.

58:04.240 --> 58:07.360
So like in stable diffusion one, we didn't really filter it.

58:07.360 --> 58:09.520
So Mona Lisa is overfit.

58:09.520 --> 58:10.880
There's too many pictures of Mona Lisa.

58:10.880 --> 58:12.960
So it's very hard to get her out of the picture frame.

58:14.320 --> 58:16.800
Now we adjusted it so you can make her swim very easily.

58:16.800 --> 58:17.920
Because it's not overfit anymore.

58:17.920 --> 58:19.600
And it'll continue improving and adapting.

58:19.600 --> 58:21.200
So we'll have better databases.

58:21.200 --> 58:23.520
And in a year's time, we'll have a very mature base

58:23.520 --> 58:24.800
that people can take and extend.

58:24.800 --> 58:27.920
So like in Japan, they took stable diffusion

58:27.920 --> 58:30.880
and they adjusted the text encoder for Japanese culture.

58:32.080 --> 58:34.240
And then it meant that if you use normal stable diffusion,

58:34.240 --> 58:37.760
because it's very Western oriented, salary man means a happy man.

58:37.760 --> 58:38.240
Yes.

58:38.240 --> 58:39.920
In Japan, diffusion is very sad man.

58:40.560 --> 58:41.520
Very sad man indeed.

58:41.520 --> 58:43.200
Because it understands that context.

58:43.200 --> 58:43.680
Fascinating.

58:43.680 --> 58:46.240
So in the future, if I wanted to,

58:47.120 --> 58:49.360
I coach a lot of entrepreneurs

58:49.360 --> 58:52.640
and a lot of CEOs through abundance 360 and so forth.

58:53.360 --> 58:57.680
If I wanted to create a virtualized version of myself

58:57.680 --> 59:01.920
that in certain circumstances would react in a certain way,

59:02.960 --> 59:04.160
that's pretty easy.

59:04.160 --> 59:04.720
It's coming.

59:04.720 --> 59:08.080
So pocket coach, if I wanted to like have

59:08.080 --> 59:11.040
Tony Robbins in my pocket in the right moment

59:11.040 --> 59:14.480
or the Dalai Lama, it's all possible.

59:14.480 --> 59:17.680
Yeah, and that'll be probably a four gigabyte file.

59:17.680 --> 59:19.120
And how far is that from now?

59:20.240 --> 59:21.200
Pretty much do it now.

59:21.200 --> 59:22.000
Go do it now.

59:22.000 --> 59:25.760
And you can do it fully realistic like, you know.

59:25.760 --> 59:27.920
I think the term is holy shit.

59:27.920 --> 59:28.800
That's amazing.

59:28.800 --> 59:31.120
Yeah, because you have the multi-modality.

59:31.120 --> 59:33.600
You can make it indistinguishable from a human like

59:33.600 --> 59:36.080
stable diffusion two is pretty much photo realistic.

59:36.080 --> 59:38.240
Now stable diffusion three will break that barrier.

59:38.800 --> 59:40.480
And obviously we can animate now.

59:40.480 --> 59:42.080
So again, that's kind of crazy.

59:42.080 --> 59:42.720
That's crazy.

59:42.800 --> 59:47.680
Meta-humans epic game style and video are doing.

59:47.680 --> 59:49.200
You can also do human realistic voices

59:49.200 --> 59:50.640
with fully emotional range as well.

59:51.520 --> 59:54.560
So like my sister-in-law runs a company called Sanantic.

59:55.600 --> 59:58.880
She reconstructed Val Kilmer's voice for Val and Top Gun.

59:58.880 --> 59:59.040
Nice.

59:59.040 --> 01:00:00.000
I was doing all the video games.

01:00:00.000 --> 01:00:02.320
And if you go to Sanantic.io, you hear an AI tell you

01:00:02.320 --> 01:00:04.480
that it loves you and it's really creepy.

01:00:05.760 --> 01:00:06.800
She just sold us Spotify.

01:00:06.800 --> 01:00:09.200
So I'm sure we'll have some really engaging podcasts

01:00:09.200 --> 01:00:09.840
and things like that.

01:00:10.160 --> 01:00:14.000
And like again, we'll cross all modalities now.

01:00:14.000 --> 01:00:17.600
In narrow, you're achieving human levels and going on

01:00:17.600 --> 01:00:19.760
to human levels of performance and benchmarks

01:00:20.480 --> 01:00:23.200
from media to understanding to output.

01:00:24.320 --> 01:00:27.040
And the barriers to putting yourself in, like I said,

01:00:27.040 --> 01:00:30.480
you can train a model now in like less than an hour

01:00:30.480 --> 01:00:33.440
with 10, 100 images of yourself to put yourself in anything

01:00:35.200 --> 01:00:35.920
for a buck.

01:00:37.120 --> 01:00:39.360
Because we've done the heavy lifting of millions of bucks

01:00:39.360 --> 01:00:42.160
of pre-training the model as it were, whereas classically,

01:00:42.160 --> 01:00:43.520
AI wasn't like that.

01:00:43.520 --> 01:00:47.600
So if I wanted to create a virtualized host of myself,

01:00:48.240 --> 01:00:53.520
being able to on-screen play me, say what I want to say,

01:00:54.160 --> 01:00:55.280
that's here now.

01:00:55.280 --> 01:00:55.840
Let's say it now.

01:00:55.840 --> 01:00:57.440
Like the technology is here now.

01:00:57.440 --> 01:00:58.640
The implementation needs to be there.

01:00:58.640 --> 01:01:01.520
So like one of the companies we work with is called

01:01:01.520 --> 01:01:02.640
atlethea.ai.

01:01:02.640 --> 01:01:04.400
And so they use our language models.

01:01:04.400 --> 01:01:06.800
And so you can upload your scripts and it'll learn how you speak.

01:01:07.040 --> 01:01:10.480
But then the voice technology, they haven't integrated the voice technology.

01:01:10.480 --> 01:01:12.160
We have an extra generation audio technology.

01:01:12.160 --> 01:01:15.280
That'll come in a month or two and then it'll learn how you speak literally.

01:01:15.280 --> 01:01:17.440
I was using last year at Advanced 360.

01:01:17.440 --> 01:01:20.720
I was using a company called Soul Machines and it had a virtual humans.

01:01:20.720 --> 01:01:20.880
Yes.

01:01:20.880 --> 01:01:21.520
All right.

01:01:21.520 --> 01:01:25.520
And so I would love to create a virtual version of myself

01:01:25.520 --> 01:01:26.720
for anybody I want.

01:01:26.720 --> 01:01:27.760
And that's here.

01:01:27.760 --> 01:01:28.720
That's here again.

01:01:28.720 --> 01:01:32.320
Soul Machines is upgrading now thanks to the technology that we're open sourcing.

01:01:32.320 --> 01:01:33.360
This is the other interesting part.

01:01:33.360 --> 01:01:34.960
Open source will always lie close source.

01:01:35.200 --> 01:01:37.680
Because close source can always take open and add in data.

01:01:37.680 --> 01:01:38.000
Yes.

01:01:38.000 --> 01:01:41.040
And they can have very focused teams focusing on certain use cases.

01:01:41.040 --> 01:01:45.840
So we're literally upgrading the foundation of all of these companies

01:01:45.840 --> 01:01:47.440
as we release these models.

01:01:47.440 --> 01:01:48.080
Interesting.

01:01:48.080 --> 01:01:50.080
And then you mix and match and that's where the value is.

01:01:50.080 --> 01:01:52.080
Actually, who was it who said a thing?

01:01:52.080 --> 01:01:53.520
Was it my friend Jason or someone else?

01:01:53.520 --> 01:01:55.920
No, it was one of the other VCs.

01:01:55.920 --> 01:01:59.120
Most of the money in the world is made by aggregation and disaggregation.

01:01:59.120 --> 01:02:01.120
It just depends on which part of the cycle you're at.

01:02:01.120 --> 01:02:02.160
Interesting.

01:02:02.320 --> 01:02:03.280
I can see that.

01:02:03.280 --> 01:02:06.480
I was having dinner with Reid Hoffman a month ago or so

01:02:06.480 --> 01:02:08.480
and he said something which is interesting.

01:02:08.480 --> 01:02:12.720
He said every profession is going to have an AI co-pilot very soon.

01:02:12.720 --> 01:02:14.320
And I've been saying this for medicine.

01:02:14.320 --> 01:02:17.920
I think it's going to be malpractice to diagnose without having AI in the loop.

01:02:17.920 --> 01:02:19.360
And we'll see what the time frame is.

01:02:19.360 --> 01:02:20.880
Five years is my guess.

01:02:22.080 --> 01:02:28.320
But I can see an AI co-pilot as an architect, as a lawyer, as a chef, as everything.

01:02:28.640 --> 01:02:30.640
How far is that?

01:02:30.640 --> 01:02:32.640
Well, I mean it's here for code right now.

01:02:32.640 --> 01:02:37.680
So co-pilot, literally what it's called, from Microsoft GitHub

01:02:37.680 --> 01:02:39.680
and then code whisperer now from Amazon.

01:02:39.680 --> 01:02:41.680
They help you write better code.

01:02:41.680 --> 01:02:43.680
It's about a 50% speed increase.

01:02:43.680 --> 01:02:44.320
Amazing.

01:02:44.320 --> 01:02:46.320
And that's what we've kind of measured so far, which is insane.

01:02:46.320 --> 01:02:49.840
Like you type it, I want to have a piece of code that does this and boom, it's there.

01:02:49.840 --> 01:02:51.840
And maybe you need to add a snippet, doesn't matter.

01:02:51.840 --> 01:02:53.840
It makes your life easier, right?

01:02:53.840 --> 01:02:55.840
Like Sayable Diffusion is a co-pilot for art.

01:02:55.840 --> 01:02:59.840
So artists use it to iterate rapidly on different concepts and they take it.

01:02:59.840 --> 01:03:02.320
And there's a Photoshop integration and they use it as part of their Photoshop process.

01:03:02.320 --> 01:03:05.840
I used to say that the crowd was the interim step until AI, right?

01:03:05.840 --> 01:03:09.360
So GitHub was the crowd and now you've got, you know.

01:03:09.360 --> 01:03:09.840
Yeah.

01:03:09.840 --> 01:03:11.840
And you know, there's questions around, you know, what was it trained on?

01:03:11.840 --> 01:03:13.840
Because it was trained on an entire snapshot of things.

01:03:13.840 --> 01:03:17.840
But again, it's like a different type of information flow to the Web 2 economy.

01:03:17.840 --> 01:03:19.840
We're going to skip over Web 3 because it was a bit crap.

01:03:19.840 --> 01:03:21.840
And we're going to go to Web 4.

01:03:21.840 --> 01:03:23.840
Or whatever it is now.

01:03:23.840 --> 01:03:25.840
And you know, maybe you could go to Web 4.

01:03:25.840 --> 01:03:27.840
It looks like AI.

01:03:27.840 --> 01:03:29.840
Do a cool logo for that.

01:03:29.840 --> 01:03:31.840
Just get that stable diffusion to make it.

01:03:31.840 --> 01:03:33.840
In fact, it's listening and it's made it.

01:03:33.840 --> 01:03:37.840
So I love Jarvis from Iron Man.

01:03:37.840 --> 01:03:39.840
Yeah.

01:03:39.840 --> 01:03:45.840
And I find Siri and Alexa and Google now kind of disappointing.

01:03:45.840 --> 01:03:47.840
How far are we from Jarvis?

01:03:47.840 --> 01:03:51.840
So I think Jarvis is probably two to three years.

01:03:51.840 --> 01:03:57.840
Because like right now you've got a MacBook M2 in front of you.

01:03:57.840 --> 01:04:01.840
16.8% of that chipset is a neural engine that's optimized for these transform based architectures.

01:04:01.840 --> 01:04:05.840
Stable diffusion is one of the first models to actually go down to that level.

01:04:05.840 --> 01:04:07.840
And so when will we see that on this machine?

01:04:07.840 --> 01:04:13.840
I think Apple is basically aiming for like 70, 80% of everyone to have this.

01:04:13.840 --> 01:04:15.840
And then you can go from Siri 1 to Siri 5.

01:04:15.840 --> 01:04:17.840
Nice.

01:04:17.840 --> 01:04:19.840
And this is why Apple has been talking about privacy and things like that.

01:04:19.840 --> 01:04:23.840
Because the new paradigm of the internet, whereby the classical Web 2 internet,

01:04:23.840 --> 01:04:29.840
was intelligence at the middle coordinating us and feeding on our dreams and hopes and emotions to sell us ads.

01:04:29.840 --> 01:04:31.840
Now it's intelligence at the edge.

01:04:31.840 --> 01:04:33.840
Whereby you've got your own Apple ID.

01:04:33.840 --> 01:04:35.840
You've got your own privacy layer.

01:04:35.840 --> 01:04:39.840
And then you've got chips that can run AI at the edge that really understand you.

01:04:39.840 --> 01:04:41.840
That's why the noggin's experience is seamless.

01:04:41.840 --> 01:04:43.840
And Google also realized that.

01:04:43.840 --> 01:04:45.840
So they're building stuff into the Pixel phones.

01:04:45.840 --> 01:04:49.840
Yeah, I think people need to realize that the power of these future systems,

01:04:49.840 --> 01:04:51.840
call it Jarvis for lack of a better term,

01:04:51.840 --> 01:04:55.840
is when you give it open access to everything in your life.

01:04:55.840 --> 01:04:57.840
You let it watch what you're eating.

01:04:57.840 --> 01:04:59.840
You let it read your emails, listen to your conversations.

01:04:59.840 --> 01:05:05.840
Because it makes the world, the term I use is automagical in that regard.

01:05:05.840 --> 01:05:07.840
Yeah, and you need to have the foundation models to do that.

01:05:07.840 --> 01:05:10.840
Because it needs generalized knowledge and then specific knowledge and contextual knowledge.

01:05:10.840 --> 01:05:12.840
So it can adapt to your needs.

01:05:12.840 --> 01:05:14.840
This human in the loop process is very important.

01:05:14.840 --> 01:05:18.840
And so there'll be big AIs in the cloud, but then a lot of AIs on the edge.

01:05:18.840 --> 01:05:20.840
And they'll interact with and talk to each other.

01:05:20.840 --> 01:05:21.840
And part of each other.

01:05:21.840 --> 01:05:25.840
Because what these models also do is they take structured data and turn it into unstructured data.

01:05:25.840 --> 01:05:26.840
So again, stable diffusion.

01:05:26.840 --> 01:05:27.840
A few words.

01:05:27.840 --> 01:05:28.840
Robert De Niro's Gandalf.

01:05:28.840 --> 01:05:31.840
You get a photorealistic picture of Robert De Niro's Gandalf.

01:05:31.840 --> 01:05:32.840
Yep.

01:05:32.840 --> 01:05:34.840
That's structured unstructured data.

01:05:34.840 --> 01:05:36.840
And then you clear both ways.

01:05:36.840 --> 01:05:38.840
A brief note from our sponsors.

01:05:38.840 --> 01:05:39.840
Let's talk about sleep.

01:05:39.840 --> 01:05:43.840
Sleep has become one of my number one longevity priorities in life.

01:05:43.840 --> 01:05:53.840
Getting eight deep uninterrupted hours of sleep is one of the most important things you can do to increase your vitality and energy and increase the health span that you have here on earth.

01:05:53.840 --> 01:05:58.840
You know, when I was in medical school years ago, I used to pride myself on how little sleep I could get.

01:05:58.840 --> 01:06:00.840
You know, it used to be five, five and a half hours.

01:06:00.840 --> 01:06:03.840
Today I pride myself on how much sleep I can get.

01:06:03.840 --> 01:06:05.840
And I shoot for eight hours every single night.

01:06:05.840 --> 01:06:08.840
Now, usually I'm great at going to sleep.

01:06:08.840 --> 01:06:12.840
If I'm exhausted, you know, I've worked a hard day, I'm right out.

01:06:12.840 --> 01:06:19.840
But if I'm having difficulty and it occurs, I'm having insomnia or my mind's overactive and I need help to get that eight hours.

01:06:19.840 --> 01:06:23.840
I turn to a supplement product by Lifeforce called Peak Rest.

01:06:23.840 --> 01:06:28.840
Now, Peak Rest has been formulated with an extraordinary scientific depth and background.

01:06:28.840 --> 01:06:35.840
Includes everything from long lasting melatonin to magnesium to elglycine to rosemary extract, just to name a few.

01:06:35.840 --> 01:06:43.840
This product is about creating a sense of rest and really giving you the depth and length of sleep that you need for recovery.

01:06:43.840 --> 01:06:45.840
It's a product I hope you'll try.

01:06:45.840 --> 01:06:48.840
It works for me and I'm sure it will work for you.

01:06:48.840 --> 01:06:57.840
If you're interested, go to mylifeforce.com backslashpeter to get a discount from Lifeforce on this product.

01:06:57.840 --> 01:07:03.840
But you'll also see a whole set of other longevity and vitality related supplements that I use.

01:07:03.840 --> 01:07:08.840
We'll talk about them some other time, but in terms of sleep, Peak Rest is my go-to supplement.

01:07:08.840 --> 01:07:09.840
Hope you'll enjoy it.

01:07:09.840 --> 01:07:14.840
Go to mylifeforce.com backslashpeter for your discount.

01:07:14.840 --> 01:07:18.840
Let's talk about something which I have an opinion about.

01:07:18.840 --> 01:07:19.840
I'm curious about yours.

01:07:19.840 --> 01:07:26.840
I think I know it, which is the idea of privacy, which fundamentally people all want privacy.

01:07:26.840 --> 01:07:29.840
I don't believe it really exists.

01:07:29.840 --> 01:07:35.840
AI can read your lips when there's data flowing everywhere or where encryption.

01:07:35.840 --> 01:07:38.840
What are your thoughts about privacy and how do we deal with it?

01:07:38.840 --> 01:07:39.840
Do you have any ideas?

01:07:39.840 --> 01:07:43.840
I think for privacy, you should always look at what the downside to not having privacy is.

01:07:43.840 --> 01:07:47.840
Actually, people are more than willing to give up their data, too willing, in my opinion.

01:07:47.840 --> 01:07:52.840
Everybody clicks that, yes, I accept there's 15,000 pages of legal.

01:07:52.840 --> 01:07:53.840
Exactly.

01:07:53.840 --> 01:07:56.840
And then you have to think as well of different paradigms.

01:07:56.840 --> 01:07:58.840
In China, will there ever be privacy?

01:07:58.840 --> 01:07:59.840
Probably not.

01:07:59.840 --> 01:08:03.840
And you have the social credit score and it's an opticon being built by AI, etc.

01:08:03.840 --> 01:08:06.840
In the Western Parasite, what's the downside on the privacy thing?

01:08:06.840 --> 01:08:07.840
What if your stuff isn't private?

01:08:07.840 --> 01:08:12.840
It's basically bad actors using you in certain ways, which can include AI algorithms trying to manipulate you.

01:08:12.840 --> 01:08:18.840
I think, again, what Apple's doing is building a paradigm for actual privacy because it's aligned with their business model.

01:08:18.840 --> 01:08:23.840
Even other companies now, Facebook and Google, have enough information they don't need your data anymore.

01:08:23.840 --> 01:08:25.840
Who actually wants your data?

01:08:25.840 --> 01:08:27.840
I think it's a question.

01:08:27.840 --> 01:08:30.840
We view ourselves as these wonderful things.

01:08:30.840 --> 01:08:32.840
Who actually wants your data at this point?

01:08:32.840 --> 01:08:36.840
But the systems have adopted to do that and policies adopted to do that as well.

01:08:36.840 --> 01:08:38.840
With GDPR and all these other things.

01:08:38.840 --> 01:08:40.840
Some of them overreach, I think, a bit.

01:08:40.840 --> 01:08:43.840
But we are moving to this area whereby nobody needs your data anymore.

01:08:43.840 --> 01:08:47.840
And also the systems are now available to give you that privacy that you want.

01:08:47.840 --> 01:08:52.840
And I think people want to opt in rather than opt out of a lot of different things to get more resources and other stuff.

01:08:52.840 --> 01:08:57.840
Finally, the final element is that federated learning has matured now.

01:08:57.840 --> 01:08:58.840
What does that mean?

01:08:58.840 --> 01:09:01.840
So federated learning is when you take the model to the data.

01:09:01.840 --> 01:09:04.840
So you used to have to ingest all this data and train the models.

01:09:04.840 --> 01:09:09.840
Now, if it's just like a freaking gigabyte, you send the model to the data, it can train.

01:09:09.840 --> 01:09:14.840
And then without saying it's PETA or MAD, it can upstream the output.

01:09:14.840 --> 01:09:18.840
So we're seeing that in HDR UK, for example, health data, we're seeing it with Melody,

01:09:18.840 --> 01:09:25.840
which is a thing with a lot of pharmaceutical companies coming through to open source analysis of patient data.

01:09:25.840 --> 01:09:30.840
You can finally get that where you don't have to sacrifice privacy to build AI and models.

01:09:30.840 --> 01:09:33.840
And that's going to be pretty amazing to, again, advance the field

01:09:33.840 --> 01:09:36.840
because you have access to so much more data to build better models.

01:09:36.840 --> 01:09:37.840
Amazing.

01:09:37.840 --> 01:09:40.840
Let's talk about the perceived downside.

01:09:40.840 --> 01:09:47.840
And I have to imagine that as much incredible compliments

01:09:47.840 --> 01:09:50.840
and the world should thank you for the work that you're doing,

01:09:50.840 --> 01:09:52.840
because of the impact it's going to have,

01:09:52.840 --> 01:09:56.840
you're going to have to have detractors who are worried about technological employment

01:09:56.840 --> 01:10:00.840
or malicious use of AI or fake news and all of that.

01:10:00.840 --> 01:10:02.840
What concerns you?

01:10:02.840 --> 01:10:07.840
And I know you're a principled man who thinks about this deeply.

01:10:07.840 --> 01:10:08.840
What concerns you most?

01:10:08.840 --> 01:10:10.840
I don't have all the answers.

01:10:10.840 --> 01:10:12.840
And that's a fair statement to make.

01:10:12.840 --> 01:10:13.840
Yeah.

01:10:13.840 --> 01:10:19.840
I mean, generally, what I saw was that very few individuals had control of this most powerful technology.

01:10:19.840 --> 01:10:21.840
And then there's very weird things.

01:10:21.840 --> 01:10:24.840
People like open source AI is like nukes and like,

01:10:24.840 --> 01:10:26.840
so why should you control the nukes?

01:10:26.840 --> 01:10:28.840
You know?

01:10:28.840 --> 01:10:31.840
It was a very strange kind of thing.

01:10:31.840 --> 01:10:34.840
They're like, no, it shouldn't be open source.

01:10:34.840 --> 01:10:36.840
So why should big companies control it?

01:10:36.840 --> 01:10:38.840
Again, we live in largely a democracy.

01:10:38.840 --> 01:10:40.840
We live in a society.

01:10:40.840 --> 01:10:44.840
And so my take was let's educate people, get this technology out there,

01:10:44.840 --> 01:10:46.840
and let's have a common conversation about it.

01:10:46.840 --> 01:10:48.840
Because I have my own viewpoints and they're there.

01:10:48.840 --> 01:10:50.840
But again, I'm not a representative of anyone.

01:10:50.840 --> 01:10:53.840
I'm just me running my own company trying to catalyze this.

01:10:53.840 --> 01:10:55.840
Because I thought it was important,

01:10:55.840 --> 01:11:01.840
given the fundamental change of society that will be caused by this technology now,

01:11:01.840 --> 01:11:05.840
because exponentials are a hell of a thing for it to get out there.

01:11:05.840 --> 01:11:06.840
And so you need to make a splash.

01:11:06.840 --> 01:11:09.840
So, you know, I've got hate mail and kind of all sorts of things because it is disruptive.

01:11:09.840 --> 01:11:11.840
And we have to be aware of that.

01:11:11.840 --> 01:11:13.840
It is crazy and it will cause fear.

01:11:13.840 --> 01:11:14.840
We have to be aware of that.

01:11:14.840 --> 01:11:17.840
And we have to decide together how to do that.

01:11:17.840 --> 01:11:22.840
For example, there are artists in the data set because it's a snapshot, right?

01:11:22.840 --> 01:11:23.840
Sure.

01:11:23.840 --> 01:11:24.840
It's less than 0.5%.

01:11:24.840 --> 01:11:29.840
And so is it ethical, legal and moral to have them in there so people can prompt an art style

01:11:29.840 --> 01:11:30.840
and then match them together?

01:11:30.840 --> 01:11:32.840
I think it is.

01:11:32.840 --> 01:11:37.840
But does that mean that we disregard artists who want to opt out of the data set?

01:11:37.840 --> 01:11:38.840
No.

01:11:38.840 --> 01:11:39.840
Because they're part of the global community.

01:11:39.840 --> 01:11:41.840
So we've built it opt out and opt out mechanisms.

01:11:41.840 --> 01:11:46.840
And by the way, those artists are influencing other artists normally in the course of just

01:11:46.840 --> 01:11:47.840
them going to museum.

01:11:47.840 --> 01:11:48.840
Yeah, exactly.

01:11:48.840 --> 01:11:52.840
And you know, what we have is now we've had like four or five thousand artists sign up

01:11:52.840 --> 01:11:53.840
of spawning.

01:11:53.840 --> 01:11:55.840
Half of them have opted out.

01:11:55.840 --> 01:11:57.840
Half of them have opted in.

01:11:57.840 --> 01:12:01.840
Because they'd love to see their work influence the world.

01:12:01.840 --> 01:12:04.840
But how many people have really absorbed the prams of the discussion?

01:12:04.840 --> 01:12:05.840
Very few.

01:12:05.840 --> 01:12:10.840
So like I said, my thing is that again, this is fundamental infrastructure.

01:12:10.840 --> 01:12:13.840
This technology is a fundamental human right.

01:12:13.840 --> 01:12:16.840
Because otherwise what you're going to have, this is a discussion that, you know, you've

01:12:16.840 --> 01:12:18.840
had many times.

01:12:18.840 --> 01:12:20.840
Superhumans and normal humans.

01:12:20.840 --> 01:12:21.840
Yeah.

01:12:21.840 --> 01:12:26.840
The ability to communicate and create makes you superhuman.

01:12:26.840 --> 01:12:30.840
Because it's just not only images like it's presentations, it's being able to, like we

01:12:30.840 --> 01:12:34.840
have voice to voice technology that can allow you to speak more confidently.

01:12:34.840 --> 01:12:35.840
It's interesting.

01:12:35.840 --> 01:12:41.160
But people need to realize that today the poorest among us in society have more than

01:12:41.160 --> 01:12:44.320
the kings and queens had, you know, a couple of centuries ago.

01:12:44.320 --> 01:12:46.840
And this is about leveling the playing field.

01:12:46.840 --> 01:12:50.320
This is about, this is about, this is the technology and this is what I care about deeply.

01:12:50.320 --> 01:12:54.560
And I know you do too, uplifting humanity, enabling every man, woman and child to have

01:12:54.560 --> 01:12:57.600
access to food, water, health care, education.

01:12:57.600 --> 01:12:58.600
And have a voice.

01:12:58.600 --> 01:12:59.600
And have a voice.

01:12:59.600 --> 01:13:00.600
They are invisible.

01:13:00.600 --> 01:13:01.600
And have dreams.

01:13:01.600 --> 01:13:02.600
Yeah.

01:13:02.600 --> 01:13:05.600
And have dreams and have the tools to fulfill those dreams.

01:13:05.600 --> 01:13:06.600
And have agency.

01:13:06.600 --> 01:13:07.600
Yes.

01:13:07.600 --> 01:13:08.600
Agency is the right word.

01:13:08.600 --> 01:13:12.600
I had a bit of a flipping comment because again, I can do what I want in my kind of role.

01:13:12.600 --> 01:13:14.600
It was like, humanity is creatively constipated.

01:13:14.600 --> 01:13:16.600
We're going to make it so it can poop rainbows.

01:13:16.600 --> 01:13:18.600
I think that's great.

01:13:18.600 --> 01:13:19.600
It's a silly comment.

01:13:19.600 --> 01:13:23.600
But again, it's the reality because people don't believe they can create.

01:13:23.600 --> 01:13:28.600
They don't believe they, the mentality and mindset is wrong.

01:13:28.600 --> 01:13:30.600
Because people have more agents than they can do.

01:13:30.600 --> 01:13:35.600
An individual can shake the world or the individual can make anything around them better.

01:13:35.600 --> 01:13:37.600
But not if they don't believe they can.

01:13:37.600 --> 01:13:42.600
And this is why art therapy is used in mental health settings to amazing things.

01:13:42.600 --> 01:13:46.600
We've been conditioned to consume rather than create.

01:13:46.600 --> 01:13:50.600
We've been conditioned to be polarized rather than talk to each other and communicate with

01:13:50.600 --> 01:13:51.600
each other.

01:13:51.600 --> 01:13:52.600
And this can again, can change that.

01:13:52.600 --> 01:13:56.600
And again, that's why, like I said, this should be, in my opinion, a human right.

01:13:56.600 --> 01:13:59.600
It is infrastructure as important as 5G.

01:13:59.600 --> 01:14:03.600
And what I'm trying to catalyze now is not that I build the company that makes the decisions

01:14:03.600 --> 01:14:07.600
for that, but that we put it out there and we're spinning off a Lutheran other things.

01:14:07.600 --> 01:14:09.600
I just figured out a governance structure.

01:14:09.600 --> 01:14:10.600
It's not the UN.

01:14:10.600 --> 01:14:11.600
It's something else.

01:14:11.600 --> 01:14:22.600
So I think one important point is, do you think a world in which individuals are held

01:14:22.600 --> 01:14:27.600
back or restricted feel they have no hope or a world where every mother knows her children

01:14:27.600 --> 01:14:28.600
have access to the best healthcare?

01:14:28.600 --> 01:14:33.600
The best education, you know, the best ability to create, that's a more peaceful world in

01:14:33.600 --> 01:14:34.600
my mind.

01:14:34.600 --> 01:14:35.600
Yes, 100%.

01:14:35.600 --> 01:14:38.600
I mean, look, all wars are based on lies.

01:14:38.600 --> 01:14:39.600
Okay.

01:14:39.600 --> 01:14:44.600
For otherwise, both sides couldn't believe.

01:14:44.600 --> 01:14:46.600
Because humans are humans.

01:14:46.600 --> 01:14:48.600
To kill another human is disgusting.

01:14:48.600 --> 01:14:49.600
Right?

01:14:49.600 --> 01:14:51.600
And so you have to tell the lie that that person is the other.

01:14:51.600 --> 01:14:54.600
And you have to communicate it and control the means of communication.

01:14:54.600 --> 01:14:59.600
You look at kind of again where conflicts are resolved when people realize they are humans

01:14:59.600 --> 01:15:01.600
and we're all part of a global society.

01:15:01.600 --> 01:15:04.600
But our infrastructure has been set up to polarize.

01:15:04.600 --> 01:15:05.600
Literally, we can see it visually.

01:15:05.600 --> 01:15:07.600
This is how it happens.

01:15:07.600 --> 01:15:09.600
The incentive structure is misaligned.

01:15:09.600 --> 01:15:13.600
So how can you fight polarization if not by communication?

01:15:13.600 --> 01:15:18.600
And how can you do that if you don't give people these tools and you create it so that there

01:15:18.600 --> 01:15:21.600
is a base foundation for the world, so that there are generalized models that are global

01:15:21.600 --> 01:15:25.600
and every country has its own AI policy using their variants of those models.

01:15:25.600 --> 01:15:29.600
And then because it's all standardized, we can hop between one and the other.

01:15:29.600 --> 01:15:31.600
That is a peaceful future.

01:15:31.600 --> 01:15:32.600
Yes.

01:15:32.600 --> 01:15:35.600
And a future worth working towards creating.

01:15:35.600 --> 01:15:36.600
Working towards creating.

01:15:36.600 --> 01:15:40.600
But it's also, now it's the first time we can build that future because of this disruption

01:15:40.600 --> 01:15:41.600
in technology.

01:15:41.600 --> 01:15:47.600
Like, you know, governments are, there's a definition of a government.

01:15:47.600 --> 01:15:51.600
It is the entity with the legitimate use of political violence.

01:15:51.600 --> 01:15:53.600
The only one.

01:15:53.600 --> 01:15:55.600
That's a sad definition.

01:15:55.600 --> 01:15:57.600
But it's the nature of it, right?

01:15:57.600 --> 01:16:00.600
Because you saw lots of political violence and then it was consolidated into one entity.

01:16:00.600 --> 01:16:01.600
They can imprison you.

01:16:01.600 --> 01:16:02.600
Yes.

01:16:02.600 --> 01:16:05.600
And they've got an army back in the currency and this and that and that, right?

01:16:05.600 --> 01:16:09.600
And governments rule on the basis of pure legitimacy to violence.

01:16:09.600 --> 01:16:11.600
And again, we see that kind of thing.

01:16:11.600 --> 01:16:12.600
Right?

01:16:12.600 --> 01:16:15.600
So against this, what typically changes a government or a society?

01:16:15.600 --> 01:16:17.600
It is an act of violence in some ways.

01:16:17.600 --> 01:16:18.600
It's an act of disruption.

01:16:18.600 --> 01:16:19.600
It can be a technology.

01:16:19.600 --> 01:16:22.600
It can be a revolution or anything like that.

01:16:22.600 --> 01:16:23.600
This is a revolution.

01:16:23.600 --> 01:16:24.600
Yeah.

01:16:24.600 --> 01:16:28.600
That's happening just after COVID when everyone's thinking, holy crap, the system was rubbish.

01:16:28.600 --> 01:16:29.600
Let's do better.

01:16:29.600 --> 01:16:30.600
Yeah.

01:16:30.600 --> 01:16:31.600
So once in a lifetime.

01:16:31.600 --> 01:16:32.600
Yeah, I agree.

01:16:32.600 --> 01:16:41.600
And the challenge is that in the world today, you can't transform a government gradually.

01:16:41.600 --> 01:16:42.600
Yes.

01:16:42.600 --> 01:16:47.600
And this is why as well, crypto, there were some amazing things in there and amazingly

01:16:47.600 --> 01:16:49.600
smart people working there.

01:16:49.600 --> 01:16:53.600
It's rubbish because it tried to build a system outside of the existing system and there was

01:16:53.600 --> 01:16:56.600
this system and then there was the interface.

01:16:56.600 --> 01:16:59.600
Fortunes were made there and fraud happened there.

01:16:59.600 --> 01:17:00.600
Yeah.

01:17:00.600 --> 01:17:04.600
Whereas this AI, because it can bridge structure and unstructured, can actually go into our

01:17:04.600 --> 01:17:07.600
systems, out compete them and make them more efficient and bring them forward.

01:17:07.600 --> 01:17:11.600
And it's the first technology that can do that dynamically and at scale.

01:17:12.600 --> 01:17:21.600
Or build virtualized systems that are de novo that we spend our time in and opt out of the

01:17:21.600 --> 01:17:23.600
existing system and into the new.

01:17:23.600 --> 01:17:24.600
Exactly.

01:17:24.600 --> 01:17:25.600
And it cannot compete.

01:17:25.600 --> 01:17:29.600
Or the final thing, of course, is that if you keep it as it is whereby it's controlled

01:17:29.600 --> 01:17:33.600
by the few, they will ultimately use it as a system of control.

01:17:33.600 --> 01:17:36.600
It is the Panopticon forever.

01:17:36.600 --> 01:17:40.600
And again, we're seeing this in China and other places with a social credit score that's

01:17:40.600 --> 01:17:41.600
about to be augmented with AI.

01:17:41.600 --> 01:17:44.600
Everyone's looking at everyone else, marching everyone else.

01:17:44.600 --> 01:17:45.600
What is freedom there?

01:17:45.600 --> 01:17:46.600
Maybe it's still happiness.

01:17:46.600 --> 01:17:47.600
Maybe it's control.

01:17:47.600 --> 01:17:49.600
Let's give people the option, right?

01:17:49.600 --> 01:17:52.600
Let's give people the infrastructure and the building blocks they need to be independent,

01:17:52.600 --> 01:17:53.600
happy.

01:17:53.600 --> 01:17:55.600
And maybe they don't choose independence.

01:17:55.600 --> 01:17:57.600
Maybe it's a bit more kind of Borg-like.

01:17:57.600 --> 01:17:58.600
That's fine.

01:17:58.600 --> 01:18:00.600
At least you've got the choice.

01:18:00.600 --> 01:18:01.600
Yeah.

01:18:01.600 --> 01:18:06.600
Because again, if we tie this all back, the fact that you can now have AI that can write

01:18:06.600 --> 01:18:13.600
better than a human, that can draw better than a human, that can emote and speak to emotional

01:18:13.600 --> 01:18:18.600
turns, means that, let's say for instance, take one aspect of it, companies that are

01:18:18.600 --> 01:18:23.600
ad-driven that sell ads, they can create the most manipulative ads in the world ever,

01:18:23.600 --> 01:18:25.600
and regulators will not regulate that.

01:18:25.600 --> 01:18:26.600
That's interesting.

01:18:26.600 --> 01:18:27.600
And they do.

01:18:27.600 --> 01:18:32.600
No, but now it's the next step up because they have these latents that resonate.

01:18:32.600 --> 01:18:37.600
So, like now, when you look at some AI art, artists can complain what they want.

01:18:37.600 --> 01:18:38.600
It's resonant.

01:18:38.600 --> 01:18:41.600
When you listen to the most advanced AI voices, it's emotional.

01:18:41.600 --> 01:18:42.600
You can feel it.

01:18:42.600 --> 01:18:43.600
It tugs at something.

01:18:43.600 --> 01:18:46.600
And again, this is a breakthrough that's literally a year old.

01:18:46.600 --> 01:18:51.600
Let's talk about the ethics and morals.

01:18:51.600 --> 01:18:54.600
Does AI have a moral compass?

01:18:54.600 --> 01:18:57.600
Should it have a moral compass?

01:18:57.600 --> 01:19:00.600
Well, I think the creators of AI, technology is not neutral.

01:19:00.600 --> 01:19:01.600
Okay.

01:19:01.600 --> 01:19:06.600
The creators of technology do have a responsibility, and they will never make it neutral because

01:19:06.600 --> 01:19:08.600
it embodies their perspective.

01:19:08.600 --> 01:19:12.600
And it bodies the data set and other biases and things like that.

01:19:12.600 --> 01:19:18.600
So, I think that AI itself, this particular type of AI, again, if we just take the model,

01:19:18.600 --> 01:19:22.600
it is the action upon the model that then leads to the output.

01:19:22.600 --> 01:19:24.600
So, there's a responsibility there.

01:19:24.600 --> 01:19:26.600
But then, like, how do we adapt the model?

01:19:26.600 --> 01:19:29.600
Do we just have a one-and-done thing that's hardly trained on Western values and norms

01:19:29.600 --> 01:19:30.600
and mores?

01:19:30.600 --> 01:19:34.600
Which is the way it's been going historically in the large corporate setting?

01:19:34.600 --> 01:19:36.600
Yeah, because there's nothing you can do.

01:19:36.600 --> 01:19:40.600
Like, you can't build a Swahili version of it because you don't have access to technology.

01:19:40.600 --> 01:19:43.600
Whereas now, with the pre-training and other things like that, you can do that with one

01:19:43.600 --> 01:19:44.600
graphics card.

01:19:44.600 --> 01:19:45.600
That's great.

01:19:45.600 --> 01:19:46.600
Right?

01:19:46.600 --> 01:19:49.600
Because, again, we've kind of flipped the paradigm of an AI needs to be going all the time to

01:19:49.600 --> 01:19:51.600
this pre-compressed foundation model.

01:19:51.600 --> 01:19:56.600
So, I think that, you know, then there is the things of, like, when you've got self-driving

01:19:56.600 --> 01:19:59.600
cars and other things like that, what are the ethical norms, the trolley problem and

01:19:59.600 --> 01:20:01.600
everything that you input on that?

01:20:01.600 --> 01:20:03.600
These are not easy questions.

01:20:03.600 --> 01:20:06.600
Because you're extending humanity, which then means you're also extending the ethics of

01:20:06.600 --> 01:20:09.600
humanity, and that is not the same around the world.

01:20:09.600 --> 01:20:10.600
Education.

01:20:10.600 --> 01:20:12.600
One of your moonshots.

01:20:12.600 --> 01:20:18.600
We first got to know each other through the Global Learning XPrize that Elon Musk and

01:20:18.600 --> 01:20:20.600
Tony Robbins funded.

01:20:20.600 --> 01:20:23.600
You were one of the leaders of the one billion team.

01:20:23.600 --> 01:20:24.600
That wasn't either.

01:20:24.600 --> 01:20:28.600
I just kind of helped implement it with Joe and the Imagine World idea.

01:20:28.600 --> 01:20:35.600
And so, what, so, if you don't mind, what did your team do in that XPrize and then where

01:20:35.600 --> 01:20:37.600
are you taking this vision?

01:20:37.600 --> 01:20:42.600
So, my co-founder Joe and I, Joe led this, made Imagine Worldwide to take the winners of

01:20:42.600 --> 01:20:46.600
the XPrize KitKat school and one billion, the real kind of champions, and implement it

01:20:46.600 --> 01:20:47.600
around the world.

01:20:47.600 --> 01:20:48.600
Yeah.

01:20:48.600 --> 01:20:50.600
Into the Rohingya camps and Malawi and camps and others.

01:20:50.600 --> 01:20:53.600
Like, I just support from the side and share him along as he kind of goes and does the

01:20:53.600 --> 01:20:55.600
really hard, valuable work.

01:20:55.600 --> 01:20:59.600
But now we've seen that, I believe, the latest stats from the randomized controlled trials

01:20:59.600 --> 01:21:01.600
because you need to actually implement it as what happens.

01:21:01.600 --> 01:21:07.600
I think 76% of kids in refugee camps learning literacy and numeracy in 13 months without

01:21:07.600 --> 01:21:08.600
internet.

01:21:08.600 --> 01:21:09.600
It's immense.

01:21:09.600 --> 01:21:13.600
It's like one hour of use per day is equivalent of them being in school.

01:21:13.600 --> 01:21:14.600
Pretty much, yeah.

01:21:14.600 --> 01:21:18.600
Because, like, it's one teacher per 300 students, 400 students.

01:21:18.600 --> 01:21:19.600
But then, is it enough?

01:21:19.600 --> 01:21:21.600
No, it's not enough what we've done at the moment.

01:21:21.600 --> 01:21:26.600
What needs to happen is there needs to be a big grand challenge whereby, you know, Malawi

01:21:26.600 --> 01:21:30.600
and Malawi kind of has said that they want to roll this out at super scale and so have

01:21:30.600 --> 01:21:33.600
multiple other governments now that we have the RCTs.

01:21:33.600 --> 01:21:37.600
Let's make an open source ecosystem that has AI at the core that teaches kids and learns

01:21:37.600 --> 01:21:38.600
from kids.

01:21:38.600 --> 01:21:42.600
So you take from what's happening in Malawi, move it to Ethiopia, Sierra Leone, Rohingya

01:21:42.600 --> 01:21:44.600
camps, Brooklyn, everywhere.

01:21:44.600 --> 01:21:52.600
And so there is an actual superstar, amazingly well created ecosystem for education.

01:21:52.600 --> 01:21:55.600
And again, go to the future and bring it back with us.

01:21:55.600 --> 01:21:56.600
Love that.

01:21:56.600 --> 01:22:00.600
This year at Abundance360, Sal Khan is going to be joining us as well.

01:22:00.600 --> 01:22:02.600
Have you spent any time with Sal?

01:22:02.600 --> 01:22:03.600
No, not yet.

01:22:03.600 --> 01:22:08.600
Okay, so I'm excited to connect you guys because, you know, he's built something pretty extraordinary,

01:22:08.600 --> 01:22:13.600
but his vision is to bring AI to it so that it's AI is generating the content, not him.

01:22:13.600 --> 01:22:18.600
And it's able to rapidly iterate for cultural appropriateness and so forth.

01:22:18.600 --> 01:22:19.600
This is why we need to build.

01:22:19.600 --> 01:22:24.600
So one of our things is that we're building national level models from India to other ones

01:22:24.600 --> 01:22:26.600
where there's localized data sets and other things.

01:22:26.600 --> 01:22:28.600
If you can get the education piece going.

01:22:28.600 --> 01:22:33.600
Remember how I said that this AI is like a kindergarten or a grade schooler?

01:22:33.600 --> 01:22:34.600
Sure.

01:22:34.600 --> 01:22:35.600
It matters what you teach it.

01:22:35.600 --> 01:22:37.600
So right now we're teaching everything.

01:22:37.600 --> 01:22:38.600
Do we need to teach everything?

01:22:38.600 --> 01:22:39.600
No.

01:22:39.600 --> 01:22:43.600
So if you have an AI that teaches the kids and learns from the kids, that's the best data

01:22:43.600 --> 01:22:48.600
in the world to teach an AI for a Kenyan model or a Nigerian model or others.

01:22:48.600 --> 01:22:50.600
And you know who should run that technology?

01:22:50.600 --> 01:22:54.600
Nigerians and Kenyans and others.

01:22:54.600 --> 01:22:56.600
And so one of the things we're doing is...

01:22:56.600 --> 01:22:59.600
It's almost like family based learning and extrapolated from there.

01:22:59.600 --> 01:23:00.600
Exactly, because we don't know best.

01:23:00.600 --> 01:23:07.600
We can give tools and we've reduced the barriers to create national level localized cultural models.

01:23:07.600 --> 01:23:12.600
And then those models together form a constellation that not only have you got base learning of

01:23:12.600 --> 01:23:16.600
like what's the optimal way to teach an Algebra, getting better, then you can go beyond that to that.

01:23:16.600 --> 01:23:21.600
And the plan is to have an integrated system where it's hardware, software, deployment curriculum.

01:23:21.600 --> 01:23:24.600
Because then we can update that through mesh networking or the amazing work of Project Giga,

01:23:24.600 --> 01:23:29.600
which is putting high speed internet into every school in the world by the UN.

01:23:29.600 --> 01:23:31.600
And then you can put healthcare on that.

01:23:31.600 --> 01:23:32.600
Yes.

01:23:32.600 --> 01:23:36.600
And then you have a self-adaptive improving healthcare system, self-adaptive improving education system.

01:23:36.600 --> 01:23:37.600
And then the world...

01:23:37.600 --> 01:23:43.600
I mean, for me, that's the calling that I think both of us have and hopefully many entrepreneurs here.

01:23:43.600 --> 01:23:48.600
It's like what greater purpose could you have in life than uplifting humanity in that fashion?

01:23:48.600 --> 01:23:49.600
Exactly.

01:23:49.600 --> 01:23:54.600
And then as an etc. manager, you can fund that at scale by bonds.

01:23:54.600 --> 01:23:55.600
Yeah.

01:23:55.600 --> 01:23:57.600
And the world's biggest problems are the world's biggest business opportunities, right?

01:23:57.600 --> 01:23:58.600
Exactly.

01:23:58.600 --> 01:24:00.600
Why to become a billionaire, help a billion people.

01:24:00.600 --> 01:24:05.600
So one of the ways that we're kind of doing it is results-oriented bonds whereby you can pledge

01:24:05.600 --> 01:24:11.600
$20 million for million kids are provably educated on this standardized architecture.

01:24:11.600 --> 01:24:13.600
The invisible become visible.

01:24:13.600 --> 01:24:14.600
And measurable.

01:24:14.600 --> 01:24:15.600
Measurable.

01:24:15.600 --> 01:24:19.600
Infrastructure banks and the World Bank fund the remainder held by the pension funds.

01:24:19.600 --> 01:24:22.600
And you can divert billions and billions of dollars into this.

01:24:22.600 --> 01:24:26.600
It's kind of the promise of one laptop a child can finally be done.

01:24:26.600 --> 01:24:30.600
But rather than building a system for today, we build a system for tomorrow that constantly adapts

01:24:30.600 --> 01:24:35.600
and improves is understandable and standardized because that is infrastructure.

01:24:35.600 --> 01:24:36.600
Yes.

01:24:36.600 --> 01:24:39.600
Again, the thing I really want to emphasize is this AI is infrastructure.

01:24:39.600 --> 01:24:41.600
It's more important than 5G or anything else.

01:24:41.600 --> 01:24:42.600
It's oxygen in the room.

01:24:42.600 --> 01:24:44.600
It's oxygen in the room.

01:24:44.600 --> 01:24:45.600
Yeah.

01:24:45.600 --> 01:24:49.600
So when I think about the future of education going out 10, 20 years and bringing it back

01:24:49.600 --> 01:24:54.600
today, for me, it's not a book and it's not a flat screen.

01:24:54.600 --> 01:24:55.600
It's going into a virtual world.

01:24:55.600 --> 01:25:00.600
If I want to learn about Plato, there's a guy sitting on a slab of marble over there.

01:25:00.600 --> 01:25:02.600
And he says, Hey, Peter, come on over.

01:25:02.600 --> 01:25:03.600
Let me show you around.

01:25:03.600 --> 01:25:05.600
Introduce my friends.

01:25:05.600 --> 01:25:07.600
And it's experiential.

01:25:07.600 --> 01:25:14.600
And that that NPC of Plato is trained up by all the knowledge about Plato by every historian

01:25:14.600 --> 01:25:18.600
and it's accurate and the imagery and so forth.

01:25:18.600 --> 01:25:25.600
And what you just said earlier about real time rendering from stable diffusion enables

01:25:25.600 --> 01:25:26.600
that, right?

01:25:26.600 --> 01:25:31.600
And the ability to take every historian's work on Plato and train up a model on Plato

01:25:31.600 --> 01:25:32.600
enables all that.

01:25:32.600 --> 01:25:33.600
Exactly.

01:25:33.600 --> 01:25:38.600
And particularly when it is at the hardware level, because typically what software happens

01:25:38.600 --> 01:25:44.600
is that, again, you build layers and layers and layers of kind of compilers and translations

01:25:44.600 --> 01:25:46.600
so you're far from the hardware.

01:25:46.600 --> 01:25:48.600
Some models are already efficient at the top level.

01:25:48.600 --> 01:25:51.600
What happens when we optimize them and push them down to the hardware level?

01:25:51.600 --> 01:25:52.600
You don't need internet.

01:25:52.600 --> 01:25:53.600
You don't need anything.

01:25:53.600 --> 01:25:54.600
You can form it.

01:25:54.600 --> 01:25:57.600
But then all of a sudden you have the young ladies illustrated primer.

01:25:57.600 --> 01:25:58.600
Yes.

01:25:58.600 --> 01:26:03.600
Neil Stevenson's incredible book and a vision for the future of education.

01:26:03.600 --> 01:26:04.600
Exactly.

01:26:04.600 --> 01:26:06.600
We can make it finally, but we can make it more.

01:26:06.600 --> 01:26:09.600
We can make it closer to the prime radiance and foundation.

01:26:09.600 --> 01:26:11.600
So are you building hardware, Imad?

01:26:11.600 --> 01:26:13.600
Getting other people to build it for me.

01:26:13.600 --> 01:26:15.600
My life is not a complicated.

01:26:15.600 --> 01:26:16.600
We're setting the specs.

01:26:16.600 --> 01:26:17.600
We're setting the specs.

01:26:17.600 --> 01:26:18.600
But this is the thing.

01:26:18.600 --> 01:26:19.600
Who is we?

01:26:19.600 --> 01:26:20.600
Okay.

01:26:20.600 --> 01:26:25.600
The way that we're going to do it is that it's similar to the grand challenges and the

01:26:25.600 --> 01:26:26.600
prizes and things like that.

01:26:26.600 --> 01:26:27.600
Let's get together.

01:26:27.600 --> 01:26:31.600
We will drive the process because the rule by committee never works.

01:26:31.600 --> 01:26:36.600
But let's invite everyone to participate from the kids using the tablets to code them

01:26:36.600 --> 01:26:39.600
to the most advanced developers in the world.

01:26:39.600 --> 01:26:41.600
And let's build something for humanity.

01:26:41.600 --> 01:26:43.600
That's the way to do this.

01:26:43.600 --> 01:26:44.600
Amazing.

01:26:44.600 --> 01:26:46.600
So you don't like the term Web 3.

01:26:46.600 --> 01:26:50.600
You wanted to jump over to Web 4.

01:26:50.600 --> 01:27:03.600
But this virtualized world, which is the convergence of AI and VR, AR, blockchain and so forth,

01:27:03.600 --> 01:27:07.600
where do you see it in the near term going?

01:27:07.600 --> 01:27:09.600
It's just going to go insane.

01:27:09.600 --> 01:27:13.600
And so like we have technology, I'm going to make an announcement in January about some

01:27:13.600 --> 01:27:14.600
of our technology.

01:27:14.600 --> 01:27:20.600
You've got Apple likely having AR glasses, snap, oculuses, all of this.

01:27:20.600 --> 01:27:23.100
It'll be a fully immersive world where you can engage.

01:27:23.100 --> 01:27:26.600
Like when I was a video game investor, I looked at time to flow, fun and frustration.

01:27:26.600 --> 01:27:27.600
Yes.

01:27:27.600 --> 01:27:30.600
And I think this technology can adjust all of those and create worlds for us.

01:27:30.600 --> 01:27:31.600
But it'll be a year or two.

01:27:31.600 --> 01:27:37.200
Because again, now it's percolating and it's getting ready to then create brand new experiences

01:27:37.200 --> 01:27:39.080
on the fly for everyone.

01:27:39.080 --> 01:27:43.800
And I think in a couple of years, it starts going in five years.

01:27:43.800 --> 01:27:47.280
Creativity, imagination, engagement, entertainment is completely transformed.

01:27:47.280 --> 01:27:48.280
Within five years.

01:27:48.280 --> 01:27:49.280
Within five years.

01:27:49.280 --> 01:27:54.040
It's going to be the biggest shift change that we've ever seen because the incumbents

01:27:54.040 --> 01:27:59.720
can't keep up with entities that are using this technology that can do everything.

01:27:59.720 --> 01:28:06.760
What does making a picture look like when you can change it live with just words?

01:28:06.760 --> 01:28:11.560
And you can say, actually, make his hair a bit longer or get rid of those nostril hairs

01:28:11.560 --> 01:28:14.760
and it just understands and does it without having to decode it.

01:28:14.760 --> 01:28:15.760
Intentionality.

01:28:15.760 --> 01:28:16.760
Yes.

01:28:16.760 --> 01:28:17.760
Intentionality and action.

01:28:17.760 --> 01:28:18.760
Yes.

01:28:18.760 --> 01:28:23.240
It's kind of the whole military thing of observe oriented, decide and act, right?

01:28:23.240 --> 01:28:25.880
These systems enable that almost flawlessly.

01:28:25.880 --> 01:28:30.000
Like if you use OpenAI's whisper technology, the translation of the podcast is just immense

01:28:30.000 --> 01:28:34.360
and all these other things because it's learned so many principles and they're tiny files.

01:28:34.360 --> 01:28:38.480
And then you can make it self-referential and say, improve it the way you think I'd

01:28:38.480 --> 01:28:39.480
want it to prove.

01:28:39.480 --> 01:28:40.480
You can.

01:28:40.480 --> 01:28:42.760
And like we're building technology, for example, that tells you how good a story is or how

01:28:42.760 --> 01:28:43.760
good code is.

01:28:43.760 --> 01:28:44.760
Yes.

01:28:44.760 --> 01:28:45.760
So then you have a creation and a discriminator.

01:28:45.760 --> 01:28:49.960
They bounce back and forth against each other and they learn from your personal context.

01:28:49.960 --> 01:28:50.960
Amazing.

01:28:50.960 --> 01:28:57.960
You took a break from being a hedge fund manager to address your son's autism, which was, I

01:28:57.960 --> 01:29:03.520
mean, there are individuals like yourself, like Martin Rothblatt and others who are like,

01:29:04.520 --> 01:29:13.200
I refuse to believe that something can't be done and you jumped in.

01:29:13.200 --> 01:29:19.360
For those who are dealing with autism personally or in their families or in their community,

01:29:19.360 --> 01:29:22.040
what were your learnings and what would you advise?

01:29:22.040 --> 01:29:26.360
There's always room for improvement and everyone struggles with something.

01:29:26.360 --> 01:29:27.360
How old is your son today?

01:29:27.360 --> 01:29:28.360
He's 14.

01:29:28.360 --> 01:29:29.360
14.

01:29:29.360 --> 01:29:30.360
He's super happy.

01:29:30.680 --> 01:29:33.680
I think they balance each other out, that's what they say.

01:29:33.680 --> 01:29:34.680
You do it pretty well.

01:29:34.680 --> 01:29:35.680
It's all right.

01:29:35.680 --> 01:29:40.280
So I had a lot of issues around those things, but people, again, everyone's different and

01:29:40.280 --> 01:29:45.920
then diversity is our strength as it were, but sometimes kids and others need help because

01:29:45.920 --> 01:29:48.640
they can't achieve their potential because there's too much going on.

01:29:48.640 --> 01:29:52.480
So like I said, I'm buying into the Gabberglute to make balance theory of it and we had some

01:29:52.480 --> 01:29:55.760
drug repurposing and other stuff on an N of one equals case.

01:29:55.760 --> 01:29:59.360
That's not science as it were, it's first principles and I don't think it's coherent

01:29:59.400 --> 01:30:01.720
enough to be brought forward, but it's interesting.

01:30:01.720 --> 01:30:05.200
Instead, I think there are certain things that benefit everyone, such as a blood behavioural

01:30:05.200 --> 01:30:10.320
analysis, whereby because you haven't built up the words, there are short trials to try

01:30:10.320 --> 01:30:12.680
and reconstruct what a cup means.

01:30:12.680 --> 01:30:16.720
It's quite an intensive process, but it's also what people do after strokes and other

01:30:16.720 --> 01:30:20.360
things that make them lose their connectivity in their brain.

01:30:20.360 --> 01:30:24.080
Most of this thing though is about noise filtering and reduction of that, but I think

01:30:24.080 --> 01:30:28.480
percolation of that means that you have to look into first principles, analysis of some

01:30:28.480 --> 01:30:31.720
of the science of what can cause it, and then you have to bring that forward to what

01:30:31.720 --> 01:30:32.720
is safe.

01:30:32.720 --> 01:30:36.160
So don't do kind of crackpot theories, but there's an emerging science and studies showing

01:30:36.160 --> 01:30:41.000
things like NSE til 16, sulfur refrain, other things that calm you down are probably the

01:30:41.000 --> 01:30:42.000
best.

01:30:42.000 --> 01:30:45.680
And a large part of it is just connection and engagement there.

01:30:45.680 --> 01:30:48.600
So one of the things that we'll be doing next year is that we're taking all the research

01:30:48.600 --> 01:30:53.160
that I've done and formalizing it properly, because again, it's not a case of, well, I

01:30:53.160 --> 01:30:55.160
can do it, so anyone should have it.

01:30:55.160 --> 01:30:59.480
I did an N equals one case, but then that should be extended out.

01:30:59.480 --> 01:31:03.240
And this is why I realized when COVID came along.

01:31:03.240 --> 01:31:08.160
So I designed and launched with the help of loads of people, collect from an augmented

01:31:08.160 --> 01:31:13.520
intelligence against COVID-19, launched at Stanford in July of 2020.

01:31:13.520 --> 01:31:16.800
Was that the actual origin of stability?

01:31:16.800 --> 01:31:18.320
That was the first origin of stability.

01:31:18.320 --> 01:31:21.280
So we didn't incorporate at that time, but kind of put it together.

01:31:21.280 --> 01:31:25.800
I mean, it's insane how far things have gone in two years.

01:31:25.800 --> 01:31:26.800
Yeah.

01:31:26.800 --> 01:31:30.800
I mean, we probably actually kicked off in 13 months ago.

01:31:30.800 --> 01:31:33.920
So yeah, it was insane because I thought, I saw it coming and I saw it like autism as

01:31:33.920 --> 01:31:39.320
a multi-systemic inflammatory condition where even now, if anyone asks you, how does COVID

01:31:39.320 --> 01:31:40.400
actually work?

01:31:40.400 --> 01:31:43.040
If you ask a scientist, they'll tell you, we don't actually, sure.

01:31:43.040 --> 01:31:44.040
Sure.

01:31:44.040 --> 01:31:46.360
Like why are ferritin levels high and why is this and that?

01:31:46.360 --> 01:31:49.400
The reality is our base foundation is not good enough.

01:31:49.400 --> 01:31:50.520
We don't have enough shared knowledge.

01:31:50.520 --> 01:31:53.520
So I created that to create a system that's comprehensive, authoritative and up-to-date.

01:31:53.520 --> 01:31:56.640
So there's a nice blog about an OCDU type site and things like that.

01:31:56.640 --> 01:32:00.520
A lot of private sector enterprises that promised a lot didn't deliver.

01:32:00.520 --> 01:32:04.560
And so I realized, again, this technology was the future and we needed to create open

01:32:04.560 --> 01:32:05.680
infrastructure for that.

01:32:05.680 --> 01:32:09.880
So when we do our autism releases next year, all of the knowledge and everything like that

01:32:09.880 --> 01:32:10.880
will be available.

01:32:10.880 --> 01:32:15.920
There will be a semantic scholar on steroids, which allows you to access the information

01:32:15.920 --> 01:32:18.640
relevant to the type of autism that your child might have.

01:32:18.640 --> 01:32:23.480
So basically, we figured out there were 16 different etiologies while driving conditions.

01:32:23.480 --> 01:32:27.680
But 30% of kids would get worse and 26% of kids would get better with the same treatment,

01:32:27.680 --> 01:32:28.680
so it doesn't work.

01:32:28.680 --> 01:32:29.680
Yeah.

01:32:29.680 --> 01:32:32.840
This era of personalized medicine needs a foundation and again, that foundation must

01:32:32.840 --> 01:32:36.480
be common, but we can't wait around to do it.

01:32:36.480 --> 01:32:37.480
And it's interesting, right?

01:32:37.480 --> 01:32:44.160
The more I learn about the fundamentals of human biology, the more complicated.

01:32:44.160 --> 01:32:46.240
You can dig layer after layer after layer.

01:32:46.240 --> 01:32:47.240
Yeah.

01:32:47.240 --> 01:32:53.640
It's possible for, I mean, we're discovering so much because the tools we're using, but

01:32:53.640 --> 01:33:02.080
we're going to need this level of AI to cognitively understand the interactions of the system.

01:33:02.080 --> 01:33:05.600
Again, it's a complex hierarchical system, you know, classical Herb Simons style.

01:33:05.600 --> 01:33:08.400
And so we have to build new tools to enable that.

01:33:08.400 --> 01:33:11.200
The question is, are these tools closed and the company is trying to, are they open?

01:33:11.200 --> 01:33:13.240
And so our take is open.

01:33:13.240 --> 01:33:16.400
And then so our business model is just scale and service around that, which is how all

01:33:16.400 --> 01:33:19.560
the servers and databases are, but open is secure as well.

01:33:19.560 --> 01:33:23.080
That's why Linux is used everywhere versus Microsoft Windows, et cetera.

01:33:23.080 --> 01:33:26.680
So I think the final part of this is that, you know, again, it's all interrelated.

01:33:26.680 --> 01:33:29.640
Like the body is such a wonderful, powerful thing.

01:33:29.640 --> 01:33:33.680
If you look at longevity and things like that, we need, again, this first principles thinking

01:33:33.680 --> 01:33:37.640
to both make us healthy and live longer and be happier.

01:33:37.640 --> 01:33:39.640
Yeah.

01:33:39.640 --> 01:33:42.960
Have you thought much about what's coming down the pike with quantum technologies and

01:33:42.960 --> 01:33:43.960
quantum computing?

01:33:44.000 --> 01:33:48.240
So like, I think very sympathetic to various approaches there, like I'm quite, I quite

01:33:48.240 --> 01:33:53.160
like the quantum annealing on the kind of D wave, because I think that, you know, Carl

01:33:53.160 --> 01:33:57.800
Friston's theory around kind of free energy principle and having these low energy states

01:33:57.800 --> 01:33:58.800
makes a lot of sense.

01:33:58.800 --> 01:34:02.200
In fact, this is similar to what the AI models do in that when you look at the latent spaces,

01:34:02.200 --> 01:34:05.520
you're going to the low energy states of what that could kind of mean.

01:34:05.520 --> 01:34:06.520
Right.

01:34:06.520 --> 01:34:09.080
So that's quite a lot of jargon, I think for a lot of lists.

01:34:09.080 --> 01:34:12.320
But basically, quantum computing kind of is another part of the puzzle.

01:34:12.680 --> 01:34:16.000
A lot of people try to take one AI model and say, Oh, we're going to put something

01:34:16.000 --> 01:34:17.000
that can do everything.

01:34:17.000 --> 01:34:19.400
You know, they look at AGI as the thing that can do everything.

01:34:19.400 --> 01:34:22.480
The human brain is made up of so many different parts.

01:34:22.480 --> 01:34:25.400
And we're just filling in some of the missing gaps right now of which quantum computing

01:34:25.400 --> 01:34:26.400
is one of them.

01:34:26.400 --> 01:34:27.400
Yeah.

01:34:27.400 --> 01:34:31.640
And it's, I think it's adding fuel to the fire of how fast things are moving.

01:34:31.640 --> 01:34:32.640
Ah, yeah.

01:34:32.640 --> 01:34:38.240
It's going to be, you know, we can see the qubits increasing and we can see it getting

01:34:38.240 --> 01:34:39.240
there.

01:34:39.240 --> 01:34:40.720
I mean, again, part of this is a supercomputer thing, right?

01:34:40.720 --> 01:34:44.640
Like our supercomputer, we've been the fastest in the world five years ago, which is insane

01:34:44.640 --> 01:34:45.960
for a private company.

01:34:45.960 --> 01:34:46.960
Right.

01:34:46.960 --> 01:34:47.960
Absolutely.

01:34:47.960 --> 01:34:51.400
You know, like think about bigger than everything.

01:34:51.400 --> 01:34:54.520
But if you look at it like Nvidia is the one that kicked off because they put AI in the

01:34:54.520 --> 01:34:58.280
core of their graphics cards before this even happened, which is why it's now a 32 billion

01:34:58.280 --> 01:35:00.800
dollar part of their business.

01:35:00.800 --> 01:35:04.640
The AV 100s, four years ago with the first iteration, the A 100s for the next.

01:35:04.640 --> 01:35:08.040
Now the H 100s are like up to nine times faster.

01:35:08.040 --> 01:35:09.560
It's literally going exponential.

01:35:09.560 --> 01:35:11.600
The compute is going exponential.

01:35:11.600 --> 01:35:16.400
The research, the technologies, we see exponential technologies everywhere and we're like, it's

01:35:16.400 --> 01:35:18.400
so difficult to piece all these things together to see.

01:35:18.400 --> 01:35:21.160
I don't even know where things will be in a year, let alone five years, or ten years.

01:35:21.160 --> 01:35:25.480
I think that's an important part, and this is what, when Ray talks about the singularity,

01:35:25.480 --> 01:35:29.280
you know, it's the notion that we're, you know, the ability to predict what's going

01:35:29.280 --> 01:35:32.160
to happen next has become impossible.

01:35:32.160 --> 01:35:35.480
And that timeframe, you know, people say, what's it going to be like in 20 years?

01:35:35.480 --> 01:35:39.440
I can barely think about 10 years from now or five years from now, let alone 20 years

01:35:39.440 --> 01:35:40.440
from now.

01:35:40.440 --> 01:35:41.440
Yeah.

01:35:41.440 --> 01:35:42.440
And this affects the way that we act.

01:35:42.440 --> 01:35:45.320
So the way that our brains work is that our default is decision making under risk.

01:35:45.320 --> 01:35:49.200
So we look at the upside down side of something and we make an expected utility calculation

01:35:49.200 --> 01:35:51.640
based on that, because systems are stable.

01:35:51.640 --> 01:35:53.160
Is the world stable now?

01:35:53.160 --> 01:35:54.160
No.

01:35:54.160 --> 01:35:55.160
Yeah.

01:35:55.160 --> 01:35:56.160
You know, so what do we do instead?

01:35:56.160 --> 01:36:00.640
We make decisions under uncertainty, which is we minimize for maximum regret, you know,

01:36:00.640 --> 01:36:03.000
or actually we just minimize for regret.

01:36:03.000 --> 01:36:06.240
So with these powerful technologies, people are like, don't give them out because I don't

01:36:06.240 --> 01:36:07.240
know what can happen.

01:36:07.240 --> 01:36:09.840
It's the, it's the, it's the amygdala.

01:36:09.840 --> 01:36:11.440
It's a dystopian point of view.

01:36:11.440 --> 01:36:13.960
It's minimizing our downside, protecting what we have.

01:36:13.960 --> 01:36:19.920
It's a scarcity and fear based animal brain, you know, reptile brain that we default to.

01:36:19.920 --> 01:36:22.080
And this happens synchronously.

01:36:22.080 --> 01:36:23.080
That's what we saw with COVID.

01:36:23.080 --> 01:36:24.080
It just happened.

01:36:24.080 --> 01:36:25.920
All of a sudden, everyone thought the same, right?

01:36:25.920 --> 01:36:29.240
It's what we're seeing with tech layoffs and things like that, like Facebook made loads

01:36:29.240 --> 01:36:30.240
of money.

01:36:30.240 --> 01:36:31.240
Why are they laying off?

01:36:31.240 --> 01:36:32.320
You know, things like that.

01:36:32.320 --> 01:36:35.640
And so we have to be kind of aware of this because what's happening now as well is that

01:36:35.640 --> 01:36:38.080
the classical system has reached its end.

01:36:38.080 --> 01:36:40.120
We've borrowed too much money from the future.

01:36:40.120 --> 01:36:43.400
And now we are going to a negative sum game with inflation and other things.

01:36:43.400 --> 01:36:45.320
People are losing wealth rapidly.

01:36:45.320 --> 01:36:48.760
That leads to unstable Nash equilibrium from a game theoretic perspective.

01:36:48.760 --> 01:36:54.680
So we go from one steady state and lurch to another inflation to deflation to maximum

01:36:54.680 --> 01:36:57.240
employment to job losses to fiscal stuff.

01:36:57.240 --> 01:36:59.360
And it's just going to be a crazy few years.

01:36:59.360 --> 01:37:01.400
And that's the reason I called it stability.

01:37:01.400 --> 01:37:05.600
I was going to, that was one of my questions where, you know, and obviously there's, there's

01:37:05.600 --> 01:37:11.640
a terminology of stability in the, in the, in the models that you're building, but stability

01:37:11.640 --> 01:37:13.520
in order to help stabilize society.

01:37:13.520 --> 01:37:14.520
Yes.

01:37:14.520 --> 01:37:19.200
Build a human OS or catalyze a human OS because it can only be built by society.

01:37:19.200 --> 01:37:20.400
We're just a capitalist.

01:37:20.400 --> 01:37:21.400
Yes.

01:37:21.400 --> 01:37:26.760
Help guide that in a way because this AI can be finally the thing that can stabilize a

01:37:26.760 --> 01:37:30.720
complex system that is humanity and then I'll just achieve our potential.

01:37:30.720 --> 01:37:35.840
That's a platform, a infrastructure platform that uplifts all of humanity.

01:37:35.840 --> 01:37:36.840
Yes.

01:37:36.840 --> 01:37:39.000
And again, it should be run by the people for the people.

01:37:39.000 --> 01:37:42.280
So like our subsidiaries in the countries, we're putting aside 10% of the equity for

01:37:42.280 --> 01:37:46.040
the kids that use the tablets and I'm never going to take a penny out of any of them until

01:37:46.040 --> 01:37:48.240
IPOing them because they should be owned by the people.

01:37:48.240 --> 01:37:49.240
I love that.

01:37:49.240 --> 01:37:50.240
Okay.

01:37:50.240 --> 01:37:58.800
I want to close this out with two, two topics around the moonshots and mindsets.

01:37:58.800 --> 01:38:08.840
If I were to launch an EMAID XPRIZE fully funded and push it out to the world, what's

01:38:08.840 --> 01:38:14.360
a grand challenge in XPRIZE that you would love to see materialized out there?

01:38:14.360 --> 01:38:16.240
I think this education is the key one.

01:38:16.240 --> 01:38:21.600
I think it's the next step of global learning, which is a grand challenge just to build this

01:38:21.600 --> 01:38:23.960
system and especially for low income countries.

01:38:23.960 --> 01:38:30.280
And again, the delta on the impact can be so massive on this because it's infrastructure

01:38:30.280 --> 01:38:31.280
for the next generation.

01:38:31.280 --> 01:38:36.400
Like a lot of these emerging markets leapfrogged straight to mobile phones to skip computers.

01:38:36.400 --> 01:38:38.920
Now they can skip to the AI age.

01:38:38.920 --> 01:38:39.920
How amazing will that be?

01:38:39.920 --> 01:38:42.880
It'll uplift everyone.

01:38:42.880 --> 01:38:49.120
And there's no greater asset to a nation, a company, anybody than the intelligence of

01:38:49.120 --> 01:38:51.720
its citizens.

01:38:51.720 --> 01:38:52.720
Minds are intelligent.

01:38:52.720 --> 01:38:57.160
They don't have access to be able to take that intelligence and build for themselves

01:38:57.160 --> 01:38:58.160
and extend that.

01:38:58.160 --> 01:39:01.720
We can make that infrastructure now to do it for the first time ever.

01:39:01.720 --> 01:39:02.720
Yeah.

01:39:02.720 --> 01:39:03.720
Amazing.

01:39:03.720 --> 01:39:04.720
Amazing.

01:39:04.720 --> 01:39:07.600
What are the mindsets that have allowed you to be successful, do you think?

01:39:07.600 --> 01:39:13.880
I talk about a abundance mindset, a longevity mindset, a moonshot mindset, exponential mindset,

01:39:13.880 --> 01:39:14.880
curiosity mindset.

01:39:14.880 --> 01:39:17.800
Do any of those resonate for you or are there other mindsets?

01:39:17.800 --> 01:39:22.040
Because I think mindsets are the most important differentiator that we have.

01:39:22.040 --> 01:39:26.360
So like, I've always been very lucky and I've achieved very interesting things.

01:39:26.360 --> 01:39:30.880
I was never really motivated for the last few years when I finally applied myself.

01:39:30.880 --> 01:39:34.680
And what I'm good at is first principles thinking on those first bits rather than atoms.

01:39:34.680 --> 01:39:36.880
But I realized there's nothing I can't do.

01:39:36.880 --> 01:39:40.120
There's nothing, people can do anything if they put their mind to it.

01:39:40.120 --> 01:39:43.640
But they have to think in a structured way and they have to let almost water flow as

01:39:43.640 --> 01:39:44.640
it were.

01:39:44.640 --> 01:39:50.040
And what I just do is I try to make it a win-win for everyone to participate, to help, to extend

01:39:50.040 --> 01:39:51.040
this.

01:39:51.040 --> 01:39:54.680
And at a time of absolute change, you can make that happen.

01:39:54.680 --> 01:39:57.800
So this is why I go to the future and I go back to the past and I work back that way.

01:39:57.800 --> 01:40:00.760
Which I think a lot of people don't because they get stuck in the present.

01:40:00.760 --> 01:40:06.880
So it's the moonshot mindset looking to the future and again, recursively back propagating.

01:40:06.880 --> 01:40:12.160
And I'll close out with, if you were going to list the moonshots that you're working

01:40:12.160 --> 01:40:13.600
on right now.

01:40:13.600 --> 01:40:14.880
You're clearly working on education.

01:40:14.880 --> 01:40:16.760
We've talked about that.

01:40:16.760 --> 01:40:23.840
But as someone who's going to disrupt all the industries, healthcare as a moonshot?

01:40:23.840 --> 01:40:26.360
I am working on everything.

01:40:26.360 --> 01:40:27.360
Everything.

01:40:27.360 --> 01:40:29.400
I've got about 18 different ones.

01:40:29.400 --> 01:40:33.480
But education is core and creativity is core.

01:40:33.480 --> 01:40:35.320
Those two enable everything else.

01:40:35.320 --> 01:40:39.800
If you want to fix climate, if you want to fix hate, if you want to fix a lot of different

01:40:39.800 --> 01:40:43.320
things, get those two right and that's the foundation for the future.

01:40:44.040 --> 01:40:50.160
It's a pleasure, my friend, with a fun conversation, excited to share your wisdom and vision with

01:40:50.160 --> 01:40:56.920
the world, excited for what you're creating as a fundamental platform and infrastructure

01:40:56.920 --> 01:40:59.440
for humanity.

01:40:59.440 --> 01:41:03.200
I wish you all the luck and look forward to supporting me any way I can.

01:41:03.200 --> 01:41:04.200
Thank you very much.

01:41:04.200 --> 01:41:05.200
Again, it's just a little catalyst.

01:41:05.200 --> 01:41:07.280
It'll be everyone else that drives this forward.

01:41:07.280 --> 01:41:08.760
See you in March at A360.

01:41:08.760 --> 01:41:09.760
Cheers.

01:41:09.760 --> 01:41:10.760
Cheers.

01:41:10.760 --> 01:41:11.760
Everyone, this is Peter again.

01:41:11.760 --> 01:41:15.960
Before you take off, I want to take a moment to just invite you to subscribe to my weekly

01:41:15.960 --> 01:41:16.960
tech blog.

01:41:16.960 --> 01:41:22.200
Today, over 200,000 people received this email twice per week.

01:41:22.200 --> 01:41:26.640
In the tech blog, I share with you my insights on converging exponential technologies, what's

01:41:26.640 --> 01:41:31.280
going on in AI, how longevity is transforming, adding decades to our life.

01:41:31.280 --> 01:41:34.880
In the tech blog, I often look at the 20 metatrends that are going to transform the

01:41:34.880 --> 01:41:40.120
decade ahead and share the conversations I've had with incredible tech thought leaders on

01:41:40.120 --> 01:41:42.040
how they're transforming industries.

01:41:42.040 --> 01:41:44.600
If that sounds cool to you and you want to try it, join me.

01:41:44.600 --> 01:41:50.160
Go to dmandus.com backslash blog, enter your email and let's start this weekly conversation.

01:41:50.160 --> 01:41:55.280
Let me share with you the incredible progress we're seeing in the world of technology and

01:41:55.280 --> 01:41:57.840
the positive impact it's having on our lives.

01:41:57.840 --> 01:42:02.240
Again, that's dmandus.com backslash blog.

01:42:02.240 --> 01:42:06.400
Looking forward to sharing my insights and incredible breakthroughs I'm seeing with you

01:42:06.400 --> 01:42:07.240
every single week.

