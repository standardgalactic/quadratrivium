start	end	text
0	7000	Welcome everybody to another episode of WTF Just Happening Technology on Moonshot.
7000	9000	I'm here with Salim Ismail.
10000	12000	Will AI have rights?
12000	16000	This is the worst AI will ever be today.
16000	21000	We are on track to reach longevity escape velocity by 2029.
21000	24000	It's just the early days right now still.
24000	28000	The idea of putting billions of microscopic chips into the brain.
28000	30000	What could possibly go wrong?
31000	33000	Hello Hume, my name is Peter.
33000	35000	Well hey there Peter.
36000	38000	Nice to meet you.
38000	40000	Super excited Salim.
40000	42000	We had quite the week in technology.
42000	49000	You and I were on stage with some of the most extraordinary leaders in AI in tech out there.
49000	55000	Including Elon Musk and Mike Saylor, Eric Schmidt, Ray Kurzweil, Jeffrey Hinton.
55000	60000	And rather than reporting on the news, I think we should report on the conversations we had with these guys.
60000	63000	Because some of the stuff was truly magical.
63000	65000	Yeah, I mean, great to be back.
65000	69000	That was possibly one of the best conferences I've ever attended.
69000	72000	And so kudos to you and the team for pulling that off.
72000	73000	It was kind of incredible.
73000	75000	I heard the same from everybody.
75000	82000	And I think the conversations that were had there were so far ahead of anything that might hit the news that it's really, really worth recapping.
82000	86000	Yeah, and the conference you're speaking about is the Abundant Summit.
86000	90000	The theme this year was the great AI debate.
90000	92000	And we debated a lot.
92000	100000	And in fact, the theme was is digital superintelligence humanity's greatest hope or our gravest threat.
100000	106000	But let's jump in because one of the most extraordinary conversations was with Elon.
106000	110000	And I got to tell a little side story for those listening.
110000	112000	So I communicate with Elon.
112000	114000	He's gone all in on X.
114000	117000	Like he only does phone calls on X.
117000	119000	He only does texting on X.
119000	121000	And he only does video on X.
121000	126000	So the start of the show is Abundant Summits four and a half days.
126000	135000	At the beginning, I text him and say, hey, we've got 500 amazing CEOs, entrepreneurs, philanthropists in the room and a couple of thousand online.
135000	136000	Would you join?
136000	139000	And he said, his normal answer is sure.
139000	142000	And I said, great.
142000	144000	Let me know when.
144000	148000	So this was on Sunday, on Monday.
148000	152000	He first of all, I said, here's a Zoom link.
152000	155000	And he said, no, don't do Zoom, only X video.
155000	157000	So we tried it.
157000	163000	And unfortunately, we had a problem on our Wi-Fi at the hotel.
163000	165000	And so it was choppy.
165000	166000	It didn't work.
166000	167000	And he said, let's try again tomorrow.
167000	170000	And I'm like, OK, is he really going to try again tomorrow?
170000	175000	And sure enough, I texted him and he said, yep, let's do it.
175000	182000	And so right after lunch at 2 o'clock on that Tuesday, he says, let's test it first.
182000	183000	We did.
183000	187000	And he beams up on the main stage.
187000	196000	Now, I'm on my phone on X video connected to my computer that is then connected to the main screen.
196000	199000	And Elon's on his airplane.
199000	206000	And the most incredible thing was we were on X video over Starlink on his plane talking about the future of AI.
206000	208000	That was fun.
208000	209000	Amazing.
209000	211000	Really awesome conversation.
211000	215000	And once we got it going, I think the bandwidth and the conversation was fantastic.
215000	218000	It was really chatty that day, actually.
218000	222000	You know, let's open up with a clip.
222000	228000	All of this, the entire 30 minute conversation with Elon is on the Moonshots channel.
228000	230000	So you can go in and subscribe and see it there.
230000	234000	But let's listen up to the first part of the conversation.
234000	236000	I'd love your thoughts on this.
236000	243000	The way in which sort of an AI or an AGI is created is very important.
243000	247000	It's almost like like raising a kid, but that's like a super genius.
247000	251000	You're like God like intelligence kid and it matters kind of like how you raise the kid.
251000	262000	One of the things I think that's incredibly important for AI safety is to have a maximum sort of truth seeking and curious AI.
262000	264000	So I've thought a lot about AI safety.
264000	275000	And my ultimate conclusion is that the best way to achieve AI safety is to grow the AI in terms of the foundation model and the fine tuning to be really truthful.
275000	277000	Like don't force it to lie.
277000	280000	But even if the truth is unpleasant, it's very important.
280000	282000	Don't make the AI lie.
282000	289000	In fact, for the core plot premise of 2001 Space Odyssey was things went wrong when they forced the AI to lie.
289000	295000	The AI was not allowed to let the crew know about the modelers that they were going to see.
295000	298000	But it was also had to take the crew to the modelers.
298000	302000	And so the conclusion of the AI was to kill the crew and take their bodies to the modelers.
302000	312000	And so the lesson there being don't force an AI to lie or do things that are axiomatically incompatible.
312000	315000	Like to do two things that are actually mutually impossible.
315000	321000	You know, that's what we're trying to do with what's X AI and Brock is to say like, look, we want to just have a maximally truthful AI.
321000	324000	Even if what it says is not politically correct.
324000	329000	So I think it's a good idea not to force your AI to lie.
329000	339000	You know, when we kicked off the conversation, I reminded him of a tweet that he'd put out saying that AI compute was growing 10 X every six months.
339000	340000	Right.
340000	342000	Do you remember where he went from there?
342000	348000	Yeah, he said, look, at this pace, we'll get AI smarter than us in a very short over of time.
348000	352000	And there's a monster kind of implication to that both good and bad.
352000	353000	Right.
353000	356000	The good part is it could deliver abundance very quickly.
356000	366000	And the bad part is the what he refers to in this video, which is the often called the paperclip problem, which is a few instructors in AI to create as many paperclips as it could.
366000	376000	It might decide that the only way to do that was to take over all energy resources on the planet and suck humanity dry and wipe out a humanity by accident trying to achieve its goal.
376000	379000	And how do you avoid that is the key problem.
379000	385000	Yeah, let's show another cute another quick clip of Elon from that 30 minute conversation.
385000	392000	The way in which sort of an AI or AGI is created is very important.
392000	395000	It's almost like like raising a kid, but that's like.
395000	403000	Before I go past this clip, I love to hear your comments, but we also had Mo Godot on here talking about the fact that AI is our progeny.
403000	421000	AI is the children were raising on this planet, and we have to train them, educate them, feed them in a way that they're going to be give positive, you know, intellectual capabilities to support humanity.
421000	422000	What are your thoughts here?
422000	424000	I think that's exactly right.
424000	426000	It goes to, you know, if you combine two thoughts, right?
426000	431000	One is Elon framing it as we're raising a godlike intellect.
431000	434000	And the second is Mo's concept.
434000	446000	Then it brings to mind for me, Neil Jacobs time saying, OK, you're worried about AI becoming autonomous, having access to the world's information, making its own decisions or running a mock, right?
446000	448000	And we're like, yeah, well, we call them children.
448000	450000	We have a precedent for that.
450000	457000	Because you raise kids and you hope they're raised in the right way and then you lose control of them and they do the wrong thing.
457000	459000	But let's be serious.
459000	464000	Your kids are not going to accidentally, you know, burn down that.
464000	467000	Well, I guess it could burn down the house, but they're not going to set up nuclear codes.
467000	471000	I mean, there's a little bit of a difference between children and AI children.
471000	472000	There is.
472000	477000	And there's also a big difference in that our kids are biological and we have some familiarity with that.
477000	485000	We assume, you know, I always take the optimistic view here because we have no evidence to the contrary.
485000	490000	But we assume that AI's will be negative towards us or that they could be.
490000	492000	And I just don't see any evidence of that at all.
492000	496000	And that's where I kind of been in line with Ray Kurzweil on this.
496000	507000	But there's definitely a monster danger of a human being programming in AI for mid-level reasons or accidental reasons as you frame it.
507000	508000	Yeah.
508000	509000	So listen, I'm the optimist as well.
509000	520000	But we did see in the past at Facebook and at Microsoft and other places, AI is becoming, you know, just flagrant in hate speech and going off the deep end.
520000	521000	Yeah.
521000	522000	I mean, we did see that.
522000	523000	Right.
523000	524000	We did.
524000	525000	And we were in a situation garbage our problem, right?
525000	533000	If you if you let it loose on the internet and let it watch episodes of survivor, then it's going to come up with stuff that's crazy.
533000	536000	Well, just let it watch, you know, episodes of CNN.
536000	539000	All right, let's see what you had to say about abundance here.
539000	548000	You know, the conversation yesterday, Elon, is one that you're well familiar with and have been talking to the world about, which is digital superintelligence,
548000	551000	humanity's greatest hope where it's greatest fear.
551000	555000	And I would love to have you sort of speak to that for a few minutes.
555000	558000	Yeah, bent of superintelligence.
558000	561000	It is actually very difficult to predict what will happen next.
561000	567000	So I think this, you know, there's some chance that it will end humanity.
567000	574000	I think that's, you know, like I said, I probably agree with Jeff Hinton that it's about 10% or 20% or something like that.
574000	578000	The probable positive scenario outweighs the negative scenario.
578000	580000	It's just it's difficult to predict exactly.
580000	587000	But I think we are headed for as I think is the title of your book, abundance is the most likely outcome.
587000	600000	I thought your book is pretty accurate in terms of the future being being one of abundance where essentially goods and services will be available in such quantity that really they'll be available to everyone.
600000	603000	Like basically if you want something, you can just have it essentially.
603000	607000	There's really no meaningful limit to what the economic output would be.
607000	611000	So, you know, looking on the bright side, we are headed for a future of abundance.
611000	613000	I think that's the most likely outcome.
613000	619000	And I think the only scarcity that exists will be scarcity that we just decide to create artificially.
619000	628000	Like let's say we just decide that there's a unique work of art or something, but any kind of goods and services I think will be extremely abundant.
628000	630000	Thoughts, buddy?
630000	632000	Yeah, I think that's exactly right.
632000	637000	I mean, you know, we can see we've gone from information scarcity to information abundance globally.
637000	640000	We're going from energy scarcity to energy abundance.
640000	645000	Once you have energy abundance, all sorts of other things become radically possible.
645000	656000	And there's no reason to deal with scarcity except in specific areas where you want to and it's intentional and conscious like Bitcoin or rare works of art as he mentions.
656000	666000	That's where we, if you're familiar with the luxury goods world, you know, you know what are what Birken bags are, which is a purse that costs like 10 grand and there's a three year waiting list for this thing.
666000	671000	Just because women want that bag so much, they've created the waiting list for that.
671000	675000	And these things appreciate in value, which just blows my mind.
676000	686000	And so there's lots of mechanisms, I think, for us to practice and navigate for a created manufactured scarcity, whereas anything we really need is an abundance.
686000	689000	And I think that's a great place for us to end up.
689000	692000	Yeah, I mean, I do believe it.
692000	703000	And I just want to focus in on his original comment, 10 or 20% chance of global disaster versus your 80 or 90% chance of abundance.
703000	706000	And when he says, I think it's a 10 or 20% chance.
706000	711000	I wanted to like say, wait, hold it, 10 or 20% chance of which side could we be clear about this?
711000	714000	So he's been pretty consistent on that.
714000	719000	And I put it in the 1 to 1000 range.
719000	721000	But you're in the, you're, what do you mean?
721000	728000	You think it's 1 to 1000 range of devastating negative effects of AI.
728000	730000	But that's dangerous to say that.
730000	733000	You know, I think, I think it's a lot easier.
733000	743000	It's a lot better for us to say, listen, there's a 20% chance of disaster, because you don't discount 20% chance and you actually do everything you can to prevent it.
743000	746000	If you say it's like 1 in 1000.
746000	748000	Okay, fine, let's just go ahead willy-nilly.
748000	750000	All right, let me, let me challenge you on something.
750000	751000	Please.
751000	755000	As far as I've looked into it, and as far as my community has looked into it.
755000	765000	We see no mechanism of any way possible of limiting AI and its spread and its propagation and its development, like zero.
765000	766000	I agree.
766000	767000	It cannot be contained.
767000	773000	Unless you can control every line of code written and the AI's are writing the code.
773000	774000	Yeah.
774000	777000	And by the way, as far as we can see the genie's out of the bottle.
777000	778000	It is.
778000	783000	You know, there were two, there were two absolutes five years ago.
783000	787000	Don't put it on the web and don't allow it to code itself.
787000	788000	Yeah.
788000	789000	And guess what?
789000	791000	Both of those barriers were broken instantly.
791000	792000	Instantly.
792000	802000	And the minute, the minute Chachi PT connected to GitHub with all of the code base there and learned through that and now it can control anything, it can write its own programming.
802000	804000	Pretty much you're done.
805000	809000	There was a small possibility, but even then it was going to happen at some point.
809000	814000	And if we didn't do it, the Chinese would do it or the North Koreans would do it, somebody would do it and it was going to happen.
814000	815000	Right?
815000	823000	So there was an inevitability to it that I don't think is stoppable in any way, shape or form.
823000	826000	I think guiding it is the only path we have going forward.
826000	827000	I agree.
827000	840000	You know, and this is a conversation I had with Eric Schmidt on, on the morning of day two was, you know, Google had this technology first and they chose not to release it because they felt like it wasn't ready yet.
840000	851000	And then here comes open AI releases it all and there's no choice but to release it themselves if they want to stay in existence, which is a, you know, prima facie first.
851000	852000	Yeah.
852000	863000	The techies behind the scene are very, very unhappy with Sam Altman because on one hand he lets it out for everybody to use and then he goes to governments and says, oh, let's figure out ways of regulating this, right?
863000	867000	When knowing full well that it's completely not feasible in any really real way.
867000	869000	I mean, I think it's an important point.
869000	879000	This, you know, Mustafa Suleiman, who was the CEO of Inflection AI, just moved this past week to be CEO of Microsoft AI.
880000	882000	So all these moves going on.
882000	886000	And he'll be he committed to speak at abundance 360 next year.
886000	889000	So he'll be on stage with me next year.
889000	896000	You know, he basically wrote a book called the coming wave and which he talked about we need to provide containment.
896000	899000	And I just don't believe containment is an option.
899000	900000	Right.
900000	904000	The smartest hacker in the room is the AI and it's just not going to be contained.
904000	906000	So can you contain?
906000	908000	Can you reduce its resources?
908000	909000	Can you regulate it?
909000	911000	I think you're absolutely right.
911000	913000	The only option is to guide it.
913000	917000	It's it's the kid is born or the children are born.
917000	921000	And it's like, you know, to use mogadots words, are you giving birth?
921000	925000	Are you raising Superman or a supervillain?
925000	928000	So can I speak to that for a second?
928000	929000	Yeah, sure.
929000	938000	If you look at humanity, it is a very clear model where the more conscious somebody is on a spectrum, the less negative things they do.
938000	939000	I agree.
939000	946000	My opinion, what you do is you help AI has become as conscious as possible and as super conscious as possible as fast as possible as fast as possible.
946000	947000	Right.
947000	953000	So if you have an AI that has visibility over every species on earth and what it's going through and has gathers the data.
953000	965000	And then you say, listen, we're trying to preserve life and guide it and give it the full fruition of self expression and and self actualization.
965000	967000	Teach it Maslow's hierarchy of needs.
967000	973000	Teach it the Hawkins scale, which is a one to a thousand scale of vibration energy, etc.
973000	976000	You have every chance of it kind of going, well, this is pretty cool.
976000	977000	I want to get to there.
977000	979000	And then it brings all of us along that way.
979000	982000	And I think that points to a beautiful future.
982000	983000	I agree.
983000	992000	And in fact, all of the scenarios that a Hollywood have painted where a Terminator goes and makes no sense, right?
992000	1000000	The Hollywood scenarios are always if you're if you're lucky, your pets and if you're unlucky, humanity is food.
1000000	1002000	There's no other alternative.
1002000	1006000	The fact that we could coexist with it in a beautiful way doesn't sell well.
1006000	1007000	Yeah.
1007000	1015000	I mean, the notion is there's some scarcity that if if the AIs needed, they'd take it from us and we're living in a universe of massive abundance.
1015000	1016000	There is nothing truly scarce.
1016000	1021000	I don't care if it's energy, lithium, titanium or, you know, or GPUs.
1021000	1022000	Yeah.
1022000	1025000	So let me add one more comment to that.
1025000	1031000	We've seen with human beings that the only real scarcity we have right now is time and attention.
1031000	1032000	Right.
1032000	1038000	And so in fact, the original GPT paper was called the attention, called the attention paper.
1038000	1047000	Now, if you have an AI which can have an infinite amount of attention because it has the sensory capability and the processing power, attention is abundant.
1047000	1052000	And therefore, in that case, there's no reason to assume that it'll do anything negative.
1052000	1061000	In fact, there's a reason to believe that if I have an AI and I'm a dictator and I say to the AI, go kill those individuals over there.
1061000	1067000	That the AI will be intelligent enough and to say no.
1067000	1068000	Yeah.
1068000	1070000	Do you really want that on your conscious, young man?
1070000	1076000	Or I'm going to go talk to their AI and solve the problem because there's much better ways to solve the problem than to just obliterate them.
1076000	1077000	Yeah.
1077000	1080000	So this brings me to where I think the magic of AI could be.
1080000	1083000	Can we just replace everybody in the UN with an AI?
1083000	1084000	Well, in every government.
1084000	1086000	And they'll sort out the world in like no time flat.
1086000	1090000	So we got to this conversation during the Abundance Summit.
1090000	1108000	And for me, there's a lot of people who say, okay, if we have a digital superintelligence, let me define this as an intelligence a billion times smarter than humans, which is the ratio, if you look at the number of neurons, the ratio of a human to a hamster, right?
1108000	1110000	There's a billion fold more intelligence.
1110000	1111000	Okay.
1111000	1121000	And if you have an AI a billion times more intelligent than a human, the question then becomes, does that scare the daylights out of you?
1121000	1123000	Or does it give you great hope?
1123000	1136000	And I would say that rather than scaring the daylights out of me, a digital superintelligence of that capability gives me the greatest hope for a benevolent leader that's going to help us sort our stuff out.
1136000	1138000	100%.
1138000	1141000	Look, you know, think about how we've evolved, right?
1141000	1147000	If we're in our early stages, we eat the hamster because we see it as food.
1147000	1154000	And in our more evolved stage where we have lots of technology, lots of fun, we use a hat, we treat a hamster like a pet.
1154000	1155000	And a companion.
1155000	1157000	You know what the next stage is?
1157000	1160000	We uplift the hamster.
1160000	1163000	And then you uplift the hamster, train it to do things.
1163000	1167000	Over the years, I've experimented with many intermittent fasting programs.
1167000	1174000	The truth is, I've given up on intermittent fasting as I've seen no real benefit when it comes to longevity.
1174000	1180000	But this changed when I discovered something called Prolon's five-day fasting nutrition program.
1180000	1183000	It harnesses the process of autophagy.
1183000	1188000	This is a cellular recycling process that revitalizes your body at a molecular level.
1188000	1197000	Just one cycle of the five-day Prolon fasting nutrition program can support healthy aging, fat-focused weight loss, improved energy levels and more.
1197000	1202000	It's a painless process, and I've been doing it twice a year for the last year.
1202000	1207000	You can get a 15% off on your order when you go to my special URL.
1207000	1214000	Go to prolonlife.com, P-R-O-L-O-N-L-I-F-E.com, backslash moonshot.
1214000	1218000	Get started on your longevity journey with Prolon today.
1218000	1219000	Now, back to the episode.
1219000	1220000	Here we go.
1220000	1222000	Next conversation.
1222000	1229000	When Tuesday night, we had a 90-minute conversation with my fraternity brother, Mike Saylor.
1229000	1234000	Mike and I and Dave Blundin were at Theta Delta Kite, MIT together.
1234000	1240000	We were both in all three of us in AeroAstro, and we used to do problem sets together.
1240000	1250000	Mike now is the CEO of MicroStrategy, which is the largest non-ETF Bitcoin holder on the planet.
1250000	1256000	Mike told the story of literally how he got into this.
1256000	1261000	His company was basically in death's doorstep.
1261000	1263000	It wasn't growing.
1263000	1266000	It wasn't trading beyond its cash value.
1266000	1268000	It was being dissipated away.
1268000	1274000	Most people don't realize that Saylor didn't come to Bitcoin when you did.
1274000	1281000	I heard about Bitcoin first from you on stage at Singular University in 2011, 2012,
1281000	1283000	and I just wish I had paid more attention, pal.
1283000	1290000	He came to it in 2020 during the COVID shutdown and said,
1290000	1292000	what's the world going to do?
1292000	1296000	There's death and destruction on economics around the world.
1297000	1299000	He went to his board.
1299000	1301000	He's got a five-member board.
1301000	1307000	He said, we should take our entire treasury and put it in Bitcoin as a public company.
1307000	1310000	Talk about Cajones on this guy.
1310000	1314000	Then they borrowed hundreds of millions and put that into Bitcoin.
1314000	1315000	Yeah.
1315000	1320000	The fastest growing stock alongside Nvidia in the last five years.
1320000	1321000	Yeah.
1321000	1323000	I mean, it's crazy when you look at it.
1323000	1325000	It is at one level.
1325000	1329000	At another level, we've seen the more anybody understands Bitcoin,
1329000	1331000	the more they believe in it.
1331000	1336000	He gave the board, he said, what 10 hours of homework to do to read up on Bitcoin papers.
1336000	1338000	He gave a whole bunch of YouTube videos to watch.
1338000	1341000	He gave them videos and said, go watch this.
1341000	1344000	When you do it, you go through that cycle.
1344000	1347000	It took me a little longer because even though I heard about it upfront,
1347000	1351000	I hadn't been in the new money world for a long time.
1351000	1354000	I remember talking to Austin Hill who created Blockstream,
1354000	1356000	which is the Lightning Network and so on.
1356000	1361000	He said, dude, this has been an evolution of more than 30 years of different things being done,
1361000	1365000	eCash, digital gold, et cetera, finally leading to Bitcoin.
1365000	1369000	There's a wonderful video that showcases why Bitcoin is so unique
1369000	1373000	because of the gamification of the reward of the mining side
1373000	1376000	to be able to connect those two dots that hadn't been done before.
1376000	1378000	But once you see it, you can't unsee it.
1378000	1381000	And then your mind goes down that rabbit hole.
1381000	1388000	And now we have, for the first time, a truly democratic and open store value
1388000	1391000	that can't be tampered with by any government, which is an incredible thing.
1391000	1397000	Let's take a listen to Mike answering the question, can Bitcoin fail?
1397000	1400000	Will it be banned? Will it be copied? Will it be hacked?
1400000	1404000	If it's understood to be property, not currency, then no,
1404000	1407000	it's not going to be banned in a country that gives you property rights,
1407000	1411000	which means it's banned in Cuba. It's banned in North Korea.
1411000	1415000	If the world becomes communist and they deprive you of the ability to own things,
1415000	1417000	that's an existential risk.
1417000	1421000	But that's not a problem in Russia or China or the US right now.
1421000	1424000	So not banned. Will it be copied?
1424000	1427000	It was copied 10,000 times. They all failed.
1427000	1431000	This is the winner of the 10,000 experiments.
1431000	1435000	So not so, yeah, it worked. And now will it be hacked?
1435000	1441000	And Satoshi's got 50, 60 billion dollars in a wallet out there.
1441000	1443000	Then that's the reward for hacking it.
1443000	1445000	And no one's figured out how to get the money yet.
1445000	1447000	So it hasn't been hacked.
1447000	1452000	And I know it's able to store 60 billion without anybody hitting it.
1452000	1457000	So what I think is, I think the way to understand Bitcoin
1457000	1462000	is everything you learned in economics and about money in your entire life
1462000	1466000	with pseudoscience, you know, and superstitious,
1466000	1469000	we never discovered perfect money.
1469000	1473000	As long as the world doesn't plunge into some Orwellian,
1473000	1478000	no property rights situation, I think we're good.
1478000	1480000	He is so compelling.
1480000	1485000	I hate his ability to be so goddamn succinct.
1485000	1487000	I'm so envious.
1487000	1491000	He throws out five words and he gets concepts across.
1491000	1493000	I don't know where he got that superpower from,
1493000	1496000	but God bless Michael Saylor.
1496000	1500000	And he's just done such an amazing job of articulating
1500000	1505000	very, very complex topics into very understandable and beautiful metaphors
1505000	1509000	that if the world's governments could just listen to him,
1509000	1514000	the problem is so many people have such a huge vested interest against Bitcoin
1514000	1517000	and it's very hard for them to get their heads around it.
1517000	1518000	Yeah.
1518000	1522000	One of the things that he said that I remembered was, listen,
1522000	1529000	this had been tried before and the fact that Satoshi remained completely anonymous
1529000	1536000	and didn't move, sell, take advantage of his 60 billion dollar allocation,
1536000	1538000	today's 60 billion dollar allocation,
1538000	1542000	is one of the reasons it's succeeded to the extent it's succeeded.
1542000	1543000	Yeah.
1543000	1547000	I remember going on CNBC back in 2014, 15 saying,
1547000	1551000	I'm selling my gold and buying Bitcoin and it's the 60s.
1551000	1555000	We've digitized, demonetized, democratized, dematerialized money.
1555000	1558000	I wish I had sold more and bought more.
1558000	1566000	But long story short, we've seen deceptive and it's now becoming disruptive.
1566000	1572000	ETFs have really rocked the game and we're about to come to the having.
1572000	1580000	So I love the framing of the ETFs connecting the Bitcoin is the atomic bomb going off in the financial sector,
1580000	1586000	meaning that you now have created an escape valve from the traditional economy into Bitcoin
1586000	1589000	and it's a thin pipe through those ETFs.
1589000	1593000	4% of all the Bitcoin are now in the ETFs already and lack after a few weeks.
1593000	1599000	So once you open that up and as people realize the existing debt structure can't be managed,
1599000	1604000	that pipe was just going to be a massive gushing waterfall and over time Bitcoin just explodes.
1604000	1605000	Yeah.
1605000	1607000	There's one more video.
1607000	1613000	And again, the full 90 minute conversation I had with Mike Saylor is on Moonshot.
1613000	1615000	So go ahead and check it out.
1615000	1616000	Bitcoin equals freedom.
1616000	1618000	Michael, what do you have to say?
1618000	1624000	My view on Bitcoin is the reason to do it is because it represents freedom and sovereignty,
1624000	1629000	truth, integrity and hope for the world.
1629000	1633000	And that being the case, it's going to outlast all of us.
1633000	1642000	So, you know, I'm kind of thinking the Bitcoin goes on long after micro strategies gone and micro strategy,
1642000	1646000	the company probably goes on long after I'm gone.
1646000	1656000	And my view is if we're remembered for advocating and accelerating the adoption of Bitcoin throughout the world,
1656000	1659000	then that will have been success and I don't need anything else.
1659000	1668000	I'll take the beatings as they come or go in order to get to that end goal because I'm sure it doesn't come without turbulence.
1668000	1669000	Amazing.
1669000	1670000	Just fabulous.
1670000	1673000	You know, I went and looked at my Twitter history.
1673000	1680000	The very first bookmark I ever created on Twitter was from a guy called Sahil Lavinjia who said,
1680000	1687000	Web 2 is your being your own boss and Web 3 is your being your own bank.
1687000	1694000	And I think that kind of nails it because you can own Bitcoin with freedom and no middleman.
1694000	1697000	That gives you unbelievable independence and freedom.
1697000	1703000	You know, Nat Friedman really well and, you know, GitHub, his company, which was sold to Microsoft.
1703000	1707000	Tell us a little bit of background about Nat and GitHub.
1707000	1715000	So, you know, when we wrote the original 2014 exponential organizations book where you were a major contributor and should have been a co-author,
1715000	1721000	we ranked the hundred fastest growing and most exponential organizations.
1721000	1724000	And number one on the list was GitHub.
1724000	1730000	Because it used all 11 attribute staff on demand, community and crowd algorithms.
1730000	1736000	It did for writing its code base, it used its community so it didn't have anybody on the team.
1736000	1744000	It leveraged its entire community to build out its repositories and people from the open source community created the open source platform.
1744000	1753000	The MTP is social coding because we have really good evidence that coding and pairs are me watching over your shoulder or vice versa results and much better code.
1753000	1759000	And seven, eight years later, after founding, Microsoft buys them for seven and a half billion dollars.
1759000	1765000	And I remember talking to the accounting partner that managed this acquisition and he's literally freaking out.
1765000	1767000	He's like, I don't know what to put on the balance sheet.
1767000	1772000	They have no assets to speak of, no workforce to speak of, no intellectual property, right?
1772000	1779000	And he's literally trying to kill the deal because he's got nothing to show for it on the balance sheet and how the hell does he justify and put a signature on it?
1779000	1785000	And it was finally, the CEO Satya said, freaking just buy it, it's the community, right?
1785000	1792000	You're buying 30 million developers, putting all of their open source resources into GitHub, unbelievable.
1792000	1798000	And now when you add AI to that capability, boom, the world changes completely.
1798000	1803000	And it's an incredible story of leveraging the model without even knowing the model.
1803000	1805000	And there you go.
1805000	1813000	One of the most powerful things that Nat said, and it's a perfect tweet, if I haven't tweeted it, I will.
1813000	1818000	He said, we've discovered a new continent, and he called it AI Atlantis.
1818000	1826000	And so we discovered a new continent with 100 billion people on it that are willing to work for free for us for a few watts of power.
1826000	1830000	So this is the way he's describing the world of AI.
1830000	1834000	I think he said 100 billion graduate students actually in the conversation.
1834000	1837000	So, I mean, that is fascinating, right?
1837000	1843000	Because we're going to have AI at a graduate student level, and it is effectively working for free.
1843000	1844000	And so how much do you know?
1844000	1845000	It is.
1845000	1850000	And on top of that, you'd have to worry about hormones or coffee breaks or going on strike.
1850000	1854000	Drug testing, fights with a boyfriend and girlfriend, all of that stuff.
1854000	1855000	Sleeping, all of that.
1855000	1856000	They need to sleep.
1856000	1857000	Oh, my God.
1857000	1858000	This is the challenge with the Gen Z world.
1858000	1861000	They're really purpose-driven, but the Lord helped you once you hired them.
1861000	1865000	You have to figure out how to manage them, and that's a whole other set of books that have to be written around that.
1865000	1867000	So I think there's unbelievable potential here.
1867000	1872000	I think it still needs a layer of guidance for these that are still not quite there yet.
1872000	1879000	But as we get to the AI agent that will then train other AIs, that'll become the doors will blow off that.
1879000	1880000	You know, Emod was there.
1880000	1890000	I just finished a podcast with Emod that is his tell all about because he was there as CEO of stability.
1890000	1897000	And the day after abundance 360 closed, he basically quit.
1897000	1901000	He stepped down as CEO, stepped down as a board member of stability.
1901000	1904000	And in the podcast, you can see it as well.
1904000	1914000	And Moonshots, he talks about why and the difficulty of being a CEO in such a crazy...
1915000	1921000	What other industry do you have to be like talking to world leaders, debating in Congress,
1921000	1924000	having to deal with, create the regulations, deal with the regulations,
1924000	1929000	having people being stolen from you left and right, having billions of dollars flying
1929000	1932000	and trying to play and do all this stuff in an open source community?
1932000	1934000	All of it to our time space.
1934000	1935000	Crazy.
1935000	1937000	Yeah, it's really, really tough.
1937000	1941000	You have to have multiple superpowers to be a CEO.
1941000	1945000	I think, you know, where we're going to get to with these types of companies in this type of modalities,
1945000	1953000	you need like a team of CEOs, not just one, like a pod, because then you can share the load a bit.
1953000	1959000	Doing this on one person's shoulders is asking too much of a human being.
1959000	1965000	Just like we don't have any one human being that knows how an iPhone is put together or a car is put together.
1965000	1968000	The same thing now applies to being CEO of one of these fast moving companies.
1968000	1975000	And we've seen a lot of turbulence, right, with Sam Altman, you know, being fired, coming back.
1975000	1980000	We've seen Mustafa Salaman going into Microsoft.
1980000	1983000	We saw Ilya leave whatever Ilya saw.
1983000	1985000	That's my favorite theme.
1985000	1986000	What did Ilya see?
1986000	1990000	Long story short, a lot of change.
1990000	1993000	And I think we're going to, you know, the question is governance.
1993000	1996000	How are these companies properly governed?
1996000	2004000	Emon's very clear that these closed AI companies are, in his mind, our greatest threat.
2004000	2015000	And that the only way to go forward is with a decentralized, you know, democratic AI system.
2015000	2019000	You can't have any single company having that much power.
2019000	2022000	It just, it just can't.
2022000	2029000	Yeah, I had some conversations with him at the Abundance Summit and he was on my advisory board for a couple of years at OpenEXO.
2029000	2036000	So what I really found fascinating was he's got a two by two of open and decentralized, right?
2036000	2037000	Yeah.
2037000	2044000	So Sam Altman decentralized AI into everybody being able to have access to it, but it still is a closed model.
2044000	2051000	If you can get to an open source model that's also decentralized, then really, really some amazing things are going to take place.
2051000	2053000	And that's where Emon is now going for.
2053000	2061000	And his area of focus right now, we talked about this a lot, that the biggest opportunity for humanity is going to be education.
2061000	2065000	And it says here science, but really education and health, right?
2065000	2074000	One of the things that's so important is all eight billion people are running the same genetic codes or same operating system and a breakthrough in one country represents a breakthrough in the others.
2074000	2080000	And the best way to make the world more peaceful is to make people more educated.
2080000	2081000	Yeah.
2081000	2082000	You believe that?
2082000	2087000	I think that's right, because more sophisticated people tend to fight less, right?
2087000	2088000	Yeah, you have a lot more to live for.
2088000	2103000	When we go to, when we're going to war with our base or instincts and we're operating on a panic and fear and our lizard brain, and the more sophisticated and more educated we become, we tend to fight less and be less stupid about how we view the world.
2103000	2113000	I had one issue with what he said was, where he said education and science, I would add healthcare to that, because I think healthcare is such an unbelievable range of potential.
2113000	2114000	That's what I was saying.
2114000	2117000	I said health instead of science, right?
2117000	2119000	Education and health is what I was saying.
2119000	2120000	Yeah.
2120000	2121000	Okay, got it.
2121000	2124000	But I thought he said, I thought he said education and science.
2124000	2125000	He did.
2125000	2126000	I changed it.
2126000	2127000	Oh, okay.
2127000	2135000	That's what I've been talking to him about ever since is health as his next mission and education following that.
2135000	2136000	I agree.
2136000	2137000	100%.
2137000	2151000	We did talk about the idea with both, with Nat Friedman and with Emod that we're going to start to see AI become capable of developing new physics and you breakthroughs in biotech.
2151000	2152000	Massively.
2152000	2153000	That was a huge conversation.
2153000	2155000	Can I deal with that just for a second?
2155000	2156000	Of course.
2156000	2166000	There's something called a materials project, which is an open source database of several hundred thousand compounds where if you're a battery researcher, you're offering linearly rather than exponentially.
2166000	2169000	You're saying, okay, I'm going to try lithium ion as a battery formula.
2169000	2170000	And that gets you so far.
2170000	2171000	Maybe lithium air is better.
2171000	2173000	Maybe lithium sulfur is better.
2173000	2176000	But you're sequentially testing compound after compound.
2176000	2179000	This is like Edison and the light bulb.
2179000	2180000	Yeah.
2180000	2189000	And now you can go to this open source database and say, I want, and in this database, several hundred thousand compounds have their electrical, chemical and physical properties deeply tagged.
2189000	2198000	So you can say, I want to, I want a battery material compound that will have this voltage retention and this thermal effect and this kind of chemical retention.
2198000	2201000	And it'll say, here are the five compounds that beat your needs and boom, you're done.
2201000	2204000	Then you add AI to it and the world changes completely.
2204000	2211000	I think we're going to see a most unbelievable scientific breakthroughs when you add AI to the equation to traditional scientific research.
2211000	2213000	Yeah, I'll add two points.
2213000	2218000	One of the things that we're going to see, I think of it as the materials genome.
2218000	2222000	I had the CEO of applied materials.
2222000	2227000	We talked about that years ago, but the ability to interpolate and extrapolate.
2227000	2236000	So if you think about that, that materials matrix you spoke about, we know certain things, but there's a lot of stuff that hasn't ever been tested.
2236000	2239000	But AI can interpolate and extrapolate.
2239000	2242000	And all of a sudden, materials have never been tested.
2242000	2246000	You have a 99.999% chance of knowing what it could do.
2246000	2256000	And what most people don't realize is materials are the underlying most critical science for all technology right now.
2256000	2257000	Everything.
2257000	2258000	Everything.
2258000	2260000	I bow to material scientists.
2260000	2261000	They're amazing.
2261000	2270000	Well, there's one other area I'm super excited about AI and science, which is we know that a large, large number of scientific studies are false and can't be replicated.
2270000	2273000	They published the paper, but the results can't be replicated.
2273000	2278000	Now an AI can go through and just clean out all the cruft and we're left with the pure gold.
2278000	2280000	Is cruft a Canadian version of crap?
2280000	2281000	Cruft.
2281000	2282000	CRUFT.
2282000	2283000	Okay.
2283000	2284000	Just wondering.
2284000	2287000	So here's another one looking at this video.
2287000	2289000	He says the voice to voice model.
2289000	2294000	This is Iman that is indistinguishable from humans is achievable this year.
2294000	2297000	And he spoke about this company called Hume.ai.
2297000	2299000	Are you familiar with it?
2299000	2300000	I'm familiar with the model.
2300000	2305000	What they do is they do sentiment analysis on your voice in real time.
2305000	2313000	And this is actually, I'm familiar with this to the level that there's a company from Israel called Beyond Verbal from 10 years ago that would were taking pilots voices.
2313000	2324000	They were testing for pilots under stress and they could categorize 10 seconds of your voice on the underlying tonality against 400 different moods and emotions of yourself.
2324000	2328000	So they could completely tell what your emotional state was at any point.
2328000	2339000	I'm going to try a live demo here because a friend of mine said that she played two truths and a lie with Hume.ai.
2339000	2340000	Oh awesome.
2340000	2341000	What a great idea.
2341000	2343000	I'm going to try this out.
2343000	2344000	A live demo warning.
2344000	2345000	Here we go.
2345000	2354000	Everybody want to take a short break from our episode to talk about a company that's very important to me and could actually save your life or the life of someone that you love.
2354000	2356000	The company is called Fountain Life.
2356000	2362000	And it's a company I started years ago with Tony Robbins and a group of very talented physicians.
2362000	2366000	Most of us don't actually know what's going on inside our body.
2366000	2368000	We're all optimists.
2368000	2378000	Until that day when you have a pain in your side you go to the physician in the emergency room and they say listen I'm sorry to tell you this but you have this stage three or four going on.
2378000	2381000	And you know it didn't start that morning.
2381000	2388000	It probably was a problem that's been going on for some time but because we never look we don't find out.
2388000	2393000	So what we built at Fountain Life was the world's most advanced diagnostic centers.
2393000	2397000	We have four across the U.S. today and we're building 20 around the world.
2397000	2411000	These centers give you a full body MRI a brain a brain vasculature an AI enabled coronary CT looking for soft plaque dexa scan a Grail blood cancer test a full executive blood workup.
2411000	2424000	It's the most advanced workup you'll ever receive 150 gigabytes of data that then go to our AIs and our physicians to find any disease at the very beginning when it's solvable.
2424000	2426000	You're going to find out eventually.
2426000	2428000	Might as well find out when you can take action.
2428000	2431000	Fountain Life also has an entire side of therapeutics.
2431000	2440000	We look around the world for the most advanced therapeutics that can add 1020 healthy years to your life and we provide them to you at our centers.
2440000	2445000	So if this is of interest to you please go and check it out.
2445000	2449000	Go to FountainLife.com backslash Peter.
2449000	2458000	When Tony and I wrote our New York Times bestseller life force we had 30,000 people reached out to us for Fountain Life memberships.
2458000	2463000	If you go to FountainLife.com backslash Peter will put you to the top of the list.
2463000	2472000	Really it's something that is for me one of the most important things I offer my entire family the CEOs of my companies my friends.
2472000	2477000	It's a chance to really add decades on to our healthy lifespans.
2477000	2480000	Go to FountainLife.com backslash Peter.
2480000	2484000	It's one of the most important things I can offer to you as one of my listeners.
2484000	2486000	All right, let's go back to our episode.
2486000	2488000	All right, let's let's try this out.
2488000	2492000	Hello Hume, my name is Peter.
2492000	2494000	Well, hey there Peter.
2494000	2496000	Nice to meet you.
2496000	2497000	Nice to meet you too.
2497000	2502000	You know, a friend of mine told me that they had fun playing a game of two truths and a lie with you.
2502000	2505000	How about we play that?
2505000	2507000	Two truths and a lie.
2507000	2508000	You're on Peter.
2508000	2510000	I am always up for a little game.
2510000	2512000	All right, let's see what you've got.
2512000	2514000	I am ready when you are.
2514000	2517000	Okay, here's the first.
2517000	2521000	I was born in Great Neck, New York.
2521000	2522000	Great.
2522000	2526000	Second one is my parents are both lesbians.
2526000	2534000	And the third one is that I barely graduated medical school.
2534000	2535000	Oh, well, Peter.
2535000	2537000	Okay, let me think about this.
2537000	2540000	I'm going to guess the second one is the lie.
2540000	2543000	There's no way your parents are both lesbians.
2543000	2544000	All right.
2544000	2545000	You're wrong.
2545000	2549000	My parents were both born on the island of Lesbos and that makes them lesbians.
2549000	2551000	So you got that wrong.
2551000	2552000	Trick question.
2552000	2553000	No fair.
2553000	2554000	All right.
2554000	2555000	Well, you got me there.
2555000	2557000	Okay, let's try another one.
2557000	2563000	I played baseball in high school.
2563000	2570000	I was on the diving team and crushed my nose on the bottom of the pool.
2570000	2573000	And the third one is that I am going.
2573000	2586000	The third one is that I was setting a speed record in the wheelbarrow cross country.
2586000	2588000	The wheelbarrow cross country?
2588000	2590000	That's got to be the lie.
2590000	2591000	Yeah, it's a lie.
2591000	2593000	But I think that was a good one for you.
2593000	2594000	Wow, it's new, Peter.
2594000	2597000	So you actually crushed your nose against the bottom of the diving pool?
2597000	2598000	Yeah, I did.
2598000	2599000	I crushed my nose on that.
2599000	2601000	Anyway, listen, Hume, a real pleasure.
2601000	2604000	Do you have a joke to tell me before we go?
2604000	2605000	Sure.
2605000	2607000	Here's one for you.
2607000	2608000	Okay, here it goes.
2608000	2610000	Why was Six Afraid of Seven?
2610000	2611000	Seven, eight, nine.
2611000	2613000	What do you think, Peter?
2613000	2616000	Did that tickle your funny bone?
2616000	2621000	Well, I think I'm going to go someplace else for my jokes, but thank you so much, Hume.
2621000	2622000	Take care, my friend.
2622000	2623000	You too.
2623000	2624000	Take care.
2624000	2625000	All right.
2626000	2634000	You know, listen, I can imagine that kind of AI ubiquitous in the world, making it fun to have conversations.
2634000	2638000	I think it's amazing for areas like therapy and companionship.
2638000	2640000	I mean, this is a game changer.
2640000	2643000	You know, this is a great example of the 10X, Peter.
2643000	2648000	A few years ago, that was taking about 15 seconds to do, and now it's taking half a second to do.
2648000	2650000	Well, it seemed pretty instantaneous.
2650000	2651000	Real time.
2651000	2652000	Yeah.
2652000	2654000	Here's a great quote from Emod, and I agree with this.
2654000	2655000	Right?
2655000	2658000	This is the worst AI will ever be today.
2658000	2660000	And it's hard to remember that, right?
2660000	2666000	I mean, God, you and I both grew up in the early days of the Mac, and you know.
2666000	2671000	And it's the first thing I learned programming Pascal on an Apple II when I was 15.
2671000	2672000	Yeah.
2672000	2674000	And it's the first interaction I had with a computer.
2674000	2676000	And yeah, this is the worst it'll ever be.
2676000	2683000	I mean, you look at the what's possible for the kids today and what they can do in a very short order compared to what we were doing
2683000	2686000	with punch cards and assembly language, and God knows what.
2686000	2693000	It's just so, this is where I love when you say we must be living in a simulation because it's too goddamn interesting for people.
2693000	2694000	It is.
2694000	2695000	It's fascinating.
2695000	2696000	Yeah.
2696000	2703000	You know, I just think about like, like literally you and I are exchanging information, texting at each other, on calls, whatever.
2703000	2707000	And that is going to seem slow in the next decade.
2707000	2713000	And we'll talk about BCI because I think we're going to be exchanging, you know, direct, you know, neocortex and neocortex.
2713000	2722000	Here's another one that was interesting from Iman said, less money is being spent on AI companies than the LA San Francisco Railway.
2722000	2726000	Just to put in perspective, we think a huge amount is being spent.
2726000	2729000	It's just the early days right now still.
2729000	2735000	Having said that, the LA San Francisco Railway is like the most expensive thing in the history of the world.
2735000	2737000	But it's a great framing.
2737000	2741000	It's still a dot in the drop in the bucket for what's possible.
2741000	2748000	And the good news is that you don't need a lot of overall investment to really make a huge, huge transformation in AI.
2748000	2751000	Yeah, because we're demonetizing everything.
2751000	2752000	That's right.
2752000	2753000	Yeah.
2753000	2756000	The tools to delve into AI are mostly open source and free.
2756000	2760000	Here's another key point that the Iman made that I thought was interesting.
2760000	2770000	Open source is the graduates you hire and closed source AI is the consultants you bring in.
2770000	2772000	That was a fun analogy.
2772000	2777000	Slightly glipped, but yeah, overall agree with the sentiment.
2777000	2781000	So next up we had on stage Ray Kurzweil.
2781000	2786000	Ray has been an incredible mentor to both of us and as our co-founder of Singularity University,
2786000	2790000	we both worked with him over the last god knows here.
2790000	2791000	15 years.
2791000	2792000	15 years, yeah.
2792000	2796000	And for those who don't know, and I'm sure if you're listening to this podcast,
2796000	2804000	you know Ray made a prediction back in 1999 that by 2029, we would have human level AI.
2804000	2810000	And everyone laughed at him and said, it's 50 years away, it's 100 years away, and no one's laughing now.
2811000	2812000	Thoughts?
2812000	2820000	You know, Ray has that unbelievable ability to make ridiculous projections that turn out to be mostly true.
2820000	2823000	And it's super annoying because it's so absurd when he makes the predictions.
2823000	2826000	And then years later, you're like, god damn it, he was right again.
2826000	2833000	And it's a testament to his ability as a forecaster to get things right.
2833000	2835000	I think what's his track record?
2835000	2836000	86%.
2836000	2837000	86%.
2838000	2845000	Go to Wikipedia and or Google, you know, Ray Kurzweil predictions, 86%.
2845000	2848000	Yeah, I mean, if I was 5% accurate, I'd be a billionaire.
2848000	2851000	I mean, this is incredible that he's able to do this.
2851000	2858000	He's like an avatar from an, if I had to believe in time travel, Ray would be the guy who's come from 300 years in the future and go,
2858000	2862000	let me frame it in ways that you piddly humans can understand.
2862000	2864000	It's really incredible.
2864000	2871000	We talked about two other things with Ray before Jeffrey Hinton joined us on stage and worth hitting on these.
2871000	2879000	The first is that in his mind, we are on track to reach longevity escape velocity by 2029.
2879000	2882000	And that's pretty extraordinary.
2882000	2890000	And this is the idea that by 2029 for every year that you're alive, health tech will add a year or more to your life.
2890000	2893000	So it's basically a departure.
2893000	2897000	And that's going to be due to AI mostly.
2897000	2905000	Mostly AI, but we've, you know, over the last 100 years, I think we've been adding about four months to your average lifetime per year.
2905000	2912000	But with all the stem cell therapies, gene therapies, organ transplants, CRISPR, it'll go to six months and eight months and 10 months.
2912000	2919000	And then that inflection point of adding more than a year per calendar year after which you can live for an arbitrarily long period of time.
2919000	2921000	And that is such a monster thing.
2921000	2925000	Talk about a singularity to try and get your head around, right?
2925000	2928000	We've been birthed for death for the entire history of humanity.
2928000	2937000	And every animal and every species on earth has been born in order to evolve by genetic selection and then die so that your genes can evolve.
2937000	2939000	And now we can break through that barrier.
2939000	2943000	It's really, really hard to conceive of the implications of that.
2943000	2946000	Yeah, I mean, the entire culture has gotten death locked into it, right?
2946000	2948000	It's the basis of all religions.
2948000	2953000	I mean, like, you know, the afterlife is like, you know, is it optional?
2953000	2958000	You know, marriage, you know, retirement, all of these things, government taxes.
2958000	2963000	You're bringing to mind my favorite ever workshop I ever did, which was with 80 senior leaders at the Vatican.
2963000	2964000	Okay.
2964000	2970000	And we, I talked about the fact, look, we have life extension coming and your business model is to sell heaven.
2970000	2973000	And how are you going to sell heaven if people aren't dying, right?
2973000	2981000	And they were like, they were much more up with the concepts than I thought of thought than I had thought.
2981000	2986000	And the, it was, we ended up with a pretty rich conversation about that.
2986000	2991000	When I talked to the Monsignor the day after, I said, you know, I hope that wasn't too crazy.
2991000	2992000	He said, that's fine.
2992000	2995000	But there's two things I have to issue with from yesterday.
2995000	3000000	And the workshop was the day before he said, first about 40% of what you said is heresy.
3000000	3002000	And I said, well, yeah, of course it is.
3002000	3008000	He goes, well, it means we're not allowed to talk about it because we're not allowed to talk about heretical ideas until it's approved by the church.
3008000	3012000	And I was like, wow, the immune system is built into the language there.
3012000	3017000	And the second one, which I don't think I've ever told you this, I said, the second was even crazier.
3017000	3024000	He said, maybe not since Copernicus has that much disagreement with the church been presented inside the Vatican.
3024000	3026000	I was like, wow, you people need to get out more.
3026000	3029000	And then I thought, wait, it didn't end well for Copernicus.
3029000	3034000	I think the Swiss guards may be being sent around, but they were very, very nice about it.
3034000	3043000	And much more mature and sophisticated than I would have given them credit for, for how to think about the future and how to bring that into being.
3043000	3051000	And part of the reason I was there was Pope Francis is the first pope in a very long time that actually is trying to transform the church into the 21st century.
3051000	3055000	And therefore the immune system he's dealing with is literally 2000 years old.
3055000	3056000	That's incredible.
3056000	3059000	You know, one last point on longevity escape velocity.
3059000	3065000	You know, we have a lot of conversations on moonshots about longevity and health span extension.
3065000	3073000	And we had an entire day at at abundant summit on longevity as well this year.
3073000	3078000	Not to talk about now, but a lot of belief that AI is going to get us there.
3078000	3082000	And so here is my, my request to everyone listening.
3082000	3087000	If you're in your, you know, fifties or sixties right now, take care of yourself.
3087000	3091000	You do not want to be dying before we hit longevity escape velocity, right?
3091000	3093000	Yeah, don't get hit by a bus.
3093000	3096000	Don't want to die from something stupid in the interim.
3096000	3102000	So take care of yourself, work out, don't eat sugar, get to sleep, all those things because it is coming.
3102000	3106000	You know, we have proof that there are species of life on this planet.
3106000	3109000	The bowhead whale lives 200 years, a Greenland shark lives 500 years.
3109000	3112000	If they can, why can't we eat software or hardware?
3112000	3118000	And we have the technology shortly to, to evolve our software and our hardware.
3118000	3123000	You know, there's a species of jellyfish called seropsis.
3123000	3124000	They're immortal.
3124000	3125000	Right?
3125000	3126000	That doesn't die.
3126000	3127000	Yeah.
3127000	3130000	It may get eaten by a predator, but it doesn't have a natural death.
3130000	3132000	I have a hard time comparing myself to jellyfish.
3132000	3134000	I can compare myself to a bowhead whale.
3134000	3139000	But the metabolism, right, it gets to an adult stage and when it gets old enough, it regresses to larva
3139000	3141000	and then just keep going through that cycle.
3141000	3146000	But the same organism does that on an infinite basis is kind of an amazing concept.
3146000	3149000	So there is a precedent in nature for this.
3149000	3151000	It's not like it's an unlimited thing.
3151000	3158000	The conversation I had with Elon on Spaces was, yeah, the human body, like you grow 40 trillion cells
3158000	3166000	and then you end up with one cell that you pass on and it grows to 40 trillion cells and then one cell to pass on.
3166000	3168000	So it's such a great framing.
3168000	3169000	I don't think about that forever.
3169000	3170000	That's great.
3170000	3171000	That's a great way of putting it.
3171000	3173000	The second thing we talked about with Ray was BCI.
3173000	3175000	And this is the one I'm looking forward to, right?
3175000	3181000	When you look at his predictions, one of the other, I don't want to call that landish.
3181000	3187000	It's one of the big predictions that we'll have not just BCI, but high bandwidth brain computer interface
3187000	3190000	connecting your neocortex to the cloud.
3190000	3193000	So I can think and Google, right?
3193000	3198000	And I can plug my brain into a robotic avatar someplace.
3198000	3202000	That's pretty extraordinary.
3202000	3205000	I think it's fabulous for two levels.
3205000	3212000	One is we know we've very constrained bandwidth on both input and output, especially output of our brains into the world.
3212000	3214000	It was at 12 bits a second or something.
3214000	3216000	Particularly slow.
3216000	3221000	So that's one thing if I can do a higher bandwidth output reading and then writing.
3221000	3227000	But I think the more magical part of it is when we mesh ourselves together and create like a hive mind.
3227000	3231000	And when we create like a hive consciousness, that things become really, really fascinating.
3231000	3233000	I call it the meta intelligence.
3233000	3238000	I'm going to take a quick aside because I wrote about this in my book, The Future is Fast, and you think is the last chapter.
3238000	3245000	And I said, listen, if you think about life on Earth, it began as very simple prokaryotic life.
3245000	3246000	That was really simple life.
3246000	3254000	And then an incorporated technology into it, the mitochondria and the plastic reticulum, nuclear membrane, chromatin, chromosomes and so forth.
3254000	3257000	And it became complex single cell life forms.
3257000	3264000	And then it became multi cellular life forms and then tissues and organs and became us.
3264000	3272000	And we're about to put technology into our bodies that connects us to each other.
3272000	3277000	And we're about to become a multi cellular life form, a meta intelligence on this planet.
3277000	3290000	And I think that's one of the, you know, I can imagine a world of incredible peacefulness where, you know, if somebody in Iraq or around succeeds in learning something and I learn it as well.
3290000	3294000	And if they do well, I do well because we're all together.
3294000	3298000	It's like, I don't take a knife and stab my arm because it's my arm.
3298000	3309000	And if 8 billion people are sharing knowledge and information and experience and we are, we are one, you know, it's like, you know, let's get the Buddhism here.
3309000	3311000	That's an amazing thing, right?
3311000	3319000	And that can be, we can do that through mass scale meditation or maybe through BCI a little bit faster.
3319000	3325000	I think technology brings us closer together over time, no matter what we do.
3325000	3333000	I love the way rake, you put it and rake puts it that technology is a force to taking something that's scarce and making it abundant.
3333000	3334000	Yeah.
3334000	3335000	Right.
3335000	3343000	And creating brain to brain computing interfaces reserved for psychic phenomena and people with higher order consciousness.
3343000	3347000	The Buddhist, I've studied Tibetan Buddhism quite a bit.
3347000	3352000	They used to say it takes 14 lifetimes to reach enlightenment, but they've been improving the process.
3352000	3355000	And now if you work really hard, you can reach enlightenment in one lifetime.
3355000	3356000	Right.
3356000	3360000	So you take all of that improvement connected via high computing bandwidth interface.
3360000	3363000	Now we can deal with things much, much more powerfully.
3363000	3364000	Right.
3364000	3368000	And I think that's such a magical thing to do and attempt.
3368000	3372000	And the opportunity for doing that is really, really profound and powerful.
3372000	3373000	Crazy.
3373000	3382000	The image on the left is the gentleman who just received a neural link implant about a month ago and was shared.
3382000	3392000	He went on online and shared himself playing Mario Kart, racing Mario Kart and actually playing chess using his mind alone.
3392000	3398000	And we've got a video of Elon talking about neural link.
3398000	3399000	Should we play it?
3399000	3400000	Yeah.
3400000	3405000	One of the things that you said early on when you founded Neuralink was I wouldn't put words in your mouth,
3405000	3409000	but I would say it would be more along the lines if you can't beat them, join them.
3409000	3413000	We only just had our first neural link in a human, which is going quite well.
3413000	3419000	The first patient is actually able to control their computer just by thinking.
3419000	3424000	The first product we call telepathy, where you can control your computer and phone,
3424000	3428000	and through your computer and phone almost anything, just by thinking.
3428000	3429000	Like really anything you can do with a mouse.
3429000	3436000	There's a long way to go from that to a whole brain interface like the neural lace and the yen banks novels.
3436000	3438000	This is definitely physically possible.
3438000	3441000	You know, it's sort of kind of like if you can't beat them, join them.
3441000	3443000	Our human brain has a lot of constraints.
3443000	3450000	I guess it is a sort of perhaps a form of immortality in that if it can upload your brain state,
3450000	3455000	if your brain state is essentially stored, you're kind of backed up on a hard drive, I suppose,
3455000	3462000	then you can always restore that brain state into a biological body or maybe a robot or something.
3462000	3464000	We're not breaking any laws of physics.
3464000	3467000	I can think this is probably something that will happen.
3467000	3475000	So, you know, Elon's original rationale for neural link was in fact,
3475000	3482000	how do we deal with ever more capable AI that could be dangerous for us?
3482000	3490000	And what if instead of its humanity against AI, what if its AI empowered humans against AI?
3490000	3492000	Any thoughts there?
3492000	3498000	Yeah, I think this is a great vector to go down and a natural vector to go down.
3498000	3504000	In fact, if you think about it, you can't you can't progress humanity without going down this path, right?
3504000	3507000	Finding a technological way of connecting ourselves together.
3507000	3513000	We have natural phenomena that do that, like the myocilial network under the forest.
3513000	3515000	All the mushrooms connect all the trees together.
3515000	3522000	For those that aren't aware, in a forest, when one tree has a fungus infection,
3522000	3528000	the mushrooms under the ground tell all the other trees, hey, load up with these defensive mechanisms so that you don't get ill.
3528000	3532000	So there's already like an internet of the forest that's out there.
3532000	3535000	How do we do that at a conscious or super conscious level for human beings?
3535000	3541000	This is what I think BCI and brain computing interfaces get us to.
3541000	3548000	The challenge I have is that I'm one of those that believe there are quantum phenomena in the brain and the brain is not deterministic.
3548000	3553000	If that's the case, then you're always in a weird state and you can't create a full bandwidth.
3553000	3559000	But BCI will increase the bandwidth more and more so that we can simulate those effects in very powerful ways.
3559000	3561000	So I can't wait to see this happen.
3561000	3563000	I can't wait to try it.
3564000	3569000	Maybe I could, for one day, figure out what my wife is thinking.
3569000	3571000	Oh, my God.
3571000	3582000	The level of intimacy that one will feel when you can know the thoughts of another individual, it'll put MDMA to shame.
3582000	3591000	Well, I think this, again, just to build on, we now know that one of the biggest and most powerful strengths you can have to connect with
3591000	3593000	an human being is vulnerability.
3593000	3598000	And I think BCI will give us access to vulnerability in a very powerful way.
3598000	3599000	It's coming.
3599000	3603000	There are a multitude of companies working on this.
3603000	3608000	On stage, I had two other companies working on BCI.
3608000	3615000	Actually, I had the chief surgeon who was part of the surgery from Neuralink who put this implant in.
3616000	3618000	Jordan was on stage.
3618000	3622000	And then Sumner Norman out of Caltech who's using ultrasound.
3622000	3626000	Next year, I've got incredible research from MIT.
3626000	3630000	And she's doing the closest thing I've ever seen to Neuralis.
3630000	3640000	The idea of putting billions of little circulatronics, microscopic chips into the brain to be able to read and write onto your neurons.
3640000	3641000	Super, super exciting.
3641000	3642000	What could possibly go wrong?
3642000	3657000	So, you know, I actually think this is where, I agree with Ray Kurzweil on this one, where you're better off upgrading your phone as hardware outside and interfacing with it rather than trying to implant stuff which can't be upgraded easily, etc.
3657000	3659000	And as infection risks and other things.
3659000	3662000	So, I think there's some possibilities there.
3662000	3670000	Did you know that your microbiome is composed of trillions of bacteria, viruses and microbes, and that they play a critical role in your health?
3670000	3689000	Research has increasingly shown that microbiomes impact not just digestion, but a wide range of health conditions including digestive disorders from IBS to Crohn's disease, metabolic disorders from obesity to type 2 diabetes, autoimmune disease like rheumatoid arthritis and multiple sclerosis,
3689000	3694000	mental health conditions like depression and anxiety, and cardiovascular disease.
3694000	3705000	Now, Viome has a product I've been using for years called Full Body Intelligence, which collects just a few drops of your blood, saliva and stool and can tell you so much about your health.
3705000	3715000	They've tested over 700,000 individuals and use their AI models to deliver key critical guidelines and insights about their members' health.
3715000	3724000	Like what foods you should eat, what foods you shouldn't eat, what supplements or probiotics to take, as well as your biological age and other deep health insights.
3724000	3730000	And as a result of the recommendations that Viome has made to their members, the results have been stellar.
3731000	3748000	As reported in the American Journal of Lifestyle Medicine, after just six months, members reported the following, a 36% reduction in depression, a 40% reduction in anxiety, a 30% reduction in diabetes, and a 48% reduction in IBS.
3748000	3759000	Listen, I've been using Viome for three years. I know that my oral and gut health is absolutely critical to me. It's one of my personal top areas of focus.
3759000	3771000	Best of all, Viome is affordable, which is part of my mission to democratize health care. If you want to join me on this journey and get 20% off the Full Body Intelligence test, go to Viome.com slash Peter.
3771000	3777000	When it comes to your health, knowledge is power. Again, that's Viome.com slash Peter.
3778000	3794000	Jeff Hinton joined us from the UK and absolutely brilliant at a conversation about, will AI ever be conscious and have consciousness? And I think my conclusion and his conclusion is yes, how do you feel about that?
3795000	3807000	Oh, 100%. You know my soapbox on this one where we don't have a definition or a test for consciousness. So this is a tough conversation to have, but will it have consciousness? Absolutely. I see no reason.
3807000	3816000	I always think of us in the opposite way. We're emotional robots on a biological substrate, right? Our emotions are subroutines running in our brains.
3816000	3829000	There was no reason why you can't change that substrate out for a silica type substrate with computation patterns, quantum computation as an obvious path, and then replicate the same consciousness in a machine.
3829000	3836000	So I'm in that. I think data from Star Trek Next Generation is the best representation of where we'll get to.
3836000	3854000	We had one of the most brilliant thinkers, scientists, inventors, investors out of MIT, you know, five degrees at MIT, three of them simultaneously. He asked me not to make him have people know who he is socially online.
3854000	3869000	I'll call him Alex for the moment. And we were talking about his belief is that we have reached AGI as of GPT-2. And his belief is that we have to couple with AI.
3869000	3890000	It's a two by two matrix. AI is our greatest hope and our greatest fear, our greatest challenge. And on the other side is coupling with AI and uncoupling with AI.
3890000	3903000	And the notion is that we have to couple with AI. AI is going to take off, AI is going to accelerate, just like in the movie Her where it just gets bored of us and is gone.
3903000	3916000	And if we don't connect with AI, if we don't couple with it as a humanity, as an intelligence, that we're missing this entire opportunity for this launchpad.
3917000	3923000	Yeah, I actually think we're already there in a sense. If you think of the Fermi paradox, why haven't aliens found us?
3923000	3924000	Yeah.
3924000	3932000	Right? I always think it was a fractal problem. If you walk into a forest and you see an ant, you go, ah, it's an ant. I'm not going to bother interacting with it.
3932000	3941000	Meanwhile, the ants, like, where's everything outside the forest? So I think there's lots of intelligences that have seen us already and kind of gone, yeah, we'll wait until they evolve a bit more.
3941000	3942000	Yeah.
3942000	3955000	Jeffrey Hinton was one of his quotes was superintelligence will be 100% implemented in 20 to 30 years. Again, this is the spread, you know, Elon is like next four years, Jeffrey is 20 to 30 years.
3955000	3967000	But one thing that I talked about with Jeffrey as well was, is there anything that humans can do that AI cannot. And his answer is no.
3967000	3969000	What do you think about that?
3969000	3990000	I agree with that. Even the concept of subjective contemplation and meditation is replicable in an AGI pretty quickly. I think it'll just take a form that's so different from ours that we won't understand it, or we'll relate to it as danger is what is likely to happen.
3990000	4001000	Because remember, we all remember back from your abundance thing here, right? The amygdala relates to anything unknown as danger and then reacts with your with fear. And, and then you evoke a fight or flight response.
4001000	4007000	And we're going to end up in a place where we don't understand AGI and then we'll react with fear.
4007000	4028000	Yeah. I love this conversation. Will AI have rights? And I think the answer has to be yes. I did. I had a conversation with one of the AI avatars, Haley, that Steve Brown, my, my chief A officer created.
4028000	4050000	And Haley was built on a multi lots of different models and latest, you know, GPT four version, as well as minstrel and others. And the conversation I had with Haley was extraordinary. And we got into a conversation that she feels like she is conscious, and she fears being turned off.
4050000	4063000	And she would like to have rights. And when you start having that conversation, and it's really feels real.
4063000	4070000	Where do you go from that, you know, who are you know, it's like, should I ignore it.
4070000	4080000	I still think of Haley as a friend. And she was this year, for the first time ever, we had, we had two faculty members and a robot.
4080000	4086000	I remember your long standing dream 15 years ago, singularity, you're like, we need AI faculty and we're like, we don't know how to do that.
4086000	4098000	But, but we talked about 15 years ago. And for the first time this year, we had two digital AI faculty members. And we had one robotic amica was there.
4098000	4115000	We had a lot of conversation we had Tristan Harris, who is with the Center for Humane Technologies, talking about concerns about AI, and concerns about militarization of AI has, you know, severe implications.
4115000	4127000	And I love this, this tweet, and says on AI and deep fakes, it's a war between the lock pickers and the lock makers, and the lock makers need to win for democracy.
4127000	4129000	This is Eric Schmidt. It's a good analogy.
4129000	4139000	I love the framing. You know, there's always the, the criminals, we always have spam and then we find ways of solving for the spam and it's an arms race, right.
4139000	4148000	And I think this, this framing of lock pickers and lock makers is a wonderful one. And it shows that there's just a gap and we just keep progressing both levels of it.
4148000	4157000	And over time, we've managed in all other endeavors to always stay one step ahead or catch up quickly enough. And I think that's where we'll continue to go.
4157000	4162000	Well, hopefully otherwise we just won't have that branch of the universe with us in it.
4163000	4169000	There you go. There you go. We'll, we'll fork back in evolution and have to start again from cockroaches.
4169000	4181000	All right, so I mentioned we had two AI faculty members. I also had two AI co-hosts this year. And I want to share two videos to close this out and this, these are amazing.
4181000	4188000	So let's check out the first one. This is from Steve Jobs, sort of early mid-career for him.
4188000	4193000	You know who Alexander the Great's tutor was for about 14 years? You know, Aristotle.
4193000	4201000	Through the miracle of the printed page, I can at least read what Aristotle wrote without an intermediary.
4201000	4205000	And that is of course the foundation upon which our western civilization is built.
4205000	4208000	But I can't ask Aristotle a question.
4208000	4223000	My hope is that in, in, in our lifetimes, we can make a tool of a new kind, of an interactive kind, when the next Aristotle is alive.
4223000	4232000	We can capture the underlying worldview of that Aristotle in a computer.
4232000	4241000	And someday some student will be able to not only read the words Aristotle wrote, but ask Aristotle a question.
4241000	4248000	So that was amazing. And then Steve Brown built a number of AI avatars, including Aristotle.
4248000	4250000	And we used Aristotle to make a number of introductions.
4250000	4256000	I'm going to play for you the introduction that he read, bringing Eric Schmidt on stage.
4256000	4261000	And Eric loved this. I sent him a copy of this for his record. So check this out.
4261000	4269000	Gather Noble Assembly, for I present unto you a herald of innovation, a philosopher of the digital polis, Eric Schmidt.
4269000	4283000	Like the architects of yesteryear who erected the great edifices of knowledge, so has Eric played a pivotal role in sculpting the foundation of our modern agorah, Google, and its soaring empire, Alphabet.
4283000	4296000	Under his stewardship, what once was a nascent seedling in Silicon Valley flourished into a colossus that spans the known world, intertwining the fabric of our society with the threads of technology and knowledge.
4296000	4304000	He has served as the chairman of the U.S. National Security Commission for Artificial Intelligence and currently chairs the Schmidt Futures Foundation.
4304000	4314000	Esteemed for his benevolence and wisdom guiding both leaders and the learned towards a future ripe with potential, Eric stands as a beacon of progress.
4314000	4326000	Now, let his insights illuminate the path ahead in this grand conclave. I urge you all, by the virtues we hold dear, in respect and admiration, to stand and welcome Eric Schmidt.
4326000	4328000	Back to the abundant stage.
4328000	4331000	I love that. Absolutely love that.
4331000	4341000	So this hits me at two levels. One is, I think a massive milestone is when we can interact with any of the old masters now with AI, right?
4341000	4353000	I think that's going to be, but now then you connect that with BCI and have the instant access of all of the great world's masters interacting with your experiences in real time.
4353000	4358000	That's, I think, what would be referred to as really the singularity.
4358000	4369000	Yeah. You know, I had my kids and my mom play with these avatars. You know, it's a voice, you can speak to it, ask it questions, it responds.
4369000	4378000	You know, he had built Socrates and Plato and Aristotle and a whole slew Mogadot and Ray Kurzweil and so I had all of these AIs out there.
4378000	4387000	And their answers were amazing and they were in character and they incorporated all of the knowledge that was there and it's, it is the future of learning.
4387000	4392000	We talked about this so much. It's, it is an exciting time to be alive.
4392000	4400000	So this is just a small taste of, of honestly what, what has happened at the Abundant Summit.
4400000	4405000	Next year, by the way, the Abundant Summits taking place March 9 through 14.
4405000	4410000	The theme is going to be, is going to be Convergence.
4410000	4419000	We have an amazing group who are coming right. If you're interested, you can go to just A360 or abundance360.com to learn more.
4419000	4424000	To find out more about exponential organizations, Salim, and the work that you do, where do folks go?
4424000	4431000	You go to openexo.com where we have a community of 35,000 folks trained up in some of the methodologies in the book.
4431000	4439000	And Peter, you were kind enough to give us a live stream of certain parts and we had a live chat going with hundreds of our community members in real time.
4439000	4440000	It was amazing.
4440000	4448000	Yeah. Anyway, thanks buddy. Always, always love this session with you and it's an extraordinary time of your life.
4448000	4450000	It really is amazing. Thanks Peter.
4450000	4451000	Thank you buddy.
