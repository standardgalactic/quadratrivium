WEBVTT

00:00.000 --> 00:03.760
These organizations are telling you that they're building something that could kill

00:03.760 --> 00:09.360
you and something that could remove all our freedom and liberty. And they're

00:09.360 --> 00:12.560
saying it's a good thing you should back them because it's cool. They don't care

00:12.560 --> 00:16.400
about the revenue. If they have the political power people are scared of

00:16.400 --> 00:21.080
them. Power should not be invested in any one individual. If I can accelerate this

00:21.080 --> 00:24.720
over the next period I don't have to make an impact. I should not have any power

00:24.880 --> 00:28.000
whereas again you see everyone else trying to get more and more power.

00:30.320 --> 00:34.720
The only way that you can beat it to create the standard that represents humanity

00:35.360 --> 00:38.880
is decentralized intelligence. It's collective intelligence.

00:39.520 --> 00:45.200
The data sets and norms from that will be ones that help children, that help people suffering,

00:45.920 --> 00:51.760
that reflect our moral upstanding and the best of us and gathers the best of us to do it.

00:55.680 --> 01:01.280
A week ago, E-Mod Mistak was on my stage at Abundance 360 talking about the future of

01:01.280 --> 01:08.240
open source AI, democratized, decentralized AI. The day after A360 he stepped down as CEO of

01:08.240 --> 01:14.080
Stability. Now five days later I've sat down with E-Mod to talk about why he's stepping down,

01:14.080 --> 01:19.280
what he's doing next, the future of AI. He takes the gloves off, he talks about the dangers of

01:19.280 --> 01:26.480
centralized AI and the potential for decentralized democratized AI to be the only avenue that truly

01:26.480 --> 01:33.440
uplifts all of humanity. All right, if you liked this episode please subscribe. Let's jump in. If

01:33.440 --> 01:38.320
you're a mood shot entrepreneur this is an episode you're not going to want to miss. All right,

01:38.320 --> 01:42.320
now on to E-Mod. Good morning E-Mod. Good to see you my friend.

01:42.400 --> 01:49.920
Actually it's always pizza. So you and I were on stage literally last week at the 2024 Abundance

01:49.920 --> 01:57.520
Summit talking about the whole open source AI movement. You were beginning to talk about

01:57.520 --> 02:03.600
decentralized AI. You were talking about where Stability was the speed of the development of

02:03.600 --> 02:11.840
the different products and the day after the Abundance Summit was over the news hit that you

02:11.840 --> 02:20.000
had stepped down as CEO of Stability and stepped off the board. So let's begin with the obvious

02:20.000 --> 02:28.640
question. Why? What happened and I have huge respect for you and I know a lot of the issues in the

02:28.640 --> 02:34.240
past but I'd like you to have a chance to share with entrepreneurs out there and folks interested in

02:34.240 --> 02:42.800
AI exactly your side of what's happening. Yeah, thanks. I think that Elon Musk once characterized

02:42.800 --> 02:51.600
being a CEO as staring into the abyss and chewing glass because you are looking at a very uncertain

02:51.600 --> 02:55.360
future having to make decisions and the chewing glass is all the problems that come to you all

02:55.360 --> 03:03.200
the time and it's required to steer the ship when things are incredibly uncertain and Stability

03:03.200 --> 03:08.560
is a pretty unique company at a unique time. We hired our first developer and researcher two

03:08.560 --> 03:14.160
years ago and then in those two years we built the best models of almost every type except for large

03:14.160 --> 03:21.520
language, image, audio, 3D etc and had over 300 million downloads of the various models we created

03:21.520 --> 03:28.880
and supported which was a bit crazy and then gent of AI is crazy. In terms of usually in a startup

03:28.880 --> 03:33.600
you don't have to deal with global leaders and policy debates about the future of humanity

03:34.240 --> 03:41.360
and AI and everything else. At the same time building code. At the same time as building code

03:41.360 --> 03:49.600
yes and especially building code to a fraction of the resources of our competitors. We had

03:50.400 --> 03:56.240
certain teams that offered triple their entire packages to move to other companies. It was

03:56.240 --> 04:05.120
grateful that only a couple of researchers left for other companies and that was just startups.

04:05.120 --> 04:09.760
No one left for another big company which I think is testament to their kind of loyalty in the mission

04:10.720 --> 04:15.600
but you know what we've seen over the last year is or last half year in particular is the question

04:15.600 --> 04:24.560
of governance in AI is something that's incredibly important and who manages, owns, controls this

04:24.560 --> 04:30.480
technology and how it's distributed. So we saw you know everything from kind of open AI to congressional

04:30.480 --> 04:35.200
testimonies to other things and as you know one of my things has always been how do we get this

04:35.200 --> 04:40.800
technology in the hands of people all around the world and then who governs it and how can we then

04:40.800 --> 04:46.080
take this technology to have an impact from education to healthcare to others. The stability

04:46.080 --> 04:51.600
is a company that build great base models and we got to that point. The revenue is going up

04:51.600 --> 04:56.640
which is always nice you know finding the business model and again I think it was always curious to

04:56.640 --> 05:01.520
see that as a deep tech company two years in people are asking us you know why aren't you

05:01.520 --> 05:05.600
profitable and I was like right it takes a bit of time and investment to get to profitability.

05:07.040 --> 05:12.240
I think opening AI when they took they're not there yet but then they have eight years I think

05:12.240 --> 05:17.360
all the comparisons are perhaps unfair. So reviewing everything and kind of looking at it I was like

05:17.360 --> 05:23.200
do I really want to be a CEO? I think the answer is no. I think there is a lot imbued in that in

05:24.000 --> 05:30.560
tech but it's a very interesting position. Just to dive into that a second because we've had this

05:30.560 --> 05:38.480
conversation because there was a lot of pressure asking whether for you to step down as CEO

05:40.400 --> 05:46.960
and I think founders typically want to see themselves or feel they need to be the CEO

05:47.520 --> 05:52.880
and I've heard you say recently you know that you view yourself more as a founder and strategist

05:52.880 --> 05:58.160
than the CEO is that a fair assessment? Yeah I think everyone's got their own skill sets right

05:58.160 --> 06:04.000
so I'm particularly great at taking creatives, developers, researchers, others and achieving

06:04.000 --> 06:10.400
their full potential in designing systems but I should not be dealing with you know HR and

06:10.400 --> 06:14.480
operations and business development and other elements they're probably better people than me

06:14.560 --> 06:20.400
to do that. So now for example our most popular thing Stable Diffusion and come to UI the system

06:20.400 --> 06:26.240
around it is the most widely used image software models in the world. There are great media CEOs

06:26.240 --> 06:30.080
that can take that and amplify that to make hundreds of millions of revenues so they should come in

06:30.080 --> 06:39.040
and meet on that. So why now pal what is there anything that specifically tipped for you that has

06:39.840 --> 06:46.400
I mean because you know it has you know you have done an extraordinary job this has been your baby

06:47.600 --> 06:55.920
I mean how you have to feel a whole slew of emotional elements and I've had to step down

06:55.920 --> 07:02.720
as CEO on two occasions over the 27 companies I've had to sell a company for pennies on the

07:02.720 --> 07:12.000
dollar and it takes an emotional hardship on you. I've had calls for me to step down as CEO

07:12.000 --> 07:19.040
since the well since 2022 you know but I always thought you know what's best for the company in

07:19.040 --> 07:24.320
the mission and where I look at the world right now there's a few things a the company has momentum

07:24.320 --> 07:29.760
it has spread it's turning into a business like last year I said let's not enter into large revenue

07:29.760 --> 07:34.480
contracts because technology isn't mature yet and our processes aren't mature yet and you have to

07:34.480 --> 07:39.120
deliver so we do a lot of experimental things we're setting up and again now it's ramping

07:39.120 --> 07:44.480
on a business side on a technology side the technology is maturing diffusion transformers

07:44.480 --> 07:48.240
such as stable diffusion 3 and saura are going to be the next big thing and again stability has

07:48.240 --> 07:54.320
got a great place there but I think there's also the macro on this so if you look at the open AI

07:54.400 --> 08:01.360
you know sam altman said the board can fire me anytime this is the governance of open AI

08:02.080 --> 08:08.320
and then they fired him and then he is back on and he appoints himself back on the board

08:08.960 --> 08:12.880
there's clearly no governance opening I mean I respect the people on the board greatly

08:12.880 --> 08:17.520
you know I think there's some great individuals but who should manage the technology that drives

08:17.520 --> 08:23.200
humanity and teaches every child and manages our governance who's really needy on that that can build

08:23.200 --> 08:28.560
these models and do those things as you know I've always wanted to build these science models and

08:28.560 --> 08:34.560
the other health teams done that I want me doing the education work and then my concept of a national

08:34.560 --> 08:39.600
model for every country owned by the people of the country all tied together I think it needs to be

08:39.600 --> 08:46.240
by a web three not crypto or necessary token framework that's something that's a brand new

08:46.240 --> 08:52.080
kind of challenge and one that I think there's only a window of a year or two to do if you have

08:52.160 --> 08:55.600
highly capable models let's put aside a geo for now which you can discuss later

08:56.240 --> 09:02.400
really accelerating then no one will be able to keep up with that unless you build in a decentralized

09:02.400 --> 09:08.320
manner and distributed manner for data talent distribution standards and more so there's only

09:08.320 --> 09:13.120
a small window of time here to do that and realistically again successful companies and

09:13.120 --> 09:19.120
these things are all great but genitifia is a bit bigger than the classical knobs just like

09:19.120 --> 09:23.120
the whole lifecycle of the company was a lot faster than the classical norms so that's why I

09:23.120 --> 09:29.360
felt you know now is the right time to make that change and hopefully play my part in making sure

09:29.360 --> 09:34.960
this technology is distributed as widely as possible and governed properly as pretty much I

09:34.960 --> 09:40.000
think I'm the only real independent agent that has built state-of-the-art models in the world right

09:40.000 --> 09:50.960
now yeah we've seen a lot of turbulence with open AI we just saw Mustafa from inflection become part

09:50.960 --> 10:00.720
of Microsoft and I am curious I mean you you had a now famous conversation with Satya a couple of

10:00.720 --> 10:08.000
days after stepping down was that investigatory on your part or was that just a touch base for

10:08.000 --> 10:13.440
an old friend oh that's just chill trolling actually you know like to let us in steam that

10:13.440 --> 10:22.080
picture was I think from when year or two year or so ago okay but you know I think Satya is an

10:22.080 --> 10:28.640
amazing CEO and you know he responds again like the top CEO's incredibly quickly when you message

10:30.400 --> 10:34.800
he's got a great vision but there is again this concern about consolidation in tech

10:34.800 --> 10:40.160
we didn't take money from any trillion dollar companies that stability you know we remained

10:40.160 --> 10:47.040
and retained full independence you know and you know to the detriment of some of the elements

10:47.040 --> 10:53.120
that we could have taken very big checks and other things and even though you have good attention

10:53.120 --> 10:58.400
to that room that companies are slight slow dumb ai's that over optimize for various things

10:58.960 --> 11:02.640
and that's more solely in the best interest of humanity when you have infrastructure

11:03.600 --> 11:09.600
it's like this is the airports the railways the roads of the future AI is an infrastructure

11:09.600 --> 11:14.480
gentle that is an infrastructure which should be and should it be consolidated under the control

11:14.480 --> 11:20.080
of a few private companies with unclear objective functions again the people in the companies may

11:20.080 --> 11:25.280
be great I don't think so and this is a key concern and part of that was that commentary again he's

11:25.280 --> 11:31.200
doing an amazing job he's consolidating a lot of power for the good of the company and also he has

11:31.360 --> 11:37.120
I think genuinely good heart mission to bring technology to the world but it is a bit concerning

11:37.120 --> 11:41.600
right especially with the new types of structure you're speaking of Sam in this case

11:42.240 --> 11:46.640
Satya here oh sat here like a really like again I think the commentary was always interesting

11:46.640 --> 11:52.000
it's like you know sat here playing 40 chess you know assembling the AI Avengers I mean he is

11:52.000 --> 11:56.960
he's building an amazing massive talent covering the bases and Microsoft is doing incredibly well

11:56.960 --> 12:01.280
here right if you asked who's doing best to join tomorrow I think would say Microsoft

12:02.320 --> 12:06.320
but there is has to be concerns about consolidation of talent and power and reach

12:06.320 --> 12:10.800
before I get to your vision going forward because it's so important and what you're doing next

12:12.400 --> 12:22.560
I just again as a founder as a CEO of a moonshot company we have a lot of those listening here

12:23.280 --> 12:29.680
can ask how are you feeling right now because the decision to step down has to have huge emotional

12:30.240 --> 12:38.160
are you feeling relief are you feeling anxiety what's what's the feeling after making a momentous

12:38.160 --> 12:49.440
decision I was a big feeling of relief you know because there's the there's a Japanese concept

12:50.400 --> 12:56.560
I know a guy and I love it yes yeah do what you like and do where you believe you're adding value

12:56.560 --> 13:03.600
and other people do too you know like realistically again I think I was an excellent research leader

13:03.600 --> 13:08.080
strategist other things but I didn't communicate properly or hire the right other leaders in

13:08.080 --> 13:12.480
certain other areas of the company and they're better people to do that and so I wasn't doing

13:12.480 --> 13:18.640
what I was best at a lot or I could have the most measurable value and it was tying down you know

13:18.640 --> 13:22.640
there's a lot of legacy you know technical organization or other debt especially when

13:22.640 --> 13:27.360
you grow so so so fast and you know we were lucky that we had higher attention in the

13:27.360 --> 13:32.160
important areas and we could execute in spite of all of that in spite of going to be company

13:32.160 --> 13:36.640
around I think you know moonshot founders you have to do because you don't have the resources at

13:36.640 --> 13:41.680
the start and you have to guide the ship you know as it goes out from port but there does come that

13:41.680 --> 13:47.600
transition point there and there is a competing thing where you typically take on VC money which

13:47.600 --> 13:53.120
holds an objective function versus your overall mission so again if you look at the genitive

13:53.120 --> 14:00.480
AI world right now how many credible intelligent independent voices are there there they've had

14:00.480 --> 14:05.600
the ability to build models and design things and make an impact you know there's not many so

14:06.160 --> 14:11.360
I was like that's where I can add my most leverage and also the design space is again

14:11.360 --> 14:17.760
is unprecedentedly huge because the entire market has just been created like where does

14:17.760 --> 14:22.800
gentify not fit and where does it not touch and what needs to be built there we need to actually

14:22.800 --> 14:30.320
have the agency to go and build that and so I felt tired relieved I felt that now there's a

14:30.320 --> 14:35.520
million options I want it rather than taking a long break get on with things I've just done the

14:35.520 --> 14:39.760
first thing in kind of web three and I've got a whole bunch of other things we were going to discuss

14:39.760 --> 14:46.880
kind of coming and catalyze stuff that can make an exponential benefit because you know like massively

14:46.880 --> 14:51.600
transformed to purpose here is I want every kid to achieve their potential and give them the tools

14:51.600 --> 14:58.400
to do that and I love you for that because you've been true to that that vision and I know on the

14:58.400 --> 15:04.320
heels of your announcement you've been reached out to by national leaders by corporate you know by

15:04.400 --> 15:10.480
CEOs and major investment groups and you have a lot of opportunity ahead of you so

15:11.760 --> 15:18.560
let's talk about where you want to go next you mentioned publicly and you discussed on our

15:18.560 --> 15:26.800
abundance stage the idea of democratized and decentralized AI let's define that first what

15:26.800 --> 15:33.360
is that why is it important and what do you want to do there yeah I think that when I said I'm going

15:34.320 --> 15:38.880
move to do my part in decentralizing AI people like isn't that just open source you give the

15:38.880 --> 15:45.040
technology right and then anyone can use it but it isn't a decentralizing AI has a few important

15:45.040 --> 15:51.040
components one is availability and accessibility everyone should be able to access this technology

15:51.040 --> 15:55.040
the fruits of labor and there's some very interesting political and other elements around that

15:55.920 --> 16:01.440
number two is the governance of this technology you have centralized governance because the models

16:01.440 --> 16:05.760
are the data there's a recent Databricks thing a model where they show that you have massive

16:05.760 --> 16:10.560
improvements from data we all know that you know who governs the data that teaches your child or

16:10.560 --> 16:16.320
manages your health or runs your government that's an important question I think too few are asking

16:16.320 --> 16:22.480
and we need this transparency and other things like that so accessibility you know you've got the

16:23.600 --> 16:29.920
governance aspect of that and then finally you have how does it all come together is a single

16:29.920 --> 16:36.400
package or is it a modularized infrastructure that people can build on and is available kind

16:36.400 --> 16:41.600
of everywhere you know does it require monoliths and central servers where if it goes down you have

16:41.600 --> 16:47.680
an outage on GPT-4 you're a bit messed up or someone can attack and corrupt it I think that

16:47.680 --> 16:51.600
those are kind of the key elements that I was looking at when I was talking about decentralizing AI

16:52.240 --> 16:59.760
and you know I've got with an infrastructure to do that I hope as well so if you don't mind let's

16:59.760 --> 17:05.920
double click on it even further so you mentioned we don't have long to get there

17:06.960 --> 17:12.800
if that's a true statement why don't we have long to get there and then what does getting

17:12.800 --> 17:20.160
there look like if you had all the capital available and if the right national leaders were

17:20.160 --> 17:27.440
hearing about this because a lot of this is supporting supporting the populace of a nation

17:28.080 --> 17:37.840
um to have AI that serves them versus top down what's it look like two five ten years from now

17:38.560 --> 17:44.720
yeah I think you'll have both proprietary and open source AI and they'll work in combination the

17:44.720 --> 17:49.040
practical example I give is that this AI is like graduates right very talented slightly

17:49.040 --> 17:54.960
enthusiastic graduates and you want those and consultants um but I really have you know on

17:54.960 --> 17:59.520
stage with Matt Friedman last week at 8 360 when we're there he said it's like we discovered this

17:59.520 --> 18:04.960
new concept what do you call it AI Atlantis yes yes with a hundred billion graduates that'll work

18:04.960 --> 18:10.000
for free yes I love that analogy it was it was a brilliant analogy yeah we used to figure out how to

18:10.000 --> 18:16.320
say Atlantis you know um but but there's a few things here first of all is the defaults you know

18:16.320 --> 18:22.560
once a government embraces centralized technology it's very difficult to decentralize it and every

18:22.560 --> 18:32.000
country needs an AI strategy a year ago one year ago was GPT-4 yeah crazy how crazy is that you know

18:32.000 --> 18:39.840
the AI safety summit uh in the UK the king of England came on stage it all came via video call

18:39.840 --> 18:46.000
and he said that this is the biggest thing since fire you know and that was like what six seven

18:46.000 --> 18:53.680
months later where are we going to be in a year yeah I think he took that from uh uh from uh uh

18:53.680 --> 19:00.960
the founder of the CEO of Google um AI is as powerful as fire and electricity yeah yeah yeah

19:00.960 --> 19:05.280
I've heard the same from like Jeff Bezos and a bunch of others you know and not kindle fire

19:05.280 --> 19:11.280
proper fire you know um but then if you think about it norms are going to be set in this next

19:11.280 --> 19:17.600
period like you know I'm in California LA at the moment if you don't set norms on rights for actors

19:17.600 --> 19:22.160
and the movie industry then you could have a massive disruption just occurring this full length

19:22.160 --> 19:28.160
Hollywood features come in a year or two generated you know if you don't have norms around open models

19:28.160 --> 19:32.400
and ownership and governance by the people it'll be top-down governance because governments can't

19:33.600 --> 19:39.040
allow that to be out of control if they don't have a reasonable alternative um and I think the

19:39.040 --> 19:42.480
window is only a year or two because every government must have a strategy by the end of the year

19:43.200 --> 19:47.840
and so I think if you provide them a good solution that has this element of democratic

19:47.840 --> 19:53.680
governance and others that will be immensely beneficial I think also it's urgent because

19:53.680 --> 19:58.560
we have the ability to make a huge difference you know as we kind of may probably discuss later

19:58.560 --> 20:03.360
having all the knowledge of cancer longevity autism at your fingertips you know the technology

20:03.360 --> 20:07.920
for that right now you have the technology that no one ever is ever alone again on those things

20:08.000 --> 20:14.000
or to give every child a superior education literally in a couple of years like there is an

20:14.000 --> 20:19.680
urgency both from there's a small window but also from we must do this now because it can scale and

20:19.680 --> 20:24.480
make that impact we have dreamed of for so long the enabling technology is finally in it's finally

20:24.480 --> 20:28.160
good enough fast enough and cheap enough everybody want to take a short break from our episode to

20:28.160 --> 20:34.000
talk about a company that's very important to me and could actually save your life or the life of

20:34.000 --> 20:39.040
someone that you love company is called fountain life and it's a company I started years ago with

20:39.040 --> 20:45.040
Tony Robbins and a group of very talented physicians you know most of us don't actually know what's

20:45.040 --> 20:51.760
going on inside our body we're all optimists until that day when you have a pain in your side you go

20:51.760 --> 20:55.680
to the physician in the emergency room and they say listen I'm sorry to tell you this but you have

20:56.320 --> 21:02.880
this stage three or four going on and you know it didn't start that morning it probably was a problem

21:02.880 --> 21:09.680
that's been going on for some time but because we never look we don't find out so what we built

21:09.680 --> 21:15.280
at fountain life was the world's most advanced diagnostic centers we have four across the U.S.

21:15.280 --> 21:22.400
today and we're building 20 around the world these centers give you a full body MRI a brain a brain

21:22.400 --> 21:29.920
vasculature an AI enabled coronary CT looking for soft plaque dexa scan a grail blood cancer test

21:29.920 --> 21:37.520
a full executive blood workup it's the most advanced workup you'll ever receive 150 gigabytes of data

21:37.520 --> 21:44.800
that then go to our AIs and our physicians to find any disease at the very beginning when it's

21:44.800 --> 21:49.760
solvable you're gonna find out eventually might as well find out when you can take action fountain

21:49.760 --> 21:54.240
life also has an entire side of therapeutics we look around the world for the most advanced

21:54.240 --> 22:01.280
therapeutics that can add 10 20 healthy years to your life and we provide them to you at our centers

22:01.280 --> 22:09.120
so if this is of interest to you please go and check it out go to fountainlife.com backslash

22:09.120 --> 22:16.160
peter when tony and i wrote our new york times bestseller life force we had 30 000 people reached

22:16.160 --> 22:22.160
out to us for fountain life memberships if you go to fountainlife.com backslash peter we'll put you

22:22.160 --> 22:28.720
to the top of the list really it's something that is for me one of the most important things i offer

22:28.720 --> 22:36.400
my entire family the CEOs of my companies my friends it's a chance to really add decades onto

22:36.400 --> 22:42.240
our healthy lifespans go to fountainlife.com backslash peter it's one of the most important

22:42.240 --> 22:47.520
things i can offer to you as one of my listeners all right let's go back to our episode so the

22:47.600 --> 22:53.360
let's talk about the objective function of democratized and decentralized AI is it that the

22:53.360 --> 23:00.880
compute is resonant in countries around the world is it that the models are owned by the

23:00.880 --> 23:08.640
citizens of the world is it that data is owned and how do you get there from here i think that

23:09.200 --> 23:14.320
you can think of the supercomputers like universities you don't need many universities

23:14.320 --> 23:18.320
honestly if someone's building good quality models that's one of the things that says stability

23:18.320 --> 23:22.640
and we did the hard task we could just stuck with image we said no we're gonna have the best 3d

23:22.640 --> 23:29.280
image audio biomedical all these models and no one else managed that apart from open ai to agree

23:29.280 --> 23:34.480
in fact i think we have more modalities than open ai again kind of what i kind of describe this

23:34.480 --> 23:42.000
is accessibility and governance and a few of these other factors so i think what it means is that

23:42.000 --> 23:46.800
this technology is available to everyone but you see now that you don't assume you need giant

23:46.800 --> 23:52.160
supercomputers to even run it you know we showed you a language model running on a laptop stable

23:52.160 --> 23:58.320
lm2 will run on a gigabyte on a mac macare faster than you can read you know we're writing some poems

23:58.320 --> 24:04.960
various things you know um we see stable diffusion now at 300 images a second or consumer graphics

24:04.960 --> 24:11.360
card our video model was like five gigabytes of v-round this really changes the equation because

24:11.360 --> 24:16.560
in web 2 all the intelligence was centralized on these giant servers and big data now you have

24:16.560 --> 24:21.840
big supercomputers i think you'll need less with better data training these graduates that can go

24:21.840 --> 24:26.400
out and customize to each country but they must reflect the culture of that country like the

24:26.400 --> 24:31.680
japanese stable diffusion model we had if you typed in salary man it gave you a very sad person

24:31.680 --> 24:36.960
versus the base model giving you a very happy person right so you must have graduates that

24:36.960 --> 24:42.880
reflect the local culture and then reflect the local knowledge and then global models again

24:44.000 --> 24:50.720
that reflect our global knowledge and can be answered by anyone but who decides what goes

24:50.720 --> 24:56.640
in that these are some very important questions and who vouchers for the quality as well what's

24:56.640 --> 25:03.040
your advice to a national leader because you know we're now starting to see ministers of AI

25:03.040 --> 25:09.600
in different nation states and what what's your advice to them right now in this area

25:10.320 --> 25:16.240
i think my advice to them would be to start collecting the data sets that they would teach

25:16.800 --> 25:21.360
a graduate that was very smart through school and kind of other things this is national broadcast

25:21.360 --> 25:26.800
data this is the curriculum this is their accounting legal and others and note that those

25:26.800 --> 25:33.920
data sets are infrastructure they will enable the local populist and others to create these models

25:33.920 --> 25:38.640
because models are just data wrapped in algorithms with a bit of compute that's the recipe compute

25:38.640 --> 25:45.040
algorithms and data and it's not going to be as hard as you think to train these models but you

25:45.040 --> 25:50.000
have to build them to get standard so by the end of next year probably year after i would estimate

25:50.000 --> 25:57.280
that a longer 7tb model or a stable diffusion model so these are two leading models in image

25:57.280 --> 26:03.040
and language will cost about under ten thousand dollars probably even one thousand dollars to

26:03.040 --> 26:07.600
train and then it comes all about the data and then it becomes about the standards

26:09.360 --> 26:16.400
you know it's it's it it's interesting um there is so much knowledge in the world that will

26:16.800 --> 26:23.040
vaporize sublimate over the decade ahead as people die you know cultural data locked up in

26:23.040 --> 26:28.640
people's minds and stories and so forth that's never been recorded it's an interesting time to

26:28.640 --> 26:36.480
actually capture that data and permanently store it into the national models yeah and again i think

26:36.480 --> 26:42.240
people over focus on the models versus the data sets i mean is data set yeah yeah yeah with the

26:43.200 --> 26:48.880
exponential compute you can recalibrate and improve the data as well so right now a lot of the

26:48.880 --> 26:54.800
improvements in models are actually synthetically improving data and data quality as you said there's

26:54.800 --> 26:59.760
so much that can be lost but now we can actually capture this and the concepts and the other

26:59.760 --> 27:05.280
guidance and have crosschecks like you can deconstruct laws you know you can translate

27:05.280 --> 27:11.600
between contexts you can make expert information available to everyone because again you have

27:11.600 --> 27:17.520
this new continent of ai around us and all these graduates seem to be specialists that are on your

27:17.520 --> 27:24.960
phone and that's incredibly democratizing you know um because otherwise the knowledge is

27:24.960 --> 27:31.200
throughout history knowledge has always been gate kept always i want to get to uh health

27:31.200 --> 27:38.320
and education next but before we go there i know you were meeting with a mutual friend

27:38.320 --> 27:46.000
jewels or back the other day um and instability announced a deal with otoi endeavor and render

27:46.000 --> 27:52.160
network um are you still an advisor to that venture you know this is part of the whole thing

27:52.160 --> 27:59.200
it's the first of many web three kind of elements there i think web three is 95 percent let's say

27:59.200 --> 28:06.800
90 percent i'll be generous um speculative and rubbish but there is that five ten percent of

28:06.800 --> 28:12.320
genuine people that would be thinking about questions of governance coordination and others

28:12.320 --> 28:17.520
and have built things that are proper so otoi is the bridge to the creative industry you know

28:17.520 --> 28:22.880
that's why we're with arie manuel and eric schmidt and others are on the board um and the render

28:22.880 --> 28:29.520
network is a million gpus largely from creative professionals that are available and so the first

28:29.520 --> 28:35.840
thing i announced there i was going to be initial 10 million gaps 250 million of distributed compute

28:35.840 --> 28:42.880
to create the best 3d data sets like stability we funded and worked with allen institution others on

28:42.880 --> 28:48.320
objeverse excel which is 10 million high quality 3d assets we're going to a billion distributed

28:48.320 --> 28:53.280
you don't need giant supercomputers but then that is a community good that is owned by the people of

28:53.280 --> 28:59.680
the network and accessible to non-academic and others as well why because you need high quality

28:59.680 --> 29:04.240
assets to create better 3d models we have a new 3d model tried by us all that can generate a 3d

29:04.240 --> 29:09.840
image from a 2d image and not go five seconds and that 3d model feeds into better 3d assets

29:10.480 --> 29:14.560
and then what does that mean it means we're heading towards the holodeck without the data

29:14.560 --> 29:19.760
you're not going to get there and that joules joules wants the holodeck for sure yeah so you

29:19.760 --> 29:24.160
know jules and i are the same phase of that you know and you're not going to get there without

29:24.160 --> 29:30.160
again a commons of data that can train the graduates that then become specialized with star trek or

29:30.800 --> 29:35.600
you know star wars or any of these other ips and then also setting standards around

29:36.800 --> 29:43.040
monetization ip rights all sorts of other things and so a network like render is really good for

29:43.040 --> 29:47.120
that but you know i've been talking to a lot of people in web three about the different elements

29:47.120 --> 29:52.160
of the stack because what i basically see is that we have the opportunity to build

29:52.960 --> 29:59.440
almost a human operating system models and data sets for every nation every sector coordinated

29:59.440 --> 30:04.720
through proper web three principles again not speculative tokens or anything like that

30:05.600 --> 30:10.960
you know making it so that every child in the world or adult can create anything they can imagine

30:11.840 --> 30:16.400
they can be protected against the harms and they have access to the right information at the right

30:16.400 --> 30:23.760
time to thrive and again that's infrastructure for everyone it's a common good access to gpu's has

30:23.760 --> 30:31.920
been sort of the limited fuel um do you think decentralized gpu structures like render is part

30:31.920 --> 30:38.560
of that future is an important part of that future i think right now it's far more efficient to train

30:38.560 --> 30:43.520
models on these again big supercomputers the university but the rate of exponential growth

30:43.520 --> 30:49.920
there again is insane last year to train lama two cost ten million dollars in a year or cost

30:49.920 --> 30:55.760
ten thousand dollars a thousand times improvement from algorithms data supercomputes speeds

30:57.040 --> 31:02.160
and that's crazy if you think about it right um so i think this will be the limiting factor i think

31:02.160 --> 31:07.200
the gpu overhang four language models probably lasts until the end of the year but then there's

31:07.200 --> 31:13.600
plentiful supply because what you have is nvidia makes amazing gpu's at an 83 or 87 percent margin

31:13.840 --> 31:20.560
right but the actual calculations aren't complicated like we took intel gpu's and we ran the stable

31:20.560 --> 31:24.240
diffusion three diffusion transformer training so this is the same technology that's used in

31:24.240 --> 31:28.720
saura and stable diffusion three is multimodal so it can train saura models with enough compute

31:29.440 --> 31:33.200
and i think us and them are the only people kind of doing this so i think maybe pixart as well

31:34.560 --> 31:42.000
and then it ran faster than the intel gpu's than the nvidia gpu's but we know that it can run even

31:42.000 --> 31:46.640
faster because it's not optimized for either and it's still running fast so what you'll see

31:46.640 --> 31:52.240
is a commoditization of the hardware once the architect just gets stabilized because gpu is

31:52.240 --> 31:58.000
just a research artifact still diffusion was just a research artifact you're not going to

31:58.000 --> 32:04.160
engineer in phase yet and you've got to the point whereby this runs on macbooks it runs on other

32:04.160 --> 32:10.080
things so i think it's a short term phenomenon of the next year because people were taking a point

32:10.080 --> 32:15.440
in time and extrapolating without taking into account efficiencies optimizations

32:16.240 --> 32:21.040
and the fact that models that work on the edge and it can go to your private data will be more

32:21.040 --> 32:26.560
impactful than generalized intelligence everyone's over indexing on generalized intelligence and

32:26.560 --> 32:35.760
building ai god versus amplified human intelligence shall we say before we leave stability it's now

32:35.760 --> 32:44.320
in the hands of the chair and your past cto what's what do you imagine the future of stability

32:44.320 --> 32:48.960
is going to be going forward i know that you're not involved anymore it's under different leadership

32:48.960 --> 32:53.280
and there's a what's your advice to them or where do you think they're going to go

32:54.400 --> 32:58.320
you know i can do the very basic advice i didn't want any conflicts or anything because

32:58.320 --> 33:03.680
i'll be setting up lots of new companies and you know being a founder and a shareholder

33:03.760 --> 33:08.320
and against stability i'm a founder shareholder in fact you're still the majority shareholder

33:08.320 --> 33:13.840
i think i said right now yeah just about just about yeah that will change i'm sure new money

33:13.840 --> 33:19.440
will come in like we saw co here yesterday and i think it's very little 20 million of revenue

33:19.440 --> 33:24.160
run rate they're raising at five billion might as well it's increasing yes yeah with the right

33:24.160 --> 33:29.760
leadership i think that um it can again have an amazing part to play in media and that's what i've

33:29.760 --> 33:35.040
suggested to it um and again there's a great team that continues to ship great models so last week

33:35.040 --> 33:40.000
there was an amazing code model next week amazing language audio and other models are coming out so

33:40.000 --> 33:46.240
you know you continue shipping and great products around that too um so that was kind of my advice

33:46.240 --> 33:51.920
to them let's focus on media and take that forward but you know i'm not the expert on the business

33:51.920 --> 33:58.560
side of things i did the best i could my expertise on setting this up and taking it 0 to 10 and uh

33:58.560 --> 34:08.240
yeah is zero zero one is is definitely um a role that that you've played here and allow it allow

34:08.240 --> 34:15.280
someone else to take it the rest of the way uh but the area that i know actually there's something

34:15.280 --> 34:19.200
i wanted to kind of discuss here that thing's quite important please or again the founder's

34:19.200 --> 34:25.360
listing and the moonshot companies um there is an imbalance of power when you have

34:25.840 --> 34:33.040
very visionary highly competent leaders there like what i found its stability is there everyone

34:33.040 --> 34:37.040
would be waiting for me no matter how competent because i was the one that could see around the

34:37.040 --> 34:42.480
corners and i was a bit good at everything even if i hired people that built billion dollar startups

34:42.480 --> 34:48.080
or were leaders and research at google or kind of whatever um because you have an outside thing

34:48.080 --> 34:52.640
this is what kind of better says you have to speak last in some cases and some people in eating

34:53.360 --> 34:58.240
because otherwise everyone just does everything you say and they also wait on you now what i

34:58.240 --> 35:02.400
find and what i told the team is that you're flat as a power dynamic you're all on the same page

35:02.400 --> 35:07.040
you're all kind of relatively equal owners and it'll be interesting to see how it evolves from that

35:07.680 --> 35:11.920
given that there's actually a business and again i think this is something that you probably had a

35:11.920 --> 35:18.400
challenge within other founders here whereby they put more on your plate because you are so visionary

35:18.400 --> 35:23.520
and because you're like up there in the future and they're always waiting on you so you was like

35:23.520 --> 35:27.360
well my schedule is completely packed my schedule now is actually quite free which is also quite

35:27.360 --> 35:31.760
nice i've had a chance to speak with you every day for the last few days so that's been a pleasure

35:31.760 --> 35:39.360
to have extra time on your schedule you know so so we do have a world of uh visionary founder led

35:40.000 --> 35:45.360
CEO companies right so you've got musk and you've got bezos historically and you had steve jobs and

35:45.360 --> 35:52.480
you have and and that's both powerful and dangerous the the power is the ability for that

35:52.480 --> 35:58.560
because we don't ever have a company that is pre-existing a new CEO comes in and has the same

36:01.920 --> 36:06.560
both hootspa and and also the power of their of their vision

36:07.280 --> 36:07.760
um

36:09.600 --> 36:16.800
the danger there your concern you're you're saying is not not allowing your team to step

36:16.800 --> 36:22.880
up with their own vision or being over overly indexed on your on your vision yeah i think that

36:22.880 --> 36:27.840
that can be the issue and that's why i wanted stability to again reach the point of spread

36:27.840 --> 36:33.440
and revenue rate increase and other things before i do anything um and i felt again this

36:33.440 --> 36:38.560
external pressure in that if nobody or there's very few people in the world actually thinking

36:38.560 --> 36:42.720
properly about governance and spread and others and a very small window giving the pace of this

36:42.720 --> 36:47.840
to make a difference in the dent i believed i had a reasonable approach to that but i couldn't

36:47.840 --> 36:53.280
while remaining CEO of this company and again it's a pretty unique scenario because you've never

36:53.280 --> 36:59.600
seen a sector move this fast that has such wide-reaching human implications and regressively

36:59.680 --> 37:07.360
there's too few people i think with the right alignment and approach in this area i've been

37:07.360 --> 37:12.800
very disappointed like usually what happens is you have power maximization equations and this

37:12.800 --> 37:19.200
is what we're seeing from the industry consolidation how many people want to genuinely bring this

37:19.200 --> 37:26.080
technology to kids in nigeria or to the global south or to help those leaders build their own

37:26.080 --> 37:30.560
models you know and believe also in a positive some game that was actually my biggest surprise

37:30.560 --> 37:37.280
from the discussions to lincoln valley almost entirely they all believe in uh flat or negative

37:37.280 --> 37:42.560
some you know zero some or negative some things where there has to be a winner everyone's a winner

37:42.560 --> 37:49.840
in this and again i was just very disappointed seeing that i've been asking for you for a while to

37:49.840 --> 37:54.720
write and distribute your vision white paper because i've heard you describe it in detail

37:54.720 --> 37:59.040
and it's brilliant and i still hope that the world will see it soon enough everybody you

37:59.040 --> 38:03.120
want to take a break from our episode to tell you about an amazing company on a mission to

38:03.120 --> 38:09.600
prevent and reverse chronic disease by decoding your biology the company is called viome and

38:09.600 --> 38:15.840
they offer cutting-edge tests and personalized products that help you optimize your gut microbiome

38:15.840 --> 38:21.360
your oral microbiome and your cellular health as you probably know your microbiome is a collection

38:21.360 --> 38:27.680
of trillions of microbes that live in your gut and mouth and these microbiomes influence everything

38:27.680 --> 38:34.560
your digestion immunity mood weight and many other aspects of your health but not all microbes are

38:34.560 --> 38:40.800
good for you some can cause inflammation toxins and actually lead to chronic diseases like diabetes

38:40.800 --> 38:48.640
heart disease obesity and even cancer viome uses advanced mRNA technology and ai to analyze your

38:48.640 --> 38:54.400
microbes and your cells and give you personalized nutrition recommendations and products designed

38:54.400 --> 39:00.080
specifically for your genetics specifically for your biology you can choose from different tests

39:00.080 --> 39:05.760
depending on your goals and needs ranging from improving your gut health your oral health cellular

39:05.760 --> 39:11.360
function or all of them i've been using viome for the past three years i can tell you that it has

39:11.360 --> 39:17.520
made a huge difference in my health and because the data they collect and the ai engine they've built

39:17.520 --> 39:23.840
it gets better every single day i love getting health scores and seeing how my diet and lifestyle

39:23.840 --> 39:29.760
affects my microbiome and my cells and i love getting precision supplements and probiotics tailored

39:29.760 --> 39:34.800
for my specific needs if you want to join me on this journey of discovery and improve your health

39:34.800 --> 39:41.840
from the inside out viome has a special offer for you for a limited time you can get up to 40% off

39:42.480 --> 39:49.440
any viome test using the code moonshots just go to viome.com backslash moonshots and order your

39:49.440 --> 39:55.360
test today trust me you won't regret it all right let's go back to our episode before we jump into

39:55.360 --> 39:59.680
into health and education let's talk about governance a second because we've seen governance

40:02.160 --> 40:06.720
complicate this what is the right governance structure for this super powerful technology

40:07.280 --> 40:10.960
we have representation of democracy that i think can be improved by this like i don't think

40:10.960 --> 40:15.760
democracy survives this technology it's current for it will either improve or it'll end

40:16.640 --> 40:22.720
i don't see anything else like yesterday though what does end what does end mean here a benign

40:22.720 --> 40:29.840
dictatorship a driven by an ai overlord yeah like yesterday there was a announcement of an app called

40:29.840 --> 40:36.000
Hume which had emotionally intelligent speech and they can understand your emotions and talk with

40:36.640 --> 40:40.800
you know i have to discuss this yes you know where that's going right it's very powerful

40:41.600 --> 40:46.880
it's incredibly powerful and governments have a tendency i mean if there's no government but

40:46.880 --> 40:52.880
say it say it here it's important for you to state what what it means because we've discussed it but

40:52.880 --> 40:59.280
help people here be ready for this and democracy is all about representation and you see the

40:59.280 --> 41:03.680
questions of deep fakes and things speech is one of the most impactful elements there but now you

41:03.680 --> 41:11.920
can't believe anything you see here everything so one path that we have is a 1984 on steroids

41:12.720 --> 41:17.920
panoptica you know where life is gamified and you listen to whatever the government says and

41:17.920 --> 41:21.760
they're incredibly convincing and you're happy and you've always been happy and you've always been

41:21.760 --> 41:27.600
at war with eurasia you know propaganda on steroids the other path that you have is things like

41:27.600 --> 41:33.280
citizen assemblies consults of democracy the ability to take right now you can take any of

41:33.280 --> 41:40.320
the bills in congress and completely deconstruct them and find what the motivations are you know

41:40.320 --> 41:46.080
you can check laws against the constitution in a second seconds this is an incredibly powerful

41:46.080 --> 41:51.040
empowering technology from a democratic perspective so i see two routes unfortunately because i think

41:51.040 --> 41:57.040
that once the right thing goes it goes really fast centralized government control increasing

41:57.040 --> 42:02.080
because the government want to protect themselves as an organization and you know every party says

42:02.080 --> 42:06.720
the other party's crap we've seen the increasing polarization in america already

42:06.720 --> 42:10.400
and you know fundamentally come on you can do better than those two leaders that are currently

42:10.400 --> 42:16.400
competing i'm saying this is clear our system is sclerotic across this like democracy is the worst

42:16.400 --> 42:21.760
of all systems except for everyone else you can have a better democracy where it's actually

42:21.760 --> 42:28.880
representative and empowers the people or we will have the end of democracy where it is in 1984

42:28.880 --> 42:33.440
panopticon in my opinion because the momentum will go then of course you'll start using this

42:33.440 --> 42:38.960
technology you're already seeing it being used but not at scale and not intelligently yet which is

42:38.960 --> 42:47.040
scary i think we finally have the technology for a direct democracy versus a representative

42:47.040 --> 42:55.120
democracy right where i can have my my desires directly represented on any specific law or

42:56.080 --> 43:05.920
but i think the point you've made before is that speech if you look back to everybody from

43:05.920 --> 43:15.360
hitler to some of the most persuasive politicians is is a powerful tool and ai can become the most

43:15.360 --> 43:20.960
persuasive speaker out there it can take anyone's speech to make it far more persuasive like i think

43:21.040 --> 43:26.080
my voice is a bit whiny i can remove the wine you know i can go in a very polished british accent

43:26.080 --> 43:30.560
and other things like that right you know must fight them on the hills and the harriers and

43:30.560 --> 43:35.680
whatever public speaking needs to be different someone took hitler's speeches and put them

43:35.680 --> 43:41.440
through an ai and took them into english because when we're not in the german context and we listen

43:41.440 --> 43:45.920
to them it sounds like he's shouting like well that's crazy thing you hear him in english it is

43:45.920 --> 43:52.000
very different in his own voice just like someone took javier mele's one in the united nations

43:52.000 --> 43:57.440
and put him into english again he sounds a bit sharp how shouty but then he sounds very reasonable

43:57.440 --> 44:03.120
when it's in english and you can take the phenomes of obama's best speech and a bit of trumpinism

44:03.120 --> 44:08.080
and a bit of churchill and you'll have full modulation wave control over all of this people

44:08.080 --> 44:14.000
are already using this technology that everyone should have a passcode with their loved ones because

44:14.000 --> 44:19.200
people are getting calls from their mother saying help i'm in an emergency and you just

44:19.200 --> 44:24.560
send money right now and you cannot tell it and it pulls at the emotional strengths and if you

44:24.560 --> 44:29.600
look at something like us radio and you know one side of the political divide is taken over imagine

44:29.600 --> 44:35.840
if you're hearing optimized speech every single day that will have a huge impact and then they

44:35.840 --> 44:41.040
control the visuals and they control the other things we're not set up for defenses yeah if it's

44:41.360 --> 44:47.200
optimized speech for you specifically for you right for the kids you have the age group they

44:47.200 --> 44:52.720
have where you live your historical background and so forth and end of one persuasive speech

44:53.360 --> 45:01.040
coming at you the brain is not set up for defenses we're not and you know we take this as an example

45:01.040 --> 45:06.320
of the youtube algorithm like youtube as an organization is not an evil organization but

45:06.320 --> 45:10.960
it's an organization optimized for engagement which optimized for more extreme content so this

45:10.960 --> 45:17.200
is a dark place in youtube which is an optimized for ices the ices video spread viral i don't know

45:17.200 --> 45:23.040
sometimes viral is good sometimes viral is bad that one was bad because it was extreme and they

45:23.040 --> 45:29.680
didn't understand why and if you look at it our two of our largest germ to bear companies are google

45:30.240 --> 45:36.720
and meta and their business is advertising their business manipulation and they are both a moral

45:36.720 --> 45:43.280
companies because why would you expect a company to have morality our governments are also a moral

45:43.920 --> 45:48.640
and again you can view these things as slow dumb ai so you can see the way they will optimize

45:48.640 --> 45:53.520
unless we do something about it and they will have full control like again you put on your vision

45:53.600 --> 46:00.960
pro headset with your spatial audio that is full sensory control not full but you know what i mean

46:02.000 --> 46:09.680
full immersion full immersion full immersion and so we have to be aware of this and there's

46:09.680 --> 46:13.840
obviously other tools that can be used like in the wake of the ab spring you know governments

46:13.840 --> 46:17.840
targeted everyone else in social media we can do that on a high-end hyper-personalized basis

46:18.720 --> 46:27.200
like we need to set some defaults and standards here to protect democracy but again why democracy

46:28.240 --> 46:31.840
we're not really trying to protect democracy you know again people have different definition

46:31.840 --> 46:37.840
that what we're trying to protect is individual liberty freedom and agency education should be

46:37.840 --> 46:43.200
about enhancing the education of every child it's not you know healthcare is sick care our government

46:43.200 --> 46:47.360
should uplift us but how many people believe our governments do that rather than put us down

46:47.920 --> 46:53.680
because they couldn't encapsulate and cater to the brilliance of each individual

46:55.680 --> 46:59.120
because they didn't have the tools until now so that's why i said which way one man

46:59.760 --> 47:06.320
yes which way an agency or massive control these are the two ways do we control the technology

47:06.320 --> 47:12.800
or do these organizations control the technology that controls us you know when we were on the stage

47:12.880 --> 47:18.320
at at the abundant summit we talked about a future of digital superintelligence right

47:18.320 --> 47:26.480
and a future in which we've got AI a billion times more capable than a human which looking at it

47:26.480 --> 47:36.800
just from a ratio of neurons is the ratio of a hamster to a human um yep uh do you believe

47:36.800 --> 47:44.480
that someday we could have a a benign superintelligence that is supporting humanity

47:45.680 --> 47:49.280
yes and i believe that it should be a collective intelligence

47:50.560 --> 47:57.040
that is made up of amplified human intelligence is amplifying all of us pilots that contain our

47:57.040 --> 48:02.160
collective knowledge and culture and the best of us and datasets that are built from helping and

48:02.160 --> 48:09.360
augmenting us versus a collected intelligence and agi that is top down and designed to effectively

48:09.360 --> 48:14.400
control us again if you look at open ai statements on the rote agi they say this technology will end

48:14.400 --> 48:21.520
democracy and capitalism and maybe kill us all i don't like that i remember i remember seeing that

48:21.520 --> 48:27.520
you you you texted me you said read this does this sound the same as it does to me yeah so what

48:27.520 --> 48:31.680
i'd prefer instead is for this to be distributed like if you have datasets formations that are

48:31.680 --> 48:36.800
built on enhancing the capability of the nation that reflect the local cultures and you push for

48:36.800 --> 48:41.680
data transparency on models which i believe we must have you know especially language models

48:41.680 --> 48:46.480
then you're more likely to have a positive thing and again the human collective can achieve anything

48:46.480 --> 48:51.360
from splitting the atom to go into space if we put our minds to it but we have lacked in

48:51.360 --> 48:57.680
coordination mechanisms they've not been good enough so if you create the human colossus and

48:57.680 --> 49:03.520
every single person has an ai that's just looking at them to enhance their potential and coordination

49:03.520 --> 49:10.000
ai's that is a far more positive view of the future and that is the agi that is a general

49:10.000 --> 49:14.720
intelligence that's the hive mind general intelligence not a bog style hive mind but one

49:14.720 --> 49:19.840
that's really thinking again every child should achieve their potential versus this embodied

49:19.840 --> 49:23.680
concept of an agi that's a very western concept and you see that as well like you know we look at

49:23.680 --> 49:29.200
the japanese concept of a robot the robot is your equal and your helper you look at the western

49:29.200 --> 49:34.400
concept of the robot it's terminator and spinet and all of that and again i think this is again

49:34.400 --> 49:38.800
where cultural norms become very interesting and what do we want to build we want to build ai god

49:39.520 --> 49:43.520
or do we want to build that ai helper that helps us and we help it you know

49:44.000 --> 49:53.680
um those listening now i mean you can see umad's brilliance and why i'm so enamored with the way

49:53.680 --> 49:59.360
you think about this because it's there are very few individuals who are looking at this from

49:59.920 --> 50:03.760
an objective function of what's best for humanity what's best for every nation state

50:04.880 --> 50:11.680
out there let's talk about your going forward future are you going to build something in the

50:12.480 --> 50:21.120
in the decentralized side of of ai the democratized size of ai is there a company there or a fund in

50:21.120 --> 50:27.040
your future for that yeah so uh you're not doing the white paper finally getting there with a bit

50:27.040 --> 50:33.120
of help from ai uh i can't i can't i can't wait to uh to help uh broadcast that white paper

50:33.840 --> 50:39.440
yeah but look i think the basic thing is this what i want to do is set up a ai champion in every

50:39.440 --> 50:44.400
nation with the brightest people of each nation working with the organization of each nation

50:44.400 --> 50:48.560
to help guide them through this next period because there will be massive job displacement

50:48.560 --> 50:53.760
from the graduates going massive uplifts and productivity from the technology being implemented

50:53.760 --> 50:59.920
and again that organization can help govern and create these data sets and these models that

50:59.920 --> 51:05.120
are so important and every nation should have that but then i also believe that every sector

51:05.120 --> 51:09.760
should have a genitive ai first infrastructure company that builds this and helps the healthcare

51:09.760 --> 51:15.200
companies finance companies and others through that and to coordinate all of that you need to have

51:15.200 --> 51:20.000
a web three type protocol what is the protocol for intelligence so what what is a web what is a

51:20.000 --> 51:25.200
web three type protocol to find that for folks listening again people talk about web three it's

51:25.200 --> 51:30.320
not about the tokens or the meme coins or anything like that what about three protocols is that

51:30.400 --> 51:33.920
everyone should have like ai's first of all if i'm going to have bank accounts

51:35.440 --> 51:40.480
they're going to need some way to pay each other or exchange value and again web three is a lot

51:40.480 --> 51:45.600
of work in that there needs to be some sort of identity attribution and other format because

51:45.600 --> 51:51.840
you'll have this mass influx of information and so again web three concepts are very useful there

51:51.840 --> 51:57.120
there needs to be an identity concept because you'll have real and digital people web three concepts

51:57.120 --> 52:02.960
are very useful there so data access station all these other things verifiability so when i look at

52:02.960 --> 52:08.560
it if you've got sectorally my plan is to launch a company for every major sector and we can talk

52:08.560 --> 52:12.240
about health and education and bring the smartest people in the world to solve that challenge of

52:12.240 --> 52:16.880
the infrastructure for the future every nation but you need to have some sort of coordinating

52:16.880 --> 52:22.800
protocol for all of that that becomes a standard and that's the substrate for this amplified human

52:22.800 --> 52:28.480
collective intelligence and and is that where you want to play and focus your energy next

52:29.200 --> 52:34.000
yeah it's setting up these organizations and bringing the brightest smartest people that

52:34.000 --> 52:38.240
really want to make a difference there because there's massive network effects in doing this

52:38.240 --> 52:42.080
but again i just need to be the founder and architect i don't want to run the day to day of

52:42.080 --> 52:48.560
any of these things um and then because the the most scarce talent that there's three types of

52:48.560 --> 52:55.040
capital as i view it there's financial capital human capital and political capital and in order

52:55.040 --> 52:59.440
to effect change in the world you actually need all three but the financial capital actually comes

52:59.440 --> 53:05.120
with the people capital and the political capital and the smartest people in the world in every sector

53:05.120 --> 53:10.960
from health care to education to finance to agriculture almost all believe that gentry

53:10.960 --> 53:15.760
is the biggest thing they've ever seen in the last year everyone's asking you all the smartest

53:15.760 --> 53:19.920
people peter what's next right and you know many of the smartest people in the world so i want to

53:19.920 --> 53:25.760
create organizations that they can come the chefs and the cooks the thinkers and the doers and think

53:25.760 --> 53:31.040
what is the future of finance that's the future of education and then the national champions that

53:31.040 --> 53:36.640
should be owned by the people of each country become the distribution for the amazing infrastructure

53:36.640 --> 53:41.440
they built and there's a vice a nice kind of vice versa but then again you need the coordination

53:41.440 --> 53:46.000
function so i'm trying to bring together people in each of these and you know there'll be public

53:46.000 --> 53:49.760
calls and things like that to build that infrastructure the future because as mentioned

53:50.560 --> 53:54.320
ai isn't infrastructure but it should be i mean maybe it's the rocket ship of the mind right

53:55.840 --> 54:00.320
i love that i love that analogy my friend it is the most important infrastructure

54:00.320 --> 54:05.040
that humanity will have going forward across everything it does and i look forward to helping

54:05.040 --> 54:10.960
you build it exciting right like again i think one of the things i got i got lots of messages

54:11.040 --> 54:15.920
that like i'm so sorry for your loss it was like my dog had died after i left to see you

54:16.560 --> 54:22.800
i was like what is that you know they're like i was nice it's nice that people care right but i'm

54:22.800 --> 54:27.920
generally excited about what's next like you know again it was like staring into the abyss

54:27.920 --> 54:34.160
and chewing gloss every single day and that's not what i'm best at or i could have the most impact

54:34.160 --> 54:38.480
but i wanted to be a point whereby if i can accelerate this over the next period i don't

54:38.480 --> 54:44.800
have to make an impact i should not have any power on this whereas again you see everyone

54:44.800 --> 54:48.560
else trying to get more and more power i want to make sure it's set up properly but i want to give

54:48.560 --> 54:54.720
it all away because power is obligation it's dragging and again it should not be invested in

54:54.720 --> 55:01.120
any one individual we shouldn't have to rely on anyone being nice or good or for this technology

55:01.120 --> 55:06.080
i was talking to uh michael sailor during the abundant summit uh that evening and you know

55:06.080 --> 55:13.520
talking about the fact that because satoshi uh when he set it up uh did not retain any power

55:13.520 --> 55:19.120
and did not trade on the founding blocks and so forth that that's the reason it's been able to

55:19.120 --> 55:25.440
succeed because there wasn't that centralized power and you know bitcoin had been he said bitcoin

55:25.440 --> 55:33.680
betrayed many times before but because it didn't have that uh initial anonymity and and and the

55:33.680 --> 55:38.880
dissolution of founding power that that's the reason it didn't succeed yeah i mean again i think

55:38.880 --> 55:43.760
you need to have it accelerate and you see this with movements right the movement starts but then

55:43.760 --> 55:49.680
it goes once you've got the dna and the story there right you know you see the profits you see the

55:49.680 --> 55:54.960
leaders you see the others but then it's about setting the framework correctly and reframing the

55:54.960 --> 56:01.120
concept this technology is not going to look stability is a company that started two years

56:01.120 --> 56:06.560
ago above a chicken shop in london right you know my first 20 employees i went to the job

56:06.560 --> 56:11.760
center and i said bring me people that have overcome adversity and i will train them young

56:11.760 --> 56:17.120
graduates and six of them are still at stability you know um like because it was a program and

56:17.120 --> 56:22.160
they're doing things from cyber security to run supercomputers we only had like 16 17 phd's

56:22.800 --> 56:27.200
yet we built the state-of-the-art models in every modality we built mind-reading models

56:27.200 --> 56:32.480
like mind's eye you know i remember that contributed to all these things yet you're

56:32.480 --> 56:37.120
told it's impossible to compete we have shown it's not impossible to compete that's a reframing

56:37.760 --> 56:43.120
the reframing is data versus models it's you don't need giant supercomputers for everyone

56:43.120 --> 56:49.760
you just need to have a trusted entity to build it right yeah you know and so i hope to kind of

56:49.760 --> 56:54.320
convey this and then figure out this organizational structure that can proliferate so i can take the

56:54.320 --> 57:00.560
holiday so before we go further let's talk about one area of your next chapter in life that

57:01.120 --> 57:06.320
we both have as a passion which is the use of generative ai in health it's an area that you've

57:06.320 --> 57:11.840
given a huge amount of thought to and i think you're excited about can you share what your vision is

57:11.840 --> 57:19.440
there yeah so i got into ai 13 years ago gosh i was a programmer before for 23 years building

57:19.440 --> 57:24.640
large-scale systems as a hedge fund manager and other things well my son was diagnosed with autism

57:24.640 --> 57:28.560
and then i built an nlp team to analyze all the clinical literature and then looked at

57:29.280 --> 57:34.080
biomolecular pathway analysis of neurotransmitters gavra and glutamate in the brain to repurpose

57:34.080 --> 57:38.800
drugs for him and he went to macy's school which is great n equals one and then i was one of i was

57:38.800 --> 57:44.160
lead architect from the one the covet ai project so the united nations for stanford and others

57:44.160 --> 57:47.040
and then because i didn't get the technology i was like well we've got to build it ourselves

57:47.440 --> 57:54.560
but what is health you know again i think we have this discussion a lot health care is sick care

57:55.280 --> 57:58.880
we don't have all the information that we should have at our fingertips health assumes

57:58.880 --> 58:04.480
ergodicity a thousand tosses of the coin the same as the coin tossed a thousand times but we are

58:04.480 --> 58:10.560
all individual and across the world there are amazing data sets that could be better because

58:10.560 --> 58:16.560
when you write down a clinical trial or your own kind of experiences you lose so much information

58:16.640 --> 58:21.440
at the same time you don't have all the information on cancer or autism multiple

58:21.440 --> 58:25.680
sclerosis at your fingertips in the comprehensive authoritative and upstate way so when i look at

58:25.680 --> 58:34.240
the health operating system we're going to build a gpt for open for cancer and it's going to mean

58:34.240 --> 58:39.360
that nobody is alone again on that journey and uses that agency because they know comprehensive

58:39.360 --> 58:45.280
authoritative upstate all the knowledge but ai models today already outperform human doctors

58:45.280 --> 58:50.000
and empathy they're not going to be a learn on that anymore can i just double click on what you

58:50.000 --> 58:54.800
just said because it's really important i've had so many people because of my role as chairman of

58:54.800 --> 58:59.920
fountain life who reach out and say i just got diagnosed with this cancer or my brother or my

58:59.920 --> 59:08.720
sister or my wife and and there is they're left with this decimating news and they're left googling

59:09.440 --> 59:16.640
um but a model that's able to have the most cutting-edge information and then incorporate

59:16.640 --> 59:23.600
all their medical data and give them advice in empathic fashion how far is that see a couple

59:23.600 --> 59:29.680
of years if we focus maybe even like next year and that's amazing because all of these topics

59:30.240 --> 59:35.280
that again we will have diagnosis that it's superior we will have research augmentation

59:35.280 --> 59:38.880
because again even researchers don't have all that knowledge of their fingertips and again

59:38.880 --> 59:44.400
this is public infrastructure in a public good you know from primary care all the way through so

59:44.400 --> 59:48.800
that what is the open infrastructure in the future where this technology can come again

59:48.800 --> 59:54.000
to your own data as well you have things like melody and other things around homomorphic

59:54.000 --> 59:58.320
encryption federated learning that they were trying to figure out how to preserve privacy

59:58.320 --> 01:00:03.760
we can run a language model on a smart phone right now that can analyze all your data and then

01:00:03.760 --> 01:00:08.560
just feedback stuff to a global collective but people are people so when I look at health care

01:00:08.560 --> 01:00:13.920
I see amazing data sets that we can activate by taking the models to the data an infrastructure

01:00:13.920 --> 01:00:19.360
that we can build like we had checks agent with stanford the top x-ray radio radiology model

01:00:20.160 --> 01:00:25.600
to build good standard things across the entire gamut of health care so we actually get to health

01:00:25.600 --> 01:00:30.400
care versus sick care so we can make it so that everyone is empowered to make the best decisions

01:00:30.400 --> 01:00:35.760
either as experts or individuals and make it so nobody is alone again as well as increasingly

01:00:35.760 --> 01:00:42.640
data quality that will then feed better models that will then save lives save suffering and again

01:00:42.640 --> 01:00:47.840
increase our potential like you've got a long job wrong stupid behind you right why don't you have

01:00:47.840 --> 01:00:54.400
all the latest knowledge of longevity at your fingertips at a gpt4 level right now that will

01:00:54.400 --> 01:00:58.720
happen over the next year we will launch stable health or whatever decide to call it and there

01:00:58.720 --> 01:01:03.120
will be the smartest people in each of these areas working on that so again you're never like

01:01:03.120 --> 01:01:09.920
doesn't matter if you're with a hundred billion dollars and your kid has autism ast there's no cure

01:01:09.920 --> 01:01:15.040
there's no treatment there's nothing doesn't matter how rich you are yet we're just a little bit of

01:01:15.040 --> 01:01:20.240
effort right now we can build it as an open infrastructure for the five percent of people

01:01:20.240 --> 01:01:24.880
in the world that know someone with autism the fifty percent of people in the world that receive

01:01:24.880 --> 01:01:29.680
a cancer diagnosis of them or someone they love and they feel that loss of agency so we're going to

01:01:29.680 --> 01:01:35.200
return agency to humanity that way and again it needs to be an open infrastructure that they can

01:01:35.200 --> 01:01:40.960
then access private datasets and compensate them appropriately so everyone is incentivized we get

01:01:40.960 --> 01:01:48.080
that fast yeah and and that's a beautiful vision it is again infrastructure it and one of the things

01:01:48.080 --> 01:01:53.600
that's so beautiful about it is guess what all eight billion people we're all human we're all

01:01:53.600 --> 01:01:59.600
running the same software and the the the breakthroughs and the knowledge accumulated in

01:02:00.480 --> 01:02:06.800
you know in Kazakhstan is going to be as useful in Kansas yeah but this is the thing operating

01:02:06.800 --> 01:02:11.360
system this is the biggest upgrade to the human operating system we can imagine because we're

01:02:11.360 --> 01:02:17.920
going from analog to digital text is black and white whereas this these models only understand

01:02:18.000 --> 01:02:24.240
context you know daniel kahneman just passed you know amazing kind of guy but you know he did

01:02:24.240 --> 01:02:29.680
have this concept of type one type two thinking and so we had one which is these big data things that

01:02:29.680 --> 01:02:34.000
can only extrapolate but now we have these models that understand context and so we have the missing

01:02:34.000 --> 01:02:38.960
part of the brain and that will allow us to extrapolate allows to have more rainbows you

01:02:38.960 --> 01:02:43.760
know have the context of each individual push intelligence to the edge and that's why again

01:02:43.760 --> 01:02:49.120
there is this imperative to do this now because there's a window on the freedom agency democracy

01:02:49.120 --> 01:02:53.600
side but the other imperative is no one should have to suffer as they're suffering now

01:02:55.280 --> 01:02:59.840
amazing and how much does actually it doesn't need that much which is the really amazing stuff

01:03:00.400 --> 01:03:04.640
this the total amount spent in janitor bail i think i said at the conference is less than

01:03:05.200 --> 01:03:08.160
the total amount spent on the los angeles san francisco railway

01:03:08.800 --> 01:03:15.440
which hasn't even started yet and and in building stable health again if that's what it's called i

01:03:15.440 --> 01:03:21.360
mean the amount of capital required to build that is de minimis compared to what's spent on a single

01:03:21.360 --> 01:03:28.880
human trial of any any drug yeah it is but then you know you build it and you get to that 8020

01:03:28.880 --> 01:03:33.360
incredibly quickly that will change hundreds of millions of lives and that will attract the smartest

01:03:33.360 --> 01:03:36.240
people in each of these areas thinking about what is the open infrastructure of multiple

01:03:36.320 --> 01:03:43.920
sclerosis of longevity of cancer and more but then you can amp that because the value is so so huge

01:03:44.880 --> 01:03:50.560
and you i hope to build a trusted organization as part of this whole human operating system upgrade

01:03:50.560 --> 01:03:54.400
you know that's what i want to build i want to build human os well at least catalyze it again i

01:03:54.400 --> 01:03:59.520
don't want to run or control or own anything i want to figure out how to give back that control

01:03:59.520 --> 01:04:04.000
because who should decide what cancer knowledge goes in there who should decide what education etc

01:04:04.000 --> 01:04:11.120
let's talk about the second half of your of your vision which is how we originally met

01:04:11.680 --> 01:04:17.120
when you were one of the winners of the global learning x prize that ilan and tony

01:04:18.080 --> 01:04:25.120
robbins had had co-funded your your vision around education to speak to us about that

01:04:26.000 --> 01:04:29.760
yeah you know so we're deploying it kind of the winners kind of separate but

01:04:30.720 --> 01:04:35.760
every child right my entire operating system is like if you think about things in terms of

01:04:35.760 --> 01:04:41.200
the rights of child and today they have no agency and so we must respect their rights climate

01:04:41.200 --> 01:04:46.400
everything becomes a lot simpler now that we have language models on a laptop like i said you can go

01:04:46.400 --> 01:04:53.600
to lmstudio.ai download stable lm and it will run on your macbook faster than you can read

01:04:53.920 --> 01:05:00.720
uh it's crazy we can have a gpt 4 level ai from us or someone else on a smartphone or a tablet by

01:05:00.720 --> 01:05:07.040
next year one laptop per child was too early you know now we have this transformative technology

01:05:07.040 --> 01:05:11.840
you have an ai that teaches the child learns from a child are you visual auditory dyslexic

01:05:11.840 --> 01:05:17.520
that's the best data in the world for a national model but also to teach these models how to be

01:05:18.480 --> 01:05:25.360
optimistic how to this really is this really is uh the young ladies illustrator primer this

01:05:25.360 --> 01:05:32.560
really is neil stevensson's vision in that regard yeah but mel shouldn't have had to find the primer

01:05:33.200 --> 01:05:41.040
she should have had it from day one as a human right as a human right yes our school's education

01:05:41.040 --> 01:05:47.280
system our child care mixed with the social status game mix with petri dish you know they teach our

01:05:47.280 --> 01:05:53.200
kids not to have agency yes whereas they should be telling the kids yeah yeah they should be teaching

01:05:53.200 --> 01:05:58.320
this is a relic of the industrial age where everyone had to be counted and you can't measure

01:05:58.320 --> 01:06:04.240
what you can't manage so you manage the creativity and belief out of people everyone in the world can

01:06:04.240 --> 01:06:09.040
do anything why because even if you don't have that talent you can convince someone else who

01:06:09.040 --> 01:06:14.400
does have that talent but they don't believe it so they can't do it so what happens if we have an

01:06:14.480 --> 01:06:20.160
entire nation of children that have this helper that brings the right information the right time

01:06:20.160 --> 01:06:26.160
and tells them they can always believe that supports them entire world what can't you do

01:06:26.880 --> 01:06:30.560
you know then they have all of the cancer knowledge of their fingertips and all of the

01:06:30.560 --> 01:06:35.760
engineering knowledge of their fingertips and it's a constantly learning adaptive and improving system

01:06:36.560 --> 01:06:43.600
again right now almost the entire agi and ai debate is about these machine gods trained on giant

01:06:43.600 --> 01:06:49.200
supercomputers that bestow their beneficence down or may kill us or whatever what about that human

01:06:49.200 --> 01:06:54.160
operating system upgrades that is a decentralized intelligence where that kid in mongolia or malawi

01:06:54.160 --> 01:06:59.440
or wherever can make a real difference to humanity some of the contributors to our open code basis for

01:06:59.440 --> 01:07:05.440
our models are 15 years old and they just taught themselves and just happen to be their wizards

01:07:05.440 --> 01:07:10.160
you don't know in this new age right and again they should contribute to the whole because

01:07:10.160 --> 01:07:15.200
once something goes into this model of the system and again it needs the verification and other

01:07:15.200 --> 01:07:20.960
things that can be dynamic they can proliferate to everyone using that system do you think that

01:07:20.960 --> 01:07:29.360
once this capability is built it will run into blocks in different nations or do you imagine

01:07:29.360 --> 01:07:37.520
that this will become a again a human right and not all need I mean listen there's no greater gift

01:07:37.600 --> 01:07:44.640
and no greater asset you can give to a nation's populist than intelligence and education but I'm

01:07:44.640 --> 01:07:50.720
not sure every national leader wants to see that and that's why I think again there is a gap here

01:07:51.360 --> 01:07:57.120
there is a year maybe where you can go to any national leader and say I'll bring this technology

01:07:57.120 --> 01:08:01.440
to your people and I will empower the smart stupid people I want to be earned by the people

01:08:01.440 --> 01:08:06.160
and what option do they have this is positive for them what happens is that a lot of the

01:08:06.160 --> 01:08:10.960
corruption in the world is because of local maxima you know actually it's weird because

01:08:10.960 --> 01:08:15.040
unpredictable corruption is the worst predictable corruption is a bit like tax you know there's

01:08:15.040 --> 01:08:19.200
a good boast book by Frosso a job about Harvard about this and then you have taxation kicking

01:08:19.200 --> 01:08:24.800
at 14 percent if you can show them something bigger and this is clearly big they will embrace

01:08:24.800 --> 01:08:28.640
this technology and set new norms and if you correct the same across all these countries

01:08:28.640 --> 01:08:33.360
with talented individuals in each of those groups and talented individuals in each of those sectors

01:08:33.360 --> 01:08:38.400
with a shared mission even though they're separate organizations that's how you set amazing

01:08:38.400 --> 01:08:43.920
standards that's how you build a network effect and if you tie them all together with a intelligent

01:08:43.920 --> 01:08:48.080
protocol and you want to talk about tokens or speculation or ramps or anything like that

01:08:48.080 --> 01:08:54.720
but taking the best of thinking around coordination that can work that can break this open you know

01:08:56.480 --> 01:09:00.400
but it's not going to be everywhere and also when you look at the current debate the current

01:09:00.400 --> 01:09:07.200
debate is for example we can't let China have this technology and you're like what about the kids in

01:09:07.200 --> 01:09:12.800
China right well you know it's dangerous they can have a giant so under what circumstance would China

01:09:12.800 --> 01:09:18.880
ever have this technology never you know Pakistan when should they have the tech never that's really

01:09:18.880 --> 01:09:22.080
what they're kind of saying it's also self-defeating because China has a hundred million people they

01:09:22.080 --> 01:09:27.040
can use to create data sets and two exaplops of computers let's put that to the side again it's

01:09:27.040 --> 01:09:32.000
a very western oriented debate whereas actually if you go to these countries and you talk to the

01:09:32.000 --> 01:09:37.360
leaders and the family officers that have power and the people they will leapfrog in the global

01:09:37.360 --> 01:09:44.080
south to an intelligence augmentation like they let frog to mobile they want to embrace this technology

01:09:44.080 --> 01:09:49.120
and again you can set norms now versus what's going to happen is you know they will get a

01:09:49.120 --> 01:09:54.560
centralized solution they'll adopt that instead if you don't right now for hundreds of millions

01:09:54.560 --> 01:10:02.320
billions of people that's why i think again it's a crossroads um is there anybody else working

01:10:02.320 --> 01:10:08.960
towards this solution that you know of no in the in the large ai town no certainly no one

01:10:08.960 --> 01:10:13.280
credibility and again that's why i had to build these models you know and i had to kind of do

01:10:13.280 --> 01:10:18.960
this everyone's working on tiny parts of this but there is expecting emergence build it and

01:10:18.960 --> 01:10:22.560
somehow it will spread and again this is why i found it fascinating the web3 community

01:10:22.560 --> 01:10:26.400
there are good people in there and i hope to be able to unite them just like hope to unite the

01:10:26.400 --> 01:10:30.720
people in health and others and peter you've seen people working on tiny parts of this

01:10:31.440 --> 01:10:35.040
but this isn't a manhattan project where we're facing an enemy

01:10:35.040 --> 01:10:41.040
unless the enemy is ourselves you know but this does require this big global coordinated push and

01:10:41.040 --> 01:10:46.000
that's why i've tried to design this system that i believe will work because all about the talent

01:10:46.000 --> 01:10:49.760
and is multiplicative is the race against uh

01:10:52.080 --> 01:10:59.280
overly powerful centralized ai systems that achieve some version of agi is that what we're racing against

01:11:03.360 --> 01:11:09.200
yeah again we're racing against ourselves like um humans can scale through stories you have

01:11:09.200 --> 01:11:14.400
organizations you know come and join a band and come and go to oxford come and do this but then

01:11:15.360 --> 01:11:20.240
when we scaled through text text was a lossy information format and there's this poem by

01:11:20.240 --> 01:11:24.320
ginsburg how about this carthaginian demon of disorder mollock that comes in

01:11:25.360 --> 01:11:29.040
mollock comes in through the data loss our organizations are slow dummy eyes

01:11:29.680 --> 01:11:34.320
but now what's happening is they're configuring to achieve the thing of getting more and more power

01:11:34.320 --> 01:11:40.080
again corporations are technically a people under law but they're not fully formed people

01:11:40.080 --> 01:11:45.120
they eat our hopes and dreams so i believe the competition here is against those organizations

01:11:45.120 --> 01:11:50.560
consolidating too much power and creating norms that almost impossible to break so we're almost

01:11:50.560 --> 01:11:55.760
competing against ourselves and again the question is this do you believe it amplified human

01:11:55.760 --> 01:11:59.680
intelligence or do you believe in artificial general intelligence do you believe in collective

01:11:59.680 --> 01:12:06.160
intelligence or do you believe in collected intelligence who decides is this infrastructure

01:12:06.800 --> 01:12:14.640
or is this a product like so it's not like a Manhattan project against you know the soviets

01:12:14.640 --> 01:12:21.920
or anything like that but it is is require us all to come together or at least the smartest people

01:12:21.920 --> 01:12:27.040
in each of these areas from coordination to government systems to health care to education

01:12:27.600 --> 01:12:32.000
with a blank slate of how do we upgrade the human operating system the time is now

01:12:32.000 --> 01:12:37.760
it's all lost chance to do it and i and i i love you for it because i think you're you're right

01:12:38.640 --> 01:12:46.960
you were there when elan beamed in on xx video over starlink and from his airplane which is

01:12:46.960 --> 01:12:54.320
which was a fun moment and we were talking about the rate of growth and his his statement

01:12:54.960 --> 01:12:58.400
because you know recurrence why was there talking about his still his prediction of

01:12:58.880 --> 01:13:06.720
agi by 2029 and ray and written elan saying we'll have agi whatever that means by next year and

01:13:07.440 --> 01:13:14.960
and the intelligence of the entire human race by 2029 so i am i am curious what just to close

01:13:14.960 --> 01:13:23.120
out what you think about that those timelines and that potential for a super intelligent

01:13:23.680 --> 01:13:30.000
agi system that is centralized because that's the people who are building that level of power

01:13:30.000 --> 01:13:36.400
are building centralized systems they're building centralized single systems that again take our

01:13:36.400 --> 01:13:41.520
collective intelligence like all of youtube in the case of open ai clearly and other things

01:13:41.520 --> 01:13:47.440
and they package it up sell it back to us but they don't care you know these organizations

01:13:47.440 --> 01:13:51.440
are trying to build a system that will take away our freedom liberty and potentially kill us all

01:13:51.440 --> 01:13:58.240
let's be kind of fair about that and sell it to us on an incremental basis the selling to us

01:13:58.240 --> 01:14:03.680
is a complete cannot they don't care about the revenue of this again let's kind of call a spade

01:14:03.680 --> 01:14:09.520
a spade they're telling you that they're building something that could kill you and something that

01:14:09.520 --> 01:14:13.760
could remove all our freedom and liberty and they're saying it's a good thing you should back

01:14:13.760 --> 01:14:18.720
them because it's cool it's not it's actually shameful if you think about it and we should

01:14:18.720 --> 01:14:21.360
not stand for it anymore and again this is another reason i want to step aside to see you

01:14:21.360 --> 01:14:26.000
because you can't say things like that i don't cancel until i can rally so many times but realistically

01:14:26.000 --> 01:14:31.920
it's ridiculous and it should not be stood for but they're going to do it anyway because they have

01:14:31.920 --> 01:14:37.920
the political power people are scared of them so there has to be an alternative and the alternative

01:14:37.920 --> 01:14:43.680
has to be distributed intelligence when i resigned i said you can't beat centralized intelligence

01:14:43.680 --> 01:14:47.120
decentralized intelligence you're not going to beat it with this stability the least of great

01:14:47.120 --> 01:14:52.160
organization it's going to do well the only way that you can beat it to create the standard that

01:14:52.160 --> 01:14:59.520
represents humanity is decentralized intelligence it's collective intelligence and the data sets

01:14:59.520 --> 01:15:06.240
and norms from that will be ones that help children that helps people suffering that reflect our

01:15:06.240 --> 01:15:12.960
moral upstanding and the best of us and gathers the best of us to do it because if you work in

01:15:12.960 --> 01:15:17.680
healthcare if you work in education if you work in finance if you work in any of these things there's

01:15:17.680 --> 01:15:24.080
no organization for you to come and join or partner with on this there's no kind of centralized mission

01:15:24.080 --> 01:15:28.000
i have looked i've wanted to help other people i didn't want to do this to myself and i don't want

01:15:28.000 --> 01:15:32.320
it to be about being very very quickly and so i'm kind of getting it out there now i hope that i can

01:15:32.320 --> 01:15:37.840
capitalize something that then people will take forward and time is now for that because agi when

01:15:37.840 --> 01:15:42.400
it comes if it comes again there's various definitions of this why on earth do you need any

01:15:42.400 --> 01:15:49.360
knowledge workers anything that can be done via a laptop doesn't need humans and so you have concepts

01:15:49.360 --> 01:15:55.680
of ubi here you have concept like when agi comes you don't need money money is a common story is a

01:15:55.680 --> 01:16:01.520
common good we hear total post capitalist society and i think the example i think you said was a

01:16:01.600 --> 01:16:06.720
star trek versus man max you know i'm like star trek versus star wars i think there's a

01:16:10.560 --> 01:16:16.560
and so you know the sith lords and all of that um but again if you kind of look at this i don't

01:16:16.560 --> 01:16:21.520
think we need money like it's cross contextual like bartering with our ai systems representing us

01:16:22.160 --> 01:16:28.160
or it's you don't need money because you're told what to do again our governments the definition

01:16:28.160 --> 01:16:33.440
of a government is the entity with the monopoly on political violence and an agi can overtake any

01:16:33.440 --> 01:16:38.560
government that they can then control the people because again listen to it whispering look at the

01:16:38.560 --> 01:16:44.400
kind of human thing so we have this opportunity to set norms right now the way that the big labs

01:16:44.400 --> 01:16:50.400
are going to agi is likely to kill us all elon and i signed that six month pause letter because even

01:16:50.400 --> 01:16:54.480
though people like emma do an acceleration as you put all this open source aia you have to think

01:16:54.480 --> 01:16:58.720
about the other side and who's involved in that discussion and again if we build an agi as a

01:16:58.720 --> 01:17:07.120
centralized thing is windows or linux safer as infrastructure our entire internet infrastructure

01:17:07.120 --> 01:17:14.320
is built on open open can be challenged open can be augmented a monolith is like to be crazy and

01:17:14.320 --> 01:17:21.120
the way that i put this is we all night both know so many geniuses you know side effect of genius

01:17:21.200 --> 01:17:27.920
is insanity honestly we're not men geniuses are not mentally stable why would you expect an agi

01:17:27.920 --> 01:17:33.760
to be so and you're putting all your ends in one basket versus creating a complex hierarchical system

01:17:34.640 --> 01:17:39.200
that is a hive mind that's the intelligence that represents us all we should be working

01:17:39.200 --> 01:17:44.720
towards building that and it's safer it's better it achieves all the benefits that people are talking

01:17:44.720 --> 01:17:49.760
about and it's possible today do you think elon shares in this vision of a decentralized ai do

01:17:49.760 --> 01:17:54.240
you think he would play in that area and do you think any of the national leaders that you've

01:17:54.240 --> 01:18:00.560
been speaking to would support that kind of a vision as well um yeah i can't speak for elon

01:18:00.560 --> 01:18:04.640
i'll speak to him and see what he thinks and then i'll get back to you know he always says what he

01:18:04.640 --> 01:18:09.920
thinks um but you know he's immensely concerned he was one of the leaders in this area saying

01:18:09.920 --> 01:18:15.120
originally why google you know now why in microsoft open ai like it can't be centralized but it's

01:18:15.120 --> 01:18:19.680
difficult it's a difficult question how many people have a feasible solution or you've even

01:18:19.680 --> 01:18:24.000
thought about this properly you and i both know just not many and that's very sad it should be

01:18:24.000 --> 01:18:29.120
everyone thinking about this on the leader side all the leaders i've met are super happy you know

01:18:29.120 --> 01:18:37.040
because they again leaders want power they want control and all of this but generally like they

01:18:37.040 --> 01:18:42.800
want to see a bundles they're not happy with where their countries are and embracing this technology

01:18:42.800 --> 01:18:47.520
they know they can leap ahead and you know they will still have a say in all of this it's not like

01:18:47.520 --> 01:18:54.480
it's kicking them out or removing them there are still various kind of mechanisms there and ultimately

01:18:54.480 --> 01:19:00.720
improving the health education and capability of your people is not a bad thing i mean like obviously

01:19:00.720 --> 01:19:05.120
i haven't talked to the completely oppressive leaders you know maybe that'll be an interesting thing

01:19:05.120 --> 01:19:09.360
but honestly i don't want to even be talking to leaders i want to create again a system

01:19:09.440 --> 01:19:12.880
that the people of the country coming together with the franchise system

01:19:12.880 --> 01:19:17.280
can then build this technology for the good of their people in the open and not be reliant on

01:19:17.280 --> 01:19:21.200
anyone politically or any other thing like that so you don't need giant super computers for where

01:19:21.200 --> 01:19:27.840
we're going recordination need a few giant super computers yeah what's your timeline for putting

01:19:27.840 --> 01:19:34.560
out this white paper i'm working as hard as i can you know i've held i've held i've held you to this

01:19:34.560 --> 01:19:39.360
a number of times i've said get the vision out there it's getting there we're about to go off

01:19:39.360 --> 01:19:43.680
this call to a four-hour session to dictate all the various bits and pieces and again it was

01:19:43.680 --> 01:19:48.000
impossible when i was a CEO of stability there was always another fire there's always another thing

01:19:48.000 --> 01:19:52.720
i didn't have time to think you know and i hope people can take that white paper and make it better

01:19:52.720 --> 01:19:57.120
i don't have all the answers i'm just trying to capitalize something man i think after i heard you

01:19:57.120 --> 01:20:04.720
step down i wrote you a text saying congratulations yeah exactly talk to miserations time to time

01:20:04.720 --> 01:20:14.320
to feel unleashed yeah yeah uh imad thank you my friend uh thank you for sharing uh where you are

01:20:14.320 --> 01:20:20.080
what led up to this where you're going next and uh and really pulling the gloves off on discussing

01:20:20.080 --> 01:20:29.200
the idea of centralized closed ai systems and their dangers uh and the importance of of the

01:20:29.200 --> 01:20:35.200
vision that you've portrayed because i'm i'm fully supportive and fully believe that what you've laid

01:20:35.200 --> 01:20:43.760
out um is probably one of the most seen visions of ai in the future that i've heard i hope other

01:20:43.760 --> 01:20:49.440
people agree you know and they can take it forward other real heroes thank you thank you pal

