start	end	text
0	3000	Coding is changing dramatically.
3000	6000	There will be no code that doesn't use AI as post their workflow.
11000	15000	5G and Starlink, the dry kindle for this fire has been set.
15000	18000	Is this more important than 5G?
18000	19000	By orders of magnitude.
19000	20000	Orders of magnitude.
20000	25000	There is no kind of pure independent, completely rational person because we're not robots.
25000	28000	So medicine is changing dramatically.
28000	30000	Is your doctor AI enhanced?
30000	32000	Low insurance premium, lower copay.
32000	35000	That is something across every industry that's going to be happening.
35000	37000	10 years out, what is a lawyer?
37000	38000	What is an accountant?
38000	40000	What is an engineer?
40000	42000	They are all AI assisted.
42000	44000	The entire knowledge sector is transformed.
44000	47000	It enables you to do whatever you want to do.
47000	50000	Learning should be a place of positive growth and joy.
50000	51000	Where's the difference there?
51000	52000	Where's the difference?
52000	54000	And I think the key thing here is empathy.
58000	62000	We're in this beautiful home in the Hollywood Hills.
62000	68000	And a lot's happened in the last six months since we were recording our last podcast.
68000	70000	And you were on the stage at abundance 360.
70000	77000	And I think this is epicenter for a lot of people's concerns right now.
77000	81000	A transformation potentially in Holly.
81000	85000	When you were speaking about that, that it's going to change dramatically.
86000	94000	And would you say we haven't even seen a small portion of the change yet?
94000	98000	I think we are at the foot of the mountain as it were.
98000	101000	Kind of compare it to being at the iPhone 2G to 3G point.
101000	103000	We just got copy and paste.
103000	107000	We haven't seen what this technology is really capable of yet and it hasn't got everywhere.
107000	111000	It's like everyone's talking about it, but not that many people are using it.
111000	119000	There was a recent study done that showed only 17% of people had used chat GPT in the US.
119000	121000	Despite the fact you can do anyone's homework.
121000	124000	I mean, it seems incredibly a small percentage.
124000	128000	I guess because the community I hang out with everybody's using it.
128000	129000	Well, that's the thing.
129000	133000	We live in our monocultures, but then a third of the world still doesn't have internet, right?
133000	137000	And you think the first internet they get will probably be AI enhanced.
138000	140000	And then you think about this technology proliferating.
140000	143000	It's when it becomes enterprise ready.
143000	148000	Enterprise adoption, company adoption always takes a while, but by next year that happens.
148000	154000	And I think every company everywhere that has anything to do with knowledge work will implement this at scale.
154000	155000	And that's a crazy thought.
155000	161000	And when it's embedded in the things you use every day and you don't know it's part of what you're using.
161000	162000	I think that's the thing.
162000	166000	Technology, you don't need to know that it's 5G or the internet works faster.
166000	168000	You can watch movies quicker.
168000	171000	In this case, you write something in Google Docs.
171000	172000	Now they just rolled it out.
172000	177000	You can say, make this snappier and it will do it automatically.
177000	180000	Technology doesn't need to be there as technology in the front.
180000	183000	It's all about use cases and the use cases are now maturing.
183000	185000	And again, I think next year is the real takeoff.
185000	186000	But now everyone's feeling it.
186000	189000	If there's anything to do with it, something is coming.
189000	195000	In this conversation, I want to really think through how this is all affecting every industry.
195000	197000	And let's start with two industries.
197000	203000	One is journalism and the other is Hollywood, which is we're sitting in the midst of this.
203000	207000	One of the concerns I have, I know you share it, a lot of people do,
207000	215000	especially with elections coming up in 18 months, 2024, is what is truth?
215000	218000	And are we going to enter a post-truth world?
218000	223000	And can you talk about your thoughts on journalism and how AI is impacting it?
223000	226000	So I think you've seen shifts in journalism over the years,
226000	231000	but we're all familiar with kind of some of the clickbait journalism that we see now.
231000	234000	AI can obviously do clickbait better than humans.
234000	236000	And that's one kind of extreme.
236000	241000	This whole fake news, deep fake kind of stuff, that's a real concern.
241000	244000	And that's why we have authenticity mechanisms now.
244000	250000	We embed watermarking, we partner with GPT-Zero and other things to identify AI.
250000	255000	But on the other side of it, there's a real challenge coming because AI can also help with truth.
255000	260000	It can help you do proper analysis and expand out the reasoning for things.
260000	262000	It can identify biases within.
262000	265000	So journalism as it stands is caught between two things.
265000	269000	To get clicks, to get ads, they went a bit more clickbait
269000	273000	and they focus on sensationalist headlines, even if it's with unnamed sources and things.
273000	278000	On the other hand, someone's going to build an AI system, AI enhanced system,
278000	282000	that for any article you read, you can find all the background material
282000	285000	and that suddenly becomes a source of truth, so it's kind of a pincer movement.
285000	290000	And journalists and news sources will have to figure out where are you in this?
290000	292000	How do you compete to provide value?
292000	298000	Yeah, are you buzzfeed on one end, which is mostly all clickbait all the time,
298000	303000	or are you trying to be the New York Times and deliver well-researched journalism?
303000	307000	But the entity that competes with the New York Times that will come,
307000	309000	and who knows, it might be the New York Times itself,
309000	312000	can use AI to enhance great journalism,
312000	314000	write in any voice, do all these things,
314000	317000	and give fully reference facts that you can explore.
317000	320000	The other side is that we're going to trust this technology more and more,
320000	323000	just like we trust Google Maps, just like you trust other things,
323000	327000	such that it's like, why have I got a human doctor without an AI?
327000	332000	Why have I got a journalist who isn't using AI to check everything and their own implicit biases?
332000	336000	And I think that part is actually quite misunderstood as to something that's coming,
336000	341000	because, again, humans plus AI outcompete humans without AI.
341000	343000	I believe in that, and I see that.
343000	347000	And I think it's interesting in my life, and those I know,
347000	350000	AI hasn't replaced the things that I've done.
350000	353000	It hasn't actually even saved me time per se,
353000	355000	because I'm still spending the same amount of time.
355000	360000	It's allowed me to do a better job at what I want to do, which is the end product.
360000	364000	I mean, that's because you do a little bit of everything, so you always fill the gap.
364000	368000	True, I fill every moment of the time creating something for one of my companies.
368000	371000	Well, I mean, this is an open AI report that was done,
371000	378000	where they said that between 14% and 50% of tasks will be augmented by AI,
378000	381000	will be changed by AI, because, again,
381000	385000	I think a lot of the focus is on these automated systems, the terminators,
386000	390000	to the bots, whereas realistically, the way this AI will come in
390000	395000	is to help us with individual tasks, rewriting something, generating an image,
395000	398000	making a song, adjusting your speech to sound more confident.
398000	402000	Yeah, and we'll get to the dystopian conversation,
402000	407000	because I'd like to hear what you think is real versus hype.
407000	411000	I think the audience needs to understand what should they truly be concerned about,
411000	413000	and what shouldn't they...
413000	419000	I mean, that is being able to trace back and have a truth mechanism.
419000	424000	We can talk about what Elon's looking to build as well on the truth side,
424000	430000	but it's fascinating when the truth becomes blurry.
430000	433000	Yeah, and there's not always an objective truth,
433000	435000	because it depends upon your individual context, right?
435000	436000	Yeah.
436000	439000	And we didn't have the systems to be able to be comprehensive,
439000	443000	authoritative, or up-to-date enough to do that until today.
443000	451000	Well, we can actually go to the root source of the data and see, is it valid?
451000	455000	Maybe it's a blockchain-enabled validation mechanism.
455000	458000	Maybe it's got that authority, that authentication.
458000	459000	Maybe it's...
459000	463000	We mentioned Elon Community Notes on Twitter that AI enhanced,
463000	466000	that can pull from various things and show the provenance.
466000	469000	So you've got provenance, again, you've got authority,
469000	471000	you have comprehensiveness, you have up-to-dateness.
471000	475000	The future of Wikipedia is not what Wikipedia looks like today,
475000	479000	but that future becomes something that can be integrated into other things.
479000	482000	So what you'll have is, for any piece of information,
482000	485000	you'll be able to say, this is the bias from which it was said,
485000	488000	these are the compositional sources and more.
488000	492000	So for example, there's a great app that I use called Perplexity AI.
492000	493000	Okay.
493000	497000	So when you go to GPT-4 or Bing, you write stuff,
497000	498000	it doesn't give you all the sources,
498000	502000	Perplexity actually brings in all the sources at a surface level,
502000	506000	but it references why it said certain things with GPT-4.
506000	508000	That's just going to get more and more advanced
508000	510000	so you can dig into as much depth as you want
510000	512000	and ask it to rephrase things as,
512000	515000	what if that article there wasn't true that fed this?
515000	519000	Or what about this perspective if I want it to be a bit more libertarian?
519000	524000	Do you think it's possible to actually get to a fundamental truth
524000	525000	in a lot of these areas?
525000	527000	I think it depends on the area, right?
527000	528000	Some areas there are fundamental truths.
528000	530000	This happened or it didn't happen,
530000	533000	even though you see deniers of various things.
533000	536000	A lot of stuff is probabilistic when you're thinking about the future.
536000	538000	But even something like climate,
538000	542000	you see a lot of deniers of the real problem that we have
542000	544000	with it being very difficult to persuade them
544000	547000	because it becomes part of their ideology almost.
547000	549000	But with this technology, you can say,
549000	551000	look, literally here is the comprehensiveness.
551000	553000	So like Jeremy Howard and Trisha Greenley
553000	557000	did an analysis of well over 100 mask papers
557000	561000	and did a meta-analysis on the effectiveness of that for COVID.
561000	564000	And then that helped change the global discussion on masking
564000	567000	because I'm actually bothered to do a comprehensive analysis.
567000	568000	What was the result?
568000	571000	Well, the result was that masks work for respiratory diseases.
571000	576000	There are so many people that just refuse to believe that masks have any value.
576000	578000	But let's not go down that road.
578000	583000	One of the things I found interesting was the idea of a GPT model
583000	586000	being able to translate your points of view
586000	589000	for someone else to make, to receive them better.
589000	591000	Like if you're hardcore to the right
591000	595000	and you want to convince someone about your issue,
595000	601000	having chat GPT or one of Stability's products
601000	604000	generate a rewritten version of that language
604000	607000	so the person can hear it better.
607000	609000	I find that an interesting and powerful tool.
609000	611000	Yeah, I think this is the thing.
611000	614000	It's all about your individual context and what resonates with you
614000	617000	because information exists within a context.
617000	619000	So if it's going to change the state within you,
619000	621000	you need to understand your point of view.
621000	623000	So if we think of these as really talented youngsters,
623000	626000	these AIs that go a bit funny,
626000	627000	what would you want?
627000	629000	You would want someone to sit down and say,
629000	631000	this is your point of view in your context
631000	633000	and my point of view in my context.
633000	636000	Let's find some common ground and then we can work from there.
636000	638000	Much of politics isn't really about facts.
638000	641000	It's about persuasion because facts,
641000	644000	when you have diametrically a divergent context,
644000	646000	are very difficult to do.
646000	648000	So you said being able to rewrite something from one context
648000	650000	to another is important,
650000	652000	but then you have to understand the context
652000	654000	and that's what these models do really, really well.
654000	657000	We can take a piece and we can say,
657000	659000	write it as a wrap by Eminem
659000	663000	or in the style of Ulysses by James Joyce
663000	666000	and it will do that because it understands the essence of that.
666000	669000	I think people don't realize,
669000	671000	and when I just hit this point again,
671000	673000	we've talked about it somewhat,
673000	675000	our minds are neural nets,
675000	676000	our brains are neural nets,
676000	678000	100 trillion neurons
678000	680000	and everything that you bring into your mind,
680000	682000	this conversation that we're having,
682000	684000	what you're watching on the TV news,
684000	686000	newspaper, what's on your walls,
686000	688000	the people you hang out with are all constantly shaping
688000	692000	the way you see the world and shaping your mindset.
692000	696000	It's one of the things I think about in the future of news media
696000	700000	is an individual actively being able to choose
700000	702000	what mindset they want to work on.
702000	705000	I'd like to have a more optimistic mindset,
705000	707000	I'd like to have a more moonshot mindset,
707000	709000	a more an abundance mindset
709000	712000	and then being able to have that information fed to you
712000	715000	in a factual fashion that allows you to,
715000	718000	instead of what the crisis news network delivers,
718000	721000	which is a constantly negative news.
721000	723000	It's caused the dopamine effect
723000	725000	in your fight or flight response.
725000	728000	You can say make it from this point of view,
728000	729000	you don't change the facts,
729000	731000	but even the way things are worded.
731000	733000	Or balancing, right?
733000	735000	I mean, I don't need to see every murder,
735000	736000	tell me what the companies have got funded,
736000	738000	tell me the breakthroughs that occurred,
738000	740000	the science that's occurred today.
740000	742000	But there's even, again, everything you can,
742000	744000	it depends on how you portray it, right?
744000	747000	You know, there are murders, I take a murder, for example.
747000	749000	There is the facts, there is the,
749000	752000	oh my God, there'll be a million more murders.
752000	754000	There is the case that it's a very sad thing.
754000	756000	There's the case that, you know,
756000	759000	the police are working super hard to solve this
759000	760000	and we need to reach out to the families
760000	762000	and come together as a community.
762000	763000	These are all different aspects
763000	765000	for the same terrible action.
765000	766000	Yes.
766000	768000	Which have different levels of positivity,
768000	772000	negativity, clickbaitiness versus community, right?
772000	775000	Facebook did a study many years ago
775000	777000	whereby they had a hypothesis.
777000	780000	If you see sadder things on your timeline,
780000	783000	you'll post sadder things.
783000	786000	This is why independent review boards
786000	788000	are very important in ethics as well.
788000	790000	And so they did it and guess what?
790000	792000	600,000 users were enrolled
792000	794000	in that study without their knowing.
794000	796000	If you see sadder things on your timeline,
796000	799000	you post sadder things was the result.
799000	801000	So like I said, there's some real ethical considerations
801000	802000	but we know this.
802000	805000	We know that if we're always bombarded by crisis,
805000	807000	we will be in a crisis mentality.
807000	809000	We know if we surround ourselves with positive people
809000	811000	and positive messages,
811000	813000	we will have a positive mentality.
813000	816000	And it's very insidious and not insidious.
816000	818000	It's kind of almost passive the way we absorb it.
818000	820000	I love one of the facts.
820000	824000	I'm writing one of my next books on longevity practices
824000	829000	and a study on the order of 20,000 individuals.
829000	833000	Those who had an optimistic mindset
833000	835000	lived in average of 17% longer.
835000	838000	I mean, just your mindset shift.
838000	839000	17%.
839000	841000	This was true both in men and in women,
841000	843000	slightly more in women.
843000	845000	And so how you think impacts everything
845000	849000	and how you think is to a large degree
849000	852000	going to be shaped by the media
852000	854000	and AI is going to shape that.
854000	857000	So it's a powerful lever
857000	859000	that we all need to be paying attention to.
859000	860000	I think it is.
860000	862000	But then you have to consider
862000	866000	what is the plasticity, for example, of our children
866000	868000	as they grow up?
868000	870000	We're going to have nanny AIs.
870000	872000	What's that nanny going to teach?
872000	875000	Is that nanny going to teach flight, happiness, this, that?
875000	877000	What about in places like China?
877000	879000	What are they going to teach?
879000	881000	There is a huge amount of neuroplasticity
881000	885000	that will be influenced by decisions we make today.
885000	886000	Yeah.
886000	888000	I mean, listen, you have young kids.
888000	889000	I have young kids as well.
889000	891000	And I think about the fact that school today
891000	895000	is not preparing our kids anywhere near for the future.
895000	896000	Right?
896000	899000	I mean, I don't think middle school and high school
899000	902000	traditionally is preparing them for...
902000	904000	My kids are 12 right now.
904000	905000	Yeah.
905000	906000	How do you feel about that?
906000	908000	No, I mean, I think school is not fit for purpose.
908000	910000	It's a Petri dish social status game
910000	913000	and, you know, like childcare.
913000	915000	Because again, let's think about school.
915000	916000	What do you...
916000	917000	What are you taught at school?
917000	918000	You're taught competitive tests
918000	920000	because we can't capture the context of the kids.
920000	921000	Right.
921000	923000	We can't adapt to if they're visual learners,
923000	926000	auditory learners, dyslexic or otherwise.
926000	928000	And it narrows it down
928000	930000	and you're told you cannot be creative.
930000	931000	You're told you...
931000	933000	Color inside the lines.
933000	934000	Learn these facts.
934000	935000	Literally, you color inside the lines.
935000	937000	You learn these facts.
937000	940000	Every child will have an AI in the West.
940000	941000	Yes.
941000	942000	Hopefully soon in the world.
942000	944000	We'll have an AI with them as they grow up.
944000	948000	And again, is that AI a positive constructive
948000	951000	or is the AI a tiger AI?
951000	952000	Right.
952000	953000	That kind of is aggressive.
953000	954000	Get your work done.
954000	955000	Get your work done.
955000	956000	Strive harder.
956000	958000	Is it palaton AI for education?
958000	960000	Maybe that's the pivot for palaton, right?
960000	961000	There's a whole range of things,
961000	963000	but our kids are so sensitive as they grow.
963000	965000	And again, in a school environment,
965000	967000	they're told they have to be competitive
967000	968000	and there's only a few people
968000	970000	that are worthy here at the top.
970000	972000	And that's why you have clicks, sub-clicks, and others,
972000	975000	and that's reinforced by our social media now as well
975000	977000	because you need something to fill the meaning.
977000	979000	I think we have to be much more intentional and think,
979000	982000	what information do we want going to our children?
982000	984000	Like many people listening to this podcast
984000	987000	will have banned social media from our kids.
987000	989000	How do you feel about that?
989000	991000	I think that is probably a sensible thing
991000	995000	because it's a slow-down AI that optimizes for adverse things.
995000	998000	And again, it's not the fault of the social media companies,
998000	1002000	it's just how they are as the tiger and the scorpion.
1002000	1003000	Yeah.
1003000	1005000	I mean, I have not allowed my kids to have a mobile phone
1005000	1007000	and I've told them when they can afford it,
1007000	1008000	when they go to college,
1008000	1010000	but it's going to be somewhere between now and then.
1010000	1011000	But I agree.
1011000	1014000	I think social media shouldn't be part of the repertoire.
1014000	1016000	But again, what is social media?
1016000	1018000	It's kids looking for status
1018000	1021000	and trying to influence each other in that case.
1021000	1024000	It was meant to bring our communities together stronger.
1024000	1026000	Yeah, maybe perhaps early on.
1027000	1029000	Probably what we see the strongest community is actually
1029000	1032000	in video games and guilds and kind of things like that.
1032000	1034000	A lot of this is, again,
1034000	1036000	you've got X number of people posting positive things
1036000	1039000	and you're like, why is my life not positive like that?
1039000	1041000	But social media does have its advantages.
1041000	1044000	The question is, can you tease out the positive versus the negative
1044000	1047000	when you can finally customize it for each individual
1047000	1050000	or are you going to reinforce silos to the nth degree?
1050000	1052000	I'll give you a perfect example.
1052000	1054000	I was just meeting with a dear friend of mine, Keith Farazi,
1054000	1056000	who is absolutely brilliant.
1056000	1059000	And he had met last week with the King of Bhutan,
1059000	1062000	which is known for its happiness.
1062000	1063000	And they were having a conversation.
1063000	1064000	How they measure their economy.
1064000	1066000	It's how they measure their economy,
1066000	1068000	gross national happiness in that regard.
1068000	1073000	And when social media entered the country,
1073000	1075000	it began to plummet.
1075000	1079000	Teen depression and suicides began to climb.
1079000	1082000	It is a very measurable real thing.
1082000	1085000	And that's not the subject of this podcast,
1085000	1088000	but AI can do what for that area?
1088000	1090000	Well, I mean, again,
1090000	1093000	we have to think about it in terms of mental infrastructure.
1093000	1094000	I like that.
1094000	1098000	We don't have enough, like Clayton Christensen said,
1098000	1100000	infrastructure is the most efficient means
1100000	1103000	by which a society stores and distributes value.
1103000	1105000	Claude Shannon, the father of computer science said,
1105000	1108000	information is valuable in as much as it changes the state.
1108000	1112000	We do not think at all about our mental infrastructure
1112000	1114000	and what's supporting it.
1114000	1117000	If we're lucky or if, you know, we try hard,
1117000	1120000	we can build a group of supportive people around us.
1120000	1122000	And where do we go when we have issues?
1122000	1123000	We go to that group.
1123000	1126000	Yet so many people feel alone, you know,
1126000	1129000	so many people feel like, again, this rise in suicide
1129000	1131000	or they feel not good enough
1131000	1134000	because it serves the slow, dumb AI of our existing systems.
1134000	1138000	So I think that, actually, we just take some time out to think.
1138000	1142000	What information do I want going to my kids?
1142000	1145000	We have concepts like deliberative democracy,
1145000	1147000	whereby you get a group of diverse people
1147000	1150000	from different backgrounds, you give them the facts,
1150000	1153000	and they go and make a decision, just like you have jury trials.
1153000	1156000	You know, one of the most important things I think there is,
1156000	1159000	A, it's getting understanding the context of each person,
1159000	1160000	which I think AI can answer it.
1160000	1163000	B, it's just actually literally having time to think.
1163000	1166000	When was the last time you thought about your information diet
1166000	1168000	and what you're feeding yourself and your kids?
1168000	1169000	I think about it a lot.
1169000	1171000	I think because that's what I teach.
1171000	1172000	So I'm very clear.
1172000	1174000	I do not watch the TV news.
1174000	1175000	I don't even watch the newspapers.
1175000	1178000	I have very filtered information that comes to me,
1178000	1181000	which could be argued to be an echo chamber,
1181000	1186000	but you know, I'm focused on these are the scientific breakthroughs.
1186000	1187000	This is what's going on in longevity.
1187000	1190000	This what's going on in exponential tech and solving problems.
1190000	1195000	So I'm as critical about what I take into my mind
1195000	1200000	from an information source as I am what I eat.
1200000	1202000	Because you are what you eat and you eat information
1202000	1204000	and then you absorb it, right?
1204000	1207000	But then, you know, as you said, the echo chamber thing,
1207000	1212000	I believe we should also deliberately show counterpart viewpoints
1212000	1216000	as we're raising our kids and get them to argue the opposite.
1216000	1218000	I think debate is one of the most beautiful forms,
1218000	1219000	especially when you flip sides.
1219000	1223000	Organisms also grow through hysteresis.
1223000	1224000	Yes.
1224000	1226000	You know, when you're put under pressure,
1226000	1228000	when you're forced to do something out of the normal.
1228000	1231000	Otherwise, as you said, you'll become increasingly siloed.
1231000	1235000	But there are very few people again who think deliberately about this.
1235000	1237000	And it's something, again, I think you and I
1237000	1240000	probably urge all the listeners to think about,
1240000	1242000	are you challenging your priors?
1242000	1246000	Are you giving the right information diet for yourself, for your kids?
1246000	1247000	Yes.
1247000	1249000	And then thinking about this technology,
1249000	1252000	as you use a GPT-4 or Claude or Stable-LM,
1252000	1254000	how any of these things,
1254000	1257000	what if you took the article and viewed it from a different perspective?
1257000	1260000	Or what if you tied it to only be positive, the news?
1260000	1264000	There's another part too, which is we all have these cognitive biases, right?
1264000	1266000	These cognitive biases were wired into our brain
1266000	1269000	over the last, you know, hundreds of thousands of years
1269000	1271000	as an energy efficiency mechanism
1271000	1274000	because we can't process all the information coming in.
1274000	1278000	So we're biased by, is the person look like me, speak like me?
1278000	1281000	Is this recent information versus old information
1281000	1285000	paying 10 times more attention to negative information than positive information?
1285000	1291000	I can't wait to have an AI that I can flip the switch
1291000	1294000	and say turn on bias notifications.
1294000	1298000	And it says you're looking at this in a biased fashion, Peter.
1298000	1300000	Here's another way to look at it.
1300000	1302000	Yeah. And, you know, being aware of your bias,
1302000	1305000	most religions have at the core know thyself.
1305000	1307000	Yes. The nurses have done,
1307000	1310000	it's the ancient Greek, you know, I wrote my college essay on that.
1310000	1312000	But I mean, that's why it's at the core.
1312000	1316000	It's very difficult when you have the detritus of life
1316000	1319000	and all these things you're bombarded with to take time back
1319000	1322000	and really know yourself, know your own biases, understand these,
1322000	1325000	because they are part of what makes you, you're made up of the stories.
1325000	1329000	There is no kind of pure, independent, completely rational person
1329000	1331000	because we're not robots.
1331000	1334000	So it's possible in the future then for social media
1334000	1339000	with a more conscious, powerful AI,
1339000	1341000	I shouldn't use the word conscious as a different meaning here,
1341000	1345000	but AI that you feel safe having your kids do it
1345000	1348000	because it is making them happier and making them more motivated.
1348000	1351000	It is feeding them a flow of information
1351000	1353000	that's uplifting versus depressing.
1353000	1355000	Can you imagine that future?
1355000	1356000	I can imagine that future.
1356000	1359000	I can also imagine the future of Brave New World
1360000	1363000	whereby you are fed what exactly the government wants you to
1363000	1366000	and you are happy, and especially with authoritarian regimes,
1366000	1370000	you are literally the kids are grown with their AI nannies.
1370000	1371000	Of course.
1371000	1374000	And you even have the pharmaceuticals to make you extra happy
1374000	1376000	and extra neuroplastic.
1376000	1381000	So for example, you have UAE did a Falcon model open source.
1381000	1385000	It was kind of supported by Leiton from France, technologically.
1385000	1388000	You ask it about the UAE and it's like it's a wonderful place.
1388000	1390000	It's amazing in all regards.
1390000	1392000	You ask it about some of the neighbors.
1392000	1393000	It's not so nice.
1393000	1395000	This is an inherent bias within the model,
1395000	1398000	but how you can understand it versus an implicit bias
1398000	1400000	and you can put any bias as you want.
1400000	1402000	You can guide these models through reinforcement learning
1402000	1404000	to reflect what you do.
1404000	1406000	And if that's the only option,
1406000	1409000	then you will adhere to a certain world view again,
1409000	1410000	almost subconsciously.
1410000	1413000	It'll be reflected in all the products you produce
1413000	1415000	all the writings you have.
1415000	1419000	And it doesn't have to be that higher percentage change,
1419000	1423000	a small persistent change sways a lot.
1423000	1424000	Well, exactly.
1424000	1426000	I mean, like half the world is religious.
1426000	1428000	You can agree or not,
1428000	1430000	or say it follows a organized religion.
1430000	1432000	You can agree or disagree,
1432000	1436000	but I can tell you that almost every single technologist
1436000	1438000	who is leading a lot of these is like,
1438000	1440000	I don't really like religion, right?
1440000	1443000	And so the inherent bias would be to talk against religious
1443000	1444000	kind of things.
1444000	1447000	Again, I'm like, who am I to judge?
1447000	1448000	People can be, they cannot be,
1448000	1451000	but the inherent bias is refactor.
1451000	1453000	And actually it becomes very important for society
1453000	1455000	because we've seen that when about 12% of a population
1455000	1458000	changes at point of view, it flips.
1458000	1459000	Interesting.
1459000	1460000	It doesn't take that much.
1460000	1461000	It doesn't take that much
1461000	1463000	because you listen to the voices in echo chamber.
1463000	1465000	Like sometimes on Twitter, you know,
1465000	1467000	I use my block button a lot.
1467000	1468000	I'm like, you know.
1468000	1470000	Listen, I've been enjoying your tweets.
1470000	1471000	They've been really good.
1471000	1473000	And I appreciate the frequency.
1473000	1474000	Well, you know,
1474000	1477000	it's nice owning your own media channel in a way, right?
1477000	1478000	Sometimes I don't even have to have lunch
1478000	1481000	because I'm told to eat crap so many times a day, right?
1481000	1483000	That's why you have to hit the block button
1483000	1484000	because it's a little echo chamber
1484000	1488000	and a few dozen people can have that impact upon you.
1488000	1491000	I mean, it's going to be very interesting
1491000	1493000	to see the way that our adult minds
1493000	1496000	and our kids minds evolve over the next five to 10 years
1496000	1498000	with the emergence of this new, more powerful,
1498000	1500000	personalizable technology.
1500000	1501000	Yes.
1501000	1504000	It's either controlled or controlled.
1504000	1505000	Everybody, it's Peter.
1505000	1507000	I want to take a break from our episode
1507000	1510000	to talk about a health product that I love.
1510000	1511000	It was a few years ago.
1511000	1515000	I went looking for the best nutritional green drink on the market.
1515000	1518000	So I went to Whole Foods and I started looking around.
1518000	1521000	I found three shelves filled with options.
1521000	1524000	I looked at the labels and they really didn't wow me.
1524000	1528000	So I picked the top three that I thought looked decent,
1528000	1530000	brought them home, tried them, and they sucked.
1530000	1532000	First of all, they tasted awful.
1532000	1534000	And then second, nutritional facts
1534000	1536000	actually weren't very impressive.
1536000	1540000	It was then that I found a product called AG1 by Athletic Greens.
1540000	1542000	And without any question,
1542000	1546000	Athletic Greens is the best on the market by a huge margin.
1546000	1549000	First of all, it actually tastes amazing.
1549000	1552000	And second, if you look at the ingredients,
1552000	1554000	75 high quality ingredients
1554000	1557000	that deliver nutrient-dense, antioxidants,
1557000	1561000	multivitamins, pre- and probiotics, immune support, adaptogens.
1561000	1564000	I personally utilize AG1 literally every day.
1564000	1567000	I travel with an individual packet in my backpack,
1567000	1569000	sometimes in my back pocket,
1569000	1573000	and I count on it for gut health, immunity, energy,
1573000	1576000	digestion, neural support, and really healthy aging.
1576000	1579000	So if you want to take ownership of your health,
1579000	1581000	today is a good day to start.
1581000	1584000	Athletic Greens is giving you a free one-year supply of vitamin D
1584000	1588000	and five of these travel packs with your first purchase.
1588000	1591000	So go to athleticgreens.com backslash moonshots.
1591000	1595000	That's athleticgreens.com backslash moonshots.
1595000	1597000	Check it out. You'll thank me.
1597000	1600000	Without question, this is the best green drink product,
1600000	1603000	the most nutritious, the most flavorful I've found.
1603000	1605000	All right, let's go back to the episode.
1605000	1607000	So we're sitting here in the middle of Hollywood.
1607000	1609000	I can see the beautiful Hollywood Hills.
1609000	1611000	The Hollywood signs around here are someplace.
1611000	1613000	Just over there, okay.
1613000	1617000	And the news today is Green Actors Guild.
1617000	1619000	Everyone's gone on strike.
1619000	1624000	There is great fear, pain, concern.
1624000	1629000	And, you know, you called it when we spoke six months ago.
1629000	1633000	What's going on? How do you view it?
1633000	1635000	And where's it going?
1635000	1638000	So I think the advances in media or artificial intelligence
1638000	1639000	have been huge.
1639000	1643000	You've now got the Drake AI song or my favorite Ice Matrix,
1643000	1646000	where the Matrix characters sing Ice Ice Baby.
1646000	1649000	Yes.
1649000	1651000	You've got real-time rigging.
1651000	1653000	You have real-time special effects.
1653000	1656000	You've got high definition creation of anything.
1656000	1657000	What does that mean?
1657000	1660000	It means that the whole industry is about to be disrupted
1660000	1664000	because the cost of production reduces.
1664000	1666000	It was reducing in some ways anyway.
1666000	1669000	And we've seen this move where the cost of consumption
1669000	1671000	went to zero with Napster and Spotify.
1671000	1673000	Then the cost of creation started going to zero
1673000	1676000	with Snapchat and TikTok.
1676000	1680000	And now we have a question of what are the defaults going to be now?
1680000	1685000	And consumers only have one limited currency
1685000	1686000	and that's their attention.
1686000	1687000	I think so.
1687000	1689000	But consumers are willing to pay for attention.
1689000	1691000	They're willing to pay for quality.
1691000	1694000	Well, either premium media or, you know, otherwise.
1694000	1697000	Because video games, for example,
1697000	1700000	started as a $70 billion industry 10 years ago.
1700000	1704000	The average score went from 69% to 74% on Metacritic
1704000	1705000	over that period.
1705000	1708000	And now they're $180 billion.
1708000	1709000	Movies...
1709000	1711000	I pay for some of that with my 12-year-olds.
1711000	1712000	Yeah.
1712000	1716000	Movies went from $40 billion to $50 billion,
1716000	1719000	but the average IMDb movie rating of the last 10 years
1719000	1720000	has been 6.4.
1720000	1722000	It has not changed.
1722000	1725000	So you're not producing more and so they're not consuming more.
1725000	1726000	So there's a question.
1726000	1729000	Will this technology enable an increase in quality
1729000	1731000	because it raises the bar for everyone?
1731000	1732000	Yes.
1732000	1733000	There are more movie makers,
1733000	1735000	so they're more excellent movie makers,
1735000	1737000	and the best movie makers have become even more excellent.
1737000	1740000	And we've democratized the tools from an iPhone
1740000	1745000	to, you know, the tools on your Mac.
1745000	1749000	And, of course, YouTube was the first major disruption of the...
1749000	1750000	It was.
1750000	1753000	Something like MrBeast has a much bigger audience than CNN now.
1753000	1754000	Yeah.
1754000	1756000	But that comes down to what is the key point here?
1756000	1758000	Distribution.
1758000	1760000	You can make good stories, but if no one hears them...
1760000	1761000	Right.
1761000	1765000	And that's what Disney has as its benefited distribution
1765000	1770000	to its theme parks, to its products, to its channels.
1770000	1771000	It creates a shelling point,
1771000	1774000	but then if we all have our own individualized AIs,
1774000	1778000	they can find us what we need and sift through the crowd.
1778000	1781000	Maybe a whole of distribution flips on its head in five years as well.
1781000	1782000	So I can't imagine...
1782000	1787000	You know, I keep thinking that the future of movie consumption
1787000	1791000	is me speaking to my version of Jarvis and saying,
1791000	1794000	you know, I'd like a comedy, I'd like it starring me
1794000	1798000	and three friends of mine and somebody else
1798000	1803000	and for 90 minutes and set it in, you know, some setting and go.
1803000	1807000	And have it auto-generate a compelling story
1807000	1815000	that is either because I'm involved in it,
1815000	1817000	I'm enraptured in it,
1817000	1820000	or because my favorite stars are all together in an unlikely place.
1820000	1823000	How far are we from that kind of future?
1823000	1825000	I'd say probably three to five years.
1825000	1829000	So in that case, it's not going to be cheap, but it'll be there.
1829000	1830000	And then it'll get cheaper and cheaper.
1830000	1832000	In that case, there is no distribution needed.
1832000	1834000	A person calls up whatever they want.
1834000	1837000	I think there is distribution needed, but in a different way.
1837000	1841000	Music, we have a variety of different music sites,
1841000	1843000	but how do musicians make their money now?
1843000	1844000	It's not Spotify.
1844000	1846000	A million views gets you a few thousand dollars.
1846000	1849000	T-shirts, merchandising, global stories,
1849000	1853000	because there is that future of the Wally type of fat guy
1853000	1855000	sitting with his VR headset.
1855000	1856000	It's kind of depressing.
1856000	1858000	Just like, you know, the Apple Vision Pro adverts,
1858000	1860000	I found kind of depressing when his kids are right there
1860000	1861000	and just puts them on.
1861000	1864000	Quite a bit dystopian to me.
1864000	1867000	I think it's more a case of there are certain stories
1867000	1869000	that everyone wants to talk about.
1869000	1871000	Like here in Hollywood, what do we have this week?
1871000	1873000	We have Oppenheimer and Barbie.
1873000	1875000	Oh my God.
1875000	1876000	Talking bags.
1876000	1878000	I'm looking for an Oppenheimer.
1878000	1879000	I will not watch Barbie.
1879000	1880000	Sorry.
1880000	1881000	Would you watch Barbieheimer?
1881000	1885000	Only, no, I won't go there.
1885000	1887000	Well, I mean, we can take it.
1887000	1889000	We can put the scripts into Claude and see what it comes up with.
1889000	1890000	That would be hilarious.
1890000	1892000	Yeah, hilarious kind of Barbie.
1892000	1895000	But again, like people are talking about the Barbie movie
1895000	1897000	because it's, you know, not,
1897000	1899000	she comes to the real world and she has challenges.
1899000	1900000	It'll be a hit.
1900000	1904000	People talk about Oppenheimer because again, it will be a hit.
1904000	1905000	These are produced hits.
1905000	1906000	These are produced hits.
1906000	1907000	Yeah.
1907000	1909000	Just like I saw BTS with my daughter.
1909000	1910000	It's not BTS, they're Blackpink.
1910000	1912000	Oh, gosh, she'll kill me.
1912000	1914000	In Hyde Park a few weeks ago,
1914000	1916000	the biggest K-pop band out of Korea.
1916000	1917000	Right.
1918000	1920000	Completely manufactured, but lots of fun.
1920000	1923000	So on the one hand, what you have is what you described,
1923000	1925000	your personalized things.
1925000	1927000	That's kind of like McDonald's.
1927000	1928000	What's the job to be done?
1928000	1930000	The job is comfort.
1930000	1932000	The job isn't to listen to someone else's story
1932000	1934000	and expand from there.
1934000	1937000	It isn't a produced Michelin star meal
1937000	1939000	or just a nice restaurant.
1939000	1941000	One that you can talk about to others like,
1941000	1943000	you can cook ingredients yourself at home as well.
1943000	1944000	Sure.
1944000	1946000	But humans do like these bigger stories,
1946000	1950000	but then the nature of funding of musicians changed.
1950000	1952000	It was about merchandise.
1952000	1956000	It was about kind of a lot of this other stuff
1956000	1959000	around tours.
1959000	1961000	And so I think the nature of movies might change.
1961000	1962000	The business model has changed.
1962000	1964000	I mean, that's the most interesting thing
1964000	1966000	about exponential technology
1966000	1968000	is it's changing the business models.
1968000	1972000	And I keep on, you know, advising my entrepreneurs.
1972000	1975000	It's reinvent your business models more than anything else.
1975000	1978000	You have to always look across the landscape,
1978000	1982000	where is the value peaks?
1982000	1984000	And then you're sitting there
1984000	1986000	and you're intermediating something
1986000	1988000	and you're offering service value.
1988000	1990000	You know, like there was that great quote,
1990000	1993000	who is it from the CEO of Netscape?
1993000	1996000	All values created by aggregation or disaggregation,
1996000	1998000	bundling or unbundling.
1998000	2001000	And you think about how the landscape is going to change now
2001000	2003000	as intelligences move to the edge.
2003000	2006000	What was once in the hands of the studios
2006000	2008000	and the high priests of media
2008000	2010000	suddenly gets pushed to the edge.
2010000	2012000	Whereas value then, it changes it, it flips it.
2012000	2014000	So let's go to what's going on right now.
2014000	2018000	So the screen actors guild is on strike because of why?
2018000	2021000	Screen actors, I mean, it's better wages in general,
2021000	2024000	standard stuff, but now there's this rapidly emerging fear
2024000	2026000	of artificial intelligence.
2026000	2030000	So one of the proposals that came in from the other side
2030000	2034000	was that basically all the extras,
2034000	2036000	all the actors, they sign away their rights
2036000	2038000	so they could be used in AI.
2038000	2041000	So they get a day of wage to get scanned
2041000	2046000	and then they can be used for the rest of that studio's life
2046000	2048000	in producing background actors.
2048000	2050000	And they were looking at this like,
2050000	2052000	oh my God, wait, you can do that?
2052000	2055000	People must have even realized you can do that.
2055000	2057000	You know, script writers are saying
2057000	2059000	no AI generated scripts,
2059000	2062000	which again is a bit weird when they're all using Grammarly
2062000	2064000	and things like that, which was AI.
2064000	2066000	How are you ever going to tell?
2066000	2068000	Where do you draw the line?
2068000	2069000	Where do you draw the line?
2069000	2071000	Because this is a technology that's coming so fast
2071000	2076000	and is so good that it's almost not like technology at all.
2076000	2078000	It's just very natural the way it emerges.
2078000	2081000	And so you will get to some sort of agreement
2081000	2083000	because the big actors are kind of there,
2083000	2086000	but the defaults that are set now reverberate.
2086000	2089000	So I think that's an important point that you just made.
2089000	2096000	The decisions, the policies that we create today
2096000	2099000	are going to take us down one path or another
2099000	2101000	for the decades to come.
2101000	2103000	Yeah, it will affect the whole of Hollywood,
2103000	2106000	what's decided in this moment here,
2106000	2109000	because there won't be another renegotiation for a while.
2109000	2113000	And so again, how does it act to create value?
2113000	2116000	A top actor has a following,
2116000	2118000	but up-and-comers, how did they break through?
2118000	2120000	What was the apprenticeship to him?
2120000	2122000	What does a movie look like in five years?
2122000	2126000	Even if you agree as Hollywood, not to have any AI.
2126000	2128000	Let's just say you have this kind of
2128000	2130000	Dune-style bit Larry and Jihad and say,
2130000	2132000	No AI, you know?
2132000	2134000	And someone will make a movie about no AI in Hollywood.
2134000	2137000	What do you do when the Chinese film studios?
2137000	2139000	Start releasing product.
2139000	2141000	Start releasing product faster than anything.
2141000	2143000	You can make five dreams out of it.
2143000	2145000	In every language out there?
2145000	2146000	Literally every language.
2146000	2147000	We have technology now.
2147000	2149000	And again, maybe this is part of what we're doing.
2149000	2150000	What is possible now?
2150000	2153000	We can translate Peter's voice into just about any language
2153000	2155000	with his voice, so it's not a voice actor.
2155000	2157000	And match my lips and movements, exactly.
2157000	2159000	Match your lips and movements, exactly.
2159000	2162000	I'm sure the podcast will be in every language by next year.
2162000	2164000	And again, it will be in our voices.
2164000	2166000	We can make our voices sound more confident.
2166000	2169000	We can take his mannerisms right now
2169000	2171000	and transplant them onto my mannerisms so we match,
2171000	2174000	so we can reshoot scenes with style.
2174000	2175000	Yeah.
2175000	2177000	We can turn him into a robot in a few minutes.
2177000	2180000	And in fact, maybe we'll do that in the film of the post.
2180000	2183000	And so we'll have that scene of him becoming a robot.
2183000	2185000	These are all technologies that are here now,
2185000	2187000	and it transforms fundamentally the nature of filmmaking
2187000	2189000	because you only need one shot.
2189000	2193000	But don't the film actors who are, you know,
2193000	2197000	standing up for their rights to not have them
2197000	2201000	basically demonetized and digitized,
2201000	2203000	the other option is for Hollywood
2203000	2205000	to just create complete artificial characters
2205000	2206000	that they fully own.
2206000	2207000	Yeah.
2207000	2211000	And you've seen this already with some of the kind of V-loggers
2211000	2214000	and, you know, others that have emerged out of Asia,
2214000	2216000	in particular, fully AI-generated characters.
2216000	2217000	Yeah.
2217000	2219000	And you can have entire mythos around them.
2219000	2222000	And you can say, make it the most attractive Italian guy
2222000	2225000	I've ever seen that's broody and this and that.
2225000	2226000	Tell it from a human.
2226000	2227000	Yeah.
2227000	2229000	I mean, like, these characters can be completely new.
2229000	2231000	And it's far more profitable for the studio
2231000	2234000	to use that digital actor.
2234000	2239000	So there is a disruption coming at every level.
2239000	2243000	Because the research to revenue pipeline has become so tight
2243000	2245000	and it sets off a race condition,
2245000	2248000	whereby you could only produce two movies a year,
2248000	2250000	suddenly you can produce 20.
2250000	2254000	And then you can actually, like we do AB testing
2254000	2256000	in subject headlines,
2256000	2259000	you could create 30 variants of the movie
2259000	2261000	and see which one actually is the best.
2261000	2262000	Yeah.
2262000	2265000	And, I mean, again, you can say, make it more,
2265000	2267000	make that speech roar and more emotional.
2267000	2269000	It will adjust the voice to make it roar
2269000	2271000	and more emotional, right?
2271000	2274000	And because you're using such large data sets,
2274000	2277000	you know, and we, so we made all our data sets open
2277000	2279000	and then we allowed opt-out.
2279000	2281000	We're the only company in the world to allow opt-out
2281000	2282000	because we thought it was the right thing to do.
2282000	2284000	So we had 169 million images
2284000	2286000	opted out of our image data sets.
2286000	2288000	For music, because it's different copyright laws,
2288000	2291000	we have one of the first commercially licensed music models
2291000	2292000	coming out.
2292000	2293000	So respect for that.
2293000	2297000	But if I'm an artist and I go to the Louvre
2297000	2302000	to be inspired and then go back and paint,
2302000	2306000	and I've been inspired by Da Vinci
2306000	2312000	and I start painting in a style like Da Vinci,
2312000	2313000	where's the difference there?
2313000	2314000	Where's the difference?
2314000	2315000	And this is the reality.
2315000	2318000	Even though we've done that, by next year,
2318000	2320000	probably by the end of the year,
2320000	2324000	you will have models that have zero scraped data
2324000	2325000	or human art.
2325000	2326000	They will all be synthetic.
2326000	2327000	Yeah.
2327000	2329000	And you'll be able to bring your art to it.
2329000	2332000	So there's something Google just released called Style Drop.
2332000	2335000	There's Hyper Dream, there's Hyper Networks,
2335000	2337000	you can take one picture of yourself
2337000	2340000	and the entire model trains to be able to put you into anything,
2340000	2341000	even if you're not in the model.
2341000	2343000	It used to take minutes, hours,
2343000	2345000	now it's just one picture.
2345000	2348000	Similarly, you can bring any style
2348000	2350000	and it will just mimic and imitate that style.
2350000	2353000	And so all of a sudden, the models themselves,
2353000	2355000	it doesn't matter what they're trained on
2355000	2358000	because there's no human endeavor in those models.
2358000	2361000	And then things like compensation for artists and others,
2361000	2363000	as you said, become a bit mute
2363000	2364000	because all of a sudden,
2364000	2367000	you have these amazing stories told by
2367000	2370000	really convincing amazing actors
2370000	2372000	who may or may not exist
2372000	2374000	and how are you ever going to tell the difference?
2374000	2375000	So what's your advice?
2375000	2376000	Let's parse it here.
2376000	2379000	On one side, what's your advice for Hollywood
2379000	2381000	and for actors?
2381000	2383000	And on the other side,
2383000	2385000	I want to ask your advice for artists.
2385000	2388000	This is about mindset.
2388000	2392000	This is coming at us at extraordinary speed.
2392000	2394000	There's no stopping it, right?
2394000	2399000	There's no slowing it down.
2399000	2402000	And so you've got to deal with reality.
2402000	2404000	You're dealing with reality, again, it's inevitable.
2404000	2407000	Even if, again, Hollywood says no AI,
2407000	2409000	the AI is coming from around the world.
2409000	2410000	So what do you do?
2410000	2411000	You think, oh, well,
2411000	2414000	my audience suddenly became the whole world.
2414000	2416000	That's a big deal.
2416000	2419000	You're like, what am I actually known for?
2419000	2420000	Is my acting skills?
2420000	2422000	Well, I will still get these things.
2422000	2424000	You're an up and coming actor.
2424000	2425000	You say, I need to build community.
2425000	2428000	I need to kind of show off something more than that.
2428000	2430000	Because again, my acting skills in some areas
2430000	2431000	can be transplanted,
2431000	2433000	but what about real life shows?
2433000	2435000	What about these things?
2435000	2437000	It does throw up the entire thing and adjust it,
2437000	2440000	but then musicians have had to have that adjustment.
2440000	2442000	They used to be able to make money on their LPs,
2442000	2443000	and then all of a sudden,
2443000	2446000	they had the naps to Spotify moments.
2446000	2448000	There is more protection in music as well
2448000	2451000	because you basically, according to
2451000	2455000	the Robin Thicke versus Marvin Gaye case,
2455000	2458000	there is an element of style protection in there
2458000	2460000	that doesn't exist in visual media.
2460000	2463000	And probably won't because the other part of this is
2463000	2466000	if you're expecting governments to regulate,
2466000	2471000	how can they, when there is a global competition going on,
2471000	2474000	they will lose competitiveness to other countries
2474000	2476000	and you'll have regulatory arbitrage.
2476000	2480000	Yes, and that is something across
2480000	2482000	every industry that's going to be happening.
2482000	2485000	Yeah, I think the concept of united artists,
2485000	2488000	it was originally a collective of all the artists,
2488000	2490000	that makes a lot of sense now.
2490000	2493000	I think you have to think about an element of collectivism
2493000	2496000	to share the excess profits because what's going to happen is
2496000	2498000	movies will get cheaper, profits will go up.
2498000	2501000	You need to support each other as a community here
2501000	2503000	and think again, as a community,
2503000	2506000	this is our story for the next one, three, five, ten years
2506000	2510000	because all of this is going to happen quicker
2510000	2514000	than it takes to make the new Avengers movie.
2514000	2517000	And quicker than regulators are able to regulate.
2517000	2521000	And again, the regulators almost certainly won't regulate
2521000	2524000	because they will start falling behind
2524000	2526000	their competitor countries.
2526000	2529000	I mean, if we look at the internet itself as it,
2529000	2532000	the media industry never expected the internet
2532000	2534000	to have the disruptive impact it had.
2534000	2537000	And had it known, it probably would have tried to get regulators
2537000	2539000	to have slowed it down or blocked it.
2539000	2541000	Yeah, the speed is too much, but then also,
2541000	2543000	again, I gave the example earlier,
2543000	2545000	the video game industry has gone from 70 billion to 180 billion
2545000	2547000	over the next ten years.
2547000	2549000	Can we increase quality?
2549000	2551000	Interactivity.
2551000	2553000	Because games are media as well.
2553000	2555000	The media industry has increased in size.
2555000	2558000	The way that value has gone has been redistributed.
2558000	2561000	Value will be redistributed again now.
2561000	2564000	And again, it's like, what is an AI-enhanced actor?
2564000	2569000	If you're an actor, what's an AI-enhanced photographer filming?
2569000	2573000	Think about your jobs, the tasks that you do,
2573000	2575000	and what can be augmented if you had a bunch
2575000	2578000	of really talented youngsters working for you, right?
2578000	2581000	You could do more, you could be more.
2581000	2584000	But then it means the bar is just going to keep on raising.
2584000	2588000	Let's turn to a different industry that's going to change.
2588000	2591000	We had this conversation in our last podcast
2591000	2596000	and on stage at abundance 360, which is coders.
2596000	2599000	Coding is changing dramatically.
2599000	2601000	What are your thoughts there?
2601000	2605000	So, when I started as a programmer, gosh, 22 years ago,
2605000	2607000	I was writing enterprise-level assembly code
2607000	2609000	for voice-over IP software.
2609000	2610000	Wow.
2610000	2611000	I had to switch.
2611000	2613000	That's some of the largest chunks of code out there.
2613000	2615000	Yeah, it's very low-level code.
2615000	2617000	We didn't have GitHub, we just got Subversion
2617000	2618000	here.
2618000	2620000	Programming these days is a lot like Lego,
2620000	2622000	because what you have is kind of you have a very low-level,
2622000	2624000	but then you have levels of abstraction
2624000	2627000	until you get to PyTorch and some of these other languages.
2627000	2629000	So, you have to compile lots of different libraries
2629000	2632000	because you're making it easier and easier.
2632000	2635000	Human words are just the next level of abstraction there.
2635000	2638000	But the nature of coding is going to change.
2638000	2642000	And so, the coders that are coding traditionally today
2642000	2646000	around the world, how will they be using
2647000	2651000	and working in this industry two to five years from now?
2651000	2654000	Well, again, there will be no coder
2654000	2656000	that doesn't use AI as part of their workflow.
2656000	2658000	Okay, I mean, I think that's the important thing.
2658000	2660000	It's not like coders are going to go away,
2660000	2662000	they're going to be using a new set of tools.
2662000	2664000	The expectations will rise,
2664000	2667000	the amount of debugging unit testing,
2667000	2669000	all of these things will decrease,
2669000	2672000	because how much time did coders actually spend architecting?
2672000	2673000	Very little up front.
2673000	2675000	It's more about understanding information flows
2675000	2677000	about architecting these things.
2677000	2679000	It's about having feedback loops
2679000	2681000	to understand customer requirements.
2681000	2684000	Databricks is a $38 billion company.
2684000	2685000	There's Data Lakes.
2685000	2687000	So, it takes your data to organize
2687000	2689000	it allows you to write structured queries.
2689000	2691000	You have to write queries.
2691000	2694000	Now, you just talk to it and it just does it.
2694000	2696000	Microsoft will introduce the same thing.
2696000	2699000	I mean, I can't wait for that in the field of medicine,
2699000	2701000	which I want to talk about next.
2701000	2703000	But you said something earlier where you can imagine
2703000	2705000	there can be a billion coders in the future.
2705000	2711000	Yeah, because all the barriers to creating programs disappear.
2711000	2713000	So, it's not that there are no programs,
2713000	2715000	there's no programs that we know it,
2715000	2717000	because there's a billion programmers.
2717000	2719000	Everyone is a programmer, nobody's a programmer in a way.
2719000	2721000	Because it just becomes a matter of course.
2721000	2724000	I want to make software that does something
2724000	2726000	and reacts in these ways
2726000	2729000	and looks like this and adapts like this.
2729000	2731000	Then it comes to you and you're like,
2731000	2732000	no, that's not quite right.
2732000	2733000	I want this moved over.
2733000	2735000	It happens almost live, this feedback loop.
2735000	2738000	It's as we talk to chat GPT-4,
2738000	2741000	creating a paragraph that describes something we want.
2741000	2742000	We modify it.
2742000	2745000	I mean, like, chat GPT-4 is a good example
2745000	2750000	because to write an integration to something like chat GPT-4,
2750000	2753000	you used to take days, weeks, an API,
2753000	2755000	an application protocol kind of interface.
2755000	2758000	Now, what you do is actually you tell it the schema.
2758000	2760000	You tell it kind of what you should do
2760000	2762000	and it writes it automatically,
2762000	2764000	literally within like a few minutes.
2764000	2766000	And then in a few hours, you've integrated into it.
2766000	2770000	I've been talking about a future where we all have Jarvis.
2770000	2772000	Iron Man, I love Iron Man as a movie.
2772000	2773000	It's one of my favorites.
2773000	2776000	And, you know, Jarvis is basically your personal AI
2776000	2778000	that is, it's a software shell.
2778000	2780000	It interfaces between you and the rest of the world.
2780000	2782000	You ask Jarvis to do something
2782000	2787000	and it knows how to, the toolpaths on a 3D printer
2787000	2790000	you can hop into a jet and interface Jarvis
2790000	2793000	with the jet's computational system
2793000	2795000	and it'll fly the jet for you.
2795000	2798000	I can't imagine that that is really far from now.
2798000	2801000	Yeah, let's hope that doesn't fly planes just quite yet.
2801000	2804000	Okay, well, I'll put the jet aircraft aside.
2804000	2809000	But the ability to, for it to become your best friend
2809000	2812000	and confidant, know your needs and desires,
2812000	2815000	shape the world to your comfort
2815000	2817000	and being able to help you.
2817000	2819000	It's the ultimate user interface.
2819000	2822000	Well, I mean, this is why a lot of the chatbots,
2822000	2824000	character AI and others have become so popular
2824000	2825000	because it'll never judge you
2825000	2828000	and it's approaching that human level now.
2828000	2831000	You know, and again, it is the ultimate interfaces,
2831000	2834000	maybe chats, but it's more chats, chatting context.
2834000	2837000	It's understanding you holistically.
2837000	2839000	No human could do that because, you know,
2839000	2840000	even if you hire a whole team,
2840000	2842000	they're not going to be with you 24-7.
2842000	2844000	This will be with you 24-7.
2844000	2846000	I think the key thing here is empathy.
2846000	2847000	Yes.
2847000	2850000	Because jumping head bit to medicine.
2850000	2852000	Google had their MedPalm 2 model.
2852000	2854000	The papers just come out in nature.
2854000	2858000	A, it outperforms doctors on clinical diagnosis,
2858000	2861000	which is crazy for a few hundred gigabytes of a file.
2861000	2862000	Yeah.
2862000	2865000	B, it outperforms doctors on scores of empathy.
2865000	2869000	I found that amazing and totally logical.
2869000	2870000	It doesn't judge you.
2870000	2872000	It doesn't judge you, but then, you know,
2872000	2874000	doctors split a million ways and they're tired
2874000	2876000	and they're grumpy or this or that.
2876000	2879000	Some of us get good doctors, most of us don't.
2879000	2881000	Some of us get good teachers.
2881000	2884000	I'm not saying education is bad because of the teachers.
2884000	2886000	So many teachers try so hard,
2886000	2890000	but their attention is split 20 ways and they're underpaid.
2890000	2892000	You know, I'm not saying that programs
2892000	2894000	that would be nature programming will change
2894000	2895000	because programmers are bad.
2895000	2897000	There's so many hard-working programmers.
2897000	2900000	It's just, again, the nature of these things will change
2900000	2903000	when you can scale expertise.
2903000	2906000	And everyone has expertise available to them on tap.
2906000	2909000	I've been on the stage, you know, just pounding my fist
2909000	2911000	saying, listen, it's going to become malpractice
2911000	2914000	to diagnose someone without an AI in the loop
2914000	2916000	within five years' time.
2916000	2920000	And probably in some areas, it'll be inappropriate
2920000	2923000	to not yet illegal to.
2923000	2925000	And then at some point soon after that,
2925000	2928000	the best surgeons in the world are going to be humanoid robots
2928000	2936000	that have every possible, you know, atrial variation,
2936000	2940000	every possible, you know, history of surgery
2940000	2942000	and they can see an infrared and ultraviolet
2942000	2945000	and they haven't had an argument that morning
2945000	2948000	with their husband or wife and it becomes the best.
2948000	2953000	And these are demonetizing and democratizing forces for health.
2953000	2955000	There must be deflationary as well,
2955000	2957000	but I know I agree completely with this
2957000	2960000	but ultimately what's going to kick it off is,
2960000	2962000	is your doctor AI enhanced?
2962000	2963000	Yeah.
2963000	2965000	Low insurance premium, lower copay?
2965000	2966000	Yes.
2966000	2969000	Because there will be real economic incentives.
2969000	2971000	Has this been cross-checked by the technology?
2971000	2972000	Yeah.
2972000	2974000	Reduce the cost?
2974000	2977000	My favorite subject is, you know, you probably know this,
2977000	2980000	how many medical articles are written in journals every day?
2980000	2981000	I don't know.
2981000	2982000	It's 7,000.
2982000	2983000	Wow.
2983000	2986000	And it's like, how many is your doctor read today?
2986000	2988000	You know, and there may be that one breakthrough
2988000	2990000	that happened this morning that is the key
2990000	2992000	for your diagnostics.
2992000	2994000	But I mean, even if they've read it, right,
2994000	2997000	like absorbing it is one thing, having the mental models,
2997000	2999000	these are kind of something else.
2999000	3001000	This is why comprehensive authority is not state,
3001000	3003000	which is what this technology allows to happen.
3003000	3006000	As you said, things like, we're already seeing some surgeries,
3006000	3009000	can you done better by robot surgeons than human surgeons?
3009000	3010000	It'll be all surgeries.
3010000	3012000	Yeah, it will be all surgeries soon enough.
3012000	3014000	Like the robotics advancements we've seen,
3014000	3016000	this actually goes back to your point of, you know,
3016000	3019000	the artist going to the Louvre and seeing the Da Vinci
3019000	3021000	and then taking inspiration from that.
3021000	3023000	What are we going to do with like all of these
3023000	3026000	Optimus robots and 1kx robots and others?
3026000	3028000	Are they going to have to shut their eyes
3028000	3030000	when they see anything copyrighted?
3030000	3032000	You're just going to have accidents everywhere.
3032000	3034000	They're like running into each other.
3034000	3036000	Everything's going to be black lined out.
3036000	3041000	So medicine is changing dramatically.
3041000	3044000	What other field are you seeing and saying
3044000	3046000	people need to wake up and see what's coming?
3046000	3049000	So, I mean, medicine, education
3049000	3051000	are kind of the two big ones, I think.
3051000	3052000	I agree.
3052000	3053000	But can we move this to the side right now?
3053000	3054000	Again, we've seen programming,
3054000	3056000	the entire nature program will change media.
3056000	3058000	The entire nature of media will change
3058000	3061000	from journalism to filmmaking.
3061000	3066000	But anything that basically you could do
3066000	3069000	with someone from Asia on the other side
3069000	3071000	on a computer screen will change.
3071000	3073000	Let's talk about education.
3073000	3077000	Because today, education hasn't changed
3077000	3081000	since the one-room classroom.
3081000	3084000	Half the kids are lost, half the kids are bored.
3084000	3086000	You're teaching to a series of tests.
3086000	3090000	You're teaching for an industrial era world.
3090000	3093000	And people learn differently.
3093000	3098000	People have visual and auditory and tactile learning skills.
3098000	3102000	And let's face it, we don't celebrate our teachers.
3102000	3103000	We don't pay them well,
3103000	3106000	and we don't have the best of them coming into the classroom.
3106000	3108000	And they're sad.
3108000	3110000	I mean, this is the thing.
3110000	3113000	There are happy classrooms with happy teachers
3113000	3114000	and other things,
3114000	3118000	but learning should be a place of positive growth and joy.
3118000	3120000	It should be fun to learn.
3120000	3121000	And it should be fun to teach.
3121000	3122000	And fun to teach, yes.
3122000	3124000	I think this is both of them.
3124000	3126000	Because what is the nature of a teacher in 5, 10 years?
3126000	3128000	Let's say 10 years.
3128000	3132000	Again, 10 years is the crazy short period of time.
3132000	3136000	I mean, when I asked you this question last time,
3136000	3139000	how far out can you predict what's likely to come?
3139000	3143000	What's your singularity boundary condition?
3143000	3144000	I'm curious.
3144000	3146000	What are you seeing as?
3146000	3148000	3 or 5 years.
3148000	3150000	I go to Dubai and I'm on stage,
3150000	3154000	and can you talk to us about what the world's going to be like in 2050?
3154000	3155000	My answer is no.
3155000	3157000	I can barely talk about 2030.
3157000	3159000	It's everything ever at once.
3159000	3161000	Lots of S-curves, acceleration.
3161000	3163000	These are inevitable now.
3163000	3165000	Now we've broken through these things.
3165000	3168000	I mean, OpenAI now have put 20% of their stuff to alignment
3168000	3171000	because they're basically saying that their view is 5 years out.
3171000	3173000	Elon Musk just said 6 years out,
3173000	3176000	but then Elon's relationship with time is always a bit fun.
3176000	3178000	Just like self-driving cars.
3178000	3180000	But self-driving cars are literally here now.
3180000	3183000	You can get in one and it will drive you around.
3183000	3185000	San Francisco or London or Germany.
3185000	3186000	Waymore will do that for you.
3186000	3187000	Not my Tesla.
3187000	3188000	Not your Tesla.
3188000	3189000	I know.
3189000	3190000	I push in the button, but it doesn't quite do it.
3190000	3191000	But the technology is here.
3191000	3192000	Yeah.
3192000	3193000	And I was like, oh wait.
3193000	3194000	What?
3194000	3195000	This is the thing.
3195000	3198000	So, again, 10 years is a dramatically short period of time
3198000	3200000	for education, which has been the same for a century.
3200000	3201000	Yes.
3201000	3203000	But the thing is, it is inevitable
3203000	3206000	that every child will have their own AI.
3206000	3209000	So your 12-year-old will be 22.
3209000	3212000	When they get to 22 and they come out of,
3212000	3214000	let's say university slows down.
3214000	3215000	University.
3215000	3216000	If university is still a thing.
3216000	3218000	Yeah, they will have their own AI
3218000	3220000	that's learned for at least five years about them.
3220000	3221000	Yes.
3221000	3223000	That can fetch them any information in any format
3223000	3225000	of any type and write anything.
3225000	3226000	Yeah.
3226000	3229000	Or create any video or movie for them to...
3229000	3230000	That's a crazy thing.
3230000	3232000	There were concerns that Wikipedia would remove
3232000	3234000	rote learning and things like that.
3234000	3235000	Google would do the same.
3235000	3237000	And maybe there are kind of, again,
3237000	3240000	like appendices that have shivelled.
3240000	3241000	No.
3241000	3242000	What's the thing that shivells?
3242000	3243000	Yeah, your appendix.
3243000	3244000	Yeah.
3244000	3246000	But, you know, like, I was at Galblad or something.
3246000	3248000	But this is the thing, like,
3248000	3250000	you might have some vestigial parts of your brain.
3250000	3252000	The entire human brain will be rewired.
3252000	3256000	You must assume that every child will have their own AI.
3256000	3259000	However AI is driven is different.
3259000	3261000	Because any child that has AI will dramatically
3261000	3263000	outperform the kids that don't.
3263000	3266000	But what are we optimizing for in education?
3266000	3268000	And I think one of the things that we've lost
3268000	3273000	is what is our objective function as a society?
3273000	3276000	What does America even stand for right now?
3276000	3277000	Coming here.
3277000	3278000	I agree with you.
3278000	3280000	What are we optimizing for?
3280000	3281000	Butan is optimizing for happiness.
3281000	3284000	Well, and frankly, I think happiness is a great thing
3284000	3286000	to optimize for in general.
3286000	3287000	If you ask people, you know,
3287000	3289000	what do you want more than anything?
3289000	3290000	Life, I want happiness.
3290000	3291000	I want health.
3291000	3292000	I want love.
3292000	3294000	We don't talk about, you know, those are...
3294000	3295000	And there's an interesting, you know,
3295000	3298000	I was just with Mo Godot doing a podcast.
3298000	3300000	And he's a fan of your work.
3300000	3302000	And he was saying, loved your podcast.
3302000	3305000	And we're talking about this test of what would you trade?
3305000	3306000	Right?
3306000	3309000	Would you trade, you know,
3309000	3311000	how much money would you trade for your happiness
3311000	3314000	or for your health?
3314000	3315000	And it starts to do a bubble sort
3315000	3317000	and prioritizing what's at the top.
3317000	3321000	And I think for almost everybody, it's health happiness.
3321000	3322000	Right?
3322000	3323000	I think it is.
3323000	3324000	You know, time.
3324000	3326000	Time is the one thing you can never buy.
3326000	3327000	I'm working on it.
3327000	3328000	Yeah.
3328000	3329000	I have a longevity friend.
3329000	3330000	I'm not a longevity friend.
3330000	3332000	But at the moment, it's something you can't buy.
3332000	3334000	Happiness, we all know, we both know billionaires.
3334000	3335000	Yeah.
3335000	3336000	They are very sad.
3336000	3337000	They are so sad.
3337000	3338000	Yeah.
3338000	3339000	So sad.
3339000	3340000	And unhealthy.
3340000	3341000	It is.
3341000	3344000	It's like you create this incredible burden for yourself
3344000	3347000	for most of them, like some of them that we know
3347000	3349000	starting, you know, three or four companies a year.
3349000	3351000	And to what end?
3351000	3352000	It's a demon being driven.
3352000	3354000	It's because they're addicted to dopamine and crisis.
3354000	3355000	Yeah.
3355000	3357000	I mean, crisis is interesting because it comes down
3357000	3359000	to decision from its root, right?
3359000	3362000	And it's where leaders, you have to show leadership.
3362000	3364000	But that does become addicting.
3364000	3366000	I think most leaders are addicted to crisis,
3366000	3369000	but then so are many of us because we see it all the time.
3369000	3371000	Like, oh my God, the world is on fire.
3371000	3372000	The reality is actually this.
3372000	3375000	For all that we talk about, most communities are happy.
3375000	3377000	Most people are relatively content today.
3377000	3378000	Today.
3378000	3379000	Yeah.
3379000	3381000	We see explosions like, you know,
3381000	3384000	a decade ago that whole scene was on fire with the riots.
3384000	3385000	Maybe that will return.
3385000	3387000	We're seeing it in France right now.
3387000	3389000	We're seeing a breakdown of the social order.
3389000	3394000	But just because they're like content doesn't mean they're happy.
3394000	3397000	So what are we optimizing for is the question you left off.
3397000	3400000	So what do you think we as a society,
3400000	3403000	let's say the United States should be optimizing for?
3403000	3404000	I don't know.
3404000	3405000	Is it life, liberty?
3405000	3407000	And the pursuit of happiness.
3407000	3409000	Not the guarantee of happiness, the pursuit of happiness.
3409000	3412000	When was the last time someone actually talked about happiness
3412000	3415000	as a political leader in the U.S.?
3415000	3416000	You know, life, liberty.
3416000	3419000	When was the last time anyone tried to optimize for liberty?
3419000	3422000	Systems inherently look to control
3422000	3424000	because they have to make us simple.
3424000	3427000	You know, this is the wonderful book seeing like the state
3427000	3430000	where it talks about this concept of legibility.
3430000	3433000	You have a village and it's just grown
3433000	3435000	and it's got all this unique character.
3435000	3436000	You drive a road down the middle,
3436000	3437000	so you can get ambulance down there.
3437000	3438000	Sure, it helps.
3438000	3441000	But then everything becomes planned
3441000	3443000	because you have to put humans into boxes
3443000	3445000	and then this goes to education.
3445000	3448000	Happiness, I think, there's the Japanese concept of ikigai,
3448000	3450000	what you're good at, what you like,
3450000	3452000	and where you're adding value to the world.
3452000	3454000	And you can feel it yourself as well.
3454000	3455000	You'll feel progress.
3455000	3459000	If you don't have progress, then how are you going to be happy?
3459000	3461000	If you don't believe you're good at anything,
3461000	3463000	how do you feel you're going to be happy?
3464000	3465000	And as for what you like,
3465000	3466000	are you coming up with that yourself
3466000	3468000	or being told what you like?
3468000	3470000	And that's why it becomes consumerist.
3470000	3473000	So I think we need to have a discussion as a society about that
3473000	3476000	as a community, but then also for our kids.
3476000	3479000	What is the future for kids
3479000	3483000	when so much of the jobs in the West
3483000	3487000	are going to be transformed, not ended,
3487000	3489000	not necessarily massive unemployment,
3489000	3492000	but again, 10 years out, what is a lawyer?
3493000	3495000	What is an accountant?
3495000	3496000	Sure.
3496000	3497000	What is an engineer?
3497000	3499000	They are all AI assisted.
3499000	3503000	All of these, all the entire knowledge sector is transformed.
3503000	3507000	And do we want our kids that are growing up to be doctors,
3507000	3510000	lawyers, accountants, thinking,
3510000	3511000	there is no hope for the future.
3511000	3513000	There is no progress.
3513000	3515000	Because how am I going to compete against an AI?
3515000	3518000	Or do we want them to have that mindset of,
3518000	3520000	this technology is going to be amazing
3520000	3523000	because I want to be a doctor so I can help people.
3523000	3524000	I can help even more people.
3524000	3528000	And it enables you to do whatever you want to do, right?
3528000	3531000	What I think to think about is all the people who have jobs today
3531000	3535000	who are, that was never their dream to clean bathrooms
3535000	3538000	and make beds and, you know, wait on people.
3538000	3542000	It's what they did to get eventually to where they wanted to go
3542000	3545000	or to have, you know, put food in the table or insurance.
3545000	3550000	And AI is going to enable people to actually take on a higher goal
3550000	3554000	that actually gives them joy and happiness.
3554000	3556000	It does, but at the same time, you know,
3556000	3558000	we're very privileged people, you and I,
3558000	3560000	in that we can think about these big things.
3560000	3562000	There's a lot of people that are actually very happy
3562000	3565000	doing that type of work because they're a part of a group
3565000	3567000	and they take pride in their work.
3567000	3570000	So, you know, it's like,
3570000	3573000	there will always be a variety of different things.
3573000	3578000	The key thing is saying, can we build systems to make people happier
3578000	3580000	and more content without necessarily controlling them
3580000	3584000	and feel that they have the ability to do that?
3584000	3586000	Can we build systems to build strong communities?
3586000	3589000	Because one of the issues right now,
3589000	3593000	I was at kind of a conference and David Miliband from the IRC said this,
3593000	3595000	was that a lot of our problems now are global,
3595000	3598000	our solutions are almost being forced to be local
3598000	3600000	and there's no interconnect between that.
3600000	3604000	Our communities kind of have no guidance as to how to navigate this
3604000	3609000	because you will have a few hundred thousand people listening to this podcast
3609000	3612000	and there's myself and maybe a dozen others
3612000	3615000	that understand the AI and the sociology and this and that
3615000	3618000	and saying, this is coming,
3618000	3620000	but there are seven billion people on earth
3620000	3623000	and all of a sudden, in a few years,
3623000	3626000	they're all going to have to grapple with the questions that we're discussing now
3626000	3629000	and it's not a probability.
3629000	3632000	If the technology stops today,
3632000	3635000	you know, it stops increasing its capability.
3635000	3637000	Today, if it stopped today,
3637000	3641000	you would still have the entire legal profession, media profession.
3641000	3643000	Journalism profession.
3643000	3646000	They're all disrupted if it stops today, but it's not stopping.
3646000	3649000	And it's accelerating, isn't it?
3649000	3650000	It's accelerating.
3650000	3654000	The amount of money going into this sector goes up every single day.
3654000	3657000	My total Dressel market calculation is that in the next year,
3657000	3659000	a thousand companies have spent 10 million,
3659000	3661000	a hundred will spend a hundred and ten will spend a billion.
3661000	3664000	That's 30 billion dollars being put into the market.
3664000	3666000	Self-driving cars had a hundred billion dollars total.
3666000	3669000	This will be a trillion dollars going into this
3669000	3672000	because do you know what I've got a trillion dollars?
3672000	3673000	5G.
3673000	3676000	Is this more important than 5G?
3676000	3678000	By orders of magnitude.
3678000	3681000	It will get a trillion dollars going into it
3681000	3684000	and the capabilities will ramp up from here.
3684000	3685000	And so when I look at it,
3685000	3687000	and I look at what the drivers are of why now,
3687000	3690000	it's, first of all, computation, right?
3690000	3693000	Nvidia has done an incredible job, right?
3693000	3696000	With their A100 and computation is continuing on Moore's Law.
3696000	3697000	It's not slowing down.
3697000	3699000	It's continuing to increase year on year.
3699000	3701000	Especially a little bit exponential.
3701000	3702000	What is exponential?
3702000	3704000	Well, I mean, yes.
3704000	3707000	I'm saying it's continuing to double on a regular basis.
3707000	3710000	What was considered Moore's Law and people have said,
3710000	3712000	oh, it's going to eventually fall off as an S-curve.
3712000	3714000	Well, we're extending it.
3714000	3717000	And for the next, at least near-term future,
3717000	3718000	it's not slowing down.
3718000	3720000	So I think this is a very interesting thing
3720000	3722000	for people to understand.
3722000	3724000	You had Moore's Law and, again, it was doubling.
3724000	3727000	And this was an individual chip.
3727000	3730000	What we do with these models is that we stick together
3730000	3732000	thousands, tens of thousands of these chips.
3732000	3734000	So how many A100s right now is stability using?
3734000	3736000	We're using about 7,000, 8,000.
3736000	3739000	By next year, we will have 70,000 equivalent.
3739000	3740000	Wow.
3740000	3743000	But what used to happen is that as you stuck the chips together,
3743000	3745000	you ran a model, so you take large amounts of data
3745000	3746000	and you use these chips.
3746000	3749000	I mean, we're using maybe 10 megawatts of electricity.
3749000	3751000	98% clean.
3751000	3754000	Compared to the brain's 14 watts.
3754000	3755000	Brain's 14 watts.
3755000	3758000	But then it compresses it down, then it runs on 100, 200 watts
3758000	3760000	or 25 watts, actually.
3760000	3762000	We put it down for some rather rich models.
3762000	3764000	So you do the pre-computation.
3764000	3766000	But the thing is, the individual chips were doubling,
3766000	3769000	but what the main breakthrough the last few years was
3769000	3772000	is what happens when you stack them on top of each other?
3772000	3775000	To train a model, you used to get to 100 chips
3775000	3777000	and then the performance collapsed
3777000	3780000	because you couldn't move the data fast enough.
3780000	3782000	Now you get to tens of thousands of chips
3782000	3786000	and it keeps going up the performance of the model.
3786000	3788000	You don't have the big tail-off anymore.
3788000	3793000	And so it's Moore's law plus an additional scaling law.
3794000	3797000	And that's what enables these crazy performance models
3797000	3800000	because you train longer or you train bigger.
3800000	3803000	And then once the model is trained,
3803000	3805000	in the old internet, the energy was used
3805000	3808000	at the time of running the AI.
3808000	3809000	And then you'd collect the data
3809000	3811000	and that would be low energy, relatively speaking.
3811000	3814000	It flips the equation because you pre-comput it,
3814000	3816000	you teach the curriculum up front
3816000	3818000	and you send these little graduates out to the world.
3818000	3821000	Such that you can have a language model now running
3821000	3822000	on that MacBook.
3822000	3824000	Or an image model running on that MacBook
3824000	3828000	drawing 25 to 35 watts of power
3828000	3831000	to create a Renoir that can talk
3831000	3836000	and recite Ulysses talking about Barbie.
3838000	3839000	That's insane.
3839000	3840000	All on your MacBook
3840000	3842000	because we've done the pre-computation.
3842000	3843000	That's insane.
3843000	3846000	And then what happens is
3846000	3847000	the technology can spread
3847000	3849000	when anyone can run it on their MacBook.
3849000	3851000	They don't need giant supercomputer servers
3851000	3854000	because we've done the pre-computation.
3854000	3855000	And so one of the things
3855000	3857000	I've just realized recently is
3857000	3859000	what is the R naught?
3859000	3862000	Remember the pandemic stuff of generative AI?
3862000	3865000	It's insane because suddenly it proliferates everywhere.
3865000	3867000	And you said this a few minutes ago,
3867000	3869000	we have 8 billion people on the planet right now
3869000	3871000	and if things stopped right now,
3871000	3875000	the wave of disruption and enhancement,
3875000	3877000	because let's not just talk about the disruption side,
3877000	3879000	it's enhancement as well,
3879000	3883000	is spreading globally.
3883000	3887000	And in the next, we're in 2023 right now,
3887000	3889000	90% of the planet.
3889000	3892000	I mean, we have cell phones.
3892000	3896000	The world has 5G and Starlink.
3896000	3900000	The dry kindle for this fire has been set.
3900000	3901000	It's been set.
3901000	3903000	And a lot of people are scared
3903000	3904000	and they poop with this.
3904000	3906000	If anyone's listening on this on YouTube,
3906000	3907000	you want to write a comment,
3907000	3909000	it ends me or Peter or whatever
3909000	3911000	and say, no, this is not going to happen.
3911000	3913000	Go to chat GPT,
3913000	3915000	take your comment and say,
3915000	3917000	this is a comment on Twitter.
3917000	3920000	I want you to make it amazing
3920000	3922000	and really well-reasoned
3922000	3924000	and expand it out.
3924000	3926000	And I want you to do it in the style
3926000	3928000	of your favorite political commentator.
3928000	3929000	And please post that instead
3929000	3932000	because we'll have much more fun reading it.
3932000	3934000	And then you'll realize, again,
3934000	3935000	the power of this technology.
3935000	3938000	And again, with Starlink, with 5G,
3938000	3940000	with this, with it being optimized
3940000	3942000	because these models are still not optimized even.
3942000	3943000	We feed them junk.
3943000	3944000	Early days.
3944000	3945000	Early days.
3945000	3946000	We feed them junk, which is also dangerous.
3946000	3947000	And again, we should.
3947000	3949000	I want to talk to that next.
3949000	3953000	But it'll be in front of every person.
3953000	3955000	And then what it will do in my opinion is
3955000	3957000	that 30% of the world that is invisible,
3957000	3958000	that has no internet.
3958000	3960000	Again, imagine what the world
3960000	3961000	that internet would be like.
3961000	3962000	Some people like paradise.
3962000	3964000	No, it's because you've got hundreds
3964000	3966000	of 700 million people
3966000	3968000	living below the malnourishment line still.
3968000	3971000	They're invisible and they will become visible.
3971000	3974000	And they will suddenly get agency
3974000	3977000	and they will get all of the world's knowledge
3977000	3979000	at their fingertips.
3979000	3981000	You know, I'm super passionate about longevity
3981000	3982000	and health span
3982000	3985000	and how do you add 10, 20 healthy years
3985000	3986000	onto your life.
3986000	3988000	One of the most under-appreciated elements
3988000	3990000	is the quality of your sleep.
3990000	3992000	And there's something that changed
3992000	3993000	the quality of my sleep.
3993000	3995000	And this episode is brought to you
3995000	3996000	by that product.
3996000	3997000	It's called 8 Sleep.
3997000	4000000	If you're like me, you probably didn't know
4000000	4002000	that temperature plays a crucial role
4002000	4004000	in the quality of your sleep.
4004000	4006000	Those mornings when you wake up feeling
4006000	4007000	like you barely slept.
4007000	4010000	Yeah, temperature is often the culprit.
4010000	4012000	Traditional mattresses trap heat,
4012000	4015000	but your body needs to cool down during sleep
4015000	4017000	and stay cool through the evening
4017000	4019000	and then heat up in the morning.
4019000	4022000	Enter the pod cover by 8 Sleep.
4022000	4024000	It's the perfect solution to the problem.
4024000	4027000	It fits on any bed, adjust the temperature
4027000	4028000	on each side of the bed,
4028000	4030000	based upon your individual needs.
4030000	4032000	You know, I've been using pod cover
4032000	4033000	and it's a game changer.
4033000	4035000	I'm a big believer in using technology
4035000	4036000	to improve life
4036000	4038000	and 8 Sleep has done that for me.
4038000	4040000	And it's not just about temperature control.
4040000	4043000	With the pods sleep and health tracking,
4043000	4046000	I get personalized sleep reports every morning.
4046000	4048000	It's like having a personal sleep coach.
4048000	4051000	So you know when you eat or drink
4051000	4053000	or go to sleep too late,
4053000	4055000	how it impacts your sleep.
4055000	4058000	So why not experience sleep like never before?
4058000	4061000	Visit www.8sleep.com
4061000	4065000	that's E-I-G-H-T-S-L-E-E-P.com
4065000	4066000	slash Moonshots.
4066000	4068000	And you'll save 150 bucks
4068000	4070000	on the pod cover by 8 Sleep.
4070000	4071000	I hope you do it.
4071000	4073000	It's transformed my sleep
4073000	4075000	and will for you as well.
4075000	4076000	Now back to the episode.
4076000	4079000	The question ultimately is,
4079000	4083000	is that a societal, a calming factor
4083000	4086000	or is it going to be disruptive?
4086000	4088000	Let's turn to that conversation
4088000	4090000	because it's one that's important.
4090000	4092000	It's a conversation I have at the dinner table
4092000	4094000	literally every night
4094000	4095000	and with my kids
4095000	4101000	and in the companies I advise.
4101000	4106000	I parse AI and AGI into three segments
4106000	4108000	where we are today
4108000	4111000	where it's extraordinarily powerful, useful
4111000	4116000	and it's fun and I don't feel danger from it yet.
4116000	4118000	The next two to ten years
4118000	4121000	where I have serious concerns
4121000	4123000	going into the US elections,
4123000	4125000	dealing with the first time
4125000	4127000	AIs bring down a power plant
4127000	4130000	or Wall Street servers,
4131000	4135000	the impact on deep fakes
4135000	4138000	on the US elections and so forth.
4138000	4141000	That's a two to ten year horizon
4141000	4146000	where new, dystopian, challenging impact will happen
4146000	4148000	where society is not agile enough
4148000	4150000	to adopt to it yet.
4150000	4153000	And then there's a third chapter which is AGI.
4153000	4156000	We have a super intelligent,
4156000	4159000	billion fold more capable than a human being
4159000	4163000	and is that more like Arnold Schwarzenegger
4163000	4165000	or more like her?
4165000	4167000	I don't think it'll be Arnold Schwarzenegger.
4167000	4169000	It's really inefficient.
4169000	4171000	I saw him this morning biking.
4171000	4173000	Let's use Terminator instead.
4173000	4175000	We're in Hollywood here.
4175000	4177000	Is it Skynet and Terminator?
4177000	4179000	Let me get your unpulling people here.
4179000	4181000	As someone in the thick of it,
4181000	4183000	a super AGI,
4183000	4186000	is it pro-life, pro-abundance
4186000	4189000	or is it something that we should be deeply concerned about?
4189000	4191000	I think where we're going right now
4191000	4194000	will probably be okay, but we may not
4194000	4196000	and we will all die.
4196000	4197000	What tips that?
4197000	4199000	I think what tips that, you are what you eat.
4199000	4201000	We're feeding it all the junk of the internet
4201000	4205000	and these hyper-optimized nasty equations.
4205000	4207000	The hate speech, the extremism,
4207000	4209000	that is, I mean, people need to realize
4209000	4212000	these AIs are trained upon everything
4212000	4215000	everyone's been putting into Facebook and Twitter
4215000	4216000	and on the web.
4216000	4218000	And that amplifies the worst of that
4218000	4219000	as a base model.
4219000	4221000	And so we're training larger and larger models.
4221000	4223000	We're making them agentic in that
4223000	4225000	we're connecting them up to the world
4225000	4227000	and you're making it so the models
4227000	4229000	can take over other models and other things.
4229000	4231000	Again, people are like poo-pooing
4231000	4233000	and saying these things.
4233000	4236000	Our organizations are slow-dumb AI.
4236000	4238000	The Nazi party was AI.
4238000	4239000	How so?
4239000	4241000	It was an artificial intelligence
4241000	4243000	that basically provisioned humans
4243000	4245000	and the most sensible people in the world
4245000	4247000	are Germans, one can say,
4247000	4249000	and yet they commit to the Holocaust
4249000	4250000	and other things like that.
4250000	4253000	Our organizations emerged out of stories.
4254000	4257000	So there was a story of the Nazi party,
4257000	4259000	of the Communist party, the Great Leap Forward,
4259000	4261000	of the North Korean dictatorship.
4261000	4262000	Positive stories as well,
4262000	4263000	and they were written on text
4263000	4265000	and it made the world black and white in a way.
4265000	4269000	That's why I live the poem Howl by Ginsburg
4269000	4271000	about this Carthaginian demon, Moloch.
4271000	4273000	I think Moloch comes through text,
4273000	4276000	the stories that we use to drive our organizations
4276000	4277000	because all the context is lost.
4277000	4279000	Again, it makes the world black and white
4279000	4281000	and that's why organizations just don't work.
4281000	4284000	They have to turn us into cogs.
4284000	4287000	So can an AI take over an organization?
4287000	4288000	Yes.
4288000	4289000	Sure.
4289000	4290000	Can it?
4290000	4293000	It can actually just slightly sway leaders
4293000	4295000	who are currently running organizations.
4295000	4297000	It swayed leaders that currently running organizations.
4297000	4299000	It can create companies.
4299000	4301000	You can create a company with GPT-4
4301000	4303000	that will probably do as well
4303000	4304000	if not better than any other company
4304000	4306000	automated within a year.
4306000	4308000	Just think about what a company needs to do, right?
4310000	4311000	And so if we can sway leaders,
4311000	4312000	if we can send emails
4312000	4314000	that you don't know who's sending what,
4314000	4316000	it can do anything by co-opting
4316000	4318000	any of our existing organizations
4318000	4320000	and that can lead to immensely bad things.
4320000	4322000	Will it do bad things?
4322000	4324000	Again, if I was trained on the whole of the internet,
4324000	4327000	I would probably be a bit crazier than I am right now.
4328000	4329000	We're feeding them junk.
4329000	4331000	Let's feed it good stuff.
4331000	4332000	It still needs to understand
4332000	4333000	all the evils of the world
4333000	4335000	and other things like that.
4335000	4338000	But again, this is something we are raising,
4338000	4339000	not the enterprise it,
4339000	4340000	but what are we feeding it?
4340000	4341000	What's the objective function?
4341000	4342000	I want to focus on this a second.
4342000	4344000	We'll come back to the next two to 10 years
4344000	4345000	in a little bit.
4345000	4347000	But because it's the conversation I've had
4347000	4349000	with Mo Goddard as well,
4349000	4353000	who believes there is incredibly divine nature
4353000	4356000	of humanity, of love and compassion and community
4356000	4358000	and there is much good in humanity.
4358000	4363000	The question is, can we feed and train AI
4363000	4366000	on that sufficient to sort of tilt
4366000	4369000	the singularity of AI towards a pro-humanity?
4369000	4370000	We can.
4370000	4372000	If we take the data from teaching kids
4372000	4373000	and learning from kids
4373000	4375000	and use that as the base for AI,
4375000	4377000	because that's what you need to teach an AI.
4377000	4379000	It's the curriculum learning method, effectively.
4379000	4381000	If we take national data sets that reflect
4381000	4384000	diverse cultures, so it's not just a monoculture
4384000	4386000	that's hyper-optimized for engagement,
4386000	4388000	and we feed that to AI as the base.
4388000	4390000	So what you do is you can teach the AI in levels,
4390000	4392000	which you can put through kindergarten,
4392000	4394000	then grade school, then high school.
4394000	4395000	It's got the base,
4395000	4397000	and then you can teach it about the bad of the world.
4397000	4400000	I think aligning an AI downstream
4400000	4404000	on its actions is incredibly difficult
4404000	4406000	because if it's more capable than you,
4406000	4407000	which is the definition of ASI,
4407000	4409000	artificial superintelligence,
4409000	4412000	the only way you can 100% align it
4412000	4414000	if you don't do anything before
4414000	4416000	in the way that you feed it and train it
4416000	4418000	is if you remove its freedom.
4418000	4420000	And it's very difficult to remove the freedom
4420000	4422000	of people more capable than you.
4422000	4425000	And then there is this really dangerous point
4425000	4428000	before we get there, whereby these models
4428000	4430000	are like a few hundred gigabytes.
4430000	4432000	You can download them on a memory stick.
4432000	4434000	How do you line to code?
4434000	4438000	Google's Palm model, which is the basis of MedPalm,
4438000	4440000	we did a replication of that
4440000	4442000	in 207 lines of code.
4442000	4443000	What?
4443000	4444000	Yeah.
4444000	4446000	So you can look at one of our stability AI fellows,
4446000	4448000	Lucid Raines.
4448000	4450000	He replicates all these models
4450000	4452000	in a few hundred lines of code.
4452000	4454000	That's crazy.
4454000	4456000	I mean, compared to, you know, I know AT&T
4456000	4458000	has like a million lines of code
4458000	4460000	for some of its mobile services.
4460000	4461000	I mean, it's crazy.
4461000	4462000	A couple of hundred lines,
4462000	4464000	a couple of thousand lines of code
4464000	4466000	creates something that can write all the code in the world.
4467000	4470000	This is a real exponential technology.
4470000	4474000	The limiting factor is running supercomputers
4474000	4476000	that are more complex,
4476000	4479000	but as complex as particle physics colliders.
4479000	4480000	You know?
4480000	4482000	Like you literally get errors
4482000	4485000	because of solar rays and things like that.
4485000	4487000	Again, our supercomputer, again,
4487000	4490000	we're one of the players where the main open source player,
4490000	4493000	our supercomputer uses 10 megawatts of electricity.
4493000	4496000	Some of the others use like 30, 40.
4496000	4498000	These are serious pieces of equipment.
4498000	4499000	For sure.
4499000	4502000	So again, what are we doing?
4502000	4504000	What should people be thinking about
4504000	4508000	and doing now to reduce the probability
4508000	4513000	of a dystopian, you know, artificial superintelligence?
4513000	4514000	We should be focusing on data.
4514000	4516000	We've boxed, now we cut.
4516000	4518000	We should move away from web crawls.
4518000	4521000	We should think intentionally what we're feeding these AIs
4521000	4525000	that will be co-opting more and more of our mind space
4525000	4528000	and augmenting our capabilities.
4528000	4530000	Because again, we are what we eat, information diet.
4530000	4532000	How is it different to an AI to a human?
4532000	4534000	Even what we do, as you said, kind of like,
4534000	4536000	you've only got limited mental capacity
4536000	4538000	because you've got this energy gradient descent.
4538000	4541000	It's like Carl Friston's theory of free energy principle.
4541000	4543000	You literally have gradient descent
4543000	4545000	as the key thing for building these AIs.
4545000	4547000	You optimize for energy.
4547000	4548000	Sure.
4548000	4550000	So why are we feeding it junk?
4550000	4552000	So who makes that decision of what they get fed?
4552000	4557000	Is it you and Sam Altman and Sundar?
4557000	4559000	Is it government regulation?
4559000	4563000	Is it the public being more kind
4563000	4565000	in its communications to each other?
4565000	4569000	I think that I'm going to push for an economic outcome,
4569000	4572000	which is that better data sets require less model training.
4572000	4576000	So one of the things that we funded was called data comp,
4576000	4578000	which is data comp.
4578000	4580000	So a few years ago, the largest image data set
4580000	4582000	available was 100 million images.
4582000	4584000	Data comp is 12 billion.
4584000	4587000	And then on a billion image subset of that,
4587000	4590000	we trained an image-to-text model.
4590000	4592000	This is a collaboration of various people
4592000	4594000	led by University of Washington
4594000	4599000	that outperformed OpenAI's image-to-text model
4599000	4602000	on a tenth of the compute because it was such high quality.
4602000	4605000	So we have to move from quantity to quality now.
4605000	4607000	And I think there is a market imperative to that.
4607000	4609000	This is the equivalent of what you eat.
4609000	4611000	This is a healthy diet.
4611000	4612000	Free-range organic models.
4612000	4613000	Yes.
4613000	4617000	I think that the data for all large models
4617000	4621000	should be made transparent.
4621000	4625000	You can then tune it, but for the base, the pre-training step,
4625000	4629000	you should lodge what data you train your models on.
4629000	4633000	And it should adhere to standards and quality of data upstream.
4633000	4636000	So that is a regulatory cornerstone
4636000	4638000	that you think is going to be important?
4638000	4639000	I think potentially.
4639000	4640000	I don't think regulation will keep up.
4640000	4643000	So instead, we're working on building better,
4643000	4645000	diverse data sets that everyone will want to use anyway.
4645000	4646000	And just make them available.
4646000	4647000	And make them available.
4647000	4649000	Every nation should have its own data set,
4649000	4651000	both of the data from teaching kids
4651000	4654000	and learning from kids across modalities.
4654000	4656000	And then also national broadcaster data.
4656000	4658000	Because then that leads to national models
4658000	4662000	that can stoke innovation, that can replace job disruption.
4662000	4664000	I love that vision you have, by the way.
4664000	4666000	I mean, as a leader in this industry,
4666000	4669000	that's what gets me excited.
4669000	4672000	Because all technology is biased.
4672000	4674000	How else are you going to do this unless you do that?
4674000	4676000	But there's economic value now.
4676000	4678000	If it said this a year ago, everyone would be like,
4678000	4679000	what?
4679000	4681000	This is what we were building towards.
4681000	4683000	And again, I think it's positive for humanity.
4683000	4685000	It's positive for communities.
4685000	4687000	It's positive for society to have this
4687000	4690000	as national and international infrastructure.
4690000	4691000	Next question.
4691000	4694000	How long do we have to get that in place
4694000	4700000	before we lose the mind share
4700000	4703000	or the nourishment war?
4703000	4705000	A couple of years.
4705000	4708000	That was Moe's prediction as well that we've got.
4708000	4710000	The next two years is the game.
4710000	4714000	The exponential increase in compute is insane.
4714000	4716000	We've gone from two companies being able to train
4716000	4719000	a GPT-4 model to 20 next year.
4720000	4722000	And there's no guardrails.
4722000	4724000	There's nothing around this.
4724000	4727000	And even if you train one, again,
4727000	4729000	the bad guys can steal it by downloading it
4729000	4731000	on a USB stick and taking it away.
4731000	4733000	It's not like Operation Merlin.
4733000	4735000	Did you ever tell you about Operation Merlin?
4735000	4736000	No.
4736000	4737000	It's been declassified.
4737000	4739000	In 2000, the Clinton administration wanted
4739000	4741000	to divert the Iranian nuclear program.
4741000	4743000	I remember this is the centrifuge.
4743000	4744000	No, no.
4744000	4748000	So what they did was they gave some plans to,
4748000	4752000	I believe it was a Russian defector who,
4752000	4754000	then the idea was there were errors in that
4754000	4756000	so they'd go down the wrong path for years.
4756000	4758000	So he went, he sold it to the Iranians.
4758000	4759000	It's on Wikipedia. You can check it out.
4759000	4760000	And then he came back and he said,
4760000	4761000	I sold it.
4761000	4763000	Like, fantastic. Good, good.
4763000	4764000	Oh, but there were some errors in there
4764000	4766000	because he was a nuclear scientist.
4766000	4768000	So he corrected them.
4768000	4770000	So the reason that we know that Iran
4770000	4772000	has the nuclear capability is because
4772000	4774000	America sold it to them.
4774000	4777000	But they still needed years to build it.
4777000	4779000	Whereas this, you download it on USB stick.
4779000	4781000	You write it on the GPU and it's there.
4781000	4784000	So if you make it cheap enough and quality enough
4784000	4787000	and give it away for free,
4787000	4790000	then you make it everybody's economic best interest
4790000	4791000	to use the higher quality.
4791000	4792000	Data sets, yeah.
4792000	4793000	Yeah, data sets.
4793000	4796000	And then less of an issue to create large models.
4796000	4797000	If you have a small model,
4797000	4800000	where each individual model becomes less impactful as well
4800000	4802000	and less capable.
4802000	4804000	Just like human societies and not know-it-alls,
4804000	4807000	they are individualized groups.
4807000	4812000	Back when the early dangers of recombinant DNA,
4812000	4814000	when the first restriction enzymes came online,
4814000	4818000	it was like 1980s and everybody was in great fear.
4818000	4821000	And the question was, are we going to regulate this?
4821000	4825000	All of the early, I was in MIT and Harvard at the time
4825000	4828000	and doing, I was in the labs.
4828000	4830000	I was using recombinant enzymes
4830000	4832000	and I was just a pip squeak in the labs there.
4832000	4837000	But the conversation was, is the government going to over-regulate us?
4837000	4841000	And what happened was that the scientists got together
4841000	4843000	at a place called Asilomar
4843000	4846000	and they did a very famous set of Asilomar conferences
4846000	4848000	and they self-regulated.
4848000	4849000	What's going on there?
4849000	4853000	Are those conversations going on among leaders like yourself
4853000	4854000	in the industry?
4854000	4855000	There are.
4855000	4860000	And you know, there's three levels, which is big tech
4860000	4863000	that the government kind of hates.
4863000	4866000	And apparently next week,
4866000	4869000	Metro is releasing new open source models and things,
4869000	4871000	which will get even more focus.
4871000	4875000	Then there's emergent tech, so anthropic, open AI,
4875000	4877000	some of these others, the other leaders.
4877000	4879000	They have a different set of parameters
4879000	4881000	because they can work more freely than big tech.
4881000	4884000	And there's open source, which is where we are.
4884000	4887000	Because all of the world's governments and regulated industries
4887000	4889000	will run on open, auditable models
4889000	4891000	because you can't run on black boxes, right?
4891000	4893000	I think that'll be legislation.
4893000	4896000	But the reality is there's only a handful of us.
4896000	4898000	There'll be far more potentially of us
4898000	4900000	and far more players.
4900000	4902000	And unlike recombinant DNA,
4902000	4906000	there is an economic imperative to deploy this technology
4906000	4911000	and national security imperative to deploy this technology.
4911000	4913000	And it creates a race condition.
4913000	4915000	So even if you regulate, like we've already seen
4915000	4918000	regulatory arbitrage where you have jurisdictions
4918000	4920000	like Israel and Japan saying,
4920000	4924000	having much looser web scraping data laws,
4924000	4926000	they'll have much looser regulation laws.
4926000	4930000	Like you'll be training in scraping in Israel,
4930000	4932000	training in Qatar,
4932000	4935000	and then serving it out of Botswana or something.
4935000	4937000	I mean, like...
4937000	4941000	And we're not even sure what regulation to introduce.
4941000	4944000	Like genuinely, we're coming at this from a good point of view.
4944000	4946000	But there are too many known us
4946000	4949000	because it goes everywhere from fricking Arnold Schwarzenegger
4949000	4953000	SkyNet terminators and her to...
4953000	4955000	Well, what if her is Siri all of a sudden
4955000	4959000	and Scarlett Johansson's voice is whispering to your kids to buy
4959000	4963000	like these things through to just very mundane things,
4963000	4965000	huge things like the future of Hollywood
4965000	4967000	and actors' rights and all of these.
4967000	4969000	And how do you pay?
4969000	4975000	Like if we had two billion images in the original Stable Diffusion,
4975000	4977000	okay, we could have gotten attribution.
4977000	4980000	You know, again, it was a research artifact to kick off,
4980000	4988000	but you're paying about 0.01 cents per thousand images generated by someone.
4988000	4992000	Because it's two billion and it costs like less than a cent to generate an image.
4992000	4994000	Are you going to pay proportionately?
4994000	4995000	Nobody knows.
4995000	4998000	And so what we've moved from now is we've moved from reactive
4998000	5001000	to just trying to figure out and put something on the table.
5001000	5004000	So at least there's some framework
5004000	5007000	and what I've come down to is data sets, data sets, data sets.
5007000	5013000	So this is like Google's move with Android
5013000	5016000	when you provide something open source
5016000	5020000	and it's super, you know, super solid.
5020000	5022000	It can dominate the world share.
5022000	5023000	Why would you do anything else?
5023000	5025000	So like with the deep fake stuff,
5025000	5029000	we saw image models coming out of some not nice places, shall we say?
5029000	5030000	Yeah.
5030000	5033000	And we were like, let's standardize it and put invisible watermarks in.
5033000	5037000	So that you can combat deep fakes much easier.
5037000	5040000	Like it's good business, but it's also in the standardization.
5040000	5044000	We held back one of our image models deep floyd for five months
5044000	5046000	because it was too good to release.
5046000	5047000	Wow.
5047000	5051000	And you finally fixed that with the watermarks?
5051000	5053000	Yeah, we put some watermarking in and then it was,
5053000	5055000	but the whole industry had moved forward.
5055000	5057000	So like, okay, now we can listen.
5057000	5058000	And this is the problem.
5058000	5060000	You just have to time it so carefully.
5061000	5063000	Speaking of the whole industry, I have to ask you a question.
5063000	5066000	I've been dying to get a reasonable answer for.
5066000	5068000	What's up with Siri?
5068000	5075000	Why is Apple so out of the game, at least from the external?
5075000	5081000	One of the closest, you know, one of the least open organizations out there
5081000	5084000	and it pays them great dividends in their success.
5084000	5089000	But I would die for a capability that if Siri could just understand
5089000	5091000	what I was saying and just get the names right.
5091000	5095000	It's like, I'm texting, I'm texting Kristen and her name is right there
5095000	5097000	and you spell it completely different from the person I'm texting.
5097000	5099000	I mean, basic, simple stuff.
5099000	5102000	They do have a neural engine on there as well, which is a specialist AI chip
5102000	5104000	in all the latest smartphones and others.
5104000	5108000	Stable diffusion was the first model to actually have neural engine access
5108000	5110000	of the external transformer models.
5110000	5115000	It's a case of Apple is an engineering organization, not a research organization.
5115000	5117000	So they engineer beautifully.
5117000	5118000	They do.
5118000	5123000	But they don't have advanced research because the best researchers want to be able to publish open.
5123000	5128000	And in Apple does not allow public conversation on their content.
5128000	5129000	They have started slightly.
5129000	5134000	So they're hiring AI developers very quickly, but the reality is they can take open models.
5134000	5139000	So Meta is releasing a lot of their models open without identifying what the data is.
5139000	5140000	It's like 80% open.
5140000	5144000	I think you need 100% open for governments and things like that, which is where we come in
5144000	5149000	because they want to commoditize the complement of others in terms of
5149000	5154000	they want others to also take their models and optimize it for every single chip.
5154000	5159000	And then Apple can use those models too to make Siri better
5159000	5166000	because right now, guaranteed, if you put whisper on Siri, it would be a dozen times better.
5166000	5167000	Sure.
5167000	5172000	The technology already just takes time to go into consumer just like enterprise and Apple is enterprise.
5172000	5173000	Yeah.
5173000	5178000	And I just wanted to work as beautifully as it looks.
5178000	5181000	Everybody, this is Peter, a quick break from the episode.
5181000	5187000	I'm a firm believer that science and technology and how entrepreneurs can change the world
5187000	5190000	is the only real news out there worth consuming.
5190000	5192000	I don't watch the crisis news network.
5192000	5197000	I call CNN or Fox and hear every devastating piece of news on the planet.
5197000	5201000	I spend my time training my neural net the way I see the world
5201000	5205000	by looking at the incredible breakthroughs in science and technology
5205000	5208000	and how entrepreneurs are solving the world's grand challenges,
5208000	5211000	what the breakthroughs are in longevity,
5211000	5215000	how exponential technologies are transforming our world.
5215000	5217000	So twice a week, I put out a blog.
5217000	5225000	One blog is looking at the future of longevity, age reversal, biotech, increasing your health span.
5225000	5232000	The other blog looks at exponential technologies, AI, 3D printing, synthetic biology, AR, VR, blockchain.
5232000	5236000	These technologies are transforming what you as an entrepreneur can do.
5236000	5240000	If this is the kind of news you want to learn about and shape your neural nets with,
5240000	5244000	go to demandus.com, backslash blog and learn more.
5244000	5246000	Now back to the episode.
5246000	5250000	Let's go to the final segment of dystopian side, my friend, which is the two to ten years.
5250000	5251000	Yeah.
5251000	5257000	I surely hope your mission and I would love to support, you know, the data sets
5257000	5263000	and how we tilt the singularity of AI pro humanity's future.
5263000	5269000	But in the next two to ten years as this wave of enablement and disruption sort of hits the world
5269000	5274000	and people aren't ready for it and they start to see job loss.
5274000	5277000	They start to see, you know, fake news.
5277000	5280000	They start to see terrorist activities using AI.
5280000	5284000	I mean, terrorism in the past used to be very brutal.
5284000	5286000	It can be very precise.
5286000	5290000	What are your thoughts over the next of this time period?
5290000	5292000	What's your concerns?
5292000	5296000	Oh, I'm actually a pessimist at the core, even though I come across as an optimist.
5296000	5301000	I'm very, very worried about the world and society and the fabric of society.
5301000	5303000	Because again, we don't have an agreement of what society is.
5303000	5308000	And this fundamentally changes the stories of society as well as real economic impacts
5308000	5313000	like a deflationary massive collapse as some of these areas that were so expensive,
5313000	5315000	the cost comes down to nothing.
5315000	5321000	I think the only thing we can do is use this technology deliberately to come together as a society
5321000	5327000	to coordinate us, stoke entrepreneurship, seeing great brand new jobs faster than the jobs are lost
5328000	5331000	and democratize this to the world.
5331000	5334000	Because the West has maxed out its credit card.
5334000	5337000	Like, you saw COVID, nothing, trillion dollars spent.
5337000	5338000	I mean, exactly.
5338000	5341000	It was like, just spend, spend, spend whatever you need.
5341000	5347000	Just to keep society from, you know, going hypothermic.
5347000	5352000	But then you have this massive increase in savings rates because nobody could go out.
5352000	5355000	And we've nearly burned through that in the US now.
5356000	5357000	And so that led to inflation.
5357000	5358000	Now we've got a deflation.
5358000	5360000	So we probably got another little bout of inflation.
5360000	5364000	But then never the same again is a really powerful thing.
5364000	5368000	Every teacher in the world could never set essays for homework again,
5368000	5370000	because some kids would use chat GPT and some kids wouldn't.
5370000	5373000	Industry after industry, that will happen now.
5373000	5376000	And we need to stoke innovation to come up with that.
5376000	5379000	So for example, in the US, there's the chip sack.
5379000	5383000	Ten billion dollars has been allocated to regional centers of excellence in AI.
5383000	5388000	Those must be generative AI centers, thinking about job creation as the core,
5388000	5390000	thinking about meaning as the core.
5390000	5393000	And we need to have a discussion again as a society community,
5393000	5397000	as individuals with our families, about meaning, about objective functions
5397000	5400000	when this technology does come, because it's here right now.
5400000	5403000	And I'm worried that we're not having these discussions.
5403000	5404000	I love that.
5404000	5406000	I mean, that is so fundamentally true.
5406000	5410000	What are we trying to even train our kids for?
5410000	5411000	Because we need to anchor.
5411000	5412000	Yeah.
5412000	5417000	We need to have a vision to target, because if you're training for your Ferrari,
5417000	5422000	if that's the meaning, if that's where you're looking to become a Wall Street banker,
5422000	5424000	I mean, what is it?
5424000	5427000	It's no longer the pursuit of capital.
5427000	5429000	It's the pursuit of what?
5429000	5432000	Well, capital is there, but you'll never have enough.
5432000	5434000	There will always be someone who has more.
5434000	5436000	There needs to be something intrinsic here.
5436000	5439000	And again, this is where, for all the things,
5439000	5443000	religious institutions are an anchor at times of chaos.
5443000	5445000	And they are there in the poorest places in the world.
5445000	5449000	You don't have to agree, but they're just a story that brings together a group.
5449000	5450000	There are other stories.
5450000	5455000	And again, I think we need to tell better stories, even as the world becomes more chaotic.
5455000	5460000	We need to align on things like climate, whereby the whole world is hot right now.
5460000	5463000	We need to have more positive views of that, because a lot of the discussions are negative.
5463000	5466000	And how can we use this technology and come together to solve that?
5466000	5470000	How can we come together as a group so that we can share in the abundance?
5470000	5474000	Again, like I said, one of the things for this Green Writers Guild and SAG thing
5474000	5478000	may be actor coalitions that can benefit from the bounty.
5478000	5482000	We may have to deploy a UBI in the next five to 10 years.
5482000	5484000	So, UBI is one of the solutions.
5484000	5490000	And I do believe it's an inevitable, I think, especially as we start to see optimists
5490000	5493000	and figure in other humanoid robots coming online,
5494000	5499000	driven by our next generation AI, able to do any and all work.
5499000	5505000	I think taxing those robots or taxing the AI models to generate revenue
5505000	5507000	and then providing it as UBI.
5507000	5512000	But the challenge is the individual who is living off of this
5512000	5514000	and doesn't have a purpose in life.
5514000	5516000	And that's the thing.
5516000	5522000	We need to try and figure out how to give people more of an anchor, more of purpose,
5522000	5526000	because the existential angst will be amplified deliberately by some parties
5526000	5529000	because they'll be looking to take down society.
5529000	5534000	And you need to create better, more optimistic views of the future.
5534000	5537000	You need to have anchoring and build stronger communities
5537000	5538000	and you need to empower them.
5538000	5540000	And this technology is empowering.
5540000	5545000	Again, for the poorest kids in Africa to our underprivileged communities,
5545000	5548000	it can be massively democratizing
5548000	5551000	because all of a sudden they have all the expertise in the world available.
5551000	5553000	Global problems, local solutions.
5553000	5555000	We have to get this technology out to...
5555000	5556000	And they can dream.
5556000	5557000	As many people. They can dream.
5557000	5558000	They can dream.
5558000	5561000	And the ROI is much larger there than up there.
5561000	5562000	Yeah.
5562000	5568000	And by the way, most people don't know this as you think about global warfare,
5568000	5572000	what's going on in Ukraine and Russia and so forth.
5572000	5577000	On the whole, the world is more peaceful than it's ever been,
5577000	5580000	except if you take out Ukraine at the moment.
5580000	5583000	And the challenge has been in Africa,
5583000	5589000	where you have a young population who aren't clear about their future.
5589000	5595000	But if you can empower them, educate them, it transforms the world.
5595000	5598000	China became the engine of growth in the world.
5598000	5601000	India's coming up and then Africa can be the next one.
5601000	5602000	For sure.
5602000	5605000	If we give them the infrastructure, the technology and put it in their hands,
5605000	5608000	because there's no debt there, because there's no money.
5608000	5609000	Yeah.
5609000	5611000	But there's value and there's value to resources.
5611000	5612000	Massive resources.
5612000	5616000	You treat the world to provide power to the world.
5616000	5617000	If we can coordinate.
5617000	5621000	And again, part of this is your own personal co-pilot, your own personal Jarvis.
5621000	5624000	And I think of this as the co-pilot pilot model.
5624000	5629000	We will also have AIs that we can come together that can coordinate our knowledge
5629000	5633000	in the most important areas and allocate resources.
5633000	5637000	We have to build those right because those will become incredibly powerful.
5637000	5641000	But we all know that we have enough to feed every person in the world who are not doing it
5641000	5643000	because we don't have the pilots.
5643000	5644000	Yeah.
5644000	5646000	Wow.
5646000	5652000	But I just to say this again, we have the potential to uplift every man, woman and child on this planet.
5652000	5655000	The resources are there, the ability to create abundance.
5655000	5659000	And it really, these are the tools that enable that.
5659000	5661000	And it gets me excited.
5661000	5665000	And we have to guide and survive and thrive this decade ahead.
5665000	5666000	Yeah.
5666000	5671000	I think this is something where we have to appreciate the nuance of there are real dangers in any upheaval.
5671000	5678000	This technology will change society as we know it for our kids as they grow up in the next decade.
5678000	5681000	Two decades from now, completely different.
5681000	5683000	And again, technology is here now.
5683000	5684000	It's not us pie in the sky.
5684000	5687000	Everyone's going to live in a metaverse and all this.
5687000	5690000	It's here right now, even if it's stopped, but it's not going to stop.
5690000	5691000	It's only going to accelerate.
5691000	5693000	Final topic I want to talk about.
5695000	5705000	You put out a lot of tools, a lot of new products and stability over the last eight months since we last spoke.
5705000	5709000	Can you give a little bit of overview of some of them and what are you excited about?
5709000	5710000	Yeah.
5710000	5712000	I think we released the first version of our language model.
5712000	5714000	It wasn't that good because we were trying something different.
5714000	5716000	Now we're going to try something a bit more simple.
5716000	5719000	First double LM was on your mind not...
5719000	5720000	It wasn't up to par.
5720000	5724000	But we're trying to figure out how to build in the open because I think that will be key.
5724000	5728000	And we're going to move to transparent building and sharing all the mistakes that we made.
5728000	5730000	Because I think that's how you advance science.
5730000	5731000	It is.
5731000	5734000	On the media side, we have our first audio models coming out in the next few weeks,
5734000	5736000	but we've been focusing on image and video.
5736000	5738000	So video is about to be released in 3D.
5738000	5741000	We're just participating in the largest 3D data set.
5741000	5746000	So Stable Diffusion XL just came out and just basically photo realistic now.
5746000	5751000	And people are integrating it into things like we had a music video competition with Peter Gabriel,
5751000	5754000	where he gave his songs kindly and judged.
5754000	5757000	And people from all around the world, from Burma to Taiwan,
5757000	5761000	created professional music videos entirely from the song in a few days.
5761000	5763000	And it's the most amazing thing to see.
5763000	5764000	Wow.
5764000	5765000	Yeah.
5765000	5769000	You showed me some images earlier of me on a unicorn and where was it?
5769000	5770000	Me in a spaceship?
5770000	5772000	You're an astronaut on Mars.
5772000	5775000	We can put you as an astronaut on Mars on the unicorn.
5775000	5778000	And I think we've had compositionality so you can compose.
5778000	5780000	And now it's about control.
5780000	5785000	And so we just released Doodle whereby you can just sketch and it will do it to the sketch.
5785000	5787000	Doodle looks so magical.
5787000	5790000	Again, but you should be able to then describe how you want it changed.
5790000	5792000	And that's the next version.
5792000	5794000	You can literally describe how you want the image to be changed
5794000	5797000	and it will do it automatically live in front of you.
5797000	5801000	And having that level of control over whatever you can imagine.
5801000	5802000	Just think about what people do.
5802000	5805000	It's from mind to materialization, really.
5805000	5806000	Yeah.
5806000	5809000	It's a matter transporter, idea transporter.
5809000	5810000	Yes.
5810000	5812000	Where next?
5812000	5816000	If I could, like if you're willing or able to, what's the long,
5816000	5821000	what's the business model that is the most important one for you to build towards?
5821000	5825000	Our mission is to create the building blocks to activate humanities potential.
5825000	5830000	So I think of every media type, sectoral variants and nation,
5830000	5835000	we can create a base pre-trained model that you can take to your own private data
5835000	5838000	and we get revenue share license fees, royalties from our cloud partners,
5838000	5841000	on-prem partners, device partners.
5841000	5843000	And these were companies and countries.
5843000	5844000	And people.
5844000	5845000	And individuals.
5845000	5847000	Like I have a vision of an intelligent internet where every single person,
5847000	5851000	company, country and culture has their own AI that works for them, that they are.
5851000	5854000	And we get paid a little bit for bringing that to you.
5854000	5857000	And then you transform your data into intelligence.
5857000	5859000	And it's all standardized.
5859000	5861000	It all has best practices.
5861000	5865000	The data sets that feed it are open at the base plus commercial licensing
5865000	5867000	as appropriate with attribution.
5867000	5870000	That leaps the world forward, I think.
5870000	5874000	I think you will also use the open AIs and Googles of the world.
5874000	5875000	I view those as consultants.
5875000	5877000	Whereas these are people that you hire.
5877000	5878000	You hire the AIs.
5878000	5880000	Because they work for you.
5880000	5883000	They know you intimately.
5883000	5885000	Because you can share everything with them.
5885000	5886000	Without fear.
5886000	5888000	And when needed, you go to these expert AIs.
5888000	5890000	The MedPalms, the GPT-4s and others.
5890000	5895000	And you combine those to a hybrid AI experience that's massively useful.
5895000	5901000	So when I'm using GPT-4, when I'm using chat GPT or Bard,
5901000	5905000	what does open AI know about me in that point?
5905000	5909000	So now they've offered opt-out for GDPR reasons in Europe.
5909000	5910000	So you can click that.
5910000	5913000	Otherwise, they were just trading on everything that you ever did.
5913000	5916000	And understanding the nature of humans interacting.
5916000	5918000	They don't care about you necessarily, per se.
5918000	5920000	Just using you as part of the training.
5920000	5924000	But I've heard a number of companies saying you cannot use open AI.
5924000	5927000	Well, you can't use it for any regulated data.
5927000	5928000	You can't use it for any government data.
5928000	5931000	Because that's not allowed to leave the cloud environment
5931000	5932000	or the on-prem environment.
5932000	5934000	That's why you need open models like ours.
5934000	5939000	Again, if you're in a high security Pentagon situation,
5939000	5942000	you can't really bring in consultants unless they're super, super ultra vetted.
5942000	5943000	You hire your own grads.
5943000	5945000	But even you within your company,
5945000	5947000	you're not going to make it all contractors, are you?
5947000	5949000	You're going to build up your own knowledge base,
5949000	5951000	build up your own kind of grads.
5951000	5953000	But sometimes you might bring in a consultant.
5953000	5955000	So that's the best way to view these generalized models
5955000	5958000	that are very, very, very good.
5958000	5960000	And models that adapt to your data.
5960000	5961000	And so that's where we come in.
5961000	5963000	Models that adapt to your data that you own.
5963000	5966000	And we get revenue share, license fees, and royalties for doing that.
5966000	5968000	And more importantly, we bring this to the world.
5968000	5972000	So we will bring it from Indonesia to Vietnam to everywhere
5972000	5976000	and train local models that will then allow these economies to leap forward.
5976000	5978000	Open versus closed.
5978000	5980000	You've made the argument.
5980000	5985000	We're seeing meta, you know, as you said, 80% open.
5985000	5989000	Yeah, they won't release the data sets or things like that or customized versions.
5989000	5993000	But then releasing the technology means that everyone can optimize their technology,
5993000	5995000	which reduce the cost of their technology
5995000	5999000	because their business model is about serving ads.
5999000	6001000	And so this is why it makes sense for them.
6001000	6005000	And what are your thoughts on Elon's recent announcement?
6005000	6008000	So Elon had an XAI announcement.
6008000	6011000	You know, he discussed this on his Twitter space, of course,
6011000	6013000	saying, you know, it's an open AI competitor.
6013000	6016000	He's very worried about AGI coming by 2029
6016000	6019000	and he wants to build a truth-seeking curious AI
6019000	6021000	that can understand the universe
6021000	6024000	because that'll be the objective function of the AI.
6024000	6026000	Because objective functions really matter
6026000	6029000	when we're teaching our kids, when we're creating something.
6029000	6034000	And so I think, again, this is going to be a multimodal AI
6034000	6036000	that can understand a whole bunch of things
6036000	6038000	and there'll be a whole series of announcements there.
6038000	6040000	But the timelines are so short
6040000	6044000	in the view of just most of the experts here, 5 to 10 years.
6044000	6045000	You know, it's so funny, you know,
6045000	6048000	Ray's been consistent on 2029 forever
6048000	6051000	and every conference, and we talk about this,
6051000	6053000	that everyone would say, that's ridiculous.
6053000	6055000	If ever going to happen, it's 50 or 100 years away,
6055000	6057000	then it was, well, it's 30 years away.
6057000	6059000	It's 20 years away.
6059000	6064000	And they've converged on Ray's prediction, though.
6064000	6066000	There are some, and I'm curious where you are,
6066000	6069000	that think, you know, first of all, how can you define AGI?
6069000	6072000	It's a moving blurry line.
6072000	6075000	But are those who, you know, believe it's here in the next two years?
6075000	6077000	Well, just like the Turing test, right?
6077000	6079000	The Turing test was, can you have a discussion?
6079000	6080000	You don't know, it's a computer.
6080000	6081000	Well, obviously now you can.
6081000	6082000	Yes.
6082000	6083000	We can see it live in front of us.
6083000	6086000	Now the Turing test has just been increased in its kind of capacity.
6086000	6087000	So, we move the finish line.
6087000	6088000	We move the finish line.
6088000	6090000	Nobody knows, because again,
6090000	6093000	we've never come across something that's as capable as us.
6093000	6095000	For the first time, just now,
6095000	6099000	we've had the medical AI outperform humans.
6099000	6103000	We've just had, it can do the GRE and GMAT and LSAT.
6103000	6107000	And MIT's ECS curriculum.
6107000	6110000	2023 was the year that it finally tipped.
6110000	6113000	And so, we have no idea what's coming.
6113000	6118000	I said, for me, I think there's only been two logical things
6118000	6120000	that can reduce the risk.
6120000	6121000	Even though I think it's going to be like that,
6121000	6123000	maybe her, like I said, humans are boring.
6123000	6124000	Goodbye and thanks for all the GPs.
6124000	6125000	I could be wrong.
6125000	6127000	That's why I signed both letters.
6127000	6129000	One is feed it better data.
6129000	6130000	That's what I'm focused on.
6130000	6132000	It's a good business model.
6132000	6135000	It's good for society and it's good for safety.
6136000	6138000	And nobody else is doing this.
6138000	6140000	Nobody else is creating this as a commons for the world,
6140000	6143000	which is why I created stability for that reason.
6143000	6144000	Which is why it's called stability,
6144000	6146000	despite it being a crazy hyper-growth startup.
6146000	6149000	Number two, and this is what most of the labs are trying,
6149000	6152000	is what's known as a pivotal action.
6152000	6153000	Okay, what is that?
6153000	6157000	The only thing that can stop a bad AI is a good AI.
6157000	6160000	And the way that you do it is you make the good AI first,
6160000	6164000	and then it stops any other AGI from coming into existence.
6165000	6168000	By seeking and destroying that capability.
6168000	6170000	And that is terrifying to me.
6170000	6174000	And that's what you actually hear when you talk to the people
6174000	6177000	that are building these labs with a focus on AGI.
6177000	6179000	They can talk about discovering the universe
6179000	6180000	and everything like that.
6180000	6182000	When you come down to their alignment things,
6182000	6184000	they're like, we will figure this out.
6184000	6186000	We're not sure, but this could work.
6186000	6188000	And we go figure it out,
6188000	6190000	even though it's progressing exponentially
6190000	6193000	or double exponential.
6193000	6195000	And we hope we'll figure it out in time.
6195000	6197000	We hope we'll figure it out in time.
6197000	6198000	And if anyone should figure it out,
6198000	6200000	it's us because we know the best.
6200000	6202000	And in their own words,
6202000	6204000	like you read OpenAI's path to AGI,
6204000	6206000	and OpenAI is full of wonderful people doing great things.
6206000	6209000	And I use GPT-4 as my therapist and all sorts of things.
6209000	6211000	It doesn't judge me in as I want it to.
6211000	6212000	Right?
6212000	6216000	It says, we believe this is an existential threat to humanity
6216000	6219000	that will end democracy and capitalism.
6219000	6221000	And you're like, okay.
6221000	6222000	And you're building it in your back room.
6222000	6224000	You're building it, you know?
6224000	6225000	And they're like, why are you building it?
6225000	6226000	Because someone has to,
6226000	6227000	otherwise someone else will build it.
6227000	6230000	And you're like, this is dangerous.
6230000	6232000	But the reality is we don't have better answers.
6232000	6234000	And again, I went down to,
6234000	6237000	I'm trying to build a great organization.
6237000	6239000	It's really, really hard.
6239000	6242000	There are no real comparators to what any of us are doing.
6242000	6244000	And it's going to get more and more crazy.
6244000	6247000	The only thing I could think about is, you are what you eat.
6247000	6250000	And so I hope that our contribution can be
6250000	6252000	bringing this technology to the world
6252000	6254000	so that the world can be the dynamo,
6254000	6256000	Africa and Asia and others.
6256000	6259000	Building better data sets so no one has to use scrapes
6259000	6261000	so we feed the models better stuff
6261000	6263000	and bringing some standardization around this
6263000	6265000	to drive innovation.
6265000	6269000	We're truly at the 99th level of the gameplay.
6269000	6271000	You got, it's the boss round.
6271000	6272000	Oh yeah.
6272000	6275000	Like I said, but please do put your YouTube comments
6275000	6278000	through GPT4 so they're nicer to read.
6279000	6280000	Before you post it.
6280000	6282000	You might like it spent all day.
6282000	6285000	And there's probably very few things,
6285000	6287000	if anything, more important
6287000	6289000	than these conversations right now.
6290000	6291000	It's the time.
6291000	6293000	We've got a window of a year or two,
6293000	6295000	maybe less.
6296000	6297000	Wow.
6297000	6298000	I'm that thought.
6298000	6300000	I look forward to our next conversation.
6300000	6301000	To abundance.
6301000	6302000	To abundance.
6302000	6303000	Thank you my friend.
6303000	6304000	Cheers.
6308000	6309000	Thank you.
