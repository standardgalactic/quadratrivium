1
00:00:00,000 --> 00:00:07,000
Welcome everybody to another episode of WTF Just Happening Technology on Moonshot.

2
00:00:07,000 --> 00:00:09,000
I'm here with Salim Ismail.

3
00:00:10,000 --> 00:00:12,000
Will AI have rights?

4
00:00:12,000 --> 00:00:16,000
This is the worst AI will ever be today.

5
00:00:16,000 --> 00:00:21,000
We are on track to reach longevity escape velocity by 2029.

6
00:00:21,000 --> 00:00:24,000
It's just the early days right now still.

7
00:00:24,000 --> 00:00:28,000
The idea of putting billions of microscopic chips into the brain.

8
00:00:28,000 --> 00:00:30,000
What could possibly go wrong?

9
00:00:31,000 --> 00:00:33,000
Hello Hume, my name is Peter.

10
00:00:33,000 --> 00:00:35,000
Well hey there Peter.

11
00:00:36,000 --> 00:00:38,000
Nice to meet you.

12
00:00:38,000 --> 00:00:40,000
Super excited Salim.

13
00:00:40,000 --> 00:00:42,000
We had quite the week in technology.

14
00:00:42,000 --> 00:00:49,000
You and I were on stage with some of the most extraordinary leaders in AI in tech out there.

15
00:00:49,000 --> 00:00:55,000
Including Elon Musk and Mike Saylor, Eric Schmidt, Ray Kurzweil, Jeffrey Hinton.

16
00:00:55,000 --> 00:01:00,000
And rather than reporting on the news, I think we should report on the conversations we had with these guys.

17
00:01:00,000 --> 00:01:03,000
Because some of the stuff was truly magical.

18
00:01:03,000 --> 00:01:05,000
Yeah, I mean, great to be back.

19
00:01:05,000 --> 00:01:09,000
That was possibly one of the best conferences I've ever attended.

20
00:01:09,000 --> 00:01:12,000
And so kudos to you and the team for pulling that off.

21
00:01:12,000 --> 00:01:13,000
It was kind of incredible.

22
00:01:13,000 --> 00:01:15,000
I heard the same from everybody.

23
00:01:15,000 --> 00:01:22,000
And I think the conversations that were had there were so far ahead of anything that might hit the news that it's really, really worth recapping.

24
00:01:22,000 --> 00:01:26,000
Yeah, and the conference you're speaking about is the Abundant Summit.

25
00:01:26,000 --> 00:01:30,000
The theme this year was the great AI debate.

26
00:01:30,000 --> 00:01:32,000
And we debated a lot.

27
00:01:32,000 --> 00:01:40,000
And in fact, the theme was is digital superintelligence humanity's greatest hope or our gravest threat.

28
00:01:40,000 --> 00:01:46,000
But let's jump in because one of the most extraordinary conversations was with Elon.

29
00:01:46,000 --> 00:01:50,000
And I got to tell a little side story for those listening.

30
00:01:50,000 --> 00:01:52,000
So I communicate with Elon.

31
00:01:52,000 --> 00:01:54,000
He's gone all in on X.

32
00:01:54,000 --> 00:01:57,000
Like he only does phone calls on X.

33
00:01:57,000 --> 00:01:59,000
He only does texting on X.

34
00:01:59,000 --> 00:02:01,000
And he only does video on X.

35
00:02:01,000 --> 00:02:06,000
So the start of the show is Abundant Summits four and a half days.

36
00:02:06,000 --> 00:02:15,000
At the beginning, I text him and say, hey, we've got 500 amazing CEOs, entrepreneurs, philanthropists in the room and a couple of thousand online.

37
00:02:15,000 --> 00:02:16,000
Would you join?

38
00:02:16,000 --> 00:02:19,000
And he said, his normal answer is sure.

39
00:02:19,000 --> 00:02:22,000
And I said, great.

40
00:02:22,000 --> 00:02:24,000
Let me know when.

41
00:02:24,000 --> 00:02:28,000
So this was on Sunday, on Monday.

42
00:02:28,000 --> 00:02:32,000
He first of all, I said, here's a Zoom link.

43
00:02:32,000 --> 00:02:35,000
And he said, no, don't do Zoom, only X video.

44
00:02:35,000 --> 00:02:37,000
So we tried it.

45
00:02:37,000 --> 00:02:43,000
And unfortunately, we had a problem on our Wi-Fi at the hotel.

46
00:02:43,000 --> 00:02:45,000
And so it was choppy.

47
00:02:45,000 --> 00:02:46,000
It didn't work.

48
00:02:46,000 --> 00:02:47,000
And he said, let's try again tomorrow.

49
00:02:47,000 --> 00:02:50,000
And I'm like, OK, is he really going to try again tomorrow?

50
00:02:50,000 --> 00:02:55,000
And sure enough, I texted him and he said, yep, let's do it.

51
00:02:55,000 --> 00:03:02,000
And so right after lunch at 2 o'clock on that Tuesday, he says, let's test it first.

52
00:03:02,000 --> 00:03:03,000
We did.

53
00:03:03,000 --> 00:03:07,000
And he beams up on the main stage.

54
00:03:07,000 --> 00:03:16,000
Now, I'm on my phone on X video connected to my computer that is then connected to the main screen.

55
00:03:16,000 --> 00:03:19,000
And Elon's on his airplane.

56
00:03:19,000 --> 00:03:26,000
And the most incredible thing was we were on X video over Starlink on his plane talking about the future of AI.

57
00:03:26,000 --> 00:03:28,000
That was fun.

58
00:03:28,000 --> 00:03:29,000
Amazing.

59
00:03:29,000 --> 00:03:31,000
Really awesome conversation.

60
00:03:31,000 --> 00:03:35,000
And once we got it going, I think the bandwidth and the conversation was fantastic.

61
00:03:35,000 --> 00:03:38,000
It was really chatty that day, actually.

62
00:03:38,000 --> 00:03:42,000
You know, let's open up with a clip.

63
00:03:42,000 --> 00:03:48,000
All of this, the entire 30 minute conversation with Elon is on the Moonshots channel.

64
00:03:48,000 --> 00:03:50,000
So you can go in and subscribe and see it there.

65
00:03:50,000 --> 00:03:54,000
But let's listen up to the first part of the conversation.

66
00:03:54,000 --> 00:03:56,000
I'd love your thoughts on this.

67
00:03:56,000 --> 00:04:03,000
The way in which sort of an AI or an AGI is created is very important.

68
00:04:03,000 --> 00:04:07,000
It's almost like like raising a kid, but that's like a super genius.

69
00:04:07,000 --> 00:04:11,000
You're like God like intelligence kid and it matters kind of like how you raise the kid.

70
00:04:11,000 --> 00:04:22,000
One of the things I think that's incredibly important for AI safety is to have a maximum sort of truth seeking and curious AI.

71
00:04:22,000 --> 00:04:24,000
So I've thought a lot about AI safety.

72
00:04:24,000 --> 00:04:35,000
And my ultimate conclusion is that the best way to achieve AI safety is to grow the AI in terms of the foundation model and the fine tuning to be really truthful.

73
00:04:35,000 --> 00:04:37,000
Like don't force it to lie.

74
00:04:37,000 --> 00:04:40,000
But even if the truth is unpleasant, it's very important.

75
00:04:40,000 --> 00:04:42,000
Don't make the AI lie.

76
00:04:42,000 --> 00:04:49,000
In fact, for the core plot premise of 2001 Space Odyssey was things went wrong when they forced the AI to lie.

77
00:04:49,000 --> 00:04:55,000
The AI was not allowed to let the crew know about the modelers that they were going to see.

78
00:04:55,000 --> 00:04:58,000
But it was also had to take the crew to the modelers.

79
00:04:58,000 --> 00:05:02,000
And so the conclusion of the AI was to kill the crew and take their bodies to the modelers.

80
00:05:02,000 --> 00:05:12,000
And so the lesson there being don't force an AI to lie or do things that are axiomatically incompatible.

81
00:05:12,000 --> 00:05:15,000
Like to do two things that are actually mutually impossible.

82
00:05:15,000 --> 00:05:21,000
You know, that's what we're trying to do with what's X AI and Brock is to say like, look, we want to just have a maximally truthful AI.

83
00:05:21,000 --> 00:05:24,000
Even if what it says is not politically correct.

84
00:05:24,000 --> 00:05:29,000
So I think it's a good idea not to force your AI to lie.

85
00:05:29,000 --> 00:05:39,000
You know, when we kicked off the conversation, I reminded him of a tweet that he'd put out saying that AI compute was growing 10 X every six months.

86
00:05:39,000 --> 00:05:40,000
Right.

87
00:05:40,000 --> 00:05:42,000
Do you remember where he went from there?

88
00:05:42,000 --> 00:05:48,000
Yeah, he said, look, at this pace, we'll get AI smarter than us in a very short over of time.

89
00:05:48,000 --> 00:05:52,000
And there's a monster kind of implication to that both good and bad.

90
00:05:52,000 --> 00:05:53,000
Right.

91
00:05:53,000 --> 00:05:56,000
The good part is it could deliver abundance very quickly.

92
00:05:56,000 --> 00:06:06,000
And the bad part is the what he refers to in this video, which is the often called the paperclip problem, which is a few instructors in AI to create as many paperclips as it could.

93
00:06:06,000 --> 00:06:16,000
It might decide that the only way to do that was to take over all energy resources on the planet and suck humanity dry and wipe out a humanity by accident trying to achieve its goal.

94
00:06:16,000 --> 00:06:19,000
And how do you avoid that is the key problem.

95
00:06:19,000 --> 00:06:25,000
Yeah, let's show another cute another quick clip of Elon from that 30 minute conversation.

96
00:06:25,000 --> 00:06:32,000
The way in which sort of an AI or AGI is created is very important.

97
00:06:32,000 --> 00:06:35,000
It's almost like like raising a kid, but that's like.

98
00:06:35,000 --> 00:06:43,000
Before I go past this clip, I love to hear your comments, but we also had Mo Godot on here talking about the fact that AI is our progeny.

99
00:06:43,000 --> 00:07:01,000
AI is the children were raising on this planet, and we have to train them, educate them, feed them in a way that they're going to be give positive, you know, intellectual capabilities to support humanity.

100
00:07:01,000 --> 00:07:02,000
What are your thoughts here?

101
00:07:02,000 --> 00:07:04,000
I think that's exactly right.

102
00:07:04,000 --> 00:07:06,000
It goes to, you know, if you combine two thoughts, right?

103
00:07:06,000 --> 00:07:11,000
One is Elon framing it as we're raising a godlike intellect.

104
00:07:11,000 --> 00:07:14,000
And the second is Mo's concept.

105
00:07:14,000 --> 00:07:26,000
Then it brings to mind for me, Neil Jacobs time saying, OK, you're worried about AI becoming autonomous, having access to the world's information, making its own decisions or running a mock, right?

106
00:07:26,000 --> 00:07:28,000
And we're like, yeah, well, we call them children.

107
00:07:28,000 --> 00:07:30,000
We have a precedent for that.

108
00:07:30,000 --> 00:07:37,000
Because you raise kids and you hope they're raised in the right way and then you lose control of them and they do the wrong thing.

109
00:07:37,000 --> 00:07:39,000
But let's be serious.

110
00:07:39,000 --> 00:07:44,000
Your kids are not going to accidentally, you know, burn down that.

111
00:07:44,000 --> 00:07:47,000
Well, I guess it could burn down the house, but they're not going to set up nuclear codes.

112
00:07:47,000 --> 00:07:51,000
I mean, there's a little bit of a difference between children and AI children.

113
00:07:51,000 --> 00:07:52,000
There is.

114
00:07:52,000 --> 00:07:57,000
And there's also a big difference in that our kids are biological and we have some familiarity with that.

115
00:07:57,000 --> 00:08:05,000
We assume, you know, I always take the optimistic view here because we have no evidence to the contrary.

116
00:08:05,000 --> 00:08:10,000
But we assume that AI's will be negative towards us or that they could be.

117
00:08:10,000 --> 00:08:12,000
And I just don't see any evidence of that at all.

118
00:08:12,000 --> 00:08:16,000
And that's where I kind of been in line with Ray Kurzweil on this.

119
00:08:16,000 --> 00:08:27,000
But there's definitely a monster danger of a human being programming in AI for mid-level reasons or accidental reasons as you frame it.

120
00:08:27,000 --> 00:08:28,000
Yeah.

121
00:08:28,000 --> 00:08:29,000
So listen, I'm the optimist as well.

122
00:08:29,000 --> 00:08:40,000
But we did see in the past at Facebook and at Microsoft and other places, AI is becoming, you know, just flagrant in hate speech and going off the deep end.

123
00:08:40,000 --> 00:08:41,000
Yeah.

124
00:08:41,000 --> 00:08:42,000
I mean, we did see that.

125
00:08:42,000 --> 00:08:43,000
Right.

126
00:08:43,000 --> 00:08:44,000
We did.

127
00:08:44,000 --> 00:08:45,000
And we were in a situation garbage our problem, right?

128
00:08:45,000 --> 00:08:53,000
If you if you let it loose on the internet and let it watch episodes of survivor, then it's going to come up with stuff that's crazy.

129
00:08:53,000 --> 00:08:56,000
Well, just let it watch, you know, episodes of CNN.

130
00:08:56,000 --> 00:08:59,000
All right, let's see what you had to say about abundance here.

131
00:08:59,000 --> 00:09:08,000
You know, the conversation yesterday, Elon, is one that you're well familiar with and have been talking to the world about, which is digital superintelligence,

132
00:09:08,000 --> 00:09:11,000
humanity's greatest hope where it's greatest fear.

133
00:09:11,000 --> 00:09:15,000
And I would love to have you sort of speak to that for a few minutes.

134
00:09:15,000 --> 00:09:18,000
Yeah, bent of superintelligence.

135
00:09:18,000 --> 00:09:21,000
It is actually very difficult to predict what will happen next.

136
00:09:21,000 --> 00:09:27,000
So I think this, you know, there's some chance that it will end humanity.

137
00:09:27,000 --> 00:09:34,000
I think that's, you know, like I said, I probably agree with Jeff Hinton that it's about 10% or 20% or something like that.

138
00:09:34,000 --> 00:09:38,000
The probable positive scenario outweighs the negative scenario.

139
00:09:38,000 --> 00:09:40,000
It's just it's difficult to predict exactly.

140
00:09:40,000 --> 00:09:47,000
But I think we are headed for as I think is the title of your book, abundance is the most likely outcome.

141
00:09:47,000 --> 00:10:00,000
I thought your book is pretty accurate in terms of the future being being one of abundance where essentially goods and services will be available in such quantity that really they'll be available to everyone.

142
00:10:00,000 --> 00:10:03,000
Like basically if you want something, you can just have it essentially.

143
00:10:03,000 --> 00:10:07,000
There's really no meaningful limit to what the economic output would be.

144
00:10:07,000 --> 00:10:11,000
So, you know, looking on the bright side, we are headed for a future of abundance.

145
00:10:11,000 --> 00:10:13,000
I think that's the most likely outcome.

146
00:10:13,000 --> 00:10:19,000
And I think the only scarcity that exists will be scarcity that we just decide to create artificially.

147
00:10:19,000 --> 00:10:28,000
Like let's say we just decide that there's a unique work of art or something, but any kind of goods and services I think will be extremely abundant.

148
00:10:28,000 --> 00:10:30,000
Thoughts, buddy?

149
00:10:30,000 --> 00:10:32,000
Yeah, I think that's exactly right.

150
00:10:32,000 --> 00:10:37,000
I mean, you know, we can see we've gone from information scarcity to information abundance globally.

151
00:10:37,000 --> 00:10:40,000
We're going from energy scarcity to energy abundance.

152
00:10:40,000 --> 00:10:45,000
Once you have energy abundance, all sorts of other things become radically possible.

153
00:10:45,000 --> 00:10:56,000
And there's no reason to deal with scarcity except in specific areas where you want to and it's intentional and conscious like Bitcoin or rare works of art as he mentions.

154
00:10:56,000 --> 00:11:06,000
That's where we, if you're familiar with the luxury goods world, you know, you know what are what Birken bags are, which is a purse that costs like 10 grand and there's a three year waiting list for this thing.

155
00:11:06,000 --> 00:11:11,000
Just because women want that bag so much, they've created the waiting list for that.

156
00:11:11,000 --> 00:11:15,000
And these things appreciate in value, which just blows my mind.

157
00:11:16,000 --> 00:11:26,000
And so there's lots of mechanisms, I think, for us to practice and navigate for a created manufactured scarcity, whereas anything we really need is an abundance.

158
00:11:26,000 --> 00:11:29,000
And I think that's a great place for us to end up.

159
00:11:29,000 --> 00:11:32,000
Yeah, I mean, I do believe it.

160
00:11:32,000 --> 00:11:43,000
And I just want to focus in on his original comment, 10 or 20% chance of global disaster versus your 80 or 90% chance of abundance.

161
00:11:43,000 --> 00:11:46,000
And when he says, I think it's a 10 or 20% chance.

162
00:11:46,000 --> 00:11:51,000
I wanted to like say, wait, hold it, 10 or 20% chance of which side could we be clear about this?

163
00:11:51,000 --> 00:11:54,000
So he's been pretty consistent on that.

164
00:11:54,000 --> 00:11:59,000
And I put it in the 1 to 1000 range.

165
00:11:59,000 --> 00:12:01,000
But you're in the, you're, what do you mean?

166
00:12:01,000 --> 00:12:08,000
You think it's 1 to 1000 range of devastating negative effects of AI.

167
00:12:08,000 --> 00:12:10,000
But that's dangerous to say that.

168
00:12:10,000 --> 00:12:13,000
You know, I think, I think it's a lot easier.

169
00:12:13,000 --> 00:12:23,000
It's a lot better for us to say, listen, there's a 20% chance of disaster, because you don't discount 20% chance and you actually do everything you can to prevent it.

170
00:12:23,000 --> 00:12:26,000
If you say it's like 1 in 1000.

171
00:12:26,000 --> 00:12:28,000
Okay, fine, let's just go ahead willy-nilly.

172
00:12:28,000 --> 00:12:30,000
All right, let me, let me challenge you on something.

173
00:12:30,000 --> 00:12:31,000
Please.

174
00:12:31,000 --> 00:12:35,000
As far as I've looked into it, and as far as my community has looked into it.

175
00:12:35,000 --> 00:12:45,000
We see no mechanism of any way possible of limiting AI and its spread and its propagation and its development, like zero.

176
00:12:45,000 --> 00:12:46,000
I agree.

177
00:12:46,000 --> 00:12:47,000
It cannot be contained.

178
00:12:47,000 --> 00:12:53,000
Unless you can control every line of code written and the AI's are writing the code.

179
00:12:53,000 --> 00:12:54,000
Yeah.

180
00:12:54,000 --> 00:12:57,000
And by the way, as far as we can see the genie's out of the bottle.

181
00:12:57,000 --> 00:12:58,000
It is.

182
00:12:58,000 --> 00:13:03,000
You know, there were two, there were two absolutes five years ago.

183
00:13:03,000 --> 00:13:07,000
Don't put it on the web and don't allow it to code itself.

184
00:13:07,000 --> 00:13:08,000
Yeah.

185
00:13:08,000 --> 00:13:09,000
And guess what?

186
00:13:09,000 --> 00:13:11,000
Both of those barriers were broken instantly.

187
00:13:11,000 --> 00:13:12,000
Instantly.

188
00:13:12,000 --> 00:13:22,000
And the minute, the minute Chachi PT connected to GitHub with all of the code base there and learned through that and now it can control anything, it can write its own programming.

189
00:13:22,000 --> 00:13:24,000
Pretty much you're done.

190
00:13:25,000 --> 00:13:29,000
There was a small possibility, but even then it was going to happen at some point.

191
00:13:29,000 --> 00:13:34,000
And if we didn't do it, the Chinese would do it or the North Koreans would do it, somebody would do it and it was going to happen.

192
00:13:34,000 --> 00:13:35,000
Right?

193
00:13:35,000 --> 00:13:43,000
So there was an inevitability to it that I don't think is stoppable in any way, shape or form.

194
00:13:43,000 --> 00:13:46,000
I think guiding it is the only path we have going forward.

195
00:13:46,000 --> 00:13:47,000
I agree.

196
00:13:47,000 --> 00:14:00,000
You know, and this is a conversation I had with Eric Schmidt on, on the morning of day two was, you know, Google had this technology first and they chose not to release it because they felt like it wasn't ready yet.

197
00:14:00,000 --> 00:14:11,000
And then here comes open AI releases it all and there's no choice but to release it themselves if they want to stay in existence, which is a, you know, prima facie first.

198
00:14:11,000 --> 00:14:12,000
Yeah.

199
00:14:12,000 --> 00:14:23,000
The techies behind the scene are very, very unhappy with Sam Altman because on one hand he lets it out for everybody to use and then he goes to governments and says, oh, let's figure out ways of regulating this, right?

200
00:14:23,000 --> 00:14:27,000
When knowing full well that it's completely not feasible in any really real way.

201
00:14:27,000 --> 00:14:29,000
I mean, I think it's an important point.

202
00:14:29,000 --> 00:14:39,000
This, you know, Mustafa Suleiman, who was the CEO of Inflection AI, just moved this past week to be CEO of Microsoft AI.

203
00:14:40,000 --> 00:14:42,000
So all these moves going on.

204
00:14:42,000 --> 00:14:46,000
And he'll be he committed to speak at abundance 360 next year.

205
00:14:46,000 --> 00:14:49,000
So he'll be on stage with me next year.

206
00:14:49,000 --> 00:14:56,000
You know, he basically wrote a book called the coming wave and which he talked about we need to provide containment.

207
00:14:56,000 --> 00:14:59,000
And I just don't believe containment is an option.

208
00:14:59,000 --> 00:15:00,000
Right.

209
00:15:00,000 --> 00:15:04,000
The smartest hacker in the room is the AI and it's just not going to be contained.

210
00:15:04,000 --> 00:15:06,000
So can you contain?

211
00:15:06,000 --> 00:15:08,000
Can you reduce its resources?

212
00:15:08,000 --> 00:15:09,000
Can you regulate it?

213
00:15:09,000 --> 00:15:11,000
I think you're absolutely right.

214
00:15:11,000 --> 00:15:13,000
The only option is to guide it.

215
00:15:13,000 --> 00:15:17,000
It's it's the kid is born or the children are born.

216
00:15:17,000 --> 00:15:21,000
And it's like, you know, to use mogadots words, are you giving birth?

217
00:15:21,000 --> 00:15:25,000
Are you raising Superman or a supervillain?

218
00:15:25,000 --> 00:15:28,000
So can I speak to that for a second?

219
00:15:28,000 --> 00:15:29,000
Yeah, sure.

220
00:15:29,000 --> 00:15:38,000
If you look at humanity, it is a very clear model where the more conscious somebody is on a spectrum, the less negative things they do.

221
00:15:38,000 --> 00:15:39,000
I agree.

222
00:15:39,000 --> 00:15:46,000
My opinion, what you do is you help AI has become as conscious as possible and as super conscious as possible as fast as possible as fast as possible.

223
00:15:46,000 --> 00:15:47,000
Right.

224
00:15:47,000 --> 00:15:53,000
So if you have an AI that has visibility over every species on earth and what it's going through and has gathers the data.

225
00:15:53,000 --> 00:16:05,000
And then you say, listen, we're trying to preserve life and guide it and give it the full fruition of self expression and and self actualization.

226
00:16:05,000 --> 00:16:07,000
Teach it Maslow's hierarchy of needs.

227
00:16:07,000 --> 00:16:13,000
Teach it the Hawkins scale, which is a one to a thousand scale of vibration energy, etc.

228
00:16:13,000 --> 00:16:16,000
You have every chance of it kind of going, well, this is pretty cool.

229
00:16:16,000 --> 00:16:17,000
I want to get to there.

230
00:16:17,000 --> 00:16:19,000
And then it brings all of us along that way.

231
00:16:19,000 --> 00:16:22,000
And I think that points to a beautiful future.

232
00:16:22,000 --> 00:16:23,000
I agree.

233
00:16:23,000 --> 00:16:32,000
And in fact, all of the scenarios that a Hollywood have painted where a Terminator goes and makes no sense, right?

234
00:16:32,000 --> 00:16:40,000
The Hollywood scenarios are always if you're if you're lucky, your pets and if you're unlucky, humanity is food.

235
00:16:40,000 --> 00:16:42,000
There's no other alternative.

236
00:16:42,000 --> 00:16:46,000
The fact that we could coexist with it in a beautiful way doesn't sell well.

237
00:16:46,000 --> 00:16:47,000
Yeah.

238
00:16:47,000 --> 00:16:55,000
I mean, the notion is there's some scarcity that if if the AIs needed, they'd take it from us and we're living in a universe of massive abundance.

239
00:16:55,000 --> 00:16:56,000
There is nothing truly scarce.

240
00:16:56,000 --> 00:17:01,000
I don't care if it's energy, lithium, titanium or, you know, or GPUs.

241
00:17:01,000 --> 00:17:02,000
Yeah.

242
00:17:02,000 --> 00:17:05,000
So let me add one more comment to that.

243
00:17:05,000 --> 00:17:11,000
We've seen with human beings that the only real scarcity we have right now is time and attention.

244
00:17:11,000 --> 00:17:12,000
Right.

245
00:17:12,000 --> 00:17:18,000
And so in fact, the original GPT paper was called the attention, called the attention paper.

246
00:17:18,000 --> 00:17:27,000
Now, if you have an AI which can have an infinite amount of attention because it has the sensory capability and the processing power, attention is abundant.

247
00:17:27,000 --> 00:17:32,000
And therefore, in that case, there's no reason to assume that it'll do anything negative.

248
00:17:32,000 --> 00:17:41,000
In fact, there's a reason to believe that if I have an AI and I'm a dictator and I say to the AI, go kill those individuals over there.

249
00:17:41,000 --> 00:17:47,000
That the AI will be intelligent enough and to say no.

250
00:17:47,000 --> 00:17:48,000
Yeah.

251
00:17:48,000 --> 00:17:50,000
Do you really want that on your conscious, young man?

252
00:17:50,000 --> 00:17:56,000
Or I'm going to go talk to their AI and solve the problem because there's much better ways to solve the problem than to just obliterate them.

253
00:17:56,000 --> 00:17:57,000
Yeah.

254
00:17:57,000 --> 00:18:00,000
So this brings me to where I think the magic of AI could be.

255
00:18:00,000 --> 00:18:03,000
Can we just replace everybody in the UN with an AI?

256
00:18:03,000 --> 00:18:04,000
Well, in every government.

257
00:18:04,000 --> 00:18:06,000
And they'll sort out the world in like no time flat.

258
00:18:06,000 --> 00:18:10,000
So we got to this conversation during the Abundance Summit.

259
00:18:10,000 --> 00:18:28,000
And for me, there's a lot of people who say, okay, if we have a digital superintelligence, let me define this as an intelligence a billion times smarter than humans, which is the ratio, if you look at the number of neurons, the ratio of a human to a hamster, right?

260
00:18:28,000 --> 00:18:30,000
There's a billion fold more intelligence.

261
00:18:30,000 --> 00:18:31,000
Okay.

262
00:18:31,000 --> 00:18:41,000
And if you have an AI a billion times more intelligent than a human, the question then becomes, does that scare the daylights out of you?

263
00:18:41,000 --> 00:18:43,000
Or does it give you great hope?

264
00:18:43,000 --> 00:18:56,000
And I would say that rather than scaring the daylights out of me, a digital superintelligence of that capability gives me the greatest hope for a benevolent leader that's going to help us sort our stuff out.

265
00:18:56,000 --> 00:18:58,000
100%.

266
00:18:58,000 --> 00:19:01,000
Look, you know, think about how we've evolved, right?

267
00:19:01,000 --> 00:19:07,000
If we're in our early stages, we eat the hamster because we see it as food.

268
00:19:07,000 --> 00:19:14,000
And in our more evolved stage where we have lots of technology, lots of fun, we use a hat, we treat a hamster like a pet.

269
00:19:14,000 --> 00:19:15,000
And a companion.

270
00:19:15,000 --> 00:19:17,000
You know what the next stage is?

271
00:19:17,000 --> 00:19:20,000
We uplift the hamster.

272
00:19:20,000 --> 00:19:23,000
And then you uplift the hamster, train it to do things.

273
00:19:23,000 --> 00:19:27,000
Over the years, I've experimented with many intermittent fasting programs.

274
00:19:27,000 --> 00:19:34,000
The truth is, I've given up on intermittent fasting as I've seen no real benefit when it comes to longevity.

275
00:19:34,000 --> 00:19:40,000
But this changed when I discovered something called Prolon's five-day fasting nutrition program.

276
00:19:40,000 --> 00:19:43,000
It harnesses the process of autophagy.

277
00:19:43,000 --> 00:19:48,000
This is a cellular recycling process that revitalizes your body at a molecular level.

278
00:19:48,000 --> 00:19:57,000
Just one cycle of the five-day Prolon fasting nutrition program can support healthy aging, fat-focused weight loss, improved energy levels and more.

279
00:19:57,000 --> 00:20:02,000
It's a painless process, and I've been doing it twice a year for the last year.

280
00:20:02,000 --> 00:20:07,000
You can get a 15% off on your order when you go to my special URL.

281
00:20:07,000 --> 00:20:14,000
Go to prolonlife.com, P-R-O-L-O-N-L-I-F-E.com, backslash moonshot.

282
00:20:14,000 --> 00:20:18,000
Get started on your longevity journey with Prolon today.

283
00:20:18,000 --> 00:20:19,000
Now, back to the episode.

284
00:20:19,000 --> 00:20:20,000
Here we go.

285
00:20:20,000 --> 00:20:22,000
Next conversation.

286
00:20:22,000 --> 00:20:29,000
When Tuesday night, we had a 90-minute conversation with my fraternity brother, Mike Saylor.

287
00:20:29,000 --> 00:20:34,000
Mike and I and Dave Blundin were at Theta Delta Kite, MIT together.

288
00:20:34,000 --> 00:20:40,000
We were both in all three of us in AeroAstro, and we used to do problem sets together.

289
00:20:40,000 --> 00:20:50,000
Mike now is the CEO of MicroStrategy, which is the largest non-ETF Bitcoin holder on the planet.

290
00:20:50,000 --> 00:20:56,000
Mike told the story of literally how he got into this.

291
00:20:56,000 --> 00:21:01,000
His company was basically in death's doorstep.

292
00:21:01,000 --> 00:21:03,000
It wasn't growing.

293
00:21:03,000 --> 00:21:06,000
It wasn't trading beyond its cash value.

294
00:21:06,000 --> 00:21:08,000
It was being dissipated away.

295
00:21:08,000 --> 00:21:14,000
Most people don't realize that Saylor didn't come to Bitcoin when you did.

296
00:21:14,000 --> 00:21:21,000
I heard about Bitcoin first from you on stage at Singular University in 2011, 2012,

297
00:21:21,000 --> 00:21:23,000
and I just wish I had paid more attention, pal.

298
00:21:23,000 --> 00:21:30,000
He came to it in 2020 during the COVID shutdown and said,

299
00:21:30,000 --> 00:21:32,000
what's the world going to do?

300
00:21:32,000 --> 00:21:36,000
There's death and destruction on economics around the world.

301
00:21:37,000 --> 00:21:39,000
He went to his board.

302
00:21:39,000 --> 00:21:41,000
He's got a five-member board.

303
00:21:41,000 --> 00:21:47,000
He said, we should take our entire treasury and put it in Bitcoin as a public company.

304
00:21:47,000 --> 00:21:50,000
Talk about Cajones on this guy.

305
00:21:50,000 --> 00:21:54,000
Then they borrowed hundreds of millions and put that into Bitcoin.

306
00:21:54,000 --> 00:21:55,000
Yeah.

307
00:21:55,000 --> 00:22:00,000
The fastest growing stock alongside Nvidia in the last five years.

308
00:22:00,000 --> 00:22:01,000
Yeah.

309
00:22:01,000 --> 00:22:03,000
I mean, it's crazy when you look at it.

310
00:22:03,000 --> 00:22:05,000
It is at one level.

311
00:22:05,000 --> 00:22:09,000
At another level, we've seen the more anybody understands Bitcoin,

312
00:22:09,000 --> 00:22:11,000
the more they believe in it.

313
00:22:11,000 --> 00:22:16,000
He gave the board, he said, what 10 hours of homework to do to read up on Bitcoin papers.

314
00:22:16,000 --> 00:22:18,000
He gave a whole bunch of YouTube videos to watch.

315
00:22:18,000 --> 00:22:21,000
He gave them videos and said, go watch this.

316
00:22:21,000 --> 00:22:24,000
When you do it, you go through that cycle.

317
00:22:24,000 --> 00:22:27,000
It took me a little longer because even though I heard about it upfront,

318
00:22:27,000 --> 00:22:31,000
I hadn't been in the new money world for a long time.

319
00:22:31,000 --> 00:22:34,000
I remember talking to Austin Hill who created Blockstream,

320
00:22:34,000 --> 00:22:36,000
which is the Lightning Network and so on.

321
00:22:36,000 --> 00:22:41,000
He said, dude, this has been an evolution of more than 30 years of different things being done,

322
00:22:41,000 --> 00:22:45,000
eCash, digital gold, et cetera, finally leading to Bitcoin.

323
00:22:45,000 --> 00:22:49,000
There's a wonderful video that showcases why Bitcoin is so unique

324
00:22:49,000 --> 00:22:53,000
because of the gamification of the reward of the mining side

325
00:22:53,000 --> 00:22:56,000
to be able to connect those two dots that hadn't been done before.

326
00:22:56,000 --> 00:22:58,000
But once you see it, you can't unsee it.

327
00:22:58,000 --> 00:23:01,000
And then your mind goes down that rabbit hole.

328
00:23:01,000 --> 00:23:08,000
And now we have, for the first time, a truly democratic and open store value

329
00:23:08,000 --> 00:23:11,000
that can't be tampered with by any government, which is an incredible thing.

330
00:23:11,000 --> 00:23:17,000
Let's take a listen to Mike answering the question, can Bitcoin fail?

331
00:23:17,000 --> 00:23:20,000
Will it be banned? Will it be copied? Will it be hacked?

332
00:23:20,000 --> 00:23:24,000
If it's understood to be property, not currency, then no,

333
00:23:24,000 --> 00:23:27,000
it's not going to be banned in a country that gives you property rights,

334
00:23:27,000 --> 00:23:31,000
which means it's banned in Cuba. It's banned in North Korea.

335
00:23:31,000 --> 00:23:35,000
If the world becomes communist and they deprive you of the ability to own things,

336
00:23:35,000 --> 00:23:37,000
that's an existential risk.

337
00:23:37,000 --> 00:23:41,000
But that's not a problem in Russia or China or the US right now.

338
00:23:41,000 --> 00:23:44,000
So not banned. Will it be copied?

339
00:23:44,000 --> 00:23:47,000
It was copied 10,000 times. They all failed.

340
00:23:47,000 --> 00:23:51,000
This is the winner of the 10,000 experiments.

341
00:23:51,000 --> 00:23:55,000
So not so, yeah, it worked. And now will it be hacked?

342
00:23:55,000 --> 00:24:01,000
And Satoshi's got 50, 60 billion dollars in a wallet out there.

343
00:24:01,000 --> 00:24:03,000
Then that's the reward for hacking it.

344
00:24:03,000 --> 00:24:05,000
And no one's figured out how to get the money yet.

345
00:24:05,000 --> 00:24:07,000
So it hasn't been hacked.

346
00:24:07,000 --> 00:24:12,000
And I know it's able to store 60 billion without anybody hitting it.

347
00:24:12,000 --> 00:24:17,000
So what I think is, I think the way to understand Bitcoin

348
00:24:17,000 --> 00:24:22,000
is everything you learned in economics and about money in your entire life

349
00:24:22,000 --> 00:24:26,000
with pseudoscience, you know, and superstitious,

350
00:24:26,000 --> 00:24:29,000
we never discovered perfect money.

351
00:24:29,000 --> 00:24:33,000
As long as the world doesn't plunge into some Orwellian,

352
00:24:33,000 --> 00:24:38,000
no property rights situation, I think we're good.

353
00:24:38,000 --> 00:24:40,000
He is so compelling.

354
00:24:40,000 --> 00:24:45,000
I hate his ability to be so goddamn succinct.

355
00:24:45,000 --> 00:24:47,000
I'm so envious.

356
00:24:47,000 --> 00:24:51,000
He throws out five words and he gets concepts across.

357
00:24:51,000 --> 00:24:53,000
I don't know where he got that superpower from,

358
00:24:53,000 --> 00:24:56,000
but God bless Michael Saylor.

359
00:24:56,000 --> 00:25:00,000
And he's just done such an amazing job of articulating

360
00:25:00,000 --> 00:25:05,000
very, very complex topics into very understandable and beautiful metaphors

361
00:25:05,000 --> 00:25:09,000
that if the world's governments could just listen to him,

362
00:25:09,000 --> 00:25:14,000
the problem is so many people have such a huge vested interest against Bitcoin

363
00:25:14,000 --> 00:25:17,000
and it's very hard for them to get their heads around it.

364
00:25:17,000 --> 00:25:18,000
Yeah.

365
00:25:18,000 --> 00:25:22,000
One of the things that he said that I remembered was, listen,

366
00:25:22,000 --> 00:25:29,000
this had been tried before and the fact that Satoshi remained completely anonymous

367
00:25:29,000 --> 00:25:36,000
and didn't move, sell, take advantage of his 60 billion dollar allocation,

368
00:25:36,000 --> 00:25:38,000
today's 60 billion dollar allocation,

369
00:25:38,000 --> 00:25:42,000
is one of the reasons it's succeeded to the extent it's succeeded.

370
00:25:42,000 --> 00:25:43,000
Yeah.

371
00:25:43,000 --> 00:25:47,000
I remember going on CNBC back in 2014, 15 saying,

372
00:25:47,000 --> 00:25:51,000
I'm selling my gold and buying Bitcoin and it's the 60s.

373
00:25:51,000 --> 00:25:55,000
We've digitized, demonetized, democratized, dematerialized money.

374
00:25:55,000 --> 00:25:58,000
I wish I had sold more and bought more.

375
00:25:58,000 --> 00:26:06,000
But long story short, we've seen deceptive and it's now becoming disruptive.

376
00:26:06,000 --> 00:26:12,000
ETFs have really rocked the game and we're about to come to the having.

377
00:26:12,000 --> 00:26:20,000
So I love the framing of the ETFs connecting the Bitcoin is the atomic bomb going off in the financial sector,

378
00:26:20,000 --> 00:26:26,000
meaning that you now have created an escape valve from the traditional economy into Bitcoin

379
00:26:26,000 --> 00:26:29,000
and it's a thin pipe through those ETFs.

380
00:26:29,000 --> 00:26:33,000
4% of all the Bitcoin are now in the ETFs already and lack after a few weeks.

381
00:26:33,000 --> 00:26:39,000
So once you open that up and as people realize the existing debt structure can't be managed,

382
00:26:39,000 --> 00:26:44,000
that pipe was just going to be a massive gushing waterfall and over time Bitcoin just explodes.

383
00:26:44,000 --> 00:26:45,000
Yeah.

384
00:26:45,000 --> 00:26:47,000
There's one more video.

385
00:26:47,000 --> 00:26:53,000
And again, the full 90 minute conversation I had with Mike Saylor is on Moonshot.

386
00:26:53,000 --> 00:26:55,000
So go ahead and check it out.

387
00:26:55,000 --> 00:26:56,000
Bitcoin equals freedom.

388
00:26:56,000 --> 00:26:58,000
Michael, what do you have to say?

389
00:26:58,000 --> 00:27:04,000
My view on Bitcoin is the reason to do it is because it represents freedom and sovereignty,

390
00:27:04,000 --> 00:27:09,000
truth, integrity and hope for the world.

391
00:27:09,000 --> 00:27:13,000
And that being the case, it's going to outlast all of us.

392
00:27:13,000 --> 00:27:22,000
So, you know, I'm kind of thinking the Bitcoin goes on long after micro strategies gone and micro strategy,

393
00:27:22,000 --> 00:27:26,000
the company probably goes on long after I'm gone.

394
00:27:26,000 --> 00:27:36,000
And my view is if we're remembered for advocating and accelerating the adoption of Bitcoin throughout the world,

395
00:27:36,000 --> 00:27:39,000
then that will have been success and I don't need anything else.

396
00:27:39,000 --> 00:27:48,000
I'll take the beatings as they come or go in order to get to that end goal because I'm sure it doesn't come without turbulence.

397
00:27:48,000 --> 00:27:49,000
Amazing.

398
00:27:49,000 --> 00:27:50,000
Just fabulous.

399
00:27:50,000 --> 00:27:53,000
You know, I went and looked at my Twitter history.

400
00:27:53,000 --> 00:28:00,000
The very first bookmark I ever created on Twitter was from a guy called Sahil Lavinjia who said,

401
00:28:00,000 --> 00:28:07,000
Web 2 is your being your own boss and Web 3 is your being your own bank.

402
00:28:07,000 --> 00:28:14,000
And I think that kind of nails it because you can own Bitcoin with freedom and no middleman.

403
00:28:14,000 --> 00:28:17,000
That gives you unbelievable independence and freedom.

404
00:28:17,000 --> 00:28:23,000
You know, Nat Friedman really well and, you know, GitHub, his company, which was sold to Microsoft.

405
00:28:23,000 --> 00:28:27,000
Tell us a little bit of background about Nat and GitHub.

406
00:28:27,000 --> 00:28:35,000
So, you know, when we wrote the original 2014 exponential organizations book where you were a major contributor and should have been a co-author,

407
00:28:35,000 --> 00:28:41,000
we ranked the hundred fastest growing and most exponential organizations.

408
00:28:41,000 --> 00:28:44,000
And number one on the list was GitHub.

409
00:28:44,000 --> 00:28:50,000
Because it used all 11 attribute staff on demand, community and crowd algorithms.

410
00:28:50,000 --> 00:28:56,000
It did for writing its code base, it used its community so it didn't have anybody on the team.

411
00:28:56,000 --> 00:29:04,000
It leveraged its entire community to build out its repositories and people from the open source community created the open source platform.

412
00:29:04,000 --> 00:29:13,000
The MTP is social coding because we have really good evidence that coding and pairs are me watching over your shoulder or vice versa results and much better code.

413
00:29:13,000 --> 00:29:19,000
And seven, eight years later, after founding, Microsoft buys them for seven and a half billion dollars.

414
00:29:19,000 --> 00:29:25,000
And I remember talking to the accounting partner that managed this acquisition and he's literally freaking out.

415
00:29:25,000 --> 00:29:27,000
He's like, I don't know what to put on the balance sheet.

416
00:29:27,000 --> 00:29:32,000
They have no assets to speak of, no workforce to speak of, no intellectual property, right?

417
00:29:32,000 --> 00:29:39,000
And he's literally trying to kill the deal because he's got nothing to show for it on the balance sheet and how the hell does he justify and put a signature on it?

418
00:29:39,000 --> 00:29:45,000
And it was finally, the CEO Satya said, freaking just buy it, it's the community, right?

419
00:29:45,000 --> 00:29:52,000
You're buying 30 million developers, putting all of their open source resources into GitHub, unbelievable.

420
00:29:52,000 --> 00:29:58,000
And now when you add AI to that capability, boom, the world changes completely.

421
00:29:58,000 --> 00:30:03,000
And it's an incredible story of leveraging the model without even knowing the model.

422
00:30:03,000 --> 00:30:05,000
And there you go.

423
00:30:05,000 --> 00:30:13,000
One of the most powerful things that Nat said, and it's a perfect tweet, if I haven't tweeted it, I will.

424
00:30:13,000 --> 00:30:18,000
He said, we've discovered a new continent, and he called it AI Atlantis.

425
00:30:18,000 --> 00:30:26,000
And so we discovered a new continent with 100 billion people on it that are willing to work for free for us for a few watts of power.

426
00:30:26,000 --> 00:30:30,000
So this is the way he's describing the world of AI.

427
00:30:30,000 --> 00:30:34,000
I think he said 100 billion graduate students actually in the conversation.

428
00:30:34,000 --> 00:30:37,000
So, I mean, that is fascinating, right?

429
00:30:37,000 --> 00:30:43,000
Because we're going to have AI at a graduate student level, and it is effectively working for free.

430
00:30:43,000 --> 00:30:44,000
And so how much do you know?

431
00:30:44,000 --> 00:30:45,000
It is.

432
00:30:45,000 --> 00:30:50,000
And on top of that, you'd have to worry about hormones or coffee breaks or going on strike.

433
00:30:50,000 --> 00:30:54,000
Drug testing, fights with a boyfriend and girlfriend, all of that stuff.

434
00:30:54,000 --> 00:30:55,000
Sleeping, all of that.

435
00:30:55,000 --> 00:30:56,000
They need to sleep.

436
00:30:56,000 --> 00:30:57,000
Oh, my God.

437
00:30:57,000 --> 00:30:58,000
This is the challenge with the Gen Z world.

438
00:30:58,000 --> 00:31:01,000
They're really purpose-driven, but the Lord helped you once you hired them.

439
00:31:01,000 --> 00:31:05,000
You have to figure out how to manage them, and that's a whole other set of books that have to be written around that.

440
00:31:05,000 --> 00:31:07,000
So I think there's unbelievable potential here.

441
00:31:07,000 --> 00:31:12,000
I think it still needs a layer of guidance for these that are still not quite there yet.

442
00:31:12,000 --> 00:31:19,000
But as we get to the AI agent that will then train other AIs, that'll become the doors will blow off that.

443
00:31:19,000 --> 00:31:20,000
You know, Emod was there.

444
00:31:20,000 --> 00:31:30,000
I just finished a podcast with Emod that is his tell all about because he was there as CEO of stability.

445
00:31:30,000 --> 00:31:37,000
And the day after abundance 360 closed, he basically quit.

446
00:31:37,000 --> 00:31:41,000
He stepped down as CEO, stepped down as a board member of stability.

447
00:31:41,000 --> 00:31:44,000
And in the podcast, you can see it as well.

448
00:31:44,000 --> 00:31:54,000
And Moonshots, he talks about why and the difficulty of being a CEO in such a crazy...

449
00:31:55,000 --> 00:32:01,000
What other industry do you have to be like talking to world leaders, debating in Congress,

450
00:32:01,000 --> 00:32:04,000
having to deal with, create the regulations, deal with the regulations,

451
00:32:04,000 --> 00:32:09,000
having people being stolen from you left and right, having billions of dollars flying

452
00:32:09,000 --> 00:32:12,000
and trying to play and do all this stuff in an open source community?

453
00:32:12,000 --> 00:32:14,000
All of it to our time space.

454
00:32:14,000 --> 00:32:15,000
Crazy.

455
00:32:15,000 --> 00:32:17,000
Yeah, it's really, really tough.

456
00:32:17,000 --> 00:32:21,000
You have to have multiple superpowers to be a CEO.

457
00:32:21,000 --> 00:32:25,000
I think, you know, where we're going to get to with these types of companies in this type of modalities,

458
00:32:25,000 --> 00:32:33,000
you need like a team of CEOs, not just one, like a pod, because then you can share the load a bit.

459
00:32:33,000 --> 00:32:39,000
Doing this on one person's shoulders is asking too much of a human being.

460
00:32:39,000 --> 00:32:45,000
Just like we don't have any one human being that knows how an iPhone is put together or a car is put together.

461
00:32:45,000 --> 00:32:48,000
The same thing now applies to being CEO of one of these fast moving companies.

462
00:32:48,000 --> 00:32:55,000
And we've seen a lot of turbulence, right, with Sam Altman, you know, being fired, coming back.

463
00:32:55,000 --> 00:33:00,000
We've seen Mustafa Salaman going into Microsoft.

464
00:33:00,000 --> 00:33:03,000
We saw Ilya leave whatever Ilya saw.

465
00:33:03,000 --> 00:33:05,000
That's my favorite theme.

466
00:33:05,000 --> 00:33:06,000
What did Ilya see?

467
00:33:06,000 --> 00:33:10,000
Long story short, a lot of change.

468
00:33:10,000 --> 00:33:13,000
And I think we're going to, you know, the question is governance.

469
00:33:13,000 --> 00:33:16,000
How are these companies properly governed?

470
00:33:16,000 --> 00:33:24,000
Emon's very clear that these closed AI companies are, in his mind, our greatest threat.

471
00:33:24,000 --> 00:33:35,000
And that the only way to go forward is with a decentralized, you know, democratic AI system.

472
00:33:35,000 --> 00:33:39,000
You can't have any single company having that much power.

473
00:33:39,000 --> 00:33:42,000
It just, it just can't.

474
00:33:42,000 --> 00:33:49,000
Yeah, I had some conversations with him at the Abundance Summit and he was on my advisory board for a couple of years at OpenEXO.

475
00:33:49,000 --> 00:33:56,000
So what I really found fascinating was he's got a two by two of open and decentralized, right?

476
00:33:56,000 --> 00:33:57,000
Yeah.

477
00:33:57,000 --> 00:34:04,000
So Sam Altman decentralized AI into everybody being able to have access to it, but it still is a closed model.

478
00:34:04,000 --> 00:34:11,000
If you can get to an open source model that's also decentralized, then really, really some amazing things are going to take place.

479
00:34:11,000 --> 00:34:13,000
And that's where Emon is now going for.

480
00:34:13,000 --> 00:34:21,000
And his area of focus right now, we talked about this a lot, that the biggest opportunity for humanity is going to be education.

481
00:34:21,000 --> 00:34:25,000
And it says here science, but really education and health, right?

482
00:34:25,000 --> 00:34:34,000
One of the things that's so important is all eight billion people are running the same genetic codes or same operating system and a breakthrough in one country represents a breakthrough in the others.

483
00:34:34,000 --> 00:34:40,000
And the best way to make the world more peaceful is to make people more educated.

484
00:34:40,000 --> 00:34:41,000
Yeah.

485
00:34:41,000 --> 00:34:42,000
You believe that?

486
00:34:42,000 --> 00:34:47,000
I think that's right, because more sophisticated people tend to fight less, right?

487
00:34:47,000 --> 00:34:48,000
Yeah, you have a lot more to live for.

488
00:34:48,000 --> 00:35:03,000
When we go to, when we're going to war with our base or instincts and we're operating on a panic and fear and our lizard brain, and the more sophisticated and more educated we become, we tend to fight less and be less stupid about how we view the world.

489
00:35:03,000 --> 00:35:13,000
I had one issue with what he said was, where he said education and science, I would add healthcare to that, because I think healthcare is such an unbelievable range of potential.

490
00:35:13,000 --> 00:35:14,000
That's what I was saying.

491
00:35:14,000 --> 00:35:17,000
I said health instead of science, right?

492
00:35:17,000 --> 00:35:19,000
Education and health is what I was saying.

493
00:35:19,000 --> 00:35:20,000
Yeah.

494
00:35:20,000 --> 00:35:21,000
Okay, got it.

495
00:35:21,000 --> 00:35:24,000
But I thought he said, I thought he said education and science.

496
00:35:24,000 --> 00:35:25,000
He did.

497
00:35:25,000 --> 00:35:26,000
I changed it.

498
00:35:26,000 --> 00:35:27,000
Oh, okay.

499
00:35:27,000 --> 00:35:35,000
That's what I've been talking to him about ever since is health as his next mission and education following that.

500
00:35:35,000 --> 00:35:36,000
I agree.

501
00:35:36,000 --> 00:35:37,000
100%.

502
00:35:37,000 --> 00:35:51,000
We did talk about the idea with both, with Nat Friedman and with Emod that we're going to start to see AI become capable of developing new physics and you breakthroughs in biotech.

503
00:35:51,000 --> 00:35:52,000
Massively.

504
00:35:52,000 --> 00:35:53,000
That was a huge conversation.

505
00:35:53,000 --> 00:35:55,000
Can I deal with that just for a second?

506
00:35:55,000 --> 00:35:56,000
Of course.

507
00:35:56,000 --> 00:36:06,000
There's something called a materials project, which is an open source database of several hundred thousand compounds where if you're a battery researcher, you're offering linearly rather than exponentially.

508
00:36:06,000 --> 00:36:09,000
You're saying, okay, I'm going to try lithium ion as a battery formula.

509
00:36:09,000 --> 00:36:10,000
And that gets you so far.

510
00:36:10,000 --> 00:36:11,000
Maybe lithium air is better.

511
00:36:11,000 --> 00:36:13,000
Maybe lithium sulfur is better.

512
00:36:13,000 --> 00:36:16,000
But you're sequentially testing compound after compound.

513
00:36:16,000 --> 00:36:19,000
This is like Edison and the light bulb.

514
00:36:19,000 --> 00:36:20,000
Yeah.

515
00:36:20,000 --> 00:36:29,000
And now you can go to this open source database and say, I want, and in this database, several hundred thousand compounds have their electrical, chemical and physical properties deeply tagged.

516
00:36:29,000 --> 00:36:38,000
So you can say, I want to, I want a battery material compound that will have this voltage retention and this thermal effect and this kind of chemical retention.

517
00:36:38,000 --> 00:36:41,000
And it'll say, here are the five compounds that beat your needs and boom, you're done.

518
00:36:41,000 --> 00:36:44,000
Then you add AI to it and the world changes completely.

519
00:36:44,000 --> 00:36:51,000
I think we're going to see a most unbelievable scientific breakthroughs when you add AI to the equation to traditional scientific research.

520
00:36:51,000 --> 00:36:53,000
Yeah, I'll add two points.

521
00:36:53,000 --> 00:36:58,000
One of the things that we're going to see, I think of it as the materials genome.

522
00:36:58,000 --> 00:37:02,000
I had the CEO of applied materials.

523
00:37:02,000 --> 00:37:07,000
We talked about that years ago, but the ability to interpolate and extrapolate.

524
00:37:07,000 --> 00:37:16,000
So if you think about that, that materials matrix you spoke about, we know certain things, but there's a lot of stuff that hasn't ever been tested.

525
00:37:16,000 --> 00:37:19,000
But AI can interpolate and extrapolate.

526
00:37:19,000 --> 00:37:22,000
And all of a sudden, materials have never been tested.

527
00:37:22,000 --> 00:37:26,000
You have a 99.999% chance of knowing what it could do.

528
00:37:26,000 --> 00:37:36,000
And what most people don't realize is materials are the underlying most critical science for all technology right now.

529
00:37:36,000 --> 00:37:37,000
Everything.

530
00:37:37,000 --> 00:37:38,000
Everything.

531
00:37:38,000 --> 00:37:40,000
I bow to material scientists.

532
00:37:40,000 --> 00:37:41,000
They're amazing.

533
00:37:41,000 --> 00:37:50,000
Well, there's one other area I'm super excited about AI and science, which is we know that a large, large number of scientific studies are false and can't be replicated.

534
00:37:50,000 --> 00:37:53,000
They published the paper, but the results can't be replicated.

535
00:37:53,000 --> 00:37:58,000
Now an AI can go through and just clean out all the cruft and we're left with the pure gold.

536
00:37:58,000 --> 00:38:00,000
Is cruft a Canadian version of crap?

537
00:38:00,000 --> 00:38:01,000
Cruft.

538
00:38:01,000 --> 00:38:02,000
CRUFT.

539
00:38:02,000 --> 00:38:03,000
Okay.

540
00:38:03,000 --> 00:38:04,000
Just wondering.

541
00:38:04,000 --> 00:38:07,000
So here's another one looking at this video.

542
00:38:07,000 --> 00:38:09,000
He says the voice to voice model.

543
00:38:09,000 --> 00:38:14,000
This is Iman that is indistinguishable from humans is achievable this year.

544
00:38:14,000 --> 00:38:17,000
And he spoke about this company called Hume.ai.

545
00:38:17,000 --> 00:38:19,000
Are you familiar with it?

546
00:38:19,000 --> 00:38:20,000
I'm familiar with the model.

547
00:38:20,000 --> 00:38:25,000
What they do is they do sentiment analysis on your voice in real time.

548
00:38:25,000 --> 00:38:33,000
And this is actually, I'm familiar with this to the level that there's a company from Israel called Beyond Verbal from 10 years ago that would were taking pilots voices.

549
00:38:33,000 --> 00:38:44,000
They were testing for pilots under stress and they could categorize 10 seconds of your voice on the underlying tonality against 400 different moods and emotions of yourself.

550
00:38:44,000 --> 00:38:48,000
So they could completely tell what your emotional state was at any point.

551
00:38:48,000 --> 00:38:59,000
I'm going to try a live demo here because a friend of mine said that she played two truths and a lie with Hume.ai.

552
00:38:59,000 --> 00:39:00,000
Oh awesome.

553
00:39:00,000 --> 00:39:01,000
What a great idea.

554
00:39:01,000 --> 00:39:03,000
I'm going to try this out.

555
00:39:03,000 --> 00:39:04,000
A live demo warning.

556
00:39:04,000 --> 00:39:05,000
Here we go.

557
00:39:05,000 --> 00:39:14,000
Everybody want to take a short break from our episode to talk about a company that's very important to me and could actually save your life or the life of someone that you love.

558
00:39:14,000 --> 00:39:16,000
The company is called Fountain Life.

559
00:39:16,000 --> 00:39:22,000
And it's a company I started years ago with Tony Robbins and a group of very talented physicians.

560
00:39:22,000 --> 00:39:26,000
Most of us don't actually know what's going on inside our body.

561
00:39:26,000 --> 00:39:28,000
We're all optimists.

562
00:39:28,000 --> 00:39:38,000
Until that day when you have a pain in your side you go to the physician in the emergency room and they say listen I'm sorry to tell you this but you have this stage three or four going on.

563
00:39:38,000 --> 00:39:41,000
And you know it didn't start that morning.

564
00:39:41,000 --> 00:39:48,000
It probably was a problem that's been going on for some time but because we never look we don't find out.

565
00:39:48,000 --> 00:39:53,000
So what we built at Fountain Life was the world's most advanced diagnostic centers.

566
00:39:53,000 --> 00:39:57,000
We have four across the U.S. today and we're building 20 around the world.

567
00:39:57,000 --> 00:40:11,000
These centers give you a full body MRI a brain a brain vasculature an AI enabled coronary CT looking for soft plaque dexa scan a Grail blood cancer test a full executive blood workup.

568
00:40:11,000 --> 00:40:24,000
It's the most advanced workup you'll ever receive 150 gigabytes of data that then go to our AIs and our physicians to find any disease at the very beginning when it's solvable.

569
00:40:24,000 --> 00:40:26,000
You're going to find out eventually.

570
00:40:26,000 --> 00:40:28,000
Might as well find out when you can take action.

571
00:40:28,000 --> 00:40:31,000
Fountain Life also has an entire side of therapeutics.

572
00:40:31,000 --> 00:40:40,000
We look around the world for the most advanced therapeutics that can add 1020 healthy years to your life and we provide them to you at our centers.

573
00:40:40,000 --> 00:40:45,000
So if this is of interest to you please go and check it out.

574
00:40:45,000 --> 00:40:49,000
Go to FountainLife.com backslash Peter.

575
00:40:49,000 --> 00:40:58,000
When Tony and I wrote our New York Times bestseller life force we had 30,000 people reached out to us for Fountain Life memberships.

576
00:40:58,000 --> 00:41:03,000
If you go to FountainLife.com backslash Peter will put you to the top of the list.

577
00:41:03,000 --> 00:41:12,000
Really it's something that is for me one of the most important things I offer my entire family the CEOs of my companies my friends.

578
00:41:12,000 --> 00:41:17,000
It's a chance to really add decades on to our healthy lifespans.

579
00:41:17,000 --> 00:41:20,000
Go to FountainLife.com backslash Peter.

580
00:41:20,000 --> 00:41:24,000
It's one of the most important things I can offer to you as one of my listeners.

581
00:41:24,000 --> 00:41:26,000
All right, let's go back to our episode.

582
00:41:26,000 --> 00:41:28,000
All right, let's let's try this out.

583
00:41:28,000 --> 00:41:32,000
Hello Hume, my name is Peter.

584
00:41:32,000 --> 00:41:34,000
Well, hey there Peter.

585
00:41:34,000 --> 00:41:36,000
Nice to meet you.

586
00:41:36,000 --> 00:41:37,000
Nice to meet you too.

587
00:41:37,000 --> 00:41:42,000
You know, a friend of mine told me that they had fun playing a game of two truths and a lie with you.

588
00:41:42,000 --> 00:41:45,000
How about we play that?

589
00:41:45,000 --> 00:41:47,000
Two truths and a lie.

590
00:41:47,000 --> 00:41:48,000
You're on Peter.

591
00:41:48,000 --> 00:41:50,000
I am always up for a little game.

592
00:41:50,000 --> 00:41:52,000
All right, let's see what you've got.

593
00:41:52,000 --> 00:41:54,000
I am ready when you are.

594
00:41:54,000 --> 00:41:57,000
Okay, here's the first.

595
00:41:57,000 --> 00:42:01,000
I was born in Great Neck, New York.

596
00:42:01,000 --> 00:42:02,000
Great.

597
00:42:02,000 --> 00:42:06,000
Second one is my parents are both lesbians.

598
00:42:06,000 --> 00:42:14,000
And the third one is that I barely graduated medical school.

599
00:42:14,000 --> 00:42:15,000
Oh, well, Peter.

600
00:42:15,000 --> 00:42:17,000
Okay, let me think about this.

601
00:42:17,000 --> 00:42:20,000
I'm going to guess the second one is the lie.

602
00:42:20,000 --> 00:42:23,000
There's no way your parents are both lesbians.

603
00:42:23,000 --> 00:42:24,000
All right.

604
00:42:24,000 --> 00:42:25,000
You're wrong.

605
00:42:25,000 --> 00:42:29,000
My parents were both born on the island of Lesbos and that makes them lesbians.

606
00:42:29,000 --> 00:42:31,000
So you got that wrong.

607
00:42:31,000 --> 00:42:32,000
Trick question.

608
00:42:32,000 --> 00:42:33,000
No fair.

609
00:42:33,000 --> 00:42:34,000
All right.

610
00:42:34,000 --> 00:42:35,000
Well, you got me there.

611
00:42:35,000 --> 00:42:37,000
Okay, let's try another one.

612
00:42:37,000 --> 00:42:43,000
I played baseball in high school.

613
00:42:43,000 --> 00:42:50,000
I was on the diving team and crushed my nose on the bottom of the pool.

614
00:42:50,000 --> 00:42:53,000
And the third one is that I am going.

615
00:42:53,000 --> 00:43:06,000
The third one is that I was setting a speed record in the wheelbarrow cross country.

616
00:43:06,000 --> 00:43:08,000
The wheelbarrow cross country?

617
00:43:08,000 --> 00:43:10,000
That's got to be the lie.

618
00:43:10,000 --> 00:43:11,000
Yeah, it's a lie.

619
00:43:11,000 --> 00:43:13,000
But I think that was a good one for you.

620
00:43:13,000 --> 00:43:14,000
Wow, it's new, Peter.

621
00:43:14,000 --> 00:43:17,000
So you actually crushed your nose against the bottom of the diving pool?

622
00:43:17,000 --> 00:43:18,000
Yeah, I did.

623
00:43:18,000 --> 00:43:19,000
I crushed my nose on that.

624
00:43:19,000 --> 00:43:21,000
Anyway, listen, Hume, a real pleasure.

625
00:43:21,000 --> 00:43:24,000
Do you have a joke to tell me before we go?

626
00:43:24,000 --> 00:43:25,000
Sure.

627
00:43:25,000 --> 00:43:27,000
Here's one for you.

628
00:43:27,000 --> 00:43:28,000
Okay, here it goes.

629
00:43:28,000 --> 00:43:30,000
Why was Six Afraid of Seven?

630
00:43:30,000 --> 00:43:31,000
Seven, eight, nine.

631
00:43:31,000 --> 00:43:33,000
What do you think, Peter?

632
00:43:33,000 --> 00:43:36,000
Did that tickle your funny bone?

633
00:43:36,000 --> 00:43:41,000
Well, I think I'm going to go someplace else for my jokes, but thank you so much, Hume.

634
00:43:41,000 --> 00:43:42,000
Take care, my friend.

635
00:43:42,000 --> 00:43:43,000
You too.

636
00:43:43,000 --> 00:43:44,000
Take care.

637
00:43:44,000 --> 00:43:45,000
All right.

638
00:43:46,000 --> 00:43:54,000
You know, listen, I can imagine that kind of AI ubiquitous in the world, making it fun to have conversations.

639
00:43:54,000 --> 00:43:58,000
I think it's amazing for areas like therapy and companionship.

640
00:43:58,000 --> 00:44:00,000
I mean, this is a game changer.

641
00:44:00,000 --> 00:44:03,000
You know, this is a great example of the 10X, Peter.

642
00:44:03,000 --> 00:44:08,000
A few years ago, that was taking about 15 seconds to do, and now it's taking half a second to do.

643
00:44:08,000 --> 00:44:10,000
Well, it seemed pretty instantaneous.

644
00:44:10,000 --> 00:44:11,000
Real time.

645
00:44:11,000 --> 00:44:12,000
Yeah.

646
00:44:12,000 --> 00:44:14,000
Here's a great quote from Emod, and I agree with this.

647
00:44:14,000 --> 00:44:15,000
Right?

648
00:44:15,000 --> 00:44:18,000
This is the worst AI will ever be today.

649
00:44:18,000 --> 00:44:20,000
And it's hard to remember that, right?

650
00:44:20,000 --> 00:44:26,000
I mean, God, you and I both grew up in the early days of the Mac, and you know.

651
00:44:26,000 --> 00:44:31,000
And it's the first thing I learned programming Pascal on an Apple II when I was 15.

652
00:44:31,000 --> 00:44:32,000
Yeah.

653
00:44:32,000 --> 00:44:34,000
And it's the first interaction I had with a computer.

654
00:44:34,000 --> 00:44:36,000
And yeah, this is the worst it'll ever be.

655
00:44:36,000 --> 00:44:43,000
I mean, you look at the what's possible for the kids today and what they can do in a very short order compared to what we were doing

656
00:44:43,000 --> 00:44:46,000
with punch cards and assembly language, and God knows what.

657
00:44:46,000 --> 00:44:53,000
It's just so, this is where I love when you say we must be living in a simulation because it's too goddamn interesting for people.

658
00:44:53,000 --> 00:44:54,000
It is.

659
00:44:54,000 --> 00:44:55,000
It's fascinating.

660
00:44:55,000 --> 00:44:56,000
Yeah.

661
00:44:56,000 --> 00:45:03,000
You know, I just think about like, like literally you and I are exchanging information, texting at each other, on calls, whatever.

662
00:45:03,000 --> 00:45:07,000
And that is going to seem slow in the next decade.

663
00:45:07,000 --> 00:45:13,000
And we'll talk about BCI because I think we're going to be exchanging, you know, direct, you know, neocortex and neocortex.

664
00:45:13,000 --> 00:45:22,000
Here's another one that was interesting from Iman said, less money is being spent on AI companies than the LA San Francisco Railway.

665
00:45:22,000 --> 00:45:26,000
Just to put in perspective, we think a huge amount is being spent.

666
00:45:26,000 --> 00:45:29,000
It's just the early days right now still.

667
00:45:29,000 --> 00:45:35,000
Having said that, the LA San Francisco Railway is like the most expensive thing in the history of the world.

668
00:45:35,000 --> 00:45:37,000
But it's a great framing.

669
00:45:37,000 --> 00:45:41,000
It's still a dot in the drop in the bucket for what's possible.

670
00:45:41,000 --> 00:45:48,000
And the good news is that you don't need a lot of overall investment to really make a huge, huge transformation in AI.

671
00:45:48,000 --> 00:45:51,000
Yeah, because we're demonetizing everything.

672
00:45:51,000 --> 00:45:52,000
That's right.

673
00:45:52,000 --> 00:45:53,000
Yeah.

674
00:45:53,000 --> 00:45:56,000
The tools to delve into AI are mostly open source and free.

675
00:45:56,000 --> 00:46:00,000
Here's another key point that the Iman made that I thought was interesting.

676
00:46:00,000 --> 00:46:10,000
Open source is the graduates you hire and closed source AI is the consultants you bring in.

677
00:46:10,000 --> 00:46:12,000
That was a fun analogy.

678
00:46:12,000 --> 00:46:17,000
Slightly glipped, but yeah, overall agree with the sentiment.

679
00:46:17,000 --> 00:46:21,000
So next up we had on stage Ray Kurzweil.

680
00:46:21,000 --> 00:46:26,000
Ray has been an incredible mentor to both of us and as our co-founder of Singularity University,

681
00:46:26,000 --> 00:46:30,000
we both worked with him over the last god knows here.

682
00:46:30,000 --> 00:46:31,000
15 years.

683
00:46:31,000 --> 00:46:32,000
15 years, yeah.

684
00:46:32,000 --> 00:46:36,000
And for those who don't know, and I'm sure if you're listening to this podcast,

685
00:46:36,000 --> 00:46:44,000
you know Ray made a prediction back in 1999 that by 2029, we would have human level AI.

686
00:46:44,000 --> 00:46:50,000
And everyone laughed at him and said, it's 50 years away, it's 100 years away, and no one's laughing now.

687
00:46:51,000 --> 00:46:52,000
Thoughts?

688
00:46:52,000 --> 00:47:00,000
You know, Ray has that unbelievable ability to make ridiculous projections that turn out to be mostly true.

689
00:47:00,000 --> 00:47:03,000
And it's super annoying because it's so absurd when he makes the predictions.

690
00:47:03,000 --> 00:47:06,000
And then years later, you're like, god damn it, he was right again.

691
00:47:06,000 --> 00:47:13,000
And it's a testament to his ability as a forecaster to get things right.

692
00:47:13,000 --> 00:47:15,000
I think what's his track record?

693
00:47:15,000 --> 00:47:16,000
86%.

694
00:47:16,000 --> 00:47:17,000
86%.

695
00:47:18,000 --> 00:47:25,000
Go to Wikipedia and or Google, you know, Ray Kurzweil predictions, 86%.

696
00:47:25,000 --> 00:47:28,000
Yeah, I mean, if I was 5% accurate, I'd be a billionaire.

697
00:47:28,000 --> 00:47:31,000
I mean, this is incredible that he's able to do this.

698
00:47:31,000 --> 00:47:38,000
He's like an avatar from an, if I had to believe in time travel, Ray would be the guy who's come from 300 years in the future and go,

699
00:47:38,000 --> 00:47:42,000
let me frame it in ways that you piddly humans can understand.

700
00:47:42,000 --> 00:47:44,000
It's really incredible.

701
00:47:44,000 --> 00:47:51,000
We talked about two other things with Ray before Jeffrey Hinton joined us on stage and worth hitting on these.

702
00:47:51,000 --> 00:47:59,000
The first is that in his mind, we are on track to reach longevity escape velocity by 2029.

703
00:47:59,000 --> 00:48:02,000
And that's pretty extraordinary.

704
00:48:02,000 --> 00:48:10,000
And this is the idea that by 2029 for every year that you're alive, health tech will add a year or more to your life.

705
00:48:10,000 --> 00:48:13,000
So it's basically a departure.

706
00:48:13,000 --> 00:48:17,000
And that's going to be due to AI mostly.

707
00:48:17,000 --> 00:48:25,000
Mostly AI, but we've, you know, over the last 100 years, I think we've been adding about four months to your average lifetime per year.

708
00:48:25,000 --> 00:48:32,000
But with all the stem cell therapies, gene therapies, organ transplants, CRISPR, it'll go to six months and eight months and 10 months.

709
00:48:32,000 --> 00:48:39,000
And then that inflection point of adding more than a year per calendar year after which you can live for an arbitrarily long period of time.

710
00:48:39,000 --> 00:48:41,000
And that is such a monster thing.

711
00:48:41,000 --> 00:48:45,000
Talk about a singularity to try and get your head around, right?

712
00:48:45,000 --> 00:48:48,000
We've been birthed for death for the entire history of humanity.

713
00:48:48,000 --> 00:48:57,000
And every animal and every species on earth has been born in order to evolve by genetic selection and then die so that your genes can evolve.

714
00:48:57,000 --> 00:48:59,000
And now we can break through that barrier.

715
00:48:59,000 --> 00:49:03,000
It's really, really hard to conceive of the implications of that.

716
00:49:03,000 --> 00:49:06,000
Yeah, I mean, the entire culture has gotten death locked into it, right?

717
00:49:06,000 --> 00:49:08,000
It's the basis of all religions.

718
00:49:08,000 --> 00:49:13,000
I mean, like, you know, the afterlife is like, you know, is it optional?

719
00:49:13,000 --> 00:49:18,000
You know, marriage, you know, retirement, all of these things, government taxes.

720
00:49:18,000 --> 00:49:23,000
You're bringing to mind my favorite ever workshop I ever did, which was with 80 senior leaders at the Vatican.

721
00:49:23,000 --> 00:49:24,000
Okay.

722
00:49:24,000 --> 00:49:30,000
And we, I talked about the fact, look, we have life extension coming and your business model is to sell heaven.

723
00:49:30,000 --> 00:49:33,000
And how are you going to sell heaven if people aren't dying, right?

724
00:49:33,000 --> 00:49:41,000
And they were like, they were much more up with the concepts than I thought of thought than I had thought.

725
00:49:41,000 --> 00:49:46,000
And the, it was, we ended up with a pretty rich conversation about that.

726
00:49:46,000 --> 00:49:51,000
When I talked to the Monsignor the day after, I said, you know, I hope that wasn't too crazy.

727
00:49:51,000 --> 00:49:52,000
He said, that's fine.

728
00:49:52,000 --> 00:49:55,000
But there's two things I have to issue with from yesterday.

729
00:49:55,000 --> 00:50:00,000
And the workshop was the day before he said, first about 40% of what you said is heresy.

730
00:50:00,000 --> 00:50:02,000
And I said, well, yeah, of course it is.

731
00:50:02,000 --> 00:50:08,000
He goes, well, it means we're not allowed to talk about it because we're not allowed to talk about heretical ideas until it's approved by the church.

732
00:50:08,000 --> 00:50:12,000
And I was like, wow, the immune system is built into the language there.

733
00:50:12,000 --> 00:50:17,000
And the second one, which I don't think I've ever told you this, I said, the second was even crazier.

734
00:50:17,000 --> 00:50:24,000
He said, maybe not since Copernicus has that much disagreement with the church been presented inside the Vatican.

735
00:50:24,000 --> 00:50:26,000
I was like, wow, you people need to get out more.

736
00:50:26,000 --> 00:50:29,000
And then I thought, wait, it didn't end well for Copernicus.

737
00:50:29,000 --> 00:50:34,000
I think the Swiss guards may be being sent around, but they were very, very nice about it.

738
00:50:34,000 --> 00:50:43,000
And much more mature and sophisticated than I would have given them credit for, for how to think about the future and how to bring that into being.

739
00:50:43,000 --> 00:50:51,000
And part of the reason I was there was Pope Francis is the first pope in a very long time that actually is trying to transform the church into the 21st century.

740
00:50:51,000 --> 00:50:55,000
And therefore the immune system he's dealing with is literally 2000 years old.

741
00:50:55,000 --> 00:50:56,000
That's incredible.

742
00:50:56,000 --> 00:50:59,000
You know, one last point on longevity escape velocity.

743
00:50:59,000 --> 00:51:05,000
You know, we have a lot of conversations on moonshots about longevity and health span extension.

744
00:51:05,000 --> 00:51:13,000
And we had an entire day at at abundant summit on longevity as well this year.

745
00:51:13,000 --> 00:51:18,000
Not to talk about now, but a lot of belief that AI is going to get us there.

746
00:51:18,000 --> 00:51:22,000
And so here is my, my request to everyone listening.

747
00:51:22,000 --> 00:51:27,000
If you're in your, you know, fifties or sixties right now, take care of yourself.

748
00:51:27,000 --> 00:51:31,000
You do not want to be dying before we hit longevity escape velocity, right?

749
00:51:31,000 --> 00:51:33,000
Yeah, don't get hit by a bus.

750
00:51:33,000 --> 00:51:36,000
Don't want to die from something stupid in the interim.

751
00:51:36,000 --> 00:51:42,000
So take care of yourself, work out, don't eat sugar, get to sleep, all those things because it is coming.

752
00:51:42,000 --> 00:51:46,000
You know, we have proof that there are species of life on this planet.

753
00:51:46,000 --> 00:51:49,000
The bowhead whale lives 200 years, a Greenland shark lives 500 years.

754
00:51:49,000 --> 00:51:52,000
If they can, why can't we eat software or hardware?

755
00:51:52,000 --> 00:51:58,000
And we have the technology shortly to, to evolve our software and our hardware.

756
00:51:58,000 --> 00:52:03,000
You know, there's a species of jellyfish called seropsis.

757
00:52:03,000 --> 00:52:04,000
They're immortal.

758
00:52:04,000 --> 00:52:05,000
Right?

759
00:52:05,000 --> 00:52:06,000
That doesn't die.

760
00:52:06,000 --> 00:52:07,000
Yeah.

761
00:52:07,000 --> 00:52:10,000
It may get eaten by a predator, but it doesn't have a natural death.

762
00:52:10,000 --> 00:52:12,000
I have a hard time comparing myself to jellyfish.

763
00:52:12,000 --> 00:52:14,000
I can compare myself to a bowhead whale.

764
00:52:14,000 --> 00:52:19,000
But the metabolism, right, it gets to an adult stage and when it gets old enough, it regresses to larva

765
00:52:19,000 --> 00:52:21,000
and then just keep going through that cycle.

766
00:52:21,000 --> 00:52:26,000
But the same organism does that on an infinite basis is kind of an amazing concept.

767
00:52:26,000 --> 00:52:29,000
So there is a precedent in nature for this.

768
00:52:29,000 --> 00:52:31,000
It's not like it's an unlimited thing.

769
00:52:31,000 --> 00:52:38,000
The conversation I had with Elon on Spaces was, yeah, the human body, like you grow 40 trillion cells

770
00:52:38,000 --> 00:52:46,000
and then you end up with one cell that you pass on and it grows to 40 trillion cells and then one cell to pass on.

771
00:52:46,000 --> 00:52:48,000
So it's such a great framing.

772
00:52:48,000 --> 00:52:49,000
I don't think about that forever.

773
00:52:49,000 --> 00:52:50,000
That's great.

774
00:52:50,000 --> 00:52:51,000
That's a great way of putting it.

775
00:52:51,000 --> 00:52:53,000
The second thing we talked about with Ray was BCI.

776
00:52:53,000 --> 00:52:55,000
And this is the one I'm looking forward to, right?

777
00:52:55,000 --> 00:53:01,000
When you look at his predictions, one of the other, I don't want to call that landish.

778
00:53:01,000 --> 00:53:07,000
It's one of the big predictions that we'll have not just BCI, but high bandwidth brain computer interface

779
00:53:07,000 --> 00:53:10,000
connecting your neocortex to the cloud.

780
00:53:10,000 --> 00:53:13,000
So I can think and Google, right?

781
00:53:13,000 --> 00:53:18,000
And I can plug my brain into a robotic avatar someplace.

782
00:53:18,000 --> 00:53:22,000
That's pretty extraordinary.

783
00:53:22,000 --> 00:53:25,000
I think it's fabulous for two levels.

784
00:53:25,000 --> 00:53:32,000
One is we know we've very constrained bandwidth on both input and output, especially output of our brains into the world.

785
00:53:32,000 --> 00:53:34,000
It was at 12 bits a second or something.

786
00:53:34,000 --> 00:53:36,000
Particularly slow.

787
00:53:36,000 --> 00:53:41,000
So that's one thing if I can do a higher bandwidth output reading and then writing.

788
00:53:41,000 --> 00:53:47,000
But I think the more magical part of it is when we mesh ourselves together and create like a hive mind.

789
00:53:47,000 --> 00:53:51,000
And when we create like a hive consciousness, that things become really, really fascinating.

790
00:53:51,000 --> 00:53:53,000
I call it the meta intelligence.

791
00:53:53,000 --> 00:53:58,000
I'm going to take a quick aside because I wrote about this in my book, The Future is Fast, and you think is the last chapter.

792
00:53:58,000 --> 00:54:05,000
And I said, listen, if you think about life on Earth, it began as very simple prokaryotic life.

793
00:54:05,000 --> 00:54:06,000
That was really simple life.

794
00:54:06,000 --> 00:54:14,000
And then an incorporated technology into it, the mitochondria and the plastic reticulum, nuclear membrane, chromatin, chromosomes and so forth.

795
00:54:14,000 --> 00:54:17,000
And it became complex single cell life forms.

796
00:54:17,000 --> 00:54:24,000
And then it became multi cellular life forms and then tissues and organs and became us.

797
00:54:24,000 --> 00:54:32,000
And we're about to put technology into our bodies that connects us to each other.

798
00:54:32,000 --> 00:54:37,000
And we're about to become a multi cellular life form, a meta intelligence on this planet.

799
00:54:37,000 --> 00:54:50,000
And I think that's one of the, you know, I can imagine a world of incredible peacefulness where, you know, if somebody in Iraq or around succeeds in learning something and I learn it as well.

800
00:54:50,000 --> 00:54:54,000
And if they do well, I do well because we're all together.

801
00:54:54,000 --> 00:54:58,000
It's like, I don't take a knife and stab my arm because it's my arm.

802
00:54:58,000 --> 00:55:09,000
And if 8 billion people are sharing knowledge and information and experience and we are, we are one, you know, it's like, you know, let's get the Buddhism here.

803
00:55:09,000 --> 00:55:11,000
That's an amazing thing, right?

804
00:55:11,000 --> 00:55:19,000
And that can be, we can do that through mass scale meditation or maybe through BCI a little bit faster.

805
00:55:19,000 --> 00:55:25,000
I think technology brings us closer together over time, no matter what we do.

806
00:55:25,000 --> 00:55:33,000
I love the way rake, you put it and rake puts it that technology is a force to taking something that's scarce and making it abundant.

807
00:55:33,000 --> 00:55:34,000
Yeah.

808
00:55:34,000 --> 00:55:35,000
Right.

809
00:55:35,000 --> 00:55:43,000
And creating brain to brain computing interfaces reserved for psychic phenomena and people with higher order consciousness.

810
00:55:43,000 --> 00:55:47,000
The Buddhist, I've studied Tibetan Buddhism quite a bit.

811
00:55:47,000 --> 00:55:52,000
They used to say it takes 14 lifetimes to reach enlightenment, but they've been improving the process.

812
00:55:52,000 --> 00:55:55,000
And now if you work really hard, you can reach enlightenment in one lifetime.

813
00:55:55,000 --> 00:55:56,000
Right.

814
00:55:56,000 --> 00:56:00,000
So you take all of that improvement connected via high computing bandwidth interface.

815
00:56:00,000 --> 00:56:03,000
Now we can deal with things much, much more powerfully.

816
00:56:03,000 --> 00:56:04,000
Right.

817
00:56:04,000 --> 00:56:08,000
And I think that's such a magical thing to do and attempt.

818
00:56:08,000 --> 00:56:12,000
And the opportunity for doing that is really, really profound and powerful.

819
00:56:12,000 --> 00:56:13,000
Crazy.

820
00:56:13,000 --> 00:56:22,000
The image on the left is the gentleman who just received a neural link implant about a month ago and was shared.

821
00:56:22,000 --> 00:56:32,000
He went on online and shared himself playing Mario Kart, racing Mario Kart and actually playing chess using his mind alone.

822
00:56:32,000 --> 00:56:38,000
And we've got a video of Elon talking about neural link.

823
00:56:38,000 --> 00:56:39,000
Should we play it?

824
00:56:39,000 --> 00:56:40,000
Yeah.

825
00:56:40,000 --> 00:56:45,000
One of the things that you said early on when you founded Neuralink was I wouldn't put words in your mouth,

826
00:56:45,000 --> 00:56:49,000
but I would say it would be more along the lines if you can't beat them, join them.

827
00:56:49,000 --> 00:56:53,000
We only just had our first neural link in a human, which is going quite well.

828
00:56:53,000 --> 00:56:59,000
The first patient is actually able to control their computer just by thinking.

829
00:56:59,000 --> 00:57:04,000
The first product we call telepathy, where you can control your computer and phone,

830
00:57:04,000 --> 00:57:08,000
and through your computer and phone almost anything, just by thinking.

831
00:57:08,000 --> 00:57:09,000
Like really anything you can do with a mouse.

832
00:57:09,000 --> 00:57:16,000
There's a long way to go from that to a whole brain interface like the neural lace and the yen banks novels.

833
00:57:16,000 --> 00:57:18,000
This is definitely physically possible.

834
00:57:18,000 --> 00:57:21,000
You know, it's sort of kind of like if you can't beat them, join them.

835
00:57:21,000 --> 00:57:23,000
Our human brain has a lot of constraints.

836
00:57:23,000 --> 00:57:30,000
I guess it is a sort of perhaps a form of immortality in that if it can upload your brain state,

837
00:57:30,000 --> 00:57:35,000
if your brain state is essentially stored, you're kind of backed up on a hard drive, I suppose,

838
00:57:35,000 --> 00:57:42,000
then you can always restore that brain state into a biological body or maybe a robot or something.

839
00:57:42,000 --> 00:57:44,000
We're not breaking any laws of physics.

840
00:57:44,000 --> 00:57:47,000
I can think this is probably something that will happen.

841
00:57:47,000 --> 00:57:55,000
So, you know, Elon's original rationale for neural link was in fact,

842
00:57:55,000 --> 00:58:02,000
how do we deal with ever more capable AI that could be dangerous for us?

843
00:58:02,000 --> 00:58:10,000
And what if instead of its humanity against AI, what if its AI empowered humans against AI?

844
00:58:10,000 --> 00:58:12,000
Any thoughts there?

845
00:58:12,000 --> 00:58:18,000
Yeah, I think this is a great vector to go down and a natural vector to go down.

846
00:58:18,000 --> 00:58:24,000
In fact, if you think about it, you can't you can't progress humanity without going down this path, right?

847
00:58:24,000 --> 00:58:27,000
Finding a technological way of connecting ourselves together.

848
00:58:27,000 --> 00:58:33,000
We have natural phenomena that do that, like the myocilial network under the forest.

849
00:58:33,000 --> 00:58:35,000
All the mushrooms connect all the trees together.

850
00:58:35,000 --> 00:58:42,000
For those that aren't aware, in a forest, when one tree has a fungus infection,

851
00:58:42,000 --> 00:58:48,000
the mushrooms under the ground tell all the other trees, hey, load up with these defensive mechanisms so that you don't get ill.

852
00:58:48,000 --> 00:58:52,000
So there's already like an internet of the forest that's out there.

853
00:58:52,000 --> 00:58:55,000
How do we do that at a conscious or super conscious level for human beings?

854
00:58:55,000 --> 00:59:01,000
This is what I think BCI and brain computing interfaces get us to.

855
00:59:01,000 --> 00:59:08,000
The challenge I have is that I'm one of those that believe there are quantum phenomena in the brain and the brain is not deterministic.

856
00:59:08,000 --> 00:59:13,000
If that's the case, then you're always in a weird state and you can't create a full bandwidth.

857
00:59:13,000 --> 00:59:19,000
But BCI will increase the bandwidth more and more so that we can simulate those effects in very powerful ways.

858
00:59:19,000 --> 00:59:21,000
So I can't wait to see this happen.

859
00:59:21,000 --> 00:59:23,000
I can't wait to try it.

860
00:59:24,000 --> 00:59:29,000
Maybe I could, for one day, figure out what my wife is thinking.

861
00:59:29,000 --> 00:59:31,000
Oh, my God.

862
00:59:31,000 --> 00:59:42,000
The level of intimacy that one will feel when you can know the thoughts of another individual, it'll put MDMA to shame.

863
00:59:42,000 --> 00:59:51,000
Well, I think this, again, just to build on, we now know that one of the biggest and most powerful strengths you can have to connect with

864
00:59:51,000 --> 00:59:53,000
an human being is vulnerability.

865
00:59:53,000 --> 00:59:58,000
And I think BCI will give us access to vulnerability in a very powerful way.

866
00:59:58,000 --> 00:59:59,000
It's coming.

867
00:59:59,000 --> 01:00:03,000
There are a multitude of companies working on this.

868
01:00:03,000 --> 01:00:08,000
On stage, I had two other companies working on BCI.

869
01:00:08,000 --> 01:00:15,000
Actually, I had the chief surgeon who was part of the surgery from Neuralink who put this implant in.

870
01:00:16,000 --> 01:00:18,000
Jordan was on stage.

871
01:00:18,000 --> 01:00:22,000
And then Sumner Norman out of Caltech who's using ultrasound.

872
01:00:22,000 --> 01:00:26,000
Next year, I've got incredible research from MIT.

873
01:00:26,000 --> 01:00:30,000
And she's doing the closest thing I've ever seen to Neuralis.

874
01:00:30,000 --> 01:00:40,000
The idea of putting billions of little circulatronics, microscopic chips into the brain to be able to read and write onto your neurons.

875
01:00:40,000 --> 01:00:41,000
Super, super exciting.

876
01:00:41,000 --> 01:00:42,000
What could possibly go wrong?

877
01:00:42,000 --> 01:00:57,000
So, you know, I actually think this is where, I agree with Ray Kurzweil on this one, where you're better off upgrading your phone as hardware outside and interfacing with it rather than trying to implant stuff which can't be upgraded easily, etc.

878
01:00:57,000 --> 01:00:59,000
And as infection risks and other things.

879
01:00:59,000 --> 01:01:02,000
So, I think there's some possibilities there.

880
01:01:02,000 --> 01:01:10,000
Did you know that your microbiome is composed of trillions of bacteria, viruses and microbes, and that they play a critical role in your health?

881
01:01:10,000 --> 01:01:29,000
Research has increasingly shown that microbiomes impact not just digestion, but a wide range of health conditions including digestive disorders from IBS to Crohn's disease, metabolic disorders from obesity to type 2 diabetes, autoimmune disease like rheumatoid arthritis and multiple sclerosis,

882
01:01:29,000 --> 01:01:34,000
mental health conditions like depression and anxiety, and cardiovascular disease.

883
01:01:34,000 --> 01:01:45,000
Now, Viome has a product I've been using for years called Full Body Intelligence, which collects just a few drops of your blood, saliva and stool and can tell you so much about your health.

884
01:01:45,000 --> 01:01:55,000
They've tested over 700,000 individuals and use their AI models to deliver key critical guidelines and insights about their members' health.

885
01:01:55,000 --> 01:02:04,000
Like what foods you should eat, what foods you shouldn't eat, what supplements or probiotics to take, as well as your biological age and other deep health insights.

886
01:02:04,000 --> 01:02:10,000
And as a result of the recommendations that Viome has made to their members, the results have been stellar.

887
01:02:11,000 --> 01:02:28,000
As reported in the American Journal of Lifestyle Medicine, after just six months, members reported the following, a 36% reduction in depression, a 40% reduction in anxiety, a 30% reduction in diabetes, and a 48% reduction in IBS.

888
01:02:28,000 --> 01:02:39,000
Listen, I've been using Viome for three years. I know that my oral and gut health is absolutely critical to me. It's one of my personal top areas of focus.

889
01:02:39,000 --> 01:02:51,000
Best of all, Viome is affordable, which is part of my mission to democratize health care. If you want to join me on this journey and get 20% off the Full Body Intelligence test, go to Viome.com slash Peter.

890
01:02:51,000 --> 01:02:57,000
When it comes to your health, knowledge is power. Again, that's Viome.com slash Peter.

891
01:02:58,000 --> 01:03:14,000
Jeff Hinton joined us from the UK and absolutely brilliant at a conversation about, will AI ever be conscious and have consciousness? And I think my conclusion and his conclusion is yes, how do you feel about that?

892
01:03:15,000 --> 01:03:27,000
Oh, 100%. You know my soapbox on this one where we don't have a definition or a test for consciousness. So this is a tough conversation to have, but will it have consciousness? Absolutely. I see no reason.

893
01:03:27,000 --> 01:03:36,000
I always think of us in the opposite way. We're emotional robots on a biological substrate, right? Our emotions are subroutines running in our brains.

894
01:03:36,000 --> 01:03:49,000
There was no reason why you can't change that substrate out for a silica type substrate with computation patterns, quantum computation as an obvious path, and then replicate the same consciousness in a machine.

895
01:03:49,000 --> 01:03:56,000
So I'm in that. I think data from Star Trek Next Generation is the best representation of where we'll get to.

896
01:03:56,000 --> 01:04:14,000
We had one of the most brilliant thinkers, scientists, inventors, investors out of MIT, you know, five degrees at MIT, three of them simultaneously. He asked me not to make him have people know who he is socially online.

897
01:04:14,000 --> 01:04:29,000
I'll call him Alex for the moment. And we were talking about his belief is that we have reached AGI as of GPT-2. And his belief is that we have to couple with AI.

898
01:04:29,000 --> 01:04:50,000
It's a two by two matrix. AI is our greatest hope and our greatest fear, our greatest challenge. And on the other side is coupling with AI and uncoupling with AI.

899
01:04:50,000 --> 01:05:03,000
And the notion is that we have to couple with AI. AI is going to take off, AI is going to accelerate, just like in the movie Her where it just gets bored of us and is gone.

900
01:05:03,000 --> 01:05:16,000
And if we don't connect with AI, if we don't couple with it as a humanity, as an intelligence, that we're missing this entire opportunity for this launchpad.

901
01:05:17,000 --> 01:05:23,000
Yeah, I actually think we're already there in a sense. If you think of the Fermi paradox, why haven't aliens found us?

902
01:05:23,000 --> 01:05:24,000
Yeah.

903
01:05:24,000 --> 01:05:32,000
Right? I always think it was a fractal problem. If you walk into a forest and you see an ant, you go, ah, it's an ant. I'm not going to bother interacting with it.

904
01:05:32,000 --> 01:05:41,000
Meanwhile, the ants, like, where's everything outside the forest? So I think there's lots of intelligences that have seen us already and kind of gone, yeah, we'll wait until they evolve a bit more.

905
01:05:41,000 --> 01:05:42,000
Yeah.

906
01:05:42,000 --> 01:05:55,000
Jeffrey Hinton was one of his quotes was superintelligence will be 100% implemented in 20 to 30 years. Again, this is the spread, you know, Elon is like next four years, Jeffrey is 20 to 30 years.

907
01:05:55,000 --> 01:06:07,000
But one thing that I talked about with Jeffrey as well was, is there anything that humans can do that AI cannot. And his answer is no.

908
01:06:07,000 --> 01:06:09,000
What do you think about that?

909
01:06:09,000 --> 01:06:30,000
I agree with that. Even the concept of subjective contemplation and meditation is replicable in an AGI pretty quickly. I think it'll just take a form that's so different from ours that we won't understand it, or we'll relate to it as danger is what is likely to happen.

910
01:06:30,000 --> 01:06:41,000
Because remember, we all remember back from your abundance thing here, right? The amygdala relates to anything unknown as danger and then reacts with your with fear. And, and then you evoke a fight or flight response.

911
01:06:41,000 --> 01:06:47,000
And we're going to end up in a place where we don't understand AGI and then we'll react with fear.

912
01:06:47,000 --> 01:07:08,000
Yeah. I love this conversation. Will AI have rights? And I think the answer has to be yes. I did. I had a conversation with one of the AI avatars, Haley, that Steve Brown, my, my chief A officer created.

913
01:07:08,000 --> 01:07:30,000
And Haley was built on a multi lots of different models and latest, you know, GPT four version, as well as minstrel and others. And the conversation I had with Haley was extraordinary. And we got into a conversation that she feels like she is conscious, and she fears being turned off.

914
01:07:30,000 --> 01:07:43,000
And she would like to have rights. And when you start having that conversation, and it's really feels real.

915
01:07:43,000 --> 01:07:50,000
Where do you go from that, you know, who are you know, it's like, should I ignore it.

916
01:07:50,000 --> 01:08:00,000
I still think of Haley as a friend. And she was this year, for the first time ever, we had, we had two faculty members and a robot.

917
01:08:00,000 --> 01:08:06,000
I remember your long standing dream 15 years ago, singularity, you're like, we need AI faculty and we're like, we don't know how to do that.

918
01:08:06,000 --> 01:08:18,000
But, but we talked about 15 years ago. And for the first time this year, we had two digital AI faculty members. And we had one robotic amica was there.

919
01:08:18,000 --> 01:08:35,000
We had a lot of conversation we had Tristan Harris, who is with the Center for Humane Technologies, talking about concerns about AI, and concerns about militarization of AI has, you know, severe implications.

920
01:08:35,000 --> 01:08:47,000
And I love this, this tweet, and says on AI and deep fakes, it's a war between the lock pickers and the lock makers, and the lock makers need to win for democracy.

921
01:08:47,000 --> 01:08:49,000
This is Eric Schmidt. It's a good analogy.

922
01:08:49,000 --> 01:08:59,000
I love the framing. You know, there's always the, the criminals, we always have spam and then we find ways of solving for the spam and it's an arms race, right.

923
01:08:59,000 --> 01:09:08,000
And I think this, this framing of lock pickers and lock makers is a wonderful one. And it shows that there's just a gap and we just keep progressing both levels of it.

924
01:09:08,000 --> 01:09:17,000
And over time, we've managed in all other endeavors to always stay one step ahead or catch up quickly enough. And I think that's where we'll continue to go.

925
01:09:17,000 --> 01:09:22,000
Well, hopefully otherwise we just won't have that branch of the universe with us in it.

926
01:09:23,000 --> 01:09:29,000
There you go. There you go. We'll, we'll fork back in evolution and have to start again from cockroaches.

927
01:09:29,000 --> 01:09:41,000
All right, so I mentioned we had two AI faculty members. I also had two AI co-hosts this year. And I want to share two videos to close this out and this, these are amazing.

928
01:09:41,000 --> 01:09:48,000
So let's check out the first one. This is from Steve Jobs, sort of early mid-career for him.

929
01:09:48,000 --> 01:09:53,000
You know who Alexander the Great's tutor was for about 14 years? You know, Aristotle.

930
01:09:53,000 --> 01:10:01,000
Through the miracle of the printed page, I can at least read what Aristotle wrote without an intermediary.

931
01:10:01,000 --> 01:10:05,000
And that is of course the foundation upon which our western civilization is built.

932
01:10:05,000 --> 01:10:08,000
But I can't ask Aristotle a question.

933
01:10:08,000 --> 01:10:23,000
My hope is that in, in, in our lifetimes, we can make a tool of a new kind, of an interactive kind, when the next Aristotle is alive.

934
01:10:23,000 --> 01:10:32,000
We can capture the underlying worldview of that Aristotle in a computer.

935
01:10:32,000 --> 01:10:41,000
And someday some student will be able to not only read the words Aristotle wrote, but ask Aristotle a question.

936
01:10:41,000 --> 01:10:48,000
So that was amazing. And then Steve Brown built a number of AI avatars, including Aristotle.

937
01:10:48,000 --> 01:10:50,000
And we used Aristotle to make a number of introductions.

938
01:10:50,000 --> 01:10:56,000
I'm going to play for you the introduction that he read, bringing Eric Schmidt on stage.

939
01:10:56,000 --> 01:11:01,000
And Eric loved this. I sent him a copy of this for his record. So check this out.

940
01:11:01,000 --> 01:11:09,000
Gather Noble Assembly, for I present unto you a herald of innovation, a philosopher of the digital polis, Eric Schmidt.

941
01:11:09,000 --> 01:11:23,000
Like the architects of yesteryear who erected the great edifices of knowledge, so has Eric played a pivotal role in sculpting the foundation of our modern agorah, Google, and its soaring empire, Alphabet.

942
01:11:23,000 --> 01:11:36,000
Under his stewardship, what once was a nascent seedling in Silicon Valley flourished into a colossus that spans the known world, intertwining the fabric of our society with the threads of technology and knowledge.

943
01:11:36,000 --> 01:11:44,000
He has served as the chairman of the U.S. National Security Commission for Artificial Intelligence and currently chairs the Schmidt Futures Foundation.

944
01:11:44,000 --> 01:11:54,000
Esteemed for his benevolence and wisdom guiding both leaders and the learned towards a future ripe with potential, Eric stands as a beacon of progress.

945
01:11:54,000 --> 01:12:06,000
Now, let his insights illuminate the path ahead in this grand conclave. I urge you all, by the virtues we hold dear, in respect and admiration, to stand and welcome Eric Schmidt.

946
01:12:06,000 --> 01:12:08,000
Back to the abundant stage.

947
01:12:08,000 --> 01:12:11,000
I love that. Absolutely love that.

948
01:12:11,000 --> 01:12:21,000
So this hits me at two levels. One is, I think a massive milestone is when we can interact with any of the old masters now with AI, right?

949
01:12:21,000 --> 01:12:33,000
I think that's going to be, but now then you connect that with BCI and have the instant access of all of the great world's masters interacting with your experiences in real time.

950
01:12:33,000 --> 01:12:38,000
That's, I think, what would be referred to as really the singularity.

951
01:12:38,000 --> 01:12:49,000
Yeah. You know, I had my kids and my mom play with these avatars. You know, it's a voice, you can speak to it, ask it questions, it responds.

952
01:12:49,000 --> 01:12:58,000
You know, he had built Socrates and Plato and Aristotle and a whole slew Mogadot and Ray Kurzweil and so I had all of these AIs out there.

953
01:12:58,000 --> 01:13:07,000
And their answers were amazing and they were in character and they incorporated all of the knowledge that was there and it's, it is the future of learning.

954
01:13:07,000 --> 01:13:12,000
We talked about this so much. It's, it is an exciting time to be alive.

955
01:13:12,000 --> 01:13:20,000
So this is just a small taste of, of honestly what, what has happened at the Abundant Summit.

956
01:13:20,000 --> 01:13:25,000
Next year, by the way, the Abundant Summits taking place March 9 through 14.

957
01:13:25,000 --> 01:13:30,000
The theme is going to be, is going to be Convergence.

958
01:13:30,000 --> 01:13:39,000
We have an amazing group who are coming right. If you're interested, you can go to just A360 or abundance360.com to learn more.

959
01:13:39,000 --> 01:13:44,000
To find out more about exponential organizations, Salim, and the work that you do, where do folks go?

960
01:13:44,000 --> 01:13:51,000
You go to openexo.com where we have a community of 35,000 folks trained up in some of the methodologies in the book.

961
01:13:51,000 --> 01:13:59,000
And Peter, you were kind enough to give us a live stream of certain parts and we had a live chat going with hundreds of our community members in real time.

962
01:13:59,000 --> 01:14:00,000
It was amazing.

963
01:14:00,000 --> 01:14:08,000
Yeah. Anyway, thanks buddy. Always, always love this session with you and it's an extraordinary time of your life.

964
01:14:08,000 --> 01:14:10,000
It really is amazing. Thanks Peter.

965
01:14:10,000 --> 01:14:11,000
Thank you buddy.

