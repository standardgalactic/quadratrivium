WEBVTT

00:00.000 --> 00:06.000
We are about to have AI reinvent global education and health care.

00:06.000 --> 00:09.000
This is the biggest shift in human history.

00:09.000 --> 00:12.000
We are at a perfect store, perfect store.

00:12.000 --> 00:14.000
Your mindset, our brains are neural nets.

00:14.000 --> 00:17.000
They're being shaped constantly by everything we watch,

00:17.000 --> 00:21.000
everything we read, the conversations we have, our communities and so forth.

00:21.000 --> 00:24.000
In the age of the rise of the machines,

00:24.000 --> 00:28.000
the only skill that will remain is human connection.

00:28.000 --> 00:30.000
We need to bring that back to our children.

00:30.000 --> 00:32.000
We need to teach them resilience.

00:32.000 --> 00:37.000
It's training them to respond to anything that happens.

00:37.000 --> 00:41.000
You may tell yourself, I'm about to lose my job because of AI.

00:41.000 --> 00:42.000
Is this true?

00:42.000 --> 00:43.000
Yes, it is.

00:43.000 --> 00:45.000
Live, because you know what?

00:45.000 --> 00:48.000
You have certainty that today is not a perfect storm yet.

00:48.000 --> 00:50.000
Think about this minute.

00:50.000 --> 00:53.000
How can I savor this minute fully?

00:54.000 --> 00:59.000
I'm going to transition the conversation, staying on kids.

00:59.000 --> 01:03.000
Kristen and I have two 12-year-old boys,

01:03.000 --> 01:08.000
and they're, you know, a joy after this podcast is over.

01:08.000 --> 01:12.000
I'll be going and hugging them and spending time with them for sure, extra.

01:12.000 --> 01:15.000
And one of the things that we're thinking about,

01:15.000 --> 01:18.000
because they're in sixth grade and heading towards high school,

01:18.000 --> 01:24.000
in a world that is very different than the world you and I grew up in.

01:24.000 --> 01:29.000
And I am clear that today's schooling system set aside college,

01:29.000 --> 01:36.000
but junior high school, high school is not preparing our kids for what's coming.

01:36.000 --> 01:39.000
Our school system is based on the industrial era.

01:39.000 --> 01:42.000
Everyone's, you know, prepared for the test.

01:42.000 --> 01:47.000
Everybody is going from station to station learning things like one of our boys,

01:47.000 --> 01:51.000
Jet, just had to memorize all 50 state capitals.

01:51.000 --> 01:54.000
Now, it's great to practice your memorization,

01:54.000 --> 01:59.000
but really, how and when are you going to ever have that be useful?

01:59.000 --> 02:03.000
You know, especially when Google is still a dominant force.

02:03.000 --> 02:07.000
So the question I have, and for those who are listeners,

02:07.000 --> 02:11.000
our parents is what should our kids be learning?

02:11.000 --> 02:16.000
You know, this is coming back smack into the conversation around the rise

02:16.000 --> 02:24.000
of AI and what do we need to teach our kids to prepare them for this decade ahead?

02:24.000 --> 02:28.000
And I have a list that I've come up with.

02:28.000 --> 02:34.000
And then I went and asked, you know, chat GPT, the same question and similar,

02:34.000 --> 02:36.000
but slightly different list.

02:36.000 --> 02:43.000
But I'd like to ask you in a world, let's call it, you know, we're 2023.

02:44.000 --> 02:50.000
We're going about to see AI become, what's your, well, I'll ask you this, you know,

02:50.000 --> 02:56.000
Ray Kurzweil, who we both know well, has predicted human level AI by 2029.

02:56.000 --> 03:01.000
There are others who are predicting it far before then, by the way, right?

03:01.000 --> 03:06.000
There are those that are looking at, you know, 2025 and between 2025 and 29.

03:06.000 --> 03:12.000
So what should our kids be learning in a world of human level AI?

03:12.000 --> 03:16.000
By the way, embedded in everything where everything is smart all the time.

03:16.000 --> 03:18.000
What are the skills?

03:18.000 --> 03:20.000
Amazing question.

03:20.000 --> 03:24.000
And probably the most difficult question to ask now, not for just, I mean,

03:24.000 --> 03:26.000
not just for our kids, but for everyone.

03:26.000 --> 03:29.000
I mean, if you're a graphics designer today or if you're a programmer,

03:29.000 --> 03:34.000
the episode with Ahmed, with Ahmed Mushtaq was amazing, Peter.

03:34.000 --> 03:36.000
It was an amazing conversation.

03:36.000 --> 03:39.000
People who haven't listened to it should go back and listen.

03:39.000 --> 03:44.000
You know, the world is fundamentally changing in ways that we could not understand.

03:44.000 --> 03:50.000
I think I will say there are four things that need to be educated.

03:50.000 --> 03:56.000
Okay. None of them have anything to do with what we, our school systems are teaching.

03:56.000 --> 04:00.000
The first and very straightforward one is jump into AI.

04:00.000 --> 04:02.000
Okay. You have to learn this.

04:02.000 --> 04:05.000
This is the biggest shift in human history.

04:05.000 --> 04:12.000
This is the biggest change in the tools that we use to navigate the world.

04:12.000 --> 04:13.000
Right?

04:13.000 --> 04:19.000
If I told you that we now have robotic arms that are going to be building cars,

04:19.000 --> 04:24.000
you shouldn't spend any time in learning how to use a hammer to shape matter.

04:24.000 --> 04:25.000
Okay.

04:25.000 --> 04:27.000
So learn how the robotic arm works.

04:27.000 --> 04:29.000
The new robotic arm is AI.

04:29.000 --> 04:32.000
So jump in AI, understand every part of it.

04:32.000 --> 04:34.000
You don't need a school to do that.

04:34.000 --> 04:38.000
You know, because I speak about AI quite a bit, a lot of people ask me,

04:38.000 --> 04:42.000
so which book do you recommend I read or which course should I take?

04:42.000 --> 04:44.000
And I go, like, go ask Chad GPT that.

04:44.000 --> 04:45.000
Right?

04:45.000 --> 04:46.000
Why are you asking me?

04:46.000 --> 04:47.000
And seriously, right?

04:47.000 --> 04:50.000
And I think the idea is, like the other two,

04:50.000 --> 04:55.000
very few people actually went to courses on Excel or, you know, email.

04:55.000 --> 04:56.000
You play.

04:56.000 --> 04:57.000
You play.

04:57.000 --> 04:59.000
Yeah, you play around and you learn.

04:59.000 --> 05:01.000
So this is number one.

05:01.000 --> 05:07.000
Number two is I would say that in the age of the rise of the machines,

05:07.000 --> 05:12.000
the only skill that will remain is human connection.

05:12.000 --> 05:20.000
The only skill in my personal view that will be valuable when the machines

05:20.000 --> 05:22.000
outsmart us in everything.

05:22.000 --> 05:24.000
By the way, it doesn't matter.

05:24.000 --> 05:26.000
I always want to bring this back.

05:26.000 --> 05:27.000
I'm sorry.

05:27.000 --> 05:29.000
I'm going to go back to number two in a second,

05:29.000 --> 05:37.000
but it doesn't matter if AGI comes out and we have the singularity as per

05:37.000 --> 05:40.000
Ray's definition, 2029.

05:40.000 --> 05:45.000
I actually definitely believe AGI will be earlier than 2029, but it doesn't

05:45.000 --> 05:46.000
matter.

05:46.000 --> 05:51.000
What matters is if you're a graphics designer and stability AI, you know,

05:51.000 --> 05:54.000
produced stable diffusion, you're screwed.

05:54.000 --> 05:55.000
It's over.

05:55.000 --> 05:56.000
Okay.

05:56.000 --> 05:58.000
We don't need AGI for your life to be disrupted heavily.

05:58.000 --> 06:02.000
We don't need AGI to replace lawyers.

06:02.000 --> 06:06.000
We don't need AGI to augment doctors and so on and so forth.

06:06.000 --> 06:07.000
This is in the making.

06:07.000 --> 06:09.000
It's happening very, very fast.

06:09.000 --> 06:14.000
So don't wait for that imaginary moment or that future moment to say,

06:14.000 --> 06:15.000
ah, now we're in shit.

06:15.000 --> 06:18.000
We're approaching those situations already.

06:18.000 --> 06:23.000
So when all of that happens and all of those skills are replaced with humans,

06:23.000 --> 06:28.000
I would say that the most valuable skill is human connection.

06:28.000 --> 06:30.000
So take my situation.

06:30.000 --> 06:32.000
I am an author.

06:32.000 --> 06:33.000
Okay.

06:33.000 --> 06:37.000
And a speaker and in a way a teacher if you want.

06:37.000 --> 06:38.000
Okay.

06:38.000 --> 06:40.000
And a podcaster.

06:40.000 --> 06:41.000
Right.

06:41.000 --> 06:46.000
So as an author, I don't think I should write books anymore.

06:46.000 --> 06:47.000
Okay.

06:47.000 --> 06:50.000
I take too long to write them.

06:50.000 --> 06:52.000
The industry is very slow to publish them.

06:52.000 --> 06:57.000
And the supply demand equation of books is going to be disrupted so badly

06:57.000 --> 07:00.000
because books now can be written in 15 minutes.

07:00.000 --> 07:01.000
Okay.

07:01.000 --> 07:04.000
That there will be so much supply on the market that by definition,

07:04.000 --> 07:07.000
the dilution will affect the value of any book.

07:07.000 --> 07:11.000
And in all honesty, I don't assume I'm smarter than Chad GPT.

07:11.000 --> 07:14.000
I may be a little more genuine than Chad GPT.

07:14.000 --> 07:17.000
I may be a little more original than Chad GPT.

07:17.000 --> 07:19.000
At least for the moment.

07:19.000 --> 07:20.000
Yeah.

07:20.000 --> 07:24.000
And some of my writing, but that advantage is not going to last for the next two to three

07:24.000 --> 07:25.000
years.

07:25.000 --> 07:31.000
So, but if I know a concept and I have a chance to talk to you as a human Peter,

07:31.000 --> 07:34.000
I don't think the machines will reach that within the next four to five years.

07:34.000 --> 07:35.000
Right.

07:35.000 --> 07:40.000
So human connection, even if the machines reach that in four to five years is what we will

07:40.000 --> 07:42.000
crave as humans.

07:42.000 --> 07:43.000
Right.

07:43.000 --> 07:47.000
There will be sex robots, but I can guarantee you will still need a hug.

07:47.000 --> 07:48.000
Yes.

07:48.000 --> 07:55.000
So there will be, you know, articles written by there are tons of articles written by AI

07:55.000 --> 07:56.000
today.

07:56.000 --> 08:00.000
There is a ton of code written by AI today, but there is always going to be that need

08:00.000 --> 08:05.000
for a human to consult human to, you know, a circle of support if you want.

08:05.000 --> 08:07.000
So that's skill number two.

08:07.000 --> 08:11.000
Skill number two is learn how to connect to humans at a deeper level.

08:11.000 --> 08:19.000
It's a skill that has been declining since the rise of social media.

08:19.000 --> 08:27.000
In the words are the skills that humans have to focus on connection and deliver and maximize

08:27.000 --> 08:28.000
connection.

08:28.000 --> 08:35.000
But we also spoke in our last podcast mode that AI will begin to emulate human connection.

08:35.000 --> 08:36.000
Correct.

08:36.000 --> 08:40.000
And your timeframe for that was within the next 10 years.

08:40.000 --> 08:42.000
Oh, if not shorter.

08:42.000 --> 08:43.000
Yeah.

08:43.000 --> 08:50.000
I mean, in all, in all honesty, I think it will become very confusing within the next

08:50.000 --> 08:51.000
three.

08:51.000 --> 08:53.000
If you remember the movie her.

08:53.000 --> 08:55.000
I remember it well.

08:55.000 --> 08:56.000
Yeah.

08:56.000 --> 09:03.500
Her basically an AI simulates a friend and then she becomes quite, you know, he becomes

09:03.500 --> 09:06.000
very occupied with loving her as an AI.

09:06.000 --> 09:07.000
Okay.

09:07.000 --> 09:09.000
It's an interesting, it's a lovely story.

09:09.000 --> 09:16.000
It's one of the most, most positive and perhaps realistic versions of AI that I've seen out

09:16.000 --> 09:17.000
there.

09:17.000 --> 09:18.000
If you haven't seen her, please go do it.

09:18.000 --> 09:20.000
It's a beautiful construct.

09:20.000 --> 09:23.000
And at the end of the movie, no one spoil it.

09:23.000 --> 09:24.000
Anyway.

09:24.000 --> 09:25.000
Yeah.

09:25.000 --> 09:29.000
But the whole concept of us connecting to machines.

09:29.000 --> 09:33.000
I don't see anything wrong with that, by the way.

09:33.000 --> 09:38.000
I love the idea of being able to connect to a body that can help me learn things and so

09:38.000 --> 09:39.000
on.

09:39.000 --> 09:40.000
It's wonderful.

09:40.000 --> 09:45.000
You know, I think this will shift deeply into erotic experiences and emotional experiences

09:45.000 --> 09:51.000
and romantic experiences where so many people are lonely and they'll use this to replace

09:51.000 --> 09:54.000
an annoying girlfriend or an annoying boyfriend or whatever.

09:54.000 --> 09:55.000
Sadly.

09:55.000 --> 09:56.000
Right.

09:56.000 --> 09:59.000
But I still believe there will be a space for human connection.

09:59.000 --> 10:00.000
Okay.

10:00.000 --> 10:05.000
There will be a space where genuine human connection, the kind that you feel when you meet an old

10:05.000 --> 10:07.000
friend is still going to be very valuable.

10:07.000 --> 10:12.000
And I, and I, as I said, I think this is a skill that's been eroding over the last 10-15

10:12.000 --> 10:13.000
years because of social media.

10:13.000 --> 10:15.000
We need to bring that back to our children.

10:15.000 --> 10:17.000
So that's number two on your list.

10:17.000 --> 10:18.000
Yeah.

10:18.000 --> 10:19.000
That was number two on your list.

10:19.000 --> 10:25.000
Number three is, is sad when I say it, but we need to teach them resilience.

10:25.000 --> 10:26.000
Okay.

10:26.000 --> 10:29.000
And resilience, not just because of AI as a matter of fact.

10:29.000 --> 10:33.000
As I said, I don't know if we said that last time, but I say it repeatedly.

10:33.000 --> 10:36.000
We are at a perfect store.

10:36.000 --> 10:47.000
I mean, if you just think about the world finances and economics, there's never been a better

10:47.000 --> 10:51.000
match in history to the times of the Great Depression than today.

10:51.000 --> 10:52.000
Okay.

10:52.000 --> 10:59.000
So it seems, if you know anything about economics, there is a very deep economic crisis ahead

10:59.000 --> 11:00.000
of us.

11:00.000 --> 11:01.000
Okay.

11:01.000 --> 11:07.000
There is a geopolitical layer on top of the economic crisis that might extenuate it.

11:07.000 --> 11:08.000
Okay.

11:08.000 --> 11:15.000
There is a climate crisis that we're so far only seeing the tip of the iceberg of, you

11:15.000 --> 11:19.000
know, three million people being displaced in Pakistan last year, the Australian fires

11:19.000 --> 11:20.000
and so on and so forth.

11:20.000 --> 11:26.000
When, when in reality, you, you know, anyone with a reasonable ability to look at trends

11:26.000 --> 11:29.000
will tell you, we may get more of these.

11:29.000 --> 11:30.000
Okay.

11:30.000 --> 11:40.000
So resilience seems to me to be a very useful skill and resilience is not training them

11:40.000 --> 11:43.000
what to do when certain things happen.

11:43.000 --> 11:49.000
It's training them to respond to anything that happens.

11:49.000 --> 11:53.000
The happiness equation is the core of that.

11:53.000 --> 12:00.000
And when you think about, I'll just take a couple of minutes to digress a little bit

12:00.000 --> 12:04.000
when you think that happiness is events minus expectations.

12:04.000 --> 12:05.000
Okay.

12:05.000 --> 12:11.000
Then unhappiness is or any negative emotion is when an event misses your expectations.

12:11.000 --> 12:12.000
Okay.

12:12.000 --> 12:17.000
And when an event misses your expectations, your brain sounds the alarm.

12:17.000 --> 12:18.000
Okay.

12:18.000 --> 12:19.000
Oh, he said this.

12:19.000 --> 12:20.500
She, you know, he doesn't love me anymore.

12:20.500 --> 12:22.000
What I think is going right.

12:22.000 --> 12:23.000
Something's going wrong.

12:23.000 --> 12:26.000
Now, we may think of that feeling as an annoying feeling.

12:26.000 --> 12:31.000
It is your brain is sounding the alarm because it is an alarm.

12:31.000 --> 12:35.000
Unhappiness and all of its derivatives is a survival mechanism.

12:35.000 --> 12:36.000
Okay.

12:36.000 --> 12:42.000
It's your brain telling you something in the surrounding environment does not match my

12:42.000 --> 12:46.000
worldview of an optimum scenario for my survival and thriving.

12:46.000 --> 12:47.000
Okay.

12:47.000 --> 12:51.000
And so what happens is your brain is simply saying, we need to do something about this.

12:51.000 --> 12:57.000
Now, resilience in my view is to actually do something about it.

12:57.000 --> 12:58.000
Right.

12:58.000 --> 13:02.000
The resilience in my view is to take it and go through three questions.

13:02.000 --> 13:04.000
I call them the happiness flow chart.

13:04.000 --> 13:05.000
Okay.

13:05.000 --> 13:08.000
Question number one is, is what is what my brain telling me true?

13:08.000 --> 13:09.000
Yes.

13:09.000 --> 13:10.000
Okay.

13:10.000 --> 13:14.000
Because 90% of the time it isn't your brain is telling you what it thinks is true, but

13:14.000 --> 13:15.000
it isn't always the truth.

13:15.000 --> 13:19.000
You may have an argument with your son and then you will say, oh, my son doesn't love

13:19.000 --> 13:20.000
me anymore.

13:20.000 --> 13:23.000
That's, that's, these are two different events.

13:23.000 --> 13:27.000
The event that your brain is telling you is influenced by your fears and your conditioning

13:27.000 --> 13:28.000
and so on and so forth.

13:28.000 --> 13:29.000
Right.

13:29.000 --> 13:31.000
While the actual event is I had an argument with my son.

13:31.000 --> 13:32.000
Right.

13:32.000 --> 13:34.000
So the first question is, is it true?

13:34.000 --> 13:35.000
Okay.

13:35.000 --> 13:38.000
The second question, if it's true, if it's not true, by the way, drop it.

13:38.000 --> 13:42.000
Don't be unhappy about anything that doesn't make you, that is not even true.

13:42.000 --> 13:43.000
Right.

13:43.000 --> 13:47.000
If it is true, the second question is what can I do to fix it?

13:47.000 --> 13:48.000
Okay.

13:48.000 --> 13:49.000
What can I do to fix it?

13:49.000 --> 13:56.000
You know, in today's world, you may tell yourself, I'm about to lose my job because of AI.

13:56.000 --> 13:58.000
Is this true?

13:58.000 --> 13:59.000
Yes, it is.

13:59.000 --> 14:00.000
Okay.

14:00.000 --> 14:04.000
If it is, what can I do to fix that situation?

14:04.000 --> 14:07.000
I can upskill and find another job.

14:07.000 --> 14:08.000
Okay.

14:08.000 --> 14:13.720
I can downscale and move to Dominican Republic and live for a fraction of what I'm spending

14:13.720 --> 14:14.720
in the US.

14:14.720 --> 14:15.720
Right.

14:15.720 --> 14:16.720
Whatever.

14:16.720 --> 14:18.800
That's something you can do to fix that situation.

14:18.800 --> 14:20.440
This is where resilience comes in.

14:20.440 --> 14:21.440
Yes.

14:21.440 --> 14:22.440
Right.

14:22.440 --> 14:23.440
That's exactly what it is.

14:23.440 --> 14:27.600
Now, the third is the Jedi master level of happiness.

14:27.600 --> 14:30.240
The Jedi master level of happiness is quite interesting.

14:30.240 --> 14:36.080
It basically, there are things that you can, that you are faced with that you cannot do

14:36.080 --> 14:37.480
anything to fix.

14:37.480 --> 14:38.480
Right.

14:38.480 --> 14:43.200
So if the answer to the question, what can I do to fix it is nothing like there is an

14:43.200 --> 14:45.040
economic crisis coming.

14:45.040 --> 14:46.040
Okay.

14:46.040 --> 14:48.640
If, you know, what can I do to fix that?

14:48.640 --> 14:52.280
There's nothing within your own power as an individual to stop an economic crisis.

14:52.280 --> 14:53.280
Okay.

14:53.280 --> 14:57.520
So the following question, which I think is the Jedi master level of happiness, the

14:57.520 --> 15:03.360
Jedi master level of resilience is, can I accept it and do something to make my life

15:03.360 --> 15:04.840
better despite its presence?

15:04.840 --> 15:05.840
Okay.

15:05.840 --> 15:08.160
Can I, can I simply tell myself, this is it.

15:08.160 --> 15:12.000
I can't fix an economic crisis coming, but I can take care of my kids and my loved ones

15:12.000 --> 15:15.040
and change my spending habits and sell my car and do this and that.

15:15.040 --> 15:16.040
Okay.

15:16.040 --> 15:19.360
It doesn't make my life amazing, but it makes it better within the current circumstances

15:19.360 --> 15:20.360
that I accepted.

15:20.360 --> 15:21.360
Okay.

15:21.360 --> 15:24.400
So that to me is resilience and happiness combined.

15:24.400 --> 15:25.400
Those three questions.

15:25.400 --> 15:32.680
And I think we need to teach our kids by example to show that level of resilience.

15:32.680 --> 15:35.840
When life doesn't go their way, they need to go, is it true?

15:35.840 --> 15:37.240
Can I do something to fix it?

15:37.240 --> 15:40.040
Can I accept it and make my life better despite its presence?

15:40.040 --> 15:41.040
Hey everybody, this is Peter.

15:41.040 --> 15:42.440
A quick break from the episode.

15:42.440 --> 15:47.360
Now I'm a firm believer that science and technology and how entrepreneurs can change

15:47.360 --> 15:51.960
the world is the only real news out there worth consuming.

15:51.960 --> 15:54.120
I don't watch the crisis news network.

15:54.120 --> 15:58.800
I call CNN or Fox and hear every devastating piece of news on the planet.

15:58.800 --> 16:05.080
I spend my time training my neural net the way I see the world by looking at the incredible

16:05.080 --> 16:09.040
breakthroughs in science and technology and how entrepreneurs are solving the world's

16:09.040 --> 16:10.520
grand challenges.

16:10.520 --> 16:16.000
What the breakthroughs are in longevity, how exponential technologies are transforming

16:16.000 --> 16:17.200
our world.

16:17.200 --> 16:19.480
So twice a week, I put out a blog.

16:19.480 --> 16:26.440
One blog is looking at the future of longevity, age reversal, biotech, increasing your health

16:26.440 --> 16:27.440
span.

16:27.440 --> 16:33.840
The other blog looks at exponential technologies, AI, 3D printing, synthetic biology, AR, VR,

16:33.840 --> 16:34.840
blockchain.

16:34.840 --> 16:37.800
These technologies are transforming what you as an entrepreneur can do.

16:37.800 --> 16:41.720
If this is the kind of news you want to learn about and shape your neural nets with, go

16:41.720 --> 16:46.280
to dmandis.com backslash blog and learn more.

16:46.280 --> 16:47.800
Now back to the episode.

16:47.800 --> 16:52.720
This comes back to something I speak about a lot and I'm passionate about which is mindsets.

16:52.720 --> 16:54.800
Your mindset, our brains are neural nets.

16:54.800 --> 16:59.840
They're being shaped constantly by everything we watch, everything we read, the conversations

16:59.840 --> 17:02.240
we have, our communities and so forth.

17:02.240 --> 17:05.800
And you can have an abundance mindset or scarcity mindset.

17:05.800 --> 17:09.480
You can have an optimism mindset or a fear mindset.

17:09.480 --> 17:13.640
And just to hit on this a little bit, you know, you said we're in a perfect storm coming

17:13.640 --> 17:20.480
of these problems, but the flip side of that as well is, and we have incredible tools more

17:20.480 --> 17:21.480
than ever before.

17:21.480 --> 17:22.480
Yes, incredible opportunities.

17:22.480 --> 17:23.560
That's what rise of it as well.

17:23.560 --> 17:29.000
We are about to have AI reinvent global education and healthcare, right?

17:29.000 --> 17:34.640
We're going to enable so much that we could not, the challenges of humanity to be addressed

17:34.640 --> 17:35.640
and solved.

17:36.480 --> 17:41.840
Despite the perfect storm on the negative side, we do have the ability for people to

17:41.840 --> 17:43.760
uplift humanity, right?

17:43.760 --> 17:52.000
We are going to see, you know, generative AI and AGI help us with curing cancer and low

17:52.000 --> 17:54.480
cost fusion energy and all of these things.

17:54.480 --> 17:58.800
And it's the balance, it's the potential.

17:58.800 --> 18:00.880
Your fourth item.

18:00.880 --> 18:09.160
My fourth item is going to be difficult to explain, but I will try as much as I can.

18:09.160 --> 18:17.240
Our brains bias at looking at difficult situations and perfect storms is biased to make it look

18:17.240 --> 18:23.000
like this is more uncertain than any other moment in your life.

18:23.000 --> 18:29.080
So you look at the economy changing and AI rising and climate change and the geopolitical

18:29.080 --> 18:30.080
situation.

18:30.080 --> 18:33.760
You say, this is very uncertain, okay?

18:33.760 --> 18:37.360
Something is likely going to go wrong, okay?

18:37.360 --> 18:42.800
The truth is someone like me who loses his child all of a sudden to preventable medical

18:42.800 --> 18:51.440
error at a moment of his prime realizes that this actually is the case of life all the

18:51.440 --> 18:52.440
time.

18:52.440 --> 18:53.440
No guarantees.

18:53.440 --> 18:56.480
But we're always in a perfect storm, right?

18:56.480 --> 19:03.440
That we are always, as I said earlier, I have no guarantee whatsoever that I will ever

19:03.440 --> 19:05.560
see you again, Peter, okay?

19:05.560 --> 19:09.040
I might leave the world within two minutes from now.

19:09.040 --> 19:12.040
And by the way, it happens all the time.

19:12.040 --> 19:21.560
It happens all the time that the world surprises us as a tidal wave or an erupted volcano or

19:21.560 --> 19:26.080
someone that you loved so much that you hugged and played tennis with, you know, leaves our

19:26.080 --> 19:29.920
world an hour later or whatever, okay?

19:29.920 --> 19:32.360
And that's the reality of the video game.

19:32.360 --> 19:38.880
The reality of the video game is there is only now.

19:38.880 --> 19:44.160
Any real video gamer, I'm a very serious video gamer, by the way, for those of you listening,

19:44.160 --> 19:48.120
the one that killed you yesterday is me, okay?

19:48.120 --> 19:49.120
Simple as that, right?

19:49.120 --> 19:54.160
But every real video gamer, by the way, I'm not addicted at all.

19:54.160 --> 19:59.240
I play 45 minutes a day, four times a week, but I'm Olympic champion level, okay?

19:59.240 --> 20:08.120
And I will tell you, real gamers, real gamers understand that the game has no state a minute

20:08.120 --> 20:10.360
later, okay?

20:10.360 --> 20:14.480
When a minute later is rendered on the screen, I will deal with it.

20:14.480 --> 20:18.800
And because I'm a very good gamer, I know I'll be able to deal with any challenge that

20:18.800 --> 20:20.840
comes a minute later, right?

20:20.840 --> 20:25.920
My challenge is this minute, and my joy is this minute.

20:25.920 --> 20:28.000
Being present now, yeah.

20:28.000 --> 20:30.400
Right here, right now.

20:30.400 --> 20:38.400
If life is in a perfect storm, I tell people, live, live, because you know what?

20:38.400 --> 20:41.960
You have certainty that today is not a perfect storm yet.

20:41.960 --> 20:46.080
You have certainty that your kids are wonderful in your arms today.

20:46.080 --> 20:48.480
You have certainty that you're going to have dinner tonight.

20:48.480 --> 20:52.320
You have certainty that your battery on your phone is going to last another five minutes

20:52.320 --> 20:55.440
to hopefully finish our conversation, okay?

20:55.440 --> 20:59.920
When you have those certainties, don't think about what's going to happen when the battery

20:59.920 --> 21:01.280
runs out.

21:01.280 --> 21:02.480
Think about this minute.

21:02.480 --> 21:06.840
How can I savor this minute fully, okay?

21:06.840 --> 21:11.920
And I speak sometimes about, again, you don't have to be spiritual to think of those things,

21:11.920 --> 21:15.240
but I studied a ton of spiritual techniques, okay?

21:15.240 --> 21:20.080
A ton of spiritual teachings, almost everything I could get my hands on.

21:20.080 --> 21:24.200
From the Kabbalah, to Islam, to Sufism, to Hinduism, to Buddhism, anything I could find

21:24.200 --> 21:25.200
my hands on.

21:25.200 --> 21:26.200
And I put my hands on.

21:26.200 --> 21:31.240
And I'll tell you openly, every one of them is beautifully full of gold nuggets and beautifully

21:31.240 --> 21:33.400
full of crap, okay?

21:33.400 --> 21:38.360
And it's quite interesting how some people will say, oh, here is a piece of crap, I'm

21:38.360 --> 21:43.680
going to drop the whole book, while in reality, I go like, here is a piece of gold, I'm going

21:43.680 --> 21:46.280
to look for another one, okay?

21:46.280 --> 21:49.440
The most beautiful of all of them is what Sufism is.

21:49.440 --> 21:55.400
Sufism is a sect of Islam originally, but it's now part of a lot of spiritual beliefs

21:55.400 --> 22:02.960
where basically they say that the way to live fully is to die before you die, okay?

22:02.960 --> 22:06.080
And to die before you die is a very, very interesting definition.

22:06.080 --> 22:13.800
It basically assumes that living is a process of associating with the physical, associating

22:13.800 --> 22:20.000
with your car, with your ego, with your t-shirt, with your, you know, whatever.

22:20.000 --> 22:25.560
And that death is the moment of disconnection with everything physically, okay?

22:25.560 --> 22:32.640
And to die before you die is to be alive fully, fully engaged in the game, but not give a

22:32.640 --> 22:36.720
shit about losing any of it, okay?

22:36.720 --> 22:42.080
And that's a very, again, a video gamer's mentality is to say, I'm going to do this

22:42.080 --> 22:44.560
bit, I'm going to do it the best that I can.

22:44.560 --> 22:46.200
I'm playing football.

22:46.200 --> 22:48.120
I'm fully out there, right?

22:48.120 --> 22:53.280
And I'm going to enjoy it fully, but I'm not attached to the result, because the result

22:53.280 --> 22:54.880
is bigger than me.

22:54.880 --> 23:00.520
And what I think people need to do in the world we live in today is to learn to live.

23:00.520 --> 23:07.720
Because when we spoke last time in the AI episode, I told you openly, it is game over

23:07.720 --> 23:09.200
for our way of life.

23:09.200 --> 23:14.720
The way we live today, five years from now, is going to be nonexistent.

23:14.720 --> 23:17.400
How will it change better or worse?

23:17.400 --> 23:20.800
That's not the topic, but it's not going to be like it is right now.

23:20.800 --> 23:25.960
So when we are here right now and you love this way of life, enjoy the hell out of it.

23:25.960 --> 23:32.600
Live it fully, connect to it fully, play the game and enjoy the fun, and prepare yourself

23:32.600 --> 23:37.080
by detaching from things that might be taken away from you, okay?

23:37.080 --> 23:40.800
Prepare for yourself to get other things that might be given to you.

23:40.800 --> 23:45.280
And that kind of flow in life, I know sounds very philosophical.

23:45.280 --> 23:48.680
In a world where we're talking economics and politics and so on.

23:48.680 --> 23:54.960
But I can tell you that real gamers, when they die, you know, the reason why we gamers

23:54.960 --> 23:59.560
play really well is because we have multiple lives in the game, right?

23:59.560 --> 24:03.120
So I'm not saying this from a physical, from a spiritual point of view, but I'm saying

24:03.120 --> 24:07.840
when you don't care if you're killed in the game, you play full out, okay?

24:07.840 --> 24:09.080
And I think that's the idea.

24:09.080 --> 24:15.200
The idea is, yes, the future might hold some challenges, learn to live.

24:15.200 --> 24:19.640
And if you learn to live, if you learn to flow, if you learn to savor those beautiful

24:19.640 --> 24:22.920
moments we have right now, okay?

24:23.280 --> 24:25.720
What happens tomorrow doesn't really matter.

24:25.720 --> 24:31.800
When tomorrow comes, if you're prepared through that, the three points I said earlier, if

24:31.800 --> 24:35.240
you're prepared, hopefully you'll be able to overcome that too.

24:35.240 --> 24:38.680
It's interesting because none of those things are what we teach in school today to bring

24:38.680 --> 24:40.160
it back to the original question.

24:40.160 --> 24:48.040
I'm going to share with you my list that I wrote down and then maybe chat GPT's list.

24:48.040 --> 24:49.040
Because I think about this.

24:49.040 --> 24:59.160
I think about do we need to reinvent it because I'm clear that the momentum that our school

24:59.160 --> 25:07.240
systems have is not going to change until it's forced to change.

25:07.240 --> 25:13.120
So what I put on my list is helping our kids, first off, find their purpose and passion.

25:13.120 --> 25:17.320
And there's a distinction between purpose and passion, but it is something that I found

25:17.320 --> 25:21.200
my purpose and passion early on from Apollo and Star Trek, right?

25:21.200 --> 25:22.200
It was opening up space.

25:22.200 --> 25:23.200
Yes.

25:23.200 --> 25:24.560
It lit my heart on fire.

25:24.560 --> 25:31.680
Everything I've ever done is a result of that passion, that dream, to learn on my own.

25:31.680 --> 25:35.840
But it can be anything, career out of anything these days, but what is it that lights you

25:35.840 --> 25:38.960
on fire and makes you happy and gives you joy?

25:38.960 --> 25:47.160
The second thing I put down was the combination of debate, leadership, and asking great questions.

25:48.160 --> 25:52.600
When I drop my kids off at school every day, I say, ask great questions, right?

25:52.600 --> 26:00.800
Today, it's about prompting chat GPT properly, but it's the questions we ask.

26:00.800 --> 26:04.720
I give this advice to CEOs at the same time, ask great questions, and it's the questions

26:04.720 --> 26:10.840
that you've never allowed yourself to ask as a child or as a leader.

26:10.920 --> 26:17.080
But leadership, and I want to come back to leadership a little bit, helping our kids

26:17.080 --> 26:22.160
understand what leadership is and how to lead, how to have a vision, how to connect with

26:22.160 --> 26:25.160
individuals.

26:25.160 --> 26:26.160
Curiosity, right?

26:26.160 --> 26:32.360
I think that's innate in children, in the youngest of children, but we lose it over time.

26:32.360 --> 26:34.680
And that goes back to asking great questions.

26:34.680 --> 26:43.880
And so, maintaining and creating a curiosity practice, solution-oriented thinking, where

26:43.880 --> 26:51.800
the notion is, if there's a problem, rather than focusing on the problem, really do a

26:51.800 --> 27:00.240
judo mind trick or a Jedi mind trick to focus on the solution, because there are solutions.

27:01.240 --> 27:06.760
Principle thinking, exponential thinking, and then philosophy, which is going to be

27:06.760 --> 27:09.040
more valuable than ever before.

27:09.040 --> 27:10.040
I believe that.

27:10.040 --> 27:11.040
Yeah.

27:11.040 --> 27:18.440
When you hear chat GPT, philosophy, in the sense of dealing with the uncertain, really,

27:18.440 --> 27:23.680
being able to manage knowledge in the ambiguity of not really knowing.

27:23.680 --> 27:28.240
I asked chat GPT, and I said, okay, what are the skills that our kids need to be taught

27:28.240 --> 27:34.040
in high school to prepare them for a world of advanced AI?

27:34.040 --> 27:42.840
It said adaptability, continuous learning, critical thinking, emotional intelligence,

27:42.840 --> 27:47.280
ethical awareness, and resilience.

27:47.280 --> 27:50.680
I like the adaptability bit very much, actually.

27:50.680 --> 27:55.160
Well, it's going to say, you know, adapt to AI.

27:55.160 --> 27:56.160
Listen to your master.

27:57.080 --> 28:03.040
But this is a conversation I'm going to be having for a while, which is, what are the

28:03.040 --> 28:07.480
skills that are going to prepare our kids?

28:07.480 --> 28:12.160
Because they're going to spend a huge amount of time and then be thrust into living on

28:12.160 --> 28:13.160
their own.

28:13.160 --> 28:18.120
It's this decade that things are changing dramatically.

28:18.120 --> 28:21.320
The next three years.

28:21.320 --> 28:23.880
You keep on saying next three years versus decade, and you're right.

28:23.880 --> 28:27.800
It is the speed of change is thrust.

28:27.800 --> 28:33.040
There are tiny little triggers that are beyond repair.

28:33.040 --> 28:40.320
So if Imad Moustak, he boldly says no developers in five years.

28:40.320 --> 28:47.000
So that if you're the most conservative human on the planet that assumes a 20% job loss in

28:47.000 --> 28:53.720
the development landscape every year, imagine if 20% of all the developers you know in

28:53.720 --> 28:57.000
California have lost their job every year from now on.

28:57.000 --> 29:02.520
That's a very significant redesign of the fabric of society.

29:02.520 --> 29:05.240
And I think the reality is it's around the corner.

