1
00:00:00,000 --> 00:00:03,840
By next year, end of the year, I believe you'll have ChatGPT on your mobile phone.

2
00:00:03,840 --> 00:00:04,760
Say that one more time.

3
00:00:04,760 --> 00:00:08,360
By the end of next year, you'll have ChatGPT on your mobile phone without internet.

4
00:00:10,160 --> 00:00:12,080
Today, the world changes.

5
00:00:12,080 --> 00:00:13,880
This is a regime change.

6
00:00:13,880 --> 00:00:17,680
Four of the top 10 apps or apps in December were based on stable diffusion.

7
00:00:17,680 --> 00:00:20,880
41% of all code on GitHub right now is AI generated.

8
00:00:20,880 --> 00:00:22,960
We have figured out how to make humans scale.

9
00:00:22,960 --> 00:00:25,560
So you have this amazing thing that can create anything.

10
00:00:25,560 --> 00:00:28,000
How far out are you able to see?

11
00:00:28,040 --> 00:00:32,160
Can you have a sense of predicting where we are in 10 years?

12
00:00:32,160 --> 00:00:33,480
I can't see past my tears.

13
00:00:33,480 --> 00:00:37,800
There's enough fear and some of it is deserved, but there also needs to be hope.

14
00:00:37,800 --> 00:00:39,240
Humans are humans.

15
00:00:39,240 --> 00:00:42,560
Bring them the information that creates the most value and you will change the world.

16
00:00:46,360 --> 00:00:49,640
I'm excited to share with you one of the most extraordinary conversations

17
00:00:49,640 --> 00:00:52,880
I had at this year's annual Abundance 360 Summit.

18
00:00:53,600 --> 00:00:57,480
It was with a friend, someone I've had on this podcast before,

19
00:00:57,480 --> 00:01:01,800
Imad Moustak, who's the founder and CEO of Stability AI.

20
00:01:01,800 --> 00:01:06,320
You know his company from a number of his products, including stable diffusion.

21
00:01:06,320 --> 00:01:10,520
We sat down to talk about his moonshots to talk about how

22
00:01:10,520 --> 00:01:12,640
generative AI is changing the world.

23
00:01:12,640 --> 00:01:16,720
What do you do if you're a 20-year-old just getting involved?

24
00:01:16,720 --> 00:01:19,840
Or if you're a seasoned CEO or entrepreneur,

25
00:01:19,840 --> 00:01:24,720
and how is this going to change the world of Hollywood education, health care?

26
00:01:24,720 --> 00:01:27,280
You know, it was an incredible conversation.

27
00:01:27,280 --> 00:01:29,120
I'm excited for you to hear.

28
00:01:29,120 --> 00:01:34,560
You know, my mission through this podcast is to help you hear the voices

29
00:01:34,560 --> 00:01:37,760
that are filled with optimism and abundance.

30
00:01:37,760 --> 00:01:41,080
If that's of interest to you, then please subscribe to this podcast.

31
00:01:41,080 --> 00:01:42,400
All right, enjoy the episode.

32
00:01:42,400 --> 00:01:47,440
By the end of this episode, you should have a pretty good idea where you stand on AI.

33
00:01:47,440 --> 00:01:48,480
Are you excited about it?

34
00:01:48,480 --> 00:01:49,840
Are you fearful of it?

35
00:01:49,840 --> 00:01:50,800
I'd love to know.

36
00:01:50,800 --> 00:01:53,720
Feel free to share your stance with me on Twitter.

37
00:01:53,720 --> 00:01:54,200
Hey, pal.

38
00:01:54,200 --> 00:01:54,800
Hey.

39
00:01:54,800 --> 00:01:56,000
How's it going, buddy?

40
00:01:56,000 --> 00:02:00,360
Listen, I know you have so much going on right now.

41
00:02:00,360 --> 00:02:01,320
It's insane.

42
00:02:01,320 --> 00:02:06,680
We're in the midst of the most extraordinary period of technology ever,

43
00:02:06,680 --> 00:02:08,480
and you're in the thick of it.

44
00:02:08,480 --> 00:02:10,080
Yeah, I think, what's it?

45
00:02:10,080 --> 00:02:12,320
A good phrase is, everything ever, all at once?

46
00:02:12,320 --> 00:02:13,320
Yes.

47
00:02:13,320 --> 00:02:13,800
Yeah.

48
00:02:13,800 --> 00:02:16,640
That's basically what it is.

49
00:02:16,640 --> 00:02:19,680
I can't mention the conversations he's been having the last 30 minutes,

50
00:02:19,680 --> 00:02:22,360
but it would blow your mind.

51
00:02:22,360 --> 00:02:23,480
I'm going to give you this clicker.

52
00:02:23,480 --> 00:02:26,440
You have some slides to show.

53
00:02:26,440 --> 00:02:30,200
I think the very first question to ask

54
00:02:30,200 --> 00:02:35,200
is if you were going to bring people up to speed on where we are right now,

55
00:02:35,200 --> 00:02:38,080
would you share some of what your thoughts are here?

56
00:02:38,080 --> 00:02:40,960
We have figured out how to make human scale.

57
00:02:40,960 --> 00:02:43,400
How do you make human scale?

58
00:02:43,400 --> 00:02:45,800
That's the most difficult thing.

59
00:02:45,800 --> 00:02:49,800
Basically, classical AI was all about taking stuff and extrapolating it.

60
00:02:49,840 --> 00:02:53,920
We figured out how to figure out principles and latent knowledge

61
00:02:53,920 --> 00:02:57,160
with huge compression in these files, as we all talk about in a bit.

62
00:02:57,160 --> 00:03:01,560
The most valuable thing you can have is a good EA or a really good analyst.

63
00:03:01,560 --> 00:03:03,400
I have asked her if she's amazing.

64
00:03:03,400 --> 00:03:04,360
It makes such a difference.

65
00:03:04,360 --> 00:03:06,920
I'm sure all of you in this audience will realize that.

66
00:03:06,920 --> 00:03:08,160
We now have that.

67
00:03:08,160 --> 00:03:12,160
Basically, what we've done is we've trained these things on huge amounts of data

68
00:03:12,160 --> 00:03:15,200
to understand principles, and now when you use a GPT-4,

69
00:03:15,200 --> 00:03:18,840
it's like a really talented intern analyst with a bad memory,

70
00:03:18,880 --> 00:03:20,880
and we're about to fix the memory.

71
00:03:20,880 --> 00:03:25,120
It knows how to pass the medical bar exam or medical licensing and others.

72
00:03:25,120 --> 00:03:29,640
It knows how to draw beautiful pictures or create audio of any type,

73
00:03:29,640 --> 00:03:32,320
and it's basically available to everyone for pennies.

74
00:03:32,320 --> 00:03:34,040
That's a great thing. What does that mean?

75
00:03:34,040 --> 00:03:36,080
It can mean huge disruption,

76
00:03:36,080 --> 00:03:40,600
or it can mean basically we're at the point of utopia and abundance.

77
00:03:40,600 --> 00:03:44,440
If you don't mind, people know stable diffusion,

78
00:03:44,440 --> 00:03:49,800
and you'll show them the chart that really rocked me when I first saw it.

79
00:03:49,800 --> 00:03:55,680
But what are the wide range of things that Stability AI is working on right now?

80
00:03:55,680 --> 00:03:57,040
Where do you want to go?

81
00:03:58,400 --> 00:04:00,000
Can you tell us about it, please?

82
00:04:00,000 --> 00:04:01,000
No, I can tell you.

83
00:04:01,000 --> 00:04:06,200
My aim is to provide the building blocks for a society OS.

84
00:04:06,200 --> 00:04:10,080
Every part of society, if you have skilled experts in them,

85
00:04:11,080 --> 00:04:14,520
our mission is to build the foundation to activate human entities' potential.

86
00:04:14,520 --> 00:04:17,320
Across every single modality, we're building models,

87
00:04:17,320 --> 00:04:23,560
so audio, video, protein folding, DNA, chemical reactions, language, and others.

88
00:04:23,560 --> 00:04:27,120
We're doing it for every sector, so you've got a bank of GPT, a board GPT,

89
00:04:27,120 --> 00:04:29,720
to replace all your crap board members,

90
00:04:29,720 --> 00:04:32,080
all the way down to national models.

91
00:04:32,080 --> 00:04:35,720
We're doing an Indian national model at the moment, an Indonesian one, a Japanese one,

92
00:04:36,160 --> 00:04:41,080
so that everyone can have a file that they put words in,

93
00:04:41,080 --> 00:04:45,600
and images, audio, text, anything pops out that's appropriate to you to enhance you.

94
00:04:45,600 --> 00:04:49,240
So I want to click on that one because it's very important.

95
00:04:49,240 --> 00:04:52,720
People talk about biases in the system,

96
00:04:52,720 --> 00:04:56,920
but what you're talking about is creating what you're calling a foundational model

97
00:04:56,920 --> 00:05:01,400
that is biased to you, or to your company, or your country.

98
00:05:01,400 --> 00:05:02,040
Yes?

99
00:05:02,040 --> 00:05:05,120
The existing internet has centralized artificial intelligence

100
00:05:05,120 --> 00:05:09,880
that guides our attention, our memorization, where we go.

101
00:05:09,880 --> 00:05:13,040
With this technology, we can push internet intelligence to the edge,

102
00:05:13,040 --> 00:05:17,720
so every single person, country, culture, company can have their own AIs.

103
00:05:17,720 --> 00:05:21,400
And that's an amazing thing because it can work for you, not against you.

104
00:05:21,400 --> 00:05:24,800
And so that's the personalization because what is bias, right?

105
00:05:24,800 --> 00:05:27,200
There are inherent biases in our society and things,

106
00:05:27,200 --> 00:05:28,760
but we can never move fast on this thing.

107
00:05:28,760 --> 00:05:31,760
We can't move fast enough to address them.

108
00:05:31,760 --> 00:05:33,200
But then what if you could have your own stories?

109
00:05:33,240 --> 00:05:34,440
Everyone here has a story.

110
00:05:34,440 --> 00:05:37,400
Your members of A360, you believe in abundance.

111
00:05:37,400 --> 00:05:39,680
There's a whole variety of things that make us who we are.

112
00:05:39,680 --> 00:05:43,360
Have the AI respond to your own story and work for you.

113
00:05:43,360 --> 00:05:45,240
And I think that's the intelligent internet

114
00:05:45,240 --> 00:05:47,160
and that's the amazing future we have coming.

115
00:05:47,160 --> 00:05:47,840
That is.

116
00:05:47,840 --> 00:05:52,560
And you built Stability AI as an open platform,

117
00:05:52,560 --> 00:05:54,920
which is very different from everybody else.

118
00:05:54,920 --> 00:05:56,800
I mean, it's interesting, right?

119
00:05:56,800 --> 00:05:58,920
OpenAI has that name.

120
00:05:58,920 --> 00:06:02,080
And very funny, if you go to OpenAI.com,

121
00:06:02,120 --> 00:06:04,240
it doesn't go to OpenAI, it goes to stable.

122
00:06:04,240 --> 00:06:05,640
Well, actually, open.ai here.

123
00:06:05,640 --> 00:06:06,800
Open.ai, sorry.

124
00:06:06,800 --> 00:06:08,040
Yeah.

125
00:06:08,040 --> 00:06:09,200
Which is pretty funny.

126
00:06:09,200 --> 00:06:14,920
So inappropriate because you are the open AI platform.

127
00:06:14,920 --> 00:06:15,280
We are.

128
00:06:15,280 --> 00:06:17,880
I mean, actually, openai.com also works.

129
00:06:17,880 --> 00:06:20,880
But yeah, I mean, basically where we were

130
00:06:20,880 --> 00:06:25,000
was that we had this explosion of research.

131
00:06:25,000 --> 00:06:26,400
Look at this is, again, exponential,

132
00:06:26,400 --> 00:06:29,280
literal exponential of the amount of ML research that occurred.

133
00:06:29,280 --> 00:06:31,280
And we were like, let's build a generalized intelligence.

134
00:06:31,280 --> 00:06:34,080
When OpenAI and others started, we didn't know how to do it.

135
00:06:34,080 --> 00:06:35,960
And then we found this one thing called a transformer

136
00:06:35,960 --> 00:06:37,680
that pays attention to the important things.

137
00:06:37,680 --> 00:06:39,160
Like, you guys are paying attention

138
00:06:39,160 --> 00:06:41,240
to the important things that we're saying right now,

139
00:06:41,240 --> 00:06:43,240
as opposed to just treating everything the same.

140
00:06:43,240 --> 00:06:45,440
And it figured out that we could scale it with these GPUs,

141
00:06:45,440 --> 00:06:47,400
like, gigantic supercomputers.

142
00:06:47,400 --> 00:06:48,920
They didn't really know exactly how to do it,

143
00:06:48,920 --> 00:06:51,320
so they just kept on adding more and more supercomputers.

144
00:06:51,320 --> 00:06:51,920
And we've got a lot.

145
00:06:51,920 --> 00:06:54,840
Like, our supercomputer is 10 times faster than NASA's, right?

146
00:06:54,840 --> 00:06:57,400
Our new supercomputer is 10 times faster than that.

147
00:06:57,400 --> 00:06:59,640
So again, exponential.

148
00:06:59,680 --> 00:07:03,920
But then they thought this solves everything, scale solves everything.

149
00:07:03,920 --> 00:07:06,080
But have you ever seen a generalized system

150
00:07:06,080 --> 00:07:08,120
that outperforms a specialized system?

151
00:07:08,120 --> 00:07:09,120
No.

152
00:07:09,120 --> 00:07:10,760
But that means you have to give people the tools

153
00:07:10,760 --> 00:07:12,480
to make it a specialized system.

154
00:07:12,480 --> 00:07:15,800
You're never going to send all your internal company data

155
00:07:15,800 --> 00:07:17,720
to chat GPT or GPT-4.

156
00:07:17,720 --> 00:07:19,600
So that's an interesting point.

157
00:07:19,600 --> 00:07:21,880
And I don't know if I heard it from you or elsewhere,

158
00:07:21,880 --> 00:07:24,120
where some of the banks and large corporations

159
00:07:24,120 --> 00:07:28,080
are not sending their data and allowing employees

160
00:07:28,120 --> 00:07:29,960
to use chat GPT.

161
00:07:29,960 --> 00:07:31,720
They don't want Microsoft having access to it.

162
00:07:31,720 --> 00:07:32,720
So what's the alternative?

163
00:07:32,720 --> 00:07:33,920
How do you go around that?

164
00:07:33,920 --> 00:07:35,960
We have the most popular language models in the world,

165
00:07:35,960 --> 00:07:38,320
GPT-J, Neo-NX, 25 million downloads.

166
00:07:38,320 --> 00:07:41,840
We're about to release our next generation open source models.

167
00:07:41,840 --> 00:07:42,880
Again, base models.

168
00:07:42,880 --> 00:07:44,760
And then you'll have the banker, board versions,

169
00:07:44,760 --> 00:07:45,760
Indian versions.

170
00:07:45,760 --> 00:07:48,240
So I get to use it on my own system

171
00:07:48,240 --> 00:07:49,600
that no one else has access to.

172
00:07:49,600 --> 00:07:52,600
You get to go, is it via Amazon, Google, Intel, anyone?

173
00:07:52,600 --> 00:07:55,280
On-prem, in your cloud, you own it.

174
00:07:55,280 --> 00:07:57,160
Again, it's like you basically.

175
00:07:57,160 --> 00:07:58,880
It's like actually one thing.

176
00:07:58,880 --> 00:08:03,280
It's like, again, these are like very talented analysts.

177
00:08:03,280 --> 00:08:05,920
Getting an analyst on secondment from Microsoft,

178
00:08:05,920 --> 00:08:09,520
he goes back to Microsoft versus hiring your own analyst.

179
00:08:09,520 --> 00:08:10,600
These are the two modalities.

180
00:08:10,600 --> 00:08:13,240
So you'll have these big private proprietary models

181
00:08:13,240 --> 00:08:14,360
where there's lots of secret sources.

182
00:08:14,360 --> 00:08:16,440
They have these open interpretable models

183
00:08:16,440 --> 00:08:19,480
where you see how the cookie is made, as it were,

184
00:08:19,480 --> 00:08:20,800
that you own.

185
00:08:20,800 --> 00:08:23,280
And I think the latter is far more powerful than the former

186
00:08:23,280 --> 00:08:25,240
because you don't need a model that does everything then.

187
00:08:25,240 --> 00:08:28,800
You give people the tools they need and the ownership

188
00:08:28,800 --> 00:08:32,200
to create their own experiences and accelerate themselves.

189
00:08:32,200 --> 00:08:34,320
Let me ask you, we're all here to understand

190
00:08:34,320 --> 00:08:35,720
where this is going.

191
00:08:35,720 --> 00:08:38,120
How far out are you able to see?

192
00:08:38,120 --> 00:08:40,720
Can you have a sense of predicting where we are

193
00:08:40,720 --> 00:08:43,360
in 10 years, five years?

194
00:08:43,360 --> 00:08:44,920
I can't see past five years.

195
00:08:44,920 --> 00:08:45,920
You can't see past five years.

196
00:08:45,920 --> 00:08:48,200
No, because by next year, end of the year,

197
00:08:48,200 --> 00:08:50,680
I believe you'll have chat GPT on your mobile phone.

198
00:08:50,680 --> 00:08:51,600
Say that one more time.

199
00:08:51,600 --> 00:08:53,480
By the end of next year, you'll have chat GPT

200
00:08:53,480 --> 00:08:55,520
on your mobile phone without internet.

201
00:08:55,520 --> 00:08:56,000
Wow.

202
00:08:56,000 --> 00:09:02,560
So we're talking about a stability chat, right?

203
00:09:02,560 --> 00:09:03,360
Yeah, a stable chat.

204
00:09:03,360 --> 00:09:06,280
Stable chat, let's name it properly.

205
00:09:06,280 --> 00:09:08,640
So incredible because you've talked

206
00:09:08,640 --> 00:09:10,920
about taking how many terabytes of data

207
00:09:10,920 --> 00:09:12,440
down to a couple of gigabytes.

208
00:09:12,440 --> 00:09:15,720
So like, actually, this is probably one thing

209
00:09:15,720 --> 00:09:17,160
that I wanted to talk about quickly.

210
00:09:17,160 --> 00:09:19,240
Please, yeah.

211
00:09:19,240 --> 00:09:21,040
The easiest way for us to communicate

212
00:09:21,040 --> 00:09:22,640
has been speech is what we're doing now.

213
00:09:22,640 --> 00:09:24,720
Text has been harder, but you've seen with chat GPT,

214
00:09:24,720 --> 00:09:25,240
it's not easy.

215
00:09:25,240 --> 00:09:27,760
You just ask it to rewrite your kind of emails.

216
00:09:27,760 --> 00:09:29,400
And with Office 365 and Workspace,

217
00:09:29,400 --> 00:09:31,400
we'll do that, and Visual was the hardest.

218
00:09:31,400 --> 00:09:33,480
These have all been flattened by this technology now,

219
00:09:33,480 --> 00:09:34,960
so we can communicate anything.

220
00:09:34,960 --> 00:09:37,880
But the technology behind it, as you said, was a bit crazy.

221
00:09:37,880 --> 00:09:39,880
To do this, what we did, so stable diffusion

222
00:09:39,880 --> 00:09:41,680
was our text-to-image model.

223
00:09:41,680 --> 00:09:43,720
We were like, do we do it big and do loads of layers,

224
00:09:43,720 --> 00:09:45,240
or do we make it accessible?

225
00:09:45,240 --> 00:09:47,040
And we were like, accessible.

226
00:09:47,040 --> 00:09:49,120
So we worked super hard Manhattan project,

227
00:09:49,120 --> 00:09:52,880
and we took 100,000 gigabytes of images, 2 billion,

228
00:09:52,880 --> 00:09:55,280
and created a 2-gigabyte file that can generate anything.

229
00:09:55,280 --> 00:09:57,920
OK, let's slow that down, because the meaning of that

230
00:09:57,920 --> 00:10:00,080
is insane.

231
00:10:00,080 --> 00:10:02,240
How many terabytes was it?

232
00:10:02,240 --> 00:10:07,560
It was 0.1, 100 terabytes, 100,000 gigabytes,

233
00:10:07,560 --> 00:10:11,520
and the output was a 2-gigabyte file that can create anything.

234
00:10:11,520 --> 00:10:16,720
And that 2 gigabytes sits on your phone, your laptop,

235
00:10:16,720 --> 00:10:17,560
in your brain.

236
00:10:17,560 --> 00:10:18,480
It was the whole stack.

237
00:10:18,480 --> 00:10:20,600
So intelligence is compression.

238
00:10:20,600 --> 00:10:22,640
You're going to go away from this event,

239
00:10:22,640 --> 00:10:25,000
and you're going to compress the things that you learn

240
00:10:25,000 --> 00:10:26,640
and have to take away.

241
00:10:26,640 --> 00:10:29,240
That's what we do, and that's what these machines do now.

242
00:10:29,240 --> 00:10:30,480
But again, it's a single file.

243
00:10:30,480 --> 00:10:33,000
Four of the top 10 app store apps in December

244
00:10:33,000 --> 00:10:34,840
were based on stable diffusion.

245
00:10:34,840 --> 00:10:39,880
And it was the entire stack, a single file of weights,

246
00:10:39,880 --> 00:10:40,840
of numbers.

247
00:10:40,840 --> 00:10:42,200
That's extraordinary.

248
00:10:42,200 --> 00:10:44,600
Words go in, images come out.

249
00:10:44,600 --> 00:10:45,640
And it's the same.

250
00:10:45,640 --> 00:10:48,000
ChatGPT is a single file, actually, it's a couple files.

251
00:10:48,000 --> 00:10:49,760
So this is a single file.

252
00:10:49,760 --> 00:10:54,960
Words go in, code comes out, amazing essays come out.

253
00:10:54,960 --> 00:10:56,320
So this is something a bit different.

254
00:10:56,320 --> 00:10:57,920
And again, it's like, again, you've

255
00:10:57,920 --> 00:11:00,640
taught an analyst just about everything,

256
00:11:00,640 --> 00:11:03,200
or as an artist, the analyst, and things like that.

257
00:11:03,200 --> 00:11:06,320
I think we'll get it down to 100 megabytes.

258
00:11:06,320 --> 00:11:08,600
And it went insane, because we made it like that.

259
00:11:08,600 --> 00:11:12,600
So this is what you texted me, and I said, huh?

260
00:11:12,600 --> 00:11:17,160
So a lot of us were doing Web 3, and we were all developers.

261
00:11:17,160 --> 00:11:19,360
Developers are cool, right?

262
00:11:19,360 --> 00:11:21,720
In three months, we ever took Bitcoin and Ethereum

263
00:11:21,720 --> 00:11:23,280
and developed for popularity.

264
00:11:23,280 --> 00:11:26,040
So on the bottom here is the years.

265
00:11:26,040 --> 00:11:32,920
And on the Y axis here is the number of GitHub developers.

266
00:11:32,920 --> 00:11:35,080
GitHub stars, yeah, developers who love it.

267
00:11:35,080 --> 00:11:40,360
And so when you sent me that, I thought that was the axis line.

268
00:11:40,360 --> 00:11:41,280
Yeah.

269
00:11:41,280 --> 00:11:43,280
So cumulatively, the whole ecosystem

270
00:11:43,280 --> 00:11:45,520
built around this thing, which is a bit like a game engine,

271
00:11:45,560 --> 00:11:47,240
has overtaken Linux.

272
00:11:47,240 --> 00:11:51,000
Again, Linux 20 years this five months,

273
00:11:51,000 --> 00:11:53,440
because we gave it to everyone, and it runs on your MacBook

274
00:11:53,440 --> 00:11:56,080
or your iPhone without internet.

275
00:11:56,080 --> 00:11:59,480
So you have this amazing thing that can create anything.

276
00:11:59,480 --> 00:12:02,200
I think that's profound, right?

277
00:12:02,200 --> 00:12:06,360
So we've seen the speed, profound indeed.

278
00:12:06,360 --> 00:12:09,560
We've seen the speed just explode.

279
00:12:09,560 --> 00:12:11,480
Can we expect it to get faster and faster?

280
00:12:11,480 --> 00:12:12,640
Yeah, I think you can.

281
00:12:12,640 --> 00:12:14,840
So look, words go in, images come out.

282
00:12:14,840 --> 00:12:17,960
A lot of you have kind of seen this, you can generate anything.

283
00:12:17,960 --> 00:12:21,320
But since the release in August, we've sped up 100 times.

284
00:12:21,320 --> 00:12:23,840
Well, we've gone from six seconds in image

285
00:12:23,840 --> 00:12:26,360
to 60 images a second.

286
00:12:26,360 --> 00:12:28,440
And the quality has improved, and basically the next version

287
00:12:28,440 --> 00:12:29,680
is photorealistic.

288
00:12:29,680 --> 00:12:33,000
So 60 images per second is like video?

289
00:12:33,000 --> 00:12:34,640
Yeah, we pretty much got video.

290
00:12:34,640 --> 00:12:36,880
So my question is, is Hollywood scared shitless,

291
00:12:36,880 --> 00:12:38,080
or are they excited?

292
00:12:38,080 --> 00:12:38,760
They're scared.

293
00:12:38,760 --> 00:12:39,600
It's a mixture.

294
00:12:39,600 --> 00:12:41,080
There's a few people who are scared shitless,

295
00:12:41,080 --> 00:12:43,280
and we'll talk about some of the stuff coming down the pipeline

296
00:12:43,280 --> 00:12:45,600
later, because, you know, Preetr and I had a discussion,

297
00:12:45,600 --> 00:12:47,760
like, should we scare everyone shitless completely,

298
00:12:47,760 --> 00:12:49,200
or should we kind of do hope?

299
00:12:49,200 --> 00:12:50,600
And so I'm going to do hope, actually.

300
00:12:50,600 --> 00:12:52,240
I think I'm going to focus on this.

301
00:12:52,240 --> 00:12:54,920
This is the most disruptive thing ever, because, again, humans

302
00:12:54,920 --> 00:12:56,800
can scale, so you don't need as many humans.

303
00:12:56,800 --> 00:12:59,000
There was an MIT study, I think, I'll send it to you,

304
00:12:59,000 --> 00:13:00,040
which just came out.

305
00:13:00,040 --> 00:13:03,200
It showed that, basically, the third to the seventh

306
00:13:03,200 --> 00:13:05,880
decile got, like, 30% better.

307
00:13:05,880 --> 00:13:08,160
The top 5% got orders of magnitude better,

308
00:13:08,160 --> 00:13:10,720
or multiples better, with this technology.

309
00:13:10,720 --> 00:13:12,880
And so it lifts humanity and the ability

310
00:13:12,880 --> 00:13:14,200
to communicate and do things.

311
00:13:14,200 --> 00:13:17,600
So this is the hopeful future that we share,

312
00:13:17,600 --> 00:13:21,560
and the abundance mindset that I think

313
00:13:21,560 --> 00:13:23,320
is really important for people to take away.

314
00:13:23,320 --> 00:13:27,040
There's enough fear, and some of it is deserved,

315
00:13:27,040 --> 00:13:28,640
but there also needs to be hope.

316
00:13:28,640 --> 00:13:31,280
I think we always have to look at the unchanging

317
00:13:31,280 --> 00:13:33,120
between versus the inevitable.

318
00:13:33,120 --> 00:13:36,520
So an inevitable is 41% of all code on GitHub right now

319
00:13:36,520 --> 00:13:37,640
is AI generated.

320
00:13:37,640 --> 00:13:38,840
Wow.

321
00:13:38,840 --> 00:13:40,400
To six months.

322
00:13:40,400 --> 00:13:43,760
Which at GPT can pass a GLU level 3 programmer exam,

323
00:13:43,760 --> 00:13:46,440
and it will run pretty much on a MacBook or a phone.

324
00:13:46,440 --> 00:13:47,320
And that's this year.

325
00:13:47,320 --> 00:13:48,080
This year, right now.

326
00:13:48,080 --> 00:13:49,120
Yeah.

327
00:13:49,120 --> 00:13:51,040
There are no programmers in five years.

328
00:13:51,040 --> 00:13:53,640
No programmers in five years.

329
00:13:53,640 --> 00:13:56,160
So those of you with kids who you are having,

330
00:13:56,160 --> 00:13:58,920
you know, with Python lessons and so forth,

331
00:13:58,920 --> 00:14:02,640
maybe it's instead helping them to understand

332
00:14:02,640 --> 00:14:05,480
how to ask great questions or give great directions or prompts.

333
00:14:05,480 --> 00:14:07,720
Yeah, like with this, this is a technological marvel

334
00:14:07,720 --> 00:14:10,160
that we've sped up 100 times, because we have amazing developers,

335
00:14:10,160 --> 00:14:12,680
and we have communities of hundreds of thousands.

336
00:14:12,680 --> 00:14:16,960
I went to GPT4, and I said, help me write some code

337
00:14:16,960 --> 00:14:19,360
to change the nature of inference

338
00:14:19,360 --> 00:14:22,400
to something called int8 to int4, which is only a year old,

339
00:14:22,400 --> 00:14:24,360
so it's not in the data set.

340
00:14:24,360 --> 00:14:25,640
And it figured it out, and it worked.

341
00:14:25,640 --> 00:14:26,120
It figured it out.

342
00:14:26,120 --> 00:14:26,520
Awesome.

343
00:14:26,520 --> 00:14:27,080
Straight out.

344
00:14:27,080 --> 00:14:29,240
I asked it to create asteroids in D3.

345
00:14:29,240 --> 00:14:29,720
I love asteroids.

346
00:14:29,720 --> 00:14:30,600
We're the high school thing.

347
00:14:30,600 --> 00:14:30,840
Yeah.

348
00:14:30,840 --> 00:14:33,480
And I copy-pasted it, and it worked the game straight out.

349
00:14:33,480 --> 00:14:34,920
So again, we have to kind of think about this,

350
00:14:34,920 --> 00:14:36,440
and we have to think, but what can you do?

351
00:14:36,440 --> 00:14:38,360
Well, the answer is you can do anything now.

352
00:14:38,400 --> 00:14:42,080
Because a lot of the stuff that blocks you isn't there anymore.

353
00:14:42,080 --> 00:14:44,200
Any of you can now be creative.

354
00:14:44,200 --> 00:14:47,480
Any of you can now build systems.

355
00:14:47,480 --> 00:14:49,960
And so you build the systems that adhere

356
00:14:49,960 --> 00:14:53,680
to the unchanging demands of people to make their lives better.

357
00:14:53,680 --> 00:14:56,440
And that's value, and they'll pay you for that value.

358
00:14:56,440 --> 00:14:59,280
We're all creators, and we're, in fact,

359
00:14:59,280 --> 00:15:02,200
we had Will I Am last night at our patron dinner,

360
00:15:02,200 --> 00:15:07,920
who's this incredible creative energy across all modalities.

361
00:15:07,920 --> 00:15:10,960
And all of a sudden, you're not restricted by what

362
00:15:10,960 --> 00:15:12,760
you learned in school.

363
00:15:12,760 --> 00:15:15,640
You can bring the best to anything you desire.

364
00:15:15,640 --> 00:15:16,600
It's the best to anything done.

365
00:15:16,600 --> 00:15:17,440
You can control it.

366
00:15:17,440 --> 00:15:19,520
So you can just say, take something

367
00:15:19,520 --> 00:15:22,880
and change it to its original form, like this.

368
00:15:22,880 --> 00:15:25,040
From mindset to materialization.

369
00:15:25,040 --> 00:15:27,560
Which version of you do you like best?

370
00:15:27,560 --> 00:15:28,320
Exactly.

371
00:15:28,320 --> 00:15:29,600
And this is just with words.

372
00:15:29,600 --> 00:15:30,480
There's no more prompts.

373
00:15:30,480 --> 00:15:33,200
I just say, I want to change it to this or that.

374
00:15:33,200 --> 00:15:37,360
And it happens instantly now with a new version.

375
00:15:38,000 --> 00:15:40,280
You can say, let's do dynamic adaptation.

376
00:15:40,280 --> 00:15:42,000
Again, it happens in one second.

377
00:15:42,000 --> 00:15:44,960
You generate all of these variants.

378
00:15:44,960 --> 00:15:46,240
And you don't need those prompts.

379
00:15:46,240 --> 00:15:48,280
People say, there's all these magical spells.

380
00:15:48,280 --> 00:15:48,880
You don't say that.

381
00:15:48,880 --> 00:15:52,160
You say, I want to turn Woody into a still from a Western,

382
00:15:52,160 --> 00:15:54,160
or a place of fruit with cake.

383
00:15:54,160 --> 00:15:55,560
So it also becomes more natural.

384
00:15:55,560 --> 00:15:57,840
What are interfaces in the future?

385
00:15:57,840 --> 00:15:58,920
Interfaces are nothing.

386
00:15:58,920 --> 00:16:01,080
It's all about human extension.

387
00:16:01,080 --> 00:16:03,120
And it's about information theories

388
00:16:03,120 --> 00:16:04,320
at the core of all computer science.

389
00:16:04,320 --> 00:16:06,360
Information is valuable in as much

390
00:16:06,360 --> 00:16:07,880
as it changes the state.

391
00:16:07,880 --> 00:16:10,760
Now we finally have a friend to be with us all the time

392
00:16:10,760 --> 00:16:12,560
that can bring us the most valuable information

393
00:16:12,560 --> 00:16:14,040
and the most valuable state changes.

394
00:16:14,040 --> 00:16:17,480
You're a 20-year-old on our Zoom audience here,

395
00:16:17,480 --> 00:16:19,280
or in the room, or 23-year-old.

396
00:16:19,280 --> 00:16:23,040
And you're trying to decide what to do now.

397
00:16:23,040 --> 00:16:24,560
What is your advice?

398
00:16:24,560 --> 00:16:26,360
You just throw yourself 100% into this.

399
00:16:26,360 --> 00:16:29,280
It's the biggest change in society ever.

400
00:16:29,280 --> 00:16:31,400
It will be more disruptive than the pandemic

401
00:16:31,400 --> 00:16:32,320
in the next year or so.

402
00:16:32,320 --> 00:16:35,680
And I led the United Nations AI Initiative against COVID-19,

403
00:16:35,720 --> 00:16:38,360
Kayak, helping organize the world's COVID knowledge

404
00:16:38,360 --> 00:16:40,160
and making it accessible and useful.

405
00:16:40,160 --> 00:16:41,400
So I saw that coming.

406
00:16:42,440 --> 00:16:44,320
It'll be for positive and negative,

407
00:16:44,320 --> 00:16:46,200
because you can do things that can never be heard of.

408
00:16:46,200 --> 00:16:49,320
As an example, we have an upscaler.

409
00:16:49,320 --> 00:16:52,840
It goes from 128 pixels to 4K in 1.5 seconds.

410
00:16:54,040 --> 00:16:54,880
That to that.

411
00:16:55,720 --> 00:16:56,560
That's amazing.

412
00:16:56,560 --> 00:16:58,600
What can you do with that?

413
00:16:58,600 --> 00:16:59,440
This is the thing.

414
00:16:59,440 --> 00:17:01,400
You are given now the recipes and tools.

415
00:17:01,400 --> 00:17:02,800
What can you do?

416
00:17:02,800 --> 00:17:05,120
So there's gonna be a thousand companies

417
00:17:05,120 --> 00:17:07,080
spending $10 million each this year.

418
00:17:07,080 --> 00:17:09,120
I spoke to one of the big four accounting firms.

419
00:17:09,120 --> 00:17:10,400
They were like, do we need auditors anymore?

420
00:17:10,400 --> 00:17:12,080
I'm like, probably not as many.

421
00:17:13,280 --> 00:17:14,120
All right?

422
00:17:14,120 --> 00:17:14,960
And then they said, okay,

423
00:17:14,960 --> 00:17:17,600
we're gonna spend $300 million in the next two quarters

424
00:17:17,600 --> 00:17:19,520
trying to figure this out.

425
00:17:19,520 --> 00:17:22,680
But that's times many, many firms.

426
00:17:22,680 --> 00:17:24,560
Everything everywhere, all at once.

427
00:17:24,560 --> 00:17:25,920
And again, this is the emeticness.

428
00:17:25,920 --> 00:17:27,160
We've seen this type of thing happen

429
00:17:27,160 --> 00:17:29,680
like with the Silicon Valley Bank collapse.

430
00:17:29,680 --> 00:17:31,120
Society's become more and more connected.

431
00:17:31,120 --> 00:17:32,560
And we've talked about expert systems.

432
00:17:32,560 --> 00:17:34,640
How long have we talked about expert systems for?

433
00:17:34,640 --> 00:17:35,480
Yeah, a bit.

434
00:17:35,480 --> 00:17:36,320
Yolks.

435
00:17:36,320 --> 00:17:37,840
They're here now, right?

436
00:17:37,840 --> 00:17:41,000
And again, this is why you can do these crazy things.

437
00:17:41,000 --> 00:17:42,840
Like, why don't you create your own studio?

438
00:17:42,840 --> 00:17:45,360
So, how do we play this?

439
00:17:45,360 --> 00:17:46,600
We play this video.

440
00:17:46,600 --> 00:17:48,960
You speak and it magically happens.

441
00:17:48,960 --> 00:17:50,120
Play.

442
00:17:50,120 --> 00:17:50,960
There you go.

443
00:17:51,840 --> 00:17:53,680
This is a video of rock, paper, scissors

444
00:17:53,680 --> 00:17:55,880
that was done using our technology by two guys

445
00:17:55,880 --> 00:17:57,240
in like four days.

446
00:17:57,240 --> 00:17:58,640
And it's a whole seven minute video

447
00:17:58,640 --> 00:18:00,000
at this level of quality.

448
00:18:01,280 --> 00:18:02,280
That's all you need.

449
00:18:02,280 --> 00:18:04,440
So why not create amazing kind of videos

450
00:18:04,440 --> 00:18:05,680
and things like that?

451
00:18:05,680 --> 00:18:07,360
But we're gonna get to a point soon enough

452
00:18:07,360 --> 00:18:09,080
and I'll ask for your prediction where I can say,

453
00:18:09,080 --> 00:18:13,320
I want a movie about this theme, 90 minutes long,

454
00:18:13,320 --> 00:18:15,740
starring my favorite stars.

455
00:18:16,920 --> 00:18:18,680
How, when do we see that?

456
00:18:18,680 --> 00:18:19,680
That's why it's a movie.

457
00:18:19,680 --> 00:18:21,600
I think you get there in the next couple of years.

458
00:18:21,600 --> 00:18:22,960
Like rock, paper, scissors too.

459
00:18:22,960 --> 00:18:24,400
Again, look it up on Google.

460
00:18:24,400 --> 00:18:26,760
We'll be five times as fast.

461
00:18:26,760 --> 00:18:28,920
The next version will be fully generated

462
00:18:28,920 --> 00:18:30,400
by putting a script in.

463
00:18:30,400 --> 00:18:32,160
I mean, like this one, if you press play

464
00:18:32,160 --> 00:18:33,320
and it automatically generates

465
00:18:33,320 --> 00:18:34,320
a loop music lyric video.

466
00:18:34,320 --> 00:18:35,840
So unfortunately we don't have sound,

467
00:18:35,840 --> 00:18:38,600
but you can go on Deep Floyd AI on Twitter and see it.

468
00:18:38,600 --> 00:18:41,640
Automatically generated kind of this music lyric video.

469
00:18:43,120 --> 00:18:44,400
Just from a song input

470
00:18:44,400 --> 00:18:46,960
with the entire kind of style of this as well.

471
00:18:46,960 --> 00:18:48,680
So everyone here is a filmmaker.

472
00:18:48,680 --> 00:18:50,720
I mean, for me, we have a common passion

473
00:18:50,720 --> 00:18:53,240
in disrupting and reinventing education.

474
00:18:54,240 --> 00:18:57,920
Where I can have in a virtual world,

475
00:18:57,920 --> 00:18:59,640
learn about anything I want,

476
00:18:59,680 --> 00:19:03,600
a time machine to go and have my favorite characters

477
00:19:03,600 --> 00:19:05,640
teaching me about what anything I need.

478
00:19:06,680 --> 00:19:08,160
And that's two years away.

479
00:19:08,160 --> 00:19:10,200
Yeah, I mean, that actually might be here right now.

480
00:19:10,200 --> 00:19:12,000
I mean, again, kind of you've seen glasses,

481
00:19:12,000 --> 00:19:15,000
three 3D displays and all sorts of crazy things.

482
00:19:15,000 --> 00:19:17,800
But one of the things with this is contextualization.

483
00:19:17,800 --> 00:19:19,160
Because again, we are the stories we make.

484
00:19:19,160 --> 00:19:20,880
So one of the things we're working on that's quite fun

485
00:19:20,880 --> 00:19:25,400
is can you remake Game of Thrones as a J-Drama from Korea?

486
00:19:26,760 --> 00:19:29,440
So you're going to end the character's turn Korean, right?

487
00:19:29,680 --> 00:19:32,160
I'm also trying to get George R.R. Martin and H.V.O.

488
00:19:32,160 --> 00:19:34,240
to let me remake Game of Thrones season eight,

489
00:19:34,240 --> 00:19:35,560
because it was terrible.

490
00:19:38,080 --> 00:19:39,800
Yeah, and actually it's very interesting why.

491
00:19:39,800 --> 00:19:41,920
Because Game of Thrones season one to seven,

492
00:19:41,920 --> 00:19:44,600
everyone had agency and the meaning was in the interaction

493
00:19:44,600 --> 00:19:46,720
between our stories or the characters.

494
00:19:46,720 --> 00:19:48,440
Whereas the last one tried to get us to an ending

495
00:19:48,440 --> 00:19:50,200
and so it felt so disjointed.

496
00:19:50,200 --> 00:19:51,480
So I'm like, I'll help you write your book.

497
00:19:51,480 --> 00:19:53,240
We have the script writing AI coming out

498
00:19:53,240 --> 00:19:54,920
and a book writing AI coming out.

499
00:19:54,920 --> 00:19:56,240
And so we can tell better stories

500
00:19:56,240 --> 00:19:57,560
that are more empathetic and engaging.

501
00:19:57,560 --> 00:20:00,160
Because again, this is more engaging for the thing.

502
00:20:00,160 --> 00:20:02,280
You can customize your thing everywhere.

503
00:20:03,440 --> 00:20:05,400
You can customize to every learner.

504
00:20:05,400 --> 00:20:06,360
How crazy is that?

505
00:20:06,360 --> 00:20:07,200
And there's even crazy.

506
00:20:07,200 --> 00:20:09,120
All at once, a brand new theme.

507
00:20:09,120 --> 00:20:11,720
Iman, let's imagine you're in the audience here,

508
00:20:11,720 --> 00:20:14,920
you're a business owner, entrepreneur, CEO,

509
00:20:14,920 --> 00:20:19,920
and what do you do next?

510
00:20:19,920 --> 00:20:20,960
I think what you have to do next again

511
00:20:20,960 --> 00:20:21,800
is kind of what you said.

512
00:20:21,800 --> 00:20:23,120
You need to have a dedicated,

513
00:20:23,120 --> 00:20:24,440
there's nothing more important

514
00:20:24,440 --> 00:20:26,160
in your entire business than this.

515
00:20:26,160 --> 00:20:27,000
Yeah.

516
00:20:27,200 --> 00:20:29,400
So I want to echo that, right?

517
00:20:29,400 --> 00:20:31,520
Today, the world changes.

518
00:20:31,520 --> 00:20:32,920
Electricity is made available,

519
00:20:32,920 --> 00:20:34,640
the telephone is made available,

520
00:20:34,640 --> 00:20:35,800
the internet is made available,

521
00:20:35,800 --> 00:20:39,560
and this is all of those things together,

522
00:20:39,560 --> 00:20:40,600
all at once everywhere.

523
00:20:40,600 --> 00:20:41,840
Yeah, this is 5G.

524
00:20:41,840 --> 00:20:43,120
This is bigger than 5G.

525
00:20:43,120 --> 00:20:44,640
Over a trillion dollars went to 5G,

526
00:20:44,640 --> 00:20:45,840
more will go into here.

527
00:20:45,840 --> 00:20:46,800
Actually, I'll give you an example,

528
00:20:46,800 --> 00:20:48,640
because again, it's always the unchanging

529
00:20:48,640 --> 00:20:50,600
versus the inevitable, right?

530
00:20:50,600 --> 00:20:53,440
I gave the whole of stability AI

531
00:20:53,440 --> 00:20:54,480
the week off at the end of the year,

532
00:20:54,480 --> 00:20:56,000
because I was like, holy crap, you're doing a lot, right?

533
00:20:56,000 --> 00:20:57,160
Because we do everything, right?

534
00:20:57,160 --> 00:20:59,600
And we do it with, we only have 150 people,

535
00:20:59,600 --> 00:21:00,840
yet we do every modality

536
00:21:00,840 --> 00:21:02,320
because we have the power of community

537
00:21:02,320 --> 00:21:05,200
and open behind us, setting the standard.

538
00:21:05,200 --> 00:21:06,440
So I give the whole team the week off,

539
00:21:06,440 --> 00:21:09,480
said go to sleep, turned off all the things,

540
00:21:09,480 --> 00:21:11,480
because you're all going to die

541
00:21:11,480 --> 00:21:12,920
if you don't get some rest.

542
00:21:12,920 --> 00:21:14,640
So of course, business side of covered that.

543
00:21:14,640 --> 00:21:16,640
Stability AI CEO, I'm Adam Mostak,

544
00:21:16,640 --> 00:21:17,480
compared to Open AI,

545
00:21:17,480 --> 00:21:19,440
it says you're all going to die in 2023.

546
00:21:21,440 --> 00:21:22,240
What are we doing?

547
00:21:22,240 --> 00:21:23,680
We might be make press as well.

548
00:21:23,680 --> 00:21:24,520
What can I say?

549
00:21:25,520 --> 00:21:27,960
But then what happened is I was just chilling

550
00:21:27,960 --> 00:21:31,480
and I got like five calls from headmasters of UK schools.

551
00:21:32,640 --> 00:21:34,840
Emma, what's our generative AI strategy?

552
00:21:34,840 --> 00:21:37,040
I was like, what?

553
00:21:37,040 --> 00:21:41,040
All our children are using chat GPT to do their homework essays.

554
00:21:41,040 --> 00:21:44,280
I was like, don't set homework essays, get good.

555
00:21:44,280 --> 00:21:46,600
So again, homework is never the same

556
00:21:46,600 --> 00:21:48,680
for every teacher all at once.

557
00:21:48,680 --> 00:21:49,680
And so you have to look at your industry

558
00:21:49,680 --> 00:21:50,960
and again, what will not be the same

559
00:21:50,960 --> 00:21:53,400
when you have the ability to scale humans?

560
00:21:53,440 --> 00:21:55,600
This is the question that you have to ask yourself.

561
00:21:55,600 --> 00:21:58,960
Like an eaten now, a bastion of British education,

562
00:21:58,960 --> 00:22:01,720
they do their essays live by hand,

563
00:22:01,720 --> 00:22:02,680
because I told them to do that,

564
00:22:02,680 --> 00:22:03,640
because I went to Westminster,

565
00:22:03,640 --> 00:22:05,160
who I told to embrace the technology.

566
00:22:06,160 --> 00:22:08,000
It arrived at me, right?

567
00:22:08,000 --> 00:22:09,360
So again, it says Peter says,

568
00:22:09,360 --> 00:22:10,800
there will only be companies that embrace AI

569
00:22:10,800 --> 00:22:12,000
and those who don't.

570
00:22:12,000 --> 00:22:13,160
I think the way that it pays out,

571
00:22:13,160 --> 00:22:14,880
because I was a hedge fund manager,

572
00:22:14,880 --> 00:22:16,880
invested tens of billions of dollars,

573
00:22:16,880 --> 00:22:18,960
you know, like I understand markets as well,

574
00:22:18,960 --> 00:22:23,160
is that if you are in something that's a SaaS company,

575
00:22:23,240 --> 00:22:24,760
you're gonna have to really think hard,

576
00:22:24,760 --> 00:22:27,720
because GPT-4, for example,

577
00:22:27,720 --> 00:22:29,240
and again, we'll have our equivalent of that,

578
00:22:29,240 --> 00:22:30,320
but we're taking a different approach

579
00:22:30,320 --> 00:22:32,360
of swarm intelligence versus general,

580
00:22:32,360 --> 00:22:35,840
has a 32,000 token context window,

581
00:22:35,840 --> 00:22:39,640
which means you can feed it 20,000 words of instructions.

582
00:22:39,640 --> 00:22:41,480
Well, what is OpenAI at right now?

583
00:22:41,480 --> 00:22:43,120
That's their one, GPT-4.

584
00:22:43,120 --> 00:22:44,960
The one that they, OpenAI is the best language.

585
00:22:44,960 --> 00:22:46,760
And all you should use OpenAI,

586
00:22:46,760 --> 00:22:48,560
and you should use our models coming up as well,

587
00:22:48,560 --> 00:22:52,240
private data and general data, hybrid AI, you know?

588
00:22:52,240 --> 00:22:55,120
But that means you can basically tell it all of HR

589
00:22:55,120 --> 00:22:56,720
instructions for something like Workday.

590
00:22:56,720 --> 00:22:58,160
What does Workday do then?

591
00:22:59,240 --> 00:23:02,160
Because it can be replaced by a single 200 megabyte file.

592
00:23:02,160 --> 00:23:04,120
Which you can query on your laptop or your phone.

593
00:23:04,120 --> 00:23:05,920
Well, yeah, eventually, but not now,

594
00:23:05,920 --> 00:23:07,040
but it just doesn't matter.

595
00:23:07,040 --> 00:23:08,840
It costs like $2 for query, right?

596
00:23:08,840 --> 00:23:09,680
Right.

597
00:23:09,680 --> 00:23:10,800
I mean, it's insane.

598
00:23:10,800 --> 00:23:11,960
So, but then you have to think,

599
00:23:11,960 --> 00:23:13,240
if you're in a regulated industry,

600
00:23:13,240 --> 00:23:14,640
how's super normal margins?

601
00:23:15,600 --> 00:23:17,440
You know, if you're in a creative industry,

602
00:23:17,440 --> 00:23:19,640
again, you can basically either embrace it

603
00:23:19,640 --> 00:23:22,400
to have higher revenue, cheaper costs, or not.

604
00:23:22,400 --> 00:23:23,400
Like I give you an example,

605
00:23:23,400 --> 00:23:27,680
one of our movie directors did a film,

606
00:23:27,680 --> 00:23:29,200
who actually, I can't give you an example.

607
00:23:29,200 --> 00:23:30,320
Let's just say millions of dollars

608
00:23:30,320 --> 00:23:33,040
are being saved at the moment using this technology.

609
00:23:33,040 --> 00:23:34,800
And again, you just have to really think through it.

610
00:23:34,800 --> 00:23:37,320
So, going back to our audience here,

611
00:23:37,320 --> 00:23:39,760
who, you know, use technology,

612
00:23:39,760 --> 00:23:43,560
but don't have AI embedded in their DNA.

613
00:23:43,560 --> 00:23:46,600
I've shared the idea of having a chief AI officer

614
00:23:46,600 --> 00:23:48,160
who's really a strategist for you,

615
00:23:48,160 --> 00:23:50,480
because you're not building your own AIs,

616
00:23:50,480 --> 00:23:53,640
you're deciding which platforms to use, right?

617
00:23:53,640 --> 00:23:54,480
Yeah.

618
00:23:54,480 --> 00:23:56,720
And again, a very few companies will build their own AI.

619
00:23:56,720 --> 00:23:58,440
So, I think it'll just be Google, Microsoft,

620
00:23:58,440 --> 00:24:00,560
OpenAI, and us that build the foundation models

621
00:24:00,560 --> 00:24:02,600
that everyone uses, because you don't need to.

622
00:24:02,600 --> 00:24:04,080
Because again, it's like,

623
00:24:04,080 --> 00:24:06,160
where the university is churning out

624
00:24:06,160 --> 00:24:08,800
the most amazing graduates, effectively.

625
00:24:08,800 --> 00:24:10,160
You know, some of them you can borrow,

626
00:24:10,160 --> 00:24:12,000
and they're specialized by those guys,

627
00:24:12,000 --> 00:24:14,120
and some of them you can hire yourself,

628
00:24:14,120 --> 00:24:16,880
through your GPUs, or kind of whatever.

629
00:24:16,880 --> 00:24:19,040
My suggestion is that that, what's the,

630
00:24:19,040 --> 00:24:21,280
but the thing is, how do you find that person?

631
00:24:21,280 --> 00:24:22,120
Hmm.

632
00:24:22,120 --> 00:24:23,720
You know, and until they're an AI.

633
00:24:23,720 --> 00:24:24,840
No, how do you find that person?

634
00:24:24,840 --> 00:24:25,680
Advice?

635
00:24:25,680 --> 00:24:27,360
The advice is, you need to first find someone

636
00:24:27,360 --> 00:24:30,160
who's passionate about this, because this is so new.

637
00:24:30,160 --> 00:24:31,760
So, one of the things that makes us different

638
00:24:31,760 --> 00:24:33,360
is that we're a community-based organization.

639
00:24:33,360 --> 00:24:34,960
We came out of a community.

640
00:24:34,960 --> 00:24:37,520
So, we have developers, one of them was an Amazon warehouse

641
00:24:37,520 --> 00:24:39,440
worker last year, and taught himself how to code.

642
00:24:39,440 --> 00:24:40,280
Nice.

643
00:24:40,280 --> 00:24:42,120
Another one's a 19-year-old, 50-year PhD

644
00:24:42,120 --> 00:24:44,000
with three graduate degrees, you know?

645
00:24:44,000 --> 00:24:46,600
It'll be clear, it's not gonna be the person

646
00:24:46,680 --> 00:24:49,160
who is traditionally the person you would think about.

647
00:24:49,160 --> 00:24:50,320
It is driven by passion.

648
00:24:50,320 --> 00:24:51,160
It's driven by passion.

649
00:24:51,160 --> 00:24:53,520
Everyone here understands the importance of passion,

650
00:24:53,520 --> 00:24:54,640
and passion is what you need for this,

651
00:24:54,640 --> 00:24:57,160
because this is a regime change.

652
00:24:57,160 --> 00:24:59,160
It is not more of what came before.

653
00:24:59,160 --> 00:25:00,480
So, you have to throw yourself in it,

654
00:25:00,480 --> 00:25:03,240
and have that flexibility of mind.

655
00:25:03,240 --> 00:25:04,680
And the key thing is, can it bring you

656
00:25:04,680 --> 00:25:07,120
what the next thing is for your industry?

657
00:25:07,120 --> 00:25:08,120
You know?

658
00:25:08,120 --> 00:25:10,920
Everybody, this is Peter, a quick break from the episode.

659
00:25:10,920 --> 00:25:14,000
I'm a firm believer that science and technology,

660
00:25:14,000 --> 00:25:16,680
and how entrepreneurs can change the world,

661
00:25:16,680 --> 00:25:20,000
is the only real news out there worth consuming.

662
00:25:20,000 --> 00:25:22,160
I don't watch the Crisis News Network.

663
00:25:22,160 --> 00:25:25,520
I call CNN or Fox, and hear every devastating piece

664
00:25:25,520 --> 00:25:26,880
of news on the planet.

665
00:25:26,880 --> 00:25:29,960
I spend my time training my neural net,

666
00:25:29,960 --> 00:25:31,480
the way I see the world,

667
00:25:31,480 --> 00:25:34,400
by looking at the incredible breakthroughs in science

668
00:25:34,400 --> 00:25:36,160
and technology, and how entrepreneurs

669
00:25:36,160 --> 00:25:38,400
are solving the world's grand challenges,

670
00:25:38,400 --> 00:25:41,200
what the breakthroughs are in longevity,

671
00:25:41,200 --> 00:25:45,120
how exponential technologies are transforming our world.

672
00:25:45,120 --> 00:25:47,400
So, twice a week, I put out a blog.

673
00:25:47,400 --> 00:25:50,720
One blog is looking at the future of longevity,

674
00:25:50,720 --> 00:25:54,960
age reversal, biotech, increasing your health span.

675
00:25:54,960 --> 00:25:59,080
The other blog looks at exponential technologies, AI,

676
00:25:59,080 --> 00:26:02,440
3D printing, synthetic biology, AR, VR, blockchain.

677
00:26:02,440 --> 00:26:04,120
These technologies are transforming

678
00:26:04,120 --> 00:26:05,720
what you as an entrepreneur can do.

679
00:26:05,720 --> 00:26:07,800
If this is the kind of news you wanna learn about,

680
00:26:07,800 --> 00:26:09,600
and shape your neural nets with,

681
00:26:09,640 --> 00:26:14,200
go to dmandis.com, backslash blog, and learn more.

682
00:26:14,200 --> 00:26:15,640
Now, back to the episode.

683
00:26:15,640 --> 00:26:18,480
We share a moonshot in education.

684
00:26:18,480 --> 00:26:21,080
I've had the ability to support, to some degree,

685
00:26:21,080 --> 00:26:22,360
what you're doing.

686
00:26:22,360 --> 00:26:25,640
Just chat one second about your vision on Malawi.

687
00:26:25,640 --> 00:26:29,080
Yeah, so, you know, X Prize for Learning,

688
00:26:29,080 --> 00:26:31,760
fantastic prize by Tony, who's here next week tomorrow,

689
00:26:31,760 --> 00:26:33,760
I think, and then Elon Musk.

690
00:26:33,760 --> 00:26:34,600
Yeah.

691
00:26:34,600 --> 00:26:36,280
For the first app that could teach literacy and numeracy

692
00:26:36,280 --> 00:26:38,840
in 18 months without internet.

693
00:26:38,880 --> 00:26:40,600
We've been deploying that into refugee camps,

694
00:26:40,600 --> 00:26:42,520
the winner of that, all around the world,

695
00:26:42,520 --> 00:26:44,560
from Rohingya to Malawi to Kenya,

696
00:26:44,560 --> 00:26:48,520
and we're teaching 76% of kids literacy and numeracy

697
00:26:48,520 --> 00:26:50,680
in one hour a day in 13 months,

698
00:26:50,680 --> 00:26:53,480
in the worst places in the world with adaptive learning.

699
00:26:53,480 --> 00:26:54,960
Yeah, let's give it up for this.

700
00:26:54,960 --> 00:26:55,800
Yeah.

701
00:26:58,800 --> 00:27:02,800
That's been the core of Elon's passion and moonshot,

702
00:27:02,800 --> 00:27:04,320
but he's taking it a step further.

703
00:27:04,320 --> 00:27:07,440
Taking it a step further, because by doing that,

704
00:27:07,480 --> 00:27:08,880
this is without this AI.

705
00:27:09,800 --> 00:27:11,720
So we work with the Malawi in government,

706
00:27:11,720 --> 00:27:13,280
as the first, for example,

707
00:27:13,280 --> 00:27:15,320
we're feeding 30% of all the kids in Malawi,

708
00:27:15,320 --> 00:27:17,960
we're gonna go to 100% and give every child in Malawi

709
00:27:17,960 --> 00:27:19,160
their own AI.

710
00:27:19,160 --> 00:27:20,280
I wanna call it one AI for a child,

711
00:27:20,280 --> 00:27:21,440
but I've been told not to.

712
00:27:22,440 --> 00:27:25,040
And that's kind of what our charity Imagine Worldwide,

713
00:27:25,040 --> 00:27:26,680
but then we've built a special type of bond

714
00:27:26,680 --> 00:27:27,920
with the World Bank and UBS,

715
00:27:27,920 --> 00:27:29,480
whereby you put down $20 million,

716
00:27:29,480 --> 00:27:32,360
you only pay if a million kids are provably educated.

717
00:27:33,640 --> 00:27:35,320
And so we're gonna use that,

718
00:27:35,320 --> 00:27:36,440
and the World Bank pays up front,

719
00:27:36,440 --> 00:27:37,640
and then you can have it in your pension fund

720
00:27:37,640 --> 00:27:39,200
and see how a million kids are doing.

721
00:27:39,200 --> 00:27:40,840
We're gonna use that to scale these tablets,

722
00:27:40,840 --> 00:27:42,480
plus high-speed internet in every school,

723
00:27:42,480 --> 00:27:44,520
to every child across Africa.

724
00:27:44,520 --> 00:27:46,160
Nine out of 10 kids in Africa

725
00:27:46,160 --> 00:27:48,640
cannot read and write a sentence by the age of 10.

726
00:27:48,640 --> 00:27:50,880
What happens when they all have their own AI

727
00:27:50,880 --> 00:27:51,920
that works for them,

728
00:27:51,920 --> 00:27:54,440
that's in as intelligent as chat GPT,

729
00:27:54,440 --> 00:27:56,200
and healthcare on that tablet?

730
00:27:56,200 --> 00:27:57,040
Yes.

731
00:27:57,040 --> 00:27:59,280
The AI teaches the kids, learns from the kids,

732
00:27:59,280 --> 00:28:01,120
and that will also create the national models

733
00:28:01,120 --> 00:28:02,280
for every country.

734
00:28:02,280 --> 00:28:04,080
Right now, we hold the eyes open of the AI,

735
00:28:04,080 --> 00:28:05,480
and we teach the whole internet,

736
00:28:05,520 --> 00:28:08,480
which is why it goes a bit crazy, you know?

737
00:28:08,480 --> 00:28:09,760
And again, we use reinforcement learning

738
00:28:09,760 --> 00:28:10,800
and other things to guide it.

739
00:28:10,800 --> 00:28:11,960
This will be different.

740
00:28:11,960 --> 00:28:14,640
So that technology scales.

741
00:28:14,640 --> 00:28:16,880
Again, there's nothing more as impactful

742
00:28:16,880 --> 00:28:18,000
as one-to-one tuition,

743
00:28:18,000 --> 00:28:19,920
and now we can do it in an empathetic way

744
00:28:19,920 --> 00:28:21,400
that adapts to each child.

745
00:28:21,400 --> 00:28:23,120
I believe that we can also address things

746
00:28:23,120 --> 00:28:24,280
like dyslexia and other things,

747
00:28:24,280 --> 00:28:26,240
so it's just information processing.

748
00:28:26,240 --> 00:28:27,240
And you will see that coming.

749
00:28:27,240 --> 00:28:29,440
We're doing a big drive towards that.

750
00:28:29,440 --> 00:28:32,040
Any information processing issues will be solved by this.

751
00:28:32,040 --> 00:28:33,720
Are you an auditory learner, visual learner?

752
00:28:33,720 --> 00:28:34,760
There are no more interfaces.

753
00:28:34,760 --> 00:28:36,360
So it's completely customized learning

754
00:28:36,360 --> 00:28:37,480
for the individual,

755
00:28:37,480 --> 00:28:39,320
their favorite color sports star,

756
00:28:39,320 --> 00:28:40,640
way of learning modality,

757
00:28:40,640 --> 00:28:43,880
knows exactly where they are and where they need to go.

758
00:28:43,880 --> 00:28:46,640
Building the foundation to activate humanity's potential.

759
00:28:46,640 --> 00:28:49,600
And so we're gonna open source on new language models

760
00:28:49,600 --> 00:28:51,520
in our next month,

761
00:28:51,520 --> 00:28:54,480
and then we're gonna announce the next generation of this.

762
00:28:54,480 --> 00:28:57,160
An open model for all of the world

763
00:28:57,160 --> 00:28:58,720
that you deserve for education and health

764
00:28:58,720 --> 00:29:01,240
and other things, because humans are humans.

765
00:29:01,240 --> 00:29:03,480
Bring them the information that creates the most value

766
00:29:03,480 --> 00:29:05,000
and you will change the world.

767
00:29:06,280 --> 00:29:09,560
Is this gonna revolutionize healthcare to the point of...

768
00:29:09,560 --> 00:29:11,560
Yes, let's give it up for Imad on that one, yes?

769
00:29:11,560 --> 00:29:12,400
Yes.

770
00:29:12,400 --> 00:29:13,240
Yes.

771
00:29:16,240 --> 00:29:17,840
You got just two minutes left here, health,

772
00:29:17,840 --> 00:29:19,240
and then you're coming back to the Q&A,

773
00:29:19,240 --> 00:29:20,120
which will be the fun part,

774
00:29:20,120 --> 00:29:22,080
because the audience gets to participate.

775
00:29:23,280 --> 00:29:25,320
I've always talked about the best diagnosticians,

776
00:29:25,320 --> 00:29:27,560
the best healthcare is gonna be AI,

777
00:29:27,560 --> 00:29:29,400
which is gonna level the playing field,

778
00:29:29,400 --> 00:29:31,480
the poorest child, the wealthiest child,

779
00:29:31,480 --> 00:29:34,120
at the same level across the board, yes?

780
00:29:34,120 --> 00:29:35,560
Humans are humans.

781
00:29:35,560 --> 00:29:37,320
This is why I kind of did the Kayak project,

782
00:29:37,320 --> 00:29:38,160
lots of years ago.

783
00:29:38,160 --> 00:29:39,600
What a great advantage that is.

784
00:29:39,600 --> 00:29:43,120
It's a huge advantage, and again, this is the thing.

785
00:29:43,120 --> 00:29:45,680
This is the great equalizer, or it's the great controller,

786
00:29:45,680 --> 00:29:48,040
which is why I'm open versus closed.

787
00:29:48,040 --> 00:29:49,960
This cannot be controlled by any entity.

788
00:29:49,960 --> 00:29:51,560
We have to distribute this,

789
00:29:51,560 --> 00:29:53,520
because I believe this is a human right,

790
00:29:53,520 --> 00:29:55,920
because it's the next element we're gonna introduce you.

791
00:29:55,920 --> 00:30:00,920
So, speaking about that,

792
00:30:01,280 --> 00:30:04,040
we're in the middle of the AI wars,

793
00:30:04,040 --> 00:30:06,720
and a little rumor told me

794
00:30:06,720 --> 00:30:08,560
that you got kicked off LinkedIn today.

795
00:30:08,560 --> 00:30:12,200
Yeah, they identified our website as Malware first last week,

796
00:30:12,200 --> 00:30:13,560
and then made all our job descriptions.

797
00:30:13,560 --> 00:30:14,400
Today we're going to be kids.

798
00:30:14,400 --> 00:30:15,440
Who owns LinkedIn, by the way?

799
00:30:15,440 --> 00:30:16,480
I couldn't say.

800
00:30:17,320 --> 00:30:19,120
Oh, Microsoft, huh?

801
00:30:20,400 --> 00:30:21,600
So, it's gonna be very interesting.

802
00:30:21,600 --> 00:30:23,400
The stakes are really high.

803
00:30:23,400 --> 00:30:24,440
So, there's a lot of people

804
00:30:24,440 --> 00:30:26,920
that don't want this technology to go out to the world.

805
00:30:26,920 --> 00:30:29,000
Again, I think putting it on these tablets

806
00:30:29,000 --> 00:30:31,680
for all these kids, building a national mold to everyone,

807
00:30:31,680 --> 00:30:34,040
again, our mission is to build the foundation

808
00:30:34,040 --> 00:30:36,080
to activate humanized potential,

809
00:30:36,080 --> 00:30:37,640
and our motto is not, don't be evil,

810
00:30:37,640 --> 00:30:39,160
let's make people happier.

811
00:30:39,160 --> 00:30:41,720
And the happiest you can be is when you have agency.

812
00:30:41,720 --> 00:30:43,200
Let everyone have their own agency

813
00:30:43,200 --> 00:30:45,600
and create insane things.

814
00:30:45,600 --> 00:30:48,160
On that note, let's give it up for Emon Scott.

815
00:30:48,160 --> 00:30:49,000
Good.

816
00:30:49,000 --> 00:30:49,840
Good.

