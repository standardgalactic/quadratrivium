start	end	text
0	2560	So again, like, if you're an entrepreneur, you'd be an entrepreneur in this.
2560	7920	If you are someone who can communicate, you communicate this to other people and you get paid a million bucks a year as a consultant.
7920	9560	Right. You organize information.
9560	14960	If you're an artist, if you're creative, if you're an artist, you become the most efficient artist in the world when you lean in on this.
15560	18680	You know, like systems can be outcompeted.
18680	20400	It's like the example of the steel mill, right?
20400	24600	There were big vertically integrated steel mills that were outcompeted by lots of little steel mills.
24600	26440	Sure. Micro mills, right?
26440	34480	The big corporations, the big programs, the big things will be outcompeted by just individuals and small groups.
34480	36520	Building on top of this technology can do anything.
38200	41720	And a massive transform to purpose is what you're telling the world.
41720	44920	It's like, this is who I am. This is what I'm going to do.
44920	47080	This is the dent I'm going to make in the universe.
50520	52960	Welcome, everybody. Welcome to Moonshots and Mindsets.
53000	57840	I am here with an old friend and a new friend, Imad Mustak.
57840	60320	Imad, welcome. It's a pleasure to have you here.
60560	61240	Thank you for having me.
61240	71080	We're going to have such a fun, wide-ranging conversation across everything that is of, I think, importance to anyone listening to any entrepreneurs,
71080	73560	any CEOs, any government leaders, any kids.
74080	77160	Everything that you're doing is really transforming the world.
77160	78480	Let me do a proper introduction.
78480	86960	Imad Mustak is the founder and CEO of stability.ai, focused on amplifying humanity's potential through AI.
86960	93920	You probably know stability AI because of its text-to-image model, Stable Diffusion, released in 2022,
93920	98800	which rocked the developed world and I think broke the internet as a good way to describe it.
98800	103200	Previously, Imad has been a hedge fund manager and autism researcher.
103200	109200	Now, he's led multiple technology initiatives across multilateral organizations and governments
110320	111920	and an XPRIZE competitor.
111920	118720	Today, Imad is pursuing an incredible Moonshot, actually a series of Moonshots that can transform many industries.
118720	123200	You know, I started by saying, which industries are you looking to disrupt?
123520	127360	And your answer was, all of them, could do better.
128080	130560	And I think that's not hype.
130560	135280	I think that's actually, from what I understand, what we're going to explore, true.
137280	140800	Scientists and technologists have been talking about AI for decades.
142640	144960	But today is different, isn't it?
145520	148880	Yeah. I mean, we always say this time is different, but this time really is different.
149520	153040	I mean, if you kind of look at it, AI is basically information classification.
153040	157520	And we had classic AI, which was information goes in and then you extrapolate from a dataset,
157520	159760	you create really custom models and it goes out.
159760	164960	Like the big AIs, Internet2 was Google and Facebook taking all that big data and then
164960	170880	targeting PETA with like ads for rockets, you know, and things like that, you know, like rockets.
172720	177680	In 2017, we had a bit of a step change where there was a paper called attention is all you need.
178480	181360	About how to teach an AI to pay attention to the important things.
181360	182160	Fascinating.
182160	183760	And learn principles.
183760	185360	Yeah, principle based analysis.
186240	190880	Yeah. So it's not good old fashioned AI or kind of logic causal based AI, but it's kind of that.
190880	196400	Because you remember, like, who was it by type one, type two thinking?
197200	197760	A con.
198960	203040	So, you know, there's the very logical thing and there's a freaking tiger in that bush over there.
204160	205680	And we didn't have that second part.
205680	208800	We didn't have the ability to kind of just leap to conclusions,
208800	210800	principle based, heuristic based thinking.
210800	212960	You know, this is kind of the mindset thing, right?
213040	217840	Whereby you construct these things and it allows you to go very fast to just amazing conclusions.
217840	219600	That's part of what makes humans humans.
219600	220320	Yes.
220320	221040	It's here now.
221040	222560	It actually works.
222560	227120	Leaping to conclusion, but so it's no longer extrapolation based upon data.
227120	233120	It's actually learning to understand the meaning hidden in the data.
233120	233920	Exactly.
233920	239040	It's kind of semantic understanding, kind of literally, it's kind of called latent spaces,
239040	240720	hidden layers of understanding.
240720	243440	So, like, I'll give you an example.
243440	248160	One of the ways that I kind of entered AI was working with AI to do drug repurposing
248160	249360	for my son who has autism.
249360	249920	Yes.
249920	253680	So, he was two years old at the time and the doctor said nothing could be done.
253680	256000	And I was like, of course, there's something can be done.
256000	257840	Like, let's try and figure out information.
257840	260000	Because all we're looking for is information that needs to come in time.
260000	262400	It's kind of the Claude Shannon theory of information theories.
262400	264880	Information is valuable in as much as it changes the state.
264880	268160	So, how do you find the valuable information amongst all those autism studies
268160	269040	and figure out what's going on?
269040	271760	Because no one can really explain to me what caused it.
271760	278400	So, just one second there, because typically, people are looking for a very clear cut answer
278400	280560	in the data that's obvious.
281120	283040	But there are answers, but it's hidden.
283680	284400	It's hidden.
284400	285120	It's kind of hidden.
285120	287760	And so, you have to look at things from a first principles basis when you can't see
287760	289680	things just from scanning everything.
289680	292240	You have to dig down to another layer, another layer, another layer.
292800	298800	And kind of when I dig down, I kind of used AI to do natural entity recognition
299120	301600	look at all the different compounds, all the different trials that were tried,
301600	306400	built a team, then built a biomolecular pathway analysis model of neurotransmitters
306400	306720	in the brain.
306720	307920	So, it's like just like a common cold.
307920	309760	It seems like something very similar is happening.
310800	312800	It turns out there's two neurotransmitters in the brain.
312800	313920	One of them is GABA.
313920	315840	You pop a valium and chill out.
315840	316480	It chills you out.
317120	318640	And then another one is Glucimate.
318640	321360	And Glucimate kind of makes your brain accelerate.
321360	324240	And so, you know, when you're tapping your leg and you can't focus and concentrate,
324240	325120	that's what happens.
325120	328560	Because humans are really a filtering entity, as it were.
328560	331120	We have so much information in the world is just too much.
331120	332320	So, our brain creates a simulation.
332320	334960	That's why we've got the optic nerve, for example, filled it instantly.
336000	339920	We are kind of constrained a little bit and that filtering allows us to focus.
339920	342800	We all know what it's like when you're tapping your leg because there's too much stuff going on.
342800	345280	So, with kids with ASD, they couldn't filter for various reasons,
345280	346400	but it was different reasons.
346400	348800	Not enough GABA, too much Glutamate.
348800	353040	When you're born, GABA and Glutamate are actually both excitatory and oxytocin flips the switch
353040	353600	on GABA.
353600	354080	Interesting.
354080	356080	So, we started digging down into kind of the things and we're like,
356080	358080	ah, different compounds can affect this in different ways.
358880	360640	But then what was the upshot of that?
360640	362720	And how is this long story coming to a conclusion?
363840	366080	Because there's too much noise, my son couldn't speak,
366080	370800	because he couldn't formulate the hidden layers of meaning and connectivity on concepts.
370800	372240	So, kind of we've got cup here.
372240	372640	Yes.
372640	376000	And you can cup your hands, you have a World Cup, which England hopefully will win,
376000	379680	you know, and you've got various other meanings of the World Cup.
379680	382800	You form this latent space of hidden connective meaning to that.
382800	385920	So, it can be applied in different things, the principle-based analysis.
385920	390080	By reducing the amount of noise, you can then filter and pay attention and build that out.
390080	391920	The AI can now do the same thing.
391920	395520	So, you don't need to have huge petabytes, exabytes of data anymore.
395520	398080	With structured data, it can figure out interconnectivity and connection.
398080	403440	And I want to come back to that because that's the revolution that you're creating right now.
404080	410160	Just, you know, these personalized data sets that are good for you, your country,
410160	411600	your company, yourself.
411600	412640	Your context, exactly.
412640	412720	Yes.
412720	415520	Because humans are heuristic animals.
415520	417600	We learn by principle-based analysis.
418240	421200	And we're animals that are story-based.
421200	424400	So, you have multiple stories that make you up, you know, from your work on the X-Price
424400	427440	to your humanitarian to, you know, bold VC fund and all the things.
428160	430320	We all kind of form connectivity there.
430320	432560	But those stories are very hard to map initially.
432560	434320	Now, we can do it dynamically.
434320	438000	And we can start building tools to re-augment human potential by doing this.
438000	439920	Because we can finally have that assistance.
439920	442640	We're going to go so much, have so much fun on these conversations.
442640	448960	But I want to take us back to some people around the world for the last year or so
448960	453040	have heard of Dolly and Dolly 2.
453040	455040	And here comes stable diffusion.
455680	459520	That is an open source version of Dolly and Dolly 2.
459520	466320	And as I said, broken internet, you sent me an image of the speed of growth on GitHub.
466320	466800	Yeah.
466800	469680	Of people using stable diffusion.
469680	472240	And it was so funny because when you first sent me this image,
472720	479120	here you see on GitHub the users who are on Ethereum over time.
480240	486800	And then there's this line that goes straight up that I thought was one of the axes for the graph.
488080	489120	Like what happened there?
489120	491920	So first of all, what is stable diffusion?
491920	494080	It's one product of your company, right?
494080	500640	Well, stable diffusion is a community effort to build an AI that allows anyone to create anything.
500640	505680	So words go in and then anything you describe comes out, which is a bit insane.
506720	511040	So what we did is, you know, in collaboration with various entities,
511040	514240	which we paid an important part in stable diffusion to is kind of led by us.
514800	519360	We took 100,000 gigabytes of image label pairs, two billion images,
519360	523760	and created a 1.6 gigabyte file that can run offline in your MacBook.
523840	532480	1.6 gigabyte file, which is relatively you can transmit it over the phone network.
532480	535200	You transmit it over the phone network, but you only need to transmit it once.
536080	540320	Everyone having this tiny file that basically compresses the visual information of a snapshot
540320	546880	of the internet can create anything from a high-resolution render of an apartment
547440	550880	to Robert De Niro's Gandalf or anything else.
551520	556000	And they can now do it in a second, something a week.
556000	558400	They can do one of those images in 30 seconds.
559440	560880	You mean one 30th of a second?
560880	561600	One 30th of a second.
561600	562560	We just broke in real time.
562560	564240	We had a 30-time speed up in the last week.
564240	568320	So when you mentioned that, just as we were getting ready for this,
568320	573680	I said, so you're basically able to render a virtualized world in real time.
573920	574240	Yes.
574960	580800	And that's one step away from being able to render a movie in real time.
581760	589280	So are we in the verge of basically, and we're sitting here in Los Angeles,
589280	591360	and you've been in the Hollywood world.
591360	596080	I mean, are we talking about rendering motion pictures by just writing,
596080	600080	reading a script and having some stable diffusion,
600080	603040	three-bottled, create a film in real time?
603040	604160	Well, who needs to write a script?
604160	604960	Okay, I did that.
607760	608640	You just say it.
608640	610240	I want something to make me happy, right?
610240	612080	And then it'll pull together various models,
612080	613280	and it can live-generate a movie.
613280	613760	Yeah.
613760	615040	So we're live-generating.
615040	618160	So Hollywood, so in the next five years,
618160	620800	yes, I think that ready player one awaits this world,
620800	623120	minus the microtransactors of 18 ages will be here.
625440	625680	Great.
625680	629280	Anything you can imagine, iterate it, dynamic,
629280	633520	new forms of communication, new affordances,
633520	634880	like all these clicks and things like that.
634880	635680	You don't need that anymore.
637120	639920	Then this is one of the biggest evolutions in humanity ever,
640560	643120	because the easiest way for us to communicate is what we're doing now
643120	644160	and having a nice chat, right?
644160	645280	Back and forth.
645280	646640	Then the Gutenberg press came,
647360	649840	and suddenly you could communicate through written form,
649840	650720	and it's harder.
650720	651440	Yes.
651440	655360	But still, you see now GPT-3 and other of these AIs are made easier now.
655360	657520	For anyone to do anything like chat, GPT just came out.
657520	658080	It's amazing.
658400	660080	Yes, I saw it from open AI.
660080	662000	Everybody's making extraordinary claims
662000	663440	about what it's going to do to Google.
663440	664000	Exactly.
664000	666720	Well, this is a very interesting thing.
667840	669840	But then visual communication is the hardest,
670880	675520	like you toil to be an artist or a lot of people on this call.
675520	677520	How much have we toiled with PowerPoint?
678240	680640	We're going to rid the world of the tyranny of PowerPoint.
680640	683360	But that's my expertise from a PowerPoint expert.
683360	684400	You're not a PowerPoint expert.
684400	685920	You're a communication expert.
685920	689120	So we remove the barriers whereby any visual medium can be created instantly,
689120	691360	and you just describe to it how it shifts.
691360	695200	And because it's got these latencies, it understands happier or sadder,
695200	698320	or you can say flamboyant, and that's not a word,
698320	700000	but it figures out what that kind of means.
700000	702560	But the ability to rapidly iterate it and say,
702560	704000	okay, I don't like that movie scene.
704000	707600	Make it happier, make it brighter, make it more dramatic.
708160	712480	And then being able to render any form of communications,
712560	717200	like you just said, a movie and have a thousand variants,
717200	719360	I mean, what's going to happen to Hollywood?
719920	722240	I imagine Hollywood will be quite disintermediated.
722240	723040	Yes.
723040	724560	Hollywood in everyone's thing.
724560	725840	I mean, what is Hollywood?
725840	729520	Hollywood emerged because it was far enough away from the east coast
729520	730880	that IP laws didn't apply.
732720	737520	And then it got constructed around here as an entity
737520	740880	that extracted rents from performers, ultimately, and creatives.
741520	745200	Like how many creatives like the Hollywood or the music industry or others?
745200	748000	Not many, because they tend to be treated quite badly.
748000	750000	Some people achieve superstardom.
750000	752720	But a lot of this technology now is just truly democratizing
752720	755680	in that things are going from centralized to the edge,
755680	758240	and we had to create centralized organizations as a people
758240	760880	because we didn't have the information classification
760880	762880	and communication tools to be more dynamic.
762880	764640	It's called the representative democracy.
764640	765840	It's not a true democracy.
765840	767440	It's representative democracy, exactly.
767440	770160	At the same time, everyone speaking at the same time
770160	771360	doesn't make sense.
771360	773920	But what we need, like the most successful organizations
773920	775360	are complex hierarchical systems.
776080	779440	It's groups of people working together loosely bound for a bigger story.
779440	780880	And this is what we can achieve as humanity,
780880	783440	like the human colossus when we all come together to do one big thing.
784000	785680	But it gets blocked by a lot of the stuff
785680	787760	because we're not communicating properly.
787760	789840	Like one of the things I tell my team is that, you know,
789840	792160	roadmaps are not about resources, they're about communication.
792800	795040	If you communicate properly and something's good,
795040	796640	you will always get resources for anything.
796720	799200	It's also about creating a common vision
799200	801520	that Erby's aiming towards in order to get there.
802160	804720	So you're not diversified in a thousand directions.
804720	808480	So like Google is full of amazingly smart people, right?
808480	810640	They did an amazing study called Product Aristotle.
810640	810880	Yes.
810880	813360	Because it's like, why is one smart team better than another?
813360	814400	What was the finding?
814400	816160	The finding was it came up with two things.
816160	819840	Common narrative and ideally something that kind of engages you
819840	822720	and there's a bit of sacrifice, you know, like the...
822720	826000	So this is what Salim and I and Salim is male who you know
826000	827440	well who sends his regards.
827440	829520	Talk about it as a massive transformative purpose,
829520	832320	having a unifying, really compelling,
832320	835040	emotionally charged vision that you're heading towards.
835040	837440	Something deep and something that you've put yourself into as well.
837440	840320	So it becomes part of your story as it were.
840320	842240	And the second part was psychological safety.
843200	843840	Which I think was very...
843840	844800	Amongst the team.
844800	846000	Amongst the team.
846000	848080	The ability to actually express yourself
848080	850400	and talk about something without fear of reproach.
850400	852400	That's what makes the most successful teams.
852400	854880	Because we're too scared often about kind of our status
855120	856960	and worrying people and things like that.
856960	859360	That's obviously on the research side of things primarily.
859360	861360	But I think some of these lessons kind of come because
861360	862960	when you feel comfortable as a community
862960	864960	and you're working towards a massive transformative purpose,
865520	866800	you can do a lot more.
866800	868640	Because when you're falling behind, you can communicate it
868640	870400	and you're not scared of people judging you.
870400	873040	And if you have an idea that's considered, you know,
873040	878400	divergent from the center, you feel open to being able to share it.
878400	880320	In fact, it may well be the right idea.
880320	882720	And I talk about the day before something is truly a breakthrough.
882720	883680	It's a crazy idea.
883760	886400	If you're scared about actually putting forward a crazy idea,
886400	887440	then you're stuck.
887440	889120	Well, again, it comes back to information theory,
889120	889920	Shannon style, right?
889920	890640	Yep.
890640	893520	Information is only valuable in as much as it changes the state.
893520	894080	Yes.
894080	895920	Everyone's on the same page with everything.
895920	897440	Flat equals dead.
897440	898000	Exactly.
898000	900160	And this is a time where we basically have to have
900160	901600	exponential progress.
901600	903200	And now we actually have the tools to help us
903200	904960	to make exponential progress.
904960	908320	Like, you know, Facebook released Galactica recently,
908800	912640	which is their language model trained on 42 million science papers.
912640	916000	They made some claims that were a bit hypey and then people
916000	918800	like, oh, you can use this to create racist science papers.
918800	920560	And so they were forced to take it down.
920560	921920	But I mean, it's an amazing piece of tech
921920	923120	and we're going to help re-release it.
924240	925840	And this is an interesting thing.
925840	928640	You can use it to do things like a null hypothesis creator.
928640	931040	You can use this AI to do all the sorts of things
931040	934240	that enable more divergent original thoughts
934240	936480	or creativity or any of these other things.
936480	940160	Like, classically, I couldn't because it was out of mode.
940880	942240	It was out of the data set.
942720	946640	Of course, if it's not in the data set, then you're screwed.
946640	948400	Whereas now you don't need it to be in the data set.
948400	949600	You can create new data.
949600	951120	Yeah, that's extraordinary.
951120	952880	This episode is brought to you by Levels.
952880	956080	One of the most important things that I do to try and maintain
956080	960720	my peak vitality and longevity is to monitor my blood glucose.
960720	963040	More importantly, the foods that I eat
963040	965840	and how they peak the glucose levels in my blood.
965840	968240	Now, glucose is the fuel that powers your brain.
968240	969520	It's really important.
969520	971520	High, prolonged levels of glucose,
971520	974880	which is called hyperglycemia, leads to everything
974880	980000	from heart disease to Alzheimer's to sexual dysfunction to diabetes.
980000	981280	And it's not good.
981280	983120	The challenges, all of us are different.
983120	985680	All of us respond to different foods in different ways.
985680	989760	Like, for me, if I eat bananas, it spikes my blood glucose.
989760	991680	If I eat grapes, it doesn't.
991680	995840	If I eat bread by itself, I get this prolonged spike
995840	997120	in my blood glucose levels.
997120	1000640	But if I dip that bread in olive oil, it blunts it.
1000640	1003200	And these are things that I've learned from wearing
1003200	1006400	a continuous glucose monitor and using the Levels app.
1006400	1011200	So Levels is a company that helps you in analyzing
1011200	1013120	what's going on in your body.
1013120	1015520	It's continuous monitoring 24-7.
1015520	1016880	I wear it all the time.
1016880	1019840	It really helps me to stay on top of the food I eat,
1019840	1022080	remain conscious of the food that I eat,
1022080	1024880	and to understand which foods affect me
1024880	1027840	based upon my physiology and my genetics.
1027840	1030960	You know, on this podcast, I only recommend products
1030960	1034080	and services that I use, that I use not only for myself,
1034080	1035760	but my friends and my family,
1035760	1038320	that I think are high quality and safe
1038320	1040720	and really impact a person's life.
1040720	1041920	So, check it out.
1041920	1044800	Levels.link slash Peter.
1044800	1047520	We give you two additional months of membership.
1047520	1050640	And it's something that I think everyone should be doing.
1050640	1052880	Eventually, this stuff is going to be in your body,
1052880	1056400	on your body, part of our future of medicine today.
1056400	1059120	It's a product that I think I'm going to be using
1059120	1061600	for the years ahead and hope you'll consider as well.
1061600	1064640	You know, when you go to your website,
1064640	1069600	I love the notion of AI by the people, for the people.
1070160	1073360	And then you say, stability AI is building open AI tools
1073360	1075680	that will let us reach our potential.
1076400	1077200	Let's talk about that.
1077200	1080160	Because, you know, there's a lot of individuals
1080160	1084240	that you and I both know that our fearmonger is around AI.
1084240	1085280	You know, it's the devil.
1085280	1086400	It's going to destroy us.
1086400	1087280	It's going to...
1087280	1090400	Superhuman AI is the end of humanity as we know it.
1090400	1094240	And I mean, my position is AI is the single most important tool
1094240	1096720	we're ever creating to solve the world's biggest problems.
1096720	1099200	And we can't solve our problems from where we were before,
1099200	1101120	but these are the tools that are going to allow us.
1101120	1102080	How do you...
1102080	1104960	So, how do you address the Bill Gates,
1104960	1108400	the Elon to the world on that side, the fear side?
1108400	1109440	Well, I think it fares a valid,
1109440	1111120	because it's the most powerful technology
1111120	1112960	we've ever created and it comes from us.
1113520	1114960	But then who is us, right?
1114960	1116320	Like, if you look our current data sets,
1116320	1117760	they're massively biased.
1117760	1118960	They're fixed towards the internet
1118960	1120240	and they're fixed towards manipulation.
1120880	1123760	The way I kind of look at AI is that organizations themselves are AI.
1124320	1127680	They are slow dumb AI that feeds on us and turns us into cogs.
1127680	1129520	In fact, this is concept of Molok, right?
1129520	1132320	From the Ginsburg poem, you know, how?
1132320	1134480	Talking about this Carthaginian demon
1134480	1136720	that pervades our organizational structures
1136720	1139760	and turns us into these cogs that feeds on us effectively.
1139760	1142000	I think this is the first thing that can actually defeat that.
1142640	1144480	This particular technology that we have today.
1145200	1146480	Because is the world happy now?
1147200	1149040	How many people in organizations are happy?
1149040	1150160	We all know, you know?
1150720	1153040	We should talk about my idea for a happiness X-Prize,
1153040	1154080	but that's a different conversation.
1154080	1155520	Happiness X-Prize, but what is happiness?
1155520	1156400	Happiness is agency.
1156400	1158000	Happiness is achieving your potential.
1158000	1160000	And it's kind of going out there, so we need some help.
1160000	1161520	But I think a lot of the AI discourse
1161520	1163920	has been focused on gigantic language models
1163920	1166080	and fricking supercomputers, which we still have,
1166720	1170560	to create something that will equal and surpass us.
1170560	1172320	It's very religious in its way.
1172320	1174240	And you see parallels to religion across this.
1174240	1174640	Sure.
1174640	1177360	In that you have the kind of declaiming people who like for it,
1177360	1179200	the people who call it heretical.
1179200	1182080	And then you kind of even have most of ethics in AI.
1182080	1183360	It's not actually ethics.
1184000	1187120	Instead, it's ultra-orthodoxy, as it were.
1187680	1188800	Like in Islamic terms,
1188800	1190640	everything is haram unless it's declared halal.
1191760	1193360	You know, classic Jewish,
1193360	1194720	ultra-orthodox Judaism, etc.
1194720	1195760	It's all the same thing.
1195760	1197120	People look at red teaming.
1197120	1199440	They don't look at green teaming, right?
1199440	1202160	And they look at this technology as being too powerful.
1202160	1204400	So just like cryptography, we should keep it from the people.
1204400	1206640	And we definitely shouldn't give it to emerging markets
1206640	1208160	and people who aren't smart enough to do it.
1209040	1211360	And that's very interesting because, again,
1211360	1212800	if you think about the power of it
1212800	1213920	and you believe it's powerful,
1214640	1216640	then the question should be, what is this?
1216640	1218720	And I believe that this AI is infrastructure.
1219680	1223600	Clayton Christensen, kind of the departed mental mind,
1223600	1225840	had an amazing quote, which is that,
1225840	1227600	infrastructure is the most efficient means
1227600	1229840	by which society stores and distributes value.
1230880	1232640	Now, obviously, that's ports and things like that.
1232640	1233040	Sure.
1233040	1234400	But it's also information.
1234400	1235040	Sure.
1235040	1236240	And I think that's valuable when you
1236240	1238000	confine it with the Shannon theory.
1238000	1240000	So this AI is infrastructure
1240000	1241600	for the next generation of human thought.
1242160	1243040	And what does that mean?
1243040	1244400	It should mean that it should be a commons
1245120	1246720	that is accessible to everyone.
1247280	1252320	And you made a very definitive decision
1252320	1255520	to make this an open-source movement here.
1255520	1255760	Yeah.
1255760	1256880	Yeah.
1256880	1258640	Your community's how large now?
1258640	1261280	I think we've got about 120,000.
1261280	1261840	Amazing.
1261840	1262640	In the communities.
1262640	1264720	And we created communities across verticals.
1264720	1267120	Classical open-source product related,
1267120	1269200	where you could have MongoDB or something like that,
1269200	1270080	and then you've got there.
1270080	1274720	Whereas we said language, healthcare, BioML, audio.
1274720	1276400	Let's get all the people who are fantastic
1276400	1277600	in the private sector, public sector,
1277600	1279200	actively independent together.
1279200	1280560	And let's jam on,
1280560	1282640	how do we build a next generation infrastructure?
1282640	1284400	The way I put it is, let's go to the future
1284400	1285520	and bring back AI with us.
1286160	1286400	Nice.
1286400	1287920	And we can choose if it's a panopticon
1287920	1290160	controlled by corporations, like Web2,
1290160	1291520	or we can choose if it's open,
1291520	1293120	an infrastructure for humanity.
1293120	1294160	And if it's infrastructure for humanity,
1294160	1295440	I think it's an important point,
1295440	1296400	because I'm just finally saying that.
1297040	1299520	A lot of the naysayers around AGI and ASI and others,
1300800	1303280	I would agree with them if it's controlled
1303280	1304320	by corporations,
1304320	1306000	which are this type of weird entity
1306000	1306800	that we've created.
1307440	1309840	Like YouTube optimized for extremism,
1309840	1310880	because it was engaging,
1310880	1312880	so ISIS co-opted those algorithms.
1312880	1314400	And they adjusted eventually, but it was slow.
1315040	1316640	If it comes from us and it's biased,
1316640	1317760	and we've brought in the conversation,
1317760	1319280	I think AI is more finalist,
1319280	1320400	likely to kill us all,
1320400	1321760	if it ever becomes sentient,
1321760	1323200	which is a big question, you know?
1324320	1325920	Especially if it's representative.
1325920	1329680	So I want to come back to this in greater detail later,
1329680	1332560	but I think we share a common belief
1332560	1334880	that humanity ultimately is good.
1335440	1336240	Yes.
1336240	1337440	At its fundamental level,
1337440	1339760	because that's a very important distinction.
1339760	1342000	If you believe that humans,
1342000	1343440	at their base, are good,
1343440	1345120	and you're enabling humans
1345120	1346960	with more and more powerful technology,
1346960	1348080	they're going to be using that
1348080	1349360	to make the world a better place
1349360	1352080	and solve problems on the whole.
1352080	1353680	On the whole, but it depends on
1353680	1355600	what the perspective of humanity is.
1355600	1357280	You know, you've kind of got,
1357280	1358640	what's it called, the thing when you go to space
1358640	1359440	and you look at Earth?
1359440	1360400	Yeah, the overview effect.
1360400	1361280	The overview effect.
1361280	1361680	Yeah.
1361680	1363120	You know, like hopefully more and more people
1363120	1364400	get to space and have that,
1364400	1365920	because people are very narrow,
1365920	1367840	and they view themselves like that.
1368480	1370640	They're, Rabbi Sacks,
1370720	1371760	former Chief Rabbi of the UK,
1371760	1372960	had a very wonderful concept
1372960	1374000	called altruistic evil.
1375120	1376160	Those who actually do evil
1376160	1376960	believe they're doing good.
1377680	1378160	Interesting.
1378800	1379600	And you see that.
1379600	1381120	Like, you know, if you talk to them.
1381120	1382800	Especially in the religious realm.
1382800	1383680	Religious or anything like that.
1383680	1385920	I'm like, Isaiah Berlin's conceptualization,
1387040	1389920	British philosopher from like the 1940s,
1389920	1392320	he had a conceptualization of positive liberty
1392320	1393600	versus negative liberty.
1393600	1395040	So negative liberty was the freedom
1395040	1396480	from anyone telling you what to do,
1396480	1397440	and then to kind of,
1397440	1399440	laissez-faire capitalism and things like that.
1399440	1402000	Positive liberty was the ability to believe in an ism.
1402000	1404240	Something big, like fascism,
1404240	1407600	communism, capitalism, Islamism, etc.
1407600	1409120	And people use that as excuses,
1409120	1410800	being they were doing good to actually do bad.
1411360	1412800	But then how does this all relate to this?
1412800	1414560	I believe people inherently want to do good.
1414560	1416080	It's just that what good is,
1416720	1418480	can become misdefined and corrupted.
1419200	1420320	And so my take was,
1420320	1421760	if we start building infrastructure,
1421760	1423520	where people can see bigger perspectives,
1423520	1425200	because they get the information they need,
1425200	1425920	what does that look like?
1425920	1428480	What if we mapped all the religious texts in the world,
1428480	1431200	so that a child in Egypt could see Judaism
1431200	1432960	from the perspective of a child in Jerusalem?
1434080	1435120	We have the technology to do that.
1435120	1436640	What if you could automatically translate
1437680	1441200	tea-party republicanism into libertarianism,
1441200	1442160	and have commonality there?
1442160	1443920	Again, we finally have the technology to do that.
1444480	1445520	But we didn't before,
1445520	1447520	so people remain in their huddles,
1447520	1448640	they look at the other,
1448640	1449680	and they're encouraged to do so.
1449680	1452000	We've seen that increase in political polarization.
1452000	1453200	So society is a tipping point.
1453200	1454320	And by the way, we've seen,
1454320	1456160	I mean, that's been what social media
1456160	1458240	has effectively done so efficiently.
1458480	1461040	Because it's beneficial for the algorithms,
1461040	1464160	and for the slow, dumb AIs of the corporations that drive it.
1465120	1468240	Let's come back to the company and the products,
1468240	1470880	just to lay out the tapestry here.
1471920	1475920	Stable diffusion is one product vertical area.
1475920	1477040	What are the other ones?
1477040	1481840	So for example, our Luther AI community has GPT Neo and X.
1481840	1484560	It's the open source version of GPT-3 by OpenAI.
1485280	1487520	The most popular open language model in the world,
1487600	1489760	it's been downloaded 25 million times.
1489760	1490640	Incredible.
1490640	1491680	And so can you take it?
1491680	1493120	You customize it for yourself?
1493120	1494240	No permission needed, right?
1495040	1496720	Harmony has danced diffusion,
1496720	1498960	which is the most advanced audio model in the world.
1499600	1500800	You'll be able to create your music,
1500800	1501840	so you put your own music in it,
1501840	1502960	and then you have your own music model
1502960	1504560	that can create more music of your style.
1504560	1508080	And you can just basically produce it,
1508800	1510000	produce your own concerts,
1510560	1512400	extrapolate in any direction you want.
1512400	1513280	Exactly.
1513280	1514480	We have OpenBioML,
1514480	1516640	where we kind of have OpenFold and LibreFold,
1516640	1520640	protein folding, DNA diffusion for DNA protein matching,
1520640	1525680	and OpenBioML for, again, some protein kind of stuff.
1525680	1529520	So we've spent some time talking about
1530400	1532960	a passion that I have and you share,
1532960	1536560	which is the ability to truly transform
1536560	1539280	the healthcare industry and human longevity,
1539280	1543520	understanding why we age, maybe why we don't have to.
1543520	1546080	I think the why is the important part, right?
1546080	1547200	Most of medicine.
1547200	1549360	So again, when my son was diagnosed
1549360	1551360	and scratching a wall to his fingernails bled,
1552320	1553680	eventually went to mainstream school,
1553680	1555040	thanks to the interventions.
1555040	1556480	And I want to come back to those,
1556480	1557440	for those who are listening,
1557440	1561840	who have a child with autism or know someone,
1561840	1563520	I want to come back afterwards.
1563520	1566320	What were your learnings and what's your advice?
1566320	1567680	Yeah, there's a lot in this space
1567680	1569200	that we kind of occupy, of course.
1569920	1573440	But what I realized is that conventional medicine
1574400	1577040	and a lot of things view humans as a girdic.
1577040	1578720	A thousand tosses of a coin is the same
1578720	1580640	as a thousand toins cost at once.
1582320	1583360	Coins tossed at once.
1584400	1585600	But humans are individual.
1585600	1587520	So for example, a good proportion of humans
1587520	1590160	have a cytochrome P450 mutation in their liver,
1590160	1593440	which means they metabolize things like fentanyl
1593440	1594880	and opioids far quicker.
1594880	1595360	Yeah.
1595360	1597760	It's a very simple SMP test, but we don't do it.
1597760	1599360	So we just prescribe everyone the same thing
1599360	1600480	and then a bunch get addicted.
1601440	1602080	You know?
1602080	1604640	Because we are individualistic kind of creatures.
1604640	1606160	This is why I went to the first principles
1606160	1607760	thinking kind of approach on this.
1607760	1609600	And the question is this personalized medicine thing
1609600	1611440	has always been kind of out there.
1611440	1612800	We've not been able to reach it.
1612800	1614720	Again, I think the technology we have right now
1614720	1616320	enables personalized medicine.
1616320	1618880	We've seen things like CRISPR, obviously, and others.
1619680	1622320	But more than that, it's about data availability
1622320	1623200	and viability.
1623200	1624320	But we do need to get to a point
1624320	1626560	where every child at birth is sequenced
1626560	1629440	and that is plugged into their personal AI
1629440	1631760	and they understand exactly how every food, every medicine,
1632400	1636240	and every aspect of our living affects our physiology.
1636240	1639600	Again, you go to the future and you bring back the AI with you
1639600	1641680	and you say, what should it look like?
1641680	1643520	So you take a whole country and you say,
1643520	1645440	how do we build an amazing health care system,
1645440	1646720	education system, et cetera?
1647600	1650880	Is there any doubt that health care and education
1650880	1653440	will have AI at the core in 20 years?
1653440	1653840	Zero.
1653840	1656320	Well, I think it's not 20 years, it's 10 years.
1656320	1656640	I know.
1656640	1657680	But let's just say 20 years.
1657680	1658720	Fine.
1658800	1660800	You and me, we're like, now, now, now.
1660800	1661920	Let's say 20 years, right?
1662960	1666080	But then is that AI open or closed?
1667120	1667520	Yeah.
1667520	1669600	And is it better for it to be open or closed?
1669600	1670400	There's no question.
1670400	1673120	Open is fundamentally critical.
1673120	1674320	Open is fundamentally critical
1674320	1676320	because then we build, it is infrastructure,
1676320	1677520	it is valuable.
1677520	1679520	Like the way that I actually orient my rights,
1680240	1682560	Vinay Gupta, I was one of the Ethereum guys,
1682560	1684000	I think you probably know him.
1684000	1685600	Big thinker, like crazy.
1686320	1688320	He had a very great conceptualization of rights
1688320	1690720	which I agree with, which is the rights of children.
1691600	1694480	And so like effective altruism and all of that,
1694480	1696000	looking at people a million years in advance,
1696000	1697200	it's kind of difficult.
1697200	1698640	And it comes down to utilitarianism,
1698640	1699520	all sorts of weird stuff.
1700080	1701440	But the rights of a child.
1701440	1702560	Rights of a child today.
1702560	1702880	Yes.
1703440	1704000	Today.
1704000	1705440	What does that child have the right to?
1705440	1706640	Achieving their potential.
1706640	1707120	Yes.
1707120	1709040	What infrastructure do we need to give that child
1709040	1713120	in a refugee camp or in Brooklyn or in Kensington
1713120	1714640	to help them achieve their potential?
1714640	1716400	And this goes back to what's on your website
1716400	1721280	in terms of using AI to help, in this case, a child,
1721280	1724720	and in the broader case, humanity, achieve its potential.
1724720	1725200	Achieve it.
1725200	1726880	I mean, that's everything.
1726880	1727280	Yeah.
1727280	1728480	And so for me, it's about...
1729440	1731600	I've oriented that on agency and happiness.
1732240	1734640	And it sounds fuzzy, but it's literally,
1734640	1737040	when you have the tools to be able to do anything,
1737040	1738400	you know you can do anything.
1738400	1740000	People underestimate their agency.
1740640	1742720	Sometimes you've got to go for your shot, as it were.
1742720	1743360	Yeah.
1743360	1744400	But again, it's information.
1744400	1746000	What information can I bring to that person?
1746000	1749120	What tools can I give them so that they can be creative?
1749120	1751600	Because we lose that creative spark as we get older, right?
1751600	1751920	Yeah.
1751920	1753360	Now it's coming back of it, you know?
1753360	1755520	What can I give them so they can access the information
1755520	1757040	they need to be educated?
1757040	1759440	And what's the optimal way to teach linear algebra?
1759440	1761120	And by the way, it's all about play.
1761120	1762160	It's all about play.
1762160	1763600	It is all about play.
1763600	1767040	Happiness play, all the neurochemistry of your brain
1767040	1770000	is maximized for learning, retention, experimentation
1770000	1770720	around that.
1770720	1771440	It's flow.
1771440	1772160	It's flow.
1772160	1774000	So I used to be a video game investor.
1774000	1774960	That was my big sector.
1774960	1776080	It was one of the biggest in the world.
1776080	1778720	And I used to judge video games by time to fun, flow,
1778720	1779360	and frustration.
1780960	1782880	And so if we're building systems for humanity,
1782880	1785120	we have to look at fun, flow, and frustration.
1785920	1788480	Because if we can get those, you know when you're just
1788480	1790320	learning something, like, wow, this is amazing,
1790320	1791120	and how do you get in there?
1791840	1793840	It absorbs amazingly quickly.
1793840	1795680	But our education system is not set up for that.
1795680	1797040	Our health care system definitely isn't.
1797040	1798400	And you've got to mess up health care system.
1798400	1798720	Yeah, I'd say.
1798720	1799280	Oh, my gosh.
1799280	1801280	It's maximizing for frustration.
1801280	1803120	Yeah, that's what I speak about it.
1803200	1806480	Openly saying my mission is to crumble and destroy
1806480	1807600	the health care system.
1807600	1811120	And also education system, which is really sad.
1811120	1812320	So again, it's very interesting.
1812320	1814240	You look at the US in inflation numbers.
1814880	1817040	Education and health care, massive inflation.
1817040	1817760	Yes.
1817760	1819200	Everything else, not really.
1819200	1821760	Yeah, and it's the percentage of the person's income.
1823280	1826000	And it should be going to zero, right?
1826000	1828080	The top health care and the top education
1828080	1829360	should be all AI driven.
1829360	1831760	It's all basically the cost of electrons.
1831760	1833920	Life expectancy is falling in the US.
1833920	1836160	A little bit over the last couple of years due to COVID.
1836160	1836480	Yeah.
1836480	1837680	Yeah, but actually, no.
1837680	1838880	No, before COVID.
1838880	1840480	Before COVID, it was falling.
1840480	1840560	Yeah.
1840560	1841120	I mean, that's the thing.
1841120	1844400	Whereas, again, it's not complicated,
1844400	1846320	but it does require coordination.
1846320	1848480	So the question is, can you create shelling points?
1848480	1852320	So for me, open source software, this next generation,
1852320	1854240	this model based one,
1854240	1856720	whereby stable diffusion is basically a programming primitive.
1857440	1859600	Just like you have a library to do various things.
1859600	1861120	So the program is out there.
1861120	1863280	So in the good old days, when we started programming,
1863280	1864960	we used to have to code everything by hand.
1864960	1866240	Now, you've got GitHub and things like that.
1866240	1867600	It's more like playing with Lego, right?
1867600	1868000	Yes.
1868000	1869360	And you're sticking it all together.
1869360	1872480	It's a new type of thing, a 1.6 GB file that is hashed.
1873040	1875600	So it can be common across every single computer in the world
1876480	1879440	that you can call something in and an image comes out.
1880080	1882000	And you know predictably what that is,
1882000	1882800	no matter where you are.
1882800	1885520	And you can take an image and put text on the other way.
1886640	1886960	Yeah.
1886960	1888240	That changes the paradigm.
1888240	1890320	But then what if you have that for language, audio,
1890320	1891440	all these different modalities?
1891440	1891760	Okay.
1891760	1894160	I'm going to go here next then on that,
1894160	1898000	which is what happens to electoral property rights, IP rights?
1898000	1901840	Who owns the IP of those images?
1901840	1904480	Is it the person who puts the prompt in?
1905520	1909680	Is it, you know, help me understand where that evolves to?
1909680	1910640	Nobody knows.
1910640	1911200	Nobody knows.
1911200	1911360	Okay.
1911360	1912240	It's a fair answer.
1912240	1912400	Yeah.
1912400	1913040	I mean, nobody knows.
1913040	1916560	I personally think it should belong to the person that prompts it.
1916560	1919520	Because again, we put this out as a commons for humanity.
1920320	1923280	Like we did put an ethical use license around it for various reasons
1923280	1925840	that will be replaced by a purely open source license.
1926560	1929360	But again, I'm viewing this as building blocks, right?
1929360	1931280	It's really the person that has the action
1931280	1933120	because these models do not have agency.
1933760	1935360	If you're going to breach property right,
1935360	1937760	you can type in, I want a picture of Mickey Mouse.
1937760	1939520	And then if you sell that, done.
1939520	1940560	But it's like photos.
1940560	1941440	It's like telephone.
1941440	1942240	It's like internet.
1942240	1943200	It's like Photoshop.
1943920	1946320	This is a tool, but it's a tool of a very different type.
1946960	1950080	Because like, something like Photoshop is a million lines of code.
1951280	1954160	This is a binary file that can do anything that Photoshop does.
1954160	1955760	But you still need to act on it.
1955760	1960080	Last year, there was a patent awarded to an AI in South Africa,
1960080	1960800	which was interesting.
1960800	1962400	I think more of a gimmick than anything else.
1963200	1967440	Do you imagine these models will lead
1967440	1970400	towards significant intellectual property
1970400	1972080	on the invention side of the equation?
1972080	1972640	Oh, 100%.
1972640	1973280	They already are.
1973280	1974800	If you look at Google's new TPU chips,
1974800	1976640	they're partially built by AI that they built.
1976640	1976960	Sure.
1976960	1977360	Yes.
1977360	1977840	I know.
1977840	1983760	Like we just released OpenLM from our Carpalab,
1983760	1986960	which is an evolutionary algorithm for code for robots.
1988160	1990240	So we're trying to optimize robotics via that.
1990240	1992560	And then when we bring in video, we'll have even better robots.
1993120	1995200	So are we going to get to a place where I'm saying,
1995200	1999120	listen, please invent a device that does this for me,
1999120	2000720	that's under this price point,
2000720	2002720	that's made of commonly available materials
2003280	2006000	and constrain it left, right, and center,
2006000	2008880	and then design it, now go and print it for me
2008880	2010160	and deliver it for me the next day.
2010160	2013200	We're going from mine to materialization in one sense.
2013200	2013760	Yes.
2013760	2014400	Yeah.
2014400	2016320	I mean, again, it's kind of the,
2016320	2017600	was it the Star Trek thing?
2017600	2019440	Yeah, it's a little bit longer.
2019440	2022400	Yeah, it's the, oh my God.
2022400	2023200	We've both forgotten it.
2023200	2023600	Yeah.
2023600	2025600	Anyway, you know, when Joe Geordi gets the replicator.
2025600	2026960	The replicator, exactly.
2026960	2028880	They're all great to your heart.
2028880	2029600	Exactly.
2029600	2031040	But then you can kind of see this already,
2031040	2032960	because there's an app on the App Store right now.
2033600	2034640	You pull your Lego out.
2035600	2036880	And it scans all the Lego.
2036880	2037440	Oh, nice.
2037440	2038560	What can you build with it?
2038560	2039040	Yes.
2039040	2041120	It comes up with, this is how you build it as well.
2042080	2044720	So things like you can build birds and cars and things like that.
2044720	2045200	Awesome.
2045200	2046000	So you go, da-da-da.
2047360	2048480	So this is just an extension of that.
2048480	2050880	Again, they use a transformer based architecture for that.
2050880	2053680	Well, I'm interested when it'd be fun to say,
2053680	2055360	okay, create new life forms.
2056080	2057840	I mean, we're not far from that.
2057840	2060000	Well, you know, I'm going to leave that to other bioethicists.
2060000	2061280	I don't want to get stuck in that one.
2061280	2065520	Wait, wait, there's ethics involved in that?
2065520	2066880	Okay, exactly.
2066880	2069040	I mean, the life form thing is very interesting though, right?
2069040	2072000	Because again, everything is happening all at once.
2072800	2077520	And so it's not just biology or physics or sociology.
2077520	2079840	All these models, all these technologies,
2079840	2081600	all seem to be converging at once.
2081600	2083120	So you can create anything.
2083120	2084960	And all the barriers are dropping at once.
2084960	2086160	And that's complicated.
2086160	2086640	Yeah.
2086640	2091760	So the world is, do you think anybody truly understands
2091760	2094240	how fast the world is about to change?
2094800	2095120	No.
2095120	2096720	I mean, like, look at creative industry.
2096720	2099120	Videogames, $180 billion a year,
2099120	2103760	like Disney spends $10 billion a year, Amazon $16 billion a year on content.
2104400	2106880	All of that's going to change in the next couple of years alone,
2106880	2109760	just from one tiny little two gigabyte file.
2109760	2110720	Extraordinary.
2110720	2113200	As well as healthcare, as well as education,
2113200	2115120	as well as you said, every single industry,
2115120	2118240	we're going to, so here's another question.
2118240	2118960	Go ahead, please.
2118960	2122160	Well, I was going to say industry is about, again, information theory.
2122160	2122560	Yeah.
2122560	2124400	That's, most industries are based on that,
2124400	2125760	especially service-based industries.
2126400	2129600	And so once you can basically take a system,
2129600	2132080	you can have human input in the loop to train a system
2132080	2133920	that's a generalist, to understand principles.
2134640	2136240	You disrupt just about everything.
2136240	2136800	Yeah.
2136800	2140480	I'm going to pause on that because it's probably one of the most important things.
2140480	2144080	Any entrepreneur, any CEO, any parent, any kid,
2144080	2151120	anyone needs to understand, we're about to enter a period of hyper-disruption
2151120	2153520	and growth and hyper-opportunity creation.
2153520	2154000	Yes.
2154000	2157760	Like, what happens is that in any area you create,
2157760	2158800	there are value spikes.
2158800	2160480	So you can look at it as like a flat area,
2160480	2163520	and then companies and individuals occupy certain areas.
2163520	2165440	So they've got like a mix of skills, right?
2165440	2166640	And that's how you earn your living.
2167280	2169440	And these are like spikes.
2169440	2170640	That's all going to get shaken up,
2170640	2172320	and it's going to be the value is elsewhere.
2172880	2173040	Yeah.
2173040	2174400	We don't know where the value will be.
2174400	2175120	We've got some guesses.
2175120	2177600	So like in a time when everyone can make anything,
2177600	2178880	what becomes valuable?
2178880	2179360	Something.
2179920	2180160	Yes.
2180160	2181600	So if a model that can make anything,
2181600	2183360	that means Disney should have their own models, right?
2183920	2186160	To create Mickey Mouse's and things like that.
2186160	2188080	But now they can use that not only internally
2188080	2190640	to save money on creation, they can use externally.
2190640	2193600	Why can't we have Mickey Mouse having coffee
2193600	2196960	with Master Chief at a Starbucks and microtransact pay that?
2196960	2198880	It's been the promise of the NFTs and things like that.
2198880	2198960	Yes.
2198960	2201120	Now with these models, you can do that seamlessly.
2201120	2202720	Yeah, on demand.
2203040	2204000	Over and over again.
2204000	2206640	So one of the realizations for me is that
2207440	2211440	this open source technology made available to everyone
2212320	2218160	effectively has the potential to make everyone the equivalent
2218160	2221120	of millionaires, billionaires, trillionaires
2221120	2224240	when whatever you want can be manifested, right?
2224240	2226800	You can have the world's best education for your child,
2226800	2228560	independent of where you live and what wealth you have.
2228560	2231600	You can have the best healthcare available.
2231600	2233840	You can have customized entertainment.
2235840	2239360	It's, we end up in a world in which the cost of anything
2239360	2244320	is raw materials in IP, which is going to be disrupted itself,
2245360	2246640	and electricity.
2246640	2247680	Yeah, pretty much.
2247680	2251840	Again, you look at what Elon Musk did with SpaceX, right?
2251840	2252240	Yeah.
2252240	2255280	He's like, let's break it down to what is the constituent cost of this?
2255280	2255760	Yeah.
2255760	2258000	And basically with the first principle of thinking on rockets.
2258000	2258720	First principle of thinking.
2258720	2260880	So what I've been doing is first principle of thinking
2260960	2264640	on information flow, social theory, and our systems.
2264640	2266720	It all comes down to just information being in the right place
2266720	2268640	at the right time to make the maximum impact.
2268640	2270800	And if we can give that as an open architect to the world,
2271360	2274880	then we can augment and then replace our existing systems
2274880	2277040	with better ones because they outcompete.
2277680	2280960	You go to an African nation and you teach every child
2280960	2283120	with an AI that teaches them and learns from them.
2283120	2286480	Within a few years, they will be out competing children
2286480	2288240	in the top schools in New York.
2288240	2290480	What if you give them the ability to code the system as well
2290480	2291040	and improve it?
2292000	2293120	It becomes very interesting.
2293120	2296720	The kids are helping train the AI and the AI is helping train the kids.
2296720	2300400	Yes, but then also the kids can improve the actual code
2300400	2301200	that they have there.
2301920	2303600	This is something that we've seen in our things.
2303600	2304560	A virtuous cycle.
2304560	2305440	A virtuous cycle.
2305440	2307760	And you make that open and then you make it transplantable
2307760	2311360	because what you have then is you have models that are standard,
2311360	2312400	like a base.
2312400	2315840	So I like to call Stable Diffusion 1 was a precocious kindergartner.
2315840	2318320	Then it was a precocious high school, Stable Diffusion 2.
2318320	2320560	Stable Diffusion 3 looks freaking amazing.
2320560	2322400	It's going to be like a university level student.
2322400	2325920	So Stable Diffusion 3, by the way, is the real-time rendering?
2325920	2326400	No.
2326400	2327520	That's Stable Diffusion 2.
2327520	2328080	That's 2.
2328080	2328560	Okay.
2328560	2329360	Stable Diffusion 3.
2329360	2331680	So again, to give you an example of the thing,
2331680	2335360	when we launched Stable Diffusion 1 on a top-end A100,
2335360	2339120	which is like a super-souped-up, super-computer chip, right?
2339120	2339760	We have a lot of those.
2339760	2341440	We can talk about the supercomputers in a bit.
2341440	2344880	It took 5.6 seconds on August the 23rd when we launched it.
2344880	2346000	Do you render a single image?
2346080	2348400	We render a single image in 5.1.2 by 5.1.2.
2348400	2348640	Okay.
2349520	2353200	Today, it takes 0.9 seconds in 7.6.8 by 7.6.8.
2354400	2355680	We've just sped it up 30 times.
2356240	2356640	Amazing.
2357440	2358160	30 times.
2358160	2362640	And I'm still blown away by the real-time rendering of the world.
2362640	2366720	Once you get below 200 to 220 milliseconds of response time,
2366720	2369120	it opens up entirely new URUX.
2369120	2371360	And we didn't just release Stable Diffusion as an image creator.
2371360	2373680	We also released an in-painting model.
2373680	2377920	So you could take, you know, Emma's hat and you could turn it red just by describing it.
2377920	2380080	We released a depth of image model so you could do transformations.
2380080	2381440	We released an upscaler.
2381440	2385360	So you can have a 64x64 image and the AI fills in all the details to take it
2385360	2388800	to 1.024x1.024, transforms the storage industry from media.
2389360	2389840	Amazing.
2389840	2391120	And this is real-time now as well.
2391840	2394640	So listen, I have to ask you the question, but come back to it later,
2394640	2396880	which is do you believe we're living in a simulation?
2398880	2399920	Yes or no?
2399920	2400320	Yes.
2400320	2401440	Yeah, as do I.
2401440	2403280	I think we're living in an nth generation.
2403760	2405520	Simulation, but that's a different story.
2405520	2407760	Okay, we'll come back to that for folks who are interested.
2407760	2418400	But okay, how far out are you able to imagine this world that you're creating the disruption
2418400	2420080	of industries, the transformations?
2422400	2423680	How many years out?
2423680	2424560	I don't know anymore.
2425520	2430240	Like when I started, so I funded the entire open source art space from January of last year
2430240	2433600	when it started because you had a generator model and then I released this clip
2433600	2435120	model which took images to text.
2435120	2437040	We advanced things back and forth for each other.
2437040	2438480	We're like, wow, that's the way to do things.
2439040	2441040	Creators and discriminators as it works.
2441040	2441920	And it's advanced, advanced.
2441920	2443280	I was like, this is the next big thing.
2443280	2445760	Humans can now communicate visually, right?
2445760	2447440	It's the next big thing since we're going to make press.
2447440	2451600	And it kind of went as expected, plus minus six months.
2451600	2455360	I thought we'd get to stable diffusion in Q1, Q2 of next year.
2455360	2456400	And that was a big massive bit.
2456400	2458560	I built a gigantic supercomputer and everything.
2458560	2460800	To get to real time, I think it would take another year or two.
2461760	2463760	And instead, it took like four weeks.
2464720	2466960	And again, this is what you mentioned earlier.
2466960	2470720	The number of GitHub stars now for stable diffusion is more than Ethereum and Bitcoin
2470720	2471840	and just about everything else.
2472800	2475200	And that took them 10 years in three months.
2476320	2481840	Again, it's the notion that people have no idea how fast the world is changing and is accelerating.
2481840	2485760	And it's what you came back to with the common mission and the energy.
2486640	2490560	When you sent your first Bitcoin, it was an amazing experience.
2490560	2495120	They got overtaken by raccoons and divinity and they tried to create an alternative system
2495120	2496080	outside the existing system.
2496080	2498720	The interfaces were all the robbery and all the profits were made.
2499440	2503760	This is something different whereby when we talk to developers and the contributors
2503760	2506880	that are increasing in the ecosystem, they're so energized.
2507520	2509200	And this is what drives things forward.
2509200	2512960	Like when you see teams that do the biggest things, they have the energy.
2512960	2514000	It's almost palpable, right?
2514640	2516160	Where it's like that driven thing.
2516160	2518320	But we've got people from all over the world.
2518880	2522720	One of our latest developers was an Amazon warehouse worker at the start of this year
2522720	2524000	who taught himself to code.
2524000	2525840	And now he's building the most advanced models in the world.
2525840	2527520	He's got 16-year-olds and 62-year-olds.
2527520	2529840	And it's a team of how big?
2529840	2531520	So our team is 137.
2531520	2533760	But the developers who are creating...
2533760	2534560	500.
2534560	2536560	Well, but they're teams of one, right?
2536560	2539280	Their team and their individuals are able to use this to create...
2539280	2539600	Oh yeah.
2539600	2542240	So like if you want to create with this, you can just do it by yourself.
2542240	2545520	So there's a fantastic Twitter, you can look at levels.io.
2546160	2550480	And he's like, I'm going to create businesses by myself that are making millions of dollars
2550480	2553600	of things just because you could took this primitive and he wrapped things around it.
2553600	2555440	And you're just making money.
2555440	2558640	Like Avatar me, you can put your own face into the model.
2558640	2561920	10 images, you can create a Peter Diamandis model and we can put you in space.
2561920	2563040	In fact, we'll do that a bit later.
2563680	2566000	So listen, you're listening.
2566000	2567440	You're a 20-year-old entrepreneur.
2567440	2572640	You're in college, you're finished, you're skipped college, whatever it is.
2573600	2577120	What's your advice to that 20-year-old listening right now?
2577120	2578000	You should drop everything.
2578000	2580080	You should focus entirely on this.
2580080	2581440	This is the biggest shift ever.
2582320	2584560	Self-driving cars, $100 billion went into.
2584560	2586640	Crypto, hundreds of billions of dollars went into.
2587440	2590320	$100 billion, a trillion dollars going to go into this sector.
2590320	2591120	Say that again, how much?
2591120	2593920	$100 billion in the next few years and then a trillion dollars will go into this sector
2593920	2596560	because it's so transformative and so few people understand it.
2597120	2601200	There's never been something a technological advance that will diffuse
2601920	2602960	as fast as this.
2602960	2609760	So this is electricity, this is the Gutenberg press times a billion.
2609760	2613760	This is the Gutenberg press times a billion because it's not just writing.
2613760	2615280	It's image, 3D.
2615280	2615760	It's everything.
2615760	2616240	It's creation.
2616240	2617120	Protein folding.
2617120	2620960	It's creativity and extrapolation.
2622080	2624000	So what does it look like to focus on this?
2624000	2626960	So again, you say drop everything in focus and I get that.
2627840	2631840	And part of me is like, maybe I should do that too.
2633280	2634400	But what does it look like to focus?
2634400	2635280	What would a person do?
2635280	2637840	So again, if you're an entrepreneur, you'd be an entrepreneur in this.
2637840	2641120	If you are someone who can communicate, you communicate this to other people
2641120	2643120	and you get paid a million bucks a year as a consultant.
2643680	2644800	You organize information.
2644800	2647200	If you're an artist, if you're a creative, use a tool.
2647200	2650160	You become the most efficient artist in the world when you lean in on this.
2652400	2653920	Systems can be outcompeted.
2653920	2655600	It's like the example of the steel mill.
2655680	2658720	There were big vertically integrated steel mills that were outcompeted by
2658720	2660720	lots of little steel mills, micromills.
2661760	2666320	The big corporations, the big programs, the big things,
2666320	2669200	will be outcompeted by just individuals and small groups.
2669760	2671760	Building on top of this technology can do anything.
2672560	2677920	And we've seen that over and over again in every converging exponential field.
2677920	2682000	We've seen entrepreneurs disrupting and the rate of...
2682000	2687200	I think of this as the asteroid impact that the slow-lumbering dinosaurs all die
2687200	2689520	and the furry mammals are the ones that rapidly evolve.
2689520	2691520	Yeah. I mean, let's take a practical example.
2691520	2693760	And this is what we said a bit earlier about ChatJPT.
2694720	2696880	Why does anyone need to use Google Image Search in a year?
2697680	2697920	Yeah.
2697920	2701520	When you can create any image just by describing it and then iterate it just with your words.
2701520	2702000	Yes.
2702000	2702720	Just attribution.
2702720	2702960	That's it.
2702960	2704400	But that's an easy lookup table.
2704400	2704720	Yeah.
2705360	2706000	All right.
2706000	2707360	True. Absolutely.
2708320	2709600	So now let's take it...
2709600	2714080	Okay. The 20-year-old drop it, start experimenting, start playing, start using.
2715840	2717680	Now you're running a company.
2717680	2720480	You're running a $100 million company,
2721040	2723840	worse off a billion or $10 billion company.
2723840	2725280	And you see this.
2725280	2728880	How fearful should you be and what should you be doing?
2728880	2731360	Again, you should be leaning in to understand this.
2731360	2735120	The classical innovation process inside a company is very limited.
2735120	2740400	You should be having a crack team of people who've just given freedom to say,
2740400	2742560	how can this potentially change my entire company?
2744880	2748240	But it's hard because you are fighting against the inertia of your company.
2748240	2751440	You're fighting people being used to certain ways of interacting
2751440	2753040	or certain ways of distributing stuff.
2753760	2758480	So this is why I think you have to think and think again from first principles.
2758480	2761280	If this technology is real-time, fast, across modalities,
2762000	2763520	and it can look and understand stuff,
2764480	2766400	let's go forward five, 10 years and work back.
2767120	2769200	Does my company still exist in this current format?
2769200	2770160	Can I out-compete that?
2771120	2775440	If not, how can I fold these into my current processes and procedures, etc.?
2776400	2777600	Because this is the other thing.
2778240	2782160	As a true exponential, the AI research actually in this area,
2782160	2784720	80% of all AI research has become in this area in the last few years.
2785440	2789440	And it's exponential with a 23-month rate of doubling, a true exponential.
2789680	2791840	The adoption of this now is also exponential,
2791840	2793840	because everyone from around the world is using it,
2793840	2795920	and then each of those introduces another two people,
2795920	2797600	and it goes four and four and four.
2797600	2801360	So right now, it's like a wave that's under the surface.
2801360	2802560	It just hasn't come yet.
2803280	2806640	Next year is when it cracks, and the year after is when it crashes.
2811200	2817440	I hear you, and my reaction is, I'm excited as hell about that.
2818400	2818720	Right?
2818720	2824560	It's like because it's about transforming the inefficiency of the world today.
2825280	2828480	It's about taking individuals and empowering them to do,
2828480	2830800	to giving them agency, like you said.
2832080	2833600	But this is very interesting.
2833600	2837200	Inefficiency is where value is created in the current paradigm,
2838400	2840480	in some ways, or where value is created.
2840480	2842080	Because efficiencies exist.
2842080	2844160	Where value is captured by inefficiencies.
2845040	2847120	So we have to have, we go to the gatekeepers,
2847120	2850800	and we give them, you pay your lawyers loads of money, right?
2850800	2852320	Pay your accountants and other things like that,
2852320	2854320	because there are inefficiencies in the system.
2854320	2856560	So you pay them to remove the inefficiencies, as it were.
2857360	2860400	Some of these inefficiencies will no longer remain in the system,
2860400	2862720	but that means that that value capture will also disappear.
2863840	2867280	And again, this is why you have to start thinking,
2867280	2870160	where does the value look like in the new reconfigured landscape?
2871120	2874720	Do you have an example in an industry that exists today,
2874720	2877200	and just to make this concrete for someone?
2877200	2879920	Sure, you don't need lawyers anymore for everything.
2881040	2883840	I mean, like, you know, you've got donotpay.com,
2883840	2884960	those are massive things.
2884960	2886640	They automatically write your tickets for you,
2886640	2887280	and kind of, yes.
2888240	2890800	For those who don't know, it's if you got a parking ticket,
2890800	2892640	and you go to do not pay,
2892640	2896080	they will figure out the legal loopholes and arguments
2896080	2897440	to get you off your ticket.
2897440	2899760	Yeah, but like you still need litigators, right?
2900240	2902080	At what point can you have a robot litigator?
2902080	2904480	It's like Phoenix Wright on steroids, right?
2905840	2908720	You know, if you've got the, again, movie creation thing,
2909680	2912320	movie creation is just going to transform completely.
2912320	2914800	Video games, you'll be able to have your own user-generated content.
2915440	2919040	You know, like, again, people can right now,
2919040	2921040	use stable diffusion, create any image.
2921040	2923920	They can go to chat GPT, it's an amazing system by OpenAI,
2924480	2925600	and they can chat with it, and they're like,
2925600	2928080	oh, okay, that disrupts a lot of industries as well,
2928080	2929840	because it knows the different things.
2929840	2931680	But it's just a model, it's a blob.
2931680	2932960	It doesn't look on the internet.
2932960	2934800	What happens when you hook up these models
2934800	2936880	that are principle-based to the internet?
2936880	2938640	Stable diffusion plus Google Image Search
2938640	2939920	is actually incredibly powerful.
2940960	2942640	But people are just looking at it as stable diffusion.
2942640	2944640	Stable diffusion as part of a pro-
2944640	2946240	architecture is incredibly powerful.
2946240	2947120	People are making movies.
2947120	2949600	So like if you look at our friends at the Corridor crew,
2949600	2952240	for example, they did a movie called Spiderman,
2952880	2955680	Everyone's Home, where they created a custom model
2955680	2958240	based on the Spider-Verse movie with Mars Morales.
2959200	2961840	Couple of days, they created a three-minute trailer
2961840	2963920	that blows away anything big studios can do.
2964480	2966640	Just a few people, a few thousand bucks.
2967360	2969760	Let's go to a few general questions on AI.
2969760	2972720	First of all, you come out of the hedge fund industry.
2973760	2978560	I have to imagine that the day of the trader
2979760	2982560	investing on their own without the use of
2984160	2986640	any of these technologies is long gone.
2986640	2990560	Does anybody have any advantage on their own?
2990560	2991360	I mean, I think that's the thing.
2991360	2993120	What's an edge in trading, right?
2993120	2995200	And again, it comes down to information theory.
2995200	2999120	What can move the Apple stock 50% very little information?
2999120	3002720	What can move it 5% a decent amount of information?
3002720	3004960	1% quite a lot of information, right?
3004960	3006480	So we're just looking at the narrative
3006480	3008000	and the incremental narrative of this,
3008000	3010240	because again, as humans, we're heuristic creatures.
3010240	3012480	So like I used to be a specialist in a number of markets,
3012480	3013280	oil market was one.
3014000	3016080	An oil barrel is fungible going around the world
3016080	3017440	because you can just shift it on a ship.
3017440	3017760	Sure.
3017760	3019680	But the impact of a Libyan barrel going offline
3019680	3022560	was the third as impactful as an Oklahoma barrel.
3022560	3023360	Why?
3023360	3025120	Because most of the money is in the West
3025120	3026640	as opposed to near Libya.
3026640	3028960	So they feel it more and the market reacts more.
3028960	3031920	The market is a counting mechanism and a voting mechanism.
3031920	3034160	I think the tools that people will use for investment
3034160	3034960	are going to change now
3035520	3037840	because you'll be able to actually visualize stories better
3037840	3039440	and people will kind of introduce that.
3039440	3042480	This is kind of why people invest on themes like ESG
3042480	3045040	and you have these index trackers and all these other things.
3045040	3046480	So I think that's going to be very interesting.
3046480	3049760	But then which industries will be disrupted faster than others?
3049760	3050800	We don't know.
3050800	3053280	Who will be at the edge and the forefront of this
3053280	3054480	embracing this technology?
3055120	3056480	And then who can be left behind?
3056480	3057600	We don't know.
3057600	3060640	So I think the entire stock market's going to change and adjust.
3060640	3063200	It could be a period of supernormal profits
3063200	3065040	and then outcompetition as well.
3065600	3067920	But this is against a backdrop of inflation
3067920	3070720	and recession and all sorts of crazy stuff
3070720	3071920	all at the same time.
3071920	3075120	So will you be using this technology
3075120	3082000	to assist a hedge fund manager out there on investing?
3082000	3083200	Yeah, I can run to my own hedge fund.
3084640	3087760	No, look, again, I think this technology would be pervasive
3087760	3089280	because people again will see the power of this.
3089840	3091680	And ultimately, like I said,
3092400	3094000	investing usually comes down to stories.
3096080	3099280	Influencing human desires and intention.
3099280	3099760	Exactly.
3100000	3102000	If you're doing VC to fund management or whatever,
3102000	3103680	you've just basically got a story.
3103680	3105680	You can say for all you want that you're trying to be
3105680	3107840	like quantitative and this and that.
3109360	3111120	But nobody knows the future.
3111120	3114320	So you construct a story, but you're saying,
3114320	3116400	what is the evolution of that story going to be
3116400	3118480	such that someone will buy this from me at a higher price?
3119360	3120240	That's all that matters.
3120240	3122240	And part of it, for entrepreneurs,
3122240	3123920	one way that I suggest to look at things is,
3124640	3126000	what's the terminal value?
3126000	3127520	You get someone to agree to that.
3127520	3128880	All you're doing when you're raising money
3128880	3131120	is you're de-risking your path for that terminal value.
3132560	3135360	And people don't realize that's all of investment.
3135360	3137040	Yeah, it's a very simplified view of it.
3137600	3141440	So I'm excited to have you at speaking at abundance360 this year.
3141440	3143680	Thank you for joining us in March.
3143680	3144320	Thank you for having me.
3145360	3147520	And I've dedicated an entire day.
3147520	3150720	We've added a fourth day of the program focused on AI
3150720	3153680	because it's just, I think people need to understand
3153680	3157120	how powerful and disruptive and a view of what's coming.
3158000	3162160	Another one of our rock star speakers there,
3162160	3164080	someone who you know, Ray Kurzweil,
3164080	3166960	and Ray's going to be joining us to speak as well.
3167920	3170800	And it's been Ray's long held belief
3170800	3174560	that we're going to reach human level AI of like 2029.
3176080	3177680	However you want to define that,
3179360	3181760	what's your thought about that?
3181760	3184640	Do you think that that is the case?
3185680	3186960	How do you think about it?
3187600	3188800	It's singularity, Niere.
3191920	3193040	Again, how have you defined it?
3193040	3199600	I think my ideal view is that a future that's an intelligent internet.
3200320	3203360	Every person, country, culture, and company has their own models
3203360	3207520	and they're all interacting with each other in an optimal way for humanity.
3207520	3211200	So again, I kind of go to this thing of Tim Urban and Wait But Why
3211200	3212640	when he wrote the article about neural link,
3212640	3215200	had this option, this discussion of the human colossus.
3216000	3222320	I would like AGI to be something of all of us working for us to make us better.
3222320	3225840	And I think part of this is why I'm basically pushing open source
3225840	3227280	and these open source models.
3227280	3229520	And I've created an organization that goes
3229520	3232960	and has all these verticals that will then be spin off into independent organizations.
3232960	3235440	So we can standardize the architecture across that
3235440	3238160	and it can be an emergent global consciousness as it were.
3239040	3244320	I think it's important for us to talk about the idea of these models
3244320	3247840	and these personalized models to understand what that means.
3247840	3254800	So when you build a massive model that is trained on everything out there,
3255360	3257200	it's not really necessarily useful.
3258080	3261040	But you talk about creating models for nations,
3261040	3263120	for cultures, for companies, for individuals.
3263680	3266000	Can you just give us the 101 on that
3266000	3268880	so people understand the power of that and the value of that?
3268880	3271360	Knowledge evolves at different paces, right?
3271360	3274960	There's knowledge on how to use a toilet that's been with us for many, many years,
3274960	3278160	you know, versus knowledge on foundation models, which is just very new.
3278160	3280720	You kind of see that fashions is like pace layering, as it were.
3282160	3285920	The thing, the way that models will be built is that you should look at them as like pizza bases.
3285920	3289600	So we're trying to figure out what an optimal pizza base is for a generalized image model.
3289600	3292560	Then you've got maybe an Indian model with a bit of culture in there
3292560	3294880	and then Indian fashion in there.
3294880	3299440	And then Indian fashion for MAD to try and be fashionable when he's in India.
3300080	3302080	And so you can kind of look at those as bases.
3302080	3304080	Because what we do is, again, we flip the paradigm.
3304080	3310000	So to train a stable diffusion, like we built a 4,800 cluster with our buddies at Amazon.
3310720	3312560	To put that in context, the fastest supercomputer...
3312560	3314880	4,800 cluster.
3314880	3315840	Yeah, it's huge.
3315840	3319600	Yeah, to put it in context, the fastest supercomputer in the UK, Cambridge 1 is 640.
3320160	3323920	We've got about 10 times the compute of NASA, roughly speaking.
3323920	3325360	It's one of the top 10 in the world.
3325360	3327280	And next year I'll go 10 times bigger.
3327280	3328480	Yours will go 10 times bigger.
3329440	3330720	And we make that available to everyone.
3330720	3332800	And it's on the Amazon cloud?
3332800	3334080	It's on the Amazon cloud, yeah.
3334080	3336400	Because it would have been a bit too much for us to build it by ourselves.
3337280	3339520	So we got them to lean in on that and kind of build this.
3340560	3344240	Facebook has 16,000, for example, because they're pushing big metaverse.
3344240	3346880	That's the fastest in the world, if you want to have an idea.
3346880	3351440	But again, this is an exponential thing whereby we have more than just about every single country.
3351440	3355920	And we use that to take 100,000 gigabytes of images and compress it into...
3355920	3357200	100 terabytes.
3357200	3359440	Yeah, 100 terabytes and compress it down.
3359440	3362240	So into two, effectively.
3362240	3364480	So 50,000 to one compression of knowledge.
3366480	3370000	So when you say that compression, when you're talking about a neural net,
3372480	3377200	not all of the connections or the pathways in the neural net are useful or valid.
3378080	3381360	So what you're doing is you're choosing which ones are?
3381360	3383280	Yeah, because it pays attention to what the most important lines are.
3383280	3385120	So this is part of the attention attention.
3385120	3387520	And then it creates these latent spaces that you poke with the prompts,
3387520	3388800	which are the words that you put in.
3389360	3394560	So like OpenAI, for example, made GPT-3 was 175 billion parameter model.
3394560	3397120	The next step after that deep learning, because that's called deep learning,
3397120	3401280	this thing, because we spend all the energy and all that compute, you don't have to.
3401280	3403360	You can then take that base, that pizza base.
3403360	3405360	You can inject some PETA and then you have a PETA model.
3405360	3408080	It just takes 10 images and then you can put yourself in anything.
3408080	3411520	Or you can spend 100,000 images and do an even more refined model.
3411520	3412320	But it forms a base.
3412320	3414080	It's what's called the foundation model in a way.
3414720	3416000	But you can even make it even more efficient.
3416000	3418480	So GPT-3 is 175 billion parameters.
3418480	3419920	So it's like 80 times bigger.
3419920	3423440	And they said GPT-4 was going to be like 100 trillion parameters.
3423440	3424640	Could be, yeah.
3424640	3428000	I think that chat GPT is actually training GPT-4 right now.
3428000	3428320	Interesting.
3428320	3429840	Because what happens is that you have this foundation,
3430480	3436960	but then in the classical day age of big data, what mattered was who you were.
3436960	3440880	So they took your data, they built these big models and they targeted you.
3440880	3443760	What matters now in the day age of big models
3443840	3445360	is how you use the models.
3445360	3446720	Because not all those neurons are needed.
3446720	3448400	We don't need all two billion images, right?
3448400	3448640	Right.
3449280	3451600	So GPT-3 was 175 billion.
3451600	3453040	And they saw how people used it.
3453040	3454880	And they identified the neurons that lit up.
3455600	3458000	And they compressed it down to 1.3 billion parameters.
3458000	3460560	So now when you're using chat GPT, you're training one of those models.
3461360	3463280	And they're looking at how people are using it to compress it down.
3463280	3464800	So it's able to usually compress down even more.
3465360	3467600	But then this is personalized model that's very interesting.
3467600	3470800	Because you can have almost this standardized base.
3471120	3474960	And then you can inject your own context into it.
3474960	3479040	And the context of your culture, your company, your community, and others.
3479760	3481600	And so that base is manual built.
3482320	3484240	In that you can extend out the latent spaces.
3484240	3487360	So like in stable diffusion one, we didn't really filter it.
3487360	3489520	So Mona Lisa is overfit.
3489520	3490880	There's too many pictures of Mona Lisa.
3490880	3492960	So it's very hard to get her out of the picture frame.
3494320	3496800	Now we adjusted it so you can make her swim very easily.
3496800	3497920	Because it's not overfit anymore.
3497920	3499600	And it'll continue improving and adapting.
3499600	3501200	So we'll have better databases.
3501200	3503520	And in a year's time, we'll have a very mature base
3503520	3504800	that people can take and extend.
3504800	3507920	So like in Japan, they took stable diffusion
3507920	3510880	and they adjusted the text encoder for Japanese culture.
3512080	3514240	And then it meant that if you use normal stable diffusion,
3514240	3517760	because it's very Western oriented, salary man means a happy man.
3517760	3518240	Yes.
3518240	3519920	In Japan, diffusion is very sad man.
3520560	3521520	Very sad man indeed.
3521520	3523200	Because it understands that context.
3523200	3523680	Fascinating.
3523680	3526240	So in the future, if I wanted to,
3527120	3529360	I coach a lot of entrepreneurs
3529360	3532640	and a lot of CEOs through abundance 360 and so forth.
3533360	3537680	If I wanted to create a virtualized version of myself
3537680	3541920	that in certain circumstances would react in a certain way,
3542960	3544160	that's pretty easy.
3544160	3544720	It's coming.
3544720	3548080	So pocket coach, if I wanted to like have
3548080	3551040	Tony Robbins in my pocket in the right moment
3551040	3554480	or the Dalai Lama, it's all possible.
3554480	3557680	Yeah, and that'll be probably a four gigabyte file.
3557680	3559120	And how far is that from now?
3560240	3561200	Pretty much do it now.
3561200	3562000	Go do it now.
3562000	3565760	And you can do it fully realistic like, you know.
3565760	3567920	I think the term is holy shit.
3567920	3568800	That's amazing.
3568800	3571120	Yeah, because you have the multi-modality.
3571120	3573600	You can make it indistinguishable from a human like
3573600	3576080	stable diffusion two is pretty much photo realistic.
3576080	3578240	Now stable diffusion three will break that barrier.
3578800	3580480	And obviously we can animate now.
3580480	3582080	So again, that's kind of crazy.
3582080	3582720	That's crazy.
3582800	3587680	Meta-humans epic game style and video are doing.
3587680	3589200	You can also do human realistic voices
3589200	3590640	with fully emotional range as well.
3591520	3594560	So like my sister-in-law runs a company called Sanantic.
3595600	3598880	She reconstructed Val Kilmer's voice for Val and Top Gun.
3598880	3599040	Nice.
3599040	3600000	I was doing all the video games.
3600000	3602320	And if you go to Sanantic.io, you hear an AI tell you
3602320	3604480	that it loves you and it's really creepy.
3605760	3606800	She just sold us Spotify.
3606800	3609200	So I'm sure we'll have some really engaging podcasts
3609200	3609840	and things like that.
3610160	3614000	And like again, we'll cross all modalities now.
3614000	3617600	In narrow, you're achieving human levels and going on
3617600	3619760	to human levels of performance and benchmarks
3620480	3623200	from media to understanding to output.
3624320	3627040	And the barriers to putting yourself in, like I said,
3627040	3630480	you can train a model now in like less than an hour
3630480	3633440	with 10, 100 images of yourself to put yourself in anything
3635200	3635920	for a buck.
3637120	3639360	Because we've done the heavy lifting of millions of bucks
3639360	3642160	of pre-training the model as it were, whereas classically,
3642160	3643520	AI wasn't like that.
3643520	3647600	So if I wanted to create a virtualized host of myself,
3648240	3653520	being able to on-screen play me, say what I want to say,
3654160	3655280	that's here now.
3655280	3655840	Let's say it now.
3655840	3657440	Like the technology is here now.
3657440	3658640	The implementation needs to be there.
3658640	3661520	So like one of the companies we work with is called
3661520	3662640	atlethea.ai.
3662640	3664400	And so they use our language models.
3664400	3666800	And so you can upload your scripts and it'll learn how you speak.
3667040	3670480	But then the voice technology, they haven't integrated the voice technology.
3670480	3672160	We have an extra generation audio technology.
3672160	3675280	That'll come in a month or two and then it'll learn how you speak literally.
3675280	3677440	I was using last year at Advanced 360.
3677440	3680720	I was using a company called Soul Machines and it had a virtual humans.
3680720	3680880	Yes.
3680880	3681520	All right.
3681520	3685520	And so I would love to create a virtual version of myself
3685520	3686720	for anybody I want.
3686720	3687760	And that's here.
3687760	3688720	That's here again.
3688720	3692320	Soul Machines is upgrading now thanks to the technology that we're open sourcing.
3692320	3693360	This is the other interesting part.
3693360	3694960	Open source will always lie close source.
3695200	3697680	Because close source can always take open and add in data.
3697680	3698000	Yes.
3698000	3701040	And they can have very focused teams focusing on certain use cases.
3701040	3705840	So we're literally upgrading the foundation of all of these companies
3705840	3707440	as we release these models.
3707440	3708080	Interesting.
3708080	3710080	And then you mix and match and that's where the value is.
3710080	3712080	Actually, who was it who said a thing?
3712080	3713520	Was it my friend Jason or someone else?
3713520	3715920	No, it was one of the other VCs.
3715920	3719120	Most of the money in the world is made by aggregation and disaggregation.
3719120	3721120	It just depends on which part of the cycle you're at.
3721120	3722160	Interesting.
3722320	3723280	I can see that.
3723280	3726480	I was having dinner with Reid Hoffman a month ago or so
3726480	3728480	and he said something which is interesting.
3728480	3732720	He said every profession is going to have an AI co-pilot very soon.
3732720	3734320	And I've been saying this for medicine.
3734320	3737920	I think it's going to be malpractice to diagnose without having AI in the loop.
3737920	3739360	And we'll see what the time frame is.
3739360	3740880	Five years is my guess.
3742080	3748320	But I can see an AI co-pilot as an architect, as a lawyer, as a chef, as everything.
3748640	3750640	How far is that?
3750640	3752640	Well, I mean it's here for code right now.
3752640	3757680	So co-pilot, literally what it's called, from Microsoft GitHub
3757680	3759680	and then code whisperer now from Amazon.
3759680	3761680	They help you write better code.
3761680	3763680	It's about a 50% speed increase.
3763680	3764320	Amazing.
3764320	3766320	And that's what we've kind of measured so far, which is insane.
3766320	3769840	Like you type it, I want to have a piece of code that does this and boom, it's there.
3769840	3771840	And maybe you need to add a snippet, doesn't matter.
3771840	3773840	It makes your life easier, right?
3773840	3775840	Like Sayable Diffusion is a co-pilot for art.
3775840	3779840	So artists use it to iterate rapidly on different concepts and they take it.
3779840	3782320	And there's a Photoshop integration and they use it as part of their Photoshop process.
3782320	3785840	I used to say that the crowd was the interim step until AI, right?
3785840	3789360	So GitHub was the crowd and now you've got, you know.
3789360	3789840	Yeah.
3789840	3791840	And you know, there's questions around, you know, what was it trained on?
3791840	3793840	Because it was trained on an entire snapshot of things.
3793840	3797840	But again, it's like a different type of information flow to the Web 2 economy.
3797840	3799840	We're going to skip over Web 3 because it was a bit crap.
3799840	3801840	And we're going to go to Web 4.
3801840	3803840	Or whatever it is now.
3803840	3805840	And you know, maybe you could go to Web 4.
3805840	3807840	It looks like AI.
3807840	3809840	Do a cool logo for that.
3809840	3811840	Just get that stable diffusion to make it.
3811840	3813840	In fact, it's listening and it's made it.
3813840	3817840	So I love Jarvis from Iron Man.
3817840	3819840	Yeah.
3819840	3825840	And I find Siri and Alexa and Google now kind of disappointing.
3825840	3827840	How far are we from Jarvis?
3827840	3831840	So I think Jarvis is probably two to three years.
3831840	3837840	Because like right now you've got a MacBook M2 in front of you.
3837840	3841840	16.8% of that chipset is a neural engine that's optimized for these transform based architectures.
3841840	3845840	Stable diffusion is one of the first models to actually go down to that level.
3845840	3847840	And so when will we see that on this machine?
3847840	3853840	I think Apple is basically aiming for like 70, 80% of everyone to have this.
3853840	3855840	And then you can go from Siri 1 to Siri 5.
3855840	3857840	Nice.
3857840	3859840	And this is why Apple has been talking about privacy and things like that.
3859840	3863840	Because the new paradigm of the internet, whereby the classical Web 2 internet,
3863840	3869840	was intelligence at the middle coordinating us and feeding on our dreams and hopes and emotions to sell us ads.
3869840	3871840	Now it's intelligence at the edge.
3871840	3873840	Whereby you've got your own Apple ID.
3873840	3875840	You've got your own privacy layer.
3875840	3879840	And then you've got chips that can run AI at the edge that really understand you.
3879840	3881840	That's why the noggin's experience is seamless.
3881840	3883840	And Google also realized that.
3883840	3885840	So they're building stuff into the Pixel phones.
3885840	3889840	Yeah, I think people need to realize that the power of these future systems,
3889840	3891840	call it Jarvis for lack of a better term,
3891840	3895840	is when you give it open access to everything in your life.
3895840	3897840	You let it watch what you're eating.
3897840	3899840	You let it read your emails, listen to your conversations.
3899840	3905840	Because it makes the world, the term I use is automagical in that regard.
3905840	3907840	Yeah, and you need to have the foundation models to do that.
3907840	3910840	Because it needs generalized knowledge and then specific knowledge and contextual knowledge.
3910840	3912840	So it can adapt to your needs.
3912840	3914840	This human in the loop process is very important.
3914840	3918840	And so there'll be big AIs in the cloud, but then a lot of AIs on the edge.
3918840	3920840	And they'll interact with and talk to each other.
3920840	3921840	And part of each other.
3921840	3925840	Because what these models also do is they take structured data and turn it into unstructured data.
3925840	3926840	So again, stable diffusion.
3926840	3927840	A few words.
3927840	3928840	Robert De Niro's Gandalf.
3928840	3931840	You get a photorealistic picture of Robert De Niro's Gandalf.
3931840	3932840	Yep.
3932840	3934840	That's structured unstructured data.
3934840	3936840	And then you clear both ways.
3936840	3938840	A brief note from our sponsors.
3938840	3939840	Let's talk about sleep.
3939840	3943840	Sleep has become one of my number one longevity priorities in life.
3943840	3953840	Getting eight deep uninterrupted hours of sleep is one of the most important things you can do to increase your vitality and energy and increase the health span that you have here on earth.
3953840	3958840	You know, when I was in medical school years ago, I used to pride myself on how little sleep I could get.
3958840	3960840	You know, it used to be five, five and a half hours.
3960840	3963840	Today I pride myself on how much sleep I can get.
3963840	3965840	And I shoot for eight hours every single night.
3965840	3968840	Now, usually I'm great at going to sleep.
3968840	3972840	If I'm exhausted, you know, I've worked a hard day, I'm right out.
3972840	3979840	But if I'm having difficulty and it occurs, I'm having insomnia or my mind's overactive and I need help to get that eight hours.
3979840	3983840	I turn to a supplement product by Lifeforce called Peak Rest.
3983840	3988840	Now, Peak Rest has been formulated with an extraordinary scientific depth and background.
3988840	3995840	Includes everything from long lasting melatonin to magnesium to elglycine to rosemary extract, just to name a few.
3995840	4003840	This product is about creating a sense of rest and really giving you the depth and length of sleep that you need for recovery.
4003840	4005840	It's a product I hope you'll try.
4005840	4008840	It works for me and I'm sure it will work for you.
4008840	4017840	If you're interested, go to mylifeforce.com backslashpeter to get a discount from Lifeforce on this product.
4017840	4023840	But you'll also see a whole set of other longevity and vitality related supplements that I use.
4023840	4028840	We'll talk about them some other time, but in terms of sleep, Peak Rest is my go-to supplement.
4028840	4029840	Hope you'll enjoy it.
4029840	4034840	Go to mylifeforce.com backslashpeter for your discount.
4034840	4038840	Let's talk about something which I have an opinion about.
4038840	4039840	I'm curious about yours.
4039840	4046840	I think I know it, which is the idea of privacy, which fundamentally people all want privacy.
4046840	4049840	I don't believe it really exists.
4049840	4055840	AI can read your lips when there's data flowing everywhere or where encryption.
4055840	4058840	What are your thoughts about privacy and how do we deal with it?
4058840	4059840	Do you have any ideas?
4059840	4063840	I think for privacy, you should always look at what the downside to not having privacy is.
4063840	4067840	Actually, people are more than willing to give up their data, too willing, in my opinion.
4067840	4072840	Everybody clicks that, yes, I accept there's 15,000 pages of legal.
4072840	4073840	Exactly.
4073840	4076840	And then you have to think as well of different paradigms.
4076840	4078840	In China, will there ever be privacy?
4078840	4079840	Probably not.
4079840	4083840	And you have the social credit score and it's an opticon being built by AI, etc.
4083840	4086840	In the Western Parasite, what's the downside on the privacy thing?
4086840	4087840	What if your stuff isn't private?
4087840	4092840	It's basically bad actors using you in certain ways, which can include AI algorithms trying to manipulate you.
4092840	4098840	I think, again, what Apple's doing is building a paradigm for actual privacy because it's aligned with their business model.
4098840	4103840	Even other companies now, Facebook and Google, have enough information they don't need your data anymore.
4103840	4105840	Who actually wants your data?
4105840	4107840	I think it's a question.
4107840	4110840	We view ourselves as these wonderful things.
4110840	4112840	Who actually wants your data at this point?
4112840	4116840	But the systems have adopted to do that and policies adopted to do that as well.
4116840	4118840	With GDPR and all these other things.
4118840	4120840	Some of them overreach, I think, a bit.
4120840	4123840	But we are moving to this area whereby nobody needs your data anymore.
4123840	4127840	And also the systems are now available to give you that privacy that you want.
4127840	4132840	And I think people want to opt in rather than opt out of a lot of different things to get more resources and other stuff.
4132840	4137840	Finally, the final element is that federated learning has matured now.
4137840	4138840	What does that mean?
4138840	4141840	So federated learning is when you take the model to the data.
4141840	4144840	So you used to have to ingest all this data and train the models.
4144840	4149840	Now, if it's just like a freaking gigabyte, you send the model to the data, it can train.
4149840	4154840	And then without saying it's PETA or MAD, it can upstream the output.
4154840	4158840	So we're seeing that in HDR UK, for example, health data, we're seeing it with Melody,
4158840	4165840	which is a thing with a lot of pharmaceutical companies coming through to open source analysis of patient data.
4165840	4170840	You can finally get that where you don't have to sacrifice privacy to build AI and models.
4170840	4173840	And that's going to be pretty amazing to, again, advance the field
4173840	4176840	because you have access to so much more data to build better models.
4176840	4177840	Amazing.
4177840	4180840	Let's talk about the perceived downside.
4180840	4187840	And I have to imagine that as much incredible compliments
4187840	4190840	and the world should thank you for the work that you're doing,
4190840	4192840	because of the impact it's going to have,
4192840	4196840	you're going to have to have detractors who are worried about technological employment
4196840	4200840	or malicious use of AI or fake news and all of that.
4200840	4202840	What concerns you?
4202840	4207840	And I know you're a principled man who thinks about this deeply.
4207840	4208840	What concerns you most?
4208840	4210840	I don't have all the answers.
4210840	4212840	And that's a fair statement to make.
4212840	4213840	Yeah.
4213840	4219840	I mean, generally, what I saw was that very few individuals had control of this most powerful technology.
4219840	4221840	And then there's very weird things.
4221840	4224840	People like open source AI is like nukes and like,
4224840	4226840	so why should you control the nukes?
4226840	4228840	You know?
4228840	4231840	It was a very strange kind of thing.
4231840	4234840	They're like, no, it shouldn't be open source.
4234840	4236840	So why should big companies control it?
4236840	4238840	Again, we live in largely a democracy.
4238840	4240840	We live in a society.
4240840	4244840	And so my take was let's educate people, get this technology out there,
4244840	4246840	and let's have a common conversation about it.
4246840	4248840	Because I have my own viewpoints and they're there.
4248840	4250840	But again, I'm not a representative of anyone.
4250840	4253840	I'm just me running my own company trying to catalyze this.
4253840	4255840	Because I thought it was important,
4255840	4261840	given the fundamental change of society that will be caused by this technology now,
4261840	4265840	because exponentials are a hell of a thing for it to get out there.
4265840	4266840	And so you need to make a splash.
4266840	4269840	So, you know, I've got hate mail and kind of all sorts of things because it is disruptive.
4269840	4271840	And we have to be aware of that.
4271840	4273840	It is crazy and it will cause fear.
4273840	4274840	We have to be aware of that.
4274840	4277840	And we have to decide together how to do that.
4277840	4282840	For example, there are artists in the data set because it's a snapshot, right?
4282840	4283840	Sure.
4283840	4284840	It's less than 0.5%.
4284840	4289840	And so is it ethical, legal and moral to have them in there so people can prompt an art style
4289840	4290840	and then match them together?
4290840	4292840	I think it is.
4292840	4297840	But does that mean that we disregard artists who want to opt out of the data set?
4297840	4298840	No.
4298840	4299840	Because they're part of the global community.
4299840	4301840	So we've built it opt out and opt out mechanisms.
4301840	4306840	And by the way, those artists are influencing other artists normally in the course of just
4306840	4307840	them going to museum.
4307840	4308840	Yeah, exactly.
4308840	4312840	And you know, what we have is now we've had like four or five thousand artists sign up
4312840	4313840	of spawning.
4313840	4315840	Half of them have opted out.
4315840	4317840	Half of them have opted in.
4317840	4321840	Because they'd love to see their work influence the world.
4321840	4324840	But how many people have really absorbed the prams of the discussion?
4324840	4325840	Very few.
4325840	4330840	So like I said, my thing is that again, this is fundamental infrastructure.
4330840	4333840	This technology is a fundamental human right.
4333840	4336840	Because otherwise what you're going to have, this is a discussion that, you know, you've
4336840	4338840	had many times.
4338840	4340840	Superhumans and normal humans.
4340840	4341840	Yeah.
4341840	4346840	The ability to communicate and create makes you superhuman.
4346840	4350840	Because it's just not only images like it's presentations, it's being able to, like we
4350840	4354840	have voice to voice technology that can allow you to speak more confidently.
4354840	4355840	It's interesting.
4355840	4361160	But people need to realize that today the poorest among us in society have more than
4361160	4364320	the kings and queens had, you know, a couple of centuries ago.
4364320	4366840	And this is about leveling the playing field.
4366840	4370320	This is about, this is about, this is the technology and this is what I care about deeply.
4370320	4374560	And I know you do too, uplifting humanity, enabling every man, woman and child to have
4374560	4377600	access to food, water, health care, education.
4377600	4378600	And have a voice.
4378600	4379600	And have a voice.
4379600	4380600	They are invisible.
4380600	4381600	And have dreams.
4381600	4382600	Yeah.
4382600	4385600	And have dreams and have the tools to fulfill those dreams.
4385600	4386600	And have agency.
4386600	4387600	Yes.
4387600	4388600	Agency is the right word.
4388600	4392600	I had a bit of a flipping comment because again, I can do what I want in my kind of role.
4392600	4394600	It was like, humanity is creatively constipated.
4394600	4396600	We're going to make it so it can poop rainbows.
4396600	4398600	I think that's great.
4398600	4399600	It's a silly comment.
4399600	4403600	But again, it's the reality because people don't believe they can create.
4403600	4408600	They don't believe they, the mentality and mindset is wrong.
4408600	4410600	Because people have more agents than they can do.
4410600	4415600	An individual can shake the world or the individual can make anything around them better.
4415600	4417600	But not if they don't believe they can.
4417600	4422600	And this is why art therapy is used in mental health settings to amazing things.
4422600	4426600	We've been conditioned to consume rather than create.
4426600	4430600	We've been conditioned to be polarized rather than talk to each other and communicate with
4430600	4431600	each other.
4431600	4432600	And this can again, can change that.
4432600	4436600	And again, that's why, like I said, this should be, in my opinion, a human right.
4436600	4439600	It is infrastructure as important as 5G.
4439600	4443600	And what I'm trying to catalyze now is not that I build the company that makes the decisions
4443600	4447600	for that, but that we put it out there and we're spinning off a Lutheran other things.
4447600	4449600	I just figured out a governance structure.
4449600	4450600	It's not the UN.
4450600	4451600	It's something else.
4451600	4462600	So I think one important point is, do you think a world in which individuals are held
4462600	4467600	back or restricted feel they have no hope or a world where every mother knows her children
4467600	4468600	have access to the best healthcare?
4468600	4473600	The best education, you know, the best ability to create, that's a more peaceful world in
4473600	4474600	my mind.
4474600	4475600	Yes, 100%.
4475600	4478600	I mean, look, all wars are based on lies.
4478600	4479600	Okay.
4479600	4484600	For otherwise, both sides couldn't believe.
4484600	4486600	Because humans are humans.
4486600	4488600	To kill another human is disgusting.
4488600	4489600	Right?
4489600	4491600	And so you have to tell the lie that that person is the other.
4491600	4494600	And you have to communicate it and control the means of communication.
4494600	4499600	You look at kind of again where conflicts are resolved when people realize they are humans
4499600	4501600	and we're all part of a global society.
4501600	4504600	But our infrastructure has been set up to polarize.
4504600	4505600	Literally, we can see it visually.
4505600	4507600	This is how it happens.
4507600	4509600	The incentive structure is misaligned.
4509600	4513600	So how can you fight polarization if not by communication?
4513600	4518600	And how can you do that if you don't give people these tools and you create it so that there
4518600	4521600	is a base foundation for the world, so that there are generalized models that are global
4521600	4525600	and every country has its own AI policy using their variants of those models.
4525600	4529600	And then because it's all standardized, we can hop between one and the other.
4529600	4531600	That is a peaceful future.
4531600	4532600	Yes.
4532600	4535600	And a future worth working towards creating.
4535600	4536600	Working towards creating.
4536600	4540600	But it's also, now it's the first time we can build that future because of this disruption
4540600	4541600	in technology.
4541600	4547600	Like, you know, governments are, there's a definition of a government.
4547600	4551600	It is the entity with the legitimate use of political violence.
4551600	4553600	The only one.
4553600	4555600	That's a sad definition.
4555600	4557600	But it's the nature of it, right?
4557600	4560600	Because you saw lots of political violence and then it was consolidated into one entity.
4560600	4561600	They can imprison you.
4561600	4562600	Yes.
4562600	4565600	And they've got an army back in the currency and this and that and that, right?
4565600	4569600	And governments rule on the basis of pure legitimacy to violence.
4569600	4571600	And again, we see that kind of thing.
4571600	4572600	Right?
4572600	4575600	So against this, what typically changes a government or a society?
4575600	4577600	It is an act of violence in some ways.
4577600	4578600	It's an act of disruption.
4578600	4579600	It can be a technology.
4579600	4582600	It can be a revolution or anything like that.
4582600	4583600	This is a revolution.
4583600	4584600	Yeah.
4584600	4588600	That's happening just after COVID when everyone's thinking, holy crap, the system was rubbish.
4588600	4589600	Let's do better.
4589600	4590600	Yeah.
4590600	4591600	So once in a lifetime.
4591600	4592600	Yeah, I agree.
4592600	4601600	And the challenge is that in the world today, you can't transform a government gradually.
4601600	4602600	Yes.
4602600	4607600	And this is why as well, crypto, there were some amazing things in there and amazingly
4607600	4609600	smart people working there.
4609600	4613600	It's rubbish because it tried to build a system outside of the existing system and there was
4613600	4616600	this system and then there was the interface.
4616600	4619600	Fortunes were made there and fraud happened there.
4619600	4620600	Yeah.
4620600	4624600	Whereas this AI, because it can bridge structure and unstructured, can actually go into our
4624600	4627600	systems, out compete them and make them more efficient and bring them forward.
4627600	4631600	And it's the first technology that can do that dynamically and at scale.
4632600	4641600	Or build virtualized systems that are de novo that we spend our time in and opt out of the
4641600	4643600	existing system and into the new.
4643600	4644600	Exactly.
4644600	4645600	And it cannot compete.
4645600	4649600	Or the final thing, of course, is that if you keep it as it is whereby it's controlled
4649600	4653600	by the few, they will ultimately use it as a system of control.
4653600	4656600	It is the Panopticon forever.
4656600	4660600	And again, we're seeing this in China and other places with a social credit score that's
4660600	4661600	about to be augmented with AI.
4661600	4664600	Everyone's looking at everyone else, marching everyone else.
4664600	4665600	What is freedom there?
4665600	4666600	Maybe it's still happiness.
4666600	4667600	Maybe it's control.
4667600	4669600	Let's give people the option, right?
4669600	4672600	Let's give people the infrastructure and the building blocks they need to be independent,
4672600	4673600	happy.
4673600	4675600	And maybe they don't choose independence.
4675600	4677600	Maybe it's a bit more kind of Borg-like.
4677600	4678600	That's fine.
4678600	4680600	At least you've got the choice.
4680600	4681600	Yeah.
4681600	4686600	Because again, if we tie this all back, the fact that you can now have AI that can write
4686600	4693600	better than a human, that can draw better than a human, that can emote and speak to emotional
4693600	4698600	turns, means that, let's say for instance, take one aspect of it, companies that are
4698600	4703600	ad-driven that sell ads, they can create the most manipulative ads in the world ever,
4703600	4705600	and regulators will not regulate that.
4705600	4706600	That's interesting.
4706600	4707600	And they do.
4707600	4712600	No, but now it's the next step up because they have these latents that resonate.
4712600	4717600	So, like now, when you look at some AI art, artists can complain what they want.
4717600	4718600	It's resonant.
4718600	4721600	When you listen to the most advanced AI voices, it's emotional.
4721600	4722600	You can feel it.
4722600	4723600	It tugs at something.
4723600	4726600	And again, this is a breakthrough that's literally a year old.
4726600	4731600	Let's talk about the ethics and morals.
4731600	4734600	Does AI have a moral compass?
4734600	4737600	Should it have a moral compass?
4737600	4740600	Well, I think the creators of AI, technology is not neutral.
4740600	4741600	Okay.
4741600	4746600	The creators of technology do have a responsibility, and they will never make it neutral because
4746600	4748600	it embodies their perspective.
4748600	4752600	And it bodies the data set and other biases and things like that.
4752600	4758600	So, I think that AI itself, this particular type of AI, again, if we just take the model,
4758600	4762600	it is the action upon the model that then leads to the output.
4762600	4764600	So, there's a responsibility there.
4764600	4766600	But then, like, how do we adapt the model?
4766600	4769600	Do we just have a one-and-done thing that's hardly trained on Western values and norms
4769600	4770600	and mores?
4770600	4774600	Which is the way it's been going historically in the large corporate setting?
4774600	4776600	Yeah, because there's nothing you can do.
4776600	4780600	Like, you can't build a Swahili version of it because you don't have access to technology.
4780600	4783600	Whereas now, with the pre-training and other things like that, you can do that with one
4783600	4784600	graphics card.
4784600	4785600	That's great.
4785600	4786600	Right?
4786600	4789600	Because, again, we've kind of flipped the paradigm of an AI needs to be going all the time to
4789600	4791600	this pre-compressed foundation model.
4791600	4796600	So, I think that, you know, then there is the things of, like, when you've got self-driving
4796600	4799600	cars and other things like that, what are the ethical norms, the trolley problem and
4799600	4801600	everything that you input on that?
4801600	4803600	These are not easy questions.
4803600	4806600	Because you're extending humanity, which then means you're also extending the ethics of
4806600	4809600	humanity, and that is not the same around the world.
4809600	4810600	Education.
4810600	4812600	One of your moonshots.
4812600	4818600	We first got to know each other through the Global Learning XPrize that Elon Musk and
4818600	4820600	Tony Robbins funded.
4820600	4823600	You were one of the leaders of the one billion team.
4823600	4824600	That wasn't either.
4824600	4828600	I just kind of helped implement it with Joe and the Imagine World idea.
4828600	4835600	And so, what, so, if you don't mind, what did your team do in that XPrize and then where
4835600	4837600	are you taking this vision?
4837600	4842600	So, my co-founder Joe and I, Joe led this, made Imagine Worldwide to take the winners of
4842600	4846600	the XPrize KitKat school and one billion, the real kind of champions, and implement it
4846600	4847600	around the world.
4847600	4848600	Yeah.
4848600	4850600	Into the Rohingya camps and Malawi and camps and others.
4850600	4853600	Like, I just support from the side and share him along as he kind of goes and does the
4853600	4855600	really hard, valuable work.
4855600	4859600	But now we've seen that, I believe, the latest stats from the randomized controlled trials
4859600	4861600	because you need to actually implement it as what happens.
4861600	4867600	I think 76% of kids in refugee camps learning literacy and numeracy in 13 months without
4867600	4868600	internet.
4868600	4869600	It's immense.
4869600	4873600	It's like one hour of use per day is equivalent of them being in school.
4873600	4874600	Pretty much, yeah.
4874600	4878600	Because, like, it's one teacher per 300 students, 400 students.
4878600	4879600	But then, is it enough?
4879600	4881600	No, it's not enough what we've done at the moment.
4881600	4886600	What needs to happen is there needs to be a big grand challenge whereby, you know, Malawi
4886600	4890600	and Malawi kind of has said that they want to roll this out at super scale and so have
4890600	4893600	multiple other governments now that we have the RCTs.
4893600	4897600	Let's make an open source ecosystem that has AI at the core that teaches kids and learns
4897600	4898600	from kids.
4898600	4902600	So you take from what's happening in Malawi, move it to Ethiopia, Sierra Leone, Rohingya
4902600	4904600	camps, Brooklyn, everywhere.
4904600	4912600	And so there is an actual superstar, amazingly well created ecosystem for education.
4912600	4915600	And again, go to the future and bring it back with us.
4915600	4916600	Love that.
4916600	4920600	This year at Abundance360, Sal Khan is going to be joining us as well.
4920600	4922600	Have you spent any time with Sal?
4922600	4923600	No, not yet.
4923600	4928600	Okay, so I'm excited to connect you guys because, you know, he's built something pretty extraordinary,
4928600	4933600	but his vision is to bring AI to it so that it's AI is generating the content, not him.
4933600	4938600	And it's able to rapidly iterate for cultural appropriateness and so forth.
4938600	4939600	This is why we need to build.
4939600	4944600	So one of our things is that we're building national level models from India to other ones
4944600	4946600	where there's localized data sets and other things.
4946600	4948600	If you can get the education piece going.
4948600	4953600	Remember how I said that this AI is like a kindergarten or a grade schooler?
4953600	4954600	Sure.
4954600	4955600	It matters what you teach it.
4955600	4957600	So right now we're teaching everything.
4957600	4958600	Do we need to teach everything?
4958600	4959600	No.
4959600	4963600	So if you have an AI that teaches the kids and learns from the kids, that's the best data
4963600	4968600	in the world to teach an AI for a Kenyan model or a Nigerian model or others.
4968600	4970600	And you know who should run that technology?
4970600	4974600	Nigerians and Kenyans and others.
4974600	4976600	And so one of the things we're doing is...
4976600	4979600	It's almost like family based learning and extrapolated from there.
4979600	4980600	Exactly, because we don't know best.
4980600	4987600	We can give tools and we've reduced the barriers to create national level localized cultural models.
4987600	4992600	And then those models together form a constellation that not only have you got base learning of
4992600	4996600	like what's the optimal way to teach an Algebra, getting better, then you can go beyond that to that.
4996600	5001600	And the plan is to have an integrated system where it's hardware, software, deployment curriculum.
5001600	5004600	Because then we can update that through mesh networking or the amazing work of Project Giga,
5004600	5009600	which is putting high speed internet into every school in the world by the UN.
5009600	5011600	And then you can put healthcare on that.
5011600	5012600	Yes.
5012600	5016600	And then you have a self-adaptive improving healthcare system, self-adaptive improving education system.
5016600	5017600	And then the world...
5017600	5023600	I mean, for me, that's the calling that I think both of us have and hopefully many entrepreneurs here.
5023600	5028600	It's like what greater purpose could you have in life than uplifting humanity in that fashion?
5028600	5029600	Exactly.
5029600	5034600	And then as an etc. manager, you can fund that at scale by bonds.
5034600	5035600	Yeah.
5035600	5037600	And the world's biggest problems are the world's biggest business opportunities, right?
5037600	5038600	Exactly.
5038600	5040600	Why to become a billionaire, help a billion people.
5040600	5045600	So one of the ways that we're kind of doing it is results-oriented bonds whereby you can pledge
5045600	5051600	$20 million for million kids are provably educated on this standardized architecture.
5051600	5053600	The invisible become visible.
5053600	5054600	And measurable.
5054600	5055600	Measurable.
5055600	5059600	Infrastructure banks and the World Bank fund the remainder held by the pension funds.
5059600	5062600	And you can divert billions and billions of dollars into this.
5062600	5066600	It's kind of the promise of one laptop a child can finally be done.
5066600	5070600	But rather than building a system for today, we build a system for tomorrow that constantly adapts
5070600	5075600	and improves is understandable and standardized because that is infrastructure.
5075600	5076600	Yes.
5076600	5079600	Again, the thing I really want to emphasize is this AI is infrastructure.
5079600	5081600	It's more important than 5G or anything else.
5081600	5082600	It's oxygen in the room.
5082600	5084600	It's oxygen in the room.
5084600	5085600	Yeah.
5085600	5089600	So when I think about the future of education going out 10, 20 years and bringing it back
5089600	5094600	today, for me, it's not a book and it's not a flat screen.
5094600	5095600	It's going into a virtual world.
5095600	5100600	If I want to learn about Plato, there's a guy sitting on a slab of marble over there.
5100600	5102600	And he says, Hey, Peter, come on over.
5102600	5103600	Let me show you around.
5103600	5105600	Introduce my friends.
5105600	5107600	And it's experiential.
5107600	5114600	And that that NPC of Plato is trained up by all the knowledge about Plato by every historian
5114600	5118600	and it's accurate and the imagery and so forth.
5118600	5125600	And what you just said earlier about real time rendering from stable diffusion enables
5125600	5126600	that, right?
5126600	5131600	And the ability to take every historian's work on Plato and train up a model on Plato
5131600	5132600	enables all that.
5132600	5133600	Exactly.
5133600	5138600	And particularly when it is at the hardware level, because typically what software happens
5138600	5144600	is that, again, you build layers and layers and layers of kind of compilers and translations
5144600	5146600	so you're far from the hardware.
5146600	5148600	Some models are already efficient at the top level.
5148600	5151600	What happens when we optimize them and push them down to the hardware level?
5151600	5152600	You don't need internet.
5152600	5153600	You don't need anything.
5153600	5154600	You can form it.
5154600	5157600	But then all of a sudden you have the young ladies illustrated primer.
5157600	5158600	Yes.
5158600	5163600	Neil Stevenson's incredible book and a vision for the future of education.
5163600	5164600	Exactly.
5164600	5166600	We can make it finally, but we can make it more.
5166600	5169600	We can make it closer to the prime radiance and foundation.
5169600	5171600	So are you building hardware, Imad?
5171600	5173600	Getting other people to build it for me.
5173600	5175600	My life is not a complicated.
5175600	5176600	We're setting the specs.
5176600	5177600	We're setting the specs.
5177600	5178600	But this is the thing.
5178600	5179600	Who is we?
5179600	5180600	Okay.
5180600	5185600	The way that we're going to do it is that it's similar to the grand challenges and the
5185600	5186600	prizes and things like that.
5186600	5187600	Let's get together.
5187600	5191600	We will drive the process because the rule by committee never works.
5191600	5196600	But let's invite everyone to participate from the kids using the tablets to code them
5196600	5199600	to the most advanced developers in the world.
5199600	5201600	And let's build something for humanity.
5201600	5203600	That's the way to do this.
5203600	5204600	Amazing.
5204600	5206600	So you don't like the term Web 3.
5206600	5210600	You wanted to jump over to Web 4.
5210600	5223600	But this virtualized world, which is the convergence of AI and VR, AR, blockchain and so forth,
5223600	5227600	where do you see it in the near term going?
5227600	5229600	It's just going to go insane.
5229600	5233600	And so like we have technology, I'm going to make an announcement in January about some
5233600	5234600	of our technology.
5234600	5240600	You've got Apple likely having AR glasses, snap, oculuses, all of this.
5240600	5243100	It'll be a fully immersive world where you can engage.
5243100	5246600	Like when I was a video game investor, I looked at time to flow, fun and frustration.
5246600	5247600	Yes.
5247600	5250600	And I think this technology can adjust all of those and create worlds for us.
5250600	5251600	But it'll be a year or two.
5251600	5257200	Because again, now it's percolating and it's getting ready to then create brand new experiences
5257200	5259080	on the fly for everyone.
5259080	5263800	And I think in a couple of years, it starts going in five years.
5263800	5267280	Creativity, imagination, engagement, entertainment is completely transformed.
5267280	5268280	Within five years.
5268280	5269280	Within five years.
5269280	5274040	It's going to be the biggest shift change that we've ever seen because the incumbents
5274040	5279720	can't keep up with entities that are using this technology that can do everything.
5279720	5286760	What does making a picture look like when you can change it live with just words?
5286760	5291560	And you can say, actually, make his hair a bit longer or get rid of those nostril hairs
5291560	5294760	and it just understands and does it without having to decode it.
5294760	5295760	Intentionality.
5295760	5296760	Yes.
5296760	5297760	Intentionality and action.
5297760	5298760	Yes.
5298760	5303240	It's kind of the whole military thing of observe oriented, decide and act, right?
5303240	5305880	These systems enable that almost flawlessly.
5305880	5310000	Like if you use OpenAI's whisper technology, the translation of the podcast is just immense
5310000	5314360	and all these other things because it's learned so many principles and they're tiny files.
5314360	5318480	And then you can make it self-referential and say, improve it the way you think I'd
5318480	5319480	want it to prove.
5319480	5320480	You can.
5320480	5322760	And like we're building technology, for example, that tells you how good a story is or how
5322760	5323760	good code is.
5323760	5324760	Yes.
5324760	5325760	So then you have a creation and a discriminator.
5325760	5329960	They bounce back and forth against each other and they learn from your personal context.
5329960	5330960	Amazing.
5330960	5337960	You took a break from being a hedge fund manager to address your son's autism, which was, I
5337960	5343520	mean, there are individuals like yourself, like Martin Rothblatt and others who are like,
5344520	5353200	I refuse to believe that something can't be done and you jumped in.
5353200	5359360	For those who are dealing with autism personally or in their families or in their community,
5359360	5362040	what were your learnings and what would you advise?
5362040	5366360	There's always room for improvement and everyone struggles with something.
5366360	5367360	How old is your son today?
5367360	5368360	He's 14.
5368360	5369360	14.
5369360	5370360	He's super happy.
5370680	5373680	I think they balance each other out, that's what they say.
5373680	5374680	You do it pretty well.
5374680	5375680	It's all right.
5375680	5380280	So I had a lot of issues around those things, but people, again, everyone's different and
5380280	5385920	then diversity is our strength as it were, but sometimes kids and others need help because
5385920	5388640	they can't achieve their potential because there's too much going on.
5388640	5392480	So like I said, I'm buying into the Gabberglute to make balance theory of it and we had some
5392480	5395760	drug repurposing and other stuff on an N of one equals case.
5395760	5399360	That's not science as it were, it's first principles and I don't think it's coherent
5399400	5401720	enough to be brought forward, but it's interesting.
5401720	5405200	Instead, I think there are certain things that benefit everyone, such as a blood behavioural
5405200	5410320	analysis, whereby because you haven't built up the words, there are short trials to try
5410320	5412680	and reconstruct what a cup means.
5412680	5416720	It's quite an intensive process, but it's also what people do after strokes and other
5416720	5420360	things that make them lose their connectivity in their brain.
5420360	5424080	Most of this thing though is about noise filtering and reduction of that, but I think
5424080	5428480	percolation of that means that you have to look into first principles, analysis of some
5428480	5431720	of the science of what can cause it, and then you have to bring that forward to what
5431720	5432720	is safe.
5432720	5436160	So don't do kind of crackpot theories, but there's an emerging science and studies showing
5436160	5441000	things like NSE til 16, sulfur refrain, other things that calm you down are probably the
5441000	5442000	best.
5442000	5445680	And a large part of it is just connection and engagement there.
5445680	5448600	So one of the things that we'll be doing next year is that we're taking all the research
5448600	5453160	that I've done and formalizing it properly, because again, it's not a case of, well, I
5453160	5455160	can do it, so anyone should have it.
5455160	5459480	I did an N equals one case, but then that should be extended out.
5459480	5463240	And this is why I realized when COVID came along.
5463240	5468160	So I designed and launched with the help of loads of people, collect from an augmented
5468160	5473520	intelligence against COVID-19, launched at Stanford in July of 2020.
5473520	5476800	Was that the actual origin of stability?
5476800	5478320	That was the first origin of stability.
5478320	5481280	So we didn't incorporate at that time, but kind of put it together.
5481280	5485800	I mean, it's insane how far things have gone in two years.
5485800	5486800	Yeah.
5486800	5490800	I mean, we probably actually kicked off in 13 months ago.
5490800	5493920	So yeah, it was insane because I thought, I saw it coming and I saw it like autism as
5493920	5499320	a multi-systemic inflammatory condition where even now, if anyone asks you, how does COVID
5499320	5500400	actually work?
5500400	5503040	If you ask a scientist, they'll tell you, we don't actually, sure.
5503040	5504040	Sure.
5504040	5506360	Like why are ferritin levels high and why is this and that?
5506360	5509400	The reality is our base foundation is not good enough.
5509400	5510520	We don't have enough shared knowledge.
5510520	5513520	So I created that to create a system that's comprehensive, authoritative and up-to-date.
5513520	5516640	So there's a nice blog about an OCDU type site and things like that.
5516640	5520520	A lot of private sector enterprises that promised a lot didn't deliver.
5520520	5524560	And so I realized, again, this technology was the future and we needed to create open
5524560	5525680	infrastructure for that.
5525680	5529880	So when we do our autism releases next year, all of the knowledge and everything like that
5529880	5530880	will be available.
5530880	5535920	There will be a semantic scholar on steroids, which allows you to access the information
5535920	5538640	relevant to the type of autism that your child might have.
5538640	5543480	So basically, we figured out there were 16 different etiologies while driving conditions.
5543480	5547680	But 30% of kids would get worse and 26% of kids would get better with the same treatment,
5547680	5548680	so it doesn't work.
5548680	5549680	Yeah.
5549680	5552840	This era of personalized medicine needs a foundation and again, that foundation must
5552840	5556480	be common, but we can't wait around to do it.
5556480	5557480	And it's interesting, right?
5557480	5564160	The more I learn about the fundamentals of human biology, the more complicated.
5564160	5566240	You can dig layer after layer after layer.
5566240	5567240	Yeah.
5567240	5573640	It's possible for, I mean, we're discovering so much because the tools we're using, but
5573640	5582080	we're going to need this level of AI to cognitively understand the interactions of the system.
5582080	5585600	Again, it's a complex hierarchical system, you know, classical Herb Simons style.
5585600	5588400	And so we have to build new tools to enable that.
5588400	5591200	The question is, are these tools closed and the company is trying to, are they open?
5591200	5593240	And so our take is open.
5593240	5596400	And then so our business model is just scale and service around that, which is how all
5596400	5599560	the servers and databases are, but open is secure as well.
5599560	5603080	That's why Linux is used everywhere versus Microsoft Windows, et cetera.
5603080	5606680	So I think the final part of this is that, you know, again, it's all interrelated.
5606680	5609640	Like the body is such a wonderful, powerful thing.
5609640	5613680	If you look at longevity and things like that, we need, again, this first principles thinking
5613680	5617640	to both make us healthy and live longer and be happier.
5617640	5619640	Yeah.
5619640	5622960	Have you thought much about what's coming down the pike with quantum technologies and
5622960	5623960	quantum computing?
5624000	5628240	So like, I think very sympathetic to various approaches there, like I'm quite, I quite
5628240	5633160	like the quantum annealing on the kind of D wave, because I think that, you know, Carl
5633160	5637800	Friston's theory around kind of free energy principle and having these low energy states
5637800	5638800	makes a lot of sense.
5638800	5642200	In fact, this is similar to what the AI models do in that when you look at the latent spaces,
5642200	5645520	you're going to the low energy states of what that could kind of mean.
5645520	5646520	Right.
5646520	5649080	So that's quite a lot of jargon, I think for a lot of lists.
5649080	5652320	But basically, quantum computing kind of is another part of the puzzle.
5652680	5656000	A lot of people try to take one AI model and say, Oh, we're going to put something
5656000	5657000	that can do everything.
5657000	5659400	You know, they look at AGI as the thing that can do everything.
5659400	5662480	The human brain is made up of so many different parts.
5662480	5665400	And we're just filling in some of the missing gaps right now of which quantum computing
5665400	5666400	is one of them.
5666400	5667400	Yeah.
5667400	5671640	And it's, I think it's adding fuel to the fire of how fast things are moving.
5671640	5672640	Ah, yeah.
5672640	5678240	It's going to be, you know, we can see the qubits increasing and we can see it getting
5678240	5679240	there.
5679240	5680720	I mean, again, part of this is a supercomputer thing, right?
5680720	5684640	Like our supercomputer, we've been the fastest in the world five years ago, which is insane
5684640	5685960	for a private company.
5685960	5686960	Right.
5686960	5687960	Absolutely.
5687960	5691400	You know, like think about bigger than everything.
5691400	5694520	But if you look at it like Nvidia is the one that kicked off because they put AI in the
5694520	5698280	core of their graphics cards before this even happened, which is why it's now a 32 billion
5698280	5700800	dollar part of their business.
5700800	5704640	The AV 100s, four years ago with the first iteration, the A 100s for the next.
5704640	5708040	Now the H 100s are like up to nine times faster.
5708040	5709560	It's literally going exponential.
5709560	5711600	The compute is going exponential.
5711600	5716400	The research, the technologies, we see exponential technologies everywhere and we're like, it's
5716400	5718400	so difficult to piece all these things together to see.
5718400	5721160	I don't even know where things will be in a year, let alone five years, or ten years.
5721160	5725480	I think that's an important part, and this is what, when Ray talks about the singularity,
5725480	5729280	you know, it's the notion that we're, you know, the ability to predict what's going
5729280	5732160	to happen next has become impossible.
5732160	5735480	And that timeframe, you know, people say, what's it going to be like in 20 years?
5735480	5739440	I can barely think about 10 years from now or five years from now, let alone 20 years
5739440	5740440	from now.
5740440	5741440	Yeah.
5741440	5742440	And this affects the way that we act.
5742440	5745320	So the way that our brains work is that our default is decision making under risk.
5745320	5749200	So we look at the upside down side of something and we make an expected utility calculation
5749200	5751640	based on that, because systems are stable.
5751640	5753160	Is the world stable now?
5753160	5754160	No.
5754160	5755160	Yeah.
5755160	5756160	You know, so what do we do instead?
5756160	5760640	We make decisions under uncertainty, which is we minimize for maximum regret, you know,
5760640	5763000	or actually we just minimize for regret.
5763000	5766240	So with these powerful technologies, people are like, don't give them out because I don't
5766240	5767240	know what can happen.
5767240	5769840	It's the, it's the, it's the amygdala.
5769840	5771440	It's a dystopian point of view.
5771440	5773960	It's minimizing our downside, protecting what we have.
5773960	5779920	It's a scarcity and fear based animal brain, you know, reptile brain that we default to.
5779920	5782080	And this happens synchronously.
5782080	5783080	That's what we saw with COVID.
5783080	5784080	It just happened.
5784080	5785920	All of a sudden, everyone thought the same, right?
5785920	5789240	It's what we're seeing with tech layoffs and things like that, like Facebook made loads
5789240	5790240	of money.
5790240	5791240	Why are they laying off?
5791240	5792320	You know, things like that.
5792320	5795640	And so we have to be kind of aware of this because what's happening now as well is that
5795640	5798080	the classical system has reached its end.
5798080	5800120	We've borrowed too much money from the future.
5800120	5803400	And now we are going to a negative sum game with inflation and other things.
5803400	5805320	People are losing wealth rapidly.
5805320	5808760	That leads to unstable Nash equilibrium from a game theoretic perspective.
5808760	5814680	So we go from one steady state and lurch to another inflation to deflation to maximum
5814680	5817240	employment to job losses to fiscal stuff.
5817240	5819360	And it's just going to be a crazy few years.
5819360	5821400	And that's the reason I called it stability.
5821400	5825600	I was going to, that was one of my questions where, you know, and obviously there's, there's
5825600	5831640	a terminology of stability in the, in the, in the models that you're building, but stability
5831640	5833520	in order to help stabilize society.
5833520	5834520	Yes.
5834520	5839200	Build a human OS or catalyze a human OS because it can only be built by society.
5839200	5840400	We're just a capitalist.
5840400	5841400	Yes.
5841400	5846760	Help guide that in a way because this AI can be finally the thing that can stabilize a
5846760	5850720	complex system that is humanity and then I'll just achieve our potential.
5850720	5855840	That's a platform, a infrastructure platform that uplifts all of humanity.
5855840	5856840	Yes.
5856840	5859000	And again, it should be run by the people for the people.
5859000	5862280	So like our subsidiaries in the countries, we're putting aside 10% of the equity for
5862280	5866040	the kids that use the tablets and I'm never going to take a penny out of any of them until
5866040	5868240	IPOing them because they should be owned by the people.
5868240	5869240	I love that.
5869240	5870240	Okay.
5870240	5878800	I want to close this out with two, two topics around the moonshots and mindsets.
5878800	5888840	If I were to launch an EMAID XPRIZE fully funded and push it out to the world, what's
5888840	5894360	a grand challenge in XPRIZE that you would love to see materialized out there?
5894360	5896240	I think this education is the key one.
5896240	5901600	I think it's the next step of global learning, which is a grand challenge just to build this
5901600	5903960	system and especially for low income countries.
5903960	5910280	And again, the delta on the impact can be so massive on this because it's infrastructure
5910280	5911280	for the next generation.
5911280	5916400	Like a lot of these emerging markets leapfrogged straight to mobile phones to skip computers.
5916400	5918920	Now they can skip to the AI age.
5918920	5919920	How amazing will that be?
5919920	5922880	It'll uplift everyone.
5922880	5929120	And there's no greater asset to a nation, a company, anybody than the intelligence of
5929120	5931720	its citizens.
5931720	5932720	Minds are intelligent.
5932720	5937160	They don't have access to be able to take that intelligence and build for themselves
5937160	5938160	and extend that.
5938160	5941720	We can make that infrastructure now to do it for the first time ever.
5941720	5942720	Yeah.
5942720	5943720	Amazing.
5943720	5944720	Amazing.
5944720	5947600	What are the mindsets that have allowed you to be successful, do you think?
5947600	5953880	I talk about a abundance mindset, a longevity mindset, a moonshot mindset, exponential mindset,
5953880	5954880	curiosity mindset.
5954880	5957800	Do any of those resonate for you or are there other mindsets?
5957800	5962040	Because I think mindsets are the most important differentiator that we have.
5962040	5966360	So like, I've always been very lucky and I've achieved very interesting things.
5966360	5970880	I was never really motivated for the last few years when I finally applied myself.
5970880	5974680	And what I'm good at is first principles thinking on those first bits rather than atoms.
5974680	5976880	But I realized there's nothing I can't do.
5976880	5980120	There's nothing, people can do anything if they put their mind to it.
5980120	5983640	But they have to think in a structured way and they have to let almost water flow as
5983640	5984640	it were.
5984640	5990040	And what I just do is I try to make it a win-win for everyone to participate, to help, to extend
5990040	5991040	this.
5991040	5994680	And at a time of absolute change, you can make that happen.
5994680	5997800	So this is why I go to the future and I go back to the past and I work back that way.
5997800	6000760	Which I think a lot of people don't because they get stuck in the present.
6000760	6006880	So it's the moonshot mindset looking to the future and again, recursively back propagating.
6006880	6012160	And I'll close out with, if you were going to list the moonshots that you're working
6012160	6013600	on right now.
6013600	6014880	You're clearly working on education.
6014880	6016760	We've talked about that.
6016760	6023840	But as someone who's going to disrupt all the industries, healthcare as a moonshot?
6023840	6026360	I am working on everything.
6026360	6027360	Everything.
6027360	6029400	I've got about 18 different ones.
6029400	6033480	But education is core and creativity is core.
6033480	6035320	Those two enable everything else.
6035320	6039800	If you want to fix climate, if you want to fix hate, if you want to fix a lot of different
6039800	6043320	things, get those two right and that's the foundation for the future.
6044040	6050160	It's a pleasure, my friend, with a fun conversation, excited to share your wisdom and vision with
6050160	6056920	the world, excited for what you're creating as a fundamental platform and infrastructure
6056920	6059440	for humanity.
6059440	6063200	I wish you all the luck and look forward to supporting me any way I can.
6063200	6064200	Thank you very much.
6064200	6065200	Again, it's just a little catalyst.
6065200	6067280	It'll be everyone else that drives this forward.
6067280	6068760	See you in March at A360.
6068760	6069760	Cheers.
6069760	6070760	Cheers.
6070760	6071760	Everyone, this is Peter again.
6071760	6075960	Before you take off, I want to take a moment to just invite you to subscribe to my weekly
6075960	6076960	tech blog.
6076960	6082200	Today, over 200,000 people received this email twice per week.
6082200	6086640	In the tech blog, I share with you my insights on converging exponential technologies, what's
6086640	6091280	going on in AI, how longevity is transforming, adding decades to our life.
6091280	6094880	In the tech blog, I often look at the 20 metatrends that are going to transform the
6094880	6100120	decade ahead and share the conversations I've had with incredible tech thought leaders on
6100120	6102040	how they're transforming industries.
6102040	6104600	If that sounds cool to you and you want to try it, join me.
6104600	6110160	Go to dmandus.com backslash blog, enter your email and let's start this weekly conversation.
6110160	6115280	Let me share with you the incredible progress we're seeing in the world of technology and
6115280	6117840	the positive impact it's having on our lives.
6117840	6122240	Again, that's dmandus.com backslash blog.
6122240	6126400	Looking forward to sharing my insights and incredible breakthroughs I'm seeing with you
6126400	6127240	every single week.
