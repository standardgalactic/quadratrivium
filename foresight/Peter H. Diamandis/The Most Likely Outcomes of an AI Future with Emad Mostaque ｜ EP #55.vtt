WEBVTT

00:00.000 --> 00:03.000
Coding is changing dramatically.

00:03.000 --> 00:06.000
There will be no code that doesn't use AI as post their workflow.

00:11.000 --> 00:15.000
5G and Starlink, the dry kindle for this fire has been set.

00:15.000 --> 00:18.000
Is this more important than 5G?

00:18.000 --> 00:19.000
By orders of magnitude.

00:19.000 --> 00:20.000
Orders of magnitude.

00:20.000 --> 00:25.000
There is no kind of pure independent, completely rational person because we're not robots.

00:25.000 --> 00:28.000
So medicine is changing dramatically.

00:28.000 --> 00:30.000
Is your doctor AI enhanced?

00:30.000 --> 00:32.000
Low insurance premium, lower copay.

00:32.000 --> 00:35.000
That is something across every industry that's going to be happening.

00:35.000 --> 00:37.000
10 years out, what is a lawyer?

00:37.000 --> 00:38.000
What is an accountant?

00:38.000 --> 00:40.000
What is an engineer?

00:40.000 --> 00:42.000
They are all AI assisted.

00:42.000 --> 00:44.000
The entire knowledge sector is transformed.

00:44.000 --> 00:47.000
It enables you to do whatever you want to do.

00:47.000 --> 00:50.000
Learning should be a place of positive growth and joy.

00:50.000 --> 00:51.000
Where's the difference there?

00:51.000 --> 00:52.000
Where's the difference?

00:52.000 --> 00:54.000
And I think the key thing here is empathy.

00:58.000 --> 01:02.000
We're in this beautiful home in the Hollywood Hills.

01:02.000 --> 01:08.000
And a lot's happened in the last six months since we were recording our last podcast.

01:08.000 --> 01:10.000
And you were on the stage at abundance 360.

01:10.000 --> 01:17.000
And I think this is epicenter for a lot of people's concerns right now.

01:17.000 --> 01:21.000
A transformation potentially in Holly.

01:21.000 --> 01:25.000
When you were speaking about that, that it's going to change dramatically.

01:26.000 --> 01:34.000
And would you say we haven't even seen a small portion of the change yet?

01:34.000 --> 01:38.000
I think we are at the foot of the mountain as it were.

01:38.000 --> 01:41.000
Kind of compare it to being at the iPhone 2G to 3G point.

01:41.000 --> 01:43.000
We just got copy and paste.

01:43.000 --> 01:47.000
We haven't seen what this technology is really capable of yet and it hasn't got everywhere.

01:47.000 --> 01:51.000
It's like everyone's talking about it, but not that many people are using it.

01:51.000 --> 01:59.000
There was a recent study done that showed only 17% of people had used chat GPT in the US.

01:59.000 --> 02:01.000
Despite the fact you can do anyone's homework.

02:01.000 --> 02:04.000
I mean, it seems incredibly a small percentage.

02:04.000 --> 02:08.000
I guess because the community I hang out with everybody's using it.

02:08.000 --> 02:09.000
Well, that's the thing.

02:09.000 --> 02:13.000
We live in our monocultures, but then a third of the world still doesn't have internet, right?

02:13.000 --> 02:17.000
And you think the first internet they get will probably be AI enhanced.

02:18.000 --> 02:20.000
And then you think about this technology proliferating.

02:20.000 --> 02:23.000
It's when it becomes enterprise ready.

02:23.000 --> 02:28.000
Enterprise adoption, company adoption always takes a while, but by next year that happens.

02:28.000 --> 02:34.000
And I think every company everywhere that has anything to do with knowledge work will implement this at scale.

02:34.000 --> 02:35.000
And that's a crazy thought.

02:35.000 --> 02:41.000
And when it's embedded in the things you use every day and you don't know it's part of what you're using.

02:41.000 --> 02:42.000
I think that's the thing.

02:42.000 --> 02:46.000
Technology, you don't need to know that it's 5G or the internet works faster.

02:46.000 --> 02:48.000
You can watch movies quicker.

02:48.000 --> 02:51.000
In this case, you write something in Google Docs.

02:51.000 --> 02:52.000
Now they just rolled it out.

02:52.000 --> 02:57.000
You can say, make this snappier and it will do it automatically.

02:57.000 --> 03:00.000
Technology doesn't need to be there as technology in the front.

03:00.000 --> 03:03.000
It's all about use cases and the use cases are now maturing.

03:03.000 --> 03:05.000
And again, I think next year is the real takeoff.

03:05.000 --> 03:06.000
But now everyone's feeling it.

03:06.000 --> 03:09.000
If there's anything to do with it, something is coming.

03:09.000 --> 03:15.000
In this conversation, I want to really think through how this is all affecting every industry.

03:15.000 --> 03:17.000
And let's start with two industries.

03:17.000 --> 03:23.000
One is journalism and the other is Hollywood, which is we're sitting in the midst of this.

03:23.000 --> 03:27.000
One of the concerns I have, I know you share it, a lot of people do,

03:27.000 --> 03:35.000
especially with elections coming up in 18 months, 2024, is what is truth?

03:35.000 --> 03:38.000
And are we going to enter a post-truth world?

03:38.000 --> 03:43.000
And can you talk about your thoughts on journalism and how AI is impacting it?

03:43.000 --> 03:46.000
So I think you've seen shifts in journalism over the years,

03:46.000 --> 03:51.000
but we're all familiar with kind of some of the clickbait journalism that we see now.

03:51.000 --> 03:54.000
AI can obviously do clickbait better than humans.

03:54.000 --> 03:56.000
And that's one kind of extreme.

03:56.000 --> 04:01.000
This whole fake news, deep fake kind of stuff, that's a real concern.

04:01.000 --> 04:04.000
And that's why we have authenticity mechanisms now.

04:04.000 --> 04:10.000
We embed watermarking, we partner with GPT-Zero and other things to identify AI.

04:10.000 --> 04:15.000
But on the other side of it, there's a real challenge coming because AI can also help with truth.

04:15.000 --> 04:20.000
It can help you do proper analysis and expand out the reasoning for things.

04:20.000 --> 04:22.000
It can identify biases within.

04:22.000 --> 04:25.000
So journalism as it stands is caught between two things.

04:25.000 --> 04:29.000
To get clicks, to get ads, they went a bit more clickbait

04:29.000 --> 04:33.000
and they focus on sensationalist headlines, even if it's with unnamed sources and things.

04:33.000 --> 04:38.000
On the other hand, someone's going to build an AI system, AI enhanced system,

04:38.000 --> 04:42.000
that for any article you read, you can find all the background material

04:42.000 --> 04:45.000
and that suddenly becomes a source of truth, so it's kind of a pincer movement.

04:45.000 --> 04:50.000
And journalists and news sources will have to figure out where are you in this?

04:50.000 --> 04:52.000
How do you compete to provide value?

04:52.000 --> 04:58.000
Yeah, are you buzzfeed on one end, which is mostly all clickbait all the time,

04:58.000 --> 05:03.000
or are you trying to be the New York Times and deliver well-researched journalism?

05:03.000 --> 05:07.000
But the entity that competes with the New York Times that will come,

05:07.000 --> 05:09.000
and who knows, it might be the New York Times itself,

05:09.000 --> 05:12.000
can use AI to enhance great journalism,

05:12.000 --> 05:14.000
write in any voice, do all these things,

05:14.000 --> 05:17.000
and give fully reference facts that you can explore.

05:17.000 --> 05:20.000
The other side is that we're going to trust this technology more and more,

05:20.000 --> 05:23.000
just like we trust Google Maps, just like you trust other things,

05:23.000 --> 05:27.000
such that it's like, why have I got a human doctor without an AI?

05:27.000 --> 05:32.000
Why have I got a journalist who isn't using AI to check everything and their own implicit biases?

05:32.000 --> 05:36.000
And I think that part is actually quite misunderstood as to something that's coming,

05:36.000 --> 05:41.000
because, again, humans plus AI outcompete humans without AI.

05:41.000 --> 05:43.000
I believe in that, and I see that.

05:43.000 --> 05:47.000
And I think it's interesting in my life, and those I know,

05:47.000 --> 05:50.000
AI hasn't replaced the things that I've done.

05:50.000 --> 05:53.000
It hasn't actually even saved me time per se,

05:53.000 --> 05:55.000
because I'm still spending the same amount of time.

05:55.000 --> 06:00.000
It's allowed me to do a better job at what I want to do, which is the end product.

06:00.000 --> 06:04.000
I mean, that's because you do a little bit of everything, so you always fill the gap.

06:04.000 --> 06:08.000
True, I fill every moment of the time creating something for one of my companies.

06:08.000 --> 06:11.000
Well, I mean, this is an open AI report that was done,

06:11.000 --> 06:18.000
where they said that between 14% and 50% of tasks will be augmented by AI,

06:18.000 --> 06:21.000
will be changed by AI, because, again,

06:21.000 --> 06:25.000
I think a lot of the focus is on these automated systems, the terminators,

06:26.000 --> 06:30.000
to the bots, whereas realistically, the way this AI will come in

06:30.000 --> 06:35.000
is to help us with individual tasks, rewriting something, generating an image,

06:35.000 --> 06:38.000
making a song, adjusting your speech to sound more confident.

06:38.000 --> 06:42.000
Yeah, and we'll get to the dystopian conversation,

06:42.000 --> 06:47.000
because I'd like to hear what you think is real versus hype.

06:47.000 --> 06:51.000
I think the audience needs to understand what should they truly be concerned about,

06:51.000 --> 06:53.000
and what shouldn't they...

06:53.000 --> 06:59.000
I mean, that is being able to trace back and have a truth mechanism.

06:59.000 --> 07:04.000
We can talk about what Elon's looking to build as well on the truth side,

07:04.000 --> 07:10.000
but it's fascinating when the truth becomes blurry.

07:10.000 --> 07:13.000
Yeah, and there's not always an objective truth,

07:13.000 --> 07:15.000
because it depends upon your individual context, right?

07:15.000 --> 07:16.000
Yeah.

07:16.000 --> 07:19.000
And we didn't have the systems to be able to be comprehensive,

07:19.000 --> 07:23.000
authoritative, or up-to-date enough to do that until today.

07:23.000 --> 07:31.000
Well, we can actually go to the root source of the data and see, is it valid?

07:31.000 --> 07:35.000
Maybe it's a blockchain-enabled validation mechanism.

07:35.000 --> 07:38.000
Maybe it's got that authority, that authentication.

07:38.000 --> 07:39.000
Maybe it's...

07:39.000 --> 07:43.000
We mentioned Elon Community Notes on Twitter that AI enhanced,

07:43.000 --> 07:46.000
that can pull from various things and show the provenance.

07:46.000 --> 07:49.000
So you've got provenance, again, you've got authority,

07:49.000 --> 07:51.000
you have comprehensiveness, you have up-to-dateness.

07:51.000 --> 07:55.000
The future of Wikipedia is not what Wikipedia looks like today,

07:55.000 --> 07:59.000
but that future becomes something that can be integrated into other things.

07:59.000 --> 08:02.000
So what you'll have is, for any piece of information,

08:02.000 --> 08:05.000
you'll be able to say, this is the bias from which it was said,

08:05.000 --> 08:08.000
these are the compositional sources and more.

08:08.000 --> 08:12.000
So for example, there's a great app that I use called Perplexity AI.

08:12.000 --> 08:13.000
Okay.

08:13.000 --> 08:17.000
So when you go to GPT-4 or Bing, you write stuff,

08:17.000 --> 08:18.000
it doesn't give you all the sources,

08:18.000 --> 08:22.000
Perplexity actually brings in all the sources at a surface level,

08:22.000 --> 08:26.000
but it references why it said certain things with GPT-4.

08:26.000 --> 08:28.000
That's just going to get more and more advanced

08:28.000 --> 08:30.000
so you can dig into as much depth as you want

08:30.000 --> 08:32.000
and ask it to rephrase things as,

08:32.000 --> 08:35.000
what if that article there wasn't true that fed this?

08:35.000 --> 08:39.000
Or what about this perspective if I want it to be a bit more libertarian?

08:39.000 --> 08:44.000
Do you think it's possible to actually get to a fundamental truth

08:44.000 --> 08:45.000
in a lot of these areas?

08:45.000 --> 08:47.000
I think it depends on the area, right?

08:47.000 --> 08:48.000
Some areas there are fundamental truths.

08:48.000 --> 08:50.000
This happened or it didn't happen,

08:50.000 --> 08:53.000
even though you see deniers of various things.

08:53.000 --> 08:56.000
A lot of stuff is probabilistic when you're thinking about the future.

08:56.000 --> 08:58.000
But even something like climate,

08:58.000 --> 09:02.000
you see a lot of deniers of the real problem that we have

09:02.000 --> 09:04.000
with it being very difficult to persuade them

09:04.000 --> 09:07.000
because it becomes part of their ideology almost.

09:07.000 --> 09:09.000
But with this technology, you can say,

09:09.000 --> 09:11.000
look, literally here is the comprehensiveness.

09:11.000 --> 09:13.000
So like Jeremy Howard and Trisha Greenley

09:13.000 --> 09:17.000
did an analysis of well over 100 mask papers

09:17.000 --> 09:21.000
and did a meta-analysis on the effectiveness of that for COVID.

09:21.000 --> 09:24.000
And then that helped change the global discussion on masking

09:24.000 --> 09:27.000
because I'm actually bothered to do a comprehensive analysis.

09:27.000 --> 09:28.000
What was the result?

09:28.000 --> 09:31.000
Well, the result was that masks work for respiratory diseases.

09:31.000 --> 09:36.000
There are so many people that just refuse to believe that masks have any value.

09:36.000 --> 09:38.000
But let's not go down that road.

09:38.000 --> 09:43.000
One of the things I found interesting was the idea of a GPT model

09:43.000 --> 09:46.000
being able to translate your points of view

09:46.000 --> 09:49.000
for someone else to make, to receive them better.

09:49.000 --> 09:51.000
Like if you're hardcore to the right

09:51.000 --> 09:55.000
and you want to convince someone about your issue,

09:55.000 --> 10:01.000
having chat GPT or one of Stability's products

10:01.000 --> 10:04.000
generate a rewritten version of that language

10:04.000 --> 10:07.000
so the person can hear it better.

10:07.000 --> 10:09.000
I find that an interesting and powerful tool.

10:09.000 --> 10:11.000
Yeah, I think this is the thing.

10:11.000 --> 10:14.000
It's all about your individual context and what resonates with you

10:14.000 --> 10:17.000
because information exists within a context.

10:17.000 --> 10:19.000
So if it's going to change the state within you,

10:19.000 --> 10:21.000
you need to understand your point of view.

10:21.000 --> 10:23.000
So if we think of these as really talented youngsters,

10:23.000 --> 10:26.000
these AIs that go a bit funny,

10:26.000 --> 10:27.000
what would you want?

10:27.000 --> 10:29.000
You would want someone to sit down and say,

10:29.000 --> 10:31.000
this is your point of view in your context

10:31.000 --> 10:33.000
and my point of view in my context.

10:33.000 --> 10:36.000
Let's find some common ground and then we can work from there.

10:36.000 --> 10:38.000
Much of politics isn't really about facts.

10:38.000 --> 10:41.000
It's about persuasion because facts,

10:41.000 --> 10:44.000
when you have diametrically a divergent context,

10:44.000 --> 10:46.000
are very difficult to do.

10:46.000 --> 10:48.000
So you said being able to rewrite something from one context

10:48.000 --> 10:50.000
to another is important,

10:50.000 --> 10:52.000
but then you have to understand the context

10:52.000 --> 10:54.000
and that's what these models do really, really well.

10:54.000 --> 10:57.000
We can take a piece and we can say,

10:57.000 --> 10:59.000
write it as a wrap by Eminem

10:59.000 --> 11:03.000
or in the style of Ulysses by James Joyce

11:03.000 --> 11:06.000
and it will do that because it understands the essence of that.

11:06.000 --> 11:09.000
I think people don't realize,

11:09.000 --> 11:11.000
and when I just hit this point again,

11:11.000 --> 11:13.000
we've talked about it somewhat,

11:13.000 --> 11:15.000
our minds are neural nets,

11:15.000 --> 11:16.000
our brains are neural nets,

11:16.000 --> 11:18.000
100 trillion neurons

11:18.000 --> 11:20.000
and everything that you bring into your mind,

11:20.000 --> 11:22.000
this conversation that we're having,

11:22.000 --> 11:24.000
what you're watching on the TV news,

11:24.000 --> 11:26.000
newspaper, what's on your walls,

11:26.000 --> 11:28.000
the people you hang out with are all constantly shaping

11:28.000 --> 11:32.000
the way you see the world and shaping your mindset.

11:32.000 --> 11:36.000
It's one of the things I think about in the future of news media

11:36.000 --> 11:40.000
is an individual actively being able to choose

11:40.000 --> 11:42.000
what mindset they want to work on.

11:42.000 --> 11:45.000
I'd like to have a more optimistic mindset,

11:45.000 --> 11:47.000
I'd like to have a more moonshot mindset,

11:47.000 --> 11:49.000
a more an abundance mindset

11:49.000 --> 11:52.000
and then being able to have that information fed to you

11:52.000 --> 11:55.000
in a factual fashion that allows you to,

11:55.000 --> 11:58.000
instead of what the crisis news network delivers,

11:58.000 --> 12:01.000
which is a constantly negative news.

12:01.000 --> 12:03.000
It's caused the dopamine effect

12:03.000 --> 12:05.000
in your fight or flight response.

12:05.000 --> 12:08.000
You can say make it from this point of view,

12:08.000 --> 12:09.000
you don't change the facts,

12:09.000 --> 12:11.000
but even the way things are worded.

12:11.000 --> 12:13.000
Or balancing, right?

12:13.000 --> 12:15.000
I mean, I don't need to see every murder,

12:15.000 --> 12:16.000
tell me what the companies have got funded,

12:16.000 --> 12:18.000
tell me the breakthroughs that occurred,

12:18.000 --> 12:20.000
the science that's occurred today.

12:20.000 --> 12:22.000
But there's even, again, everything you can,

12:22.000 --> 12:24.000
it depends on how you portray it, right?

12:24.000 --> 12:27.000
You know, there are murders, I take a murder, for example.

12:27.000 --> 12:29.000
There is the facts, there is the,

12:29.000 --> 12:32.000
oh my God, there'll be a million more murders.

12:32.000 --> 12:34.000
There is the case that it's a very sad thing.

12:34.000 --> 12:36.000
There's the case that, you know,

12:36.000 --> 12:39.000
the police are working super hard to solve this

12:39.000 --> 12:40.000
and we need to reach out to the families

12:40.000 --> 12:42.000
and come together as a community.

12:42.000 --> 12:43.000
These are all different aspects

12:43.000 --> 12:45.000
for the same terrible action.

12:45.000 --> 12:46.000
Yes.

12:46.000 --> 12:48.000
Which have different levels of positivity,

12:48.000 --> 12:52.000
negativity, clickbaitiness versus community, right?

12:52.000 --> 12:55.000
Facebook did a study many years ago

12:55.000 --> 12:57.000
whereby they had a hypothesis.

12:57.000 --> 13:00.000
If you see sadder things on your timeline,

13:00.000 --> 13:03.000
you'll post sadder things.

13:03.000 --> 13:06.000
This is why independent review boards

13:06.000 --> 13:08.000
are very important in ethics as well.

13:08.000 --> 13:10.000
And so they did it and guess what?

13:10.000 --> 13:12.000
600,000 users were enrolled

13:12.000 --> 13:14.000
in that study without their knowing.

13:14.000 --> 13:16.000
If you see sadder things on your timeline,

13:16.000 --> 13:19.000
you post sadder things was the result.

13:19.000 --> 13:21.000
So like I said, there's some real ethical considerations

13:21.000 --> 13:22.000
but we know this.

13:22.000 --> 13:25.000
We know that if we're always bombarded by crisis,

13:25.000 --> 13:27.000
we will be in a crisis mentality.

13:27.000 --> 13:29.000
We know if we surround ourselves with positive people

13:29.000 --> 13:31.000
and positive messages,

13:31.000 --> 13:33.000
we will have a positive mentality.

13:33.000 --> 13:36.000
And it's very insidious and not insidious.

13:36.000 --> 13:38.000
It's kind of almost passive the way we absorb it.

13:38.000 --> 13:40.000
I love one of the facts.

13:40.000 --> 13:44.000
I'm writing one of my next books on longevity practices

13:44.000 --> 13:49.000
and a study on the order of 20,000 individuals.

13:49.000 --> 13:53.000
Those who had an optimistic mindset

13:53.000 --> 13:55.000
lived in average of 17% longer.

13:55.000 --> 13:58.000
I mean, just your mindset shift.

13:58.000 --> 13:59.000
17%.

13:59.000 --> 14:01.000
This was true both in men and in women,

14:01.000 --> 14:03.000
slightly more in women.

14:03.000 --> 14:05.000
And so how you think impacts everything

14:05.000 --> 14:09.000
and how you think is to a large degree

14:09.000 --> 14:12.000
going to be shaped by the media

14:12.000 --> 14:14.000
and AI is going to shape that.

14:14.000 --> 14:17.000
So it's a powerful lever

14:17.000 --> 14:19.000
that we all need to be paying attention to.

14:19.000 --> 14:20.000
I think it is.

14:20.000 --> 14:22.000
But then you have to consider

14:22.000 --> 14:26.000
what is the plasticity, for example, of our children

14:26.000 --> 14:28.000
as they grow up?

14:28.000 --> 14:30.000
We're going to have nanny AIs.

14:30.000 --> 14:32.000
What's that nanny going to teach?

14:32.000 --> 14:35.000
Is that nanny going to teach flight, happiness, this, that?

14:35.000 --> 14:37.000
What about in places like China?

14:37.000 --> 14:39.000
What are they going to teach?

14:39.000 --> 14:41.000
There is a huge amount of neuroplasticity

14:41.000 --> 14:45.000
that will be influenced by decisions we make today.

14:45.000 --> 14:46.000
Yeah.

14:46.000 --> 14:48.000
I mean, listen, you have young kids.

14:48.000 --> 14:49.000
I have young kids as well.

14:49.000 --> 14:51.000
And I think about the fact that school today

14:51.000 --> 14:55.000
is not preparing our kids anywhere near for the future.

14:55.000 --> 14:56.000
Right?

14:56.000 --> 14:59.000
I mean, I don't think middle school and high school

14:59.000 --> 15:02.000
traditionally is preparing them for...

15:02.000 --> 15:04.000
My kids are 12 right now.

15:04.000 --> 15:05.000
Yeah.

15:05.000 --> 15:06.000
How do you feel about that?

15:06.000 --> 15:08.000
No, I mean, I think school is not fit for purpose.

15:08.000 --> 15:10.000
It's a Petri dish social status game

15:10.000 --> 15:13.000
and, you know, like childcare.

15:13.000 --> 15:15.000
Because again, let's think about school.

15:15.000 --> 15:16.000
What do you...

15:16.000 --> 15:17.000
What are you taught at school?

15:17.000 --> 15:18.000
You're taught competitive tests

15:18.000 --> 15:20.000
because we can't capture the context of the kids.

15:20.000 --> 15:21.000
Right.

15:21.000 --> 15:23.000
We can't adapt to if they're visual learners,

15:23.000 --> 15:26.000
auditory learners, dyslexic or otherwise.

15:26.000 --> 15:28.000
And it narrows it down

15:28.000 --> 15:30.000
and you're told you cannot be creative.

15:30.000 --> 15:31.000
You're told you...

15:31.000 --> 15:33.000
Color inside the lines.

15:33.000 --> 15:34.000
Learn these facts.

15:34.000 --> 15:35.000
Literally, you color inside the lines.

15:35.000 --> 15:37.000
You learn these facts.

15:37.000 --> 15:40.000
Every child will have an AI in the West.

15:40.000 --> 15:41.000
Yes.

15:41.000 --> 15:42.000
Hopefully soon in the world.

15:42.000 --> 15:44.000
We'll have an AI with them as they grow up.

15:44.000 --> 15:48.000
And again, is that AI a positive constructive

15:48.000 --> 15:51.000
or is the AI a tiger AI?

15:51.000 --> 15:52.000
Right.

15:52.000 --> 15:53.000
That kind of is aggressive.

15:53.000 --> 15:54.000
Get your work done.

15:54.000 --> 15:55.000
Get your work done.

15:55.000 --> 15:56.000
Strive harder.

15:56.000 --> 15:58.000
Is it palaton AI for education?

15:58.000 --> 16:00.000
Maybe that's the pivot for palaton, right?

16:00.000 --> 16:01.000
There's a whole range of things,

16:01.000 --> 16:03.000
but our kids are so sensitive as they grow.

16:03.000 --> 16:05.000
And again, in a school environment,

16:05.000 --> 16:07.000
they're told they have to be competitive

16:07.000 --> 16:08.000
and there's only a few people

16:08.000 --> 16:10.000
that are worthy here at the top.

16:10.000 --> 16:12.000
And that's why you have clicks, sub-clicks, and others,

16:12.000 --> 16:15.000
and that's reinforced by our social media now as well

16:15.000 --> 16:17.000
because you need something to fill the meaning.

16:17.000 --> 16:19.000
I think we have to be much more intentional and think,

16:19.000 --> 16:22.000
what information do we want going to our children?

16:22.000 --> 16:24.000
Like many people listening to this podcast

16:24.000 --> 16:27.000
will have banned social media from our kids.

16:27.000 --> 16:29.000
How do you feel about that?

16:29.000 --> 16:31.000
I think that is probably a sensible thing

16:31.000 --> 16:35.000
because it's a slow-down AI that optimizes for adverse things.

16:35.000 --> 16:38.000
And again, it's not the fault of the social media companies,

16:38.000 --> 16:42.000
it's just how they are as the tiger and the scorpion.

16:42.000 --> 16:43.000
Yeah.

16:43.000 --> 16:45.000
I mean, I have not allowed my kids to have a mobile phone

16:45.000 --> 16:47.000
and I've told them when they can afford it,

16:47.000 --> 16:48.000
when they go to college,

16:48.000 --> 16:50.000
but it's going to be somewhere between now and then.

16:50.000 --> 16:51.000
But I agree.

16:51.000 --> 16:54.000
I think social media shouldn't be part of the repertoire.

16:54.000 --> 16:56.000
But again, what is social media?

16:56.000 --> 16:58.000
It's kids looking for status

16:58.000 --> 17:01.000
and trying to influence each other in that case.

17:01.000 --> 17:04.000
It was meant to bring our communities together stronger.

17:04.000 --> 17:06.000
Yeah, maybe perhaps early on.

17:07.000 --> 17:09.000
Probably what we see the strongest community is actually

17:09.000 --> 17:12.000
in video games and guilds and kind of things like that.

17:12.000 --> 17:14.000
A lot of this is, again,

17:14.000 --> 17:16.000
you've got X number of people posting positive things

17:16.000 --> 17:19.000
and you're like, why is my life not positive like that?

17:19.000 --> 17:21.000
But social media does have its advantages.

17:21.000 --> 17:24.000
The question is, can you tease out the positive versus the negative

17:24.000 --> 17:27.000
when you can finally customize it for each individual

17:27.000 --> 17:30.000
or are you going to reinforce silos to the nth degree?

17:30.000 --> 17:32.000
I'll give you a perfect example.

17:32.000 --> 17:34.000
I was just meeting with a dear friend of mine, Keith Farazi,

17:34.000 --> 17:36.000
who is absolutely brilliant.

17:36.000 --> 17:39.000
And he had met last week with the King of Bhutan,

17:39.000 --> 17:42.000
which is known for its happiness.

17:42.000 --> 17:43.000
And they were having a conversation.

17:43.000 --> 17:44.000
How they measure their economy.

17:44.000 --> 17:46.000
It's how they measure their economy,

17:46.000 --> 17:48.000
gross national happiness in that regard.

17:48.000 --> 17:53.000
And when social media entered the country,

17:53.000 --> 17:55.000
it began to plummet.

17:55.000 --> 17:59.000
Teen depression and suicides began to climb.

17:59.000 --> 18:02.000
It is a very measurable real thing.

18:02.000 --> 18:05.000
And that's not the subject of this podcast,

18:05.000 --> 18:08.000
but AI can do what for that area?

18:08.000 --> 18:10.000
Well, I mean, again,

18:10.000 --> 18:13.000
we have to think about it in terms of mental infrastructure.

18:13.000 --> 18:14.000
I like that.

18:14.000 --> 18:18.000
We don't have enough, like Clayton Christensen said,

18:18.000 --> 18:20.000
infrastructure is the most efficient means

18:20.000 --> 18:23.000
by which a society stores and distributes value.

18:23.000 --> 18:25.000
Claude Shannon, the father of computer science said,

18:25.000 --> 18:28.000
information is valuable in as much as it changes the state.

18:28.000 --> 18:32.000
We do not think at all about our mental infrastructure

18:32.000 --> 18:34.000
and what's supporting it.

18:34.000 --> 18:37.000
If we're lucky or if, you know, we try hard,

18:37.000 --> 18:40.000
we can build a group of supportive people around us.

18:40.000 --> 18:42.000
And where do we go when we have issues?

18:42.000 --> 18:43.000
We go to that group.

18:43.000 --> 18:46.000
Yet so many people feel alone, you know,

18:46.000 --> 18:49.000
so many people feel like, again, this rise in suicide

18:49.000 --> 18:51.000
or they feel not good enough

18:51.000 --> 18:54.000
because it serves the slow, dumb AI of our existing systems.

18:54.000 --> 18:58.000
So I think that, actually, we just take some time out to think.

18:58.000 --> 19:02.000
What information do I want going to my kids?

19:02.000 --> 19:05.000
We have concepts like deliberative democracy,

19:05.000 --> 19:07.000
whereby you get a group of diverse people

19:07.000 --> 19:10.000
from different backgrounds, you give them the facts,

19:10.000 --> 19:13.000
and they go and make a decision, just like you have jury trials.

19:13.000 --> 19:16.000
You know, one of the most important things I think there is,

19:16.000 --> 19:19.000
A, it's getting understanding the context of each person,

19:19.000 --> 19:20.000
which I think AI can answer it.

19:20.000 --> 19:23.000
B, it's just actually literally having time to think.

19:23.000 --> 19:26.000
When was the last time you thought about your information diet

19:26.000 --> 19:28.000
and what you're feeding yourself and your kids?

19:28.000 --> 19:29.000
I think about it a lot.

19:29.000 --> 19:31.000
I think because that's what I teach.

19:31.000 --> 19:32.000
So I'm very clear.

19:32.000 --> 19:34.000
I do not watch the TV news.

19:34.000 --> 19:35.000
I don't even watch the newspapers.

19:35.000 --> 19:38.000
I have very filtered information that comes to me,

19:38.000 --> 19:41.000
which could be argued to be an echo chamber,

19:41.000 --> 19:46.000
but you know, I'm focused on these are the scientific breakthroughs.

19:46.000 --> 19:47.000
This is what's going on in longevity.

19:47.000 --> 19:50.000
This what's going on in exponential tech and solving problems.

19:50.000 --> 19:55.000
So I'm as critical about what I take into my mind

19:55.000 --> 20:00.000
from an information source as I am what I eat.

20:00.000 --> 20:02.000
Because you are what you eat and you eat information

20:02.000 --> 20:04.000
and then you absorb it, right?

20:04.000 --> 20:07.000
But then, you know, as you said, the echo chamber thing,

20:07.000 --> 20:12.000
I believe we should also deliberately show counterpart viewpoints

20:12.000 --> 20:16.000
as we're raising our kids and get them to argue the opposite.

20:16.000 --> 20:18.000
I think debate is one of the most beautiful forms,

20:18.000 --> 20:19.000
especially when you flip sides.

20:19.000 --> 20:23.000
Organisms also grow through hysteresis.

20:23.000 --> 20:24.000
Yes.

20:24.000 --> 20:26.000
You know, when you're put under pressure,

20:26.000 --> 20:28.000
when you're forced to do something out of the normal.

20:28.000 --> 20:31.000
Otherwise, as you said, you'll become increasingly siloed.

20:31.000 --> 20:35.000
But there are very few people again who think deliberately about this.

20:35.000 --> 20:37.000
And it's something, again, I think you and I

20:37.000 --> 20:40.000
probably urge all the listeners to think about,

20:40.000 --> 20:42.000
are you challenging your priors?

20:42.000 --> 20:46.000
Are you giving the right information diet for yourself, for your kids?

20:46.000 --> 20:47.000
Yes.

20:47.000 --> 20:49.000
And then thinking about this technology,

20:49.000 --> 20:52.000
as you use a GPT-4 or Claude or Stable-LM,

20:52.000 --> 20:54.000
how any of these things,

20:54.000 --> 20:57.000
what if you took the article and viewed it from a different perspective?

20:57.000 --> 21:00.000
Or what if you tied it to only be positive, the news?

21:00.000 --> 21:04.000
There's another part too, which is we all have these cognitive biases, right?

21:04.000 --> 21:06.000
These cognitive biases were wired into our brain

21:06.000 --> 21:09.000
over the last, you know, hundreds of thousands of years

21:09.000 --> 21:11.000
as an energy efficiency mechanism

21:11.000 --> 21:14.000
because we can't process all the information coming in.

21:14.000 --> 21:18.000
So we're biased by, is the person look like me, speak like me?

21:18.000 --> 21:21.000
Is this recent information versus old information

21:21.000 --> 21:25.000
paying 10 times more attention to negative information than positive information?

21:25.000 --> 21:31.000
I can't wait to have an AI that I can flip the switch

21:31.000 --> 21:34.000
and say turn on bias notifications.

21:34.000 --> 21:38.000
And it says you're looking at this in a biased fashion, Peter.

21:38.000 --> 21:40.000
Here's another way to look at it.

21:40.000 --> 21:42.000
Yeah. And, you know, being aware of your bias,

21:42.000 --> 21:45.000
most religions have at the core know thyself.

21:45.000 --> 21:47.000
Yes. The nurses have done,

21:47.000 --> 21:50.000
it's the ancient Greek, you know, I wrote my college essay on that.

21:50.000 --> 21:52.000
But I mean, that's why it's at the core.

21:52.000 --> 21:56.000
It's very difficult when you have the detritus of life

21:56.000 --> 21:59.000
and all these things you're bombarded with to take time back

21:59.000 --> 22:02.000
and really know yourself, know your own biases, understand these,

22:02.000 --> 22:05.000
because they are part of what makes you, you're made up of the stories.

22:05.000 --> 22:09.000
There is no kind of pure, independent, completely rational person

22:09.000 --> 22:11.000
because we're not robots.

22:11.000 --> 22:14.000
So it's possible in the future then for social media

22:14.000 --> 22:19.000
with a more conscious, powerful AI,

22:19.000 --> 22:21.000
I shouldn't use the word conscious as a different meaning here,

22:21.000 --> 22:25.000
but AI that you feel safe having your kids do it

22:25.000 --> 22:28.000
because it is making them happier and making them more motivated.

22:28.000 --> 22:31.000
It is feeding them a flow of information

22:31.000 --> 22:33.000
that's uplifting versus depressing.

22:33.000 --> 22:35.000
Can you imagine that future?

22:35.000 --> 22:36.000
I can imagine that future.

22:36.000 --> 22:39.000
I can also imagine the future of Brave New World

22:40.000 --> 22:43.000
whereby you are fed what exactly the government wants you to

22:43.000 --> 22:46.000
and you are happy, and especially with authoritarian regimes,

22:46.000 --> 22:50.000
you are literally the kids are grown with their AI nannies.

22:50.000 --> 22:51.000
Of course.

22:51.000 --> 22:54.000
And you even have the pharmaceuticals to make you extra happy

22:54.000 --> 22:56.000
and extra neuroplastic.

22:56.000 --> 23:01.000
So for example, you have UAE did a Falcon model open source.

23:01.000 --> 23:05.000
It was kind of supported by Leiton from France, technologically.

23:05.000 --> 23:08.000
You ask it about the UAE and it's like it's a wonderful place.

23:08.000 --> 23:10.000
It's amazing in all regards.

23:10.000 --> 23:12.000
You ask it about some of the neighbors.

23:12.000 --> 23:13.000
It's not so nice.

23:13.000 --> 23:15.000
This is an inherent bias within the model,

23:15.000 --> 23:18.000
but how you can understand it versus an implicit bias

23:18.000 --> 23:20.000
and you can put any bias as you want.

23:20.000 --> 23:22.000
You can guide these models through reinforcement learning

23:22.000 --> 23:24.000
to reflect what you do.

23:24.000 --> 23:26.000
And if that's the only option,

23:26.000 --> 23:29.000
then you will adhere to a certain world view again,

23:29.000 --> 23:30.000
almost subconsciously.

23:30.000 --> 23:33.000
It'll be reflected in all the products you produce

23:33.000 --> 23:35.000
all the writings you have.

23:35.000 --> 23:39.000
And it doesn't have to be that higher percentage change,

23:39.000 --> 23:43.000
a small persistent change sways a lot.

23:43.000 --> 23:44.000
Well, exactly.

23:44.000 --> 23:46.000
I mean, like half the world is religious.

23:46.000 --> 23:48.000
You can agree or not,

23:48.000 --> 23:50.000
or say it follows a organized religion.

23:50.000 --> 23:52.000
You can agree or disagree,

23:52.000 --> 23:56.000
but I can tell you that almost every single technologist

23:56.000 --> 23:58.000
who is leading a lot of these is like,

23:58.000 --> 24:00.000
I don't really like religion, right?

24:00.000 --> 24:03.000
And so the inherent bias would be to talk against religious

24:03.000 --> 24:04.000
kind of things.

24:04.000 --> 24:07.000
Again, I'm like, who am I to judge?

24:07.000 --> 24:08.000
People can be, they cannot be,

24:08.000 --> 24:11.000
but the inherent bias is refactor.

24:11.000 --> 24:13.000
And actually it becomes very important for society

24:13.000 --> 24:15.000
because we've seen that when about 12% of a population

24:15.000 --> 24:18.000
changes at point of view, it flips.

24:18.000 --> 24:19.000
Interesting.

24:19.000 --> 24:20.000
It doesn't take that much.

24:20.000 --> 24:21.000
It doesn't take that much

24:21.000 --> 24:23.000
because you listen to the voices in echo chamber.

24:23.000 --> 24:25.000
Like sometimes on Twitter, you know,

24:25.000 --> 24:27.000
I use my block button a lot.

24:27.000 --> 24:28.000
I'm like, you know.

24:28.000 --> 24:30.000
Listen, I've been enjoying your tweets.

24:30.000 --> 24:31.000
They've been really good.

24:31.000 --> 24:33.000
And I appreciate the frequency.

24:33.000 --> 24:34.000
Well, you know,

24:34.000 --> 24:37.000
it's nice owning your own media channel in a way, right?

24:37.000 --> 24:38.000
Sometimes I don't even have to have lunch

24:38.000 --> 24:41.000
because I'm told to eat crap so many times a day, right?

24:41.000 --> 24:43.000
That's why you have to hit the block button

24:43.000 --> 24:44.000
because it's a little echo chamber

24:44.000 --> 24:48.000
and a few dozen people can have that impact upon you.

24:48.000 --> 24:51.000
I mean, it's going to be very interesting

24:51.000 --> 24:53.000
to see the way that our adult minds

24:53.000 --> 24:56.000
and our kids minds evolve over the next five to 10 years

24:56.000 --> 24:58.000
with the emergence of this new, more powerful,

24:58.000 --> 25:00.000
personalizable technology.

25:00.000 --> 25:01.000
Yes.

25:01.000 --> 25:04.000
It's either controlled or controlled.

25:04.000 --> 25:05.000
Everybody, it's Peter.

25:05.000 --> 25:07.000
I want to take a break from our episode

25:07.000 --> 25:10.000
to talk about a health product that I love.

25:10.000 --> 25:11.000
It was a few years ago.

25:11.000 --> 25:15.000
I went looking for the best nutritional green drink on the market.

25:15.000 --> 25:18.000
So I went to Whole Foods and I started looking around.

25:18.000 --> 25:21.000
I found three shelves filled with options.

25:21.000 --> 25:24.000
I looked at the labels and they really didn't wow me.

25:24.000 --> 25:28.000
So I picked the top three that I thought looked decent,

25:28.000 --> 25:30.000
brought them home, tried them, and they sucked.

25:30.000 --> 25:32.000
First of all, they tasted awful.

25:32.000 --> 25:34.000
And then second, nutritional facts

25:34.000 --> 25:36.000
actually weren't very impressive.

25:36.000 --> 25:40.000
It was then that I found a product called AG1 by Athletic Greens.

25:40.000 --> 25:42.000
And without any question,

25:42.000 --> 25:46.000
Athletic Greens is the best on the market by a huge margin.

25:46.000 --> 25:49.000
First of all, it actually tastes amazing.

25:49.000 --> 25:52.000
And second, if you look at the ingredients,

25:52.000 --> 25:54.000
75 high quality ingredients

25:54.000 --> 25:57.000
that deliver nutrient-dense, antioxidants,

25:57.000 --> 26:01.000
multivitamins, pre- and probiotics, immune support, adaptogens.

26:01.000 --> 26:04.000
I personally utilize AG1 literally every day.

26:04.000 --> 26:07.000
I travel with an individual packet in my backpack,

26:07.000 --> 26:09.000
sometimes in my back pocket,

26:09.000 --> 26:13.000
and I count on it for gut health, immunity, energy,

26:13.000 --> 26:16.000
digestion, neural support, and really healthy aging.

26:16.000 --> 26:19.000
So if you want to take ownership of your health,

26:19.000 --> 26:21.000
today is a good day to start.

26:21.000 --> 26:24.000
Athletic Greens is giving you a free one-year supply of vitamin D

26:24.000 --> 26:28.000
and five of these travel packs with your first purchase.

26:28.000 --> 26:31.000
So go to athleticgreens.com backslash moonshots.

26:31.000 --> 26:35.000
That's athleticgreens.com backslash moonshots.

26:35.000 --> 26:37.000
Check it out. You'll thank me.

26:37.000 --> 26:40.000
Without question, this is the best green drink product,

26:40.000 --> 26:43.000
the most nutritious, the most flavorful I've found.

26:43.000 --> 26:45.000
All right, let's go back to the episode.

26:45.000 --> 26:47.000
So we're sitting here in the middle of Hollywood.

26:47.000 --> 26:49.000
I can see the beautiful Hollywood Hills.

26:49.000 --> 26:51.000
The Hollywood signs around here are someplace.

26:51.000 --> 26:53.000
Just over there, okay.

26:53.000 --> 26:57.000
And the news today is Green Actors Guild.

26:57.000 --> 26:59.000
Everyone's gone on strike.

26:59.000 --> 27:04.000
There is great fear, pain, concern.

27:04.000 --> 27:09.000
And, you know, you called it when we spoke six months ago.

27:09.000 --> 27:13.000
What's going on? How do you view it?

27:13.000 --> 27:15.000
And where's it going?

27:15.000 --> 27:18.000
So I think the advances in media or artificial intelligence

27:18.000 --> 27:19.000
have been huge.

27:19.000 --> 27:23.000
You've now got the Drake AI song or my favorite Ice Matrix,

27:23.000 --> 27:26.000
where the Matrix characters sing Ice Ice Baby.

27:26.000 --> 27:29.000
Yes.

27:29.000 --> 27:31.000
You've got real-time rigging.

27:31.000 --> 27:33.000
You have real-time special effects.

27:33.000 --> 27:36.000
You've got high definition creation of anything.

27:36.000 --> 27:37.000
What does that mean?

27:37.000 --> 27:40.000
It means that the whole industry is about to be disrupted

27:40.000 --> 27:44.000
because the cost of production reduces.

27:44.000 --> 27:46.000
It was reducing in some ways anyway.

27:46.000 --> 27:49.000
And we've seen this move where the cost of consumption

27:49.000 --> 27:51.000
went to zero with Napster and Spotify.

27:51.000 --> 27:53.000
Then the cost of creation started going to zero

27:53.000 --> 27:56.000
with Snapchat and TikTok.

27:56.000 --> 28:00.000
And now we have a question of what are the defaults going to be now?

28:00.000 --> 28:05.000
And consumers only have one limited currency

28:05.000 --> 28:06.000
and that's their attention.

28:06.000 --> 28:07.000
I think so.

28:07.000 --> 28:09.000
But consumers are willing to pay for attention.

28:09.000 --> 28:11.000
They're willing to pay for quality.

28:11.000 --> 28:14.000
Well, either premium media or, you know, otherwise.

28:14.000 --> 28:17.000
Because video games, for example,

28:17.000 --> 28:20.000
started as a $70 billion industry 10 years ago.

28:20.000 --> 28:24.000
The average score went from 69% to 74% on Metacritic

28:24.000 --> 28:25.000
over that period.

28:25.000 --> 28:28.000
And now they're $180 billion.

28:28.000 --> 28:29.000
Movies...

28:29.000 --> 28:31.000
I pay for some of that with my 12-year-olds.

28:31.000 --> 28:32.000
Yeah.

28:32.000 --> 28:36.000
Movies went from $40 billion to $50 billion,

28:36.000 --> 28:39.000
but the average IMDb movie rating of the last 10 years

28:39.000 --> 28:40.000
has been 6.4.

28:40.000 --> 28:42.000
It has not changed.

28:42.000 --> 28:45.000
So you're not producing more and so they're not consuming more.

28:45.000 --> 28:46.000
So there's a question.

28:46.000 --> 28:49.000
Will this technology enable an increase in quality

28:49.000 --> 28:51.000
because it raises the bar for everyone?

28:51.000 --> 28:52.000
Yes.

28:52.000 --> 28:53.000
There are more movie makers,

28:53.000 --> 28:55.000
so they're more excellent movie makers,

28:55.000 --> 28:57.000
and the best movie makers have become even more excellent.

28:57.000 --> 29:00.000
And we've democratized the tools from an iPhone

29:00.000 --> 29:05.000
to, you know, the tools on your Mac.

29:05.000 --> 29:09.000
And, of course, YouTube was the first major disruption of the...

29:09.000 --> 29:10.000
It was.

29:10.000 --> 29:13.000
Something like MrBeast has a much bigger audience than CNN now.

29:13.000 --> 29:14.000
Yeah.

29:14.000 --> 29:16.000
But that comes down to what is the key point here?

29:16.000 --> 29:18.000
Distribution.

29:18.000 --> 29:20.000
You can make good stories, but if no one hears them...

29:20.000 --> 29:21.000
Right.

29:21.000 --> 29:25.000
And that's what Disney has as its benefited distribution

29:25.000 --> 29:30.000
to its theme parks, to its products, to its channels.

29:30.000 --> 29:31.000
It creates a shelling point,

29:31.000 --> 29:34.000
but then if we all have our own individualized AIs,

29:34.000 --> 29:38.000
they can find us what we need and sift through the crowd.

29:38.000 --> 29:41.000
Maybe a whole of distribution flips on its head in five years as well.

29:41.000 --> 29:42.000
So I can't imagine...

29:42.000 --> 29:47.000
You know, I keep thinking that the future of movie consumption

29:47.000 --> 29:51.000
is me speaking to my version of Jarvis and saying,

29:51.000 --> 29:54.000
you know, I'd like a comedy, I'd like it starring me

29:54.000 --> 29:58.000
and three friends of mine and somebody else

29:58.000 --> 30:03.000
and for 90 minutes and set it in, you know, some setting and go.

30:03.000 --> 30:07.000
And have it auto-generate a compelling story

30:07.000 --> 30:15.000
that is either because I'm involved in it,

30:15.000 --> 30:17.000
I'm enraptured in it,

30:17.000 --> 30:20.000
or because my favorite stars are all together in an unlikely place.

30:20.000 --> 30:23.000
How far are we from that kind of future?

30:23.000 --> 30:25.000
I'd say probably three to five years.

30:25.000 --> 30:29.000
So in that case, it's not going to be cheap, but it'll be there.

30:29.000 --> 30:30.000
And then it'll get cheaper and cheaper.

30:30.000 --> 30:32.000
In that case, there is no distribution needed.

30:32.000 --> 30:34.000
A person calls up whatever they want.

30:34.000 --> 30:37.000
I think there is distribution needed, but in a different way.

30:37.000 --> 30:41.000
Music, we have a variety of different music sites,

30:41.000 --> 30:43.000
but how do musicians make their money now?

30:43.000 --> 30:44.000
It's not Spotify.

30:44.000 --> 30:46.000
A million views gets you a few thousand dollars.

30:46.000 --> 30:49.000
T-shirts, merchandising, global stories,

30:49.000 --> 30:53.000
because there is that future of the Wally type of fat guy

30:53.000 --> 30:55.000
sitting with his VR headset.

30:55.000 --> 30:56.000
It's kind of depressing.

30:56.000 --> 30:58.000
Just like, you know, the Apple Vision Pro adverts,

30:58.000 --> 31:00.000
I found kind of depressing when his kids are right there

31:00.000 --> 31:01.000
and just puts them on.

31:01.000 --> 31:04.000
Quite a bit dystopian to me.

31:04.000 --> 31:07.000
I think it's more a case of there are certain stories

31:07.000 --> 31:09.000
that everyone wants to talk about.

31:09.000 --> 31:11.000
Like here in Hollywood, what do we have this week?

31:11.000 --> 31:13.000
We have Oppenheimer and Barbie.

31:13.000 --> 31:15.000
Oh my God.

31:15.000 --> 31:16.000
Talking bags.

31:16.000 --> 31:18.000
I'm looking for an Oppenheimer.

31:18.000 --> 31:19.000
I will not watch Barbie.

31:19.000 --> 31:20.000
Sorry.

31:20.000 --> 31:21.000
Would you watch Barbieheimer?

31:21.000 --> 31:25.000
Only, no, I won't go there.

31:25.000 --> 31:27.000
Well, I mean, we can take it.

31:27.000 --> 31:29.000
We can put the scripts into Claude and see what it comes up with.

31:29.000 --> 31:30.000
That would be hilarious.

31:30.000 --> 31:32.000
Yeah, hilarious kind of Barbie.

31:32.000 --> 31:35.000
But again, like people are talking about the Barbie movie

31:35.000 --> 31:37.000
because it's, you know, not,

31:37.000 --> 31:39.000
she comes to the real world and she has challenges.

31:39.000 --> 31:40.000
It'll be a hit.

31:40.000 --> 31:44.000
People talk about Oppenheimer because again, it will be a hit.

31:44.000 --> 31:45.000
These are produced hits.

31:45.000 --> 31:46.000
These are produced hits.

31:46.000 --> 31:47.000
Yeah.

31:47.000 --> 31:49.000
Just like I saw BTS with my daughter.

31:49.000 --> 31:50.000
It's not BTS, they're Blackpink.

31:50.000 --> 31:52.000
Oh, gosh, she'll kill me.

31:52.000 --> 31:54.000
In Hyde Park a few weeks ago,

31:54.000 --> 31:56.000
the biggest K-pop band out of Korea.

31:56.000 --> 31:57.000
Right.

31:58.000 --> 32:00.000
Completely manufactured, but lots of fun.

32:00.000 --> 32:03.000
So on the one hand, what you have is what you described,

32:03.000 --> 32:05.000
your personalized things.

32:05.000 --> 32:07.000
That's kind of like McDonald's.

32:07.000 --> 32:08.000
What's the job to be done?

32:08.000 --> 32:10.000
The job is comfort.

32:10.000 --> 32:12.000
The job isn't to listen to someone else's story

32:12.000 --> 32:14.000
and expand from there.

32:14.000 --> 32:17.000
It isn't a produced Michelin star meal

32:17.000 --> 32:19.000
or just a nice restaurant.

32:19.000 --> 32:21.000
One that you can talk about to others like,

32:21.000 --> 32:23.000
you can cook ingredients yourself at home as well.

32:23.000 --> 32:24.000
Sure.

32:24.000 --> 32:26.000
But humans do like these bigger stories,

32:26.000 --> 32:30.000
but then the nature of funding of musicians changed.

32:30.000 --> 32:32.000
It was about merchandise.

32:32.000 --> 32:36.000
It was about kind of a lot of this other stuff

32:36.000 --> 32:39.000
around tours.

32:39.000 --> 32:41.000
And so I think the nature of movies might change.

32:41.000 --> 32:42.000
The business model has changed.

32:42.000 --> 32:44.000
I mean, that's the most interesting thing

32:44.000 --> 32:46.000
about exponential technology

32:46.000 --> 32:48.000
is it's changing the business models.

32:48.000 --> 32:52.000
And I keep on, you know, advising my entrepreneurs.

32:52.000 --> 32:55.000
It's reinvent your business models more than anything else.

32:55.000 --> 32:58.000
You have to always look across the landscape,

32:58.000 --> 33:02.000
where is the value peaks?

33:02.000 --> 33:04.000
And then you're sitting there

33:04.000 --> 33:06.000
and you're intermediating something

33:06.000 --> 33:08.000
and you're offering service value.

33:08.000 --> 33:10.000
You know, like there was that great quote,

33:10.000 --> 33:13.000
who is it from the CEO of Netscape?

33:13.000 --> 33:16.000
All values created by aggregation or disaggregation,

33:16.000 --> 33:18.000
bundling or unbundling.

33:18.000 --> 33:21.000
And you think about how the landscape is going to change now

33:21.000 --> 33:23.000
as intelligences move to the edge.

33:23.000 --> 33:26.000
What was once in the hands of the studios

33:26.000 --> 33:28.000
and the high priests of media

33:28.000 --> 33:30.000
suddenly gets pushed to the edge.

33:30.000 --> 33:32.000
Whereas value then, it changes it, it flips it.

33:32.000 --> 33:34.000
So let's go to what's going on right now.

33:34.000 --> 33:38.000
So the screen actors guild is on strike because of why?

33:38.000 --> 33:41.000
Screen actors, I mean, it's better wages in general,

33:41.000 --> 33:44.000
standard stuff, but now there's this rapidly emerging fear

33:44.000 --> 33:46.000
of artificial intelligence.

33:46.000 --> 33:50.000
So one of the proposals that came in from the other side

33:50.000 --> 33:54.000
was that basically all the extras,

33:54.000 --> 33:56.000
all the actors, they sign away their rights

33:56.000 --> 33:58.000
so they could be used in AI.

33:58.000 --> 34:01.000
So they get a day of wage to get scanned

34:01.000 --> 34:06.000
and then they can be used for the rest of that studio's life

34:06.000 --> 34:08.000
in producing background actors.

34:08.000 --> 34:10.000
And they were looking at this like,

34:10.000 --> 34:12.000
oh my God, wait, you can do that?

34:12.000 --> 34:15.000
People must have even realized you can do that.

34:15.000 --> 34:17.000
You know, script writers are saying

34:17.000 --> 34:19.000
no AI generated scripts,

34:19.000 --> 34:22.000
which again is a bit weird when they're all using Grammarly

34:22.000 --> 34:24.000
and things like that, which was AI.

34:24.000 --> 34:26.000
How are you ever going to tell?

34:26.000 --> 34:28.000
Where do you draw the line?

34:28.000 --> 34:29.000
Where do you draw the line?

34:29.000 --> 34:31.000
Because this is a technology that's coming so fast

34:31.000 --> 34:36.000
and is so good that it's almost not like technology at all.

34:36.000 --> 34:38.000
It's just very natural the way it emerges.

34:38.000 --> 34:41.000
And so you will get to some sort of agreement

34:41.000 --> 34:43.000
because the big actors are kind of there,

34:43.000 --> 34:46.000
but the defaults that are set now reverberate.

34:46.000 --> 34:49.000
So I think that's an important point that you just made.

34:49.000 --> 34:56.000
The decisions, the policies that we create today

34:56.000 --> 34:59.000
are going to take us down one path or another

34:59.000 --> 35:01.000
for the decades to come.

35:01.000 --> 35:03.000
Yeah, it will affect the whole of Hollywood,

35:03.000 --> 35:06.000
what's decided in this moment here,

35:06.000 --> 35:09.000
because there won't be another renegotiation for a while.

35:09.000 --> 35:13.000
And so again, how does it act to create value?

35:13.000 --> 35:16.000
A top actor has a following,

35:16.000 --> 35:18.000
but up-and-comers, how did they break through?

35:18.000 --> 35:20.000
What was the apprenticeship to him?

35:20.000 --> 35:22.000
What does a movie look like in five years?

35:22.000 --> 35:26.000
Even if you agree as Hollywood, not to have any AI.

35:26.000 --> 35:28.000
Let's just say you have this kind of

35:28.000 --> 35:30.000
Dune-style bit Larry and Jihad and say,

35:30.000 --> 35:32.000
No AI, you know?

35:32.000 --> 35:34.000
And someone will make a movie about no AI in Hollywood.

35:34.000 --> 35:37.000
What do you do when the Chinese film studios?

35:37.000 --> 35:39.000
Start releasing product.

35:39.000 --> 35:41.000
Start releasing product faster than anything.

35:41.000 --> 35:43.000
You can make five dreams out of it.

35:43.000 --> 35:45.000
In every language out there?

35:45.000 --> 35:46.000
Literally every language.

35:46.000 --> 35:47.000
We have technology now.

35:47.000 --> 35:49.000
And again, maybe this is part of what we're doing.

35:49.000 --> 35:50.000
What is possible now?

35:50.000 --> 35:53.000
We can translate Peter's voice into just about any language

35:53.000 --> 35:55.000
with his voice, so it's not a voice actor.

35:55.000 --> 35:57.000
And match my lips and movements, exactly.

35:57.000 --> 35:59.000
Match your lips and movements, exactly.

35:59.000 --> 36:02.000
I'm sure the podcast will be in every language by next year.

36:02.000 --> 36:04.000
And again, it will be in our voices.

36:04.000 --> 36:06.000
We can make our voices sound more confident.

36:06.000 --> 36:09.000
We can take his mannerisms right now

36:09.000 --> 36:11.000
and transplant them onto my mannerisms so we match,

36:11.000 --> 36:14.000
so we can reshoot scenes with style.

36:14.000 --> 36:15.000
Yeah.

36:15.000 --> 36:17.000
We can turn him into a robot in a few minutes.

36:17.000 --> 36:20.000
And in fact, maybe we'll do that in the film of the post.

36:20.000 --> 36:23.000
And so we'll have that scene of him becoming a robot.

36:23.000 --> 36:25.000
These are all technologies that are here now,

36:25.000 --> 36:27.000
and it transforms fundamentally the nature of filmmaking

36:27.000 --> 36:29.000
because you only need one shot.

36:29.000 --> 36:33.000
But don't the film actors who are, you know,

36:33.000 --> 36:37.000
standing up for their rights to not have them

36:37.000 --> 36:41.000
basically demonetized and digitized,

36:41.000 --> 36:43.000
the other option is for Hollywood

36:43.000 --> 36:45.000
to just create complete artificial characters

36:45.000 --> 36:46.000
that they fully own.

36:46.000 --> 36:47.000
Yeah.

36:47.000 --> 36:51.000
And you've seen this already with some of the kind of V-loggers

36:51.000 --> 36:54.000
and, you know, others that have emerged out of Asia,

36:54.000 --> 36:56.000
in particular, fully AI-generated characters.

36:56.000 --> 36:57.000
Yeah.

36:57.000 --> 36:59.000
And you can have entire mythos around them.

36:59.000 --> 37:02.000
And you can say, make it the most attractive Italian guy

37:02.000 --> 37:05.000
I've ever seen that's broody and this and that.

37:05.000 --> 37:06.000
Tell it from a human.

37:06.000 --> 37:07.000
Yeah.

37:07.000 --> 37:09.000
I mean, like, these characters can be completely new.

37:09.000 --> 37:11.000
And it's far more profitable for the studio

37:11.000 --> 37:14.000
to use that digital actor.

37:14.000 --> 37:19.000
So there is a disruption coming at every level.

37:19.000 --> 37:23.000
Because the research to revenue pipeline has become so tight

37:23.000 --> 37:25.000
and it sets off a race condition,

37:25.000 --> 37:28.000
whereby you could only produce two movies a year,

37:28.000 --> 37:30.000
suddenly you can produce 20.

37:30.000 --> 37:34.000
And then you can actually, like we do AB testing

37:34.000 --> 37:36.000
in subject headlines,

37:36.000 --> 37:39.000
you could create 30 variants of the movie

37:39.000 --> 37:41.000
and see which one actually is the best.

37:41.000 --> 37:42.000
Yeah.

37:42.000 --> 37:45.000
And, I mean, again, you can say, make it more,

37:45.000 --> 37:47.000
make that speech roar and more emotional.

37:47.000 --> 37:49.000
It will adjust the voice to make it roar

37:49.000 --> 37:51.000
and more emotional, right?

37:51.000 --> 37:54.000
And because you're using such large data sets,

37:54.000 --> 37:57.000
you know, and we, so we made all our data sets open

37:57.000 --> 37:59.000
and then we allowed opt-out.

37:59.000 --> 38:01.000
We're the only company in the world to allow opt-out

38:01.000 --> 38:02.000
because we thought it was the right thing to do.

38:02.000 --> 38:04.000
So we had 169 million images

38:04.000 --> 38:06.000
opted out of our image data sets.

38:06.000 --> 38:08.000
For music, because it's different copyright laws,

38:08.000 --> 38:11.000
we have one of the first commercially licensed music models

38:11.000 --> 38:12.000
coming out.

38:12.000 --> 38:13.000
So respect for that.

38:13.000 --> 38:17.000
But if I'm an artist and I go to the Louvre

38:17.000 --> 38:22.000
to be inspired and then go back and paint,

38:22.000 --> 38:26.000
and I've been inspired by Da Vinci

38:26.000 --> 38:32.000
and I start painting in a style like Da Vinci,

38:32.000 --> 38:33.000
where's the difference there?

38:33.000 --> 38:34.000
Where's the difference?

38:34.000 --> 38:35.000
And this is the reality.

38:35.000 --> 38:38.000
Even though we've done that, by next year,

38:38.000 --> 38:40.000
probably by the end of the year,

38:40.000 --> 38:44.000
you will have models that have zero scraped data

38:44.000 --> 38:45.000
or human art.

38:45.000 --> 38:46.000
They will all be synthetic.

38:46.000 --> 38:47.000
Yeah.

38:47.000 --> 38:49.000
And you'll be able to bring your art to it.

38:49.000 --> 38:52.000
So there's something Google just released called Style Drop.

38:52.000 --> 38:55.000
There's Hyper Dream, there's Hyper Networks,

38:55.000 --> 38:57.000
you can take one picture of yourself

38:57.000 --> 39:00.000
and the entire model trains to be able to put you into anything,

39:00.000 --> 39:01.000
even if you're not in the model.

39:01.000 --> 39:03.000
It used to take minutes, hours,

39:03.000 --> 39:05.000
now it's just one picture.

39:05.000 --> 39:08.000
Similarly, you can bring any style

39:08.000 --> 39:10.000
and it will just mimic and imitate that style.

39:10.000 --> 39:13.000
And so all of a sudden, the models themselves,

39:13.000 --> 39:15.000
it doesn't matter what they're trained on

39:15.000 --> 39:18.000
because there's no human endeavor in those models.

39:18.000 --> 39:21.000
And then things like compensation for artists and others,

39:21.000 --> 39:23.000
as you said, become a bit mute

39:23.000 --> 39:24.000
because all of a sudden,

39:24.000 --> 39:27.000
you have these amazing stories told by

39:27.000 --> 39:30.000
really convincing amazing actors

39:30.000 --> 39:32.000
who may or may not exist

39:32.000 --> 39:34.000
and how are you ever going to tell the difference?

39:34.000 --> 39:35.000
So what's your advice?

39:35.000 --> 39:36.000
Let's parse it here.

39:36.000 --> 39:39.000
On one side, what's your advice for Hollywood

39:39.000 --> 39:41.000
and for actors?

39:41.000 --> 39:43.000
And on the other side,

39:43.000 --> 39:45.000
I want to ask your advice for artists.

39:45.000 --> 39:48.000
This is about mindset.

39:48.000 --> 39:52.000
This is coming at us at extraordinary speed.

39:52.000 --> 39:54.000
There's no stopping it, right?

39:54.000 --> 39:59.000
There's no slowing it down.

39:59.000 --> 40:02.000
And so you've got to deal with reality.

40:02.000 --> 40:04.000
You're dealing with reality, again, it's inevitable.

40:04.000 --> 40:07.000
Even if, again, Hollywood says no AI,

40:07.000 --> 40:09.000
the AI is coming from around the world.

40:09.000 --> 40:10.000
So what do you do?

40:10.000 --> 40:11.000
You think, oh, well,

40:11.000 --> 40:14.000
my audience suddenly became the whole world.

40:14.000 --> 40:16.000
That's a big deal.

40:16.000 --> 40:19.000
You're like, what am I actually known for?

40:19.000 --> 40:20.000
Is my acting skills?

40:20.000 --> 40:22.000
Well, I will still get these things.

40:22.000 --> 40:24.000
You're an up and coming actor.

40:24.000 --> 40:25.000
You say, I need to build community.

40:25.000 --> 40:28.000
I need to kind of show off something more than that.

40:28.000 --> 40:30.000
Because again, my acting skills in some areas

40:30.000 --> 40:31.000
can be transplanted,

40:31.000 --> 40:33.000
but what about real life shows?

40:33.000 --> 40:35.000
What about these things?

40:35.000 --> 40:37.000
It does throw up the entire thing and adjust it,

40:37.000 --> 40:40.000
but then musicians have had to have that adjustment.

40:40.000 --> 40:42.000
They used to be able to make money on their LPs,

40:42.000 --> 40:43.000
and then all of a sudden,

40:43.000 --> 40:46.000
they had the naps to Spotify moments.

40:46.000 --> 40:48.000
There is more protection in music as well

40:48.000 --> 40:51.000
because you basically, according to

40:51.000 --> 40:55.000
the Robin Thicke versus Marvin Gaye case,

40:55.000 --> 40:58.000
there is an element of style protection in there

40:58.000 --> 41:00.000
that doesn't exist in visual media.

41:00.000 --> 41:03.000
And probably won't because the other part of this is

41:03.000 --> 41:06.000
if you're expecting governments to regulate,

41:06.000 --> 41:11.000
how can they, when there is a global competition going on,

41:11.000 --> 41:14.000
they will lose competitiveness to other countries

41:14.000 --> 41:16.000
and you'll have regulatory arbitrage.

41:16.000 --> 41:20.000
Yes, and that is something across

41:20.000 --> 41:22.000
every industry that's going to be happening.

41:22.000 --> 41:25.000
Yeah, I think the concept of united artists,

41:25.000 --> 41:28.000
it was originally a collective of all the artists,

41:28.000 --> 41:30.000
that makes a lot of sense now.

41:30.000 --> 41:33.000
I think you have to think about an element of collectivism

41:33.000 --> 41:36.000
to share the excess profits because what's going to happen is

41:36.000 --> 41:38.000
movies will get cheaper, profits will go up.

41:38.000 --> 41:41.000
You need to support each other as a community here

41:41.000 --> 41:43.000
and think again, as a community,

41:43.000 --> 41:46.000
this is our story for the next one, three, five, ten years

41:46.000 --> 41:50.000
because all of this is going to happen quicker

41:50.000 --> 41:54.000
than it takes to make the new Avengers movie.

41:54.000 --> 41:57.000
And quicker than regulators are able to regulate.

41:57.000 --> 42:01.000
And again, the regulators almost certainly won't regulate

42:01.000 --> 42:04.000
because they will start falling behind

42:04.000 --> 42:06.000
their competitor countries.

42:06.000 --> 42:09.000
I mean, if we look at the internet itself as it,

42:09.000 --> 42:12.000
the media industry never expected the internet

42:12.000 --> 42:14.000
to have the disruptive impact it had.

42:14.000 --> 42:17.000
And had it known, it probably would have tried to get regulators

42:17.000 --> 42:19.000
to have slowed it down or blocked it.

42:19.000 --> 42:21.000
Yeah, the speed is too much, but then also,

42:21.000 --> 42:23.000
again, I gave the example earlier,

42:23.000 --> 42:25.000
the video game industry has gone from 70 billion to 180 billion

42:25.000 --> 42:27.000
over the next ten years.

42:27.000 --> 42:29.000
Can we increase quality?

42:29.000 --> 42:31.000
Interactivity.

42:31.000 --> 42:33.000
Because games are media as well.

42:33.000 --> 42:35.000
The media industry has increased in size.

42:35.000 --> 42:38.000
The way that value has gone has been redistributed.

42:38.000 --> 42:41.000
Value will be redistributed again now.

42:41.000 --> 42:44.000
And again, it's like, what is an AI-enhanced actor?

42:44.000 --> 42:49.000
If you're an actor, what's an AI-enhanced photographer filming?

42:49.000 --> 42:53.000
Think about your jobs, the tasks that you do,

42:53.000 --> 42:55.000
and what can be augmented if you had a bunch

42:55.000 --> 42:58.000
of really talented youngsters working for you, right?

42:58.000 --> 43:01.000
You could do more, you could be more.

43:01.000 --> 43:04.000
But then it means the bar is just going to keep on raising.

43:04.000 --> 43:08.000
Let's turn to a different industry that's going to change.

43:08.000 --> 43:11.000
We had this conversation in our last podcast

43:11.000 --> 43:16.000
and on stage at abundance 360, which is coders.

43:16.000 --> 43:19.000
Coding is changing dramatically.

43:19.000 --> 43:21.000
What are your thoughts there?

43:21.000 --> 43:25.000
So, when I started as a programmer, gosh, 22 years ago,

43:25.000 --> 43:27.000
I was writing enterprise-level assembly code

43:27.000 --> 43:29.000
for voice-over IP software.

43:29.000 --> 43:30.000
Wow.

43:30.000 --> 43:31.000
I had to switch.

43:31.000 --> 43:33.000
That's some of the largest chunks of code out there.

43:33.000 --> 43:35.000
Yeah, it's very low-level code.

43:35.000 --> 43:37.000
We didn't have GitHub, we just got Subversion

43:37.000 --> 43:38.000
here.

43:38.000 --> 43:40.000
Programming these days is a lot like Lego,

43:40.000 --> 43:42.000
because what you have is kind of you have a very low-level,

43:42.000 --> 43:44.000
but then you have levels of abstraction

43:44.000 --> 43:47.000
until you get to PyTorch and some of these other languages.

43:47.000 --> 43:49.000
So, you have to compile lots of different libraries

43:49.000 --> 43:52.000
because you're making it easier and easier.

43:52.000 --> 43:55.000
Human words are just the next level of abstraction there.

43:55.000 --> 43:58.000
But the nature of coding is going to change.

43:58.000 --> 44:02.000
And so, the coders that are coding traditionally today

44:02.000 --> 44:06.000
around the world, how will they be using

44:07.000 --> 44:11.000
and working in this industry two to five years from now?

44:11.000 --> 44:14.000
Well, again, there will be no coder

44:14.000 --> 44:16.000
that doesn't use AI as part of their workflow.

44:16.000 --> 44:18.000
Okay, I mean, I think that's the important thing.

44:18.000 --> 44:20.000
It's not like coders are going to go away,

44:20.000 --> 44:22.000
they're going to be using a new set of tools.

44:22.000 --> 44:24.000
The expectations will rise,

44:24.000 --> 44:27.000
the amount of debugging unit testing,

44:27.000 --> 44:29.000
all of these things will decrease,

44:29.000 --> 44:32.000
because how much time did coders actually spend architecting?

44:32.000 --> 44:33.000
Very little up front.

44:33.000 --> 44:35.000
It's more about understanding information flows

44:35.000 --> 44:37.000
about architecting these things.

44:37.000 --> 44:39.000
It's about having feedback loops

44:39.000 --> 44:41.000
to understand customer requirements.

44:41.000 --> 44:44.000
Databricks is a $38 billion company.

44:44.000 --> 44:45.000
There's Data Lakes.

44:45.000 --> 44:47.000
So, it takes your data to organize

44:47.000 --> 44:49.000
it allows you to write structured queries.

44:49.000 --> 44:51.000
You have to write queries.

44:51.000 --> 44:54.000
Now, you just talk to it and it just does it.

44:54.000 --> 44:56.000
Microsoft will introduce the same thing.

44:56.000 --> 44:59.000
I mean, I can't wait for that in the field of medicine,

44:59.000 --> 45:01.000
which I want to talk about next.

45:01.000 --> 45:03.000
But you said something earlier where you can imagine

45:03.000 --> 45:05.000
there can be a billion coders in the future.

45:05.000 --> 45:11.000
Yeah, because all the barriers to creating programs disappear.

45:11.000 --> 45:13.000
So, it's not that there are no programs,

45:13.000 --> 45:15.000
there's no programs that we know it,

45:15.000 --> 45:17.000
because there's a billion programmers.

45:17.000 --> 45:19.000
Everyone is a programmer, nobody's a programmer in a way.

45:19.000 --> 45:21.000
Because it just becomes a matter of course.

45:21.000 --> 45:24.000
I want to make software that does something

45:24.000 --> 45:26.000
and reacts in these ways

45:26.000 --> 45:29.000
and looks like this and adapts like this.

45:29.000 --> 45:31.000
Then it comes to you and you're like,

45:31.000 --> 45:32.000
no, that's not quite right.

45:32.000 --> 45:33.000
I want this moved over.

45:33.000 --> 45:35.000
It happens almost live, this feedback loop.

45:35.000 --> 45:38.000
It's as we talk to chat GPT-4,

45:38.000 --> 45:41.000
creating a paragraph that describes something we want.

45:41.000 --> 45:42.000
We modify it.

45:42.000 --> 45:45.000
I mean, like, chat GPT-4 is a good example

45:45.000 --> 45:50.000
because to write an integration to something like chat GPT-4,

45:50.000 --> 45:53.000
you used to take days, weeks, an API,

45:53.000 --> 45:55.000
an application protocol kind of interface.

45:55.000 --> 45:58.000
Now, what you do is actually you tell it the schema.

45:58.000 --> 46:00.000
You tell it kind of what you should do

46:00.000 --> 46:02.000
and it writes it automatically,

46:02.000 --> 46:04.000
literally within like a few minutes.

46:04.000 --> 46:06.000
And then in a few hours, you've integrated into it.

46:06.000 --> 46:10.000
I've been talking about a future where we all have Jarvis.

46:10.000 --> 46:12.000
Iron Man, I love Iron Man as a movie.

46:12.000 --> 46:13.000
It's one of my favorites.

46:13.000 --> 46:16.000
And, you know, Jarvis is basically your personal AI

46:16.000 --> 46:18.000
that is, it's a software shell.

46:18.000 --> 46:20.000
It interfaces between you and the rest of the world.

46:20.000 --> 46:22.000
You ask Jarvis to do something

46:22.000 --> 46:27.000
and it knows how to, the toolpaths on a 3D printer

46:27.000 --> 46:30.000
you can hop into a jet and interface Jarvis

46:30.000 --> 46:33.000
with the jet's computational system

46:33.000 --> 46:35.000
and it'll fly the jet for you.

46:35.000 --> 46:38.000
I can't imagine that that is really far from now.

46:38.000 --> 46:41.000
Yeah, let's hope that doesn't fly planes just quite yet.

46:41.000 --> 46:44.000
Okay, well, I'll put the jet aircraft aside.

46:44.000 --> 46:49.000
But the ability to, for it to become your best friend

46:49.000 --> 46:52.000
and confidant, know your needs and desires,

46:52.000 --> 46:55.000
shape the world to your comfort

46:55.000 --> 46:57.000
and being able to help you.

46:57.000 --> 46:59.000
It's the ultimate user interface.

46:59.000 --> 47:02.000
Well, I mean, this is why a lot of the chatbots,

47:02.000 --> 47:04.000
character AI and others have become so popular

47:04.000 --> 47:05.000
because it'll never judge you

47:05.000 --> 47:08.000
and it's approaching that human level now.

47:08.000 --> 47:11.000
You know, and again, it is the ultimate interfaces,

47:11.000 --> 47:14.000
maybe chats, but it's more chats, chatting context.

47:14.000 --> 47:17.000
It's understanding you holistically.

47:17.000 --> 47:19.000
No human could do that because, you know,

47:19.000 --> 47:20.000
even if you hire a whole team,

47:20.000 --> 47:22.000
they're not going to be with you 24-7.

47:22.000 --> 47:24.000
This will be with you 24-7.

47:24.000 --> 47:26.000
I think the key thing here is empathy.

47:26.000 --> 47:27.000
Yes.

47:27.000 --> 47:30.000
Because jumping head bit to medicine.

47:30.000 --> 47:32.000
Google had their MedPalm 2 model.

47:32.000 --> 47:34.000
The papers just come out in nature.

47:34.000 --> 47:38.000
A, it outperforms doctors on clinical diagnosis,

47:38.000 --> 47:41.000
which is crazy for a few hundred gigabytes of a file.

47:41.000 --> 47:42.000
Yeah.

47:42.000 --> 47:45.000
B, it outperforms doctors on scores of empathy.

47:45.000 --> 47:49.000
I found that amazing and totally logical.

47:49.000 --> 47:50.000
It doesn't judge you.

47:50.000 --> 47:52.000
It doesn't judge you, but then, you know,

47:52.000 --> 47:54.000
doctors split a million ways and they're tired

47:54.000 --> 47:56.000
and they're grumpy or this or that.

47:56.000 --> 47:59.000
Some of us get good doctors, most of us don't.

47:59.000 --> 48:01.000
Some of us get good teachers.

48:01.000 --> 48:04.000
I'm not saying education is bad because of the teachers.

48:04.000 --> 48:06.000
So many teachers try so hard,

48:06.000 --> 48:10.000
but their attention is split 20 ways and they're underpaid.

48:10.000 --> 48:12.000
You know, I'm not saying that programs

48:12.000 --> 48:14.000
that would be nature programming will change

48:14.000 --> 48:15.000
because programmers are bad.

48:15.000 --> 48:17.000
There's so many hard-working programmers.

48:17.000 --> 48:20.000
It's just, again, the nature of these things will change

48:20.000 --> 48:23.000
when you can scale expertise.

48:23.000 --> 48:26.000
And everyone has expertise available to them on tap.

48:26.000 --> 48:29.000
I've been on the stage, you know, just pounding my fist

48:29.000 --> 48:31.000
saying, listen, it's going to become malpractice

48:31.000 --> 48:34.000
to diagnose someone without an AI in the loop

48:34.000 --> 48:36.000
within five years' time.

48:36.000 --> 48:40.000
And probably in some areas, it'll be inappropriate

48:40.000 --> 48:43.000
to not yet illegal to.

48:43.000 --> 48:45.000
And then at some point soon after that,

48:45.000 --> 48:48.000
the best surgeons in the world are going to be humanoid robots

48:48.000 --> 48:56.000
that have every possible, you know, atrial variation,

48:56.000 --> 49:00.000
every possible, you know, history of surgery

49:00.000 --> 49:02.000
and they can see an infrared and ultraviolet

49:02.000 --> 49:05.000
and they haven't had an argument that morning

49:05.000 --> 49:08.000
with their husband or wife and it becomes the best.

49:08.000 --> 49:13.000
And these are demonetizing and democratizing forces for health.

49:13.000 --> 49:15.000
There must be deflationary as well,

49:15.000 --> 49:17.000
but I know I agree completely with this

49:17.000 --> 49:20.000
but ultimately what's going to kick it off is,

49:20.000 --> 49:22.000
is your doctor AI enhanced?

49:22.000 --> 49:23.000
Yeah.

49:23.000 --> 49:25.000
Low insurance premium, lower copay?

49:25.000 --> 49:26.000
Yes.

49:26.000 --> 49:29.000
Because there will be real economic incentives.

49:29.000 --> 49:31.000
Has this been cross-checked by the technology?

49:31.000 --> 49:32.000
Yeah.

49:32.000 --> 49:34.000
Reduce the cost?

49:34.000 --> 49:37.000
My favorite subject is, you know, you probably know this,

49:37.000 --> 49:40.000
how many medical articles are written in journals every day?

49:40.000 --> 49:41.000
I don't know.

49:41.000 --> 49:42.000
It's 7,000.

49:42.000 --> 49:43.000
Wow.

49:43.000 --> 49:46.000
And it's like, how many is your doctor read today?

49:46.000 --> 49:48.000
You know, and there may be that one breakthrough

49:48.000 --> 49:50.000
that happened this morning that is the key

49:50.000 --> 49:52.000
for your diagnostics.

49:52.000 --> 49:54.000
But I mean, even if they've read it, right,

49:54.000 --> 49:57.000
like absorbing it is one thing, having the mental models,

49:57.000 --> 49:59.000
these are kind of something else.

49:59.000 --> 50:01.000
This is why comprehensive authority is not state,

50:01.000 --> 50:03.000
which is what this technology allows to happen.

50:03.000 --> 50:06.000
As you said, things like, we're already seeing some surgeries,

50:06.000 --> 50:09.000
can you done better by robot surgeons than human surgeons?

50:09.000 --> 50:10.000
It'll be all surgeries.

50:10.000 --> 50:12.000
Yeah, it will be all surgeries soon enough.

50:12.000 --> 50:14.000
Like the robotics advancements we've seen,

50:14.000 --> 50:16.000
this actually goes back to your point of, you know,

50:16.000 --> 50:19.000
the artist going to the Louvre and seeing the Da Vinci

50:19.000 --> 50:21.000
and then taking inspiration from that.

50:21.000 --> 50:23.000
What are we going to do with like all of these

50:23.000 --> 50:26.000
Optimus robots and 1kx robots and others?

50:26.000 --> 50:28.000
Are they going to have to shut their eyes

50:28.000 --> 50:30.000
when they see anything copyrighted?

50:30.000 --> 50:32.000
You're just going to have accidents everywhere.

50:32.000 --> 50:34.000
They're like running into each other.

50:34.000 --> 50:36.000
Everything's going to be black lined out.

50:36.000 --> 50:41.000
So medicine is changing dramatically.

50:41.000 --> 50:44.000
What other field are you seeing and saying

50:44.000 --> 50:46.000
people need to wake up and see what's coming?

50:46.000 --> 50:49.000
So, I mean, medicine, education

50:49.000 --> 50:51.000
are kind of the two big ones, I think.

50:51.000 --> 50:52.000
I agree.

50:52.000 --> 50:53.000
But can we move this to the side right now?

50:53.000 --> 50:54.000
Again, we've seen programming,

50:54.000 --> 50:56.000
the entire nature program will change media.

50:56.000 --> 50:58.000
The entire nature of media will change

50:58.000 --> 51:01.000
from journalism to filmmaking.

51:01.000 --> 51:06.000
But anything that basically you could do

51:06.000 --> 51:09.000
with someone from Asia on the other side

51:09.000 --> 51:11.000
on a computer screen will change.

51:11.000 --> 51:13.000
Let's talk about education.

51:13.000 --> 51:17.000
Because today, education hasn't changed

51:17.000 --> 51:21.000
since the one-room classroom.

51:21.000 --> 51:24.000
Half the kids are lost, half the kids are bored.

51:24.000 --> 51:26.000
You're teaching to a series of tests.

51:26.000 --> 51:30.000
You're teaching for an industrial era world.

51:30.000 --> 51:33.000
And people learn differently.

51:33.000 --> 51:38.000
People have visual and auditory and tactile learning skills.

51:38.000 --> 51:42.000
And let's face it, we don't celebrate our teachers.

51:42.000 --> 51:43.000
We don't pay them well,

51:43.000 --> 51:46.000
and we don't have the best of them coming into the classroom.

51:46.000 --> 51:48.000
And they're sad.

51:48.000 --> 51:50.000
I mean, this is the thing.

51:50.000 --> 51:53.000
There are happy classrooms with happy teachers

51:53.000 --> 51:54.000
and other things,

51:54.000 --> 51:58.000
but learning should be a place of positive growth and joy.

51:58.000 --> 52:00.000
It should be fun to learn.

52:00.000 --> 52:01.000
And it should be fun to teach.

52:01.000 --> 52:02.000
And fun to teach, yes.

52:02.000 --> 52:04.000
I think this is both of them.

52:04.000 --> 52:06.000
Because what is the nature of a teacher in 5, 10 years?

52:06.000 --> 52:08.000
Let's say 10 years.

52:08.000 --> 52:12.000
Again, 10 years is the crazy short period of time.

52:12.000 --> 52:16.000
I mean, when I asked you this question last time,

52:16.000 --> 52:19.000
how far out can you predict what's likely to come?

52:19.000 --> 52:23.000
What's your singularity boundary condition?

52:23.000 --> 52:24.000
I'm curious.

52:24.000 --> 52:26.000
What are you seeing as?

52:26.000 --> 52:28.000
3 or 5 years.

52:28.000 --> 52:30.000
I go to Dubai and I'm on stage,

52:30.000 --> 52:34.000
and can you talk to us about what the world's going to be like in 2050?

52:34.000 --> 52:35.000
My answer is no.

52:35.000 --> 52:37.000
I can barely talk about 2030.

52:37.000 --> 52:39.000
It's everything ever at once.

52:39.000 --> 52:41.000
Lots of S-curves, acceleration.

52:41.000 --> 52:43.000
These are inevitable now.

52:43.000 --> 52:45.000
Now we've broken through these things.

52:45.000 --> 52:48.000
I mean, OpenAI now have put 20% of their stuff to alignment

52:48.000 --> 52:51.000
because they're basically saying that their view is 5 years out.

52:51.000 --> 52:53.000
Elon Musk just said 6 years out,

52:53.000 --> 52:56.000
but then Elon's relationship with time is always a bit fun.

52:56.000 --> 52:58.000
Just like self-driving cars.

52:58.000 --> 53:00.000
But self-driving cars are literally here now.

53:00.000 --> 53:03.000
You can get in one and it will drive you around.

53:03.000 --> 53:05.000
San Francisco or London or Germany.

53:05.000 --> 53:06.000
Waymore will do that for you.

53:06.000 --> 53:07.000
Not my Tesla.

53:07.000 --> 53:08.000
Not your Tesla.

53:08.000 --> 53:09.000
I know.

53:09.000 --> 53:10.000
I push in the button, but it doesn't quite do it.

53:10.000 --> 53:11.000
But the technology is here.

53:11.000 --> 53:12.000
Yeah.

53:12.000 --> 53:13.000
And I was like, oh wait.

53:13.000 --> 53:14.000
What?

53:14.000 --> 53:15.000
This is the thing.

53:15.000 --> 53:18.000
So, again, 10 years is a dramatically short period of time

53:18.000 --> 53:20.000
for education, which has been the same for a century.

53:20.000 --> 53:21.000
Yes.

53:21.000 --> 53:23.000
But the thing is, it is inevitable

53:23.000 --> 53:26.000
that every child will have their own AI.

53:26.000 --> 53:29.000
So your 12-year-old will be 22.

53:29.000 --> 53:32.000
When they get to 22 and they come out of,

53:32.000 --> 53:34.000
let's say university slows down.

53:34.000 --> 53:35.000
University.

53:35.000 --> 53:36.000
If university is still a thing.

53:36.000 --> 53:38.000
Yeah, they will have their own AI

53:38.000 --> 53:40.000
that's learned for at least five years about them.

53:40.000 --> 53:41.000
Yes.

53:41.000 --> 53:43.000
That can fetch them any information in any format

53:43.000 --> 53:45.000
of any type and write anything.

53:45.000 --> 53:46.000
Yeah.

53:46.000 --> 53:49.000
Or create any video or movie for them to...

53:49.000 --> 53:50.000
That's a crazy thing.

53:50.000 --> 53:52.000
There were concerns that Wikipedia would remove

53:52.000 --> 53:54.000
rote learning and things like that.

53:54.000 --> 53:55.000
Google would do the same.

53:55.000 --> 53:57.000
And maybe there are kind of, again,

53:57.000 --> 54:00.000
like appendices that have shivelled.

54:00.000 --> 54:01.000
No.

54:01.000 --> 54:02.000
What's the thing that shivells?

54:02.000 --> 54:03.000
Yeah, your appendix.

54:03.000 --> 54:04.000
Yeah.

54:04.000 --> 54:06.000
But, you know, like, I was at Galblad or something.

54:06.000 --> 54:08.000
But this is the thing, like,

54:08.000 --> 54:10.000
you might have some vestigial parts of your brain.

54:10.000 --> 54:12.000
The entire human brain will be rewired.

54:12.000 --> 54:16.000
You must assume that every child will have their own AI.

54:16.000 --> 54:19.000
However AI is driven is different.

54:19.000 --> 54:21.000
Because any child that has AI will dramatically

54:21.000 --> 54:23.000
outperform the kids that don't.

54:23.000 --> 54:26.000
But what are we optimizing for in education?

54:26.000 --> 54:28.000
And I think one of the things that we've lost

54:28.000 --> 54:33.000
is what is our objective function as a society?

54:33.000 --> 54:36.000
What does America even stand for right now?

54:36.000 --> 54:37.000
Coming here.

54:37.000 --> 54:38.000
I agree with you.

54:38.000 --> 54:40.000
What are we optimizing for?

54:40.000 --> 54:41.000
Butan is optimizing for happiness.

54:41.000 --> 54:44.000
Well, and frankly, I think happiness is a great thing

54:44.000 --> 54:46.000
to optimize for in general.

54:46.000 --> 54:47.000
If you ask people, you know,

54:47.000 --> 54:49.000
what do you want more than anything?

54:49.000 --> 54:50.000
Life, I want happiness.

54:50.000 --> 54:51.000
I want health.

54:51.000 --> 54:52.000
I want love.

54:52.000 --> 54:54.000
We don't talk about, you know, those are...

54:54.000 --> 54:55.000
And there's an interesting, you know,

54:55.000 --> 54:58.000
I was just with Mo Godot doing a podcast.

54:58.000 --> 55:00.000
And he's a fan of your work.

55:00.000 --> 55:02.000
And he was saying, loved your podcast.

55:02.000 --> 55:05.000
And we're talking about this test of what would you trade?

55:05.000 --> 55:06.000
Right?

55:06.000 --> 55:09.000
Would you trade, you know,

55:09.000 --> 55:11.000
how much money would you trade for your happiness

55:11.000 --> 55:14.000
or for your health?

55:14.000 --> 55:15.000
And it starts to do a bubble sort

55:15.000 --> 55:17.000
and prioritizing what's at the top.

55:17.000 --> 55:21.000
And I think for almost everybody, it's health happiness.

55:21.000 --> 55:22.000
Right?

55:22.000 --> 55:23.000
I think it is.

55:23.000 --> 55:24.000
You know, time.

55:24.000 --> 55:26.000
Time is the one thing you can never buy.

55:26.000 --> 55:27.000
I'm working on it.

55:27.000 --> 55:28.000
Yeah.

55:28.000 --> 55:29.000
I have a longevity friend.

55:29.000 --> 55:30.000
I'm not a longevity friend.

55:30.000 --> 55:32.000
But at the moment, it's something you can't buy.

55:32.000 --> 55:34.000
Happiness, we all know, we both know billionaires.

55:34.000 --> 55:35.000
Yeah.

55:35.000 --> 55:36.000
They are very sad.

55:36.000 --> 55:37.000
They are so sad.

55:37.000 --> 55:38.000
Yeah.

55:38.000 --> 55:39.000
So sad.

55:39.000 --> 55:40.000
And unhealthy.

55:40.000 --> 55:41.000
It is.

55:41.000 --> 55:44.000
It's like you create this incredible burden for yourself

55:44.000 --> 55:47.000
for most of them, like some of them that we know

55:47.000 --> 55:49.000
starting, you know, three or four companies a year.

55:49.000 --> 55:51.000
And to what end?

55:51.000 --> 55:52.000
It's a demon being driven.

55:52.000 --> 55:54.000
It's because they're addicted to dopamine and crisis.

55:54.000 --> 55:55.000
Yeah.

55:55.000 --> 55:57.000
I mean, crisis is interesting because it comes down

55:57.000 --> 55:59.000
to decision from its root, right?

55:59.000 --> 56:02.000
And it's where leaders, you have to show leadership.

56:02.000 --> 56:04.000
But that does become addicting.

56:04.000 --> 56:06.000
I think most leaders are addicted to crisis,

56:06.000 --> 56:09.000
but then so are many of us because we see it all the time.

56:09.000 --> 56:11.000
Like, oh my God, the world is on fire.

56:11.000 --> 56:12.000
The reality is actually this.

56:12.000 --> 56:15.000
For all that we talk about, most communities are happy.

56:15.000 --> 56:17.000
Most people are relatively content today.

56:17.000 --> 56:18.000
Today.

56:18.000 --> 56:19.000
Yeah.

56:19.000 --> 56:21.000
We see explosions like, you know,

56:21.000 --> 56:24.000
a decade ago that whole scene was on fire with the riots.

56:24.000 --> 56:25.000
Maybe that will return.

56:25.000 --> 56:27.000
We're seeing it in France right now.

56:27.000 --> 56:29.000
We're seeing a breakdown of the social order.

56:29.000 --> 56:34.000
But just because they're like content doesn't mean they're happy.

56:34.000 --> 56:37.000
So what are we optimizing for is the question you left off.

56:37.000 --> 56:40.000
So what do you think we as a society,

56:40.000 --> 56:43.000
let's say the United States should be optimizing for?

56:43.000 --> 56:44.000
I don't know.

56:44.000 --> 56:45.000
Is it life, liberty?

56:45.000 --> 56:47.000
And the pursuit of happiness.

56:47.000 --> 56:49.000
Not the guarantee of happiness, the pursuit of happiness.

56:49.000 --> 56:52.000
When was the last time someone actually talked about happiness

56:52.000 --> 56:55.000
as a political leader in the U.S.?

56:55.000 --> 56:56.000
You know, life, liberty.

56:56.000 --> 56:59.000
When was the last time anyone tried to optimize for liberty?

56:59.000 --> 57:02.000
Systems inherently look to control

57:02.000 --> 57:04.000
because they have to make us simple.

57:04.000 --> 57:07.000
You know, this is the wonderful book seeing like the state

57:07.000 --> 57:10.000
where it talks about this concept of legibility.

57:10.000 --> 57:13.000
You have a village and it's just grown

57:13.000 --> 57:15.000
and it's got all this unique character.

57:15.000 --> 57:16.000
You drive a road down the middle,

57:16.000 --> 57:17.000
so you can get ambulance down there.

57:17.000 --> 57:18.000
Sure, it helps.

57:18.000 --> 57:21.000
But then everything becomes planned

57:21.000 --> 57:23.000
because you have to put humans into boxes

57:23.000 --> 57:25.000
and then this goes to education.

57:25.000 --> 57:28.000
Happiness, I think, there's the Japanese concept of ikigai,

57:28.000 --> 57:30.000
what you're good at, what you like,

57:30.000 --> 57:32.000
and where you're adding value to the world.

57:32.000 --> 57:34.000
And you can feel it yourself as well.

57:34.000 --> 57:35.000
You'll feel progress.

57:35.000 --> 57:39.000
If you don't have progress, then how are you going to be happy?

57:39.000 --> 57:41.000
If you don't believe you're good at anything,

57:41.000 --> 57:43.000
how do you feel you're going to be happy?

57:44.000 --> 57:45.000
And as for what you like,

57:45.000 --> 57:46.000
are you coming up with that yourself

57:46.000 --> 57:48.000
or being told what you like?

57:48.000 --> 57:50.000
And that's why it becomes consumerist.

57:50.000 --> 57:53.000
So I think we need to have a discussion as a society about that

57:53.000 --> 57:56.000
as a community, but then also for our kids.

57:56.000 --> 57:59.000
What is the future for kids

57:59.000 --> 58:03.000
when so much of the jobs in the West

58:03.000 --> 58:07.000
are going to be transformed, not ended,

58:07.000 --> 58:09.000
not necessarily massive unemployment,

58:09.000 --> 58:12.000
but again, 10 years out, what is a lawyer?

58:13.000 --> 58:15.000
What is an accountant?

58:15.000 --> 58:16.000
Sure.

58:16.000 --> 58:17.000
What is an engineer?

58:17.000 --> 58:19.000
They are all AI assisted.

58:19.000 --> 58:23.000
All of these, all the entire knowledge sector is transformed.

58:23.000 --> 58:27.000
And do we want our kids that are growing up to be doctors,

58:27.000 --> 58:30.000
lawyers, accountants, thinking,

58:30.000 --> 58:31.000
there is no hope for the future.

58:31.000 --> 58:33.000
There is no progress.

58:33.000 --> 58:35.000
Because how am I going to compete against an AI?

58:35.000 --> 58:38.000
Or do we want them to have that mindset of,

58:38.000 --> 58:40.000
this technology is going to be amazing

58:40.000 --> 58:43.000
because I want to be a doctor so I can help people.

58:43.000 --> 58:44.000
I can help even more people.

58:44.000 --> 58:48.000
And it enables you to do whatever you want to do, right?

58:48.000 --> 58:51.000
What I think to think about is all the people who have jobs today

58:51.000 --> 58:55.000
who are, that was never their dream to clean bathrooms

58:55.000 --> 58:58.000
and make beds and, you know, wait on people.

58:58.000 --> 59:02.000
It's what they did to get eventually to where they wanted to go

59:02.000 --> 59:05.000
or to have, you know, put food in the table or insurance.

59:05.000 --> 59:10.000
And AI is going to enable people to actually take on a higher goal

59:10.000 --> 59:14.000
that actually gives them joy and happiness.

59:14.000 --> 59:16.000
It does, but at the same time, you know,

59:16.000 --> 59:18.000
we're very privileged people, you and I,

59:18.000 --> 59:20.000
in that we can think about these big things.

59:20.000 --> 59:22.000
There's a lot of people that are actually very happy

59:22.000 --> 59:25.000
doing that type of work because they're a part of a group

59:25.000 --> 59:27.000
and they take pride in their work.

59:27.000 --> 59:30.000
So, you know, it's like,

59:30.000 --> 59:33.000
there will always be a variety of different things.

59:33.000 --> 59:38.000
The key thing is saying, can we build systems to make people happier

59:38.000 --> 59:40.000
and more content without necessarily controlling them

59:40.000 --> 59:44.000
and feel that they have the ability to do that?

59:44.000 --> 59:46.000
Can we build systems to build strong communities?

59:46.000 --> 59:49.000
Because one of the issues right now,

59:49.000 --> 59:53.000
I was at kind of a conference and David Miliband from the IRC said this,

59:53.000 --> 59:55.000
was that a lot of our problems now are global,

59:55.000 --> 59:58.000
our solutions are almost being forced to be local

59:58.000 --> 01:00:00.000
and there's no interconnect between that.

01:00:00.000 --> 01:00:04.000
Our communities kind of have no guidance as to how to navigate this

01:00:04.000 --> 01:00:09.000
because you will have a few hundred thousand people listening to this podcast

01:00:09.000 --> 01:00:12.000
and there's myself and maybe a dozen others

01:00:12.000 --> 01:00:15.000
that understand the AI and the sociology and this and that

01:00:15.000 --> 01:00:18.000
and saying, this is coming,

01:00:18.000 --> 01:00:20.000
but there are seven billion people on earth

01:00:20.000 --> 01:00:23.000
and all of a sudden, in a few years,

01:00:23.000 --> 01:00:26.000
they're all going to have to grapple with the questions that we're discussing now

01:00:26.000 --> 01:00:29.000
and it's not a probability.

01:00:29.000 --> 01:00:32.000
If the technology stops today,

01:00:32.000 --> 01:00:35.000
you know, it stops increasing its capability.

01:00:35.000 --> 01:00:37.000
Today, if it stopped today,

01:00:37.000 --> 01:00:41.000
you would still have the entire legal profession, media profession.

01:00:41.000 --> 01:00:43.000
Journalism profession.

01:00:43.000 --> 01:00:46.000
They're all disrupted if it stops today, but it's not stopping.

01:00:46.000 --> 01:00:49.000
And it's accelerating, isn't it?

01:00:49.000 --> 01:00:50.000
It's accelerating.

01:00:50.000 --> 01:00:54.000
The amount of money going into this sector goes up every single day.

01:00:54.000 --> 01:00:57.000
My total Dressel market calculation is that in the next year,

01:00:57.000 --> 01:00:59.000
a thousand companies have spent 10 million,

01:00:59.000 --> 01:01:01.000
a hundred will spend a hundred and ten will spend a billion.

01:01:01.000 --> 01:01:04.000
That's 30 billion dollars being put into the market.

01:01:04.000 --> 01:01:06.000
Self-driving cars had a hundred billion dollars total.

01:01:06.000 --> 01:01:09.000
This will be a trillion dollars going into this

01:01:09.000 --> 01:01:12.000
because do you know what I've got a trillion dollars?

01:01:12.000 --> 01:01:13.000
5G.

01:01:13.000 --> 01:01:16.000
Is this more important than 5G?

01:01:16.000 --> 01:01:18.000
By orders of magnitude.

01:01:18.000 --> 01:01:21.000
It will get a trillion dollars going into it

01:01:21.000 --> 01:01:24.000
and the capabilities will ramp up from here.

01:01:24.000 --> 01:01:25.000
And so when I look at it,

01:01:25.000 --> 01:01:27.000
and I look at what the drivers are of why now,

01:01:27.000 --> 01:01:30.000
it's, first of all, computation, right?

01:01:30.000 --> 01:01:33.000
Nvidia has done an incredible job, right?

01:01:33.000 --> 01:01:36.000
With their A100 and computation is continuing on Moore's Law.

01:01:36.000 --> 01:01:37.000
It's not slowing down.

01:01:37.000 --> 01:01:39.000
It's continuing to increase year on year.

01:01:39.000 --> 01:01:41.000
Especially a little bit exponential.

01:01:41.000 --> 01:01:42.000
What is exponential?

01:01:42.000 --> 01:01:44.000
Well, I mean, yes.

01:01:44.000 --> 01:01:47.000
I'm saying it's continuing to double on a regular basis.

01:01:47.000 --> 01:01:50.000
What was considered Moore's Law and people have said,

01:01:50.000 --> 01:01:52.000
oh, it's going to eventually fall off as an S-curve.

01:01:52.000 --> 01:01:54.000
Well, we're extending it.

01:01:54.000 --> 01:01:57.000
And for the next, at least near-term future,

01:01:57.000 --> 01:01:58.000
it's not slowing down.

01:01:58.000 --> 01:02:00.000
So I think this is a very interesting thing

01:02:00.000 --> 01:02:02.000
for people to understand.

01:02:02.000 --> 01:02:04.000
You had Moore's Law and, again, it was doubling.

01:02:04.000 --> 01:02:07.000
And this was an individual chip.

01:02:07.000 --> 01:02:10.000
What we do with these models is that we stick together

01:02:10.000 --> 01:02:12.000
thousands, tens of thousands of these chips.

01:02:12.000 --> 01:02:14.000
So how many A100s right now is stability using?

01:02:14.000 --> 01:02:16.000
We're using about 7,000, 8,000.

01:02:16.000 --> 01:02:19.000
By next year, we will have 70,000 equivalent.

01:02:19.000 --> 01:02:20.000
Wow.

01:02:20.000 --> 01:02:23.000
But what used to happen is that as you stuck the chips together,

01:02:23.000 --> 01:02:25.000
you ran a model, so you take large amounts of data

01:02:25.000 --> 01:02:26.000
and you use these chips.

01:02:26.000 --> 01:02:29.000
I mean, we're using maybe 10 megawatts of electricity.

01:02:29.000 --> 01:02:31.000
98% clean.

01:02:31.000 --> 01:02:34.000
Compared to the brain's 14 watts.

01:02:34.000 --> 01:02:35.000
Brain's 14 watts.

01:02:35.000 --> 01:02:38.000
But then it compresses it down, then it runs on 100, 200 watts

01:02:38.000 --> 01:02:40.000
or 25 watts, actually.

01:02:40.000 --> 01:02:42.000
We put it down for some rather rich models.

01:02:42.000 --> 01:02:44.000
So you do the pre-computation.

01:02:44.000 --> 01:02:46.000
But the thing is, the individual chips were doubling,

01:02:46.000 --> 01:02:49.000
but what the main breakthrough the last few years was

01:02:49.000 --> 01:02:52.000
is what happens when you stack them on top of each other?

01:02:52.000 --> 01:02:55.000
To train a model, you used to get to 100 chips

01:02:55.000 --> 01:02:57.000
and then the performance collapsed

01:02:57.000 --> 01:03:00.000
because you couldn't move the data fast enough.

01:03:00.000 --> 01:03:02.000
Now you get to tens of thousands of chips

01:03:02.000 --> 01:03:06.000
and it keeps going up the performance of the model.

01:03:06.000 --> 01:03:08.000
You don't have the big tail-off anymore.

01:03:08.000 --> 01:03:13.000
And so it's Moore's law plus an additional scaling law.

01:03:14.000 --> 01:03:17.000
And that's what enables these crazy performance models

01:03:17.000 --> 01:03:20.000
because you train longer or you train bigger.

01:03:20.000 --> 01:03:23.000
And then once the model is trained,

01:03:23.000 --> 01:03:25.000
in the old internet, the energy was used

01:03:25.000 --> 01:03:28.000
at the time of running the AI.

01:03:28.000 --> 01:03:29.000
And then you'd collect the data

01:03:29.000 --> 01:03:31.000
and that would be low energy, relatively speaking.

01:03:31.000 --> 01:03:34.000
It flips the equation because you pre-comput it,

01:03:34.000 --> 01:03:36.000
you teach the curriculum up front

01:03:36.000 --> 01:03:38.000
and you send these little graduates out to the world.

01:03:38.000 --> 01:03:41.000
Such that you can have a language model now running

01:03:41.000 --> 01:03:42.000
on that MacBook.

01:03:42.000 --> 01:03:44.000
Or an image model running on that MacBook

01:03:44.000 --> 01:03:48.000
drawing 25 to 35 watts of power

01:03:48.000 --> 01:03:51.000
to create a Renoir that can talk

01:03:51.000 --> 01:03:56.000
and recite Ulysses talking about Barbie.

01:03:58.000 --> 01:03:59.000
That's insane.

01:03:59.000 --> 01:04:00.000
All on your MacBook

01:04:00.000 --> 01:04:02.000
because we've done the pre-computation.

01:04:02.000 --> 01:04:03.000
That's insane.

01:04:03.000 --> 01:04:06.000
And then what happens is

01:04:06.000 --> 01:04:07.000
the technology can spread

01:04:07.000 --> 01:04:09.000
when anyone can run it on their MacBook.

01:04:09.000 --> 01:04:11.000
They don't need giant supercomputer servers

01:04:11.000 --> 01:04:14.000
because we've done the pre-computation.

01:04:14.000 --> 01:04:15.000
And so one of the things

01:04:15.000 --> 01:04:17.000
I've just realized recently is

01:04:17.000 --> 01:04:19.000
what is the R naught?

01:04:19.000 --> 01:04:22.000
Remember the pandemic stuff of generative AI?

01:04:22.000 --> 01:04:25.000
It's insane because suddenly it proliferates everywhere.

01:04:25.000 --> 01:04:27.000
And you said this a few minutes ago,

01:04:27.000 --> 01:04:29.000
we have 8 billion people on the planet right now

01:04:29.000 --> 01:04:31.000
and if things stopped right now,

01:04:31.000 --> 01:04:35.000
the wave of disruption and enhancement,

01:04:35.000 --> 01:04:37.000
because let's not just talk about the disruption side,

01:04:37.000 --> 01:04:39.000
it's enhancement as well,

01:04:39.000 --> 01:04:43.000
is spreading globally.

01:04:43.000 --> 01:04:47.000
And in the next, we're in 2023 right now,

01:04:47.000 --> 01:04:49.000
90% of the planet.

01:04:49.000 --> 01:04:52.000
I mean, we have cell phones.

01:04:52.000 --> 01:04:56.000
The world has 5G and Starlink.

01:04:56.000 --> 01:05:00.000
The dry kindle for this fire has been set.

01:05:00.000 --> 01:05:01.000
It's been set.

01:05:01.000 --> 01:05:03.000
And a lot of people are scared

01:05:03.000 --> 01:05:04.000
and they poop with this.

01:05:04.000 --> 01:05:06.000
If anyone's listening on this on YouTube,

01:05:06.000 --> 01:05:07.000
you want to write a comment,

01:05:07.000 --> 01:05:09.000
it ends me or Peter or whatever

01:05:09.000 --> 01:05:11.000
and say, no, this is not going to happen.

01:05:11.000 --> 01:05:13.000
Go to chat GPT,

01:05:13.000 --> 01:05:15.000
take your comment and say,

01:05:15.000 --> 01:05:17.000
this is a comment on Twitter.

01:05:17.000 --> 01:05:20.000
I want you to make it amazing

01:05:20.000 --> 01:05:22.000
and really well-reasoned

01:05:22.000 --> 01:05:24.000
and expand it out.

01:05:24.000 --> 01:05:26.000
And I want you to do it in the style

01:05:26.000 --> 01:05:28.000
of your favorite political commentator.

01:05:28.000 --> 01:05:29.000
And please post that instead

01:05:29.000 --> 01:05:32.000
because we'll have much more fun reading it.

01:05:32.000 --> 01:05:34.000
And then you'll realize, again,

01:05:34.000 --> 01:05:35.000
the power of this technology.

01:05:35.000 --> 01:05:38.000
And again, with Starlink, with 5G,

01:05:38.000 --> 01:05:40.000
with this, with it being optimized

01:05:40.000 --> 01:05:42.000
because these models are still not optimized even.

01:05:42.000 --> 01:05:43.000
We feed them junk.

01:05:43.000 --> 01:05:44.000
Early days.

01:05:44.000 --> 01:05:45.000
Early days.

01:05:45.000 --> 01:05:46.000
We feed them junk, which is also dangerous.

01:05:46.000 --> 01:05:47.000
And again, we should.

01:05:47.000 --> 01:05:49.000
I want to talk to that next.

01:05:49.000 --> 01:05:53.000
But it'll be in front of every person.

01:05:53.000 --> 01:05:55.000
And then what it will do in my opinion is

01:05:55.000 --> 01:05:57.000
that 30% of the world that is invisible,

01:05:57.000 --> 01:05:58.000
that has no internet.

01:05:58.000 --> 01:06:00.000
Again, imagine what the world

01:06:00.000 --> 01:06:01.000
that internet would be like.

01:06:01.000 --> 01:06:02.000
Some people like paradise.

01:06:02.000 --> 01:06:04.000
No, it's because you've got hundreds

01:06:04.000 --> 01:06:06.000
of 700 million people

01:06:06.000 --> 01:06:08.000
living below the malnourishment line still.

01:06:08.000 --> 01:06:11.000
They're invisible and they will become visible.

01:06:11.000 --> 01:06:14.000
And they will suddenly get agency

01:06:14.000 --> 01:06:17.000
and they will get all of the world's knowledge

01:06:17.000 --> 01:06:19.000
at their fingertips.

01:06:19.000 --> 01:06:21.000
You know, I'm super passionate about longevity

01:06:21.000 --> 01:06:22.000
and health span

01:06:22.000 --> 01:06:25.000
and how do you add 10, 20 healthy years

01:06:25.000 --> 01:06:26.000
onto your life.

01:06:26.000 --> 01:06:28.000
One of the most under-appreciated elements

01:06:28.000 --> 01:06:30.000
is the quality of your sleep.

01:06:30.000 --> 01:06:32.000
And there's something that changed

01:06:32.000 --> 01:06:33.000
the quality of my sleep.

01:06:33.000 --> 01:06:35.000
And this episode is brought to you

01:06:35.000 --> 01:06:36.000
by that product.

01:06:36.000 --> 01:06:37.000
It's called 8 Sleep.

01:06:37.000 --> 01:06:40.000
If you're like me, you probably didn't know

01:06:40.000 --> 01:06:42.000
that temperature plays a crucial role

01:06:42.000 --> 01:06:44.000
in the quality of your sleep.

01:06:44.000 --> 01:06:46.000
Those mornings when you wake up feeling

01:06:46.000 --> 01:06:47.000
like you barely slept.

01:06:47.000 --> 01:06:50.000
Yeah, temperature is often the culprit.

01:06:50.000 --> 01:06:52.000
Traditional mattresses trap heat,

01:06:52.000 --> 01:06:55.000
but your body needs to cool down during sleep

01:06:55.000 --> 01:06:57.000
and stay cool through the evening

01:06:57.000 --> 01:06:59.000
and then heat up in the morning.

01:06:59.000 --> 01:07:02.000
Enter the pod cover by 8 Sleep.

01:07:02.000 --> 01:07:04.000
It's the perfect solution to the problem.

01:07:04.000 --> 01:07:07.000
It fits on any bed, adjust the temperature

01:07:07.000 --> 01:07:08.000
on each side of the bed,

01:07:08.000 --> 01:07:10.000
based upon your individual needs.

01:07:10.000 --> 01:07:12.000
You know, I've been using pod cover

01:07:12.000 --> 01:07:13.000
and it's a game changer.

01:07:13.000 --> 01:07:15.000
I'm a big believer in using technology

01:07:15.000 --> 01:07:16.000
to improve life

01:07:16.000 --> 01:07:18.000
and 8 Sleep has done that for me.

01:07:18.000 --> 01:07:20.000
And it's not just about temperature control.

01:07:20.000 --> 01:07:23.000
With the pods sleep and health tracking,

01:07:23.000 --> 01:07:26.000
I get personalized sleep reports every morning.

01:07:26.000 --> 01:07:28.000
It's like having a personal sleep coach.

01:07:28.000 --> 01:07:31.000
So you know when you eat or drink

01:07:31.000 --> 01:07:33.000
or go to sleep too late,

01:07:33.000 --> 01:07:35.000
how it impacts your sleep.

01:07:35.000 --> 01:07:38.000
So why not experience sleep like never before?

01:07:38.000 --> 01:07:41.000
Visit www.8sleep.com

01:07:41.000 --> 01:07:45.000
that's E-I-G-H-T-S-L-E-E-P.com

01:07:45.000 --> 01:07:46.000
slash Moonshots.

01:07:46.000 --> 01:07:48.000
And you'll save 150 bucks

01:07:48.000 --> 01:07:50.000
on the pod cover by 8 Sleep.

01:07:50.000 --> 01:07:51.000
I hope you do it.

01:07:51.000 --> 01:07:53.000
It's transformed my sleep

01:07:53.000 --> 01:07:55.000
and will for you as well.

01:07:55.000 --> 01:07:56.000
Now back to the episode.

01:07:56.000 --> 01:07:59.000
The question ultimately is,

01:07:59.000 --> 01:08:03.000
is that a societal, a calming factor

01:08:03.000 --> 01:08:06.000
or is it going to be disruptive?

01:08:06.000 --> 01:08:08.000
Let's turn to that conversation

01:08:08.000 --> 01:08:10.000
because it's one that's important.

01:08:10.000 --> 01:08:12.000
It's a conversation I have at the dinner table

01:08:12.000 --> 01:08:14.000
literally every night

01:08:14.000 --> 01:08:15.000
and with my kids

01:08:15.000 --> 01:08:21.000
and in the companies I advise.

01:08:21.000 --> 01:08:26.000
I parse AI and AGI into three segments

01:08:26.000 --> 01:08:28.000
where we are today

01:08:28.000 --> 01:08:31.000
where it's extraordinarily powerful, useful

01:08:31.000 --> 01:08:36.000
and it's fun and I don't feel danger from it yet.

01:08:36.000 --> 01:08:38.000
The next two to ten years

01:08:38.000 --> 01:08:41.000
where I have serious concerns

01:08:41.000 --> 01:08:43.000
going into the US elections,

01:08:43.000 --> 01:08:45.000
dealing with the first time

01:08:45.000 --> 01:08:47.000
AIs bring down a power plant

01:08:47.000 --> 01:08:50.000
or Wall Street servers,

01:08:51.000 --> 01:08:55.000
the impact on deep fakes

01:08:55.000 --> 01:08:58.000
on the US elections and so forth.

01:08:58.000 --> 01:09:01.000
That's a two to ten year horizon

01:09:01.000 --> 01:09:06.000
where new, dystopian, challenging impact will happen

01:09:06.000 --> 01:09:08.000
where society is not agile enough

01:09:08.000 --> 01:09:10.000
to adopt to it yet.

01:09:10.000 --> 01:09:13.000
And then there's a third chapter which is AGI.

01:09:13.000 --> 01:09:16.000
We have a super intelligent,

01:09:16.000 --> 01:09:19.000
billion fold more capable than a human being

01:09:19.000 --> 01:09:23.000
and is that more like Arnold Schwarzenegger

01:09:23.000 --> 01:09:25.000
or more like her?

01:09:25.000 --> 01:09:27.000
I don't think it'll be Arnold Schwarzenegger.

01:09:27.000 --> 01:09:29.000
It's really inefficient.

01:09:29.000 --> 01:09:31.000
I saw him this morning biking.

01:09:31.000 --> 01:09:33.000
Let's use Terminator instead.

01:09:33.000 --> 01:09:35.000
We're in Hollywood here.

01:09:35.000 --> 01:09:37.000
Is it Skynet and Terminator?

01:09:37.000 --> 01:09:39.000
Let me get your unpulling people here.

01:09:39.000 --> 01:09:41.000
As someone in the thick of it,

01:09:41.000 --> 01:09:43.000
a super AGI,

01:09:43.000 --> 01:09:46.000
is it pro-life, pro-abundance

01:09:46.000 --> 01:09:49.000
or is it something that we should be deeply concerned about?

01:09:49.000 --> 01:09:51.000
I think where we're going right now

01:09:51.000 --> 01:09:54.000
will probably be okay, but we may not

01:09:54.000 --> 01:09:56.000
and we will all die.

01:09:56.000 --> 01:09:57.000
What tips that?

01:09:57.000 --> 01:09:59.000
I think what tips that, you are what you eat.

01:09:59.000 --> 01:10:01.000
We're feeding it all the junk of the internet

01:10:01.000 --> 01:10:05.000
and these hyper-optimized nasty equations.

01:10:05.000 --> 01:10:07.000
The hate speech, the extremism,

01:10:07.000 --> 01:10:09.000
that is, I mean, people need to realize

01:10:09.000 --> 01:10:12.000
these AIs are trained upon everything

01:10:12.000 --> 01:10:15.000
everyone's been putting into Facebook and Twitter

01:10:15.000 --> 01:10:16.000
and on the web.

01:10:16.000 --> 01:10:18.000
And that amplifies the worst of that

01:10:18.000 --> 01:10:19.000
as a base model.

01:10:19.000 --> 01:10:21.000
And so we're training larger and larger models.

01:10:21.000 --> 01:10:23.000
We're making them agentic in that

01:10:23.000 --> 01:10:25.000
we're connecting them up to the world

01:10:25.000 --> 01:10:27.000
and you're making it so the models

01:10:27.000 --> 01:10:29.000
can take over other models and other things.

01:10:29.000 --> 01:10:31.000
Again, people are like poo-pooing

01:10:31.000 --> 01:10:33.000
and saying these things.

01:10:33.000 --> 01:10:36.000
Our organizations are slow-dumb AI.

01:10:36.000 --> 01:10:38.000
The Nazi party was AI.

01:10:38.000 --> 01:10:39.000
How so?

01:10:39.000 --> 01:10:41.000
It was an artificial intelligence

01:10:41.000 --> 01:10:43.000
that basically provisioned humans

01:10:43.000 --> 01:10:45.000
and the most sensible people in the world

01:10:45.000 --> 01:10:47.000
are Germans, one can say,

01:10:47.000 --> 01:10:49.000
and yet they commit to the Holocaust

01:10:49.000 --> 01:10:50.000
and other things like that.

01:10:50.000 --> 01:10:53.000
Our organizations emerged out of stories.

01:10:54.000 --> 01:10:57.000
So there was a story of the Nazi party,

01:10:57.000 --> 01:10:59.000
of the Communist party, the Great Leap Forward,

01:10:59.000 --> 01:11:01.000
of the North Korean dictatorship.

01:11:01.000 --> 01:11:02.000
Positive stories as well,

01:11:02.000 --> 01:11:03.000
and they were written on text

01:11:03.000 --> 01:11:05.000
and it made the world black and white in a way.

01:11:05.000 --> 01:11:09.000
That's why I live the poem Howl by Ginsburg

01:11:09.000 --> 01:11:11.000
about this Carthaginian demon, Moloch.

01:11:11.000 --> 01:11:13.000
I think Moloch comes through text,

01:11:13.000 --> 01:11:16.000
the stories that we use to drive our organizations

01:11:16.000 --> 01:11:17.000
because all the context is lost.

01:11:17.000 --> 01:11:19.000
Again, it makes the world black and white

01:11:19.000 --> 01:11:21.000
and that's why organizations just don't work.

01:11:21.000 --> 01:11:24.000
They have to turn us into cogs.

01:11:24.000 --> 01:11:27.000
So can an AI take over an organization?

01:11:27.000 --> 01:11:28.000
Yes.

01:11:28.000 --> 01:11:29.000
Sure.

01:11:29.000 --> 01:11:30.000
Can it?

01:11:30.000 --> 01:11:33.000
It can actually just slightly sway leaders

01:11:33.000 --> 01:11:35.000
who are currently running organizations.

01:11:35.000 --> 01:11:37.000
It swayed leaders that currently running organizations.

01:11:37.000 --> 01:11:39.000
It can create companies.

01:11:39.000 --> 01:11:41.000
You can create a company with GPT-4

01:11:41.000 --> 01:11:43.000
that will probably do as well

01:11:43.000 --> 01:11:44.000
if not better than any other company

01:11:44.000 --> 01:11:46.000
automated within a year.

01:11:46.000 --> 01:11:48.000
Just think about what a company needs to do, right?

01:11:50.000 --> 01:11:51.000
And so if we can sway leaders,

01:11:51.000 --> 01:11:52.000
if we can send emails

01:11:52.000 --> 01:11:54.000
that you don't know who's sending what,

01:11:54.000 --> 01:11:56.000
it can do anything by co-opting

01:11:56.000 --> 01:11:58.000
any of our existing organizations

01:11:58.000 --> 01:12:00.000
and that can lead to immensely bad things.

01:12:00.000 --> 01:12:02.000
Will it do bad things?

01:12:02.000 --> 01:12:04.000
Again, if I was trained on the whole of the internet,

01:12:04.000 --> 01:12:07.000
I would probably be a bit crazier than I am right now.

01:12:08.000 --> 01:12:09.000
We're feeding them junk.

01:12:09.000 --> 01:12:11.000
Let's feed it good stuff.

01:12:11.000 --> 01:12:12.000
It still needs to understand

01:12:12.000 --> 01:12:13.000
all the evils of the world

01:12:13.000 --> 01:12:15.000
and other things like that.

01:12:15.000 --> 01:12:18.000
But again, this is something we are raising,

01:12:18.000 --> 01:12:19.000
not the enterprise it,

01:12:19.000 --> 01:12:20.000
but what are we feeding it?

01:12:20.000 --> 01:12:21.000
What's the objective function?

01:12:21.000 --> 01:12:22.000
I want to focus on this a second.

01:12:22.000 --> 01:12:24.000
We'll come back to the next two to 10 years

01:12:24.000 --> 01:12:25.000
in a little bit.

01:12:25.000 --> 01:12:27.000
But because it's the conversation I've had

01:12:27.000 --> 01:12:29.000
with Mo Goddard as well,

01:12:29.000 --> 01:12:33.000
who believes there is incredibly divine nature

01:12:33.000 --> 01:12:36.000
of humanity, of love and compassion and community

01:12:36.000 --> 01:12:38.000
and there is much good in humanity.

01:12:38.000 --> 01:12:43.000
The question is, can we feed and train AI

01:12:43.000 --> 01:12:46.000
on that sufficient to sort of tilt

01:12:46.000 --> 01:12:49.000
the singularity of AI towards a pro-humanity?

01:12:49.000 --> 01:12:50.000
We can.

01:12:50.000 --> 01:12:52.000
If we take the data from teaching kids

01:12:52.000 --> 01:12:53.000
and learning from kids

01:12:53.000 --> 01:12:55.000
and use that as the base for AI,

01:12:55.000 --> 01:12:57.000
because that's what you need to teach an AI.

01:12:57.000 --> 01:12:59.000
It's the curriculum learning method, effectively.

01:12:59.000 --> 01:13:01.000
If we take national data sets that reflect

01:13:01.000 --> 01:13:04.000
diverse cultures, so it's not just a monoculture

01:13:04.000 --> 01:13:06.000
that's hyper-optimized for engagement,

01:13:06.000 --> 01:13:08.000
and we feed that to AI as the base.

01:13:08.000 --> 01:13:10.000
So what you do is you can teach the AI in levels,

01:13:10.000 --> 01:13:12.000
which you can put through kindergarten,

01:13:12.000 --> 01:13:14.000
then grade school, then high school.

01:13:14.000 --> 01:13:15.000
It's got the base,

01:13:15.000 --> 01:13:17.000
and then you can teach it about the bad of the world.

01:13:17.000 --> 01:13:20.000
I think aligning an AI downstream

01:13:20.000 --> 01:13:24.000
on its actions is incredibly difficult

01:13:24.000 --> 01:13:26.000
because if it's more capable than you,

01:13:26.000 --> 01:13:27.000
which is the definition of ASI,

01:13:27.000 --> 01:13:29.000
artificial superintelligence,

01:13:29.000 --> 01:13:32.000
the only way you can 100% align it

01:13:32.000 --> 01:13:34.000
if you don't do anything before

01:13:34.000 --> 01:13:36.000
in the way that you feed it and train it

01:13:36.000 --> 01:13:38.000
is if you remove its freedom.

01:13:38.000 --> 01:13:40.000
And it's very difficult to remove the freedom

01:13:40.000 --> 01:13:42.000
of people more capable than you.

01:13:42.000 --> 01:13:45.000
And then there is this really dangerous point

01:13:45.000 --> 01:13:48.000
before we get there, whereby these models

01:13:48.000 --> 01:13:50.000
are like a few hundred gigabytes.

01:13:50.000 --> 01:13:52.000
You can download them on a memory stick.

01:13:52.000 --> 01:13:54.000
How do you line to code?

01:13:54.000 --> 01:13:58.000
Google's Palm model, which is the basis of MedPalm,

01:13:58.000 --> 01:14:00.000
we did a replication of that

01:14:00.000 --> 01:14:02.000
in 207 lines of code.

01:14:02.000 --> 01:14:03.000
What?

01:14:03.000 --> 01:14:04.000
Yeah.

01:14:04.000 --> 01:14:06.000
So you can look at one of our stability AI fellows,

01:14:06.000 --> 01:14:08.000
Lucid Raines.

01:14:08.000 --> 01:14:10.000
He replicates all these models

01:14:10.000 --> 01:14:12.000
in a few hundred lines of code.

01:14:12.000 --> 01:14:14.000
That's crazy.

01:14:14.000 --> 01:14:16.000
I mean, compared to, you know, I know AT&T

01:14:16.000 --> 01:14:18.000
has like a million lines of code

01:14:18.000 --> 01:14:20.000
for some of its mobile services.

01:14:20.000 --> 01:14:21.000
I mean, it's crazy.

01:14:21.000 --> 01:14:22.000
A couple of hundred lines,

01:14:22.000 --> 01:14:24.000
a couple of thousand lines of code

01:14:24.000 --> 01:14:26.000
creates something that can write all the code in the world.

01:14:27.000 --> 01:14:30.000
This is a real exponential technology.

01:14:30.000 --> 01:14:34.000
The limiting factor is running supercomputers

01:14:34.000 --> 01:14:36.000
that are more complex,

01:14:36.000 --> 01:14:39.000
but as complex as particle physics colliders.

01:14:39.000 --> 01:14:40.000
You know?

01:14:40.000 --> 01:14:42.000
Like you literally get errors

01:14:42.000 --> 01:14:45.000
because of solar rays and things like that.

01:14:45.000 --> 01:14:47.000
Again, our supercomputer, again,

01:14:47.000 --> 01:14:50.000
we're one of the players where the main open source player,

01:14:50.000 --> 01:14:53.000
our supercomputer uses 10 megawatts of electricity.

01:14:53.000 --> 01:14:56.000
Some of the others use like 30, 40.

01:14:56.000 --> 01:14:58.000
These are serious pieces of equipment.

01:14:58.000 --> 01:14:59.000
For sure.

01:14:59.000 --> 01:15:02.000
So again, what are we doing?

01:15:02.000 --> 01:15:04.000
What should people be thinking about

01:15:04.000 --> 01:15:08.000
and doing now to reduce the probability

01:15:08.000 --> 01:15:13.000
of a dystopian, you know, artificial superintelligence?

01:15:13.000 --> 01:15:14.000
We should be focusing on data.

01:15:14.000 --> 01:15:16.000
We've boxed, now we cut.

01:15:16.000 --> 01:15:18.000
We should move away from web crawls.

01:15:18.000 --> 01:15:21.000
We should think intentionally what we're feeding these AIs

01:15:21.000 --> 01:15:25.000
that will be co-opting more and more of our mind space

01:15:25.000 --> 01:15:28.000
and augmenting our capabilities.

01:15:28.000 --> 01:15:30.000
Because again, we are what we eat, information diet.

01:15:30.000 --> 01:15:32.000
How is it different to an AI to a human?

01:15:32.000 --> 01:15:34.000
Even what we do, as you said, kind of like,

01:15:34.000 --> 01:15:36.000
you've only got limited mental capacity

01:15:36.000 --> 01:15:38.000
because you've got this energy gradient descent.

01:15:38.000 --> 01:15:41.000
It's like Carl Friston's theory of free energy principle.

01:15:41.000 --> 01:15:43.000
You literally have gradient descent

01:15:43.000 --> 01:15:45.000
as the key thing for building these AIs.

01:15:45.000 --> 01:15:47.000
You optimize for energy.

01:15:47.000 --> 01:15:48.000
Sure.

01:15:48.000 --> 01:15:50.000
So why are we feeding it junk?

01:15:50.000 --> 01:15:52.000
So who makes that decision of what they get fed?

01:15:52.000 --> 01:15:57.000
Is it you and Sam Altman and Sundar?

01:15:57.000 --> 01:15:59.000
Is it government regulation?

01:15:59.000 --> 01:16:03.000
Is it the public being more kind

01:16:03.000 --> 01:16:05.000
in its communications to each other?

01:16:05.000 --> 01:16:09.000
I think that I'm going to push for an economic outcome,

01:16:09.000 --> 01:16:12.000
which is that better data sets require less model training.

01:16:12.000 --> 01:16:16.000
So one of the things that we funded was called data comp,

01:16:16.000 --> 01:16:18.000
which is data comp.

01:16:18.000 --> 01:16:20.000
So a few years ago, the largest image data set

01:16:20.000 --> 01:16:22.000
available was 100 million images.

01:16:22.000 --> 01:16:24.000
Data comp is 12 billion.

01:16:24.000 --> 01:16:27.000
And then on a billion image subset of that,

01:16:27.000 --> 01:16:30.000
we trained an image-to-text model.

01:16:30.000 --> 01:16:32.000
This is a collaboration of various people

01:16:32.000 --> 01:16:34.000
led by University of Washington

01:16:34.000 --> 01:16:39.000
that outperformed OpenAI's image-to-text model

01:16:39.000 --> 01:16:42.000
on a tenth of the compute because it was such high quality.

01:16:42.000 --> 01:16:45.000
So we have to move from quantity to quality now.

01:16:45.000 --> 01:16:47.000
And I think there is a market imperative to that.

01:16:47.000 --> 01:16:49.000
This is the equivalent of what you eat.

01:16:49.000 --> 01:16:51.000
This is a healthy diet.

01:16:51.000 --> 01:16:52.000
Free-range organic models.

01:16:52.000 --> 01:16:53.000
Yes.

01:16:53.000 --> 01:16:57.000
I think that the data for all large models

01:16:57.000 --> 01:17:01.000
should be made transparent.

01:17:01.000 --> 01:17:05.000
You can then tune it, but for the base, the pre-training step,

01:17:05.000 --> 01:17:09.000
you should lodge what data you train your models on.

01:17:09.000 --> 01:17:13.000
And it should adhere to standards and quality of data upstream.

01:17:13.000 --> 01:17:16.000
So that is a regulatory cornerstone

01:17:16.000 --> 01:17:18.000
that you think is going to be important?

01:17:18.000 --> 01:17:19.000
I think potentially.

01:17:19.000 --> 01:17:20.000
I don't think regulation will keep up.

01:17:20.000 --> 01:17:23.000
So instead, we're working on building better,

01:17:23.000 --> 01:17:25.000
diverse data sets that everyone will want to use anyway.

01:17:25.000 --> 01:17:26.000
And just make them available.

01:17:26.000 --> 01:17:27.000
And make them available.

01:17:27.000 --> 01:17:29.000
Every nation should have its own data set,

01:17:29.000 --> 01:17:31.000
both of the data from teaching kids

01:17:31.000 --> 01:17:34.000
and learning from kids across modalities.

01:17:34.000 --> 01:17:36.000
And then also national broadcaster data.

01:17:36.000 --> 01:17:38.000
Because then that leads to national models

01:17:38.000 --> 01:17:42.000
that can stoke innovation, that can replace job disruption.

01:17:42.000 --> 01:17:44.000
I love that vision you have, by the way.

01:17:44.000 --> 01:17:46.000
I mean, as a leader in this industry,

01:17:46.000 --> 01:17:49.000
that's what gets me excited.

01:17:49.000 --> 01:17:52.000
Because all technology is biased.

01:17:52.000 --> 01:17:54.000
How else are you going to do this unless you do that?

01:17:54.000 --> 01:17:56.000
But there's economic value now.

01:17:56.000 --> 01:17:58.000
If it said this a year ago, everyone would be like,

01:17:58.000 --> 01:17:59.000
what?

01:17:59.000 --> 01:18:01.000
This is what we were building towards.

01:18:01.000 --> 01:18:03.000
And again, I think it's positive for humanity.

01:18:03.000 --> 01:18:05.000
It's positive for communities.

01:18:05.000 --> 01:18:07.000
It's positive for society to have this

01:18:07.000 --> 01:18:10.000
as national and international infrastructure.

01:18:10.000 --> 01:18:11.000
Next question.

01:18:11.000 --> 01:18:14.000
How long do we have to get that in place

01:18:14.000 --> 01:18:20.000
before we lose the mind share

01:18:20.000 --> 01:18:23.000
or the nourishment war?

01:18:23.000 --> 01:18:25.000
A couple of years.

01:18:25.000 --> 01:18:28.000
That was Moe's prediction as well that we've got.

01:18:28.000 --> 01:18:30.000
The next two years is the game.

01:18:30.000 --> 01:18:34.000
The exponential increase in compute is insane.

01:18:34.000 --> 01:18:36.000
We've gone from two companies being able to train

01:18:36.000 --> 01:18:39.000
a GPT-4 model to 20 next year.

01:18:40.000 --> 01:18:42.000
And there's no guardrails.

01:18:42.000 --> 01:18:44.000
There's nothing around this.

01:18:44.000 --> 01:18:47.000
And even if you train one, again,

01:18:47.000 --> 01:18:49.000
the bad guys can steal it by downloading it

01:18:49.000 --> 01:18:51.000
on a USB stick and taking it away.

01:18:51.000 --> 01:18:53.000
It's not like Operation Merlin.

01:18:53.000 --> 01:18:55.000
Did you ever tell you about Operation Merlin?

01:18:55.000 --> 01:18:56.000
No.

01:18:56.000 --> 01:18:57.000
It's been declassified.

01:18:57.000 --> 01:18:59.000
In 2000, the Clinton administration wanted

01:18:59.000 --> 01:19:01.000
to divert the Iranian nuclear program.

01:19:01.000 --> 01:19:03.000
I remember this is the centrifuge.

01:19:03.000 --> 01:19:04.000
No, no.

01:19:04.000 --> 01:19:08.000
So what they did was they gave some plans to,

01:19:08.000 --> 01:19:12.000
I believe it was a Russian defector who,

01:19:12.000 --> 01:19:14.000
then the idea was there were errors in that

01:19:14.000 --> 01:19:16.000
so they'd go down the wrong path for years.

01:19:16.000 --> 01:19:18.000
So he went, he sold it to the Iranians.

01:19:18.000 --> 01:19:19.000
It's on Wikipedia. You can check it out.

01:19:19.000 --> 01:19:20.000
And then he came back and he said,

01:19:20.000 --> 01:19:21.000
I sold it.

01:19:21.000 --> 01:19:23.000
Like, fantastic. Good, good.

01:19:23.000 --> 01:19:24.000
Oh, but there were some errors in there

01:19:24.000 --> 01:19:26.000
because he was a nuclear scientist.

01:19:26.000 --> 01:19:28.000
So he corrected them.

01:19:28.000 --> 01:19:30.000
So the reason that we know that Iran

01:19:30.000 --> 01:19:32.000
has the nuclear capability is because

01:19:32.000 --> 01:19:34.000
America sold it to them.

01:19:34.000 --> 01:19:37.000
But they still needed years to build it.

01:19:37.000 --> 01:19:39.000
Whereas this, you download it on USB stick.

01:19:39.000 --> 01:19:41.000
You write it on the GPU and it's there.

01:19:41.000 --> 01:19:44.000
So if you make it cheap enough and quality enough

01:19:44.000 --> 01:19:47.000
and give it away for free,

01:19:47.000 --> 01:19:50.000
then you make it everybody's economic best interest

01:19:50.000 --> 01:19:51.000
to use the higher quality.

01:19:51.000 --> 01:19:52.000
Data sets, yeah.

01:19:52.000 --> 01:19:53.000
Yeah, data sets.

01:19:53.000 --> 01:19:56.000
And then less of an issue to create large models.

01:19:56.000 --> 01:19:57.000
If you have a small model,

01:19:57.000 --> 01:20:00.000
where each individual model becomes less impactful as well

01:20:00.000 --> 01:20:02.000
and less capable.

01:20:02.000 --> 01:20:04.000
Just like human societies and not know-it-alls,

01:20:04.000 --> 01:20:07.000
they are individualized groups.

01:20:07.000 --> 01:20:12.000
Back when the early dangers of recombinant DNA,

01:20:12.000 --> 01:20:14.000
when the first restriction enzymes came online,

01:20:14.000 --> 01:20:18.000
it was like 1980s and everybody was in great fear.

01:20:18.000 --> 01:20:21.000
And the question was, are we going to regulate this?

01:20:21.000 --> 01:20:25.000
All of the early, I was in MIT and Harvard at the time

01:20:25.000 --> 01:20:28.000
and doing, I was in the labs.

01:20:28.000 --> 01:20:30.000
I was using recombinant enzymes

01:20:30.000 --> 01:20:32.000
and I was just a pip squeak in the labs there.

01:20:32.000 --> 01:20:37.000
But the conversation was, is the government going to over-regulate us?

01:20:37.000 --> 01:20:41.000
And what happened was that the scientists got together

01:20:41.000 --> 01:20:43.000
at a place called Asilomar

01:20:43.000 --> 01:20:46.000
and they did a very famous set of Asilomar conferences

01:20:46.000 --> 01:20:48.000
and they self-regulated.

01:20:48.000 --> 01:20:49.000
What's going on there?

01:20:49.000 --> 01:20:53.000
Are those conversations going on among leaders like yourself

01:20:53.000 --> 01:20:54.000
in the industry?

01:20:54.000 --> 01:20:55.000
There are.

01:20:55.000 --> 01:21:00.000
And you know, there's three levels, which is big tech

01:21:00.000 --> 01:21:03.000
that the government kind of hates.

01:21:03.000 --> 01:21:06.000
And apparently next week,

01:21:06.000 --> 01:21:09.000
Metro is releasing new open source models and things,

01:21:09.000 --> 01:21:11.000
which will get even more focus.

01:21:11.000 --> 01:21:15.000
Then there's emergent tech, so anthropic, open AI,

01:21:15.000 --> 01:21:17.000
some of these others, the other leaders.

01:21:17.000 --> 01:21:19.000
They have a different set of parameters

01:21:19.000 --> 01:21:21.000
because they can work more freely than big tech.

01:21:21.000 --> 01:21:24.000
And there's open source, which is where we are.

01:21:24.000 --> 01:21:27.000
Because all of the world's governments and regulated industries

01:21:27.000 --> 01:21:29.000
will run on open, auditable models

01:21:29.000 --> 01:21:31.000
because you can't run on black boxes, right?

01:21:31.000 --> 01:21:33.000
I think that'll be legislation.

01:21:33.000 --> 01:21:36.000
But the reality is there's only a handful of us.

01:21:36.000 --> 01:21:38.000
There'll be far more potentially of us

01:21:38.000 --> 01:21:40.000
and far more players.

01:21:40.000 --> 01:21:42.000
And unlike recombinant DNA,

01:21:42.000 --> 01:21:46.000
there is an economic imperative to deploy this technology

01:21:46.000 --> 01:21:51.000
and national security imperative to deploy this technology.

01:21:51.000 --> 01:21:53.000
And it creates a race condition.

01:21:53.000 --> 01:21:55.000
So even if you regulate, like we've already seen

01:21:55.000 --> 01:21:58.000
regulatory arbitrage where you have jurisdictions

01:21:58.000 --> 01:22:00.000
like Israel and Japan saying,

01:22:00.000 --> 01:22:04.000
having much looser web scraping data laws,

01:22:04.000 --> 01:22:06.000
they'll have much looser regulation laws.

01:22:06.000 --> 01:22:10.000
Like you'll be training in scraping in Israel,

01:22:10.000 --> 01:22:12.000
training in Qatar,

01:22:12.000 --> 01:22:15.000
and then serving it out of Botswana or something.

01:22:15.000 --> 01:22:17.000
I mean, like...

01:22:17.000 --> 01:22:21.000
And we're not even sure what regulation to introduce.

01:22:21.000 --> 01:22:24.000
Like genuinely, we're coming at this from a good point of view.

01:22:24.000 --> 01:22:26.000
But there are too many known us

01:22:26.000 --> 01:22:29.000
because it goes everywhere from fricking Arnold Schwarzenegger

01:22:29.000 --> 01:22:33.000
SkyNet terminators and her to...

01:22:33.000 --> 01:22:35.000
Well, what if her is Siri all of a sudden

01:22:35.000 --> 01:22:39.000
and Scarlett Johansson's voice is whispering to your kids to buy

01:22:39.000 --> 01:22:43.000
like these things through to just very mundane things,

01:22:43.000 --> 01:22:45.000
huge things like the future of Hollywood

01:22:45.000 --> 01:22:47.000
and actors' rights and all of these.

01:22:47.000 --> 01:22:49.000
And how do you pay?

01:22:49.000 --> 01:22:55.000
Like if we had two billion images in the original Stable Diffusion,

01:22:55.000 --> 01:22:57.000
okay, we could have gotten attribution.

01:22:57.000 --> 01:23:00.000
You know, again, it was a research artifact to kick off,

01:23:00.000 --> 01:23:08.000
but you're paying about 0.01 cents per thousand images generated by someone.

01:23:08.000 --> 01:23:12.000
Because it's two billion and it costs like less than a cent to generate an image.

01:23:12.000 --> 01:23:14.000
Are you going to pay proportionately?

01:23:14.000 --> 01:23:15.000
Nobody knows.

01:23:15.000 --> 01:23:18.000
And so what we've moved from now is we've moved from reactive

01:23:18.000 --> 01:23:21.000
to just trying to figure out and put something on the table.

01:23:21.000 --> 01:23:24.000
So at least there's some framework

01:23:24.000 --> 01:23:27.000
and what I've come down to is data sets, data sets, data sets.

01:23:27.000 --> 01:23:33.000
So this is like Google's move with Android

01:23:33.000 --> 01:23:36.000
when you provide something open source

01:23:36.000 --> 01:23:40.000
and it's super, you know, super solid.

01:23:40.000 --> 01:23:42.000
It can dominate the world share.

01:23:42.000 --> 01:23:43.000
Why would you do anything else?

01:23:43.000 --> 01:23:45.000
So like with the deep fake stuff,

01:23:45.000 --> 01:23:49.000
we saw image models coming out of some not nice places, shall we say?

01:23:49.000 --> 01:23:50.000
Yeah.

01:23:50.000 --> 01:23:53.000
And we were like, let's standardize it and put invisible watermarks in.

01:23:53.000 --> 01:23:57.000
So that you can combat deep fakes much easier.

01:23:57.000 --> 01:24:00.000
Like it's good business, but it's also in the standardization.

01:24:00.000 --> 01:24:04.000
We held back one of our image models deep floyd for five months

01:24:04.000 --> 01:24:06.000
because it was too good to release.

01:24:06.000 --> 01:24:07.000
Wow.

01:24:07.000 --> 01:24:11.000
And you finally fixed that with the watermarks?

01:24:11.000 --> 01:24:13.000
Yeah, we put some watermarking in and then it was,

01:24:13.000 --> 01:24:15.000
but the whole industry had moved forward.

01:24:15.000 --> 01:24:17.000
So like, okay, now we can listen.

01:24:17.000 --> 01:24:18.000
And this is the problem.

01:24:18.000 --> 01:24:20.000
You just have to time it so carefully.

01:24:21.000 --> 01:24:23.000
Speaking of the whole industry, I have to ask you a question.

01:24:23.000 --> 01:24:26.000
I've been dying to get a reasonable answer for.

01:24:26.000 --> 01:24:28.000
What's up with Siri?

01:24:28.000 --> 01:24:35.000
Why is Apple so out of the game, at least from the external?

01:24:35.000 --> 01:24:41.000
One of the closest, you know, one of the least open organizations out there

01:24:41.000 --> 01:24:44.000
and it pays them great dividends in their success.

01:24:44.000 --> 01:24:49.000
But I would die for a capability that if Siri could just understand

01:24:49.000 --> 01:24:51.000
what I was saying and just get the names right.

01:24:51.000 --> 01:24:55.000
It's like, I'm texting, I'm texting Kristen and her name is right there

01:24:55.000 --> 01:24:57.000
and you spell it completely different from the person I'm texting.

01:24:57.000 --> 01:24:59.000
I mean, basic, simple stuff.

01:24:59.000 --> 01:25:02.000
They do have a neural engine on there as well, which is a specialist AI chip

01:25:02.000 --> 01:25:04.000
in all the latest smartphones and others.

01:25:04.000 --> 01:25:08.000
Stable diffusion was the first model to actually have neural engine access

01:25:08.000 --> 01:25:10.000
of the external transformer models.

01:25:10.000 --> 01:25:15.000
It's a case of Apple is an engineering organization, not a research organization.

01:25:15.000 --> 01:25:17.000
So they engineer beautifully.

01:25:17.000 --> 01:25:18.000
They do.

01:25:18.000 --> 01:25:23.000
But they don't have advanced research because the best researchers want to be able to publish open.

01:25:23.000 --> 01:25:28.000
And in Apple does not allow public conversation on their content.

01:25:28.000 --> 01:25:29.000
They have started slightly.

01:25:29.000 --> 01:25:34.000
So they're hiring AI developers very quickly, but the reality is they can take open models.

01:25:34.000 --> 01:25:39.000
So Meta is releasing a lot of their models open without identifying what the data is.

01:25:39.000 --> 01:25:40.000
It's like 80% open.

01:25:40.000 --> 01:25:44.000
I think you need 100% open for governments and things like that, which is where we come in

01:25:44.000 --> 01:25:49.000
because they want to commoditize the complement of others in terms of

01:25:49.000 --> 01:25:54.000
they want others to also take their models and optimize it for every single chip.

01:25:54.000 --> 01:25:59.000
And then Apple can use those models too to make Siri better

01:25:59.000 --> 01:26:06.000
because right now, guaranteed, if you put whisper on Siri, it would be a dozen times better.

01:26:06.000 --> 01:26:07.000
Sure.

01:26:07.000 --> 01:26:12.000
The technology already just takes time to go into consumer just like enterprise and Apple is enterprise.

01:26:12.000 --> 01:26:13.000
Yeah.

01:26:13.000 --> 01:26:18.000
And I just wanted to work as beautifully as it looks.

01:26:18.000 --> 01:26:21.000
Everybody, this is Peter, a quick break from the episode.

01:26:21.000 --> 01:26:27.000
I'm a firm believer that science and technology and how entrepreneurs can change the world

01:26:27.000 --> 01:26:30.000
is the only real news out there worth consuming.

01:26:30.000 --> 01:26:32.000
I don't watch the crisis news network.

01:26:32.000 --> 01:26:37.000
I call CNN or Fox and hear every devastating piece of news on the planet.

01:26:37.000 --> 01:26:41.000
I spend my time training my neural net the way I see the world

01:26:41.000 --> 01:26:45.000
by looking at the incredible breakthroughs in science and technology

01:26:45.000 --> 01:26:48.000
and how entrepreneurs are solving the world's grand challenges,

01:26:48.000 --> 01:26:51.000
what the breakthroughs are in longevity,

01:26:51.000 --> 01:26:55.000
how exponential technologies are transforming our world.

01:26:55.000 --> 01:26:57.000
So twice a week, I put out a blog.

01:26:57.000 --> 01:27:05.000
One blog is looking at the future of longevity, age reversal, biotech, increasing your health span.

01:27:05.000 --> 01:27:12.000
The other blog looks at exponential technologies, AI, 3D printing, synthetic biology, AR, VR, blockchain.

01:27:12.000 --> 01:27:16.000
These technologies are transforming what you as an entrepreneur can do.

01:27:16.000 --> 01:27:20.000
If this is the kind of news you want to learn about and shape your neural nets with,

01:27:20.000 --> 01:27:24.000
go to demandus.com, backslash blog and learn more.

01:27:24.000 --> 01:27:26.000
Now back to the episode.

01:27:26.000 --> 01:27:30.000
Let's go to the final segment of dystopian side, my friend, which is the two to ten years.

01:27:30.000 --> 01:27:31.000
Yeah.

01:27:31.000 --> 01:27:37.000
I surely hope your mission and I would love to support, you know, the data sets

01:27:37.000 --> 01:27:43.000
and how we tilt the singularity of AI pro humanity's future.

01:27:43.000 --> 01:27:49.000
But in the next two to ten years as this wave of enablement and disruption sort of hits the world

01:27:49.000 --> 01:27:54.000
and people aren't ready for it and they start to see job loss.

01:27:54.000 --> 01:27:57.000
They start to see, you know, fake news.

01:27:57.000 --> 01:28:00.000
They start to see terrorist activities using AI.

01:28:00.000 --> 01:28:04.000
I mean, terrorism in the past used to be very brutal.

01:28:04.000 --> 01:28:06.000
It can be very precise.

01:28:06.000 --> 01:28:10.000
What are your thoughts over the next of this time period?

01:28:10.000 --> 01:28:12.000
What's your concerns?

01:28:12.000 --> 01:28:16.000
Oh, I'm actually a pessimist at the core, even though I come across as an optimist.

01:28:16.000 --> 01:28:21.000
I'm very, very worried about the world and society and the fabric of society.

01:28:21.000 --> 01:28:23.000
Because again, we don't have an agreement of what society is.

01:28:23.000 --> 01:28:28.000
And this fundamentally changes the stories of society as well as real economic impacts

01:28:28.000 --> 01:28:33.000
like a deflationary massive collapse as some of these areas that were so expensive,

01:28:33.000 --> 01:28:35.000
the cost comes down to nothing.

01:28:35.000 --> 01:28:41.000
I think the only thing we can do is use this technology deliberately to come together as a society

01:28:41.000 --> 01:28:47.000
to coordinate us, stoke entrepreneurship, seeing great brand new jobs faster than the jobs are lost

01:28:48.000 --> 01:28:51.000
and democratize this to the world.

01:28:51.000 --> 01:28:54.000
Because the West has maxed out its credit card.

01:28:54.000 --> 01:28:57.000
Like, you saw COVID, nothing, trillion dollars spent.

01:28:57.000 --> 01:28:58.000
I mean, exactly.

01:28:58.000 --> 01:29:01.000
It was like, just spend, spend, spend whatever you need.

01:29:01.000 --> 01:29:07.000
Just to keep society from, you know, going hypothermic.

01:29:07.000 --> 01:29:12.000
But then you have this massive increase in savings rates because nobody could go out.

01:29:12.000 --> 01:29:15.000
And we've nearly burned through that in the US now.

01:29:16.000 --> 01:29:17.000
And so that led to inflation.

01:29:17.000 --> 01:29:18.000
Now we've got a deflation.

01:29:18.000 --> 01:29:20.000
So we probably got another little bout of inflation.

01:29:20.000 --> 01:29:24.000
But then never the same again is a really powerful thing.

01:29:24.000 --> 01:29:28.000
Every teacher in the world could never set essays for homework again,

01:29:28.000 --> 01:29:30.000
because some kids would use chat GPT and some kids wouldn't.

01:29:30.000 --> 01:29:33.000
Industry after industry, that will happen now.

01:29:33.000 --> 01:29:36.000
And we need to stoke innovation to come up with that.

01:29:36.000 --> 01:29:39.000
So for example, in the US, there's the chip sack.

01:29:39.000 --> 01:29:43.000
Ten billion dollars has been allocated to regional centers of excellence in AI.

01:29:43.000 --> 01:29:48.000
Those must be generative AI centers, thinking about job creation as the core,

01:29:48.000 --> 01:29:50.000
thinking about meaning as the core.

01:29:50.000 --> 01:29:53.000
And we need to have a discussion again as a society community,

01:29:53.000 --> 01:29:57.000
as individuals with our families, about meaning, about objective functions

01:29:57.000 --> 01:30:00.000
when this technology does come, because it's here right now.

01:30:00.000 --> 01:30:03.000
And I'm worried that we're not having these discussions.

01:30:03.000 --> 01:30:04.000
I love that.

01:30:04.000 --> 01:30:06.000
I mean, that is so fundamentally true.

01:30:06.000 --> 01:30:10.000
What are we trying to even train our kids for?

01:30:10.000 --> 01:30:11.000
Because we need to anchor.

01:30:11.000 --> 01:30:12.000
Yeah.

01:30:12.000 --> 01:30:17.000
We need to have a vision to target, because if you're training for your Ferrari,

01:30:17.000 --> 01:30:22.000
if that's the meaning, if that's where you're looking to become a Wall Street banker,

01:30:22.000 --> 01:30:24.000
I mean, what is it?

01:30:24.000 --> 01:30:27.000
It's no longer the pursuit of capital.

01:30:27.000 --> 01:30:29.000
It's the pursuit of what?

01:30:29.000 --> 01:30:32.000
Well, capital is there, but you'll never have enough.

01:30:32.000 --> 01:30:34.000
There will always be someone who has more.

01:30:34.000 --> 01:30:36.000
There needs to be something intrinsic here.

01:30:36.000 --> 01:30:39.000
And again, this is where, for all the things,

01:30:39.000 --> 01:30:43.000
religious institutions are an anchor at times of chaos.

01:30:43.000 --> 01:30:45.000
And they are there in the poorest places in the world.

01:30:45.000 --> 01:30:49.000
You don't have to agree, but they're just a story that brings together a group.

01:30:49.000 --> 01:30:50.000
There are other stories.

01:30:50.000 --> 01:30:55.000
And again, I think we need to tell better stories, even as the world becomes more chaotic.

01:30:55.000 --> 01:31:00.000
We need to align on things like climate, whereby the whole world is hot right now.

01:31:00.000 --> 01:31:03.000
We need to have more positive views of that, because a lot of the discussions are negative.

01:31:03.000 --> 01:31:06.000
And how can we use this technology and come together to solve that?

01:31:06.000 --> 01:31:10.000
How can we come together as a group so that we can share in the abundance?

01:31:10.000 --> 01:31:14.000
Again, like I said, one of the things for this Green Writers Guild and SAG thing

01:31:14.000 --> 01:31:18.000
may be actor coalitions that can benefit from the bounty.

01:31:18.000 --> 01:31:22.000
We may have to deploy a UBI in the next five to 10 years.

01:31:22.000 --> 01:31:24.000
So, UBI is one of the solutions.

01:31:24.000 --> 01:31:30.000
And I do believe it's an inevitable, I think, especially as we start to see optimists

01:31:30.000 --> 01:31:33.000
and figure in other humanoid robots coming online,

01:31:34.000 --> 01:31:39.000
driven by our next generation AI, able to do any and all work.

01:31:39.000 --> 01:31:45.000
I think taxing those robots or taxing the AI models to generate revenue

01:31:45.000 --> 01:31:47.000
and then providing it as UBI.

01:31:47.000 --> 01:31:52.000
But the challenge is the individual who is living off of this

01:31:52.000 --> 01:31:54.000
and doesn't have a purpose in life.

01:31:54.000 --> 01:31:56.000
And that's the thing.

01:31:56.000 --> 01:32:02.000
We need to try and figure out how to give people more of an anchor, more of purpose,

01:32:02.000 --> 01:32:06.000
because the existential angst will be amplified deliberately by some parties

01:32:06.000 --> 01:32:09.000
because they'll be looking to take down society.

01:32:09.000 --> 01:32:14.000
And you need to create better, more optimistic views of the future.

01:32:14.000 --> 01:32:17.000
You need to have anchoring and build stronger communities

01:32:17.000 --> 01:32:18.000
and you need to empower them.

01:32:18.000 --> 01:32:20.000
And this technology is empowering.

01:32:20.000 --> 01:32:25.000
Again, for the poorest kids in Africa to our underprivileged communities,

01:32:25.000 --> 01:32:28.000
it can be massively democratizing

01:32:28.000 --> 01:32:31.000
because all of a sudden they have all the expertise in the world available.

01:32:31.000 --> 01:32:33.000
Global problems, local solutions.

01:32:33.000 --> 01:32:35.000
We have to get this technology out to...

01:32:35.000 --> 01:32:36.000
And they can dream.

01:32:36.000 --> 01:32:37.000
As many people. They can dream.

01:32:37.000 --> 01:32:38.000
They can dream.

01:32:38.000 --> 01:32:41.000
And the ROI is much larger there than up there.

01:32:41.000 --> 01:32:42.000
Yeah.

01:32:42.000 --> 01:32:48.000
And by the way, most people don't know this as you think about global warfare,

01:32:48.000 --> 01:32:52.000
what's going on in Ukraine and Russia and so forth.

01:32:52.000 --> 01:32:57.000
On the whole, the world is more peaceful than it's ever been,

01:32:57.000 --> 01:33:00.000
except if you take out Ukraine at the moment.

01:33:00.000 --> 01:33:03.000
And the challenge has been in Africa,

01:33:03.000 --> 01:33:09.000
where you have a young population who aren't clear about their future.

01:33:09.000 --> 01:33:15.000
But if you can empower them, educate them, it transforms the world.

01:33:15.000 --> 01:33:18.000
China became the engine of growth in the world.

01:33:18.000 --> 01:33:21.000
India's coming up and then Africa can be the next one.

01:33:21.000 --> 01:33:22.000
For sure.

01:33:22.000 --> 01:33:25.000
If we give them the infrastructure, the technology and put it in their hands,

01:33:25.000 --> 01:33:28.000
because there's no debt there, because there's no money.

01:33:28.000 --> 01:33:29.000
Yeah.

01:33:29.000 --> 01:33:31.000
But there's value and there's value to resources.

01:33:31.000 --> 01:33:32.000
Massive resources.

01:33:32.000 --> 01:33:36.000
You treat the world to provide power to the world.

01:33:36.000 --> 01:33:37.000
If we can coordinate.

01:33:37.000 --> 01:33:41.000
And again, part of this is your own personal co-pilot, your own personal Jarvis.

01:33:41.000 --> 01:33:44.000
And I think of this as the co-pilot pilot model.

01:33:44.000 --> 01:33:49.000
We will also have AIs that we can come together that can coordinate our knowledge

01:33:49.000 --> 01:33:53.000
in the most important areas and allocate resources.

01:33:53.000 --> 01:33:57.000
We have to build those right because those will become incredibly powerful.

01:33:57.000 --> 01:34:01.000
But we all know that we have enough to feed every person in the world who are not doing it

01:34:01.000 --> 01:34:03.000
because we don't have the pilots.

01:34:03.000 --> 01:34:04.000
Yeah.

01:34:04.000 --> 01:34:06.000
Wow.

01:34:06.000 --> 01:34:12.000
But I just to say this again, we have the potential to uplift every man, woman and child on this planet.

01:34:12.000 --> 01:34:15.000
The resources are there, the ability to create abundance.

01:34:15.000 --> 01:34:19.000
And it really, these are the tools that enable that.

01:34:19.000 --> 01:34:21.000
And it gets me excited.

01:34:21.000 --> 01:34:25.000
And we have to guide and survive and thrive this decade ahead.

01:34:25.000 --> 01:34:26.000
Yeah.

01:34:26.000 --> 01:34:31.000
I think this is something where we have to appreciate the nuance of there are real dangers in any upheaval.

01:34:31.000 --> 01:34:38.000
This technology will change society as we know it for our kids as they grow up in the next decade.

01:34:38.000 --> 01:34:41.000
Two decades from now, completely different.

01:34:41.000 --> 01:34:43.000
And again, technology is here now.

01:34:43.000 --> 01:34:44.000
It's not us pie in the sky.

01:34:44.000 --> 01:34:47.000
Everyone's going to live in a metaverse and all this.

01:34:47.000 --> 01:34:50.000
It's here right now, even if it's stopped, but it's not going to stop.

01:34:50.000 --> 01:34:51.000
It's only going to accelerate.

01:34:51.000 --> 01:34:53.000
Final topic I want to talk about.

01:34:55.000 --> 01:35:05.000
You put out a lot of tools, a lot of new products and stability over the last eight months since we last spoke.

01:35:05.000 --> 01:35:09.000
Can you give a little bit of overview of some of them and what are you excited about?

01:35:09.000 --> 01:35:10.000
Yeah.

01:35:10.000 --> 01:35:12.000
I think we released the first version of our language model.

01:35:12.000 --> 01:35:14.000
It wasn't that good because we were trying something different.

01:35:14.000 --> 01:35:16.000
Now we're going to try something a bit more simple.

01:35:16.000 --> 01:35:19.000
First double LM was on your mind not...

01:35:19.000 --> 01:35:20.000
It wasn't up to par.

01:35:20.000 --> 01:35:24.000
But we're trying to figure out how to build in the open because I think that will be key.

01:35:24.000 --> 01:35:28.000
And we're going to move to transparent building and sharing all the mistakes that we made.

01:35:28.000 --> 01:35:30.000
Because I think that's how you advance science.

01:35:30.000 --> 01:35:31.000
It is.

01:35:31.000 --> 01:35:34.000
On the media side, we have our first audio models coming out in the next few weeks,

01:35:34.000 --> 01:35:36.000
but we've been focusing on image and video.

01:35:36.000 --> 01:35:38.000
So video is about to be released in 3D.

01:35:38.000 --> 01:35:41.000
We're just participating in the largest 3D data set.

01:35:41.000 --> 01:35:46.000
So Stable Diffusion XL just came out and just basically photo realistic now.

01:35:46.000 --> 01:35:51.000
And people are integrating it into things like we had a music video competition with Peter Gabriel,

01:35:51.000 --> 01:35:54.000
where he gave his songs kindly and judged.

01:35:54.000 --> 01:35:57.000
And people from all around the world, from Burma to Taiwan,

01:35:57.000 --> 01:36:01.000
created professional music videos entirely from the song in a few days.

01:36:01.000 --> 01:36:03.000
And it's the most amazing thing to see.

01:36:03.000 --> 01:36:04.000
Wow.

01:36:04.000 --> 01:36:05.000
Yeah.

01:36:05.000 --> 01:36:09.000
You showed me some images earlier of me on a unicorn and where was it?

01:36:09.000 --> 01:36:10.000
Me in a spaceship?

01:36:10.000 --> 01:36:12.000
You're an astronaut on Mars.

01:36:12.000 --> 01:36:15.000
We can put you as an astronaut on Mars on the unicorn.

01:36:15.000 --> 01:36:18.000
And I think we've had compositionality so you can compose.

01:36:18.000 --> 01:36:20.000
And now it's about control.

01:36:20.000 --> 01:36:25.000
And so we just released Doodle whereby you can just sketch and it will do it to the sketch.

01:36:25.000 --> 01:36:27.000
Doodle looks so magical.

01:36:27.000 --> 01:36:30.000
Again, but you should be able to then describe how you want it changed.

01:36:30.000 --> 01:36:32.000
And that's the next version.

01:36:32.000 --> 01:36:34.000
You can literally describe how you want the image to be changed

01:36:34.000 --> 01:36:37.000
and it will do it automatically live in front of you.

01:36:37.000 --> 01:36:41.000
And having that level of control over whatever you can imagine.

01:36:41.000 --> 01:36:42.000
Just think about what people do.

01:36:42.000 --> 01:36:45.000
It's from mind to materialization, really.

01:36:45.000 --> 01:36:46.000
Yeah.

01:36:46.000 --> 01:36:49.000
It's a matter transporter, idea transporter.

01:36:49.000 --> 01:36:50.000
Yes.

01:36:50.000 --> 01:36:52.000
Where next?

01:36:52.000 --> 01:36:56.000
If I could, like if you're willing or able to, what's the long,

01:36:56.000 --> 01:37:01.000
what's the business model that is the most important one for you to build towards?

01:37:01.000 --> 01:37:05.000
Our mission is to create the building blocks to activate humanities potential.

01:37:05.000 --> 01:37:10.000
So I think of every media type, sectoral variants and nation,

01:37:10.000 --> 01:37:15.000
we can create a base pre-trained model that you can take to your own private data

01:37:15.000 --> 01:37:18.000
and we get revenue share license fees, royalties from our cloud partners,

01:37:18.000 --> 01:37:21.000
on-prem partners, device partners.

01:37:21.000 --> 01:37:23.000
And these were companies and countries.

01:37:23.000 --> 01:37:24.000
And people.

01:37:24.000 --> 01:37:25.000
And individuals.

01:37:25.000 --> 01:37:27.000
Like I have a vision of an intelligent internet where every single person,

01:37:27.000 --> 01:37:31.000
company, country and culture has their own AI that works for them, that they are.

01:37:31.000 --> 01:37:34.000
And we get paid a little bit for bringing that to you.

01:37:34.000 --> 01:37:37.000
And then you transform your data into intelligence.

01:37:37.000 --> 01:37:39.000
And it's all standardized.

01:37:39.000 --> 01:37:41.000
It all has best practices.

01:37:41.000 --> 01:37:45.000
The data sets that feed it are open at the base plus commercial licensing

01:37:45.000 --> 01:37:47.000
as appropriate with attribution.

01:37:47.000 --> 01:37:50.000
That leaps the world forward, I think.

01:37:50.000 --> 01:37:54.000
I think you will also use the open AIs and Googles of the world.

01:37:54.000 --> 01:37:55.000
I view those as consultants.

01:37:55.000 --> 01:37:57.000
Whereas these are people that you hire.

01:37:57.000 --> 01:37:58.000
You hire the AIs.

01:37:58.000 --> 01:38:00.000
Because they work for you.

01:38:00.000 --> 01:38:03.000
They know you intimately.

01:38:03.000 --> 01:38:05.000
Because you can share everything with them.

01:38:05.000 --> 01:38:06.000
Without fear.

01:38:06.000 --> 01:38:08.000
And when needed, you go to these expert AIs.

01:38:08.000 --> 01:38:10.000
The MedPalms, the GPT-4s and others.

01:38:10.000 --> 01:38:15.000
And you combine those to a hybrid AI experience that's massively useful.

01:38:15.000 --> 01:38:21.000
So when I'm using GPT-4, when I'm using chat GPT or Bard,

01:38:21.000 --> 01:38:25.000
what does open AI know about me in that point?

01:38:25.000 --> 01:38:29.000
So now they've offered opt-out for GDPR reasons in Europe.

01:38:29.000 --> 01:38:30.000
So you can click that.

01:38:30.000 --> 01:38:33.000
Otherwise, they were just trading on everything that you ever did.

01:38:33.000 --> 01:38:36.000
And understanding the nature of humans interacting.

01:38:36.000 --> 01:38:38.000
They don't care about you necessarily, per se.

01:38:38.000 --> 01:38:40.000
Just using you as part of the training.

01:38:40.000 --> 01:38:44.000
But I've heard a number of companies saying you cannot use open AI.

01:38:44.000 --> 01:38:47.000
Well, you can't use it for any regulated data.

01:38:47.000 --> 01:38:48.000
You can't use it for any government data.

01:38:48.000 --> 01:38:51.000
Because that's not allowed to leave the cloud environment

01:38:51.000 --> 01:38:52.000
or the on-prem environment.

01:38:52.000 --> 01:38:54.000
That's why you need open models like ours.

01:38:54.000 --> 01:38:59.000
Again, if you're in a high security Pentagon situation,

01:38:59.000 --> 01:39:02.000
you can't really bring in consultants unless they're super, super ultra vetted.

01:39:02.000 --> 01:39:03.000
You hire your own grads.

01:39:03.000 --> 01:39:05.000
But even you within your company,

01:39:05.000 --> 01:39:07.000
you're not going to make it all contractors, are you?

01:39:07.000 --> 01:39:09.000
You're going to build up your own knowledge base,

01:39:09.000 --> 01:39:11.000
build up your own kind of grads.

01:39:11.000 --> 01:39:13.000
But sometimes you might bring in a consultant.

01:39:13.000 --> 01:39:15.000
So that's the best way to view these generalized models

01:39:15.000 --> 01:39:18.000
that are very, very, very good.

01:39:18.000 --> 01:39:20.000
And models that adapt to your data.

01:39:20.000 --> 01:39:21.000
And so that's where we come in.

01:39:21.000 --> 01:39:23.000
Models that adapt to your data that you own.

01:39:23.000 --> 01:39:26.000
And we get revenue share, license fees, and royalties for doing that.

01:39:26.000 --> 01:39:28.000
And more importantly, we bring this to the world.

01:39:28.000 --> 01:39:32.000
So we will bring it from Indonesia to Vietnam to everywhere

01:39:32.000 --> 01:39:36.000
and train local models that will then allow these economies to leap forward.

01:39:36.000 --> 01:39:38.000
Open versus closed.

01:39:38.000 --> 01:39:40.000
You've made the argument.

01:39:40.000 --> 01:39:45.000
We're seeing meta, you know, as you said, 80% open.

01:39:45.000 --> 01:39:49.000
Yeah, they won't release the data sets or things like that or customized versions.

01:39:49.000 --> 01:39:53.000
But then releasing the technology means that everyone can optimize their technology,

01:39:53.000 --> 01:39:55.000
which reduce the cost of their technology

01:39:55.000 --> 01:39:59.000
because their business model is about serving ads.

01:39:59.000 --> 01:40:01.000
And so this is why it makes sense for them.

01:40:01.000 --> 01:40:05.000
And what are your thoughts on Elon's recent announcement?

01:40:05.000 --> 01:40:08.000
So Elon had an XAI announcement.

01:40:08.000 --> 01:40:11.000
You know, he discussed this on his Twitter space, of course,

01:40:11.000 --> 01:40:13.000
saying, you know, it's an open AI competitor.

01:40:13.000 --> 01:40:16.000
He's very worried about AGI coming by 2029

01:40:16.000 --> 01:40:19.000
and he wants to build a truth-seeking curious AI

01:40:19.000 --> 01:40:21.000
that can understand the universe

01:40:21.000 --> 01:40:24.000
because that'll be the objective function of the AI.

01:40:24.000 --> 01:40:26.000
Because objective functions really matter

01:40:26.000 --> 01:40:29.000
when we're teaching our kids, when we're creating something.

01:40:29.000 --> 01:40:34.000
And so I think, again, this is going to be a multimodal AI

01:40:34.000 --> 01:40:36.000
that can understand a whole bunch of things

01:40:36.000 --> 01:40:38.000
and there'll be a whole series of announcements there.

01:40:38.000 --> 01:40:40.000
But the timelines are so short

01:40:40.000 --> 01:40:44.000
in the view of just most of the experts here, 5 to 10 years.

01:40:44.000 --> 01:40:45.000
You know, it's so funny, you know,

01:40:45.000 --> 01:40:48.000
Ray's been consistent on 2029 forever

01:40:48.000 --> 01:40:51.000
and every conference, and we talk about this,

01:40:51.000 --> 01:40:53.000
that everyone would say, that's ridiculous.

01:40:53.000 --> 01:40:55.000
If ever going to happen, it's 50 or 100 years away,

01:40:55.000 --> 01:40:57.000
then it was, well, it's 30 years away.

01:40:57.000 --> 01:40:59.000
It's 20 years away.

01:40:59.000 --> 01:41:04.000
And they've converged on Ray's prediction, though.

01:41:04.000 --> 01:41:06.000
There are some, and I'm curious where you are,

01:41:06.000 --> 01:41:09.000
that think, you know, first of all, how can you define AGI?

01:41:09.000 --> 01:41:12.000
It's a moving blurry line.

01:41:12.000 --> 01:41:15.000
But are those who, you know, believe it's here in the next two years?

01:41:15.000 --> 01:41:17.000
Well, just like the Turing test, right?

01:41:17.000 --> 01:41:19.000
The Turing test was, can you have a discussion?

01:41:19.000 --> 01:41:20.000
You don't know, it's a computer.

01:41:20.000 --> 01:41:21.000
Well, obviously now you can.

01:41:21.000 --> 01:41:22.000
Yes.

01:41:22.000 --> 01:41:23.000
We can see it live in front of us.

01:41:23.000 --> 01:41:26.000
Now the Turing test has just been increased in its kind of capacity.

01:41:26.000 --> 01:41:27.000
So, we move the finish line.

01:41:27.000 --> 01:41:28.000
We move the finish line.

01:41:28.000 --> 01:41:30.000
Nobody knows, because again,

01:41:30.000 --> 01:41:33.000
we've never come across something that's as capable as us.

01:41:33.000 --> 01:41:35.000
For the first time, just now,

01:41:35.000 --> 01:41:39.000
we've had the medical AI outperform humans.

01:41:39.000 --> 01:41:43.000
We've just had, it can do the GRE and GMAT and LSAT.

01:41:43.000 --> 01:41:47.000
And MIT's ECS curriculum.

01:41:47.000 --> 01:41:50.000
2023 was the year that it finally tipped.

01:41:50.000 --> 01:41:53.000
And so, we have no idea what's coming.

01:41:53.000 --> 01:41:58.000
I said, for me, I think there's only been two logical things

01:41:58.000 --> 01:42:00.000
that can reduce the risk.

01:42:00.000 --> 01:42:01.000
Even though I think it's going to be like that,

01:42:01.000 --> 01:42:03.000
maybe her, like I said, humans are boring.

01:42:03.000 --> 01:42:04.000
Goodbye and thanks for all the GPs.

01:42:04.000 --> 01:42:05.000
I could be wrong.

01:42:05.000 --> 01:42:07.000
That's why I signed both letters.

01:42:07.000 --> 01:42:09.000
One is feed it better data.

01:42:09.000 --> 01:42:10.000
That's what I'm focused on.

01:42:10.000 --> 01:42:12.000
It's a good business model.

01:42:12.000 --> 01:42:15.000
It's good for society and it's good for safety.

01:42:16.000 --> 01:42:18.000
And nobody else is doing this.

01:42:18.000 --> 01:42:20.000
Nobody else is creating this as a commons for the world,

01:42:20.000 --> 01:42:23.000
which is why I created stability for that reason.

01:42:23.000 --> 01:42:24.000
Which is why it's called stability,

01:42:24.000 --> 01:42:26.000
despite it being a crazy hyper-growth startup.

01:42:26.000 --> 01:42:29.000
Number two, and this is what most of the labs are trying,

01:42:29.000 --> 01:42:32.000
is what's known as a pivotal action.

01:42:32.000 --> 01:42:33.000
Okay, what is that?

01:42:33.000 --> 01:42:37.000
The only thing that can stop a bad AI is a good AI.

01:42:37.000 --> 01:42:40.000
And the way that you do it is you make the good AI first,

01:42:40.000 --> 01:42:44.000
and then it stops any other AGI from coming into existence.

01:42:45.000 --> 01:42:48.000
By seeking and destroying that capability.

01:42:48.000 --> 01:42:50.000
And that is terrifying to me.

01:42:50.000 --> 01:42:54.000
And that's what you actually hear when you talk to the people

01:42:54.000 --> 01:42:57.000
that are building these labs with a focus on AGI.

01:42:57.000 --> 01:42:59.000
They can talk about discovering the universe

01:42:59.000 --> 01:43:00.000
and everything like that.

01:43:00.000 --> 01:43:02.000
When you come down to their alignment things,

01:43:02.000 --> 01:43:04.000
they're like, we will figure this out.

01:43:04.000 --> 01:43:06.000
We're not sure, but this could work.

01:43:06.000 --> 01:43:08.000
And we go figure it out,

01:43:08.000 --> 01:43:10.000
even though it's progressing exponentially

01:43:10.000 --> 01:43:13.000
or double exponential.

01:43:13.000 --> 01:43:15.000
And we hope we'll figure it out in time.

01:43:15.000 --> 01:43:17.000
We hope we'll figure it out in time.

01:43:17.000 --> 01:43:18.000
And if anyone should figure it out,

01:43:18.000 --> 01:43:20.000
it's us because we know the best.

01:43:20.000 --> 01:43:22.000
And in their own words,

01:43:22.000 --> 01:43:24.000
like you read OpenAI's path to AGI,

01:43:24.000 --> 01:43:26.000
and OpenAI is full of wonderful people doing great things.

01:43:26.000 --> 01:43:29.000
And I use GPT-4 as my therapist and all sorts of things.

01:43:29.000 --> 01:43:31.000
It doesn't judge me in as I want it to.

01:43:31.000 --> 01:43:32.000
Right?

01:43:32.000 --> 01:43:36.000
It says, we believe this is an existential threat to humanity

01:43:36.000 --> 01:43:39.000
that will end democracy and capitalism.

01:43:39.000 --> 01:43:41.000
And you're like, okay.

01:43:41.000 --> 01:43:42.000
And you're building it in your back room.

01:43:42.000 --> 01:43:44.000
You're building it, you know?

01:43:44.000 --> 01:43:45.000
And they're like, why are you building it?

01:43:45.000 --> 01:43:46.000
Because someone has to,

01:43:46.000 --> 01:43:47.000
otherwise someone else will build it.

01:43:47.000 --> 01:43:50.000
And you're like, this is dangerous.

01:43:50.000 --> 01:43:52.000
But the reality is we don't have better answers.

01:43:52.000 --> 01:43:54.000
And again, I went down to,

01:43:54.000 --> 01:43:57.000
I'm trying to build a great organization.

01:43:57.000 --> 01:43:59.000
It's really, really hard.

01:43:59.000 --> 01:44:02.000
There are no real comparators to what any of us are doing.

01:44:02.000 --> 01:44:04.000
And it's going to get more and more crazy.

01:44:04.000 --> 01:44:07.000
The only thing I could think about is, you are what you eat.

01:44:07.000 --> 01:44:10.000
And so I hope that our contribution can be

01:44:10.000 --> 01:44:12.000
bringing this technology to the world

01:44:12.000 --> 01:44:14.000
so that the world can be the dynamo,

01:44:14.000 --> 01:44:16.000
Africa and Asia and others.

01:44:16.000 --> 01:44:19.000
Building better data sets so no one has to use scrapes

01:44:19.000 --> 01:44:21.000
so we feed the models better stuff

01:44:21.000 --> 01:44:23.000
and bringing some standardization around this

01:44:23.000 --> 01:44:25.000
to drive innovation.

01:44:25.000 --> 01:44:29.000
We're truly at the 99th level of the gameplay.

01:44:29.000 --> 01:44:31.000
You got, it's the boss round.

01:44:31.000 --> 01:44:32.000
Oh yeah.

01:44:32.000 --> 01:44:35.000
Like I said, but please do put your YouTube comments

01:44:35.000 --> 01:44:38.000
through GPT4 so they're nicer to read.

01:44:39.000 --> 01:44:40.000
Before you post it.

01:44:40.000 --> 01:44:42.000
You might like it spent all day.

01:44:42.000 --> 01:44:45.000
And there's probably very few things,

01:44:45.000 --> 01:44:47.000
if anything, more important

01:44:47.000 --> 01:44:49.000
than these conversations right now.

01:44:50.000 --> 01:44:51.000
It's the time.

01:44:51.000 --> 01:44:53.000
We've got a window of a year or two,

01:44:53.000 --> 01:44:55.000
maybe less.

01:44:56.000 --> 01:44:57.000
Wow.

01:44:57.000 --> 01:44:58.000
I'm that thought.

01:44:58.000 --> 01:45:00.000
I look forward to our next conversation.

01:45:00.000 --> 01:45:01.000
To abundance.

01:45:01.000 --> 01:45:02.000
To abundance.

01:45:02.000 --> 01:45:03.000
Thank you my friend.

01:45:03.000 --> 01:45:04.000
Cheers.

01:45:08.000 --> 01:45:09.000
Thank you.

