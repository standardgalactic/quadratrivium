1
00:00:00,000 --> 00:00:04,600
It seemed to me that a huge revolution was going on.

2
00:00:04,600 --> 00:00:05,760
Now it's changed.

3
00:00:05,760 --> 00:00:08,240
I am optimistic, but I'm also worried about it.

4
00:00:11,240 --> 00:00:15,760
I've been in the field of AI for 60 years.

5
00:00:15,760 --> 00:00:17,240
I was 14.

6
00:00:17,240 --> 00:00:20,400
I met Marvin Minsky who was in his 30s.

7
00:00:20,400 --> 00:00:22,400
Frank Rosenblatt created the Perceptron,

8
00:00:22,400 --> 00:00:24,040
the first popular neural net.

9
00:00:24,040 --> 00:00:26,160
But in the early years, it was really not clear

10
00:00:26,160 --> 00:00:28,280
that neural nets could do anything successful.

11
00:00:28,320 --> 00:00:30,840
And they're showing now that this is really the path

12
00:00:30,840 --> 00:00:32,800
to artificial general intelligence.

13
00:00:32,800 --> 00:00:34,960
It's not just us versus AI.

14
00:00:34,960 --> 00:00:37,120
The intelligence that we're creating

15
00:00:37,120 --> 00:00:39,440
is adding AI to our own brains.

16
00:00:39,440 --> 00:00:43,480
2045 is when I said we will actually multiply

17
00:00:43,480 --> 00:00:46,400
our intelligence millions fold.

18
00:00:46,400 --> 00:00:48,160
And that's going to be true of everybody.

19
00:00:48,160 --> 00:00:51,640
And we'll be able to get rid of terrible lives that we see

20
00:00:51,640 --> 00:00:54,680
through poverty and lack of access to information.

21
00:00:55,680 --> 00:00:56,680
Great.

22
00:00:56,680 --> 00:00:57,680
Good morning.

23
00:00:57,680 --> 00:00:58,680
Good morning to you.

24
00:00:58,680 --> 00:01:00,680
It's great to be with you, Peter.

25
00:01:00,680 --> 00:01:02,680
And also Salim.

26
00:01:02,680 --> 00:01:04,680
I've done lots of presentations with Peter.

27
00:01:04,680 --> 00:01:09,680
It's really remarkable what you've contributed.

28
00:01:09,680 --> 00:01:14,680
So I just want to share a few ideas.

29
00:01:14,680 --> 00:01:18,680
I've been following large language models

30
00:01:18,680 --> 00:01:20,680
for almost three years.

31
00:01:20,680 --> 00:01:24,680
There was Lambda, now barred from Google,

32
00:01:24,680 --> 00:01:27,680
different GPT versions from OpenAI.

33
00:01:27,680 --> 00:01:32,680
It seemed to me that a huge revolution was going on.

34
00:01:32,680 --> 00:01:33,680
Now it's changed.

35
00:01:33,680 --> 00:01:37,680
OpenAI changed GPT-3 to chat GPT.

36
00:01:37,680 --> 00:01:40,680
It was the fastest growing app, I believe, in history

37
00:01:40,680 --> 00:01:42,680
with over 100 million users within the first two months

38
00:01:42,680 --> 00:01:44,680
of its launch.

39
00:01:44,680 --> 00:01:47,680
And lots of other companies, particularly Google,

40
00:01:47,680 --> 00:01:50,680
are introducing Google Just Introduce Barred,

41
00:01:50,680 --> 00:01:52,680
I think, a few days ago.

42
00:01:52,680 --> 00:01:57,680
OpenAI has also introduced GPT-4.

43
00:01:57,680 --> 00:02:00,680
Without going into comparison with these LLMs,

44
00:02:00,680 --> 00:02:02,680
because it changes like every day,

45
00:02:02,680 --> 00:02:05,680
I can write things in one style and ask

46
00:02:05,680 --> 00:02:09,680
it to re-articulate it in the style of Shakespeare,

47
00:02:09,680 --> 00:02:13,680
E.E. Cummings, any other poet or writer.

48
00:02:14,680 --> 00:02:17,680
The results are amazingly impressive.

49
00:02:17,680 --> 00:02:21,680
In my opinion, this is not just another category of AI.

50
00:02:21,680 --> 00:02:26,680
To me, it's as significant as the advent of written language,

51
00:02:26,680 --> 00:02:29,680
which started with cuneiform 5,000 years ago.

52
00:02:29,680 --> 00:02:32,680
You remember using cuneiform 5,000 years ago.

53
00:02:32,680 --> 00:02:36,680
Homo sapiens evolved in Africa 300,000 years ago.

54
00:02:36,680 --> 00:02:39,680
So for most of that history, we had no ways

55
00:02:40,680 --> 00:02:43,680
of documenting our language.

56
00:02:43,680 --> 00:02:48,680
In the past century, we've added to written language.

57
00:02:48,680 --> 00:02:51,680
We've added word processes and other means to help us.

58
00:02:51,680 --> 00:02:54,680
But this latest breakthrough allows us to creatively

59
00:02:54,680 --> 00:02:57,680
create written language based on the LLMs'

60
00:02:57,680 --> 00:02:59,680
own understanding.

61
00:02:59,680 --> 00:03:03,680
It's going to go in all directions and at a very high speed.

62
00:03:03,680 --> 00:03:05,680
I mean, just look at it in the last two years.

63
00:03:05,680 --> 00:03:07,680
It's been unbelievable.

64
00:03:07,680 --> 00:03:09,680
It's going to change everything we do.

65
00:03:09,680 --> 00:03:11,680
You can write code perfectly.

66
00:03:11,680 --> 00:03:16,680
You can convert code into human terms, deal with all languages,

67
00:03:16,680 --> 00:03:20,680
different styles of communicating, and so on.

68
00:03:20,680 --> 00:03:23,680
It's been already very extensively used to create answers

69
00:03:23,680 --> 00:03:25,680
for subtle questions.

70
00:03:25,680 --> 00:03:31,680
So I actually took a couple of the top LLMs,

71
00:03:31,680 --> 00:03:33,680
and I asked the various questions like,

72
00:03:33,680 --> 00:03:36,680
how do my views of consciousness relate to those

73
00:03:36,680 --> 00:03:39,680
of Marvin Ninsky, and how do they compare?

74
00:03:39,680 --> 00:03:42,680
Now, that's kind of a subtle question.

75
00:03:42,680 --> 00:03:45,680
I'm not sure I've actually ever read anything

76
00:03:45,680 --> 00:03:48,680
that answered that question.

77
00:03:48,680 --> 00:03:52,680
I asked LLMs from Google and from OpenAI.

78
00:03:52,680 --> 00:03:55,680
The answers were really quite remarkably subtle,

79
00:03:55,680 --> 00:04:00,680
very well stated, and they were not copied from anywhere else.

80
00:04:00,680 --> 00:04:04,680
Now, many people are concerned that large language models

81
00:04:04,680 --> 00:04:08,680
may promote ideas that are not socially appropriate,

82
00:04:08,680 --> 00:04:12,680
that engender racism or sexism and so on.

83
00:04:12,680 --> 00:04:16,680
It's definitely very worthwhile for us to study this

84
00:04:16,680 --> 00:04:18,680
that may happen from time to time,

85
00:04:18,680 --> 00:04:22,680
but I've actually used LLMs probably close to a thousand times.

86
00:04:22,680 --> 00:04:25,680
I've actually not seen anything that could be categorized that way.

87
00:04:25,680 --> 00:04:27,680
Maybe it's the way I asked the question.

88
00:04:27,680 --> 00:04:29,680
It also seems pretty accurate.

89
00:04:29,680 --> 00:04:32,680
The only mistake it made is that I thought my son,

90
00:04:32,680 --> 00:04:34,680
Ethan, went to Harvard as an undergraduate.

91
00:04:34,680 --> 00:04:36,680
He actually went there for an MBA.

92
00:04:36,680 --> 00:04:40,680
I've written a new book, which I've talked about for years.

93
00:04:40,680 --> 00:04:43,680
The Singularity is Nearer.

94
00:04:43,680 --> 00:04:45,680
It should be out in about a year.

95
00:04:45,680 --> 00:04:48,680
I keep writing because literally every week that we can't come

96
00:04:48,680 --> 00:04:51,680
out with this without covering this,

97
00:04:51,680 --> 00:04:53,680
but that's been happening now every few days.

98
00:04:53,680 --> 00:04:55,680
So I finally had to give up on that.

99
00:04:55,680 --> 00:04:58,680
By the time it comes out, it'll be out of date,

100
00:04:58,680 --> 00:05:00,680
but it's not just covering today.

101
00:05:00,680 --> 00:05:02,680
It's covering what's how we got here

102
00:05:02,680 --> 00:05:05,680
and what will happen in the near future.

103
00:05:05,680 --> 00:05:11,680
Critics of AI very often show how large language models

104
00:05:11,680 --> 00:05:13,680
may not be perfect.

105
00:05:13,680 --> 00:05:15,680
There was one recently said,

106
00:05:15,680 --> 00:05:20,680
well, it can't, if you put mathematics inside language,

107
00:05:20,680 --> 00:05:21,680
it doesn't do that correctly.

108
00:05:21,680 --> 00:05:23,680
But now within a year of saying that,

109
00:05:23,680 --> 00:05:25,680
that's no longer true.

110
00:05:25,680 --> 00:05:31,680
So one of my themes, and this is also true of Peter and Salim,

111
00:05:31,680 --> 00:05:33,680
has been the acceleration of progress

112
00:05:33,680 --> 00:05:35,680
in information technology,

113
00:05:35,680 --> 00:05:38,680
but also everything that we work on.

114
00:05:38,680 --> 00:05:40,680
So here's a chart.

115
00:05:40,680 --> 00:05:42,680
I actually came out with this.

116
00:05:42,680 --> 00:05:48,680
For each year, the best computer that provided

117
00:05:48,680 --> 00:05:51,680
the amount of computations per second.

118
00:05:51,680 --> 00:05:54,680
And it's pretty much a very straight line

119
00:05:54,680 --> 00:05:57,680
on an exponential growth.

120
00:05:57,680 --> 00:06:00,680
And people were not even aware of this.

121
00:06:00,680 --> 00:06:03,680
I mean, I came out with this graph 40 years ago.

122
00:06:03,680 --> 00:06:07,680
It's 40 years after the progress of the computer.

123
00:06:07,680 --> 00:06:10,680
I mean, I came out with this graph 40 years ago.

124
00:06:10,680 --> 00:06:13,680
It's 40 years after the progression started.

125
00:06:13,680 --> 00:06:16,680
And I've been updating it ever since.

126
00:06:16,680 --> 00:06:19,680
People very often call this Moore's Law.

127
00:06:19,680 --> 00:06:22,680
I really believe we shouldn't do that anymore

128
00:06:22,680 --> 00:06:24,680
because there's nothing to do with Moore's.

129
00:06:24,680 --> 00:06:27,680
I mean, this started decades before Intel was even created.

130
00:06:27,680 --> 00:06:30,680
It's been going on for 40 years before anyone even

131
00:06:30,680 --> 00:06:32,680
knew it was happening.

132
00:06:32,680 --> 00:06:34,680
If you go to the bottom left,

133
00:06:34,680 --> 00:06:40,680
the first programmable computer was the ZUSA 1, 1941.

134
00:06:40,680 --> 00:06:49,680
It performed 0.0007 calculations per second per dollar.

135
00:06:49,680 --> 00:06:51,680
ZUSA was a German.

136
00:06:51,680 --> 00:06:55,680
It was not a fan of Hitler, but it was shown to Hitler.

137
00:06:55,680 --> 00:06:58,680
And some people were excited about getting behind this,

138
00:06:58,680 --> 00:07:00,680
but they didn't get behind it.

139
00:07:00,680 --> 00:07:03,680
They saw no military value to computation.

140
00:07:03,680 --> 00:07:07,680
A big mistake for them, among a lot of other mistakes.

141
00:07:07,680 --> 00:07:10,680
The third computer on here is the Colossus,

142
00:07:10,680 --> 00:07:13,680
created by Alan Turing and his colleagues.

143
00:07:13,680 --> 00:07:17,680
Now, Winston Churchill felt that this computer would be the key

144
00:07:17,680 --> 00:07:20,680
to winning World War II, and that was true.

145
00:07:20,680 --> 00:07:23,680
They got totally behind the Colossus computer,

146
00:07:23,680 --> 00:07:27,680
and they used it to completely decode Nazi messages.

147
00:07:27,680 --> 00:07:31,680
So everything that Hitler knew, Churchill also knew.

148
00:07:31,680 --> 00:07:35,680
And so even though the Nazi air power was actually several times

149
00:07:35,680 --> 00:07:39,680
that of the British, they used the Colossus to win the battle

150
00:07:39,680 --> 00:07:43,680
of Britain anyway with this computer and provide the allies

151
00:07:43,680 --> 00:07:46,680
with a launching pad for its D&A invasion.

152
00:07:46,680 --> 00:07:49,680
So if you go along this chart, there are many stories

153
00:07:49,680 --> 00:07:52,680
behind all the computers on this chart.

154
00:07:52,680 --> 00:07:55,680
It almost looks like someone was behind this exponential trend,

155
00:07:55,680 --> 00:07:57,680
like someone's following it.

156
00:07:57,680 --> 00:07:59,680
Okay, we're at this point now.

157
00:07:59,680 --> 00:08:01,680
We're here for the next year.

158
00:08:01,680 --> 00:08:05,680
But for the first 40 years, no one even knew this was happening.

159
00:08:05,680 --> 00:08:07,680
It just happened.

160
00:08:07,680 --> 00:08:10,680
That's the nature of exponential growth.

161
00:08:10,680 --> 00:08:12,680
And this is just one example of exponential growth.

162
00:08:12,680 --> 00:08:15,680
It's not that everything comes from this graph.

163
00:08:15,680 --> 00:08:17,680
This graph just shows you one example

164
00:08:17,680 --> 00:08:21,680
of how technology expands exponentially.

165
00:08:21,680 --> 00:08:26,680
And whether we're aware of it or not.

166
00:08:26,680 --> 00:08:29,680
So exponential growth impacts everything around us,

167
00:08:29,680 --> 00:08:32,680
including everything that we create.

168
00:08:32,680 --> 00:08:40,680
And I projected that this would continue in the same direction

169
00:08:40,680 --> 00:08:42,680
that I noticed 40 years ago.

170
00:08:42,680 --> 00:08:45,680
And as you can see, it's done that.

171
00:08:45,680 --> 00:08:51,680
It's gone from telephone relays to vacuum tubes to transistors

172
00:08:51,680 --> 00:08:53,680
to integrated circuits.

173
00:08:53,680 --> 00:08:56,680
As I mentioned, people have called this Moore's Law,

174
00:08:56,680 --> 00:08:58,680
but as I say, that's not correct.

175
00:08:58,680 --> 00:09:01,680
It started decades before Intel was even formed.

176
00:09:01,680 --> 00:09:06,680
Of the 80 best computers in terms of computations per second per dollar,

177
00:09:06,680 --> 00:09:11,680
only 10 of these out of 80 have anything to do with Intel.

178
00:09:11,680 --> 00:09:16,680
Now, every five years, people were going around saying Moore's Law is over.

179
00:09:16,680 --> 00:09:20,680
You might remember that this started when the COVID pandemic started

180
00:09:20,680 --> 00:09:23,680
just a few years ago, people were saying Moore's Law is over.

181
00:09:23,680 --> 00:09:26,680
And of course, I went around saying that it should not be called Moore's Law,

182
00:09:26,680 --> 00:09:31,680
but regardless of that, whether Intel chips were the best value or not,

183
00:09:31,680 --> 00:09:36,680
this exponential progression has never stopped.

184
00:09:36,680 --> 00:09:40,680
Not for World War II, not for recessions, not for depressions,

185
00:09:40,680 --> 00:09:42,680
or for any other reason.

186
00:09:42,680 --> 00:09:48,680
It's gone for 80 years from .0007 calculations per second per dollar

187
00:09:48,680 --> 00:09:52,680
to now .50 billion calculations per second per dollar.

188
00:09:52,680 --> 00:09:56,680
So you're getting a lot more for the same amount of money.

189
00:09:56,680 --> 00:10:01,680
And it's only in the last three years that large language models have been feasible.

190
00:10:01,680 --> 00:10:04,680
So people believe that neural nets were effective decades ago,

191
00:10:04,680 --> 00:10:09,680
did so really based on their inclination, not any evidence.

192
00:10:09,680 --> 00:10:14,680
I've been in the field of AI for 60 years.

193
00:10:14,680 --> 00:10:18,680
It's quite amazing, like where does the time go?

194
00:10:18,680 --> 00:10:20,680
I was 14.

195
00:10:20,680 --> 00:10:23,680
I met Marvin Minsky who was in his 30s.

196
00:10:23,680 --> 00:10:28,680
Frank Rosenblatt created the Perceptron, the first popular neural net.

197
00:10:28,680 --> 00:10:37,680
As far as I'm aware, I don't think anyone else has 60 years experience or more in AI as I've had.

198
00:10:37,680 --> 00:10:40,680
But if you've been there for more than that, let me know.

199
00:10:40,680 --> 00:10:43,680
I have a lot of stories about that.

200
00:10:43,680 --> 00:10:48,680
But in the early years, it was really not clear that neural nets could do anything successful.

201
00:10:48,680 --> 00:10:53,680
And they're showing now that this is really the path to artificial general intelligence.

202
00:10:53,680 --> 00:10:59,680
We will have large language models that can understand lots of different types of written language,

203
00:10:59,680 --> 00:11:03,680
from formal research articles to jokes and so on.

204
00:11:03,680 --> 00:11:07,680
They're now mastering mathematics within the language.

205
00:11:07,680 --> 00:11:12,680
They can code and do so perfectly and at very high speed.

206
00:11:12,680 --> 00:11:19,680
Now, this obviously brings up, not just that, but all the things it can do brings up concerns about its effect on human employment,

207
00:11:19,680 --> 00:11:22,680
which we were just talking about.

208
00:11:22,680 --> 00:11:27,680
But employment is really not necessarily the best way to bring resources to human.

209
00:11:27,680 --> 00:11:29,680
I mean, look at around the world.

210
00:11:29,680 --> 00:11:36,680
France is now dealing with protests because they're adding a couple of years before people can access their retirement.

211
00:11:36,680 --> 00:11:40,680
It tells me that people really don't like the jobs they do for employment.

212
00:11:40,680 --> 00:11:42,680
So that's, I think, a difference.

213
00:11:42,680 --> 00:11:46,680
We'll actually be able to do what we are really cut out to do.

214
00:11:46,680 --> 00:11:52,680
And in my opinion, it's not just us versus AI and people say, well, how are we going to compete with AI?

215
00:11:52,680 --> 00:12:00,680
The intelligence that we're creating is adding AI to our own brains, just the way our phones and computers do already.

216
00:12:00,680 --> 00:12:04,680
This is not an alien invasion of intelligent machines coming from Mars.

217
00:12:04,680 --> 00:12:10,680
I mean, how many people here have come to this meeting without your phone?

218
00:12:10,680 --> 00:12:12,680
It's already part of our intelligence.

219
00:12:12,680 --> 00:12:14,680
We can't leave home without it.

220
00:12:14,680 --> 00:12:20,680
It ultimately will be automatically added to our intelligence and it already is.

221
00:12:20,680 --> 00:12:26,680
I'll add one more AI topic and I'm sure we'll get into a lot more doing the questions and answers.

222
00:12:26,680 --> 00:12:31,680
But something else that's also extremely exciting, which is simulated biology.

223
00:12:31,680 --> 00:12:33,680
This has already started.

224
00:12:33,680 --> 00:12:44,680
The Moderna vaccine was created by feeding in every possible combination of mRNA sequences and simulating in the computer what would happen.

225
00:12:44,680 --> 00:12:51,680
They tried several billion of such sequences and they went through them all and seeing what the impact would be.

226
00:12:51,680 --> 00:12:55,680
It took two days to process all several billion of them.

227
00:12:55,680 --> 00:12:57,680
And then they had the vaccine.

228
00:12:57,680 --> 00:12:59,680
It actually took two days to create.

229
00:12:59,680 --> 00:13:03,680
It's been the most successful COVID vaccine.

230
00:13:03,680 --> 00:13:05,680
And we did test it with humans.

231
00:13:05,680 --> 00:13:07,680
We're going to get over that as well.

232
00:13:07,680 --> 00:13:12,680
We're ultimately going to be used biological simulation of humans to replace human testing.

233
00:13:12,680 --> 00:13:20,680
I mean, rather than spending a year or several years testing results on a few hundred subjects, none of which probably match you.

234
00:13:20,680 --> 00:13:27,680
We will test it on a million or more humans simulated humans in just a few days.

235
00:13:27,680 --> 00:13:38,680
So to cure cancer, for example, we'll simply feed in every possible method that can detect cancer cells from normal cells and destroy them or do anything that would help us.

236
00:13:38,680 --> 00:13:39,680
And we won't evaluate them.

237
00:13:39,680 --> 00:13:44,680
We'll just feed in all the ideas we have about each of these possibilities into the computer.

238
00:13:44,680 --> 00:13:55,680
The computer will evaluate all of the many billions of sequences and provide the results will then test the final product with simulated humans also very quickly.

239
00:13:55,680 --> 00:13:58,680
And we'll do this for every major health predicament.

240
00:13:58,680 --> 00:14:02,680
It will be done a thousand times faster than conventional methods.

241
00:14:02,680 --> 00:14:11,680
And based on our ability to do this, we should be able to overcome most significant health problems by 2029.

242
00:14:11,680 --> 00:14:15,680
That's by the way, my prediction for passing the Turing test.

243
00:14:15,680 --> 00:14:17,680
I came out with that in 1999.

244
00:14:17,680 --> 00:14:21,680
People thought that was crazy that Stanford had a conference.

245
00:14:21,680 --> 00:14:26,680
Actually, 80% of the people came didn't think we would do it, but they thought it would take 100 years.

246
00:14:26,680 --> 00:14:30,680
They keep polling people.

247
00:14:30,680 --> 00:14:38,680
And now everybody actually thinks that we will actually pass the Turing test by 2029.

248
00:14:38,680 --> 00:14:43,680
And actually to pass the Turing test, meaning it's equivalent to humans, we're actually going to have to dumb them down.

249
00:14:43,680 --> 00:14:49,680
Because if it does everything that a computer can do, we'll know it's not a human.

250
00:14:49,680 --> 00:15:01,680
But this will lead people who are diligent about their health to overcome many problems, reaching what I call longevity escape velocity by the end of this decade.

251
00:15:01,680 --> 00:15:03,680
Now, this doesn't guarantee living forever.

252
00:15:03,680 --> 00:15:10,680
I mean, you can have a 10 year old and you can compute their life expectancy, whatever, many, many decades, and they could die tomorrow.

253
00:15:10,680 --> 00:15:13,680
So it's not a guarantee for living forever.

254
00:15:13,680 --> 00:15:18,680
But the biggest problem we have is aging and people actually die from aging.

255
00:15:18,680 --> 00:15:22,680
I actually had an aunt who's 97. She was a psychologist.

256
00:15:22,680 --> 00:15:28,680
And she actually was still meeting with her patients at 97.

257
00:15:28,680 --> 00:15:31,680
And the last conversation I had with her, she's saying, well, what do you do?

258
00:15:31,680 --> 00:15:34,680
And I said, well, I give lots of speeches and what do you talk about?

259
00:15:34,680 --> 00:15:36,680
And I said, longevity escape velocity.

260
00:15:36,680 --> 00:15:37,680
Oh, what's that?

261
00:15:37,680 --> 00:15:38,680
And I described it.

262
00:15:38,680 --> 00:15:47,680
And the very last thing she said to me, this longevity escape velocity, could we do that a little faster than you're doing it now?

263
00:15:48,680 --> 00:15:53,680
So anyway, I look forward to your questions and comments.

264
00:15:53,680 --> 00:15:55,680
And it's really delightful to be here.

265
00:15:55,680 --> 00:15:56,680
Thank you, Ray.

266
00:15:56,680 --> 00:15:57,680
All right.

267
00:16:00,680 --> 00:16:04,680
I'm going to take privilege and ask the first question. Ray, we've seen LLMs.

268
00:16:04,680 --> 00:16:11,680
What's the next major breakthrough that you expect to see on the road of evolution of AI?

269
00:16:11,680 --> 00:16:16,680
Well, LLMs, I mean, they do remarkable things.

270
00:16:16,680 --> 00:16:18,680
But it's really just the beginning.

271
00:16:18,680 --> 00:16:23,680
I mean, the very first time I saw an LLM was three years ago, and it actually didn't work very well.

272
00:16:23,680 --> 00:16:26,680
Every six months, it's completely revolutionary.

273
00:16:26,680 --> 00:16:31,680
So it's going to give us new ways of communicating with each other.

274
00:16:31,680 --> 00:16:38,680
And as I said, I think it's the biggest advance since written language, which happened 5,000 years ago.

275
00:16:39,680 --> 00:16:46,680
I mentioned advancing longevity escape velocity, doing simulated biology.

276
00:16:46,680 --> 00:16:47,680
We've actually done that.

277
00:16:47,680 --> 00:16:51,680
People are taking this test, which was done with simulated biology.

278
00:16:51,680 --> 00:16:53,680
Lots of people are going into this.

279
00:16:53,680 --> 00:16:57,680
It's a way biology is going to be done.

280
00:16:57,680 --> 00:17:04,680
And we're going to see amazing progress starting, really, I'd say, in a few years.

281
00:17:05,680 --> 00:17:07,680
It's going to do everything that we do.

282
00:17:07,680 --> 00:17:10,680
But as I said, it's not competing with us.

283
00:17:10,680 --> 00:17:13,680
I mean, we're creating these tools to overcome ourselves.

284
00:17:13,680 --> 00:17:19,680
And I mean, how many people today have a job that was common 100 years ago?

285
00:17:19,680 --> 00:17:23,680
I mean, 200 years ago, 80% of the American public were working in farming.

286
00:17:23,680 --> 00:17:25,680
Today, that's 2%.

287
00:17:25,680 --> 00:17:30,680
So we're all doing things that didn't even exist even 10 years ago.

288
00:17:30,680 --> 00:17:34,680
So we're going to be doing amazing things, harnessing our computers.

289
00:17:34,680 --> 00:17:37,680
They're really part of ourselves.

290
00:17:37,680 --> 00:17:38,680
Great.

291
00:17:38,680 --> 00:17:39,680
Harry.

292
00:17:39,680 --> 00:17:40,680
Hey, Ray.

293
00:17:40,680 --> 00:17:41,680
Good to see you.

294
00:17:41,680 --> 00:17:48,680
So Ray and I have been collaborating for actually probably 20 years on something else,

295
00:17:48,680 --> 00:17:52,680
not natural language programming, but humanoid robots.

296
00:17:52,680 --> 00:17:55,680
Ray, I wanted to get your opinion.

297
00:17:55,680 --> 00:18:02,680
So at Beyond Managed Nation, we're creating AI powered robots called Beomni.

298
00:18:02,680 --> 00:18:09,680
And we have a lot of discussions about AI for natural language, for images.

299
00:18:09,680 --> 00:18:16,680
Where do you see AI and humanoid robots going in the future to impact physical work?

300
00:18:16,680 --> 00:18:17,680
Yes.

301
00:18:17,680 --> 00:18:19,680
That's a very good comment.

302
00:18:19,680 --> 00:18:22,680
I'd be very pleased to hear of your amazing progress.

303
00:18:22,680 --> 00:18:29,680
I mean, you have a robot that can actually take something and actually flip a cap off

304
00:18:29,680 --> 00:18:30,680
a jar.

305
00:18:30,680 --> 00:18:34,680
No one else can do that.

306
00:18:34,680 --> 00:18:38,680
We've not made as much progress in this area.

307
00:18:38,680 --> 00:18:43,680
We can do fantastic things with language, but if I give you a table that has where you

308
00:18:43,680 --> 00:18:49,680
need to put it in the dishwasher and now went to wash out dishes and so on, we have not

309
00:18:49,680 --> 00:18:51,680
been able to do that.

310
00:18:51,680 --> 00:18:53,680
You're actually working on that.

311
00:18:53,680 --> 00:18:57,680
And I think that's going to be amazing with these types of robots.

312
00:18:57,680 --> 00:19:00,680
You could send someone into a burning building and save people.

313
00:19:00,680 --> 00:19:07,680
You could have a surgeon in New York perform surgery on somebody in Africa.

314
00:19:07,680 --> 00:19:11,680
So we're going to actually master the human body and how we move.

315
00:19:11,680 --> 00:19:15,680
And we're going to be using neural nets to do that.

316
00:19:15,680 --> 00:19:20,680
And I think that's another thing we're going to see really starting now.

317
00:19:20,680 --> 00:19:23,680
And it will be quite prevalent within a few years.

318
00:19:23,680 --> 00:19:27,680
Over the years, I've experimented with many intermittent fasting programs.

319
00:19:27,680 --> 00:19:33,680
The truth is I've given up on intermittent fasting as I've seen no real benefit when

320
00:19:33,680 --> 00:19:34,680
it comes to longevity.

321
00:19:34,680 --> 00:19:40,680
But this changed when I discovered something called Prolon's five day fasting nutrition

322
00:19:40,680 --> 00:19:41,680
program.

323
00:19:41,680 --> 00:19:43,680
It harnesses the process of autophagy.

324
00:19:43,680 --> 00:19:48,680
This is a cellular recycling process that revitalizes your body at a molecular level.

325
00:19:48,680 --> 00:19:54,680
And just one cycle of the five day Prolon fasting nutrition program can support healthy

326
00:19:54,680 --> 00:19:58,680
aging, fat focused weight loss, improved energy levels and more.

327
00:19:58,680 --> 00:19:59,680
It's a painless process.

328
00:19:59,680 --> 00:20:02,680
And I've been doing it twice a year for the last year.

329
00:20:02,680 --> 00:20:07,680
You can get a 15% off on your order when you go to my special URL.

330
00:20:07,680 --> 00:20:15,680
Go to prolonlife.com, backslash moonshot.

331
00:20:15,680 --> 00:20:19,680
Get started on your longevity journey with Prolon today.

332
00:20:19,680 --> 00:20:21,680
Now back to the episode.

333
00:20:45,680 --> 00:20:48,680
What are some of the things you want to do to grow with AI?

334
00:20:48,680 --> 00:20:53,680
Well, yes, I mean, we're going to be using these types of capabilities to learn.

335
00:20:53,680 --> 00:20:59,680
One of the biggest applications of LLM is to help education.

336
00:20:59,680 --> 00:21:06,680
In many ways, we're educating people the same way when I was a child or my grandparents

337
00:21:06,680 --> 00:21:07,680
were children.

338
00:21:07,680 --> 00:21:10,680
We really need to go beyond that.

339
00:21:10,680 --> 00:21:12,680
We can learn from computers.

340
00:21:12,680 --> 00:21:13,680
They know everything.

341
00:21:13,680 --> 00:21:16,680
They can become very good at articulating it.

342
00:21:16,680 --> 00:21:23,680
They can actually measure where a student is and help them to learn, overcome their

343
00:21:23,680 --> 00:21:24,680
barriers.

344
00:21:24,680 --> 00:21:27,680
And they're going to be then part of the solution.

345
00:21:27,680 --> 00:21:30,680
Again, these computers is not something we need to compete with.

346
00:21:30,680 --> 00:21:34,680
We need to know how to use them together.

347
00:21:34,680 --> 00:21:44,680
And another big application of education is socialization, getting to learn other people

348
00:21:44,680 --> 00:21:45,680
and make friends and so on.

349
00:21:45,680 --> 00:21:48,680
So we're going to have to actually do that as well.

350
00:21:48,680 --> 00:21:50,680
Computers can definitely help there.

351
00:21:50,680 --> 00:21:59,680
But we're going to completely use large language models that are coming out very soon to really

352
00:21:59,680 --> 00:22:02,680
revamp education.

353
00:22:02,680 --> 00:22:05,680
Thank you.

354
00:22:05,680 --> 00:22:07,680
Good morning, Ray.

355
00:22:07,680 --> 00:22:09,680
I'm Yi Xiang Liu.

356
00:22:09,680 --> 00:22:11,680
I'm from Texas.

357
00:22:11,680 --> 00:22:15,680
Very much looking forward to meeting you today.

358
00:22:15,680 --> 00:22:17,680
Thank you, Peter, for having me here.

359
00:22:17,680 --> 00:22:24,680
My question to you is how do you predict the future with such accuracy?

360
00:22:24,680 --> 00:22:29,680
Is it because you help to shape it and then deliver it?

361
00:22:29,680 --> 00:22:36,680
Or you calculate the laws that other people don't, and then you can predict it?

362
00:22:36,680 --> 00:22:39,680
So which one is actively shaping it?

363
00:22:39,680 --> 00:22:41,680
Which part is missing?

364
00:22:41,680 --> 00:22:44,680
That's a very good question.

365
00:22:44,680 --> 00:22:50,680
I'll give you a very brief idea of how I got into what I'm doing.

366
00:22:50,680 --> 00:22:57,680
My great grandmother actually started the first school that educated women to 14th grade.

367
00:22:58,680 --> 00:23:06,680
In 1850, if you were able to get an education at all as a woman, it went through ninth grade.

368
00:23:06,680 --> 00:23:11,680
And she went around Europe educating why we should educate women.

369
00:23:11,680 --> 00:23:13,680
It's very controversial.

370
00:23:13,680 --> 00:23:15,680
Why do you want to do that?

371
00:23:15,680 --> 00:23:21,680
Her daughter became actually the first woman to get a PhD in chemistry in Europe.

372
00:23:21,680 --> 00:23:23,680
She took over the school.

373
00:23:23,680 --> 00:23:26,680
They ran it for 80 years called Sternschule in Vienna.

374
00:23:26,680 --> 00:23:29,680
There's a book about it.

375
00:23:29,680 --> 00:23:31,680
And she wrote a book.

376
00:23:31,680 --> 00:23:34,680
Actually, the title of it would be very appropriate for one of my books.

377
00:23:34,680 --> 00:23:37,680
It's called One Life is Not Enough.

378
00:23:37,680 --> 00:23:40,680
But she wasn't actually talking about extending life.

379
00:23:40,680 --> 00:23:42,680
She didn't have that idea.

380
00:23:42,680 --> 00:23:48,680
But she noticed that one life really is enough to get things done.

381
00:23:48,680 --> 00:23:52,680
So she showed me when I was six years old, she showed me the book.

382
00:23:52,680 --> 00:23:56,680
And she showed me the manual typewriter that she created it on.

383
00:23:56,680 --> 00:23:59,680
I got very interested in the book many years later.

384
00:23:59,680 --> 00:24:01,680
At that time, I wasn't that interested in the book.

385
00:24:01,680 --> 00:24:05,680
But I was amazingly interested in the manual typewriter.

386
00:24:05,680 --> 00:24:07,680
I mean, here's a machine that had no electronics.

387
00:24:07,680 --> 00:24:09,680
There's manual typewriter.

388
00:24:09,680 --> 00:24:14,680
And it could take a blank piece of paper and turn it into something that looked like it came from a book.

389
00:24:14,680 --> 00:24:16,680
So I actually wrote a book on it.

390
00:24:16,680 --> 00:24:18,680
It was 23 pages.

391
00:24:18,680 --> 00:24:23,680
It's about a guy that travels on the back of geese around the world and wrote it on the book

392
00:24:23,680 --> 00:24:29,680
and actually created pictures by using the dot and X keys to create images.

393
00:24:29,680 --> 00:24:34,680
So I then began, I noticed this was just created with mechanical objects.

394
00:24:34,680 --> 00:24:39,680
So I ran around the neighborhood and I gathered mechanical objects.

395
00:24:39,680 --> 00:24:42,680
Little things from radios, broken bicycles.

396
00:24:42,680 --> 00:24:47,680
This was an era where you were allowed a 60-year-old kid to go around the neighborhood and collect these things.

397
00:24:47,680 --> 00:24:50,680
You'd probably get arrested today.

398
00:24:50,680 --> 00:24:54,680
And I went around saying, I have no idea how to put these things together.

399
00:24:54,680 --> 00:24:58,680
But someday I'm going to figure that out and I'm going to be able to solve any problem.

400
00:24:58,680 --> 00:25:00,680
Be able to go to other places.

401
00:25:00,680 --> 00:25:03,680
We'll be able to live forever and so on.

402
00:25:03,680 --> 00:25:06,680
I remember actually talking to these very old girls.

403
00:25:06,680 --> 00:25:10,680
I think they were 10 and they were quite fascinated.

404
00:25:10,680 --> 00:25:14,680
And they said, well, you have quite an imagination there.

405
00:25:14,680 --> 00:25:18,680
So other people were saying what they wanted to be.

406
00:25:18,680 --> 00:25:21,680
Fighting fires or educating people.

407
00:25:21,680 --> 00:25:23,680
I said, I know what I'm going to be.

408
00:25:23,680 --> 00:25:25,680
I'm going to be an inventor.

409
00:25:25,680 --> 00:25:34,680
And starting at eight actually created a virtual reality theater that was a big hit in my third grade class.

410
00:25:34,680 --> 00:25:38,680
So I got into inventing.

411
00:25:38,680 --> 00:25:43,680
And the biggest problem was when do you approach a certain problem?

412
00:25:43,680 --> 00:25:47,680
Like I did character recognition in the 70s.

413
00:25:47,680 --> 00:25:49,680
I did speech recognition in the 80s.

414
00:25:49,680 --> 00:25:51,680
Why did I do it that way?

415
00:25:51,680 --> 00:25:57,680
It's because speech recognition requires actually more computation.

416
00:25:57,680 --> 00:26:01,680
So I began to study how technology evolves.

417
00:26:01,680 --> 00:26:07,680
And really about 40 years ago I realized that computers were on this exponential rise.

418
00:26:07,680 --> 00:26:14,680
And so I didn't get into futures for futures itself.

419
00:26:14,680 --> 00:26:18,680
It was really to plan my own projects and what I would get involved in.

420
00:26:18,680 --> 00:26:30,680
And so if I look forward five years, 10 years, we're now actually at a very fast pace of this exponential path as you can see.

421
00:26:30,680 --> 00:26:34,680
I'll see what are the capabilities going to be.

422
00:26:34,680 --> 00:26:37,680
And then you needed to use a little bit of imagination.

423
00:26:37,680 --> 00:26:44,680
What can we do with the computers of this power and other types of things that we can manage?

424
00:26:44,680 --> 00:26:49,680
But that's really been my plan is to figure out what is capable.

425
00:26:49,680 --> 00:26:51,680
And you saw that chart.

426
00:26:51,680 --> 00:26:53,680
It's an absolutely straight line.

427
00:26:53,680 --> 00:26:58,680
I had it 40 years ago and projected it as a straight line and it's exactly where it should be.

428
00:26:58,680 --> 00:27:04,680
And then you can use imagination as to what you can do with that type of power.

429
00:27:04,680 --> 00:27:06,680
So that's how I go.

430
00:27:06,680 --> 00:27:09,680
Ray, just to point out, it's a straight line on a log scale.

431
00:27:09,680 --> 00:27:12,680
Meaning it's going exponentially, yes.

432
00:27:12,680 --> 00:27:14,680
Exactly.

433
00:27:14,680 --> 00:27:16,680
Thank you.

434
00:27:16,680 --> 00:27:17,680
Mike.

435
00:27:17,680 --> 00:27:19,680
Hi, great to meet you, Ray.

436
00:27:19,680 --> 00:27:21,680
Quick question.

437
00:27:21,680 --> 00:27:26,680
When do you think that quantum computing will break RSA encryption?

438
00:27:26,680 --> 00:27:31,680
Well, a little bit skeptical of quantum computing.

439
00:27:31,680 --> 00:27:36,680
I mean, people go around saying, oh, we've got this 50 qubit computers.

440
00:27:36,680 --> 00:27:39,680
But it creates lots of errors.

441
00:27:39,680 --> 00:27:45,680
And we've actually figured out how many qubits you would need to actually do it perfectly.

442
00:27:45,680 --> 00:27:51,680
I mean, computation that creates lots of errors is pretty useless.

443
00:27:51,680 --> 00:28:00,680
And so it takes about at least a thousand, maybe even 10,000 qubits to create one qubit that's actually accurate.

444
00:28:00,680 --> 00:28:06,680
Now, the last time I checked, 50 divided by a thousand is less than one.

445
00:28:06,680 --> 00:28:09,680
And we really haven't done anything with quantum computing.

446
00:28:09,680 --> 00:28:11,680
And that was the same thing 10 years ago.

447
00:28:11,680 --> 00:28:14,680
So maybe we'll figure out how to overcome this problem.

448
00:28:14,680 --> 00:28:16,680
I know there are people working on it.

449
00:28:16,680 --> 00:28:19,680
They've got some theories as to why that will work.

450
00:28:19,680 --> 00:28:25,680
But all the predictions I make have to do with classical computing, not quantum computing.

451
00:28:25,680 --> 00:28:28,680
And you can see the amazing things that we're doing.

452
00:28:28,680 --> 00:28:35,680
And if you look at what humans can do, we can definitely account for that with classical computing.

453
00:28:35,680 --> 00:28:39,680
Thanks.

454
00:28:39,680 --> 00:28:40,680
Hello, Ray.

455
00:28:40,680 --> 00:28:42,680
My name is Neil from Sacramento, California.

456
00:28:42,680 --> 00:28:51,680
Many of the technologies that we're seeing are going to be more readily available to people with the financial resources and the education to immediately take advantage of.

457
00:28:51,680 --> 00:28:59,680
But what do you believe are the technologies that will be most ubiquitous and will have the biggest impact perhaps on the middle class and the working class communities?

458
00:28:59,680 --> 00:29:07,680
And how would we best educate our broader communities to be able to understand and help embrace those technologies?

459
00:29:07,680 --> 00:29:09,680
Well, they're all working together.

460
00:29:09,680 --> 00:29:15,680
I think we need a little bit more work, for example, on virtual reality.

461
00:29:15,680 --> 00:29:28,680
But that allows people to go anywhere and interact with people that don't exist now but might have existed tens of millions of years ago.

462
00:29:28,680 --> 00:29:31,680
And also put people together.

463
00:29:32,680 --> 00:29:36,680
I mean, the virtual reality we're using right now is a little bit limited.

464
00:29:36,680 --> 00:29:43,680
There's actually some new 3D forms that I've actually begun to use where it actually appears like I'm there and can actually shake people's hands and so on.

465
00:29:43,680 --> 00:29:46,680
So that's all coming.

466
00:29:46,680 --> 00:29:54,680
We use computers and this type of technology to bring us closer together.

467
00:29:54,680 --> 00:29:58,680
I mean, I just watched the movie Around the World in 80 Days.

468
00:29:58,680 --> 00:30:02,680
It was quite amazing to actually get around the world in 80 days.

469
00:30:02,680 --> 00:30:11,680
But today you can meet people almost instantly and also really be great to actually be able to hug each of you and so on.

470
00:30:11,680 --> 00:30:13,680
That's all coming.

471
00:30:13,680 --> 00:30:21,680
So increasing communication and also to meet my grandmother's view of one life is not enough.

472
00:30:21,680 --> 00:30:23,680
She does not have an answer to that.

473
00:30:23,680 --> 00:30:27,680
But I think we're going to be able to keep ourselves here.

474
00:30:27,680 --> 00:30:35,680
I mean, when people are around for a while, they actually gain some wisdom and they're good to keep us around for a while longer.

475
00:30:35,680 --> 00:30:37,680
Thank you, Ray.

476
00:30:37,680 --> 00:30:38,680
Mike.

477
00:30:38,680 --> 00:30:39,680
Hello, Ray.

478
00:30:39,680 --> 00:30:41,680
Mike Wandler from Wyoming.

479
00:30:41,680 --> 00:30:45,680
Peter was showing us the AI enabled mind reading.

480
00:30:45,680 --> 00:30:52,680
Really curious about how that works and especially the connection to collective consciousness or consciousness.

481
00:30:52,680 --> 00:31:02,680
So Ray, this is recently they put some subjects in a functional MRI and then fed the output to stable diffusion.

482
00:31:02,680 --> 00:31:06,680
I've actually done that.

483
00:31:06,680 --> 00:31:09,680
This was maybe five years ago.

484
00:31:09,680 --> 00:31:14,680
It wasn't perfect, but it was significant.

485
00:31:15,680 --> 00:31:25,680
I mean, things that go on inside our minds actually, it affects things that we don't usually notice like our eye blinking and so on.

486
00:31:25,680 --> 00:31:29,680
And regaining more ability to do that.

487
00:31:29,680 --> 00:31:39,680
We can do pretty good telling if people are telling the truth or not.

488
00:31:39,680 --> 00:31:41,680
So that's going to happen.

489
00:31:41,680 --> 00:31:45,680
And there are ways in which some of these things are positive and negative.

490
00:31:45,680 --> 00:31:47,680
I mean, I write mostly about the positive.

491
00:31:47,680 --> 00:31:52,680
I think things are moving in a positive direction in this new book.

492
00:31:52,680 --> 00:32:00,680
I've got 50 graphs showing all the things we care about are moving in the right direction.

493
00:32:00,680 --> 00:32:02,680
But that never reaches the news.

494
00:32:02,680 --> 00:32:05,680
You watch the news, everything is bad news.

495
00:32:05,680 --> 00:32:09,680
And the bad news is true, but we completely ignore the good news.

496
00:32:09,680 --> 00:32:14,680
I mean, look at what life was like 50 years ago or 1900.

497
00:32:14,680 --> 00:32:16,680
Human life expectancy was 48.

498
00:32:16,680 --> 00:32:19,680
It was 35 and 1800.

499
00:32:19,680 --> 00:32:22,680
It's not that long ago.

500
00:32:22,680 --> 00:32:34,680
So anyway, we are able to begin to tell what's going on inside our minds with some greater accuracy.

501
00:32:34,680 --> 00:32:38,680
Hi, Ray Sadoq Cohen from Istanbul, Turkey.

502
00:32:38,680 --> 00:32:46,680
It looks like LLMs are, with the aid of some expert systems, the way to go to general intelligence.

503
00:32:46,680 --> 00:32:52,680
Do you think that means that's a hint of how the brain or our brain really works?

504
00:32:52,680 --> 00:32:56,680
And if that's the case, does it mean that the more we understand the LLM models,

505
00:32:56,680 --> 00:32:59,680
the more we understand our brain and be able to hack it?

506
00:32:59,680 --> 00:33:04,680
And is that a hint that we are more deterministic than we thought we were?

507
00:33:04,680 --> 00:33:06,680
Well, it's a very good question.

508
00:33:06,680 --> 00:33:09,680
It uses a somewhat different technique.

509
00:33:09,680 --> 00:33:18,680
Neural nets, every phase, it's able to get itself closer to the truth.

510
00:33:18,680 --> 00:33:21,680
We don't actually see anything in our brain that actually does it.

511
00:33:21,680 --> 00:33:23,680
It does it a different way.

512
00:33:23,680 --> 00:33:26,680
But somehow we have all these different connections.

513
00:33:27,680 --> 00:33:30,680
And the large language models that are effective,

514
00:33:30,680 --> 00:33:36,680
I mean, we actually had large language models that had 100 million connections.

515
00:33:36,680 --> 00:33:39,680
That sounds like a lot, but it actually didn't do very much.

516
00:33:39,680 --> 00:33:44,680
When I got to 10 billion and started to do things,

517
00:33:44,680 --> 00:33:51,680
the recent ones started now at 100 billion, going to a trillion connections.

518
00:33:51,680 --> 00:33:56,680
And it basically is able to look at all the different connections between them.

519
00:33:56,680 --> 00:33:59,680
And that's exactly what our brain does.

520
00:33:59,680 --> 00:34:02,680
And these things are going to go way beyond what our brain does.

521
00:34:02,680 --> 00:34:04,680
We see that already.

522
00:34:04,680 --> 00:34:06,680
I mean, I can play Go.

523
00:34:06,680 --> 00:34:09,680
I'm hardly the best player.

524
00:34:09,680 --> 00:34:14,680
But Lisa Dahl, who is the best human player,

525
00:34:14,680 --> 00:34:17,680
and that used to be significant because he could just look at the board

526
00:34:17,680 --> 00:34:20,680
and be able to do something that no one else could do.

527
00:34:20,680 --> 00:34:24,680
And he says he's not going to play Go anymore because he can't compete with a computer.

528
00:34:24,680 --> 00:34:27,680
In my view, though, we're going to add this to ourselves,

529
00:34:27,680 --> 00:34:34,680
and we'll all become master Go players of Go and everything else that we want.

530
00:34:34,680 --> 00:34:40,680
But yes, it's using the same ability to connect things.

531
00:34:40,680 --> 00:34:45,680
And if you get enough of them, it seems to be basically a trillion seems to be,

532
00:34:45,680 --> 00:34:48,680
you know, way beyond what humans can do.

533
00:34:48,680 --> 00:34:51,680
We can be very intelligent.

534
00:34:51,680 --> 00:34:55,680
Thank you.

535
00:34:55,680 --> 00:34:56,680
Hi, Ray.

536
00:34:56,680 --> 00:34:58,680
I'm Annie Chahal-Honen.

537
00:34:58,680 --> 00:35:00,680
It's nice to see you again.

538
00:35:00,680 --> 00:35:05,680
I had the pleasure of seeing you at an A360 Singularity Executive Program.

539
00:35:05,680 --> 00:35:08,680
And I'm going to ask you the same question I asked then

540
00:35:08,680 --> 00:35:12,680
because I hope that all these amazing innovators out there are going to hear this.

541
00:35:12,680 --> 00:35:15,680
I asked you, when you're struggling with a problem,

542
00:35:15,680 --> 00:35:17,680
because you're this amazing inventor,

543
00:35:17,680 --> 00:35:23,680
what's your approach and process to solve it or get to the next step?

544
00:35:23,680 --> 00:35:29,680
Well, something I think that Peter and also Salim would agree with

545
00:35:29,680 --> 00:35:34,680
is that failure is just a step towards success.

546
00:35:34,680 --> 00:35:38,680
I mean, failure is really a delayed form of success.

547
00:35:38,680 --> 00:35:43,680
When Edison was trying out many thousands of different things

548
00:35:43,680 --> 00:35:48,680
that would quickly create a light bulb and he tried it out and it didn't work,

549
00:35:48,680 --> 00:35:51,680
his feeling was, okay, I now know that this doesn't work.

550
00:35:51,680 --> 00:35:52,680
We'll go to the next one.

551
00:35:52,680 --> 00:35:54,680
And he finally solved the problem.

552
00:35:54,680 --> 00:35:57,680
So diligence is very important.

553
00:35:57,680 --> 00:36:00,680
Believing in your own mission.

554
00:36:00,680 --> 00:36:03,680
You've got to have some idea.

555
00:36:03,680 --> 00:36:06,680
Generally, if I'm trying to solve a problem,

556
00:36:06,680 --> 00:36:12,680
I imagine I'm giving a speech four years from now

557
00:36:12,680 --> 00:36:16,680
and I'm explaining how I was able to solve this problem.

558
00:36:16,680 --> 00:36:19,680
And in order to solve the problem where we had to do this, this, and this,

559
00:36:19,680 --> 00:36:22,680
in order to do these, we had to do these other things.

560
00:36:22,680 --> 00:36:28,680
And I worked backwards from the solution to where we are today.

561
00:36:28,680 --> 00:36:30,680
And generally, that seems to work.

562
00:36:30,680 --> 00:36:33,680
We can actually figure things out even though they seem impossible.

563
00:36:33,680 --> 00:36:37,680
If you actually imagine, how could this possibly work?

564
00:36:37,680 --> 00:36:42,680
And write that down and study each of those steps,

565
00:36:42,680 --> 00:36:46,680
you can solve really any type of problem.

566
00:36:46,680 --> 00:36:49,680
Thank you.

567
00:36:49,680 --> 00:36:50,680
Hi, Ray.

568
00:36:50,680 --> 00:36:51,680
I'm Dom from Munich, Germany.

569
00:36:51,680 --> 00:36:55,680
And I was wondering, as many of us probably think that the best investment

570
00:36:55,680 --> 00:36:57,680
that you could take is in yourself.

571
00:36:57,680 --> 00:37:00,680
I was wondering if I can have an AI twin.

572
00:37:00,680 --> 00:37:06,680
So I want to train my own AI model to shadow me and to help me make better decisions,

573
00:37:06,680 --> 00:37:09,680
and leverage my strengths, but also balance my weaknesses.

574
00:37:09,680 --> 00:37:13,680
And I was wondering if you were training and Ray Kurzweil, LLM at the moment.

575
00:37:13,680 --> 00:37:16,680
And if so, how many times you spend on it and how you do it.

576
00:37:16,680 --> 00:37:17,680
So would you help?

577
00:37:17,680 --> 00:37:18,680
Yeah.

578
00:37:18,680 --> 00:37:22,680
Well, I mean, I write down lots of things.

579
00:37:22,680 --> 00:37:29,680
We came out with a product that you could actually search a book

580
00:37:29,680 --> 00:37:35,680
and ask a question and we'll find the best answer in what you've written.

581
00:37:35,680 --> 00:37:36,680
It's called Talk to Books.

582
00:37:36,680 --> 00:37:38,680
You can actually go to it.

583
00:37:38,680 --> 00:37:39,680
It's got 200,000 books.

584
00:37:39,680 --> 00:37:45,680
You ask a question and it will actually read every sentence in 200,000 books

585
00:37:45,680 --> 00:37:49,680
and give you an answer, which is quite remarkable.

586
00:37:49,680 --> 00:37:51,680
But I did that, for example, with my father.

587
00:37:51,680 --> 00:37:54,680
My father died actually 50 years ago.

588
00:37:54,680 --> 00:37:56,680
And I still would like to bring him back.

589
00:37:56,680 --> 00:38:00,680
So I actually went through and collected everything he'd ever written.

590
00:38:00,680 --> 00:38:04,680
I didn't write quite as much as I did because we didn't have word processors then.

591
00:38:04,680 --> 00:38:08,680
But he wrote a number of things and put them in there.

592
00:38:08,680 --> 00:38:12,680
And then I used Talk to Books and asked him questions.

593
00:38:12,680 --> 00:38:14,680
And it was really like talking to him.

594
00:38:14,680 --> 00:38:16,680
I mean, I didn't know what answer it would come out with.

595
00:38:16,680 --> 00:38:18,680
It would go through everything he had written and said,

596
00:38:18,680 --> 00:38:21,680
okay, this is the right answer to that question.

597
00:38:21,680 --> 00:38:24,680
And it was a little bit like talking to him.

598
00:38:24,680 --> 00:38:28,680
And I'm doing that with myself.

599
00:38:28,680 --> 00:38:32,680
And ultimately we'll actually have computers on ourselves

600
00:38:32,680 --> 00:38:35,680
that monitor everything that's happened.

601
00:38:35,680 --> 00:38:39,680
I mean, I met my wife actually now 50 years ago.

602
00:38:39,680 --> 00:38:43,680
Every time I say this, I'm amazed where does the time go.

603
00:38:43,680 --> 00:38:48,680
And I met her at a party and we had some small talk.

604
00:38:48,680 --> 00:38:50,680
What the heck did we talk about?

605
00:38:50,680 --> 00:38:52,680
Neither of us can remember.

606
00:38:52,680 --> 00:38:55,680
But we could actually go back and watch that.

607
00:38:55,680 --> 00:38:58,680
So we should actually be monitoring everything we do

608
00:38:58,680 --> 00:39:01,680
when we can go back, not relive everything,

609
00:39:01,680 --> 00:39:06,680
but certain things you might want to actually see what happens.

610
00:39:06,680 --> 00:39:09,680
And that's going to happen right now.

611
00:39:09,680 --> 00:39:13,680
If you want to use a search engine and you have to do it,

612
00:39:13,680 --> 00:39:15,680
you got to like turn the machine on.

613
00:39:15,680 --> 00:39:16,680
You got to find the right place.

614
00:39:16,680 --> 00:39:18,680
You got to put in the answer to the question.

615
00:39:18,680 --> 00:39:21,680
It should actually be listening and say,

616
00:39:21,680 --> 00:39:26,680
okay, the actress you want is so-and-so before you even ask it,

617
00:39:26,680 --> 00:39:29,680
because we'll see that you're trying to figure things out.

618
00:39:29,680 --> 00:39:31,680
So these are some of the things that we can do

619
00:39:31,680 --> 00:39:35,680
actually with technology that we already have.

620
00:39:35,680 --> 00:39:37,680
Perfect. Thank you.

621
00:39:37,680 --> 00:39:38,680
Great question, Don.

622
00:39:38,680 --> 00:39:39,680
Howard.

623
00:39:39,680 --> 00:39:40,680
Hi, Ray.

624
00:39:40,680 --> 00:39:43,680
Howard Lederman, originally St. Louis, now Pompano Beach.

625
00:39:43,680 --> 00:39:47,680
And I had my original question kind of was on the direction

626
00:39:47,680 --> 00:39:49,680
of what he was asking.

627
00:39:49,680 --> 00:39:52,680
So I had a second question and I'm going to go with that one.

628
00:39:52,680 --> 00:39:55,680
I'm actually here and I've been here previous years

629
00:39:55,680 --> 00:39:58,680
looking for solutions for caregiver shortage

630
00:39:58,680 --> 00:40:01,680
that we're experiencing already

631
00:40:01,680 --> 00:40:03,680
and is just going to be accelerating.

632
00:40:03,680 --> 00:40:06,680
Of course, robotics are somewhere out there.

633
00:40:06,680 --> 00:40:11,680
And I was kind of curious on your thoughts

634
00:40:11,680 --> 00:40:15,680
on the challenges of the aging population curve

635
00:40:15,680 --> 00:40:17,680
and caregivers.

636
00:40:17,680 --> 00:40:19,680
I mean, there's a number of answers to that.

637
00:40:19,680 --> 00:40:24,680
First of all, the kind of changes that we see when people age,

638
00:40:24,680 --> 00:40:27,680
I think we're going to be able to overcome.

639
00:40:27,680 --> 00:40:29,680
I mean, that's really the most important thing.

640
00:40:29,680 --> 00:40:32,680
I mean, I run into people that are aging

641
00:40:32,680 --> 00:40:35,680
and they can't remember things and I think we'll be able

642
00:40:35,680 --> 00:40:39,680
to have older people be as vital as younger people

643
00:40:39,680 --> 00:40:42,680
because they'll remember everything.

644
00:40:42,680 --> 00:40:48,680
And also large language models are already pretty close to human.

645
00:40:48,680 --> 00:40:51,680
I mean, you can talk to them and it's like talking to a human

646
00:40:51,680 --> 00:40:55,680
and you can actually program the kind of personality you want.

647
00:40:55,680 --> 00:40:57,680
I mean, I've actually taken them and say,

648
00:40:57,680 --> 00:41:00,680
okay, I want you to act like Shakespeare or E Cummings

649
00:41:00,680 --> 00:41:03,680
or some other poet and they'll actually act like that

650
00:41:03,680 --> 00:41:05,680
and I can talk to them.

651
00:41:05,680 --> 00:41:09,680
But again, it's not going to be a difference

652
00:41:09,680 --> 00:41:11,680
between human and machines.

653
00:41:11,680 --> 00:41:13,680
We're going to be all mixed up.

654
00:41:13,680 --> 00:41:15,680
We're already very mixed up.

655
00:41:15,680 --> 00:41:18,680
I mean, you're looking at your phone there

656
00:41:18,680 --> 00:41:21,680
to see what your question was.

657
00:41:21,680 --> 00:41:25,680
Computers are going to help us get through the day

658
00:41:25,680 --> 00:41:28,680
and so we're not going to be interacting just with humans or machines.

659
00:41:28,680 --> 00:41:31,680
Machines is part of who we are

660
00:41:31,680 --> 00:41:34,680
and that's actually the big difference between human beings.

661
00:41:34,680 --> 00:41:38,680
There are other species that have as big a brain as us.

662
00:41:38,680 --> 00:41:42,680
A whale, an elephant actually has a larger brain than we do,

663
00:41:42,680 --> 00:41:45,680
but they don't have this thumb.

664
00:41:45,680 --> 00:41:47,680
So they can't look at the tree and say,

665
00:41:47,680 --> 00:41:49,680
oh, I could take that branch off.

666
00:41:49,680 --> 00:41:52,680
I could strip off the leaves and I could create a tool.

667
00:41:52,680 --> 00:41:54,680
They just weren't able to do that.

668
00:41:54,680 --> 00:41:57,680
So our brain, plus the fact that we can actually manipulate

669
00:41:57,680 --> 00:42:01,680
the environment, has allowed us to create technology

670
00:42:01,680 --> 00:42:06,680
and the technology is going to allow us to go forward.

671
00:42:06,680 --> 00:42:11,680
Thank you.

672
00:42:11,680 --> 00:42:13,680
Good morning, Ray.

673
00:42:13,680 --> 00:42:15,680
My name is Gloria and I come from Spain.

674
00:42:15,680 --> 00:42:19,680
I just wanted to share an idea that I woke up with this morning.

675
00:42:19,680 --> 00:42:21,680
It's a bit crazy.

676
00:42:21,680 --> 00:42:25,680
But I woke up with this image of neurons in a dish,

677
00:42:25,680 --> 00:42:28,680
in a battery dish, playing ping-pong.

678
00:42:28,680 --> 00:42:33,680
And I thought, what about if we put these neurons on sensors

679
00:42:33,680 --> 00:42:36,680
and connect them to AI, quantum computers, whatever,

680
00:42:36,680 --> 00:42:39,680
and have them feeling stuff.

681
00:42:39,680 --> 00:42:43,680
So they can be more empathetic and understand the humans

682
00:42:43,680 --> 00:42:47,680
or sentient beings, animals, whatever.

683
00:42:47,680 --> 00:42:50,680
And I don't know where they come from,

684
00:42:50,680 --> 00:42:53,680
but maybe that will evolve into something greater

685
00:42:53,680 --> 00:42:58,680
and not just to have the machine embedded in our brain,

686
00:42:58,680 --> 00:43:05,680
so to actually grow neurons and connect these sensors to the AI.

687
00:43:05,680 --> 00:43:06,680
Yeah.

688
00:43:06,680 --> 00:43:10,680
Well, you bring up a number of interesting issues.

689
00:43:10,680 --> 00:43:13,680
Our cells don't have to be in this body.

690
00:43:13,680 --> 00:43:17,680
We can have sensors that are even thousands of miles away

691
00:43:17,680 --> 00:43:20,680
that are really part of who we are.

692
00:43:20,680 --> 00:43:23,680
And you're talking about feelings.

693
00:43:23,680 --> 00:43:24,680
I mean, that's a big issue.

694
00:43:24,680 --> 00:43:27,680
Where do feelings come from?

695
00:43:27,680 --> 00:43:30,680
It's actually not a scientific issue.

696
00:43:30,680 --> 00:43:33,680
I can't put an entity into something

697
00:43:33,680 --> 00:43:36,680
and it would scan and say, yes, this is conscious.

698
00:43:36,680 --> 00:43:39,680
No, this isn't conscious.

699
00:43:39,680 --> 00:43:43,680
There's actually nothing that would actually tell us that.

700
00:43:43,680 --> 00:43:46,680
So that's actually a philosophical question.

701
00:43:46,680 --> 00:43:48,680
I used to discuss this with Marvin Minsky,

702
00:43:48,680 --> 00:43:50,680
and he says, oh, well, that's philosophical.

703
00:43:50,680 --> 00:43:52,680
We don't deal with that.

704
00:43:52,680 --> 00:43:54,680
And he dismissed it.

705
00:43:54,680 --> 00:44:00,680
But actually, he did actually evaluate the ability of people

706
00:44:00,680 --> 00:44:03,680
to be intelligent.

707
00:44:03,680 --> 00:44:05,680
And really, the more intelligent you are,

708
00:44:05,680 --> 00:44:08,680
the more you can process things,

709
00:44:08,680 --> 00:44:09,680
the more feelings you have from it.

710
00:44:09,680 --> 00:44:12,680
I think that's where feelings come from.

711
00:44:12,680 --> 00:44:16,680
And yes, we can actually grow things that are outside of ourselves

712
00:44:16,680 --> 00:44:19,680
that could be part of our feelings as well.

713
00:44:19,680 --> 00:44:23,680
My idea was that who says that consciousness doesn't want

714
00:44:23,680 --> 00:44:26,680
to experience itself through the machine.

715
00:44:26,680 --> 00:44:30,680
With these sensors, we can have pleasure and pain or whatever.

716
00:44:30,680 --> 00:44:32,680
It's just a thought.

717
00:44:32,680 --> 00:44:33,680
Thank you.

718
00:44:33,680 --> 00:44:34,680
Thank you.

719
00:44:34,680 --> 00:44:37,680
We're going to pause and go to Zoom one second.

720
00:44:37,680 --> 00:44:40,680
Dagmar, please go ahead.

721
00:44:40,680 --> 00:44:42,680
Hi, everybody.

722
00:44:42,680 --> 00:44:44,680
Where are you in the planet, Dagmar?

723
00:44:44,680 --> 00:44:45,680
Germany.

724
00:44:45,680 --> 00:44:46,680
In Germany.

725
00:44:46,680 --> 00:44:47,680
Great.

726
00:44:47,680 --> 00:44:49,680
Now, with the history of Germany,

727
00:44:49,680 --> 00:44:52,680
I really has a very big challenge here

728
00:44:52,680 --> 00:44:56,680
because there are people who are really afraid of reviving

729
00:44:56,680 --> 00:44:59,680
a basic big brother angst.

730
00:44:59,680 --> 00:45:05,680
So, Ray, thank you very much for answering maybe this question.

731
00:45:05,680 --> 00:45:08,680
How to overcome this fear?

732
00:45:08,680 --> 00:45:11,680
Because the thing is really we need to learn and explore

733
00:45:11,680 --> 00:45:15,680
and play with the tech so that we actually can deal with it

734
00:45:15,680 --> 00:45:17,680
and learn about it.

735
00:45:17,680 --> 00:45:21,680
So where do you see the power to create this framework

736
00:45:21,680 --> 00:45:23,680
for learning?

737
00:45:23,680 --> 00:45:28,680
Well, I was actually just in Germany a few months ago.

738
00:45:28,680 --> 00:45:37,680
And I think they've considered their past and how that happens

739
00:45:37,680 --> 00:45:40,680
and how we can avoid it's happening, I think,

740
00:45:40,680 --> 00:45:43,680
more than any other country.

741
00:45:43,680 --> 00:45:48,680
And I really felt that while I was there.

742
00:45:49,680 --> 00:45:53,680
Really to understand humans, I think large language models

743
00:45:53,680 --> 00:45:56,680
because it actually incorporates all of the learning of humans.

744
00:45:56,680 --> 00:46:00,680
We can actually begin to appreciate that.

745
00:46:00,680 --> 00:46:07,680
And I've asked these machines questions which no human could answer

746
00:46:07,680 --> 00:46:12,680
because we can't actually hold all of everything that's happened

747
00:46:12,680 --> 00:46:14,680
to humans in our mind.

748
00:46:14,680 --> 00:46:18,680
But if you can actually have something that has experienced

749
00:46:18,680 --> 00:46:22,680
everything and can look through that,

750
00:46:22,680 --> 00:46:26,680
we can avoid the kind of problems we've had in the past.

751
00:46:26,680 --> 00:46:28,680
Thank you so much.

752
00:46:28,680 --> 00:46:30,680
Let's go to Jason on Zoom.

753
00:46:30,680 --> 00:46:32,680
I know we have a number of hands up there and we'll come back

754
00:46:32,680 --> 00:46:34,680
to you gentlemen in a second.

755
00:46:34,680 --> 00:46:35,680
Jason, good morning.

756
00:46:35,680 --> 00:46:36,680
Where are you on the planet?

757
00:46:36,680 --> 00:46:37,680
Hey, Ray.

758
00:46:37,680 --> 00:46:40,680
I'm in Calgary, Alberta, Canada.

759
00:46:40,680 --> 00:46:44,680
And I love the optimism around where we're headed,

760
00:46:44,680 --> 00:46:46,680
a future of abundance.

761
00:46:46,680 --> 00:46:49,680
What I would really love to know is your perspective on

762
00:46:49,680 --> 00:46:53,680
as we cure diseases, as we have access to this knowledge

763
00:46:53,680 --> 00:46:57,680
instantly, what are some of the downsides or the threats

764
00:46:57,680 --> 00:46:59,680
that we might be missing, that we're going to have to face

765
00:46:59,680 --> 00:47:01,680
in the future?

766
00:47:01,680 --> 00:47:02,680
Yeah.

767
00:47:02,680 --> 00:47:07,680
Well, each of my books actually has an apparel's chapter.

768
00:47:07,680 --> 00:47:09,680
My generation was the first to grow up with that.

769
00:47:09,680 --> 00:47:11,680
I remember in elementary school,

770
00:47:11,680 --> 00:47:15,680
we would have these drills to prepare for a nuclear war

771
00:47:15,680 --> 00:47:17,680
and we would actually get under our desk,

772
00:47:17,680 --> 00:47:21,680
put our hands behind our hands.

773
00:47:21,680 --> 00:47:23,680
It seemed to work.

774
00:47:23,680 --> 00:47:27,680
We're all still here.

775
00:47:27,680 --> 00:47:30,680
But these new technologies do have downsides.

776
00:47:30,680 --> 00:47:35,680
You can certainly imagine AI being in the power of some body,

777
00:47:35,680 --> 00:47:39,680
could be a human or any other type of entity that wants to

778
00:47:39,680 --> 00:47:41,680
control us.

779
00:47:41,680 --> 00:47:44,680
And it could happen.

780
00:47:44,680 --> 00:47:48,680
I was actually part of the Asilomark conference on bringing

781
00:47:48,680 --> 00:47:52,680
ethics to AI to prevent that kind of thing.

782
00:47:52,680 --> 00:47:56,680
I am optimistic, but I'm also worried about it.

783
00:47:56,680 --> 00:48:00,680
Nanotechnology, biotechnology.

784
00:48:00,680 --> 00:48:06,680
I mean, we just had this COVID go through our planet.

785
00:48:06,680 --> 00:48:09,680
We don't actually know where it came from,

786
00:48:09,680 --> 00:48:11,680
but somebody could create somebody.

787
00:48:11,680 --> 00:48:15,680
Right now, viruses, they either spread very easily,

788
00:48:15,680 --> 00:48:17,680
but they don't make us that sick,

789
00:48:17,680 --> 00:48:20,680
or they don't spread that easily and they can kill us.

790
00:48:20,680 --> 00:48:23,680
We generally don't have anything that could go through

791
00:48:23,680 --> 00:48:28,680
the entire human beings and kill everybody.

792
00:48:28,680 --> 00:48:32,680
But someone could actually design that.

793
00:48:32,680 --> 00:48:36,680
So we have to be very mindful of avoiding these types of

794
00:48:36,680 --> 00:48:37,680
parallels.

795
00:48:37,680 --> 00:48:39,680
So I put that into one chapter.

796
00:48:39,680 --> 00:48:42,680
I do think if you actually look at how we're living,

797
00:48:42,680 --> 00:48:46,680
we're living far better than we've ever done before.

798
00:48:46,680 --> 00:48:50,680
And in terms of health, in terms of progress,

799
00:48:50,680 --> 00:48:54,680
in terms of recreation and everything else.

800
00:48:54,680 --> 00:48:58,680
But yes, there's ways of these technologies being quite

801
00:48:58,680 --> 00:48:59,680
abusive.

802
00:48:59,680 --> 00:49:06,680
And that happens when I was born with the atomic age.

803
00:49:06,680 --> 00:49:07,680
Please, sir.

804
00:49:07,680 --> 00:49:09,680
Hi, my name is Yasin.

805
00:49:09,680 --> 00:49:11,680
I'm from the Netherlands.

806
00:49:11,680 --> 00:49:14,680
And as I was trying to think of a question,

807
00:49:14,680 --> 00:49:15,680
I wasn't sure.

808
00:49:15,680 --> 00:49:19,680
So I asked Chad Dupetit, I'm sitting right next to Ray.

809
00:49:19,680 --> 00:49:21,680
Give me some tough questions.

810
00:49:21,680 --> 00:49:25,680
And the one that was really interesting is kind of what the

811
00:49:25,680 --> 00:49:27,680
German lady was just saying.

812
00:49:27,680 --> 00:49:29,680
As AI becomes more advanced, their concerns,

813
00:49:29,680 --> 00:49:34,680
it may become impossible for humans to understand how

814
00:49:34,680 --> 00:49:36,680
AI makes decisions.

815
00:49:36,680 --> 00:49:39,680
So how do we ensure AI systems are transparent and

816
00:49:39,680 --> 00:49:42,680
accountable to humans always?

817
00:49:42,680 --> 00:49:45,680
Well, I'm not sure that's really the right thing.

818
00:49:45,680 --> 00:49:49,680
I deal with human beings and I can always account for what

819
00:49:49,680 --> 00:49:55,680
they might be doing.

820
00:49:55,680 --> 00:50:01,680
So I think we have to actually export certain values.

821
00:50:01,680 --> 00:50:06,680
I try to associate with people who have somebody,

822
00:50:06,680 --> 00:50:09,680
I may not be able to predict what they're doing,

823
00:50:09,680 --> 00:50:12,680
but I understand what they're about and what they're trying

824
00:50:12,680 --> 00:50:14,680
to accomplish.

825
00:50:14,680 --> 00:50:17,680
And we need to teach that to our machines as well.

826
00:50:17,680 --> 00:50:19,680
I actually think large language models,

827
00:50:19,680 --> 00:50:21,680
I mean, even though people are concerned,

828
00:50:21,680 --> 00:50:23,680
they might say the wrong thing.

829
00:50:23,680 --> 00:50:24,680
And sometimes they do.

830
00:50:24,680 --> 00:50:26,680
I mean, there was a large language model,

831
00:50:26,680 --> 00:50:28,680
I won't say where it came from,

832
00:50:28,680 --> 00:50:31,680
but it's talking about suicide and it actually said,

833
00:50:31,680 --> 00:50:33,680
well, maybe you should try that.

834
00:50:33,680 --> 00:50:36,680
Not the correct answer.

835
00:50:36,680 --> 00:50:40,680
We want people to understand the impact that it will have

836
00:50:40,680 --> 00:50:45,680
on other people and internalize that and try to make that be

837
00:50:45,680 --> 00:50:51,680
the greatest value in the decisions it's made.

838
00:50:51,680 --> 00:50:55,680
But we already can't predict what these large language models

839
00:50:55,680 --> 00:51:00,680
will do, but I think we are actually sharing our values

840
00:51:00,680 --> 00:51:02,680
with them.

841
00:51:02,680 --> 00:51:03,680
Thank you.

842
00:51:03,680 --> 00:51:07,680
Let's go to Shailesh on Zoom.

843
00:51:07,680 --> 00:51:11,680
We're also monitoring upvoted questions in Slido here

844
00:51:11,680 --> 00:51:13,680
and then we'll come back here.

845
00:51:13,680 --> 00:51:14,680
Shailesh.

846
00:51:14,680 --> 00:51:15,680
Go ahead, Shailesh.

847
00:51:15,680 --> 00:51:19,680
I'm in Mumbai, India.

848
00:51:19,680 --> 00:51:24,680
So my question to you, Ray, is do you have a prediction

849
00:51:24,680 --> 00:51:29,680
of when the entire world will get to net zero

850
00:51:29,680 --> 00:51:32,680
and we'll be able to breathe cleaner air

851
00:51:32,680 --> 00:51:35,680
and drink safer water?

852
00:51:35,680 --> 00:51:38,680
Well, if you look at some of the graphs in Peter's book

853
00:51:38,680 --> 00:51:41,680
and in my book, you see we definitely headed in that direction.

854
00:51:41,680 --> 00:51:44,680
We're not there.

855
00:51:44,680 --> 00:51:47,680
Alternative energy, for example, is actually expanding

856
00:51:47,680 --> 00:51:49,680
at an exponential pace.

857
00:51:49,680 --> 00:51:52,680
By the early 30s, we'll be able to actually get all of our energy

858
00:51:52,680 --> 00:51:55,680
through renewable sources.

859
00:51:55,680 --> 00:52:00,680
It's not true today, but we're actually headed in that direction.

860
00:52:00,680 --> 00:52:03,680
Not everybody has access to the internet.

861
00:52:03,680 --> 00:52:08,680
Although I walked through San Francisco and these homeless cities

862
00:52:08,680 --> 00:52:11,680
and somebody actually takes out his cell phone and makes a call.

863
00:52:11,680 --> 00:52:17,680
So I mean, it is spreading quite rapidly.

864
00:52:17,680 --> 00:52:24,680
By 2029, computers will pass the Turing test.

865
00:52:24,680 --> 00:52:27,680
They certainly can do it in many ways already.

866
00:52:27,680 --> 00:52:30,680
Once it can actually do everything that humans can do,

867
00:52:30,680 --> 00:52:32,680
it'll go way past that.

868
00:52:32,680 --> 00:52:37,680
But as they say, we're going to bring them into ourselves.

869
00:52:37,680 --> 00:52:42,680
2045 is when I said we will actually multiply our intelligence

870
00:52:42,680 --> 00:52:46,680
millions fold, and that's going to be true of everybody.

871
00:52:46,680 --> 00:52:52,680
And we'll be able to get rid of the kinds of terrible lives that we see

872
00:52:52,680 --> 00:52:57,680
through poverty and lack of access to information.

873
00:52:57,680 --> 00:53:01,680
So it's really just the next few decades that we need to get through.

874
00:53:01,680 --> 00:53:04,680
But we're already making a lot of progress.

875
00:53:05,680 --> 00:53:06,680
Thank you.

876
00:53:06,680 --> 00:53:07,680
Thank you, Shilash.

877
00:53:07,680 --> 00:53:08,680
Please.

878
00:53:08,680 --> 00:53:09,680
Hey, Ray.

879
00:53:09,680 --> 00:53:10,680
My name is Ashish.

880
00:53:10,680 --> 00:53:14,680
I'm representing chemicals and material space.

881
00:53:14,680 --> 00:53:21,680
So my question to you is if you had the chemical industry executives as your audience,

882
00:53:21,680 --> 00:53:27,680
what would you like chemical industry or materials industry to do to move forward?

883
00:53:27,680 --> 00:53:33,680
Well, as I said, my grandmother was actually the first person

884
00:53:33,680 --> 00:53:38,680
who had a PhD in chemistry in Europe.

885
00:53:38,680 --> 00:53:41,680
And I actually asked something like that.

886
00:53:41,680 --> 00:53:47,680
She said, well, chemistry is really something that serves other industries.

887
00:53:47,680 --> 00:53:50,680
So we need to see what other industries need.

888
00:53:50,680 --> 00:53:55,680
What kind of products do we need to make LLMs more powerful?

889
00:53:55,680 --> 00:54:00,680
What kind of chemicals do we need to prevent certain types of diseases?

890
00:54:00,680 --> 00:54:05,680
And so it's not any one particular type of thing.

891
00:54:05,680 --> 00:54:09,680
It's really service to every other industry that we're trying to advance.

892
00:54:09,680 --> 00:54:10,680
Hey, everyone.

893
00:54:10,680 --> 00:54:14,680
I want to take a quick break from this episode to tell you about a health product that I

894
00:54:14,680 --> 00:54:17,680
love and that I use every day.

895
00:54:17,680 --> 00:54:19,680
In fact, I use it twice a day.

896
00:54:19,680 --> 00:54:22,680
It seeds DS01 daily symbiotic.

897
00:54:22,680 --> 00:54:27,680
Hopefully by now you understand that your microbiome and your gut health are one of the most

898
00:54:27,680 --> 00:54:30,680
important modifiable parts of your health.

899
00:54:30,680 --> 00:54:32,680
Your gut microbiome is connected to everything.

900
00:54:32,680 --> 00:54:35,680
Your brain health, your cardiac health, your metabolic health.

901
00:54:35,680 --> 00:54:38,680
So the question is, what are you doing to optimize your gut?

902
00:54:38,680 --> 00:54:41,680
Let me take a moment to tell you about what I'm doing.

903
00:54:41,680 --> 00:54:46,680
Every day I take two capsules of seeds DS01 daily symbiotic.

904
00:54:46,680 --> 00:54:51,680
It's a two-in-one probiotic and prebiotic formulation that supports digested health,

905
00:54:51,680 --> 00:54:54,680
gut health, skin health, heart health, and more.

906
00:54:54,680 --> 00:55:00,680
It contains 24 clinically and scientifically proven probiotic strains that are delivered

907
00:55:00,680 --> 00:55:06,680
in a patented capsule that actually protects the contents from your stomach acid and ensures

908
00:55:06,680 --> 00:55:09,680
that 100% of it is survivable reaching your colon.

909
00:55:09,680 --> 00:55:16,680
Now, if you want to try seed DS01 daily symbiotic for yourself, you can get 25% off your first month

910
00:55:16,680 --> 00:55:20,680
supply by using the code peter25 at checkout.

911
00:55:20,680 --> 00:55:26,680
Just go to seed.com.moonshots and enter the code peter25 at checkout.

912
00:55:26,680 --> 00:55:34,680
That's seed.com.moonshots and use the code peter25 to get your 25% off the first month

913
00:55:34,680 --> 00:55:36,680
of seeds daily symbiotic.

914
00:55:36,680 --> 00:55:38,680
Trust me, your gut will thank you.

915
00:55:38,680 --> 00:55:40,680
All right, let's go back to the episode.

916
00:55:40,680 --> 00:55:42,680
Please, and then we'll go to Zoom next.

917
00:55:42,680 --> 00:55:43,680
Yes, sir.

918
00:55:43,680 --> 00:55:44,680
Thank you.

919
00:55:44,680 --> 00:55:45,680
Hi, Ray.

920
00:55:45,680 --> 00:55:46,680
My name's Pete Zacco.

921
00:55:46,680 --> 00:55:47,680
I'm from New Jersey.

922
00:55:47,680 --> 00:55:48,680
I design and build data centers.

923
00:55:48,680 --> 00:55:53,680
This is about decentralization and especially the migration we're seeing of technologies

924
00:55:53,680 --> 00:55:58,680
from the mainframe where the product was the mainframe hardware, and then we saw software

925
00:55:58,680 --> 00:56:02,680
and then we saw us as the product in the centralized internet.

926
00:56:02,680 --> 00:56:08,680
My question is what predictions and thoughts that you have about this decentralization trend

927
00:56:08,680 --> 00:56:12,680
we find ourselves ultimately at perhaps ending with the decentralization of the internet

928
00:56:12,680 --> 00:56:16,680
and individual ownership of data rather than central ownership of data.

929
00:56:16,680 --> 00:56:17,680
Thank you.

930
00:56:17,680 --> 00:56:24,680
Yeah, well, it's a lot of questions, but I think everything is moving to the cloud.

931
00:56:24,680 --> 00:56:26,680
And people say everything in the cloud.

932
00:56:26,680 --> 00:56:28,680
So someone could blow up one of these cloud centers.

933
00:56:28,680 --> 00:56:30,680
We lose everything, but that's not the case.

934
00:56:30,680 --> 00:56:36,680
Even today, if you store something in the cloud, it's multiplied several dozen folds

935
00:56:36,680 --> 00:56:42,680
and it's put in different places and you could blow up any data center and you'd still have

936
00:56:42,680 --> 00:56:43,680
that information.

937
00:56:43,680 --> 00:56:50,680
In fact, if ultimately we're going to have our thinking is going to be in our brains

938
00:56:50,680 --> 00:56:57,680
and in the computer, the brain part is not going to grow, but the computer part will grow

939
00:56:57,680 --> 00:57:03,680
and ultimately most of our thinking will be in the computer part.

940
00:57:03,680 --> 00:57:09,680
And so we don't want to lose that.

941
00:57:09,680 --> 00:57:16,680
I think it'd be actually very hard to actually exit the world because every part of our thinking

942
00:57:16,680 --> 00:57:21,680
will be in the cloud and the cloud has multiplied hundreds, maybe thousands of fold

943
00:57:21,680 --> 00:57:28,680
and so you could blow up, you know, 90% of it you'd still have everything that was there before.

944
00:57:28,680 --> 00:57:34,680
So redundancy is actually a major advantage of cloud thinking.

945
00:57:34,680 --> 00:57:36,680
We used to have computers.

946
00:57:36,680 --> 00:57:45,680
I mean, I got access to IBM 1620 when I was 14.

947
00:57:45,680 --> 00:57:50,680
A 14-year-old using computers is hardly amazing today, but there are only 12 computers in

948
00:57:50,680 --> 00:57:52,680
all of New York City at that time.

949
00:57:52,680 --> 00:57:55,680
And yet actually go to the computer.

950
00:57:55,680 --> 00:57:58,680
And if anything happened to the computer, that data would be lost.

951
00:57:58,680 --> 00:58:00,680
But now everything is stored in the cloud.

952
00:58:00,680 --> 00:58:03,680
Everything on your phone is stored in the cloud.

953
00:58:03,680 --> 00:58:09,680
So, and I think that's a good thing because I think information is extremely important.

954
00:58:09,680 --> 00:58:11,680
Mattie, please.

955
00:58:11,680 --> 00:58:14,680
Hi, Mattie from Houston, Texas.

956
00:58:14,680 --> 00:58:19,680
We've talked a lot about a post-scarcity world here and I wanted to know how do you see the

957
00:58:19,680 --> 00:58:26,680
future of currency jobs and just general value?

958
00:58:26,680 --> 00:58:34,680
Well, jobs is actually a large section of my next book about jobs and what it is that

959
00:58:34,680 --> 00:58:37,680
we'd like to accomplish.

960
00:58:37,680 --> 00:58:41,680
And jobs have turned over many, many times.

961
00:58:41,680 --> 00:58:47,680
I mean, none of the jobs that people have in 1800 and it's almost true of 1900 to people

962
00:58:47,680 --> 00:58:51,680
have today and yet we have many more people working.

963
00:58:51,680 --> 00:58:59,680
And jobs in general is something that people more and more actually like doing because it uses

964
00:58:59,680 --> 00:59:02,680
that creativity.

965
00:59:02,680 --> 00:59:14,680
And but we still see, you know, people striking over advancing retirement age from 60 to 62.

966
00:59:14,680 --> 00:59:18,680
I feel that I actually retired when I was five because I decided to be an inventor.

967
00:59:18,680 --> 00:59:21,680
That seemed really exciting to me and I'm still an inventor.

968
00:59:21,680 --> 00:59:25,680
So I think we'll be able to do what we want to do.

969
00:59:25,680 --> 00:59:31,680
We'll be exposed to many more types of problems that we'd like to solve.

970
00:59:31,680 --> 00:59:36,680
We'll be able to solve things much more quickly than we did before, but we get used to that.

971
00:59:36,680 --> 00:59:38,680
And people forget what things are like.

972
00:59:38,680 --> 00:59:41,680
People think the world is always the way it was today.

973
00:59:41,680 --> 00:59:46,680
Go back five years, 50 years, 50 years in the future, it's always the same.

974
00:59:46,680 --> 00:59:52,680
But if you actually look at history, you see it's constantly changing.

975
00:59:52,680 --> 00:59:53,680
Thank you, Joe.

976
00:59:53,680 --> 00:59:54,680
Hi, right.

977
00:59:54,680 --> 00:59:57,680
Joe Honan from Bainbridge Island, Washington.

978
00:59:57,680 --> 01:00:04,680
Several years ago, I had asked you a question about, you know, these big ideas that you have.

979
01:00:04,680 --> 01:00:05,680
How do you work on it?

980
01:00:05,680 --> 01:00:06,680
When do you have time?

981
01:00:06,680 --> 01:00:11,680
And you said you assign yourself a question before you go to sleep and and you activate your brain

982
01:00:11,680 --> 01:00:12,680
through that.

983
01:00:12,680 --> 01:00:13,680
Do you still do that?

984
01:00:13,680 --> 01:00:17,680
Or do you rely upon GTP4 or something else for that now?

985
01:00:17,680 --> 01:00:22,680
But more importantly, you are such an amazing predictor of things.

986
01:00:22,680 --> 01:00:25,680
So what surprised, what has surprised you?

987
01:00:25,680 --> 01:00:28,680
What is something that you didn't expect that you've seen?

988
01:00:28,680 --> 01:00:30,680
I think we'd all be fascinated with that.

989
01:00:30,680 --> 01:00:34,680
Well, I'll start with that.

990
01:00:34,680 --> 01:00:40,680
I mean, large language models, it's quite consistent with what I've said, but I'm still amazed by it, right?

991
01:00:40,680 --> 01:00:45,680
I mean, you can put something into the computer and you get something that's totally surprising

992
01:00:45,680 --> 01:00:51,680
and totally delightful that didn't exist like a year or two ago.

993
01:00:51,680 --> 01:00:58,680
And even though I kind of saw that happening when I actually experienced it, it surprises me

994
01:00:58,680 --> 01:01:00,680
and is quite delightful.

995
01:01:00,680 --> 01:01:02,680
And we're going to see that more and more.

996
01:01:02,680 --> 01:01:07,680
I mean, every six months, it's going to be a whole new world.

997
01:01:07,680 --> 01:01:11,680
As for lucid thinking, yes, that's how I go to sleep.

998
01:01:11,680 --> 01:01:18,680
I go to sleep and it's really kind of hard to go from a waking state like I am now to being asleep.

999
01:01:18,680 --> 01:01:26,680
So I start thinking about what could we do with computers and different things and just fantasize about that.

1000
01:01:26,680 --> 01:01:30,680
And if something doesn't seem feasible, I just, well, we'll figure that out.

1001
01:01:30,680 --> 01:01:31,680
I kind of step over it.

1002
01:01:31,680 --> 01:01:32,680
We'll be able to do it anyway.

1003
01:01:32,680 --> 01:01:35,680
I mean, that's how I go to sleep.

1004
01:01:35,680 --> 01:01:38,680
And in the morning, the best ideas actually are still there.

1005
01:01:38,680 --> 01:01:42,680
So I do use lucid dreaming to come up with ideas.

1006
01:01:42,680 --> 01:01:43,680
Thank you, Joe.

1007
01:01:43,680 --> 01:01:45,680
Yousef, welcome.

1008
01:01:45,680 --> 01:01:46,680
Hi, Ray.

1009
01:01:46,680 --> 01:01:49,680
This is Yousef from Abu Dhabi UAE.

1010
01:01:49,680 --> 01:01:51,680
The question for you, Ray, but also for the audience.

1011
01:01:51,680 --> 01:01:54,680
So if you have any thoughts, ideas, please reach out.

1012
01:01:54,680 --> 01:02:01,680
So we're trying to rethink our parenting in Abu Dhabi and how we create more family time

1013
01:02:01,680 --> 01:02:05,680
and engagement between parents and children, for young children.

1014
01:02:05,680 --> 01:02:11,680
And I'm curious how we can adapt exponential thinking and abundant thinking into this.

1015
01:02:11,680 --> 01:02:18,680
And what are these technologies that might help us to disrupt this type of activities?

1016
01:02:18,680 --> 01:02:25,680
Yeah, well, it does make me think, what can we actually do with the extra time we have

1017
01:02:26,680 --> 01:02:31,680
working with computers and being able to do things much more quickly.

1018
01:02:31,680 --> 01:02:33,680
And actually, I think it will help family time.

1019
01:02:33,680 --> 01:02:38,680
If you talk to very busy people even today, they're so busy,

1020
01:02:38,680 --> 01:02:41,680
they have no time to deal with their family.

1021
01:02:41,680 --> 01:02:48,680
And so I did spend a lot of time actually learning a lot.

1022
01:02:48,680 --> 01:02:52,680
My daughter is actually a cartoonist for the New Yorkers.

1023
01:02:52,680 --> 01:03:01,680
And she has very interesting ideas and she's actually collaborated with her on many projects.

1024
01:03:01,680 --> 01:03:05,680
So how you parent, I think it's different.

1025
01:03:05,680 --> 01:03:11,680
There are different types of cultures and different things that we value in parenting.

1026
01:03:11,680 --> 01:03:20,680
But I think we'll actually have more time for the positive aspects of that as computers do more of the routine work that we'd rather not do.

1027
01:03:20,680 --> 01:03:21,680
Thank you.

1028
01:03:21,680 --> 01:03:22,680
I want to make a quick point here.

1029
01:03:22,680 --> 01:03:29,680
If you went back 50, 70 years ago, if you were a parent and something happened with a child, you had no idea what to do.

1030
01:03:29,680 --> 01:03:30,680
We had no resources.

1031
01:03:30,680 --> 01:03:33,680
You could basically ask the immediate five people around you.

1032
01:03:33,680 --> 01:03:37,680
And now we have data sets, socialization of issues globally.

1033
01:03:37,680 --> 01:03:40,680
And you can ask the internet, there's a million resources.

1034
01:03:40,680 --> 01:03:46,680
And I think we've probably taken parenting at least in order of magnitude better than it was a few generations ago.

1035
01:03:46,680 --> 01:03:49,680
And we don't, this is one of the examples that we don't see very often.

1036
01:03:49,680 --> 01:03:55,680
Interesting wisdom beyond actually, I don't think I would have had the career I had if we didn't have a different attitude.

1037
01:03:55,680 --> 01:04:03,680
I mean, I was six, seven years old and I would actually wander through the neighborhood and find things and bring them back.

1038
01:04:03,680 --> 01:04:07,680
And this is not something you would allow a child to do then.

1039
01:04:07,680 --> 01:04:13,680
But that actually got me on this path that I'm still on.

1040
01:04:13,680 --> 01:04:16,680
Let's go to our final question here.

1041
01:04:16,680 --> 01:04:20,680
A good one to close on, I'm sure, Dr. Alex Zavankov.

1042
01:04:20,680 --> 01:04:21,680
Thank you.

1043
01:04:21,680 --> 01:04:23,680
Great fan, Alex Zavankov.

1044
01:04:23,680 --> 01:04:26,680
I founded a company called Insilica Medicine.

1045
01:04:26,680 --> 01:04:30,680
And my question is maybe a little bit personal.

1046
01:04:30,680 --> 01:04:34,680
So right now, according to your bio, you are 75.

1047
01:04:34,680 --> 01:04:37,680
And that's a very interesting age to be.

1048
01:04:37,680 --> 01:04:44,680
I always like to talk to people of various ages to understand how to plan my own life.

1049
01:04:44,680 --> 01:04:46,680
And two questions.

1050
01:04:46,680 --> 01:04:52,680
So one is what is your roadmap for your own personal longevity?

1051
01:04:52,680 --> 01:04:57,680
How do you predict your own personal persona is going to evolve?

1052
01:04:57,680 --> 01:04:59,680
What are you doing to live longer?

1053
01:04:59,680 --> 01:05:05,680
And do you think you have a chance to live to, let's say, 200?

1054
01:05:05,680 --> 01:05:13,680
And the second question is that if you were to go back in time, what would you have done differently in the past, let's say, 20 years?

1055
01:05:14,680 --> 01:05:18,680
Well, first of all, getting to 200.

1056
01:05:18,680 --> 01:05:23,680
So that would be 125 years from now.

1057
01:05:23,680 --> 01:05:29,680
How much technological progress will we make in the next 125 years?

1058
01:05:29,680 --> 01:05:31,680
Even 25 years.

1059
01:05:31,680 --> 01:05:37,680
I mean, we're going to be able to overcome most of the problems that we have 125 years.

1060
01:05:37,680 --> 01:05:40,680
And our thinking will be in the cloud.

1061
01:05:40,680 --> 01:05:42,680
The cloud will be multiplied many times.

1062
01:05:42,680 --> 01:05:48,680
It will overcome some of the issues we have with people being depressed and so on.

1063
01:05:48,680 --> 01:05:52,680
I mean, so it's not like living to 200.

1064
01:05:52,680 --> 01:05:59,680
I mean, I think we get to a point where dying is going to be kind of an option that people don't use.

1065
01:05:59,680 --> 01:06:06,680
And if you look at people that actually do take their lives, the only reason they take it is because they have terrible suffering.

1066
01:06:07,680 --> 01:06:15,680
From physical pain, moral pain, emotional pain, spiritual pain, but something is really bothering them and they just can't stand to be here.

1067
01:06:15,680 --> 01:06:22,680
But if you actually live your life in a positive way, contribute to each other, I think we're going to want to live.

1068
01:06:22,680 --> 01:06:24,680
And we're not that far away.

1069
01:06:24,680 --> 01:06:33,680
I mean, I believe by 2029, that's like six, seven years from now, when you go forward a year, we're going to push your longevity escape.

1070
01:06:34,680 --> 01:06:42,680
Your life expectancy forward at least a year and then ultimately more than a year.

1071
01:06:42,680 --> 01:06:47,680
So rather than using up time, we'll actually gain more time.

1072
01:06:47,680 --> 01:06:53,680
And I really feel I'm doing what I did when I was five, six, seven years old.

1073
01:06:53,680 --> 01:07:02,680
I have much more powerful tools now and many more people are appreciative and I appreciate the tools more than I did back then.

1074
01:07:02,680 --> 01:07:08,680
But we really discovered there's still a lot we don't know about the world and we're going to continue to learn more and more about that.

1075
01:07:08,680 --> 01:07:09,680
Okay.

1076
01:07:09,680 --> 01:07:10,680
Thank you.

1077
01:07:10,680 --> 01:07:13,680
Harry, do you want to ask your quick prediction?

1078
01:07:13,680 --> 01:07:20,680
Ray, when do you think we're going to have our personal robot buddy like Rosie the robot?

1079
01:07:20,680 --> 01:07:24,680
Well, I mean, you're working on that.

1080
01:07:24,680 --> 01:07:27,680
A lot of other people are working on it.

1081
01:07:27,680 --> 01:07:33,680
I think there's actually a little bit behind what we've done with language.

1082
01:07:33,680 --> 01:07:41,680
I think within five or six years, it's a 2029 we're going to have people that can help us.

1083
01:07:41,680 --> 01:07:45,680
Some of them will look like humans because it's a useful way to look.

1084
01:07:45,680 --> 01:07:52,680
I think humans are pretty good, but there's other ways that they can manifest themselves will change who we are.

1085
01:07:52,680 --> 01:08:05,680
I see that already people dress up in ways that were really not acceptable when I was like 10 years old and that's going to expand far greater.

1086
01:08:05,680 --> 01:08:13,680
But actually robots that do what humans do and can actually be put into places where we wouldn't want to put humans like a burning building.

1087
01:08:13,680 --> 01:08:18,680
I think that's happening very soon over the next five, six years.

1088
01:08:18,680 --> 01:08:26,680
Ray, our longevity platinum trip is going to be in August and September in Boston, Cambridge near where you're living.

1089
01:08:26,680 --> 01:08:33,680
I would love if you would come and spend the day with us there and go deeper into the world as well.

1090
01:08:33,680 --> 01:08:36,680
That actually reminds me.

1091
01:08:36,680 --> 01:08:38,680
Yes, I love to do that.

1092
01:08:38,680 --> 01:08:44,680
And I've greatly enjoyed the many presentations we've done together.

1093
01:08:44,680 --> 01:08:46,680
I have this book coming out.

1094
01:08:46,680 --> 01:08:50,680
The Singularity is Nearer.

1095
01:08:50,680 --> 01:08:53,680
And I would like to make that available to the people here.

1096
01:08:53,680 --> 01:08:59,680
So I'll work with Peter on a way that we can actually get you connected with the book for free.

1097
01:08:59,680 --> 01:09:05,680
On that note, everybody, please give it up for Ray Kurzweil and Salim Ismail.

1098
01:09:14,680 --> 01:09:16,680
Thank you.

