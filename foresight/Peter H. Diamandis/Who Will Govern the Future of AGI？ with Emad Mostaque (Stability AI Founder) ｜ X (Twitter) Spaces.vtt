WEBVTT

00:00.000 --> 00:11.760
Hey, Amade, good to hear you.

00:11.760 --> 00:14.120
It was always a pleasure.

00:14.120 --> 00:15.120
Yeah.

00:15.120 --> 00:16.120
So, where are you today?

00:16.120 --> 00:17.120
I'm in London.

00:17.120 --> 00:18.120
Good.

00:18.120 --> 00:22.960
Other side of the planet, I'm in Santa Monica.

00:22.960 --> 00:29.640
It's been quite the extraordinary game of ping pong out there these last four or five

00:29.640 --> 00:30.640
days.

00:30.640 --> 00:36.160
I didn't think the first thing that AI would disrupt would be reality TV, right?

00:36.160 --> 00:37.160
Yeah.

00:37.160 --> 00:46.880
It's been fascinating how X has become sort of the go-to place to find out the latest

00:46.880 --> 00:51.600
of where Sam is working and what's going on with the AI industry.

00:51.600 --> 00:55.160
Yeah, you found the notifications in the wake, guys.

00:55.160 --> 00:59.320
I mean, it's the thing, like, what else will move at the speed of this?

00:59.320 --> 01:03.320
Like, I was saying to someone recently, AI research doesn't really move at the speed

01:03.320 --> 01:05.720
of conferences or even PDFs anymore, right?

01:05.720 --> 01:07.920
You just wake up and you're like, oh, it's 10 times faster.

01:07.920 --> 01:10.880
So, I think that's why X is quite good.

01:10.880 --> 01:16.840
I actually, like, unfollow just about everyone and just the AI algorithms find the most interesting

01:16.840 --> 01:17.840
things for me.

01:17.840 --> 01:21.400
So, I've got, like, 10 people that I follow, and it's actually working really well.

01:21.400 --> 01:22.400
It's getting better.

01:22.400 --> 01:25.120
Well, it has been.

01:25.120 --> 01:26.960
I've been enjoying the conversation.

01:26.960 --> 01:33.360
It really feels like you're inside an intimate conversation among friends as this is going

01:33.360 --> 01:34.360
back and forth.

01:34.360 --> 01:42.760
I think this entire four or five days has been an extraordinary, up-close, intimate conversation

01:42.760 --> 01:48.440
around governance and around, you know, what's the future of AI?

01:48.800 --> 01:56.200
Honestly, you know, as it gets faster and more powerful, the cost of missteps is going

01:56.200 --> 01:57.960
to increase exponentially.

01:59.960 --> 02:01.960
Let's begin here.

02:01.960 --> 02:08.040
I mean, you've been making the argument about open source as one of the most critical elements

02:08.040 --> 02:09.600
of governance for a while now.

02:09.600 --> 02:11.400
Can you just, let's hop into that.

02:13.080 --> 02:17.080
Yeah, I think that, you know, open source is the difficult one because it means a few

02:17.080 --> 02:18.080
different things.

02:18.120 --> 02:20.280
Like, is it models you can download and use?

02:20.280 --> 02:23.720
Do you make all the data available and free?

02:23.720 --> 02:28.240
And then when you actually look at what all these big companies do, all their stuff is

02:28.240 --> 02:33.560
built on open source basis, you know, it's built on the transformer paper.

02:33.560 --> 02:41.200
It's built on, like, the new model by Khayfouli and Vira1.ai is basically Lama.

02:41.200 --> 02:45.400
Like it's actually got the same variable names and other things like that, plus a gigantic

02:45.400 --> 02:46.520
supercomputer, right?

02:46.520 --> 02:51.800
The whole conversation has been, you know, how important is openness and transparency?

02:51.800 --> 02:56.440
And what are the governance models that are going to allow the most powerful technology

02:56.440 --> 03:03.040
on the planet to enable the most benefit for humanity and the safety?

03:03.040 --> 03:10.000
So, I mean, you've been thinking about this and speaking to transparency, openness, governance

03:10.000 --> 03:12.000
for a while.

03:12.000 --> 03:16.400
Could you, I mean, what do you think is going to be, what do you think is we need to be

03:16.400 --> 03:17.400
focused on?

03:17.400 --> 03:19.600
Where do we need to evolve to?

03:19.600 --> 03:23.080
Yeah, it's a complicated topic.

03:23.080 --> 03:28.840
I think that most of the infrastructure of the Internet is open source, Linux, everything

03:28.840 --> 03:29.960
like that.

03:29.960 --> 03:36.600
I think these models, it's unlikely that our governments will be run on GPT-7 or Baal

03:36.600 --> 03:37.600
or anything like that.

03:37.600 --> 03:41.600
How are you going to have black boxes that run these things?

03:41.600 --> 03:46.000
I think a lot of the governance debate has been hijacked by the AI safety debate where

03:46.000 --> 03:49.800
people are talking about AGI killing us all, and then there's this precautionary principle

03:49.800 --> 03:50.800
that kicks in.

03:50.800 --> 03:53.880
It's too dangerous to let out because what if China gets it?

03:53.880 --> 03:56.080
What if someone builds an AGI that kills us all?

03:56.080 --> 04:01.600
It'd be great to have this amazing board that could pull the off switch, you know, whereas

04:01.600 --> 04:08.320
in reality, I think that you're seeing a real social impact from this technology, and it's

04:08.320 --> 04:13.800
about who advances forward and who's left behind if we're thinking about risk.

04:13.800 --> 04:19.120
Because governance is always about finding, as you said, the best outcomes and also mitigating

04:19.120 --> 04:21.040
against the harms, right?

04:21.040 --> 04:24.800
And there's some very real, amazingly positive outcomes that are now emerging that people

04:24.800 --> 04:30.920
can agree on, but also some very real social impacts that we have to mitigate against.

04:30.920 --> 04:39.280
So I mean, let's begin, how is stability governed?

04:39.280 --> 04:43.920
Stability is basically governed by me.

04:43.920 --> 04:47.640
So I looked in foundations and DAOs and everything like that, and I thought to take it to where

04:47.640 --> 04:48.640
we are now.

04:48.640 --> 04:52.400
I needed to have very singular governance, but now we're looking at other alternatives.

04:52.400 --> 04:53.720
And what do you think it's going to be?

04:53.720 --> 04:55.000
Where would you head in the future?

04:55.000 --> 04:59.600
I mean, let's actually jump away from this in particular.

04:59.600 --> 05:04.040
But do you recommend the most powerful technologies on the planet?

05:04.040 --> 05:05.760
How should they be governed?

05:05.760 --> 05:07.160
How should they be owned?

05:07.160 --> 05:12.840
You know, where should we be in five years?

05:12.840 --> 05:17.400
I think there need to be public goods that are collectively owned and then individually

05:17.400 --> 05:18.720
owned as well.

05:18.720 --> 05:24.840
So for example, there was the tweet kind of storm, the kind of I am Spartacus or his

05:24.840 --> 05:31.520
name is Robert Boulson from the OpenAI team saying, OpenAI is nothing without its people.

05:31.520 --> 05:37.560
Stability, we have amazing people, 190 and 65 top researchers.

05:37.560 --> 05:42.600
Without its people, we're open models used by hundreds of millions, it continues.

05:42.600 --> 05:46.040
And if you think about where you need to go, you can never have a choke point on this technology,

05:46.040 --> 05:49.640
I think it becomes part of your life.

05:49.640 --> 05:53.440
Like the phrase I have is not your models, not your mind.

05:53.440 --> 05:59.480
So these models, again, are just such interesting things to take billions of images or trillions

05:59.480 --> 06:05.120
of words and you get this file out that can do magic, right, trade or magic sand.

06:05.120 --> 06:10.840
I think that you will have pilots that gather our global knowledge on various modalities

06:10.840 --> 06:15.160
and you'll have co-pilots that you individually own that guide you through life.

06:15.160 --> 06:19.440
And I can't see how that can be controlled by any one organization.

06:19.440 --> 06:26.640
You've been on record talking about having models owned by the citizens of nations.

06:26.640 --> 06:28.520
Can you speak to that a little bit?

06:28.520 --> 06:29.520
Sure.

06:29.520 --> 06:34.680
So we just released some of the top Japanese models from visual language to language to

06:34.680 --> 06:38.640
Japanese SDXL as an example.

06:38.640 --> 06:41.560
So we're training for half a dozen different nations and models now.

06:41.560 --> 06:46.840
And the plan is to figure out a way to give ownership of these datasets and models back

06:46.840 --> 06:48.280
to the people of that nation.

06:48.760 --> 06:53.680
So you get the smartest people in Mexico to run a stability Mexico or maybe a different

06:53.680 --> 06:58.840
structure that then makes decisions for Mexicans with the Mexicans about the data and what

06:58.840 --> 07:00.520
goes in it.

07:00.520 --> 07:04.480
Because everyone's been focusing on the outputs, the inputs actually are the things that matter

07:04.480 --> 07:06.600
the most.

07:06.600 --> 07:10.440
The best way I've thought about thinking of these models is like very enthusiastic graduates.

07:10.440 --> 07:13.400
So hallucinations isn't just probably too hard.

07:13.400 --> 07:17.080
A lot of the things about like, oh, what about these bad things the models can output?

07:17.080 --> 07:19.320
It's about what you've input.

07:19.320 --> 07:24.240
And so what you put into that Mexican dataset or the Chinese or Vietnamese one will impact

07:24.240 --> 07:25.240
the outputs.

07:25.240 --> 07:31.400
And there's a great paper in Nature, Human Behavior today about that, about how foundational

07:31.400 --> 07:33.200
models are cultural technologies.

07:33.200 --> 07:40.600
So again, how can you outsource your culture and your brains to other countries, to people

07:40.600 --> 07:43.720
that are from a very different place?

07:43.720 --> 07:45.720
I think it eventually has to be localized.

07:46.080 --> 07:50.240
I think one of the points you said originally is we have to separate the issue of governance

07:50.240 --> 07:55.160
versus safety and alignment.

07:55.160 --> 08:00.920
Are they actually different?

08:00.920 --> 08:06.800
So I think that a lot of the safety discussion or this AGI risk discussion is because the

08:06.800 --> 08:11.640
future is so uncertain because it is so powerful.

08:11.640 --> 08:15.240
And we didn't have a good view of where we're going.

08:15.240 --> 08:19.040
So when you go on a journey and you don't know where you're going, you will minimize

08:19.040 --> 08:23.040
from acts and regret you'll have the precautionary principle.

08:23.040 --> 08:27.280
And then that means you basically go towards authority, you go towards trying to control

08:27.280 --> 08:31.080
this technology when it's so difficult to control.

08:31.080 --> 08:35.000
And you end up not doing much, you know, because there's anything to go wrong.

08:35.000 --> 08:38.080
When you have an idea of where we're going, like you should have all the cancer knowledge

08:38.080 --> 08:42.080
in the world at your fingertips or climate knowledge, or anybody should be able to create

08:42.080 --> 08:44.480
whole worlds and share them.

08:44.480 --> 08:49.600
When you align your safety discussions against the goal, against the location that you're

08:49.600 --> 08:53.920
going to, again, just like setting out on a journey, I think that's a big change.

08:53.920 --> 08:58.680
Similarly, most of the safety discussions have been on outputs, not inputs.

08:58.680 --> 09:02.480
If you have a high quality data set without knowledge about anthrax, your language model

09:02.480 --> 09:09.240
is unlikely to tell you how to build anthrax, you know, and transparency around that will

09:09.240 --> 09:10.240
be very useful.

09:10.240 --> 09:17.600
So let's dive into that safety alignment issue for a moment because it's an area you and

09:17.600 --> 09:19.120
I have been talking a lot about.

09:19.120 --> 09:25.000
So Mustafa wrote a book, Mustafa Suleiman wrote a book called The Coming Wave in which

09:25.000 --> 09:30.480
he talks about containment as the mechanism by which we're going to be making sure we

09:30.480 --> 09:33.200
have safe AI.

09:33.200 --> 09:38.320
You and I have had the conversation of it's really how you educate and raise and train

09:38.320 --> 09:44.920
your AI systems in making sure that there's full transparency and openness on the data

09:44.920 --> 09:45.920
sets that are utilized.

09:45.920 --> 09:49.000
Do you think containment is an option for safety?

09:49.000 --> 09:52.400
No, not at all.

09:52.400 --> 09:58.800
Like a number of leaders say, what if China gets open source AI?

09:58.800 --> 10:02.600
The reality is that China, Russia, everyone already has the weights for GPT-PORC.

10:02.600 --> 10:06.080
They just downloaded it on a USM stick.

10:06.080 --> 10:09.360
You know that there's been compromised, right?

10:09.360 --> 10:10.360
There's no way they couldn't.

10:10.360 --> 10:12.480
The rewards are too great.

10:12.480 --> 10:19.320
And there is a absolutely false dichotomy here and a lot of the companies want you to believe

10:19.320 --> 10:24.280
that giant models are the main thing and you need to have these gigantic, ridiculous

10:24.280 --> 10:26.080
supercomputers that only they can run.

10:26.080 --> 10:31.680
I mean, look, we run gigantic supercomputers, but the reality is this.

10:31.680 --> 10:38.480
The supercomputers and the giant trillion zillion data sets are just a shortcut for bad quality

10:38.480 --> 10:40.600
data.

10:40.600 --> 10:45.160
It's like using a hot pot or sous viding a steak that's bad quality.

10:45.160 --> 10:48.880
You cook it for longer and it organizes the information.

10:48.880 --> 10:54.080
With stable diffusion, we did a study and we showed that basically 92% of the data isn't

10:54.080 --> 11:00.560
used 99% of the time, you know, because now you're seeing this with, for example, Microsoft's

11:00.640 --> 11:04.520
by release, it's trained entirely on synthetic data.

11:04.520 --> 11:09.320
Dali three is trained on RV AE and entirely synthetic data.

11:09.320 --> 11:10.800
You are what you eat.

11:10.800 --> 11:14.320
And again, we cooked it for longer to get past that.

11:14.320 --> 11:22.080
But the implications of this are that I believe within 12 to 18 months, you'll see GPT four

11:22.080 --> 11:24.840
level performance on a smartphone.

11:24.840 --> 11:26.520
How do you contain that?

11:26.520 --> 11:31.080
And how do you contain it when China can do distributed training at scale and release

11:31.080 --> 11:32.800
open source models?

11:32.800 --> 11:41.720
So Google recently did 50,000 TPU training run on their V5 is the new V5 is their TPUs

11:41.720 --> 11:44.840
are very low powered relative to what we've seen.

11:44.840 --> 11:48.240
But again, you can do distributed dynamic training.

11:48.240 --> 11:55.000
Similarly, like we funded five mind and we've seen Google DeepMind just a new paper on localization

11:55.000 --> 11:56.640
through distributed training.

11:56.640 --> 12:00.960
The models are good for fast enough and cheap enough that you can swarm them and you don't

12:00.960 --> 12:03.560
need giant supercomputers anymore.

12:03.560 --> 12:08.480
And that has a lot of implications and how are you going to contain that?

12:08.480 --> 12:13.440
So coming back to the question of do you mandate training training sets?

12:13.440 --> 12:20.440
Do you does, you know, does the government set out what all companies should be utilizing

12:20.440 --> 12:21.440
in mandate?

12:21.440 --> 12:27.600
If you're going to have a aligned AI, it has to be trained on these sets.

12:27.600 --> 12:30.840
How do we how do we possibly govern that?

12:30.840 --> 12:33.680
Look, we have food standards, right?

12:33.680 --> 12:34.680
For ingredients.

12:34.680 --> 12:39.840
Why don't you have data standards for the ingredients that make up a model?

12:39.840 --> 12:43.360
It's just data compute and some algorithms, right?

12:43.360 --> 12:48.080
And so you should say they are the standards and then you can make it compulsory.

12:48.080 --> 12:51.680
That will take a while or you can just have an ISR type standard.

12:51.680 --> 12:55.960
This is good quality model training, good quality data, you know, and people will naturally

12:55.960 --> 12:58.480
gravitate towards that and it becomes a default.

12:58.480 --> 13:01.400
Are you working towards that right now?

13:01.400 --> 13:07.200
Yeah, I mean, look, we spun out a Luther AI as an independent 501c3 so they could look

13:07.200 --> 13:12.960
at data standards and things like that independent of us and the opposite of open AI.

13:12.960 --> 13:15.960
And this is something I've been talking to many people about and we're getting national

13:15.960 --> 13:20.960
data sets and more so that hopefully we can implement good standards similar to how we

13:20.960 --> 13:24.640
offered opt out and how the billion images opted out of our image data set because everyone

13:24.640 --> 13:26.520
was just training on everything.

13:26.520 --> 13:27.520
Is required?

13:27.520 --> 13:28.520
No.

13:28.520 --> 13:29.520
But is it good?

13:29.520 --> 13:30.520
Yes.

13:30.520 --> 13:32.440
And everyone will benefit from better quality data.

13:32.440 --> 13:36.000
So there's no reason that for these very large model training runs, the data sets should

13:36.000 --> 13:37.880
not be transparent and logged.

13:37.880 --> 13:40.680
But again, we want to know what goes into that.

13:40.680 --> 13:44.920
And again, if we have the graduate analogy, what was the curriculum that the graduate

13:44.960 --> 13:45.960
was taught at?

13:45.960 --> 13:47.960
Which university did they go to?

13:47.960 --> 13:49.960
It's something that we'd want to know.

13:49.960 --> 13:53.680
But then why do we talk to GPT-4 where we don't know where it went to university or where

13:53.680 --> 13:54.680
it's been trained on?

13:54.680 --> 13:57.400
It's a bit weird that, isn't it?

13:57.400 --> 14:05.040
What do you think the lesson is going to be from the last four days?

14:05.040 --> 14:06.040
I'm just confused.

14:06.040 --> 14:08.040
I don't know who was against who or what.

14:08.040 --> 14:09.040
I'm going to just post it.

14:09.040 --> 14:12.040
Are we against misalignment or mollock?

14:12.160 --> 14:16.640
I think probably the biggest lesson is it's very hard to align humans, right?

14:16.640 --> 14:18.440
And the stakes are very large.

14:18.440 --> 14:20.440
Why is this so interesting to us?

14:20.440 --> 14:21.920
The stakes are so high.

14:21.920 --> 14:26.520
You tweeted something that was serious and unfortunately funny, which was how can we

14:26.520 --> 14:32.720
align AI with humanity's best interests if we can't align our company's board with its

14:32.720 --> 14:34.720
employee's best interests?

14:34.720 --> 14:35.720
Yeah.

14:35.720 --> 14:39.320
Well, the thing is it's not the employee's best interests.

14:39.320 --> 14:47.680
It's like the board was set up as a lever to ensure the charter of open AI.

14:47.680 --> 14:53.760
So if you look at the original founding document of open AI from 2015, it is a beautiful document

14:53.760 --> 14:55.760
talking about open collaboration, everything.

14:55.760 --> 14:57.920
And then it kind of changed in 2019.

14:57.920 --> 15:02.400
But the charter still emphasizes cooperation, safety, and fundamental.

15:02.400 --> 15:06.000
I posted about this back in March when I said the board and the government structure of

15:06.000 --> 15:09.640
open AI is weird.

15:09.640 --> 15:10.640
What is it for?

15:10.640 --> 15:11.640
What are they trying to do?

15:11.640 --> 15:17.800
Because if you say you're building AGI, in their own road to AGI, they say, this will

15:17.800 --> 15:19.240
end democracy most likely.

15:19.240 --> 15:20.240
I remember reading that.

15:20.240 --> 15:25.840
Because democracy, there's no way democracy survives AGI.

15:25.840 --> 15:29.440
Because either, obviously, it'll be better and you get it to do it or can dissuade everyone

15:29.440 --> 15:31.160
or we all die.

15:31.160 --> 15:33.200
Or it's utopia crap, all right?

15:33.200 --> 15:34.200
Abundance, baby.

15:34.520 --> 15:40.000
There's no way it survives AGI.

15:40.000 --> 15:42.120
There's no way capitalism survives AGI.

15:42.120 --> 15:45.720
The AGI will be the best trader in the world, right?

15:45.720 --> 15:52.640
And it's like, who should be making the decisions on the AGI, assuming that they achieve those

15:52.640 --> 15:53.640
things?

15:53.640 --> 15:54.800
And that's in their own words.

15:54.800 --> 16:01.520
So I think that people are kind of waking up to, oh, there's no real way to do this properly.

16:01.520 --> 16:07.280
And previously, we were scared of open and being transparent, everyone getting this,

16:07.280 --> 16:09.440
which can with the original thing of open air.

16:09.440 --> 16:15.200
And now we're scared of, who are these clowns, you know, and put it in the nicest way.

16:15.200 --> 16:16.880
Because this was ridiculous.

16:16.880 --> 16:20.680
Like you see better politics in a teenage sorority, right?

16:20.680 --> 16:27.840
And it's fundamentally scary, but unelected people, no matter how great they are, and

16:27.840 --> 16:32.880
I think some of the board members are great, should have a say in something that could

16:32.880 --> 16:38.280
literally upend our entire society according to their own words.

16:38.280 --> 16:45.360
I find that inherently anti-democratic and illiberal.

16:45.360 --> 16:51.560
At the end of the day, you know, capitalism has worked, and it's the best system that

16:51.560 --> 16:54.080
we have thus far.

16:54.080 --> 17:03.440
And it's a self, you know, it's built on self-interest and built on continuous optimization, maximization.

17:03.440 --> 17:11.800
I'm still wondering where you go in terms of governing these companies at one level,

17:11.800 --> 17:18.320
internal governance, and then governing the companies at a national and global level.

17:18.320 --> 17:24.440
Has anybody put forward a plan that you think is worth highlighting here?

17:24.440 --> 17:26.240
Not really.

17:26.240 --> 17:30.720
I mean, organizations are a weird artificial intelligence, right?

17:30.720 --> 17:36.480
They have the status of people, and they're slow dumb AI, and they eat up hopes and dreams.

17:36.480 --> 17:40.000
That's what they feed on, I think.

17:40.000 --> 17:41.160
This AI can upgrade them.

17:41.160 --> 17:42.160
It can make them smarter.

17:42.160 --> 17:43.640
They can how do you coordinate?

17:43.640 --> 17:47.000
And from a mechanism design perspective, it's super interesting.

17:47.000 --> 17:52.160
AI can market, so I think we will have AI market makers that can tell stories.

17:52.160 --> 17:54.880
The story of Silicon Valley Bank went around the world in two seconds.

17:54.880 --> 17:56.320
The story of open AI goes around.

17:56.320 --> 17:58.200
AI can tell better stories than humans.

17:58.200 --> 17:59.200
It's inevitable.

17:59.200 --> 18:03.760
And I think that gives hope for coordination, but then also it's dangers of disruption.

18:03.760 --> 18:11.800
I want to double click one second on the two words that you use most, openness and transparency,

18:11.800 --> 18:16.840
and understand fully what those mean one moment, because, you know, and the question is,

18:16.840 --> 18:21.680
not only what they mean, but how fundamental it needs to be.

18:21.680 --> 18:27.800
So openness right now and your definition in terms of AI means what?

18:27.800 --> 18:31.120
It means different things, but different things, unfortunately.

18:31.120 --> 18:33.520
I don't think it means open source.

18:33.520 --> 18:39.680
I think, for me, open means more about access and ownership of the models so that you don't

18:39.680 --> 18:46.000
have a lockstep, like you can hire your own graduates as opposed to relying on consultants.

18:46.000 --> 18:49.720
Security comes down to, I think, for language models in particular, I don't think this holds

18:49.720 --> 18:51.360
some media models.

18:51.360 --> 18:54.400
You really need to know what it's been taught.

18:54.400 --> 18:56.440
That's the only way to safety.

18:56.440 --> 19:00.120
You should not engage with something or use something if you don't know what its credentials

19:00.120 --> 19:04.120
are and how it's been taught, because I think that's inherently dangerous as these get more

19:04.120 --> 19:05.120
and more capabilities.

19:05.120 --> 19:07.520
And again, I don't know if we get to SGI.

19:07.520 --> 19:11.040
If we do, I think it'll probably be like Scarlett Johansson and her, you know, like just to

19:11.040 --> 19:14.240
combine thanks to the GPs, but I'm assuming we don't.

19:14.240 --> 19:15.240
You still need transparency.

19:16.000 --> 19:22.880
Again, how can any government or regulated industry not run on a transparent model?

19:22.880 --> 19:24.680
They can't run on Blackpops.

19:24.680 --> 19:31.160
I get that, and I understand the rationale for it, but now the question is, can you prove

19:31.160 --> 19:32.160
transparency?

19:32.160 --> 19:37.680
I think that, again, a model is only three things, really.

19:37.680 --> 19:41.520
It's the data, the algorithm, and the compute.

19:41.520 --> 19:43.600
And they come and the binary file pops out.

19:43.640 --> 19:49.280
You can tune it with RLHF or DPO or genetic algorithms or whatever, but that's really

19:49.280 --> 19:50.720
the recipe, right?

19:50.720 --> 19:55.520
And so the algorithms, you don't need algorithmic transparency here versus classical AI because

19:55.520 --> 19:57.320
they're very simple.

19:57.320 --> 20:01.400
One of our fellows recreated the Palm 540 billion parameter model.

20:01.400 --> 20:03.200
This is Lucid Raines on GitHub.

20:03.200 --> 20:04.200
You look at that.

20:04.200 --> 20:05.960
If you're a developer and you want to cry, it's GitHub.

20:05.960 --> 20:06.960
It's crazy.

20:06.960 --> 20:10.720
In 206 lines of PyTorch, and that's it.

20:10.720 --> 20:13.480
The algorithms are not very complicated.

20:13.480 --> 20:17.400
Having a gigantic super computer is complicated, and this is why they freaked out when Greg

20:17.400 --> 20:20.920
Brockman kind of stepped down because he's one of the most talented engineers of our

20:20.920 --> 20:21.920
time.

20:21.920 --> 20:25.120
He built these amazing gigantic clusters.

20:25.120 --> 20:28.560
And then the data and how you structure data is complicated.

20:28.560 --> 20:32.040
So I think you can have transparency there because if the data is transparent and who

20:32.040 --> 20:35.320
cares about the supercomputer, who really cares about the algorithm?

20:35.320 --> 20:40.080
Now, let's talk about the next term, alignment here.

20:40.080 --> 20:44.960
Alignments thrown around in lots of different ways.

20:44.960 --> 20:46.960
How do you define alignment?

20:46.960 --> 20:51.600
I define alignment in terms of objective function.

20:51.600 --> 20:59.680
So YouTube was used by the extremists to serve ads for their nastiness.

20:59.680 --> 21:00.760
Why?

21:00.760 --> 21:06.560
Because the algorithm optimized for engagement, which then optimized for extreme content, which

21:06.560 --> 21:08.440
then optimized for the extremists.

21:08.960 --> 21:13.040
Did YouTube mean that, no, but they're just trying to serve ads up, right?

21:13.040 --> 21:16.560
But it meant it wasn't aligned with its users' interests.

21:16.560 --> 21:20.480
And so for me, if you have these technologies that we're going to outsource for more of our

21:20.480 --> 21:25.720
mind, our culture, our children's futures, to you that are very persuasive, we have to

21:25.720 --> 21:31.520
ensure they're aligned with our individual community and societal best interests.

21:31.520 --> 21:37.280
I think this is where the tension with corporations will come in.

21:37.320 --> 21:41.960
Because whoever licenses Scarlett Johansson's voice will sell a lot of ads, you know, they

21:41.960 --> 21:46.520
can be very, very persuasive, but then what are their controls on that?

21:46.520 --> 21:48.280
No one talks about that.

21:48.280 --> 21:54.120
The bigger question of alignment is not killerism, making sure that AI doesn't kill us.

21:54.120 --> 22:00.520
But again, I feel that if we build AI that is transparent, that we can test, that people

22:00.520 --> 22:06.560
can build mitigations around, we are more likely to survive and thrive.

22:06.560 --> 22:11.400
And also, I think there's a final element to here, which is who's alignment.

22:11.400 --> 22:14.400
Different cultures are different, different people are different.

22:14.400 --> 22:18.440
What we found with stable diffusion is that when we merge together the models that different

22:18.440 --> 22:22.640
people around the world have built, the model gets so much better.

22:22.640 --> 22:27.320
I think that makes sense because a monoculture will always be less fragile than a diversity.

22:27.320 --> 22:32.600
Again, I'm not talking about in the DEI kind of way, I'm talking about it in the actual

22:32.600 --> 22:33.600
logical way.

22:33.640 --> 22:39.840
So we have a paper from our reinforcement learning lab called CARPA called QDHF, QD-AIF, Quality

22:39.840 --> 22:43.800
and Diversity through artificial intelligence feedback.

22:43.800 --> 22:48.560
Because you find these models do get better with high quality and diverse inputs, just

22:48.560 --> 22:55.320
like you will get better if you have high quality and diverse experiences, you know?

22:55.320 --> 22:59.680
And saying that's something that's important, that we'll get lost if all these models are

22:59.680 --> 23:00.680
centralized.

23:01.640 --> 23:06.720
You and I have had a lot of conversations about timelines here.

23:06.720 --> 23:13.080
We can get into a conversation of when and if we see AGI.

23:13.080 --> 23:17.320
But we're seeing more and more powerful capabilities coming online right now that are going to

23:17.320 --> 23:23.480
cause a lot of amazing progress and disruption.

23:23.480 --> 23:30.160
How much time do we have, EMI, and we had a conversation when we were together at FII

23:30.160 --> 23:38.840
about the disenfranchised youth coming off of COVID.

23:38.840 --> 23:41.600
So let's talk one second about timelines.

23:41.600 --> 23:54.800
How long do we have to get our shit together, both as AI companies and investors and governors

23:54.800 --> 23:57.800
of society?

23:58.800 --> 24:03.560
The speed here is awesome and frightening.

24:03.560 --> 24:06.480
How long do we have?

24:06.480 --> 24:08.920
Everything everywhere, all at once, right?

24:08.920 --> 24:09.920
We don't have long.

24:09.920 --> 24:13.760
Like, AGI timelines for every definition of AGI, I have no idea.

24:13.760 --> 24:16.720
It will never be less than 12 months, right?

24:16.720 --> 24:21.360
Because it's such a step change, so let's put that to the side.

24:21.360 --> 24:26.280
Right now, everyone that's listening, are you all going to hire the same amount of graduates

24:26.280 --> 24:27.600
that you hired before?

24:27.600 --> 24:29.960
The answer is no.

24:29.960 --> 24:33.920
Some people might not hire any because this is a productivity and an answer and we have

24:33.920 --> 24:38.160
the data for that across any type of knowledge industry.

24:38.160 --> 24:43.640
You just had a great app that you can sketch and it does a whole iPhone app for you, right?

24:43.640 --> 24:48.080
I've gone on record and saying there are no programmers who know in five years, why would

24:48.080 --> 24:49.080
there be?

24:49.080 --> 24:50.600
What are interfaces?

24:50.600 --> 24:57.160
You had a 50% drop, I just put it on my Twitter, in hiring from Indian IITs.

24:57.160 --> 24:58.160
That's crazy.

24:58.160 --> 25:03.480
What you're going to have in a couple of years is around the world at the same time, these

25:03.480 --> 25:10.960
kids that have gone through the trauma of COVID, highly educated STEM, programming, accountancy

25:10.960 --> 25:18.080
law, simultaneously people will hire massively less of them because productivity enhances

25:18.080 --> 25:20.400
and you don't need as many of them.

25:20.400 --> 25:22.680
Why would you need as many paralegals?

25:22.680 --> 25:27.920
That for me is a gigantic societal issue and the only thing I can think of is the stoke

25:27.920 --> 25:33.400
of innovation and the generative jobs of the future through open source technology because

25:33.400 --> 25:38.000
I don't know how else we're going to mitigate that because, Peter, you're a student of history.

25:38.000 --> 25:41.920
What happens when you have large amounts of intelligent, disenfranchised youth history?

25:41.920 --> 25:43.400
We've had that happen a few times.

25:43.400 --> 25:48.760
We just had Arab Spring that long ago, revolt.

25:48.760 --> 25:54.320
People war if not international law.

25:54.320 --> 26:01.680
War is a good way to soak up the excess youth, but it's not pleasant for society and fundamentally

26:01.680 --> 26:07.720
the cost of information gathering, organization has collapsed, like you look at stable video

26:07.720 --> 26:10.080
that we released yesterday, right?

26:10.080 --> 26:13.720
It's going to get so much better so quickly, just like stable diffusion, the cost of creating

26:13.720 --> 26:21.120
movies increases, the demand for quality stuff increases, but there's a few years where demand

26:21.120 --> 26:26.120
and supply don't match and that's such a turbulent thing to navigate.

26:26.120 --> 26:29.880
That's one of the reasons I'm creating stabilities for different countries, so the best and brightest

26:29.880 --> 26:32.360
for me which can help navigate them.

26:32.360 --> 26:33.360
And people don't talk about it.

26:33.360 --> 26:42.080
I loved your idea that the stability models and systems will be owned by the nation.

26:42.440 --> 26:46.600
In fact, the one idea that I heard you say, which I thought was fantastic, was you graduate

26:46.600 --> 26:51.360
college in India, you're an owner in that system.

26:51.360 --> 26:56.280
You graduate from Nigeria, you're an owner in that system, basically to incentivize people

26:56.280 --> 27:01.960
to complete their education and to have them have ownership in what is ultimately the most

27:01.960 --> 27:04.800
important asset that nation has.

27:04.800 --> 27:06.520
And talk about it as infrastructure as well.

27:06.520 --> 27:10.440
I think that's an important analogy that people don't get.

27:10.440 --> 27:14.240
This is the knowledge infrastructure of the future, it's the biggest leap forward we have

27:14.240 --> 27:18.240
because you'll always have a co-pilot that knows everything in a few years and that can

27:18.240 --> 27:23.320
create anything in any type, but it must be embedded to your cultural values and you can't

27:23.320 --> 27:25.160
let anyone else own that.

27:25.160 --> 27:29.000
So it is the infrastructure of the mind and who would outsource their infrastructure to

27:29.000 --> 27:30.000
someone else.

27:30.000 --> 27:34.440
So that's why I think Nigerians should own the models of Nigerians for Nigerians and

27:34.440 --> 27:37.560
should be the next generation that does that.

27:37.560 --> 27:41.040
That's why you give the equity to the graduates, that's why you list it, that's why you make

27:41.040 --> 27:46.440
national champions because, again, that has to be that way.

27:46.440 --> 27:49.800
This is far more important than 5G and this gives you an idea of the scale, we're just

27:49.800 --> 27:51.760
at the start, the earlier doctor base.

27:51.760 --> 27:57.360
A trillion dollars was spent on 5G, this is clearly more important, more than a trillion

27:57.360 --> 28:00.840
dollars that we spend on this and again it flips the world.

28:00.840 --> 28:07.480
And so there is huge threat for our societal balance and again I think open is a potential

28:07.480 --> 28:11.720
antidote to create the jobs of the future and there's huge opportunity on the side

28:11.720 --> 28:16.840
because no one will ever be alone and we can use this to coordinate our systems, give everyone

28:16.840 --> 28:21.680
all the knowledge that they need at their fingertips and help guide everyone if we build

28:21.680 --> 28:26.960
this infrastructure correctly and I don't see the highlight can be closed.

28:26.960 --> 28:36.600
AGI, the conversation and the definition of AGI has basically been all over the place,

28:36.600 --> 28:43.080
because while prediction has been for 30 years that it's 2029, again, that's a blurry

28:43.080 --> 28:51.960
line of what we're trying to target but Elon's talked about anywhere from 2025 to 2028, what

28:51.960 --> 29:00.040
are you thinking, what's your timeline for even digital superintelligence?

29:00.040 --> 29:04.920
I honestly have no idea.

29:04.920 --> 29:10.120
People looking at the scaling laws and applying it but as I've said, data is the key and it's

29:10.120 --> 29:14.480
clear that we already have, you could build a board GPT and it would be better than most

29:14.480 --> 29:16.440
corporate boards, right?

29:16.440 --> 29:20.840
So I think we're already seeing improvements over the existing.

29:20.840 --> 29:23.760
One of the complications here is swarm AI.

29:23.760 --> 29:27.680
So even like it's the whole thing, like a duck-sized human or a hundred human sized

29:27.760 --> 29:34.160
ducks, we're just at the start of swarm intelligence and that reflects and represents how companies

29:34.160 --> 29:35.160
are organized.

29:35.160 --> 29:40.600
Andre Carpethi has some great analogies on this in terms of the new knowledge OS and

29:40.600 --> 29:47.080
that could take off at any time but the function of format of that may not be this whole Western

29:47.080 --> 29:51.880
anti-conformized consciousness that we think of but just incredibly efficient systems that

29:51.880 --> 29:55.800
displace existing human decision-making, right?

29:55.800 --> 30:01.440
And so there's an entire actual range of different AGI outcomes depending on your definition

30:01.440 --> 30:07.440
and I just don't know but I feel again like I wake up and I'm like, oh, look, it's fed

30:07.440 --> 30:12.520
up 10 times the model, you know, like I'm just not, no one can predict this.

30:12.520 --> 30:17.240
But there is a point at which, I mean, we were heading towards an AI singularity, using

30:17.280 --> 30:22.120
a definition of a singularity as a point after which you cannot predict what's coming

30:22.120 --> 30:24.800
next and that isn't far away.

30:24.800 --> 30:30.440
I mean, how far out is it for you a year or two years?

30:30.440 --> 30:35.160
I think you're heading towards it in the next few years but like I said, every company,

30:35.160 --> 30:38.640
organization, individual has an objective function.

30:38.640 --> 30:44.680
My objective function is to allow the next generation to navigate what's coming in the

30:44.680 --> 30:47.400
optimal way and achieve their potential.

30:47.400 --> 30:51.280
So I don't want to build an AGI, I don't want to do any of this, amplified human intelligence

30:51.280 --> 30:57.480
is my preference and trying to mitigate against some of the harms of these agentic things

30:57.480 --> 31:02.880
through data transparency, good standards and making it so people don't need to build

31:02.880 --> 31:09.160
gigantic models on crap, which I think is a major danger if even if not from AGI.

31:09.160 --> 31:13.960
But again, we just don't understand because it's difficult for us to comprehend super

31:14.000 --> 31:18.640
human capabilities, but again, we're already seeing that in narrow fields.

31:18.640 --> 31:21.160
We already know that it's a better rider than us.

31:21.160 --> 31:24.360
So we already know that it can make better pictures than us.

31:24.360 --> 31:30.600
And a better physician and a better educator and a better surgeon and a better everything.

31:30.600 --> 31:38.680
Yeah, and again, I think it's this mythos of these big labs being AGI focused, whereas

31:38.680 --> 31:42.480
you can be better than us in like 5% of the stuff that humans can do and that's still

31:42.520 --> 31:45.800
a massive impact on the world and they can still take over companies and things like

31:45.800 --> 31:46.800
that, right?

31:46.800 --> 31:50.800
Like if you take over a company, then you can impact the world.

31:50.800 --> 31:55.400
And there's clearly with a GPT for 1000 of them orchestrated correctly, that can call

31:55.400 --> 31:59.600
up people and stuff, you wouldn't know it's not CEO, you know, I can make an MA GPT and

31:59.600 --> 32:03.000
then they won't have to make all these tough decisions in any of that.

32:03.000 --> 32:07.680
And most of my decisions aren't that good, so it's probably better.

32:07.680 --> 32:12.520
So I think that we're getting to that point, it's very difficult and the design patterns

32:12.520 --> 32:13.520
are going fast.

32:13.520 --> 32:19.080
We're at the iPhone 2G, 3G stage, it's got copy and paste, and we just got the first

32:19.080 --> 32:23.160
stage as well of this technology, which is the creation step, it creates stuff.

32:23.160 --> 32:29.000
The next step is control and then composition, where you're annoyed because chat GPT doesn't

32:29.000 --> 32:32.240
remember all the stuff that you've written, that won't be the case in a year.

32:32.240 --> 32:36.920
And the final bit is collaboration, where these AIs collaborate together and with humans

32:36.920 --> 32:40.320
to build the information superstructures of the future, and I don't feel that's more

32:40.320 --> 32:41.320
than a few years away.

32:41.320 --> 32:44.280
And it's completely unpredictable what that will create.

32:44.280 --> 32:51.560
Let's talk about responsibility that AI companies have for making sure that their technology

32:51.560 --> 32:57.040
is used in a pro-human and not a disruptive fashion.

32:57.040 --> 33:02.160
Do you think that is a responsibility of a company, of a company's board, of a company's

33:02.160 --> 33:03.160
leadership?

33:03.160 --> 33:05.160
How do you think about that?

33:05.600 --> 33:10.200
Again, with the corporate capitalist system, it typically isn't, because you're maximizing

33:10.200 --> 33:13.800
shareholder value and there aren't laws and regulations, which is why I think there's

33:13.800 --> 33:18.640
a moral, a social, and legal-slash-regulatory aspect to this.

33:18.640 --> 33:22.480
Companies will just look at the legal-slash-regulatory, in some cases they'll just ignore them, right?

33:22.480 --> 33:28.080
But I do think, again, we have a bigger moral and social obligation to this.

33:28.080 --> 33:31.520
This is why I don't subscribe to EA or EAAC or any of these things.

33:31.520 --> 33:34.600
I think it's complicated and it's hard.

33:34.640 --> 33:38.400
Given the uncertainty and how this technology proliferates, and you've got to do your best

33:38.400 --> 33:44.800
and be as straight as possible to people about doing your best, because none of us have qualified

33:44.800 --> 33:47.120
to understand or do this.

33:47.120 --> 33:51.240
And none of us should be trusted to have the power over this technology, right?

33:51.240 --> 33:54.240
You should be questioned and you should be challenged with that.

33:54.240 --> 33:58.560
And again, if you're not transparent, how are you going to challenge?

33:58.560 --> 34:03.320
When I think of the most linear organizations on the planet, I think of governments, maybe

34:03.360 --> 34:07.080
religions, but governments, let's leave it there.

34:07.080 --> 34:09.080
How can...

34:09.080 --> 34:15.480
Let's talk about Western government, at least the U.S., I would have said Europe, but I'll

34:15.480 --> 34:19.880
say the U.K. and Europe.

34:19.880 --> 34:21.200
What should they be...

34:21.200 --> 34:24.600
What steps should they be taking right now?

34:24.600 --> 34:30.520
If you were given the reins to say, how would you regulate, what would you want them to

34:30.560 --> 34:32.560
do or not do?

34:32.560 --> 34:38.000
I believe it's a complicated one, so I signed the first FLI letter.

34:38.000 --> 34:42.840
I think I was the only AI CEO to do that back before it was cool, because I said, I don't

34:42.840 --> 34:45.360
think AGI will kill us all, but I just don't know.

34:45.360 --> 34:49.400
I think it's a conversation that deserves to be had, and it's a good way to have a conversation.

34:49.400 --> 34:53.880
And then we flipped the wrong way, where we went overly AI death risk and other things

34:53.880 --> 34:58.080
like that, and governments were doing that, at the AI Safety Summit in the U.K., and then

34:58.120 --> 35:01.840
we had the King of England come out, and so this is the biggest thing since fire.

35:01.840 --> 35:06.000
I was like, okay, that's a big change in the world, that's right.

35:06.000 --> 35:10.240
The King of England said it, so I must be on the right track.

35:10.240 --> 35:12.640
But I think if you look at it, regulation doesn't move faster.

35:12.640 --> 35:14.600
Even the executive order will take a long time.

35:14.600 --> 35:16.400
The EU things will kind of come in.

35:16.400 --> 35:20.680
Instead, I think that governments have to focus on the tangibles.

35:20.680 --> 35:24.920
AI killerism, again, it can be addressed by considering this as infrastructure and what

35:24.920 --> 35:27.960
infrastructure we need to give our people to survive and thrive.

35:28.000 --> 35:32.000
The U.S. is in a good initial place with the CHIPS Act, but I think you need national

35:32.000 --> 35:36.880
data sets, you need to provide open models to stoke innovation, and think about what

35:36.880 --> 35:40.800
the jobs of the future are, because things are never the same again.

35:40.800 --> 35:44.520
You don't need all those programmers when co-pilot is so good, and you're moving co-pilot

35:44.520 --> 35:50.240
from level above, which is compositional co-pilot, and then collaborative co-pilot, right?

35:50.240 --> 35:53.240
You would be able to talk and computers can talk to computers better than humans can talk

35:53.240 --> 35:54.760
to computers.

35:54.840 --> 35:58.200
We need to articulate the future on that side, but then the other side.

35:58.200 --> 36:04.200
One of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer,

36:04.200 --> 36:05.200
right?

36:05.200 --> 36:07.200
We did a United Order of this.

36:07.200 --> 36:11.280
The loss of agency you feel, and many of you on this call will have had that diagnosis

36:11.280 --> 36:13.000
to the near and dear, is huge.

36:13.000 --> 36:18.400
Then I had 1,000 AI agents fighting every piece of information about pancreatic cancer,

36:18.400 --> 36:20.560
and then after that, I felt a bit more control.

36:20.840 --> 36:24.840
Why don't we have a global cancer model that gives you all the knowledge about cancer and

36:24.840 --> 36:30.160
helps you talk to your kids and connect with people like you, not for diagnosis or research,

36:30.160 --> 36:31.880
but for humans?

36:31.880 --> 36:36.320
This is the Google MedPalM2 model, for example, that outperforms humans in diagnosis, but

36:36.320 --> 36:37.320
also empathy.

36:37.320 --> 36:43.520
What if we arm our graduates to go out and give support to the humans that are being

36:43.520 --> 36:45.400
diagnosed in this way?

36:45.400 --> 36:49.360
That makes society better, and it's valuable, you know?

36:49.360 --> 36:51.920
That's an example of a job of the future, I think.

36:51.920 --> 36:52.920
I don't believe in UBI.

36:52.920 --> 36:57.080
I think we've been universal basic jobs as well, or used jobs.

36:57.080 --> 36:58.080
Universal basic opportunity.

36:58.080 --> 36:59.080
Right?

36:59.080 --> 37:00.080
Yeah.

37:00.080 --> 37:03.880
Universal basic opportunity, universal based jobs, but then post-makers need to think

37:03.880 --> 37:10.080
about it now, because the graduate unemployment wave is literally a few years away, and it

37:10.080 --> 37:11.080
will happen.

37:11.080 --> 37:15.200
Yeah, that is, I mean, when I think about what, I parse the challenges we're going

37:15.200 --> 37:19.720
to be facing in society into a few different elements.

37:19.720 --> 37:25.360
I think what we have today is amazing, and if generative AI froze here, we'd have an

37:25.360 --> 37:30.440
incredible set of tools to help humanity across all of its areas.

37:30.440 --> 37:34.280
And then we've got what's coming in the next zero to five years.

37:34.280 --> 37:40.040
We've talked about patient zero, perhaps being the U.S. elections, and I think you

37:40.040 --> 37:43.120
had said it was Cambridge Analytica that required interference.

37:43.360 --> 37:48.080
Now it's any kid in the garage that could play with the elections.

37:48.080 --> 37:53.960
That's a challenging period of time, and this graduate unemployment wave, as you mentioned,

37:53.960 --> 37:59.280
coming right on its heels.

37:59.280 --> 38:04.480
The question becomes, is the only thing that can create alignment and help us overcome

38:04.480 --> 38:10.760
this AGI at the highest level, meaning it is causing challenges, but ultimately is a

38:10.800 --> 38:13.880
tool that will allow us to be able to solve these challenges as well.

38:15.680 --> 38:17.320
I mean, that's a crazy thought, right?

38:18.320 --> 38:22.440
Like, all this stuff is crazy, like the sheer scale and impact of it.

38:22.440 --> 38:27.440
And, you know, these discussions, we had them last year, Peter, and now everyone's

38:27.440 --> 38:28.320
like, yeah, that makes sense.

38:28.320 --> 38:33.360
And I go, wow, right, it may be AGI, it may be these coordinating automated

38:33.360 --> 38:35.600
story makers and balances from the market, right?

38:36.200 --> 38:39.880
Next year, there's 56 elections with four billion people heading to the polls.

38:41.360 --> 38:42.440
What could possibly go wrong?

38:42.440 --> 38:45.960
Okay, possibly go wrong, you know?

38:46.120 --> 38:46.760
Oh, my God.

38:46.800 --> 38:49.160
But again, the technology isn't going to stop.

38:49.160 --> 38:53.880
Like, even if stability puts down things, if open AI puts down things, it will

38:53.880 --> 38:58.760
continue from around the world because you don't need much to train these models.

38:58.760 --> 39:00.720
Again, the supercomputer thing is a myth.

39:01.200 --> 39:02.800
You've got another year or two where you need them.

39:02.800 --> 39:05.600
You don't need them after that, and that is insane to think about.

39:06.680 --> 39:08.160
You just released stability video.

39:08.160 --> 39:10.520
Congratulations on our stable visual diffusion.

39:10.520 --> 39:11.160
Thank you.

39:12.720 --> 39:14.840
And I'm enjoying some of the clips.

39:15.320 --> 39:23.680
How far are we away from me telling a story to my kids and saying, let's make

39:23.680 --> 39:24.480
that into a movie?

39:27.080 --> 39:27.920
Almost two years away.

39:27.920 --> 39:28.520
Two years away.

39:29.840 --> 39:31.160
So this is a building block.

39:31.160 --> 39:32.400
It's the best creation step.

39:32.440 --> 39:36.120
And then, like I said, you have the control step, composition, and then

39:36.120 --> 39:39.520
collaboration and self-learning systems around that.

39:39.520 --> 39:43.480
So we have Kung Fu UI, which is our node-based system where you have all

39:43.480 --> 39:47.240
the logic that makes up an image, like you can take a dress and a pose and

39:47.240 --> 39:48.520
a face that combines them all.

39:49.080 --> 39:53.080
And it's all encoded in the image because you can move beyond files to

39:53.120 --> 39:55.120
intelligent workflows that you can collaborate with.

39:55.560 --> 39:59.760
If I send you that image file and you put it into your Kung Fu UI, it gives you

39:59.760 --> 40:00.840
all the logic that made that up.

40:01.320 --> 40:02.200
How insane is that?

40:03.480 --> 40:05.200
So we're going to step up there.

40:05.320 --> 40:11.560
And what's happened now is that people are looking at this AI like instant versus

40:11.720 --> 40:15.160
again, the huge amount of effort it took to take this information and

40:15.160 --> 40:16.200
structure it before.

40:17.040 --> 40:19.600
But the value is actually in stuff that takes a bit longer.

40:20.160 --> 40:24.160
Like when you're shooting a movie, you don't just say, do it all in one shot,

40:24.160 --> 40:24.520
right?

40:24.680 --> 40:29.560
Unless you are a very talented director and actor, you know, you have

40:29.560 --> 40:32.640
mise en place, you have staging, you have blocking, you have cinematography.

40:33.160 --> 40:35.880
It takes a while to composite the scenes together.

40:36.480 --> 40:40.280
It'll be the same for this, but a large part of it will then be automated for

40:40.280 --> 40:43.440
creating the story that can resonate with you and you can turn it into

40:43.440 --> 40:44.400
Korean or whatever.

40:44.920 --> 40:47.960
And there'll still be big blog clusters like Oppenheimer and Barbie.

40:48.440 --> 40:51.480
But again, the flaw will be raised overall.

40:52.200 --> 40:55.960
Similarly, like we had a music video competition check it on YouTube with Peter

40:55.960 --> 41:00.200
Gabriel, he allows us to use kindly his songs and people from all around the

41:00.200 --> 41:03.440
world made amazing music videos to his thing, but they took weeks.

41:04.360 --> 41:06.800
So I think that's somewhere in the middle here where again, we're just at

41:06.800 --> 41:12.440
that early stage, because chat GPT isn't even a year old, you know, stable

41:12.440 --> 41:13.760
diffusion is only 14, 15 months.

41:13.760 --> 41:19.520
And I think you'd agree that neither of them is the end hole and be all.

41:19.520 --> 41:21.880
It's just, it's the earliest days of this field.

41:23.000 --> 41:23.760
I had the conversation.

41:23.840 --> 41:24.920
The tiniest building.

41:24.920 --> 41:28.400
Yeah, I had this conversation with Ray Kurzweil two weeks ago.

41:29.360 --> 41:33.200
We're just after a singularity board meeting we had, and we're just on a

41:33.200 --> 41:34.160
zoom and chatted.

41:34.560 --> 41:40.080
And, you know, the realization is that, unfortunately, the human mind is

41:40.320 --> 41:42.080
awful at exponential projections.

41:42.080 --> 41:47.080
And despite the convergence of all these technologies, we tend to project

41:47.080 --> 41:52.120
the future as a linear extrapolation of, you know, the world we're living in right

41:52.120 --> 41:52.480
now.

41:53.480 --> 41:58.040
But the best I can say is that we're going to see in the next decade, right,

41:58.040 --> 42:01.920
between now and 2033, we're going to see a century worth of progress.

42:02.520 --> 42:05.560
But it's going to get very weird, very fast, isn't it?

42:07.440 --> 42:10.200
I mean, there's two way doors and there's one way doors, right?

42:10.760 --> 42:15.040
Like in December of last year, multiple headmasters called me and said,

42:15.640 --> 42:17.560
we can't set essays for our homework anymore.

42:17.920 --> 42:20.200
And every headmaster in the world had to do that same thing.

42:20.200 --> 42:20.960
It's a one-way door.

42:21.320 --> 42:21.840
Yes.

42:22.680 --> 42:25.240
And this is the scary part, the one-way doors, right?

42:26.080 --> 42:31.120
Like when you have an AI that can do your taxes, what does that mean for

42:31.120 --> 42:31.720
accountants?

42:33.440 --> 42:34.640
All the accountants at the same time.

42:36.560 --> 42:37.360
Kind of crazy, right?

42:37.440 --> 42:38.280
It is.

42:39.080 --> 42:43.160
And the challenge, I mean, one of my biggest concerns, so listen, I'm the

42:43.200 --> 42:44.080
eternal optimist.

42:44.080 --> 42:47.320
I'm not the guy who's the glasses half full, it's the glasses overflowing.

42:48.280 --> 42:57.000
And one of the challenges I think through when I think about where AI, AGI, ASI,

42:57.040 --> 43:03.720
however you want to project it to is the innate importance of human purpose.

43:04.600 --> 43:09.600
And unfortunately, most of us derive our purpose from the work that we do.

43:10.240 --> 43:14.800
You know, I ask you, you know, tell me about yourself and you jump into your

43:14.840 --> 43:15.920
work and what you do.

43:16.400 --> 43:21.320
And so when AI systems are able to do most everything we do, not just a little

43:21.320 --> 43:27.680
bit better, but, you know, orders of magnitude better, redefining purpose and

43:27.680 --> 43:36.320
redefining my role in achieving a moonshot or a transformation is, it's the,

43:36.680 --> 43:45.880
you know, it's the impedance mismatch between human societal growth rates

43:46.040 --> 43:47.600
and tech growth rates.

43:48.160 --> 43:49.000
What are your thoughts there?

43:50.640 --> 43:52.640
Yeah, I mean, I think again, exponentials are hard.

43:52.760 --> 43:58.840
Like if I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well,

43:58.840 --> 43:59.520
that's not possible.

43:59.560 --> 44:00.160
Why?

44:00.680 --> 44:04.840
You know, like GPT-4 is impossible, stable diffusion is impossible, right?

44:05.520 --> 44:09.200
Like now they've almost become commonplace, but why would you need super

44:09.200 --> 44:10.080
computers and these things?

44:10.720 --> 44:15.880
I do agree this mismatch and that's why we're in for five years of chaos.

44:16.040 --> 44:20.200
That's why I called it stability because I saw this coming a few years ago and

44:20.200 --> 44:23.400
I was like, holy crap, we have to build this company.

44:24.080 --> 44:28.600
And now we have the most downloads of any models of any company, like 50 million

44:28.600 --> 44:31.480
last month versus 700,000 from Astral, for example.

44:33.000 --> 44:36.640
And we will have the best model of every type except for very large language

44:36.640 --> 44:37.920
models by the end of the year.

44:38.520 --> 44:43.160
So we have audio, 3D, video, code, everything and a lovely, amazing

44:43.160 --> 44:48.520
community because it's just so hard again for us to imagine this mismatch.

44:48.560 --> 44:49.720
There's a period of chaos.

44:50.040 --> 44:54.000
But then on the other side, like there's this PDoom question, right?

44:54.000 --> 44:55.280
The probability of doom.

44:56.640 --> 45:00.040
I can say something with this technology, the probability of doom is lower

45:00.040 --> 45:03.280
than without this technology because we're killing ourselves.

45:04.360 --> 45:07.720
And this can be used to enhance every human and coordinate us all.

45:08.600 --> 45:11.360
And I think what we're aiming for is that Star Trek future versus that Star

45:11.360 --> 45:11.600
Wars.

45:11.680 --> 45:13.320
Yes, I meant to that.

45:15.200 --> 45:20.240
And I think that's an important point, the level of complexity that we have

45:20.240 --> 45:24.240
in society, we don't need AI to destroy the planet.

45:24.840 --> 45:27.280
We're doing that very, very well.

45:27.280 --> 45:27.680
Thank you.

45:28.680 --> 45:30.760
But the ability to coordinate.

45:30.920 --> 45:35.680
So one of the things I think about is a world in which everyone has access to

45:35.680 --> 45:39.200
all the food, water, energy, healthcare, education that they want.

45:39.840 --> 45:47.800
Really, a world of true abundance in my mind is a piece more peaceful world, right?

45:47.840 --> 45:51.160
Why would you want to destroy things if you have access to everything that you

45:51.160 --> 45:51.480
need?

45:52.240 --> 45:58.160
And that kind of a world of abundance is on the backside of this kind of

45:58.160 --> 45:59.080
awesome technology.

46:00.440 --> 46:02.040
We have to navigate the next period.

46:02.080 --> 46:05.040
I believe we'll see it within our lifetimes, particularly if we get

46:05.040 --> 46:06.160
longevity songs, right?

46:07.760 --> 46:09.240
And that's so amazing, right?

46:09.480 --> 46:11.640
But then we think about, as you said, why peace?

46:12.600 --> 46:15.200
A child in Israel is the same as a child in Gaza.

46:15.720 --> 46:17.120
And then something happens.

46:17.120 --> 46:21.720
A liar is told that you are not like others and the other person is not human

46:21.720 --> 46:22.160
like you.

46:22.640 --> 46:24.280
All wars are based on that same line.

46:25.760 --> 46:29.720
And so again, if we have AI that is aligned with the potential of each human

46:29.720 --> 46:34.520
that can help mitigate those lies, then we can get away from war because

46:35.840 --> 46:37.160
the world is not scarce.

46:38.160 --> 46:39.360
There is enough food for everyone.

46:39.400 --> 46:40.840
It's a coordination failure.

46:42.360 --> 46:44.040
And that can be addressed by this technology.

46:44.040 --> 46:44.160
I agree.

46:44.160 --> 46:49.200
One of the most interesting and basic functions or capabilities of generative

46:49.240 --> 46:54.960
AI has been the ability to translate my ideas into concepts that someone who is

46:54.960 --> 46:56.960
a different frame of thought can understand.

46:57.760 --> 46:58.040
Right?

47:00.000 --> 47:01.840
But that's what this generative AI is.

47:01.880 --> 47:03.880
It's a universal translator.

47:04.040 --> 47:04.440
Sure.

47:04.760 --> 47:06.320
It does not have facts.

47:06.320 --> 47:08.360
The fact that it knows anything is insane.

47:08.400 --> 47:10.600
Hallucinations is a crazy thing to say.

47:10.680 --> 47:12.440
Again, it's just like a graduate trying so hard.

47:13.560 --> 47:17.760
GPT-4 with 10 trillion words and 100 gigabytes is insane.

47:18.440 --> 47:23.200
Stable diffusion has like 100,000 gigabytes and a two gigabyte file.

47:23.200 --> 47:26.760
50,000 to one compression is something else.

47:26.800 --> 47:28.000
It's learned principles.

47:28.160 --> 47:28.640
Yes.

47:28.880 --> 47:31.280
And this is it's knowledge, knowledge versus data.

47:31.440 --> 47:31.720
Yeah.

47:32.080 --> 47:33.640
It's knowledge versus data.

47:33.640 --> 47:35.640
And if you have some experience, you get the wisdom, right?

47:35.640 --> 47:36.000
Yes.

47:36.000 --> 47:40.760
Because it's learned the principles and contexts and it can map them to

47:40.760 --> 47:43.880
transform data because that's how you navigate.

47:44.520 --> 47:49.120
You don't navigate based on like logical flow.

47:49.120 --> 47:52.000
We have those two parts of our brain navigate sometimes based on instinct

47:52.000 --> 47:53.560
based on the principles you've learned.

47:53.920 --> 47:58.640
So Tesla's new auto driving model, self driving model is entirely based on

47:58.640 --> 48:02.000
a console, which are protection, like they said it publicly is based on this

48:02.000 --> 48:02.520
technology.

48:02.520 --> 48:03.840
It doesn't have any rules.

48:04.440 --> 48:07.680
It's just learned the principles of how to drive from massive amounts of Tesla

48:07.680 --> 48:11.120
data that now fits on the hardware without internet.

48:11.880 --> 48:15.560
And so they went from self driving being impossible to now, hey, it looks pretty

48:15.560 --> 48:18.920
well, you know, because it's learned the principles.

48:19.280 --> 48:22.720
And so that's why this technology can help solve the problem.

48:22.720 --> 48:26.720
This is why it can help us amplify our intelligence and innovation.

48:27.720 --> 48:31.160
Because the missing part, the second part of the frame, you know, next, I can't

48:31.160 --> 48:35.600
give more details yet, but next week we're announcing the largest X prize ever.

48:35.600 --> 48:37.200
It's a hundred and one million dollars.

48:38.200 --> 48:39.360
It's a hundred and one.

48:39.360 --> 48:45.360
So it's Elon had the had a hundred million dollar prize that kind of defund a few

48:45.360 --> 48:50.360
years ago for carbon sequestration and the funder, the first funder of this prize

48:50.360 --> 48:51.920
wanted to be larger than Elon's.

48:51.920 --> 48:53.800
I said, okay, you had the extra million.

48:53.880 --> 48:54.800
It's for luck.

48:54.800 --> 48:55.400
It's for luck.

48:55.400 --> 48:56.880
We did our seed round 101 million.

48:56.880 --> 48:57.400
Oh, really?

48:57.400 --> 48:57.880
Okay.

48:57.880 --> 48:58.880
Hi, that's great.

48:59.880 --> 49:00.880
It's on your popular number.

49:01.880 --> 49:06.280
Anyway, the and it's in the field of health.

49:06.280 --> 49:12.040
I'll leave it at that folks go to XPRIZE.org to register to see the live event on

49:12.040 --> 49:12.880
November 29th.

49:12.880 --> 49:16.240
We're going to be debuting the prize, what it is, what it's going to impact

49:16.240 --> 49:21.000
eight billion people, long story short, it's, it's a nonlinear,

49:21.080 --> 49:28.440
future because we are able to utilize AI and make things that were seemingly

49:28.440 --> 49:32.480
crazy before, likely to become inevitable.

49:33.480 --> 49:35.880
And that's an amazing future we have to live into.

49:38.080 --> 49:43.240
Yeah, I mean, again, because it's one way doors, the moment we create a cancer

49:43.240 --> 49:46.680
GPT, and this is something that we're building, we have trillions of tokens

49:46.680 --> 49:48.080
and then you Google TV.

49:48.160 --> 49:52.960
Things like that, that organizes global cancer knowledge and makes it accessible

49:52.960 --> 49:57.160
and useful, even if it's just for guiding people that have been diagnosed.

49:57.160 --> 49:58.560
The world changes.

49:58.560 --> 50:02.560
The 50% of people that have a cancer diagnosed in their lives in every language

50:02.560 --> 50:06.360
and every level will have someone to talk to and connect them with the resources

50:06.360 --> 50:09.760
they need and other people like them and talk to their families.

50:09.760 --> 50:11.760
You know, and how insane is that?

50:12.760 --> 50:16.560
And so again, the least positive thing is that we're going to be able to

50:16.560 --> 50:20.040
and so again, the least positive stories in the future need to be told, right?

50:20.040 --> 50:25.120
Because that will align us to where we need to go as opposed to a future full

50:25.120 --> 50:26.920
of uncertainty and craziness and doom.

50:28.240 --> 50:33.440
In our last couple minutes here, buddy, what can we look forward to from stability

50:34.520 --> 50:37.120
in the months and years ahead?

50:38.080 --> 50:41.040
We have every model of every type and we'll build it for every nation

50:41.040 --> 50:42.920
and we'll give back control to every nation.

50:42.920 --> 50:45.520
So coming back to governance here.

50:47.160 --> 50:51.040
Again, is the nation state the unit of control?

50:52.400 --> 50:56.880
Is it? No, I my my thinking is disabilities in every nation

50:56.880 --> 51:00.360
should have the best and brightest of each because what you've seen is

51:00.360 --> 51:04.360
there are amazing people in this sphere, the best and brightest in the world.

51:04.360 --> 51:07.880
Now, this is the biggest thing ever and they all want to work in it.

51:07.960 --> 51:10.720
And it's just finding the right people with the right intention.

51:11.240 --> 51:14.120
The brightest people go back to Singapore or Malaysia or others

51:14.600 --> 51:16.440
because of the future of their nations.

51:16.440 --> 51:19.440
And again, now we're doing a big change and we don't talk

51:19.440 --> 51:21.160
about all the cool stuff we do.

51:21.160 --> 51:23.240
We've just taken it because you need to articulate

51:23.240 --> 51:26.280
that positive vision of the future because the only scarce resource

51:26.280 --> 51:27.800
is actually this is human capital.

51:27.800 --> 51:29.400
It's not GPUs.

51:29.400 --> 51:30.720
It's not data.

51:30.720 --> 51:35.280
It's about the humans that can see this technology and realize that

51:35.280 --> 51:38.320
they can play a part in guiding it for the good of everyone,

51:39.080 --> 51:40.800
their own societies and more.

51:40.800 --> 51:43.520
And that's again, what I hope stability can be.

51:43.520 --> 51:45.600
Well, I wish you the best of luck, pal.

51:45.600 --> 51:48.160
Thank you for joining me in this conversation.

51:48.160 --> 51:50.920
It's it's been a crazy four or five days.

51:52.200 --> 51:55.880
And wish Sam and Greg and the entire

51:56.880 --> 52:00.520
opening I team stability in their lives.

52:00.960 --> 52:03.520
Yeah, I have a nice Thanksgiving.

52:03.520 --> 52:07.560
They're absolutely they're an amazing team building world changing technology.

52:07.960 --> 52:09.440
It's such a concentration of talent.

52:09.480 --> 52:13.760
I think, again, I really felt for them over the last few days,

52:13.840 --> 52:15.880
you know, much as I kind of post memes and everything.

52:15.880 --> 52:17.840
I posted that as well.

52:17.840 --> 52:21.280
I think this will bring them closer together and hopefully they can solve

52:21.280 --> 52:24.280
the number one problem that I've asked them to solve, which is email.

52:25.000 --> 52:27.080
Solve email, right?

52:27.160 --> 52:29.000
And then we'll crack on from there.

52:29.000 --> 52:30.000
All right, cheers, my friend.

