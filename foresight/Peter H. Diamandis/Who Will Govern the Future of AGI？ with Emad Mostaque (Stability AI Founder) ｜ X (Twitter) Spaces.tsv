start	end	text
0	11760	Hey, Amade, good to hear you.
11760	14120	It was always a pleasure.
14120	15120	Yeah.
15120	16120	So, where are you today?
16120	17120	I'm in London.
17120	18120	Good.
18120	22960	Other side of the planet, I'm in Santa Monica.
22960	29640	It's been quite the extraordinary game of ping pong out there these last four or five
29640	30640	days.
30640	36160	I didn't think the first thing that AI would disrupt would be reality TV, right?
36160	37160	Yeah.
37160	46880	It's been fascinating how X has become sort of the go-to place to find out the latest
46880	51600	of where Sam is working and what's going on with the AI industry.
51600	55160	Yeah, you found the notifications in the wake, guys.
55160	59320	I mean, it's the thing, like, what else will move at the speed of this?
59320	63320	Like, I was saying to someone recently, AI research doesn't really move at the speed
63320	65720	of conferences or even PDFs anymore, right?
65720	67920	You just wake up and you're like, oh, it's 10 times faster.
67920	70880	So, I think that's why X is quite good.
70880	76840	I actually, like, unfollow just about everyone and just the AI algorithms find the most interesting
76840	77840	things for me.
77840	81400	So, I've got, like, 10 people that I follow, and it's actually working really well.
81400	82400	It's getting better.
82400	85120	Well, it has been.
85120	86960	I've been enjoying the conversation.
86960	93360	It really feels like you're inside an intimate conversation among friends as this is going
93360	94360	back and forth.
94360	102760	I think this entire four or five days has been an extraordinary, up-close, intimate conversation
102760	108440	around governance and around, you know, what's the future of AI?
108800	116200	Honestly, you know, as it gets faster and more powerful, the cost of missteps is going
116200	117960	to increase exponentially.
119960	121960	Let's begin here.
121960	128040	I mean, you've been making the argument about open source as one of the most critical elements
128040	129600	of governance for a while now.
129600	131400	Can you just, let's hop into that.
133080	137080	Yeah, I think that, you know, open source is the difficult one because it means a few
137080	138080	different things.
138120	140280	Like, is it models you can download and use?
140280	143720	Do you make all the data available and free?
143720	148240	And then when you actually look at what all these big companies do, all their stuff is
148240	153560	built on open source basis, you know, it's built on the transformer paper.
153560	161200	It's built on, like, the new model by Khayfouli and Vira1.ai is basically Lama.
161200	165400	Like it's actually got the same variable names and other things like that, plus a gigantic
165400	166520	supercomputer, right?
166520	171800	The whole conversation has been, you know, how important is openness and transparency?
171800	176440	And what are the governance models that are going to allow the most powerful technology
176440	183040	on the planet to enable the most benefit for humanity and the safety?
183040	190000	So, I mean, you've been thinking about this and speaking to transparency, openness, governance
190000	192000	for a while.
192000	196400	Could you, I mean, what do you think is going to be, what do you think is we need to be
196400	197400	focused on?
197400	199600	Where do we need to evolve to?
199600	203080	Yeah, it's a complicated topic.
203080	208840	I think that most of the infrastructure of the Internet is open source, Linux, everything
208840	209960	like that.
209960	216600	I think these models, it's unlikely that our governments will be run on GPT-7 or Baal
216600	217600	or anything like that.
217600	221600	How are you going to have black boxes that run these things?
221600	226000	I think a lot of the governance debate has been hijacked by the AI safety debate where
226000	229800	people are talking about AGI killing us all, and then there's this precautionary principle
229800	230800	that kicks in.
230800	233880	It's too dangerous to let out because what if China gets it?
233880	236080	What if someone builds an AGI that kills us all?
236080	241600	It'd be great to have this amazing board that could pull the off switch, you know, whereas
241600	248320	in reality, I think that you're seeing a real social impact from this technology, and it's
248320	253800	about who advances forward and who's left behind if we're thinking about risk.
253800	259120	Because governance is always about finding, as you said, the best outcomes and also mitigating
259120	261040	against the harms, right?
261040	264800	And there's some very real, amazingly positive outcomes that are now emerging that people
264800	270920	can agree on, but also some very real social impacts that we have to mitigate against.
270920	279280	So I mean, let's begin, how is stability governed?
279280	283920	Stability is basically governed by me.
283920	287640	So I looked in foundations and DAOs and everything like that, and I thought to take it to where
287640	288640	we are now.
288640	292400	I needed to have very singular governance, but now we're looking at other alternatives.
292400	293720	And what do you think it's going to be?
293720	295000	Where would you head in the future?
295000	299600	I mean, let's actually jump away from this in particular.
299600	304040	But do you recommend the most powerful technologies on the planet?
304040	305760	How should they be governed?
305760	307160	How should they be owned?
307160	312840	You know, where should we be in five years?
312840	317400	I think there need to be public goods that are collectively owned and then individually
317400	318720	owned as well.
318720	324840	So for example, there was the tweet kind of storm, the kind of I am Spartacus or his
324840	331520	name is Robert Boulson from the OpenAI team saying, OpenAI is nothing without its people.
331520	337560	Stability, we have amazing people, 190 and 65 top researchers.
337560	342600	Without its people, we're open models used by hundreds of millions, it continues.
342600	346040	And if you think about where you need to go, you can never have a choke point on this technology,
346040	349640	I think it becomes part of your life.
349640	353440	Like the phrase I have is not your models, not your mind.
353440	359480	So these models, again, are just such interesting things to take billions of images or trillions
359480	365120	of words and you get this file out that can do magic, right, trade or magic sand.
365120	370840	I think that you will have pilots that gather our global knowledge on various modalities
370840	375160	and you'll have co-pilots that you individually own that guide you through life.
375160	379440	And I can't see how that can be controlled by any one organization.
379440	386640	You've been on record talking about having models owned by the citizens of nations.
386640	388520	Can you speak to that a little bit?
388520	389520	Sure.
389520	394680	So we just released some of the top Japanese models from visual language to language to
394680	398640	Japanese SDXL as an example.
398640	401560	So we're training for half a dozen different nations and models now.
401560	406840	And the plan is to figure out a way to give ownership of these datasets and models back
406840	408280	to the people of that nation.
408760	413680	So you get the smartest people in Mexico to run a stability Mexico or maybe a different
413680	418840	structure that then makes decisions for Mexicans with the Mexicans about the data and what
418840	420520	goes in it.
420520	424480	Because everyone's been focusing on the outputs, the inputs actually are the things that matter
424480	426600	the most.
426600	430440	The best way I've thought about thinking of these models is like very enthusiastic graduates.
430440	433400	So hallucinations isn't just probably too hard.
433400	437080	A lot of the things about like, oh, what about these bad things the models can output?
437080	439320	It's about what you've input.
439320	444240	And so what you put into that Mexican dataset or the Chinese or Vietnamese one will impact
444240	445240	the outputs.
445240	451400	And there's a great paper in Nature, Human Behavior today about that, about how foundational
451400	453200	models are cultural technologies.
453200	460600	So again, how can you outsource your culture and your brains to other countries, to people
460600	463720	that are from a very different place?
463720	465720	I think it eventually has to be localized.
466080	470240	I think one of the points you said originally is we have to separate the issue of governance
470240	475160	versus safety and alignment.
475160	480920	Are they actually different?
480920	486800	So I think that a lot of the safety discussion or this AGI risk discussion is because the
486800	491640	future is so uncertain because it is so powerful.
491640	495240	And we didn't have a good view of where we're going.
495240	499040	So when you go on a journey and you don't know where you're going, you will minimize
499040	503040	from acts and regret you'll have the precautionary principle.
503040	507280	And then that means you basically go towards authority, you go towards trying to control
507280	511080	this technology when it's so difficult to control.
511080	515000	And you end up not doing much, you know, because there's anything to go wrong.
515000	518080	When you have an idea of where we're going, like you should have all the cancer knowledge
518080	522080	in the world at your fingertips or climate knowledge, or anybody should be able to create
522080	524480	whole worlds and share them.
524480	529600	When you align your safety discussions against the goal, against the location that you're
529600	533920	going to, again, just like setting out on a journey, I think that's a big change.
533920	538680	Similarly, most of the safety discussions have been on outputs, not inputs.
538680	542480	If you have a high quality data set without knowledge about anthrax, your language model
542480	549240	is unlikely to tell you how to build anthrax, you know, and transparency around that will
549240	550240	be very useful.
550240	557600	So let's dive into that safety alignment issue for a moment because it's an area you and
557600	559120	I have been talking a lot about.
559120	565000	So Mustafa wrote a book, Mustafa Suleiman wrote a book called The Coming Wave in which
565000	570480	he talks about containment as the mechanism by which we're going to be making sure we
570480	573200	have safe AI.
573200	578320	You and I have had the conversation of it's really how you educate and raise and train
578320	584920	your AI systems in making sure that there's full transparency and openness on the data
584920	585920	sets that are utilized.
585920	589000	Do you think containment is an option for safety?
589000	592400	No, not at all.
592400	598800	Like a number of leaders say, what if China gets open source AI?
598800	602600	The reality is that China, Russia, everyone already has the weights for GPT-PORC.
602600	606080	They just downloaded it on a USM stick.
606080	609360	You know that there's been compromised, right?
609360	610360	There's no way they couldn't.
610360	612480	The rewards are too great.
612480	619320	And there is a absolutely false dichotomy here and a lot of the companies want you to believe
619320	624280	that giant models are the main thing and you need to have these gigantic, ridiculous
624280	626080	supercomputers that only they can run.
626080	631680	I mean, look, we run gigantic supercomputers, but the reality is this.
631680	638480	The supercomputers and the giant trillion zillion data sets are just a shortcut for bad quality
638480	640600	data.
640600	645160	It's like using a hot pot or sous viding a steak that's bad quality.
645160	648880	You cook it for longer and it organizes the information.
648880	654080	With stable diffusion, we did a study and we showed that basically 92% of the data isn't
654080	660560	used 99% of the time, you know, because now you're seeing this with, for example, Microsoft's
660640	664520	by release, it's trained entirely on synthetic data.
664520	669320	Dali three is trained on RV AE and entirely synthetic data.
669320	670800	You are what you eat.
670800	674320	And again, we cooked it for longer to get past that.
674320	682080	But the implications of this are that I believe within 12 to 18 months, you'll see GPT four
682080	684840	level performance on a smartphone.
684840	686520	How do you contain that?
686520	691080	And how do you contain it when China can do distributed training at scale and release
691080	692800	open source models?
692800	701720	So Google recently did 50,000 TPU training run on their V5 is the new V5 is their TPUs
701720	704840	are very low powered relative to what we've seen.
704840	708240	But again, you can do distributed dynamic training.
708240	715000	Similarly, like we funded five mind and we've seen Google DeepMind just a new paper on localization
715000	716640	through distributed training.
716640	720960	The models are good for fast enough and cheap enough that you can swarm them and you don't
720960	723560	need giant supercomputers anymore.
723560	728480	And that has a lot of implications and how are you going to contain that?
728480	733440	So coming back to the question of do you mandate training training sets?
733440	740440	Do you does, you know, does the government set out what all companies should be utilizing
740440	741440	in mandate?
741440	747600	If you're going to have a aligned AI, it has to be trained on these sets.
747600	750840	How do we how do we possibly govern that?
750840	753680	Look, we have food standards, right?
753680	754680	For ingredients.
754680	759840	Why don't you have data standards for the ingredients that make up a model?
759840	763360	It's just data compute and some algorithms, right?
763360	768080	And so you should say they are the standards and then you can make it compulsory.
768080	771680	That will take a while or you can just have an ISR type standard.
771680	775960	This is good quality model training, good quality data, you know, and people will naturally
775960	778480	gravitate towards that and it becomes a default.
778480	781400	Are you working towards that right now?
781400	787200	Yeah, I mean, look, we spun out a Luther AI as an independent 501c3 so they could look
787200	792960	at data standards and things like that independent of us and the opposite of open AI.
792960	795960	And this is something I've been talking to many people about and we're getting national
795960	800960	data sets and more so that hopefully we can implement good standards similar to how we
800960	804640	offered opt out and how the billion images opted out of our image data set because everyone
804640	806520	was just training on everything.
806520	807520	Is required?
807520	808520	No.
808520	809520	But is it good?
809520	810520	Yes.
810520	812440	And everyone will benefit from better quality data.
812440	816000	So there's no reason that for these very large model training runs, the data sets should
816000	817880	not be transparent and logged.
817880	820680	But again, we want to know what goes into that.
820680	824920	And again, if we have the graduate analogy, what was the curriculum that the graduate
824960	825960	was taught at?
825960	827960	Which university did they go to?
827960	829960	It's something that we'd want to know.
829960	833680	But then why do we talk to GPT-4 where we don't know where it went to university or where
833680	834680	it's been trained on?
834680	837400	It's a bit weird that, isn't it?
837400	845040	What do you think the lesson is going to be from the last four days?
845040	846040	I'm just confused.
846040	848040	I don't know who was against who or what.
848040	849040	I'm going to just post it.
849040	852040	Are we against misalignment or mollock?
852160	856640	I think probably the biggest lesson is it's very hard to align humans, right?
856640	858440	And the stakes are very large.
858440	860440	Why is this so interesting to us?
860440	861920	The stakes are so high.
861920	866520	You tweeted something that was serious and unfortunately funny, which was how can we
866520	872720	align AI with humanity's best interests if we can't align our company's board with its
872720	874720	employee's best interests?
874720	875720	Yeah.
875720	879320	Well, the thing is it's not the employee's best interests.
879320	887680	It's like the board was set up as a lever to ensure the charter of open AI.
887680	893760	So if you look at the original founding document of open AI from 2015, it is a beautiful document
893760	895760	talking about open collaboration, everything.
895760	897920	And then it kind of changed in 2019.
897920	902400	But the charter still emphasizes cooperation, safety, and fundamental.
902400	906000	I posted about this back in March when I said the board and the government structure of
906000	909640	open AI is weird.
909640	910640	What is it for?
910640	911640	What are they trying to do?
911640	917800	Because if you say you're building AGI, in their own road to AGI, they say, this will
917800	919240	end democracy most likely.
919240	920240	I remember reading that.
920240	925840	Because democracy, there's no way democracy survives AGI.
925840	929440	Because either, obviously, it'll be better and you get it to do it or can dissuade everyone
929440	931160	or we all die.
931160	933200	Or it's utopia crap, all right?
933200	934200	Abundance, baby.
934520	940000	There's no way it survives AGI.
940000	942120	There's no way capitalism survives AGI.
942120	945720	The AGI will be the best trader in the world, right?
945720	952640	And it's like, who should be making the decisions on the AGI, assuming that they achieve those
952640	953640	things?
953640	954800	And that's in their own words.
954800	961520	So I think that people are kind of waking up to, oh, there's no real way to do this properly.
961520	967280	And previously, we were scared of open and being transparent, everyone getting this,
967280	969440	which can with the original thing of open air.
969440	975200	And now we're scared of, who are these clowns, you know, and put it in the nicest way.
975200	976880	Because this was ridiculous.
976880	980680	Like you see better politics in a teenage sorority, right?
980680	987840	And it's fundamentally scary, but unelected people, no matter how great they are, and
987840	992880	I think some of the board members are great, should have a say in something that could
992880	998280	literally upend our entire society according to their own words.
998280	1005360	I find that inherently anti-democratic and illiberal.
1005360	1011560	At the end of the day, you know, capitalism has worked, and it's the best system that
1011560	1014080	we have thus far.
1014080	1023440	And it's a self, you know, it's built on self-interest and built on continuous optimization, maximization.
1023440	1031800	I'm still wondering where you go in terms of governing these companies at one level,
1031800	1038320	internal governance, and then governing the companies at a national and global level.
1038320	1044440	Has anybody put forward a plan that you think is worth highlighting here?
1044440	1046240	Not really.
1046240	1050720	I mean, organizations are a weird artificial intelligence, right?
1050720	1056480	They have the status of people, and they're slow dumb AI, and they eat up hopes and dreams.
1056480	1060000	That's what they feed on, I think.
1060000	1061160	This AI can upgrade them.
1061160	1062160	It can make them smarter.
1062160	1063640	They can how do you coordinate?
1063640	1067000	And from a mechanism design perspective, it's super interesting.
1067000	1072160	AI can market, so I think we will have AI market makers that can tell stories.
1072160	1074880	The story of Silicon Valley Bank went around the world in two seconds.
1074880	1076320	The story of open AI goes around.
1076320	1078200	AI can tell better stories than humans.
1078200	1079200	It's inevitable.
1079200	1083760	And I think that gives hope for coordination, but then also it's dangers of disruption.
1083760	1091800	I want to double click one second on the two words that you use most, openness and transparency,
1091800	1096840	and understand fully what those mean one moment, because, you know, and the question is,
1096840	1101680	not only what they mean, but how fundamental it needs to be.
1101680	1107800	So openness right now and your definition in terms of AI means what?
1107800	1111120	It means different things, but different things, unfortunately.
1111120	1113520	I don't think it means open source.
1113520	1119680	I think, for me, open means more about access and ownership of the models so that you don't
1119680	1126000	have a lockstep, like you can hire your own graduates as opposed to relying on consultants.
1126000	1129720	Security comes down to, I think, for language models in particular, I don't think this holds
1129720	1131360	some media models.
1131360	1134400	You really need to know what it's been taught.
1134400	1136440	That's the only way to safety.
1136440	1140120	You should not engage with something or use something if you don't know what its credentials
1140120	1144120	are and how it's been taught, because I think that's inherently dangerous as these get more
1144120	1145120	and more capabilities.
1145120	1147520	And again, I don't know if we get to SGI.
1147520	1151040	If we do, I think it'll probably be like Scarlett Johansson and her, you know, like just to
1151040	1154240	combine thanks to the GPs, but I'm assuming we don't.
1154240	1155240	You still need transparency.
1156000	1162880	Again, how can any government or regulated industry not run on a transparent model?
1162880	1164680	They can't run on Blackpops.
1164680	1171160	I get that, and I understand the rationale for it, but now the question is, can you prove
1171160	1172160	transparency?
1172160	1177680	I think that, again, a model is only three things, really.
1177680	1181520	It's the data, the algorithm, and the compute.
1181520	1183600	And they come and the binary file pops out.
1183640	1189280	You can tune it with RLHF or DPO or genetic algorithms or whatever, but that's really
1189280	1190720	the recipe, right?
1190720	1195520	And so the algorithms, you don't need algorithmic transparency here versus classical AI because
1195520	1197320	they're very simple.
1197320	1201400	One of our fellows recreated the Palm 540 billion parameter model.
1201400	1203200	This is Lucid Raines on GitHub.
1203200	1204200	You look at that.
1204200	1205960	If you're a developer and you want to cry, it's GitHub.
1205960	1206960	It's crazy.
1206960	1210720	In 206 lines of PyTorch, and that's it.
1210720	1213480	The algorithms are not very complicated.
1213480	1217400	Having a gigantic super computer is complicated, and this is why they freaked out when Greg
1217400	1220920	Brockman kind of stepped down because he's one of the most talented engineers of our
1220920	1221920	time.
1221920	1225120	He built these amazing gigantic clusters.
1225120	1228560	And then the data and how you structure data is complicated.
1228560	1232040	So I think you can have transparency there because if the data is transparent and who
1232040	1235320	cares about the supercomputer, who really cares about the algorithm?
1235320	1240080	Now, let's talk about the next term, alignment here.
1240080	1244960	Alignments thrown around in lots of different ways.
1244960	1246960	How do you define alignment?
1246960	1251600	I define alignment in terms of objective function.
1251600	1259680	So YouTube was used by the extremists to serve ads for their nastiness.
1259680	1260760	Why?
1260760	1266560	Because the algorithm optimized for engagement, which then optimized for extreme content, which
1266560	1268440	then optimized for the extremists.
1268960	1273040	Did YouTube mean that, no, but they're just trying to serve ads up, right?
1273040	1276560	But it meant it wasn't aligned with its users' interests.
1276560	1280480	And so for me, if you have these technologies that we're going to outsource for more of our
1280480	1285720	mind, our culture, our children's futures, to you that are very persuasive, we have to
1285720	1291520	ensure they're aligned with our individual community and societal best interests.
1291520	1297280	I think this is where the tension with corporations will come in.
1297320	1301960	Because whoever licenses Scarlett Johansson's voice will sell a lot of ads, you know, they
1301960	1306520	can be very, very persuasive, but then what are their controls on that?
1306520	1308280	No one talks about that.
1308280	1314120	The bigger question of alignment is not killerism, making sure that AI doesn't kill us.
1314120	1320520	But again, I feel that if we build AI that is transparent, that we can test, that people
1320520	1326560	can build mitigations around, we are more likely to survive and thrive.
1326560	1331400	And also, I think there's a final element to here, which is who's alignment.
1331400	1334400	Different cultures are different, different people are different.
1334400	1338440	What we found with stable diffusion is that when we merge together the models that different
1338440	1342640	people around the world have built, the model gets so much better.
1342640	1347320	I think that makes sense because a monoculture will always be less fragile than a diversity.
1347320	1352600	Again, I'm not talking about in the DEI kind of way, I'm talking about it in the actual
1352600	1353600	logical way.
1353640	1359840	So we have a paper from our reinforcement learning lab called CARPA called QDHF, QD-AIF, Quality
1359840	1363800	and Diversity through artificial intelligence feedback.
1363800	1368560	Because you find these models do get better with high quality and diverse inputs, just
1368560	1375320	like you will get better if you have high quality and diverse experiences, you know?
1375320	1379680	And saying that's something that's important, that we'll get lost if all these models are
1379680	1380680	centralized.
1381640	1386720	You and I have had a lot of conversations about timelines here.
1386720	1393080	We can get into a conversation of when and if we see AGI.
1393080	1397320	But we're seeing more and more powerful capabilities coming online right now that are going to
1397320	1403480	cause a lot of amazing progress and disruption.
1403480	1410160	How much time do we have, EMI, and we had a conversation when we were together at FII
1410160	1418840	about the disenfranchised youth coming off of COVID.
1418840	1421600	So let's talk one second about timelines.
1421600	1434800	How long do we have to get our shit together, both as AI companies and investors and governors
1434800	1437800	of society?
1438800	1443560	The speed here is awesome and frightening.
1443560	1446480	How long do we have?
1446480	1448920	Everything everywhere, all at once, right?
1448920	1449920	We don't have long.
1449920	1453760	Like, AGI timelines for every definition of AGI, I have no idea.
1453760	1456720	It will never be less than 12 months, right?
1456720	1461360	Because it's such a step change, so let's put that to the side.
1461360	1466280	Right now, everyone that's listening, are you all going to hire the same amount of graduates
1466280	1467600	that you hired before?
1467600	1469960	The answer is no.
1469960	1473920	Some people might not hire any because this is a productivity and an answer and we have
1473920	1478160	the data for that across any type of knowledge industry.
1478160	1483640	You just had a great app that you can sketch and it does a whole iPhone app for you, right?
1483640	1488080	I've gone on record and saying there are no programmers who know in five years, why would
1488080	1489080	there be?
1489080	1490600	What are interfaces?
1490600	1497160	You had a 50% drop, I just put it on my Twitter, in hiring from Indian IITs.
1497160	1498160	That's crazy.
1498160	1503480	What you're going to have in a couple of years is around the world at the same time, these
1503480	1510960	kids that have gone through the trauma of COVID, highly educated STEM, programming, accountancy
1510960	1518080	law, simultaneously people will hire massively less of them because productivity enhances
1518080	1520400	and you don't need as many of them.
1520400	1522680	Why would you need as many paralegals?
1522680	1527920	That for me is a gigantic societal issue and the only thing I can think of is the stoke
1527920	1533400	of innovation and the generative jobs of the future through open source technology because
1533400	1538000	I don't know how else we're going to mitigate that because, Peter, you're a student of history.
1538000	1541920	What happens when you have large amounts of intelligent, disenfranchised youth history?
1541920	1543400	We've had that happen a few times.
1543400	1548760	We just had Arab Spring that long ago, revolt.
1548760	1554320	People war if not international law.
1554320	1561680	War is a good way to soak up the excess youth, but it's not pleasant for society and fundamentally
1561680	1567720	the cost of information gathering, organization has collapsed, like you look at stable video
1567720	1570080	that we released yesterday, right?
1570080	1573720	It's going to get so much better so quickly, just like stable diffusion, the cost of creating
1573720	1581120	movies increases, the demand for quality stuff increases, but there's a few years where demand
1581120	1586120	and supply don't match and that's such a turbulent thing to navigate.
1586120	1589880	That's one of the reasons I'm creating stabilities for different countries, so the best and brightest
1589880	1592360	for me which can help navigate them.
1592360	1593360	And people don't talk about it.
1593360	1602080	I loved your idea that the stability models and systems will be owned by the nation.
1602440	1606600	In fact, the one idea that I heard you say, which I thought was fantastic, was you graduate
1606600	1611360	college in India, you're an owner in that system.
1611360	1616280	You graduate from Nigeria, you're an owner in that system, basically to incentivize people
1616280	1621960	to complete their education and to have them have ownership in what is ultimately the most
1621960	1624800	important asset that nation has.
1624800	1626520	And talk about it as infrastructure as well.
1626520	1630440	I think that's an important analogy that people don't get.
1630440	1634240	This is the knowledge infrastructure of the future, it's the biggest leap forward we have
1634240	1638240	because you'll always have a co-pilot that knows everything in a few years and that can
1638240	1643320	create anything in any type, but it must be embedded to your cultural values and you can't
1643320	1645160	let anyone else own that.
1645160	1649000	So it is the infrastructure of the mind and who would outsource their infrastructure to
1649000	1650000	someone else.
1650000	1654440	So that's why I think Nigerians should own the models of Nigerians for Nigerians and
1654440	1657560	should be the next generation that does that.
1657560	1661040	That's why you give the equity to the graduates, that's why you list it, that's why you make
1661040	1666440	national champions because, again, that has to be that way.
1666440	1669800	This is far more important than 5G and this gives you an idea of the scale, we're just
1669800	1671760	at the start, the earlier doctor base.
1671760	1677360	A trillion dollars was spent on 5G, this is clearly more important, more than a trillion
1677360	1680840	dollars that we spend on this and again it flips the world.
1680840	1687480	And so there is huge threat for our societal balance and again I think open is a potential
1687480	1691720	antidote to create the jobs of the future and there's huge opportunity on the side
1691720	1696840	because no one will ever be alone and we can use this to coordinate our systems, give everyone
1696840	1701680	all the knowledge that they need at their fingertips and help guide everyone if we build
1701680	1706960	this infrastructure correctly and I don't see the highlight can be closed.
1706960	1716600	AGI, the conversation and the definition of AGI has basically been all over the place,
1716600	1723080	because while prediction has been for 30 years that it's 2029, again, that's a blurry
1723080	1731960	line of what we're trying to target but Elon's talked about anywhere from 2025 to 2028, what
1731960	1740040	are you thinking, what's your timeline for even digital superintelligence?
1740040	1744920	I honestly have no idea.
1744920	1750120	People looking at the scaling laws and applying it but as I've said, data is the key and it's
1750120	1754480	clear that we already have, you could build a board GPT and it would be better than most
1754480	1756440	corporate boards, right?
1756440	1760840	So I think we're already seeing improvements over the existing.
1760840	1763760	One of the complications here is swarm AI.
1763760	1767680	So even like it's the whole thing, like a duck-sized human or a hundred human sized
1767760	1774160	ducks, we're just at the start of swarm intelligence and that reflects and represents how companies
1774160	1775160	are organized.
1775160	1780600	Andre Carpethi has some great analogies on this in terms of the new knowledge OS and
1780600	1787080	that could take off at any time but the function of format of that may not be this whole Western
1787080	1791880	anti-conformized consciousness that we think of but just incredibly efficient systems that
1791880	1795800	displace existing human decision-making, right?
1795800	1801440	And so there's an entire actual range of different AGI outcomes depending on your definition
1801440	1807440	and I just don't know but I feel again like I wake up and I'm like, oh, look, it's fed
1807440	1812520	up 10 times the model, you know, like I'm just not, no one can predict this.
1812520	1817240	But there is a point at which, I mean, we were heading towards an AI singularity, using
1817280	1822120	a definition of a singularity as a point after which you cannot predict what's coming
1822120	1824800	next and that isn't far away.
1824800	1830440	I mean, how far out is it for you a year or two years?
1830440	1835160	I think you're heading towards it in the next few years but like I said, every company,
1835160	1838640	organization, individual has an objective function.
1838640	1844680	My objective function is to allow the next generation to navigate what's coming in the
1844680	1847400	optimal way and achieve their potential.
1847400	1851280	So I don't want to build an AGI, I don't want to do any of this, amplified human intelligence
1851280	1857480	is my preference and trying to mitigate against some of the harms of these agentic things
1857480	1862880	through data transparency, good standards and making it so people don't need to build
1862880	1869160	gigantic models on crap, which I think is a major danger if even if not from AGI.
1869160	1873960	But again, we just don't understand because it's difficult for us to comprehend super
1874000	1878640	human capabilities, but again, we're already seeing that in narrow fields.
1878640	1881160	We already know that it's a better rider than us.
1881160	1884360	So we already know that it can make better pictures than us.
1884360	1890600	And a better physician and a better educator and a better surgeon and a better everything.
1890600	1898680	Yeah, and again, I think it's this mythos of these big labs being AGI focused, whereas
1898680	1902480	you can be better than us in like 5% of the stuff that humans can do and that's still
1902520	1905800	a massive impact on the world and they can still take over companies and things like
1905800	1906800	that, right?
1906800	1910800	Like if you take over a company, then you can impact the world.
1910800	1915400	And there's clearly with a GPT for 1000 of them orchestrated correctly, that can call
1915400	1919600	up people and stuff, you wouldn't know it's not CEO, you know, I can make an MA GPT and
1919600	1923000	then they won't have to make all these tough decisions in any of that.
1923000	1927680	And most of my decisions aren't that good, so it's probably better.
1927680	1932520	So I think that we're getting to that point, it's very difficult and the design patterns
1932520	1933520	are going fast.
1933520	1939080	We're at the iPhone 2G, 3G stage, it's got copy and paste, and we just got the first
1939080	1943160	stage as well of this technology, which is the creation step, it creates stuff.
1943160	1949000	The next step is control and then composition, where you're annoyed because chat GPT doesn't
1949000	1952240	remember all the stuff that you've written, that won't be the case in a year.
1952240	1956920	And the final bit is collaboration, where these AIs collaborate together and with humans
1956920	1960320	to build the information superstructures of the future, and I don't feel that's more
1960320	1961320	than a few years away.
1961320	1964280	And it's completely unpredictable what that will create.
1964280	1971560	Let's talk about responsibility that AI companies have for making sure that their technology
1971560	1977040	is used in a pro-human and not a disruptive fashion.
1977040	1982160	Do you think that is a responsibility of a company, of a company's board, of a company's
1982160	1983160	leadership?
1983160	1985160	How do you think about that?
1985600	1990200	Again, with the corporate capitalist system, it typically isn't, because you're maximizing
1990200	1993800	shareholder value and there aren't laws and regulations, which is why I think there's
1993800	1998640	a moral, a social, and legal-slash-regulatory aspect to this.
1998640	2002480	Companies will just look at the legal-slash-regulatory, in some cases they'll just ignore them, right?
2002480	2008080	But I do think, again, we have a bigger moral and social obligation to this.
2008080	2011520	This is why I don't subscribe to EA or EAAC or any of these things.
2011520	2014600	I think it's complicated and it's hard.
2014640	2018400	Given the uncertainty and how this technology proliferates, and you've got to do your best
2018400	2024800	and be as straight as possible to people about doing your best, because none of us have qualified
2024800	2027120	to understand or do this.
2027120	2031240	And none of us should be trusted to have the power over this technology, right?
2031240	2034240	You should be questioned and you should be challenged with that.
2034240	2038560	And again, if you're not transparent, how are you going to challenge?
2038560	2043320	When I think of the most linear organizations on the planet, I think of governments, maybe
2043360	2047080	religions, but governments, let's leave it there.
2047080	2049080	How can...
2049080	2055480	Let's talk about Western government, at least the U.S., I would have said Europe, but I'll
2055480	2059880	say the U.K. and Europe.
2059880	2061200	What should they be...
2061200	2064600	What steps should they be taking right now?
2064600	2070520	If you were given the reins to say, how would you regulate, what would you want them to
2070560	2072560	do or not do?
2072560	2078000	I believe it's a complicated one, so I signed the first FLI letter.
2078000	2082840	I think I was the only AI CEO to do that back before it was cool, because I said, I don't
2082840	2085360	think AGI will kill us all, but I just don't know.
2085360	2089400	I think it's a conversation that deserves to be had, and it's a good way to have a conversation.
2089400	2093880	And then we flipped the wrong way, where we went overly AI death risk and other things
2093880	2098080	like that, and governments were doing that, at the AI Safety Summit in the U.K., and then
2098120	2101840	we had the King of England come out, and so this is the biggest thing since fire.
2101840	2106000	I was like, okay, that's a big change in the world, that's right.
2106000	2110240	The King of England said it, so I must be on the right track.
2110240	2112640	But I think if you look at it, regulation doesn't move faster.
2112640	2114600	Even the executive order will take a long time.
2114600	2116400	The EU things will kind of come in.
2116400	2120680	Instead, I think that governments have to focus on the tangibles.
2120680	2124920	AI killerism, again, it can be addressed by considering this as infrastructure and what
2124920	2127960	infrastructure we need to give our people to survive and thrive.
2128000	2132000	The U.S. is in a good initial place with the CHIPS Act, but I think you need national
2132000	2136880	data sets, you need to provide open models to stoke innovation, and think about what
2136880	2140800	the jobs of the future are, because things are never the same again.
2140800	2144520	You don't need all those programmers when co-pilot is so good, and you're moving co-pilot
2144520	2150240	from level above, which is compositional co-pilot, and then collaborative co-pilot, right?
2150240	2153240	You would be able to talk and computers can talk to computers better than humans can talk
2153240	2154760	to computers.
2154840	2158200	We need to articulate the future on that side, but then the other side.
2158200	2164200	One of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer,
2164200	2165200	right?
2165200	2167200	We did a United Order of this.
2167200	2171280	The loss of agency you feel, and many of you on this call will have had that diagnosis
2171280	2173000	to the near and dear, is huge.
2173000	2178400	Then I had 1,000 AI agents fighting every piece of information about pancreatic cancer,
2178400	2180560	and then after that, I felt a bit more control.
2180840	2184840	Why don't we have a global cancer model that gives you all the knowledge about cancer and
2184840	2190160	helps you talk to your kids and connect with people like you, not for diagnosis or research,
2190160	2191880	but for humans?
2191880	2196320	This is the Google MedPalM2 model, for example, that outperforms humans in diagnosis, but
2196320	2197320	also empathy.
2197320	2203520	What if we arm our graduates to go out and give support to the humans that are being
2203520	2205400	diagnosed in this way?
2205400	2209360	That makes society better, and it's valuable, you know?
2209360	2211920	That's an example of a job of the future, I think.
2211920	2212920	I don't believe in UBI.
2212920	2217080	I think we've been universal basic jobs as well, or used jobs.
2217080	2218080	Universal basic opportunity.
2218080	2219080	Right?
2219080	2220080	Yeah.
2220080	2223880	Universal basic opportunity, universal based jobs, but then post-makers need to think
2223880	2230080	about it now, because the graduate unemployment wave is literally a few years away, and it
2230080	2231080	will happen.
2231080	2235200	Yeah, that is, I mean, when I think about what, I parse the challenges we're going
2235200	2239720	to be facing in society into a few different elements.
2239720	2245360	I think what we have today is amazing, and if generative AI froze here, we'd have an
2245360	2250440	incredible set of tools to help humanity across all of its areas.
2250440	2254280	And then we've got what's coming in the next zero to five years.
2254280	2260040	We've talked about patient zero, perhaps being the U.S. elections, and I think you
2260040	2263120	had said it was Cambridge Analytica that required interference.
2263360	2268080	Now it's any kid in the garage that could play with the elections.
2268080	2273960	That's a challenging period of time, and this graduate unemployment wave, as you mentioned,
2273960	2279280	coming right on its heels.
2279280	2284480	The question becomes, is the only thing that can create alignment and help us overcome
2284480	2290760	this AGI at the highest level, meaning it is causing challenges, but ultimately is a
2290800	2293880	tool that will allow us to be able to solve these challenges as well.
2295680	2297320	I mean, that's a crazy thought, right?
2298320	2302440	Like, all this stuff is crazy, like the sheer scale and impact of it.
2302440	2307440	And, you know, these discussions, we had them last year, Peter, and now everyone's
2307440	2308320	like, yeah, that makes sense.
2308320	2313360	And I go, wow, right, it may be AGI, it may be these coordinating automated
2313360	2315600	story makers and balances from the market, right?
2316200	2319880	Next year, there's 56 elections with four billion people heading to the polls.
2321360	2322440	What could possibly go wrong?
2322440	2325960	Okay, possibly go wrong, you know?
2326120	2326760	Oh, my God.
2326800	2329160	But again, the technology isn't going to stop.
2329160	2333880	Like, even if stability puts down things, if open AI puts down things, it will
2333880	2338760	continue from around the world because you don't need much to train these models.
2338760	2340720	Again, the supercomputer thing is a myth.
2341200	2342800	You've got another year or two where you need them.
2342800	2345600	You don't need them after that, and that is insane to think about.
2346680	2348160	You just released stability video.
2348160	2350520	Congratulations on our stable visual diffusion.
2350520	2351160	Thank you.
2352720	2354840	And I'm enjoying some of the clips.
2355320	2363680	How far are we away from me telling a story to my kids and saying, let's make
2363680	2364480	that into a movie?
2367080	2367920	Almost two years away.
2367920	2368520	Two years away.
2369840	2371160	So this is a building block.
2371160	2372400	It's the best creation step.
2372440	2376120	And then, like I said, you have the control step, composition, and then
2376120	2379520	collaboration and self-learning systems around that.
2379520	2383480	So we have Kung Fu UI, which is our node-based system where you have all
2383480	2387240	the logic that makes up an image, like you can take a dress and a pose and
2387240	2388520	a face that combines them all.
2389080	2393080	And it's all encoded in the image because you can move beyond files to
2393120	2395120	intelligent workflows that you can collaborate with.
2395560	2399760	If I send you that image file and you put it into your Kung Fu UI, it gives you
2399760	2400840	all the logic that made that up.
2401320	2402200	How insane is that?
2403480	2405200	So we're going to step up there.
2405320	2411560	And what's happened now is that people are looking at this AI like instant versus
2411720	2415160	again, the huge amount of effort it took to take this information and
2415160	2416200	structure it before.
2417040	2419600	But the value is actually in stuff that takes a bit longer.
2420160	2424160	Like when you're shooting a movie, you don't just say, do it all in one shot,
2424160	2424520	right?
2424680	2429560	Unless you are a very talented director and actor, you know, you have
2429560	2432640	mise en place, you have staging, you have blocking, you have cinematography.
2433160	2435880	It takes a while to composite the scenes together.
2436480	2440280	It'll be the same for this, but a large part of it will then be automated for
2440280	2443440	creating the story that can resonate with you and you can turn it into
2443440	2444400	Korean or whatever.
2444920	2447960	And there'll still be big blog clusters like Oppenheimer and Barbie.
2448440	2451480	But again, the flaw will be raised overall.
2452200	2455960	Similarly, like we had a music video competition check it on YouTube with Peter
2455960	2460200	Gabriel, he allows us to use kindly his songs and people from all around the
2460200	2463440	world made amazing music videos to his thing, but they took weeks.
2464360	2466800	So I think that's somewhere in the middle here where again, we're just at
2466800	2472440	that early stage, because chat GPT isn't even a year old, you know, stable
2472440	2473760	diffusion is only 14, 15 months.
2473760	2479520	And I think you'd agree that neither of them is the end hole and be all.
2479520	2481880	It's just, it's the earliest days of this field.
2483000	2483760	I had the conversation.
2483840	2484920	The tiniest building.
2484920	2488400	Yeah, I had this conversation with Ray Kurzweil two weeks ago.
2489360	2493200	We're just after a singularity board meeting we had, and we're just on a
2493200	2494160	zoom and chatted.
2494560	2500080	And, you know, the realization is that, unfortunately, the human mind is
2500320	2502080	awful at exponential projections.
2502080	2507080	And despite the convergence of all these technologies, we tend to project
2507080	2512120	the future as a linear extrapolation of, you know, the world we're living in right
2512120	2512480	now.
2513480	2518040	But the best I can say is that we're going to see in the next decade, right,
2518040	2521920	between now and 2033, we're going to see a century worth of progress.
2522520	2525560	But it's going to get very weird, very fast, isn't it?
2527440	2530200	I mean, there's two way doors and there's one way doors, right?
2530760	2535040	Like in December of last year, multiple headmasters called me and said,
2535640	2537560	we can't set essays for our homework anymore.
2537920	2540200	And every headmaster in the world had to do that same thing.
2540200	2540960	It's a one-way door.
2541320	2541840	Yes.
2542680	2545240	And this is the scary part, the one-way doors, right?
2546080	2551120	Like when you have an AI that can do your taxes, what does that mean for
2551120	2551720	accountants?
2553440	2554640	All the accountants at the same time.
2556560	2557360	Kind of crazy, right?
2557440	2558280	It is.
2559080	2563160	And the challenge, I mean, one of my biggest concerns, so listen, I'm the
2563200	2564080	eternal optimist.
2564080	2567320	I'm not the guy who's the glasses half full, it's the glasses overflowing.
2568280	2577000	And one of the challenges I think through when I think about where AI, AGI, ASI,
2577040	2583720	however you want to project it to is the innate importance of human purpose.
2584600	2589600	And unfortunately, most of us derive our purpose from the work that we do.
2590240	2594800	You know, I ask you, you know, tell me about yourself and you jump into your
2594840	2595920	work and what you do.
2596400	2601320	And so when AI systems are able to do most everything we do, not just a little
2601320	2607680	bit better, but, you know, orders of magnitude better, redefining purpose and
2607680	2616320	redefining my role in achieving a moonshot or a transformation is, it's the,
2616680	2625880	you know, it's the impedance mismatch between human societal growth rates
2626040	2627600	and tech growth rates.
2628160	2629000	What are your thoughts there?
2630640	2632640	Yeah, I mean, I think again, exponentials are hard.
2632760	2638840	Like if I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well,
2638840	2639520	that's not possible.
2639560	2640160	Why?
2640680	2644840	You know, like GPT-4 is impossible, stable diffusion is impossible, right?
2645520	2649200	Like now they've almost become commonplace, but why would you need super
2649200	2650080	computers and these things?
2650720	2655880	I do agree this mismatch and that's why we're in for five years of chaos.
2656040	2660200	That's why I called it stability because I saw this coming a few years ago and
2660200	2663400	I was like, holy crap, we have to build this company.
2664080	2668600	And now we have the most downloads of any models of any company, like 50 million
2668600	2671480	last month versus 700,000 from Astral, for example.
2673000	2676640	And we will have the best model of every type except for very large language
2676640	2677920	models by the end of the year.
2678520	2683160	So we have audio, 3D, video, code, everything and a lovely, amazing
2683160	2688520	community because it's just so hard again for us to imagine this mismatch.
2688560	2689720	There's a period of chaos.
2690040	2694000	But then on the other side, like there's this PDoom question, right?
2694000	2695280	The probability of doom.
2696640	2700040	I can say something with this technology, the probability of doom is lower
2700040	2703280	than without this technology because we're killing ourselves.
2704360	2707720	And this can be used to enhance every human and coordinate us all.
2708600	2711360	And I think what we're aiming for is that Star Trek future versus that Star
2711360	2711600	Wars.
2711680	2713320	Yes, I meant to that.
2715200	2720240	And I think that's an important point, the level of complexity that we have
2720240	2724240	in society, we don't need AI to destroy the planet.
2724840	2727280	We're doing that very, very well.
2727280	2727680	Thank you.
2728680	2730760	But the ability to coordinate.
2730920	2735680	So one of the things I think about is a world in which everyone has access to
2735680	2739200	all the food, water, energy, healthcare, education that they want.
2739840	2747800	Really, a world of true abundance in my mind is a piece more peaceful world, right?
2747840	2751160	Why would you want to destroy things if you have access to everything that you
2751160	2751480	need?
2752240	2758160	And that kind of a world of abundance is on the backside of this kind of
2758160	2759080	awesome technology.
2760440	2762040	We have to navigate the next period.
2762080	2765040	I believe we'll see it within our lifetimes, particularly if we get
2765040	2766160	longevity songs, right?
2767760	2769240	And that's so amazing, right?
2769480	2771640	But then we think about, as you said, why peace?
2772600	2775200	A child in Israel is the same as a child in Gaza.
2775720	2777120	And then something happens.
2777120	2781720	A liar is told that you are not like others and the other person is not human
2781720	2782160	like you.
2782640	2784280	All wars are based on that same line.
2785760	2789720	And so again, if we have AI that is aligned with the potential of each human
2789720	2794520	that can help mitigate those lies, then we can get away from war because
2795840	2797160	the world is not scarce.
2798160	2799360	There is enough food for everyone.
2799400	2800840	It's a coordination failure.
2802360	2804040	And that can be addressed by this technology.
2804040	2804160	I agree.
2804160	2809200	One of the most interesting and basic functions or capabilities of generative
2809240	2814960	AI has been the ability to translate my ideas into concepts that someone who is
2814960	2816960	a different frame of thought can understand.
2817760	2818040	Right?
2820000	2821840	But that's what this generative AI is.
2821880	2823880	It's a universal translator.
2824040	2824440	Sure.
2824760	2826320	It does not have facts.
2826320	2828360	The fact that it knows anything is insane.
2828400	2830600	Hallucinations is a crazy thing to say.
2830680	2832440	Again, it's just like a graduate trying so hard.
2833560	2837760	GPT-4 with 10 trillion words and 100 gigabytes is insane.
2838440	2843200	Stable diffusion has like 100,000 gigabytes and a two gigabyte file.
2843200	2846760	50,000 to one compression is something else.
2846800	2848000	It's learned principles.
2848160	2848640	Yes.
2848880	2851280	And this is it's knowledge, knowledge versus data.
2851440	2851720	Yeah.
2852080	2853640	It's knowledge versus data.
2853640	2855640	And if you have some experience, you get the wisdom, right?
2855640	2856000	Yes.
2856000	2860760	Because it's learned the principles and contexts and it can map them to
2860760	2863880	transform data because that's how you navigate.
2864520	2869120	You don't navigate based on like logical flow.
2869120	2872000	We have those two parts of our brain navigate sometimes based on instinct
2872000	2873560	based on the principles you've learned.
2873920	2878640	So Tesla's new auto driving model, self driving model is entirely based on
2878640	2882000	a console, which are protection, like they said it publicly is based on this
2882000	2882520	technology.
2882520	2883840	It doesn't have any rules.
2884440	2887680	It's just learned the principles of how to drive from massive amounts of Tesla
2887680	2891120	data that now fits on the hardware without internet.
2891880	2895560	And so they went from self driving being impossible to now, hey, it looks pretty
2895560	2898920	well, you know, because it's learned the principles.
2899280	2902720	And so that's why this technology can help solve the problem.
2902720	2906720	This is why it can help us amplify our intelligence and innovation.
2907720	2911160	Because the missing part, the second part of the frame, you know, next, I can't
2911160	2915600	give more details yet, but next week we're announcing the largest X prize ever.
2915600	2917200	It's a hundred and one million dollars.
2918200	2919360	It's a hundred and one.
2919360	2925360	So it's Elon had the had a hundred million dollar prize that kind of defund a few
2925360	2930360	years ago for carbon sequestration and the funder, the first funder of this prize
2930360	2931920	wanted to be larger than Elon's.
2931920	2933800	I said, okay, you had the extra million.
2933880	2934800	It's for luck.
2934800	2935400	It's for luck.
2935400	2936880	We did our seed round 101 million.
2936880	2937400	Oh, really?
2937400	2937880	Okay.
2937880	2938880	Hi, that's great.
2939880	2940880	It's on your popular number.
2941880	2946280	Anyway, the and it's in the field of health.
2946280	2952040	I'll leave it at that folks go to XPRIZE.org to register to see the live event on
2952040	2952880	November 29th.
2952880	2956240	We're going to be debuting the prize, what it is, what it's going to impact
2956240	2961000	eight billion people, long story short, it's, it's a nonlinear,
2961080	2968440	future because we are able to utilize AI and make things that were seemingly
2968440	2972480	crazy before, likely to become inevitable.
2973480	2975880	And that's an amazing future we have to live into.
2978080	2983240	Yeah, I mean, again, because it's one way doors, the moment we create a cancer
2983240	2986680	GPT, and this is something that we're building, we have trillions of tokens
2986680	2988080	and then you Google TV.
2988160	2992960	Things like that, that organizes global cancer knowledge and makes it accessible
2992960	2997160	and useful, even if it's just for guiding people that have been diagnosed.
2997160	2998560	The world changes.
2998560	3002560	The 50% of people that have a cancer diagnosed in their lives in every language
3002560	3006360	and every level will have someone to talk to and connect them with the resources
3006360	3009760	they need and other people like them and talk to their families.
3009760	3011760	You know, and how insane is that?
3012760	3016560	And so again, the least positive thing is that we're going to be able to
3016560	3020040	and so again, the least positive stories in the future need to be told, right?
3020040	3025120	Because that will align us to where we need to go as opposed to a future full
3025120	3026920	of uncertainty and craziness and doom.
3028240	3033440	In our last couple minutes here, buddy, what can we look forward to from stability
3034520	3037120	in the months and years ahead?
3038080	3041040	We have every model of every type and we'll build it for every nation
3041040	3042920	and we'll give back control to every nation.
3042920	3045520	So coming back to governance here.
3047160	3051040	Again, is the nation state the unit of control?
3052400	3056880	Is it? No, I my my thinking is disabilities in every nation
3056880	3060360	should have the best and brightest of each because what you've seen is
3060360	3064360	there are amazing people in this sphere, the best and brightest in the world.
3064360	3067880	Now, this is the biggest thing ever and they all want to work in it.
3067960	3070720	And it's just finding the right people with the right intention.
3071240	3074120	The brightest people go back to Singapore or Malaysia or others
3074600	3076440	because of the future of their nations.
3076440	3079440	And again, now we're doing a big change and we don't talk
3079440	3081160	about all the cool stuff we do.
3081160	3083240	We've just taken it because you need to articulate
3083240	3086280	that positive vision of the future because the only scarce resource
3086280	3087800	is actually this is human capital.
3087800	3089400	It's not GPUs.
3089400	3090720	It's not data.
3090720	3095280	It's about the humans that can see this technology and realize that
3095280	3098320	they can play a part in guiding it for the good of everyone,
3099080	3100800	their own societies and more.
3100800	3103520	And that's again, what I hope stability can be.
3103520	3105600	Well, I wish you the best of luck, pal.
3105600	3108160	Thank you for joining me in this conversation.
3108160	3110920	It's it's been a crazy four or five days.
3112200	3115880	And wish Sam and Greg and the entire
3116880	3120520	opening I team stability in their lives.
3120960	3123520	Yeah, I have a nice Thanksgiving.
3123520	3127560	They're absolutely they're an amazing team building world changing technology.
3127960	3129440	It's such a concentration of talent.
3129480	3133760	I think, again, I really felt for them over the last few days,
3133840	3135880	you know, much as I kind of post memes and everything.
3135880	3137840	I posted that as well.
3137840	3141280	I think this will bring them closer together and hopefully they can solve
3141280	3144280	the number one problem that I've asked them to solve, which is email.
3145000	3147080	Solve email, right?
3147160	3149000	And then we'll crack on from there.
3149000	3150000	All right, cheers, my friend.
