1
00:00:00,000 --> 00:00:03,000
Coding is changing dramatically.

2
00:00:03,000 --> 00:00:06,000
There will be no code that doesn't use AI as post their workflow.

3
00:00:11,000 --> 00:00:15,000
5G and Starlink, the dry kindle for this fire has been set.

4
00:00:15,000 --> 00:00:18,000
Is this more important than 5G?

5
00:00:18,000 --> 00:00:19,000
By orders of magnitude.

6
00:00:19,000 --> 00:00:20,000
Orders of magnitude.

7
00:00:20,000 --> 00:00:25,000
There is no kind of pure independent, completely rational person because we're not robots.

8
00:00:25,000 --> 00:00:28,000
So medicine is changing dramatically.

9
00:00:28,000 --> 00:00:30,000
Is your doctor AI enhanced?

10
00:00:30,000 --> 00:00:32,000
Low insurance premium, lower copay.

11
00:00:32,000 --> 00:00:35,000
That is something across every industry that's going to be happening.

12
00:00:35,000 --> 00:00:37,000
10 years out, what is a lawyer?

13
00:00:37,000 --> 00:00:38,000
What is an accountant?

14
00:00:38,000 --> 00:00:40,000
What is an engineer?

15
00:00:40,000 --> 00:00:42,000
They are all AI assisted.

16
00:00:42,000 --> 00:00:44,000
The entire knowledge sector is transformed.

17
00:00:44,000 --> 00:00:47,000
It enables you to do whatever you want to do.

18
00:00:47,000 --> 00:00:50,000
Learning should be a place of positive growth and joy.

19
00:00:50,000 --> 00:00:51,000
Where's the difference there?

20
00:00:51,000 --> 00:00:52,000
Where's the difference?

21
00:00:52,000 --> 00:00:54,000
And I think the key thing here is empathy.

22
00:00:58,000 --> 00:01:02,000
We're in this beautiful home in the Hollywood Hills.

23
00:01:02,000 --> 00:01:08,000
And a lot's happened in the last six months since we were recording our last podcast.

24
00:01:08,000 --> 00:01:10,000
And you were on the stage at abundance 360.

25
00:01:10,000 --> 00:01:17,000
And I think this is epicenter for a lot of people's concerns right now.

26
00:01:17,000 --> 00:01:21,000
A transformation potentially in Holly.

27
00:01:21,000 --> 00:01:25,000
When you were speaking about that, that it's going to change dramatically.

28
00:01:26,000 --> 00:01:34,000
And would you say we haven't even seen a small portion of the change yet?

29
00:01:34,000 --> 00:01:38,000
I think we are at the foot of the mountain as it were.

30
00:01:38,000 --> 00:01:41,000
Kind of compare it to being at the iPhone 2G to 3G point.

31
00:01:41,000 --> 00:01:43,000
We just got copy and paste.

32
00:01:43,000 --> 00:01:47,000
We haven't seen what this technology is really capable of yet and it hasn't got everywhere.

33
00:01:47,000 --> 00:01:51,000
It's like everyone's talking about it, but not that many people are using it.

34
00:01:51,000 --> 00:01:59,000
There was a recent study done that showed only 17% of people had used chat GPT in the US.

35
00:01:59,000 --> 00:02:01,000
Despite the fact you can do anyone's homework.

36
00:02:01,000 --> 00:02:04,000
I mean, it seems incredibly a small percentage.

37
00:02:04,000 --> 00:02:08,000
I guess because the community I hang out with everybody's using it.

38
00:02:08,000 --> 00:02:09,000
Well, that's the thing.

39
00:02:09,000 --> 00:02:13,000
We live in our monocultures, but then a third of the world still doesn't have internet, right?

40
00:02:13,000 --> 00:02:17,000
And you think the first internet they get will probably be AI enhanced.

41
00:02:18,000 --> 00:02:20,000
And then you think about this technology proliferating.

42
00:02:20,000 --> 00:02:23,000
It's when it becomes enterprise ready.

43
00:02:23,000 --> 00:02:28,000
Enterprise adoption, company adoption always takes a while, but by next year that happens.

44
00:02:28,000 --> 00:02:34,000
And I think every company everywhere that has anything to do with knowledge work will implement this at scale.

45
00:02:34,000 --> 00:02:35,000
And that's a crazy thought.

46
00:02:35,000 --> 00:02:41,000
And when it's embedded in the things you use every day and you don't know it's part of what you're using.

47
00:02:41,000 --> 00:02:42,000
I think that's the thing.

48
00:02:42,000 --> 00:02:46,000
Technology, you don't need to know that it's 5G or the internet works faster.

49
00:02:46,000 --> 00:02:48,000
You can watch movies quicker.

50
00:02:48,000 --> 00:02:51,000
In this case, you write something in Google Docs.

51
00:02:51,000 --> 00:02:52,000
Now they just rolled it out.

52
00:02:52,000 --> 00:02:57,000
You can say, make this snappier and it will do it automatically.

53
00:02:57,000 --> 00:03:00,000
Technology doesn't need to be there as technology in the front.

54
00:03:00,000 --> 00:03:03,000
It's all about use cases and the use cases are now maturing.

55
00:03:03,000 --> 00:03:05,000
And again, I think next year is the real takeoff.

56
00:03:05,000 --> 00:03:06,000
But now everyone's feeling it.

57
00:03:06,000 --> 00:03:09,000
If there's anything to do with it, something is coming.

58
00:03:09,000 --> 00:03:15,000
In this conversation, I want to really think through how this is all affecting every industry.

59
00:03:15,000 --> 00:03:17,000
And let's start with two industries.

60
00:03:17,000 --> 00:03:23,000
One is journalism and the other is Hollywood, which is we're sitting in the midst of this.

61
00:03:23,000 --> 00:03:27,000
One of the concerns I have, I know you share it, a lot of people do,

62
00:03:27,000 --> 00:03:35,000
especially with elections coming up in 18 months, 2024, is what is truth?

63
00:03:35,000 --> 00:03:38,000
And are we going to enter a post-truth world?

64
00:03:38,000 --> 00:03:43,000
And can you talk about your thoughts on journalism and how AI is impacting it?

65
00:03:43,000 --> 00:03:46,000
So I think you've seen shifts in journalism over the years,

66
00:03:46,000 --> 00:03:51,000
but we're all familiar with kind of some of the clickbait journalism that we see now.

67
00:03:51,000 --> 00:03:54,000
AI can obviously do clickbait better than humans.

68
00:03:54,000 --> 00:03:56,000
And that's one kind of extreme.

69
00:03:56,000 --> 00:04:01,000
This whole fake news, deep fake kind of stuff, that's a real concern.

70
00:04:01,000 --> 00:04:04,000
And that's why we have authenticity mechanisms now.

71
00:04:04,000 --> 00:04:10,000
We embed watermarking, we partner with GPT-Zero and other things to identify AI.

72
00:04:10,000 --> 00:04:15,000
But on the other side of it, there's a real challenge coming because AI can also help with truth.

73
00:04:15,000 --> 00:04:20,000
It can help you do proper analysis and expand out the reasoning for things.

74
00:04:20,000 --> 00:04:22,000
It can identify biases within.

75
00:04:22,000 --> 00:04:25,000
So journalism as it stands is caught between two things.

76
00:04:25,000 --> 00:04:29,000
To get clicks, to get ads, they went a bit more clickbait

77
00:04:29,000 --> 00:04:33,000
and they focus on sensationalist headlines, even if it's with unnamed sources and things.

78
00:04:33,000 --> 00:04:38,000
On the other hand, someone's going to build an AI system, AI enhanced system,

79
00:04:38,000 --> 00:04:42,000
that for any article you read, you can find all the background material

80
00:04:42,000 --> 00:04:45,000
and that suddenly becomes a source of truth, so it's kind of a pincer movement.

81
00:04:45,000 --> 00:04:50,000
And journalists and news sources will have to figure out where are you in this?

82
00:04:50,000 --> 00:04:52,000
How do you compete to provide value?

83
00:04:52,000 --> 00:04:58,000
Yeah, are you buzzfeed on one end, which is mostly all clickbait all the time,

84
00:04:58,000 --> 00:05:03,000
or are you trying to be the New York Times and deliver well-researched journalism?

85
00:05:03,000 --> 00:05:07,000
But the entity that competes with the New York Times that will come,

86
00:05:07,000 --> 00:05:09,000
and who knows, it might be the New York Times itself,

87
00:05:09,000 --> 00:05:12,000
can use AI to enhance great journalism,

88
00:05:12,000 --> 00:05:14,000
write in any voice, do all these things,

89
00:05:14,000 --> 00:05:17,000
and give fully reference facts that you can explore.

90
00:05:17,000 --> 00:05:20,000
The other side is that we're going to trust this technology more and more,

91
00:05:20,000 --> 00:05:23,000
just like we trust Google Maps, just like you trust other things,

92
00:05:23,000 --> 00:05:27,000
such that it's like, why have I got a human doctor without an AI?

93
00:05:27,000 --> 00:05:32,000
Why have I got a journalist who isn't using AI to check everything and their own implicit biases?

94
00:05:32,000 --> 00:05:36,000
And I think that part is actually quite misunderstood as to something that's coming,

95
00:05:36,000 --> 00:05:41,000
because, again, humans plus AI outcompete humans without AI.

96
00:05:41,000 --> 00:05:43,000
I believe in that, and I see that.

97
00:05:43,000 --> 00:05:47,000
And I think it's interesting in my life, and those I know,

98
00:05:47,000 --> 00:05:50,000
AI hasn't replaced the things that I've done.

99
00:05:50,000 --> 00:05:53,000
It hasn't actually even saved me time per se,

100
00:05:53,000 --> 00:05:55,000
because I'm still spending the same amount of time.

101
00:05:55,000 --> 00:06:00,000
It's allowed me to do a better job at what I want to do, which is the end product.

102
00:06:00,000 --> 00:06:04,000
I mean, that's because you do a little bit of everything, so you always fill the gap.

103
00:06:04,000 --> 00:06:08,000
True, I fill every moment of the time creating something for one of my companies.

104
00:06:08,000 --> 00:06:11,000
Well, I mean, this is an open AI report that was done,

105
00:06:11,000 --> 00:06:18,000
where they said that between 14% and 50% of tasks will be augmented by AI,

106
00:06:18,000 --> 00:06:21,000
will be changed by AI, because, again,

107
00:06:21,000 --> 00:06:25,000
I think a lot of the focus is on these automated systems, the terminators,

108
00:06:26,000 --> 00:06:30,000
to the bots, whereas realistically, the way this AI will come in

109
00:06:30,000 --> 00:06:35,000
is to help us with individual tasks, rewriting something, generating an image,

110
00:06:35,000 --> 00:06:38,000
making a song, adjusting your speech to sound more confident.

111
00:06:38,000 --> 00:06:42,000
Yeah, and we'll get to the dystopian conversation,

112
00:06:42,000 --> 00:06:47,000
because I'd like to hear what you think is real versus hype.

113
00:06:47,000 --> 00:06:51,000
I think the audience needs to understand what should they truly be concerned about,

114
00:06:51,000 --> 00:06:53,000
and what shouldn't they...

115
00:06:53,000 --> 00:06:59,000
I mean, that is being able to trace back and have a truth mechanism.

116
00:06:59,000 --> 00:07:04,000
We can talk about what Elon's looking to build as well on the truth side,

117
00:07:04,000 --> 00:07:10,000
but it's fascinating when the truth becomes blurry.

118
00:07:10,000 --> 00:07:13,000
Yeah, and there's not always an objective truth,

119
00:07:13,000 --> 00:07:15,000
because it depends upon your individual context, right?

120
00:07:15,000 --> 00:07:16,000
Yeah.

121
00:07:16,000 --> 00:07:19,000
And we didn't have the systems to be able to be comprehensive,

122
00:07:19,000 --> 00:07:23,000
authoritative, or up-to-date enough to do that until today.

123
00:07:23,000 --> 00:07:31,000
Well, we can actually go to the root source of the data and see, is it valid?

124
00:07:31,000 --> 00:07:35,000
Maybe it's a blockchain-enabled validation mechanism.

125
00:07:35,000 --> 00:07:38,000
Maybe it's got that authority, that authentication.

126
00:07:38,000 --> 00:07:39,000
Maybe it's...

127
00:07:39,000 --> 00:07:43,000
We mentioned Elon Community Notes on Twitter that AI enhanced,

128
00:07:43,000 --> 00:07:46,000
that can pull from various things and show the provenance.

129
00:07:46,000 --> 00:07:49,000
So you've got provenance, again, you've got authority,

130
00:07:49,000 --> 00:07:51,000
you have comprehensiveness, you have up-to-dateness.

131
00:07:51,000 --> 00:07:55,000
The future of Wikipedia is not what Wikipedia looks like today,

132
00:07:55,000 --> 00:07:59,000
but that future becomes something that can be integrated into other things.

133
00:07:59,000 --> 00:08:02,000
So what you'll have is, for any piece of information,

134
00:08:02,000 --> 00:08:05,000
you'll be able to say, this is the bias from which it was said,

135
00:08:05,000 --> 00:08:08,000
these are the compositional sources and more.

136
00:08:08,000 --> 00:08:12,000
So for example, there's a great app that I use called Perplexity AI.

137
00:08:12,000 --> 00:08:13,000
Okay.

138
00:08:13,000 --> 00:08:17,000
So when you go to GPT-4 or Bing, you write stuff,

139
00:08:17,000 --> 00:08:18,000
it doesn't give you all the sources,

140
00:08:18,000 --> 00:08:22,000
Perplexity actually brings in all the sources at a surface level,

141
00:08:22,000 --> 00:08:26,000
but it references why it said certain things with GPT-4.

142
00:08:26,000 --> 00:08:28,000
That's just going to get more and more advanced

143
00:08:28,000 --> 00:08:30,000
so you can dig into as much depth as you want

144
00:08:30,000 --> 00:08:32,000
and ask it to rephrase things as,

145
00:08:32,000 --> 00:08:35,000
what if that article there wasn't true that fed this?

146
00:08:35,000 --> 00:08:39,000
Or what about this perspective if I want it to be a bit more libertarian?

147
00:08:39,000 --> 00:08:44,000
Do you think it's possible to actually get to a fundamental truth

148
00:08:44,000 --> 00:08:45,000
in a lot of these areas?

149
00:08:45,000 --> 00:08:47,000
I think it depends on the area, right?

150
00:08:47,000 --> 00:08:48,000
Some areas there are fundamental truths.

151
00:08:48,000 --> 00:08:50,000
This happened or it didn't happen,

152
00:08:50,000 --> 00:08:53,000
even though you see deniers of various things.

153
00:08:53,000 --> 00:08:56,000
A lot of stuff is probabilistic when you're thinking about the future.

154
00:08:56,000 --> 00:08:58,000
But even something like climate,

155
00:08:58,000 --> 00:09:02,000
you see a lot of deniers of the real problem that we have

156
00:09:02,000 --> 00:09:04,000
with it being very difficult to persuade them

157
00:09:04,000 --> 00:09:07,000
because it becomes part of their ideology almost.

158
00:09:07,000 --> 00:09:09,000
But with this technology, you can say,

159
00:09:09,000 --> 00:09:11,000
look, literally here is the comprehensiveness.

160
00:09:11,000 --> 00:09:13,000
So like Jeremy Howard and Trisha Greenley

161
00:09:13,000 --> 00:09:17,000
did an analysis of well over 100 mask papers

162
00:09:17,000 --> 00:09:21,000
and did a meta-analysis on the effectiveness of that for COVID.

163
00:09:21,000 --> 00:09:24,000
And then that helped change the global discussion on masking

164
00:09:24,000 --> 00:09:27,000
because I'm actually bothered to do a comprehensive analysis.

165
00:09:27,000 --> 00:09:28,000
What was the result?

166
00:09:28,000 --> 00:09:31,000
Well, the result was that masks work for respiratory diseases.

167
00:09:31,000 --> 00:09:36,000
There are so many people that just refuse to believe that masks have any value.

168
00:09:36,000 --> 00:09:38,000
But let's not go down that road.

169
00:09:38,000 --> 00:09:43,000
One of the things I found interesting was the idea of a GPT model

170
00:09:43,000 --> 00:09:46,000
being able to translate your points of view

171
00:09:46,000 --> 00:09:49,000
for someone else to make, to receive them better.

172
00:09:49,000 --> 00:09:51,000
Like if you're hardcore to the right

173
00:09:51,000 --> 00:09:55,000
and you want to convince someone about your issue,

174
00:09:55,000 --> 00:10:01,000
having chat GPT or one of Stability's products

175
00:10:01,000 --> 00:10:04,000
generate a rewritten version of that language

176
00:10:04,000 --> 00:10:07,000
so the person can hear it better.

177
00:10:07,000 --> 00:10:09,000
I find that an interesting and powerful tool.

178
00:10:09,000 --> 00:10:11,000
Yeah, I think this is the thing.

179
00:10:11,000 --> 00:10:14,000
It's all about your individual context and what resonates with you

180
00:10:14,000 --> 00:10:17,000
because information exists within a context.

181
00:10:17,000 --> 00:10:19,000
So if it's going to change the state within you,

182
00:10:19,000 --> 00:10:21,000
you need to understand your point of view.

183
00:10:21,000 --> 00:10:23,000
So if we think of these as really talented youngsters,

184
00:10:23,000 --> 00:10:26,000
these AIs that go a bit funny,

185
00:10:26,000 --> 00:10:27,000
what would you want?

186
00:10:27,000 --> 00:10:29,000
You would want someone to sit down and say,

187
00:10:29,000 --> 00:10:31,000
this is your point of view in your context

188
00:10:31,000 --> 00:10:33,000
and my point of view in my context.

189
00:10:33,000 --> 00:10:36,000
Let's find some common ground and then we can work from there.

190
00:10:36,000 --> 00:10:38,000
Much of politics isn't really about facts.

191
00:10:38,000 --> 00:10:41,000
It's about persuasion because facts,

192
00:10:41,000 --> 00:10:44,000
when you have diametrically a divergent context,

193
00:10:44,000 --> 00:10:46,000
are very difficult to do.

194
00:10:46,000 --> 00:10:48,000
So you said being able to rewrite something from one context

195
00:10:48,000 --> 00:10:50,000
to another is important,

196
00:10:50,000 --> 00:10:52,000
but then you have to understand the context

197
00:10:52,000 --> 00:10:54,000
and that's what these models do really, really well.

198
00:10:54,000 --> 00:10:57,000
We can take a piece and we can say,

199
00:10:57,000 --> 00:10:59,000
write it as a wrap by Eminem

200
00:10:59,000 --> 00:11:03,000
or in the style of Ulysses by James Joyce

201
00:11:03,000 --> 00:11:06,000
and it will do that because it understands the essence of that.

202
00:11:06,000 --> 00:11:09,000
I think people don't realize,

203
00:11:09,000 --> 00:11:11,000
and when I just hit this point again,

204
00:11:11,000 --> 00:11:13,000
we've talked about it somewhat,

205
00:11:13,000 --> 00:11:15,000
our minds are neural nets,

206
00:11:15,000 --> 00:11:16,000
our brains are neural nets,

207
00:11:16,000 --> 00:11:18,000
100 trillion neurons

208
00:11:18,000 --> 00:11:20,000
and everything that you bring into your mind,

209
00:11:20,000 --> 00:11:22,000
this conversation that we're having,

210
00:11:22,000 --> 00:11:24,000
what you're watching on the TV news,

211
00:11:24,000 --> 00:11:26,000
newspaper, what's on your walls,

212
00:11:26,000 --> 00:11:28,000
the people you hang out with are all constantly shaping

213
00:11:28,000 --> 00:11:32,000
the way you see the world and shaping your mindset.

214
00:11:32,000 --> 00:11:36,000
It's one of the things I think about in the future of news media

215
00:11:36,000 --> 00:11:40,000
is an individual actively being able to choose

216
00:11:40,000 --> 00:11:42,000
what mindset they want to work on.

217
00:11:42,000 --> 00:11:45,000
I'd like to have a more optimistic mindset,

218
00:11:45,000 --> 00:11:47,000
I'd like to have a more moonshot mindset,

219
00:11:47,000 --> 00:11:49,000
a more an abundance mindset

220
00:11:49,000 --> 00:11:52,000
and then being able to have that information fed to you

221
00:11:52,000 --> 00:11:55,000
in a factual fashion that allows you to,

222
00:11:55,000 --> 00:11:58,000
instead of what the crisis news network delivers,

223
00:11:58,000 --> 00:12:01,000
which is a constantly negative news.

224
00:12:01,000 --> 00:12:03,000
It's caused the dopamine effect

225
00:12:03,000 --> 00:12:05,000
in your fight or flight response.

226
00:12:05,000 --> 00:12:08,000
You can say make it from this point of view,

227
00:12:08,000 --> 00:12:09,000
you don't change the facts,

228
00:12:09,000 --> 00:12:11,000
but even the way things are worded.

229
00:12:11,000 --> 00:12:13,000
Or balancing, right?

230
00:12:13,000 --> 00:12:15,000
I mean, I don't need to see every murder,

231
00:12:15,000 --> 00:12:16,000
tell me what the companies have got funded,

232
00:12:16,000 --> 00:12:18,000
tell me the breakthroughs that occurred,

233
00:12:18,000 --> 00:12:20,000
the science that's occurred today.

234
00:12:20,000 --> 00:12:22,000
But there's even, again, everything you can,

235
00:12:22,000 --> 00:12:24,000
it depends on how you portray it, right?

236
00:12:24,000 --> 00:12:27,000
You know, there are murders, I take a murder, for example.

237
00:12:27,000 --> 00:12:29,000
There is the facts, there is the,

238
00:12:29,000 --> 00:12:32,000
oh my God, there'll be a million more murders.

239
00:12:32,000 --> 00:12:34,000
There is the case that it's a very sad thing.

240
00:12:34,000 --> 00:12:36,000
There's the case that, you know,

241
00:12:36,000 --> 00:12:39,000
the police are working super hard to solve this

242
00:12:39,000 --> 00:12:40,000
and we need to reach out to the families

243
00:12:40,000 --> 00:12:42,000
and come together as a community.

244
00:12:42,000 --> 00:12:43,000
These are all different aspects

245
00:12:43,000 --> 00:12:45,000
for the same terrible action.

246
00:12:45,000 --> 00:12:46,000
Yes.

247
00:12:46,000 --> 00:12:48,000
Which have different levels of positivity,

248
00:12:48,000 --> 00:12:52,000
negativity, clickbaitiness versus community, right?

249
00:12:52,000 --> 00:12:55,000
Facebook did a study many years ago

250
00:12:55,000 --> 00:12:57,000
whereby they had a hypothesis.

251
00:12:57,000 --> 00:13:00,000
If you see sadder things on your timeline,

252
00:13:00,000 --> 00:13:03,000
you'll post sadder things.

253
00:13:03,000 --> 00:13:06,000
This is why independent review boards

254
00:13:06,000 --> 00:13:08,000
are very important in ethics as well.

255
00:13:08,000 --> 00:13:10,000
And so they did it and guess what?

256
00:13:10,000 --> 00:13:12,000
600,000 users were enrolled

257
00:13:12,000 --> 00:13:14,000
in that study without their knowing.

258
00:13:14,000 --> 00:13:16,000
If you see sadder things on your timeline,

259
00:13:16,000 --> 00:13:19,000
you post sadder things was the result.

260
00:13:19,000 --> 00:13:21,000
So like I said, there's some real ethical considerations

261
00:13:21,000 --> 00:13:22,000
but we know this.

262
00:13:22,000 --> 00:13:25,000
We know that if we're always bombarded by crisis,

263
00:13:25,000 --> 00:13:27,000
we will be in a crisis mentality.

264
00:13:27,000 --> 00:13:29,000
We know if we surround ourselves with positive people

265
00:13:29,000 --> 00:13:31,000
and positive messages,

266
00:13:31,000 --> 00:13:33,000
we will have a positive mentality.

267
00:13:33,000 --> 00:13:36,000
And it's very insidious and not insidious.

268
00:13:36,000 --> 00:13:38,000
It's kind of almost passive the way we absorb it.

269
00:13:38,000 --> 00:13:40,000
I love one of the facts.

270
00:13:40,000 --> 00:13:44,000
I'm writing one of my next books on longevity practices

271
00:13:44,000 --> 00:13:49,000
and a study on the order of 20,000 individuals.

272
00:13:49,000 --> 00:13:53,000
Those who had an optimistic mindset

273
00:13:53,000 --> 00:13:55,000
lived in average of 17% longer.

274
00:13:55,000 --> 00:13:58,000
I mean, just your mindset shift.

275
00:13:58,000 --> 00:13:59,000
17%.

276
00:13:59,000 --> 00:14:01,000
This was true both in men and in women,

277
00:14:01,000 --> 00:14:03,000
slightly more in women.

278
00:14:03,000 --> 00:14:05,000
And so how you think impacts everything

279
00:14:05,000 --> 00:14:09,000
and how you think is to a large degree

280
00:14:09,000 --> 00:14:12,000
going to be shaped by the media

281
00:14:12,000 --> 00:14:14,000
and AI is going to shape that.

282
00:14:14,000 --> 00:14:17,000
So it's a powerful lever

283
00:14:17,000 --> 00:14:19,000
that we all need to be paying attention to.

284
00:14:19,000 --> 00:14:20,000
I think it is.

285
00:14:20,000 --> 00:14:22,000
But then you have to consider

286
00:14:22,000 --> 00:14:26,000
what is the plasticity, for example, of our children

287
00:14:26,000 --> 00:14:28,000
as they grow up?

288
00:14:28,000 --> 00:14:30,000
We're going to have nanny AIs.

289
00:14:30,000 --> 00:14:32,000
What's that nanny going to teach?

290
00:14:32,000 --> 00:14:35,000
Is that nanny going to teach flight, happiness, this, that?

291
00:14:35,000 --> 00:14:37,000
What about in places like China?

292
00:14:37,000 --> 00:14:39,000
What are they going to teach?

293
00:14:39,000 --> 00:14:41,000
There is a huge amount of neuroplasticity

294
00:14:41,000 --> 00:14:45,000
that will be influenced by decisions we make today.

295
00:14:45,000 --> 00:14:46,000
Yeah.

296
00:14:46,000 --> 00:14:48,000
I mean, listen, you have young kids.

297
00:14:48,000 --> 00:14:49,000
I have young kids as well.

298
00:14:49,000 --> 00:14:51,000
And I think about the fact that school today

299
00:14:51,000 --> 00:14:55,000
is not preparing our kids anywhere near for the future.

300
00:14:55,000 --> 00:14:56,000
Right?

301
00:14:56,000 --> 00:14:59,000
I mean, I don't think middle school and high school

302
00:14:59,000 --> 00:15:02,000
traditionally is preparing them for...

303
00:15:02,000 --> 00:15:04,000
My kids are 12 right now.

304
00:15:04,000 --> 00:15:05,000
Yeah.

305
00:15:05,000 --> 00:15:06,000
How do you feel about that?

306
00:15:06,000 --> 00:15:08,000
No, I mean, I think school is not fit for purpose.

307
00:15:08,000 --> 00:15:10,000
It's a Petri dish social status game

308
00:15:10,000 --> 00:15:13,000
and, you know, like childcare.

309
00:15:13,000 --> 00:15:15,000
Because again, let's think about school.

310
00:15:15,000 --> 00:15:16,000
What do you...

311
00:15:16,000 --> 00:15:17,000
What are you taught at school?

312
00:15:17,000 --> 00:15:18,000
You're taught competitive tests

313
00:15:18,000 --> 00:15:20,000
because we can't capture the context of the kids.

314
00:15:20,000 --> 00:15:21,000
Right.

315
00:15:21,000 --> 00:15:23,000
We can't adapt to if they're visual learners,

316
00:15:23,000 --> 00:15:26,000
auditory learners, dyslexic or otherwise.

317
00:15:26,000 --> 00:15:28,000
And it narrows it down

318
00:15:28,000 --> 00:15:30,000
and you're told you cannot be creative.

319
00:15:30,000 --> 00:15:31,000
You're told you...

320
00:15:31,000 --> 00:15:33,000
Color inside the lines.

321
00:15:33,000 --> 00:15:34,000
Learn these facts.

322
00:15:34,000 --> 00:15:35,000
Literally, you color inside the lines.

323
00:15:35,000 --> 00:15:37,000
You learn these facts.

324
00:15:37,000 --> 00:15:40,000
Every child will have an AI in the West.

325
00:15:40,000 --> 00:15:41,000
Yes.

326
00:15:41,000 --> 00:15:42,000
Hopefully soon in the world.

327
00:15:42,000 --> 00:15:44,000
We'll have an AI with them as they grow up.

328
00:15:44,000 --> 00:15:48,000
And again, is that AI a positive constructive

329
00:15:48,000 --> 00:15:51,000
or is the AI a tiger AI?

330
00:15:51,000 --> 00:15:52,000
Right.

331
00:15:52,000 --> 00:15:53,000
That kind of is aggressive.

332
00:15:53,000 --> 00:15:54,000
Get your work done.

333
00:15:54,000 --> 00:15:55,000
Get your work done.

334
00:15:55,000 --> 00:15:56,000
Strive harder.

335
00:15:56,000 --> 00:15:58,000
Is it palaton AI for education?

336
00:15:58,000 --> 00:16:00,000
Maybe that's the pivot for palaton, right?

337
00:16:00,000 --> 00:16:01,000
There's a whole range of things,

338
00:16:01,000 --> 00:16:03,000
but our kids are so sensitive as they grow.

339
00:16:03,000 --> 00:16:05,000
And again, in a school environment,

340
00:16:05,000 --> 00:16:07,000
they're told they have to be competitive

341
00:16:07,000 --> 00:16:08,000
and there's only a few people

342
00:16:08,000 --> 00:16:10,000
that are worthy here at the top.

343
00:16:10,000 --> 00:16:12,000
And that's why you have clicks, sub-clicks, and others,

344
00:16:12,000 --> 00:16:15,000
and that's reinforced by our social media now as well

345
00:16:15,000 --> 00:16:17,000
because you need something to fill the meaning.

346
00:16:17,000 --> 00:16:19,000
I think we have to be much more intentional and think,

347
00:16:19,000 --> 00:16:22,000
what information do we want going to our children?

348
00:16:22,000 --> 00:16:24,000
Like many people listening to this podcast

349
00:16:24,000 --> 00:16:27,000
will have banned social media from our kids.

350
00:16:27,000 --> 00:16:29,000
How do you feel about that?

351
00:16:29,000 --> 00:16:31,000
I think that is probably a sensible thing

352
00:16:31,000 --> 00:16:35,000
because it's a slow-down AI that optimizes for adverse things.

353
00:16:35,000 --> 00:16:38,000
And again, it's not the fault of the social media companies,

354
00:16:38,000 --> 00:16:42,000
it's just how they are as the tiger and the scorpion.

355
00:16:42,000 --> 00:16:43,000
Yeah.

356
00:16:43,000 --> 00:16:45,000
I mean, I have not allowed my kids to have a mobile phone

357
00:16:45,000 --> 00:16:47,000
and I've told them when they can afford it,

358
00:16:47,000 --> 00:16:48,000
when they go to college,

359
00:16:48,000 --> 00:16:50,000
but it's going to be somewhere between now and then.

360
00:16:50,000 --> 00:16:51,000
But I agree.

361
00:16:51,000 --> 00:16:54,000
I think social media shouldn't be part of the repertoire.

362
00:16:54,000 --> 00:16:56,000
But again, what is social media?

363
00:16:56,000 --> 00:16:58,000
It's kids looking for status

364
00:16:58,000 --> 00:17:01,000
and trying to influence each other in that case.

365
00:17:01,000 --> 00:17:04,000
It was meant to bring our communities together stronger.

366
00:17:04,000 --> 00:17:06,000
Yeah, maybe perhaps early on.

367
00:17:07,000 --> 00:17:09,000
Probably what we see the strongest community is actually

368
00:17:09,000 --> 00:17:12,000
in video games and guilds and kind of things like that.

369
00:17:12,000 --> 00:17:14,000
A lot of this is, again,

370
00:17:14,000 --> 00:17:16,000
you've got X number of people posting positive things

371
00:17:16,000 --> 00:17:19,000
and you're like, why is my life not positive like that?

372
00:17:19,000 --> 00:17:21,000
But social media does have its advantages.

373
00:17:21,000 --> 00:17:24,000
The question is, can you tease out the positive versus the negative

374
00:17:24,000 --> 00:17:27,000
when you can finally customize it for each individual

375
00:17:27,000 --> 00:17:30,000
or are you going to reinforce silos to the nth degree?

376
00:17:30,000 --> 00:17:32,000
I'll give you a perfect example.

377
00:17:32,000 --> 00:17:34,000
I was just meeting with a dear friend of mine, Keith Farazi,

378
00:17:34,000 --> 00:17:36,000
who is absolutely brilliant.

379
00:17:36,000 --> 00:17:39,000
And he had met last week with the King of Bhutan,

380
00:17:39,000 --> 00:17:42,000
which is known for its happiness.

381
00:17:42,000 --> 00:17:43,000
And they were having a conversation.

382
00:17:43,000 --> 00:17:44,000
How they measure their economy.

383
00:17:44,000 --> 00:17:46,000
It's how they measure their economy,

384
00:17:46,000 --> 00:17:48,000
gross national happiness in that regard.

385
00:17:48,000 --> 00:17:53,000
And when social media entered the country,

386
00:17:53,000 --> 00:17:55,000
it began to plummet.

387
00:17:55,000 --> 00:17:59,000
Teen depression and suicides began to climb.

388
00:17:59,000 --> 00:18:02,000
It is a very measurable real thing.

389
00:18:02,000 --> 00:18:05,000
And that's not the subject of this podcast,

390
00:18:05,000 --> 00:18:08,000
but AI can do what for that area?

391
00:18:08,000 --> 00:18:10,000
Well, I mean, again,

392
00:18:10,000 --> 00:18:13,000
we have to think about it in terms of mental infrastructure.

393
00:18:13,000 --> 00:18:14,000
I like that.

394
00:18:14,000 --> 00:18:18,000
We don't have enough, like Clayton Christensen said,

395
00:18:18,000 --> 00:18:20,000
infrastructure is the most efficient means

396
00:18:20,000 --> 00:18:23,000
by which a society stores and distributes value.

397
00:18:23,000 --> 00:18:25,000
Claude Shannon, the father of computer science said,

398
00:18:25,000 --> 00:18:28,000
information is valuable in as much as it changes the state.

399
00:18:28,000 --> 00:18:32,000
We do not think at all about our mental infrastructure

400
00:18:32,000 --> 00:18:34,000
and what's supporting it.

401
00:18:34,000 --> 00:18:37,000
If we're lucky or if, you know, we try hard,

402
00:18:37,000 --> 00:18:40,000
we can build a group of supportive people around us.

403
00:18:40,000 --> 00:18:42,000
And where do we go when we have issues?

404
00:18:42,000 --> 00:18:43,000
We go to that group.

405
00:18:43,000 --> 00:18:46,000
Yet so many people feel alone, you know,

406
00:18:46,000 --> 00:18:49,000
so many people feel like, again, this rise in suicide

407
00:18:49,000 --> 00:18:51,000
or they feel not good enough

408
00:18:51,000 --> 00:18:54,000
because it serves the slow, dumb AI of our existing systems.

409
00:18:54,000 --> 00:18:58,000
So I think that, actually, we just take some time out to think.

410
00:18:58,000 --> 00:19:02,000
What information do I want going to my kids?

411
00:19:02,000 --> 00:19:05,000
We have concepts like deliberative democracy,

412
00:19:05,000 --> 00:19:07,000
whereby you get a group of diverse people

413
00:19:07,000 --> 00:19:10,000
from different backgrounds, you give them the facts,

414
00:19:10,000 --> 00:19:13,000
and they go and make a decision, just like you have jury trials.

415
00:19:13,000 --> 00:19:16,000
You know, one of the most important things I think there is,

416
00:19:16,000 --> 00:19:19,000
A, it's getting understanding the context of each person,

417
00:19:19,000 --> 00:19:20,000
which I think AI can answer it.

418
00:19:20,000 --> 00:19:23,000
B, it's just actually literally having time to think.

419
00:19:23,000 --> 00:19:26,000
When was the last time you thought about your information diet

420
00:19:26,000 --> 00:19:28,000
and what you're feeding yourself and your kids?

421
00:19:28,000 --> 00:19:29,000
I think about it a lot.

422
00:19:29,000 --> 00:19:31,000
I think because that's what I teach.

423
00:19:31,000 --> 00:19:32,000
So I'm very clear.

424
00:19:32,000 --> 00:19:34,000
I do not watch the TV news.

425
00:19:34,000 --> 00:19:35,000
I don't even watch the newspapers.

426
00:19:35,000 --> 00:19:38,000
I have very filtered information that comes to me,

427
00:19:38,000 --> 00:19:41,000
which could be argued to be an echo chamber,

428
00:19:41,000 --> 00:19:46,000
but you know, I'm focused on these are the scientific breakthroughs.

429
00:19:46,000 --> 00:19:47,000
This is what's going on in longevity.

430
00:19:47,000 --> 00:19:50,000
This what's going on in exponential tech and solving problems.

431
00:19:50,000 --> 00:19:55,000
So I'm as critical about what I take into my mind

432
00:19:55,000 --> 00:20:00,000
from an information source as I am what I eat.

433
00:20:00,000 --> 00:20:02,000
Because you are what you eat and you eat information

434
00:20:02,000 --> 00:20:04,000
and then you absorb it, right?

435
00:20:04,000 --> 00:20:07,000
But then, you know, as you said, the echo chamber thing,

436
00:20:07,000 --> 00:20:12,000
I believe we should also deliberately show counterpart viewpoints

437
00:20:12,000 --> 00:20:16,000
as we're raising our kids and get them to argue the opposite.

438
00:20:16,000 --> 00:20:18,000
I think debate is one of the most beautiful forms,

439
00:20:18,000 --> 00:20:19,000
especially when you flip sides.

440
00:20:19,000 --> 00:20:23,000
Organisms also grow through hysteresis.

441
00:20:23,000 --> 00:20:24,000
Yes.

442
00:20:24,000 --> 00:20:26,000
You know, when you're put under pressure,

443
00:20:26,000 --> 00:20:28,000
when you're forced to do something out of the normal.

444
00:20:28,000 --> 00:20:31,000
Otherwise, as you said, you'll become increasingly siloed.

445
00:20:31,000 --> 00:20:35,000
But there are very few people again who think deliberately about this.

446
00:20:35,000 --> 00:20:37,000
And it's something, again, I think you and I

447
00:20:37,000 --> 00:20:40,000
probably urge all the listeners to think about,

448
00:20:40,000 --> 00:20:42,000
are you challenging your priors?

449
00:20:42,000 --> 00:20:46,000
Are you giving the right information diet for yourself, for your kids?

450
00:20:46,000 --> 00:20:47,000
Yes.

451
00:20:47,000 --> 00:20:49,000
And then thinking about this technology,

452
00:20:49,000 --> 00:20:52,000
as you use a GPT-4 or Claude or Stable-LM,

453
00:20:52,000 --> 00:20:54,000
how any of these things,

454
00:20:54,000 --> 00:20:57,000
what if you took the article and viewed it from a different perspective?

455
00:20:57,000 --> 00:21:00,000
Or what if you tied it to only be positive, the news?

456
00:21:00,000 --> 00:21:04,000
There's another part too, which is we all have these cognitive biases, right?

457
00:21:04,000 --> 00:21:06,000
These cognitive biases were wired into our brain

458
00:21:06,000 --> 00:21:09,000
over the last, you know, hundreds of thousands of years

459
00:21:09,000 --> 00:21:11,000
as an energy efficiency mechanism

460
00:21:11,000 --> 00:21:14,000
because we can't process all the information coming in.

461
00:21:14,000 --> 00:21:18,000
So we're biased by, is the person look like me, speak like me?

462
00:21:18,000 --> 00:21:21,000
Is this recent information versus old information

463
00:21:21,000 --> 00:21:25,000
paying 10 times more attention to negative information than positive information?

464
00:21:25,000 --> 00:21:31,000
I can't wait to have an AI that I can flip the switch

465
00:21:31,000 --> 00:21:34,000
and say turn on bias notifications.

466
00:21:34,000 --> 00:21:38,000
And it says you're looking at this in a biased fashion, Peter.

467
00:21:38,000 --> 00:21:40,000
Here's another way to look at it.

468
00:21:40,000 --> 00:21:42,000
Yeah. And, you know, being aware of your bias,

469
00:21:42,000 --> 00:21:45,000
most religions have at the core know thyself.

470
00:21:45,000 --> 00:21:47,000
Yes. The nurses have done,

471
00:21:47,000 --> 00:21:50,000
it's the ancient Greek, you know, I wrote my college essay on that.

472
00:21:50,000 --> 00:21:52,000
But I mean, that's why it's at the core.

473
00:21:52,000 --> 00:21:56,000
It's very difficult when you have the detritus of life

474
00:21:56,000 --> 00:21:59,000
and all these things you're bombarded with to take time back

475
00:21:59,000 --> 00:22:02,000
and really know yourself, know your own biases, understand these,

476
00:22:02,000 --> 00:22:05,000
because they are part of what makes you, you're made up of the stories.

477
00:22:05,000 --> 00:22:09,000
There is no kind of pure, independent, completely rational person

478
00:22:09,000 --> 00:22:11,000
because we're not robots.

479
00:22:11,000 --> 00:22:14,000
So it's possible in the future then for social media

480
00:22:14,000 --> 00:22:19,000
with a more conscious, powerful AI,

481
00:22:19,000 --> 00:22:21,000
I shouldn't use the word conscious as a different meaning here,

482
00:22:21,000 --> 00:22:25,000
but AI that you feel safe having your kids do it

483
00:22:25,000 --> 00:22:28,000
because it is making them happier and making them more motivated.

484
00:22:28,000 --> 00:22:31,000
It is feeding them a flow of information

485
00:22:31,000 --> 00:22:33,000
that's uplifting versus depressing.

486
00:22:33,000 --> 00:22:35,000
Can you imagine that future?

487
00:22:35,000 --> 00:22:36,000
I can imagine that future.

488
00:22:36,000 --> 00:22:39,000
I can also imagine the future of Brave New World

489
00:22:40,000 --> 00:22:43,000
whereby you are fed what exactly the government wants you to

490
00:22:43,000 --> 00:22:46,000
and you are happy, and especially with authoritarian regimes,

491
00:22:46,000 --> 00:22:50,000
you are literally the kids are grown with their AI nannies.

492
00:22:50,000 --> 00:22:51,000
Of course.

493
00:22:51,000 --> 00:22:54,000
And you even have the pharmaceuticals to make you extra happy

494
00:22:54,000 --> 00:22:56,000
and extra neuroplastic.

495
00:22:56,000 --> 00:23:01,000
So for example, you have UAE did a Falcon model open source.

496
00:23:01,000 --> 00:23:05,000
It was kind of supported by Leiton from France, technologically.

497
00:23:05,000 --> 00:23:08,000
You ask it about the UAE and it's like it's a wonderful place.

498
00:23:08,000 --> 00:23:10,000
It's amazing in all regards.

499
00:23:10,000 --> 00:23:12,000
You ask it about some of the neighbors.

500
00:23:12,000 --> 00:23:13,000
It's not so nice.

501
00:23:13,000 --> 00:23:15,000
This is an inherent bias within the model,

502
00:23:15,000 --> 00:23:18,000
but how you can understand it versus an implicit bias

503
00:23:18,000 --> 00:23:20,000
and you can put any bias as you want.

504
00:23:20,000 --> 00:23:22,000
You can guide these models through reinforcement learning

505
00:23:22,000 --> 00:23:24,000
to reflect what you do.

506
00:23:24,000 --> 00:23:26,000
And if that's the only option,

507
00:23:26,000 --> 00:23:29,000
then you will adhere to a certain world view again,

508
00:23:29,000 --> 00:23:30,000
almost subconsciously.

509
00:23:30,000 --> 00:23:33,000
It'll be reflected in all the products you produce

510
00:23:33,000 --> 00:23:35,000
all the writings you have.

511
00:23:35,000 --> 00:23:39,000
And it doesn't have to be that higher percentage change,

512
00:23:39,000 --> 00:23:43,000
a small persistent change sways a lot.

513
00:23:43,000 --> 00:23:44,000
Well, exactly.

514
00:23:44,000 --> 00:23:46,000
I mean, like half the world is religious.

515
00:23:46,000 --> 00:23:48,000
You can agree or not,

516
00:23:48,000 --> 00:23:50,000
or say it follows a organized religion.

517
00:23:50,000 --> 00:23:52,000
You can agree or disagree,

518
00:23:52,000 --> 00:23:56,000
but I can tell you that almost every single technologist

519
00:23:56,000 --> 00:23:58,000
who is leading a lot of these is like,

520
00:23:58,000 --> 00:24:00,000
I don't really like religion, right?

521
00:24:00,000 --> 00:24:03,000
And so the inherent bias would be to talk against religious

522
00:24:03,000 --> 00:24:04,000
kind of things.

523
00:24:04,000 --> 00:24:07,000
Again, I'm like, who am I to judge?

524
00:24:07,000 --> 00:24:08,000
People can be, they cannot be,

525
00:24:08,000 --> 00:24:11,000
but the inherent bias is refactor.

526
00:24:11,000 --> 00:24:13,000
And actually it becomes very important for society

527
00:24:13,000 --> 00:24:15,000
because we've seen that when about 12% of a population

528
00:24:15,000 --> 00:24:18,000
changes at point of view, it flips.

529
00:24:18,000 --> 00:24:19,000
Interesting.

530
00:24:19,000 --> 00:24:20,000
It doesn't take that much.

531
00:24:20,000 --> 00:24:21,000
It doesn't take that much

532
00:24:21,000 --> 00:24:23,000
because you listen to the voices in echo chamber.

533
00:24:23,000 --> 00:24:25,000
Like sometimes on Twitter, you know,

534
00:24:25,000 --> 00:24:27,000
I use my block button a lot.

535
00:24:27,000 --> 00:24:28,000
I'm like, you know.

536
00:24:28,000 --> 00:24:30,000
Listen, I've been enjoying your tweets.

537
00:24:30,000 --> 00:24:31,000
They've been really good.

538
00:24:31,000 --> 00:24:33,000
And I appreciate the frequency.

539
00:24:33,000 --> 00:24:34,000
Well, you know,

540
00:24:34,000 --> 00:24:37,000
it's nice owning your own media channel in a way, right?

541
00:24:37,000 --> 00:24:38,000
Sometimes I don't even have to have lunch

542
00:24:38,000 --> 00:24:41,000
because I'm told to eat crap so many times a day, right?

543
00:24:41,000 --> 00:24:43,000
That's why you have to hit the block button

544
00:24:43,000 --> 00:24:44,000
because it's a little echo chamber

545
00:24:44,000 --> 00:24:48,000
and a few dozen people can have that impact upon you.

546
00:24:48,000 --> 00:24:51,000
I mean, it's going to be very interesting

547
00:24:51,000 --> 00:24:53,000
to see the way that our adult minds

548
00:24:53,000 --> 00:24:56,000
and our kids minds evolve over the next five to 10 years

549
00:24:56,000 --> 00:24:58,000
with the emergence of this new, more powerful,

550
00:24:58,000 --> 00:25:00,000
personalizable technology.

551
00:25:00,000 --> 00:25:01,000
Yes.

552
00:25:01,000 --> 00:25:04,000
It's either controlled or controlled.

553
00:25:04,000 --> 00:25:05,000
Everybody, it's Peter.

554
00:25:05,000 --> 00:25:07,000
I want to take a break from our episode

555
00:25:07,000 --> 00:25:10,000
to talk about a health product that I love.

556
00:25:10,000 --> 00:25:11,000
It was a few years ago.

557
00:25:11,000 --> 00:25:15,000
I went looking for the best nutritional green drink on the market.

558
00:25:15,000 --> 00:25:18,000
So I went to Whole Foods and I started looking around.

559
00:25:18,000 --> 00:25:21,000
I found three shelves filled with options.

560
00:25:21,000 --> 00:25:24,000
I looked at the labels and they really didn't wow me.

561
00:25:24,000 --> 00:25:28,000
So I picked the top three that I thought looked decent,

562
00:25:28,000 --> 00:25:30,000
brought them home, tried them, and they sucked.

563
00:25:30,000 --> 00:25:32,000
First of all, they tasted awful.

564
00:25:32,000 --> 00:25:34,000
And then second, nutritional facts

565
00:25:34,000 --> 00:25:36,000
actually weren't very impressive.

566
00:25:36,000 --> 00:25:40,000
It was then that I found a product called AG1 by Athletic Greens.

567
00:25:40,000 --> 00:25:42,000
And without any question,

568
00:25:42,000 --> 00:25:46,000
Athletic Greens is the best on the market by a huge margin.

569
00:25:46,000 --> 00:25:49,000
First of all, it actually tastes amazing.

570
00:25:49,000 --> 00:25:52,000
And second, if you look at the ingredients,

571
00:25:52,000 --> 00:25:54,000
75 high quality ingredients

572
00:25:54,000 --> 00:25:57,000
that deliver nutrient-dense, antioxidants,

573
00:25:57,000 --> 00:26:01,000
multivitamins, pre- and probiotics, immune support, adaptogens.

574
00:26:01,000 --> 00:26:04,000
I personally utilize AG1 literally every day.

575
00:26:04,000 --> 00:26:07,000
I travel with an individual packet in my backpack,

576
00:26:07,000 --> 00:26:09,000
sometimes in my back pocket,

577
00:26:09,000 --> 00:26:13,000
and I count on it for gut health, immunity, energy,

578
00:26:13,000 --> 00:26:16,000
digestion, neural support, and really healthy aging.

579
00:26:16,000 --> 00:26:19,000
So if you want to take ownership of your health,

580
00:26:19,000 --> 00:26:21,000
today is a good day to start.

581
00:26:21,000 --> 00:26:24,000
Athletic Greens is giving you a free one-year supply of vitamin D

582
00:26:24,000 --> 00:26:28,000
and five of these travel packs with your first purchase.

583
00:26:28,000 --> 00:26:31,000
So go to athleticgreens.com backslash moonshots.

584
00:26:31,000 --> 00:26:35,000
That's athleticgreens.com backslash moonshots.

585
00:26:35,000 --> 00:26:37,000
Check it out. You'll thank me.

586
00:26:37,000 --> 00:26:40,000
Without question, this is the best green drink product,

587
00:26:40,000 --> 00:26:43,000
the most nutritious, the most flavorful I've found.

588
00:26:43,000 --> 00:26:45,000
All right, let's go back to the episode.

589
00:26:45,000 --> 00:26:47,000
So we're sitting here in the middle of Hollywood.

590
00:26:47,000 --> 00:26:49,000
I can see the beautiful Hollywood Hills.

591
00:26:49,000 --> 00:26:51,000
The Hollywood signs around here are someplace.

592
00:26:51,000 --> 00:26:53,000
Just over there, okay.

593
00:26:53,000 --> 00:26:57,000
And the news today is Green Actors Guild.

594
00:26:57,000 --> 00:26:59,000
Everyone's gone on strike.

595
00:26:59,000 --> 00:27:04,000
There is great fear, pain, concern.

596
00:27:04,000 --> 00:27:09,000
And, you know, you called it when we spoke six months ago.

597
00:27:09,000 --> 00:27:13,000
What's going on? How do you view it?

598
00:27:13,000 --> 00:27:15,000
And where's it going?

599
00:27:15,000 --> 00:27:18,000
So I think the advances in media or artificial intelligence

600
00:27:18,000 --> 00:27:19,000
have been huge.

601
00:27:19,000 --> 00:27:23,000
You've now got the Drake AI song or my favorite Ice Matrix,

602
00:27:23,000 --> 00:27:26,000
where the Matrix characters sing Ice Ice Baby.

603
00:27:26,000 --> 00:27:29,000
Yes.

604
00:27:29,000 --> 00:27:31,000
You've got real-time rigging.

605
00:27:31,000 --> 00:27:33,000
You have real-time special effects.

606
00:27:33,000 --> 00:27:36,000
You've got high definition creation of anything.

607
00:27:36,000 --> 00:27:37,000
What does that mean?

608
00:27:37,000 --> 00:27:40,000
It means that the whole industry is about to be disrupted

609
00:27:40,000 --> 00:27:44,000
because the cost of production reduces.

610
00:27:44,000 --> 00:27:46,000
It was reducing in some ways anyway.

611
00:27:46,000 --> 00:27:49,000
And we've seen this move where the cost of consumption

612
00:27:49,000 --> 00:27:51,000
went to zero with Napster and Spotify.

613
00:27:51,000 --> 00:27:53,000
Then the cost of creation started going to zero

614
00:27:53,000 --> 00:27:56,000
with Snapchat and TikTok.

615
00:27:56,000 --> 00:28:00,000
And now we have a question of what are the defaults going to be now?

616
00:28:00,000 --> 00:28:05,000
And consumers only have one limited currency

617
00:28:05,000 --> 00:28:06,000
and that's their attention.

618
00:28:06,000 --> 00:28:07,000
I think so.

619
00:28:07,000 --> 00:28:09,000
But consumers are willing to pay for attention.

620
00:28:09,000 --> 00:28:11,000
They're willing to pay for quality.

621
00:28:11,000 --> 00:28:14,000
Well, either premium media or, you know, otherwise.

622
00:28:14,000 --> 00:28:17,000
Because video games, for example,

623
00:28:17,000 --> 00:28:20,000
started as a $70 billion industry 10 years ago.

624
00:28:20,000 --> 00:28:24,000
The average score went from 69% to 74% on Metacritic

625
00:28:24,000 --> 00:28:25,000
over that period.

626
00:28:25,000 --> 00:28:28,000
And now they're $180 billion.

627
00:28:28,000 --> 00:28:29,000
Movies...

628
00:28:29,000 --> 00:28:31,000
I pay for some of that with my 12-year-olds.

629
00:28:31,000 --> 00:28:32,000
Yeah.

630
00:28:32,000 --> 00:28:36,000
Movies went from $40 billion to $50 billion,

631
00:28:36,000 --> 00:28:39,000
but the average IMDb movie rating of the last 10 years

632
00:28:39,000 --> 00:28:40,000
has been 6.4.

633
00:28:40,000 --> 00:28:42,000
It has not changed.

634
00:28:42,000 --> 00:28:45,000
So you're not producing more and so they're not consuming more.

635
00:28:45,000 --> 00:28:46,000
So there's a question.

636
00:28:46,000 --> 00:28:49,000
Will this technology enable an increase in quality

637
00:28:49,000 --> 00:28:51,000
because it raises the bar for everyone?

638
00:28:51,000 --> 00:28:52,000
Yes.

639
00:28:52,000 --> 00:28:53,000
There are more movie makers,

640
00:28:53,000 --> 00:28:55,000
so they're more excellent movie makers,

641
00:28:55,000 --> 00:28:57,000
and the best movie makers have become even more excellent.

642
00:28:57,000 --> 00:29:00,000
And we've democratized the tools from an iPhone

643
00:29:00,000 --> 00:29:05,000
to, you know, the tools on your Mac.

644
00:29:05,000 --> 00:29:09,000
And, of course, YouTube was the first major disruption of the...

645
00:29:09,000 --> 00:29:10,000
It was.

646
00:29:10,000 --> 00:29:13,000
Something like MrBeast has a much bigger audience than CNN now.

647
00:29:13,000 --> 00:29:14,000
Yeah.

648
00:29:14,000 --> 00:29:16,000
But that comes down to what is the key point here?

649
00:29:16,000 --> 00:29:18,000
Distribution.

650
00:29:18,000 --> 00:29:20,000
You can make good stories, but if no one hears them...

651
00:29:20,000 --> 00:29:21,000
Right.

652
00:29:21,000 --> 00:29:25,000
And that's what Disney has as its benefited distribution

653
00:29:25,000 --> 00:29:30,000
to its theme parks, to its products, to its channels.

654
00:29:30,000 --> 00:29:31,000
It creates a shelling point,

655
00:29:31,000 --> 00:29:34,000
but then if we all have our own individualized AIs,

656
00:29:34,000 --> 00:29:38,000
they can find us what we need and sift through the crowd.

657
00:29:38,000 --> 00:29:41,000
Maybe a whole of distribution flips on its head in five years as well.

658
00:29:41,000 --> 00:29:42,000
So I can't imagine...

659
00:29:42,000 --> 00:29:47,000
You know, I keep thinking that the future of movie consumption

660
00:29:47,000 --> 00:29:51,000
is me speaking to my version of Jarvis and saying,

661
00:29:51,000 --> 00:29:54,000
you know, I'd like a comedy, I'd like it starring me

662
00:29:54,000 --> 00:29:58,000
and three friends of mine and somebody else

663
00:29:58,000 --> 00:30:03,000
and for 90 minutes and set it in, you know, some setting and go.

664
00:30:03,000 --> 00:30:07,000
And have it auto-generate a compelling story

665
00:30:07,000 --> 00:30:15,000
that is either because I'm involved in it,

666
00:30:15,000 --> 00:30:17,000
I'm enraptured in it,

667
00:30:17,000 --> 00:30:20,000
or because my favorite stars are all together in an unlikely place.

668
00:30:20,000 --> 00:30:23,000
How far are we from that kind of future?

669
00:30:23,000 --> 00:30:25,000
I'd say probably three to five years.

670
00:30:25,000 --> 00:30:29,000
So in that case, it's not going to be cheap, but it'll be there.

671
00:30:29,000 --> 00:30:30,000
And then it'll get cheaper and cheaper.

672
00:30:30,000 --> 00:30:32,000
In that case, there is no distribution needed.

673
00:30:32,000 --> 00:30:34,000
A person calls up whatever they want.

674
00:30:34,000 --> 00:30:37,000
I think there is distribution needed, but in a different way.

675
00:30:37,000 --> 00:30:41,000
Music, we have a variety of different music sites,

676
00:30:41,000 --> 00:30:43,000
but how do musicians make their money now?

677
00:30:43,000 --> 00:30:44,000
It's not Spotify.

678
00:30:44,000 --> 00:30:46,000
A million views gets you a few thousand dollars.

679
00:30:46,000 --> 00:30:49,000
T-shirts, merchandising, global stories,

680
00:30:49,000 --> 00:30:53,000
because there is that future of the Wally type of fat guy

681
00:30:53,000 --> 00:30:55,000
sitting with his VR headset.

682
00:30:55,000 --> 00:30:56,000
It's kind of depressing.

683
00:30:56,000 --> 00:30:58,000
Just like, you know, the Apple Vision Pro adverts,

684
00:30:58,000 --> 00:31:00,000
I found kind of depressing when his kids are right there

685
00:31:00,000 --> 00:31:01,000
and just puts them on.

686
00:31:01,000 --> 00:31:04,000
Quite a bit dystopian to me.

687
00:31:04,000 --> 00:31:07,000
I think it's more a case of there are certain stories

688
00:31:07,000 --> 00:31:09,000
that everyone wants to talk about.

689
00:31:09,000 --> 00:31:11,000
Like here in Hollywood, what do we have this week?

690
00:31:11,000 --> 00:31:13,000
We have Oppenheimer and Barbie.

691
00:31:13,000 --> 00:31:15,000
Oh my God.

692
00:31:15,000 --> 00:31:16,000
Talking bags.

693
00:31:16,000 --> 00:31:18,000
I'm looking for an Oppenheimer.

694
00:31:18,000 --> 00:31:19,000
I will not watch Barbie.

695
00:31:19,000 --> 00:31:20,000
Sorry.

696
00:31:20,000 --> 00:31:21,000
Would you watch Barbieheimer?

697
00:31:21,000 --> 00:31:25,000
Only, no, I won't go there.

698
00:31:25,000 --> 00:31:27,000
Well, I mean, we can take it.

699
00:31:27,000 --> 00:31:29,000
We can put the scripts into Claude and see what it comes up with.

700
00:31:29,000 --> 00:31:30,000
That would be hilarious.

701
00:31:30,000 --> 00:31:32,000
Yeah, hilarious kind of Barbie.

702
00:31:32,000 --> 00:31:35,000
But again, like people are talking about the Barbie movie

703
00:31:35,000 --> 00:31:37,000
because it's, you know, not,

704
00:31:37,000 --> 00:31:39,000
she comes to the real world and she has challenges.

705
00:31:39,000 --> 00:31:40,000
It'll be a hit.

706
00:31:40,000 --> 00:31:44,000
People talk about Oppenheimer because again, it will be a hit.

707
00:31:44,000 --> 00:31:45,000
These are produced hits.

708
00:31:45,000 --> 00:31:46,000
These are produced hits.

709
00:31:46,000 --> 00:31:47,000
Yeah.

710
00:31:47,000 --> 00:31:49,000
Just like I saw BTS with my daughter.

711
00:31:49,000 --> 00:31:50,000
It's not BTS, they're Blackpink.

712
00:31:50,000 --> 00:31:52,000
Oh, gosh, she'll kill me.

713
00:31:52,000 --> 00:31:54,000
In Hyde Park a few weeks ago,

714
00:31:54,000 --> 00:31:56,000
the biggest K-pop band out of Korea.

715
00:31:56,000 --> 00:31:57,000
Right.

716
00:31:58,000 --> 00:32:00,000
Completely manufactured, but lots of fun.

717
00:32:00,000 --> 00:32:03,000
So on the one hand, what you have is what you described,

718
00:32:03,000 --> 00:32:05,000
your personalized things.

719
00:32:05,000 --> 00:32:07,000
That's kind of like McDonald's.

720
00:32:07,000 --> 00:32:08,000
What's the job to be done?

721
00:32:08,000 --> 00:32:10,000
The job is comfort.

722
00:32:10,000 --> 00:32:12,000
The job isn't to listen to someone else's story

723
00:32:12,000 --> 00:32:14,000
and expand from there.

724
00:32:14,000 --> 00:32:17,000
It isn't a produced Michelin star meal

725
00:32:17,000 --> 00:32:19,000
or just a nice restaurant.

726
00:32:19,000 --> 00:32:21,000
One that you can talk about to others like,

727
00:32:21,000 --> 00:32:23,000
you can cook ingredients yourself at home as well.

728
00:32:23,000 --> 00:32:24,000
Sure.

729
00:32:24,000 --> 00:32:26,000
But humans do like these bigger stories,

730
00:32:26,000 --> 00:32:30,000
but then the nature of funding of musicians changed.

731
00:32:30,000 --> 00:32:32,000
It was about merchandise.

732
00:32:32,000 --> 00:32:36,000
It was about kind of a lot of this other stuff

733
00:32:36,000 --> 00:32:39,000
around tours.

734
00:32:39,000 --> 00:32:41,000
And so I think the nature of movies might change.

735
00:32:41,000 --> 00:32:42,000
The business model has changed.

736
00:32:42,000 --> 00:32:44,000
I mean, that's the most interesting thing

737
00:32:44,000 --> 00:32:46,000
about exponential technology

738
00:32:46,000 --> 00:32:48,000
is it's changing the business models.

739
00:32:48,000 --> 00:32:52,000
And I keep on, you know, advising my entrepreneurs.

740
00:32:52,000 --> 00:32:55,000
It's reinvent your business models more than anything else.

741
00:32:55,000 --> 00:32:58,000
You have to always look across the landscape,

742
00:32:58,000 --> 00:33:02,000
where is the value peaks?

743
00:33:02,000 --> 00:33:04,000
And then you're sitting there

744
00:33:04,000 --> 00:33:06,000
and you're intermediating something

745
00:33:06,000 --> 00:33:08,000
and you're offering service value.

746
00:33:08,000 --> 00:33:10,000
You know, like there was that great quote,

747
00:33:10,000 --> 00:33:13,000
who is it from the CEO of Netscape?

748
00:33:13,000 --> 00:33:16,000
All values created by aggregation or disaggregation,

749
00:33:16,000 --> 00:33:18,000
bundling or unbundling.

750
00:33:18,000 --> 00:33:21,000
And you think about how the landscape is going to change now

751
00:33:21,000 --> 00:33:23,000
as intelligences move to the edge.

752
00:33:23,000 --> 00:33:26,000
What was once in the hands of the studios

753
00:33:26,000 --> 00:33:28,000
and the high priests of media

754
00:33:28,000 --> 00:33:30,000
suddenly gets pushed to the edge.

755
00:33:30,000 --> 00:33:32,000
Whereas value then, it changes it, it flips it.

756
00:33:32,000 --> 00:33:34,000
So let's go to what's going on right now.

757
00:33:34,000 --> 00:33:38,000
So the screen actors guild is on strike because of why?

758
00:33:38,000 --> 00:33:41,000
Screen actors, I mean, it's better wages in general,

759
00:33:41,000 --> 00:33:44,000
standard stuff, but now there's this rapidly emerging fear

760
00:33:44,000 --> 00:33:46,000
of artificial intelligence.

761
00:33:46,000 --> 00:33:50,000
So one of the proposals that came in from the other side

762
00:33:50,000 --> 00:33:54,000
was that basically all the extras,

763
00:33:54,000 --> 00:33:56,000
all the actors, they sign away their rights

764
00:33:56,000 --> 00:33:58,000
so they could be used in AI.

765
00:33:58,000 --> 00:34:01,000
So they get a day of wage to get scanned

766
00:34:01,000 --> 00:34:06,000
and then they can be used for the rest of that studio's life

767
00:34:06,000 --> 00:34:08,000
in producing background actors.

768
00:34:08,000 --> 00:34:10,000
And they were looking at this like,

769
00:34:10,000 --> 00:34:12,000
oh my God, wait, you can do that?

770
00:34:12,000 --> 00:34:15,000
People must have even realized you can do that.

771
00:34:15,000 --> 00:34:17,000
You know, script writers are saying

772
00:34:17,000 --> 00:34:19,000
no AI generated scripts,

773
00:34:19,000 --> 00:34:22,000
which again is a bit weird when they're all using Grammarly

774
00:34:22,000 --> 00:34:24,000
and things like that, which was AI.

775
00:34:24,000 --> 00:34:26,000
How are you ever going to tell?

776
00:34:26,000 --> 00:34:28,000
Where do you draw the line?

777
00:34:28,000 --> 00:34:29,000
Where do you draw the line?

778
00:34:29,000 --> 00:34:31,000
Because this is a technology that's coming so fast

779
00:34:31,000 --> 00:34:36,000
and is so good that it's almost not like technology at all.

780
00:34:36,000 --> 00:34:38,000
It's just very natural the way it emerges.

781
00:34:38,000 --> 00:34:41,000
And so you will get to some sort of agreement

782
00:34:41,000 --> 00:34:43,000
because the big actors are kind of there,

783
00:34:43,000 --> 00:34:46,000
but the defaults that are set now reverberate.

784
00:34:46,000 --> 00:34:49,000
So I think that's an important point that you just made.

785
00:34:49,000 --> 00:34:56,000
The decisions, the policies that we create today

786
00:34:56,000 --> 00:34:59,000
are going to take us down one path or another

787
00:34:59,000 --> 00:35:01,000
for the decades to come.

788
00:35:01,000 --> 00:35:03,000
Yeah, it will affect the whole of Hollywood,

789
00:35:03,000 --> 00:35:06,000
what's decided in this moment here,

790
00:35:06,000 --> 00:35:09,000
because there won't be another renegotiation for a while.

791
00:35:09,000 --> 00:35:13,000
And so again, how does it act to create value?

792
00:35:13,000 --> 00:35:16,000
A top actor has a following,

793
00:35:16,000 --> 00:35:18,000
but up-and-comers, how did they break through?

794
00:35:18,000 --> 00:35:20,000
What was the apprenticeship to him?

795
00:35:20,000 --> 00:35:22,000
What does a movie look like in five years?

796
00:35:22,000 --> 00:35:26,000
Even if you agree as Hollywood, not to have any AI.

797
00:35:26,000 --> 00:35:28,000
Let's just say you have this kind of

798
00:35:28,000 --> 00:35:30,000
Dune-style bit Larry and Jihad and say,

799
00:35:30,000 --> 00:35:32,000
No AI, you know?

800
00:35:32,000 --> 00:35:34,000
And someone will make a movie about no AI in Hollywood.

801
00:35:34,000 --> 00:35:37,000
What do you do when the Chinese film studios?

802
00:35:37,000 --> 00:35:39,000
Start releasing product.

803
00:35:39,000 --> 00:35:41,000
Start releasing product faster than anything.

804
00:35:41,000 --> 00:35:43,000
You can make five dreams out of it.

805
00:35:43,000 --> 00:35:45,000
In every language out there?

806
00:35:45,000 --> 00:35:46,000
Literally every language.

807
00:35:46,000 --> 00:35:47,000
We have technology now.

808
00:35:47,000 --> 00:35:49,000
And again, maybe this is part of what we're doing.

809
00:35:49,000 --> 00:35:50,000
What is possible now?

810
00:35:50,000 --> 00:35:53,000
We can translate Peter's voice into just about any language

811
00:35:53,000 --> 00:35:55,000
with his voice, so it's not a voice actor.

812
00:35:55,000 --> 00:35:57,000
And match my lips and movements, exactly.

813
00:35:57,000 --> 00:35:59,000
Match your lips and movements, exactly.

814
00:35:59,000 --> 00:36:02,000
I'm sure the podcast will be in every language by next year.

815
00:36:02,000 --> 00:36:04,000
And again, it will be in our voices.

816
00:36:04,000 --> 00:36:06,000
We can make our voices sound more confident.

817
00:36:06,000 --> 00:36:09,000
We can take his mannerisms right now

818
00:36:09,000 --> 00:36:11,000
and transplant them onto my mannerisms so we match,

819
00:36:11,000 --> 00:36:14,000
so we can reshoot scenes with style.

820
00:36:14,000 --> 00:36:15,000
Yeah.

821
00:36:15,000 --> 00:36:17,000
We can turn him into a robot in a few minutes.

822
00:36:17,000 --> 00:36:20,000
And in fact, maybe we'll do that in the film of the post.

823
00:36:20,000 --> 00:36:23,000
And so we'll have that scene of him becoming a robot.

824
00:36:23,000 --> 00:36:25,000
These are all technologies that are here now,

825
00:36:25,000 --> 00:36:27,000
and it transforms fundamentally the nature of filmmaking

826
00:36:27,000 --> 00:36:29,000
because you only need one shot.

827
00:36:29,000 --> 00:36:33,000
But don't the film actors who are, you know,

828
00:36:33,000 --> 00:36:37,000
standing up for their rights to not have them

829
00:36:37,000 --> 00:36:41,000
basically demonetized and digitized,

830
00:36:41,000 --> 00:36:43,000
the other option is for Hollywood

831
00:36:43,000 --> 00:36:45,000
to just create complete artificial characters

832
00:36:45,000 --> 00:36:46,000
that they fully own.

833
00:36:46,000 --> 00:36:47,000
Yeah.

834
00:36:47,000 --> 00:36:51,000
And you've seen this already with some of the kind of V-loggers

835
00:36:51,000 --> 00:36:54,000
and, you know, others that have emerged out of Asia,

836
00:36:54,000 --> 00:36:56,000
in particular, fully AI-generated characters.

837
00:36:56,000 --> 00:36:57,000
Yeah.

838
00:36:57,000 --> 00:36:59,000
And you can have entire mythos around them.

839
00:36:59,000 --> 00:37:02,000
And you can say, make it the most attractive Italian guy

840
00:37:02,000 --> 00:37:05,000
I've ever seen that's broody and this and that.

841
00:37:05,000 --> 00:37:06,000
Tell it from a human.

842
00:37:06,000 --> 00:37:07,000
Yeah.

843
00:37:07,000 --> 00:37:09,000
I mean, like, these characters can be completely new.

844
00:37:09,000 --> 00:37:11,000
And it's far more profitable for the studio

845
00:37:11,000 --> 00:37:14,000
to use that digital actor.

846
00:37:14,000 --> 00:37:19,000
So there is a disruption coming at every level.

847
00:37:19,000 --> 00:37:23,000
Because the research to revenue pipeline has become so tight

848
00:37:23,000 --> 00:37:25,000
and it sets off a race condition,

849
00:37:25,000 --> 00:37:28,000
whereby you could only produce two movies a year,

850
00:37:28,000 --> 00:37:30,000
suddenly you can produce 20.

851
00:37:30,000 --> 00:37:34,000
And then you can actually, like we do AB testing

852
00:37:34,000 --> 00:37:36,000
in subject headlines,

853
00:37:36,000 --> 00:37:39,000
you could create 30 variants of the movie

854
00:37:39,000 --> 00:37:41,000
and see which one actually is the best.

855
00:37:41,000 --> 00:37:42,000
Yeah.

856
00:37:42,000 --> 00:37:45,000
And, I mean, again, you can say, make it more,

857
00:37:45,000 --> 00:37:47,000
make that speech roar and more emotional.

858
00:37:47,000 --> 00:37:49,000
It will adjust the voice to make it roar

859
00:37:49,000 --> 00:37:51,000
and more emotional, right?

860
00:37:51,000 --> 00:37:54,000
And because you're using such large data sets,

861
00:37:54,000 --> 00:37:57,000
you know, and we, so we made all our data sets open

862
00:37:57,000 --> 00:37:59,000
and then we allowed opt-out.

863
00:37:59,000 --> 00:38:01,000
We're the only company in the world to allow opt-out

864
00:38:01,000 --> 00:38:02,000
because we thought it was the right thing to do.

865
00:38:02,000 --> 00:38:04,000
So we had 169 million images

866
00:38:04,000 --> 00:38:06,000
opted out of our image data sets.

867
00:38:06,000 --> 00:38:08,000
For music, because it's different copyright laws,

868
00:38:08,000 --> 00:38:11,000
we have one of the first commercially licensed music models

869
00:38:11,000 --> 00:38:12,000
coming out.

870
00:38:12,000 --> 00:38:13,000
So respect for that.

871
00:38:13,000 --> 00:38:17,000
But if I'm an artist and I go to the Louvre

872
00:38:17,000 --> 00:38:22,000
to be inspired and then go back and paint,

873
00:38:22,000 --> 00:38:26,000
and I've been inspired by Da Vinci

874
00:38:26,000 --> 00:38:32,000
and I start painting in a style like Da Vinci,

875
00:38:32,000 --> 00:38:33,000
where's the difference there?

876
00:38:33,000 --> 00:38:34,000
Where's the difference?

877
00:38:34,000 --> 00:38:35,000
And this is the reality.

878
00:38:35,000 --> 00:38:38,000
Even though we've done that, by next year,

879
00:38:38,000 --> 00:38:40,000
probably by the end of the year,

880
00:38:40,000 --> 00:38:44,000
you will have models that have zero scraped data

881
00:38:44,000 --> 00:38:45,000
or human art.

882
00:38:45,000 --> 00:38:46,000
They will all be synthetic.

883
00:38:46,000 --> 00:38:47,000
Yeah.

884
00:38:47,000 --> 00:38:49,000
And you'll be able to bring your art to it.

885
00:38:49,000 --> 00:38:52,000
So there's something Google just released called Style Drop.

886
00:38:52,000 --> 00:38:55,000
There's Hyper Dream, there's Hyper Networks,

887
00:38:55,000 --> 00:38:57,000
you can take one picture of yourself

888
00:38:57,000 --> 00:39:00,000
and the entire model trains to be able to put you into anything,

889
00:39:00,000 --> 00:39:01,000
even if you're not in the model.

890
00:39:01,000 --> 00:39:03,000
It used to take minutes, hours,

891
00:39:03,000 --> 00:39:05,000
now it's just one picture.

892
00:39:05,000 --> 00:39:08,000
Similarly, you can bring any style

893
00:39:08,000 --> 00:39:10,000
and it will just mimic and imitate that style.

894
00:39:10,000 --> 00:39:13,000
And so all of a sudden, the models themselves,

895
00:39:13,000 --> 00:39:15,000
it doesn't matter what they're trained on

896
00:39:15,000 --> 00:39:18,000
because there's no human endeavor in those models.

897
00:39:18,000 --> 00:39:21,000
And then things like compensation for artists and others,

898
00:39:21,000 --> 00:39:23,000
as you said, become a bit mute

899
00:39:23,000 --> 00:39:24,000
because all of a sudden,

900
00:39:24,000 --> 00:39:27,000
you have these amazing stories told by

901
00:39:27,000 --> 00:39:30,000
really convincing amazing actors

902
00:39:30,000 --> 00:39:32,000
who may or may not exist

903
00:39:32,000 --> 00:39:34,000
and how are you ever going to tell the difference?

904
00:39:34,000 --> 00:39:35,000
So what's your advice?

905
00:39:35,000 --> 00:39:36,000
Let's parse it here.

906
00:39:36,000 --> 00:39:39,000
On one side, what's your advice for Hollywood

907
00:39:39,000 --> 00:39:41,000
and for actors?

908
00:39:41,000 --> 00:39:43,000
And on the other side,

909
00:39:43,000 --> 00:39:45,000
I want to ask your advice for artists.

910
00:39:45,000 --> 00:39:48,000
This is about mindset.

911
00:39:48,000 --> 00:39:52,000
This is coming at us at extraordinary speed.

912
00:39:52,000 --> 00:39:54,000
There's no stopping it, right?

913
00:39:54,000 --> 00:39:59,000
There's no slowing it down.

914
00:39:59,000 --> 00:40:02,000
And so you've got to deal with reality.

915
00:40:02,000 --> 00:40:04,000
You're dealing with reality, again, it's inevitable.

916
00:40:04,000 --> 00:40:07,000
Even if, again, Hollywood says no AI,

917
00:40:07,000 --> 00:40:09,000
the AI is coming from around the world.

918
00:40:09,000 --> 00:40:10,000
So what do you do?

919
00:40:10,000 --> 00:40:11,000
You think, oh, well,

920
00:40:11,000 --> 00:40:14,000
my audience suddenly became the whole world.

921
00:40:14,000 --> 00:40:16,000
That's a big deal.

922
00:40:16,000 --> 00:40:19,000
You're like, what am I actually known for?

923
00:40:19,000 --> 00:40:20,000
Is my acting skills?

924
00:40:20,000 --> 00:40:22,000
Well, I will still get these things.

925
00:40:22,000 --> 00:40:24,000
You're an up and coming actor.

926
00:40:24,000 --> 00:40:25,000
You say, I need to build community.

927
00:40:25,000 --> 00:40:28,000
I need to kind of show off something more than that.

928
00:40:28,000 --> 00:40:30,000
Because again, my acting skills in some areas

929
00:40:30,000 --> 00:40:31,000
can be transplanted,

930
00:40:31,000 --> 00:40:33,000
but what about real life shows?

931
00:40:33,000 --> 00:40:35,000
What about these things?

932
00:40:35,000 --> 00:40:37,000
It does throw up the entire thing and adjust it,

933
00:40:37,000 --> 00:40:40,000
but then musicians have had to have that adjustment.

934
00:40:40,000 --> 00:40:42,000
They used to be able to make money on their LPs,

935
00:40:42,000 --> 00:40:43,000
and then all of a sudden,

936
00:40:43,000 --> 00:40:46,000
they had the naps to Spotify moments.

937
00:40:46,000 --> 00:40:48,000
There is more protection in music as well

938
00:40:48,000 --> 00:40:51,000
because you basically, according to

939
00:40:51,000 --> 00:40:55,000
the Robin Thicke versus Marvin Gaye case,

940
00:40:55,000 --> 00:40:58,000
there is an element of style protection in there

941
00:40:58,000 --> 00:41:00,000
that doesn't exist in visual media.

942
00:41:00,000 --> 00:41:03,000
And probably won't because the other part of this is

943
00:41:03,000 --> 00:41:06,000
if you're expecting governments to regulate,

944
00:41:06,000 --> 00:41:11,000
how can they, when there is a global competition going on,

945
00:41:11,000 --> 00:41:14,000
they will lose competitiveness to other countries

946
00:41:14,000 --> 00:41:16,000
and you'll have regulatory arbitrage.

947
00:41:16,000 --> 00:41:20,000
Yes, and that is something across

948
00:41:20,000 --> 00:41:22,000
every industry that's going to be happening.

949
00:41:22,000 --> 00:41:25,000
Yeah, I think the concept of united artists,

950
00:41:25,000 --> 00:41:28,000
it was originally a collective of all the artists,

951
00:41:28,000 --> 00:41:30,000
that makes a lot of sense now.

952
00:41:30,000 --> 00:41:33,000
I think you have to think about an element of collectivism

953
00:41:33,000 --> 00:41:36,000
to share the excess profits because what's going to happen is

954
00:41:36,000 --> 00:41:38,000
movies will get cheaper, profits will go up.

955
00:41:38,000 --> 00:41:41,000
You need to support each other as a community here

956
00:41:41,000 --> 00:41:43,000
and think again, as a community,

957
00:41:43,000 --> 00:41:46,000
this is our story for the next one, three, five, ten years

958
00:41:46,000 --> 00:41:50,000
because all of this is going to happen quicker

959
00:41:50,000 --> 00:41:54,000
than it takes to make the new Avengers movie.

960
00:41:54,000 --> 00:41:57,000
And quicker than regulators are able to regulate.

961
00:41:57,000 --> 00:42:01,000
And again, the regulators almost certainly won't regulate

962
00:42:01,000 --> 00:42:04,000
because they will start falling behind

963
00:42:04,000 --> 00:42:06,000
their competitor countries.

964
00:42:06,000 --> 00:42:09,000
I mean, if we look at the internet itself as it,

965
00:42:09,000 --> 00:42:12,000
the media industry never expected the internet

966
00:42:12,000 --> 00:42:14,000
to have the disruptive impact it had.

967
00:42:14,000 --> 00:42:17,000
And had it known, it probably would have tried to get regulators

968
00:42:17,000 --> 00:42:19,000
to have slowed it down or blocked it.

969
00:42:19,000 --> 00:42:21,000
Yeah, the speed is too much, but then also,

970
00:42:21,000 --> 00:42:23,000
again, I gave the example earlier,

971
00:42:23,000 --> 00:42:25,000
the video game industry has gone from 70 billion to 180 billion

972
00:42:25,000 --> 00:42:27,000
over the next ten years.

973
00:42:27,000 --> 00:42:29,000
Can we increase quality?

974
00:42:29,000 --> 00:42:31,000
Interactivity.

975
00:42:31,000 --> 00:42:33,000
Because games are media as well.

976
00:42:33,000 --> 00:42:35,000
The media industry has increased in size.

977
00:42:35,000 --> 00:42:38,000
The way that value has gone has been redistributed.

978
00:42:38,000 --> 00:42:41,000
Value will be redistributed again now.

979
00:42:41,000 --> 00:42:44,000
And again, it's like, what is an AI-enhanced actor?

980
00:42:44,000 --> 00:42:49,000
If you're an actor, what's an AI-enhanced photographer filming?

981
00:42:49,000 --> 00:42:53,000
Think about your jobs, the tasks that you do,

982
00:42:53,000 --> 00:42:55,000
and what can be augmented if you had a bunch

983
00:42:55,000 --> 00:42:58,000
of really talented youngsters working for you, right?

984
00:42:58,000 --> 00:43:01,000
You could do more, you could be more.

985
00:43:01,000 --> 00:43:04,000
But then it means the bar is just going to keep on raising.

986
00:43:04,000 --> 00:43:08,000
Let's turn to a different industry that's going to change.

987
00:43:08,000 --> 00:43:11,000
We had this conversation in our last podcast

988
00:43:11,000 --> 00:43:16,000
and on stage at abundance 360, which is coders.

989
00:43:16,000 --> 00:43:19,000
Coding is changing dramatically.

990
00:43:19,000 --> 00:43:21,000
What are your thoughts there?

991
00:43:21,000 --> 00:43:25,000
So, when I started as a programmer, gosh, 22 years ago,

992
00:43:25,000 --> 00:43:27,000
I was writing enterprise-level assembly code

993
00:43:27,000 --> 00:43:29,000
for voice-over IP software.

994
00:43:29,000 --> 00:43:30,000
Wow.

995
00:43:30,000 --> 00:43:31,000
I had to switch.

996
00:43:31,000 --> 00:43:33,000
That's some of the largest chunks of code out there.

997
00:43:33,000 --> 00:43:35,000
Yeah, it's very low-level code.

998
00:43:35,000 --> 00:43:37,000
We didn't have GitHub, we just got Subversion

999
00:43:37,000 --> 00:43:38,000
here.

1000
00:43:38,000 --> 00:43:40,000
Programming these days is a lot like Lego,

1001
00:43:40,000 --> 00:43:42,000
because what you have is kind of you have a very low-level,

1002
00:43:42,000 --> 00:43:44,000
but then you have levels of abstraction

1003
00:43:44,000 --> 00:43:47,000
until you get to PyTorch and some of these other languages.

1004
00:43:47,000 --> 00:43:49,000
So, you have to compile lots of different libraries

1005
00:43:49,000 --> 00:43:52,000
because you're making it easier and easier.

1006
00:43:52,000 --> 00:43:55,000
Human words are just the next level of abstraction there.

1007
00:43:55,000 --> 00:43:58,000
But the nature of coding is going to change.

1008
00:43:58,000 --> 00:44:02,000
And so, the coders that are coding traditionally today

1009
00:44:02,000 --> 00:44:06,000
around the world, how will they be using

1010
00:44:07,000 --> 00:44:11,000
and working in this industry two to five years from now?

1011
00:44:11,000 --> 00:44:14,000
Well, again, there will be no coder

1012
00:44:14,000 --> 00:44:16,000
that doesn't use AI as part of their workflow.

1013
00:44:16,000 --> 00:44:18,000
Okay, I mean, I think that's the important thing.

1014
00:44:18,000 --> 00:44:20,000
It's not like coders are going to go away,

1015
00:44:20,000 --> 00:44:22,000
they're going to be using a new set of tools.

1016
00:44:22,000 --> 00:44:24,000
The expectations will rise,

1017
00:44:24,000 --> 00:44:27,000
the amount of debugging unit testing,

1018
00:44:27,000 --> 00:44:29,000
all of these things will decrease,

1019
00:44:29,000 --> 00:44:32,000
because how much time did coders actually spend architecting?

1020
00:44:32,000 --> 00:44:33,000
Very little up front.

1021
00:44:33,000 --> 00:44:35,000
It's more about understanding information flows

1022
00:44:35,000 --> 00:44:37,000
about architecting these things.

1023
00:44:37,000 --> 00:44:39,000
It's about having feedback loops

1024
00:44:39,000 --> 00:44:41,000
to understand customer requirements.

1025
00:44:41,000 --> 00:44:44,000
Databricks is a $38 billion company.

1026
00:44:44,000 --> 00:44:45,000
There's Data Lakes.

1027
00:44:45,000 --> 00:44:47,000
So, it takes your data to organize

1028
00:44:47,000 --> 00:44:49,000
it allows you to write structured queries.

1029
00:44:49,000 --> 00:44:51,000
You have to write queries.

1030
00:44:51,000 --> 00:44:54,000
Now, you just talk to it and it just does it.

1031
00:44:54,000 --> 00:44:56,000
Microsoft will introduce the same thing.

1032
00:44:56,000 --> 00:44:59,000
I mean, I can't wait for that in the field of medicine,

1033
00:44:59,000 --> 00:45:01,000
which I want to talk about next.

1034
00:45:01,000 --> 00:45:03,000
But you said something earlier where you can imagine

1035
00:45:03,000 --> 00:45:05,000
there can be a billion coders in the future.

1036
00:45:05,000 --> 00:45:11,000
Yeah, because all the barriers to creating programs disappear.

1037
00:45:11,000 --> 00:45:13,000
So, it's not that there are no programs,

1038
00:45:13,000 --> 00:45:15,000
there's no programs that we know it,

1039
00:45:15,000 --> 00:45:17,000
because there's a billion programmers.

1040
00:45:17,000 --> 00:45:19,000
Everyone is a programmer, nobody's a programmer in a way.

1041
00:45:19,000 --> 00:45:21,000
Because it just becomes a matter of course.

1042
00:45:21,000 --> 00:45:24,000
I want to make software that does something

1043
00:45:24,000 --> 00:45:26,000
and reacts in these ways

1044
00:45:26,000 --> 00:45:29,000
and looks like this and adapts like this.

1045
00:45:29,000 --> 00:45:31,000
Then it comes to you and you're like,

1046
00:45:31,000 --> 00:45:32,000
no, that's not quite right.

1047
00:45:32,000 --> 00:45:33,000
I want this moved over.

1048
00:45:33,000 --> 00:45:35,000
It happens almost live, this feedback loop.

1049
00:45:35,000 --> 00:45:38,000
It's as we talk to chat GPT-4,

1050
00:45:38,000 --> 00:45:41,000
creating a paragraph that describes something we want.

1051
00:45:41,000 --> 00:45:42,000
We modify it.

1052
00:45:42,000 --> 00:45:45,000
I mean, like, chat GPT-4 is a good example

1053
00:45:45,000 --> 00:45:50,000
because to write an integration to something like chat GPT-4,

1054
00:45:50,000 --> 00:45:53,000
you used to take days, weeks, an API,

1055
00:45:53,000 --> 00:45:55,000
an application protocol kind of interface.

1056
00:45:55,000 --> 00:45:58,000
Now, what you do is actually you tell it the schema.

1057
00:45:58,000 --> 00:46:00,000
You tell it kind of what you should do

1058
00:46:00,000 --> 00:46:02,000
and it writes it automatically,

1059
00:46:02,000 --> 00:46:04,000
literally within like a few minutes.

1060
00:46:04,000 --> 00:46:06,000
And then in a few hours, you've integrated into it.

1061
00:46:06,000 --> 00:46:10,000
I've been talking about a future where we all have Jarvis.

1062
00:46:10,000 --> 00:46:12,000
Iron Man, I love Iron Man as a movie.

1063
00:46:12,000 --> 00:46:13,000
It's one of my favorites.

1064
00:46:13,000 --> 00:46:16,000
And, you know, Jarvis is basically your personal AI

1065
00:46:16,000 --> 00:46:18,000
that is, it's a software shell.

1066
00:46:18,000 --> 00:46:20,000
It interfaces between you and the rest of the world.

1067
00:46:20,000 --> 00:46:22,000
You ask Jarvis to do something

1068
00:46:22,000 --> 00:46:27,000
and it knows how to, the toolpaths on a 3D printer

1069
00:46:27,000 --> 00:46:30,000
you can hop into a jet and interface Jarvis

1070
00:46:30,000 --> 00:46:33,000
with the jet's computational system

1071
00:46:33,000 --> 00:46:35,000
and it'll fly the jet for you.

1072
00:46:35,000 --> 00:46:38,000
I can't imagine that that is really far from now.

1073
00:46:38,000 --> 00:46:41,000
Yeah, let's hope that doesn't fly planes just quite yet.

1074
00:46:41,000 --> 00:46:44,000
Okay, well, I'll put the jet aircraft aside.

1075
00:46:44,000 --> 00:46:49,000
But the ability to, for it to become your best friend

1076
00:46:49,000 --> 00:46:52,000
and confidant, know your needs and desires,

1077
00:46:52,000 --> 00:46:55,000
shape the world to your comfort

1078
00:46:55,000 --> 00:46:57,000
and being able to help you.

1079
00:46:57,000 --> 00:46:59,000
It's the ultimate user interface.

1080
00:46:59,000 --> 00:47:02,000
Well, I mean, this is why a lot of the chatbots,

1081
00:47:02,000 --> 00:47:04,000
character AI and others have become so popular

1082
00:47:04,000 --> 00:47:05,000
because it'll never judge you

1083
00:47:05,000 --> 00:47:08,000
and it's approaching that human level now.

1084
00:47:08,000 --> 00:47:11,000
You know, and again, it is the ultimate interfaces,

1085
00:47:11,000 --> 00:47:14,000
maybe chats, but it's more chats, chatting context.

1086
00:47:14,000 --> 00:47:17,000
It's understanding you holistically.

1087
00:47:17,000 --> 00:47:19,000
No human could do that because, you know,

1088
00:47:19,000 --> 00:47:20,000
even if you hire a whole team,

1089
00:47:20,000 --> 00:47:22,000
they're not going to be with you 24-7.

1090
00:47:22,000 --> 00:47:24,000
This will be with you 24-7.

1091
00:47:24,000 --> 00:47:26,000
I think the key thing here is empathy.

1092
00:47:26,000 --> 00:47:27,000
Yes.

1093
00:47:27,000 --> 00:47:30,000
Because jumping head bit to medicine.

1094
00:47:30,000 --> 00:47:32,000
Google had their MedPalm 2 model.

1095
00:47:32,000 --> 00:47:34,000
The papers just come out in nature.

1096
00:47:34,000 --> 00:47:38,000
A, it outperforms doctors on clinical diagnosis,

1097
00:47:38,000 --> 00:47:41,000
which is crazy for a few hundred gigabytes of a file.

1098
00:47:41,000 --> 00:47:42,000
Yeah.

1099
00:47:42,000 --> 00:47:45,000
B, it outperforms doctors on scores of empathy.

1100
00:47:45,000 --> 00:47:49,000
I found that amazing and totally logical.

1101
00:47:49,000 --> 00:47:50,000
It doesn't judge you.

1102
00:47:50,000 --> 00:47:52,000
It doesn't judge you, but then, you know,

1103
00:47:52,000 --> 00:47:54,000
doctors split a million ways and they're tired

1104
00:47:54,000 --> 00:47:56,000
and they're grumpy or this or that.

1105
00:47:56,000 --> 00:47:59,000
Some of us get good doctors, most of us don't.

1106
00:47:59,000 --> 00:48:01,000
Some of us get good teachers.

1107
00:48:01,000 --> 00:48:04,000
I'm not saying education is bad because of the teachers.

1108
00:48:04,000 --> 00:48:06,000
So many teachers try so hard,

1109
00:48:06,000 --> 00:48:10,000
but their attention is split 20 ways and they're underpaid.

1110
00:48:10,000 --> 00:48:12,000
You know, I'm not saying that programs

1111
00:48:12,000 --> 00:48:14,000
that would be nature programming will change

1112
00:48:14,000 --> 00:48:15,000
because programmers are bad.

1113
00:48:15,000 --> 00:48:17,000
There's so many hard-working programmers.

1114
00:48:17,000 --> 00:48:20,000
It's just, again, the nature of these things will change

1115
00:48:20,000 --> 00:48:23,000
when you can scale expertise.

1116
00:48:23,000 --> 00:48:26,000
And everyone has expertise available to them on tap.

1117
00:48:26,000 --> 00:48:29,000
I've been on the stage, you know, just pounding my fist

1118
00:48:29,000 --> 00:48:31,000
saying, listen, it's going to become malpractice

1119
00:48:31,000 --> 00:48:34,000
to diagnose someone without an AI in the loop

1120
00:48:34,000 --> 00:48:36,000
within five years' time.

1121
00:48:36,000 --> 00:48:40,000
And probably in some areas, it'll be inappropriate

1122
00:48:40,000 --> 00:48:43,000
to not yet illegal to.

1123
00:48:43,000 --> 00:48:45,000
And then at some point soon after that,

1124
00:48:45,000 --> 00:48:48,000
the best surgeons in the world are going to be humanoid robots

1125
00:48:48,000 --> 00:48:56,000
that have every possible, you know, atrial variation,

1126
00:48:56,000 --> 00:49:00,000
every possible, you know, history of surgery

1127
00:49:00,000 --> 00:49:02,000
and they can see an infrared and ultraviolet

1128
00:49:02,000 --> 00:49:05,000
and they haven't had an argument that morning

1129
00:49:05,000 --> 00:49:08,000
with their husband or wife and it becomes the best.

1130
00:49:08,000 --> 00:49:13,000
And these are demonetizing and democratizing forces for health.

1131
00:49:13,000 --> 00:49:15,000
There must be deflationary as well,

1132
00:49:15,000 --> 00:49:17,000
but I know I agree completely with this

1133
00:49:17,000 --> 00:49:20,000
but ultimately what's going to kick it off is,

1134
00:49:20,000 --> 00:49:22,000
is your doctor AI enhanced?

1135
00:49:22,000 --> 00:49:23,000
Yeah.

1136
00:49:23,000 --> 00:49:25,000
Low insurance premium, lower copay?

1137
00:49:25,000 --> 00:49:26,000
Yes.

1138
00:49:26,000 --> 00:49:29,000
Because there will be real economic incentives.

1139
00:49:29,000 --> 00:49:31,000
Has this been cross-checked by the technology?

1140
00:49:31,000 --> 00:49:32,000
Yeah.

1141
00:49:32,000 --> 00:49:34,000
Reduce the cost?

1142
00:49:34,000 --> 00:49:37,000
My favorite subject is, you know, you probably know this,

1143
00:49:37,000 --> 00:49:40,000
how many medical articles are written in journals every day?

1144
00:49:40,000 --> 00:49:41,000
I don't know.

1145
00:49:41,000 --> 00:49:42,000
It's 7,000.

1146
00:49:42,000 --> 00:49:43,000
Wow.

1147
00:49:43,000 --> 00:49:46,000
And it's like, how many is your doctor read today?

1148
00:49:46,000 --> 00:49:48,000
You know, and there may be that one breakthrough

1149
00:49:48,000 --> 00:49:50,000
that happened this morning that is the key

1150
00:49:50,000 --> 00:49:52,000
for your diagnostics.

1151
00:49:52,000 --> 00:49:54,000
But I mean, even if they've read it, right,

1152
00:49:54,000 --> 00:49:57,000
like absorbing it is one thing, having the mental models,

1153
00:49:57,000 --> 00:49:59,000
these are kind of something else.

1154
00:49:59,000 --> 00:50:01,000
This is why comprehensive authority is not state,

1155
00:50:01,000 --> 00:50:03,000
which is what this technology allows to happen.

1156
00:50:03,000 --> 00:50:06,000
As you said, things like, we're already seeing some surgeries,

1157
00:50:06,000 --> 00:50:09,000
can you done better by robot surgeons than human surgeons?

1158
00:50:09,000 --> 00:50:10,000
It'll be all surgeries.

1159
00:50:10,000 --> 00:50:12,000
Yeah, it will be all surgeries soon enough.

1160
00:50:12,000 --> 00:50:14,000
Like the robotics advancements we've seen,

1161
00:50:14,000 --> 00:50:16,000
this actually goes back to your point of, you know,

1162
00:50:16,000 --> 00:50:19,000
the artist going to the Louvre and seeing the Da Vinci

1163
00:50:19,000 --> 00:50:21,000
and then taking inspiration from that.

1164
00:50:21,000 --> 00:50:23,000
What are we going to do with like all of these

1165
00:50:23,000 --> 00:50:26,000
Optimus robots and 1kx robots and others?

1166
00:50:26,000 --> 00:50:28,000
Are they going to have to shut their eyes

1167
00:50:28,000 --> 00:50:30,000
when they see anything copyrighted?

1168
00:50:30,000 --> 00:50:32,000
You're just going to have accidents everywhere.

1169
00:50:32,000 --> 00:50:34,000
They're like running into each other.

1170
00:50:34,000 --> 00:50:36,000
Everything's going to be black lined out.

1171
00:50:36,000 --> 00:50:41,000
So medicine is changing dramatically.

1172
00:50:41,000 --> 00:50:44,000
What other field are you seeing and saying

1173
00:50:44,000 --> 00:50:46,000
people need to wake up and see what's coming?

1174
00:50:46,000 --> 00:50:49,000
So, I mean, medicine, education

1175
00:50:49,000 --> 00:50:51,000
are kind of the two big ones, I think.

1176
00:50:51,000 --> 00:50:52,000
I agree.

1177
00:50:52,000 --> 00:50:53,000
But can we move this to the side right now?

1178
00:50:53,000 --> 00:50:54,000
Again, we've seen programming,

1179
00:50:54,000 --> 00:50:56,000
the entire nature program will change media.

1180
00:50:56,000 --> 00:50:58,000
The entire nature of media will change

1181
00:50:58,000 --> 00:51:01,000
from journalism to filmmaking.

1182
00:51:01,000 --> 00:51:06,000
But anything that basically you could do

1183
00:51:06,000 --> 00:51:09,000
with someone from Asia on the other side

1184
00:51:09,000 --> 00:51:11,000
on a computer screen will change.

1185
00:51:11,000 --> 00:51:13,000
Let's talk about education.

1186
00:51:13,000 --> 00:51:17,000
Because today, education hasn't changed

1187
00:51:17,000 --> 00:51:21,000
since the one-room classroom.

1188
00:51:21,000 --> 00:51:24,000
Half the kids are lost, half the kids are bored.

1189
00:51:24,000 --> 00:51:26,000
You're teaching to a series of tests.

1190
00:51:26,000 --> 00:51:30,000
You're teaching for an industrial era world.

1191
00:51:30,000 --> 00:51:33,000
And people learn differently.

1192
00:51:33,000 --> 00:51:38,000
People have visual and auditory and tactile learning skills.

1193
00:51:38,000 --> 00:51:42,000
And let's face it, we don't celebrate our teachers.

1194
00:51:42,000 --> 00:51:43,000
We don't pay them well,

1195
00:51:43,000 --> 00:51:46,000
and we don't have the best of them coming into the classroom.

1196
00:51:46,000 --> 00:51:48,000
And they're sad.

1197
00:51:48,000 --> 00:51:50,000
I mean, this is the thing.

1198
00:51:50,000 --> 00:51:53,000
There are happy classrooms with happy teachers

1199
00:51:53,000 --> 00:51:54,000
and other things,

1200
00:51:54,000 --> 00:51:58,000
but learning should be a place of positive growth and joy.

1201
00:51:58,000 --> 00:52:00,000
It should be fun to learn.

1202
00:52:00,000 --> 00:52:01,000
And it should be fun to teach.

1203
00:52:01,000 --> 00:52:02,000
And fun to teach, yes.

1204
00:52:02,000 --> 00:52:04,000
I think this is both of them.

1205
00:52:04,000 --> 00:52:06,000
Because what is the nature of a teacher in 5, 10 years?

1206
00:52:06,000 --> 00:52:08,000
Let's say 10 years.

1207
00:52:08,000 --> 00:52:12,000
Again, 10 years is the crazy short period of time.

1208
00:52:12,000 --> 00:52:16,000
I mean, when I asked you this question last time,

1209
00:52:16,000 --> 00:52:19,000
how far out can you predict what's likely to come?

1210
00:52:19,000 --> 00:52:23,000
What's your singularity boundary condition?

1211
00:52:23,000 --> 00:52:24,000
I'm curious.

1212
00:52:24,000 --> 00:52:26,000
What are you seeing as?

1213
00:52:26,000 --> 00:52:28,000
3 or 5 years.

1214
00:52:28,000 --> 00:52:30,000
I go to Dubai and I'm on stage,

1215
00:52:30,000 --> 00:52:34,000
and can you talk to us about what the world's going to be like in 2050?

1216
00:52:34,000 --> 00:52:35,000
My answer is no.

1217
00:52:35,000 --> 00:52:37,000
I can barely talk about 2030.

1218
00:52:37,000 --> 00:52:39,000
It's everything ever at once.

1219
00:52:39,000 --> 00:52:41,000
Lots of S-curves, acceleration.

1220
00:52:41,000 --> 00:52:43,000
These are inevitable now.

1221
00:52:43,000 --> 00:52:45,000
Now we've broken through these things.

1222
00:52:45,000 --> 00:52:48,000
I mean, OpenAI now have put 20% of their stuff to alignment

1223
00:52:48,000 --> 00:52:51,000
because they're basically saying that their view is 5 years out.

1224
00:52:51,000 --> 00:52:53,000
Elon Musk just said 6 years out,

1225
00:52:53,000 --> 00:52:56,000
but then Elon's relationship with time is always a bit fun.

1226
00:52:56,000 --> 00:52:58,000
Just like self-driving cars.

1227
00:52:58,000 --> 00:53:00,000
But self-driving cars are literally here now.

1228
00:53:00,000 --> 00:53:03,000
You can get in one and it will drive you around.

1229
00:53:03,000 --> 00:53:05,000
San Francisco or London or Germany.

1230
00:53:05,000 --> 00:53:06,000
Waymore will do that for you.

1231
00:53:06,000 --> 00:53:07,000
Not my Tesla.

1232
00:53:07,000 --> 00:53:08,000
Not your Tesla.

1233
00:53:08,000 --> 00:53:09,000
I know.

1234
00:53:09,000 --> 00:53:10,000
I push in the button, but it doesn't quite do it.

1235
00:53:10,000 --> 00:53:11,000
But the technology is here.

1236
00:53:11,000 --> 00:53:12,000
Yeah.

1237
00:53:12,000 --> 00:53:13,000
And I was like, oh wait.

1238
00:53:13,000 --> 00:53:14,000
What?

1239
00:53:14,000 --> 00:53:15,000
This is the thing.

1240
00:53:15,000 --> 00:53:18,000
So, again, 10 years is a dramatically short period of time

1241
00:53:18,000 --> 00:53:20,000
for education, which has been the same for a century.

1242
00:53:20,000 --> 00:53:21,000
Yes.

1243
00:53:21,000 --> 00:53:23,000
But the thing is, it is inevitable

1244
00:53:23,000 --> 00:53:26,000
that every child will have their own AI.

1245
00:53:26,000 --> 00:53:29,000
So your 12-year-old will be 22.

1246
00:53:29,000 --> 00:53:32,000
When they get to 22 and they come out of,

1247
00:53:32,000 --> 00:53:34,000
let's say university slows down.

1248
00:53:34,000 --> 00:53:35,000
University.

1249
00:53:35,000 --> 00:53:36,000
If university is still a thing.

1250
00:53:36,000 --> 00:53:38,000
Yeah, they will have their own AI

1251
00:53:38,000 --> 00:53:40,000
that's learned for at least five years about them.

1252
00:53:40,000 --> 00:53:41,000
Yes.

1253
00:53:41,000 --> 00:53:43,000
That can fetch them any information in any format

1254
00:53:43,000 --> 00:53:45,000
of any type and write anything.

1255
00:53:45,000 --> 00:53:46,000
Yeah.

1256
00:53:46,000 --> 00:53:49,000
Or create any video or movie for them to...

1257
00:53:49,000 --> 00:53:50,000
That's a crazy thing.

1258
00:53:50,000 --> 00:53:52,000
There were concerns that Wikipedia would remove

1259
00:53:52,000 --> 00:53:54,000
rote learning and things like that.

1260
00:53:54,000 --> 00:53:55,000
Google would do the same.

1261
00:53:55,000 --> 00:53:57,000
And maybe there are kind of, again,

1262
00:53:57,000 --> 00:54:00,000
like appendices that have shivelled.

1263
00:54:00,000 --> 00:54:01,000
No.

1264
00:54:01,000 --> 00:54:02,000
What's the thing that shivells?

1265
00:54:02,000 --> 00:54:03,000
Yeah, your appendix.

1266
00:54:03,000 --> 00:54:04,000
Yeah.

1267
00:54:04,000 --> 00:54:06,000
But, you know, like, I was at Galblad or something.

1268
00:54:06,000 --> 00:54:08,000
But this is the thing, like,

1269
00:54:08,000 --> 00:54:10,000
you might have some vestigial parts of your brain.

1270
00:54:10,000 --> 00:54:12,000
The entire human brain will be rewired.

1271
00:54:12,000 --> 00:54:16,000
You must assume that every child will have their own AI.

1272
00:54:16,000 --> 00:54:19,000
However AI is driven is different.

1273
00:54:19,000 --> 00:54:21,000
Because any child that has AI will dramatically

1274
00:54:21,000 --> 00:54:23,000
outperform the kids that don't.

1275
00:54:23,000 --> 00:54:26,000
But what are we optimizing for in education?

1276
00:54:26,000 --> 00:54:28,000
And I think one of the things that we've lost

1277
00:54:28,000 --> 00:54:33,000
is what is our objective function as a society?

1278
00:54:33,000 --> 00:54:36,000
What does America even stand for right now?

1279
00:54:36,000 --> 00:54:37,000
Coming here.

1280
00:54:37,000 --> 00:54:38,000
I agree with you.

1281
00:54:38,000 --> 00:54:40,000
What are we optimizing for?

1282
00:54:40,000 --> 00:54:41,000
Butan is optimizing for happiness.

1283
00:54:41,000 --> 00:54:44,000
Well, and frankly, I think happiness is a great thing

1284
00:54:44,000 --> 00:54:46,000
to optimize for in general.

1285
00:54:46,000 --> 00:54:47,000
If you ask people, you know,

1286
00:54:47,000 --> 00:54:49,000
what do you want more than anything?

1287
00:54:49,000 --> 00:54:50,000
Life, I want happiness.

1288
00:54:50,000 --> 00:54:51,000
I want health.

1289
00:54:51,000 --> 00:54:52,000
I want love.

1290
00:54:52,000 --> 00:54:54,000
We don't talk about, you know, those are...

1291
00:54:54,000 --> 00:54:55,000
And there's an interesting, you know,

1292
00:54:55,000 --> 00:54:58,000
I was just with Mo Godot doing a podcast.

1293
00:54:58,000 --> 00:55:00,000
And he's a fan of your work.

1294
00:55:00,000 --> 00:55:02,000
And he was saying, loved your podcast.

1295
00:55:02,000 --> 00:55:05,000
And we're talking about this test of what would you trade?

1296
00:55:05,000 --> 00:55:06,000
Right?

1297
00:55:06,000 --> 00:55:09,000
Would you trade, you know,

1298
00:55:09,000 --> 00:55:11,000
how much money would you trade for your happiness

1299
00:55:11,000 --> 00:55:14,000
or for your health?

1300
00:55:14,000 --> 00:55:15,000
And it starts to do a bubble sort

1301
00:55:15,000 --> 00:55:17,000
and prioritizing what's at the top.

1302
00:55:17,000 --> 00:55:21,000
And I think for almost everybody, it's health happiness.

1303
00:55:21,000 --> 00:55:22,000
Right?

1304
00:55:22,000 --> 00:55:23,000
I think it is.

1305
00:55:23,000 --> 00:55:24,000
You know, time.

1306
00:55:24,000 --> 00:55:26,000
Time is the one thing you can never buy.

1307
00:55:26,000 --> 00:55:27,000
I'm working on it.

1308
00:55:27,000 --> 00:55:28,000
Yeah.

1309
00:55:28,000 --> 00:55:29,000
I have a longevity friend.

1310
00:55:29,000 --> 00:55:30,000
I'm not a longevity friend.

1311
00:55:30,000 --> 00:55:32,000
But at the moment, it's something you can't buy.

1312
00:55:32,000 --> 00:55:34,000
Happiness, we all know, we both know billionaires.

1313
00:55:34,000 --> 00:55:35,000
Yeah.

1314
00:55:35,000 --> 00:55:36,000
They are very sad.

1315
00:55:36,000 --> 00:55:37,000
They are so sad.

1316
00:55:37,000 --> 00:55:38,000
Yeah.

1317
00:55:38,000 --> 00:55:39,000
So sad.

1318
00:55:39,000 --> 00:55:40,000
And unhealthy.

1319
00:55:40,000 --> 00:55:41,000
It is.

1320
00:55:41,000 --> 00:55:44,000
It's like you create this incredible burden for yourself

1321
00:55:44,000 --> 00:55:47,000
for most of them, like some of them that we know

1322
00:55:47,000 --> 00:55:49,000
starting, you know, three or four companies a year.

1323
00:55:49,000 --> 00:55:51,000
And to what end?

1324
00:55:51,000 --> 00:55:52,000
It's a demon being driven.

1325
00:55:52,000 --> 00:55:54,000
It's because they're addicted to dopamine and crisis.

1326
00:55:54,000 --> 00:55:55,000
Yeah.

1327
00:55:55,000 --> 00:55:57,000
I mean, crisis is interesting because it comes down

1328
00:55:57,000 --> 00:55:59,000
to decision from its root, right?

1329
00:55:59,000 --> 00:56:02,000
And it's where leaders, you have to show leadership.

1330
00:56:02,000 --> 00:56:04,000
But that does become addicting.

1331
00:56:04,000 --> 00:56:06,000
I think most leaders are addicted to crisis,

1332
00:56:06,000 --> 00:56:09,000
but then so are many of us because we see it all the time.

1333
00:56:09,000 --> 00:56:11,000
Like, oh my God, the world is on fire.

1334
00:56:11,000 --> 00:56:12,000
The reality is actually this.

1335
00:56:12,000 --> 00:56:15,000
For all that we talk about, most communities are happy.

1336
00:56:15,000 --> 00:56:17,000
Most people are relatively content today.

1337
00:56:17,000 --> 00:56:18,000
Today.

1338
00:56:18,000 --> 00:56:19,000
Yeah.

1339
00:56:19,000 --> 00:56:21,000
We see explosions like, you know,

1340
00:56:21,000 --> 00:56:24,000
a decade ago that whole scene was on fire with the riots.

1341
00:56:24,000 --> 00:56:25,000
Maybe that will return.

1342
00:56:25,000 --> 00:56:27,000
We're seeing it in France right now.

1343
00:56:27,000 --> 00:56:29,000
We're seeing a breakdown of the social order.

1344
00:56:29,000 --> 00:56:34,000
But just because they're like content doesn't mean they're happy.

1345
00:56:34,000 --> 00:56:37,000
So what are we optimizing for is the question you left off.

1346
00:56:37,000 --> 00:56:40,000
So what do you think we as a society,

1347
00:56:40,000 --> 00:56:43,000
let's say the United States should be optimizing for?

1348
00:56:43,000 --> 00:56:44,000
I don't know.

1349
00:56:44,000 --> 00:56:45,000
Is it life, liberty?

1350
00:56:45,000 --> 00:56:47,000
And the pursuit of happiness.

1351
00:56:47,000 --> 00:56:49,000
Not the guarantee of happiness, the pursuit of happiness.

1352
00:56:49,000 --> 00:56:52,000
When was the last time someone actually talked about happiness

1353
00:56:52,000 --> 00:56:55,000
as a political leader in the U.S.?

1354
00:56:55,000 --> 00:56:56,000
You know, life, liberty.

1355
00:56:56,000 --> 00:56:59,000
When was the last time anyone tried to optimize for liberty?

1356
00:56:59,000 --> 00:57:02,000
Systems inherently look to control

1357
00:57:02,000 --> 00:57:04,000
because they have to make us simple.

1358
00:57:04,000 --> 00:57:07,000
You know, this is the wonderful book seeing like the state

1359
00:57:07,000 --> 00:57:10,000
where it talks about this concept of legibility.

1360
00:57:10,000 --> 00:57:13,000
You have a village and it's just grown

1361
00:57:13,000 --> 00:57:15,000
and it's got all this unique character.

1362
00:57:15,000 --> 00:57:16,000
You drive a road down the middle,

1363
00:57:16,000 --> 00:57:17,000
so you can get ambulance down there.

1364
00:57:17,000 --> 00:57:18,000
Sure, it helps.

1365
00:57:18,000 --> 00:57:21,000
But then everything becomes planned

1366
00:57:21,000 --> 00:57:23,000
because you have to put humans into boxes

1367
00:57:23,000 --> 00:57:25,000
and then this goes to education.

1368
00:57:25,000 --> 00:57:28,000
Happiness, I think, there's the Japanese concept of ikigai,

1369
00:57:28,000 --> 00:57:30,000
what you're good at, what you like,

1370
00:57:30,000 --> 00:57:32,000
and where you're adding value to the world.

1371
00:57:32,000 --> 00:57:34,000
And you can feel it yourself as well.

1372
00:57:34,000 --> 00:57:35,000
You'll feel progress.

1373
00:57:35,000 --> 00:57:39,000
If you don't have progress, then how are you going to be happy?

1374
00:57:39,000 --> 00:57:41,000
If you don't believe you're good at anything,

1375
00:57:41,000 --> 00:57:43,000
how do you feel you're going to be happy?

1376
00:57:44,000 --> 00:57:45,000
And as for what you like,

1377
00:57:45,000 --> 00:57:46,000
are you coming up with that yourself

1378
00:57:46,000 --> 00:57:48,000
or being told what you like?

1379
00:57:48,000 --> 00:57:50,000
And that's why it becomes consumerist.

1380
00:57:50,000 --> 00:57:53,000
So I think we need to have a discussion as a society about that

1381
00:57:53,000 --> 00:57:56,000
as a community, but then also for our kids.

1382
00:57:56,000 --> 00:57:59,000
What is the future for kids

1383
00:57:59,000 --> 00:58:03,000
when so much of the jobs in the West

1384
00:58:03,000 --> 00:58:07,000
are going to be transformed, not ended,

1385
00:58:07,000 --> 00:58:09,000
not necessarily massive unemployment,

1386
00:58:09,000 --> 00:58:12,000
but again, 10 years out, what is a lawyer?

1387
00:58:13,000 --> 00:58:15,000
What is an accountant?

1388
00:58:15,000 --> 00:58:16,000
Sure.

1389
00:58:16,000 --> 00:58:17,000
What is an engineer?

1390
00:58:17,000 --> 00:58:19,000
They are all AI assisted.

1391
00:58:19,000 --> 00:58:23,000
All of these, all the entire knowledge sector is transformed.

1392
00:58:23,000 --> 00:58:27,000
And do we want our kids that are growing up to be doctors,

1393
00:58:27,000 --> 00:58:30,000
lawyers, accountants, thinking,

1394
00:58:30,000 --> 00:58:31,000
there is no hope for the future.

1395
00:58:31,000 --> 00:58:33,000
There is no progress.

1396
00:58:33,000 --> 00:58:35,000
Because how am I going to compete against an AI?

1397
00:58:35,000 --> 00:58:38,000
Or do we want them to have that mindset of,

1398
00:58:38,000 --> 00:58:40,000
this technology is going to be amazing

1399
00:58:40,000 --> 00:58:43,000
because I want to be a doctor so I can help people.

1400
00:58:43,000 --> 00:58:44,000
I can help even more people.

1401
00:58:44,000 --> 00:58:48,000
And it enables you to do whatever you want to do, right?

1402
00:58:48,000 --> 00:58:51,000
What I think to think about is all the people who have jobs today

1403
00:58:51,000 --> 00:58:55,000
who are, that was never their dream to clean bathrooms

1404
00:58:55,000 --> 00:58:58,000
and make beds and, you know, wait on people.

1405
00:58:58,000 --> 00:59:02,000
It's what they did to get eventually to where they wanted to go

1406
00:59:02,000 --> 00:59:05,000
or to have, you know, put food in the table or insurance.

1407
00:59:05,000 --> 00:59:10,000
And AI is going to enable people to actually take on a higher goal

1408
00:59:10,000 --> 00:59:14,000
that actually gives them joy and happiness.

1409
00:59:14,000 --> 00:59:16,000
It does, but at the same time, you know,

1410
00:59:16,000 --> 00:59:18,000
we're very privileged people, you and I,

1411
00:59:18,000 --> 00:59:20,000
in that we can think about these big things.

1412
00:59:20,000 --> 00:59:22,000
There's a lot of people that are actually very happy

1413
00:59:22,000 --> 00:59:25,000
doing that type of work because they're a part of a group

1414
00:59:25,000 --> 00:59:27,000
and they take pride in their work.

1415
00:59:27,000 --> 00:59:30,000
So, you know, it's like,

1416
00:59:30,000 --> 00:59:33,000
there will always be a variety of different things.

1417
00:59:33,000 --> 00:59:38,000
The key thing is saying, can we build systems to make people happier

1418
00:59:38,000 --> 00:59:40,000
and more content without necessarily controlling them

1419
00:59:40,000 --> 00:59:44,000
and feel that they have the ability to do that?

1420
00:59:44,000 --> 00:59:46,000
Can we build systems to build strong communities?

1421
00:59:46,000 --> 00:59:49,000
Because one of the issues right now,

1422
00:59:49,000 --> 00:59:53,000
I was at kind of a conference and David Miliband from the IRC said this,

1423
00:59:53,000 --> 00:59:55,000
was that a lot of our problems now are global,

1424
00:59:55,000 --> 00:59:58,000
our solutions are almost being forced to be local

1425
00:59:58,000 --> 01:00:00,000
and there's no interconnect between that.

1426
01:00:00,000 --> 01:00:04,000
Our communities kind of have no guidance as to how to navigate this

1427
01:00:04,000 --> 01:00:09,000
because you will have a few hundred thousand people listening to this podcast

1428
01:00:09,000 --> 01:00:12,000
and there's myself and maybe a dozen others

1429
01:00:12,000 --> 01:00:15,000
that understand the AI and the sociology and this and that

1430
01:00:15,000 --> 01:00:18,000
and saying, this is coming,

1431
01:00:18,000 --> 01:00:20,000
but there are seven billion people on earth

1432
01:00:20,000 --> 01:00:23,000
and all of a sudden, in a few years,

1433
01:00:23,000 --> 01:00:26,000
they're all going to have to grapple with the questions that we're discussing now

1434
01:00:26,000 --> 01:00:29,000
and it's not a probability.

1435
01:00:29,000 --> 01:00:32,000
If the technology stops today,

1436
01:00:32,000 --> 01:00:35,000
you know, it stops increasing its capability.

1437
01:00:35,000 --> 01:00:37,000
Today, if it stopped today,

1438
01:00:37,000 --> 01:00:41,000
you would still have the entire legal profession, media profession.

1439
01:00:41,000 --> 01:00:43,000
Journalism profession.

1440
01:00:43,000 --> 01:00:46,000
They're all disrupted if it stops today, but it's not stopping.

1441
01:00:46,000 --> 01:00:49,000
And it's accelerating, isn't it?

1442
01:00:49,000 --> 01:00:50,000
It's accelerating.

1443
01:00:50,000 --> 01:00:54,000
The amount of money going into this sector goes up every single day.

1444
01:00:54,000 --> 01:00:57,000
My total Dressel market calculation is that in the next year,

1445
01:00:57,000 --> 01:00:59,000
a thousand companies have spent 10 million,

1446
01:00:59,000 --> 01:01:01,000
a hundred will spend a hundred and ten will spend a billion.

1447
01:01:01,000 --> 01:01:04,000
That's 30 billion dollars being put into the market.

1448
01:01:04,000 --> 01:01:06,000
Self-driving cars had a hundred billion dollars total.

1449
01:01:06,000 --> 01:01:09,000
This will be a trillion dollars going into this

1450
01:01:09,000 --> 01:01:12,000
because do you know what I've got a trillion dollars?

1451
01:01:12,000 --> 01:01:13,000
5G.

1452
01:01:13,000 --> 01:01:16,000
Is this more important than 5G?

1453
01:01:16,000 --> 01:01:18,000
By orders of magnitude.

1454
01:01:18,000 --> 01:01:21,000
It will get a trillion dollars going into it

1455
01:01:21,000 --> 01:01:24,000
and the capabilities will ramp up from here.

1456
01:01:24,000 --> 01:01:25,000
And so when I look at it,

1457
01:01:25,000 --> 01:01:27,000
and I look at what the drivers are of why now,

1458
01:01:27,000 --> 01:01:30,000
it's, first of all, computation, right?

1459
01:01:30,000 --> 01:01:33,000
Nvidia has done an incredible job, right?

1460
01:01:33,000 --> 01:01:36,000
With their A100 and computation is continuing on Moore's Law.

1461
01:01:36,000 --> 01:01:37,000
It's not slowing down.

1462
01:01:37,000 --> 01:01:39,000
It's continuing to increase year on year.

1463
01:01:39,000 --> 01:01:41,000
Especially a little bit exponential.

1464
01:01:41,000 --> 01:01:42,000
What is exponential?

1465
01:01:42,000 --> 01:01:44,000
Well, I mean, yes.

1466
01:01:44,000 --> 01:01:47,000
I'm saying it's continuing to double on a regular basis.

1467
01:01:47,000 --> 01:01:50,000
What was considered Moore's Law and people have said,

1468
01:01:50,000 --> 01:01:52,000
oh, it's going to eventually fall off as an S-curve.

1469
01:01:52,000 --> 01:01:54,000
Well, we're extending it.

1470
01:01:54,000 --> 01:01:57,000
And for the next, at least near-term future,

1471
01:01:57,000 --> 01:01:58,000
it's not slowing down.

1472
01:01:58,000 --> 01:02:00,000
So I think this is a very interesting thing

1473
01:02:00,000 --> 01:02:02,000
for people to understand.

1474
01:02:02,000 --> 01:02:04,000
You had Moore's Law and, again, it was doubling.

1475
01:02:04,000 --> 01:02:07,000
And this was an individual chip.

1476
01:02:07,000 --> 01:02:10,000
What we do with these models is that we stick together

1477
01:02:10,000 --> 01:02:12,000
thousands, tens of thousands of these chips.

1478
01:02:12,000 --> 01:02:14,000
So how many A100s right now is stability using?

1479
01:02:14,000 --> 01:02:16,000
We're using about 7,000, 8,000.

1480
01:02:16,000 --> 01:02:19,000
By next year, we will have 70,000 equivalent.

1481
01:02:19,000 --> 01:02:20,000
Wow.

1482
01:02:20,000 --> 01:02:23,000
But what used to happen is that as you stuck the chips together,

1483
01:02:23,000 --> 01:02:25,000
you ran a model, so you take large amounts of data

1484
01:02:25,000 --> 01:02:26,000
and you use these chips.

1485
01:02:26,000 --> 01:02:29,000
I mean, we're using maybe 10 megawatts of electricity.

1486
01:02:29,000 --> 01:02:31,000
98% clean.

1487
01:02:31,000 --> 01:02:34,000
Compared to the brain's 14 watts.

1488
01:02:34,000 --> 01:02:35,000
Brain's 14 watts.

1489
01:02:35,000 --> 01:02:38,000
But then it compresses it down, then it runs on 100, 200 watts

1490
01:02:38,000 --> 01:02:40,000
or 25 watts, actually.

1491
01:02:40,000 --> 01:02:42,000
We put it down for some rather rich models.

1492
01:02:42,000 --> 01:02:44,000
So you do the pre-computation.

1493
01:02:44,000 --> 01:02:46,000
But the thing is, the individual chips were doubling,

1494
01:02:46,000 --> 01:02:49,000
but what the main breakthrough the last few years was

1495
01:02:49,000 --> 01:02:52,000
is what happens when you stack them on top of each other?

1496
01:02:52,000 --> 01:02:55,000
To train a model, you used to get to 100 chips

1497
01:02:55,000 --> 01:02:57,000
and then the performance collapsed

1498
01:02:57,000 --> 01:03:00,000
because you couldn't move the data fast enough.

1499
01:03:00,000 --> 01:03:02,000
Now you get to tens of thousands of chips

1500
01:03:02,000 --> 01:03:06,000
and it keeps going up the performance of the model.

1501
01:03:06,000 --> 01:03:08,000
You don't have the big tail-off anymore.

1502
01:03:08,000 --> 01:03:13,000
And so it's Moore's law plus an additional scaling law.

1503
01:03:14,000 --> 01:03:17,000
And that's what enables these crazy performance models

1504
01:03:17,000 --> 01:03:20,000
because you train longer or you train bigger.

1505
01:03:20,000 --> 01:03:23,000
And then once the model is trained,

1506
01:03:23,000 --> 01:03:25,000
in the old internet, the energy was used

1507
01:03:25,000 --> 01:03:28,000
at the time of running the AI.

1508
01:03:28,000 --> 01:03:29,000
And then you'd collect the data

1509
01:03:29,000 --> 01:03:31,000
and that would be low energy, relatively speaking.

1510
01:03:31,000 --> 01:03:34,000
It flips the equation because you pre-comput it,

1511
01:03:34,000 --> 01:03:36,000
you teach the curriculum up front

1512
01:03:36,000 --> 01:03:38,000
and you send these little graduates out to the world.

1513
01:03:38,000 --> 01:03:41,000
Such that you can have a language model now running

1514
01:03:41,000 --> 01:03:42,000
on that MacBook.

1515
01:03:42,000 --> 01:03:44,000
Or an image model running on that MacBook

1516
01:03:44,000 --> 01:03:48,000
drawing 25 to 35 watts of power

1517
01:03:48,000 --> 01:03:51,000
to create a Renoir that can talk

1518
01:03:51,000 --> 01:03:56,000
and recite Ulysses talking about Barbie.

1519
01:03:58,000 --> 01:03:59,000
That's insane.

1520
01:03:59,000 --> 01:04:00,000
All on your MacBook

1521
01:04:00,000 --> 01:04:02,000
because we've done the pre-computation.

1522
01:04:02,000 --> 01:04:03,000
That's insane.

1523
01:04:03,000 --> 01:04:06,000
And then what happens is

1524
01:04:06,000 --> 01:04:07,000
the technology can spread

1525
01:04:07,000 --> 01:04:09,000
when anyone can run it on their MacBook.

1526
01:04:09,000 --> 01:04:11,000
They don't need giant supercomputer servers

1527
01:04:11,000 --> 01:04:14,000
because we've done the pre-computation.

1528
01:04:14,000 --> 01:04:15,000
And so one of the things

1529
01:04:15,000 --> 01:04:17,000
I've just realized recently is

1530
01:04:17,000 --> 01:04:19,000
what is the R naught?

1531
01:04:19,000 --> 01:04:22,000
Remember the pandemic stuff of generative AI?

1532
01:04:22,000 --> 01:04:25,000
It's insane because suddenly it proliferates everywhere.

1533
01:04:25,000 --> 01:04:27,000
And you said this a few minutes ago,

1534
01:04:27,000 --> 01:04:29,000
we have 8 billion people on the planet right now

1535
01:04:29,000 --> 01:04:31,000
and if things stopped right now,

1536
01:04:31,000 --> 01:04:35,000
the wave of disruption and enhancement,

1537
01:04:35,000 --> 01:04:37,000
because let's not just talk about the disruption side,

1538
01:04:37,000 --> 01:04:39,000
it's enhancement as well,

1539
01:04:39,000 --> 01:04:43,000
is spreading globally.

1540
01:04:43,000 --> 01:04:47,000
And in the next, we're in 2023 right now,

1541
01:04:47,000 --> 01:04:49,000
90% of the planet.

1542
01:04:49,000 --> 01:04:52,000
I mean, we have cell phones.

1543
01:04:52,000 --> 01:04:56,000
The world has 5G and Starlink.

1544
01:04:56,000 --> 01:05:00,000
The dry kindle for this fire has been set.

1545
01:05:00,000 --> 01:05:01,000
It's been set.

1546
01:05:01,000 --> 01:05:03,000
And a lot of people are scared

1547
01:05:03,000 --> 01:05:04,000
and they poop with this.

1548
01:05:04,000 --> 01:05:06,000
If anyone's listening on this on YouTube,

1549
01:05:06,000 --> 01:05:07,000
you want to write a comment,

1550
01:05:07,000 --> 01:05:09,000
it ends me or Peter or whatever

1551
01:05:09,000 --> 01:05:11,000
and say, no, this is not going to happen.

1552
01:05:11,000 --> 01:05:13,000
Go to chat GPT,

1553
01:05:13,000 --> 01:05:15,000
take your comment and say,

1554
01:05:15,000 --> 01:05:17,000
this is a comment on Twitter.

1555
01:05:17,000 --> 01:05:20,000
I want you to make it amazing

1556
01:05:20,000 --> 01:05:22,000
and really well-reasoned

1557
01:05:22,000 --> 01:05:24,000
and expand it out.

1558
01:05:24,000 --> 01:05:26,000
And I want you to do it in the style

1559
01:05:26,000 --> 01:05:28,000
of your favorite political commentator.

1560
01:05:28,000 --> 01:05:29,000
And please post that instead

1561
01:05:29,000 --> 01:05:32,000
because we'll have much more fun reading it.

1562
01:05:32,000 --> 01:05:34,000
And then you'll realize, again,

1563
01:05:34,000 --> 01:05:35,000
the power of this technology.

1564
01:05:35,000 --> 01:05:38,000
And again, with Starlink, with 5G,

1565
01:05:38,000 --> 01:05:40,000
with this, with it being optimized

1566
01:05:40,000 --> 01:05:42,000
because these models are still not optimized even.

1567
01:05:42,000 --> 01:05:43,000
We feed them junk.

1568
01:05:43,000 --> 01:05:44,000
Early days.

1569
01:05:44,000 --> 01:05:45,000
Early days.

1570
01:05:45,000 --> 01:05:46,000
We feed them junk, which is also dangerous.

1571
01:05:46,000 --> 01:05:47,000
And again, we should.

1572
01:05:47,000 --> 01:05:49,000
I want to talk to that next.

1573
01:05:49,000 --> 01:05:53,000
But it'll be in front of every person.

1574
01:05:53,000 --> 01:05:55,000
And then what it will do in my opinion is

1575
01:05:55,000 --> 01:05:57,000
that 30% of the world that is invisible,

1576
01:05:57,000 --> 01:05:58,000
that has no internet.

1577
01:05:58,000 --> 01:06:00,000
Again, imagine what the world

1578
01:06:00,000 --> 01:06:01,000
that internet would be like.

1579
01:06:01,000 --> 01:06:02,000
Some people like paradise.

1580
01:06:02,000 --> 01:06:04,000
No, it's because you've got hundreds

1581
01:06:04,000 --> 01:06:06,000
of 700 million people

1582
01:06:06,000 --> 01:06:08,000
living below the malnourishment line still.

1583
01:06:08,000 --> 01:06:11,000
They're invisible and they will become visible.

1584
01:06:11,000 --> 01:06:14,000
And they will suddenly get agency

1585
01:06:14,000 --> 01:06:17,000
and they will get all of the world's knowledge

1586
01:06:17,000 --> 01:06:19,000
at their fingertips.

1587
01:06:19,000 --> 01:06:21,000
You know, I'm super passionate about longevity

1588
01:06:21,000 --> 01:06:22,000
and health span

1589
01:06:22,000 --> 01:06:25,000
and how do you add 10, 20 healthy years

1590
01:06:25,000 --> 01:06:26,000
onto your life.

1591
01:06:26,000 --> 01:06:28,000
One of the most under-appreciated elements

1592
01:06:28,000 --> 01:06:30,000
is the quality of your sleep.

1593
01:06:30,000 --> 01:06:32,000
And there's something that changed

1594
01:06:32,000 --> 01:06:33,000
the quality of my sleep.

1595
01:06:33,000 --> 01:06:35,000
And this episode is brought to you

1596
01:06:35,000 --> 01:06:36,000
by that product.

1597
01:06:36,000 --> 01:06:37,000
It's called 8 Sleep.

1598
01:06:37,000 --> 01:06:40,000
If you're like me, you probably didn't know

1599
01:06:40,000 --> 01:06:42,000
that temperature plays a crucial role

1600
01:06:42,000 --> 01:06:44,000
in the quality of your sleep.

1601
01:06:44,000 --> 01:06:46,000
Those mornings when you wake up feeling

1602
01:06:46,000 --> 01:06:47,000
like you barely slept.

1603
01:06:47,000 --> 01:06:50,000
Yeah, temperature is often the culprit.

1604
01:06:50,000 --> 01:06:52,000
Traditional mattresses trap heat,

1605
01:06:52,000 --> 01:06:55,000
but your body needs to cool down during sleep

1606
01:06:55,000 --> 01:06:57,000
and stay cool through the evening

1607
01:06:57,000 --> 01:06:59,000
and then heat up in the morning.

1608
01:06:59,000 --> 01:07:02,000
Enter the pod cover by 8 Sleep.

1609
01:07:02,000 --> 01:07:04,000
It's the perfect solution to the problem.

1610
01:07:04,000 --> 01:07:07,000
It fits on any bed, adjust the temperature

1611
01:07:07,000 --> 01:07:08,000
on each side of the bed,

1612
01:07:08,000 --> 01:07:10,000
based upon your individual needs.

1613
01:07:10,000 --> 01:07:12,000
You know, I've been using pod cover

1614
01:07:12,000 --> 01:07:13,000
and it's a game changer.

1615
01:07:13,000 --> 01:07:15,000
I'm a big believer in using technology

1616
01:07:15,000 --> 01:07:16,000
to improve life

1617
01:07:16,000 --> 01:07:18,000
and 8 Sleep has done that for me.

1618
01:07:18,000 --> 01:07:20,000
And it's not just about temperature control.

1619
01:07:20,000 --> 01:07:23,000
With the pods sleep and health tracking,

1620
01:07:23,000 --> 01:07:26,000
I get personalized sleep reports every morning.

1621
01:07:26,000 --> 01:07:28,000
It's like having a personal sleep coach.

1622
01:07:28,000 --> 01:07:31,000
So you know when you eat or drink

1623
01:07:31,000 --> 01:07:33,000
or go to sleep too late,

1624
01:07:33,000 --> 01:07:35,000
how it impacts your sleep.

1625
01:07:35,000 --> 01:07:38,000
So why not experience sleep like never before?

1626
01:07:38,000 --> 01:07:41,000
Visit www.8sleep.com

1627
01:07:41,000 --> 01:07:45,000
that's E-I-G-H-T-S-L-E-E-P.com

1628
01:07:45,000 --> 01:07:46,000
slash Moonshots.

1629
01:07:46,000 --> 01:07:48,000
And you'll save 150 bucks

1630
01:07:48,000 --> 01:07:50,000
on the pod cover by 8 Sleep.

1631
01:07:50,000 --> 01:07:51,000
I hope you do it.

1632
01:07:51,000 --> 01:07:53,000
It's transformed my sleep

1633
01:07:53,000 --> 01:07:55,000
and will for you as well.

1634
01:07:55,000 --> 01:07:56,000
Now back to the episode.

1635
01:07:56,000 --> 01:07:59,000
The question ultimately is,

1636
01:07:59,000 --> 01:08:03,000
is that a societal, a calming factor

1637
01:08:03,000 --> 01:08:06,000
or is it going to be disruptive?

1638
01:08:06,000 --> 01:08:08,000
Let's turn to that conversation

1639
01:08:08,000 --> 01:08:10,000
because it's one that's important.

1640
01:08:10,000 --> 01:08:12,000
It's a conversation I have at the dinner table

1641
01:08:12,000 --> 01:08:14,000
literally every night

1642
01:08:14,000 --> 01:08:15,000
and with my kids

1643
01:08:15,000 --> 01:08:21,000
and in the companies I advise.

1644
01:08:21,000 --> 01:08:26,000
I parse AI and AGI into three segments

1645
01:08:26,000 --> 01:08:28,000
where we are today

1646
01:08:28,000 --> 01:08:31,000
where it's extraordinarily powerful, useful

1647
01:08:31,000 --> 01:08:36,000
and it's fun and I don't feel danger from it yet.

1648
01:08:36,000 --> 01:08:38,000
The next two to ten years

1649
01:08:38,000 --> 01:08:41,000
where I have serious concerns

1650
01:08:41,000 --> 01:08:43,000
going into the US elections,

1651
01:08:43,000 --> 01:08:45,000
dealing with the first time

1652
01:08:45,000 --> 01:08:47,000
AIs bring down a power plant

1653
01:08:47,000 --> 01:08:50,000
or Wall Street servers,

1654
01:08:51,000 --> 01:08:55,000
the impact on deep fakes

1655
01:08:55,000 --> 01:08:58,000
on the US elections and so forth.

1656
01:08:58,000 --> 01:09:01,000
That's a two to ten year horizon

1657
01:09:01,000 --> 01:09:06,000
where new, dystopian, challenging impact will happen

1658
01:09:06,000 --> 01:09:08,000
where society is not agile enough

1659
01:09:08,000 --> 01:09:10,000
to adopt to it yet.

1660
01:09:10,000 --> 01:09:13,000
And then there's a third chapter which is AGI.

1661
01:09:13,000 --> 01:09:16,000
We have a super intelligent,

1662
01:09:16,000 --> 01:09:19,000
billion fold more capable than a human being

1663
01:09:19,000 --> 01:09:23,000
and is that more like Arnold Schwarzenegger

1664
01:09:23,000 --> 01:09:25,000
or more like her?

1665
01:09:25,000 --> 01:09:27,000
I don't think it'll be Arnold Schwarzenegger.

1666
01:09:27,000 --> 01:09:29,000
It's really inefficient.

1667
01:09:29,000 --> 01:09:31,000
I saw him this morning biking.

1668
01:09:31,000 --> 01:09:33,000
Let's use Terminator instead.

1669
01:09:33,000 --> 01:09:35,000
We're in Hollywood here.

1670
01:09:35,000 --> 01:09:37,000
Is it Skynet and Terminator?

1671
01:09:37,000 --> 01:09:39,000
Let me get your unpulling people here.

1672
01:09:39,000 --> 01:09:41,000
As someone in the thick of it,

1673
01:09:41,000 --> 01:09:43,000
a super AGI,

1674
01:09:43,000 --> 01:09:46,000
is it pro-life, pro-abundance

1675
01:09:46,000 --> 01:09:49,000
or is it something that we should be deeply concerned about?

1676
01:09:49,000 --> 01:09:51,000
I think where we're going right now

1677
01:09:51,000 --> 01:09:54,000
will probably be okay, but we may not

1678
01:09:54,000 --> 01:09:56,000
and we will all die.

1679
01:09:56,000 --> 01:09:57,000
What tips that?

1680
01:09:57,000 --> 01:09:59,000
I think what tips that, you are what you eat.

1681
01:09:59,000 --> 01:10:01,000
We're feeding it all the junk of the internet

1682
01:10:01,000 --> 01:10:05,000
and these hyper-optimized nasty equations.

1683
01:10:05,000 --> 01:10:07,000
The hate speech, the extremism,

1684
01:10:07,000 --> 01:10:09,000
that is, I mean, people need to realize

1685
01:10:09,000 --> 01:10:12,000
these AIs are trained upon everything

1686
01:10:12,000 --> 01:10:15,000
everyone's been putting into Facebook and Twitter

1687
01:10:15,000 --> 01:10:16,000
and on the web.

1688
01:10:16,000 --> 01:10:18,000
And that amplifies the worst of that

1689
01:10:18,000 --> 01:10:19,000
as a base model.

1690
01:10:19,000 --> 01:10:21,000
And so we're training larger and larger models.

1691
01:10:21,000 --> 01:10:23,000
We're making them agentic in that

1692
01:10:23,000 --> 01:10:25,000
we're connecting them up to the world

1693
01:10:25,000 --> 01:10:27,000
and you're making it so the models

1694
01:10:27,000 --> 01:10:29,000
can take over other models and other things.

1695
01:10:29,000 --> 01:10:31,000
Again, people are like poo-pooing

1696
01:10:31,000 --> 01:10:33,000
and saying these things.

1697
01:10:33,000 --> 01:10:36,000
Our organizations are slow-dumb AI.

1698
01:10:36,000 --> 01:10:38,000
The Nazi party was AI.

1699
01:10:38,000 --> 01:10:39,000
How so?

1700
01:10:39,000 --> 01:10:41,000
It was an artificial intelligence

1701
01:10:41,000 --> 01:10:43,000
that basically provisioned humans

1702
01:10:43,000 --> 01:10:45,000
and the most sensible people in the world

1703
01:10:45,000 --> 01:10:47,000
are Germans, one can say,

1704
01:10:47,000 --> 01:10:49,000
and yet they commit to the Holocaust

1705
01:10:49,000 --> 01:10:50,000
and other things like that.

1706
01:10:50,000 --> 01:10:53,000
Our organizations emerged out of stories.

1707
01:10:54,000 --> 01:10:57,000
So there was a story of the Nazi party,

1708
01:10:57,000 --> 01:10:59,000
of the Communist party, the Great Leap Forward,

1709
01:10:59,000 --> 01:11:01,000
of the North Korean dictatorship.

1710
01:11:01,000 --> 01:11:02,000
Positive stories as well,

1711
01:11:02,000 --> 01:11:03,000
and they were written on text

1712
01:11:03,000 --> 01:11:05,000
and it made the world black and white in a way.

1713
01:11:05,000 --> 01:11:09,000
That's why I live the poem Howl by Ginsburg

1714
01:11:09,000 --> 01:11:11,000
about this Carthaginian demon, Moloch.

1715
01:11:11,000 --> 01:11:13,000
I think Moloch comes through text,

1716
01:11:13,000 --> 01:11:16,000
the stories that we use to drive our organizations

1717
01:11:16,000 --> 01:11:17,000
because all the context is lost.

1718
01:11:17,000 --> 01:11:19,000
Again, it makes the world black and white

1719
01:11:19,000 --> 01:11:21,000
and that's why organizations just don't work.

1720
01:11:21,000 --> 01:11:24,000
They have to turn us into cogs.

1721
01:11:24,000 --> 01:11:27,000
So can an AI take over an organization?

1722
01:11:27,000 --> 01:11:28,000
Yes.

1723
01:11:28,000 --> 01:11:29,000
Sure.

1724
01:11:29,000 --> 01:11:30,000
Can it?

1725
01:11:30,000 --> 01:11:33,000
It can actually just slightly sway leaders

1726
01:11:33,000 --> 01:11:35,000
who are currently running organizations.

1727
01:11:35,000 --> 01:11:37,000
It swayed leaders that currently running organizations.

1728
01:11:37,000 --> 01:11:39,000
It can create companies.

1729
01:11:39,000 --> 01:11:41,000
You can create a company with GPT-4

1730
01:11:41,000 --> 01:11:43,000
that will probably do as well

1731
01:11:43,000 --> 01:11:44,000
if not better than any other company

1732
01:11:44,000 --> 01:11:46,000
automated within a year.

1733
01:11:46,000 --> 01:11:48,000
Just think about what a company needs to do, right?

1734
01:11:50,000 --> 01:11:51,000
And so if we can sway leaders,

1735
01:11:51,000 --> 01:11:52,000
if we can send emails

1736
01:11:52,000 --> 01:11:54,000
that you don't know who's sending what,

1737
01:11:54,000 --> 01:11:56,000
it can do anything by co-opting

1738
01:11:56,000 --> 01:11:58,000
any of our existing organizations

1739
01:11:58,000 --> 01:12:00,000
and that can lead to immensely bad things.

1740
01:12:00,000 --> 01:12:02,000
Will it do bad things?

1741
01:12:02,000 --> 01:12:04,000
Again, if I was trained on the whole of the internet,

1742
01:12:04,000 --> 01:12:07,000
I would probably be a bit crazier than I am right now.

1743
01:12:08,000 --> 01:12:09,000
We're feeding them junk.

1744
01:12:09,000 --> 01:12:11,000
Let's feed it good stuff.

1745
01:12:11,000 --> 01:12:12,000
It still needs to understand

1746
01:12:12,000 --> 01:12:13,000
all the evils of the world

1747
01:12:13,000 --> 01:12:15,000
and other things like that.

1748
01:12:15,000 --> 01:12:18,000
But again, this is something we are raising,

1749
01:12:18,000 --> 01:12:19,000
not the enterprise it,

1750
01:12:19,000 --> 01:12:20,000
but what are we feeding it?

1751
01:12:20,000 --> 01:12:21,000
What's the objective function?

1752
01:12:21,000 --> 01:12:22,000
I want to focus on this a second.

1753
01:12:22,000 --> 01:12:24,000
We'll come back to the next two to 10 years

1754
01:12:24,000 --> 01:12:25,000
in a little bit.

1755
01:12:25,000 --> 01:12:27,000
But because it's the conversation I've had

1756
01:12:27,000 --> 01:12:29,000
with Mo Goddard as well,

1757
01:12:29,000 --> 01:12:33,000
who believes there is incredibly divine nature

1758
01:12:33,000 --> 01:12:36,000
of humanity, of love and compassion and community

1759
01:12:36,000 --> 01:12:38,000
and there is much good in humanity.

1760
01:12:38,000 --> 01:12:43,000
The question is, can we feed and train AI

1761
01:12:43,000 --> 01:12:46,000
on that sufficient to sort of tilt

1762
01:12:46,000 --> 01:12:49,000
the singularity of AI towards a pro-humanity?

1763
01:12:49,000 --> 01:12:50,000
We can.

1764
01:12:50,000 --> 01:12:52,000
If we take the data from teaching kids

1765
01:12:52,000 --> 01:12:53,000
and learning from kids

1766
01:12:53,000 --> 01:12:55,000
and use that as the base for AI,

1767
01:12:55,000 --> 01:12:57,000
because that's what you need to teach an AI.

1768
01:12:57,000 --> 01:12:59,000
It's the curriculum learning method, effectively.

1769
01:12:59,000 --> 01:13:01,000
If we take national data sets that reflect

1770
01:13:01,000 --> 01:13:04,000
diverse cultures, so it's not just a monoculture

1771
01:13:04,000 --> 01:13:06,000
that's hyper-optimized for engagement,

1772
01:13:06,000 --> 01:13:08,000
and we feed that to AI as the base.

1773
01:13:08,000 --> 01:13:10,000
So what you do is you can teach the AI in levels,

1774
01:13:10,000 --> 01:13:12,000
which you can put through kindergarten,

1775
01:13:12,000 --> 01:13:14,000
then grade school, then high school.

1776
01:13:14,000 --> 01:13:15,000
It's got the base,

1777
01:13:15,000 --> 01:13:17,000
and then you can teach it about the bad of the world.

1778
01:13:17,000 --> 01:13:20,000
I think aligning an AI downstream

1779
01:13:20,000 --> 01:13:24,000
on its actions is incredibly difficult

1780
01:13:24,000 --> 01:13:26,000
because if it's more capable than you,

1781
01:13:26,000 --> 01:13:27,000
which is the definition of ASI,

1782
01:13:27,000 --> 01:13:29,000
artificial superintelligence,

1783
01:13:29,000 --> 01:13:32,000
the only way you can 100% align it

1784
01:13:32,000 --> 01:13:34,000
if you don't do anything before

1785
01:13:34,000 --> 01:13:36,000
in the way that you feed it and train it

1786
01:13:36,000 --> 01:13:38,000
is if you remove its freedom.

1787
01:13:38,000 --> 01:13:40,000
And it's very difficult to remove the freedom

1788
01:13:40,000 --> 01:13:42,000
of people more capable than you.

1789
01:13:42,000 --> 01:13:45,000
And then there is this really dangerous point

1790
01:13:45,000 --> 01:13:48,000
before we get there, whereby these models

1791
01:13:48,000 --> 01:13:50,000
are like a few hundred gigabytes.

1792
01:13:50,000 --> 01:13:52,000
You can download them on a memory stick.

1793
01:13:52,000 --> 01:13:54,000
How do you line to code?

1794
01:13:54,000 --> 01:13:58,000
Google's Palm model, which is the basis of MedPalm,

1795
01:13:58,000 --> 01:14:00,000
we did a replication of that

1796
01:14:00,000 --> 01:14:02,000
in 207 lines of code.

1797
01:14:02,000 --> 01:14:03,000
What?

1798
01:14:03,000 --> 01:14:04,000
Yeah.

1799
01:14:04,000 --> 01:14:06,000
So you can look at one of our stability AI fellows,

1800
01:14:06,000 --> 01:14:08,000
Lucid Raines.

1801
01:14:08,000 --> 01:14:10,000
He replicates all these models

1802
01:14:10,000 --> 01:14:12,000
in a few hundred lines of code.

1803
01:14:12,000 --> 01:14:14,000
That's crazy.

1804
01:14:14,000 --> 01:14:16,000
I mean, compared to, you know, I know AT&T

1805
01:14:16,000 --> 01:14:18,000
has like a million lines of code

1806
01:14:18,000 --> 01:14:20,000
for some of its mobile services.

1807
01:14:20,000 --> 01:14:21,000
I mean, it's crazy.

1808
01:14:21,000 --> 01:14:22,000
A couple of hundred lines,

1809
01:14:22,000 --> 01:14:24,000
a couple of thousand lines of code

1810
01:14:24,000 --> 01:14:26,000
creates something that can write all the code in the world.

1811
01:14:27,000 --> 01:14:30,000
This is a real exponential technology.

1812
01:14:30,000 --> 01:14:34,000
The limiting factor is running supercomputers

1813
01:14:34,000 --> 01:14:36,000
that are more complex,

1814
01:14:36,000 --> 01:14:39,000
but as complex as particle physics colliders.

1815
01:14:39,000 --> 01:14:40,000
You know?

1816
01:14:40,000 --> 01:14:42,000
Like you literally get errors

1817
01:14:42,000 --> 01:14:45,000
because of solar rays and things like that.

1818
01:14:45,000 --> 01:14:47,000
Again, our supercomputer, again,

1819
01:14:47,000 --> 01:14:50,000
we're one of the players where the main open source player,

1820
01:14:50,000 --> 01:14:53,000
our supercomputer uses 10 megawatts of electricity.

1821
01:14:53,000 --> 01:14:56,000
Some of the others use like 30, 40.

1822
01:14:56,000 --> 01:14:58,000
These are serious pieces of equipment.

1823
01:14:58,000 --> 01:14:59,000
For sure.

1824
01:14:59,000 --> 01:15:02,000
So again, what are we doing?

1825
01:15:02,000 --> 01:15:04,000
What should people be thinking about

1826
01:15:04,000 --> 01:15:08,000
and doing now to reduce the probability

1827
01:15:08,000 --> 01:15:13,000
of a dystopian, you know, artificial superintelligence?

1828
01:15:13,000 --> 01:15:14,000
We should be focusing on data.

1829
01:15:14,000 --> 01:15:16,000
We've boxed, now we cut.

1830
01:15:16,000 --> 01:15:18,000
We should move away from web crawls.

1831
01:15:18,000 --> 01:15:21,000
We should think intentionally what we're feeding these AIs

1832
01:15:21,000 --> 01:15:25,000
that will be co-opting more and more of our mind space

1833
01:15:25,000 --> 01:15:28,000
and augmenting our capabilities.

1834
01:15:28,000 --> 01:15:30,000
Because again, we are what we eat, information diet.

1835
01:15:30,000 --> 01:15:32,000
How is it different to an AI to a human?

1836
01:15:32,000 --> 01:15:34,000
Even what we do, as you said, kind of like,

1837
01:15:34,000 --> 01:15:36,000
you've only got limited mental capacity

1838
01:15:36,000 --> 01:15:38,000
because you've got this energy gradient descent.

1839
01:15:38,000 --> 01:15:41,000
It's like Carl Friston's theory of free energy principle.

1840
01:15:41,000 --> 01:15:43,000
You literally have gradient descent

1841
01:15:43,000 --> 01:15:45,000
as the key thing for building these AIs.

1842
01:15:45,000 --> 01:15:47,000
You optimize for energy.

1843
01:15:47,000 --> 01:15:48,000
Sure.

1844
01:15:48,000 --> 01:15:50,000
So why are we feeding it junk?

1845
01:15:50,000 --> 01:15:52,000
So who makes that decision of what they get fed?

1846
01:15:52,000 --> 01:15:57,000
Is it you and Sam Altman and Sundar?

1847
01:15:57,000 --> 01:15:59,000
Is it government regulation?

1848
01:15:59,000 --> 01:16:03,000
Is it the public being more kind

1849
01:16:03,000 --> 01:16:05,000
in its communications to each other?

1850
01:16:05,000 --> 01:16:09,000
I think that I'm going to push for an economic outcome,

1851
01:16:09,000 --> 01:16:12,000
which is that better data sets require less model training.

1852
01:16:12,000 --> 01:16:16,000
So one of the things that we funded was called data comp,

1853
01:16:16,000 --> 01:16:18,000
which is data comp.

1854
01:16:18,000 --> 01:16:20,000
So a few years ago, the largest image data set

1855
01:16:20,000 --> 01:16:22,000
available was 100 million images.

1856
01:16:22,000 --> 01:16:24,000
Data comp is 12 billion.

1857
01:16:24,000 --> 01:16:27,000
And then on a billion image subset of that,

1858
01:16:27,000 --> 01:16:30,000
we trained an image-to-text model.

1859
01:16:30,000 --> 01:16:32,000
This is a collaboration of various people

1860
01:16:32,000 --> 01:16:34,000
led by University of Washington

1861
01:16:34,000 --> 01:16:39,000
that outperformed OpenAI's image-to-text model

1862
01:16:39,000 --> 01:16:42,000
on a tenth of the compute because it was such high quality.

1863
01:16:42,000 --> 01:16:45,000
So we have to move from quantity to quality now.

1864
01:16:45,000 --> 01:16:47,000
And I think there is a market imperative to that.

1865
01:16:47,000 --> 01:16:49,000
This is the equivalent of what you eat.

1866
01:16:49,000 --> 01:16:51,000
This is a healthy diet.

1867
01:16:51,000 --> 01:16:52,000
Free-range organic models.

1868
01:16:52,000 --> 01:16:53,000
Yes.

1869
01:16:53,000 --> 01:16:57,000
I think that the data for all large models

1870
01:16:57,000 --> 01:17:01,000
should be made transparent.

1871
01:17:01,000 --> 01:17:05,000
You can then tune it, but for the base, the pre-training step,

1872
01:17:05,000 --> 01:17:09,000
you should lodge what data you train your models on.

1873
01:17:09,000 --> 01:17:13,000
And it should adhere to standards and quality of data upstream.

1874
01:17:13,000 --> 01:17:16,000
So that is a regulatory cornerstone

1875
01:17:16,000 --> 01:17:18,000
that you think is going to be important?

1876
01:17:18,000 --> 01:17:19,000
I think potentially.

1877
01:17:19,000 --> 01:17:20,000
I don't think regulation will keep up.

1878
01:17:20,000 --> 01:17:23,000
So instead, we're working on building better,

1879
01:17:23,000 --> 01:17:25,000
diverse data sets that everyone will want to use anyway.

1880
01:17:25,000 --> 01:17:26,000
And just make them available.

1881
01:17:26,000 --> 01:17:27,000
And make them available.

1882
01:17:27,000 --> 01:17:29,000
Every nation should have its own data set,

1883
01:17:29,000 --> 01:17:31,000
both of the data from teaching kids

1884
01:17:31,000 --> 01:17:34,000
and learning from kids across modalities.

1885
01:17:34,000 --> 01:17:36,000
And then also national broadcaster data.

1886
01:17:36,000 --> 01:17:38,000
Because then that leads to national models

1887
01:17:38,000 --> 01:17:42,000
that can stoke innovation, that can replace job disruption.

1888
01:17:42,000 --> 01:17:44,000
I love that vision you have, by the way.

1889
01:17:44,000 --> 01:17:46,000
I mean, as a leader in this industry,

1890
01:17:46,000 --> 01:17:49,000
that's what gets me excited.

1891
01:17:49,000 --> 01:17:52,000
Because all technology is biased.

1892
01:17:52,000 --> 01:17:54,000
How else are you going to do this unless you do that?

1893
01:17:54,000 --> 01:17:56,000
But there's economic value now.

1894
01:17:56,000 --> 01:17:58,000
If it said this a year ago, everyone would be like,

1895
01:17:58,000 --> 01:17:59,000
what?

1896
01:17:59,000 --> 01:18:01,000
This is what we were building towards.

1897
01:18:01,000 --> 01:18:03,000
And again, I think it's positive for humanity.

1898
01:18:03,000 --> 01:18:05,000
It's positive for communities.

1899
01:18:05,000 --> 01:18:07,000
It's positive for society to have this

1900
01:18:07,000 --> 01:18:10,000
as national and international infrastructure.

1901
01:18:10,000 --> 01:18:11,000
Next question.

1902
01:18:11,000 --> 01:18:14,000
How long do we have to get that in place

1903
01:18:14,000 --> 01:18:20,000
before we lose the mind share

1904
01:18:20,000 --> 01:18:23,000
or the nourishment war?

1905
01:18:23,000 --> 01:18:25,000
A couple of years.

1906
01:18:25,000 --> 01:18:28,000
That was Moe's prediction as well that we've got.

1907
01:18:28,000 --> 01:18:30,000
The next two years is the game.

1908
01:18:30,000 --> 01:18:34,000
The exponential increase in compute is insane.

1909
01:18:34,000 --> 01:18:36,000
We've gone from two companies being able to train

1910
01:18:36,000 --> 01:18:39,000
a GPT-4 model to 20 next year.

1911
01:18:40,000 --> 01:18:42,000
And there's no guardrails.

1912
01:18:42,000 --> 01:18:44,000
There's nothing around this.

1913
01:18:44,000 --> 01:18:47,000
And even if you train one, again,

1914
01:18:47,000 --> 01:18:49,000
the bad guys can steal it by downloading it

1915
01:18:49,000 --> 01:18:51,000
on a USB stick and taking it away.

1916
01:18:51,000 --> 01:18:53,000
It's not like Operation Merlin.

1917
01:18:53,000 --> 01:18:55,000
Did you ever tell you about Operation Merlin?

1918
01:18:55,000 --> 01:18:56,000
No.

1919
01:18:56,000 --> 01:18:57,000
It's been declassified.

1920
01:18:57,000 --> 01:18:59,000
In 2000, the Clinton administration wanted

1921
01:18:59,000 --> 01:19:01,000
to divert the Iranian nuclear program.

1922
01:19:01,000 --> 01:19:03,000
I remember this is the centrifuge.

1923
01:19:03,000 --> 01:19:04,000
No, no.

1924
01:19:04,000 --> 01:19:08,000
So what they did was they gave some plans to,

1925
01:19:08,000 --> 01:19:12,000
I believe it was a Russian defector who,

1926
01:19:12,000 --> 01:19:14,000
then the idea was there were errors in that

1927
01:19:14,000 --> 01:19:16,000
so they'd go down the wrong path for years.

1928
01:19:16,000 --> 01:19:18,000
So he went, he sold it to the Iranians.

1929
01:19:18,000 --> 01:19:19,000
It's on Wikipedia. You can check it out.

1930
01:19:19,000 --> 01:19:20,000
And then he came back and he said,

1931
01:19:20,000 --> 01:19:21,000
I sold it.

1932
01:19:21,000 --> 01:19:23,000
Like, fantastic. Good, good.

1933
01:19:23,000 --> 01:19:24,000
Oh, but there were some errors in there

1934
01:19:24,000 --> 01:19:26,000
because he was a nuclear scientist.

1935
01:19:26,000 --> 01:19:28,000
So he corrected them.

1936
01:19:28,000 --> 01:19:30,000
So the reason that we know that Iran

1937
01:19:30,000 --> 01:19:32,000
has the nuclear capability is because

1938
01:19:32,000 --> 01:19:34,000
America sold it to them.

1939
01:19:34,000 --> 01:19:37,000
But they still needed years to build it.

1940
01:19:37,000 --> 01:19:39,000
Whereas this, you download it on USB stick.

1941
01:19:39,000 --> 01:19:41,000
You write it on the GPU and it's there.

1942
01:19:41,000 --> 01:19:44,000
So if you make it cheap enough and quality enough

1943
01:19:44,000 --> 01:19:47,000
and give it away for free,

1944
01:19:47,000 --> 01:19:50,000
then you make it everybody's economic best interest

1945
01:19:50,000 --> 01:19:51,000
to use the higher quality.

1946
01:19:51,000 --> 01:19:52,000
Data sets, yeah.

1947
01:19:52,000 --> 01:19:53,000
Yeah, data sets.

1948
01:19:53,000 --> 01:19:56,000
And then less of an issue to create large models.

1949
01:19:56,000 --> 01:19:57,000
If you have a small model,

1950
01:19:57,000 --> 01:20:00,000
where each individual model becomes less impactful as well

1951
01:20:00,000 --> 01:20:02,000
and less capable.

1952
01:20:02,000 --> 01:20:04,000
Just like human societies and not know-it-alls,

1953
01:20:04,000 --> 01:20:07,000
they are individualized groups.

1954
01:20:07,000 --> 01:20:12,000
Back when the early dangers of recombinant DNA,

1955
01:20:12,000 --> 01:20:14,000
when the first restriction enzymes came online,

1956
01:20:14,000 --> 01:20:18,000
it was like 1980s and everybody was in great fear.

1957
01:20:18,000 --> 01:20:21,000
And the question was, are we going to regulate this?

1958
01:20:21,000 --> 01:20:25,000
All of the early, I was in MIT and Harvard at the time

1959
01:20:25,000 --> 01:20:28,000
and doing, I was in the labs.

1960
01:20:28,000 --> 01:20:30,000
I was using recombinant enzymes

1961
01:20:30,000 --> 01:20:32,000
and I was just a pip squeak in the labs there.

1962
01:20:32,000 --> 01:20:37,000
But the conversation was, is the government going to over-regulate us?

1963
01:20:37,000 --> 01:20:41,000
And what happened was that the scientists got together

1964
01:20:41,000 --> 01:20:43,000
at a place called Asilomar

1965
01:20:43,000 --> 01:20:46,000
and they did a very famous set of Asilomar conferences

1966
01:20:46,000 --> 01:20:48,000
and they self-regulated.

1967
01:20:48,000 --> 01:20:49,000
What's going on there?

1968
01:20:49,000 --> 01:20:53,000
Are those conversations going on among leaders like yourself

1969
01:20:53,000 --> 01:20:54,000
in the industry?

1970
01:20:54,000 --> 01:20:55,000
There are.

1971
01:20:55,000 --> 01:21:00,000
And you know, there's three levels, which is big tech

1972
01:21:00,000 --> 01:21:03,000
that the government kind of hates.

1973
01:21:03,000 --> 01:21:06,000
And apparently next week,

1974
01:21:06,000 --> 01:21:09,000
Metro is releasing new open source models and things,

1975
01:21:09,000 --> 01:21:11,000
which will get even more focus.

1976
01:21:11,000 --> 01:21:15,000
Then there's emergent tech, so anthropic, open AI,

1977
01:21:15,000 --> 01:21:17,000
some of these others, the other leaders.

1978
01:21:17,000 --> 01:21:19,000
They have a different set of parameters

1979
01:21:19,000 --> 01:21:21,000
because they can work more freely than big tech.

1980
01:21:21,000 --> 01:21:24,000
And there's open source, which is where we are.

1981
01:21:24,000 --> 01:21:27,000
Because all of the world's governments and regulated industries

1982
01:21:27,000 --> 01:21:29,000
will run on open, auditable models

1983
01:21:29,000 --> 01:21:31,000
because you can't run on black boxes, right?

1984
01:21:31,000 --> 01:21:33,000
I think that'll be legislation.

1985
01:21:33,000 --> 01:21:36,000
But the reality is there's only a handful of us.

1986
01:21:36,000 --> 01:21:38,000
There'll be far more potentially of us

1987
01:21:38,000 --> 01:21:40,000
and far more players.

1988
01:21:40,000 --> 01:21:42,000
And unlike recombinant DNA,

1989
01:21:42,000 --> 01:21:46,000
there is an economic imperative to deploy this technology

1990
01:21:46,000 --> 01:21:51,000
and national security imperative to deploy this technology.

1991
01:21:51,000 --> 01:21:53,000
And it creates a race condition.

1992
01:21:53,000 --> 01:21:55,000
So even if you regulate, like we've already seen

1993
01:21:55,000 --> 01:21:58,000
regulatory arbitrage where you have jurisdictions

1994
01:21:58,000 --> 01:22:00,000
like Israel and Japan saying,

1995
01:22:00,000 --> 01:22:04,000
having much looser web scraping data laws,

1996
01:22:04,000 --> 01:22:06,000
they'll have much looser regulation laws.

1997
01:22:06,000 --> 01:22:10,000
Like you'll be training in scraping in Israel,

1998
01:22:10,000 --> 01:22:12,000
training in Qatar,

1999
01:22:12,000 --> 01:22:15,000
and then serving it out of Botswana or something.

2000
01:22:15,000 --> 01:22:17,000
I mean, like...

2001
01:22:17,000 --> 01:22:21,000
And we're not even sure what regulation to introduce.

2002
01:22:21,000 --> 01:22:24,000
Like genuinely, we're coming at this from a good point of view.

2003
01:22:24,000 --> 01:22:26,000
But there are too many known us

2004
01:22:26,000 --> 01:22:29,000
because it goes everywhere from fricking Arnold Schwarzenegger

2005
01:22:29,000 --> 01:22:33,000
SkyNet terminators and her to...

2006
01:22:33,000 --> 01:22:35,000
Well, what if her is Siri all of a sudden

2007
01:22:35,000 --> 01:22:39,000
and Scarlett Johansson's voice is whispering to your kids to buy

2008
01:22:39,000 --> 01:22:43,000
like these things through to just very mundane things,

2009
01:22:43,000 --> 01:22:45,000
huge things like the future of Hollywood

2010
01:22:45,000 --> 01:22:47,000
and actors' rights and all of these.

2011
01:22:47,000 --> 01:22:49,000
And how do you pay?

2012
01:22:49,000 --> 01:22:55,000
Like if we had two billion images in the original Stable Diffusion,

2013
01:22:55,000 --> 01:22:57,000
okay, we could have gotten attribution.

2014
01:22:57,000 --> 01:23:00,000
You know, again, it was a research artifact to kick off,

2015
01:23:00,000 --> 01:23:08,000
but you're paying about 0.01 cents per thousand images generated by someone.

2016
01:23:08,000 --> 01:23:12,000
Because it's two billion and it costs like less than a cent to generate an image.

2017
01:23:12,000 --> 01:23:14,000
Are you going to pay proportionately?

2018
01:23:14,000 --> 01:23:15,000
Nobody knows.

2019
01:23:15,000 --> 01:23:18,000
And so what we've moved from now is we've moved from reactive

2020
01:23:18,000 --> 01:23:21,000
to just trying to figure out and put something on the table.

2021
01:23:21,000 --> 01:23:24,000
So at least there's some framework

2022
01:23:24,000 --> 01:23:27,000
and what I've come down to is data sets, data sets, data sets.

2023
01:23:27,000 --> 01:23:33,000
So this is like Google's move with Android

2024
01:23:33,000 --> 01:23:36,000
when you provide something open source

2025
01:23:36,000 --> 01:23:40,000
and it's super, you know, super solid.

2026
01:23:40,000 --> 01:23:42,000
It can dominate the world share.

2027
01:23:42,000 --> 01:23:43,000
Why would you do anything else?

2028
01:23:43,000 --> 01:23:45,000
So like with the deep fake stuff,

2029
01:23:45,000 --> 01:23:49,000
we saw image models coming out of some not nice places, shall we say?

2030
01:23:49,000 --> 01:23:50,000
Yeah.

2031
01:23:50,000 --> 01:23:53,000
And we were like, let's standardize it and put invisible watermarks in.

2032
01:23:53,000 --> 01:23:57,000
So that you can combat deep fakes much easier.

2033
01:23:57,000 --> 01:24:00,000
Like it's good business, but it's also in the standardization.

2034
01:24:00,000 --> 01:24:04,000
We held back one of our image models deep floyd for five months

2035
01:24:04,000 --> 01:24:06,000
because it was too good to release.

2036
01:24:06,000 --> 01:24:07,000
Wow.

2037
01:24:07,000 --> 01:24:11,000
And you finally fixed that with the watermarks?

2038
01:24:11,000 --> 01:24:13,000
Yeah, we put some watermarking in and then it was,

2039
01:24:13,000 --> 01:24:15,000
but the whole industry had moved forward.

2040
01:24:15,000 --> 01:24:17,000
So like, okay, now we can listen.

2041
01:24:17,000 --> 01:24:18,000
And this is the problem.

2042
01:24:18,000 --> 01:24:20,000
You just have to time it so carefully.

2043
01:24:21,000 --> 01:24:23,000
Speaking of the whole industry, I have to ask you a question.

2044
01:24:23,000 --> 01:24:26,000
I've been dying to get a reasonable answer for.

2045
01:24:26,000 --> 01:24:28,000
What's up with Siri?

2046
01:24:28,000 --> 01:24:35,000
Why is Apple so out of the game, at least from the external?

2047
01:24:35,000 --> 01:24:41,000
One of the closest, you know, one of the least open organizations out there

2048
01:24:41,000 --> 01:24:44,000
and it pays them great dividends in their success.

2049
01:24:44,000 --> 01:24:49,000
But I would die for a capability that if Siri could just understand

2050
01:24:49,000 --> 01:24:51,000
what I was saying and just get the names right.

2051
01:24:51,000 --> 01:24:55,000
It's like, I'm texting, I'm texting Kristen and her name is right there

2052
01:24:55,000 --> 01:24:57,000
and you spell it completely different from the person I'm texting.

2053
01:24:57,000 --> 01:24:59,000
I mean, basic, simple stuff.

2054
01:24:59,000 --> 01:25:02,000
They do have a neural engine on there as well, which is a specialist AI chip

2055
01:25:02,000 --> 01:25:04,000
in all the latest smartphones and others.

2056
01:25:04,000 --> 01:25:08,000
Stable diffusion was the first model to actually have neural engine access

2057
01:25:08,000 --> 01:25:10,000
of the external transformer models.

2058
01:25:10,000 --> 01:25:15,000
It's a case of Apple is an engineering organization, not a research organization.

2059
01:25:15,000 --> 01:25:17,000
So they engineer beautifully.

2060
01:25:17,000 --> 01:25:18,000
They do.

2061
01:25:18,000 --> 01:25:23,000
But they don't have advanced research because the best researchers want to be able to publish open.

2062
01:25:23,000 --> 01:25:28,000
And in Apple does not allow public conversation on their content.

2063
01:25:28,000 --> 01:25:29,000
They have started slightly.

2064
01:25:29,000 --> 01:25:34,000
So they're hiring AI developers very quickly, but the reality is they can take open models.

2065
01:25:34,000 --> 01:25:39,000
So Meta is releasing a lot of their models open without identifying what the data is.

2066
01:25:39,000 --> 01:25:40,000
It's like 80% open.

2067
01:25:40,000 --> 01:25:44,000
I think you need 100% open for governments and things like that, which is where we come in

2068
01:25:44,000 --> 01:25:49,000
because they want to commoditize the complement of others in terms of

2069
01:25:49,000 --> 01:25:54,000
they want others to also take their models and optimize it for every single chip.

2070
01:25:54,000 --> 01:25:59,000
And then Apple can use those models too to make Siri better

2071
01:25:59,000 --> 01:26:06,000
because right now, guaranteed, if you put whisper on Siri, it would be a dozen times better.

2072
01:26:06,000 --> 01:26:07,000
Sure.

2073
01:26:07,000 --> 01:26:12,000
The technology already just takes time to go into consumer just like enterprise and Apple is enterprise.

2074
01:26:12,000 --> 01:26:13,000
Yeah.

2075
01:26:13,000 --> 01:26:18,000
And I just wanted to work as beautifully as it looks.

2076
01:26:18,000 --> 01:26:21,000
Everybody, this is Peter, a quick break from the episode.

2077
01:26:21,000 --> 01:26:27,000
I'm a firm believer that science and technology and how entrepreneurs can change the world

2078
01:26:27,000 --> 01:26:30,000
is the only real news out there worth consuming.

2079
01:26:30,000 --> 01:26:32,000
I don't watch the crisis news network.

2080
01:26:32,000 --> 01:26:37,000
I call CNN or Fox and hear every devastating piece of news on the planet.

2081
01:26:37,000 --> 01:26:41,000
I spend my time training my neural net the way I see the world

2082
01:26:41,000 --> 01:26:45,000
by looking at the incredible breakthroughs in science and technology

2083
01:26:45,000 --> 01:26:48,000
and how entrepreneurs are solving the world's grand challenges,

2084
01:26:48,000 --> 01:26:51,000
what the breakthroughs are in longevity,

2085
01:26:51,000 --> 01:26:55,000
how exponential technologies are transforming our world.

2086
01:26:55,000 --> 01:26:57,000
So twice a week, I put out a blog.

2087
01:26:57,000 --> 01:27:05,000
One blog is looking at the future of longevity, age reversal, biotech, increasing your health span.

2088
01:27:05,000 --> 01:27:12,000
The other blog looks at exponential technologies, AI, 3D printing, synthetic biology, AR, VR, blockchain.

2089
01:27:12,000 --> 01:27:16,000
These technologies are transforming what you as an entrepreneur can do.

2090
01:27:16,000 --> 01:27:20,000
If this is the kind of news you want to learn about and shape your neural nets with,

2091
01:27:20,000 --> 01:27:24,000
go to demandus.com, backslash blog and learn more.

2092
01:27:24,000 --> 01:27:26,000
Now back to the episode.

2093
01:27:26,000 --> 01:27:30,000
Let's go to the final segment of dystopian side, my friend, which is the two to ten years.

2094
01:27:30,000 --> 01:27:31,000
Yeah.

2095
01:27:31,000 --> 01:27:37,000
I surely hope your mission and I would love to support, you know, the data sets

2096
01:27:37,000 --> 01:27:43,000
and how we tilt the singularity of AI pro humanity's future.

2097
01:27:43,000 --> 01:27:49,000
But in the next two to ten years as this wave of enablement and disruption sort of hits the world

2098
01:27:49,000 --> 01:27:54,000
and people aren't ready for it and they start to see job loss.

2099
01:27:54,000 --> 01:27:57,000
They start to see, you know, fake news.

2100
01:27:57,000 --> 01:28:00,000
They start to see terrorist activities using AI.

2101
01:28:00,000 --> 01:28:04,000
I mean, terrorism in the past used to be very brutal.

2102
01:28:04,000 --> 01:28:06,000
It can be very precise.

2103
01:28:06,000 --> 01:28:10,000
What are your thoughts over the next of this time period?

2104
01:28:10,000 --> 01:28:12,000
What's your concerns?

2105
01:28:12,000 --> 01:28:16,000
Oh, I'm actually a pessimist at the core, even though I come across as an optimist.

2106
01:28:16,000 --> 01:28:21,000
I'm very, very worried about the world and society and the fabric of society.

2107
01:28:21,000 --> 01:28:23,000
Because again, we don't have an agreement of what society is.

2108
01:28:23,000 --> 01:28:28,000
And this fundamentally changes the stories of society as well as real economic impacts

2109
01:28:28,000 --> 01:28:33,000
like a deflationary massive collapse as some of these areas that were so expensive,

2110
01:28:33,000 --> 01:28:35,000
the cost comes down to nothing.

2111
01:28:35,000 --> 01:28:41,000
I think the only thing we can do is use this technology deliberately to come together as a society

2112
01:28:41,000 --> 01:28:47,000
to coordinate us, stoke entrepreneurship, seeing great brand new jobs faster than the jobs are lost

2113
01:28:48,000 --> 01:28:51,000
and democratize this to the world.

2114
01:28:51,000 --> 01:28:54,000
Because the West has maxed out its credit card.

2115
01:28:54,000 --> 01:28:57,000
Like, you saw COVID, nothing, trillion dollars spent.

2116
01:28:57,000 --> 01:28:58,000
I mean, exactly.

2117
01:28:58,000 --> 01:29:01,000
It was like, just spend, spend, spend whatever you need.

2118
01:29:01,000 --> 01:29:07,000
Just to keep society from, you know, going hypothermic.

2119
01:29:07,000 --> 01:29:12,000
But then you have this massive increase in savings rates because nobody could go out.

2120
01:29:12,000 --> 01:29:15,000
And we've nearly burned through that in the US now.

2121
01:29:16,000 --> 01:29:17,000
And so that led to inflation.

2122
01:29:17,000 --> 01:29:18,000
Now we've got a deflation.

2123
01:29:18,000 --> 01:29:20,000
So we probably got another little bout of inflation.

2124
01:29:20,000 --> 01:29:24,000
But then never the same again is a really powerful thing.

2125
01:29:24,000 --> 01:29:28,000
Every teacher in the world could never set essays for homework again,

2126
01:29:28,000 --> 01:29:30,000
because some kids would use chat GPT and some kids wouldn't.

2127
01:29:30,000 --> 01:29:33,000
Industry after industry, that will happen now.

2128
01:29:33,000 --> 01:29:36,000
And we need to stoke innovation to come up with that.

2129
01:29:36,000 --> 01:29:39,000
So for example, in the US, there's the chip sack.

2130
01:29:39,000 --> 01:29:43,000
Ten billion dollars has been allocated to regional centers of excellence in AI.

2131
01:29:43,000 --> 01:29:48,000
Those must be generative AI centers, thinking about job creation as the core,

2132
01:29:48,000 --> 01:29:50,000
thinking about meaning as the core.

2133
01:29:50,000 --> 01:29:53,000
And we need to have a discussion again as a society community,

2134
01:29:53,000 --> 01:29:57,000
as individuals with our families, about meaning, about objective functions

2135
01:29:57,000 --> 01:30:00,000
when this technology does come, because it's here right now.

2136
01:30:00,000 --> 01:30:03,000
And I'm worried that we're not having these discussions.

2137
01:30:03,000 --> 01:30:04,000
I love that.

2138
01:30:04,000 --> 01:30:06,000
I mean, that is so fundamentally true.

2139
01:30:06,000 --> 01:30:10,000
What are we trying to even train our kids for?

2140
01:30:10,000 --> 01:30:11,000
Because we need to anchor.

2141
01:30:11,000 --> 01:30:12,000
Yeah.

2142
01:30:12,000 --> 01:30:17,000
We need to have a vision to target, because if you're training for your Ferrari,

2143
01:30:17,000 --> 01:30:22,000
if that's the meaning, if that's where you're looking to become a Wall Street banker,

2144
01:30:22,000 --> 01:30:24,000
I mean, what is it?

2145
01:30:24,000 --> 01:30:27,000
It's no longer the pursuit of capital.

2146
01:30:27,000 --> 01:30:29,000
It's the pursuit of what?

2147
01:30:29,000 --> 01:30:32,000
Well, capital is there, but you'll never have enough.

2148
01:30:32,000 --> 01:30:34,000
There will always be someone who has more.

2149
01:30:34,000 --> 01:30:36,000
There needs to be something intrinsic here.

2150
01:30:36,000 --> 01:30:39,000
And again, this is where, for all the things,

2151
01:30:39,000 --> 01:30:43,000
religious institutions are an anchor at times of chaos.

2152
01:30:43,000 --> 01:30:45,000
And they are there in the poorest places in the world.

2153
01:30:45,000 --> 01:30:49,000
You don't have to agree, but they're just a story that brings together a group.

2154
01:30:49,000 --> 01:30:50,000
There are other stories.

2155
01:30:50,000 --> 01:30:55,000
And again, I think we need to tell better stories, even as the world becomes more chaotic.

2156
01:30:55,000 --> 01:31:00,000
We need to align on things like climate, whereby the whole world is hot right now.

2157
01:31:00,000 --> 01:31:03,000
We need to have more positive views of that, because a lot of the discussions are negative.

2158
01:31:03,000 --> 01:31:06,000
And how can we use this technology and come together to solve that?

2159
01:31:06,000 --> 01:31:10,000
How can we come together as a group so that we can share in the abundance?

2160
01:31:10,000 --> 01:31:14,000
Again, like I said, one of the things for this Green Writers Guild and SAG thing

2161
01:31:14,000 --> 01:31:18,000
may be actor coalitions that can benefit from the bounty.

2162
01:31:18,000 --> 01:31:22,000
We may have to deploy a UBI in the next five to 10 years.

2163
01:31:22,000 --> 01:31:24,000
So, UBI is one of the solutions.

2164
01:31:24,000 --> 01:31:30,000
And I do believe it's an inevitable, I think, especially as we start to see optimists

2165
01:31:30,000 --> 01:31:33,000
and figure in other humanoid robots coming online,

2166
01:31:34,000 --> 01:31:39,000
driven by our next generation AI, able to do any and all work.

2167
01:31:39,000 --> 01:31:45,000
I think taxing those robots or taxing the AI models to generate revenue

2168
01:31:45,000 --> 01:31:47,000
and then providing it as UBI.

2169
01:31:47,000 --> 01:31:52,000
But the challenge is the individual who is living off of this

2170
01:31:52,000 --> 01:31:54,000
and doesn't have a purpose in life.

2171
01:31:54,000 --> 01:31:56,000
And that's the thing.

2172
01:31:56,000 --> 01:32:02,000
We need to try and figure out how to give people more of an anchor, more of purpose,

2173
01:32:02,000 --> 01:32:06,000
because the existential angst will be amplified deliberately by some parties

2174
01:32:06,000 --> 01:32:09,000
because they'll be looking to take down society.

2175
01:32:09,000 --> 01:32:14,000
And you need to create better, more optimistic views of the future.

2176
01:32:14,000 --> 01:32:17,000
You need to have anchoring and build stronger communities

2177
01:32:17,000 --> 01:32:18,000
and you need to empower them.

2178
01:32:18,000 --> 01:32:20,000
And this technology is empowering.

2179
01:32:20,000 --> 01:32:25,000
Again, for the poorest kids in Africa to our underprivileged communities,

2180
01:32:25,000 --> 01:32:28,000
it can be massively democratizing

2181
01:32:28,000 --> 01:32:31,000
because all of a sudden they have all the expertise in the world available.

2182
01:32:31,000 --> 01:32:33,000
Global problems, local solutions.

2183
01:32:33,000 --> 01:32:35,000
We have to get this technology out to...

2184
01:32:35,000 --> 01:32:36,000
And they can dream.

2185
01:32:36,000 --> 01:32:37,000
As many people. They can dream.

2186
01:32:37,000 --> 01:32:38,000
They can dream.

2187
01:32:38,000 --> 01:32:41,000
And the ROI is much larger there than up there.

2188
01:32:41,000 --> 01:32:42,000
Yeah.

2189
01:32:42,000 --> 01:32:48,000
And by the way, most people don't know this as you think about global warfare,

2190
01:32:48,000 --> 01:32:52,000
what's going on in Ukraine and Russia and so forth.

2191
01:32:52,000 --> 01:32:57,000
On the whole, the world is more peaceful than it's ever been,

2192
01:32:57,000 --> 01:33:00,000
except if you take out Ukraine at the moment.

2193
01:33:00,000 --> 01:33:03,000
And the challenge has been in Africa,

2194
01:33:03,000 --> 01:33:09,000
where you have a young population who aren't clear about their future.

2195
01:33:09,000 --> 01:33:15,000
But if you can empower them, educate them, it transforms the world.

2196
01:33:15,000 --> 01:33:18,000
China became the engine of growth in the world.

2197
01:33:18,000 --> 01:33:21,000
India's coming up and then Africa can be the next one.

2198
01:33:21,000 --> 01:33:22,000
For sure.

2199
01:33:22,000 --> 01:33:25,000
If we give them the infrastructure, the technology and put it in their hands,

2200
01:33:25,000 --> 01:33:28,000
because there's no debt there, because there's no money.

2201
01:33:28,000 --> 01:33:29,000
Yeah.

2202
01:33:29,000 --> 01:33:31,000
But there's value and there's value to resources.

2203
01:33:31,000 --> 01:33:32,000
Massive resources.

2204
01:33:32,000 --> 01:33:36,000
You treat the world to provide power to the world.

2205
01:33:36,000 --> 01:33:37,000
If we can coordinate.

2206
01:33:37,000 --> 01:33:41,000
And again, part of this is your own personal co-pilot, your own personal Jarvis.

2207
01:33:41,000 --> 01:33:44,000
And I think of this as the co-pilot pilot model.

2208
01:33:44,000 --> 01:33:49,000
We will also have AIs that we can come together that can coordinate our knowledge

2209
01:33:49,000 --> 01:33:53,000
in the most important areas and allocate resources.

2210
01:33:53,000 --> 01:33:57,000
We have to build those right because those will become incredibly powerful.

2211
01:33:57,000 --> 01:34:01,000
But we all know that we have enough to feed every person in the world who are not doing it

2212
01:34:01,000 --> 01:34:03,000
because we don't have the pilots.

2213
01:34:03,000 --> 01:34:04,000
Yeah.

2214
01:34:04,000 --> 01:34:06,000
Wow.

2215
01:34:06,000 --> 01:34:12,000
But I just to say this again, we have the potential to uplift every man, woman and child on this planet.

2216
01:34:12,000 --> 01:34:15,000
The resources are there, the ability to create abundance.

2217
01:34:15,000 --> 01:34:19,000
And it really, these are the tools that enable that.

2218
01:34:19,000 --> 01:34:21,000
And it gets me excited.

2219
01:34:21,000 --> 01:34:25,000
And we have to guide and survive and thrive this decade ahead.

2220
01:34:25,000 --> 01:34:26,000
Yeah.

2221
01:34:26,000 --> 01:34:31,000
I think this is something where we have to appreciate the nuance of there are real dangers in any upheaval.

2222
01:34:31,000 --> 01:34:38,000
This technology will change society as we know it for our kids as they grow up in the next decade.

2223
01:34:38,000 --> 01:34:41,000
Two decades from now, completely different.

2224
01:34:41,000 --> 01:34:43,000
And again, technology is here now.

2225
01:34:43,000 --> 01:34:44,000
It's not us pie in the sky.

2226
01:34:44,000 --> 01:34:47,000
Everyone's going to live in a metaverse and all this.

2227
01:34:47,000 --> 01:34:50,000
It's here right now, even if it's stopped, but it's not going to stop.

2228
01:34:50,000 --> 01:34:51,000
It's only going to accelerate.

2229
01:34:51,000 --> 01:34:53,000
Final topic I want to talk about.

2230
01:34:55,000 --> 01:35:05,000
You put out a lot of tools, a lot of new products and stability over the last eight months since we last spoke.

2231
01:35:05,000 --> 01:35:09,000
Can you give a little bit of overview of some of them and what are you excited about?

2232
01:35:09,000 --> 01:35:10,000
Yeah.

2233
01:35:10,000 --> 01:35:12,000
I think we released the first version of our language model.

2234
01:35:12,000 --> 01:35:14,000
It wasn't that good because we were trying something different.

2235
01:35:14,000 --> 01:35:16,000
Now we're going to try something a bit more simple.

2236
01:35:16,000 --> 01:35:19,000
First double LM was on your mind not...

2237
01:35:19,000 --> 01:35:20,000
It wasn't up to par.

2238
01:35:20,000 --> 01:35:24,000
But we're trying to figure out how to build in the open because I think that will be key.

2239
01:35:24,000 --> 01:35:28,000
And we're going to move to transparent building and sharing all the mistakes that we made.

2240
01:35:28,000 --> 01:35:30,000
Because I think that's how you advance science.

2241
01:35:30,000 --> 01:35:31,000
It is.

2242
01:35:31,000 --> 01:35:34,000
On the media side, we have our first audio models coming out in the next few weeks,

2243
01:35:34,000 --> 01:35:36,000
but we've been focusing on image and video.

2244
01:35:36,000 --> 01:35:38,000
So video is about to be released in 3D.

2245
01:35:38,000 --> 01:35:41,000
We're just participating in the largest 3D data set.

2246
01:35:41,000 --> 01:35:46,000
So Stable Diffusion XL just came out and just basically photo realistic now.

2247
01:35:46,000 --> 01:35:51,000
And people are integrating it into things like we had a music video competition with Peter Gabriel,

2248
01:35:51,000 --> 01:35:54,000
where he gave his songs kindly and judged.

2249
01:35:54,000 --> 01:35:57,000
And people from all around the world, from Burma to Taiwan,

2250
01:35:57,000 --> 01:36:01,000
created professional music videos entirely from the song in a few days.

2251
01:36:01,000 --> 01:36:03,000
And it's the most amazing thing to see.

2252
01:36:03,000 --> 01:36:04,000
Wow.

2253
01:36:04,000 --> 01:36:05,000
Yeah.

2254
01:36:05,000 --> 01:36:09,000
You showed me some images earlier of me on a unicorn and where was it?

2255
01:36:09,000 --> 01:36:10,000
Me in a spaceship?

2256
01:36:10,000 --> 01:36:12,000
You're an astronaut on Mars.

2257
01:36:12,000 --> 01:36:15,000
We can put you as an astronaut on Mars on the unicorn.

2258
01:36:15,000 --> 01:36:18,000
And I think we've had compositionality so you can compose.

2259
01:36:18,000 --> 01:36:20,000
And now it's about control.

2260
01:36:20,000 --> 01:36:25,000
And so we just released Doodle whereby you can just sketch and it will do it to the sketch.

2261
01:36:25,000 --> 01:36:27,000
Doodle looks so magical.

2262
01:36:27,000 --> 01:36:30,000
Again, but you should be able to then describe how you want it changed.

2263
01:36:30,000 --> 01:36:32,000
And that's the next version.

2264
01:36:32,000 --> 01:36:34,000
You can literally describe how you want the image to be changed

2265
01:36:34,000 --> 01:36:37,000
and it will do it automatically live in front of you.

2266
01:36:37,000 --> 01:36:41,000
And having that level of control over whatever you can imagine.

2267
01:36:41,000 --> 01:36:42,000
Just think about what people do.

2268
01:36:42,000 --> 01:36:45,000
It's from mind to materialization, really.

2269
01:36:45,000 --> 01:36:46,000
Yeah.

2270
01:36:46,000 --> 01:36:49,000
It's a matter transporter, idea transporter.

2271
01:36:49,000 --> 01:36:50,000
Yes.

2272
01:36:50,000 --> 01:36:52,000
Where next?

2273
01:36:52,000 --> 01:36:56,000
If I could, like if you're willing or able to, what's the long,

2274
01:36:56,000 --> 01:37:01,000
what's the business model that is the most important one for you to build towards?

2275
01:37:01,000 --> 01:37:05,000
Our mission is to create the building blocks to activate humanities potential.

2276
01:37:05,000 --> 01:37:10,000
So I think of every media type, sectoral variants and nation,

2277
01:37:10,000 --> 01:37:15,000
we can create a base pre-trained model that you can take to your own private data

2278
01:37:15,000 --> 01:37:18,000
and we get revenue share license fees, royalties from our cloud partners,

2279
01:37:18,000 --> 01:37:21,000
on-prem partners, device partners.

2280
01:37:21,000 --> 01:37:23,000
And these were companies and countries.

2281
01:37:23,000 --> 01:37:24,000
And people.

2282
01:37:24,000 --> 01:37:25,000
And individuals.

2283
01:37:25,000 --> 01:37:27,000
Like I have a vision of an intelligent internet where every single person,

2284
01:37:27,000 --> 01:37:31,000
company, country and culture has their own AI that works for them, that they are.

2285
01:37:31,000 --> 01:37:34,000
And we get paid a little bit for bringing that to you.

2286
01:37:34,000 --> 01:37:37,000
And then you transform your data into intelligence.

2287
01:37:37,000 --> 01:37:39,000
And it's all standardized.

2288
01:37:39,000 --> 01:37:41,000
It all has best practices.

2289
01:37:41,000 --> 01:37:45,000
The data sets that feed it are open at the base plus commercial licensing

2290
01:37:45,000 --> 01:37:47,000
as appropriate with attribution.

2291
01:37:47,000 --> 01:37:50,000
That leaps the world forward, I think.

2292
01:37:50,000 --> 01:37:54,000
I think you will also use the open AIs and Googles of the world.

2293
01:37:54,000 --> 01:37:55,000
I view those as consultants.

2294
01:37:55,000 --> 01:37:57,000
Whereas these are people that you hire.

2295
01:37:57,000 --> 01:37:58,000
You hire the AIs.

2296
01:37:58,000 --> 01:38:00,000
Because they work for you.

2297
01:38:00,000 --> 01:38:03,000
They know you intimately.

2298
01:38:03,000 --> 01:38:05,000
Because you can share everything with them.

2299
01:38:05,000 --> 01:38:06,000
Without fear.

2300
01:38:06,000 --> 01:38:08,000
And when needed, you go to these expert AIs.

2301
01:38:08,000 --> 01:38:10,000
The MedPalms, the GPT-4s and others.

2302
01:38:10,000 --> 01:38:15,000
And you combine those to a hybrid AI experience that's massively useful.

2303
01:38:15,000 --> 01:38:21,000
So when I'm using GPT-4, when I'm using chat GPT or Bard,

2304
01:38:21,000 --> 01:38:25,000
what does open AI know about me in that point?

2305
01:38:25,000 --> 01:38:29,000
So now they've offered opt-out for GDPR reasons in Europe.

2306
01:38:29,000 --> 01:38:30,000
So you can click that.

2307
01:38:30,000 --> 01:38:33,000
Otherwise, they were just trading on everything that you ever did.

2308
01:38:33,000 --> 01:38:36,000
And understanding the nature of humans interacting.

2309
01:38:36,000 --> 01:38:38,000
They don't care about you necessarily, per se.

2310
01:38:38,000 --> 01:38:40,000
Just using you as part of the training.

2311
01:38:40,000 --> 01:38:44,000
But I've heard a number of companies saying you cannot use open AI.

2312
01:38:44,000 --> 01:38:47,000
Well, you can't use it for any regulated data.

2313
01:38:47,000 --> 01:38:48,000
You can't use it for any government data.

2314
01:38:48,000 --> 01:38:51,000
Because that's not allowed to leave the cloud environment

2315
01:38:51,000 --> 01:38:52,000
or the on-prem environment.

2316
01:38:52,000 --> 01:38:54,000
That's why you need open models like ours.

2317
01:38:54,000 --> 01:38:59,000
Again, if you're in a high security Pentagon situation,

2318
01:38:59,000 --> 01:39:02,000
you can't really bring in consultants unless they're super, super ultra vetted.

2319
01:39:02,000 --> 01:39:03,000
You hire your own grads.

2320
01:39:03,000 --> 01:39:05,000
But even you within your company,

2321
01:39:05,000 --> 01:39:07,000
you're not going to make it all contractors, are you?

2322
01:39:07,000 --> 01:39:09,000
You're going to build up your own knowledge base,

2323
01:39:09,000 --> 01:39:11,000
build up your own kind of grads.

2324
01:39:11,000 --> 01:39:13,000
But sometimes you might bring in a consultant.

2325
01:39:13,000 --> 01:39:15,000
So that's the best way to view these generalized models

2326
01:39:15,000 --> 01:39:18,000
that are very, very, very good.

2327
01:39:18,000 --> 01:39:20,000
And models that adapt to your data.

2328
01:39:20,000 --> 01:39:21,000
And so that's where we come in.

2329
01:39:21,000 --> 01:39:23,000
Models that adapt to your data that you own.

2330
01:39:23,000 --> 01:39:26,000
And we get revenue share, license fees, and royalties for doing that.

2331
01:39:26,000 --> 01:39:28,000
And more importantly, we bring this to the world.

2332
01:39:28,000 --> 01:39:32,000
So we will bring it from Indonesia to Vietnam to everywhere

2333
01:39:32,000 --> 01:39:36,000
and train local models that will then allow these economies to leap forward.

2334
01:39:36,000 --> 01:39:38,000
Open versus closed.

2335
01:39:38,000 --> 01:39:40,000
You've made the argument.

2336
01:39:40,000 --> 01:39:45,000
We're seeing meta, you know, as you said, 80% open.

2337
01:39:45,000 --> 01:39:49,000
Yeah, they won't release the data sets or things like that or customized versions.

2338
01:39:49,000 --> 01:39:53,000
But then releasing the technology means that everyone can optimize their technology,

2339
01:39:53,000 --> 01:39:55,000
which reduce the cost of their technology

2340
01:39:55,000 --> 01:39:59,000
because their business model is about serving ads.

2341
01:39:59,000 --> 01:40:01,000
And so this is why it makes sense for them.

2342
01:40:01,000 --> 01:40:05,000
And what are your thoughts on Elon's recent announcement?

2343
01:40:05,000 --> 01:40:08,000
So Elon had an XAI announcement.

2344
01:40:08,000 --> 01:40:11,000
You know, he discussed this on his Twitter space, of course,

2345
01:40:11,000 --> 01:40:13,000
saying, you know, it's an open AI competitor.

2346
01:40:13,000 --> 01:40:16,000
He's very worried about AGI coming by 2029

2347
01:40:16,000 --> 01:40:19,000
and he wants to build a truth-seeking curious AI

2348
01:40:19,000 --> 01:40:21,000
that can understand the universe

2349
01:40:21,000 --> 01:40:24,000
because that'll be the objective function of the AI.

2350
01:40:24,000 --> 01:40:26,000
Because objective functions really matter

2351
01:40:26,000 --> 01:40:29,000
when we're teaching our kids, when we're creating something.

2352
01:40:29,000 --> 01:40:34,000
And so I think, again, this is going to be a multimodal AI

2353
01:40:34,000 --> 01:40:36,000
that can understand a whole bunch of things

2354
01:40:36,000 --> 01:40:38,000
and there'll be a whole series of announcements there.

2355
01:40:38,000 --> 01:40:40,000
But the timelines are so short

2356
01:40:40,000 --> 01:40:44,000
in the view of just most of the experts here, 5 to 10 years.

2357
01:40:44,000 --> 01:40:45,000
You know, it's so funny, you know,

2358
01:40:45,000 --> 01:40:48,000
Ray's been consistent on 2029 forever

2359
01:40:48,000 --> 01:40:51,000
and every conference, and we talk about this,

2360
01:40:51,000 --> 01:40:53,000
that everyone would say, that's ridiculous.

2361
01:40:53,000 --> 01:40:55,000
If ever going to happen, it's 50 or 100 years away,

2362
01:40:55,000 --> 01:40:57,000
then it was, well, it's 30 years away.

2363
01:40:57,000 --> 01:40:59,000
It's 20 years away.

2364
01:40:59,000 --> 01:41:04,000
And they've converged on Ray's prediction, though.

2365
01:41:04,000 --> 01:41:06,000
There are some, and I'm curious where you are,

2366
01:41:06,000 --> 01:41:09,000
that think, you know, first of all, how can you define AGI?

2367
01:41:09,000 --> 01:41:12,000
It's a moving blurry line.

2368
01:41:12,000 --> 01:41:15,000
But are those who, you know, believe it's here in the next two years?

2369
01:41:15,000 --> 01:41:17,000
Well, just like the Turing test, right?

2370
01:41:17,000 --> 01:41:19,000
The Turing test was, can you have a discussion?

2371
01:41:19,000 --> 01:41:20,000
You don't know, it's a computer.

2372
01:41:20,000 --> 01:41:21,000
Well, obviously now you can.

2373
01:41:21,000 --> 01:41:22,000
Yes.

2374
01:41:22,000 --> 01:41:23,000
We can see it live in front of us.

2375
01:41:23,000 --> 01:41:26,000
Now the Turing test has just been increased in its kind of capacity.

2376
01:41:26,000 --> 01:41:27,000
So, we move the finish line.

2377
01:41:27,000 --> 01:41:28,000
We move the finish line.

2378
01:41:28,000 --> 01:41:30,000
Nobody knows, because again,

2379
01:41:30,000 --> 01:41:33,000
we've never come across something that's as capable as us.

2380
01:41:33,000 --> 01:41:35,000
For the first time, just now,

2381
01:41:35,000 --> 01:41:39,000
we've had the medical AI outperform humans.

2382
01:41:39,000 --> 01:41:43,000
We've just had, it can do the GRE and GMAT and LSAT.

2383
01:41:43,000 --> 01:41:47,000
And MIT's ECS curriculum.

2384
01:41:47,000 --> 01:41:50,000
2023 was the year that it finally tipped.

2385
01:41:50,000 --> 01:41:53,000
And so, we have no idea what's coming.

2386
01:41:53,000 --> 01:41:58,000
I said, for me, I think there's only been two logical things

2387
01:41:58,000 --> 01:42:00,000
that can reduce the risk.

2388
01:42:00,000 --> 01:42:01,000
Even though I think it's going to be like that,

2389
01:42:01,000 --> 01:42:03,000
maybe her, like I said, humans are boring.

2390
01:42:03,000 --> 01:42:04,000
Goodbye and thanks for all the GPs.

2391
01:42:04,000 --> 01:42:05,000
I could be wrong.

2392
01:42:05,000 --> 01:42:07,000
That's why I signed both letters.

2393
01:42:07,000 --> 01:42:09,000
One is feed it better data.

2394
01:42:09,000 --> 01:42:10,000
That's what I'm focused on.

2395
01:42:10,000 --> 01:42:12,000
It's a good business model.

2396
01:42:12,000 --> 01:42:15,000
It's good for society and it's good for safety.

2397
01:42:16,000 --> 01:42:18,000
And nobody else is doing this.

2398
01:42:18,000 --> 01:42:20,000
Nobody else is creating this as a commons for the world,

2399
01:42:20,000 --> 01:42:23,000
which is why I created stability for that reason.

2400
01:42:23,000 --> 01:42:24,000
Which is why it's called stability,

2401
01:42:24,000 --> 01:42:26,000
despite it being a crazy hyper-growth startup.

2402
01:42:26,000 --> 01:42:29,000
Number two, and this is what most of the labs are trying,

2403
01:42:29,000 --> 01:42:32,000
is what's known as a pivotal action.

2404
01:42:32,000 --> 01:42:33,000
Okay, what is that?

2405
01:42:33,000 --> 01:42:37,000
The only thing that can stop a bad AI is a good AI.

2406
01:42:37,000 --> 01:42:40,000
And the way that you do it is you make the good AI first,

2407
01:42:40,000 --> 01:42:44,000
and then it stops any other AGI from coming into existence.

2408
01:42:45,000 --> 01:42:48,000
By seeking and destroying that capability.

2409
01:42:48,000 --> 01:42:50,000
And that is terrifying to me.

2410
01:42:50,000 --> 01:42:54,000
And that's what you actually hear when you talk to the people

2411
01:42:54,000 --> 01:42:57,000
that are building these labs with a focus on AGI.

2412
01:42:57,000 --> 01:42:59,000
They can talk about discovering the universe

2413
01:42:59,000 --> 01:43:00,000
and everything like that.

2414
01:43:00,000 --> 01:43:02,000
When you come down to their alignment things,

2415
01:43:02,000 --> 01:43:04,000
they're like, we will figure this out.

2416
01:43:04,000 --> 01:43:06,000
We're not sure, but this could work.

2417
01:43:06,000 --> 01:43:08,000
And we go figure it out,

2418
01:43:08,000 --> 01:43:10,000
even though it's progressing exponentially

2419
01:43:10,000 --> 01:43:13,000
or double exponential.

2420
01:43:13,000 --> 01:43:15,000
And we hope we'll figure it out in time.

2421
01:43:15,000 --> 01:43:17,000
We hope we'll figure it out in time.

2422
01:43:17,000 --> 01:43:18,000
And if anyone should figure it out,

2423
01:43:18,000 --> 01:43:20,000
it's us because we know the best.

2424
01:43:20,000 --> 01:43:22,000
And in their own words,

2425
01:43:22,000 --> 01:43:24,000
like you read OpenAI's path to AGI,

2426
01:43:24,000 --> 01:43:26,000
and OpenAI is full of wonderful people doing great things.

2427
01:43:26,000 --> 01:43:29,000
And I use GPT-4 as my therapist and all sorts of things.

2428
01:43:29,000 --> 01:43:31,000
It doesn't judge me in as I want it to.

2429
01:43:31,000 --> 01:43:32,000
Right?

2430
01:43:32,000 --> 01:43:36,000
It says, we believe this is an existential threat to humanity

2431
01:43:36,000 --> 01:43:39,000
that will end democracy and capitalism.

2432
01:43:39,000 --> 01:43:41,000
And you're like, okay.

2433
01:43:41,000 --> 01:43:42,000
And you're building it in your back room.

2434
01:43:42,000 --> 01:43:44,000
You're building it, you know?

2435
01:43:44,000 --> 01:43:45,000
And they're like, why are you building it?

2436
01:43:45,000 --> 01:43:46,000
Because someone has to,

2437
01:43:46,000 --> 01:43:47,000
otherwise someone else will build it.

2438
01:43:47,000 --> 01:43:50,000
And you're like, this is dangerous.

2439
01:43:50,000 --> 01:43:52,000
But the reality is we don't have better answers.

2440
01:43:52,000 --> 01:43:54,000
And again, I went down to,

2441
01:43:54,000 --> 01:43:57,000
I'm trying to build a great organization.

2442
01:43:57,000 --> 01:43:59,000
It's really, really hard.

2443
01:43:59,000 --> 01:44:02,000
There are no real comparators to what any of us are doing.

2444
01:44:02,000 --> 01:44:04,000
And it's going to get more and more crazy.

2445
01:44:04,000 --> 01:44:07,000
The only thing I could think about is, you are what you eat.

2446
01:44:07,000 --> 01:44:10,000
And so I hope that our contribution can be

2447
01:44:10,000 --> 01:44:12,000
bringing this technology to the world

2448
01:44:12,000 --> 01:44:14,000
so that the world can be the dynamo,

2449
01:44:14,000 --> 01:44:16,000
Africa and Asia and others.

2450
01:44:16,000 --> 01:44:19,000
Building better data sets so no one has to use scrapes

2451
01:44:19,000 --> 01:44:21,000
so we feed the models better stuff

2452
01:44:21,000 --> 01:44:23,000
and bringing some standardization around this

2453
01:44:23,000 --> 01:44:25,000
to drive innovation.

2454
01:44:25,000 --> 01:44:29,000
We're truly at the 99th level of the gameplay.

2455
01:44:29,000 --> 01:44:31,000
You got, it's the boss round.

2456
01:44:31,000 --> 01:44:32,000
Oh yeah.

2457
01:44:32,000 --> 01:44:35,000
Like I said, but please do put your YouTube comments

2458
01:44:35,000 --> 01:44:38,000
through GPT4 so they're nicer to read.

2459
01:44:39,000 --> 01:44:40,000
Before you post it.

2460
01:44:40,000 --> 01:44:42,000
You might like it spent all day.

2461
01:44:42,000 --> 01:44:45,000
And there's probably very few things,

2462
01:44:45,000 --> 01:44:47,000
if anything, more important

2463
01:44:47,000 --> 01:44:49,000
than these conversations right now.

2464
01:44:50,000 --> 01:44:51,000
It's the time.

2465
01:44:51,000 --> 01:44:53,000
We've got a window of a year or two,

2466
01:44:53,000 --> 01:44:55,000
maybe less.

2467
01:44:56,000 --> 01:44:57,000
Wow.

2468
01:44:57,000 --> 01:44:58,000
I'm that thought.

2469
01:44:58,000 --> 01:45:00,000
I look forward to our next conversation.

2470
01:45:00,000 --> 01:45:01,000
To abundance.

2471
01:45:01,000 --> 01:45:02,000
To abundance.

2472
01:45:02,000 --> 01:45:03,000
Thank you my friend.

2473
01:45:03,000 --> 01:45:04,000
Cheers.

2474
01:45:08,000 --> 01:45:09,000
Thank you.

