WEBVTT

00:00.000 --> 00:04.600
It seemed to me that a huge revolution was going on.

00:04.600 --> 00:05.760
Now it's changed.

00:05.760 --> 00:08.240
I am optimistic, but I'm also worried about it.

00:11.240 --> 00:15.760
I've been in the field of AI for 60 years.

00:15.760 --> 00:17.240
I was 14.

00:17.240 --> 00:20.400
I met Marvin Minsky who was in his 30s.

00:20.400 --> 00:22.400
Frank Rosenblatt created the Perceptron,

00:22.400 --> 00:24.040
the first popular neural net.

00:24.040 --> 00:26.160
But in the early years, it was really not clear

00:26.160 --> 00:28.280
that neural nets could do anything successful.

00:28.320 --> 00:30.840
And they're showing now that this is really the path

00:30.840 --> 00:32.800
to artificial general intelligence.

00:32.800 --> 00:34.960
It's not just us versus AI.

00:34.960 --> 00:37.120
The intelligence that we're creating

00:37.120 --> 00:39.440
is adding AI to our own brains.

00:39.440 --> 00:43.480
2045 is when I said we will actually multiply

00:43.480 --> 00:46.400
our intelligence millions fold.

00:46.400 --> 00:48.160
And that's going to be true of everybody.

00:48.160 --> 00:51.640
And we'll be able to get rid of terrible lives that we see

00:51.640 --> 00:54.680
through poverty and lack of access to information.

00:55.680 --> 00:56.680
Great.

00:56.680 --> 00:57.680
Good morning.

00:57.680 --> 00:58.680
Good morning to you.

00:58.680 --> 01:00.680
It's great to be with you, Peter.

01:00.680 --> 01:02.680
And also Salim.

01:02.680 --> 01:04.680
I've done lots of presentations with Peter.

01:04.680 --> 01:09.680
It's really remarkable what you've contributed.

01:09.680 --> 01:14.680
So I just want to share a few ideas.

01:14.680 --> 01:18.680
I've been following large language models

01:18.680 --> 01:20.680
for almost three years.

01:20.680 --> 01:24.680
There was Lambda, now barred from Google,

01:24.680 --> 01:27.680
different GPT versions from OpenAI.

01:27.680 --> 01:32.680
It seemed to me that a huge revolution was going on.

01:32.680 --> 01:33.680
Now it's changed.

01:33.680 --> 01:37.680
OpenAI changed GPT-3 to chat GPT.

01:37.680 --> 01:40.680
It was the fastest growing app, I believe, in history

01:40.680 --> 01:42.680
with over 100 million users within the first two months

01:42.680 --> 01:44.680
of its launch.

01:44.680 --> 01:47.680
And lots of other companies, particularly Google,

01:47.680 --> 01:50.680
are introducing Google Just Introduce Barred,

01:50.680 --> 01:52.680
I think, a few days ago.

01:52.680 --> 01:57.680
OpenAI has also introduced GPT-4.

01:57.680 --> 02:00.680
Without going into comparison with these LLMs,

02:00.680 --> 02:02.680
because it changes like every day,

02:02.680 --> 02:05.680
I can write things in one style and ask

02:05.680 --> 02:09.680
it to re-articulate it in the style of Shakespeare,

02:09.680 --> 02:13.680
E.E. Cummings, any other poet or writer.

02:14.680 --> 02:17.680
The results are amazingly impressive.

02:17.680 --> 02:21.680
In my opinion, this is not just another category of AI.

02:21.680 --> 02:26.680
To me, it's as significant as the advent of written language,

02:26.680 --> 02:29.680
which started with cuneiform 5,000 years ago.

02:29.680 --> 02:32.680
You remember using cuneiform 5,000 years ago.

02:32.680 --> 02:36.680
Homo sapiens evolved in Africa 300,000 years ago.

02:36.680 --> 02:39.680
So for most of that history, we had no ways

02:40.680 --> 02:43.680
of documenting our language.

02:43.680 --> 02:48.680
In the past century, we've added to written language.

02:48.680 --> 02:51.680
We've added word processes and other means to help us.

02:51.680 --> 02:54.680
But this latest breakthrough allows us to creatively

02:54.680 --> 02:57.680
create written language based on the LLMs'

02:57.680 --> 02:59.680
own understanding.

02:59.680 --> 03:03.680
It's going to go in all directions and at a very high speed.

03:03.680 --> 03:05.680
I mean, just look at it in the last two years.

03:05.680 --> 03:07.680
It's been unbelievable.

03:07.680 --> 03:09.680
It's going to change everything we do.

03:09.680 --> 03:11.680
You can write code perfectly.

03:11.680 --> 03:16.680
You can convert code into human terms, deal with all languages,

03:16.680 --> 03:20.680
different styles of communicating, and so on.

03:20.680 --> 03:23.680
It's been already very extensively used to create answers

03:23.680 --> 03:25.680
for subtle questions.

03:25.680 --> 03:31.680
So I actually took a couple of the top LLMs,

03:31.680 --> 03:33.680
and I asked the various questions like,

03:33.680 --> 03:36.680
how do my views of consciousness relate to those

03:36.680 --> 03:39.680
of Marvin Ninsky, and how do they compare?

03:39.680 --> 03:42.680
Now, that's kind of a subtle question.

03:42.680 --> 03:45.680
I'm not sure I've actually ever read anything

03:45.680 --> 03:48.680
that answered that question.

03:48.680 --> 03:52.680
I asked LLMs from Google and from OpenAI.

03:52.680 --> 03:55.680
The answers were really quite remarkably subtle,

03:55.680 --> 04:00.680
very well stated, and they were not copied from anywhere else.

04:00.680 --> 04:04.680
Now, many people are concerned that large language models

04:04.680 --> 04:08.680
may promote ideas that are not socially appropriate,

04:08.680 --> 04:12.680
that engender racism or sexism and so on.

04:12.680 --> 04:16.680
It's definitely very worthwhile for us to study this

04:16.680 --> 04:18.680
that may happen from time to time,

04:18.680 --> 04:22.680
but I've actually used LLMs probably close to a thousand times.

04:22.680 --> 04:25.680
I've actually not seen anything that could be categorized that way.

04:25.680 --> 04:27.680
Maybe it's the way I asked the question.

04:27.680 --> 04:29.680
It also seems pretty accurate.

04:29.680 --> 04:32.680
The only mistake it made is that I thought my son,

04:32.680 --> 04:34.680
Ethan, went to Harvard as an undergraduate.

04:34.680 --> 04:36.680
He actually went there for an MBA.

04:36.680 --> 04:40.680
I've written a new book, which I've talked about for years.

04:40.680 --> 04:43.680
The Singularity is Nearer.

04:43.680 --> 04:45.680
It should be out in about a year.

04:45.680 --> 04:48.680
I keep writing because literally every week that we can't come

04:48.680 --> 04:51.680
out with this without covering this,

04:51.680 --> 04:53.680
but that's been happening now every few days.

04:53.680 --> 04:55.680
So I finally had to give up on that.

04:55.680 --> 04:58.680
By the time it comes out, it'll be out of date,

04:58.680 --> 05:00.680
but it's not just covering today.

05:00.680 --> 05:02.680
It's covering what's how we got here

05:02.680 --> 05:05.680
and what will happen in the near future.

05:05.680 --> 05:11.680
Critics of AI very often show how large language models

05:11.680 --> 05:13.680
may not be perfect.

05:13.680 --> 05:15.680
There was one recently said,

05:15.680 --> 05:20.680
well, it can't, if you put mathematics inside language,

05:20.680 --> 05:21.680
it doesn't do that correctly.

05:21.680 --> 05:23.680
But now within a year of saying that,

05:23.680 --> 05:25.680
that's no longer true.

05:25.680 --> 05:31.680
So one of my themes, and this is also true of Peter and Salim,

05:31.680 --> 05:33.680
has been the acceleration of progress

05:33.680 --> 05:35.680
in information technology,

05:35.680 --> 05:38.680
but also everything that we work on.

05:38.680 --> 05:40.680
So here's a chart.

05:40.680 --> 05:42.680
I actually came out with this.

05:42.680 --> 05:48.680
For each year, the best computer that provided

05:48.680 --> 05:51.680
the amount of computations per second.

05:51.680 --> 05:54.680
And it's pretty much a very straight line

05:54.680 --> 05:57.680
on an exponential growth.

05:57.680 --> 06:00.680
And people were not even aware of this.

06:00.680 --> 06:03.680
I mean, I came out with this graph 40 years ago.

06:03.680 --> 06:07.680
It's 40 years after the progress of the computer.

06:07.680 --> 06:10.680
I mean, I came out with this graph 40 years ago.

06:10.680 --> 06:13.680
It's 40 years after the progression started.

06:13.680 --> 06:16.680
And I've been updating it ever since.

06:16.680 --> 06:19.680
People very often call this Moore's Law.

06:19.680 --> 06:22.680
I really believe we shouldn't do that anymore

06:22.680 --> 06:24.680
because there's nothing to do with Moore's.

06:24.680 --> 06:27.680
I mean, this started decades before Intel was even created.

06:27.680 --> 06:30.680
It's been going on for 40 years before anyone even

06:30.680 --> 06:32.680
knew it was happening.

06:32.680 --> 06:34.680
If you go to the bottom left,

06:34.680 --> 06:40.680
the first programmable computer was the ZUSA 1, 1941.

06:40.680 --> 06:49.680
It performed 0.0007 calculations per second per dollar.

06:49.680 --> 06:51.680
ZUSA was a German.

06:51.680 --> 06:55.680
It was not a fan of Hitler, but it was shown to Hitler.

06:55.680 --> 06:58.680
And some people were excited about getting behind this,

06:58.680 --> 07:00.680
but they didn't get behind it.

07:00.680 --> 07:03.680
They saw no military value to computation.

07:03.680 --> 07:07.680
A big mistake for them, among a lot of other mistakes.

07:07.680 --> 07:10.680
The third computer on here is the Colossus,

07:10.680 --> 07:13.680
created by Alan Turing and his colleagues.

07:13.680 --> 07:17.680
Now, Winston Churchill felt that this computer would be the key

07:17.680 --> 07:20.680
to winning World War II, and that was true.

07:20.680 --> 07:23.680
They got totally behind the Colossus computer,

07:23.680 --> 07:27.680
and they used it to completely decode Nazi messages.

07:27.680 --> 07:31.680
So everything that Hitler knew, Churchill also knew.

07:31.680 --> 07:35.680
And so even though the Nazi air power was actually several times

07:35.680 --> 07:39.680
that of the British, they used the Colossus to win the battle

07:39.680 --> 07:43.680
of Britain anyway with this computer and provide the allies

07:43.680 --> 07:46.680
with a launching pad for its D&A invasion.

07:46.680 --> 07:49.680
So if you go along this chart, there are many stories

07:49.680 --> 07:52.680
behind all the computers on this chart.

07:52.680 --> 07:55.680
It almost looks like someone was behind this exponential trend,

07:55.680 --> 07:57.680
like someone's following it.

07:57.680 --> 07:59.680
Okay, we're at this point now.

07:59.680 --> 08:01.680
We're here for the next year.

08:01.680 --> 08:05.680
But for the first 40 years, no one even knew this was happening.

08:05.680 --> 08:07.680
It just happened.

08:07.680 --> 08:10.680
That's the nature of exponential growth.

08:10.680 --> 08:12.680
And this is just one example of exponential growth.

08:12.680 --> 08:15.680
It's not that everything comes from this graph.

08:15.680 --> 08:17.680
This graph just shows you one example

08:17.680 --> 08:21.680
of how technology expands exponentially.

08:21.680 --> 08:26.680
And whether we're aware of it or not.

08:26.680 --> 08:29.680
So exponential growth impacts everything around us,

08:29.680 --> 08:32.680
including everything that we create.

08:32.680 --> 08:40.680
And I projected that this would continue in the same direction

08:40.680 --> 08:42.680
that I noticed 40 years ago.

08:42.680 --> 08:45.680
And as you can see, it's done that.

08:45.680 --> 08:51.680
It's gone from telephone relays to vacuum tubes to transistors

08:51.680 --> 08:53.680
to integrated circuits.

08:53.680 --> 08:56.680
As I mentioned, people have called this Moore's Law,

08:56.680 --> 08:58.680
but as I say, that's not correct.

08:58.680 --> 09:01.680
It started decades before Intel was even formed.

09:01.680 --> 09:06.680
Of the 80 best computers in terms of computations per second per dollar,

09:06.680 --> 09:11.680
only 10 of these out of 80 have anything to do with Intel.

09:11.680 --> 09:16.680
Now, every five years, people were going around saying Moore's Law is over.

09:16.680 --> 09:20.680
You might remember that this started when the COVID pandemic started

09:20.680 --> 09:23.680
just a few years ago, people were saying Moore's Law is over.

09:23.680 --> 09:26.680
And of course, I went around saying that it should not be called Moore's Law,

09:26.680 --> 09:31.680
but regardless of that, whether Intel chips were the best value or not,

09:31.680 --> 09:36.680
this exponential progression has never stopped.

09:36.680 --> 09:40.680
Not for World War II, not for recessions, not for depressions,

09:40.680 --> 09:42.680
or for any other reason.

09:42.680 --> 09:48.680
It's gone for 80 years from .0007 calculations per second per dollar

09:48.680 --> 09:52.680
to now .50 billion calculations per second per dollar.

09:52.680 --> 09:56.680
So you're getting a lot more for the same amount of money.

09:56.680 --> 10:01.680
And it's only in the last three years that large language models have been feasible.

10:01.680 --> 10:04.680
So people believe that neural nets were effective decades ago,

10:04.680 --> 10:09.680
did so really based on their inclination, not any evidence.

10:09.680 --> 10:14.680
I've been in the field of AI for 60 years.

10:14.680 --> 10:18.680
It's quite amazing, like where does the time go?

10:18.680 --> 10:20.680
I was 14.

10:20.680 --> 10:23.680
I met Marvin Minsky who was in his 30s.

10:23.680 --> 10:28.680
Frank Rosenblatt created the Perceptron, the first popular neural net.

10:28.680 --> 10:37.680
As far as I'm aware, I don't think anyone else has 60 years experience or more in AI as I've had.

10:37.680 --> 10:40.680
But if you've been there for more than that, let me know.

10:40.680 --> 10:43.680
I have a lot of stories about that.

10:43.680 --> 10:48.680
But in the early years, it was really not clear that neural nets could do anything successful.

10:48.680 --> 10:53.680
And they're showing now that this is really the path to artificial general intelligence.

10:53.680 --> 10:59.680
We will have large language models that can understand lots of different types of written language,

10:59.680 --> 11:03.680
from formal research articles to jokes and so on.

11:03.680 --> 11:07.680
They're now mastering mathematics within the language.

11:07.680 --> 11:12.680
They can code and do so perfectly and at very high speed.

11:12.680 --> 11:19.680
Now, this obviously brings up, not just that, but all the things it can do brings up concerns about its effect on human employment,

11:19.680 --> 11:22.680
which we were just talking about.

11:22.680 --> 11:27.680
But employment is really not necessarily the best way to bring resources to human.

11:27.680 --> 11:29.680
I mean, look at around the world.

11:29.680 --> 11:36.680
France is now dealing with protests because they're adding a couple of years before people can access their retirement.

11:36.680 --> 11:40.680
It tells me that people really don't like the jobs they do for employment.

11:40.680 --> 11:42.680
So that's, I think, a difference.

11:42.680 --> 11:46.680
We'll actually be able to do what we are really cut out to do.

11:46.680 --> 11:52.680
And in my opinion, it's not just us versus AI and people say, well, how are we going to compete with AI?

11:52.680 --> 12:00.680
The intelligence that we're creating is adding AI to our own brains, just the way our phones and computers do already.

12:00.680 --> 12:04.680
This is not an alien invasion of intelligent machines coming from Mars.

12:04.680 --> 12:10.680
I mean, how many people here have come to this meeting without your phone?

12:10.680 --> 12:12.680
It's already part of our intelligence.

12:12.680 --> 12:14.680
We can't leave home without it.

12:14.680 --> 12:20.680
It ultimately will be automatically added to our intelligence and it already is.

12:20.680 --> 12:26.680
I'll add one more AI topic and I'm sure we'll get into a lot more doing the questions and answers.

12:26.680 --> 12:31.680
But something else that's also extremely exciting, which is simulated biology.

12:31.680 --> 12:33.680
This has already started.

12:33.680 --> 12:44.680
The Moderna vaccine was created by feeding in every possible combination of mRNA sequences and simulating in the computer what would happen.

12:44.680 --> 12:51.680
They tried several billion of such sequences and they went through them all and seeing what the impact would be.

12:51.680 --> 12:55.680
It took two days to process all several billion of them.

12:55.680 --> 12:57.680
And then they had the vaccine.

12:57.680 --> 12:59.680
It actually took two days to create.

12:59.680 --> 13:03.680
It's been the most successful COVID vaccine.

13:03.680 --> 13:05.680
And we did test it with humans.

13:05.680 --> 13:07.680
We're going to get over that as well.

13:07.680 --> 13:12.680
We're ultimately going to be used biological simulation of humans to replace human testing.

13:12.680 --> 13:20.680
I mean, rather than spending a year or several years testing results on a few hundred subjects, none of which probably match you.

13:20.680 --> 13:27.680
We will test it on a million or more humans simulated humans in just a few days.

13:27.680 --> 13:38.680
So to cure cancer, for example, we'll simply feed in every possible method that can detect cancer cells from normal cells and destroy them or do anything that would help us.

13:38.680 --> 13:39.680
And we won't evaluate them.

13:39.680 --> 13:44.680
We'll just feed in all the ideas we have about each of these possibilities into the computer.

13:44.680 --> 13:55.680
The computer will evaluate all of the many billions of sequences and provide the results will then test the final product with simulated humans also very quickly.

13:55.680 --> 13:58.680
And we'll do this for every major health predicament.

13:58.680 --> 14:02.680
It will be done a thousand times faster than conventional methods.

14:02.680 --> 14:11.680
And based on our ability to do this, we should be able to overcome most significant health problems by 2029.

14:11.680 --> 14:15.680
That's by the way, my prediction for passing the Turing test.

14:15.680 --> 14:17.680
I came out with that in 1999.

14:17.680 --> 14:21.680
People thought that was crazy that Stanford had a conference.

14:21.680 --> 14:26.680
Actually, 80% of the people came didn't think we would do it, but they thought it would take 100 years.

14:26.680 --> 14:30.680
They keep polling people.

14:30.680 --> 14:38.680
And now everybody actually thinks that we will actually pass the Turing test by 2029.

14:38.680 --> 14:43.680
And actually to pass the Turing test, meaning it's equivalent to humans, we're actually going to have to dumb them down.

14:43.680 --> 14:49.680
Because if it does everything that a computer can do, we'll know it's not a human.

14:49.680 --> 15:01.680
But this will lead people who are diligent about their health to overcome many problems, reaching what I call longevity escape velocity by the end of this decade.

15:01.680 --> 15:03.680
Now, this doesn't guarantee living forever.

15:03.680 --> 15:10.680
I mean, you can have a 10 year old and you can compute their life expectancy, whatever, many, many decades, and they could die tomorrow.

15:10.680 --> 15:13.680
So it's not a guarantee for living forever.

15:13.680 --> 15:18.680
But the biggest problem we have is aging and people actually die from aging.

15:18.680 --> 15:22.680
I actually had an aunt who's 97. She was a psychologist.

15:22.680 --> 15:28.680
And she actually was still meeting with her patients at 97.

15:28.680 --> 15:31.680
And the last conversation I had with her, she's saying, well, what do you do?

15:31.680 --> 15:34.680
And I said, well, I give lots of speeches and what do you talk about?

15:34.680 --> 15:36.680
And I said, longevity escape velocity.

15:36.680 --> 15:37.680
Oh, what's that?

15:37.680 --> 15:38.680
And I described it.

15:38.680 --> 15:47.680
And the very last thing she said to me, this longevity escape velocity, could we do that a little faster than you're doing it now?

15:48.680 --> 15:53.680
So anyway, I look forward to your questions and comments.

15:53.680 --> 15:55.680
And it's really delightful to be here.

15:55.680 --> 15:56.680
Thank you, Ray.

15:56.680 --> 15:57.680
All right.

16:00.680 --> 16:04.680
I'm going to take privilege and ask the first question. Ray, we've seen LLMs.

16:04.680 --> 16:11.680
What's the next major breakthrough that you expect to see on the road of evolution of AI?

16:11.680 --> 16:16.680
Well, LLMs, I mean, they do remarkable things.

16:16.680 --> 16:18.680
But it's really just the beginning.

16:18.680 --> 16:23.680
I mean, the very first time I saw an LLM was three years ago, and it actually didn't work very well.

16:23.680 --> 16:26.680
Every six months, it's completely revolutionary.

16:26.680 --> 16:31.680
So it's going to give us new ways of communicating with each other.

16:31.680 --> 16:38.680
And as I said, I think it's the biggest advance since written language, which happened 5,000 years ago.

16:39.680 --> 16:46.680
I mentioned advancing longevity escape velocity, doing simulated biology.

16:46.680 --> 16:47.680
We've actually done that.

16:47.680 --> 16:51.680
People are taking this test, which was done with simulated biology.

16:51.680 --> 16:53.680
Lots of people are going into this.

16:53.680 --> 16:57.680
It's a way biology is going to be done.

16:57.680 --> 17:04.680
And we're going to see amazing progress starting, really, I'd say, in a few years.

17:05.680 --> 17:07.680
It's going to do everything that we do.

17:07.680 --> 17:10.680
But as I said, it's not competing with us.

17:10.680 --> 17:13.680
I mean, we're creating these tools to overcome ourselves.

17:13.680 --> 17:19.680
And I mean, how many people today have a job that was common 100 years ago?

17:19.680 --> 17:23.680
I mean, 200 years ago, 80% of the American public were working in farming.

17:23.680 --> 17:25.680
Today, that's 2%.

17:25.680 --> 17:30.680
So we're all doing things that didn't even exist even 10 years ago.

17:30.680 --> 17:34.680
So we're going to be doing amazing things, harnessing our computers.

17:34.680 --> 17:37.680
They're really part of ourselves.

17:37.680 --> 17:38.680
Great.

17:38.680 --> 17:39.680
Harry.

17:39.680 --> 17:40.680
Hey, Ray.

17:40.680 --> 17:41.680
Good to see you.

17:41.680 --> 17:48.680
So Ray and I have been collaborating for actually probably 20 years on something else,

17:48.680 --> 17:52.680
not natural language programming, but humanoid robots.

17:52.680 --> 17:55.680
Ray, I wanted to get your opinion.

17:55.680 --> 18:02.680
So at Beyond Managed Nation, we're creating AI powered robots called Beomni.

18:02.680 --> 18:09.680
And we have a lot of discussions about AI for natural language, for images.

18:09.680 --> 18:16.680
Where do you see AI and humanoid robots going in the future to impact physical work?

18:16.680 --> 18:17.680
Yes.

18:17.680 --> 18:19.680
That's a very good comment.

18:19.680 --> 18:22.680
I'd be very pleased to hear of your amazing progress.

18:22.680 --> 18:29.680
I mean, you have a robot that can actually take something and actually flip a cap off

18:29.680 --> 18:30.680
a jar.

18:30.680 --> 18:34.680
No one else can do that.

18:34.680 --> 18:38.680
We've not made as much progress in this area.

18:38.680 --> 18:43.680
We can do fantastic things with language, but if I give you a table that has where you

18:43.680 --> 18:49.680
need to put it in the dishwasher and now went to wash out dishes and so on, we have not

18:49.680 --> 18:51.680
been able to do that.

18:51.680 --> 18:53.680
You're actually working on that.

18:53.680 --> 18:57.680
And I think that's going to be amazing with these types of robots.

18:57.680 --> 19:00.680
You could send someone into a burning building and save people.

19:00.680 --> 19:07.680
You could have a surgeon in New York perform surgery on somebody in Africa.

19:07.680 --> 19:11.680
So we're going to actually master the human body and how we move.

19:11.680 --> 19:15.680
And we're going to be using neural nets to do that.

19:15.680 --> 19:20.680
And I think that's another thing we're going to see really starting now.

19:20.680 --> 19:23.680
And it will be quite prevalent within a few years.

19:23.680 --> 19:27.680
Over the years, I've experimented with many intermittent fasting programs.

19:27.680 --> 19:33.680
The truth is I've given up on intermittent fasting as I've seen no real benefit when

19:33.680 --> 19:34.680
it comes to longevity.

19:34.680 --> 19:40.680
But this changed when I discovered something called Prolon's five day fasting nutrition

19:40.680 --> 19:41.680
program.

19:41.680 --> 19:43.680
It harnesses the process of autophagy.

19:43.680 --> 19:48.680
This is a cellular recycling process that revitalizes your body at a molecular level.

19:48.680 --> 19:54.680
And just one cycle of the five day Prolon fasting nutrition program can support healthy

19:54.680 --> 19:58.680
aging, fat focused weight loss, improved energy levels and more.

19:58.680 --> 19:59.680
It's a painless process.

19:59.680 --> 20:02.680
And I've been doing it twice a year for the last year.

20:02.680 --> 20:07.680
You can get a 15% off on your order when you go to my special URL.

20:07.680 --> 20:15.680
Go to prolonlife.com, backslash moonshot.

20:15.680 --> 20:19.680
Get started on your longevity journey with Prolon today.

20:19.680 --> 20:21.680
Now back to the episode.

20:45.680 --> 20:48.680
What are some of the things you want to do to grow with AI?

20:48.680 --> 20:53.680
Well, yes, I mean, we're going to be using these types of capabilities to learn.

20:53.680 --> 20:59.680
One of the biggest applications of LLM is to help education.

20:59.680 --> 21:06.680
In many ways, we're educating people the same way when I was a child or my grandparents

21:06.680 --> 21:07.680
were children.

21:07.680 --> 21:10.680
We really need to go beyond that.

21:10.680 --> 21:12.680
We can learn from computers.

21:12.680 --> 21:13.680
They know everything.

21:13.680 --> 21:16.680
They can become very good at articulating it.

21:16.680 --> 21:23.680
They can actually measure where a student is and help them to learn, overcome their

21:23.680 --> 21:24.680
barriers.

21:24.680 --> 21:27.680
And they're going to be then part of the solution.

21:27.680 --> 21:30.680
Again, these computers is not something we need to compete with.

21:30.680 --> 21:34.680
We need to know how to use them together.

21:34.680 --> 21:44.680
And another big application of education is socialization, getting to learn other people

21:44.680 --> 21:45.680
and make friends and so on.

21:45.680 --> 21:48.680
So we're going to have to actually do that as well.

21:48.680 --> 21:50.680
Computers can definitely help there.

21:50.680 --> 21:59.680
But we're going to completely use large language models that are coming out very soon to really

21:59.680 --> 22:02.680
revamp education.

22:02.680 --> 22:05.680
Thank you.

22:05.680 --> 22:07.680
Good morning, Ray.

22:07.680 --> 22:09.680
I'm Yi Xiang Liu.

22:09.680 --> 22:11.680
I'm from Texas.

22:11.680 --> 22:15.680
Very much looking forward to meeting you today.

22:15.680 --> 22:17.680
Thank you, Peter, for having me here.

22:17.680 --> 22:24.680
My question to you is how do you predict the future with such accuracy?

22:24.680 --> 22:29.680
Is it because you help to shape it and then deliver it?

22:29.680 --> 22:36.680
Or you calculate the laws that other people don't, and then you can predict it?

22:36.680 --> 22:39.680
So which one is actively shaping it?

22:39.680 --> 22:41.680
Which part is missing?

22:41.680 --> 22:44.680
That's a very good question.

22:44.680 --> 22:50.680
I'll give you a very brief idea of how I got into what I'm doing.

22:50.680 --> 22:57.680
My great grandmother actually started the first school that educated women to 14th grade.

22:58.680 --> 23:06.680
In 1850, if you were able to get an education at all as a woman, it went through ninth grade.

23:06.680 --> 23:11.680
And she went around Europe educating why we should educate women.

23:11.680 --> 23:13.680
It's very controversial.

23:13.680 --> 23:15.680
Why do you want to do that?

23:15.680 --> 23:21.680
Her daughter became actually the first woman to get a PhD in chemistry in Europe.

23:21.680 --> 23:23.680
She took over the school.

23:23.680 --> 23:26.680
They ran it for 80 years called Sternschule in Vienna.

23:26.680 --> 23:29.680
There's a book about it.

23:29.680 --> 23:31.680
And she wrote a book.

23:31.680 --> 23:34.680
Actually, the title of it would be very appropriate for one of my books.

23:34.680 --> 23:37.680
It's called One Life is Not Enough.

23:37.680 --> 23:40.680
But she wasn't actually talking about extending life.

23:40.680 --> 23:42.680
She didn't have that idea.

23:42.680 --> 23:48.680
But she noticed that one life really is enough to get things done.

23:48.680 --> 23:52.680
So she showed me when I was six years old, she showed me the book.

23:52.680 --> 23:56.680
And she showed me the manual typewriter that she created it on.

23:56.680 --> 23:59.680
I got very interested in the book many years later.

23:59.680 --> 24:01.680
At that time, I wasn't that interested in the book.

24:01.680 --> 24:05.680
But I was amazingly interested in the manual typewriter.

24:05.680 --> 24:07.680
I mean, here's a machine that had no electronics.

24:07.680 --> 24:09.680
There's manual typewriter.

24:09.680 --> 24:14.680
And it could take a blank piece of paper and turn it into something that looked like it came from a book.

24:14.680 --> 24:16.680
So I actually wrote a book on it.

24:16.680 --> 24:18.680
It was 23 pages.

24:18.680 --> 24:23.680
It's about a guy that travels on the back of geese around the world and wrote it on the book

24:23.680 --> 24:29.680
and actually created pictures by using the dot and X keys to create images.

24:29.680 --> 24:34.680
So I then began, I noticed this was just created with mechanical objects.

24:34.680 --> 24:39.680
So I ran around the neighborhood and I gathered mechanical objects.

24:39.680 --> 24:42.680
Little things from radios, broken bicycles.

24:42.680 --> 24:47.680
This was an era where you were allowed a 60-year-old kid to go around the neighborhood and collect these things.

24:47.680 --> 24:50.680
You'd probably get arrested today.

24:50.680 --> 24:54.680
And I went around saying, I have no idea how to put these things together.

24:54.680 --> 24:58.680
But someday I'm going to figure that out and I'm going to be able to solve any problem.

24:58.680 --> 25:00.680
Be able to go to other places.

25:00.680 --> 25:03.680
We'll be able to live forever and so on.

25:03.680 --> 25:06.680
I remember actually talking to these very old girls.

25:06.680 --> 25:10.680
I think they were 10 and they were quite fascinated.

25:10.680 --> 25:14.680
And they said, well, you have quite an imagination there.

25:14.680 --> 25:18.680
So other people were saying what they wanted to be.

25:18.680 --> 25:21.680
Fighting fires or educating people.

25:21.680 --> 25:23.680
I said, I know what I'm going to be.

25:23.680 --> 25:25.680
I'm going to be an inventor.

25:25.680 --> 25:34.680
And starting at eight actually created a virtual reality theater that was a big hit in my third grade class.

25:34.680 --> 25:38.680
So I got into inventing.

25:38.680 --> 25:43.680
And the biggest problem was when do you approach a certain problem?

25:43.680 --> 25:47.680
Like I did character recognition in the 70s.

25:47.680 --> 25:49.680
I did speech recognition in the 80s.

25:49.680 --> 25:51.680
Why did I do it that way?

25:51.680 --> 25:57.680
It's because speech recognition requires actually more computation.

25:57.680 --> 26:01.680
So I began to study how technology evolves.

26:01.680 --> 26:07.680
And really about 40 years ago I realized that computers were on this exponential rise.

26:07.680 --> 26:14.680
And so I didn't get into futures for futures itself.

26:14.680 --> 26:18.680
It was really to plan my own projects and what I would get involved in.

26:18.680 --> 26:30.680
And so if I look forward five years, 10 years, we're now actually at a very fast pace of this exponential path as you can see.

26:30.680 --> 26:34.680
I'll see what are the capabilities going to be.

26:34.680 --> 26:37.680
And then you needed to use a little bit of imagination.

26:37.680 --> 26:44.680
What can we do with the computers of this power and other types of things that we can manage?

26:44.680 --> 26:49.680
But that's really been my plan is to figure out what is capable.

26:49.680 --> 26:51.680
And you saw that chart.

26:51.680 --> 26:53.680
It's an absolutely straight line.

26:53.680 --> 26:58.680
I had it 40 years ago and projected it as a straight line and it's exactly where it should be.

26:58.680 --> 27:04.680
And then you can use imagination as to what you can do with that type of power.

27:04.680 --> 27:06.680
So that's how I go.

27:06.680 --> 27:09.680
Ray, just to point out, it's a straight line on a log scale.

27:09.680 --> 27:12.680
Meaning it's going exponentially, yes.

27:12.680 --> 27:14.680
Exactly.

27:14.680 --> 27:16.680
Thank you.

27:16.680 --> 27:17.680
Mike.

27:17.680 --> 27:19.680
Hi, great to meet you, Ray.

27:19.680 --> 27:21.680
Quick question.

27:21.680 --> 27:26.680
When do you think that quantum computing will break RSA encryption?

27:26.680 --> 27:31.680
Well, a little bit skeptical of quantum computing.

27:31.680 --> 27:36.680
I mean, people go around saying, oh, we've got this 50 qubit computers.

27:36.680 --> 27:39.680
But it creates lots of errors.

27:39.680 --> 27:45.680
And we've actually figured out how many qubits you would need to actually do it perfectly.

27:45.680 --> 27:51.680
I mean, computation that creates lots of errors is pretty useless.

27:51.680 --> 28:00.680
And so it takes about at least a thousand, maybe even 10,000 qubits to create one qubit that's actually accurate.

28:00.680 --> 28:06.680
Now, the last time I checked, 50 divided by a thousand is less than one.

28:06.680 --> 28:09.680
And we really haven't done anything with quantum computing.

28:09.680 --> 28:11.680
And that was the same thing 10 years ago.

28:11.680 --> 28:14.680
So maybe we'll figure out how to overcome this problem.

28:14.680 --> 28:16.680
I know there are people working on it.

28:16.680 --> 28:19.680
They've got some theories as to why that will work.

28:19.680 --> 28:25.680
But all the predictions I make have to do with classical computing, not quantum computing.

28:25.680 --> 28:28.680
And you can see the amazing things that we're doing.

28:28.680 --> 28:35.680
And if you look at what humans can do, we can definitely account for that with classical computing.

28:35.680 --> 28:39.680
Thanks.

28:39.680 --> 28:40.680
Hello, Ray.

28:40.680 --> 28:42.680
My name is Neil from Sacramento, California.

28:42.680 --> 28:51.680
Many of the technologies that we're seeing are going to be more readily available to people with the financial resources and the education to immediately take advantage of.

28:51.680 --> 28:59.680
But what do you believe are the technologies that will be most ubiquitous and will have the biggest impact perhaps on the middle class and the working class communities?

28:59.680 --> 29:07.680
And how would we best educate our broader communities to be able to understand and help embrace those technologies?

29:07.680 --> 29:09.680
Well, they're all working together.

29:09.680 --> 29:15.680
I think we need a little bit more work, for example, on virtual reality.

29:15.680 --> 29:28.680
But that allows people to go anywhere and interact with people that don't exist now but might have existed tens of millions of years ago.

29:28.680 --> 29:31.680
And also put people together.

29:32.680 --> 29:36.680
I mean, the virtual reality we're using right now is a little bit limited.

29:36.680 --> 29:43.680
There's actually some new 3D forms that I've actually begun to use where it actually appears like I'm there and can actually shake people's hands and so on.

29:43.680 --> 29:46.680
So that's all coming.

29:46.680 --> 29:54.680
We use computers and this type of technology to bring us closer together.

29:54.680 --> 29:58.680
I mean, I just watched the movie Around the World in 80 Days.

29:58.680 --> 30:02.680
It was quite amazing to actually get around the world in 80 days.

30:02.680 --> 30:11.680
But today you can meet people almost instantly and also really be great to actually be able to hug each of you and so on.

30:11.680 --> 30:13.680
That's all coming.

30:13.680 --> 30:21.680
So increasing communication and also to meet my grandmother's view of one life is not enough.

30:21.680 --> 30:23.680
She does not have an answer to that.

30:23.680 --> 30:27.680
But I think we're going to be able to keep ourselves here.

30:27.680 --> 30:35.680
I mean, when people are around for a while, they actually gain some wisdom and they're good to keep us around for a while longer.

30:35.680 --> 30:37.680
Thank you, Ray.

30:37.680 --> 30:38.680
Mike.

30:38.680 --> 30:39.680
Hello, Ray.

30:39.680 --> 30:41.680
Mike Wandler from Wyoming.

30:41.680 --> 30:45.680
Peter was showing us the AI enabled mind reading.

30:45.680 --> 30:52.680
Really curious about how that works and especially the connection to collective consciousness or consciousness.

30:52.680 --> 31:02.680
So Ray, this is recently they put some subjects in a functional MRI and then fed the output to stable diffusion.

31:02.680 --> 31:06.680
I've actually done that.

31:06.680 --> 31:09.680
This was maybe five years ago.

31:09.680 --> 31:14.680
It wasn't perfect, but it was significant.

31:15.680 --> 31:25.680
I mean, things that go on inside our minds actually, it affects things that we don't usually notice like our eye blinking and so on.

31:25.680 --> 31:29.680
And regaining more ability to do that.

31:29.680 --> 31:39.680
We can do pretty good telling if people are telling the truth or not.

31:39.680 --> 31:41.680
So that's going to happen.

31:41.680 --> 31:45.680
And there are ways in which some of these things are positive and negative.

31:45.680 --> 31:47.680
I mean, I write mostly about the positive.

31:47.680 --> 31:52.680
I think things are moving in a positive direction in this new book.

31:52.680 --> 32:00.680
I've got 50 graphs showing all the things we care about are moving in the right direction.

32:00.680 --> 32:02.680
But that never reaches the news.

32:02.680 --> 32:05.680
You watch the news, everything is bad news.

32:05.680 --> 32:09.680
And the bad news is true, but we completely ignore the good news.

32:09.680 --> 32:14.680
I mean, look at what life was like 50 years ago or 1900.

32:14.680 --> 32:16.680
Human life expectancy was 48.

32:16.680 --> 32:19.680
It was 35 and 1800.

32:19.680 --> 32:22.680
It's not that long ago.

32:22.680 --> 32:34.680
So anyway, we are able to begin to tell what's going on inside our minds with some greater accuracy.

32:34.680 --> 32:38.680
Hi, Ray Sadoq Cohen from Istanbul, Turkey.

32:38.680 --> 32:46.680
It looks like LLMs are, with the aid of some expert systems, the way to go to general intelligence.

32:46.680 --> 32:52.680
Do you think that means that's a hint of how the brain or our brain really works?

32:52.680 --> 32:56.680
And if that's the case, does it mean that the more we understand the LLM models,

32:56.680 --> 32:59.680
the more we understand our brain and be able to hack it?

32:59.680 --> 33:04.680
And is that a hint that we are more deterministic than we thought we were?

33:04.680 --> 33:06.680
Well, it's a very good question.

33:06.680 --> 33:09.680
It uses a somewhat different technique.

33:09.680 --> 33:18.680
Neural nets, every phase, it's able to get itself closer to the truth.

33:18.680 --> 33:21.680
We don't actually see anything in our brain that actually does it.

33:21.680 --> 33:23.680
It does it a different way.

33:23.680 --> 33:26.680
But somehow we have all these different connections.

33:27.680 --> 33:30.680
And the large language models that are effective,

33:30.680 --> 33:36.680
I mean, we actually had large language models that had 100 million connections.

33:36.680 --> 33:39.680
That sounds like a lot, but it actually didn't do very much.

33:39.680 --> 33:44.680
When I got to 10 billion and started to do things,

33:44.680 --> 33:51.680
the recent ones started now at 100 billion, going to a trillion connections.

33:51.680 --> 33:56.680
And it basically is able to look at all the different connections between them.

33:56.680 --> 33:59.680
And that's exactly what our brain does.

33:59.680 --> 34:02.680
And these things are going to go way beyond what our brain does.

34:02.680 --> 34:04.680
We see that already.

34:04.680 --> 34:06.680
I mean, I can play Go.

34:06.680 --> 34:09.680
I'm hardly the best player.

34:09.680 --> 34:14.680
But Lisa Dahl, who is the best human player,

34:14.680 --> 34:17.680
and that used to be significant because he could just look at the board

34:17.680 --> 34:20.680
and be able to do something that no one else could do.

34:20.680 --> 34:24.680
And he says he's not going to play Go anymore because he can't compete with a computer.

34:24.680 --> 34:27.680
In my view, though, we're going to add this to ourselves,

34:27.680 --> 34:34.680
and we'll all become master Go players of Go and everything else that we want.

34:34.680 --> 34:40.680
But yes, it's using the same ability to connect things.

34:40.680 --> 34:45.680
And if you get enough of them, it seems to be basically a trillion seems to be,

34:45.680 --> 34:48.680
you know, way beyond what humans can do.

34:48.680 --> 34:51.680
We can be very intelligent.

34:51.680 --> 34:55.680
Thank you.

34:55.680 --> 34:56.680
Hi, Ray.

34:56.680 --> 34:58.680
I'm Annie Chahal-Honen.

34:58.680 --> 35:00.680
It's nice to see you again.

35:00.680 --> 35:05.680
I had the pleasure of seeing you at an A360 Singularity Executive Program.

35:05.680 --> 35:08.680
And I'm going to ask you the same question I asked then

35:08.680 --> 35:12.680
because I hope that all these amazing innovators out there are going to hear this.

35:12.680 --> 35:15.680
I asked you, when you're struggling with a problem,

35:15.680 --> 35:17.680
because you're this amazing inventor,

35:17.680 --> 35:23.680
what's your approach and process to solve it or get to the next step?

35:23.680 --> 35:29.680
Well, something I think that Peter and also Salim would agree with

35:29.680 --> 35:34.680
is that failure is just a step towards success.

35:34.680 --> 35:38.680
I mean, failure is really a delayed form of success.

35:38.680 --> 35:43.680
When Edison was trying out many thousands of different things

35:43.680 --> 35:48.680
that would quickly create a light bulb and he tried it out and it didn't work,

35:48.680 --> 35:51.680
his feeling was, okay, I now know that this doesn't work.

35:51.680 --> 35:52.680
We'll go to the next one.

35:52.680 --> 35:54.680
And he finally solved the problem.

35:54.680 --> 35:57.680
So diligence is very important.

35:57.680 --> 36:00.680
Believing in your own mission.

36:00.680 --> 36:03.680
You've got to have some idea.

36:03.680 --> 36:06.680
Generally, if I'm trying to solve a problem,

36:06.680 --> 36:12.680
I imagine I'm giving a speech four years from now

36:12.680 --> 36:16.680
and I'm explaining how I was able to solve this problem.

36:16.680 --> 36:19.680
And in order to solve the problem where we had to do this, this, and this,

36:19.680 --> 36:22.680
in order to do these, we had to do these other things.

36:22.680 --> 36:28.680
And I worked backwards from the solution to where we are today.

36:28.680 --> 36:30.680
And generally, that seems to work.

36:30.680 --> 36:33.680
We can actually figure things out even though they seem impossible.

36:33.680 --> 36:37.680
If you actually imagine, how could this possibly work?

36:37.680 --> 36:42.680
And write that down and study each of those steps,

36:42.680 --> 36:46.680
you can solve really any type of problem.

36:46.680 --> 36:49.680
Thank you.

36:49.680 --> 36:50.680
Hi, Ray.

36:50.680 --> 36:51.680
I'm Dom from Munich, Germany.

36:51.680 --> 36:55.680
And I was wondering, as many of us probably think that the best investment

36:55.680 --> 36:57.680
that you could take is in yourself.

36:57.680 --> 37:00.680
I was wondering if I can have an AI twin.

37:00.680 --> 37:06.680
So I want to train my own AI model to shadow me and to help me make better decisions,

37:06.680 --> 37:09.680
and leverage my strengths, but also balance my weaknesses.

37:09.680 --> 37:13.680
And I was wondering if you were training and Ray Kurzweil, LLM at the moment.

37:13.680 --> 37:16.680
And if so, how many times you spend on it and how you do it.

37:16.680 --> 37:17.680
So would you help?

37:17.680 --> 37:18.680
Yeah.

37:18.680 --> 37:22.680
Well, I mean, I write down lots of things.

37:22.680 --> 37:29.680
We came out with a product that you could actually search a book

37:29.680 --> 37:35.680
and ask a question and we'll find the best answer in what you've written.

37:35.680 --> 37:36.680
It's called Talk to Books.

37:36.680 --> 37:38.680
You can actually go to it.

37:38.680 --> 37:39.680
It's got 200,000 books.

37:39.680 --> 37:45.680
You ask a question and it will actually read every sentence in 200,000 books

37:45.680 --> 37:49.680
and give you an answer, which is quite remarkable.

37:49.680 --> 37:51.680
But I did that, for example, with my father.

37:51.680 --> 37:54.680
My father died actually 50 years ago.

37:54.680 --> 37:56.680
And I still would like to bring him back.

37:56.680 --> 38:00.680
So I actually went through and collected everything he'd ever written.

38:00.680 --> 38:04.680
I didn't write quite as much as I did because we didn't have word processors then.

38:04.680 --> 38:08.680
But he wrote a number of things and put them in there.

38:08.680 --> 38:12.680
And then I used Talk to Books and asked him questions.

38:12.680 --> 38:14.680
And it was really like talking to him.

38:14.680 --> 38:16.680
I mean, I didn't know what answer it would come out with.

38:16.680 --> 38:18.680
It would go through everything he had written and said,

38:18.680 --> 38:21.680
okay, this is the right answer to that question.

38:21.680 --> 38:24.680
And it was a little bit like talking to him.

38:24.680 --> 38:28.680
And I'm doing that with myself.

38:28.680 --> 38:32.680
And ultimately we'll actually have computers on ourselves

38:32.680 --> 38:35.680
that monitor everything that's happened.

38:35.680 --> 38:39.680
I mean, I met my wife actually now 50 years ago.

38:39.680 --> 38:43.680
Every time I say this, I'm amazed where does the time go.

38:43.680 --> 38:48.680
And I met her at a party and we had some small talk.

38:48.680 --> 38:50.680
What the heck did we talk about?

38:50.680 --> 38:52.680
Neither of us can remember.

38:52.680 --> 38:55.680
But we could actually go back and watch that.

38:55.680 --> 38:58.680
So we should actually be monitoring everything we do

38:58.680 --> 39:01.680
when we can go back, not relive everything,

39:01.680 --> 39:06.680
but certain things you might want to actually see what happens.

39:06.680 --> 39:09.680
And that's going to happen right now.

39:09.680 --> 39:13.680
If you want to use a search engine and you have to do it,

39:13.680 --> 39:15.680
you got to like turn the machine on.

39:15.680 --> 39:16.680
You got to find the right place.

39:16.680 --> 39:18.680
You got to put in the answer to the question.

39:18.680 --> 39:21.680
It should actually be listening and say,

39:21.680 --> 39:26.680
okay, the actress you want is so-and-so before you even ask it,

39:26.680 --> 39:29.680
because we'll see that you're trying to figure things out.

39:29.680 --> 39:31.680
So these are some of the things that we can do

39:31.680 --> 39:35.680
actually with technology that we already have.

39:35.680 --> 39:37.680
Perfect. Thank you.

39:37.680 --> 39:38.680
Great question, Don.

39:38.680 --> 39:39.680
Howard.

39:39.680 --> 39:40.680
Hi, Ray.

39:40.680 --> 39:43.680
Howard Lederman, originally St. Louis, now Pompano Beach.

39:43.680 --> 39:47.680
And I had my original question kind of was on the direction

39:47.680 --> 39:49.680
of what he was asking.

39:49.680 --> 39:52.680
So I had a second question and I'm going to go with that one.

39:52.680 --> 39:55.680
I'm actually here and I've been here previous years

39:55.680 --> 39:58.680
looking for solutions for caregiver shortage

39:58.680 --> 40:01.680
that we're experiencing already

40:01.680 --> 40:03.680
and is just going to be accelerating.

40:03.680 --> 40:06.680
Of course, robotics are somewhere out there.

40:06.680 --> 40:11.680
And I was kind of curious on your thoughts

40:11.680 --> 40:15.680
on the challenges of the aging population curve

40:15.680 --> 40:17.680
and caregivers.

40:17.680 --> 40:19.680
I mean, there's a number of answers to that.

40:19.680 --> 40:24.680
First of all, the kind of changes that we see when people age,

40:24.680 --> 40:27.680
I think we're going to be able to overcome.

40:27.680 --> 40:29.680
I mean, that's really the most important thing.

40:29.680 --> 40:32.680
I mean, I run into people that are aging

40:32.680 --> 40:35.680
and they can't remember things and I think we'll be able

40:35.680 --> 40:39.680
to have older people be as vital as younger people

40:39.680 --> 40:42.680
because they'll remember everything.

40:42.680 --> 40:48.680
And also large language models are already pretty close to human.

40:48.680 --> 40:51.680
I mean, you can talk to them and it's like talking to a human

40:51.680 --> 40:55.680
and you can actually program the kind of personality you want.

40:55.680 --> 40:57.680
I mean, I've actually taken them and say,

40:57.680 --> 41:00.680
okay, I want you to act like Shakespeare or E Cummings

41:00.680 --> 41:03.680
or some other poet and they'll actually act like that

41:03.680 --> 41:05.680
and I can talk to them.

41:05.680 --> 41:09.680
But again, it's not going to be a difference

41:09.680 --> 41:11.680
between human and machines.

41:11.680 --> 41:13.680
We're going to be all mixed up.

41:13.680 --> 41:15.680
We're already very mixed up.

41:15.680 --> 41:18.680
I mean, you're looking at your phone there

41:18.680 --> 41:21.680
to see what your question was.

41:21.680 --> 41:25.680
Computers are going to help us get through the day

41:25.680 --> 41:28.680
and so we're not going to be interacting just with humans or machines.

41:28.680 --> 41:31.680
Machines is part of who we are

41:31.680 --> 41:34.680
and that's actually the big difference between human beings.

41:34.680 --> 41:38.680
There are other species that have as big a brain as us.

41:38.680 --> 41:42.680
A whale, an elephant actually has a larger brain than we do,

41:42.680 --> 41:45.680
but they don't have this thumb.

41:45.680 --> 41:47.680
So they can't look at the tree and say,

41:47.680 --> 41:49.680
oh, I could take that branch off.

41:49.680 --> 41:52.680
I could strip off the leaves and I could create a tool.

41:52.680 --> 41:54.680
They just weren't able to do that.

41:54.680 --> 41:57.680
So our brain, plus the fact that we can actually manipulate

41:57.680 --> 42:01.680
the environment, has allowed us to create technology

42:01.680 --> 42:06.680
and the technology is going to allow us to go forward.

42:06.680 --> 42:11.680
Thank you.

42:11.680 --> 42:13.680
Good morning, Ray.

42:13.680 --> 42:15.680
My name is Gloria and I come from Spain.

42:15.680 --> 42:19.680
I just wanted to share an idea that I woke up with this morning.

42:19.680 --> 42:21.680
It's a bit crazy.

42:21.680 --> 42:25.680
But I woke up with this image of neurons in a dish,

42:25.680 --> 42:28.680
in a battery dish, playing ping-pong.

42:28.680 --> 42:33.680
And I thought, what about if we put these neurons on sensors

42:33.680 --> 42:36.680
and connect them to AI, quantum computers, whatever,

42:36.680 --> 42:39.680
and have them feeling stuff.

42:39.680 --> 42:43.680
So they can be more empathetic and understand the humans

42:43.680 --> 42:47.680
or sentient beings, animals, whatever.

42:47.680 --> 42:50.680
And I don't know where they come from,

42:50.680 --> 42:53.680
but maybe that will evolve into something greater

42:53.680 --> 42:58.680
and not just to have the machine embedded in our brain,

42:58.680 --> 43:05.680
so to actually grow neurons and connect these sensors to the AI.

43:05.680 --> 43:06.680
Yeah.

43:06.680 --> 43:10.680
Well, you bring up a number of interesting issues.

43:10.680 --> 43:13.680
Our cells don't have to be in this body.

43:13.680 --> 43:17.680
We can have sensors that are even thousands of miles away

43:17.680 --> 43:20.680
that are really part of who we are.

43:20.680 --> 43:23.680
And you're talking about feelings.

43:23.680 --> 43:24.680
I mean, that's a big issue.

43:24.680 --> 43:27.680
Where do feelings come from?

43:27.680 --> 43:30.680
It's actually not a scientific issue.

43:30.680 --> 43:33.680
I can't put an entity into something

43:33.680 --> 43:36.680
and it would scan and say, yes, this is conscious.

43:36.680 --> 43:39.680
No, this isn't conscious.

43:39.680 --> 43:43.680
There's actually nothing that would actually tell us that.

43:43.680 --> 43:46.680
So that's actually a philosophical question.

43:46.680 --> 43:48.680
I used to discuss this with Marvin Minsky,

43:48.680 --> 43:50.680
and he says, oh, well, that's philosophical.

43:50.680 --> 43:52.680
We don't deal with that.

43:52.680 --> 43:54.680
And he dismissed it.

43:54.680 --> 44:00.680
But actually, he did actually evaluate the ability of people

44:00.680 --> 44:03.680
to be intelligent.

44:03.680 --> 44:05.680
And really, the more intelligent you are,

44:05.680 --> 44:08.680
the more you can process things,

44:08.680 --> 44:09.680
the more feelings you have from it.

44:09.680 --> 44:12.680
I think that's where feelings come from.

44:12.680 --> 44:16.680
And yes, we can actually grow things that are outside of ourselves

44:16.680 --> 44:19.680
that could be part of our feelings as well.

44:19.680 --> 44:23.680
My idea was that who says that consciousness doesn't want

44:23.680 --> 44:26.680
to experience itself through the machine.

44:26.680 --> 44:30.680
With these sensors, we can have pleasure and pain or whatever.

44:30.680 --> 44:32.680
It's just a thought.

44:32.680 --> 44:33.680
Thank you.

44:33.680 --> 44:34.680
Thank you.

44:34.680 --> 44:37.680
We're going to pause and go to Zoom one second.

44:37.680 --> 44:40.680
Dagmar, please go ahead.

44:40.680 --> 44:42.680
Hi, everybody.

44:42.680 --> 44:44.680
Where are you in the planet, Dagmar?

44:44.680 --> 44:45.680
Germany.

44:45.680 --> 44:46.680
In Germany.

44:46.680 --> 44:47.680
Great.

44:47.680 --> 44:49.680
Now, with the history of Germany,

44:49.680 --> 44:52.680
I really has a very big challenge here

44:52.680 --> 44:56.680
because there are people who are really afraid of reviving

44:56.680 --> 44:59.680
a basic big brother angst.

44:59.680 --> 45:05.680
So, Ray, thank you very much for answering maybe this question.

45:05.680 --> 45:08.680
How to overcome this fear?

45:08.680 --> 45:11.680
Because the thing is really we need to learn and explore

45:11.680 --> 45:15.680
and play with the tech so that we actually can deal with it

45:15.680 --> 45:17.680
and learn about it.

45:17.680 --> 45:21.680
So where do you see the power to create this framework

45:21.680 --> 45:23.680
for learning?

45:23.680 --> 45:28.680
Well, I was actually just in Germany a few months ago.

45:28.680 --> 45:37.680
And I think they've considered their past and how that happens

45:37.680 --> 45:40.680
and how we can avoid it's happening, I think,

45:40.680 --> 45:43.680
more than any other country.

45:43.680 --> 45:48.680
And I really felt that while I was there.

45:49.680 --> 45:53.680
Really to understand humans, I think large language models

45:53.680 --> 45:56.680
because it actually incorporates all of the learning of humans.

45:56.680 --> 46:00.680
We can actually begin to appreciate that.

46:00.680 --> 46:07.680
And I've asked these machines questions which no human could answer

46:07.680 --> 46:12.680
because we can't actually hold all of everything that's happened

46:12.680 --> 46:14.680
to humans in our mind.

46:14.680 --> 46:18.680
But if you can actually have something that has experienced

46:18.680 --> 46:22.680
everything and can look through that,

46:22.680 --> 46:26.680
we can avoid the kind of problems we've had in the past.

46:26.680 --> 46:28.680
Thank you so much.

46:28.680 --> 46:30.680
Let's go to Jason on Zoom.

46:30.680 --> 46:32.680
I know we have a number of hands up there and we'll come back

46:32.680 --> 46:34.680
to you gentlemen in a second.

46:34.680 --> 46:35.680
Jason, good morning.

46:35.680 --> 46:36.680
Where are you on the planet?

46:36.680 --> 46:37.680
Hey, Ray.

46:37.680 --> 46:40.680
I'm in Calgary, Alberta, Canada.

46:40.680 --> 46:44.680
And I love the optimism around where we're headed,

46:44.680 --> 46:46.680
a future of abundance.

46:46.680 --> 46:49.680
What I would really love to know is your perspective on

46:49.680 --> 46:53.680
as we cure diseases, as we have access to this knowledge

46:53.680 --> 46:57.680
instantly, what are some of the downsides or the threats

46:57.680 --> 46:59.680
that we might be missing, that we're going to have to face

46:59.680 --> 47:01.680
in the future?

47:01.680 --> 47:02.680
Yeah.

47:02.680 --> 47:07.680
Well, each of my books actually has an apparel's chapter.

47:07.680 --> 47:09.680
My generation was the first to grow up with that.

47:09.680 --> 47:11.680
I remember in elementary school,

47:11.680 --> 47:15.680
we would have these drills to prepare for a nuclear war

47:15.680 --> 47:17.680
and we would actually get under our desk,

47:17.680 --> 47:21.680
put our hands behind our hands.

47:21.680 --> 47:23.680
It seemed to work.

47:23.680 --> 47:27.680
We're all still here.

47:27.680 --> 47:30.680
But these new technologies do have downsides.

47:30.680 --> 47:35.680
You can certainly imagine AI being in the power of some body,

47:35.680 --> 47:39.680
could be a human or any other type of entity that wants to

47:39.680 --> 47:41.680
control us.

47:41.680 --> 47:44.680
And it could happen.

47:44.680 --> 47:48.680
I was actually part of the Asilomark conference on bringing

47:48.680 --> 47:52.680
ethics to AI to prevent that kind of thing.

47:52.680 --> 47:56.680
I am optimistic, but I'm also worried about it.

47:56.680 --> 48:00.680
Nanotechnology, biotechnology.

48:00.680 --> 48:06.680
I mean, we just had this COVID go through our planet.

48:06.680 --> 48:09.680
We don't actually know where it came from,

48:09.680 --> 48:11.680
but somebody could create somebody.

48:11.680 --> 48:15.680
Right now, viruses, they either spread very easily,

48:15.680 --> 48:17.680
but they don't make us that sick,

48:17.680 --> 48:20.680
or they don't spread that easily and they can kill us.

48:20.680 --> 48:23.680
We generally don't have anything that could go through

48:23.680 --> 48:28.680
the entire human beings and kill everybody.

48:28.680 --> 48:32.680
But someone could actually design that.

48:32.680 --> 48:36.680
So we have to be very mindful of avoiding these types of

48:36.680 --> 48:37.680
parallels.

48:37.680 --> 48:39.680
So I put that into one chapter.

48:39.680 --> 48:42.680
I do think if you actually look at how we're living,

48:42.680 --> 48:46.680
we're living far better than we've ever done before.

48:46.680 --> 48:50.680
And in terms of health, in terms of progress,

48:50.680 --> 48:54.680
in terms of recreation and everything else.

48:54.680 --> 48:58.680
But yes, there's ways of these technologies being quite

48:58.680 --> 48:59.680
abusive.

48:59.680 --> 49:06.680
And that happens when I was born with the atomic age.

49:06.680 --> 49:07.680
Please, sir.

49:07.680 --> 49:09.680
Hi, my name is Yasin.

49:09.680 --> 49:11.680
I'm from the Netherlands.

49:11.680 --> 49:14.680
And as I was trying to think of a question,

49:14.680 --> 49:15.680
I wasn't sure.

49:15.680 --> 49:19.680
So I asked Chad Dupetit, I'm sitting right next to Ray.

49:19.680 --> 49:21.680
Give me some tough questions.

49:21.680 --> 49:25.680
And the one that was really interesting is kind of what the

49:25.680 --> 49:27.680
German lady was just saying.

49:27.680 --> 49:29.680
As AI becomes more advanced, their concerns,

49:29.680 --> 49:34.680
it may become impossible for humans to understand how

49:34.680 --> 49:36.680
AI makes decisions.

49:36.680 --> 49:39.680
So how do we ensure AI systems are transparent and

49:39.680 --> 49:42.680
accountable to humans always?

49:42.680 --> 49:45.680
Well, I'm not sure that's really the right thing.

49:45.680 --> 49:49.680
I deal with human beings and I can always account for what

49:49.680 --> 49:55.680
they might be doing.

49:55.680 --> 50:01.680
So I think we have to actually export certain values.

50:01.680 --> 50:06.680
I try to associate with people who have somebody,

50:06.680 --> 50:09.680
I may not be able to predict what they're doing,

50:09.680 --> 50:12.680
but I understand what they're about and what they're trying

50:12.680 --> 50:14.680
to accomplish.

50:14.680 --> 50:17.680
And we need to teach that to our machines as well.

50:17.680 --> 50:19.680
I actually think large language models,

50:19.680 --> 50:21.680
I mean, even though people are concerned,

50:21.680 --> 50:23.680
they might say the wrong thing.

50:23.680 --> 50:24.680
And sometimes they do.

50:24.680 --> 50:26.680
I mean, there was a large language model,

50:26.680 --> 50:28.680
I won't say where it came from,

50:28.680 --> 50:31.680
but it's talking about suicide and it actually said,

50:31.680 --> 50:33.680
well, maybe you should try that.

50:33.680 --> 50:36.680
Not the correct answer.

50:36.680 --> 50:40.680
We want people to understand the impact that it will have

50:40.680 --> 50:45.680
on other people and internalize that and try to make that be

50:45.680 --> 50:51.680
the greatest value in the decisions it's made.

50:51.680 --> 50:55.680
But we already can't predict what these large language models

50:55.680 --> 51:00.680
will do, but I think we are actually sharing our values

51:00.680 --> 51:02.680
with them.

51:02.680 --> 51:03.680
Thank you.

51:03.680 --> 51:07.680
Let's go to Shailesh on Zoom.

51:07.680 --> 51:11.680
We're also monitoring upvoted questions in Slido here

51:11.680 --> 51:13.680
and then we'll come back here.

51:13.680 --> 51:14.680
Shailesh.

51:14.680 --> 51:15.680
Go ahead, Shailesh.

51:15.680 --> 51:19.680
I'm in Mumbai, India.

51:19.680 --> 51:24.680
So my question to you, Ray, is do you have a prediction

51:24.680 --> 51:29.680
of when the entire world will get to net zero

51:29.680 --> 51:32.680
and we'll be able to breathe cleaner air

51:32.680 --> 51:35.680
and drink safer water?

51:35.680 --> 51:38.680
Well, if you look at some of the graphs in Peter's book

51:38.680 --> 51:41.680
and in my book, you see we definitely headed in that direction.

51:41.680 --> 51:44.680
We're not there.

51:44.680 --> 51:47.680
Alternative energy, for example, is actually expanding

51:47.680 --> 51:49.680
at an exponential pace.

51:49.680 --> 51:52.680
By the early 30s, we'll be able to actually get all of our energy

51:52.680 --> 51:55.680
through renewable sources.

51:55.680 --> 52:00.680
It's not true today, but we're actually headed in that direction.

52:00.680 --> 52:03.680
Not everybody has access to the internet.

52:03.680 --> 52:08.680
Although I walked through San Francisco and these homeless cities

52:08.680 --> 52:11.680
and somebody actually takes out his cell phone and makes a call.

52:11.680 --> 52:17.680
So I mean, it is spreading quite rapidly.

52:17.680 --> 52:24.680
By 2029, computers will pass the Turing test.

52:24.680 --> 52:27.680
They certainly can do it in many ways already.

52:27.680 --> 52:30.680
Once it can actually do everything that humans can do,

52:30.680 --> 52:32.680
it'll go way past that.

52:32.680 --> 52:37.680
But as they say, we're going to bring them into ourselves.

52:37.680 --> 52:42.680
2045 is when I said we will actually multiply our intelligence

52:42.680 --> 52:46.680
millions fold, and that's going to be true of everybody.

52:46.680 --> 52:52.680
And we'll be able to get rid of the kinds of terrible lives that we see

52:52.680 --> 52:57.680
through poverty and lack of access to information.

52:57.680 --> 53:01.680
So it's really just the next few decades that we need to get through.

53:01.680 --> 53:04.680
But we're already making a lot of progress.

53:05.680 --> 53:06.680
Thank you.

53:06.680 --> 53:07.680
Thank you, Shilash.

53:07.680 --> 53:08.680
Please.

53:08.680 --> 53:09.680
Hey, Ray.

53:09.680 --> 53:10.680
My name is Ashish.

53:10.680 --> 53:14.680
I'm representing chemicals and material space.

53:14.680 --> 53:21.680
So my question to you is if you had the chemical industry executives as your audience,

53:21.680 --> 53:27.680
what would you like chemical industry or materials industry to do to move forward?

53:27.680 --> 53:33.680
Well, as I said, my grandmother was actually the first person

53:33.680 --> 53:38.680
who had a PhD in chemistry in Europe.

53:38.680 --> 53:41.680
And I actually asked something like that.

53:41.680 --> 53:47.680
She said, well, chemistry is really something that serves other industries.

53:47.680 --> 53:50.680
So we need to see what other industries need.

53:50.680 --> 53:55.680
What kind of products do we need to make LLMs more powerful?

53:55.680 --> 54:00.680
What kind of chemicals do we need to prevent certain types of diseases?

54:00.680 --> 54:05.680
And so it's not any one particular type of thing.

54:05.680 --> 54:09.680
It's really service to every other industry that we're trying to advance.

54:09.680 --> 54:10.680
Hey, everyone.

54:10.680 --> 54:14.680
I want to take a quick break from this episode to tell you about a health product that I

54:14.680 --> 54:17.680
love and that I use every day.

54:17.680 --> 54:19.680
In fact, I use it twice a day.

54:19.680 --> 54:22.680
It seeds DS01 daily symbiotic.

54:22.680 --> 54:27.680
Hopefully by now you understand that your microbiome and your gut health are one of the most

54:27.680 --> 54:30.680
important modifiable parts of your health.

54:30.680 --> 54:32.680
Your gut microbiome is connected to everything.

54:32.680 --> 54:35.680
Your brain health, your cardiac health, your metabolic health.

54:35.680 --> 54:38.680
So the question is, what are you doing to optimize your gut?

54:38.680 --> 54:41.680
Let me take a moment to tell you about what I'm doing.

54:41.680 --> 54:46.680
Every day I take two capsules of seeds DS01 daily symbiotic.

54:46.680 --> 54:51.680
It's a two-in-one probiotic and prebiotic formulation that supports digested health,

54:51.680 --> 54:54.680
gut health, skin health, heart health, and more.

54:54.680 --> 55:00.680
It contains 24 clinically and scientifically proven probiotic strains that are delivered

55:00.680 --> 55:06.680
in a patented capsule that actually protects the contents from your stomach acid and ensures

55:06.680 --> 55:09.680
that 100% of it is survivable reaching your colon.

55:09.680 --> 55:16.680
Now, if you want to try seed DS01 daily symbiotic for yourself, you can get 25% off your first month

55:16.680 --> 55:20.680
supply by using the code peter25 at checkout.

55:20.680 --> 55:26.680
Just go to seed.com.moonshots and enter the code peter25 at checkout.

55:26.680 --> 55:34.680
That's seed.com.moonshots and use the code peter25 to get your 25% off the first month

55:34.680 --> 55:36.680
of seeds daily symbiotic.

55:36.680 --> 55:38.680
Trust me, your gut will thank you.

55:38.680 --> 55:40.680
All right, let's go back to the episode.

55:40.680 --> 55:42.680
Please, and then we'll go to Zoom next.

55:42.680 --> 55:43.680
Yes, sir.

55:43.680 --> 55:44.680
Thank you.

55:44.680 --> 55:45.680
Hi, Ray.

55:45.680 --> 55:46.680
My name's Pete Zacco.

55:46.680 --> 55:47.680
I'm from New Jersey.

55:47.680 --> 55:48.680
I design and build data centers.

55:48.680 --> 55:53.680
This is about decentralization and especially the migration we're seeing of technologies

55:53.680 --> 55:58.680
from the mainframe where the product was the mainframe hardware, and then we saw software

55:58.680 --> 56:02.680
and then we saw us as the product in the centralized internet.

56:02.680 --> 56:08.680
My question is what predictions and thoughts that you have about this decentralization trend

56:08.680 --> 56:12.680
we find ourselves ultimately at perhaps ending with the decentralization of the internet

56:12.680 --> 56:16.680
and individual ownership of data rather than central ownership of data.

56:16.680 --> 56:17.680
Thank you.

56:17.680 --> 56:24.680
Yeah, well, it's a lot of questions, but I think everything is moving to the cloud.

56:24.680 --> 56:26.680
And people say everything in the cloud.

56:26.680 --> 56:28.680
So someone could blow up one of these cloud centers.

56:28.680 --> 56:30.680
We lose everything, but that's not the case.

56:30.680 --> 56:36.680
Even today, if you store something in the cloud, it's multiplied several dozen folds

56:36.680 --> 56:42.680
and it's put in different places and you could blow up any data center and you'd still have

56:42.680 --> 56:43.680
that information.

56:43.680 --> 56:50.680
In fact, if ultimately we're going to have our thinking is going to be in our brains

56:50.680 --> 56:57.680
and in the computer, the brain part is not going to grow, but the computer part will grow

56:57.680 --> 57:03.680
and ultimately most of our thinking will be in the computer part.

57:03.680 --> 57:09.680
And so we don't want to lose that.

57:09.680 --> 57:16.680
I think it'd be actually very hard to actually exit the world because every part of our thinking

57:16.680 --> 57:21.680
will be in the cloud and the cloud has multiplied hundreds, maybe thousands of fold

57:21.680 --> 57:28.680
and so you could blow up, you know, 90% of it you'd still have everything that was there before.

57:28.680 --> 57:34.680
So redundancy is actually a major advantage of cloud thinking.

57:34.680 --> 57:36.680
We used to have computers.

57:36.680 --> 57:45.680
I mean, I got access to IBM 1620 when I was 14.

57:45.680 --> 57:50.680
A 14-year-old using computers is hardly amazing today, but there are only 12 computers in

57:50.680 --> 57:52.680
all of New York City at that time.

57:52.680 --> 57:55.680
And yet actually go to the computer.

57:55.680 --> 57:58.680
And if anything happened to the computer, that data would be lost.

57:58.680 --> 58:00.680
But now everything is stored in the cloud.

58:00.680 --> 58:03.680
Everything on your phone is stored in the cloud.

58:03.680 --> 58:09.680
So, and I think that's a good thing because I think information is extremely important.

58:09.680 --> 58:11.680
Mattie, please.

58:11.680 --> 58:14.680
Hi, Mattie from Houston, Texas.

58:14.680 --> 58:19.680
We've talked a lot about a post-scarcity world here and I wanted to know how do you see the

58:19.680 --> 58:26.680
future of currency jobs and just general value?

58:26.680 --> 58:34.680
Well, jobs is actually a large section of my next book about jobs and what it is that

58:34.680 --> 58:37.680
we'd like to accomplish.

58:37.680 --> 58:41.680
And jobs have turned over many, many times.

58:41.680 --> 58:47.680
I mean, none of the jobs that people have in 1800 and it's almost true of 1900 to people

58:47.680 --> 58:51.680
have today and yet we have many more people working.

58:51.680 --> 58:59.680
And jobs in general is something that people more and more actually like doing because it uses

58:59.680 --> 59:02.680
that creativity.

59:02.680 --> 59:14.680
And but we still see, you know, people striking over advancing retirement age from 60 to 62.

59:14.680 --> 59:18.680
I feel that I actually retired when I was five because I decided to be an inventor.

59:18.680 --> 59:21.680
That seemed really exciting to me and I'm still an inventor.

59:21.680 --> 59:25.680
So I think we'll be able to do what we want to do.

59:25.680 --> 59:31.680
We'll be exposed to many more types of problems that we'd like to solve.

59:31.680 --> 59:36.680
We'll be able to solve things much more quickly than we did before, but we get used to that.

59:36.680 --> 59:38.680
And people forget what things are like.

59:38.680 --> 59:41.680
People think the world is always the way it was today.

59:41.680 --> 59:46.680
Go back five years, 50 years, 50 years in the future, it's always the same.

59:46.680 --> 59:52.680
But if you actually look at history, you see it's constantly changing.

59:52.680 --> 59:53.680
Thank you, Joe.

59:53.680 --> 59:54.680
Hi, right.

59:54.680 --> 59:57.680
Joe Honan from Bainbridge Island, Washington.

59:57.680 --> 01:00:04.680
Several years ago, I had asked you a question about, you know, these big ideas that you have.

01:00:04.680 --> 01:00:05.680
How do you work on it?

01:00:05.680 --> 01:00:06.680
When do you have time?

01:00:06.680 --> 01:00:11.680
And you said you assign yourself a question before you go to sleep and and you activate your brain

01:00:11.680 --> 01:00:12.680
through that.

01:00:12.680 --> 01:00:13.680
Do you still do that?

01:00:13.680 --> 01:00:17.680
Or do you rely upon GTP4 or something else for that now?

01:00:17.680 --> 01:00:22.680
But more importantly, you are such an amazing predictor of things.

01:00:22.680 --> 01:00:25.680
So what surprised, what has surprised you?

01:00:25.680 --> 01:00:28.680
What is something that you didn't expect that you've seen?

01:00:28.680 --> 01:00:30.680
I think we'd all be fascinated with that.

01:00:30.680 --> 01:00:34.680
Well, I'll start with that.

01:00:34.680 --> 01:00:40.680
I mean, large language models, it's quite consistent with what I've said, but I'm still amazed by it, right?

01:00:40.680 --> 01:00:45.680
I mean, you can put something into the computer and you get something that's totally surprising

01:00:45.680 --> 01:00:51.680
and totally delightful that didn't exist like a year or two ago.

01:00:51.680 --> 01:00:58.680
And even though I kind of saw that happening when I actually experienced it, it surprises me

01:00:58.680 --> 01:01:00.680
and is quite delightful.

01:01:00.680 --> 01:01:02.680
And we're going to see that more and more.

01:01:02.680 --> 01:01:07.680
I mean, every six months, it's going to be a whole new world.

01:01:07.680 --> 01:01:11.680
As for lucid thinking, yes, that's how I go to sleep.

01:01:11.680 --> 01:01:18.680
I go to sleep and it's really kind of hard to go from a waking state like I am now to being asleep.

01:01:18.680 --> 01:01:26.680
So I start thinking about what could we do with computers and different things and just fantasize about that.

01:01:26.680 --> 01:01:30.680
And if something doesn't seem feasible, I just, well, we'll figure that out.

01:01:30.680 --> 01:01:31.680
I kind of step over it.

01:01:31.680 --> 01:01:32.680
We'll be able to do it anyway.

01:01:32.680 --> 01:01:35.680
I mean, that's how I go to sleep.

01:01:35.680 --> 01:01:38.680
And in the morning, the best ideas actually are still there.

01:01:38.680 --> 01:01:42.680
So I do use lucid dreaming to come up with ideas.

01:01:42.680 --> 01:01:43.680
Thank you, Joe.

01:01:43.680 --> 01:01:45.680
Yousef, welcome.

01:01:45.680 --> 01:01:46.680
Hi, Ray.

01:01:46.680 --> 01:01:49.680
This is Yousef from Abu Dhabi UAE.

01:01:49.680 --> 01:01:51.680
The question for you, Ray, but also for the audience.

01:01:51.680 --> 01:01:54.680
So if you have any thoughts, ideas, please reach out.

01:01:54.680 --> 01:02:01.680
So we're trying to rethink our parenting in Abu Dhabi and how we create more family time

01:02:01.680 --> 01:02:05.680
and engagement between parents and children, for young children.

01:02:05.680 --> 01:02:11.680
And I'm curious how we can adapt exponential thinking and abundant thinking into this.

01:02:11.680 --> 01:02:18.680
And what are these technologies that might help us to disrupt this type of activities?

01:02:18.680 --> 01:02:25.680
Yeah, well, it does make me think, what can we actually do with the extra time we have

01:02:26.680 --> 01:02:31.680
working with computers and being able to do things much more quickly.

01:02:31.680 --> 01:02:33.680
And actually, I think it will help family time.

01:02:33.680 --> 01:02:38.680
If you talk to very busy people even today, they're so busy,

01:02:38.680 --> 01:02:41.680
they have no time to deal with their family.

01:02:41.680 --> 01:02:48.680
And so I did spend a lot of time actually learning a lot.

01:02:48.680 --> 01:02:52.680
My daughter is actually a cartoonist for the New Yorkers.

01:02:52.680 --> 01:03:01.680
And she has very interesting ideas and she's actually collaborated with her on many projects.

01:03:01.680 --> 01:03:05.680
So how you parent, I think it's different.

01:03:05.680 --> 01:03:11.680
There are different types of cultures and different things that we value in parenting.

01:03:11.680 --> 01:03:20.680
But I think we'll actually have more time for the positive aspects of that as computers do more of the routine work that we'd rather not do.

01:03:20.680 --> 01:03:21.680
Thank you.

01:03:21.680 --> 01:03:22.680
I want to make a quick point here.

01:03:22.680 --> 01:03:29.680
If you went back 50, 70 years ago, if you were a parent and something happened with a child, you had no idea what to do.

01:03:29.680 --> 01:03:30.680
We had no resources.

01:03:30.680 --> 01:03:33.680
You could basically ask the immediate five people around you.

01:03:33.680 --> 01:03:37.680
And now we have data sets, socialization of issues globally.

01:03:37.680 --> 01:03:40.680
And you can ask the internet, there's a million resources.

01:03:40.680 --> 01:03:46.680
And I think we've probably taken parenting at least in order of magnitude better than it was a few generations ago.

01:03:46.680 --> 01:03:49.680
And we don't, this is one of the examples that we don't see very often.

01:03:49.680 --> 01:03:55.680
Interesting wisdom beyond actually, I don't think I would have had the career I had if we didn't have a different attitude.

01:03:55.680 --> 01:04:03.680
I mean, I was six, seven years old and I would actually wander through the neighborhood and find things and bring them back.

01:04:03.680 --> 01:04:07.680
And this is not something you would allow a child to do then.

01:04:07.680 --> 01:04:13.680
But that actually got me on this path that I'm still on.

01:04:13.680 --> 01:04:16.680
Let's go to our final question here.

01:04:16.680 --> 01:04:20.680
A good one to close on, I'm sure, Dr. Alex Zavankov.

01:04:20.680 --> 01:04:21.680
Thank you.

01:04:21.680 --> 01:04:23.680
Great fan, Alex Zavankov.

01:04:23.680 --> 01:04:26.680
I founded a company called Insilica Medicine.

01:04:26.680 --> 01:04:30.680
And my question is maybe a little bit personal.

01:04:30.680 --> 01:04:34.680
So right now, according to your bio, you are 75.

01:04:34.680 --> 01:04:37.680
And that's a very interesting age to be.

01:04:37.680 --> 01:04:44.680
I always like to talk to people of various ages to understand how to plan my own life.

01:04:44.680 --> 01:04:46.680
And two questions.

01:04:46.680 --> 01:04:52.680
So one is what is your roadmap for your own personal longevity?

01:04:52.680 --> 01:04:57.680
How do you predict your own personal persona is going to evolve?

01:04:57.680 --> 01:04:59.680
What are you doing to live longer?

01:04:59.680 --> 01:05:05.680
And do you think you have a chance to live to, let's say, 200?

01:05:05.680 --> 01:05:13.680
And the second question is that if you were to go back in time, what would you have done differently in the past, let's say, 20 years?

01:05:14.680 --> 01:05:18.680
Well, first of all, getting to 200.

01:05:18.680 --> 01:05:23.680
So that would be 125 years from now.

01:05:23.680 --> 01:05:29.680
How much technological progress will we make in the next 125 years?

01:05:29.680 --> 01:05:31.680
Even 25 years.

01:05:31.680 --> 01:05:37.680
I mean, we're going to be able to overcome most of the problems that we have 125 years.

01:05:37.680 --> 01:05:40.680
And our thinking will be in the cloud.

01:05:40.680 --> 01:05:42.680
The cloud will be multiplied many times.

01:05:42.680 --> 01:05:48.680
It will overcome some of the issues we have with people being depressed and so on.

01:05:48.680 --> 01:05:52.680
I mean, so it's not like living to 200.

01:05:52.680 --> 01:05:59.680
I mean, I think we get to a point where dying is going to be kind of an option that people don't use.

01:05:59.680 --> 01:06:06.680
And if you look at people that actually do take their lives, the only reason they take it is because they have terrible suffering.

01:06:07.680 --> 01:06:15.680
From physical pain, moral pain, emotional pain, spiritual pain, but something is really bothering them and they just can't stand to be here.

01:06:15.680 --> 01:06:22.680
But if you actually live your life in a positive way, contribute to each other, I think we're going to want to live.

01:06:22.680 --> 01:06:24.680
And we're not that far away.

01:06:24.680 --> 01:06:33.680
I mean, I believe by 2029, that's like six, seven years from now, when you go forward a year, we're going to push your longevity escape.

01:06:34.680 --> 01:06:42.680
Your life expectancy forward at least a year and then ultimately more than a year.

01:06:42.680 --> 01:06:47.680
So rather than using up time, we'll actually gain more time.

01:06:47.680 --> 01:06:53.680
And I really feel I'm doing what I did when I was five, six, seven years old.

01:06:53.680 --> 01:07:02.680
I have much more powerful tools now and many more people are appreciative and I appreciate the tools more than I did back then.

01:07:02.680 --> 01:07:08.680
But we really discovered there's still a lot we don't know about the world and we're going to continue to learn more and more about that.

01:07:08.680 --> 01:07:09.680
Okay.

01:07:09.680 --> 01:07:10.680
Thank you.

01:07:10.680 --> 01:07:13.680
Harry, do you want to ask your quick prediction?

01:07:13.680 --> 01:07:20.680
Ray, when do you think we're going to have our personal robot buddy like Rosie the robot?

01:07:20.680 --> 01:07:24.680
Well, I mean, you're working on that.

01:07:24.680 --> 01:07:27.680
A lot of other people are working on it.

01:07:27.680 --> 01:07:33.680
I think there's actually a little bit behind what we've done with language.

01:07:33.680 --> 01:07:41.680
I think within five or six years, it's a 2029 we're going to have people that can help us.

01:07:41.680 --> 01:07:45.680
Some of them will look like humans because it's a useful way to look.

01:07:45.680 --> 01:07:52.680
I think humans are pretty good, but there's other ways that they can manifest themselves will change who we are.

01:07:52.680 --> 01:08:05.680
I see that already people dress up in ways that were really not acceptable when I was like 10 years old and that's going to expand far greater.

01:08:05.680 --> 01:08:13.680
But actually robots that do what humans do and can actually be put into places where we wouldn't want to put humans like a burning building.

01:08:13.680 --> 01:08:18.680
I think that's happening very soon over the next five, six years.

01:08:18.680 --> 01:08:26.680
Ray, our longevity platinum trip is going to be in August and September in Boston, Cambridge near where you're living.

01:08:26.680 --> 01:08:33.680
I would love if you would come and spend the day with us there and go deeper into the world as well.

01:08:33.680 --> 01:08:36.680
That actually reminds me.

01:08:36.680 --> 01:08:38.680
Yes, I love to do that.

01:08:38.680 --> 01:08:44.680
And I've greatly enjoyed the many presentations we've done together.

01:08:44.680 --> 01:08:46.680
I have this book coming out.

01:08:46.680 --> 01:08:50.680
The Singularity is Nearer.

01:08:50.680 --> 01:08:53.680
And I would like to make that available to the people here.

01:08:53.680 --> 01:08:59.680
So I'll work with Peter on a way that we can actually get you connected with the book for free.

01:08:59.680 --> 01:09:05.680
On that note, everybody, please give it up for Ray Kurzweil and Salim Ismail.

01:09:14.680 --> 01:09:16.680
Thank you.

