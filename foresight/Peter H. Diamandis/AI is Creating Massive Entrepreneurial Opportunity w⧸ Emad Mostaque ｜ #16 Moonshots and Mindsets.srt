1
00:00:00,000 --> 00:00:02,560
So again, like, if you're an entrepreneur, you'd be an entrepreneur in this.

2
00:00:02,560 --> 00:00:07,920
If you are someone who can communicate, you communicate this to other people and you get paid a million bucks a year as a consultant.

3
00:00:07,920 --> 00:00:09,560
Right. You organize information.

4
00:00:09,560 --> 00:00:14,960
If you're an artist, if you're creative, if you're an artist, you become the most efficient artist in the world when you lean in on this.

5
00:00:15,560 --> 00:00:18,680
You know, like systems can be outcompeted.

6
00:00:18,680 --> 00:00:20,400
It's like the example of the steel mill, right?

7
00:00:20,400 --> 00:00:24,600
There were big vertically integrated steel mills that were outcompeted by lots of little steel mills.

8
00:00:24,600 --> 00:00:26,440
Sure. Micro mills, right?

9
00:00:26,440 --> 00:00:34,480
The big corporations, the big programs, the big things will be outcompeted by just individuals and small groups.

10
00:00:34,480 --> 00:00:36,520
Building on top of this technology can do anything.

11
00:00:38,200 --> 00:00:41,720
And a massive transform to purpose is what you're telling the world.

12
00:00:41,720 --> 00:00:44,920
It's like, this is who I am. This is what I'm going to do.

13
00:00:44,920 --> 00:00:47,080
This is the dent I'm going to make in the universe.

14
00:00:50,520 --> 00:00:52,960
Welcome, everybody. Welcome to Moonshots and Mindsets.

15
00:00:53,000 --> 00:00:57,840
I am here with an old friend and a new friend, Imad Mustak.

16
00:00:57,840 --> 00:01:00,320
Imad, welcome. It's a pleasure to have you here.

17
00:01:00,560 --> 00:01:01,240
Thank you for having me.

18
00:01:01,240 --> 00:01:11,080
We're going to have such a fun, wide-ranging conversation across everything that is of, I think, importance to anyone listening to any entrepreneurs,

19
00:01:11,080 --> 00:01:13,560
any CEOs, any government leaders, any kids.

20
00:01:14,080 --> 00:01:17,160
Everything that you're doing is really transforming the world.

21
00:01:17,160 --> 00:01:18,480
Let me do a proper introduction.

22
00:01:18,480 --> 00:01:26,960
Imad Mustak is the founder and CEO of stability.ai, focused on amplifying humanity's potential through AI.

23
00:01:26,960 --> 00:01:33,920
You probably know stability AI because of its text-to-image model, Stable Diffusion, released in 2022,

24
00:01:33,920 --> 00:01:38,800
which rocked the developed world and I think broke the internet as a good way to describe it.

25
00:01:38,800 --> 00:01:43,200
Previously, Imad has been a hedge fund manager and autism researcher.

26
00:01:43,200 --> 00:01:49,200
Now, he's led multiple technology initiatives across multilateral organizations and governments

27
00:01:50,320 --> 00:01:51,920
and an XPRIZE competitor.

28
00:01:51,920 --> 00:01:58,720
Today, Imad is pursuing an incredible Moonshot, actually a series of Moonshots that can transform many industries.

29
00:01:58,720 --> 00:02:03,200
You know, I started by saying, which industries are you looking to disrupt?

30
00:02:03,520 --> 00:02:07,360
And your answer was, all of them, could do better.

31
00:02:08,080 --> 00:02:10,560
And I think that's not hype.

32
00:02:10,560 --> 00:02:15,280
I think that's actually, from what I understand, what we're going to explore, true.

33
00:02:17,280 --> 00:02:20,800
Scientists and technologists have been talking about AI for decades.

34
00:02:22,640 --> 00:02:24,960
But today is different, isn't it?

35
00:02:25,520 --> 00:02:28,880
Yeah. I mean, we always say this time is different, but this time really is different.

36
00:02:29,520 --> 00:02:33,040
I mean, if you kind of look at it, AI is basically information classification.

37
00:02:33,040 --> 00:02:37,520
And we had classic AI, which was information goes in and then you extrapolate from a dataset,

38
00:02:37,520 --> 00:02:39,760
you create really custom models and it goes out.

39
00:02:39,760 --> 00:02:44,960
Like the big AIs, Internet2 was Google and Facebook taking all that big data and then

40
00:02:44,960 --> 00:02:50,880
targeting PETA with like ads for rockets, you know, and things like that, you know, like rockets.

41
00:02:52,720 --> 00:02:57,680
In 2017, we had a bit of a step change where there was a paper called attention is all you need.

42
00:02:58,480 --> 00:03:01,360
About how to teach an AI to pay attention to the important things.

43
00:03:01,360 --> 00:03:02,160
Fascinating.

44
00:03:02,160 --> 00:03:03,760
And learn principles.

45
00:03:03,760 --> 00:03:05,360
Yeah, principle based analysis.

46
00:03:06,240 --> 00:03:10,880
Yeah. So it's not good old fashioned AI or kind of logic causal based AI, but it's kind of that.

47
00:03:10,880 --> 00:03:16,400
Because you remember, like, who was it by type one, type two thinking?

48
00:03:17,200 --> 00:03:17,760
A con.

49
00:03:18,960 --> 00:03:23,040
So, you know, there's the very logical thing and there's a freaking tiger in that bush over there.

50
00:03:24,160 --> 00:03:25,680
And we didn't have that second part.

51
00:03:25,680 --> 00:03:28,800
We didn't have the ability to kind of just leap to conclusions,

52
00:03:28,800 --> 00:03:30,800
principle based, heuristic based thinking.

53
00:03:30,800 --> 00:03:32,960
You know, this is kind of the mindset thing, right?

54
00:03:33,040 --> 00:03:37,840
Whereby you construct these things and it allows you to go very fast to just amazing conclusions.

55
00:03:37,840 --> 00:03:39,600
That's part of what makes humans humans.

56
00:03:39,600 --> 00:03:40,320
Yes.

57
00:03:40,320 --> 00:03:41,040
It's here now.

58
00:03:41,040 --> 00:03:42,560
It actually works.

59
00:03:42,560 --> 00:03:47,120
Leaping to conclusion, but so it's no longer extrapolation based upon data.

60
00:03:47,120 --> 00:03:53,120
It's actually learning to understand the meaning hidden in the data.

61
00:03:53,120 --> 00:03:53,920
Exactly.

62
00:03:53,920 --> 00:03:59,040
It's kind of semantic understanding, kind of literally, it's kind of called latent spaces,

63
00:03:59,040 --> 00:04:00,720
hidden layers of understanding.

64
00:04:00,720 --> 00:04:03,440
So, like, I'll give you an example.

65
00:04:03,440 --> 00:04:08,160
One of the ways that I kind of entered AI was working with AI to do drug repurposing

66
00:04:08,160 --> 00:04:09,360
for my son who has autism.

67
00:04:09,360 --> 00:04:09,920
Yes.

68
00:04:09,920 --> 00:04:13,680
So, he was two years old at the time and the doctor said nothing could be done.

69
00:04:13,680 --> 00:04:16,000
And I was like, of course, there's something can be done.

70
00:04:16,000 --> 00:04:17,840
Like, let's try and figure out information.

71
00:04:17,840 --> 00:04:20,000
Because all we're looking for is information that needs to come in time.

72
00:04:20,000 --> 00:04:22,400
It's kind of the Claude Shannon theory of information theories.

73
00:04:22,400 --> 00:04:24,880
Information is valuable in as much as it changes the state.

74
00:04:24,880 --> 00:04:28,160
So, how do you find the valuable information amongst all those autism studies

75
00:04:28,160 --> 00:04:29,040
and figure out what's going on?

76
00:04:29,040 --> 00:04:31,760
Because no one can really explain to me what caused it.

77
00:04:31,760 --> 00:04:38,400
So, just one second there, because typically, people are looking for a very clear cut answer

78
00:04:38,400 --> 00:04:40,560
in the data that's obvious.

79
00:04:41,120 --> 00:04:43,040
But there are answers, but it's hidden.

80
00:04:43,680 --> 00:04:44,400
It's hidden.

81
00:04:44,400 --> 00:04:45,120
It's kind of hidden.

82
00:04:45,120 --> 00:04:47,760
And so, you have to look at things from a first principles basis when you can't see

83
00:04:47,760 --> 00:04:49,680
things just from scanning everything.

84
00:04:49,680 --> 00:04:52,240
You have to dig down to another layer, another layer, another layer.

85
00:04:52,800 --> 00:04:58,800
And kind of when I dig down, I kind of used AI to do natural entity recognition

86
00:04:59,120 --> 00:05:01,600
look at all the different compounds, all the different trials that were tried,

87
00:05:01,600 --> 00:05:06,400
built a team, then built a biomolecular pathway analysis model of neurotransmitters

88
00:05:06,400 --> 00:05:06,720
in the brain.

89
00:05:06,720 --> 00:05:07,920
So, it's like just like a common cold.

90
00:05:07,920 --> 00:05:09,760
It seems like something very similar is happening.

91
00:05:10,800 --> 00:05:12,800
It turns out there's two neurotransmitters in the brain.

92
00:05:12,800 --> 00:05:13,920
One of them is GABA.

93
00:05:13,920 --> 00:05:15,840
You pop a valium and chill out.

94
00:05:15,840 --> 00:05:16,480
It chills you out.

95
00:05:17,120 --> 00:05:18,640
And then another one is Glucimate.

96
00:05:18,640 --> 00:05:21,360
And Glucimate kind of makes your brain accelerate.

97
00:05:21,360 --> 00:05:24,240
And so, you know, when you're tapping your leg and you can't focus and concentrate,

98
00:05:24,240 --> 00:05:25,120
that's what happens.

99
00:05:25,120 --> 00:05:28,560
Because humans are really a filtering entity, as it were.

100
00:05:28,560 --> 00:05:31,120
We have so much information in the world is just too much.

101
00:05:31,120 --> 00:05:32,320
So, our brain creates a simulation.

102
00:05:32,320 --> 00:05:34,960
That's why we've got the optic nerve, for example, filled it instantly.

103
00:05:36,000 --> 00:05:39,920
We are kind of constrained a little bit and that filtering allows us to focus.

104
00:05:39,920 --> 00:05:42,800
We all know what it's like when you're tapping your leg because there's too much stuff going on.

105
00:05:42,800 --> 00:05:45,280
So, with kids with ASD, they couldn't filter for various reasons,

106
00:05:45,280 --> 00:05:46,400
but it was different reasons.

107
00:05:46,400 --> 00:05:48,800
Not enough GABA, too much Glutamate.

108
00:05:48,800 --> 00:05:53,040
When you're born, GABA and Glutamate are actually both excitatory and oxytocin flips the switch

109
00:05:53,040 --> 00:05:53,600
on GABA.

110
00:05:53,600 --> 00:05:54,080
Interesting.

111
00:05:54,080 --> 00:05:56,080
So, we started digging down into kind of the things and we're like,

112
00:05:56,080 --> 00:05:58,080
ah, different compounds can affect this in different ways.

113
00:05:58,880 --> 00:06:00,640
But then what was the upshot of that?

114
00:06:00,640 --> 00:06:02,720
And how is this long story coming to a conclusion?

115
00:06:03,840 --> 00:06:06,080
Because there's too much noise, my son couldn't speak,

116
00:06:06,080 --> 00:06:10,800
because he couldn't formulate the hidden layers of meaning and connectivity on concepts.

117
00:06:10,800 --> 00:06:12,240
So, kind of we've got cup here.

118
00:06:12,240 --> 00:06:12,640
Yes.

119
00:06:12,640 --> 00:06:16,000
And you can cup your hands, you have a World Cup, which England hopefully will win,

120
00:06:16,000 --> 00:06:19,680
you know, and you've got various other meanings of the World Cup.

121
00:06:19,680 --> 00:06:22,800
You form this latent space of hidden connective meaning to that.

122
00:06:22,800 --> 00:06:25,920
So, it can be applied in different things, the principle-based analysis.

123
00:06:25,920 --> 00:06:30,080
By reducing the amount of noise, you can then filter and pay attention and build that out.

124
00:06:30,080 --> 00:06:31,920
The AI can now do the same thing.

125
00:06:31,920 --> 00:06:35,520
So, you don't need to have huge petabytes, exabytes of data anymore.

126
00:06:35,520 --> 00:06:38,080
With structured data, it can figure out interconnectivity and connection.

127
00:06:38,080 --> 00:06:43,440
And I want to come back to that because that's the revolution that you're creating right now.

128
00:06:44,080 --> 00:06:50,160
Just, you know, these personalized data sets that are good for you, your country,

129
00:06:50,160 --> 00:06:51,600
your company, yourself.

130
00:06:51,600 --> 00:06:52,640
Your context, exactly.

131
00:06:52,640 --> 00:06:52,720
Yes.

132
00:06:52,720 --> 00:06:55,520
Because humans are heuristic animals.

133
00:06:55,520 --> 00:06:57,600
We learn by principle-based analysis.

134
00:06:58,240 --> 00:07:01,200
And we're animals that are story-based.

135
00:07:01,200 --> 00:07:04,400
So, you have multiple stories that make you up, you know, from your work on the X-Price

136
00:07:04,400 --> 00:07:07,440
to your humanitarian to, you know, bold VC fund and all the things.

137
00:07:08,160 --> 00:07:10,320
We all kind of form connectivity there.

138
00:07:10,320 --> 00:07:12,560
But those stories are very hard to map initially.

139
00:07:12,560 --> 00:07:14,320
Now, we can do it dynamically.

140
00:07:14,320 --> 00:07:18,000
And we can start building tools to re-augment human potential by doing this.

141
00:07:18,000 --> 00:07:19,920
Because we can finally have that assistance.

142
00:07:19,920 --> 00:07:22,640
We're going to go so much, have so much fun on these conversations.

143
00:07:22,640 --> 00:07:28,960
But I want to take us back to some people around the world for the last year or so

144
00:07:28,960 --> 00:07:33,040
have heard of Dolly and Dolly 2.

145
00:07:33,040 --> 00:07:35,040
And here comes stable diffusion.

146
00:07:35,680 --> 00:07:39,520
That is an open source version of Dolly and Dolly 2.

147
00:07:39,520 --> 00:07:46,320
And as I said, broken internet, you sent me an image of the speed of growth on GitHub.

148
00:07:46,320 --> 00:07:46,800
Yeah.

149
00:07:46,800 --> 00:07:49,680
Of people using stable diffusion.

150
00:07:49,680 --> 00:07:52,240
And it was so funny because when you first sent me this image,

151
00:07:52,720 --> 00:07:59,120
here you see on GitHub the users who are on Ethereum over time.

152
00:08:00,240 --> 00:08:06,800
And then there's this line that goes straight up that I thought was one of the axes for the graph.

153
00:08:08,080 --> 00:08:09,120
Like what happened there?

154
00:08:09,120 --> 00:08:11,920
So first of all, what is stable diffusion?

155
00:08:11,920 --> 00:08:14,080
It's one product of your company, right?

156
00:08:14,080 --> 00:08:20,640
Well, stable diffusion is a community effort to build an AI that allows anyone to create anything.

157
00:08:20,640 --> 00:08:25,680
So words go in and then anything you describe comes out, which is a bit insane.

158
00:08:26,720 --> 00:08:31,040
So what we did is, you know, in collaboration with various entities,

159
00:08:31,040 --> 00:08:34,240
which we paid an important part in stable diffusion to is kind of led by us.

160
00:08:34,800 --> 00:08:39,360
We took 100,000 gigabytes of image label pairs, two billion images,

161
00:08:39,360 --> 00:08:43,760
and created a 1.6 gigabyte file that can run offline in your MacBook.

162
00:08:43,840 --> 00:08:52,480
1.6 gigabyte file, which is relatively you can transmit it over the phone network.

163
00:08:52,480 --> 00:08:55,200
You transmit it over the phone network, but you only need to transmit it once.

164
00:08:56,080 --> 00:09:00,320
Everyone having this tiny file that basically compresses the visual information of a snapshot

165
00:09:00,320 --> 00:09:06,880
of the internet can create anything from a high-resolution render of an apartment

166
00:09:07,440 --> 00:09:10,880
to Robert De Niro's Gandalf or anything else.

167
00:09:11,520 --> 00:09:16,000
And they can now do it in a second, something a week.

168
00:09:16,000 --> 00:09:18,400
They can do one of those images in 30 seconds.

169
00:09:19,440 --> 00:09:20,880
You mean one 30th of a second?

170
00:09:20,880 --> 00:09:21,600
One 30th of a second.

171
00:09:21,600 --> 00:09:22,560
We just broke in real time.

172
00:09:22,560 --> 00:09:24,240
We had a 30-time speed up in the last week.

173
00:09:24,240 --> 00:09:28,320
So when you mentioned that, just as we were getting ready for this,

174
00:09:28,320 --> 00:09:33,680
I said, so you're basically able to render a virtualized world in real time.

175
00:09:33,920 --> 00:09:34,240
Yes.

176
00:09:34,960 --> 00:09:40,800
And that's one step away from being able to render a movie in real time.

177
00:09:41,760 --> 00:09:49,280
So are we in the verge of basically, and we're sitting here in Los Angeles,

178
00:09:49,280 --> 00:09:51,360
and you've been in the Hollywood world.

179
00:09:51,360 --> 00:09:56,080
I mean, are we talking about rendering motion pictures by just writing,

180
00:09:56,080 --> 00:10:00,080
reading a script and having some stable diffusion,

181
00:10:00,080 --> 00:10:03,040
three-bottled, create a film in real time?

182
00:10:03,040 --> 00:10:04,160
Well, who needs to write a script?

183
00:10:04,160 --> 00:10:04,960
Okay, I did that.

184
00:10:07,760 --> 00:10:08,640
You just say it.

185
00:10:08,640 --> 00:10:10,240
I want something to make me happy, right?

186
00:10:10,240 --> 00:10:12,080
And then it'll pull together various models,

187
00:10:12,080 --> 00:10:13,280
and it can live-generate a movie.

188
00:10:13,280 --> 00:10:13,760
Yeah.

189
00:10:13,760 --> 00:10:15,040
So we're live-generating.

190
00:10:15,040 --> 00:10:18,160
So Hollywood, so in the next five years,

191
00:10:18,160 --> 00:10:20,800
yes, I think that ready player one awaits this world,

192
00:10:20,800 --> 00:10:23,120
minus the microtransactors of 18 ages will be here.

193
00:10:25,440 --> 00:10:25,680
Great.

194
00:10:25,680 --> 00:10:29,280
Anything you can imagine, iterate it, dynamic,

195
00:10:29,280 --> 00:10:33,520
new forms of communication, new affordances,

196
00:10:33,520 --> 00:10:34,880
like all these clicks and things like that.

197
00:10:34,880 --> 00:10:35,680
You don't need that anymore.

198
00:10:37,120 --> 00:10:39,920
Then this is one of the biggest evolutions in humanity ever,

199
00:10:40,560 --> 00:10:43,120
because the easiest way for us to communicate is what we're doing now

200
00:10:43,120 --> 00:10:44,160
and having a nice chat, right?

201
00:10:44,160 --> 00:10:45,280
Back and forth.

202
00:10:45,280 --> 00:10:46,640
Then the Gutenberg press came,

203
00:10:47,360 --> 00:10:49,840
and suddenly you could communicate through written form,

204
00:10:49,840 --> 00:10:50,720
and it's harder.

205
00:10:50,720 --> 00:10:51,440
Yes.

206
00:10:51,440 --> 00:10:55,360
But still, you see now GPT-3 and other of these AIs are made easier now.

207
00:10:55,360 --> 00:10:57,520
For anyone to do anything like chat, GPT just came out.

208
00:10:57,520 --> 00:10:58,080
It's amazing.

209
00:10:58,400 --> 00:11:00,080
Yes, I saw it from open AI.

210
00:11:00,080 --> 00:11:02,000
Everybody's making extraordinary claims

211
00:11:02,000 --> 00:11:03,440
about what it's going to do to Google.

212
00:11:03,440 --> 00:11:04,000
Exactly.

213
00:11:04,000 --> 00:11:06,720
Well, this is a very interesting thing.

214
00:11:07,840 --> 00:11:09,840
But then visual communication is the hardest,

215
00:11:10,880 --> 00:11:15,520
like you toil to be an artist or a lot of people on this call.

216
00:11:15,520 --> 00:11:17,520
How much have we toiled with PowerPoint?

217
00:11:18,240 --> 00:11:20,640
We're going to rid the world of the tyranny of PowerPoint.

218
00:11:20,640 --> 00:11:23,360
But that's my expertise from a PowerPoint expert.

219
00:11:23,360 --> 00:11:24,400
You're not a PowerPoint expert.

220
00:11:24,400 --> 00:11:25,920
You're a communication expert.

221
00:11:25,920 --> 00:11:29,120
So we remove the barriers whereby any visual medium can be created instantly,

222
00:11:29,120 --> 00:11:31,360
and you just describe to it how it shifts.

223
00:11:31,360 --> 00:11:35,200
And because it's got these latencies, it understands happier or sadder,

224
00:11:35,200 --> 00:11:38,320
or you can say flamboyant, and that's not a word,

225
00:11:38,320 --> 00:11:40,000
but it figures out what that kind of means.

226
00:11:40,000 --> 00:11:42,560
But the ability to rapidly iterate it and say,

227
00:11:42,560 --> 00:11:44,000
okay, I don't like that movie scene.

228
00:11:44,000 --> 00:11:47,600
Make it happier, make it brighter, make it more dramatic.

229
00:11:48,160 --> 00:11:52,480
And then being able to render any form of communications,

230
00:11:52,560 --> 00:11:57,200
like you just said, a movie and have a thousand variants,

231
00:11:57,200 --> 00:11:59,360
I mean, what's going to happen to Hollywood?

232
00:11:59,920 --> 00:12:02,240
I imagine Hollywood will be quite disintermediated.

233
00:12:02,240 --> 00:12:03,040
Yes.

234
00:12:03,040 --> 00:12:04,560
Hollywood in everyone's thing.

235
00:12:04,560 --> 00:12:05,840
I mean, what is Hollywood?

236
00:12:05,840 --> 00:12:09,520
Hollywood emerged because it was far enough away from the east coast

237
00:12:09,520 --> 00:12:10,880
that IP laws didn't apply.

238
00:12:12,720 --> 00:12:17,520
And then it got constructed around here as an entity

239
00:12:17,520 --> 00:12:20,880
that extracted rents from performers, ultimately, and creatives.

240
00:12:21,520 --> 00:12:25,200
Like how many creatives like the Hollywood or the music industry or others?

241
00:12:25,200 --> 00:12:28,000
Not many, because they tend to be treated quite badly.

242
00:12:28,000 --> 00:12:30,000
Some people achieve superstardom.

243
00:12:30,000 --> 00:12:32,720
But a lot of this technology now is just truly democratizing

244
00:12:32,720 --> 00:12:35,680
in that things are going from centralized to the edge,

245
00:12:35,680 --> 00:12:38,240
and we had to create centralized organizations as a people

246
00:12:38,240 --> 00:12:40,880
because we didn't have the information classification

247
00:12:40,880 --> 00:12:42,880
and communication tools to be more dynamic.

248
00:12:42,880 --> 00:12:44,640
It's called the representative democracy.

249
00:12:44,640 --> 00:12:45,840
It's not a true democracy.

250
00:12:45,840 --> 00:12:47,440
It's representative democracy, exactly.

251
00:12:47,440 --> 00:12:50,160
At the same time, everyone speaking at the same time

252
00:12:50,160 --> 00:12:51,360
doesn't make sense.

253
00:12:51,360 --> 00:12:53,920
But what we need, like the most successful organizations

254
00:12:53,920 --> 00:12:55,360
are complex hierarchical systems.

255
00:12:56,080 --> 00:12:59,440
It's groups of people working together loosely bound for a bigger story.

256
00:12:59,440 --> 00:13:00,880
And this is what we can achieve as humanity,

257
00:13:00,880 --> 00:13:03,440
like the human colossus when we all come together to do one big thing.

258
00:13:04,000 --> 00:13:05,680
But it gets blocked by a lot of the stuff

259
00:13:05,680 --> 00:13:07,760
because we're not communicating properly.

260
00:13:07,760 --> 00:13:09,840
Like one of the things I tell my team is that, you know,

261
00:13:09,840 --> 00:13:12,160
roadmaps are not about resources, they're about communication.

262
00:13:12,800 --> 00:13:15,040
If you communicate properly and something's good,

263
00:13:15,040 --> 00:13:16,640
you will always get resources for anything.

264
00:13:16,720 --> 00:13:19,200
It's also about creating a common vision

265
00:13:19,200 --> 00:13:21,520
that Erby's aiming towards in order to get there.

266
00:13:22,160 --> 00:13:24,720
So you're not diversified in a thousand directions.

267
00:13:24,720 --> 00:13:28,480
So like Google is full of amazingly smart people, right?

268
00:13:28,480 --> 00:13:30,640
They did an amazing study called Product Aristotle.

269
00:13:30,640 --> 00:13:30,880
Yes.

270
00:13:30,880 --> 00:13:33,360
Because it's like, why is one smart team better than another?

271
00:13:33,360 --> 00:13:34,400
What was the finding?

272
00:13:34,400 --> 00:13:36,160
The finding was it came up with two things.

273
00:13:36,160 --> 00:13:39,840
Common narrative and ideally something that kind of engages you

274
00:13:39,840 --> 00:13:42,720
and there's a bit of sacrifice, you know, like the...

275
00:13:42,720 --> 00:13:46,000
So this is what Salim and I and Salim is male who you know

276
00:13:46,000 --> 00:13:47,440
well who sends his regards.

277
00:13:47,440 --> 00:13:49,520
Talk about it as a massive transformative purpose,

278
00:13:49,520 --> 00:13:52,320
having a unifying, really compelling,

279
00:13:52,320 --> 00:13:55,040
emotionally charged vision that you're heading towards.

280
00:13:55,040 --> 00:13:57,440
Something deep and something that you've put yourself into as well.

281
00:13:57,440 --> 00:14:00,320
So it becomes part of your story as it were.

282
00:14:00,320 --> 00:14:02,240
And the second part was psychological safety.

283
00:14:03,200 --> 00:14:03,840
Which I think was very...

284
00:14:03,840 --> 00:14:04,800
Amongst the team.

285
00:14:04,800 --> 00:14:06,000
Amongst the team.

286
00:14:06,000 --> 00:14:08,080
The ability to actually express yourself

287
00:14:08,080 --> 00:14:10,400
and talk about something without fear of reproach.

288
00:14:10,400 --> 00:14:12,400
That's what makes the most successful teams.

289
00:14:12,400 --> 00:14:14,880
Because we're too scared often about kind of our status

290
00:14:15,120 --> 00:14:16,960
and worrying people and things like that.

291
00:14:16,960 --> 00:14:19,360
That's obviously on the research side of things primarily.

292
00:14:19,360 --> 00:14:21,360
But I think some of these lessons kind of come because

293
00:14:21,360 --> 00:14:22,960
when you feel comfortable as a community

294
00:14:22,960 --> 00:14:24,960
and you're working towards a massive transformative purpose,

295
00:14:25,520 --> 00:14:26,800
you can do a lot more.

296
00:14:26,800 --> 00:14:28,640
Because when you're falling behind, you can communicate it

297
00:14:28,640 --> 00:14:30,400
and you're not scared of people judging you.

298
00:14:30,400 --> 00:14:33,040
And if you have an idea that's considered, you know,

299
00:14:33,040 --> 00:14:38,400
divergent from the center, you feel open to being able to share it.

300
00:14:38,400 --> 00:14:40,320
In fact, it may well be the right idea.

301
00:14:40,320 --> 00:14:42,720
And I talk about the day before something is truly a breakthrough.

302
00:14:42,720 --> 00:14:43,680
It's a crazy idea.

303
00:14:43,760 --> 00:14:46,400
If you're scared about actually putting forward a crazy idea,

304
00:14:46,400 --> 00:14:47,440
then you're stuck.

305
00:14:47,440 --> 00:14:49,120
Well, again, it comes back to information theory,

306
00:14:49,120 --> 00:14:49,920
Shannon style, right?

307
00:14:49,920 --> 00:14:50,640
Yep.

308
00:14:50,640 --> 00:14:53,520
Information is only valuable in as much as it changes the state.

309
00:14:53,520 --> 00:14:54,080
Yes.

310
00:14:54,080 --> 00:14:55,920
Everyone's on the same page with everything.

311
00:14:55,920 --> 00:14:57,440
Flat equals dead.

312
00:14:57,440 --> 00:14:58,000
Exactly.

313
00:14:58,000 --> 00:15:00,160
And this is a time where we basically have to have

314
00:15:00,160 --> 00:15:01,600
exponential progress.

315
00:15:01,600 --> 00:15:03,200
And now we actually have the tools to help us

316
00:15:03,200 --> 00:15:04,960
to make exponential progress.

317
00:15:04,960 --> 00:15:08,320
Like, you know, Facebook released Galactica recently,

318
00:15:08,800 --> 00:15:12,640
which is their language model trained on 42 million science papers.

319
00:15:12,640 --> 00:15:16,000
They made some claims that were a bit hypey and then people

320
00:15:16,000 --> 00:15:18,800
like, oh, you can use this to create racist science papers.

321
00:15:18,800 --> 00:15:20,560
And so they were forced to take it down.

322
00:15:20,560 --> 00:15:21,920
But I mean, it's an amazing piece of tech

323
00:15:21,920 --> 00:15:23,120
and we're going to help re-release it.

324
00:15:24,240 --> 00:15:25,840
And this is an interesting thing.

325
00:15:25,840 --> 00:15:28,640
You can use it to do things like a null hypothesis creator.

326
00:15:28,640 --> 00:15:31,040
You can use this AI to do all the sorts of things

327
00:15:31,040 --> 00:15:34,240
that enable more divergent original thoughts

328
00:15:34,240 --> 00:15:36,480
or creativity or any of these other things.

329
00:15:36,480 --> 00:15:40,160
Like, classically, I couldn't because it was out of mode.

330
00:15:40,880 --> 00:15:42,240
It was out of the data set.

331
00:15:42,720 --> 00:15:46,640
Of course, if it's not in the data set, then you're screwed.

332
00:15:46,640 --> 00:15:48,400
Whereas now you don't need it to be in the data set.

333
00:15:48,400 --> 00:15:49,600
You can create new data.

334
00:15:49,600 --> 00:15:51,120
Yeah, that's extraordinary.

335
00:15:51,120 --> 00:15:52,880
This episode is brought to you by Levels.

336
00:15:52,880 --> 00:15:56,080
One of the most important things that I do to try and maintain

337
00:15:56,080 --> 00:16:00,720
my peak vitality and longevity is to monitor my blood glucose.

338
00:16:00,720 --> 00:16:03,040
More importantly, the foods that I eat

339
00:16:03,040 --> 00:16:05,840
and how they peak the glucose levels in my blood.

340
00:16:05,840 --> 00:16:08,240
Now, glucose is the fuel that powers your brain.

341
00:16:08,240 --> 00:16:09,520
It's really important.

342
00:16:09,520 --> 00:16:11,520
High, prolonged levels of glucose,

343
00:16:11,520 --> 00:16:14,880
which is called hyperglycemia, leads to everything

344
00:16:14,880 --> 00:16:20,000
from heart disease to Alzheimer's to sexual dysfunction to diabetes.

345
00:16:20,000 --> 00:16:21,280
And it's not good.

346
00:16:21,280 --> 00:16:23,120
The challenges, all of us are different.

347
00:16:23,120 --> 00:16:25,680
All of us respond to different foods in different ways.

348
00:16:25,680 --> 00:16:29,760
Like, for me, if I eat bananas, it spikes my blood glucose.

349
00:16:29,760 --> 00:16:31,680
If I eat grapes, it doesn't.

350
00:16:31,680 --> 00:16:35,840
If I eat bread by itself, I get this prolonged spike

351
00:16:35,840 --> 00:16:37,120
in my blood glucose levels.

352
00:16:37,120 --> 00:16:40,640
But if I dip that bread in olive oil, it blunts it.

353
00:16:40,640 --> 00:16:43,200
And these are things that I've learned from wearing

354
00:16:43,200 --> 00:16:46,400
a continuous glucose monitor and using the Levels app.

355
00:16:46,400 --> 00:16:51,200
So Levels is a company that helps you in analyzing

356
00:16:51,200 --> 00:16:53,120
what's going on in your body.

357
00:16:53,120 --> 00:16:55,520
It's continuous monitoring 24-7.

358
00:16:55,520 --> 00:16:56,880
I wear it all the time.

359
00:16:56,880 --> 00:16:59,840
It really helps me to stay on top of the food I eat,

360
00:16:59,840 --> 00:17:02,080
remain conscious of the food that I eat,

361
00:17:02,080 --> 00:17:04,880
and to understand which foods affect me

362
00:17:04,880 --> 00:17:07,840
based upon my physiology and my genetics.

363
00:17:07,840 --> 00:17:10,960
You know, on this podcast, I only recommend products

364
00:17:10,960 --> 00:17:14,080
and services that I use, that I use not only for myself,

365
00:17:14,080 --> 00:17:15,760
but my friends and my family,

366
00:17:15,760 --> 00:17:18,320
that I think are high quality and safe

367
00:17:18,320 --> 00:17:20,720
and really impact a person's life.

368
00:17:20,720 --> 00:17:21,920
So, check it out.

369
00:17:21,920 --> 00:17:24,800
Levels.link slash Peter.

370
00:17:24,800 --> 00:17:27,520
We give you two additional months of membership.

371
00:17:27,520 --> 00:17:30,640
And it's something that I think everyone should be doing.

372
00:17:30,640 --> 00:17:32,880
Eventually, this stuff is going to be in your body,

373
00:17:32,880 --> 00:17:36,400
on your body, part of our future of medicine today.

374
00:17:36,400 --> 00:17:39,120
It's a product that I think I'm going to be using

375
00:17:39,120 --> 00:17:41,600
for the years ahead and hope you'll consider as well.

376
00:17:41,600 --> 00:17:44,640
You know, when you go to your website,

377
00:17:44,640 --> 00:17:49,600
I love the notion of AI by the people, for the people.

378
00:17:50,160 --> 00:17:53,360
And then you say, stability AI is building open AI tools

379
00:17:53,360 --> 00:17:55,680
that will let us reach our potential.

380
00:17:56,400 --> 00:17:57,200
Let's talk about that.

381
00:17:57,200 --> 00:18:00,160
Because, you know, there's a lot of individuals

382
00:18:00,160 --> 00:18:04,240
that you and I both know that our fearmonger is around AI.

383
00:18:04,240 --> 00:18:05,280
You know, it's the devil.

384
00:18:05,280 --> 00:18:06,400
It's going to destroy us.

385
00:18:06,400 --> 00:18:07,280
It's going to...

386
00:18:07,280 --> 00:18:10,400
Superhuman AI is the end of humanity as we know it.

387
00:18:10,400 --> 00:18:14,240
And I mean, my position is AI is the single most important tool

388
00:18:14,240 --> 00:18:16,720
we're ever creating to solve the world's biggest problems.

389
00:18:16,720 --> 00:18:19,200
And we can't solve our problems from where we were before,

390
00:18:19,200 --> 00:18:21,120
but these are the tools that are going to allow us.

391
00:18:21,120 --> 00:18:22,080
How do you...

392
00:18:22,080 --> 00:18:24,960
So, how do you address the Bill Gates,

393
00:18:24,960 --> 00:18:28,400
the Elon to the world on that side, the fear side?

394
00:18:28,400 --> 00:18:29,440
Well, I think it fares a valid,

395
00:18:29,440 --> 00:18:31,120
because it's the most powerful technology

396
00:18:31,120 --> 00:18:32,960
we've ever created and it comes from us.

397
00:18:33,520 --> 00:18:34,960
But then who is us, right?

398
00:18:34,960 --> 00:18:36,320
Like, if you look our current data sets,

399
00:18:36,320 --> 00:18:37,760
they're massively biased.

400
00:18:37,760 --> 00:18:38,960
They're fixed towards the internet

401
00:18:38,960 --> 00:18:40,240
and they're fixed towards manipulation.

402
00:18:40,880 --> 00:18:43,760
The way I kind of look at AI is that organizations themselves are AI.

403
00:18:44,320 --> 00:18:47,680
They are slow dumb AI that feeds on us and turns us into cogs.

404
00:18:47,680 --> 00:18:49,520
In fact, this is concept of Molok, right?

405
00:18:49,520 --> 00:18:52,320
From the Ginsburg poem, you know, how?

406
00:18:52,320 --> 00:18:54,480
Talking about this Carthaginian demon

407
00:18:54,480 --> 00:18:56,720
that pervades our organizational structures

408
00:18:56,720 --> 00:18:59,760
and turns us into these cogs that feeds on us effectively.

409
00:18:59,760 --> 00:19:02,000
I think this is the first thing that can actually defeat that.

410
00:19:02,640 --> 00:19:04,480
This particular technology that we have today.

411
00:19:05,200 --> 00:19:06,480
Because is the world happy now?

412
00:19:07,200 --> 00:19:09,040
How many people in organizations are happy?

413
00:19:09,040 --> 00:19:10,160
We all know, you know?

414
00:19:10,720 --> 00:19:13,040
We should talk about my idea for a happiness X-Prize,

415
00:19:13,040 --> 00:19:14,080
but that's a different conversation.

416
00:19:14,080 --> 00:19:15,520
Happiness X-Prize, but what is happiness?

417
00:19:15,520 --> 00:19:16,400
Happiness is agency.

418
00:19:16,400 --> 00:19:18,000
Happiness is achieving your potential.

419
00:19:18,000 --> 00:19:20,000
And it's kind of going out there, so we need some help.

420
00:19:20,000 --> 00:19:21,520
But I think a lot of the AI discourse

421
00:19:21,520 --> 00:19:23,920
has been focused on gigantic language models

422
00:19:23,920 --> 00:19:26,080
and fricking supercomputers, which we still have,

423
00:19:26,720 --> 00:19:30,560
to create something that will equal and surpass us.

424
00:19:30,560 --> 00:19:32,320
It's very religious in its way.

425
00:19:32,320 --> 00:19:34,240
And you see parallels to religion across this.

426
00:19:34,240 --> 00:19:34,640
Sure.

427
00:19:34,640 --> 00:19:37,360
In that you have the kind of declaiming people who like for it,

428
00:19:37,360 --> 00:19:39,200
the people who call it heretical.

429
00:19:39,200 --> 00:19:42,080
And then you kind of even have most of ethics in AI.

430
00:19:42,080 --> 00:19:43,360
It's not actually ethics.

431
00:19:44,000 --> 00:19:47,120
Instead, it's ultra-orthodoxy, as it were.

432
00:19:47,680 --> 00:19:48,800
Like in Islamic terms,

433
00:19:48,800 --> 00:19:50,640
everything is haram unless it's declared halal.

434
00:19:51,760 --> 00:19:53,360
You know, classic Jewish,

435
00:19:53,360 --> 00:19:54,720
ultra-orthodox Judaism, etc.

436
00:19:54,720 --> 00:19:55,760
It's all the same thing.

437
00:19:55,760 --> 00:19:57,120
People look at red teaming.

438
00:19:57,120 --> 00:19:59,440
They don't look at green teaming, right?

439
00:19:59,440 --> 00:20:02,160
And they look at this technology as being too powerful.

440
00:20:02,160 --> 00:20:04,400
So just like cryptography, we should keep it from the people.

441
00:20:04,400 --> 00:20:06,640
And we definitely shouldn't give it to emerging markets

442
00:20:06,640 --> 00:20:08,160
and people who aren't smart enough to do it.

443
00:20:09,040 --> 00:20:11,360
And that's very interesting because, again,

444
00:20:11,360 --> 00:20:12,800
if you think about the power of it

445
00:20:12,800 --> 00:20:13,920
and you believe it's powerful,

446
00:20:14,640 --> 00:20:16,640
then the question should be, what is this?

447
00:20:16,640 --> 00:20:18,720
And I believe that this AI is infrastructure.

448
00:20:19,680 --> 00:20:23,600
Clayton Christensen, kind of the departed mental mind,

449
00:20:23,600 --> 00:20:25,840
had an amazing quote, which is that,

450
00:20:25,840 --> 00:20:27,600
infrastructure is the most efficient means

451
00:20:27,600 --> 00:20:29,840
by which society stores and distributes value.

452
00:20:30,880 --> 00:20:32,640
Now, obviously, that's ports and things like that.

453
00:20:32,640 --> 00:20:33,040
Sure.

454
00:20:33,040 --> 00:20:34,400
But it's also information.

455
00:20:34,400 --> 00:20:35,040
Sure.

456
00:20:35,040 --> 00:20:36,240
And I think that's valuable when you

457
00:20:36,240 --> 00:20:38,000
confine it with the Shannon theory.

458
00:20:38,000 --> 00:20:40,000
So this AI is infrastructure

459
00:20:40,000 --> 00:20:41,600
for the next generation of human thought.

460
00:20:42,160 --> 00:20:43,040
And what does that mean?

461
00:20:43,040 --> 00:20:44,400
It should mean that it should be a commons

462
00:20:45,120 --> 00:20:46,720
that is accessible to everyone.

463
00:20:47,280 --> 00:20:52,320
And you made a very definitive decision

464
00:20:52,320 --> 00:20:55,520
to make this an open-source movement here.

465
00:20:55,520 --> 00:20:55,760
Yeah.

466
00:20:55,760 --> 00:20:56,880
Yeah.

467
00:20:56,880 --> 00:20:58,640
Your community's how large now?

468
00:20:58,640 --> 00:21:01,280
I think we've got about 120,000.

469
00:21:01,280 --> 00:21:01,840
Amazing.

470
00:21:01,840 --> 00:21:02,640
In the communities.

471
00:21:02,640 --> 00:21:04,720
And we created communities across verticals.

472
00:21:04,720 --> 00:21:07,120
Classical open-source product related,

473
00:21:07,120 --> 00:21:09,200
where you could have MongoDB or something like that,

474
00:21:09,200 --> 00:21:10,080
and then you've got there.

475
00:21:10,080 --> 00:21:14,720
Whereas we said language, healthcare, BioML, audio.

476
00:21:14,720 --> 00:21:16,400
Let's get all the people who are fantastic

477
00:21:16,400 --> 00:21:17,600
in the private sector, public sector,

478
00:21:17,600 --> 00:21:19,200
actively independent together.

479
00:21:19,200 --> 00:21:20,560
And let's jam on,

480
00:21:20,560 --> 00:21:22,640
how do we build a next generation infrastructure?

481
00:21:22,640 --> 00:21:24,400
The way I put it is, let's go to the future

482
00:21:24,400 --> 00:21:25,520
and bring back AI with us.

483
00:21:26,160 --> 00:21:26,400
Nice.

484
00:21:26,400 --> 00:21:27,920
And we can choose if it's a panopticon

485
00:21:27,920 --> 00:21:30,160
controlled by corporations, like Web2,

486
00:21:30,160 --> 00:21:31,520
or we can choose if it's open,

487
00:21:31,520 --> 00:21:33,120
an infrastructure for humanity.

488
00:21:33,120 --> 00:21:34,160
And if it's infrastructure for humanity,

489
00:21:34,160 --> 00:21:35,440
I think it's an important point,

490
00:21:35,440 --> 00:21:36,400
because I'm just finally saying that.

491
00:21:37,040 --> 00:21:39,520
A lot of the naysayers around AGI and ASI and others,

492
00:21:40,800 --> 00:21:43,280
I would agree with them if it's controlled

493
00:21:43,280 --> 00:21:44,320
by corporations,

494
00:21:44,320 --> 00:21:46,000
which are this type of weird entity

495
00:21:46,000 --> 00:21:46,800
that we've created.

496
00:21:47,440 --> 00:21:49,840
Like YouTube optimized for extremism,

497
00:21:49,840 --> 00:21:50,880
because it was engaging,

498
00:21:50,880 --> 00:21:52,880
so ISIS co-opted those algorithms.

499
00:21:52,880 --> 00:21:54,400
And they adjusted eventually, but it was slow.

500
00:21:55,040 --> 00:21:56,640
If it comes from us and it's biased,

501
00:21:56,640 --> 00:21:57,760
and we've brought in the conversation,

502
00:21:57,760 --> 00:21:59,280
I think AI is more finalist,

503
00:21:59,280 --> 00:22:00,400
likely to kill us all,

504
00:22:00,400 --> 00:22:01,760
if it ever becomes sentient,

505
00:22:01,760 --> 00:22:03,200
which is a big question, you know?

506
00:22:04,320 --> 00:22:05,920
Especially if it's representative.

507
00:22:05,920 --> 00:22:09,680
So I want to come back to this in greater detail later,

508
00:22:09,680 --> 00:22:12,560
but I think we share a common belief

509
00:22:12,560 --> 00:22:14,880
that humanity ultimately is good.

510
00:22:15,440 --> 00:22:16,240
Yes.

511
00:22:16,240 --> 00:22:17,440
At its fundamental level,

512
00:22:17,440 --> 00:22:19,760
because that's a very important distinction.

513
00:22:19,760 --> 00:22:22,000
If you believe that humans,

514
00:22:22,000 --> 00:22:23,440
at their base, are good,

515
00:22:23,440 --> 00:22:25,120
and you're enabling humans

516
00:22:25,120 --> 00:22:26,960
with more and more powerful technology,

517
00:22:26,960 --> 00:22:28,080
they're going to be using that

518
00:22:28,080 --> 00:22:29,360
to make the world a better place

519
00:22:29,360 --> 00:22:32,080
and solve problems on the whole.

520
00:22:32,080 --> 00:22:33,680
On the whole, but it depends on

521
00:22:33,680 --> 00:22:35,600
what the perspective of humanity is.

522
00:22:35,600 --> 00:22:37,280
You know, you've kind of got,

523
00:22:37,280 --> 00:22:38,640
what's it called, the thing when you go to space

524
00:22:38,640 --> 00:22:39,440
and you look at Earth?

525
00:22:39,440 --> 00:22:40,400
Yeah, the overview effect.

526
00:22:40,400 --> 00:22:41,280
The overview effect.

527
00:22:41,280 --> 00:22:41,680
Yeah.

528
00:22:41,680 --> 00:22:43,120
You know, like hopefully more and more people

529
00:22:43,120 --> 00:22:44,400
get to space and have that,

530
00:22:44,400 --> 00:22:45,920
because people are very narrow,

531
00:22:45,920 --> 00:22:47,840
and they view themselves like that.

532
00:22:48,480 --> 00:22:50,640
They're, Rabbi Sacks,

533
00:22:50,720 --> 00:22:51,760
former Chief Rabbi of the UK,

534
00:22:51,760 --> 00:22:52,960
had a very wonderful concept

535
00:22:52,960 --> 00:22:54,000
called altruistic evil.

536
00:22:55,120 --> 00:22:56,160
Those who actually do evil

537
00:22:56,160 --> 00:22:56,960
believe they're doing good.

538
00:22:57,680 --> 00:22:58,160
Interesting.

539
00:22:58,800 --> 00:22:59,600
And you see that.

540
00:22:59,600 --> 00:23:01,120
Like, you know, if you talk to them.

541
00:23:01,120 --> 00:23:02,800
Especially in the religious realm.

542
00:23:02,800 --> 00:23:03,680
Religious or anything like that.

543
00:23:03,680 --> 00:23:05,920
I'm like, Isaiah Berlin's conceptualization,

544
00:23:07,040 --> 00:23:09,920
British philosopher from like the 1940s,

545
00:23:09,920 --> 00:23:12,320
he had a conceptualization of positive liberty

546
00:23:12,320 --> 00:23:13,600
versus negative liberty.

547
00:23:13,600 --> 00:23:15,040
So negative liberty was the freedom

548
00:23:15,040 --> 00:23:16,480
from anyone telling you what to do,

549
00:23:16,480 --> 00:23:17,440
and then to kind of,

550
00:23:17,440 --> 00:23:19,440
laissez-faire capitalism and things like that.

551
00:23:19,440 --> 00:23:22,000
Positive liberty was the ability to believe in an ism.

552
00:23:22,000 --> 00:23:24,240
Something big, like fascism,

553
00:23:24,240 --> 00:23:27,600
communism, capitalism, Islamism, etc.

554
00:23:27,600 --> 00:23:29,120
And people use that as excuses,

555
00:23:29,120 --> 00:23:30,800
being they were doing good to actually do bad.

556
00:23:31,360 --> 00:23:32,800
But then how does this all relate to this?

557
00:23:32,800 --> 00:23:34,560
I believe people inherently want to do good.

558
00:23:34,560 --> 00:23:36,080
It's just that what good is,

559
00:23:36,720 --> 00:23:38,480
can become misdefined and corrupted.

560
00:23:39,200 --> 00:23:40,320
And so my take was,

561
00:23:40,320 --> 00:23:41,760
if we start building infrastructure,

562
00:23:41,760 --> 00:23:43,520
where people can see bigger perspectives,

563
00:23:43,520 --> 00:23:45,200
because they get the information they need,

564
00:23:45,200 --> 00:23:45,920
what does that look like?

565
00:23:45,920 --> 00:23:48,480
What if we mapped all the religious texts in the world,

566
00:23:48,480 --> 00:23:51,200
so that a child in Egypt could see Judaism

567
00:23:51,200 --> 00:23:52,960
from the perspective of a child in Jerusalem?

568
00:23:54,080 --> 00:23:55,120
We have the technology to do that.

569
00:23:55,120 --> 00:23:56,640
What if you could automatically translate

570
00:23:57,680 --> 00:24:01,200
tea-party republicanism into libertarianism,

571
00:24:01,200 --> 00:24:02,160
and have commonality there?

572
00:24:02,160 --> 00:24:03,920
Again, we finally have the technology to do that.

573
00:24:04,480 --> 00:24:05,520
But we didn't before,

574
00:24:05,520 --> 00:24:07,520
so people remain in their huddles,

575
00:24:07,520 --> 00:24:08,640
they look at the other,

576
00:24:08,640 --> 00:24:09,680
and they're encouraged to do so.

577
00:24:09,680 --> 00:24:12,000
We've seen that increase in political polarization.

578
00:24:12,000 --> 00:24:13,200
So society is a tipping point.

579
00:24:13,200 --> 00:24:14,320
And by the way, we've seen,

580
00:24:14,320 --> 00:24:16,160
I mean, that's been what social media

581
00:24:16,160 --> 00:24:18,240
has effectively done so efficiently.

582
00:24:18,480 --> 00:24:21,040
Because it's beneficial for the algorithms,

583
00:24:21,040 --> 00:24:24,160
and for the slow, dumb AIs of the corporations that drive it.

584
00:24:25,120 --> 00:24:28,240
Let's come back to the company and the products,

585
00:24:28,240 --> 00:24:30,880
just to lay out the tapestry here.

586
00:24:31,920 --> 00:24:35,920
Stable diffusion is one product vertical area.

587
00:24:35,920 --> 00:24:37,040
What are the other ones?

588
00:24:37,040 --> 00:24:41,840
So for example, our Luther AI community has GPT Neo and X.

589
00:24:41,840 --> 00:24:44,560
It's the open source version of GPT-3 by OpenAI.

590
00:24:45,280 --> 00:24:47,520
The most popular open language model in the world,

591
00:24:47,600 --> 00:24:49,760
it's been downloaded 25 million times.

592
00:24:49,760 --> 00:24:50,640
Incredible.

593
00:24:50,640 --> 00:24:51,680
And so can you take it?

594
00:24:51,680 --> 00:24:53,120
You customize it for yourself?

595
00:24:53,120 --> 00:24:54,240
No permission needed, right?

596
00:24:55,040 --> 00:24:56,720
Harmony has danced diffusion,

597
00:24:56,720 --> 00:24:58,960
which is the most advanced audio model in the world.

598
00:24:59,600 --> 00:25:00,800
You'll be able to create your music,

599
00:25:00,800 --> 00:25:01,840
so you put your own music in it,

600
00:25:01,840 --> 00:25:02,960
and then you have your own music model

601
00:25:02,960 --> 00:25:04,560
that can create more music of your style.

602
00:25:04,560 --> 00:25:08,080
And you can just basically produce it,

603
00:25:08,800 --> 00:25:10,000
produce your own concerts,

604
00:25:10,560 --> 00:25:12,400
extrapolate in any direction you want.

605
00:25:12,400 --> 00:25:13,280
Exactly.

606
00:25:13,280 --> 00:25:14,480
We have OpenBioML,

607
00:25:14,480 --> 00:25:16,640
where we kind of have OpenFold and LibreFold,

608
00:25:16,640 --> 00:25:20,640
protein folding, DNA diffusion for DNA protein matching,

609
00:25:20,640 --> 00:25:25,680
and OpenBioML for, again, some protein kind of stuff.

610
00:25:25,680 --> 00:25:29,520
So we've spent some time talking about

611
00:25:30,400 --> 00:25:32,960
a passion that I have and you share,

612
00:25:32,960 --> 00:25:36,560
which is the ability to truly transform

613
00:25:36,560 --> 00:25:39,280
the healthcare industry and human longevity,

614
00:25:39,280 --> 00:25:43,520
understanding why we age, maybe why we don't have to.

615
00:25:43,520 --> 00:25:46,080
I think the why is the important part, right?

616
00:25:46,080 --> 00:25:47,200
Most of medicine.

617
00:25:47,200 --> 00:25:49,360
So again, when my son was diagnosed

618
00:25:49,360 --> 00:25:51,360
and scratching a wall to his fingernails bled,

619
00:25:52,320 --> 00:25:53,680
eventually went to mainstream school,

620
00:25:53,680 --> 00:25:55,040
thanks to the interventions.

621
00:25:55,040 --> 00:25:56,480
And I want to come back to those,

622
00:25:56,480 --> 00:25:57,440
for those who are listening,

623
00:25:57,440 --> 00:26:01,840
who have a child with autism or know someone,

624
00:26:01,840 --> 00:26:03,520
I want to come back afterwards.

625
00:26:03,520 --> 00:26:06,320
What were your learnings and what's your advice?

626
00:26:06,320 --> 00:26:07,680
Yeah, there's a lot in this space

627
00:26:07,680 --> 00:26:09,200
that we kind of occupy, of course.

628
00:26:09,920 --> 00:26:13,440
But what I realized is that conventional medicine

629
00:26:14,400 --> 00:26:17,040
and a lot of things view humans as a girdic.

630
00:26:17,040 --> 00:26:18,720
A thousand tosses of a coin is the same

631
00:26:18,720 --> 00:26:20,640
as a thousand toins cost at once.

632
00:26:22,320 --> 00:26:23,360
Coins tossed at once.

633
00:26:24,400 --> 00:26:25,600
But humans are individual.

634
00:26:25,600 --> 00:26:27,520
So for example, a good proportion of humans

635
00:26:27,520 --> 00:26:30,160
have a cytochrome P450 mutation in their liver,

636
00:26:30,160 --> 00:26:33,440
which means they metabolize things like fentanyl

637
00:26:33,440 --> 00:26:34,880
and opioids far quicker.

638
00:26:34,880 --> 00:26:35,360
Yeah.

639
00:26:35,360 --> 00:26:37,760
It's a very simple SMP test, but we don't do it.

640
00:26:37,760 --> 00:26:39,360
So we just prescribe everyone the same thing

641
00:26:39,360 --> 00:26:40,480
and then a bunch get addicted.

642
00:26:41,440 --> 00:26:42,080
You know?

643
00:26:42,080 --> 00:26:44,640
Because we are individualistic kind of creatures.

644
00:26:44,640 --> 00:26:46,160
This is why I went to the first principles

645
00:26:46,160 --> 00:26:47,760
thinking kind of approach on this.

646
00:26:47,760 --> 00:26:49,600
And the question is this personalized medicine thing

647
00:26:49,600 --> 00:26:51,440
has always been kind of out there.

648
00:26:51,440 --> 00:26:52,800
We've not been able to reach it.

649
00:26:52,800 --> 00:26:54,720
Again, I think the technology we have right now

650
00:26:54,720 --> 00:26:56,320
enables personalized medicine.

651
00:26:56,320 --> 00:26:58,880
We've seen things like CRISPR, obviously, and others.

652
00:26:59,680 --> 00:27:02,320
But more than that, it's about data availability

653
00:27:02,320 --> 00:27:03,200
and viability.

654
00:27:03,200 --> 00:27:04,320
But we do need to get to a point

655
00:27:04,320 --> 00:27:06,560
where every child at birth is sequenced

656
00:27:06,560 --> 00:27:09,440
and that is plugged into their personal AI

657
00:27:09,440 --> 00:27:11,760
and they understand exactly how every food, every medicine,

658
00:27:12,400 --> 00:27:16,240
and every aspect of our living affects our physiology.

659
00:27:16,240 --> 00:27:19,600
Again, you go to the future and you bring back the AI with you

660
00:27:19,600 --> 00:27:21,680
and you say, what should it look like?

661
00:27:21,680 --> 00:27:23,520
So you take a whole country and you say,

662
00:27:23,520 --> 00:27:25,440
how do we build an amazing health care system,

663
00:27:25,440 --> 00:27:26,720
education system, et cetera?

664
00:27:27,600 --> 00:27:30,880
Is there any doubt that health care and education

665
00:27:30,880 --> 00:27:33,440
will have AI at the core in 20 years?

666
00:27:33,440 --> 00:27:33,840
Zero.

667
00:27:33,840 --> 00:27:36,320
Well, I think it's not 20 years, it's 10 years.

668
00:27:36,320 --> 00:27:36,640
I know.

669
00:27:36,640 --> 00:27:37,680
But let's just say 20 years.

670
00:27:37,680 --> 00:27:38,720
Fine.

671
00:27:38,800 --> 00:27:40,800
You and me, we're like, now, now, now.

672
00:27:40,800 --> 00:27:41,920
Let's say 20 years, right?

673
00:27:42,960 --> 00:27:46,080
But then is that AI open or closed?

674
00:27:47,120 --> 00:27:47,520
Yeah.

675
00:27:47,520 --> 00:27:49,600
And is it better for it to be open or closed?

676
00:27:49,600 --> 00:27:50,400
There's no question.

677
00:27:50,400 --> 00:27:53,120
Open is fundamentally critical.

678
00:27:53,120 --> 00:27:54,320
Open is fundamentally critical

679
00:27:54,320 --> 00:27:56,320
because then we build, it is infrastructure,

680
00:27:56,320 --> 00:27:57,520
it is valuable.

681
00:27:57,520 --> 00:27:59,520
Like the way that I actually orient my rights,

682
00:28:00,240 --> 00:28:02,560
Vinay Gupta, I was one of the Ethereum guys,

683
00:28:02,560 --> 00:28:04,000
I think you probably know him.

684
00:28:04,000 --> 00:28:05,600
Big thinker, like crazy.

685
00:28:06,320 --> 00:28:08,320
He had a very great conceptualization of rights

686
00:28:08,320 --> 00:28:10,720
which I agree with, which is the rights of children.

687
00:28:11,600 --> 00:28:14,480
And so like effective altruism and all of that,

688
00:28:14,480 --> 00:28:16,000
looking at people a million years in advance,

689
00:28:16,000 --> 00:28:17,200
it's kind of difficult.

690
00:28:17,200 --> 00:28:18,640
And it comes down to utilitarianism,

691
00:28:18,640 --> 00:28:19,520
all sorts of weird stuff.

692
00:28:20,080 --> 00:28:21,440
But the rights of a child.

693
00:28:21,440 --> 00:28:22,560
Rights of a child today.

694
00:28:22,560 --> 00:28:22,880
Yes.

695
00:28:23,440 --> 00:28:24,000
Today.

696
00:28:24,000 --> 00:28:25,440
What does that child have the right to?

697
00:28:25,440 --> 00:28:26,640
Achieving their potential.

698
00:28:26,640 --> 00:28:27,120
Yes.

699
00:28:27,120 --> 00:28:29,040
What infrastructure do we need to give that child

700
00:28:29,040 --> 00:28:33,120
in a refugee camp or in Brooklyn or in Kensington

701
00:28:33,120 --> 00:28:34,640
to help them achieve their potential?

702
00:28:34,640 --> 00:28:36,400
And this goes back to what's on your website

703
00:28:36,400 --> 00:28:41,280
in terms of using AI to help, in this case, a child,

704
00:28:41,280 --> 00:28:44,720
and in the broader case, humanity, achieve its potential.

705
00:28:44,720 --> 00:28:45,200
Achieve it.

706
00:28:45,200 --> 00:28:46,880
I mean, that's everything.

707
00:28:46,880 --> 00:28:47,280
Yeah.

708
00:28:47,280 --> 00:28:48,480
And so for me, it's about...

709
00:28:49,440 --> 00:28:51,600
I've oriented that on agency and happiness.

710
00:28:52,240 --> 00:28:54,640
And it sounds fuzzy, but it's literally,

711
00:28:54,640 --> 00:28:57,040
when you have the tools to be able to do anything,

712
00:28:57,040 --> 00:28:58,400
you know you can do anything.

713
00:28:58,400 --> 00:29:00,000
People underestimate their agency.

714
00:29:00,640 --> 00:29:02,720
Sometimes you've got to go for your shot, as it were.

715
00:29:02,720 --> 00:29:03,360
Yeah.

716
00:29:03,360 --> 00:29:04,400
But again, it's information.

717
00:29:04,400 --> 00:29:06,000
What information can I bring to that person?

718
00:29:06,000 --> 00:29:09,120
What tools can I give them so that they can be creative?

719
00:29:09,120 --> 00:29:11,600
Because we lose that creative spark as we get older, right?

720
00:29:11,600 --> 00:29:11,920
Yeah.

721
00:29:11,920 --> 00:29:13,360
Now it's coming back of it, you know?

722
00:29:13,360 --> 00:29:15,520
What can I give them so they can access the information

723
00:29:15,520 --> 00:29:17,040
they need to be educated?

724
00:29:17,040 --> 00:29:19,440
And what's the optimal way to teach linear algebra?

725
00:29:19,440 --> 00:29:21,120
And by the way, it's all about play.

726
00:29:21,120 --> 00:29:22,160
It's all about play.

727
00:29:22,160 --> 00:29:23,600
It is all about play.

728
00:29:23,600 --> 00:29:27,040
Happiness play, all the neurochemistry of your brain

729
00:29:27,040 --> 00:29:30,000
is maximized for learning, retention, experimentation

730
00:29:30,000 --> 00:29:30,720
around that.

731
00:29:30,720 --> 00:29:31,440
It's flow.

732
00:29:31,440 --> 00:29:32,160
It's flow.

733
00:29:32,160 --> 00:29:34,000
So I used to be a video game investor.

734
00:29:34,000 --> 00:29:34,960
That was my big sector.

735
00:29:34,960 --> 00:29:36,080
It was one of the biggest in the world.

736
00:29:36,080 --> 00:29:38,720
And I used to judge video games by time to fun, flow,

737
00:29:38,720 --> 00:29:39,360
and frustration.

738
00:29:40,960 --> 00:29:42,880
And so if we're building systems for humanity,

739
00:29:42,880 --> 00:29:45,120
we have to look at fun, flow, and frustration.

740
00:29:45,920 --> 00:29:48,480
Because if we can get those, you know when you're just

741
00:29:48,480 --> 00:29:50,320
learning something, like, wow, this is amazing,

742
00:29:50,320 --> 00:29:51,120
and how do you get in there?

743
00:29:51,840 --> 00:29:53,840
It absorbs amazingly quickly.

744
00:29:53,840 --> 00:29:55,680
But our education system is not set up for that.

745
00:29:55,680 --> 00:29:57,040
Our health care system definitely isn't.

746
00:29:57,040 --> 00:29:58,400
And you've got to mess up health care system.

747
00:29:58,400 --> 00:29:58,720
Yeah, I'd say.

748
00:29:58,720 --> 00:29:59,280
Oh, my gosh.

749
00:29:59,280 --> 00:30:01,280
It's maximizing for frustration.

750
00:30:01,280 --> 00:30:03,120
Yeah, that's what I speak about it.

751
00:30:03,200 --> 00:30:06,480
Openly saying my mission is to crumble and destroy

752
00:30:06,480 --> 00:30:07,600
the health care system.

753
00:30:07,600 --> 00:30:11,120
And also education system, which is really sad.

754
00:30:11,120 --> 00:30:12,320
So again, it's very interesting.

755
00:30:12,320 --> 00:30:14,240
You look at the US in inflation numbers.

756
00:30:14,880 --> 00:30:17,040
Education and health care, massive inflation.

757
00:30:17,040 --> 00:30:17,760
Yes.

758
00:30:17,760 --> 00:30:19,200
Everything else, not really.

759
00:30:19,200 --> 00:30:21,760
Yeah, and it's the percentage of the person's income.

760
00:30:23,280 --> 00:30:26,000
And it should be going to zero, right?

761
00:30:26,000 --> 00:30:28,080
The top health care and the top education

762
00:30:28,080 --> 00:30:29,360
should be all AI driven.

763
00:30:29,360 --> 00:30:31,760
It's all basically the cost of electrons.

764
00:30:31,760 --> 00:30:33,920
Life expectancy is falling in the US.

765
00:30:33,920 --> 00:30:36,160
A little bit over the last couple of years due to COVID.

766
00:30:36,160 --> 00:30:36,480
Yeah.

767
00:30:36,480 --> 00:30:37,680
Yeah, but actually, no.

768
00:30:37,680 --> 00:30:38,880
No, before COVID.

769
00:30:38,880 --> 00:30:40,480
Before COVID, it was falling.

770
00:30:40,480 --> 00:30:40,560
Yeah.

771
00:30:40,560 --> 00:30:41,120
I mean, that's the thing.

772
00:30:41,120 --> 00:30:44,400
Whereas, again, it's not complicated,

773
00:30:44,400 --> 00:30:46,320
but it does require coordination.

774
00:30:46,320 --> 00:30:48,480
So the question is, can you create shelling points?

775
00:30:48,480 --> 00:30:52,320
So for me, open source software, this next generation,

776
00:30:52,320 --> 00:30:54,240
this model based one,

777
00:30:54,240 --> 00:30:56,720
whereby stable diffusion is basically a programming primitive.

778
00:30:57,440 --> 00:30:59,600
Just like you have a library to do various things.

779
00:30:59,600 --> 00:31:01,120
So the program is out there.

780
00:31:01,120 --> 00:31:03,280
So in the good old days, when we started programming,

781
00:31:03,280 --> 00:31:04,960
we used to have to code everything by hand.

782
00:31:04,960 --> 00:31:06,240
Now, you've got GitHub and things like that.

783
00:31:06,240 --> 00:31:07,600
It's more like playing with Lego, right?

784
00:31:07,600 --> 00:31:08,000
Yes.

785
00:31:08,000 --> 00:31:09,360
And you're sticking it all together.

786
00:31:09,360 --> 00:31:12,480
It's a new type of thing, a 1.6 GB file that is hashed.

787
00:31:13,040 --> 00:31:15,600
So it can be common across every single computer in the world

788
00:31:16,480 --> 00:31:19,440
that you can call something in and an image comes out.

789
00:31:20,080 --> 00:31:22,000
And you know predictably what that is,

790
00:31:22,000 --> 00:31:22,800
no matter where you are.

791
00:31:22,800 --> 00:31:25,520
And you can take an image and put text on the other way.

792
00:31:26,640 --> 00:31:26,960
Yeah.

793
00:31:26,960 --> 00:31:28,240
That changes the paradigm.

794
00:31:28,240 --> 00:31:30,320
But then what if you have that for language, audio,

795
00:31:30,320 --> 00:31:31,440
all these different modalities?

796
00:31:31,440 --> 00:31:31,760
Okay.

797
00:31:31,760 --> 00:31:34,160
I'm going to go here next then on that,

798
00:31:34,160 --> 00:31:38,000
which is what happens to electoral property rights, IP rights?

799
00:31:38,000 --> 00:31:41,840
Who owns the IP of those images?

800
00:31:41,840 --> 00:31:44,480
Is it the person who puts the prompt in?

801
00:31:45,520 --> 00:31:49,680
Is it, you know, help me understand where that evolves to?

802
00:31:49,680 --> 00:31:50,640
Nobody knows.

803
00:31:50,640 --> 00:31:51,200
Nobody knows.

804
00:31:51,200 --> 00:31:51,360
Okay.

805
00:31:51,360 --> 00:31:52,240
It's a fair answer.

806
00:31:52,240 --> 00:31:52,400
Yeah.

807
00:31:52,400 --> 00:31:53,040
I mean, nobody knows.

808
00:31:53,040 --> 00:31:56,560
I personally think it should belong to the person that prompts it.

809
00:31:56,560 --> 00:31:59,520
Because again, we put this out as a commons for humanity.

810
00:32:00,320 --> 00:32:03,280
Like we did put an ethical use license around it for various reasons

811
00:32:03,280 --> 00:32:05,840
that will be replaced by a purely open source license.

812
00:32:06,560 --> 00:32:09,360
But again, I'm viewing this as building blocks, right?

813
00:32:09,360 --> 00:32:11,280
It's really the person that has the action

814
00:32:11,280 --> 00:32:13,120
because these models do not have agency.

815
00:32:13,760 --> 00:32:15,360
If you're going to breach property right,

816
00:32:15,360 --> 00:32:17,760
you can type in, I want a picture of Mickey Mouse.

817
00:32:17,760 --> 00:32:19,520
And then if you sell that, done.

818
00:32:19,520 --> 00:32:20,560
But it's like photos.

819
00:32:20,560 --> 00:32:21,440
It's like telephone.

820
00:32:21,440 --> 00:32:22,240
It's like internet.

821
00:32:22,240 --> 00:32:23,200
It's like Photoshop.

822
00:32:23,920 --> 00:32:26,320
This is a tool, but it's a tool of a very different type.

823
00:32:26,960 --> 00:32:30,080
Because like, something like Photoshop is a million lines of code.

824
00:32:31,280 --> 00:32:34,160
This is a binary file that can do anything that Photoshop does.

825
00:32:34,160 --> 00:32:35,760
But you still need to act on it.

826
00:32:35,760 --> 00:32:40,080
Last year, there was a patent awarded to an AI in South Africa,

827
00:32:40,080 --> 00:32:40,800
which was interesting.

828
00:32:40,800 --> 00:32:42,400
I think more of a gimmick than anything else.

829
00:32:43,200 --> 00:32:47,440
Do you imagine these models will lead

830
00:32:47,440 --> 00:32:50,400
towards significant intellectual property

831
00:32:50,400 --> 00:32:52,080
on the invention side of the equation?

832
00:32:52,080 --> 00:32:52,640
Oh, 100%.

833
00:32:52,640 --> 00:32:53,280
They already are.

834
00:32:53,280 --> 00:32:54,800
If you look at Google's new TPU chips,

835
00:32:54,800 --> 00:32:56,640
they're partially built by AI that they built.

836
00:32:56,640 --> 00:32:56,960
Sure.

837
00:32:56,960 --> 00:32:57,360
Yes.

838
00:32:57,360 --> 00:32:57,840
I know.

839
00:32:57,840 --> 00:33:03,760
Like we just released OpenLM from our Carpalab,

840
00:33:03,760 --> 00:33:06,960
which is an evolutionary algorithm for code for robots.

841
00:33:08,160 --> 00:33:10,240
So we're trying to optimize robotics via that.

842
00:33:10,240 --> 00:33:12,560
And then when we bring in video, we'll have even better robots.

843
00:33:13,120 --> 00:33:15,200
So are we going to get to a place where I'm saying,

844
00:33:15,200 --> 00:33:19,120
listen, please invent a device that does this for me,

845
00:33:19,120 --> 00:33:20,720
that's under this price point,

846
00:33:20,720 --> 00:33:22,720
that's made of commonly available materials

847
00:33:23,280 --> 00:33:26,000
and constrain it left, right, and center,

848
00:33:26,000 --> 00:33:28,880
and then design it, now go and print it for me

849
00:33:28,880 --> 00:33:30,160
and deliver it for me the next day.

850
00:33:30,160 --> 00:33:33,200
We're going from mine to materialization in one sense.

851
00:33:33,200 --> 00:33:33,760
Yes.

852
00:33:33,760 --> 00:33:34,400
Yeah.

853
00:33:34,400 --> 00:33:36,320
I mean, again, it's kind of the,

854
00:33:36,320 --> 00:33:37,600
was it the Star Trek thing?

855
00:33:37,600 --> 00:33:39,440
Yeah, it's a little bit longer.

856
00:33:39,440 --> 00:33:42,400
Yeah, it's the, oh my God.

857
00:33:42,400 --> 00:33:43,200
We've both forgotten it.

858
00:33:43,200 --> 00:33:43,600
Yeah.

859
00:33:43,600 --> 00:33:45,600
Anyway, you know, when Joe Geordi gets the replicator.

860
00:33:45,600 --> 00:33:46,960
The replicator, exactly.

861
00:33:46,960 --> 00:33:48,880
They're all great to your heart.

862
00:33:48,880 --> 00:33:49,600
Exactly.

863
00:33:49,600 --> 00:33:51,040
But then you can kind of see this already,

864
00:33:51,040 --> 00:33:52,960
because there's an app on the App Store right now.

865
00:33:53,600 --> 00:33:54,640
You pull your Lego out.

866
00:33:55,600 --> 00:33:56,880
And it scans all the Lego.

867
00:33:56,880 --> 00:33:57,440
Oh, nice.

868
00:33:57,440 --> 00:33:58,560
What can you build with it?

869
00:33:58,560 --> 00:33:59,040
Yes.

870
00:33:59,040 --> 00:34:01,120
It comes up with, this is how you build it as well.

871
00:34:02,080 --> 00:34:04,720
So things like you can build birds and cars and things like that.

872
00:34:04,720 --> 00:34:05,200
Awesome.

873
00:34:05,200 --> 00:34:06,000
So you go, da-da-da.

874
00:34:07,360 --> 00:34:08,480
So this is just an extension of that.

875
00:34:08,480 --> 00:34:10,880
Again, they use a transformer based architecture for that.

876
00:34:10,880 --> 00:34:13,680
Well, I'm interested when it'd be fun to say,

877
00:34:13,680 --> 00:34:15,360
okay, create new life forms.

878
00:34:16,080 --> 00:34:17,840
I mean, we're not far from that.

879
00:34:17,840 --> 00:34:20,000
Well, you know, I'm going to leave that to other bioethicists.

880
00:34:20,000 --> 00:34:21,280
I don't want to get stuck in that one.

881
00:34:21,280 --> 00:34:25,520
Wait, wait, there's ethics involved in that?

882
00:34:25,520 --> 00:34:26,880
Okay, exactly.

883
00:34:26,880 --> 00:34:29,040
I mean, the life form thing is very interesting though, right?

884
00:34:29,040 --> 00:34:32,000
Because again, everything is happening all at once.

885
00:34:32,800 --> 00:34:37,520
And so it's not just biology or physics or sociology.

886
00:34:37,520 --> 00:34:39,840
All these models, all these technologies,

887
00:34:39,840 --> 00:34:41,600
all seem to be converging at once.

888
00:34:41,600 --> 00:34:43,120
So you can create anything.

889
00:34:43,120 --> 00:34:44,960
And all the barriers are dropping at once.

890
00:34:44,960 --> 00:34:46,160
And that's complicated.

891
00:34:46,160 --> 00:34:46,640
Yeah.

892
00:34:46,640 --> 00:34:51,760
So the world is, do you think anybody truly understands

893
00:34:51,760 --> 00:34:54,240
how fast the world is about to change?

894
00:34:54,800 --> 00:34:55,120
No.

895
00:34:55,120 --> 00:34:56,720
I mean, like, look at creative industry.

896
00:34:56,720 --> 00:34:59,120
Videogames, $180 billion a year,

897
00:34:59,120 --> 00:35:03,760
like Disney spends $10 billion a year, Amazon $16 billion a year on content.

898
00:35:04,400 --> 00:35:06,880
All of that's going to change in the next couple of years alone,

899
00:35:06,880 --> 00:35:09,760
just from one tiny little two gigabyte file.

900
00:35:09,760 --> 00:35:10,720
Extraordinary.

901
00:35:10,720 --> 00:35:13,200
As well as healthcare, as well as education,

902
00:35:13,200 --> 00:35:15,120
as well as you said, every single industry,

903
00:35:15,120 --> 00:35:18,240
we're going to, so here's another question.

904
00:35:18,240 --> 00:35:18,960
Go ahead, please.

905
00:35:18,960 --> 00:35:22,160
Well, I was going to say industry is about, again, information theory.

906
00:35:22,160 --> 00:35:22,560
Yeah.

907
00:35:22,560 --> 00:35:24,400
That's, most industries are based on that,

908
00:35:24,400 --> 00:35:25,760
especially service-based industries.

909
00:35:26,400 --> 00:35:29,600
And so once you can basically take a system,

910
00:35:29,600 --> 00:35:32,080
you can have human input in the loop to train a system

911
00:35:32,080 --> 00:35:33,920
that's a generalist, to understand principles.

912
00:35:34,640 --> 00:35:36,240
You disrupt just about everything.

913
00:35:36,240 --> 00:35:36,800
Yeah.

914
00:35:36,800 --> 00:35:40,480
I'm going to pause on that because it's probably one of the most important things.

915
00:35:40,480 --> 00:35:44,080
Any entrepreneur, any CEO, any parent, any kid,

916
00:35:44,080 --> 00:35:51,120
anyone needs to understand, we're about to enter a period of hyper-disruption

917
00:35:51,120 --> 00:35:53,520
and growth and hyper-opportunity creation.

918
00:35:53,520 --> 00:35:54,000
Yes.

919
00:35:54,000 --> 00:35:57,760
Like, what happens is that in any area you create,

920
00:35:57,760 --> 00:35:58,800
there are value spikes.

921
00:35:58,800 --> 00:36:00,480
So you can look at it as like a flat area,

922
00:36:00,480 --> 00:36:03,520
and then companies and individuals occupy certain areas.

923
00:36:03,520 --> 00:36:05,440
So they've got like a mix of skills, right?

924
00:36:05,440 --> 00:36:06,640
And that's how you earn your living.

925
00:36:07,280 --> 00:36:09,440
And these are like spikes.

926
00:36:09,440 --> 00:36:10,640
That's all going to get shaken up,

927
00:36:10,640 --> 00:36:12,320
and it's going to be the value is elsewhere.

928
00:36:12,880 --> 00:36:13,040
Yeah.

929
00:36:13,040 --> 00:36:14,400
We don't know where the value will be.

930
00:36:14,400 --> 00:36:15,120
We've got some guesses.

931
00:36:15,120 --> 00:36:17,600
So like in a time when everyone can make anything,

932
00:36:17,600 --> 00:36:18,880
what becomes valuable?

933
00:36:18,880 --> 00:36:19,360
Something.

934
00:36:19,920 --> 00:36:20,160
Yes.

935
00:36:20,160 --> 00:36:21,600
So if a model that can make anything,

936
00:36:21,600 --> 00:36:23,360
that means Disney should have their own models, right?

937
00:36:23,920 --> 00:36:26,160
To create Mickey Mouse's and things like that.

938
00:36:26,160 --> 00:36:28,080
But now they can use that not only internally

939
00:36:28,080 --> 00:36:30,640
to save money on creation, they can use externally.

940
00:36:30,640 --> 00:36:33,600
Why can't we have Mickey Mouse having coffee

941
00:36:33,600 --> 00:36:36,960
with Master Chief at a Starbucks and microtransact pay that?

942
00:36:36,960 --> 00:36:38,880
It's been the promise of the NFTs and things like that.

943
00:36:38,880 --> 00:36:38,960
Yes.

944
00:36:38,960 --> 00:36:41,120
Now with these models, you can do that seamlessly.

945
00:36:41,120 --> 00:36:42,720
Yeah, on demand.

946
00:36:43,040 --> 00:36:44,000
Over and over again.

947
00:36:44,000 --> 00:36:46,640
So one of the realizations for me is that

948
00:36:47,440 --> 00:36:51,440
this open source technology made available to everyone

949
00:36:52,320 --> 00:36:58,160
effectively has the potential to make everyone the equivalent

950
00:36:58,160 --> 00:37:01,120
of millionaires, billionaires, trillionaires

951
00:37:01,120 --> 00:37:04,240
when whatever you want can be manifested, right?

952
00:37:04,240 --> 00:37:06,800
You can have the world's best education for your child,

953
00:37:06,800 --> 00:37:08,560
independent of where you live and what wealth you have.

954
00:37:08,560 --> 00:37:11,600
You can have the best healthcare available.

955
00:37:11,600 --> 00:37:13,840
You can have customized entertainment.

956
00:37:15,840 --> 00:37:19,360
It's, we end up in a world in which the cost of anything

957
00:37:19,360 --> 00:37:24,320
is raw materials in IP, which is going to be disrupted itself,

958
00:37:25,360 --> 00:37:26,640
and electricity.

959
00:37:26,640 --> 00:37:27,680
Yeah, pretty much.

960
00:37:27,680 --> 00:37:31,840
Again, you look at what Elon Musk did with SpaceX, right?

961
00:37:31,840 --> 00:37:32,240
Yeah.

962
00:37:32,240 --> 00:37:35,280
He's like, let's break it down to what is the constituent cost of this?

963
00:37:35,280 --> 00:37:35,760
Yeah.

964
00:37:35,760 --> 00:37:38,000
And basically with the first principle of thinking on rockets.

965
00:37:38,000 --> 00:37:38,720
First principle of thinking.

966
00:37:38,720 --> 00:37:40,880
So what I've been doing is first principle of thinking

967
00:37:40,960 --> 00:37:44,640
on information flow, social theory, and our systems.

968
00:37:44,640 --> 00:37:46,720
It all comes down to just information being in the right place

969
00:37:46,720 --> 00:37:48,640
at the right time to make the maximum impact.

970
00:37:48,640 --> 00:37:50,800
And if we can give that as an open architect to the world,

971
00:37:51,360 --> 00:37:54,880
then we can augment and then replace our existing systems

972
00:37:54,880 --> 00:37:57,040
with better ones because they outcompete.

973
00:37:57,680 --> 00:38:00,960
You go to an African nation and you teach every child

974
00:38:00,960 --> 00:38:03,120
with an AI that teaches them and learns from them.

975
00:38:03,120 --> 00:38:06,480
Within a few years, they will be out competing children

976
00:38:06,480 --> 00:38:08,240
in the top schools in New York.

977
00:38:08,240 --> 00:38:10,480
What if you give them the ability to code the system as well

978
00:38:10,480 --> 00:38:11,040
and improve it?

979
00:38:12,000 --> 00:38:13,120
It becomes very interesting.

980
00:38:13,120 --> 00:38:16,720
The kids are helping train the AI and the AI is helping train the kids.

981
00:38:16,720 --> 00:38:20,400
Yes, but then also the kids can improve the actual code

982
00:38:20,400 --> 00:38:21,200
that they have there.

983
00:38:21,920 --> 00:38:23,600
This is something that we've seen in our things.

984
00:38:23,600 --> 00:38:24,560
A virtuous cycle.

985
00:38:24,560 --> 00:38:25,440
A virtuous cycle.

986
00:38:25,440 --> 00:38:27,760
And you make that open and then you make it transplantable

987
00:38:27,760 --> 00:38:31,360
because what you have then is you have models that are standard,

988
00:38:31,360 --> 00:38:32,400
like a base.

989
00:38:32,400 --> 00:38:35,840
So I like to call Stable Diffusion 1 was a precocious kindergartner.

990
00:38:35,840 --> 00:38:38,320
Then it was a precocious high school, Stable Diffusion 2.

991
00:38:38,320 --> 00:38:40,560
Stable Diffusion 3 looks freaking amazing.

992
00:38:40,560 --> 00:38:42,400
It's going to be like a university level student.

993
00:38:42,400 --> 00:38:45,920
So Stable Diffusion 3, by the way, is the real-time rendering?

994
00:38:45,920 --> 00:38:46,400
No.

995
00:38:46,400 --> 00:38:47,520
That's Stable Diffusion 2.

996
00:38:47,520 --> 00:38:48,080
That's 2.

997
00:38:48,080 --> 00:38:48,560
Okay.

998
00:38:48,560 --> 00:38:49,360
Stable Diffusion 3.

999
00:38:49,360 --> 00:38:51,680
So again, to give you an example of the thing,

1000
00:38:51,680 --> 00:38:55,360
when we launched Stable Diffusion 1 on a top-end A100,

1001
00:38:55,360 --> 00:38:59,120
which is like a super-souped-up, super-computer chip, right?

1002
00:38:59,120 --> 00:38:59,760
We have a lot of those.

1003
00:38:59,760 --> 00:39:01,440
We can talk about the supercomputers in a bit.

1004
00:39:01,440 --> 00:39:04,880
It took 5.6 seconds on August the 23rd when we launched it.

1005
00:39:04,880 --> 00:39:06,000
Do you render a single image?

1006
00:39:06,080 --> 00:39:08,400
We render a single image in 5.1.2 by 5.1.2.

1007
00:39:08,400 --> 00:39:08,640
Okay.

1008
00:39:09,520 --> 00:39:13,200
Today, it takes 0.9 seconds in 7.6.8 by 7.6.8.

1009
00:39:14,400 --> 00:39:15,680
We've just sped it up 30 times.

1010
00:39:16,240 --> 00:39:16,640
Amazing.

1011
00:39:17,440 --> 00:39:18,160
30 times.

1012
00:39:18,160 --> 00:39:22,640
And I'm still blown away by the real-time rendering of the world.

1013
00:39:22,640 --> 00:39:26,720
Once you get below 200 to 220 milliseconds of response time,

1014
00:39:26,720 --> 00:39:29,120
it opens up entirely new URUX.

1015
00:39:29,120 --> 00:39:31,360
And we didn't just release Stable Diffusion as an image creator.

1016
00:39:31,360 --> 00:39:33,680
We also released an in-painting model.

1017
00:39:33,680 --> 00:39:37,920
So you could take, you know, Emma's hat and you could turn it red just by describing it.

1018
00:39:37,920 --> 00:39:40,080
We released a depth of image model so you could do transformations.

1019
00:39:40,080 --> 00:39:41,440
We released an upscaler.

1020
00:39:41,440 --> 00:39:45,360
So you can have a 64x64 image and the AI fills in all the details to take it

1021
00:39:45,360 --> 00:39:48,800
to 1.024x1.024, transforms the storage industry from media.

1022
00:39:49,360 --> 00:39:49,840
Amazing.

1023
00:39:49,840 --> 00:39:51,120
And this is real-time now as well.

1024
00:39:51,840 --> 00:39:54,640
So listen, I have to ask you the question, but come back to it later,

1025
00:39:54,640 --> 00:39:56,880
which is do you believe we're living in a simulation?

1026
00:39:58,880 --> 00:39:59,920
Yes or no?

1027
00:39:59,920 --> 00:40:00,320
Yes.

1028
00:40:00,320 --> 00:40:01,440
Yeah, as do I.

1029
00:40:01,440 --> 00:40:03,280
I think we're living in an nth generation.

1030
00:40:03,760 --> 00:40:05,520
Simulation, but that's a different story.

1031
00:40:05,520 --> 00:40:07,760
Okay, we'll come back to that for folks who are interested.

1032
00:40:07,760 --> 00:40:18,400
But okay, how far out are you able to imagine this world that you're creating the disruption

1033
00:40:18,400 --> 00:40:20,080
of industries, the transformations?

1034
00:40:22,400 --> 00:40:23,680
How many years out?

1035
00:40:23,680 --> 00:40:24,560
I don't know anymore.

1036
00:40:25,520 --> 00:40:30,240
Like when I started, so I funded the entire open source art space from January of last year

1037
00:40:30,240 --> 00:40:33,600
when it started because you had a generator model and then I released this clip

1038
00:40:33,600 --> 00:40:35,120
model which took images to text.

1039
00:40:35,120 --> 00:40:37,040
We advanced things back and forth for each other.

1040
00:40:37,040 --> 00:40:38,480
We're like, wow, that's the way to do things.

1041
00:40:39,040 --> 00:40:41,040
Creators and discriminators as it works.

1042
00:40:41,040 --> 00:40:41,920
And it's advanced, advanced.

1043
00:40:41,920 --> 00:40:43,280
I was like, this is the next big thing.

1044
00:40:43,280 --> 00:40:45,760
Humans can now communicate visually, right?

1045
00:40:45,760 --> 00:40:47,440
It's the next big thing since we're going to make press.

1046
00:40:47,440 --> 00:40:51,600
And it kind of went as expected, plus minus six months.

1047
00:40:51,600 --> 00:40:55,360
I thought we'd get to stable diffusion in Q1, Q2 of next year.

1048
00:40:55,360 --> 00:40:56,400
And that was a big massive bit.

1049
00:40:56,400 --> 00:40:58,560
I built a gigantic supercomputer and everything.

1050
00:40:58,560 --> 00:41:00,800
To get to real time, I think it would take another year or two.

1051
00:41:01,760 --> 00:41:03,760
And instead, it took like four weeks.

1052
00:41:04,720 --> 00:41:06,960
And again, this is what you mentioned earlier.

1053
00:41:06,960 --> 00:41:10,720
The number of GitHub stars now for stable diffusion is more than Ethereum and Bitcoin

1054
00:41:10,720 --> 00:41:11,840
and just about everything else.

1055
00:41:12,800 --> 00:41:15,200
And that took them 10 years in three months.

1056
00:41:16,320 --> 00:41:21,840
Again, it's the notion that people have no idea how fast the world is changing and is accelerating.

1057
00:41:21,840 --> 00:41:25,760
And it's what you came back to with the common mission and the energy.

1058
00:41:26,640 --> 00:41:30,560
When you sent your first Bitcoin, it was an amazing experience.

1059
00:41:30,560 --> 00:41:35,120
They got overtaken by raccoons and divinity and they tried to create an alternative system

1060
00:41:35,120 --> 00:41:36,080
outside the existing system.

1061
00:41:36,080 --> 00:41:38,720
The interfaces were all the robbery and all the profits were made.

1062
00:41:39,440 --> 00:41:43,760
This is something different whereby when we talk to developers and the contributors

1063
00:41:43,760 --> 00:41:46,880
that are increasing in the ecosystem, they're so energized.

1064
00:41:47,520 --> 00:41:49,200
And this is what drives things forward.

1065
00:41:49,200 --> 00:41:52,960
Like when you see teams that do the biggest things, they have the energy.

1066
00:41:52,960 --> 00:41:54,000
It's almost palpable, right?

1067
00:41:54,640 --> 00:41:56,160
Where it's like that driven thing.

1068
00:41:56,160 --> 00:41:58,320
But we've got people from all over the world.

1069
00:41:58,880 --> 00:42:02,720
One of our latest developers was an Amazon warehouse worker at the start of this year

1070
00:42:02,720 --> 00:42:04,000
who taught himself to code.

1071
00:42:04,000 --> 00:42:05,840
And now he's building the most advanced models in the world.

1072
00:42:05,840 --> 00:42:07,520
He's got 16-year-olds and 62-year-olds.

1073
00:42:07,520 --> 00:42:09,840
And it's a team of how big?

1074
00:42:09,840 --> 00:42:11,520
So our team is 137.

1075
00:42:11,520 --> 00:42:13,760
But the developers who are creating...

1076
00:42:13,760 --> 00:42:14,560
500.

1077
00:42:14,560 --> 00:42:16,560
Well, but they're teams of one, right?

1078
00:42:16,560 --> 00:42:19,280
Their team and their individuals are able to use this to create...

1079
00:42:19,280 --> 00:42:19,600
Oh yeah.

1080
00:42:19,600 --> 00:42:22,240
So like if you want to create with this, you can just do it by yourself.

1081
00:42:22,240 --> 00:42:25,520
So there's a fantastic Twitter, you can look at levels.io.

1082
00:42:26,160 --> 00:42:30,480
And he's like, I'm going to create businesses by myself that are making millions of dollars

1083
00:42:30,480 --> 00:42:33,600
of things just because you could took this primitive and he wrapped things around it.

1084
00:42:33,600 --> 00:42:35,440
And you're just making money.

1085
00:42:35,440 --> 00:42:38,640
Like Avatar me, you can put your own face into the model.

1086
00:42:38,640 --> 00:42:41,920
10 images, you can create a Peter Diamandis model and we can put you in space.

1087
00:42:41,920 --> 00:42:43,040
In fact, we'll do that a bit later.

1088
00:42:43,680 --> 00:42:46,000
So listen, you're listening.

1089
00:42:46,000 --> 00:42:47,440
You're a 20-year-old entrepreneur.

1090
00:42:47,440 --> 00:42:52,640
You're in college, you're finished, you're skipped college, whatever it is.

1091
00:42:53,600 --> 00:42:57,120
What's your advice to that 20-year-old listening right now?

1092
00:42:57,120 --> 00:42:58,000
You should drop everything.

1093
00:42:58,000 --> 00:43:00,080
You should focus entirely on this.

1094
00:43:00,080 --> 00:43:01,440
This is the biggest shift ever.

1095
00:43:02,320 --> 00:43:04,560
Self-driving cars, $100 billion went into.

1096
00:43:04,560 --> 00:43:06,640
Crypto, hundreds of billions of dollars went into.

1097
00:43:07,440 --> 00:43:10,320
$100 billion, a trillion dollars going to go into this sector.

1098
00:43:10,320 --> 00:43:11,120
Say that again, how much?

1099
00:43:11,120 --> 00:43:13,920
$100 billion in the next few years and then a trillion dollars will go into this sector

1100
00:43:13,920 --> 00:43:16,560
because it's so transformative and so few people understand it.

1101
00:43:17,120 --> 00:43:21,200
There's never been something a technological advance that will diffuse

1102
00:43:21,920 --> 00:43:22,960
as fast as this.

1103
00:43:22,960 --> 00:43:29,760
So this is electricity, this is the Gutenberg press times a billion.

1104
00:43:29,760 --> 00:43:33,760
This is the Gutenberg press times a billion because it's not just writing.

1105
00:43:33,760 --> 00:43:35,280
It's image, 3D.

1106
00:43:35,280 --> 00:43:35,760
It's everything.

1107
00:43:35,760 --> 00:43:36,240
It's creation.

1108
00:43:36,240 --> 00:43:37,120
Protein folding.

1109
00:43:37,120 --> 00:43:40,960
It's creativity and extrapolation.

1110
00:43:42,080 --> 00:43:44,000
So what does it look like to focus on this?

1111
00:43:44,000 --> 00:43:46,960
So again, you say drop everything in focus and I get that.

1112
00:43:47,840 --> 00:43:51,840
And part of me is like, maybe I should do that too.

1113
00:43:53,280 --> 00:43:54,400
But what does it look like to focus?

1114
00:43:54,400 --> 00:43:55,280
What would a person do?

1115
00:43:55,280 --> 00:43:57,840
So again, if you're an entrepreneur, you'd be an entrepreneur in this.

1116
00:43:57,840 --> 00:44:01,120
If you are someone who can communicate, you communicate this to other people

1117
00:44:01,120 --> 00:44:03,120
and you get paid a million bucks a year as a consultant.

1118
00:44:03,680 --> 00:44:04,800
You organize information.

1119
00:44:04,800 --> 00:44:07,200
If you're an artist, if you're a creative, use a tool.

1120
00:44:07,200 --> 00:44:10,160
You become the most efficient artist in the world when you lean in on this.

1121
00:44:12,400 --> 00:44:13,920
Systems can be outcompeted.

1122
00:44:13,920 --> 00:44:15,600
It's like the example of the steel mill.

1123
00:44:15,680 --> 00:44:18,720
There were big vertically integrated steel mills that were outcompeted by

1124
00:44:18,720 --> 00:44:20,720
lots of little steel mills, micromills.

1125
00:44:21,760 --> 00:44:26,320
The big corporations, the big programs, the big things,

1126
00:44:26,320 --> 00:44:29,200
will be outcompeted by just individuals and small groups.

1127
00:44:29,760 --> 00:44:31,760
Building on top of this technology can do anything.

1128
00:44:32,560 --> 00:44:37,920
And we've seen that over and over again in every converging exponential field.

1129
00:44:37,920 --> 00:44:42,000
We've seen entrepreneurs disrupting and the rate of...

1130
00:44:42,000 --> 00:44:47,200
I think of this as the asteroid impact that the slow-lumbering dinosaurs all die

1131
00:44:47,200 --> 00:44:49,520
and the furry mammals are the ones that rapidly evolve.

1132
00:44:49,520 --> 00:44:51,520
Yeah. I mean, let's take a practical example.

1133
00:44:51,520 --> 00:44:53,760
And this is what we said a bit earlier about ChatJPT.

1134
00:44:54,720 --> 00:44:56,880
Why does anyone need to use Google Image Search in a year?

1135
00:44:57,680 --> 00:44:57,920
Yeah.

1136
00:44:57,920 --> 00:45:01,520
When you can create any image just by describing it and then iterate it just with your words.

1137
00:45:01,520 --> 00:45:02,000
Yes.

1138
00:45:02,000 --> 00:45:02,720
Just attribution.

1139
00:45:02,720 --> 00:45:02,960
That's it.

1140
00:45:02,960 --> 00:45:04,400
But that's an easy lookup table.

1141
00:45:04,400 --> 00:45:04,720
Yeah.

1142
00:45:05,360 --> 00:45:06,000
All right.

1143
00:45:06,000 --> 00:45:07,360
True. Absolutely.

1144
00:45:08,320 --> 00:45:09,600
So now let's take it...

1145
00:45:09,600 --> 00:45:14,080
Okay. The 20-year-old drop it, start experimenting, start playing, start using.

1146
00:45:15,840 --> 00:45:17,680
Now you're running a company.

1147
00:45:17,680 --> 00:45:20,480
You're running a $100 million company,

1148
00:45:21,040 --> 00:45:23,840
worse off a billion or $10 billion company.

1149
00:45:23,840 --> 00:45:25,280
And you see this.

1150
00:45:25,280 --> 00:45:28,880
How fearful should you be and what should you be doing?

1151
00:45:28,880 --> 00:45:31,360
Again, you should be leaning in to understand this.

1152
00:45:31,360 --> 00:45:35,120
The classical innovation process inside a company is very limited.

1153
00:45:35,120 --> 00:45:40,400
You should be having a crack team of people who've just given freedom to say,

1154
00:45:40,400 --> 00:45:42,560
how can this potentially change my entire company?

1155
00:45:44,880 --> 00:45:48,240
But it's hard because you are fighting against the inertia of your company.

1156
00:45:48,240 --> 00:45:51,440
You're fighting people being used to certain ways of interacting

1157
00:45:51,440 --> 00:45:53,040
or certain ways of distributing stuff.

1158
00:45:53,760 --> 00:45:58,480
So this is why I think you have to think and think again from first principles.

1159
00:45:58,480 --> 00:46:01,280
If this technology is real-time, fast, across modalities,

1160
00:46:02,000 --> 00:46:03,520
and it can look and understand stuff,

1161
00:46:04,480 --> 00:46:06,400
let's go forward five, 10 years and work back.

1162
00:46:07,120 --> 00:46:09,200
Does my company still exist in this current format?

1163
00:46:09,200 --> 00:46:10,160
Can I out-compete that?

1164
00:46:11,120 --> 00:46:15,440
If not, how can I fold these into my current processes and procedures, etc.?

1165
00:46:16,400 --> 00:46:17,600
Because this is the other thing.

1166
00:46:18,240 --> 00:46:22,160
As a true exponential, the AI research actually in this area,

1167
00:46:22,160 --> 00:46:24,720
80% of all AI research has become in this area in the last few years.

1168
00:46:25,440 --> 00:46:29,440
And it's exponential with a 23-month rate of doubling, a true exponential.

1169
00:46:29,680 --> 00:46:31,840
The adoption of this now is also exponential,

1170
00:46:31,840 --> 00:46:33,840
because everyone from around the world is using it,

1171
00:46:33,840 --> 00:46:35,920
and then each of those introduces another two people,

1172
00:46:35,920 --> 00:46:37,600
and it goes four and four and four.

1173
00:46:37,600 --> 00:46:41,360
So right now, it's like a wave that's under the surface.

1174
00:46:41,360 --> 00:46:42,560
It just hasn't come yet.

1175
00:46:43,280 --> 00:46:46,640
Next year is when it cracks, and the year after is when it crashes.

1176
00:46:51,200 --> 00:46:57,440
I hear you, and my reaction is, I'm excited as hell about that.

1177
00:46:58,400 --> 00:46:58,720
Right?

1178
00:46:58,720 --> 00:47:04,560
It's like because it's about transforming the inefficiency of the world today.

1179
00:47:05,280 --> 00:47:08,480
It's about taking individuals and empowering them to do,

1180
00:47:08,480 --> 00:47:10,800
to giving them agency, like you said.

1181
00:47:12,080 --> 00:47:13,600
But this is very interesting.

1182
00:47:13,600 --> 00:47:17,200
Inefficiency is where value is created in the current paradigm,

1183
00:47:18,400 --> 00:47:20,480
in some ways, or where value is created.

1184
00:47:20,480 --> 00:47:22,080
Because efficiencies exist.

1185
00:47:22,080 --> 00:47:24,160
Where value is captured by inefficiencies.

1186
00:47:25,040 --> 00:47:27,120
So we have to have, we go to the gatekeepers,

1187
00:47:27,120 --> 00:47:30,800
and we give them, you pay your lawyers loads of money, right?

1188
00:47:30,800 --> 00:47:32,320
Pay your accountants and other things like that,

1189
00:47:32,320 --> 00:47:34,320
because there are inefficiencies in the system.

1190
00:47:34,320 --> 00:47:36,560
So you pay them to remove the inefficiencies, as it were.

1191
00:47:37,360 --> 00:47:40,400
Some of these inefficiencies will no longer remain in the system,

1192
00:47:40,400 --> 00:47:42,720
but that means that that value capture will also disappear.

1193
00:47:43,840 --> 00:47:47,280
And again, this is why you have to start thinking,

1194
00:47:47,280 --> 00:47:50,160
where does the value look like in the new reconfigured landscape?

1195
00:47:51,120 --> 00:47:54,720
Do you have an example in an industry that exists today,

1196
00:47:54,720 --> 00:47:57,200
and just to make this concrete for someone?

1197
00:47:57,200 --> 00:47:59,920
Sure, you don't need lawyers anymore for everything.

1198
00:48:01,040 --> 00:48:03,840
I mean, like, you know, you've got donotpay.com,

1199
00:48:03,840 --> 00:48:04,960
those are massive things.

1200
00:48:04,960 --> 00:48:06,640
They automatically write your tickets for you,

1201
00:48:06,640 --> 00:48:07,280
and kind of, yes.

1202
00:48:08,240 --> 00:48:10,800
For those who don't know, it's if you got a parking ticket,

1203
00:48:10,800 --> 00:48:12,640
and you go to do not pay,

1204
00:48:12,640 --> 00:48:16,080
they will figure out the legal loopholes and arguments

1205
00:48:16,080 --> 00:48:17,440
to get you off your ticket.

1206
00:48:17,440 --> 00:48:19,760
Yeah, but like you still need litigators, right?

1207
00:48:20,240 --> 00:48:22,080
At what point can you have a robot litigator?

1208
00:48:22,080 --> 00:48:24,480
It's like Phoenix Wright on steroids, right?

1209
00:48:25,840 --> 00:48:28,720
You know, if you've got the, again, movie creation thing,

1210
00:48:29,680 --> 00:48:32,320
movie creation is just going to transform completely.

1211
00:48:32,320 --> 00:48:34,800
Video games, you'll be able to have your own user-generated content.

1212
00:48:35,440 --> 00:48:39,040
You know, like, again, people can right now,

1213
00:48:39,040 --> 00:48:41,040
use stable diffusion, create any image.

1214
00:48:41,040 --> 00:48:43,920
They can go to chat GPT, it's an amazing system by OpenAI,

1215
00:48:44,480 --> 00:48:45,600
and they can chat with it, and they're like,

1216
00:48:45,600 --> 00:48:48,080
oh, okay, that disrupts a lot of industries as well,

1217
00:48:48,080 --> 00:48:49,840
because it knows the different things.

1218
00:48:49,840 --> 00:48:51,680
But it's just a model, it's a blob.

1219
00:48:51,680 --> 00:48:52,960
It doesn't look on the internet.

1220
00:48:52,960 --> 00:48:54,800
What happens when you hook up these models

1221
00:48:54,800 --> 00:48:56,880
that are principle-based to the internet?

1222
00:48:56,880 --> 00:48:58,640
Stable diffusion plus Google Image Search

1223
00:48:58,640 --> 00:48:59,920
is actually incredibly powerful.

1224
00:49:00,960 --> 00:49:02,640
But people are just looking at it as stable diffusion.

1225
00:49:02,640 --> 00:49:04,640
Stable diffusion as part of a pro-

1226
00:49:04,640 --> 00:49:06,240
architecture is incredibly powerful.

1227
00:49:06,240 --> 00:49:07,120
People are making movies.

1228
00:49:07,120 --> 00:49:09,600
So like if you look at our friends at the Corridor crew,

1229
00:49:09,600 --> 00:49:12,240
for example, they did a movie called Spiderman,

1230
00:49:12,880 --> 00:49:15,680
Everyone's Home, where they created a custom model

1231
00:49:15,680 --> 00:49:18,240
based on the Spider-Verse movie with Mars Morales.

1232
00:49:19,200 --> 00:49:21,840
Couple of days, they created a three-minute trailer

1233
00:49:21,840 --> 00:49:23,920
that blows away anything big studios can do.

1234
00:49:24,480 --> 00:49:26,640
Just a few people, a few thousand bucks.

1235
00:49:27,360 --> 00:49:29,760
Let's go to a few general questions on AI.

1236
00:49:29,760 --> 00:49:32,720
First of all, you come out of the hedge fund industry.

1237
00:49:33,760 --> 00:49:38,560
I have to imagine that the day of the trader

1238
00:49:39,760 --> 00:49:42,560
investing on their own without the use of

1239
00:49:44,160 --> 00:49:46,640
any of these technologies is long gone.

1240
00:49:46,640 --> 00:49:50,560
Does anybody have any advantage on their own?

1241
00:49:50,560 --> 00:49:51,360
I mean, I think that's the thing.

1242
00:49:51,360 --> 00:49:53,120
What's an edge in trading, right?

1243
00:49:53,120 --> 00:49:55,200
And again, it comes down to information theory.

1244
00:49:55,200 --> 00:49:59,120
What can move the Apple stock 50% very little information?

1245
00:49:59,120 --> 00:50:02,720
What can move it 5% a decent amount of information?

1246
00:50:02,720 --> 00:50:04,960
1% quite a lot of information, right?

1247
00:50:04,960 --> 00:50:06,480
So we're just looking at the narrative

1248
00:50:06,480 --> 00:50:08,000
and the incremental narrative of this,

1249
00:50:08,000 --> 00:50:10,240
because again, as humans, we're heuristic creatures.

1250
00:50:10,240 --> 00:50:12,480
So like I used to be a specialist in a number of markets,

1251
00:50:12,480 --> 00:50:13,280
oil market was one.

1252
00:50:14,000 --> 00:50:16,080
An oil barrel is fungible going around the world

1253
00:50:16,080 --> 00:50:17,440
because you can just shift it on a ship.

1254
00:50:17,440 --> 00:50:17,760
Sure.

1255
00:50:17,760 --> 00:50:19,680
But the impact of a Libyan barrel going offline

1256
00:50:19,680 --> 00:50:22,560
was the third as impactful as an Oklahoma barrel.

1257
00:50:22,560 --> 00:50:23,360
Why?

1258
00:50:23,360 --> 00:50:25,120
Because most of the money is in the West

1259
00:50:25,120 --> 00:50:26,640
as opposed to near Libya.

1260
00:50:26,640 --> 00:50:28,960
So they feel it more and the market reacts more.

1261
00:50:28,960 --> 00:50:31,920
The market is a counting mechanism and a voting mechanism.

1262
00:50:31,920 --> 00:50:34,160
I think the tools that people will use for investment

1263
00:50:34,160 --> 00:50:34,960
are going to change now

1264
00:50:35,520 --> 00:50:37,840
because you'll be able to actually visualize stories better

1265
00:50:37,840 --> 00:50:39,440
and people will kind of introduce that.

1266
00:50:39,440 --> 00:50:42,480
This is kind of why people invest on themes like ESG

1267
00:50:42,480 --> 00:50:45,040
and you have these index trackers and all these other things.

1268
00:50:45,040 --> 00:50:46,480
So I think that's going to be very interesting.

1269
00:50:46,480 --> 00:50:49,760
But then which industries will be disrupted faster than others?

1270
00:50:49,760 --> 00:50:50,800
We don't know.

1271
00:50:50,800 --> 00:50:53,280
Who will be at the edge and the forefront of this

1272
00:50:53,280 --> 00:50:54,480
embracing this technology?

1273
00:50:55,120 --> 00:50:56,480
And then who can be left behind?

1274
00:50:56,480 --> 00:50:57,600
We don't know.

1275
00:50:57,600 --> 00:51:00,640
So I think the entire stock market's going to change and adjust.

1276
00:51:00,640 --> 00:51:03,200
It could be a period of supernormal profits

1277
00:51:03,200 --> 00:51:05,040
and then outcompetition as well.

1278
00:51:05,600 --> 00:51:07,920
But this is against a backdrop of inflation

1279
00:51:07,920 --> 00:51:10,720
and recession and all sorts of crazy stuff

1280
00:51:10,720 --> 00:51:11,920
all at the same time.

1281
00:51:11,920 --> 00:51:15,120
So will you be using this technology

1282
00:51:15,120 --> 00:51:22,000
to assist a hedge fund manager out there on investing?

1283
00:51:22,000 --> 00:51:23,200
Yeah, I can run to my own hedge fund.

1284
00:51:24,640 --> 00:51:27,760
No, look, again, I think this technology would be pervasive

1285
00:51:27,760 --> 00:51:29,280
because people again will see the power of this.

1286
00:51:29,840 --> 00:51:31,680
And ultimately, like I said,

1287
00:51:32,400 --> 00:51:34,000
investing usually comes down to stories.

1288
00:51:36,080 --> 00:51:39,280
Influencing human desires and intention.

1289
00:51:39,280 --> 00:51:39,760
Exactly.

1290
00:51:40,000 --> 00:51:42,000
If you're doing VC to fund management or whatever,

1291
00:51:42,000 --> 00:51:43,680
you've just basically got a story.

1292
00:51:43,680 --> 00:51:45,680
You can say for all you want that you're trying to be

1293
00:51:45,680 --> 00:51:47,840
like quantitative and this and that.

1294
00:51:49,360 --> 00:51:51,120
But nobody knows the future.

1295
00:51:51,120 --> 00:51:54,320
So you construct a story, but you're saying,

1296
00:51:54,320 --> 00:51:56,400
what is the evolution of that story going to be

1297
00:51:56,400 --> 00:51:58,480
such that someone will buy this from me at a higher price?

1298
00:51:59,360 --> 00:52:00,240
That's all that matters.

1299
00:52:00,240 --> 00:52:02,240
And part of it, for entrepreneurs,

1300
00:52:02,240 --> 00:52:03,920
one way that I suggest to look at things is,

1301
00:52:04,640 --> 00:52:06,000
what's the terminal value?

1302
00:52:06,000 --> 00:52:07,520
You get someone to agree to that.

1303
00:52:07,520 --> 00:52:08,880
All you're doing when you're raising money

1304
00:52:08,880 --> 00:52:11,120
is you're de-risking your path for that terminal value.

1305
00:52:12,560 --> 00:52:15,360
And people don't realize that's all of investment.

1306
00:52:15,360 --> 00:52:17,040
Yeah, it's a very simplified view of it.

1307
00:52:17,600 --> 00:52:21,440
So I'm excited to have you at speaking at abundance360 this year.

1308
00:52:21,440 --> 00:52:23,680
Thank you for joining us in March.

1309
00:52:23,680 --> 00:52:24,320
Thank you for having me.

1310
00:52:25,360 --> 00:52:27,520
And I've dedicated an entire day.

1311
00:52:27,520 --> 00:52:30,720
We've added a fourth day of the program focused on AI

1312
00:52:30,720 --> 00:52:33,680
because it's just, I think people need to understand

1313
00:52:33,680 --> 00:52:37,120
how powerful and disruptive and a view of what's coming.

1314
00:52:38,000 --> 00:52:42,160
Another one of our rock star speakers there,

1315
00:52:42,160 --> 00:52:44,080
someone who you know, Ray Kurzweil,

1316
00:52:44,080 --> 00:52:46,960
and Ray's going to be joining us to speak as well.

1317
00:52:47,920 --> 00:52:50,800
And it's been Ray's long held belief

1318
00:52:50,800 --> 00:52:54,560
that we're going to reach human level AI of like 2029.

1319
00:52:56,080 --> 00:52:57,680
However you want to define that,

1320
00:52:59,360 --> 00:53:01,760
what's your thought about that?

1321
00:53:01,760 --> 00:53:04,640
Do you think that that is the case?

1322
00:53:05,680 --> 00:53:06,960
How do you think about it?

1323
00:53:07,600 --> 00:53:08,800
It's singularity, Niere.

1324
00:53:11,920 --> 00:53:13,040
Again, how have you defined it?

1325
00:53:13,040 --> 00:53:19,600
I think my ideal view is that a future that's an intelligent internet.

1326
00:53:20,320 --> 00:53:23,360
Every person, country, culture, and company has their own models

1327
00:53:23,360 --> 00:53:27,520
and they're all interacting with each other in an optimal way for humanity.

1328
00:53:27,520 --> 00:53:31,200
So again, I kind of go to this thing of Tim Urban and Wait But Why

1329
00:53:31,200 --> 00:53:32,640
when he wrote the article about neural link,

1330
00:53:32,640 --> 00:53:35,200
had this option, this discussion of the human colossus.

1331
00:53:36,000 --> 00:53:42,320
I would like AGI to be something of all of us working for us to make us better.

1332
00:53:42,320 --> 00:53:45,840
And I think part of this is why I'm basically pushing open source

1333
00:53:45,840 --> 00:53:47,280
and these open source models.

1334
00:53:47,280 --> 00:53:49,520
And I've created an organization that goes

1335
00:53:49,520 --> 00:53:52,960
and has all these verticals that will then be spin off into independent organizations.

1336
00:53:52,960 --> 00:53:55,440
So we can standardize the architecture across that

1337
00:53:55,440 --> 00:53:58,160
and it can be an emergent global consciousness as it were.

1338
00:53:59,040 --> 00:54:04,320
I think it's important for us to talk about the idea of these models

1339
00:54:04,320 --> 00:54:07,840
and these personalized models to understand what that means.

1340
00:54:07,840 --> 00:54:14,800
So when you build a massive model that is trained on everything out there,

1341
00:54:15,360 --> 00:54:17,200
it's not really necessarily useful.

1342
00:54:18,080 --> 00:54:21,040
But you talk about creating models for nations,

1343
00:54:21,040 --> 00:54:23,120
for cultures, for companies, for individuals.

1344
00:54:23,680 --> 00:54:26,000
Can you just give us the 101 on that

1345
00:54:26,000 --> 00:54:28,880
so people understand the power of that and the value of that?

1346
00:54:28,880 --> 00:54:31,360
Knowledge evolves at different paces, right?

1347
00:54:31,360 --> 00:54:34,960
There's knowledge on how to use a toilet that's been with us for many, many years,

1348
00:54:34,960 --> 00:54:38,160
you know, versus knowledge on foundation models, which is just very new.

1349
00:54:38,160 --> 00:54:40,720
You kind of see that fashions is like pace layering, as it were.

1350
00:54:42,160 --> 00:54:45,920
The thing, the way that models will be built is that you should look at them as like pizza bases.

1351
00:54:45,920 --> 00:54:49,600
So we're trying to figure out what an optimal pizza base is for a generalized image model.

1352
00:54:49,600 --> 00:54:52,560
Then you've got maybe an Indian model with a bit of culture in there

1353
00:54:52,560 --> 00:54:54,880
and then Indian fashion in there.

1354
00:54:54,880 --> 00:54:59,440
And then Indian fashion for MAD to try and be fashionable when he's in India.

1355
00:55:00,080 --> 00:55:02,080
And so you can kind of look at those as bases.

1356
00:55:02,080 --> 00:55:04,080
Because what we do is, again, we flip the paradigm.

1357
00:55:04,080 --> 00:55:10,000
So to train a stable diffusion, like we built a 4,800 cluster with our buddies at Amazon.

1358
00:55:10,720 --> 00:55:12,560
To put that in context, the fastest supercomputer...

1359
00:55:12,560 --> 00:55:14,880
4,800 cluster.

1360
00:55:14,880 --> 00:55:15,840
Yeah, it's huge.

1361
00:55:15,840 --> 00:55:19,600
Yeah, to put it in context, the fastest supercomputer in the UK, Cambridge 1 is 640.

1362
00:55:20,160 --> 00:55:23,920
We've got about 10 times the compute of NASA, roughly speaking.

1363
00:55:23,920 --> 00:55:25,360
It's one of the top 10 in the world.

1364
00:55:25,360 --> 00:55:27,280
And next year I'll go 10 times bigger.

1365
00:55:27,280 --> 00:55:28,480
Yours will go 10 times bigger.

1366
00:55:29,440 --> 00:55:30,720
And we make that available to everyone.

1367
00:55:30,720 --> 00:55:32,800
And it's on the Amazon cloud?

1368
00:55:32,800 --> 00:55:34,080
It's on the Amazon cloud, yeah.

1369
00:55:34,080 --> 00:55:36,400
Because it would have been a bit too much for us to build it by ourselves.

1370
00:55:37,280 --> 00:55:39,520
So we got them to lean in on that and kind of build this.

1371
00:55:40,560 --> 00:55:44,240
Facebook has 16,000, for example, because they're pushing big metaverse.

1372
00:55:44,240 --> 00:55:46,880
That's the fastest in the world, if you want to have an idea.

1373
00:55:46,880 --> 00:55:51,440
But again, this is an exponential thing whereby we have more than just about every single country.

1374
00:55:51,440 --> 00:55:55,920
And we use that to take 100,000 gigabytes of images and compress it into...

1375
00:55:55,920 --> 00:55:57,200
100 terabytes.

1376
00:55:57,200 --> 00:55:59,440
Yeah, 100 terabytes and compress it down.

1377
00:55:59,440 --> 00:56:02,240
So into two, effectively.

1378
00:56:02,240 --> 00:56:04,480
So 50,000 to one compression of knowledge.

1379
00:56:06,480 --> 00:56:10,000
So when you say that compression, when you're talking about a neural net,

1380
00:56:12,480 --> 00:56:17,200
not all of the connections or the pathways in the neural net are useful or valid.

1381
00:56:18,080 --> 00:56:21,360
So what you're doing is you're choosing which ones are?

1382
00:56:21,360 --> 00:56:23,280
Yeah, because it pays attention to what the most important lines are.

1383
00:56:23,280 --> 00:56:25,120
So this is part of the attention attention.

1384
00:56:25,120 --> 00:56:27,520
And then it creates these latent spaces that you poke with the prompts,

1385
00:56:27,520 --> 00:56:28,800
which are the words that you put in.

1386
00:56:29,360 --> 00:56:34,560
So like OpenAI, for example, made GPT-3 was 175 billion parameter model.

1387
00:56:34,560 --> 00:56:37,120
The next step after that deep learning, because that's called deep learning,

1388
00:56:37,120 --> 00:56:41,280
this thing, because we spend all the energy and all that compute, you don't have to.

1389
00:56:41,280 --> 00:56:43,360
You can then take that base, that pizza base.

1390
00:56:43,360 --> 00:56:45,360
You can inject some PETA and then you have a PETA model.

1391
00:56:45,360 --> 00:56:48,080
It just takes 10 images and then you can put yourself in anything.

1392
00:56:48,080 --> 00:56:51,520
Or you can spend 100,000 images and do an even more refined model.

1393
00:56:51,520 --> 00:56:52,320
But it forms a base.

1394
00:56:52,320 --> 00:56:54,080
It's what's called the foundation model in a way.

1395
00:56:54,720 --> 00:56:56,000
But you can even make it even more efficient.

1396
00:56:56,000 --> 00:56:58,480
So GPT-3 is 175 billion parameters.

1397
00:56:58,480 --> 00:56:59,920
So it's like 80 times bigger.

1398
00:56:59,920 --> 00:57:03,440
And they said GPT-4 was going to be like 100 trillion parameters.

1399
00:57:03,440 --> 00:57:04,640
Could be, yeah.

1400
00:57:04,640 --> 00:57:08,000
I think that chat GPT is actually training GPT-4 right now.

1401
00:57:08,000 --> 00:57:08,320
Interesting.

1402
00:57:08,320 --> 00:57:09,840
Because what happens is that you have this foundation,

1403
00:57:10,480 --> 00:57:16,960
but then in the classical day age of big data, what mattered was who you were.

1404
00:57:16,960 --> 00:57:20,880
So they took your data, they built these big models and they targeted you.

1405
00:57:20,880 --> 00:57:23,760
What matters now in the day age of big models

1406
00:57:23,840 --> 00:57:25,360
is how you use the models.

1407
00:57:25,360 --> 00:57:26,720
Because not all those neurons are needed.

1408
00:57:26,720 --> 00:57:28,400
We don't need all two billion images, right?

1409
00:57:28,400 --> 00:57:28,640
Right.

1410
00:57:29,280 --> 00:57:31,600
So GPT-3 was 175 billion.

1411
00:57:31,600 --> 00:57:33,040
And they saw how people used it.

1412
00:57:33,040 --> 00:57:34,880
And they identified the neurons that lit up.

1413
00:57:35,600 --> 00:57:38,000
And they compressed it down to 1.3 billion parameters.

1414
00:57:38,000 --> 00:57:40,560
So now when you're using chat GPT, you're training one of those models.

1415
00:57:41,360 --> 00:57:43,280
And they're looking at how people are using it to compress it down.

1416
00:57:43,280 --> 00:57:44,800
So it's able to usually compress down even more.

1417
00:57:45,360 --> 00:57:47,600
But then this is personalized model that's very interesting.

1418
00:57:47,600 --> 00:57:50,800
Because you can have almost this standardized base.

1419
00:57:51,120 --> 00:57:54,960
And then you can inject your own context into it.

1420
00:57:54,960 --> 00:57:59,040
And the context of your culture, your company, your community, and others.

1421
00:57:59,760 --> 00:58:01,600
And so that base is manual built.

1422
00:58:02,320 --> 00:58:04,240
In that you can extend out the latent spaces.

1423
00:58:04,240 --> 00:58:07,360
So like in stable diffusion one, we didn't really filter it.

1424
00:58:07,360 --> 00:58:09,520
So Mona Lisa is overfit.

1425
00:58:09,520 --> 00:58:10,880
There's too many pictures of Mona Lisa.

1426
00:58:10,880 --> 00:58:12,960
So it's very hard to get her out of the picture frame.

1427
00:58:14,320 --> 00:58:16,800
Now we adjusted it so you can make her swim very easily.

1428
00:58:16,800 --> 00:58:17,920
Because it's not overfit anymore.

1429
00:58:17,920 --> 00:58:19,600
And it'll continue improving and adapting.

1430
00:58:19,600 --> 00:58:21,200
So we'll have better databases.

1431
00:58:21,200 --> 00:58:23,520
And in a year's time, we'll have a very mature base

1432
00:58:23,520 --> 00:58:24,800
that people can take and extend.

1433
00:58:24,800 --> 00:58:27,920
So like in Japan, they took stable diffusion

1434
00:58:27,920 --> 00:58:30,880
and they adjusted the text encoder for Japanese culture.

1435
00:58:32,080 --> 00:58:34,240
And then it meant that if you use normal stable diffusion,

1436
00:58:34,240 --> 00:58:37,760
because it's very Western oriented, salary man means a happy man.

1437
00:58:37,760 --> 00:58:38,240
Yes.

1438
00:58:38,240 --> 00:58:39,920
In Japan, diffusion is very sad man.

1439
00:58:40,560 --> 00:58:41,520
Very sad man indeed.

1440
00:58:41,520 --> 00:58:43,200
Because it understands that context.

1441
00:58:43,200 --> 00:58:43,680
Fascinating.

1442
00:58:43,680 --> 00:58:46,240
So in the future, if I wanted to,

1443
00:58:47,120 --> 00:58:49,360
I coach a lot of entrepreneurs

1444
00:58:49,360 --> 00:58:52,640
and a lot of CEOs through abundance 360 and so forth.

1445
00:58:53,360 --> 00:58:57,680
If I wanted to create a virtualized version of myself

1446
00:58:57,680 --> 00:59:01,920
that in certain circumstances would react in a certain way,

1447
00:59:02,960 --> 00:59:04,160
that's pretty easy.

1448
00:59:04,160 --> 00:59:04,720
It's coming.

1449
00:59:04,720 --> 00:59:08,080
So pocket coach, if I wanted to like have

1450
00:59:08,080 --> 00:59:11,040
Tony Robbins in my pocket in the right moment

1451
00:59:11,040 --> 00:59:14,480
or the Dalai Lama, it's all possible.

1452
00:59:14,480 --> 00:59:17,680
Yeah, and that'll be probably a four gigabyte file.

1453
00:59:17,680 --> 00:59:19,120
And how far is that from now?

1454
00:59:20,240 --> 00:59:21,200
Pretty much do it now.

1455
00:59:21,200 --> 00:59:22,000
Go do it now.

1456
00:59:22,000 --> 00:59:25,760
And you can do it fully realistic like, you know.

1457
00:59:25,760 --> 00:59:27,920
I think the term is holy shit.

1458
00:59:27,920 --> 00:59:28,800
That's amazing.

1459
00:59:28,800 --> 00:59:31,120
Yeah, because you have the multi-modality.

1460
00:59:31,120 --> 00:59:33,600
You can make it indistinguishable from a human like

1461
00:59:33,600 --> 00:59:36,080
stable diffusion two is pretty much photo realistic.

1462
00:59:36,080 --> 00:59:38,240
Now stable diffusion three will break that barrier.

1463
00:59:38,800 --> 00:59:40,480
And obviously we can animate now.

1464
00:59:40,480 --> 00:59:42,080
So again, that's kind of crazy.

1465
00:59:42,080 --> 00:59:42,720
That's crazy.

1466
00:59:42,800 --> 00:59:47,680
Meta-humans epic game style and video are doing.

1467
00:59:47,680 --> 00:59:49,200
You can also do human realistic voices

1468
00:59:49,200 --> 00:59:50,640
with fully emotional range as well.

1469
00:59:51,520 --> 00:59:54,560
So like my sister-in-law runs a company called Sanantic.

1470
00:59:55,600 --> 00:59:58,880
She reconstructed Val Kilmer's voice for Val and Top Gun.

1471
00:59:58,880 --> 00:59:59,040
Nice.

1472
00:59:59,040 --> 01:00:00,000
I was doing all the video games.

1473
01:00:00,000 --> 01:00:02,320
And if you go to Sanantic.io, you hear an AI tell you

1474
01:00:02,320 --> 01:00:04,480
that it loves you and it's really creepy.

1475
01:00:05,760 --> 01:00:06,800
She just sold us Spotify.

1476
01:00:06,800 --> 01:00:09,200
So I'm sure we'll have some really engaging podcasts

1477
01:00:09,200 --> 01:00:09,840
and things like that.

1478
01:00:10,160 --> 01:00:14,000
And like again, we'll cross all modalities now.

1479
01:00:14,000 --> 01:00:17,600
In narrow, you're achieving human levels and going on

1480
01:00:17,600 --> 01:00:19,760
to human levels of performance and benchmarks

1481
01:00:20,480 --> 01:00:23,200
from media to understanding to output.

1482
01:00:24,320 --> 01:00:27,040
And the barriers to putting yourself in, like I said,

1483
01:00:27,040 --> 01:00:30,480
you can train a model now in like less than an hour

1484
01:00:30,480 --> 01:00:33,440
with 10, 100 images of yourself to put yourself in anything

1485
01:00:35,200 --> 01:00:35,920
for a buck.

1486
01:00:37,120 --> 01:00:39,360
Because we've done the heavy lifting of millions of bucks

1487
01:00:39,360 --> 01:00:42,160
of pre-training the model as it were, whereas classically,

1488
01:00:42,160 --> 01:00:43,520
AI wasn't like that.

1489
01:00:43,520 --> 01:00:47,600
So if I wanted to create a virtualized host of myself,

1490
01:00:48,240 --> 01:00:53,520
being able to on-screen play me, say what I want to say,

1491
01:00:54,160 --> 01:00:55,280
that's here now.

1492
01:00:55,280 --> 01:00:55,840
Let's say it now.

1493
01:00:55,840 --> 01:00:57,440
Like the technology is here now.

1494
01:00:57,440 --> 01:00:58,640
The implementation needs to be there.

1495
01:00:58,640 --> 01:01:01,520
So like one of the companies we work with is called

1496
01:01:01,520 --> 01:01:02,640
atlethea.ai.

1497
01:01:02,640 --> 01:01:04,400
And so they use our language models.

1498
01:01:04,400 --> 01:01:06,800
And so you can upload your scripts and it'll learn how you speak.

1499
01:01:07,040 --> 01:01:10,480
But then the voice technology, they haven't integrated the voice technology.

1500
01:01:10,480 --> 01:01:12,160
We have an extra generation audio technology.

1501
01:01:12,160 --> 01:01:15,280
That'll come in a month or two and then it'll learn how you speak literally.

1502
01:01:15,280 --> 01:01:17,440
I was using last year at Advanced 360.

1503
01:01:17,440 --> 01:01:20,720
I was using a company called Soul Machines and it had a virtual humans.

1504
01:01:20,720 --> 01:01:20,880
Yes.

1505
01:01:20,880 --> 01:01:21,520
All right.

1506
01:01:21,520 --> 01:01:25,520
And so I would love to create a virtual version of myself

1507
01:01:25,520 --> 01:01:26,720
for anybody I want.

1508
01:01:26,720 --> 01:01:27,760
And that's here.

1509
01:01:27,760 --> 01:01:28,720
That's here again.

1510
01:01:28,720 --> 01:01:32,320
Soul Machines is upgrading now thanks to the technology that we're open sourcing.

1511
01:01:32,320 --> 01:01:33,360
This is the other interesting part.

1512
01:01:33,360 --> 01:01:34,960
Open source will always lie close source.

1513
01:01:35,200 --> 01:01:37,680
Because close source can always take open and add in data.

1514
01:01:37,680 --> 01:01:38,000
Yes.

1515
01:01:38,000 --> 01:01:41,040
And they can have very focused teams focusing on certain use cases.

1516
01:01:41,040 --> 01:01:45,840
So we're literally upgrading the foundation of all of these companies

1517
01:01:45,840 --> 01:01:47,440
as we release these models.

1518
01:01:47,440 --> 01:01:48,080
Interesting.

1519
01:01:48,080 --> 01:01:50,080
And then you mix and match and that's where the value is.

1520
01:01:50,080 --> 01:01:52,080
Actually, who was it who said a thing?

1521
01:01:52,080 --> 01:01:53,520
Was it my friend Jason or someone else?

1522
01:01:53,520 --> 01:01:55,920
No, it was one of the other VCs.

1523
01:01:55,920 --> 01:01:59,120
Most of the money in the world is made by aggregation and disaggregation.

1524
01:01:59,120 --> 01:02:01,120
It just depends on which part of the cycle you're at.

1525
01:02:01,120 --> 01:02:02,160
Interesting.

1526
01:02:02,320 --> 01:02:03,280
I can see that.

1527
01:02:03,280 --> 01:02:06,480
I was having dinner with Reid Hoffman a month ago or so

1528
01:02:06,480 --> 01:02:08,480
and he said something which is interesting.

1529
01:02:08,480 --> 01:02:12,720
He said every profession is going to have an AI co-pilot very soon.

1530
01:02:12,720 --> 01:02:14,320
And I've been saying this for medicine.

1531
01:02:14,320 --> 01:02:17,920
I think it's going to be malpractice to diagnose without having AI in the loop.

1532
01:02:17,920 --> 01:02:19,360
And we'll see what the time frame is.

1533
01:02:19,360 --> 01:02:20,880
Five years is my guess.

1534
01:02:22,080 --> 01:02:28,320
But I can see an AI co-pilot as an architect, as a lawyer, as a chef, as everything.

1535
01:02:28,640 --> 01:02:30,640
How far is that?

1536
01:02:30,640 --> 01:02:32,640
Well, I mean it's here for code right now.

1537
01:02:32,640 --> 01:02:37,680
So co-pilot, literally what it's called, from Microsoft GitHub

1538
01:02:37,680 --> 01:02:39,680
and then code whisperer now from Amazon.

1539
01:02:39,680 --> 01:02:41,680
They help you write better code.

1540
01:02:41,680 --> 01:02:43,680
It's about a 50% speed increase.

1541
01:02:43,680 --> 01:02:44,320
Amazing.

1542
01:02:44,320 --> 01:02:46,320
And that's what we've kind of measured so far, which is insane.

1543
01:02:46,320 --> 01:02:49,840
Like you type it, I want to have a piece of code that does this and boom, it's there.

1544
01:02:49,840 --> 01:02:51,840
And maybe you need to add a snippet, doesn't matter.

1545
01:02:51,840 --> 01:02:53,840
It makes your life easier, right?

1546
01:02:53,840 --> 01:02:55,840
Like Sayable Diffusion is a co-pilot for art.

1547
01:02:55,840 --> 01:02:59,840
So artists use it to iterate rapidly on different concepts and they take it.

1548
01:02:59,840 --> 01:03:02,320
And there's a Photoshop integration and they use it as part of their Photoshop process.

1549
01:03:02,320 --> 01:03:05,840
I used to say that the crowd was the interim step until AI, right?

1550
01:03:05,840 --> 01:03:09,360
So GitHub was the crowd and now you've got, you know.

1551
01:03:09,360 --> 01:03:09,840
Yeah.

1552
01:03:09,840 --> 01:03:11,840
And you know, there's questions around, you know, what was it trained on?

1553
01:03:11,840 --> 01:03:13,840
Because it was trained on an entire snapshot of things.

1554
01:03:13,840 --> 01:03:17,840
But again, it's like a different type of information flow to the Web 2 economy.

1555
01:03:17,840 --> 01:03:19,840
We're going to skip over Web 3 because it was a bit crap.

1556
01:03:19,840 --> 01:03:21,840
And we're going to go to Web 4.

1557
01:03:21,840 --> 01:03:23,840
Or whatever it is now.

1558
01:03:23,840 --> 01:03:25,840
And you know, maybe you could go to Web 4.

1559
01:03:25,840 --> 01:03:27,840
It looks like AI.

1560
01:03:27,840 --> 01:03:29,840
Do a cool logo for that.

1561
01:03:29,840 --> 01:03:31,840
Just get that stable diffusion to make it.

1562
01:03:31,840 --> 01:03:33,840
In fact, it's listening and it's made it.

1563
01:03:33,840 --> 01:03:37,840
So I love Jarvis from Iron Man.

1564
01:03:37,840 --> 01:03:39,840
Yeah.

1565
01:03:39,840 --> 01:03:45,840
And I find Siri and Alexa and Google now kind of disappointing.

1566
01:03:45,840 --> 01:03:47,840
How far are we from Jarvis?

1567
01:03:47,840 --> 01:03:51,840
So I think Jarvis is probably two to three years.

1568
01:03:51,840 --> 01:03:57,840
Because like right now you've got a MacBook M2 in front of you.

1569
01:03:57,840 --> 01:04:01,840
16.8% of that chipset is a neural engine that's optimized for these transform based architectures.

1570
01:04:01,840 --> 01:04:05,840
Stable diffusion is one of the first models to actually go down to that level.

1571
01:04:05,840 --> 01:04:07,840
And so when will we see that on this machine?

1572
01:04:07,840 --> 01:04:13,840
I think Apple is basically aiming for like 70, 80% of everyone to have this.

1573
01:04:13,840 --> 01:04:15,840
And then you can go from Siri 1 to Siri 5.

1574
01:04:15,840 --> 01:04:17,840
Nice.

1575
01:04:17,840 --> 01:04:19,840
And this is why Apple has been talking about privacy and things like that.

1576
01:04:19,840 --> 01:04:23,840
Because the new paradigm of the internet, whereby the classical Web 2 internet,

1577
01:04:23,840 --> 01:04:29,840
was intelligence at the middle coordinating us and feeding on our dreams and hopes and emotions to sell us ads.

1578
01:04:29,840 --> 01:04:31,840
Now it's intelligence at the edge.

1579
01:04:31,840 --> 01:04:33,840
Whereby you've got your own Apple ID.

1580
01:04:33,840 --> 01:04:35,840
You've got your own privacy layer.

1581
01:04:35,840 --> 01:04:39,840
And then you've got chips that can run AI at the edge that really understand you.

1582
01:04:39,840 --> 01:04:41,840
That's why the noggin's experience is seamless.

1583
01:04:41,840 --> 01:04:43,840
And Google also realized that.

1584
01:04:43,840 --> 01:04:45,840
So they're building stuff into the Pixel phones.

1585
01:04:45,840 --> 01:04:49,840
Yeah, I think people need to realize that the power of these future systems,

1586
01:04:49,840 --> 01:04:51,840
call it Jarvis for lack of a better term,

1587
01:04:51,840 --> 01:04:55,840
is when you give it open access to everything in your life.

1588
01:04:55,840 --> 01:04:57,840
You let it watch what you're eating.

1589
01:04:57,840 --> 01:04:59,840
You let it read your emails, listen to your conversations.

1590
01:04:59,840 --> 01:05:05,840
Because it makes the world, the term I use is automagical in that regard.

1591
01:05:05,840 --> 01:05:07,840
Yeah, and you need to have the foundation models to do that.

1592
01:05:07,840 --> 01:05:10,840
Because it needs generalized knowledge and then specific knowledge and contextual knowledge.

1593
01:05:10,840 --> 01:05:12,840
So it can adapt to your needs.

1594
01:05:12,840 --> 01:05:14,840
This human in the loop process is very important.

1595
01:05:14,840 --> 01:05:18,840
And so there'll be big AIs in the cloud, but then a lot of AIs on the edge.

1596
01:05:18,840 --> 01:05:20,840
And they'll interact with and talk to each other.

1597
01:05:20,840 --> 01:05:21,840
And part of each other.

1598
01:05:21,840 --> 01:05:25,840
Because what these models also do is they take structured data and turn it into unstructured data.

1599
01:05:25,840 --> 01:05:26,840
So again, stable diffusion.

1600
01:05:26,840 --> 01:05:27,840
A few words.

1601
01:05:27,840 --> 01:05:28,840
Robert De Niro's Gandalf.

1602
01:05:28,840 --> 01:05:31,840
You get a photorealistic picture of Robert De Niro's Gandalf.

1603
01:05:31,840 --> 01:05:32,840
Yep.

1604
01:05:32,840 --> 01:05:34,840
That's structured unstructured data.

1605
01:05:34,840 --> 01:05:36,840
And then you clear both ways.

1606
01:05:36,840 --> 01:05:38,840
A brief note from our sponsors.

1607
01:05:38,840 --> 01:05:39,840
Let's talk about sleep.

1608
01:05:39,840 --> 01:05:43,840
Sleep has become one of my number one longevity priorities in life.

1609
01:05:43,840 --> 01:05:53,840
Getting eight deep uninterrupted hours of sleep is one of the most important things you can do to increase your vitality and energy and increase the health span that you have here on earth.

1610
01:05:53,840 --> 01:05:58,840
You know, when I was in medical school years ago, I used to pride myself on how little sleep I could get.

1611
01:05:58,840 --> 01:06:00,840
You know, it used to be five, five and a half hours.

1612
01:06:00,840 --> 01:06:03,840
Today I pride myself on how much sleep I can get.

1613
01:06:03,840 --> 01:06:05,840
And I shoot for eight hours every single night.

1614
01:06:05,840 --> 01:06:08,840
Now, usually I'm great at going to sleep.

1615
01:06:08,840 --> 01:06:12,840
If I'm exhausted, you know, I've worked a hard day, I'm right out.

1616
01:06:12,840 --> 01:06:19,840
But if I'm having difficulty and it occurs, I'm having insomnia or my mind's overactive and I need help to get that eight hours.

1617
01:06:19,840 --> 01:06:23,840
I turn to a supplement product by Lifeforce called Peak Rest.

1618
01:06:23,840 --> 01:06:28,840
Now, Peak Rest has been formulated with an extraordinary scientific depth and background.

1619
01:06:28,840 --> 01:06:35,840
Includes everything from long lasting melatonin to magnesium to elglycine to rosemary extract, just to name a few.

1620
01:06:35,840 --> 01:06:43,840
This product is about creating a sense of rest and really giving you the depth and length of sleep that you need for recovery.

1621
01:06:43,840 --> 01:06:45,840
It's a product I hope you'll try.

1622
01:06:45,840 --> 01:06:48,840
It works for me and I'm sure it will work for you.

1623
01:06:48,840 --> 01:06:57,840
If you're interested, go to mylifeforce.com backslashpeter to get a discount from Lifeforce on this product.

1624
01:06:57,840 --> 01:07:03,840
But you'll also see a whole set of other longevity and vitality related supplements that I use.

1625
01:07:03,840 --> 01:07:08,840
We'll talk about them some other time, but in terms of sleep, Peak Rest is my go-to supplement.

1626
01:07:08,840 --> 01:07:09,840
Hope you'll enjoy it.

1627
01:07:09,840 --> 01:07:14,840
Go to mylifeforce.com backslashpeter for your discount.

1628
01:07:14,840 --> 01:07:18,840
Let's talk about something which I have an opinion about.

1629
01:07:18,840 --> 01:07:19,840
I'm curious about yours.

1630
01:07:19,840 --> 01:07:26,840
I think I know it, which is the idea of privacy, which fundamentally people all want privacy.

1631
01:07:26,840 --> 01:07:29,840
I don't believe it really exists.

1632
01:07:29,840 --> 01:07:35,840
AI can read your lips when there's data flowing everywhere or where encryption.

1633
01:07:35,840 --> 01:07:38,840
What are your thoughts about privacy and how do we deal with it?

1634
01:07:38,840 --> 01:07:39,840
Do you have any ideas?

1635
01:07:39,840 --> 01:07:43,840
I think for privacy, you should always look at what the downside to not having privacy is.

1636
01:07:43,840 --> 01:07:47,840
Actually, people are more than willing to give up their data, too willing, in my opinion.

1637
01:07:47,840 --> 01:07:52,840
Everybody clicks that, yes, I accept there's 15,000 pages of legal.

1638
01:07:52,840 --> 01:07:53,840
Exactly.

1639
01:07:53,840 --> 01:07:56,840
And then you have to think as well of different paradigms.

1640
01:07:56,840 --> 01:07:58,840
In China, will there ever be privacy?

1641
01:07:58,840 --> 01:07:59,840
Probably not.

1642
01:07:59,840 --> 01:08:03,840
And you have the social credit score and it's an opticon being built by AI, etc.

1643
01:08:03,840 --> 01:08:06,840
In the Western Parasite, what's the downside on the privacy thing?

1644
01:08:06,840 --> 01:08:07,840
What if your stuff isn't private?

1645
01:08:07,840 --> 01:08:12,840
It's basically bad actors using you in certain ways, which can include AI algorithms trying to manipulate you.

1646
01:08:12,840 --> 01:08:18,840
I think, again, what Apple's doing is building a paradigm for actual privacy because it's aligned with their business model.

1647
01:08:18,840 --> 01:08:23,840
Even other companies now, Facebook and Google, have enough information they don't need your data anymore.

1648
01:08:23,840 --> 01:08:25,840
Who actually wants your data?

1649
01:08:25,840 --> 01:08:27,840
I think it's a question.

1650
01:08:27,840 --> 01:08:30,840
We view ourselves as these wonderful things.

1651
01:08:30,840 --> 01:08:32,840
Who actually wants your data at this point?

1652
01:08:32,840 --> 01:08:36,840
But the systems have adopted to do that and policies adopted to do that as well.

1653
01:08:36,840 --> 01:08:38,840
With GDPR and all these other things.

1654
01:08:38,840 --> 01:08:40,840
Some of them overreach, I think, a bit.

1655
01:08:40,840 --> 01:08:43,840
But we are moving to this area whereby nobody needs your data anymore.

1656
01:08:43,840 --> 01:08:47,840
And also the systems are now available to give you that privacy that you want.

1657
01:08:47,840 --> 01:08:52,840
And I think people want to opt in rather than opt out of a lot of different things to get more resources and other stuff.

1658
01:08:52,840 --> 01:08:57,840
Finally, the final element is that federated learning has matured now.

1659
01:08:57,840 --> 01:08:58,840
What does that mean?

1660
01:08:58,840 --> 01:09:01,840
So federated learning is when you take the model to the data.

1661
01:09:01,840 --> 01:09:04,840
So you used to have to ingest all this data and train the models.

1662
01:09:04,840 --> 01:09:09,840
Now, if it's just like a freaking gigabyte, you send the model to the data, it can train.

1663
01:09:09,840 --> 01:09:14,840
And then without saying it's PETA or MAD, it can upstream the output.

1664
01:09:14,840 --> 01:09:18,840
So we're seeing that in HDR UK, for example, health data, we're seeing it with Melody,

1665
01:09:18,840 --> 01:09:25,840
which is a thing with a lot of pharmaceutical companies coming through to open source analysis of patient data.

1666
01:09:25,840 --> 01:09:30,840
You can finally get that where you don't have to sacrifice privacy to build AI and models.

1667
01:09:30,840 --> 01:09:33,840
And that's going to be pretty amazing to, again, advance the field

1668
01:09:33,840 --> 01:09:36,840
because you have access to so much more data to build better models.

1669
01:09:36,840 --> 01:09:37,840
Amazing.

1670
01:09:37,840 --> 01:09:40,840
Let's talk about the perceived downside.

1671
01:09:40,840 --> 01:09:47,840
And I have to imagine that as much incredible compliments

1672
01:09:47,840 --> 01:09:50,840
and the world should thank you for the work that you're doing,

1673
01:09:50,840 --> 01:09:52,840
because of the impact it's going to have,

1674
01:09:52,840 --> 01:09:56,840
you're going to have to have detractors who are worried about technological employment

1675
01:09:56,840 --> 01:10:00,840
or malicious use of AI or fake news and all of that.

1676
01:10:00,840 --> 01:10:02,840
What concerns you?

1677
01:10:02,840 --> 01:10:07,840
And I know you're a principled man who thinks about this deeply.

1678
01:10:07,840 --> 01:10:08,840
What concerns you most?

1679
01:10:08,840 --> 01:10:10,840
I don't have all the answers.

1680
01:10:10,840 --> 01:10:12,840
And that's a fair statement to make.

1681
01:10:12,840 --> 01:10:13,840
Yeah.

1682
01:10:13,840 --> 01:10:19,840
I mean, generally, what I saw was that very few individuals had control of this most powerful technology.

1683
01:10:19,840 --> 01:10:21,840
And then there's very weird things.

1684
01:10:21,840 --> 01:10:24,840
People like open source AI is like nukes and like,

1685
01:10:24,840 --> 01:10:26,840
so why should you control the nukes?

1686
01:10:26,840 --> 01:10:28,840
You know?

1687
01:10:28,840 --> 01:10:31,840
It was a very strange kind of thing.

1688
01:10:31,840 --> 01:10:34,840
They're like, no, it shouldn't be open source.

1689
01:10:34,840 --> 01:10:36,840
So why should big companies control it?

1690
01:10:36,840 --> 01:10:38,840
Again, we live in largely a democracy.

1691
01:10:38,840 --> 01:10:40,840
We live in a society.

1692
01:10:40,840 --> 01:10:44,840
And so my take was let's educate people, get this technology out there,

1693
01:10:44,840 --> 01:10:46,840
and let's have a common conversation about it.

1694
01:10:46,840 --> 01:10:48,840
Because I have my own viewpoints and they're there.

1695
01:10:48,840 --> 01:10:50,840
But again, I'm not a representative of anyone.

1696
01:10:50,840 --> 01:10:53,840
I'm just me running my own company trying to catalyze this.

1697
01:10:53,840 --> 01:10:55,840
Because I thought it was important,

1698
01:10:55,840 --> 01:11:01,840
given the fundamental change of society that will be caused by this technology now,

1699
01:11:01,840 --> 01:11:05,840
because exponentials are a hell of a thing for it to get out there.

1700
01:11:05,840 --> 01:11:06,840
And so you need to make a splash.

1701
01:11:06,840 --> 01:11:09,840
So, you know, I've got hate mail and kind of all sorts of things because it is disruptive.

1702
01:11:09,840 --> 01:11:11,840
And we have to be aware of that.

1703
01:11:11,840 --> 01:11:13,840
It is crazy and it will cause fear.

1704
01:11:13,840 --> 01:11:14,840
We have to be aware of that.

1705
01:11:14,840 --> 01:11:17,840
And we have to decide together how to do that.

1706
01:11:17,840 --> 01:11:22,840
For example, there are artists in the data set because it's a snapshot, right?

1707
01:11:22,840 --> 01:11:23,840
Sure.

1708
01:11:23,840 --> 01:11:24,840
It's less than 0.5%.

1709
01:11:24,840 --> 01:11:29,840
And so is it ethical, legal and moral to have them in there so people can prompt an art style

1710
01:11:29,840 --> 01:11:30,840
and then match them together?

1711
01:11:30,840 --> 01:11:32,840
I think it is.

1712
01:11:32,840 --> 01:11:37,840
But does that mean that we disregard artists who want to opt out of the data set?

1713
01:11:37,840 --> 01:11:38,840
No.

1714
01:11:38,840 --> 01:11:39,840
Because they're part of the global community.

1715
01:11:39,840 --> 01:11:41,840
So we've built it opt out and opt out mechanisms.

1716
01:11:41,840 --> 01:11:46,840
And by the way, those artists are influencing other artists normally in the course of just

1717
01:11:46,840 --> 01:11:47,840
them going to museum.

1718
01:11:47,840 --> 01:11:48,840
Yeah, exactly.

1719
01:11:48,840 --> 01:11:52,840
And you know, what we have is now we've had like four or five thousand artists sign up

1720
01:11:52,840 --> 01:11:53,840
of spawning.

1721
01:11:53,840 --> 01:11:55,840
Half of them have opted out.

1722
01:11:55,840 --> 01:11:57,840
Half of them have opted in.

1723
01:11:57,840 --> 01:12:01,840
Because they'd love to see their work influence the world.

1724
01:12:01,840 --> 01:12:04,840
But how many people have really absorbed the prams of the discussion?

1725
01:12:04,840 --> 01:12:05,840
Very few.

1726
01:12:05,840 --> 01:12:10,840
So like I said, my thing is that again, this is fundamental infrastructure.

1727
01:12:10,840 --> 01:12:13,840
This technology is a fundamental human right.

1728
01:12:13,840 --> 01:12:16,840
Because otherwise what you're going to have, this is a discussion that, you know, you've

1729
01:12:16,840 --> 01:12:18,840
had many times.

1730
01:12:18,840 --> 01:12:20,840
Superhumans and normal humans.

1731
01:12:20,840 --> 01:12:21,840
Yeah.

1732
01:12:21,840 --> 01:12:26,840
The ability to communicate and create makes you superhuman.

1733
01:12:26,840 --> 01:12:30,840
Because it's just not only images like it's presentations, it's being able to, like we

1734
01:12:30,840 --> 01:12:34,840
have voice to voice technology that can allow you to speak more confidently.

1735
01:12:34,840 --> 01:12:35,840
It's interesting.

1736
01:12:35,840 --> 01:12:41,160
But people need to realize that today the poorest among us in society have more than

1737
01:12:41,160 --> 01:12:44,320
the kings and queens had, you know, a couple of centuries ago.

1738
01:12:44,320 --> 01:12:46,840
And this is about leveling the playing field.

1739
01:12:46,840 --> 01:12:50,320
This is about, this is about, this is the technology and this is what I care about deeply.

1740
01:12:50,320 --> 01:12:54,560
And I know you do too, uplifting humanity, enabling every man, woman and child to have

1741
01:12:54,560 --> 01:12:57,600
access to food, water, health care, education.

1742
01:12:57,600 --> 01:12:58,600
And have a voice.

1743
01:12:58,600 --> 01:12:59,600
And have a voice.

1744
01:12:59,600 --> 01:13:00,600
They are invisible.

1745
01:13:00,600 --> 01:13:01,600
And have dreams.

1746
01:13:01,600 --> 01:13:02,600
Yeah.

1747
01:13:02,600 --> 01:13:05,600
And have dreams and have the tools to fulfill those dreams.

1748
01:13:05,600 --> 01:13:06,600
And have agency.

1749
01:13:06,600 --> 01:13:07,600
Yes.

1750
01:13:07,600 --> 01:13:08,600
Agency is the right word.

1751
01:13:08,600 --> 01:13:12,600
I had a bit of a flipping comment because again, I can do what I want in my kind of role.

1752
01:13:12,600 --> 01:13:14,600
It was like, humanity is creatively constipated.

1753
01:13:14,600 --> 01:13:16,600
We're going to make it so it can poop rainbows.

1754
01:13:16,600 --> 01:13:18,600
I think that's great.

1755
01:13:18,600 --> 01:13:19,600
It's a silly comment.

1756
01:13:19,600 --> 01:13:23,600
But again, it's the reality because people don't believe they can create.

1757
01:13:23,600 --> 01:13:28,600
They don't believe they, the mentality and mindset is wrong.

1758
01:13:28,600 --> 01:13:30,600
Because people have more agents than they can do.

1759
01:13:30,600 --> 01:13:35,600
An individual can shake the world or the individual can make anything around them better.

1760
01:13:35,600 --> 01:13:37,600
But not if they don't believe they can.

1761
01:13:37,600 --> 01:13:42,600
And this is why art therapy is used in mental health settings to amazing things.

1762
01:13:42,600 --> 01:13:46,600
We've been conditioned to consume rather than create.

1763
01:13:46,600 --> 01:13:50,600
We've been conditioned to be polarized rather than talk to each other and communicate with

1764
01:13:50,600 --> 01:13:51,600
each other.

1765
01:13:51,600 --> 01:13:52,600
And this can again, can change that.

1766
01:13:52,600 --> 01:13:56,600
And again, that's why, like I said, this should be, in my opinion, a human right.

1767
01:13:56,600 --> 01:13:59,600
It is infrastructure as important as 5G.

1768
01:13:59,600 --> 01:14:03,600
And what I'm trying to catalyze now is not that I build the company that makes the decisions

1769
01:14:03,600 --> 01:14:07,600
for that, but that we put it out there and we're spinning off a Lutheran other things.

1770
01:14:07,600 --> 01:14:09,600
I just figured out a governance structure.

1771
01:14:09,600 --> 01:14:10,600
It's not the UN.

1772
01:14:10,600 --> 01:14:11,600
It's something else.

1773
01:14:11,600 --> 01:14:22,600
So I think one important point is, do you think a world in which individuals are held

1774
01:14:22,600 --> 01:14:27,600
back or restricted feel they have no hope or a world where every mother knows her children

1775
01:14:27,600 --> 01:14:28,600
have access to the best healthcare?

1776
01:14:28,600 --> 01:14:33,600
The best education, you know, the best ability to create, that's a more peaceful world in

1777
01:14:33,600 --> 01:14:34,600
my mind.

1778
01:14:34,600 --> 01:14:35,600
Yes, 100%.

1779
01:14:35,600 --> 01:14:38,600
I mean, look, all wars are based on lies.

1780
01:14:38,600 --> 01:14:39,600
Okay.

1781
01:14:39,600 --> 01:14:44,600
For otherwise, both sides couldn't believe.

1782
01:14:44,600 --> 01:14:46,600
Because humans are humans.

1783
01:14:46,600 --> 01:14:48,600
To kill another human is disgusting.

1784
01:14:48,600 --> 01:14:49,600
Right?

1785
01:14:49,600 --> 01:14:51,600
And so you have to tell the lie that that person is the other.

1786
01:14:51,600 --> 01:14:54,600
And you have to communicate it and control the means of communication.

1787
01:14:54,600 --> 01:14:59,600
You look at kind of again where conflicts are resolved when people realize they are humans

1788
01:14:59,600 --> 01:15:01,600
and we're all part of a global society.

1789
01:15:01,600 --> 01:15:04,600
But our infrastructure has been set up to polarize.

1790
01:15:04,600 --> 01:15:05,600
Literally, we can see it visually.

1791
01:15:05,600 --> 01:15:07,600
This is how it happens.

1792
01:15:07,600 --> 01:15:09,600
The incentive structure is misaligned.

1793
01:15:09,600 --> 01:15:13,600
So how can you fight polarization if not by communication?

1794
01:15:13,600 --> 01:15:18,600
And how can you do that if you don't give people these tools and you create it so that there

1795
01:15:18,600 --> 01:15:21,600
is a base foundation for the world, so that there are generalized models that are global

1796
01:15:21,600 --> 01:15:25,600
and every country has its own AI policy using their variants of those models.

1797
01:15:25,600 --> 01:15:29,600
And then because it's all standardized, we can hop between one and the other.

1798
01:15:29,600 --> 01:15:31,600
That is a peaceful future.

1799
01:15:31,600 --> 01:15:32,600
Yes.

1800
01:15:32,600 --> 01:15:35,600
And a future worth working towards creating.

1801
01:15:35,600 --> 01:15:36,600
Working towards creating.

1802
01:15:36,600 --> 01:15:40,600
But it's also, now it's the first time we can build that future because of this disruption

1803
01:15:40,600 --> 01:15:41,600
in technology.

1804
01:15:41,600 --> 01:15:47,600
Like, you know, governments are, there's a definition of a government.

1805
01:15:47,600 --> 01:15:51,600
It is the entity with the legitimate use of political violence.

1806
01:15:51,600 --> 01:15:53,600
The only one.

1807
01:15:53,600 --> 01:15:55,600
That's a sad definition.

1808
01:15:55,600 --> 01:15:57,600
But it's the nature of it, right?

1809
01:15:57,600 --> 01:16:00,600
Because you saw lots of political violence and then it was consolidated into one entity.

1810
01:16:00,600 --> 01:16:01,600
They can imprison you.

1811
01:16:01,600 --> 01:16:02,600
Yes.

1812
01:16:02,600 --> 01:16:05,600
And they've got an army back in the currency and this and that and that, right?

1813
01:16:05,600 --> 01:16:09,600
And governments rule on the basis of pure legitimacy to violence.

1814
01:16:09,600 --> 01:16:11,600
And again, we see that kind of thing.

1815
01:16:11,600 --> 01:16:12,600
Right?

1816
01:16:12,600 --> 01:16:15,600
So against this, what typically changes a government or a society?

1817
01:16:15,600 --> 01:16:17,600
It is an act of violence in some ways.

1818
01:16:17,600 --> 01:16:18,600
It's an act of disruption.

1819
01:16:18,600 --> 01:16:19,600
It can be a technology.

1820
01:16:19,600 --> 01:16:22,600
It can be a revolution or anything like that.

1821
01:16:22,600 --> 01:16:23,600
This is a revolution.

1822
01:16:23,600 --> 01:16:24,600
Yeah.

1823
01:16:24,600 --> 01:16:28,600
That's happening just after COVID when everyone's thinking, holy crap, the system was rubbish.

1824
01:16:28,600 --> 01:16:29,600
Let's do better.

1825
01:16:29,600 --> 01:16:30,600
Yeah.

1826
01:16:30,600 --> 01:16:31,600
So once in a lifetime.

1827
01:16:31,600 --> 01:16:32,600
Yeah, I agree.

1828
01:16:32,600 --> 01:16:41,600
And the challenge is that in the world today, you can't transform a government gradually.

1829
01:16:41,600 --> 01:16:42,600
Yes.

1830
01:16:42,600 --> 01:16:47,600
And this is why as well, crypto, there were some amazing things in there and amazingly

1831
01:16:47,600 --> 01:16:49,600
smart people working there.

1832
01:16:49,600 --> 01:16:53,600
It's rubbish because it tried to build a system outside of the existing system and there was

1833
01:16:53,600 --> 01:16:56,600
this system and then there was the interface.

1834
01:16:56,600 --> 01:16:59,600
Fortunes were made there and fraud happened there.

1835
01:16:59,600 --> 01:17:00,600
Yeah.

1836
01:17:00,600 --> 01:17:04,600
Whereas this AI, because it can bridge structure and unstructured, can actually go into our

1837
01:17:04,600 --> 01:17:07,600
systems, out compete them and make them more efficient and bring them forward.

1838
01:17:07,600 --> 01:17:11,600
And it's the first technology that can do that dynamically and at scale.

1839
01:17:12,600 --> 01:17:21,600
Or build virtualized systems that are de novo that we spend our time in and opt out of the

1840
01:17:21,600 --> 01:17:23,600
existing system and into the new.

1841
01:17:23,600 --> 01:17:24,600
Exactly.

1842
01:17:24,600 --> 01:17:25,600
And it cannot compete.

1843
01:17:25,600 --> 01:17:29,600
Or the final thing, of course, is that if you keep it as it is whereby it's controlled

1844
01:17:29,600 --> 01:17:33,600
by the few, they will ultimately use it as a system of control.

1845
01:17:33,600 --> 01:17:36,600
It is the Panopticon forever.

1846
01:17:36,600 --> 01:17:40,600
And again, we're seeing this in China and other places with a social credit score that's

1847
01:17:40,600 --> 01:17:41,600
about to be augmented with AI.

1848
01:17:41,600 --> 01:17:44,600
Everyone's looking at everyone else, marching everyone else.

1849
01:17:44,600 --> 01:17:45,600
What is freedom there?

1850
01:17:45,600 --> 01:17:46,600
Maybe it's still happiness.

1851
01:17:46,600 --> 01:17:47,600
Maybe it's control.

1852
01:17:47,600 --> 01:17:49,600
Let's give people the option, right?

1853
01:17:49,600 --> 01:17:52,600
Let's give people the infrastructure and the building blocks they need to be independent,

1854
01:17:52,600 --> 01:17:53,600
happy.

1855
01:17:53,600 --> 01:17:55,600
And maybe they don't choose independence.

1856
01:17:55,600 --> 01:17:57,600
Maybe it's a bit more kind of Borg-like.

1857
01:17:57,600 --> 01:17:58,600
That's fine.

1858
01:17:58,600 --> 01:18:00,600
At least you've got the choice.

1859
01:18:00,600 --> 01:18:01,600
Yeah.

1860
01:18:01,600 --> 01:18:06,600
Because again, if we tie this all back, the fact that you can now have AI that can write

1861
01:18:06,600 --> 01:18:13,600
better than a human, that can draw better than a human, that can emote and speak to emotional

1862
01:18:13,600 --> 01:18:18,600
turns, means that, let's say for instance, take one aspect of it, companies that are

1863
01:18:18,600 --> 01:18:23,600
ad-driven that sell ads, they can create the most manipulative ads in the world ever,

1864
01:18:23,600 --> 01:18:25,600
and regulators will not regulate that.

1865
01:18:25,600 --> 01:18:26,600
That's interesting.

1866
01:18:26,600 --> 01:18:27,600
And they do.

1867
01:18:27,600 --> 01:18:32,600
No, but now it's the next step up because they have these latents that resonate.

1868
01:18:32,600 --> 01:18:37,600
So, like now, when you look at some AI art, artists can complain what they want.

1869
01:18:37,600 --> 01:18:38,600
It's resonant.

1870
01:18:38,600 --> 01:18:41,600
When you listen to the most advanced AI voices, it's emotional.

1871
01:18:41,600 --> 01:18:42,600
You can feel it.

1872
01:18:42,600 --> 01:18:43,600
It tugs at something.

1873
01:18:43,600 --> 01:18:46,600
And again, this is a breakthrough that's literally a year old.

1874
01:18:46,600 --> 01:18:51,600
Let's talk about the ethics and morals.

1875
01:18:51,600 --> 01:18:54,600
Does AI have a moral compass?

1876
01:18:54,600 --> 01:18:57,600
Should it have a moral compass?

1877
01:18:57,600 --> 01:19:00,600
Well, I think the creators of AI, technology is not neutral.

1878
01:19:00,600 --> 01:19:01,600
Okay.

1879
01:19:01,600 --> 01:19:06,600
The creators of technology do have a responsibility, and they will never make it neutral because

1880
01:19:06,600 --> 01:19:08,600
it embodies their perspective.

1881
01:19:08,600 --> 01:19:12,600
And it bodies the data set and other biases and things like that.

1882
01:19:12,600 --> 01:19:18,600
So, I think that AI itself, this particular type of AI, again, if we just take the model,

1883
01:19:18,600 --> 01:19:22,600
it is the action upon the model that then leads to the output.

1884
01:19:22,600 --> 01:19:24,600
So, there's a responsibility there.

1885
01:19:24,600 --> 01:19:26,600
But then, like, how do we adapt the model?

1886
01:19:26,600 --> 01:19:29,600
Do we just have a one-and-done thing that's hardly trained on Western values and norms

1887
01:19:29,600 --> 01:19:30,600
and mores?

1888
01:19:30,600 --> 01:19:34,600
Which is the way it's been going historically in the large corporate setting?

1889
01:19:34,600 --> 01:19:36,600
Yeah, because there's nothing you can do.

1890
01:19:36,600 --> 01:19:40,600
Like, you can't build a Swahili version of it because you don't have access to technology.

1891
01:19:40,600 --> 01:19:43,600
Whereas now, with the pre-training and other things like that, you can do that with one

1892
01:19:43,600 --> 01:19:44,600
graphics card.

1893
01:19:44,600 --> 01:19:45,600
That's great.

1894
01:19:45,600 --> 01:19:46,600
Right?

1895
01:19:46,600 --> 01:19:49,600
Because, again, we've kind of flipped the paradigm of an AI needs to be going all the time to

1896
01:19:49,600 --> 01:19:51,600
this pre-compressed foundation model.

1897
01:19:51,600 --> 01:19:56,600
So, I think that, you know, then there is the things of, like, when you've got self-driving

1898
01:19:56,600 --> 01:19:59,600
cars and other things like that, what are the ethical norms, the trolley problem and

1899
01:19:59,600 --> 01:20:01,600
everything that you input on that?

1900
01:20:01,600 --> 01:20:03,600
These are not easy questions.

1901
01:20:03,600 --> 01:20:06,600
Because you're extending humanity, which then means you're also extending the ethics of

1902
01:20:06,600 --> 01:20:09,600
humanity, and that is not the same around the world.

1903
01:20:09,600 --> 01:20:10,600
Education.

1904
01:20:10,600 --> 01:20:12,600
One of your moonshots.

1905
01:20:12,600 --> 01:20:18,600
We first got to know each other through the Global Learning XPrize that Elon Musk and

1906
01:20:18,600 --> 01:20:20,600
Tony Robbins funded.

1907
01:20:20,600 --> 01:20:23,600
You were one of the leaders of the one billion team.

1908
01:20:23,600 --> 01:20:24,600
That wasn't either.

1909
01:20:24,600 --> 01:20:28,600
I just kind of helped implement it with Joe and the Imagine World idea.

1910
01:20:28,600 --> 01:20:35,600
And so, what, so, if you don't mind, what did your team do in that XPrize and then where

1911
01:20:35,600 --> 01:20:37,600
are you taking this vision?

1912
01:20:37,600 --> 01:20:42,600
So, my co-founder Joe and I, Joe led this, made Imagine Worldwide to take the winners of

1913
01:20:42,600 --> 01:20:46,600
the XPrize KitKat school and one billion, the real kind of champions, and implement it

1914
01:20:46,600 --> 01:20:47,600
around the world.

1915
01:20:47,600 --> 01:20:48,600
Yeah.

1916
01:20:48,600 --> 01:20:50,600
Into the Rohingya camps and Malawi and camps and others.

1917
01:20:50,600 --> 01:20:53,600
Like, I just support from the side and share him along as he kind of goes and does the

1918
01:20:53,600 --> 01:20:55,600
really hard, valuable work.

1919
01:20:55,600 --> 01:20:59,600
But now we've seen that, I believe, the latest stats from the randomized controlled trials

1920
01:20:59,600 --> 01:21:01,600
because you need to actually implement it as what happens.

1921
01:21:01,600 --> 01:21:07,600
I think 76% of kids in refugee camps learning literacy and numeracy in 13 months without

1922
01:21:07,600 --> 01:21:08,600
internet.

1923
01:21:08,600 --> 01:21:09,600
It's immense.

1924
01:21:09,600 --> 01:21:13,600
It's like one hour of use per day is equivalent of them being in school.

1925
01:21:13,600 --> 01:21:14,600
Pretty much, yeah.

1926
01:21:14,600 --> 01:21:18,600
Because, like, it's one teacher per 300 students, 400 students.

1927
01:21:18,600 --> 01:21:19,600
But then, is it enough?

1928
01:21:19,600 --> 01:21:21,600
No, it's not enough what we've done at the moment.

1929
01:21:21,600 --> 01:21:26,600
What needs to happen is there needs to be a big grand challenge whereby, you know, Malawi

1930
01:21:26,600 --> 01:21:30,600
and Malawi kind of has said that they want to roll this out at super scale and so have

1931
01:21:30,600 --> 01:21:33,600
multiple other governments now that we have the RCTs.

1932
01:21:33,600 --> 01:21:37,600
Let's make an open source ecosystem that has AI at the core that teaches kids and learns

1933
01:21:37,600 --> 01:21:38,600
from kids.

1934
01:21:38,600 --> 01:21:42,600
So you take from what's happening in Malawi, move it to Ethiopia, Sierra Leone, Rohingya

1935
01:21:42,600 --> 01:21:44,600
camps, Brooklyn, everywhere.

1936
01:21:44,600 --> 01:21:52,600
And so there is an actual superstar, amazingly well created ecosystem for education.

1937
01:21:52,600 --> 01:21:55,600
And again, go to the future and bring it back with us.

1938
01:21:55,600 --> 01:21:56,600
Love that.

1939
01:21:56,600 --> 01:22:00,600
This year at Abundance360, Sal Khan is going to be joining us as well.

1940
01:22:00,600 --> 01:22:02,600
Have you spent any time with Sal?

1941
01:22:02,600 --> 01:22:03,600
No, not yet.

1942
01:22:03,600 --> 01:22:08,600
Okay, so I'm excited to connect you guys because, you know, he's built something pretty extraordinary,

1943
01:22:08,600 --> 01:22:13,600
but his vision is to bring AI to it so that it's AI is generating the content, not him.

1944
01:22:13,600 --> 01:22:18,600
And it's able to rapidly iterate for cultural appropriateness and so forth.

1945
01:22:18,600 --> 01:22:19,600
This is why we need to build.

1946
01:22:19,600 --> 01:22:24,600
So one of our things is that we're building national level models from India to other ones

1947
01:22:24,600 --> 01:22:26,600
where there's localized data sets and other things.

1948
01:22:26,600 --> 01:22:28,600
If you can get the education piece going.

1949
01:22:28,600 --> 01:22:33,600
Remember how I said that this AI is like a kindergarten or a grade schooler?

1950
01:22:33,600 --> 01:22:34,600
Sure.

1951
01:22:34,600 --> 01:22:35,600
It matters what you teach it.

1952
01:22:35,600 --> 01:22:37,600
So right now we're teaching everything.

1953
01:22:37,600 --> 01:22:38,600
Do we need to teach everything?

1954
01:22:38,600 --> 01:22:39,600
No.

1955
01:22:39,600 --> 01:22:43,600
So if you have an AI that teaches the kids and learns from the kids, that's the best data

1956
01:22:43,600 --> 01:22:48,600
in the world to teach an AI for a Kenyan model or a Nigerian model or others.

1957
01:22:48,600 --> 01:22:50,600
And you know who should run that technology?

1958
01:22:50,600 --> 01:22:54,600
Nigerians and Kenyans and others.

1959
01:22:54,600 --> 01:22:56,600
And so one of the things we're doing is...

1960
01:22:56,600 --> 01:22:59,600
It's almost like family based learning and extrapolated from there.

1961
01:22:59,600 --> 01:23:00,600
Exactly, because we don't know best.

1962
01:23:00,600 --> 01:23:07,600
We can give tools and we've reduced the barriers to create national level localized cultural models.

1963
01:23:07,600 --> 01:23:12,600
And then those models together form a constellation that not only have you got base learning of

1964
01:23:12,600 --> 01:23:16,600
like what's the optimal way to teach an Algebra, getting better, then you can go beyond that to that.

1965
01:23:16,600 --> 01:23:21,600
And the plan is to have an integrated system where it's hardware, software, deployment curriculum.

1966
01:23:21,600 --> 01:23:24,600
Because then we can update that through mesh networking or the amazing work of Project Giga,

1967
01:23:24,600 --> 01:23:29,600
which is putting high speed internet into every school in the world by the UN.

1968
01:23:29,600 --> 01:23:31,600
And then you can put healthcare on that.

1969
01:23:31,600 --> 01:23:32,600
Yes.

1970
01:23:32,600 --> 01:23:36,600
And then you have a self-adaptive improving healthcare system, self-adaptive improving education system.

1971
01:23:36,600 --> 01:23:37,600
And then the world...

1972
01:23:37,600 --> 01:23:43,600
I mean, for me, that's the calling that I think both of us have and hopefully many entrepreneurs here.

1973
01:23:43,600 --> 01:23:48,600
It's like what greater purpose could you have in life than uplifting humanity in that fashion?

1974
01:23:48,600 --> 01:23:49,600
Exactly.

1975
01:23:49,600 --> 01:23:54,600
And then as an etc. manager, you can fund that at scale by bonds.

1976
01:23:54,600 --> 01:23:55,600
Yeah.

1977
01:23:55,600 --> 01:23:57,600
And the world's biggest problems are the world's biggest business opportunities, right?

1978
01:23:57,600 --> 01:23:58,600
Exactly.

1979
01:23:58,600 --> 01:24:00,600
Why to become a billionaire, help a billion people.

1980
01:24:00,600 --> 01:24:05,600
So one of the ways that we're kind of doing it is results-oriented bonds whereby you can pledge

1981
01:24:05,600 --> 01:24:11,600
$20 million for million kids are provably educated on this standardized architecture.

1982
01:24:11,600 --> 01:24:13,600
The invisible become visible.

1983
01:24:13,600 --> 01:24:14,600
And measurable.

1984
01:24:14,600 --> 01:24:15,600
Measurable.

1985
01:24:15,600 --> 01:24:19,600
Infrastructure banks and the World Bank fund the remainder held by the pension funds.

1986
01:24:19,600 --> 01:24:22,600
And you can divert billions and billions of dollars into this.

1987
01:24:22,600 --> 01:24:26,600
It's kind of the promise of one laptop a child can finally be done.

1988
01:24:26,600 --> 01:24:30,600
But rather than building a system for today, we build a system for tomorrow that constantly adapts

1989
01:24:30,600 --> 01:24:35,600
and improves is understandable and standardized because that is infrastructure.

1990
01:24:35,600 --> 01:24:36,600
Yes.

1991
01:24:36,600 --> 01:24:39,600
Again, the thing I really want to emphasize is this AI is infrastructure.

1992
01:24:39,600 --> 01:24:41,600
It's more important than 5G or anything else.

1993
01:24:41,600 --> 01:24:42,600
It's oxygen in the room.

1994
01:24:42,600 --> 01:24:44,600
It's oxygen in the room.

1995
01:24:44,600 --> 01:24:45,600
Yeah.

1996
01:24:45,600 --> 01:24:49,600
So when I think about the future of education going out 10, 20 years and bringing it back

1997
01:24:49,600 --> 01:24:54,600
today, for me, it's not a book and it's not a flat screen.

1998
01:24:54,600 --> 01:24:55,600
It's going into a virtual world.

1999
01:24:55,600 --> 01:25:00,600
If I want to learn about Plato, there's a guy sitting on a slab of marble over there.

2000
01:25:00,600 --> 01:25:02,600
And he says, Hey, Peter, come on over.

2001
01:25:02,600 --> 01:25:03,600
Let me show you around.

2002
01:25:03,600 --> 01:25:05,600
Introduce my friends.

2003
01:25:05,600 --> 01:25:07,600
And it's experiential.

2004
01:25:07,600 --> 01:25:14,600
And that that NPC of Plato is trained up by all the knowledge about Plato by every historian

2005
01:25:14,600 --> 01:25:18,600
and it's accurate and the imagery and so forth.

2006
01:25:18,600 --> 01:25:25,600
And what you just said earlier about real time rendering from stable diffusion enables

2007
01:25:25,600 --> 01:25:26,600
that, right?

2008
01:25:26,600 --> 01:25:31,600
And the ability to take every historian's work on Plato and train up a model on Plato

2009
01:25:31,600 --> 01:25:32,600
enables all that.

2010
01:25:32,600 --> 01:25:33,600
Exactly.

2011
01:25:33,600 --> 01:25:38,600
And particularly when it is at the hardware level, because typically what software happens

2012
01:25:38,600 --> 01:25:44,600
is that, again, you build layers and layers and layers of kind of compilers and translations

2013
01:25:44,600 --> 01:25:46,600
so you're far from the hardware.

2014
01:25:46,600 --> 01:25:48,600
Some models are already efficient at the top level.

2015
01:25:48,600 --> 01:25:51,600
What happens when we optimize them and push them down to the hardware level?

2016
01:25:51,600 --> 01:25:52,600
You don't need internet.

2017
01:25:52,600 --> 01:25:53,600
You don't need anything.

2018
01:25:53,600 --> 01:25:54,600
You can form it.

2019
01:25:54,600 --> 01:25:57,600
But then all of a sudden you have the young ladies illustrated primer.

2020
01:25:57,600 --> 01:25:58,600
Yes.

2021
01:25:58,600 --> 01:26:03,600
Neil Stevenson's incredible book and a vision for the future of education.

2022
01:26:03,600 --> 01:26:04,600
Exactly.

2023
01:26:04,600 --> 01:26:06,600
We can make it finally, but we can make it more.

2024
01:26:06,600 --> 01:26:09,600
We can make it closer to the prime radiance and foundation.

2025
01:26:09,600 --> 01:26:11,600
So are you building hardware, Imad?

2026
01:26:11,600 --> 01:26:13,600
Getting other people to build it for me.

2027
01:26:13,600 --> 01:26:15,600
My life is not a complicated.

2028
01:26:15,600 --> 01:26:16,600
We're setting the specs.

2029
01:26:16,600 --> 01:26:17,600
We're setting the specs.

2030
01:26:17,600 --> 01:26:18,600
But this is the thing.

2031
01:26:18,600 --> 01:26:19,600
Who is we?

2032
01:26:19,600 --> 01:26:20,600
Okay.

2033
01:26:20,600 --> 01:26:25,600
The way that we're going to do it is that it's similar to the grand challenges and the

2034
01:26:25,600 --> 01:26:26,600
prizes and things like that.

2035
01:26:26,600 --> 01:26:27,600
Let's get together.

2036
01:26:27,600 --> 01:26:31,600
We will drive the process because the rule by committee never works.

2037
01:26:31,600 --> 01:26:36,600
But let's invite everyone to participate from the kids using the tablets to code them

2038
01:26:36,600 --> 01:26:39,600
to the most advanced developers in the world.

2039
01:26:39,600 --> 01:26:41,600
And let's build something for humanity.

2040
01:26:41,600 --> 01:26:43,600
That's the way to do this.

2041
01:26:43,600 --> 01:26:44,600
Amazing.

2042
01:26:44,600 --> 01:26:46,600
So you don't like the term Web 3.

2043
01:26:46,600 --> 01:26:50,600
You wanted to jump over to Web 4.

2044
01:26:50,600 --> 01:27:03,600
But this virtualized world, which is the convergence of AI and VR, AR, blockchain and so forth,

2045
01:27:03,600 --> 01:27:07,600
where do you see it in the near term going?

2046
01:27:07,600 --> 01:27:09,600
It's just going to go insane.

2047
01:27:09,600 --> 01:27:13,600
And so like we have technology, I'm going to make an announcement in January about some

2048
01:27:13,600 --> 01:27:14,600
of our technology.

2049
01:27:14,600 --> 01:27:20,600
You've got Apple likely having AR glasses, snap, oculuses, all of this.

2050
01:27:20,600 --> 01:27:23,100
It'll be a fully immersive world where you can engage.

2051
01:27:23,100 --> 01:27:26,600
Like when I was a video game investor, I looked at time to flow, fun and frustration.

2052
01:27:26,600 --> 01:27:27,600
Yes.

2053
01:27:27,600 --> 01:27:30,600
And I think this technology can adjust all of those and create worlds for us.

2054
01:27:30,600 --> 01:27:31,600
But it'll be a year or two.

2055
01:27:31,600 --> 01:27:37,200
Because again, now it's percolating and it's getting ready to then create brand new experiences

2056
01:27:37,200 --> 01:27:39,080
on the fly for everyone.

2057
01:27:39,080 --> 01:27:43,800
And I think in a couple of years, it starts going in five years.

2058
01:27:43,800 --> 01:27:47,280
Creativity, imagination, engagement, entertainment is completely transformed.

2059
01:27:47,280 --> 01:27:48,280
Within five years.

2060
01:27:48,280 --> 01:27:49,280
Within five years.

2061
01:27:49,280 --> 01:27:54,040
It's going to be the biggest shift change that we've ever seen because the incumbents

2062
01:27:54,040 --> 01:27:59,720
can't keep up with entities that are using this technology that can do everything.

2063
01:27:59,720 --> 01:28:06,760
What does making a picture look like when you can change it live with just words?

2064
01:28:06,760 --> 01:28:11,560
And you can say, actually, make his hair a bit longer or get rid of those nostril hairs

2065
01:28:11,560 --> 01:28:14,760
and it just understands and does it without having to decode it.

2066
01:28:14,760 --> 01:28:15,760
Intentionality.

2067
01:28:15,760 --> 01:28:16,760
Yes.

2068
01:28:16,760 --> 01:28:17,760
Intentionality and action.

2069
01:28:17,760 --> 01:28:18,760
Yes.

2070
01:28:18,760 --> 01:28:23,240
It's kind of the whole military thing of observe oriented, decide and act, right?

2071
01:28:23,240 --> 01:28:25,880
These systems enable that almost flawlessly.

2072
01:28:25,880 --> 01:28:30,000
Like if you use OpenAI's whisper technology, the translation of the podcast is just immense

2073
01:28:30,000 --> 01:28:34,360
and all these other things because it's learned so many principles and they're tiny files.

2074
01:28:34,360 --> 01:28:38,480
And then you can make it self-referential and say, improve it the way you think I'd

2075
01:28:38,480 --> 01:28:39,480
want it to prove.

2076
01:28:39,480 --> 01:28:40,480
You can.

2077
01:28:40,480 --> 01:28:42,760
And like we're building technology, for example, that tells you how good a story is or how

2078
01:28:42,760 --> 01:28:43,760
good code is.

2079
01:28:43,760 --> 01:28:44,760
Yes.

2080
01:28:44,760 --> 01:28:45,760
So then you have a creation and a discriminator.

2081
01:28:45,760 --> 01:28:49,960
They bounce back and forth against each other and they learn from your personal context.

2082
01:28:49,960 --> 01:28:50,960
Amazing.

2083
01:28:50,960 --> 01:28:57,960
You took a break from being a hedge fund manager to address your son's autism, which was, I

2084
01:28:57,960 --> 01:29:03,520
mean, there are individuals like yourself, like Martin Rothblatt and others who are like,

2085
01:29:04,520 --> 01:29:13,200
I refuse to believe that something can't be done and you jumped in.

2086
01:29:13,200 --> 01:29:19,360
For those who are dealing with autism personally or in their families or in their community,

2087
01:29:19,360 --> 01:29:22,040
what were your learnings and what would you advise?

2088
01:29:22,040 --> 01:29:26,360
There's always room for improvement and everyone struggles with something.

2089
01:29:26,360 --> 01:29:27,360
How old is your son today?

2090
01:29:27,360 --> 01:29:28,360
He's 14.

2091
01:29:28,360 --> 01:29:29,360
14.

2092
01:29:29,360 --> 01:29:30,360
He's super happy.

2093
01:29:30,680 --> 01:29:33,680
I think they balance each other out, that's what they say.

2094
01:29:33,680 --> 01:29:34,680
You do it pretty well.

2095
01:29:34,680 --> 01:29:35,680
It's all right.

2096
01:29:35,680 --> 01:29:40,280
So I had a lot of issues around those things, but people, again, everyone's different and

2097
01:29:40,280 --> 01:29:45,920
then diversity is our strength as it were, but sometimes kids and others need help because

2098
01:29:45,920 --> 01:29:48,640
they can't achieve their potential because there's too much going on.

2099
01:29:48,640 --> 01:29:52,480
So like I said, I'm buying into the Gabberglute to make balance theory of it and we had some

2100
01:29:52,480 --> 01:29:55,760
drug repurposing and other stuff on an N of one equals case.

2101
01:29:55,760 --> 01:29:59,360
That's not science as it were, it's first principles and I don't think it's coherent

2102
01:29:59,400 --> 01:30:01,720
enough to be brought forward, but it's interesting.

2103
01:30:01,720 --> 01:30:05,200
Instead, I think there are certain things that benefit everyone, such as a blood behavioural

2104
01:30:05,200 --> 01:30:10,320
analysis, whereby because you haven't built up the words, there are short trials to try

2105
01:30:10,320 --> 01:30:12,680
and reconstruct what a cup means.

2106
01:30:12,680 --> 01:30:16,720
It's quite an intensive process, but it's also what people do after strokes and other

2107
01:30:16,720 --> 01:30:20,360
things that make them lose their connectivity in their brain.

2108
01:30:20,360 --> 01:30:24,080
Most of this thing though is about noise filtering and reduction of that, but I think

2109
01:30:24,080 --> 01:30:28,480
percolation of that means that you have to look into first principles, analysis of some

2110
01:30:28,480 --> 01:30:31,720
of the science of what can cause it, and then you have to bring that forward to what

2111
01:30:31,720 --> 01:30:32,720
is safe.

2112
01:30:32,720 --> 01:30:36,160
So don't do kind of crackpot theories, but there's an emerging science and studies showing

2113
01:30:36,160 --> 01:30:41,000
things like NSE til 16, sulfur refrain, other things that calm you down are probably the

2114
01:30:41,000 --> 01:30:42,000
best.

2115
01:30:42,000 --> 01:30:45,680
And a large part of it is just connection and engagement there.

2116
01:30:45,680 --> 01:30:48,600
So one of the things that we'll be doing next year is that we're taking all the research

2117
01:30:48,600 --> 01:30:53,160
that I've done and formalizing it properly, because again, it's not a case of, well, I

2118
01:30:53,160 --> 01:30:55,160
can do it, so anyone should have it.

2119
01:30:55,160 --> 01:30:59,480
I did an N equals one case, but then that should be extended out.

2120
01:30:59,480 --> 01:31:03,240
And this is why I realized when COVID came along.

2121
01:31:03,240 --> 01:31:08,160
So I designed and launched with the help of loads of people, collect from an augmented

2122
01:31:08,160 --> 01:31:13,520
intelligence against COVID-19, launched at Stanford in July of 2020.

2123
01:31:13,520 --> 01:31:16,800
Was that the actual origin of stability?

2124
01:31:16,800 --> 01:31:18,320
That was the first origin of stability.

2125
01:31:18,320 --> 01:31:21,280
So we didn't incorporate at that time, but kind of put it together.

2126
01:31:21,280 --> 01:31:25,800
I mean, it's insane how far things have gone in two years.

2127
01:31:25,800 --> 01:31:26,800
Yeah.

2128
01:31:26,800 --> 01:31:30,800
I mean, we probably actually kicked off in 13 months ago.

2129
01:31:30,800 --> 01:31:33,920
So yeah, it was insane because I thought, I saw it coming and I saw it like autism as

2130
01:31:33,920 --> 01:31:39,320
a multi-systemic inflammatory condition where even now, if anyone asks you, how does COVID

2131
01:31:39,320 --> 01:31:40,400
actually work?

2132
01:31:40,400 --> 01:31:43,040
If you ask a scientist, they'll tell you, we don't actually, sure.

2133
01:31:43,040 --> 01:31:44,040
Sure.

2134
01:31:44,040 --> 01:31:46,360
Like why are ferritin levels high and why is this and that?

2135
01:31:46,360 --> 01:31:49,400
The reality is our base foundation is not good enough.

2136
01:31:49,400 --> 01:31:50,520
We don't have enough shared knowledge.

2137
01:31:50,520 --> 01:31:53,520
So I created that to create a system that's comprehensive, authoritative and up-to-date.

2138
01:31:53,520 --> 01:31:56,640
So there's a nice blog about an OCDU type site and things like that.

2139
01:31:56,640 --> 01:32:00,520
A lot of private sector enterprises that promised a lot didn't deliver.

2140
01:32:00,520 --> 01:32:04,560
And so I realized, again, this technology was the future and we needed to create open

2141
01:32:04,560 --> 01:32:05,680
infrastructure for that.

2142
01:32:05,680 --> 01:32:09,880
So when we do our autism releases next year, all of the knowledge and everything like that

2143
01:32:09,880 --> 01:32:10,880
will be available.

2144
01:32:10,880 --> 01:32:15,920
There will be a semantic scholar on steroids, which allows you to access the information

2145
01:32:15,920 --> 01:32:18,640
relevant to the type of autism that your child might have.

2146
01:32:18,640 --> 01:32:23,480
So basically, we figured out there were 16 different etiologies while driving conditions.

2147
01:32:23,480 --> 01:32:27,680
But 30% of kids would get worse and 26% of kids would get better with the same treatment,

2148
01:32:27,680 --> 01:32:28,680
so it doesn't work.

2149
01:32:28,680 --> 01:32:29,680
Yeah.

2150
01:32:29,680 --> 01:32:32,840
This era of personalized medicine needs a foundation and again, that foundation must

2151
01:32:32,840 --> 01:32:36,480
be common, but we can't wait around to do it.

2152
01:32:36,480 --> 01:32:37,480
And it's interesting, right?

2153
01:32:37,480 --> 01:32:44,160
The more I learn about the fundamentals of human biology, the more complicated.

2154
01:32:44,160 --> 01:32:46,240
You can dig layer after layer after layer.

2155
01:32:46,240 --> 01:32:47,240
Yeah.

2156
01:32:47,240 --> 01:32:53,640
It's possible for, I mean, we're discovering so much because the tools we're using, but

2157
01:32:53,640 --> 01:33:02,080
we're going to need this level of AI to cognitively understand the interactions of the system.

2158
01:33:02,080 --> 01:33:05,600
Again, it's a complex hierarchical system, you know, classical Herb Simons style.

2159
01:33:05,600 --> 01:33:08,400
And so we have to build new tools to enable that.

2160
01:33:08,400 --> 01:33:11,200
The question is, are these tools closed and the company is trying to, are they open?

2161
01:33:11,200 --> 01:33:13,240
And so our take is open.

2162
01:33:13,240 --> 01:33:16,400
And then so our business model is just scale and service around that, which is how all

2163
01:33:16,400 --> 01:33:19,560
the servers and databases are, but open is secure as well.

2164
01:33:19,560 --> 01:33:23,080
That's why Linux is used everywhere versus Microsoft Windows, et cetera.

2165
01:33:23,080 --> 01:33:26,680
So I think the final part of this is that, you know, again, it's all interrelated.

2166
01:33:26,680 --> 01:33:29,640
Like the body is such a wonderful, powerful thing.

2167
01:33:29,640 --> 01:33:33,680
If you look at longevity and things like that, we need, again, this first principles thinking

2168
01:33:33,680 --> 01:33:37,640
to both make us healthy and live longer and be happier.

2169
01:33:37,640 --> 01:33:39,640
Yeah.

2170
01:33:39,640 --> 01:33:42,960
Have you thought much about what's coming down the pike with quantum technologies and

2171
01:33:42,960 --> 01:33:43,960
quantum computing?

2172
01:33:44,000 --> 01:33:48,240
So like, I think very sympathetic to various approaches there, like I'm quite, I quite

2173
01:33:48,240 --> 01:33:53,160
like the quantum annealing on the kind of D wave, because I think that, you know, Carl

2174
01:33:53,160 --> 01:33:57,800
Friston's theory around kind of free energy principle and having these low energy states

2175
01:33:57,800 --> 01:33:58,800
makes a lot of sense.

2176
01:33:58,800 --> 01:34:02,200
In fact, this is similar to what the AI models do in that when you look at the latent spaces,

2177
01:34:02,200 --> 01:34:05,520
you're going to the low energy states of what that could kind of mean.

2178
01:34:05,520 --> 01:34:06,520
Right.

2179
01:34:06,520 --> 01:34:09,080
So that's quite a lot of jargon, I think for a lot of lists.

2180
01:34:09,080 --> 01:34:12,320
But basically, quantum computing kind of is another part of the puzzle.

2181
01:34:12,680 --> 01:34:16,000
A lot of people try to take one AI model and say, Oh, we're going to put something

2182
01:34:16,000 --> 01:34:17,000
that can do everything.

2183
01:34:17,000 --> 01:34:19,400
You know, they look at AGI as the thing that can do everything.

2184
01:34:19,400 --> 01:34:22,480
The human brain is made up of so many different parts.

2185
01:34:22,480 --> 01:34:25,400
And we're just filling in some of the missing gaps right now of which quantum computing

2186
01:34:25,400 --> 01:34:26,400
is one of them.

2187
01:34:26,400 --> 01:34:27,400
Yeah.

2188
01:34:27,400 --> 01:34:31,640
And it's, I think it's adding fuel to the fire of how fast things are moving.

2189
01:34:31,640 --> 01:34:32,640
Ah, yeah.

2190
01:34:32,640 --> 01:34:38,240
It's going to be, you know, we can see the qubits increasing and we can see it getting

2191
01:34:38,240 --> 01:34:39,240
there.

2192
01:34:39,240 --> 01:34:40,720
I mean, again, part of this is a supercomputer thing, right?

2193
01:34:40,720 --> 01:34:44,640
Like our supercomputer, we've been the fastest in the world five years ago, which is insane

2194
01:34:44,640 --> 01:34:45,960
for a private company.

2195
01:34:45,960 --> 01:34:46,960
Right.

2196
01:34:46,960 --> 01:34:47,960
Absolutely.

2197
01:34:47,960 --> 01:34:51,400
You know, like think about bigger than everything.

2198
01:34:51,400 --> 01:34:54,520
But if you look at it like Nvidia is the one that kicked off because they put AI in the

2199
01:34:54,520 --> 01:34:58,280
core of their graphics cards before this even happened, which is why it's now a 32 billion

2200
01:34:58,280 --> 01:35:00,800
dollar part of their business.

2201
01:35:00,800 --> 01:35:04,640
The AV 100s, four years ago with the first iteration, the A 100s for the next.

2202
01:35:04,640 --> 01:35:08,040
Now the H 100s are like up to nine times faster.

2203
01:35:08,040 --> 01:35:09,560
It's literally going exponential.

2204
01:35:09,560 --> 01:35:11,600
The compute is going exponential.

2205
01:35:11,600 --> 01:35:16,400
The research, the technologies, we see exponential technologies everywhere and we're like, it's

2206
01:35:16,400 --> 01:35:18,400
so difficult to piece all these things together to see.

2207
01:35:18,400 --> 01:35:21,160
I don't even know where things will be in a year, let alone five years, or ten years.

2208
01:35:21,160 --> 01:35:25,480
I think that's an important part, and this is what, when Ray talks about the singularity,

2209
01:35:25,480 --> 01:35:29,280
you know, it's the notion that we're, you know, the ability to predict what's going

2210
01:35:29,280 --> 01:35:32,160
to happen next has become impossible.

2211
01:35:32,160 --> 01:35:35,480
And that timeframe, you know, people say, what's it going to be like in 20 years?

2212
01:35:35,480 --> 01:35:39,440
I can barely think about 10 years from now or five years from now, let alone 20 years

2213
01:35:39,440 --> 01:35:40,440
from now.

2214
01:35:40,440 --> 01:35:41,440
Yeah.

2215
01:35:41,440 --> 01:35:42,440
And this affects the way that we act.

2216
01:35:42,440 --> 01:35:45,320
So the way that our brains work is that our default is decision making under risk.

2217
01:35:45,320 --> 01:35:49,200
So we look at the upside down side of something and we make an expected utility calculation

2218
01:35:49,200 --> 01:35:51,640
based on that, because systems are stable.

2219
01:35:51,640 --> 01:35:53,160
Is the world stable now?

2220
01:35:53,160 --> 01:35:54,160
No.

2221
01:35:54,160 --> 01:35:55,160
Yeah.

2222
01:35:55,160 --> 01:35:56,160
You know, so what do we do instead?

2223
01:35:56,160 --> 01:36:00,640
We make decisions under uncertainty, which is we minimize for maximum regret, you know,

2224
01:36:00,640 --> 01:36:03,000
or actually we just minimize for regret.

2225
01:36:03,000 --> 01:36:06,240
So with these powerful technologies, people are like, don't give them out because I don't

2226
01:36:06,240 --> 01:36:07,240
know what can happen.

2227
01:36:07,240 --> 01:36:09,840
It's the, it's the, it's the amygdala.

2228
01:36:09,840 --> 01:36:11,440
It's a dystopian point of view.

2229
01:36:11,440 --> 01:36:13,960
It's minimizing our downside, protecting what we have.

2230
01:36:13,960 --> 01:36:19,920
It's a scarcity and fear based animal brain, you know, reptile brain that we default to.

2231
01:36:19,920 --> 01:36:22,080
And this happens synchronously.

2232
01:36:22,080 --> 01:36:23,080
That's what we saw with COVID.

2233
01:36:23,080 --> 01:36:24,080
It just happened.

2234
01:36:24,080 --> 01:36:25,920
All of a sudden, everyone thought the same, right?

2235
01:36:25,920 --> 01:36:29,240
It's what we're seeing with tech layoffs and things like that, like Facebook made loads

2236
01:36:29,240 --> 01:36:30,240
of money.

2237
01:36:30,240 --> 01:36:31,240
Why are they laying off?

2238
01:36:31,240 --> 01:36:32,320
You know, things like that.

2239
01:36:32,320 --> 01:36:35,640
And so we have to be kind of aware of this because what's happening now as well is that

2240
01:36:35,640 --> 01:36:38,080
the classical system has reached its end.

2241
01:36:38,080 --> 01:36:40,120
We've borrowed too much money from the future.

2242
01:36:40,120 --> 01:36:43,400
And now we are going to a negative sum game with inflation and other things.

2243
01:36:43,400 --> 01:36:45,320
People are losing wealth rapidly.

2244
01:36:45,320 --> 01:36:48,760
That leads to unstable Nash equilibrium from a game theoretic perspective.

2245
01:36:48,760 --> 01:36:54,680
So we go from one steady state and lurch to another inflation to deflation to maximum

2246
01:36:54,680 --> 01:36:57,240
employment to job losses to fiscal stuff.

2247
01:36:57,240 --> 01:36:59,360
And it's just going to be a crazy few years.

2248
01:36:59,360 --> 01:37:01,400
And that's the reason I called it stability.

2249
01:37:01,400 --> 01:37:05,600
I was going to, that was one of my questions where, you know, and obviously there's, there's

2250
01:37:05,600 --> 01:37:11,640
a terminology of stability in the, in the, in the models that you're building, but stability

2251
01:37:11,640 --> 01:37:13,520
in order to help stabilize society.

2252
01:37:13,520 --> 01:37:14,520
Yes.

2253
01:37:14,520 --> 01:37:19,200
Build a human OS or catalyze a human OS because it can only be built by society.

2254
01:37:19,200 --> 01:37:20,400
We're just a capitalist.

2255
01:37:20,400 --> 01:37:21,400
Yes.

2256
01:37:21,400 --> 01:37:26,760
Help guide that in a way because this AI can be finally the thing that can stabilize a

2257
01:37:26,760 --> 01:37:30,720
complex system that is humanity and then I'll just achieve our potential.

2258
01:37:30,720 --> 01:37:35,840
That's a platform, a infrastructure platform that uplifts all of humanity.

2259
01:37:35,840 --> 01:37:36,840
Yes.

2260
01:37:36,840 --> 01:37:39,000
And again, it should be run by the people for the people.

2261
01:37:39,000 --> 01:37:42,280
So like our subsidiaries in the countries, we're putting aside 10% of the equity for

2262
01:37:42,280 --> 01:37:46,040
the kids that use the tablets and I'm never going to take a penny out of any of them until

2263
01:37:46,040 --> 01:37:48,240
IPOing them because they should be owned by the people.

2264
01:37:48,240 --> 01:37:49,240
I love that.

2265
01:37:49,240 --> 01:37:50,240
Okay.

2266
01:37:50,240 --> 01:37:58,800
I want to close this out with two, two topics around the moonshots and mindsets.

2267
01:37:58,800 --> 01:38:08,840
If I were to launch an EMAID XPRIZE fully funded and push it out to the world, what's

2268
01:38:08,840 --> 01:38:14,360
a grand challenge in XPRIZE that you would love to see materialized out there?

2269
01:38:14,360 --> 01:38:16,240
I think this education is the key one.

2270
01:38:16,240 --> 01:38:21,600
I think it's the next step of global learning, which is a grand challenge just to build this

2271
01:38:21,600 --> 01:38:23,960
system and especially for low income countries.

2272
01:38:23,960 --> 01:38:30,280
And again, the delta on the impact can be so massive on this because it's infrastructure

2273
01:38:30,280 --> 01:38:31,280
for the next generation.

2274
01:38:31,280 --> 01:38:36,400
Like a lot of these emerging markets leapfrogged straight to mobile phones to skip computers.

2275
01:38:36,400 --> 01:38:38,920
Now they can skip to the AI age.

2276
01:38:38,920 --> 01:38:39,920
How amazing will that be?

2277
01:38:39,920 --> 01:38:42,880
It'll uplift everyone.

2278
01:38:42,880 --> 01:38:49,120
And there's no greater asset to a nation, a company, anybody than the intelligence of

2279
01:38:49,120 --> 01:38:51,720
its citizens.

2280
01:38:51,720 --> 01:38:52,720
Minds are intelligent.

2281
01:38:52,720 --> 01:38:57,160
They don't have access to be able to take that intelligence and build for themselves

2282
01:38:57,160 --> 01:38:58,160
and extend that.

2283
01:38:58,160 --> 01:39:01,720
We can make that infrastructure now to do it for the first time ever.

2284
01:39:01,720 --> 01:39:02,720
Yeah.

2285
01:39:02,720 --> 01:39:03,720
Amazing.

2286
01:39:03,720 --> 01:39:04,720
Amazing.

2287
01:39:04,720 --> 01:39:07,600
What are the mindsets that have allowed you to be successful, do you think?

2288
01:39:07,600 --> 01:39:13,880
I talk about a abundance mindset, a longevity mindset, a moonshot mindset, exponential mindset,

2289
01:39:13,880 --> 01:39:14,880
curiosity mindset.

2290
01:39:14,880 --> 01:39:17,800
Do any of those resonate for you or are there other mindsets?

2291
01:39:17,800 --> 01:39:22,040
Because I think mindsets are the most important differentiator that we have.

2292
01:39:22,040 --> 01:39:26,360
So like, I've always been very lucky and I've achieved very interesting things.

2293
01:39:26,360 --> 01:39:30,880
I was never really motivated for the last few years when I finally applied myself.

2294
01:39:30,880 --> 01:39:34,680
And what I'm good at is first principles thinking on those first bits rather than atoms.

2295
01:39:34,680 --> 01:39:36,880
But I realized there's nothing I can't do.

2296
01:39:36,880 --> 01:39:40,120
There's nothing, people can do anything if they put their mind to it.

2297
01:39:40,120 --> 01:39:43,640
But they have to think in a structured way and they have to let almost water flow as

2298
01:39:43,640 --> 01:39:44,640
it were.

2299
01:39:44,640 --> 01:39:50,040
And what I just do is I try to make it a win-win for everyone to participate, to help, to extend

2300
01:39:50,040 --> 01:39:51,040
this.

2301
01:39:51,040 --> 01:39:54,680
And at a time of absolute change, you can make that happen.

2302
01:39:54,680 --> 01:39:57,800
So this is why I go to the future and I go back to the past and I work back that way.

2303
01:39:57,800 --> 01:40:00,760
Which I think a lot of people don't because they get stuck in the present.

2304
01:40:00,760 --> 01:40:06,880
So it's the moonshot mindset looking to the future and again, recursively back propagating.

2305
01:40:06,880 --> 01:40:12,160
And I'll close out with, if you were going to list the moonshots that you're working

2306
01:40:12,160 --> 01:40:13,600
on right now.

2307
01:40:13,600 --> 01:40:14,880
You're clearly working on education.

2308
01:40:14,880 --> 01:40:16,760
We've talked about that.

2309
01:40:16,760 --> 01:40:23,840
But as someone who's going to disrupt all the industries, healthcare as a moonshot?

2310
01:40:23,840 --> 01:40:26,360
I am working on everything.

2311
01:40:26,360 --> 01:40:27,360
Everything.

2312
01:40:27,360 --> 01:40:29,400
I've got about 18 different ones.

2313
01:40:29,400 --> 01:40:33,480
But education is core and creativity is core.

2314
01:40:33,480 --> 01:40:35,320
Those two enable everything else.

2315
01:40:35,320 --> 01:40:39,800
If you want to fix climate, if you want to fix hate, if you want to fix a lot of different

2316
01:40:39,800 --> 01:40:43,320
things, get those two right and that's the foundation for the future.

2317
01:40:44,040 --> 01:40:50,160
It's a pleasure, my friend, with a fun conversation, excited to share your wisdom and vision with

2318
01:40:50,160 --> 01:40:56,920
the world, excited for what you're creating as a fundamental platform and infrastructure

2319
01:40:56,920 --> 01:40:59,440
for humanity.

2320
01:40:59,440 --> 01:41:03,200
I wish you all the luck and look forward to supporting me any way I can.

2321
01:41:03,200 --> 01:41:04,200
Thank you very much.

2322
01:41:04,200 --> 01:41:05,200
Again, it's just a little catalyst.

2323
01:41:05,200 --> 01:41:07,280
It'll be everyone else that drives this forward.

2324
01:41:07,280 --> 01:41:08,760
See you in March at A360.

2325
01:41:08,760 --> 01:41:09,760
Cheers.

2326
01:41:09,760 --> 01:41:10,760
Cheers.

2327
01:41:10,760 --> 01:41:11,760
Everyone, this is Peter again.

2328
01:41:11,760 --> 01:41:15,960
Before you take off, I want to take a moment to just invite you to subscribe to my weekly

2329
01:41:15,960 --> 01:41:16,960
tech blog.

2330
01:41:16,960 --> 01:41:22,200
Today, over 200,000 people received this email twice per week.

2331
01:41:22,200 --> 01:41:26,640
In the tech blog, I share with you my insights on converging exponential technologies, what's

2332
01:41:26,640 --> 01:41:31,280
going on in AI, how longevity is transforming, adding decades to our life.

2333
01:41:31,280 --> 01:41:34,880
In the tech blog, I often look at the 20 metatrends that are going to transform the

2334
01:41:34,880 --> 01:41:40,120
decade ahead and share the conversations I've had with incredible tech thought leaders on

2335
01:41:40,120 --> 01:41:42,040
how they're transforming industries.

2336
01:41:42,040 --> 01:41:44,600
If that sounds cool to you and you want to try it, join me.

2337
01:41:44,600 --> 01:41:50,160
Go to dmandus.com backslash blog, enter your email and let's start this weekly conversation.

2338
01:41:50,160 --> 01:41:55,280
Let me share with you the incredible progress we're seeing in the world of technology and

2339
01:41:55,280 --> 01:41:57,840
the positive impact it's having on our lives.

2340
01:41:57,840 --> 01:42:02,240
Again, that's dmandus.com backslash blog.

2341
01:42:02,240 --> 01:42:06,400
Looking forward to sharing my insights and incredible breakthroughs I'm seeing with you

2342
01:42:06,400 --> 01:42:07,240
every single week.

