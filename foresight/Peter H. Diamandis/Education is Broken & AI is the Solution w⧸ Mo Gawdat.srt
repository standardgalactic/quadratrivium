1
00:00:00,000 --> 00:00:06,000
We are about to have AI reinvent global education and health care.

2
00:00:06,000 --> 00:00:09,000
This is the biggest shift in human history.

3
00:00:09,000 --> 00:00:12,000
We are at a perfect store, perfect store.

4
00:00:12,000 --> 00:00:14,000
Your mindset, our brains are neural nets.

5
00:00:14,000 --> 00:00:17,000
They're being shaped constantly by everything we watch,

6
00:00:17,000 --> 00:00:21,000
everything we read, the conversations we have, our communities and so forth.

7
00:00:21,000 --> 00:00:24,000
In the age of the rise of the machines,

8
00:00:24,000 --> 00:00:28,000
the only skill that will remain is human connection.

9
00:00:28,000 --> 00:00:30,000
We need to bring that back to our children.

10
00:00:30,000 --> 00:00:32,000
We need to teach them resilience.

11
00:00:32,000 --> 00:00:37,000
It's training them to respond to anything that happens.

12
00:00:37,000 --> 00:00:41,000
You may tell yourself, I'm about to lose my job because of AI.

13
00:00:41,000 --> 00:00:42,000
Is this true?

14
00:00:42,000 --> 00:00:43,000
Yes, it is.

15
00:00:43,000 --> 00:00:45,000
Live, because you know what?

16
00:00:45,000 --> 00:00:48,000
You have certainty that today is not a perfect storm yet.

17
00:00:48,000 --> 00:00:50,000
Think about this minute.

18
00:00:50,000 --> 00:00:53,000
How can I savor this minute fully?

19
00:00:54,000 --> 00:00:59,000
I'm going to transition the conversation, staying on kids.

20
00:00:59,000 --> 00:01:03,000
Kristen and I have two 12-year-old boys,

21
00:01:03,000 --> 00:01:08,000
and they're, you know, a joy after this podcast is over.

22
00:01:08,000 --> 00:01:12,000
I'll be going and hugging them and spending time with them for sure, extra.

23
00:01:12,000 --> 00:01:15,000
And one of the things that we're thinking about,

24
00:01:15,000 --> 00:01:18,000
because they're in sixth grade and heading towards high school,

25
00:01:18,000 --> 00:01:24,000
in a world that is very different than the world you and I grew up in.

26
00:01:24,000 --> 00:01:29,000
And I am clear that today's schooling system set aside college,

27
00:01:29,000 --> 00:01:36,000
but junior high school, high school is not preparing our kids for what's coming.

28
00:01:36,000 --> 00:01:39,000
Our school system is based on the industrial era.

29
00:01:39,000 --> 00:01:42,000
Everyone's, you know, prepared for the test.

30
00:01:42,000 --> 00:01:47,000
Everybody is going from station to station learning things like one of our boys,

31
00:01:47,000 --> 00:01:51,000
Jet, just had to memorize all 50 state capitals.

32
00:01:51,000 --> 00:01:54,000
Now, it's great to practice your memorization,

33
00:01:54,000 --> 00:01:59,000
but really, how and when are you going to ever have that be useful?

34
00:01:59,000 --> 00:02:03,000
You know, especially when Google is still a dominant force.

35
00:02:03,000 --> 00:02:07,000
So the question I have, and for those who are listeners,

36
00:02:07,000 --> 00:02:11,000
our parents is what should our kids be learning?

37
00:02:11,000 --> 00:02:16,000
You know, this is coming back smack into the conversation around the rise

38
00:02:16,000 --> 00:02:24,000
of AI and what do we need to teach our kids to prepare them for this decade ahead?

39
00:02:24,000 --> 00:02:28,000
And I have a list that I've come up with.

40
00:02:28,000 --> 00:02:34,000
And then I went and asked, you know, chat GPT, the same question and similar,

41
00:02:34,000 --> 00:02:36,000
but slightly different list.

42
00:02:36,000 --> 00:02:43,000
But I'd like to ask you in a world, let's call it, you know, we're 2023.

43
00:02:44,000 --> 00:02:50,000
We're going about to see AI become, what's your, well, I'll ask you this, you know,

44
00:02:50,000 --> 00:02:56,000
Ray Kurzweil, who we both know well, has predicted human level AI by 2029.

45
00:02:56,000 --> 00:03:01,000
There are others who are predicting it far before then, by the way, right?

46
00:03:01,000 --> 00:03:06,000
There are those that are looking at, you know, 2025 and between 2025 and 29.

47
00:03:06,000 --> 00:03:12,000
So what should our kids be learning in a world of human level AI?

48
00:03:12,000 --> 00:03:16,000
By the way, embedded in everything where everything is smart all the time.

49
00:03:16,000 --> 00:03:18,000
What are the skills?

50
00:03:18,000 --> 00:03:20,000
Amazing question.

51
00:03:20,000 --> 00:03:24,000
And probably the most difficult question to ask now, not for just, I mean,

52
00:03:24,000 --> 00:03:26,000
not just for our kids, but for everyone.

53
00:03:26,000 --> 00:03:29,000
I mean, if you're a graphics designer today or if you're a programmer,

54
00:03:29,000 --> 00:03:34,000
the episode with Ahmed, with Ahmed Mushtaq was amazing, Peter.

55
00:03:34,000 --> 00:03:36,000
It was an amazing conversation.

56
00:03:36,000 --> 00:03:39,000
People who haven't listened to it should go back and listen.

57
00:03:39,000 --> 00:03:44,000
You know, the world is fundamentally changing in ways that we could not understand.

58
00:03:44,000 --> 00:03:50,000
I think I will say there are four things that need to be educated.

59
00:03:50,000 --> 00:03:56,000
Okay. None of them have anything to do with what we, our school systems are teaching.

60
00:03:56,000 --> 00:04:00,000
The first and very straightforward one is jump into AI.

61
00:04:00,000 --> 00:04:02,000
Okay. You have to learn this.

62
00:04:02,000 --> 00:04:05,000
This is the biggest shift in human history.

63
00:04:05,000 --> 00:04:12,000
This is the biggest change in the tools that we use to navigate the world.

64
00:04:12,000 --> 00:04:13,000
Right?

65
00:04:13,000 --> 00:04:19,000
If I told you that we now have robotic arms that are going to be building cars,

66
00:04:19,000 --> 00:04:24,000
you shouldn't spend any time in learning how to use a hammer to shape matter.

67
00:04:24,000 --> 00:04:25,000
Okay.

68
00:04:25,000 --> 00:04:27,000
So learn how the robotic arm works.

69
00:04:27,000 --> 00:04:29,000
The new robotic arm is AI.

70
00:04:29,000 --> 00:04:32,000
So jump in AI, understand every part of it.

71
00:04:32,000 --> 00:04:34,000
You don't need a school to do that.

72
00:04:34,000 --> 00:04:38,000
You know, because I speak about AI quite a bit, a lot of people ask me,

73
00:04:38,000 --> 00:04:42,000
so which book do you recommend I read or which course should I take?

74
00:04:42,000 --> 00:04:44,000
And I go, like, go ask Chad GPT that.

75
00:04:44,000 --> 00:04:45,000
Right?

76
00:04:45,000 --> 00:04:46,000
Why are you asking me?

77
00:04:46,000 --> 00:04:47,000
And seriously, right?

78
00:04:47,000 --> 00:04:50,000
And I think the idea is, like the other two,

79
00:04:50,000 --> 00:04:55,000
very few people actually went to courses on Excel or, you know, email.

80
00:04:55,000 --> 00:04:56,000
You play.

81
00:04:56,000 --> 00:04:57,000
You play.

82
00:04:57,000 --> 00:04:59,000
Yeah, you play around and you learn.

83
00:04:59,000 --> 00:05:01,000
So this is number one.

84
00:05:01,000 --> 00:05:07,000
Number two is I would say that in the age of the rise of the machines,

85
00:05:07,000 --> 00:05:12,000
the only skill that will remain is human connection.

86
00:05:12,000 --> 00:05:20,000
The only skill in my personal view that will be valuable when the machines

87
00:05:20,000 --> 00:05:22,000
outsmart us in everything.

88
00:05:22,000 --> 00:05:24,000
By the way, it doesn't matter.

89
00:05:24,000 --> 00:05:26,000
I always want to bring this back.

90
00:05:26,000 --> 00:05:27,000
I'm sorry.

91
00:05:27,000 --> 00:05:29,000
I'm going to go back to number two in a second,

92
00:05:29,000 --> 00:05:37,000
but it doesn't matter if AGI comes out and we have the singularity as per

93
00:05:37,000 --> 00:05:40,000
Ray's definition, 2029.

94
00:05:40,000 --> 00:05:45,000
I actually definitely believe AGI will be earlier than 2029, but it doesn't

95
00:05:45,000 --> 00:05:46,000
matter.

96
00:05:46,000 --> 00:05:51,000
What matters is if you're a graphics designer and stability AI, you know,

97
00:05:51,000 --> 00:05:54,000
produced stable diffusion, you're screwed.

98
00:05:54,000 --> 00:05:55,000
It's over.

99
00:05:55,000 --> 00:05:56,000
Okay.

100
00:05:56,000 --> 00:05:58,000
We don't need AGI for your life to be disrupted heavily.

101
00:05:58,000 --> 00:06:02,000
We don't need AGI to replace lawyers.

102
00:06:02,000 --> 00:06:06,000
We don't need AGI to augment doctors and so on and so forth.

103
00:06:06,000 --> 00:06:07,000
This is in the making.

104
00:06:07,000 --> 00:06:09,000
It's happening very, very fast.

105
00:06:09,000 --> 00:06:14,000
So don't wait for that imaginary moment or that future moment to say,

106
00:06:14,000 --> 00:06:15,000
ah, now we're in shit.

107
00:06:15,000 --> 00:06:18,000
We're approaching those situations already.

108
00:06:18,000 --> 00:06:23,000
So when all of that happens and all of those skills are replaced with humans,

109
00:06:23,000 --> 00:06:28,000
I would say that the most valuable skill is human connection.

110
00:06:28,000 --> 00:06:30,000
So take my situation.

111
00:06:30,000 --> 00:06:32,000
I am an author.

112
00:06:32,000 --> 00:06:33,000
Okay.

113
00:06:33,000 --> 00:06:37,000
And a speaker and in a way a teacher if you want.

114
00:06:37,000 --> 00:06:38,000
Okay.

115
00:06:38,000 --> 00:06:40,000
And a podcaster.

116
00:06:40,000 --> 00:06:41,000
Right.

117
00:06:41,000 --> 00:06:46,000
So as an author, I don't think I should write books anymore.

118
00:06:46,000 --> 00:06:47,000
Okay.

119
00:06:47,000 --> 00:06:50,000
I take too long to write them.

120
00:06:50,000 --> 00:06:52,000
The industry is very slow to publish them.

121
00:06:52,000 --> 00:06:57,000
And the supply demand equation of books is going to be disrupted so badly

122
00:06:57,000 --> 00:07:00,000
because books now can be written in 15 minutes.

123
00:07:00,000 --> 00:07:01,000
Okay.

124
00:07:01,000 --> 00:07:04,000
That there will be so much supply on the market that by definition,

125
00:07:04,000 --> 00:07:07,000
the dilution will affect the value of any book.

126
00:07:07,000 --> 00:07:11,000
And in all honesty, I don't assume I'm smarter than Chad GPT.

127
00:07:11,000 --> 00:07:14,000
I may be a little more genuine than Chad GPT.

128
00:07:14,000 --> 00:07:17,000
I may be a little more original than Chad GPT.

129
00:07:17,000 --> 00:07:19,000
At least for the moment.

130
00:07:19,000 --> 00:07:20,000
Yeah.

131
00:07:20,000 --> 00:07:24,000
And some of my writing, but that advantage is not going to last for the next two to three

132
00:07:24,000 --> 00:07:25,000
years.

133
00:07:25,000 --> 00:07:31,000
So, but if I know a concept and I have a chance to talk to you as a human Peter,

134
00:07:31,000 --> 00:07:34,000
I don't think the machines will reach that within the next four to five years.

135
00:07:34,000 --> 00:07:35,000
Right.

136
00:07:35,000 --> 00:07:40,000
So human connection, even if the machines reach that in four to five years is what we will

137
00:07:40,000 --> 00:07:42,000
crave as humans.

138
00:07:42,000 --> 00:07:43,000
Right.

139
00:07:43,000 --> 00:07:47,000
There will be sex robots, but I can guarantee you will still need a hug.

140
00:07:47,000 --> 00:07:48,000
Yes.

141
00:07:48,000 --> 00:07:55,000
So there will be, you know, articles written by there are tons of articles written by AI

142
00:07:55,000 --> 00:07:56,000
today.

143
00:07:56,000 --> 00:08:00,000
There is a ton of code written by AI today, but there is always going to be that need

144
00:08:00,000 --> 00:08:05,000
for a human to consult human to, you know, a circle of support if you want.

145
00:08:05,000 --> 00:08:07,000
So that's skill number two.

146
00:08:07,000 --> 00:08:11,000
Skill number two is learn how to connect to humans at a deeper level.

147
00:08:11,000 --> 00:08:19,000
It's a skill that has been declining since the rise of social media.

148
00:08:19,000 --> 00:08:27,000
In the words are the skills that humans have to focus on connection and deliver and maximize

149
00:08:27,000 --> 00:08:28,000
connection.

150
00:08:28,000 --> 00:08:35,000
But we also spoke in our last podcast mode that AI will begin to emulate human connection.

151
00:08:35,000 --> 00:08:36,000
Correct.

152
00:08:36,000 --> 00:08:40,000
And your timeframe for that was within the next 10 years.

153
00:08:40,000 --> 00:08:42,000
Oh, if not shorter.

154
00:08:42,000 --> 00:08:43,000
Yeah.

155
00:08:43,000 --> 00:08:50,000
I mean, in all, in all honesty, I think it will become very confusing within the next

156
00:08:50,000 --> 00:08:51,000
three.

157
00:08:51,000 --> 00:08:53,000
If you remember the movie her.

158
00:08:53,000 --> 00:08:55,000
I remember it well.

159
00:08:55,000 --> 00:08:56,000
Yeah.

160
00:08:56,000 --> 00:09:03,500
Her basically an AI simulates a friend and then she becomes quite, you know, he becomes

161
00:09:03,500 --> 00:09:06,000
very occupied with loving her as an AI.

162
00:09:06,000 --> 00:09:07,000
Okay.

163
00:09:07,000 --> 00:09:09,000
It's an interesting, it's a lovely story.

164
00:09:09,000 --> 00:09:16,000
It's one of the most, most positive and perhaps realistic versions of AI that I've seen out

165
00:09:16,000 --> 00:09:17,000
there.

166
00:09:17,000 --> 00:09:18,000
If you haven't seen her, please go do it.

167
00:09:18,000 --> 00:09:20,000
It's a beautiful construct.

168
00:09:20,000 --> 00:09:23,000
And at the end of the movie, no one spoil it.

169
00:09:23,000 --> 00:09:24,000
Anyway.

170
00:09:24,000 --> 00:09:25,000
Yeah.

171
00:09:25,000 --> 00:09:29,000
But the whole concept of us connecting to machines.

172
00:09:29,000 --> 00:09:33,000
I don't see anything wrong with that, by the way.

173
00:09:33,000 --> 00:09:38,000
I love the idea of being able to connect to a body that can help me learn things and so

174
00:09:38,000 --> 00:09:39,000
on.

175
00:09:39,000 --> 00:09:40,000
It's wonderful.

176
00:09:40,000 --> 00:09:45,000
You know, I think this will shift deeply into erotic experiences and emotional experiences

177
00:09:45,000 --> 00:09:51,000
and romantic experiences where so many people are lonely and they'll use this to replace

178
00:09:51,000 --> 00:09:54,000
an annoying girlfriend or an annoying boyfriend or whatever.

179
00:09:54,000 --> 00:09:55,000
Sadly.

180
00:09:55,000 --> 00:09:56,000
Right.

181
00:09:56,000 --> 00:09:59,000
But I still believe there will be a space for human connection.

182
00:09:59,000 --> 00:10:00,000
Okay.

183
00:10:00,000 --> 00:10:05,000
There will be a space where genuine human connection, the kind that you feel when you meet an old

184
00:10:05,000 --> 00:10:07,000
friend is still going to be very valuable.

185
00:10:07,000 --> 00:10:12,000
And I, and I, as I said, I think this is a skill that's been eroding over the last 10-15

186
00:10:12,000 --> 00:10:13,000
years because of social media.

187
00:10:13,000 --> 00:10:15,000
We need to bring that back to our children.

188
00:10:15,000 --> 00:10:17,000
So that's number two on your list.

189
00:10:17,000 --> 00:10:18,000
Yeah.

190
00:10:18,000 --> 00:10:19,000
That was number two on your list.

191
00:10:19,000 --> 00:10:25,000
Number three is, is sad when I say it, but we need to teach them resilience.

192
00:10:25,000 --> 00:10:26,000
Okay.

193
00:10:26,000 --> 00:10:29,000
And resilience, not just because of AI as a matter of fact.

194
00:10:29,000 --> 00:10:33,000
As I said, I don't know if we said that last time, but I say it repeatedly.

195
00:10:33,000 --> 00:10:36,000
We are at a perfect store.

196
00:10:36,000 --> 00:10:47,000
I mean, if you just think about the world finances and economics, there's never been a better

197
00:10:47,000 --> 00:10:51,000
match in history to the times of the Great Depression than today.

198
00:10:51,000 --> 00:10:52,000
Okay.

199
00:10:52,000 --> 00:10:59,000
So it seems, if you know anything about economics, there is a very deep economic crisis ahead

200
00:10:59,000 --> 00:11:00,000
of us.

201
00:11:00,000 --> 00:11:01,000
Okay.

202
00:11:01,000 --> 00:11:07,000
There is a geopolitical layer on top of the economic crisis that might extenuate it.

203
00:11:07,000 --> 00:11:08,000
Okay.

204
00:11:08,000 --> 00:11:15,000
There is a climate crisis that we're so far only seeing the tip of the iceberg of, you

205
00:11:15,000 --> 00:11:19,000
know, three million people being displaced in Pakistan last year, the Australian fires

206
00:11:19,000 --> 00:11:20,000
and so on and so forth.

207
00:11:20,000 --> 00:11:26,000
When, when in reality, you, you know, anyone with a reasonable ability to look at trends

208
00:11:26,000 --> 00:11:29,000
will tell you, we may get more of these.

209
00:11:29,000 --> 00:11:30,000
Okay.

210
00:11:30,000 --> 00:11:40,000
So resilience seems to me to be a very useful skill and resilience is not training them

211
00:11:40,000 --> 00:11:43,000
what to do when certain things happen.

212
00:11:43,000 --> 00:11:49,000
It's training them to respond to anything that happens.

213
00:11:49,000 --> 00:11:53,000
The happiness equation is the core of that.

214
00:11:53,000 --> 00:12:00,000
And when you think about, I'll just take a couple of minutes to digress a little bit

215
00:12:00,000 --> 00:12:04,000
when you think that happiness is events minus expectations.

216
00:12:04,000 --> 00:12:05,000
Okay.

217
00:12:05,000 --> 00:12:11,000
Then unhappiness is or any negative emotion is when an event misses your expectations.

218
00:12:11,000 --> 00:12:12,000
Okay.

219
00:12:12,000 --> 00:12:17,000
And when an event misses your expectations, your brain sounds the alarm.

220
00:12:17,000 --> 00:12:18,000
Okay.

221
00:12:18,000 --> 00:12:19,000
Oh, he said this.

222
00:12:19,000 --> 00:12:20,500
She, you know, he doesn't love me anymore.

223
00:12:20,500 --> 00:12:22,000
What I think is going right.

224
00:12:22,000 --> 00:12:23,000
Something's going wrong.

225
00:12:23,000 --> 00:12:26,000
Now, we may think of that feeling as an annoying feeling.

226
00:12:26,000 --> 00:12:31,000
It is your brain is sounding the alarm because it is an alarm.

227
00:12:31,000 --> 00:12:35,000
Unhappiness and all of its derivatives is a survival mechanism.

228
00:12:35,000 --> 00:12:36,000
Okay.

229
00:12:36,000 --> 00:12:42,000
It's your brain telling you something in the surrounding environment does not match my

230
00:12:42,000 --> 00:12:46,000
worldview of an optimum scenario for my survival and thriving.

231
00:12:46,000 --> 00:12:47,000
Okay.

232
00:12:47,000 --> 00:12:51,000
And so what happens is your brain is simply saying, we need to do something about this.

233
00:12:51,000 --> 00:12:57,000
Now, resilience in my view is to actually do something about it.

234
00:12:57,000 --> 00:12:58,000
Right.

235
00:12:58,000 --> 00:13:02,000
The resilience in my view is to take it and go through three questions.

236
00:13:02,000 --> 00:13:04,000
I call them the happiness flow chart.

237
00:13:04,000 --> 00:13:05,000
Okay.

238
00:13:05,000 --> 00:13:08,000
Question number one is, is what is what my brain telling me true?

239
00:13:08,000 --> 00:13:09,000
Yes.

240
00:13:09,000 --> 00:13:10,000
Okay.

241
00:13:10,000 --> 00:13:14,000
Because 90% of the time it isn't your brain is telling you what it thinks is true, but

242
00:13:14,000 --> 00:13:15,000
it isn't always the truth.

243
00:13:15,000 --> 00:13:19,000
You may have an argument with your son and then you will say, oh, my son doesn't love

244
00:13:19,000 --> 00:13:20,000
me anymore.

245
00:13:20,000 --> 00:13:23,000
That's, that's, these are two different events.

246
00:13:23,000 --> 00:13:27,000
The event that your brain is telling you is influenced by your fears and your conditioning

247
00:13:27,000 --> 00:13:28,000
and so on and so forth.

248
00:13:28,000 --> 00:13:29,000
Right.

249
00:13:29,000 --> 00:13:31,000
While the actual event is I had an argument with my son.

250
00:13:31,000 --> 00:13:32,000
Right.

251
00:13:32,000 --> 00:13:34,000
So the first question is, is it true?

252
00:13:34,000 --> 00:13:35,000
Okay.

253
00:13:35,000 --> 00:13:38,000
The second question, if it's true, if it's not true, by the way, drop it.

254
00:13:38,000 --> 00:13:42,000
Don't be unhappy about anything that doesn't make you, that is not even true.

255
00:13:42,000 --> 00:13:43,000
Right.

256
00:13:43,000 --> 00:13:47,000
If it is true, the second question is what can I do to fix it?

257
00:13:47,000 --> 00:13:48,000
Okay.

258
00:13:48,000 --> 00:13:49,000
What can I do to fix it?

259
00:13:49,000 --> 00:13:56,000
You know, in today's world, you may tell yourself, I'm about to lose my job because of AI.

260
00:13:56,000 --> 00:13:58,000
Is this true?

261
00:13:58,000 --> 00:13:59,000
Yes, it is.

262
00:13:59,000 --> 00:14:00,000
Okay.

263
00:14:00,000 --> 00:14:04,000
If it is, what can I do to fix that situation?

264
00:14:04,000 --> 00:14:07,000
I can upskill and find another job.

265
00:14:07,000 --> 00:14:08,000
Okay.

266
00:14:08,000 --> 00:14:13,720
I can downscale and move to Dominican Republic and live for a fraction of what I'm spending

267
00:14:13,720 --> 00:14:14,720
in the US.

268
00:14:14,720 --> 00:14:15,720
Right.

269
00:14:15,720 --> 00:14:16,720
Whatever.

270
00:14:16,720 --> 00:14:18,800
That's something you can do to fix that situation.

271
00:14:18,800 --> 00:14:20,440
This is where resilience comes in.

272
00:14:20,440 --> 00:14:21,440
Yes.

273
00:14:21,440 --> 00:14:22,440
Right.

274
00:14:22,440 --> 00:14:23,440
That's exactly what it is.

275
00:14:23,440 --> 00:14:27,600
Now, the third is the Jedi master level of happiness.

276
00:14:27,600 --> 00:14:30,240
The Jedi master level of happiness is quite interesting.

277
00:14:30,240 --> 00:14:36,080
It basically, there are things that you can, that you are faced with that you cannot do

278
00:14:36,080 --> 00:14:37,480
anything to fix.

279
00:14:37,480 --> 00:14:38,480
Right.

280
00:14:38,480 --> 00:14:43,200
So if the answer to the question, what can I do to fix it is nothing like there is an

281
00:14:43,200 --> 00:14:45,040
economic crisis coming.

282
00:14:45,040 --> 00:14:46,040
Okay.

283
00:14:46,040 --> 00:14:48,640
If, you know, what can I do to fix that?

284
00:14:48,640 --> 00:14:52,280
There's nothing within your own power as an individual to stop an economic crisis.

285
00:14:52,280 --> 00:14:53,280
Okay.

286
00:14:53,280 --> 00:14:57,520
So the following question, which I think is the Jedi master level of happiness, the

287
00:14:57,520 --> 00:15:03,360
Jedi master level of resilience is, can I accept it and do something to make my life

288
00:15:03,360 --> 00:15:04,840
better despite its presence?

289
00:15:04,840 --> 00:15:05,840
Okay.

290
00:15:05,840 --> 00:15:08,160
Can I, can I simply tell myself, this is it.

291
00:15:08,160 --> 00:15:12,000
I can't fix an economic crisis coming, but I can take care of my kids and my loved ones

292
00:15:12,000 --> 00:15:15,040
and change my spending habits and sell my car and do this and that.

293
00:15:15,040 --> 00:15:16,040
Okay.

294
00:15:16,040 --> 00:15:19,360
It doesn't make my life amazing, but it makes it better within the current circumstances

295
00:15:19,360 --> 00:15:20,360
that I accepted.

296
00:15:20,360 --> 00:15:21,360
Okay.

297
00:15:21,360 --> 00:15:24,400
So that to me is resilience and happiness combined.

298
00:15:24,400 --> 00:15:25,400
Those three questions.

299
00:15:25,400 --> 00:15:32,680
And I think we need to teach our kids by example to show that level of resilience.

300
00:15:32,680 --> 00:15:35,840
When life doesn't go their way, they need to go, is it true?

301
00:15:35,840 --> 00:15:37,240
Can I do something to fix it?

302
00:15:37,240 --> 00:15:40,040
Can I accept it and make my life better despite its presence?

303
00:15:40,040 --> 00:15:41,040
Hey everybody, this is Peter.

304
00:15:41,040 --> 00:15:42,440
A quick break from the episode.

305
00:15:42,440 --> 00:15:47,360
Now I'm a firm believer that science and technology and how entrepreneurs can change

306
00:15:47,360 --> 00:15:51,960
the world is the only real news out there worth consuming.

307
00:15:51,960 --> 00:15:54,120
I don't watch the crisis news network.

308
00:15:54,120 --> 00:15:58,800
I call CNN or Fox and hear every devastating piece of news on the planet.

309
00:15:58,800 --> 00:16:05,080
I spend my time training my neural net the way I see the world by looking at the incredible

310
00:16:05,080 --> 00:16:09,040
breakthroughs in science and technology and how entrepreneurs are solving the world's

311
00:16:09,040 --> 00:16:10,520
grand challenges.

312
00:16:10,520 --> 00:16:16,000
What the breakthroughs are in longevity, how exponential technologies are transforming

313
00:16:16,000 --> 00:16:17,200
our world.

314
00:16:17,200 --> 00:16:19,480
So twice a week, I put out a blog.

315
00:16:19,480 --> 00:16:26,440
One blog is looking at the future of longevity, age reversal, biotech, increasing your health

316
00:16:26,440 --> 00:16:27,440
span.

317
00:16:27,440 --> 00:16:33,840
The other blog looks at exponential technologies, AI, 3D printing, synthetic biology, AR, VR,

318
00:16:33,840 --> 00:16:34,840
blockchain.

319
00:16:34,840 --> 00:16:37,800
These technologies are transforming what you as an entrepreneur can do.

320
00:16:37,800 --> 00:16:41,720
If this is the kind of news you want to learn about and shape your neural nets with, go

321
00:16:41,720 --> 00:16:46,280
to dmandis.com backslash blog and learn more.

322
00:16:46,280 --> 00:16:47,800
Now back to the episode.

323
00:16:47,800 --> 00:16:52,720
This comes back to something I speak about a lot and I'm passionate about which is mindsets.

324
00:16:52,720 --> 00:16:54,800
Your mindset, our brains are neural nets.

325
00:16:54,800 --> 00:16:59,840
They're being shaped constantly by everything we watch, everything we read, the conversations

326
00:16:59,840 --> 00:17:02,240
we have, our communities and so forth.

327
00:17:02,240 --> 00:17:05,800
And you can have an abundance mindset or scarcity mindset.

328
00:17:05,800 --> 00:17:09,480
You can have an optimism mindset or a fear mindset.

329
00:17:09,480 --> 00:17:13,640
And just to hit on this a little bit, you know, you said we're in a perfect storm coming

330
00:17:13,640 --> 00:17:20,480
of these problems, but the flip side of that as well is, and we have incredible tools more

331
00:17:20,480 --> 00:17:21,480
than ever before.

332
00:17:21,480 --> 00:17:22,480
Yes, incredible opportunities.

333
00:17:22,480 --> 00:17:23,560
That's what rise of it as well.

334
00:17:23,560 --> 00:17:29,000
We are about to have AI reinvent global education and healthcare, right?

335
00:17:29,000 --> 00:17:34,640
We're going to enable so much that we could not, the challenges of humanity to be addressed

336
00:17:34,640 --> 00:17:35,640
and solved.

337
00:17:36,480 --> 00:17:41,840
Despite the perfect storm on the negative side, we do have the ability for people to

338
00:17:41,840 --> 00:17:43,760
uplift humanity, right?

339
00:17:43,760 --> 00:17:52,000
We are going to see, you know, generative AI and AGI help us with curing cancer and low

340
00:17:52,000 --> 00:17:54,480
cost fusion energy and all of these things.

341
00:17:54,480 --> 00:17:58,800
And it's the balance, it's the potential.

342
00:17:58,800 --> 00:18:00,880
Your fourth item.

343
00:18:00,880 --> 00:18:09,160
My fourth item is going to be difficult to explain, but I will try as much as I can.

344
00:18:09,160 --> 00:18:17,240
Our brains bias at looking at difficult situations and perfect storms is biased to make it look

345
00:18:17,240 --> 00:18:23,000
like this is more uncertain than any other moment in your life.

346
00:18:23,000 --> 00:18:29,080
So you look at the economy changing and AI rising and climate change and the geopolitical

347
00:18:29,080 --> 00:18:30,080
situation.

348
00:18:30,080 --> 00:18:33,760
You say, this is very uncertain, okay?

349
00:18:33,760 --> 00:18:37,360
Something is likely going to go wrong, okay?

350
00:18:37,360 --> 00:18:42,800
The truth is someone like me who loses his child all of a sudden to preventable medical

351
00:18:42,800 --> 00:18:51,440
error at a moment of his prime realizes that this actually is the case of life all the

352
00:18:51,440 --> 00:18:52,440
time.

353
00:18:52,440 --> 00:18:53,440
No guarantees.

354
00:18:53,440 --> 00:18:56,480
But we're always in a perfect storm, right?

355
00:18:56,480 --> 00:19:03,440
That we are always, as I said earlier, I have no guarantee whatsoever that I will ever

356
00:19:03,440 --> 00:19:05,560
see you again, Peter, okay?

357
00:19:05,560 --> 00:19:09,040
I might leave the world within two minutes from now.

358
00:19:09,040 --> 00:19:12,040
And by the way, it happens all the time.

359
00:19:12,040 --> 00:19:21,560
It happens all the time that the world surprises us as a tidal wave or an erupted volcano or

360
00:19:21,560 --> 00:19:26,080
someone that you loved so much that you hugged and played tennis with, you know, leaves our

361
00:19:26,080 --> 00:19:29,920
world an hour later or whatever, okay?

362
00:19:29,920 --> 00:19:32,360
And that's the reality of the video game.

363
00:19:32,360 --> 00:19:38,880
The reality of the video game is there is only now.

364
00:19:38,880 --> 00:19:44,160
Any real video gamer, I'm a very serious video gamer, by the way, for those of you listening,

365
00:19:44,160 --> 00:19:48,120
the one that killed you yesterday is me, okay?

366
00:19:48,120 --> 00:19:49,120
Simple as that, right?

367
00:19:49,120 --> 00:19:54,160
But every real video gamer, by the way, I'm not addicted at all.

368
00:19:54,160 --> 00:19:59,240
I play 45 minutes a day, four times a week, but I'm Olympic champion level, okay?

369
00:19:59,240 --> 00:20:08,120
And I will tell you, real gamers, real gamers understand that the game has no state a minute

370
00:20:08,120 --> 00:20:10,360
later, okay?

371
00:20:10,360 --> 00:20:14,480
When a minute later is rendered on the screen, I will deal with it.

372
00:20:14,480 --> 00:20:18,800
And because I'm a very good gamer, I know I'll be able to deal with any challenge that

373
00:20:18,800 --> 00:20:20,840
comes a minute later, right?

374
00:20:20,840 --> 00:20:25,920
My challenge is this minute, and my joy is this minute.

375
00:20:25,920 --> 00:20:28,000
Being present now, yeah.

376
00:20:28,000 --> 00:20:30,400
Right here, right now.

377
00:20:30,400 --> 00:20:38,400
If life is in a perfect storm, I tell people, live, live, because you know what?

378
00:20:38,400 --> 00:20:41,960
You have certainty that today is not a perfect storm yet.

379
00:20:41,960 --> 00:20:46,080
You have certainty that your kids are wonderful in your arms today.

380
00:20:46,080 --> 00:20:48,480
You have certainty that you're going to have dinner tonight.

381
00:20:48,480 --> 00:20:52,320
You have certainty that your battery on your phone is going to last another five minutes

382
00:20:52,320 --> 00:20:55,440
to hopefully finish our conversation, okay?

383
00:20:55,440 --> 00:20:59,920
When you have those certainties, don't think about what's going to happen when the battery

384
00:20:59,920 --> 00:21:01,280
runs out.

385
00:21:01,280 --> 00:21:02,480
Think about this minute.

386
00:21:02,480 --> 00:21:06,840
How can I savor this minute fully, okay?

387
00:21:06,840 --> 00:21:11,920
And I speak sometimes about, again, you don't have to be spiritual to think of those things,

388
00:21:11,920 --> 00:21:15,240
but I studied a ton of spiritual techniques, okay?

389
00:21:15,240 --> 00:21:20,080
A ton of spiritual teachings, almost everything I could get my hands on.

390
00:21:20,080 --> 00:21:24,200
From the Kabbalah, to Islam, to Sufism, to Hinduism, to Buddhism, anything I could find

391
00:21:24,200 --> 00:21:25,200
my hands on.

392
00:21:25,200 --> 00:21:26,200
And I put my hands on.

393
00:21:26,200 --> 00:21:31,240
And I'll tell you openly, every one of them is beautifully full of gold nuggets and beautifully

394
00:21:31,240 --> 00:21:33,400
full of crap, okay?

395
00:21:33,400 --> 00:21:38,360
And it's quite interesting how some people will say, oh, here is a piece of crap, I'm

396
00:21:38,360 --> 00:21:43,680
going to drop the whole book, while in reality, I go like, here is a piece of gold, I'm going

397
00:21:43,680 --> 00:21:46,280
to look for another one, okay?

398
00:21:46,280 --> 00:21:49,440
The most beautiful of all of them is what Sufism is.

399
00:21:49,440 --> 00:21:55,400
Sufism is a sect of Islam originally, but it's now part of a lot of spiritual beliefs

400
00:21:55,400 --> 00:22:02,960
where basically they say that the way to live fully is to die before you die, okay?

401
00:22:02,960 --> 00:22:06,080
And to die before you die is a very, very interesting definition.

402
00:22:06,080 --> 00:22:13,800
It basically assumes that living is a process of associating with the physical, associating

403
00:22:13,800 --> 00:22:20,000
with your car, with your ego, with your t-shirt, with your, you know, whatever.

404
00:22:20,000 --> 00:22:25,560
And that death is the moment of disconnection with everything physically, okay?

405
00:22:25,560 --> 00:22:32,640
And to die before you die is to be alive fully, fully engaged in the game, but not give a

406
00:22:32,640 --> 00:22:36,720
shit about losing any of it, okay?

407
00:22:36,720 --> 00:22:42,080
And that's a very, again, a video gamer's mentality is to say, I'm going to do this

408
00:22:42,080 --> 00:22:44,560
bit, I'm going to do it the best that I can.

409
00:22:44,560 --> 00:22:46,200
I'm playing football.

410
00:22:46,200 --> 00:22:48,120
I'm fully out there, right?

411
00:22:48,120 --> 00:22:53,280
And I'm going to enjoy it fully, but I'm not attached to the result, because the result

412
00:22:53,280 --> 00:22:54,880
is bigger than me.

413
00:22:54,880 --> 00:23:00,520
And what I think people need to do in the world we live in today is to learn to live.

414
00:23:00,520 --> 00:23:07,720
Because when we spoke last time in the AI episode, I told you openly, it is game over

415
00:23:07,720 --> 00:23:09,200
for our way of life.

416
00:23:09,200 --> 00:23:14,720
The way we live today, five years from now, is going to be nonexistent.

417
00:23:14,720 --> 00:23:17,400
How will it change better or worse?

418
00:23:17,400 --> 00:23:20,800
That's not the topic, but it's not going to be like it is right now.

419
00:23:20,800 --> 00:23:25,960
So when we are here right now and you love this way of life, enjoy the hell out of it.

420
00:23:25,960 --> 00:23:32,600
Live it fully, connect to it fully, play the game and enjoy the fun, and prepare yourself

421
00:23:32,600 --> 00:23:37,080
by detaching from things that might be taken away from you, okay?

422
00:23:37,080 --> 00:23:40,800
Prepare for yourself to get other things that might be given to you.

423
00:23:40,800 --> 00:23:45,280
And that kind of flow in life, I know sounds very philosophical.

424
00:23:45,280 --> 00:23:48,680
In a world where we're talking economics and politics and so on.

425
00:23:48,680 --> 00:23:54,960
But I can tell you that real gamers, when they die, you know, the reason why we gamers

426
00:23:54,960 --> 00:23:59,560
play really well is because we have multiple lives in the game, right?

427
00:23:59,560 --> 00:24:03,120
So I'm not saying this from a physical, from a spiritual point of view, but I'm saying

428
00:24:03,120 --> 00:24:07,840
when you don't care if you're killed in the game, you play full out, okay?

429
00:24:07,840 --> 00:24:09,080
And I think that's the idea.

430
00:24:09,080 --> 00:24:15,200
The idea is, yes, the future might hold some challenges, learn to live.

431
00:24:15,200 --> 00:24:19,640
And if you learn to live, if you learn to flow, if you learn to savor those beautiful

432
00:24:19,640 --> 00:24:22,920
moments we have right now, okay?

433
00:24:23,280 --> 00:24:25,720
What happens tomorrow doesn't really matter.

434
00:24:25,720 --> 00:24:31,800
When tomorrow comes, if you're prepared through that, the three points I said earlier, if

435
00:24:31,800 --> 00:24:35,240
you're prepared, hopefully you'll be able to overcome that too.

436
00:24:35,240 --> 00:24:38,680
It's interesting because none of those things are what we teach in school today to bring

437
00:24:38,680 --> 00:24:40,160
it back to the original question.

438
00:24:40,160 --> 00:24:48,040
I'm going to share with you my list that I wrote down and then maybe chat GPT's list.

439
00:24:48,040 --> 00:24:49,040
Because I think about this.

440
00:24:49,040 --> 00:24:59,160
I think about do we need to reinvent it because I'm clear that the momentum that our school

441
00:24:59,160 --> 00:25:07,240
systems have is not going to change until it's forced to change.

442
00:25:07,240 --> 00:25:13,120
So what I put on my list is helping our kids, first off, find their purpose and passion.

443
00:25:13,120 --> 00:25:17,320
And there's a distinction between purpose and passion, but it is something that I found

444
00:25:17,320 --> 00:25:21,200
my purpose and passion early on from Apollo and Star Trek, right?

445
00:25:21,200 --> 00:25:22,200
It was opening up space.

446
00:25:22,200 --> 00:25:23,200
Yes.

447
00:25:23,200 --> 00:25:24,560
It lit my heart on fire.

448
00:25:24,560 --> 00:25:31,680
Everything I've ever done is a result of that passion, that dream, to learn on my own.

449
00:25:31,680 --> 00:25:35,840
But it can be anything, career out of anything these days, but what is it that lights you

450
00:25:35,840 --> 00:25:38,960
on fire and makes you happy and gives you joy?

451
00:25:38,960 --> 00:25:47,160
The second thing I put down was the combination of debate, leadership, and asking great questions.

452
00:25:48,160 --> 00:25:52,600
When I drop my kids off at school every day, I say, ask great questions, right?

453
00:25:52,600 --> 00:26:00,800
Today, it's about prompting chat GPT properly, but it's the questions we ask.

454
00:26:00,800 --> 00:26:04,720
I give this advice to CEOs at the same time, ask great questions, and it's the questions

455
00:26:04,720 --> 00:26:10,840
that you've never allowed yourself to ask as a child or as a leader.

456
00:26:10,920 --> 00:26:17,080
But leadership, and I want to come back to leadership a little bit, helping our kids

457
00:26:17,080 --> 00:26:22,160
understand what leadership is and how to lead, how to have a vision, how to connect with

458
00:26:22,160 --> 00:26:25,160
individuals.

459
00:26:25,160 --> 00:26:26,160
Curiosity, right?

460
00:26:26,160 --> 00:26:32,360
I think that's innate in children, in the youngest of children, but we lose it over time.

461
00:26:32,360 --> 00:26:34,680
And that goes back to asking great questions.

462
00:26:34,680 --> 00:26:43,880
And so, maintaining and creating a curiosity practice, solution-oriented thinking, where

463
00:26:43,880 --> 00:26:51,800
the notion is, if there's a problem, rather than focusing on the problem, really do a

464
00:26:51,800 --> 00:27:00,240
judo mind trick or a Jedi mind trick to focus on the solution, because there are solutions.

465
00:27:01,240 --> 00:27:06,760
Principle thinking, exponential thinking, and then philosophy, which is going to be

466
00:27:06,760 --> 00:27:09,040
more valuable than ever before.

467
00:27:09,040 --> 00:27:10,040
I believe that.

468
00:27:10,040 --> 00:27:11,040
Yeah.

469
00:27:11,040 --> 00:27:18,440
When you hear chat GPT, philosophy, in the sense of dealing with the uncertain, really,

470
00:27:18,440 --> 00:27:23,680
being able to manage knowledge in the ambiguity of not really knowing.

471
00:27:23,680 --> 00:27:28,240
I asked chat GPT, and I said, okay, what are the skills that our kids need to be taught

472
00:27:28,240 --> 00:27:34,040
in high school to prepare them for a world of advanced AI?

473
00:27:34,040 --> 00:27:42,840
It said adaptability, continuous learning, critical thinking, emotional intelligence,

474
00:27:42,840 --> 00:27:47,280
ethical awareness, and resilience.

475
00:27:47,280 --> 00:27:50,680
I like the adaptability bit very much, actually.

476
00:27:50,680 --> 00:27:55,160
Well, it's going to say, you know, adapt to AI.

477
00:27:55,160 --> 00:27:56,160
Listen to your master.

478
00:27:57,080 --> 00:28:03,040
But this is a conversation I'm going to be having for a while, which is, what are the

479
00:28:03,040 --> 00:28:07,480
skills that are going to prepare our kids?

480
00:28:07,480 --> 00:28:12,160
Because they're going to spend a huge amount of time and then be thrust into living on

481
00:28:12,160 --> 00:28:13,160
their own.

482
00:28:13,160 --> 00:28:18,120
It's this decade that things are changing dramatically.

483
00:28:18,120 --> 00:28:21,320
The next three years.

484
00:28:21,320 --> 00:28:23,880
You keep on saying next three years versus decade, and you're right.

485
00:28:23,880 --> 00:28:27,800
It is the speed of change is thrust.

486
00:28:27,800 --> 00:28:33,040
There are tiny little triggers that are beyond repair.

487
00:28:33,040 --> 00:28:40,320
So if Imad Moustak, he boldly says no developers in five years.

488
00:28:40,320 --> 00:28:47,000
So that if you're the most conservative human on the planet that assumes a 20% job loss in

489
00:28:47,000 --> 00:28:53,720
the development landscape every year, imagine if 20% of all the developers you know in

490
00:28:53,720 --> 00:28:57,000
California have lost their job every year from now on.

491
00:28:57,000 --> 00:29:02,520
That's a very significant redesign of the fabric of society.

492
00:29:02,520 --> 00:29:05,240
And I think the reality is it's around the corner.

