1
00:00:00,000 --> 00:00:11,760
Hey, Amade, good to hear you.

2
00:00:11,760 --> 00:00:14,120
It was always a pleasure.

3
00:00:14,120 --> 00:00:15,120
Yeah.

4
00:00:15,120 --> 00:00:16,120
So, where are you today?

5
00:00:16,120 --> 00:00:17,120
I'm in London.

6
00:00:17,120 --> 00:00:18,120
Good.

7
00:00:18,120 --> 00:00:22,960
Other side of the planet, I'm in Santa Monica.

8
00:00:22,960 --> 00:00:29,640
It's been quite the extraordinary game of ping pong out there these last four or five

9
00:00:29,640 --> 00:00:30,640
days.

10
00:00:30,640 --> 00:00:36,160
I didn't think the first thing that AI would disrupt would be reality TV, right?

11
00:00:36,160 --> 00:00:37,160
Yeah.

12
00:00:37,160 --> 00:00:46,880
It's been fascinating how X has become sort of the go-to place to find out the latest

13
00:00:46,880 --> 00:00:51,600
of where Sam is working and what's going on with the AI industry.

14
00:00:51,600 --> 00:00:55,160
Yeah, you found the notifications in the wake, guys.

15
00:00:55,160 --> 00:00:59,320
I mean, it's the thing, like, what else will move at the speed of this?

16
00:00:59,320 --> 00:01:03,320
Like, I was saying to someone recently, AI research doesn't really move at the speed

17
00:01:03,320 --> 00:01:05,720
of conferences or even PDFs anymore, right?

18
00:01:05,720 --> 00:01:07,920
You just wake up and you're like, oh, it's 10 times faster.

19
00:01:07,920 --> 00:01:10,880
So, I think that's why X is quite good.

20
00:01:10,880 --> 00:01:16,840
I actually, like, unfollow just about everyone and just the AI algorithms find the most interesting

21
00:01:16,840 --> 00:01:17,840
things for me.

22
00:01:17,840 --> 00:01:21,400
So, I've got, like, 10 people that I follow, and it's actually working really well.

23
00:01:21,400 --> 00:01:22,400
It's getting better.

24
00:01:22,400 --> 00:01:25,120
Well, it has been.

25
00:01:25,120 --> 00:01:26,960
I've been enjoying the conversation.

26
00:01:26,960 --> 00:01:33,360
It really feels like you're inside an intimate conversation among friends as this is going

27
00:01:33,360 --> 00:01:34,360
back and forth.

28
00:01:34,360 --> 00:01:42,760
I think this entire four or five days has been an extraordinary, up-close, intimate conversation

29
00:01:42,760 --> 00:01:48,440
around governance and around, you know, what's the future of AI?

30
00:01:48,800 --> 00:01:56,200
Honestly, you know, as it gets faster and more powerful, the cost of missteps is going

31
00:01:56,200 --> 00:01:57,960
to increase exponentially.

32
00:01:59,960 --> 00:02:01,960
Let's begin here.

33
00:02:01,960 --> 00:02:08,040
I mean, you've been making the argument about open source as one of the most critical elements

34
00:02:08,040 --> 00:02:09,600
of governance for a while now.

35
00:02:09,600 --> 00:02:11,400
Can you just, let's hop into that.

36
00:02:13,080 --> 00:02:17,080
Yeah, I think that, you know, open source is the difficult one because it means a few

37
00:02:17,080 --> 00:02:18,080
different things.

38
00:02:18,120 --> 00:02:20,280
Like, is it models you can download and use?

39
00:02:20,280 --> 00:02:23,720
Do you make all the data available and free?

40
00:02:23,720 --> 00:02:28,240
And then when you actually look at what all these big companies do, all their stuff is

41
00:02:28,240 --> 00:02:33,560
built on open source basis, you know, it's built on the transformer paper.

42
00:02:33,560 --> 00:02:41,200
It's built on, like, the new model by Khayfouli and Vira1.ai is basically Lama.

43
00:02:41,200 --> 00:02:45,400
Like it's actually got the same variable names and other things like that, plus a gigantic

44
00:02:45,400 --> 00:02:46,520
supercomputer, right?

45
00:02:46,520 --> 00:02:51,800
The whole conversation has been, you know, how important is openness and transparency?

46
00:02:51,800 --> 00:02:56,440
And what are the governance models that are going to allow the most powerful technology

47
00:02:56,440 --> 00:03:03,040
on the planet to enable the most benefit for humanity and the safety?

48
00:03:03,040 --> 00:03:10,000
So, I mean, you've been thinking about this and speaking to transparency, openness, governance

49
00:03:10,000 --> 00:03:12,000
for a while.

50
00:03:12,000 --> 00:03:16,400
Could you, I mean, what do you think is going to be, what do you think is we need to be

51
00:03:16,400 --> 00:03:17,400
focused on?

52
00:03:17,400 --> 00:03:19,600
Where do we need to evolve to?

53
00:03:19,600 --> 00:03:23,080
Yeah, it's a complicated topic.

54
00:03:23,080 --> 00:03:28,840
I think that most of the infrastructure of the Internet is open source, Linux, everything

55
00:03:28,840 --> 00:03:29,960
like that.

56
00:03:29,960 --> 00:03:36,600
I think these models, it's unlikely that our governments will be run on GPT-7 or Baal

57
00:03:36,600 --> 00:03:37,600
or anything like that.

58
00:03:37,600 --> 00:03:41,600
How are you going to have black boxes that run these things?

59
00:03:41,600 --> 00:03:46,000
I think a lot of the governance debate has been hijacked by the AI safety debate where

60
00:03:46,000 --> 00:03:49,800
people are talking about AGI killing us all, and then there's this precautionary principle

61
00:03:49,800 --> 00:03:50,800
that kicks in.

62
00:03:50,800 --> 00:03:53,880
It's too dangerous to let out because what if China gets it?

63
00:03:53,880 --> 00:03:56,080
What if someone builds an AGI that kills us all?

64
00:03:56,080 --> 00:04:01,600
It'd be great to have this amazing board that could pull the off switch, you know, whereas

65
00:04:01,600 --> 00:04:08,320
in reality, I think that you're seeing a real social impact from this technology, and it's

66
00:04:08,320 --> 00:04:13,800
about who advances forward and who's left behind if we're thinking about risk.

67
00:04:13,800 --> 00:04:19,120
Because governance is always about finding, as you said, the best outcomes and also mitigating

68
00:04:19,120 --> 00:04:21,040
against the harms, right?

69
00:04:21,040 --> 00:04:24,800
And there's some very real, amazingly positive outcomes that are now emerging that people

70
00:04:24,800 --> 00:04:30,920
can agree on, but also some very real social impacts that we have to mitigate against.

71
00:04:30,920 --> 00:04:39,280
So I mean, let's begin, how is stability governed?

72
00:04:39,280 --> 00:04:43,920
Stability is basically governed by me.

73
00:04:43,920 --> 00:04:47,640
So I looked in foundations and DAOs and everything like that, and I thought to take it to where

74
00:04:47,640 --> 00:04:48,640
we are now.

75
00:04:48,640 --> 00:04:52,400
I needed to have very singular governance, but now we're looking at other alternatives.

76
00:04:52,400 --> 00:04:53,720
And what do you think it's going to be?

77
00:04:53,720 --> 00:04:55,000
Where would you head in the future?

78
00:04:55,000 --> 00:04:59,600
I mean, let's actually jump away from this in particular.

79
00:04:59,600 --> 00:05:04,040
But do you recommend the most powerful technologies on the planet?

80
00:05:04,040 --> 00:05:05,760
How should they be governed?

81
00:05:05,760 --> 00:05:07,160
How should they be owned?

82
00:05:07,160 --> 00:05:12,840
You know, where should we be in five years?

83
00:05:12,840 --> 00:05:17,400
I think there need to be public goods that are collectively owned and then individually

84
00:05:17,400 --> 00:05:18,720
owned as well.

85
00:05:18,720 --> 00:05:24,840
So for example, there was the tweet kind of storm, the kind of I am Spartacus or his

86
00:05:24,840 --> 00:05:31,520
name is Robert Boulson from the OpenAI team saying, OpenAI is nothing without its people.

87
00:05:31,520 --> 00:05:37,560
Stability, we have amazing people, 190 and 65 top researchers.

88
00:05:37,560 --> 00:05:42,600
Without its people, we're open models used by hundreds of millions, it continues.

89
00:05:42,600 --> 00:05:46,040
And if you think about where you need to go, you can never have a choke point on this technology,

90
00:05:46,040 --> 00:05:49,640
I think it becomes part of your life.

91
00:05:49,640 --> 00:05:53,440
Like the phrase I have is not your models, not your mind.

92
00:05:53,440 --> 00:05:59,480
So these models, again, are just such interesting things to take billions of images or trillions

93
00:05:59,480 --> 00:06:05,120
of words and you get this file out that can do magic, right, trade or magic sand.

94
00:06:05,120 --> 00:06:10,840
I think that you will have pilots that gather our global knowledge on various modalities

95
00:06:10,840 --> 00:06:15,160
and you'll have co-pilots that you individually own that guide you through life.

96
00:06:15,160 --> 00:06:19,440
And I can't see how that can be controlled by any one organization.

97
00:06:19,440 --> 00:06:26,640
You've been on record talking about having models owned by the citizens of nations.

98
00:06:26,640 --> 00:06:28,520
Can you speak to that a little bit?

99
00:06:28,520 --> 00:06:29,520
Sure.

100
00:06:29,520 --> 00:06:34,680
So we just released some of the top Japanese models from visual language to language to

101
00:06:34,680 --> 00:06:38,640
Japanese SDXL as an example.

102
00:06:38,640 --> 00:06:41,560
So we're training for half a dozen different nations and models now.

103
00:06:41,560 --> 00:06:46,840
And the plan is to figure out a way to give ownership of these datasets and models back

104
00:06:46,840 --> 00:06:48,280
to the people of that nation.

105
00:06:48,760 --> 00:06:53,680
So you get the smartest people in Mexico to run a stability Mexico or maybe a different

106
00:06:53,680 --> 00:06:58,840
structure that then makes decisions for Mexicans with the Mexicans about the data and what

107
00:06:58,840 --> 00:07:00,520
goes in it.

108
00:07:00,520 --> 00:07:04,480
Because everyone's been focusing on the outputs, the inputs actually are the things that matter

109
00:07:04,480 --> 00:07:06,600
the most.

110
00:07:06,600 --> 00:07:10,440
The best way I've thought about thinking of these models is like very enthusiastic graduates.

111
00:07:10,440 --> 00:07:13,400
So hallucinations isn't just probably too hard.

112
00:07:13,400 --> 00:07:17,080
A lot of the things about like, oh, what about these bad things the models can output?

113
00:07:17,080 --> 00:07:19,320
It's about what you've input.

114
00:07:19,320 --> 00:07:24,240
And so what you put into that Mexican dataset or the Chinese or Vietnamese one will impact

115
00:07:24,240 --> 00:07:25,240
the outputs.

116
00:07:25,240 --> 00:07:31,400
And there's a great paper in Nature, Human Behavior today about that, about how foundational

117
00:07:31,400 --> 00:07:33,200
models are cultural technologies.

118
00:07:33,200 --> 00:07:40,600
So again, how can you outsource your culture and your brains to other countries, to people

119
00:07:40,600 --> 00:07:43,720
that are from a very different place?

120
00:07:43,720 --> 00:07:45,720
I think it eventually has to be localized.

121
00:07:46,080 --> 00:07:50,240
I think one of the points you said originally is we have to separate the issue of governance

122
00:07:50,240 --> 00:07:55,160
versus safety and alignment.

123
00:07:55,160 --> 00:08:00,920
Are they actually different?

124
00:08:00,920 --> 00:08:06,800
So I think that a lot of the safety discussion or this AGI risk discussion is because the

125
00:08:06,800 --> 00:08:11,640
future is so uncertain because it is so powerful.

126
00:08:11,640 --> 00:08:15,240
And we didn't have a good view of where we're going.

127
00:08:15,240 --> 00:08:19,040
So when you go on a journey and you don't know where you're going, you will minimize

128
00:08:19,040 --> 00:08:23,040
from acts and regret you'll have the precautionary principle.

129
00:08:23,040 --> 00:08:27,280
And then that means you basically go towards authority, you go towards trying to control

130
00:08:27,280 --> 00:08:31,080
this technology when it's so difficult to control.

131
00:08:31,080 --> 00:08:35,000
And you end up not doing much, you know, because there's anything to go wrong.

132
00:08:35,000 --> 00:08:38,080
When you have an idea of where we're going, like you should have all the cancer knowledge

133
00:08:38,080 --> 00:08:42,080
in the world at your fingertips or climate knowledge, or anybody should be able to create

134
00:08:42,080 --> 00:08:44,480
whole worlds and share them.

135
00:08:44,480 --> 00:08:49,600
When you align your safety discussions against the goal, against the location that you're

136
00:08:49,600 --> 00:08:53,920
going to, again, just like setting out on a journey, I think that's a big change.

137
00:08:53,920 --> 00:08:58,680
Similarly, most of the safety discussions have been on outputs, not inputs.

138
00:08:58,680 --> 00:09:02,480
If you have a high quality data set without knowledge about anthrax, your language model

139
00:09:02,480 --> 00:09:09,240
is unlikely to tell you how to build anthrax, you know, and transparency around that will

140
00:09:09,240 --> 00:09:10,240
be very useful.

141
00:09:10,240 --> 00:09:17,600
So let's dive into that safety alignment issue for a moment because it's an area you and

142
00:09:17,600 --> 00:09:19,120
I have been talking a lot about.

143
00:09:19,120 --> 00:09:25,000
So Mustafa wrote a book, Mustafa Suleiman wrote a book called The Coming Wave in which

144
00:09:25,000 --> 00:09:30,480
he talks about containment as the mechanism by which we're going to be making sure we

145
00:09:30,480 --> 00:09:33,200
have safe AI.

146
00:09:33,200 --> 00:09:38,320
You and I have had the conversation of it's really how you educate and raise and train

147
00:09:38,320 --> 00:09:44,920
your AI systems in making sure that there's full transparency and openness on the data

148
00:09:44,920 --> 00:09:45,920
sets that are utilized.

149
00:09:45,920 --> 00:09:49,000
Do you think containment is an option for safety?

150
00:09:49,000 --> 00:09:52,400
No, not at all.

151
00:09:52,400 --> 00:09:58,800
Like a number of leaders say, what if China gets open source AI?

152
00:09:58,800 --> 00:10:02,600
The reality is that China, Russia, everyone already has the weights for GPT-PORC.

153
00:10:02,600 --> 00:10:06,080
They just downloaded it on a USM stick.

154
00:10:06,080 --> 00:10:09,360
You know that there's been compromised, right?

155
00:10:09,360 --> 00:10:10,360
There's no way they couldn't.

156
00:10:10,360 --> 00:10:12,480
The rewards are too great.

157
00:10:12,480 --> 00:10:19,320
And there is a absolutely false dichotomy here and a lot of the companies want you to believe

158
00:10:19,320 --> 00:10:24,280
that giant models are the main thing and you need to have these gigantic, ridiculous

159
00:10:24,280 --> 00:10:26,080
supercomputers that only they can run.

160
00:10:26,080 --> 00:10:31,680
I mean, look, we run gigantic supercomputers, but the reality is this.

161
00:10:31,680 --> 00:10:38,480
The supercomputers and the giant trillion zillion data sets are just a shortcut for bad quality

162
00:10:38,480 --> 00:10:40,600
data.

163
00:10:40,600 --> 00:10:45,160
It's like using a hot pot or sous viding a steak that's bad quality.

164
00:10:45,160 --> 00:10:48,880
You cook it for longer and it organizes the information.

165
00:10:48,880 --> 00:10:54,080
With stable diffusion, we did a study and we showed that basically 92% of the data isn't

166
00:10:54,080 --> 00:11:00,560
used 99% of the time, you know, because now you're seeing this with, for example, Microsoft's

167
00:11:00,640 --> 00:11:04,520
by release, it's trained entirely on synthetic data.

168
00:11:04,520 --> 00:11:09,320
Dali three is trained on RV AE and entirely synthetic data.

169
00:11:09,320 --> 00:11:10,800
You are what you eat.

170
00:11:10,800 --> 00:11:14,320
And again, we cooked it for longer to get past that.

171
00:11:14,320 --> 00:11:22,080
But the implications of this are that I believe within 12 to 18 months, you'll see GPT four

172
00:11:22,080 --> 00:11:24,840
level performance on a smartphone.

173
00:11:24,840 --> 00:11:26,520
How do you contain that?

174
00:11:26,520 --> 00:11:31,080
And how do you contain it when China can do distributed training at scale and release

175
00:11:31,080 --> 00:11:32,800
open source models?

176
00:11:32,800 --> 00:11:41,720
So Google recently did 50,000 TPU training run on their V5 is the new V5 is their TPUs

177
00:11:41,720 --> 00:11:44,840
are very low powered relative to what we've seen.

178
00:11:44,840 --> 00:11:48,240
But again, you can do distributed dynamic training.

179
00:11:48,240 --> 00:11:55,000
Similarly, like we funded five mind and we've seen Google DeepMind just a new paper on localization

180
00:11:55,000 --> 00:11:56,640
through distributed training.

181
00:11:56,640 --> 00:12:00,960
The models are good for fast enough and cheap enough that you can swarm them and you don't

182
00:12:00,960 --> 00:12:03,560
need giant supercomputers anymore.

183
00:12:03,560 --> 00:12:08,480
And that has a lot of implications and how are you going to contain that?

184
00:12:08,480 --> 00:12:13,440
So coming back to the question of do you mandate training training sets?

185
00:12:13,440 --> 00:12:20,440
Do you does, you know, does the government set out what all companies should be utilizing

186
00:12:20,440 --> 00:12:21,440
in mandate?

187
00:12:21,440 --> 00:12:27,600
If you're going to have a aligned AI, it has to be trained on these sets.

188
00:12:27,600 --> 00:12:30,840
How do we how do we possibly govern that?

189
00:12:30,840 --> 00:12:33,680
Look, we have food standards, right?

190
00:12:33,680 --> 00:12:34,680
For ingredients.

191
00:12:34,680 --> 00:12:39,840
Why don't you have data standards for the ingredients that make up a model?

192
00:12:39,840 --> 00:12:43,360
It's just data compute and some algorithms, right?

193
00:12:43,360 --> 00:12:48,080
And so you should say they are the standards and then you can make it compulsory.

194
00:12:48,080 --> 00:12:51,680
That will take a while or you can just have an ISR type standard.

195
00:12:51,680 --> 00:12:55,960
This is good quality model training, good quality data, you know, and people will naturally

196
00:12:55,960 --> 00:12:58,480
gravitate towards that and it becomes a default.

197
00:12:58,480 --> 00:13:01,400
Are you working towards that right now?

198
00:13:01,400 --> 00:13:07,200
Yeah, I mean, look, we spun out a Luther AI as an independent 501c3 so they could look

199
00:13:07,200 --> 00:13:12,960
at data standards and things like that independent of us and the opposite of open AI.

200
00:13:12,960 --> 00:13:15,960
And this is something I've been talking to many people about and we're getting national

201
00:13:15,960 --> 00:13:20,960
data sets and more so that hopefully we can implement good standards similar to how we

202
00:13:20,960 --> 00:13:24,640
offered opt out and how the billion images opted out of our image data set because everyone

203
00:13:24,640 --> 00:13:26,520
was just training on everything.

204
00:13:26,520 --> 00:13:27,520
Is required?

205
00:13:27,520 --> 00:13:28,520
No.

206
00:13:28,520 --> 00:13:29,520
But is it good?

207
00:13:29,520 --> 00:13:30,520
Yes.

208
00:13:30,520 --> 00:13:32,440
And everyone will benefit from better quality data.

209
00:13:32,440 --> 00:13:36,000
So there's no reason that for these very large model training runs, the data sets should

210
00:13:36,000 --> 00:13:37,880
not be transparent and logged.

211
00:13:37,880 --> 00:13:40,680
But again, we want to know what goes into that.

212
00:13:40,680 --> 00:13:44,920
And again, if we have the graduate analogy, what was the curriculum that the graduate

213
00:13:44,960 --> 00:13:45,960
was taught at?

214
00:13:45,960 --> 00:13:47,960
Which university did they go to?

215
00:13:47,960 --> 00:13:49,960
It's something that we'd want to know.

216
00:13:49,960 --> 00:13:53,680
But then why do we talk to GPT-4 where we don't know where it went to university or where

217
00:13:53,680 --> 00:13:54,680
it's been trained on?

218
00:13:54,680 --> 00:13:57,400
It's a bit weird that, isn't it?

219
00:13:57,400 --> 00:14:05,040
What do you think the lesson is going to be from the last four days?

220
00:14:05,040 --> 00:14:06,040
I'm just confused.

221
00:14:06,040 --> 00:14:08,040
I don't know who was against who or what.

222
00:14:08,040 --> 00:14:09,040
I'm going to just post it.

223
00:14:09,040 --> 00:14:12,040
Are we against misalignment or mollock?

224
00:14:12,160 --> 00:14:16,640
I think probably the biggest lesson is it's very hard to align humans, right?

225
00:14:16,640 --> 00:14:18,440
And the stakes are very large.

226
00:14:18,440 --> 00:14:20,440
Why is this so interesting to us?

227
00:14:20,440 --> 00:14:21,920
The stakes are so high.

228
00:14:21,920 --> 00:14:26,520
You tweeted something that was serious and unfortunately funny, which was how can we

229
00:14:26,520 --> 00:14:32,720
align AI with humanity's best interests if we can't align our company's board with its

230
00:14:32,720 --> 00:14:34,720
employee's best interests?

231
00:14:34,720 --> 00:14:35,720
Yeah.

232
00:14:35,720 --> 00:14:39,320
Well, the thing is it's not the employee's best interests.

233
00:14:39,320 --> 00:14:47,680
It's like the board was set up as a lever to ensure the charter of open AI.

234
00:14:47,680 --> 00:14:53,760
So if you look at the original founding document of open AI from 2015, it is a beautiful document

235
00:14:53,760 --> 00:14:55,760
talking about open collaboration, everything.

236
00:14:55,760 --> 00:14:57,920
And then it kind of changed in 2019.

237
00:14:57,920 --> 00:15:02,400
But the charter still emphasizes cooperation, safety, and fundamental.

238
00:15:02,400 --> 00:15:06,000
I posted about this back in March when I said the board and the government structure of

239
00:15:06,000 --> 00:15:09,640
open AI is weird.

240
00:15:09,640 --> 00:15:10,640
What is it for?

241
00:15:10,640 --> 00:15:11,640
What are they trying to do?

242
00:15:11,640 --> 00:15:17,800
Because if you say you're building AGI, in their own road to AGI, they say, this will

243
00:15:17,800 --> 00:15:19,240
end democracy most likely.

244
00:15:19,240 --> 00:15:20,240
I remember reading that.

245
00:15:20,240 --> 00:15:25,840
Because democracy, there's no way democracy survives AGI.

246
00:15:25,840 --> 00:15:29,440
Because either, obviously, it'll be better and you get it to do it or can dissuade everyone

247
00:15:29,440 --> 00:15:31,160
or we all die.

248
00:15:31,160 --> 00:15:33,200
Or it's utopia crap, all right?

249
00:15:33,200 --> 00:15:34,200
Abundance, baby.

250
00:15:34,520 --> 00:15:40,000
There's no way it survives AGI.

251
00:15:40,000 --> 00:15:42,120
There's no way capitalism survives AGI.

252
00:15:42,120 --> 00:15:45,720
The AGI will be the best trader in the world, right?

253
00:15:45,720 --> 00:15:52,640
And it's like, who should be making the decisions on the AGI, assuming that they achieve those

254
00:15:52,640 --> 00:15:53,640
things?

255
00:15:53,640 --> 00:15:54,800
And that's in their own words.

256
00:15:54,800 --> 00:16:01,520
So I think that people are kind of waking up to, oh, there's no real way to do this properly.

257
00:16:01,520 --> 00:16:07,280
And previously, we were scared of open and being transparent, everyone getting this,

258
00:16:07,280 --> 00:16:09,440
which can with the original thing of open air.

259
00:16:09,440 --> 00:16:15,200
And now we're scared of, who are these clowns, you know, and put it in the nicest way.

260
00:16:15,200 --> 00:16:16,880
Because this was ridiculous.

261
00:16:16,880 --> 00:16:20,680
Like you see better politics in a teenage sorority, right?

262
00:16:20,680 --> 00:16:27,840
And it's fundamentally scary, but unelected people, no matter how great they are, and

263
00:16:27,840 --> 00:16:32,880
I think some of the board members are great, should have a say in something that could

264
00:16:32,880 --> 00:16:38,280
literally upend our entire society according to their own words.

265
00:16:38,280 --> 00:16:45,360
I find that inherently anti-democratic and illiberal.

266
00:16:45,360 --> 00:16:51,560
At the end of the day, you know, capitalism has worked, and it's the best system that

267
00:16:51,560 --> 00:16:54,080
we have thus far.

268
00:16:54,080 --> 00:17:03,440
And it's a self, you know, it's built on self-interest and built on continuous optimization, maximization.

269
00:17:03,440 --> 00:17:11,800
I'm still wondering where you go in terms of governing these companies at one level,

270
00:17:11,800 --> 00:17:18,320
internal governance, and then governing the companies at a national and global level.

271
00:17:18,320 --> 00:17:24,440
Has anybody put forward a plan that you think is worth highlighting here?

272
00:17:24,440 --> 00:17:26,240
Not really.

273
00:17:26,240 --> 00:17:30,720
I mean, organizations are a weird artificial intelligence, right?

274
00:17:30,720 --> 00:17:36,480
They have the status of people, and they're slow dumb AI, and they eat up hopes and dreams.

275
00:17:36,480 --> 00:17:40,000
That's what they feed on, I think.

276
00:17:40,000 --> 00:17:41,160
This AI can upgrade them.

277
00:17:41,160 --> 00:17:42,160
It can make them smarter.

278
00:17:42,160 --> 00:17:43,640
They can how do you coordinate?

279
00:17:43,640 --> 00:17:47,000
And from a mechanism design perspective, it's super interesting.

280
00:17:47,000 --> 00:17:52,160
AI can market, so I think we will have AI market makers that can tell stories.

281
00:17:52,160 --> 00:17:54,880
The story of Silicon Valley Bank went around the world in two seconds.

282
00:17:54,880 --> 00:17:56,320
The story of open AI goes around.

283
00:17:56,320 --> 00:17:58,200
AI can tell better stories than humans.

284
00:17:58,200 --> 00:17:59,200
It's inevitable.

285
00:17:59,200 --> 00:18:03,760
And I think that gives hope for coordination, but then also it's dangers of disruption.

286
00:18:03,760 --> 00:18:11,800
I want to double click one second on the two words that you use most, openness and transparency,

287
00:18:11,800 --> 00:18:16,840
and understand fully what those mean one moment, because, you know, and the question is,

288
00:18:16,840 --> 00:18:21,680
not only what they mean, but how fundamental it needs to be.

289
00:18:21,680 --> 00:18:27,800
So openness right now and your definition in terms of AI means what?

290
00:18:27,800 --> 00:18:31,120
It means different things, but different things, unfortunately.

291
00:18:31,120 --> 00:18:33,520
I don't think it means open source.

292
00:18:33,520 --> 00:18:39,680
I think, for me, open means more about access and ownership of the models so that you don't

293
00:18:39,680 --> 00:18:46,000
have a lockstep, like you can hire your own graduates as opposed to relying on consultants.

294
00:18:46,000 --> 00:18:49,720
Security comes down to, I think, for language models in particular, I don't think this holds

295
00:18:49,720 --> 00:18:51,360
some media models.

296
00:18:51,360 --> 00:18:54,400
You really need to know what it's been taught.

297
00:18:54,400 --> 00:18:56,440
That's the only way to safety.

298
00:18:56,440 --> 00:19:00,120
You should not engage with something or use something if you don't know what its credentials

299
00:19:00,120 --> 00:19:04,120
are and how it's been taught, because I think that's inherently dangerous as these get more

300
00:19:04,120 --> 00:19:05,120
and more capabilities.

301
00:19:05,120 --> 00:19:07,520
And again, I don't know if we get to SGI.

302
00:19:07,520 --> 00:19:11,040
If we do, I think it'll probably be like Scarlett Johansson and her, you know, like just to

303
00:19:11,040 --> 00:19:14,240
combine thanks to the GPs, but I'm assuming we don't.

304
00:19:14,240 --> 00:19:15,240
You still need transparency.

305
00:19:16,000 --> 00:19:22,880
Again, how can any government or regulated industry not run on a transparent model?

306
00:19:22,880 --> 00:19:24,680
They can't run on Blackpops.

307
00:19:24,680 --> 00:19:31,160
I get that, and I understand the rationale for it, but now the question is, can you prove

308
00:19:31,160 --> 00:19:32,160
transparency?

309
00:19:32,160 --> 00:19:37,680
I think that, again, a model is only three things, really.

310
00:19:37,680 --> 00:19:41,520
It's the data, the algorithm, and the compute.

311
00:19:41,520 --> 00:19:43,600
And they come and the binary file pops out.

312
00:19:43,640 --> 00:19:49,280
You can tune it with RLHF or DPO or genetic algorithms or whatever, but that's really

313
00:19:49,280 --> 00:19:50,720
the recipe, right?

314
00:19:50,720 --> 00:19:55,520
And so the algorithms, you don't need algorithmic transparency here versus classical AI because

315
00:19:55,520 --> 00:19:57,320
they're very simple.

316
00:19:57,320 --> 00:20:01,400
One of our fellows recreated the Palm 540 billion parameter model.

317
00:20:01,400 --> 00:20:03,200
This is Lucid Raines on GitHub.

318
00:20:03,200 --> 00:20:04,200
You look at that.

319
00:20:04,200 --> 00:20:05,960
If you're a developer and you want to cry, it's GitHub.

320
00:20:05,960 --> 00:20:06,960
It's crazy.

321
00:20:06,960 --> 00:20:10,720
In 206 lines of PyTorch, and that's it.

322
00:20:10,720 --> 00:20:13,480
The algorithms are not very complicated.

323
00:20:13,480 --> 00:20:17,400
Having a gigantic super computer is complicated, and this is why they freaked out when Greg

324
00:20:17,400 --> 00:20:20,920
Brockman kind of stepped down because he's one of the most talented engineers of our

325
00:20:20,920 --> 00:20:21,920
time.

326
00:20:21,920 --> 00:20:25,120
He built these amazing gigantic clusters.

327
00:20:25,120 --> 00:20:28,560
And then the data and how you structure data is complicated.

328
00:20:28,560 --> 00:20:32,040
So I think you can have transparency there because if the data is transparent and who

329
00:20:32,040 --> 00:20:35,320
cares about the supercomputer, who really cares about the algorithm?

330
00:20:35,320 --> 00:20:40,080
Now, let's talk about the next term, alignment here.

331
00:20:40,080 --> 00:20:44,960
Alignments thrown around in lots of different ways.

332
00:20:44,960 --> 00:20:46,960
How do you define alignment?

333
00:20:46,960 --> 00:20:51,600
I define alignment in terms of objective function.

334
00:20:51,600 --> 00:20:59,680
So YouTube was used by the extremists to serve ads for their nastiness.

335
00:20:59,680 --> 00:21:00,760
Why?

336
00:21:00,760 --> 00:21:06,560
Because the algorithm optimized for engagement, which then optimized for extreme content, which

337
00:21:06,560 --> 00:21:08,440
then optimized for the extremists.

338
00:21:08,960 --> 00:21:13,040
Did YouTube mean that, no, but they're just trying to serve ads up, right?

339
00:21:13,040 --> 00:21:16,560
But it meant it wasn't aligned with its users' interests.

340
00:21:16,560 --> 00:21:20,480
And so for me, if you have these technologies that we're going to outsource for more of our

341
00:21:20,480 --> 00:21:25,720
mind, our culture, our children's futures, to you that are very persuasive, we have to

342
00:21:25,720 --> 00:21:31,520
ensure they're aligned with our individual community and societal best interests.

343
00:21:31,520 --> 00:21:37,280
I think this is where the tension with corporations will come in.

344
00:21:37,320 --> 00:21:41,960
Because whoever licenses Scarlett Johansson's voice will sell a lot of ads, you know, they

345
00:21:41,960 --> 00:21:46,520
can be very, very persuasive, but then what are their controls on that?

346
00:21:46,520 --> 00:21:48,280
No one talks about that.

347
00:21:48,280 --> 00:21:54,120
The bigger question of alignment is not killerism, making sure that AI doesn't kill us.

348
00:21:54,120 --> 00:22:00,520
But again, I feel that if we build AI that is transparent, that we can test, that people

349
00:22:00,520 --> 00:22:06,560
can build mitigations around, we are more likely to survive and thrive.

350
00:22:06,560 --> 00:22:11,400
And also, I think there's a final element to here, which is who's alignment.

351
00:22:11,400 --> 00:22:14,400
Different cultures are different, different people are different.

352
00:22:14,400 --> 00:22:18,440
What we found with stable diffusion is that when we merge together the models that different

353
00:22:18,440 --> 00:22:22,640
people around the world have built, the model gets so much better.

354
00:22:22,640 --> 00:22:27,320
I think that makes sense because a monoculture will always be less fragile than a diversity.

355
00:22:27,320 --> 00:22:32,600
Again, I'm not talking about in the DEI kind of way, I'm talking about it in the actual

356
00:22:32,600 --> 00:22:33,600
logical way.

357
00:22:33,640 --> 00:22:39,840
So we have a paper from our reinforcement learning lab called CARPA called QDHF, QD-AIF, Quality

358
00:22:39,840 --> 00:22:43,800
and Diversity through artificial intelligence feedback.

359
00:22:43,800 --> 00:22:48,560
Because you find these models do get better with high quality and diverse inputs, just

360
00:22:48,560 --> 00:22:55,320
like you will get better if you have high quality and diverse experiences, you know?

361
00:22:55,320 --> 00:22:59,680
And saying that's something that's important, that we'll get lost if all these models are

362
00:22:59,680 --> 00:23:00,680
centralized.

363
00:23:01,640 --> 00:23:06,720
You and I have had a lot of conversations about timelines here.

364
00:23:06,720 --> 00:23:13,080
We can get into a conversation of when and if we see AGI.

365
00:23:13,080 --> 00:23:17,320
But we're seeing more and more powerful capabilities coming online right now that are going to

366
00:23:17,320 --> 00:23:23,480
cause a lot of amazing progress and disruption.

367
00:23:23,480 --> 00:23:30,160
How much time do we have, EMI, and we had a conversation when we were together at FII

368
00:23:30,160 --> 00:23:38,840
about the disenfranchised youth coming off of COVID.

369
00:23:38,840 --> 00:23:41,600
So let's talk one second about timelines.

370
00:23:41,600 --> 00:23:54,800
How long do we have to get our shit together, both as AI companies and investors and governors

371
00:23:54,800 --> 00:23:57,800
of society?

372
00:23:58,800 --> 00:24:03,560
The speed here is awesome and frightening.

373
00:24:03,560 --> 00:24:06,480
How long do we have?

374
00:24:06,480 --> 00:24:08,920
Everything everywhere, all at once, right?

375
00:24:08,920 --> 00:24:09,920
We don't have long.

376
00:24:09,920 --> 00:24:13,760
Like, AGI timelines for every definition of AGI, I have no idea.

377
00:24:13,760 --> 00:24:16,720
It will never be less than 12 months, right?

378
00:24:16,720 --> 00:24:21,360
Because it's such a step change, so let's put that to the side.

379
00:24:21,360 --> 00:24:26,280
Right now, everyone that's listening, are you all going to hire the same amount of graduates

380
00:24:26,280 --> 00:24:27,600
that you hired before?

381
00:24:27,600 --> 00:24:29,960
The answer is no.

382
00:24:29,960 --> 00:24:33,920
Some people might not hire any because this is a productivity and an answer and we have

383
00:24:33,920 --> 00:24:38,160
the data for that across any type of knowledge industry.

384
00:24:38,160 --> 00:24:43,640
You just had a great app that you can sketch and it does a whole iPhone app for you, right?

385
00:24:43,640 --> 00:24:48,080
I've gone on record and saying there are no programmers who know in five years, why would

386
00:24:48,080 --> 00:24:49,080
there be?

387
00:24:49,080 --> 00:24:50,600
What are interfaces?

388
00:24:50,600 --> 00:24:57,160
You had a 50% drop, I just put it on my Twitter, in hiring from Indian IITs.

389
00:24:57,160 --> 00:24:58,160
That's crazy.

390
00:24:58,160 --> 00:25:03,480
What you're going to have in a couple of years is around the world at the same time, these

391
00:25:03,480 --> 00:25:10,960
kids that have gone through the trauma of COVID, highly educated STEM, programming, accountancy

392
00:25:10,960 --> 00:25:18,080
law, simultaneously people will hire massively less of them because productivity enhances

393
00:25:18,080 --> 00:25:20,400
and you don't need as many of them.

394
00:25:20,400 --> 00:25:22,680
Why would you need as many paralegals?

395
00:25:22,680 --> 00:25:27,920
That for me is a gigantic societal issue and the only thing I can think of is the stoke

396
00:25:27,920 --> 00:25:33,400
of innovation and the generative jobs of the future through open source technology because

397
00:25:33,400 --> 00:25:38,000
I don't know how else we're going to mitigate that because, Peter, you're a student of history.

398
00:25:38,000 --> 00:25:41,920
What happens when you have large amounts of intelligent, disenfranchised youth history?

399
00:25:41,920 --> 00:25:43,400
We've had that happen a few times.

400
00:25:43,400 --> 00:25:48,760
We just had Arab Spring that long ago, revolt.

401
00:25:48,760 --> 00:25:54,320
People war if not international law.

402
00:25:54,320 --> 00:26:01,680
War is a good way to soak up the excess youth, but it's not pleasant for society and fundamentally

403
00:26:01,680 --> 00:26:07,720
the cost of information gathering, organization has collapsed, like you look at stable video

404
00:26:07,720 --> 00:26:10,080
that we released yesterday, right?

405
00:26:10,080 --> 00:26:13,720
It's going to get so much better so quickly, just like stable diffusion, the cost of creating

406
00:26:13,720 --> 00:26:21,120
movies increases, the demand for quality stuff increases, but there's a few years where demand

407
00:26:21,120 --> 00:26:26,120
and supply don't match and that's such a turbulent thing to navigate.

408
00:26:26,120 --> 00:26:29,880
That's one of the reasons I'm creating stabilities for different countries, so the best and brightest

409
00:26:29,880 --> 00:26:32,360
for me which can help navigate them.

410
00:26:32,360 --> 00:26:33,360
And people don't talk about it.

411
00:26:33,360 --> 00:26:42,080
I loved your idea that the stability models and systems will be owned by the nation.

412
00:26:42,440 --> 00:26:46,600
In fact, the one idea that I heard you say, which I thought was fantastic, was you graduate

413
00:26:46,600 --> 00:26:51,360
college in India, you're an owner in that system.

414
00:26:51,360 --> 00:26:56,280
You graduate from Nigeria, you're an owner in that system, basically to incentivize people

415
00:26:56,280 --> 00:27:01,960
to complete their education and to have them have ownership in what is ultimately the most

416
00:27:01,960 --> 00:27:04,800
important asset that nation has.

417
00:27:04,800 --> 00:27:06,520
And talk about it as infrastructure as well.

418
00:27:06,520 --> 00:27:10,440
I think that's an important analogy that people don't get.

419
00:27:10,440 --> 00:27:14,240
This is the knowledge infrastructure of the future, it's the biggest leap forward we have

420
00:27:14,240 --> 00:27:18,240
because you'll always have a co-pilot that knows everything in a few years and that can

421
00:27:18,240 --> 00:27:23,320
create anything in any type, but it must be embedded to your cultural values and you can't

422
00:27:23,320 --> 00:27:25,160
let anyone else own that.

423
00:27:25,160 --> 00:27:29,000
So it is the infrastructure of the mind and who would outsource their infrastructure to

424
00:27:29,000 --> 00:27:30,000
someone else.

425
00:27:30,000 --> 00:27:34,440
So that's why I think Nigerians should own the models of Nigerians for Nigerians and

426
00:27:34,440 --> 00:27:37,560
should be the next generation that does that.

427
00:27:37,560 --> 00:27:41,040
That's why you give the equity to the graduates, that's why you list it, that's why you make

428
00:27:41,040 --> 00:27:46,440
national champions because, again, that has to be that way.

429
00:27:46,440 --> 00:27:49,800
This is far more important than 5G and this gives you an idea of the scale, we're just

430
00:27:49,800 --> 00:27:51,760
at the start, the earlier doctor base.

431
00:27:51,760 --> 00:27:57,360
A trillion dollars was spent on 5G, this is clearly more important, more than a trillion

432
00:27:57,360 --> 00:28:00,840
dollars that we spend on this and again it flips the world.

433
00:28:00,840 --> 00:28:07,480
And so there is huge threat for our societal balance and again I think open is a potential

434
00:28:07,480 --> 00:28:11,720
antidote to create the jobs of the future and there's huge opportunity on the side

435
00:28:11,720 --> 00:28:16,840
because no one will ever be alone and we can use this to coordinate our systems, give everyone

436
00:28:16,840 --> 00:28:21,680
all the knowledge that they need at their fingertips and help guide everyone if we build

437
00:28:21,680 --> 00:28:26,960
this infrastructure correctly and I don't see the highlight can be closed.

438
00:28:26,960 --> 00:28:36,600
AGI, the conversation and the definition of AGI has basically been all over the place,

439
00:28:36,600 --> 00:28:43,080
because while prediction has been for 30 years that it's 2029, again, that's a blurry

440
00:28:43,080 --> 00:28:51,960
line of what we're trying to target but Elon's talked about anywhere from 2025 to 2028, what

441
00:28:51,960 --> 00:29:00,040
are you thinking, what's your timeline for even digital superintelligence?

442
00:29:00,040 --> 00:29:04,920
I honestly have no idea.

443
00:29:04,920 --> 00:29:10,120
People looking at the scaling laws and applying it but as I've said, data is the key and it's

444
00:29:10,120 --> 00:29:14,480
clear that we already have, you could build a board GPT and it would be better than most

445
00:29:14,480 --> 00:29:16,440
corporate boards, right?

446
00:29:16,440 --> 00:29:20,840
So I think we're already seeing improvements over the existing.

447
00:29:20,840 --> 00:29:23,760
One of the complications here is swarm AI.

448
00:29:23,760 --> 00:29:27,680
So even like it's the whole thing, like a duck-sized human or a hundred human sized

449
00:29:27,760 --> 00:29:34,160
ducks, we're just at the start of swarm intelligence and that reflects and represents how companies

450
00:29:34,160 --> 00:29:35,160
are organized.

451
00:29:35,160 --> 00:29:40,600
Andre Carpethi has some great analogies on this in terms of the new knowledge OS and

452
00:29:40,600 --> 00:29:47,080
that could take off at any time but the function of format of that may not be this whole Western

453
00:29:47,080 --> 00:29:51,880
anti-conformized consciousness that we think of but just incredibly efficient systems that

454
00:29:51,880 --> 00:29:55,800
displace existing human decision-making, right?

455
00:29:55,800 --> 00:30:01,440
And so there's an entire actual range of different AGI outcomes depending on your definition

456
00:30:01,440 --> 00:30:07,440
and I just don't know but I feel again like I wake up and I'm like, oh, look, it's fed

457
00:30:07,440 --> 00:30:12,520
up 10 times the model, you know, like I'm just not, no one can predict this.

458
00:30:12,520 --> 00:30:17,240
But there is a point at which, I mean, we were heading towards an AI singularity, using

459
00:30:17,280 --> 00:30:22,120
a definition of a singularity as a point after which you cannot predict what's coming

460
00:30:22,120 --> 00:30:24,800
next and that isn't far away.

461
00:30:24,800 --> 00:30:30,440
I mean, how far out is it for you a year or two years?

462
00:30:30,440 --> 00:30:35,160
I think you're heading towards it in the next few years but like I said, every company,

463
00:30:35,160 --> 00:30:38,640
organization, individual has an objective function.

464
00:30:38,640 --> 00:30:44,680
My objective function is to allow the next generation to navigate what's coming in the

465
00:30:44,680 --> 00:30:47,400
optimal way and achieve their potential.

466
00:30:47,400 --> 00:30:51,280
So I don't want to build an AGI, I don't want to do any of this, amplified human intelligence

467
00:30:51,280 --> 00:30:57,480
is my preference and trying to mitigate against some of the harms of these agentic things

468
00:30:57,480 --> 00:31:02,880
through data transparency, good standards and making it so people don't need to build

469
00:31:02,880 --> 00:31:09,160
gigantic models on crap, which I think is a major danger if even if not from AGI.

470
00:31:09,160 --> 00:31:13,960
But again, we just don't understand because it's difficult for us to comprehend super

471
00:31:14,000 --> 00:31:18,640
human capabilities, but again, we're already seeing that in narrow fields.

472
00:31:18,640 --> 00:31:21,160
We already know that it's a better rider than us.

473
00:31:21,160 --> 00:31:24,360
So we already know that it can make better pictures than us.

474
00:31:24,360 --> 00:31:30,600
And a better physician and a better educator and a better surgeon and a better everything.

475
00:31:30,600 --> 00:31:38,680
Yeah, and again, I think it's this mythos of these big labs being AGI focused, whereas

476
00:31:38,680 --> 00:31:42,480
you can be better than us in like 5% of the stuff that humans can do and that's still

477
00:31:42,520 --> 00:31:45,800
a massive impact on the world and they can still take over companies and things like

478
00:31:45,800 --> 00:31:46,800
that, right?

479
00:31:46,800 --> 00:31:50,800
Like if you take over a company, then you can impact the world.

480
00:31:50,800 --> 00:31:55,400
And there's clearly with a GPT for 1000 of them orchestrated correctly, that can call

481
00:31:55,400 --> 00:31:59,600
up people and stuff, you wouldn't know it's not CEO, you know, I can make an MA GPT and

482
00:31:59,600 --> 00:32:03,000
then they won't have to make all these tough decisions in any of that.

483
00:32:03,000 --> 00:32:07,680
And most of my decisions aren't that good, so it's probably better.

484
00:32:07,680 --> 00:32:12,520
So I think that we're getting to that point, it's very difficult and the design patterns

485
00:32:12,520 --> 00:32:13,520
are going fast.

486
00:32:13,520 --> 00:32:19,080
We're at the iPhone 2G, 3G stage, it's got copy and paste, and we just got the first

487
00:32:19,080 --> 00:32:23,160
stage as well of this technology, which is the creation step, it creates stuff.

488
00:32:23,160 --> 00:32:29,000
The next step is control and then composition, where you're annoyed because chat GPT doesn't

489
00:32:29,000 --> 00:32:32,240
remember all the stuff that you've written, that won't be the case in a year.

490
00:32:32,240 --> 00:32:36,920
And the final bit is collaboration, where these AIs collaborate together and with humans

491
00:32:36,920 --> 00:32:40,320
to build the information superstructures of the future, and I don't feel that's more

492
00:32:40,320 --> 00:32:41,320
than a few years away.

493
00:32:41,320 --> 00:32:44,280
And it's completely unpredictable what that will create.

494
00:32:44,280 --> 00:32:51,560
Let's talk about responsibility that AI companies have for making sure that their technology

495
00:32:51,560 --> 00:32:57,040
is used in a pro-human and not a disruptive fashion.

496
00:32:57,040 --> 00:33:02,160
Do you think that is a responsibility of a company, of a company's board, of a company's

497
00:33:02,160 --> 00:33:03,160
leadership?

498
00:33:03,160 --> 00:33:05,160
How do you think about that?

499
00:33:05,600 --> 00:33:10,200
Again, with the corporate capitalist system, it typically isn't, because you're maximizing

500
00:33:10,200 --> 00:33:13,800
shareholder value and there aren't laws and regulations, which is why I think there's

501
00:33:13,800 --> 00:33:18,640
a moral, a social, and legal-slash-regulatory aspect to this.

502
00:33:18,640 --> 00:33:22,480
Companies will just look at the legal-slash-regulatory, in some cases they'll just ignore them, right?

503
00:33:22,480 --> 00:33:28,080
But I do think, again, we have a bigger moral and social obligation to this.

504
00:33:28,080 --> 00:33:31,520
This is why I don't subscribe to EA or EAAC or any of these things.

505
00:33:31,520 --> 00:33:34,600
I think it's complicated and it's hard.

506
00:33:34,640 --> 00:33:38,400
Given the uncertainty and how this technology proliferates, and you've got to do your best

507
00:33:38,400 --> 00:33:44,800
and be as straight as possible to people about doing your best, because none of us have qualified

508
00:33:44,800 --> 00:33:47,120
to understand or do this.

509
00:33:47,120 --> 00:33:51,240
And none of us should be trusted to have the power over this technology, right?

510
00:33:51,240 --> 00:33:54,240
You should be questioned and you should be challenged with that.

511
00:33:54,240 --> 00:33:58,560
And again, if you're not transparent, how are you going to challenge?

512
00:33:58,560 --> 00:34:03,320
When I think of the most linear organizations on the planet, I think of governments, maybe

513
00:34:03,360 --> 00:34:07,080
religions, but governments, let's leave it there.

514
00:34:07,080 --> 00:34:09,080
How can...

515
00:34:09,080 --> 00:34:15,480
Let's talk about Western government, at least the U.S., I would have said Europe, but I'll

516
00:34:15,480 --> 00:34:19,880
say the U.K. and Europe.

517
00:34:19,880 --> 00:34:21,200
What should they be...

518
00:34:21,200 --> 00:34:24,600
What steps should they be taking right now?

519
00:34:24,600 --> 00:34:30,520
If you were given the reins to say, how would you regulate, what would you want them to

520
00:34:30,560 --> 00:34:32,560
do or not do?

521
00:34:32,560 --> 00:34:38,000
I believe it's a complicated one, so I signed the first FLI letter.

522
00:34:38,000 --> 00:34:42,840
I think I was the only AI CEO to do that back before it was cool, because I said, I don't

523
00:34:42,840 --> 00:34:45,360
think AGI will kill us all, but I just don't know.

524
00:34:45,360 --> 00:34:49,400
I think it's a conversation that deserves to be had, and it's a good way to have a conversation.

525
00:34:49,400 --> 00:34:53,880
And then we flipped the wrong way, where we went overly AI death risk and other things

526
00:34:53,880 --> 00:34:58,080
like that, and governments were doing that, at the AI Safety Summit in the U.K., and then

527
00:34:58,120 --> 00:35:01,840
we had the King of England come out, and so this is the biggest thing since fire.

528
00:35:01,840 --> 00:35:06,000
I was like, okay, that's a big change in the world, that's right.

529
00:35:06,000 --> 00:35:10,240
The King of England said it, so I must be on the right track.

530
00:35:10,240 --> 00:35:12,640
But I think if you look at it, regulation doesn't move faster.

531
00:35:12,640 --> 00:35:14,600
Even the executive order will take a long time.

532
00:35:14,600 --> 00:35:16,400
The EU things will kind of come in.

533
00:35:16,400 --> 00:35:20,680
Instead, I think that governments have to focus on the tangibles.

534
00:35:20,680 --> 00:35:24,920
AI killerism, again, it can be addressed by considering this as infrastructure and what

535
00:35:24,920 --> 00:35:27,960
infrastructure we need to give our people to survive and thrive.

536
00:35:28,000 --> 00:35:32,000
The U.S. is in a good initial place with the CHIPS Act, but I think you need national

537
00:35:32,000 --> 00:35:36,880
data sets, you need to provide open models to stoke innovation, and think about what

538
00:35:36,880 --> 00:35:40,800
the jobs of the future are, because things are never the same again.

539
00:35:40,800 --> 00:35:44,520
You don't need all those programmers when co-pilot is so good, and you're moving co-pilot

540
00:35:44,520 --> 00:35:50,240
from level above, which is compositional co-pilot, and then collaborative co-pilot, right?

541
00:35:50,240 --> 00:35:53,240
You would be able to talk and computers can talk to computers better than humans can talk

542
00:35:53,240 --> 00:35:54,760
to computers.

543
00:35:54,840 --> 00:35:58,200
We need to articulate the future on that side, but then the other side.

544
00:35:58,200 --> 00:36:04,200
One of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer,

545
00:36:04,200 --> 00:36:05,200
right?

546
00:36:05,200 --> 00:36:07,200
We did a United Order of this.

547
00:36:07,200 --> 00:36:11,280
The loss of agency you feel, and many of you on this call will have had that diagnosis

548
00:36:11,280 --> 00:36:13,000
to the near and dear, is huge.

549
00:36:13,000 --> 00:36:18,400
Then I had 1,000 AI agents fighting every piece of information about pancreatic cancer,

550
00:36:18,400 --> 00:36:20,560
and then after that, I felt a bit more control.

551
00:36:20,840 --> 00:36:24,840
Why don't we have a global cancer model that gives you all the knowledge about cancer and

552
00:36:24,840 --> 00:36:30,160
helps you talk to your kids and connect with people like you, not for diagnosis or research,

553
00:36:30,160 --> 00:36:31,880
but for humans?

554
00:36:31,880 --> 00:36:36,320
This is the Google MedPalM2 model, for example, that outperforms humans in diagnosis, but

555
00:36:36,320 --> 00:36:37,320
also empathy.

556
00:36:37,320 --> 00:36:43,520
What if we arm our graduates to go out and give support to the humans that are being

557
00:36:43,520 --> 00:36:45,400
diagnosed in this way?

558
00:36:45,400 --> 00:36:49,360
That makes society better, and it's valuable, you know?

559
00:36:49,360 --> 00:36:51,920
That's an example of a job of the future, I think.

560
00:36:51,920 --> 00:36:52,920
I don't believe in UBI.

561
00:36:52,920 --> 00:36:57,080
I think we've been universal basic jobs as well, or used jobs.

562
00:36:57,080 --> 00:36:58,080
Universal basic opportunity.

563
00:36:58,080 --> 00:36:59,080
Right?

564
00:36:59,080 --> 00:37:00,080
Yeah.

565
00:37:00,080 --> 00:37:03,880
Universal basic opportunity, universal based jobs, but then post-makers need to think

566
00:37:03,880 --> 00:37:10,080
about it now, because the graduate unemployment wave is literally a few years away, and it

567
00:37:10,080 --> 00:37:11,080
will happen.

568
00:37:11,080 --> 00:37:15,200
Yeah, that is, I mean, when I think about what, I parse the challenges we're going

569
00:37:15,200 --> 00:37:19,720
to be facing in society into a few different elements.

570
00:37:19,720 --> 00:37:25,360
I think what we have today is amazing, and if generative AI froze here, we'd have an

571
00:37:25,360 --> 00:37:30,440
incredible set of tools to help humanity across all of its areas.

572
00:37:30,440 --> 00:37:34,280
And then we've got what's coming in the next zero to five years.

573
00:37:34,280 --> 00:37:40,040
We've talked about patient zero, perhaps being the U.S. elections, and I think you

574
00:37:40,040 --> 00:37:43,120
had said it was Cambridge Analytica that required interference.

575
00:37:43,360 --> 00:37:48,080
Now it's any kid in the garage that could play with the elections.

576
00:37:48,080 --> 00:37:53,960
That's a challenging period of time, and this graduate unemployment wave, as you mentioned,

577
00:37:53,960 --> 00:37:59,280
coming right on its heels.

578
00:37:59,280 --> 00:38:04,480
The question becomes, is the only thing that can create alignment and help us overcome

579
00:38:04,480 --> 00:38:10,760
this AGI at the highest level, meaning it is causing challenges, but ultimately is a

580
00:38:10,800 --> 00:38:13,880
tool that will allow us to be able to solve these challenges as well.

581
00:38:15,680 --> 00:38:17,320
I mean, that's a crazy thought, right?

582
00:38:18,320 --> 00:38:22,440
Like, all this stuff is crazy, like the sheer scale and impact of it.

583
00:38:22,440 --> 00:38:27,440
And, you know, these discussions, we had them last year, Peter, and now everyone's

584
00:38:27,440 --> 00:38:28,320
like, yeah, that makes sense.

585
00:38:28,320 --> 00:38:33,360
And I go, wow, right, it may be AGI, it may be these coordinating automated

586
00:38:33,360 --> 00:38:35,600
story makers and balances from the market, right?

587
00:38:36,200 --> 00:38:39,880
Next year, there's 56 elections with four billion people heading to the polls.

588
00:38:41,360 --> 00:38:42,440
What could possibly go wrong?

589
00:38:42,440 --> 00:38:45,960
Okay, possibly go wrong, you know?

590
00:38:46,120 --> 00:38:46,760
Oh, my God.

591
00:38:46,800 --> 00:38:49,160
But again, the technology isn't going to stop.

592
00:38:49,160 --> 00:38:53,880
Like, even if stability puts down things, if open AI puts down things, it will

593
00:38:53,880 --> 00:38:58,760
continue from around the world because you don't need much to train these models.

594
00:38:58,760 --> 00:39:00,720
Again, the supercomputer thing is a myth.

595
00:39:01,200 --> 00:39:02,800
You've got another year or two where you need them.

596
00:39:02,800 --> 00:39:05,600
You don't need them after that, and that is insane to think about.

597
00:39:06,680 --> 00:39:08,160
You just released stability video.

598
00:39:08,160 --> 00:39:10,520
Congratulations on our stable visual diffusion.

599
00:39:10,520 --> 00:39:11,160
Thank you.

600
00:39:12,720 --> 00:39:14,840
And I'm enjoying some of the clips.

601
00:39:15,320 --> 00:39:23,680
How far are we away from me telling a story to my kids and saying, let's make

602
00:39:23,680 --> 00:39:24,480
that into a movie?

603
00:39:27,080 --> 00:39:27,920
Almost two years away.

604
00:39:27,920 --> 00:39:28,520
Two years away.

605
00:39:29,840 --> 00:39:31,160
So this is a building block.

606
00:39:31,160 --> 00:39:32,400
It's the best creation step.

607
00:39:32,440 --> 00:39:36,120
And then, like I said, you have the control step, composition, and then

608
00:39:36,120 --> 00:39:39,520
collaboration and self-learning systems around that.

609
00:39:39,520 --> 00:39:43,480
So we have Kung Fu UI, which is our node-based system where you have all

610
00:39:43,480 --> 00:39:47,240
the logic that makes up an image, like you can take a dress and a pose and

611
00:39:47,240 --> 00:39:48,520
a face that combines them all.

612
00:39:49,080 --> 00:39:53,080
And it's all encoded in the image because you can move beyond files to

613
00:39:53,120 --> 00:39:55,120
intelligent workflows that you can collaborate with.

614
00:39:55,560 --> 00:39:59,760
If I send you that image file and you put it into your Kung Fu UI, it gives you

615
00:39:59,760 --> 00:40:00,840
all the logic that made that up.

616
00:40:01,320 --> 00:40:02,200
How insane is that?

617
00:40:03,480 --> 00:40:05,200
So we're going to step up there.

618
00:40:05,320 --> 00:40:11,560
And what's happened now is that people are looking at this AI like instant versus

619
00:40:11,720 --> 00:40:15,160
again, the huge amount of effort it took to take this information and

620
00:40:15,160 --> 00:40:16,200
structure it before.

621
00:40:17,040 --> 00:40:19,600
But the value is actually in stuff that takes a bit longer.

622
00:40:20,160 --> 00:40:24,160
Like when you're shooting a movie, you don't just say, do it all in one shot,

623
00:40:24,160 --> 00:40:24,520
right?

624
00:40:24,680 --> 00:40:29,560
Unless you are a very talented director and actor, you know, you have

625
00:40:29,560 --> 00:40:32,640
mise en place, you have staging, you have blocking, you have cinematography.

626
00:40:33,160 --> 00:40:35,880
It takes a while to composite the scenes together.

627
00:40:36,480 --> 00:40:40,280
It'll be the same for this, but a large part of it will then be automated for

628
00:40:40,280 --> 00:40:43,440
creating the story that can resonate with you and you can turn it into

629
00:40:43,440 --> 00:40:44,400
Korean or whatever.

630
00:40:44,920 --> 00:40:47,960
And there'll still be big blog clusters like Oppenheimer and Barbie.

631
00:40:48,440 --> 00:40:51,480
But again, the flaw will be raised overall.

632
00:40:52,200 --> 00:40:55,960
Similarly, like we had a music video competition check it on YouTube with Peter

633
00:40:55,960 --> 00:41:00,200
Gabriel, he allows us to use kindly his songs and people from all around the

634
00:41:00,200 --> 00:41:03,440
world made amazing music videos to his thing, but they took weeks.

635
00:41:04,360 --> 00:41:06,800
So I think that's somewhere in the middle here where again, we're just at

636
00:41:06,800 --> 00:41:12,440
that early stage, because chat GPT isn't even a year old, you know, stable

637
00:41:12,440 --> 00:41:13,760
diffusion is only 14, 15 months.

638
00:41:13,760 --> 00:41:19,520
And I think you'd agree that neither of them is the end hole and be all.

639
00:41:19,520 --> 00:41:21,880
It's just, it's the earliest days of this field.

640
00:41:23,000 --> 00:41:23,760
I had the conversation.

641
00:41:23,840 --> 00:41:24,920
The tiniest building.

642
00:41:24,920 --> 00:41:28,400
Yeah, I had this conversation with Ray Kurzweil two weeks ago.

643
00:41:29,360 --> 00:41:33,200
We're just after a singularity board meeting we had, and we're just on a

644
00:41:33,200 --> 00:41:34,160
zoom and chatted.

645
00:41:34,560 --> 00:41:40,080
And, you know, the realization is that, unfortunately, the human mind is

646
00:41:40,320 --> 00:41:42,080
awful at exponential projections.

647
00:41:42,080 --> 00:41:47,080
And despite the convergence of all these technologies, we tend to project

648
00:41:47,080 --> 00:41:52,120
the future as a linear extrapolation of, you know, the world we're living in right

649
00:41:52,120 --> 00:41:52,480
now.

650
00:41:53,480 --> 00:41:58,040
But the best I can say is that we're going to see in the next decade, right,

651
00:41:58,040 --> 00:42:01,920
between now and 2033, we're going to see a century worth of progress.

652
00:42:02,520 --> 00:42:05,560
But it's going to get very weird, very fast, isn't it?

653
00:42:07,440 --> 00:42:10,200
I mean, there's two way doors and there's one way doors, right?

654
00:42:10,760 --> 00:42:15,040
Like in December of last year, multiple headmasters called me and said,

655
00:42:15,640 --> 00:42:17,560
we can't set essays for our homework anymore.

656
00:42:17,920 --> 00:42:20,200
And every headmaster in the world had to do that same thing.

657
00:42:20,200 --> 00:42:20,960
It's a one-way door.

658
00:42:21,320 --> 00:42:21,840
Yes.

659
00:42:22,680 --> 00:42:25,240
And this is the scary part, the one-way doors, right?

660
00:42:26,080 --> 00:42:31,120
Like when you have an AI that can do your taxes, what does that mean for

661
00:42:31,120 --> 00:42:31,720
accountants?

662
00:42:33,440 --> 00:42:34,640
All the accountants at the same time.

663
00:42:36,560 --> 00:42:37,360
Kind of crazy, right?

664
00:42:37,440 --> 00:42:38,280
It is.

665
00:42:39,080 --> 00:42:43,160
And the challenge, I mean, one of my biggest concerns, so listen, I'm the

666
00:42:43,200 --> 00:42:44,080
eternal optimist.

667
00:42:44,080 --> 00:42:47,320
I'm not the guy who's the glasses half full, it's the glasses overflowing.

668
00:42:48,280 --> 00:42:57,000
And one of the challenges I think through when I think about where AI, AGI, ASI,

669
00:42:57,040 --> 00:43:03,720
however you want to project it to is the innate importance of human purpose.

670
00:43:04,600 --> 00:43:09,600
And unfortunately, most of us derive our purpose from the work that we do.

671
00:43:10,240 --> 00:43:14,800
You know, I ask you, you know, tell me about yourself and you jump into your

672
00:43:14,840 --> 00:43:15,920
work and what you do.

673
00:43:16,400 --> 00:43:21,320
And so when AI systems are able to do most everything we do, not just a little

674
00:43:21,320 --> 00:43:27,680
bit better, but, you know, orders of magnitude better, redefining purpose and

675
00:43:27,680 --> 00:43:36,320
redefining my role in achieving a moonshot or a transformation is, it's the,

676
00:43:36,680 --> 00:43:45,880
you know, it's the impedance mismatch between human societal growth rates

677
00:43:46,040 --> 00:43:47,600
and tech growth rates.

678
00:43:48,160 --> 00:43:49,000
What are your thoughts there?

679
00:43:50,640 --> 00:43:52,640
Yeah, I mean, I think again, exponentials are hard.

680
00:43:52,760 --> 00:43:58,840
Like if I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well,

681
00:43:58,840 --> 00:43:59,520
that's not possible.

682
00:43:59,560 --> 00:44:00,160
Why?

683
00:44:00,680 --> 00:44:04,840
You know, like GPT-4 is impossible, stable diffusion is impossible, right?

684
00:44:05,520 --> 00:44:09,200
Like now they've almost become commonplace, but why would you need super

685
00:44:09,200 --> 00:44:10,080
computers and these things?

686
00:44:10,720 --> 00:44:15,880
I do agree this mismatch and that's why we're in for five years of chaos.

687
00:44:16,040 --> 00:44:20,200
That's why I called it stability because I saw this coming a few years ago and

688
00:44:20,200 --> 00:44:23,400
I was like, holy crap, we have to build this company.

689
00:44:24,080 --> 00:44:28,600
And now we have the most downloads of any models of any company, like 50 million

690
00:44:28,600 --> 00:44:31,480
last month versus 700,000 from Astral, for example.

691
00:44:33,000 --> 00:44:36,640
And we will have the best model of every type except for very large language

692
00:44:36,640 --> 00:44:37,920
models by the end of the year.

693
00:44:38,520 --> 00:44:43,160
So we have audio, 3D, video, code, everything and a lovely, amazing

694
00:44:43,160 --> 00:44:48,520
community because it's just so hard again for us to imagine this mismatch.

695
00:44:48,560 --> 00:44:49,720
There's a period of chaos.

696
00:44:50,040 --> 00:44:54,000
But then on the other side, like there's this PDoom question, right?

697
00:44:54,000 --> 00:44:55,280
The probability of doom.

698
00:44:56,640 --> 00:45:00,040
I can say something with this technology, the probability of doom is lower

699
00:45:00,040 --> 00:45:03,280
than without this technology because we're killing ourselves.

700
00:45:04,360 --> 00:45:07,720
And this can be used to enhance every human and coordinate us all.

701
00:45:08,600 --> 00:45:11,360
And I think what we're aiming for is that Star Trek future versus that Star

702
00:45:11,360 --> 00:45:11,600
Wars.

703
00:45:11,680 --> 00:45:13,320
Yes, I meant to that.

704
00:45:15,200 --> 00:45:20,240
And I think that's an important point, the level of complexity that we have

705
00:45:20,240 --> 00:45:24,240
in society, we don't need AI to destroy the planet.

706
00:45:24,840 --> 00:45:27,280
We're doing that very, very well.

707
00:45:27,280 --> 00:45:27,680
Thank you.

708
00:45:28,680 --> 00:45:30,760
But the ability to coordinate.

709
00:45:30,920 --> 00:45:35,680
So one of the things I think about is a world in which everyone has access to

710
00:45:35,680 --> 00:45:39,200
all the food, water, energy, healthcare, education that they want.

711
00:45:39,840 --> 00:45:47,800
Really, a world of true abundance in my mind is a piece more peaceful world, right?

712
00:45:47,840 --> 00:45:51,160
Why would you want to destroy things if you have access to everything that you

713
00:45:51,160 --> 00:45:51,480
need?

714
00:45:52,240 --> 00:45:58,160
And that kind of a world of abundance is on the backside of this kind of

715
00:45:58,160 --> 00:45:59,080
awesome technology.

716
00:46:00,440 --> 00:46:02,040
We have to navigate the next period.

717
00:46:02,080 --> 00:46:05,040
I believe we'll see it within our lifetimes, particularly if we get

718
00:46:05,040 --> 00:46:06,160
longevity songs, right?

719
00:46:07,760 --> 00:46:09,240
And that's so amazing, right?

720
00:46:09,480 --> 00:46:11,640
But then we think about, as you said, why peace?

721
00:46:12,600 --> 00:46:15,200
A child in Israel is the same as a child in Gaza.

722
00:46:15,720 --> 00:46:17,120
And then something happens.

723
00:46:17,120 --> 00:46:21,720
A liar is told that you are not like others and the other person is not human

724
00:46:21,720 --> 00:46:22,160
like you.

725
00:46:22,640 --> 00:46:24,280
All wars are based on that same line.

726
00:46:25,760 --> 00:46:29,720
And so again, if we have AI that is aligned with the potential of each human

727
00:46:29,720 --> 00:46:34,520
that can help mitigate those lies, then we can get away from war because

728
00:46:35,840 --> 00:46:37,160
the world is not scarce.

729
00:46:38,160 --> 00:46:39,360
There is enough food for everyone.

730
00:46:39,400 --> 00:46:40,840
It's a coordination failure.

731
00:46:42,360 --> 00:46:44,040
And that can be addressed by this technology.

732
00:46:44,040 --> 00:46:44,160
I agree.

733
00:46:44,160 --> 00:46:49,200
One of the most interesting and basic functions or capabilities of generative

734
00:46:49,240 --> 00:46:54,960
AI has been the ability to translate my ideas into concepts that someone who is

735
00:46:54,960 --> 00:46:56,960
a different frame of thought can understand.

736
00:46:57,760 --> 00:46:58,040
Right?

737
00:47:00,000 --> 00:47:01,840
But that's what this generative AI is.

738
00:47:01,880 --> 00:47:03,880
It's a universal translator.

739
00:47:04,040 --> 00:47:04,440
Sure.

740
00:47:04,760 --> 00:47:06,320
It does not have facts.

741
00:47:06,320 --> 00:47:08,360
The fact that it knows anything is insane.

742
00:47:08,400 --> 00:47:10,600
Hallucinations is a crazy thing to say.

743
00:47:10,680 --> 00:47:12,440
Again, it's just like a graduate trying so hard.

744
00:47:13,560 --> 00:47:17,760
GPT-4 with 10 trillion words and 100 gigabytes is insane.

745
00:47:18,440 --> 00:47:23,200
Stable diffusion has like 100,000 gigabytes and a two gigabyte file.

746
00:47:23,200 --> 00:47:26,760
50,000 to one compression is something else.

747
00:47:26,800 --> 00:47:28,000
It's learned principles.

748
00:47:28,160 --> 00:47:28,640
Yes.

749
00:47:28,880 --> 00:47:31,280
And this is it's knowledge, knowledge versus data.

750
00:47:31,440 --> 00:47:31,720
Yeah.

751
00:47:32,080 --> 00:47:33,640
It's knowledge versus data.

752
00:47:33,640 --> 00:47:35,640
And if you have some experience, you get the wisdom, right?

753
00:47:35,640 --> 00:47:36,000
Yes.

754
00:47:36,000 --> 00:47:40,760
Because it's learned the principles and contexts and it can map them to

755
00:47:40,760 --> 00:47:43,880
transform data because that's how you navigate.

756
00:47:44,520 --> 00:47:49,120
You don't navigate based on like logical flow.

757
00:47:49,120 --> 00:47:52,000
We have those two parts of our brain navigate sometimes based on instinct

758
00:47:52,000 --> 00:47:53,560
based on the principles you've learned.

759
00:47:53,920 --> 00:47:58,640
So Tesla's new auto driving model, self driving model is entirely based on

760
00:47:58,640 --> 00:48:02,000
a console, which are protection, like they said it publicly is based on this

761
00:48:02,000 --> 00:48:02,520
technology.

762
00:48:02,520 --> 00:48:03,840
It doesn't have any rules.

763
00:48:04,440 --> 00:48:07,680
It's just learned the principles of how to drive from massive amounts of Tesla

764
00:48:07,680 --> 00:48:11,120
data that now fits on the hardware without internet.

765
00:48:11,880 --> 00:48:15,560
And so they went from self driving being impossible to now, hey, it looks pretty

766
00:48:15,560 --> 00:48:18,920
well, you know, because it's learned the principles.

767
00:48:19,280 --> 00:48:22,720
And so that's why this technology can help solve the problem.

768
00:48:22,720 --> 00:48:26,720
This is why it can help us amplify our intelligence and innovation.

769
00:48:27,720 --> 00:48:31,160
Because the missing part, the second part of the frame, you know, next, I can't

770
00:48:31,160 --> 00:48:35,600
give more details yet, but next week we're announcing the largest X prize ever.

771
00:48:35,600 --> 00:48:37,200
It's a hundred and one million dollars.

772
00:48:38,200 --> 00:48:39,360
It's a hundred and one.

773
00:48:39,360 --> 00:48:45,360
So it's Elon had the had a hundred million dollar prize that kind of defund a few

774
00:48:45,360 --> 00:48:50,360
years ago for carbon sequestration and the funder, the first funder of this prize

775
00:48:50,360 --> 00:48:51,920
wanted to be larger than Elon's.

776
00:48:51,920 --> 00:48:53,800
I said, okay, you had the extra million.

777
00:48:53,880 --> 00:48:54,800
It's for luck.

778
00:48:54,800 --> 00:48:55,400
It's for luck.

779
00:48:55,400 --> 00:48:56,880
We did our seed round 101 million.

780
00:48:56,880 --> 00:48:57,400
Oh, really?

781
00:48:57,400 --> 00:48:57,880
Okay.

782
00:48:57,880 --> 00:48:58,880
Hi, that's great.

783
00:48:59,880 --> 00:49:00,880
It's on your popular number.

784
00:49:01,880 --> 00:49:06,280
Anyway, the and it's in the field of health.

785
00:49:06,280 --> 00:49:12,040
I'll leave it at that folks go to XPRIZE.org to register to see the live event on

786
00:49:12,040 --> 00:49:12,880
November 29th.

787
00:49:12,880 --> 00:49:16,240
We're going to be debuting the prize, what it is, what it's going to impact

788
00:49:16,240 --> 00:49:21,000
eight billion people, long story short, it's, it's a nonlinear,

789
00:49:21,080 --> 00:49:28,440
future because we are able to utilize AI and make things that were seemingly

790
00:49:28,440 --> 00:49:32,480
crazy before, likely to become inevitable.

791
00:49:33,480 --> 00:49:35,880
And that's an amazing future we have to live into.

792
00:49:38,080 --> 00:49:43,240
Yeah, I mean, again, because it's one way doors, the moment we create a cancer

793
00:49:43,240 --> 00:49:46,680
GPT, and this is something that we're building, we have trillions of tokens

794
00:49:46,680 --> 00:49:48,080
and then you Google TV.

795
00:49:48,160 --> 00:49:52,960
Things like that, that organizes global cancer knowledge and makes it accessible

796
00:49:52,960 --> 00:49:57,160
and useful, even if it's just for guiding people that have been diagnosed.

797
00:49:57,160 --> 00:49:58,560
The world changes.

798
00:49:58,560 --> 00:50:02,560
The 50% of people that have a cancer diagnosed in their lives in every language

799
00:50:02,560 --> 00:50:06,360
and every level will have someone to talk to and connect them with the resources

800
00:50:06,360 --> 00:50:09,760
they need and other people like them and talk to their families.

801
00:50:09,760 --> 00:50:11,760
You know, and how insane is that?

802
00:50:12,760 --> 00:50:16,560
And so again, the least positive thing is that we're going to be able to

803
00:50:16,560 --> 00:50:20,040
and so again, the least positive stories in the future need to be told, right?

804
00:50:20,040 --> 00:50:25,120
Because that will align us to where we need to go as opposed to a future full

805
00:50:25,120 --> 00:50:26,920
of uncertainty and craziness and doom.

806
00:50:28,240 --> 00:50:33,440
In our last couple minutes here, buddy, what can we look forward to from stability

807
00:50:34,520 --> 00:50:37,120
in the months and years ahead?

808
00:50:38,080 --> 00:50:41,040
We have every model of every type and we'll build it for every nation

809
00:50:41,040 --> 00:50:42,920
and we'll give back control to every nation.

810
00:50:42,920 --> 00:50:45,520
So coming back to governance here.

811
00:50:47,160 --> 00:50:51,040
Again, is the nation state the unit of control?

812
00:50:52,400 --> 00:50:56,880
Is it? No, I my my thinking is disabilities in every nation

813
00:50:56,880 --> 00:51:00,360
should have the best and brightest of each because what you've seen is

814
00:51:00,360 --> 00:51:04,360
there are amazing people in this sphere, the best and brightest in the world.

815
00:51:04,360 --> 00:51:07,880
Now, this is the biggest thing ever and they all want to work in it.

816
00:51:07,960 --> 00:51:10,720
And it's just finding the right people with the right intention.

817
00:51:11,240 --> 00:51:14,120
The brightest people go back to Singapore or Malaysia or others

818
00:51:14,600 --> 00:51:16,440
because of the future of their nations.

819
00:51:16,440 --> 00:51:19,440
And again, now we're doing a big change and we don't talk

820
00:51:19,440 --> 00:51:21,160
about all the cool stuff we do.

821
00:51:21,160 --> 00:51:23,240
We've just taken it because you need to articulate

822
00:51:23,240 --> 00:51:26,280
that positive vision of the future because the only scarce resource

823
00:51:26,280 --> 00:51:27,800
is actually this is human capital.

824
00:51:27,800 --> 00:51:29,400
It's not GPUs.

825
00:51:29,400 --> 00:51:30,720
It's not data.

826
00:51:30,720 --> 00:51:35,280
It's about the humans that can see this technology and realize that

827
00:51:35,280 --> 00:51:38,320
they can play a part in guiding it for the good of everyone,

828
00:51:39,080 --> 00:51:40,800
their own societies and more.

829
00:51:40,800 --> 00:51:43,520
And that's again, what I hope stability can be.

830
00:51:43,520 --> 00:51:45,600
Well, I wish you the best of luck, pal.

831
00:51:45,600 --> 00:51:48,160
Thank you for joining me in this conversation.

832
00:51:48,160 --> 00:51:50,920
It's it's been a crazy four or five days.

833
00:51:52,200 --> 00:51:55,880
And wish Sam and Greg and the entire

834
00:51:56,880 --> 00:52:00,520
opening I team stability in their lives.

835
00:52:00,960 --> 00:52:03,520
Yeah, I have a nice Thanksgiving.

836
00:52:03,520 --> 00:52:07,560
They're absolutely they're an amazing team building world changing technology.

837
00:52:07,960 --> 00:52:09,440
It's such a concentration of talent.

838
00:52:09,480 --> 00:52:13,760
I think, again, I really felt for them over the last few days,

839
00:52:13,840 --> 00:52:15,880
you know, much as I kind of post memes and everything.

840
00:52:15,880 --> 00:52:17,840
I posted that as well.

841
00:52:17,840 --> 00:52:21,280
I think this will bring them closer together and hopefully they can solve

842
00:52:21,280 --> 00:52:24,280
the number one problem that I've asked them to solve, which is email.

843
00:52:25,000 --> 00:52:27,080
Solve email, right?

844
00:52:27,160 --> 00:52:29,000
And then we'll crack on from there.

845
00:52:29,000 --> 00:52:30,000
All right, cheers, my friend.

