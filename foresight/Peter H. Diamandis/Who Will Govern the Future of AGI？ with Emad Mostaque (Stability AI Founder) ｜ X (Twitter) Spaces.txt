Hey, Amade, good to hear you.
It was always a pleasure.
Yeah.
So, where are you today?
I'm in London.
Good.
Other side of the planet, I'm in Santa Monica.
It's been quite the extraordinary game of ping pong out there these last four or five
days.
I didn't think the first thing that AI would disrupt would be reality TV, right?
Yeah.
It's been fascinating how X has become sort of the go-to place to find out the latest
of where Sam is working and what's going on with the AI industry.
Yeah, you found the notifications in the wake, guys.
I mean, it's the thing, like, what else will move at the speed of this?
Like, I was saying to someone recently, AI research doesn't really move at the speed
of conferences or even PDFs anymore, right?
You just wake up and you're like, oh, it's 10 times faster.
So, I think that's why X is quite good.
I actually, like, unfollow just about everyone and just the AI algorithms find the most interesting
things for me.
So, I've got, like, 10 people that I follow, and it's actually working really well.
It's getting better.
Well, it has been.
I've been enjoying the conversation.
It really feels like you're inside an intimate conversation among friends as this is going
back and forth.
I think this entire four or five days has been an extraordinary, up-close, intimate conversation
around governance and around, you know, what's the future of AI?
Honestly, you know, as it gets faster and more powerful, the cost of missteps is going
to increase exponentially.
Let's begin here.
I mean, you've been making the argument about open source as one of the most critical elements
of governance for a while now.
Can you just, let's hop into that.
Yeah, I think that, you know, open source is the difficult one because it means a few
different things.
Like, is it models you can download and use?
Do you make all the data available and free?
And then when you actually look at what all these big companies do, all their stuff is
built on open source basis, you know, it's built on the transformer paper.
It's built on, like, the new model by Khayfouli and Vira1.ai is basically Lama.
Like it's actually got the same variable names and other things like that, plus a gigantic
supercomputer, right?
The whole conversation has been, you know, how important is openness and transparency?
And what are the governance models that are going to allow the most powerful technology
on the planet to enable the most benefit for humanity and the safety?
So, I mean, you've been thinking about this and speaking to transparency, openness, governance
for a while.
Could you, I mean, what do you think is going to be, what do you think is we need to be
focused on?
Where do we need to evolve to?
Yeah, it's a complicated topic.
I think that most of the infrastructure of the Internet is open source, Linux, everything
like that.
I think these models, it's unlikely that our governments will be run on GPT-7 or Baal
or anything like that.
How are you going to have black boxes that run these things?
I think a lot of the governance debate has been hijacked by the AI safety debate where
people are talking about AGI killing us all, and then there's this precautionary principle
that kicks in.
It's too dangerous to let out because what if China gets it?
What if someone builds an AGI that kills us all?
It'd be great to have this amazing board that could pull the off switch, you know, whereas
in reality, I think that you're seeing a real social impact from this technology, and it's
about who advances forward and who's left behind if we're thinking about risk.
Because governance is always about finding, as you said, the best outcomes and also mitigating
against the harms, right?
And there's some very real, amazingly positive outcomes that are now emerging that people
can agree on, but also some very real social impacts that we have to mitigate against.
So I mean, let's begin, how is stability governed?
Stability is basically governed by me.
So I looked in foundations and DAOs and everything like that, and I thought to take it to where
we are now.
I needed to have very singular governance, but now we're looking at other alternatives.
And what do you think it's going to be?
Where would you head in the future?
I mean, let's actually jump away from this in particular.
But do you recommend the most powerful technologies on the planet?
How should they be governed?
How should they be owned?
You know, where should we be in five years?
I think there need to be public goods that are collectively owned and then individually
owned as well.
So for example, there was the tweet kind of storm, the kind of I am Spartacus or his
name is Robert Boulson from the OpenAI team saying, OpenAI is nothing without its people.
Stability, we have amazing people, 190 and 65 top researchers.
Without its people, we're open models used by hundreds of millions, it continues.
And if you think about where you need to go, you can never have a choke point on this technology,
I think it becomes part of your life.
Like the phrase I have is not your models, not your mind.
So these models, again, are just such interesting things to take billions of images or trillions
of words and you get this file out that can do magic, right, trade or magic sand.
I think that you will have pilots that gather our global knowledge on various modalities
and you'll have co-pilots that you individually own that guide you through life.
And I can't see how that can be controlled by any one organization.
You've been on record talking about having models owned by the citizens of nations.
Can you speak to that a little bit?
Sure.
So we just released some of the top Japanese models from visual language to language to
Japanese SDXL as an example.
So we're training for half a dozen different nations and models now.
And the plan is to figure out a way to give ownership of these datasets and models back
to the people of that nation.
So you get the smartest people in Mexico to run a stability Mexico or maybe a different
structure that then makes decisions for Mexicans with the Mexicans about the data and what
goes in it.
Because everyone's been focusing on the outputs, the inputs actually are the things that matter
the most.
The best way I've thought about thinking of these models is like very enthusiastic graduates.
So hallucinations isn't just probably too hard.
A lot of the things about like, oh, what about these bad things the models can output?
It's about what you've input.
And so what you put into that Mexican dataset or the Chinese or Vietnamese one will impact
the outputs.
And there's a great paper in Nature, Human Behavior today about that, about how foundational
models are cultural technologies.
So again, how can you outsource your culture and your brains to other countries, to people
that are from a very different place?
I think it eventually has to be localized.
I think one of the points you said originally is we have to separate the issue of governance
versus safety and alignment.
Are they actually different?
So I think that a lot of the safety discussion or this AGI risk discussion is because the
future is so uncertain because it is so powerful.
And we didn't have a good view of where we're going.
So when you go on a journey and you don't know where you're going, you will minimize
from acts and regret you'll have the precautionary principle.
And then that means you basically go towards authority, you go towards trying to control
this technology when it's so difficult to control.
And you end up not doing much, you know, because there's anything to go wrong.
When you have an idea of where we're going, like you should have all the cancer knowledge
in the world at your fingertips or climate knowledge, or anybody should be able to create
whole worlds and share them.
When you align your safety discussions against the goal, against the location that you're
going to, again, just like setting out on a journey, I think that's a big change.
Similarly, most of the safety discussions have been on outputs, not inputs.
If you have a high quality data set without knowledge about anthrax, your language model
is unlikely to tell you how to build anthrax, you know, and transparency around that will
be very useful.
So let's dive into that safety alignment issue for a moment because it's an area you and
I have been talking a lot about.
So Mustafa wrote a book, Mustafa Suleiman wrote a book called The Coming Wave in which
he talks about containment as the mechanism by which we're going to be making sure we
have safe AI.
You and I have had the conversation of it's really how you educate and raise and train
your AI systems in making sure that there's full transparency and openness on the data
sets that are utilized.
Do you think containment is an option for safety?
No, not at all.
Like a number of leaders say, what if China gets open source AI?
The reality is that China, Russia, everyone already has the weights for GPT-PORC.
They just downloaded it on a USM stick.
You know that there's been compromised, right?
There's no way they couldn't.
The rewards are too great.
And there is a absolutely false dichotomy here and a lot of the companies want you to believe
that giant models are the main thing and you need to have these gigantic, ridiculous
supercomputers that only they can run.
I mean, look, we run gigantic supercomputers, but the reality is this.
The supercomputers and the giant trillion zillion data sets are just a shortcut for bad quality
data.
It's like using a hot pot or sous viding a steak that's bad quality.
You cook it for longer and it organizes the information.
With stable diffusion, we did a study and we showed that basically 92% of the data isn't
used 99% of the time, you know, because now you're seeing this with, for example, Microsoft's
by release, it's trained entirely on synthetic data.
Dali three is trained on RV AE and entirely synthetic data.
You are what you eat.
And again, we cooked it for longer to get past that.
But the implications of this are that I believe within 12 to 18 months, you'll see GPT four
level performance on a smartphone.
How do you contain that?
And how do you contain it when China can do distributed training at scale and release
open source models?
So Google recently did 50,000 TPU training run on their V5 is the new V5 is their TPUs
are very low powered relative to what we've seen.
But again, you can do distributed dynamic training.
Similarly, like we funded five mind and we've seen Google DeepMind just a new paper on localization
through distributed training.
The models are good for fast enough and cheap enough that you can swarm them and you don't
need giant supercomputers anymore.
And that has a lot of implications and how are you going to contain that?
So coming back to the question of do you mandate training training sets?
Do you does, you know, does the government set out what all companies should be utilizing
in mandate?
If you're going to have a aligned AI, it has to be trained on these sets.
How do we how do we possibly govern that?
Look, we have food standards, right?
For ingredients.
Why don't you have data standards for the ingredients that make up a model?
It's just data compute and some algorithms, right?
And so you should say they are the standards and then you can make it compulsory.
That will take a while or you can just have an ISR type standard.
This is good quality model training, good quality data, you know, and people will naturally
gravitate towards that and it becomes a default.
Are you working towards that right now?
Yeah, I mean, look, we spun out a Luther AI as an independent 501c3 so they could look
at data standards and things like that independent of us and the opposite of open AI.
And this is something I've been talking to many people about and we're getting national
data sets and more so that hopefully we can implement good standards similar to how we
offered opt out and how the billion images opted out of our image data set because everyone
was just training on everything.
Is required?
No.
But is it good?
Yes.
And everyone will benefit from better quality data.
So there's no reason that for these very large model training runs, the data sets should
not be transparent and logged.
But again, we want to know what goes into that.
And again, if we have the graduate analogy, what was the curriculum that the graduate
was taught at?
Which university did they go to?
It's something that we'd want to know.
But then why do we talk to GPT-4 where we don't know where it went to university or where
it's been trained on?
It's a bit weird that, isn't it?
What do you think the lesson is going to be from the last four days?
I'm just confused.
I don't know who was against who or what.
I'm going to just post it.
Are we against misalignment or mollock?
I think probably the biggest lesson is it's very hard to align humans, right?
And the stakes are very large.
Why is this so interesting to us?
The stakes are so high.
You tweeted something that was serious and unfortunately funny, which was how can we
align AI with humanity's best interests if we can't align our company's board with its
employee's best interests?
Yeah.
Well, the thing is it's not the employee's best interests.
It's like the board was set up as a lever to ensure the charter of open AI.
So if you look at the original founding document of open AI from 2015, it is a beautiful document
talking about open collaboration, everything.
And then it kind of changed in 2019.
But the charter still emphasizes cooperation, safety, and fundamental.
I posted about this back in March when I said the board and the government structure of
open AI is weird.
What is it for?
What are they trying to do?
Because if you say you're building AGI, in their own road to AGI, they say, this will
end democracy most likely.
I remember reading that.
Because democracy, there's no way democracy survives AGI.
Because either, obviously, it'll be better and you get it to do it or can dissuade everyone
or we all die.
Or it's utopia crap, all right?
Abundance, baby.
There's no way it survives AGI.
There's no way capitalism survives AGI.
The AGI will be the best trader in the world, right?
And it's like, who should be making the decisions on the AGI, assuming that they achieve those
things?
And that's in their own words.
So I think that people are kind of waking up to, oh, there's no real way to do this properly.
And previously, we were scared of open and being transparent, everyone getting this,
which can with the original thing of open air.
And now we're scared of, who are these clowns, you know, and put it in the nicest way.
Because this was ridiculous.
Like you see better politics in a teenage sorority, right?
And it's fundamentally scary, but unelected people, no matter how great they are, and
I think some of the board members are great, should have a say in something that could
literally upend our entire society according to their own words.
I find that inherently anti-democratic and illiberal.
At the end of the day, you know, capitalism has worked, and it's the best system that
we have thus far.
And it's a self, you know, it's built on self-interest and built on continuous optimization, maximization.
I'm still wondering where you go in terms of governing these companies at one level,
internal governance, and then governing the companies at a national and global level.
Has anybody put forward a plan that you think is worth highlighting here?
Not really.
I mean, organizations are a weird artificial intelligence, right?
They have the status of people, and they're slow dumb AI, and they eat up hopes and dreams.
That's what they feed on, I think.
This AI can upgrade them.
It can make them smarter.
They can how do you coordinate?
And from a mechanism design perspective, it's super interesting.
AI can market, so I think we will have AI market makers that can tell stories.
The story of Silicon Valley Bank went around the world in two seconds.
The story of open AI goes around.
AI can tell better stories than humans.
It's inevitable.
And I think that gives hope for coordination, but then also it's dangers of disruption.
I want to double click one second on the two words that you use most, openness and transparency,
and understand fully what those mean one moment, because, you know, and the question is,
not only what they mean, but how fundamental it needs to be.
So openness right now and your definition in terms of AI means what?
It means different things, but different things, unfortunately.
I don't think it means open source.
I think, for me, open means more about access and ownership of the models so that you don't
have a lockstep, like you can hire your own graduates as opposed to relying on consultants.
Security comes down to, I think, for language models in particular, I don't think this holds
some media models.
You really need to know what it's been taught.
That's the only way to safety.
You should not engage with something or use something if you don't know what its credentials
are and how it's been taught, because I think that's inherently dangerous as these get more
and more capabilities.
And again, I don't know if we get to SGI.
If we do, I think it'll probably be like Scarlett Johansson and her, you know, like just to
combine thanks to the GPs, but I'm assuming we don't.
You still need transparency.
Again, how can any government or regulated industry not run on a transparent model?
They can't run on Blackpops.
I get that, and I understand the rationale for it, but now the question is, can you prove
transparency?
I think that, again, a model is only three things, really.
It's the data, the algorithm, and the compute.
And they come and the binary file pops out.
You can tune it with RLHF or DPO or genetic algorithms or whatever, but that's really
the recipe, right?
And so the algorithms, you don't need algorithmic transparency here versus classical AI because
they're very simple.
One of our fellows recreated the Palm 540 billion parameter model.
This is Lucid Raines on GitHub.
You look at that.
If you're a developer and you want to cry, it's GitHub.
It's crazy.
In 206 lines of PyTorch, and that's it.
The algorithms are not very complicated.
Having a gigantic super computer is complicated, and this is why they freaked out when Greg
Brockman kind of stepped down because he's one of the most talented engineers of our
time.
He built these amazing gigantic clusters.
And then the data and how you structure data is complicated.
So I think you can have transparency there because if the data is transparent and who
cares about the supercomputer, who really cares about the algorithm?
Now, let's talk about the next term, alignment here.
Alignments thrown around in lots of different ways.
How do you define alignment?
I define alignment in terms of objective function.
So YouTube was used by the extremists to serve ads for their nastiness.
Why?
Because the algorithm optimized for engagement, which then optimized for extreme content, which
then optimized for the extremists.
Did YouTube mean that, no, but they're just trying to serve ads up, right?
But it meant it wasn't aligned with its users' interests.
And so for me, if you have these technologies that we're going to outsource for more of our
mind, our culture, our children's futures, to you that are very persuasive, we have to
ensure they're aligned with our individual community and societal best interests.
I think this is where the tension with corporations will come in.
Because whoever licenses Scarlett Johansson's voice will sell a lot of ads, you know, they
can be very, very persuasive, but then what are their controls on that?
No one talks about that.
The bigger question of alignment is not killerism, making sure that AI doesn't kill us.
But again, I feel that if we build AI that is transparent, that we can test, that people
can build mitigations around, we are more likely to survive and thrive.
And also, I think there's a final element to here, which is who's alignment.
Different cultures are different, different people are different.
What we found with stable diffusion is that when we merge together the models that different
people around the world have built, the model gets so much better.
I think that makes sense because a monoculture will always be less fragile than a diversity.
Again, I'm not talking about in the DEI kind of way, I'm talking about it in the actual
logical way.
So we have a paper from our reinforcement learning lab called CARPA called QDHF, QD-AIF, Quality
and Diversity through artificial intelligence feedback.
Because you find these models do get better with high quality and diverse inputs, just
like you will get better if you have high quality and diverse experiences, you know?
And saying that's something that's important, that we'll get lost if all these models are
centralized.
You and I have had a lot of conversations about timelines here.
We can get into a conversation of when and if we see AGI.
But we're seeing more and more powerful capabilities coming online right now that are going to
cause a lot of amazing progress and disruption.
How much time do we have, EMI, and we had a conversation when we were together at FII
about the disenfranchised youth coming off of COVID.
So let's talk one second about timelines.
How long do we have to get our shit together, both as AI companies and investors and governors
of society?
The speed here is awesome and frightening.
How long do we have?
Everything everywhere, all at once, right?
We don't have long.
Like, AGI timelines for every definition of AGI, I have no idea.
It will never be less than 12 months, right?
Because it's such a step change, so let's put that to the side.
Right now, everyone that's listening, are you all going to hire the same amount of graduates
that you hired before?
The answer is no.
Some people might not hire any because this is a productivity and an answer and we have
the data for that across any type of knowledge industry.
You just had a great app that you can sketch and it does a whole iPhone app for you, right?
I've gone on record and saying there are no programmers who know in five years, why would
there be?
What are interfaces?
You had a 50% drop, I just put it on my Twitter, in hiring from Indian IITs.
That's crazy.
What you're going to have in a couple of years is around the world at the same time, these
kids that have gone through the trauma of COVID, highly educated STEM, programming, accountancy
law, simultaneously people will hire massively less of them because productivity enhances
and you don't need as many of them.
Why would you need as many paralegals?
That for me is a gigantic societal issue and the only thing I can think of is the stoke
of innovation and the generative jobs of the future through open source technology because
I don't know how else we're going to mitigate that because, Peter, you're a student of history.
What happens when you have large amounts of intelligent, disenfranchised youth history?
We've had that happen a few times.
We just had Arab Spring that long ago, revolt.
People war if not international law.
War is a good way to soak up the excess youth, but it's not pleasant for society and fundamentally
the cost of information gathering, organization has collapsed, like you look at stable video
that we released yesterday, right?
It's going to get so much better so quickly, just like stable diffusion, the cost of creating
movies increases, the demand for quality stuff increases, but there's a few years where demand
and supply don't match and that's such a turbulent thing to navigate.
That's one of the reasons I'm creating stabilities for different countries, so the best and brightest
for me which can help navigate them.
And people don't talk about it.
I loved your idea that the stability models and systems will be owned by the nation.
In fact, the one idea that I heard you say, which I thought was fantastic, was you graduate
college in India, you're an owner in that system.
You graduate from Nigeria, you're an owner in that system, basically to incentivize people
to complete their education and to have them have ownership in what is ultimately the most
important asset that nation has.
And talk about it as infrastructure as well.
I think that's an important analogy that people don't get.
This is the knowledge infrastructure of the future, it's the biggest leap forward we have
because you'll always have a co-pilot that knows everything in a few years and that can
create anything in any type, but it must be embedded to your cultural values and you can't
let anyone else own that.
So it is the infrastructure of the mind and who would outsource their infrastructure to
someone else.
So that's why I think Nigerians should own the models of Nigerians for Nigerians and
should be the next generation that does that.
That's why you give the equity to the graduates, that's why you list it, that's why you make
national champions because, again, that has to be that way.
This is far more important than 5G and this gives you an idea of the scale, we're just
at the start, the earlier doctor base.
A trillion dollars was spent on 5G, this is clearly more important, more than a trillion
dollars that we spend on this and again it flips the world.
And so there is huge threat for our societal balance and again I think open is a potential
antidote to create the jobs of the future and there's huge opportunity on the side
because no one will ever be alone and we can use this to coordinate our systems, give everyone
all the knowledge that they need at their fingertips and help guide everyone if we build
this infrastructure correctly and I don't see the highlight can be closed.
AGI, the conversation and the definition of AGI has basically been all over the place,
because while prediction has been for 30 years that it's 2029, again, that's a blurry
line of what we're trying to target but Elon's talked about anywhere from 2025 to 2028, what
are you thinking, what's your timeline for even digital superintelligence?
I honestly have no idea.
People looking at the scaling laws and applying it but as I've said, data is the key and it's
clear that we already have, you could build a board GPT and it would be better than most
corporate boards, right?
So I think we're already seeing improvements over the existing.
One of the complications here is swarm AI.
So even like it's the whole thing, like a duck-sized human or a hundred human sized
ducks, we're just at the start of swarm intelligence and that reflects and represents how companies
are organized.
Andre Carpethi has some great analogies on this in terms of the new knowledge OS and
that could take off at any time but the function of format of that may not be this whole Western
anti-conformized consciousness that we think of but just incredibly efficient systems that
displace existing human decision-making, right?
And so there's an entire actual range of different AGI outcomes depending on your definition
and I just don't know but I feel again like I wake up and I'm like, oh, look, it's fed
up 10 times the model, you know, like I'm just not, no one can predict this.
But there is a point at which, I mean, we were heading towards an AI singularity, using
a definition of a singularity as a point after which you cannot predict what's coming
next and that isn't far away.
I mean, how far out is it for you a year or two years?
I think you're heading towards it in the next few years but like I said, every company,
organization, individual has an objective function.
My objective function is to allow the next generation to navigate what's coming in the
optimal way and achieve their potential.
So I don't want to build an AGI, I don't want to do any of this, amplified human intelligence
is my preference and trying to mitigate against some of the harms of these agentic things
through data transparency, good standards and making it so people don't need to build
gigantic models on crap, which I think is a major danger if even if not from AGI.
But again, we just don't understand because it's difficult for us to comprehend super
human capabilities, but again, we're already seeing that in narrow fields.
We already know that it's a better rider than us.
So we already know that it can make better pictures than us.
And a better physician and a better educator and a better surgeon and a better everything.
Yeah, and again, I think it's this mythos of these big labs being AGI focused, whereas
you can be better than us in like 5% of the stuff that humans can do and that's still
a massive impact on the world and they can still take over companies and things like
that, right?
Like if you take over a company, then you can impact the world.
And there's clearly with a GPT for 1000 of them orchestrated correctly, that can call
up people and stuff, you wouldn't know it's not CEO, you know, I can make an MA GPT and
then they won't have to make all these tough decisions in any of that.
And most of my decisions aren't that good, so it's probably better.
So I think that we're getting to that point, it's very difficult and the design patterns
are going fast.
We're at the iPhone 2G, 3G stage, it's got copy and paste, and we just got the first
stage as well of this technology, which is the creation step, it creates stuff.
The next step is control and then composition, where you're annoyed because chat GPT doesn't
remember all the stuff that you've written, that won't be the case in a year.
And the final bit is collaboration, where these AIs collaborate together and with humans
to build the information superstructures of the future, and I don't feel that's more
than a few years away.
And it's completely unpredictable what that will create.
Let's talk about responsibility that AI companies have for making sure that their technology
is used in a pro-human and not a disruptive fashion.
Do you think that is a responsibility of a company, of a company's board, of a company's
leadership?
How do you think about that?
Again, with the corporate capitalist system, it typically isn't, because you're maximizing
shareholder value and there aren't laws and regulations, which is why I think there's
a moral, a social, and legal-slash-regulatory aspect to this.
Companies will just look at the legal-slash-regulatory, in some cases they'll just ignore them, right?
But I do think, again, we have a bigger moral and social obligation to this.
This is why I don't subscribe to EA or EAAC or any of these things.
I think it's complicated and it's hard.
Given the uncertainty and how this technology proliferates, and you've got to do your best
and be as straight as possible to people about doing your best, because none of us have qualified
to understand or do this.
And none of us should be trusted to have the power over this technology, right?
You should be questioned and you should be challenged with that.
And again, if you're not transparent, how are you going to challenge?
When I think of the most linear organizations on the planet, I think of governments, maybe
religions, but governments, let's leave it there.
How can...
Let's talk about Western government, at least the U.S., I would have said Europe, but I'll
say the U.K. and Europe.
What should they be...
What steps should they be taking right now?
If you were given the reins to say, how would you regulate, what would you want them to
do or not do?
I believe it's a complicated one, so I signed the first FLI letter.
I think I was the only AI CEO to do that back before it was cool, because I said, I don't
think AGI will kill us all, but I just don't know.
I think it's a conversation that deserves to be had, and it's a good way to have a conversation.
And then we flipped the wrong way, where we went overly AI death risk and other things
like that, and governments were doing that, at the AI Safety Summit in the U.K., and then
we had the King of England come out, and so this is the biggest thing since fire.
I was like, okay, that's a big change in the world, that's right.
The King of England said it, so I must be on the right track.
But I think if you look at it, regulation doesn't move faster.
Even the executive order will take a long time.
The EU things will kind of come in.
Instead, I think that governments have to focus on the tangibles.
AI killerism, again, it can be addressed by considering this as infrastructure and what
infrastructure we need to give our people to survive and thrive.
The U.S. is in a good initial place with the CHIPS Act, but I think you need national
data sets, you need to provide open models to stoke innovation, and think about what
the jobs of the future are, because things are never the same again.
You don't need all those programmers when co-pilot is so good, and you're moving co-pilot
from level above, which is compositional co-pilot, and then collaborative co-pilot, right?
You would be able to talk and computers can talk to computers better than humans can talk
to computers.
We need to articulate the future on that side, but then the other side.
One of the examples I give is a loved one had a recent misdiagnosis of pancreatic cancer,
right?
We did a United Order of this.
The loss of agency you feel, and many of you on this call will have had that diagnosis
to the near and dear, is huge.
Then I had 1,000 AI agents fighting every piece of information about pancreatic cancer,
and then after that, I felt a bit more control.
Why don't we have a global cancer model that gives you all the knowledge about cancer and
helps you talk to your kids and connect with people like you, not for diagnosis or research,
but for humans?
This is the Google MedPalM2 model, for example, that outperforms humans in diagnosis, but
also empathy.
What if we arm our graduates to go out and give support to the humans that are being
diagnosed in this way?
That makes society better, and it's valuable, you know?
That's an example of a job of the future, I think.
I don't believe in UBI.
I think we've been universal basic jobs as well, or used jobs.
Universal basic opportunity.
Right?
Yeah.
Universal basic opportunity, universal based jobs, but then post-makers need to think
about it now, because the graduate unemployment wave is literally a few years away, and it
will happen.
Yeah, that is, I mean, when I think about what, I parse the challenges we're going
to be facing in society into a few different elements.
I think what we have today is amazing, and if generative AI froze here, we'd have an
incredible set of tools to help humanity across all of its areas.
And then we've got what's coming in the next zero to five years.
We've talked about patient zero, perhaps being the U.S. elections, and I think you
had said it was Cambridge Analytica that required interference.
Now it's any kid in the garage that could play with the elections.
That's a challenging period of time, and this graduate unemployment wave, as you mentioned,
coming right on its heels.
The question becomes, is the only thing that can create alignment and help us overcome
this AGI at the highest level, meaning it is causing challenges, but ultimately is a
tool that will allow us to be able to solve these challenges as well.
I mean, that's a crazy thought, right?
Like, all this stuff is crazy, like the sheer scale and impact of it.
And, you know, these discussions, we had them last year, Peter, and now everyone's
like, yeah, that makes sense.
And I go, wow, right, it may be AGI, it may be these coordinating automated
story makers and balances from the market, right?
Next year, there's 56 elections with four billion people heading to the polls.
What could possibly go wrong?
Okay, possibly go wrong, you know?
Oh, my God.
But again, the technology isn't going to stop.
Like, even if stability puts down things, if open AI puts down things, it will
continue from around the world because you don't need much to train these models.
Again, the supercomputer thing is a myth.
You've got another year or two where you need them.
You don't need them after that, and that is insane to think about.
You just released stability video.
Congratulations on our stable visual diffusion.
Thank you.
And I'm enjoying some of the clips.
How far are we away from me telling a story to my kids and saying, let's make
that into a movie?
Almost two years away.
Two years away.
So this is a building block.
It's the best creation step.
And then, like I said, you have the control step, composition, and then
collaboration and self-learning systems around that.
So we have Kung Fu UI, which is our node-based system where you have all
the logic that makes up an image, like you can take a dress and a pose and
a face that combines them all.
And it's all encoded in the image because you can move beyond files to
intelligent workflows that you can collaborate with.
If I send you that image file and you put it into your Kung Fu UI, it gives you
all the logic that made that up.
How insane is that?
So we're going to step up there.
And what's happened now is that people are looking at this AI like instant versus
again, the huge amount of effort it took to take this information and
structure it before.
But the value is actually in stuff that takes a bit longer.
Like when you're shooting a movie, you don't just say, do it all in one shot,
right?
Unless you are a very talented director and actor, you know, you have
mise en place, you have staging, you have blocking, you have cinematography.
It takes a while to composite the scenes together.
It'll be the same for this, but a large part of it will then be automated for
creating the story that can resonate with you and you can turn it into
Korean or whatever.
And there'll still be big blog clusters like Oppenheimer and Barbie.
But again, the flaw will be raised overall.
Similarly, like we had a music video competition check it on YouTube with Peter
Gabriel, he allows us to use kindly his songs and people from all around the
world made amazing music videos to his thing, but they took weeks.
So I think that's somewhere in the middle here where again, we're just at
that early stage, because chat GPT isn't even a year old, you know, stable
diffusion is only 14, 15 months.
And I think you'd agree that neither of them is the end hole and be all.
It's just, it's the earliest days of this field.
I had the conversation.
The tiniest building.
Yeah, I had this conversation with Ray Kurzweil two weeks ago.
We're just after a singularity board meeting we had, and we're just on a
zoom and chatted.
And, you know, the realization is that, unfortunately, the human mind is
awful at exponential projections.
And despite the convergence of all these technologies, we tend to project
the future as a linear extrapolation of, you know, the world we're living in right
now.
But the best I can say is that we're going to see in the next decade, right,
between now and 2033, we're going to see a century worth of progress.
But it's going to get very weird, very fast, isn't it?
I mean, there's two way doors and there's one way doors, right?
Like in December of last year, multiple headmasters called me and said,
we can't set essays for our homework anymore.
And every headmaster in the world had to do that same thing.
It's a one-way door.
Yes.
And this is the scary part, the one-way doors, right?
Like when you have an AI that can do your taxes, what does that mean for
accountants?
All the accountants at the same time.
Kind of crazy, right?
It is.
And the challenge, I mean, one of my biggest concerns, so listen, I'm the
eternal optimist.
I'm not the guy who's the glasses half full, it's the glasses overflowing.
And one of the challenges I think through when I think about where AI, AGI, ASI,
however you want to project it to is the innate importance of human purpose.
And unfortunately, most of us derive our purpose from the work that we do.
You know, I ask you, you know, tell me about yourself and you jump into your
work and what you do.
And so when AI systems are able to do most everything we do, not just a little
bit better, but, you know, orders of magnitude better, redefining purpose and
redefining my role in achieving a moonshot or a transformation is, it's the,
you know, it's the impedance mismatch between human societal growth rates
and tech growth rates.
What are your thoughts there?
Yeah, I mean, I think again, exponentials are hard.
Like if I say GPT-4 in 12 to 18 months on a smartphone, you'd be like, well,
that's not possible.
Why?
You know, like GPT-4 is impossible, stable diffusion is impossible, right?
Like now they've almost become commonplace, but why would you need super
computers and these things?
I do agree this mismatch and that's why we're in for five years of chaos.
That's why I called it stability because I saw this coming a few years ago and
I was like, holy crap, we have to build this company.
And now we have the most downloads of any models of any company, like 50 million
last month versus 700,000 from Astral, for example.
And we will have the best model of every type except for very large language
models by the end of the year.
So we have audio, 3D, video, code, everything and a lovely, amazing
community because it's just so hard again for us to imagine this mismatch.
There's a period of chaos.
But then on the other side, like there's this PDoom question, right?
The probability of doom.
I can say something with this technology, the probability of doom is lower
than without this technology because we're killing ourselves.
And this can be used to enhance every human and coordinate us all.
And I think what we're aiming for is that Star Trek future versus that Star
Wars.
Yes, I meant to that.
And I think that's an important point, the level of complexity that we have
in society, we don't need AI to destroy the planet.
We're doing that very, very well.
Thank you.
But the ability to coordinate.
So one of the things I think about is a world in which everyone has access to
all the food, water, energy, healthcare, education that they want.
Really, a world of true abundance in my mind is a piece more peaceful world, right?
Why would you want to destroy things if you have access to everything that you
need?
And that kind of a world of abundance is on the backside of this kind of
awesome technology.
We have to navigate the next period.
I believe we'll see it within our lifetimes, particularly if we get
longevity songs, right?
And that's so amazing, right?
But then we think about, as you said, why peace?
A child in Israel is the same as a child in Gaza.
And then something happens.
A liar is told that you are not like others and the other person is not human
like you.
All wars are based on that same line.
And so again, if we have AI that is aligned with the potential of each human
that can help mitigate those lies, then we can get away from war because
the world is not scarce.
There is enough food for everyone.
It's a coordination failure.
And that can be addressed by this technology.
I agree.
One of the most interesting and basic functions or capabilities of generative
AI has been the ability to translate my ideas into concepts that someone who is
a different frame of thought can understand.
Right?
But that's what this generative AI is.
It's a universal translator.
Sure.
It does not have facts.
The fact that it knows anything is insane.
Hallucinations is a crazy thing to say.
Again, it's just like a graduate trying so hard.
GPT-4 with 10 trillion words and 100 gigabytes is insane.
Stable diffusion has like 100,000 gigabytes and a two gigabyte file.
50,000 to one compression is something else.
It's learned principles.
Yes.
And this is it's knowledge, knowledge versus data.
Yeah.
It's knowledge versus data.
And if you have some experience, you get the wisdom, right?
Yes.
Because it's learned the principles and contexts and it can map them to
transform data because that's how you navigate.
You don't navigate based on like logical flow.
We have those two parts of our brain navigate sometimes based on instinct
based on the principles you've learned.
So Tesla's new auto driving model, self driving model is entirely based on
a console, which are protection, like they said it publicly is based on this
technology.
It doesn't have any rules.
It's just learned the principles of how to drive from massive amounts of Tesla
data that now fits on the hardware without internet.
And so they went from self driving being impossible to now, hey, it looks pretty
well, you know, because it's learned the principles.
And so that's why this technology can help solve the problem.
This is why it can help us amplify our intelligence and innovation.
Because the missing part, the second part of the frame, you know, next, I can't
give more details yet, but next week we're announcing the largest X prize ever.
It's a hundred and one million dollars.
It's a hundred and one.
So it's Elon had the had a hundred million dollar prize that kind of defund a few
years ago for carbon sequestration and the funder, the first funder of this prize
wanted to be larger than Elon's.
I said, okay, you had the extra million.
It's for luck.
It's for luck.
We did our seed round 101 million.
Oh, really?
Okay.
Hi, that's great.
It's on your popular number.
Anyway, the and it's in the field of health.
I'll leave it at that folks go to XPRIZE.org to register to see the live event on
November 29th.
We're going to be debuting the prize, what it is, what it's going to impact
eight billion people, long story short, it's, it's a nonlinear,
future because we are able to utilize AI and make things that were seemingly
crazy before, likely to become inevitable.
And that's an amazing future we have to live into.
Yeah, I mean, again, because it's one way doors, the moment we create a cancer
GPT, and this is something that we're building, we have trillions of tokens
and then you Google TV.
Things like that, that organizes global cancer knowledge and makes it accessible
and useful, even if it's just for guiding people that have been diagnosed.
The world changes.
The 50% of people that have a cancer diagnosed in their lives in every language
and every level will have someone to talk to and connect them with the resources
they need and other people like them and talk to their families.
You know, and how insane is that?
And so again, the least positive thing is that we're going to be able to
and so again, the least positive stories in the future need to be told, right?
Because that will align us to where we need to go as opposed to a future full
of uncertainty and craziness and doom.
In our last couple minutes here, buddy, what can we look forward to from stability
in the months and years ahead?
We have every model of every type and we'll build it for every nation
and we'll give back control to every nation.
So coming back to governance here.
Again, is the nation state the unit of control?
Is it? No, I my my thinking is disabilities in every nation
should have the best and brightest of each because what you've seen is
there are amazing people in this sphere, the best and brightest in the world.
Now, this is the biggest thing ever and they all want to work in it.
And it's just finding the right people with the right intention.
The brightest people go back to Singapore or Malaysia or others
because of the future of their nations.
And again, now we're doing a big change and we don't talk
about all the cool stuff we do.
We've just taken it because you need to articulate
that positive vision of the future because the only scarce resource
is actually this is human capital.
It's not GPUs.
It's not data.
It's about the humans that can see this technology and realize that
they can play a part in guiding it for the good of everyone,
their own societies and more.
And that's again, what I hope stability can be.
Well, I wish you the best of luck, pal.
Thank you for joining me in this conversation.
It's it's been a crazy four or five days.
And wish Sam and Greg and the entire
opening I team stability in their lives.
Yeah, I have a nice Thanksgiving.
They're absolutely they're an amazing team building world changing technology.
It's such a concentration of talent.
I think, again, I really felt for them over the last few days,
you know, much as I kind of post memes and everything.
I posted that as well.
I think this will bring them closer together and hopefully they can solve
the number one problem that I've asked them to solve, which is email.
Solve email, right?
And then we'll crack on from there.
All right, cheers, my friend.
