1
00:00:00,000 --> 00:00:10,000
For a long time, most people whose opinion I respected were somewhat skeptical of metrics and in particular of metrics used to measure software development performance.

2
00:00:10,000 --> 00:00:16,000
Martin Fowler wrote an excellent piece called Cannot Measure Productivity some years ago.

3
00:00:16,000 --> 00:00:25,000
Then some smart people came up with a better way and discovered all sorts of interesting and useful things based on their metrics.

4
00:00:25,000 --> 00:00:34,000
These were not necessarily new ideas, but the combination was, and the model that they built from them was certainly new.

5
00:00:34,000 --> 00:00:38,000
And is, in my opinion, an important step forward for our discipline.

6
00:00:38,000 --> 00:00:46,000
So, how do you measure software delivery and what are the important lessons that we can learn from it when we do?

7
00:00:46,000 --> 00:00:59,000
Hi, I'm Dave Farley of Continuous Delivery. Welcome to my channel.

8
00:00:59,000 --> 00:01:02,000
And if you haven't been here before, please do hit subscribe.

9
00:01:02,000 --> 00:01:06,000
And if you enjoy the content today, hit like as well.

10
00:01:06,000 --> 00:01:12,000
I'd like to begin by thanking our sponsors, Equal Experts, Octopus and Speckflow.

11
00:01:12,000 --> 00:01:18,000
They're great supporters of our channel, so please support them in turn by checking their links in the description below.

12
00:01:18,000 --> 00:01:27,000
If you'd like to learn more about the engineering approach that I describe a little later in this video, check out my new book, Modern Software Engineering,

13
00:01:27,000 --> 00:01:35,000
which is currently a bestseller in several categories on Amazon and also at the time of recording on sale.

14
00:01:35,000 --> 00:01:39,000
In 2010, the book that I wrote with Jess Humble was published.

15
00:01:39,000 --> 00:01:44,000
It introduced lots of ideas to people who hadn't seen this way of working before.

16
00:01:44,000 --> 00:01:48,000
Its popularity surprised both of us.

17
00:01:48,000 --> 00:01:52,000
We certainly believe that the ideas in it were important and valuable.

18
00:01:52,000 --> 00:01:56,000
We'd already seen them work in every place that we'd tried them over several years.

19
00:01:56,000 --> 00:02:02,000
But Continuous Delivery came to be seen as a significant book, and we didn't really anticipate that.

20
00:02:02,000 --> 00:02:07,000
As we gained more experience with the practices in a wider variety of settings though,

21
00:02:07,000 --> 00:02:10,000
our understanding of this approach grew significantly.

22
00:02:10,000 --> 00:02:15,000
So now, I no longer feel embarrassed to say things like,

23
00:02:15,000 --> 00:02:19,000
this is the current state of the art in software development.

24
00:02:19,000 --> 00:02:25,000
And this represents a genuine engineering discipline for software in that if you practice it,

25
00:02:25,000 --> 00:02:31,000
you will substantially increase your chances of success, whatever it is that you're trying to build.

26
00:02:31,000 --> 00:02:38,000
One of the reasons that I feel confident to make such claims is because we have the best data on software delivery practice

27
00:02:38,000 --> 00:02:45,000
that's been collected so far, and it tells us that this stuff works better than anything else that we know of.

28
00:02:45,000 --> 00:02:51,000
A few years after our book was published, Jess met Dr. Nicole Fosgren,

29
00:02:51,000 --> 00:02:58,000
and together with some sponsors, they started work on something called the State of DevOps Report.

30
00:02:58,000 --> 00:03:03,000
Nicole brought an academic approach to the collection and interpretation of the data.

31
00:03:03,000 --> 00:03:08,000
It's scientifically justifiable in terms of how that works.

32
00:03:08,000 --> 00:03:15,000
The results are, as far as I can tell, the most thorough examination of software development practice

33
00:03:15,000 --> 00:03:18,000
than our industry's seen so far.

34
00:03:18,000 --> 00:03:25,000
Later, Jess, Nicole and Jean Kim formed Dora and released the Accelerate book,

35
00:03:25,000 --> 00:03:31,000
which describes some of their findings over the years and the science behind their approach.

36
00:03:31,000 --> 00:03:41,000
This is sociology, not physics, so research like this is complex and always open to interpretation and question to some degree.

37
00:03:41,000 --> 00:03:45,000
Human beings are messy things compared to atoms and electrons.

38
00:03:45,000 --> 00:03:50,000
Nevertheless, their findings are profound. They can make predictions.

39
00:03:50,000 --> 00:03:54,000
These predictions have been born out in thousands of organizations now.

40
00:03:54,000 --> 00:03:59,000
They say things like, there's no trade-off between speed and quality.

41
00:03:59,000 --> 00:04:05,000
Excellence in software delivery and operational performance drives organizational performance.

42
00:04:05,000 --> 00:04:14,000
And the data also tells us that continuous delivery is being successfully adopted by more and more teams year on year.

43
00:04:14,000 --> 00:04:20,000
At the heart of the State of DevOps research and the approach are a collection of metrics,

44
00:04:20,000 --> 00:04:24,000
now commonly referred to as the Dora metrics.

45
00:04:24,000 --> 00:04:29,000
For decades, people had been trying to figure out how to measure software development performance.

46
00:04:29,000 --> 00:04:35,000
They counted lines of code, man hours and later velocity in the form of story points.

47
00:04:35,000 --> 00:04:38,000
None of these things really matter.

48
00:04:38,000 --> 00:04:42,000
Why would you want more code if you could do a better job with less?

49
00:04:42,000 --> 00:04:46,000
You don't go faster by adding more people, you go slower.

50
00:04:46,000 --> 00:04:53,000
And velocity only tells us what we did last week and it's not really transferable between teams.

51
00:04:53,000 --> 00:04:56,000
Velocity is also really easy to game.

52
00:04:56,000 --> 00:04:58,000
Want to increase your velocity for your team?

53
00:04:58,000 --> 00:05:04,000
Okay, this story, which was five points last week, is ten points this week.

54
00:05:04,000 --> 00:05:10,000
I've seen all of these games being played out in real teams. They don't help us to do a better job.

55
00:05:10,000 --> 00:05:15,000
Metrics are extremely useful, but not when they're treated as a goal.

56
00:05:15,000 --> 00:05:20,000
Our goal is always to produce good software that people find useful efficiently.

57
00:05:20,000 --> 00:05:25,000
The difficulty of measuring software projects left lots of people, including me,

58
00:05:25,000 --> 00:05:29,000
thinking that you can't measure software productivity in any way that's useful.

59
00:05:29,000 --> 00:05:35,000
And that is still probably correct, but we were thinking about the problem in the wrong way.

60
00:05:35,000 --> 00:05:38,000
Productivity is really the wrong measure.

61
00:05:38,000 --> 00:05:43,000
It's not that productivity doesn't matter, but it's not enough on its own.

62
00:05:43,000 --> 00:05:49,000
If we produce loads of things, but they're all useless, then we aren't really productive.

63
00:05:49,000 --> 00:05:54,000
So if we count lines of code and optimize to create lots of code,

64
00:05:54,000 --> 00:05:58,000
then we're actively encouraging people to write nasty, complex systems

65
00:05:58,000 --> 00:06:02,000
that over time will become harder and harder to work on.

66
00:06:02,000 --> 00:06:09,000
So even by the mistaken measure of productivity, we have to be really careful about what we measure and incentivize.

67
00:06:10,000 --> 00:06:16,000
All metrics suffer from this problem. They're all subject to being misinterpreted and misapplied,

68
00:06:16,000 --> 00:06:21,000
particularly when we use them as goals in their own right.

69
00:06:21,000 --> 00:06:25,000
The people behind the Dora metrics made some good choices.

70
00:06:25,000 --> 00:06:32,000
These metrics are not easy to cheat or misinterpret, but somehow people still manage to.

71
00:06:32,000 --> 00:06:36,000
So you still have to be cautious how you use them.

72
00:06:36,000 --> 00:06:42,000
The Dora company was later unbought by Google, who now run the ongoing research

73
00:06:42,000 --> 00:06:47,000
and have lots of interesting stuff online in addition to that.

74
00:06:47,000 --> 00:06:52,000
You can get a quick assessment of where you stand on the Dora metrics, for example,

75
00:06:52,000 --> 00:06:56,000
and targeted advice for next steps.

76
00:06:56,000 --> 00:07:02,000
You can also explore the relationships between different behaviors that have been unearthed by their research.

77
00:07:02,000 --> 00:07:08,000
There are four Dora metrics divided into two groups, stability and throughput.

78
00:07:08,000 --> 00:07:14,000
The Dora research categorized teams into groups based on their scores on these metrics,

79
00:07:14,000 --> 00:07:17,000
ranging from low performers to elite.

80
00:07:17,000 --> 00:07:22,000
The difference in performance levels based on these metrics is sometimes extreme.

81
00:07:22,000 --> 00:07:28,000
The research correlates behaviors and outcomes based on the scores against these metrics too,

82
00:07:28,000 --> 00:07:32,000
and these too are often quite extreme in their range.

83
00:07:32,000 --> 00:07:37,000
For example, an elite team will deploy changes to production

84
00:07:37,000 --> 00:07:42,000
thousands of times more frequently than a low-performing team.

85
00:07:42,000 --> 00:07:47,000
At the same time, they'll produce code of significantly higher quality,

86
00:07:47,000 --> 00:07:54,000
and they will spend 44% more of their time on new features than low-performing teams.

87
00:07:54,000 --> 00:07:57,000
The measure themselves are interesting.

88
00:07:57,000 --> 00:08:00,000
They don't attempt to measure productivity directly,

89
00:08:00,000 --> 00:08:05,000
and it's important to consider them as a whole, not independently of one another.

90
00:08:05,000 --> 00:08:10,000
They're focused on measuring the flow of changes and their quality.

91
00:08:10,000 --> 00:08:15,000
Stability is a measure of the quality of the software that we produce,

92
00:08:15,000 --> 00:08:20,000
so you can't mistakenly optimize to produce loads of rubbish and get a good score

93
00:08:20,000 --> 00:08:23,000
as you can with most more naive productivity metrics.

94
00:08:23,000 --> 00:08:29,000
Stability is measured by change failure rate and mean time to recovery.

95
00:08:29,000 --> 00:08:36,000
That is, what proportion of the time does a change result in a failure of some kind

96
00:08:36,000 --> 00:08:38,000
when it reaches production?

97
00:08:38,000 --> 00:08:43,000
And how long does it take your team to recover from a failure once it finds one?

98
00:08:43,000 --> 00:08:49,000
Elite teams have changed failure rates in the 0 to 15% range.

99
00:08:49,000 --> 00:08:53,000
Everybody else is in the 16 to 30% range.

100
00:08:53,000 --> 00:08:58,000
Elite teams restore service in under one hour once a defect is found.

101
00:08:58,000 --> 00:09:03,000
Low performers often take over six months to achieve the same result.

102
00:09:03,000 --> 00:09:09,000
Throughput is measured by lead time for changes and deployment frequency.

103
00:09:09,000 --> 00:09:16,000
Lead time for changes is measured by how long does it take from commit to code running in production.

104
00:09:16,000 --> 00:09:22,000
And deployment frequency is how often do you release a change to end users?

105
00:09:22,000 --> 00:09:26,000
These are measurements of the efficiency with which we can create changes

106
00:09:26,000 --> 00:09:29,000
of the quality measured by our stability metrics.

107
00:09:29,000 --> 00:09:35,000
They measure the cost of our production processing time and the rate of flow of change into production.

108
00:09:35,000 --> 00:09:42,000
Elite teams have lead times of under one hour compared to over six months for low performers.

109
00:09:42,000 --> 00:09:46,000
And deployment frequency is on demand multiple times per day usually

110
00:09:46,000 --> 00:09:51,000
compared to less than once every six months for low performers again.

111
00:09:51,000 --> 00:09:56,000
The combination of measuring stability and throughput together is important.

112
00:09:56,000 --> 00:10:01,000
There's no value in optimizing for throughput if the stability goes down

113
00:10:01,000 --> 00:10:06,000
because over time and it's a relatively short period of time

114
00:10:06,000 --> 00:10:11,000
will incur more work as a result of the low quality of our work as measured by its stability.

115
00:10:11,000 --> 00:10:18,000
This means that we'll end up going a lot slower and so our throughput will suffer after all.

116
00:10:18,000 --> 00:10:23,000
This is why elite teams outperform poor teams quite so significantly.

117
00:10:23,000 --> 00:10:27,000
They produce higher quality work and they do it more efficiently.

118
00:10:27,000 --> 00:10:30,000
These metrics are inextricably linked.

119
00:10:30,000 --> 00:10:37,000
Optimizing for a naive version of throughput is an extremely common anti-pattern.

120
00:10:37,000 --> 00:10:40,000
I see this in dev teams all of the time.

121
00:10:40,000 --> 00:10:46,000
The organization promotes the idea that it's vital to deliver features at some high rate.

122
00:10:46,000 --> 00:10:50,000
So they cut corners all over the place to do so.

123
00:10:50,000 --> 00:10:54,000
The result is that quality goes down.

124
00:10:54,000 --> 00:11:00,000
Now dev teams are either drowning bug reports and the effort to prioritize and fix them

125
00:11:00,000 --> 00:11:07,000
or they produce such tactical crap that their systems rapidly become so difficult to change

126
00:11:07,000 --> 00:11:11,000
that they grind to a halt and make little or no forward progress at all.

127
00:11:11,000 --> 00:11:16,000
In both of these cases productivity slows down dramatically

128
00:11:16,000 --> 00:11:20,000
and better teams that don't cut corners like this steam pass them

129
00:11:20,000 --> 00:11:24,000
and produce more better features sooner overall.

130
00:11:24,000 --> 00:11:28,000
You need good scores on throughput and stability

131
00:11:28,000 --> 00:11:32,000
and you need to take a longer term view of what counts as success.

132
00:11:32,000 --> 00:11:38,000
Being able to work at a sustained pace matters a lot in software development.

133
00:11:38,000 --> 00:11:45,000
The Dora metrics are still metrics so it's still more subtle than simply striving for good scores.

134
00:11:45,000 --> 00:11:48,000
There's a very good presentation by Brian Finster.

135
00:11:48,000 --> 00:11:53,000
There's a link in the description below called How to Misuse the Dora Metrics

136
00:11:53,000 --> 00:11:57,000
where he points out several common misinterpretations and misuses.

137
00:11:57,000 --> 00:12:05,000
Throughput is not about lowering quality to make developers cogs in some feature factory machine.

138
00:12:05,000 --> 00:12:08,000
It's about reducing the back size of change.

139
00:12:08,000 --> 00:12:14,000
We want to work in smaller steps that give us the chance to gather more feedback,

140
00:12:14,000 --> 00:12:18,000
work more experimentally and so learn faster.

141
00:12:18,000 --> 00:12:22,000
These are ideas at the heart of my modern software engineering approach.

142
00:12:22,000 --> 00:12:28,000
So the Dora metrics can be used to help us amplify our engineering discipline.

143
00:12:28,000 --> 00:12:33,000
The goal is long term success, not short term gratification here.

144
00:12:33,000 --> 00:12:38,000
So if we want to maintain our ability to have a good scores in throughput

145
00:12:38,000 --> 00:12:44,000
and good scores in stability too, we need to work in ways that are sustainable over time.

146
00:12:44,000 --> 00:12:50,000
We can't afford to create rubbish now that doesn't result in us going faster over the long haul.

147
00:12:50,000 --> 00:12:54,000
That means we need to manage the complexity in our systems

148
00:12:54,000 --> 00:12:58,000
in a way that allows us to establish and maintain a fast pace.

149
00:12:58,000 --> 00:13:04,000
So we need to employ the second group of principles from my engineering book too.

150
00:13:04,000 --> 00:13:08,000
These are mutually reinforcing ideas.

151
00:13:08,000 --> 00:13:11,000
This relationship is important.

152
00:13:11,000 --> 00:13:16,000
In Brian's talk he correctly points out that in the Accelerate book

153
00:13:16,000 --> 00:13:20,000
there are lots more things than only these metrics to consider.

154
00:13:20,000 --> 00:13:24,000
I think that the Dora metrics are a great tool.

155
00:13:24,000 --> 00:13:30,000
We can use them to select choices that work better than others, but they aren't the goal.

156
00:13:30,000 --> 00:13:36,000
The goal is to produce great software efficiently and repeatedly that our users get value from.

157
00:13:36,000 --> 00:13:42,000
I've seen teams misuse the Dora metrics, but I don't think that devalues them.

158
00:13:42,000 --> 00:13:49,000
The problem with all metrics is that human beings are both smart and dumb at the same time.

159
00:13:49,000 --> 00:13:56,000
Smart in that they will attempt to gain whatever metrics you choose if it's in their interest to do so.

160
00:13:56,000 --> 00:14:01,000
I once saw a company incentivize their teams to compete with Dora scores.

161
00:14:01,000 --> 00:14:07,000
Some of the teams redefined what the stability metrics meant so that they could get better scores.

162
00:14:07,000 --> 00:14:13,000
Instead of change failure rate, they interpreted this to mean bugcat.

163
00:14:13,000 --> 00:14:16,000
A subtle difference, but an important one.

164
00:14:16,000 --> 00:14:20,000
They interpreted throughput as the number of features delivered,

165
00:14:20,000 --> 00:14:25,000
and no measure of time involved and they ignored deployment frequency altogether.

166
00:14:25,000 --> 00:14:30,000
Then they optimized for fixing bugs over delivering features.

167
00:14:30,000 --> 00:14:36,000
Now, because they'd lost focus on feature delivery, they cut more corners to try and keep feature delivery up,

168
00:14:36,000 --> 00:14:40,000
but the real change failure rate went up, not down.

169
00:14:40,000 --> 00:14:46,000
They still declared success though, because overall the bug count did go down.

170
00:14:46,000 --> 00:14:53,000
That's because the team, unsurprisingly, prioritized fixing easy bugs, even if they weren't important.

171
00:14:53,000 --> 00:14:56,000
After all, they were being incentivized to reduce the bug count.

172
00:14:56,000 --> 00:14:59,000
Nothing said which bugs they should fix.

173
00:14:59,000 --> 00:15:03,000
Overall, the bug count numbers went down, but the real stability was worse than before.

174
00:15:03,000 --> 00:15:07,000
Humans are dumb in the sense that they love a simple target.

175
00:15:07,000 --> 00:15:12,000
So even in complex situations, we were often focused only on the target,

176
00:15:12,000 --> 00:15:17,000
ignoring everything else, even when other things are more important.

177
00:15:17,000 --> 00:15:23,000
So if you think throughput means speed of feature delivery rather than rate of change,

178
00:15:23,000 --> 00:15:27,000
you'll ignore the longer term consequences of cutting corners.

179
00:15:27,000 --> 00:15:32,000
The data is in. Working with high throughput and high stability

180
00:15:32,000 --> 00:15:37,000
is the most efficient way to sustainably deliver software that we know.

181
00:15:37,000 --> 00:15:42,000
The problem with this is that there is a point at which the lines cross.

182
00:15:42,000 --> 00:15:46,000
What if you don't care about being sustainable?

183
00:15:46,000 --> 00:15:53,000
If I need to deliver a feature next week, and that is my only way of measuring success,

184
00:15:53,000 --> 00:15:57,000
then it may be in my interest to cut the corners.

185
00:15:57,000 --> 00:16:01,000
If I don't test, if I just implement the first thought that springs to mind,

186
00:16:01,000 --> 00:16:05,000
if I don't worry about the maintainability of the code that I leave behind,

187
00:16:05,000 --> 00:16:07,000
I may save some time.

188
00:16:07,000 --> 00:16:13,000
So my feature is delivered within a week, but the chances are it won't be working the following week.

189
00:16:13,000 --> 00:16:18,000
The following week, I'll get bug reports, and now when I need to add a new feature,

190
00:16:18,000 --> 00:16:20,000
my code is a mess.

191
00:16:20,000 --> 00:16:24,000
So as well as taking the time to fix the last week's bugs,

192
00:16:24,000 --> 00:16:27,000
my new feature will also take longer to create.

193
00:16:27,000 --> 00:16:30,000
This is always a stupid trade.

194
00:16:30,000 --> 00:16:35,000
The problem is in identifying the point at which the cost of doing a bad job

195
00:16:35,000 --> 00:16:39,000
is more than the cost of doing a better job.

196
00:16:39,000 --> 00:16:41,000
It's always difficult.

197
00:16:41,000 --> 00:16:47,000
In my experience though, the cost of bad work hit us a lot sooner than most of us seem to think.

198
00:16:48,000 --> 00:16:51,000
If you have a six month deadline to meet,

199
00:16:51,000 --> 00:16:57,000
there's no doubt at all that spending the extra time to do a good job will pay off.

200
00:16:57,000 --> 00:17:02,000
You'll produce more, better code over that time.

201
00:17:02,000 --> 00:17:08,000
If the deadline is only a couple of weeks and you're about to leave the project and have no conscience,

202
00:17:08,000 --> 00:17:13,000
then cutting corners may get you to a low quality delivery sooner.

203
00:17:13,000 --> 00:17:16,000
If your deadline is somewhere between these limits though,

204
00:17:16,000 --> 00:17:22,000
it's quite tricky to tell if the investment in doing a good job will get you to the finish line sooner.

205
00:17:22,000 --> 00:17:25,000
There is no silver bullet.

206
00:17:25,000 --> 00:17:29,000
Software development is complicated and difficult to do well.

207
00:17:29,000 --> 00:17:32,000
The Dora metrics are trailing indicators.

208
00:17:32,000 --> 00:17:35,000
They tell you how you did, not how you're going to do.

209
00:17:35,000 --> 00:17:40,000
Approaches like continuous integration and test driven development and continuous delivery

210
00:17:40,000 --> 00:17:44,000
predict good scores on those trailing indicators.

211
00:17:44,000 --> 00:17:48,000
So represent other tools that we can use along with them.

212
00:17:48,000 --> 00:17:53,000
Think of the Dora metrics as being rather like the instruments in your car.

213
00:17:53,000 --> 00:17:59,000
They make no sense without context and correlation with the other instruments.

214
00:17:59,000 --> 00:18:06,000
If our speedometer says we're traveling at 70 miles an hour, is that good or bad?

215
00:18:06,000 --> 00:18:07,000
Neither.

216
00:18:07,000 --> 00:18:09,000
It depends on the context.

217
00:18:09,000 --> 00:18:15,000
It may be good if we're within the speed limit and hoping to make good time to our destination.

218
00:18:15,000 --> 00:18:20,000
It's bad if we're driving through a school district or want to maximise our range.

219
00:18:20,000 --> 00:18:25,000
What does it mean if we are averaging 60 miles an hour towards our destination?

220
00:18:25,000 --> 00:18:28,000
Again, it depends on other things.

221
00:18:28,000 --> 00:18:33,000
If we're about to run out of charge or petrol, then our average speed doesn't tell us.

222
00:18:33,000 --> 00:18:39,000
We need to correlate that with those other instruments to figure out when we'll arrive.

223
00:18:39,000 --> 00:18:46,000
If our engine is overheating and about to break down, then driving this fast is just hastening the point at which our engine fails.

224
00:18:46,000 --> 00:18:50,000
We have to treat the Dora metrics for what they are.

225
00:18:50,000 --> 00:18:54,000
Useful gauges that can give us information about our progress.

226
00:18:54,000 --> 00:18:59,000
They aren't targets and how we interpret them will depend on context.

227
00:18:59,000 --> 00:19:05,000
But for the first time, we have some instruments that can help us to see how we're doing.

228
00:19:05,000 --> 00:19:09,000
As long as we're thoughtful about what the results mean.

229
00:19:09,000 --> 00:19:13,000
This is a very big step forward.

230
00:19:13,000 --> 00:19:15,000
Thank you very much for watching.

