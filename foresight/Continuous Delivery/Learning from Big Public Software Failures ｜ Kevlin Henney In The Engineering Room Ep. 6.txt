Hi, my name is Dave Farley and welcome to The Engineering Room.
If you haven't been here before, do hit subscribe and if you enjoy the content today,
hit like as well. Do join in the conversation too and let us know your thoughts and ideas in the
comments below. The Engineering Room is an occasional series and is meant as an addition
to the more usual content on the Continuous Delivery channel. These are longer form conversations
with some influential and thoughtful people from our industry. Today I'm joined by my
friend Kevlin Henney. I first came across Kevlin at a conference in Australia many years ago,
although we're both English. He gave several talks at that conference, including a keynote,
which was funny, unexpected, educational and brilliantly well presented. Over the years,
I've come to expect nothing less than that from Kevlin. That's his norm. As I started talking
more frequently at conferences, our paths crossed more often and we became friends.
Kevlin is an independent software development consultant, trainer, speaker and writer.
The people who work with Kevlin always speak very highly of his services. He's witty, nerdy, sorry,
Kevlin, and smart. He also has a Google unique name. Try it. You'll only get Kevlin.
And rather strangely, among the cognizanty, if you ever see an obvious public software failure,
a screen in an airport showing a command line or an advert in a stall showing the blue screen of
death, it's called a Kevlin Henney. But no doubt we'll get to that. Welcome, Kevlin. Did I miss
anything important? No, thank you very much, Tim. That's the perfect introduction. I'm going to copy
and paste that. Great. Thanks. Well, I've been looking forward to this talk for a little while.
The last time we met was in Copenhagen at a go-to conference. And my wife and family
took the mickey out of me for the rest of the conference, because you and I, every time we
crossed, we spent all of the time talking rapidly in great detail because we hadn't seen each other
for a while. But the funny thing is, the way that coincidences worked out is that
pretty much the moment I arrived in the hotel, you and family were there, and I bumped into
everybody, including your son and daughter-in-law at the airport when I left. It's crossed quite a
lot there. So yeah, plenty to talk about, plenty to catch up on. Cool. So let's start with that
production failure being called a Kevlin Henney. I think I know the answer to this, but tell us how
that happens. Yeah, well, so it's one of the things. I guess if you're in software or there's
something that you really into, you become sensitive to things. You start spotting things.
And I was always fascinated by the fact that, you know, see occasional failures in places,
you occasionally see a cash machine booting up and stuff like that. You kind of notice this stuff,
it makes you ask questions about it. But then we hit the, then we hit the kind of like,
cameras on phones era. And at that point, suddenly, it's like, I'm walking, everywhere I'm walking,
I have a camera. And, you know, the amount of software that we actually have everywhere
is huge. So I'm starting to take pictures of these things. I'm also taking screenshots
whenever something crashes, particularly if it's losing my work, I'll take a screenshot.
Hold on to that because I think it's fascinating because it's kind of like, okay, it's often
frustrating, but it's fascinating. It's just like, okay, I mean, this is my space. I'm in
software that collectively we somehow created this. And I'm fond of noting that we, as in
software development, are the largest creators of kind of gorilla installation art on the planet.
Nobody else comes close, you know? But I used to kind of collect these. And occasionally,
I kind of put them in talks or when running a workshop or training course, I kind of have them
on my screen, just like as a screensaver in the breaks always causes conversation. And I always
pointed out, oh, this is really interesting because in failure, when something fails,
you learn about something, you learn something about how it was constructed.
You know, it's kind of like at that moment, you lose all the encapsulation, you're presented
with something when it works that is beautiful and pristine and pixel perfect, and it offers
some kind of user experience. And maybe you can guess the technology, but most of the time,
you can't. And then it's like you drop something on the floor and it fractures. And it's just like,
oh, look, this bit's made of C sharp, or they're using that as part of their stack, or I'm pretty
sure that's out of support now, or whatever it is, you see it loses its encapsulation, it loses
that kind of like surface, and we see the insides. And if it doesn't tell us directly how it was built,
it kind of invites questions, it makes you go, oh, I wonder what, how did this arise? Did they
forget to put a tri block here? Did they forget, you know, how did this exception escape to the user?
But, you know, I just did that kind of as a point of fun as a point of personal interest
and point of showing people, and then people started sending me these things by email.
And then we hit social media era, people start sharing it more directly. And then it's Twitter,
and people are just like posting it and just adding me, then I start retweeting it. And then
that's how that's where the that's where it acquired its name, and they actually made its way
into urban dictionary, the register and a bunch of other things. So, so yeah, it's just one of
those things. And the incidental side effect is it is one, it's interesting. I find it's still
fascinating. I think there's a kind of a humor to it, as well as a serious point to be made
in all of these things. It also ends up being an accidental service, because sometimes you're
talking about rail services and train stations and things like that. And often people will
at the train company as well, wherever they are in the world. And, you know, you can tell what
the customer service is like, because often they'll go, oh, you know, instantly jump on that,
say which station was this, I'm really sorry this happened, or there'll be just silence. And so you
can kind of tell that you get a sense of the customer experience as well as the failure experience.
That's brilliant. So, so kind of the archaeology of failure.
Yeah, yeah, that's exactly it. Yeah.
That's great. I often think that we kind of we don't, well, I don't think just think I know,
we don't think too much about the failure failure routes so often. There's one of my
favorite quotable bits of research was a usenix survey from a few years ago that looked into
the cause of production failures. And something like 60% of production failures are in the error
handling path. The most common line of code in the event of a production failure is a comment
saying should should do some exception handling here. Yeah, yeah. That's a really interesting
because that one, if it's the if it's the one I'm thinking of that was 2014 paper. But
but there's probably been others. But I think I first came across that somebody did a study
the late 90s early 2000s. I really can't I'm fortunate I can't cite it. I can't remember
who it was. But they did an analysis of failures in the Linux kernel. And they said it's mostly
on the error paths. Yeah, you know, the dark alleyways that just don't get explored and tested
anywhere near as often, you know, happy day works out fine. But these edge cases,
something goes wrong. And then either it was just like this was to do, or somebody had an idea of
like, well, it should be something like this, but it was never really tested. And it kind of got
marginalized in their in their memory. And now, you know, control flows wandering down this dark
path that's been untested to deal with a really bad situation. And the situation gets worse,
you know, it's one of those things. And that that that seems to be a recurring theme that
these edge cases, it's kind of, you know, when you're at the edge case, it's not an edge case
anymore. It's your world. Yeah, yeah. Yeah. It's one of the things one of the things I did some
research for my book into the kind of history of software engineering. And of course, Margaret
Hamilton was a huge, you know, a hero in in in that in that field in the early days of that.
One of the things I loved that she that she went on about was talking about the importance of the
systems that she was working on the flight control systems for for the Apollo missions,
being man rated. And so the reason why she coined the term software engineering was to
because they were spending all of their time worrying about how things could go wrong,
like engineers do, you know, you don't build a bridge and just only worry about happy days,
you worry about when there's a storm or when the load's too heavy or all of those kinds of things
too. And we we need to be thinking the same for building, you know, serious software systems.
Yeah, I think I think a lot of the stories around Margaret Hamilton, absolutely brilliant. I mean,
the fact that, you know, there's one where she brought a daughter into work, you know,
one day and daughter recreated an error or created an error condition just by messing
about with it, put it into a state, you know, it's just like, you know, cats and children.
And, you know, and other cases where, you know, these, you know, the kind of the classic
certain McKismar associated with the fact that at that point, all astronauts would have been
Air Force pilots, and the culture and image that went with that is like, oh, these highly trained
people. And they won't make mistakes like that. And then promptly one of them does. And it's just
they go back to her. It's like, yeah, let's do that. Let's put that failure. Because it doesn't
matter that they're highly trained, the most the operative word in that sentence is people.
High train gets you so far, but you're still ultimately human. And I think that that and one
of the other points that I read that Margaret Hamilton talked about is she was trying to really
get a seat at the table, as it were, because you put it certainly at that era. This whole idea
that software could form a viable, meaningful first class component of a system, as opposed to just
a secondary component was that was completely a foreign concept. And, you know, there were even
astronauts saying, well, we don't think we're going to need software. We don't actually need
software to land on the moon. And, you know, it's to anybody who's either studied the physics of
it or is familiar with any of the modern stuff around this. It's just like, yeah, you can't
just fly by the seat of your pants and that's going to work out. You need this stuff. And so
she wanted their seat at the table. Engineering was the term that she chose. We've got to treat
this in that sense. It's up there with all the others. Because this is not just icing on the
cake. This is not just a glorified slide rule. There's something deeper and more fundamental
here. We're talking about control systems and data that is genuinely life critical. You know,
this is not just a slide rule. This is beyond that. It's the idea of it's not just a calculator,
whereas I think many people thought of this as just extension of the calculator.
Yes. Yeah, absolutely. Brilliant woman and a real pioneer in our field, I think.
Yeah. Yeah, definitely. But I think that's one of the things that I found with because one of
the other things that we had this term, engineering that kind of took off from different points.
And I've seen it misattributed. So I've seen things where people said, oh, the NATO sponsored
1968 software engineering conference was the beginning coined the term. No, no, no, no.
They use the term because it was already around. Margaret Hamilton initiated that. I think I
stumbled across a bit in communications at the ACM that it was used as a term in 1966
in there. And in other words, there's a kind of a lineage. So by the time the conference came around,
this term was currency. And it existed. And that was trying to try and say, well,
what does this look like if it's for software? I know you looked at it for your book and I did
it for, I did a whole load of talks in 2018 because it was 50 years on. So I thought I'd be in a
series to 1968. I thought, you know what, I'm going to go and read, I'm going to actually go and read
end to end the whole proceedings and also look at the 1969 one as well. But do that. And I did
that a couple of years beforehand. And I found it absolutely fascinating because one, it's an
amazing historical document capturing some really interesting insights. But also, there was a real
sense that one of the things is, you know, some of it is incredibly dated, because, you know,
it does date because they're talking about technical constraints and concepts from the 60s.
But at other times, it's just like, oh, yeah, you know, it's not that you agreed on everything,
but all of the ideas that we now debate and push forward were present. They were there,
they were alive. Yes. And problems that those people were facing in building real systems,
even though the computers that they were building them for are all in museums now. You know, it's
one of the things that got me interested in kind of talking about engineering in my book and stuff
was that, you know, I think that we discard some of those really durable ideas too readily,
that there are things at the heart of our profession. I think the most important things
about our profession that are that haven't changed since the 1960s. And as you said,
you know, I had hair standing up on the back of my neck when I was reading stuff by Alan Perlis
describing in language that sounded quite dated in many words, his choice of words sounded like
somebody from the 1960s. But nevertheless, he was expressing ideas that would that still too many
software development teams don't even think about doing to their detriment. Yeah. And
crazy. Yeah. And I think there's a really interesting things in there about, you know,
that a lot was up for discussion, but also sometimes some of the discussions were that
they're talking like old hands, you know, they're talking about 10 years, like it's a really bad
so that's the fascinating thing about doing this talk 50 years later is just like, well,
they thought they were it was old hat when they did 10 years in, we're half a century on how we
do it here, you know, and, you know, there's some really, really interesting things. So from my
perspective, one of the things I'm very interested in is testing. And interestingly, before reading
that, I had kind of presumed that unit testing as a term, I kind of I kind of dated that to the
1970s. I found, you know, I hadn't tried and really pursued it back in time. But 1970s was
what I had in my mind based on what I'd read. And I thought, you know, that would that be
terminology. But you I look at the software engineering, look at software engineering
proceedings. And there it is, the term unit testing is there without qualification or
definition. In other words, it's not presented as here as a new idea. It's presented as, Oh,
okay. I assume everybody knows it. Yeah. And it was really interesting, just looking at certain
ideas like that. As well as some other historical kind of foreshadowings. So a number of years ago,
I was quite heavily involved in the patents community. There are a number of elements in
the patents community that and patents thinking, which I think have been hugely neglected for me,
one of the most the real turn ons with the whole idea of really understanding. This is a pattern.
It is not a it's not a principle. It is not a universal. Here is an idea. And you know what,
sometimes this this works in some cases and not in others. And here's why. And people often miss
that they kind of kind of latched onto the surface as it were, but not really understood the death.
And for me, the huge, the huge influence was the trade offs, understand the trade offs. Here
it's just like, Ah, here we go. Here is here is why this works well here, but doesn't overheard
the context dependence of the idea was absolutely huge. So rather than talking about software from
the perspective of mathematics in which, which is a time, which is timeless and universal and
rest of it, here was something that was hugely dependent on, well, I can't tell you what's right.
You know, somebody says, is this the right way? Or is this wrong? It's just like, well,
you know, it's going to be that it depends. And that's not because I'm being a consultant. It's
because genuinely, there are about three or four different ways of doing this. Yeah, show me the
landscape. You know, you've got to show me the landscape that you're going to you're going to
put this into my answer will be different if you're dealing with a legacy system, perhaps
with a modern system, it'll vary from language to language, depending on certain elements.
But there might be broader ideas that are still stable. But you know, there are, if you say, oh,
this is in a multi-threaded environment, then I might retract my previous answer and go,
actually, we're going to take a different path here. And it's the contextuality and
understanding the trade offs. For me, that was really exciting. Now, we tend to, for a lot of
people, they tend to credit the gang of four, Gamma, Helen, Johnson, Felicides with the pattern
stuff. Now, certainly that initially turned me on. But even before the book was published,
I'd heard about this stuff. But it was this other stuff that was going on inside. This comes from
architecture. This doesn't come from software. This comes from architecture. And Christopher
Alexander kind of originated this idea is this whole idea of he was really big on the idea of
you've got to have an empirical design. In other words, he was trying to move,
he was trying to move building architecture away from fashion.
Which is something I think we get plagued with in software as well. He was trying to say, well,
look, there's an empirical solution to this. Does it work? You know, here are the qualities
that make something work. No, have you defined your problem? Does this solve the problem of
living or whatever context he was looking at? And he was very clear, use the language of empiricism
all the way through. This is not to say there were no artistic qualities to it. But I was always
fascinated. And his writing style, again, it catches the time is 1970. So I started reading
all the Christopher Alexander stuff. And then you eventually hear another book by Christopher
Alexander notes on the synthesis of form, which was published in 1964. And I had a vague awareness
that this had a big influence on a lot of disciplines at the time. But rereading reading
the 1968 NATO software engineering proceedings was fascinating, because they kept referring to
Christopher Alexander. But this predates his patterns work. It was all synthesis to form about
how he thought about design in terms of balance and trade offs and, and, you know, sort of isolating
systems of change from one another and all the rest was hugely, hugely influential.
But kind of, kind of forgotten that kind of got buried there. So this kind of there's this
little capsule into the kind of like, kind of the zeitgeist of the 60s, and design thinking
of all this kind of stuff, Conway gets mentioned, Melvin Conway, and this influence on architecture
of like, you know what, the way that your people communicate, it's going to have a huge exertive
force on the structure that you build, how you communicate is going to is going to influence
that because this is not maths. This is, we're creating a thing. And, and our choice of creation
is going to be influenced by how we talk to one another. And there, again, this gets multiple
references throughout the software engineering proceedings, which I think I think, I think,
I think that's, I think that's, that's deeply entwined in terms, in terms embedded really in,
what engineering really means. I must confess, I, you know, I'm, I, I love maths. I enjoy,
I actively enjoy maths and sometimes do math, solve mathematical problems as a hobby, you know,
but I don't think that what we do is maths. I don't, I don't, it's, it's, it appeals to
mathematical thinkers. But one of the differences between engineering and maths is that engineering
has that pragmatic bent, you know, so if, if you could simulate an aeroplane, the design of an
aeroplane and wholly do that, you know, in a simulated form and just build the aeroplane and
then take passengers, you know, people would do that, but they don't, they do that. And then
they go flying. And certainly, if you're Boeing, I don't know whether, I don't know if they still
do it, but certain for a very long time, the engineers that built the aeroplane were amongst
the first passengers after the test pilot went up to go for a ride in the aeroplane that they
designed. So there's, there's, there's this thing of, of, you know, trying stuff out. And I think
that's one of the principles that I get a little frustrated sometimes with people talking about,
you know, the mathematical nature of programming. It's interesting. It's fascinating. I like,
I like thinking in those sorts of terms, but I don't think that's enough because I think it's
usually harder to be able to write something that's, you know, a provable system than it,
than it is to write the system in the first place. And so you more, it's almost more error prone.
So, you know, it's, it's, it's a complex problem. I was, I was just listening to the radio
today about, actually, I was listening to a podcast from new scientists and they were saying
they've just rejected one of the quantum, supposedly quantum computing proof
encryption algorithms, because somebody managed to break it on their laptop.
And you'd think that'd be a fairly mathematical kind of area of software. So, so, so when we're
building flight control systems or, or car control systems, or even your stock control system,
there's still room for all those human errors and mistakes. Yeah. And, and, and, and, you know,
Margaret Hamilton's little girl to come in and, and screw it up in new and interesting ways.
And I think, I think that's a really important thing because it's, it's, it's,
because this kind of whole point about kind of perspectives of when we look at things,
that how do we reason about them? And that the, it's a distinction I made a number of years ago is
that, you know, software, there are lots of elements of it that are mathematical, but are not
the same as math, you know, but not mathematics. There's a distinction there. So engineering is
not mathematics, but it is mathematical. In other words, it draws very heavily, uses it as a tool,
and that tool can also give us further insights, but they are not an equivalence. There's not,
you know, and that's, and that's a really important distinction. And that idea of,
yes, but when I throw it, does it stick? You know, that kind of stuff is the real thing.
But when, when we actually, you know, yeah, sure, this works in the simulation, but it's, you know,
it's, it's, it's this, it's like, let's, let's take it back to the 80s. Aliens. And, you know,
being asked, you know, how many, how many actual, how many combat drops have you done?
And then we get one answer is like, and then the follows on simulated. It's just like,
okay, there's a big distinction here. You know, you've actually, you know, this is your first
time in properly in the field is a big distinction. So in other words, there's that whole kind of
idea of like the math. And, and I think for, I think for software, the term, there's a lot of
mathematics that is in bits, there are things that are genuine mathematics, there's a lot that
is mathematical, but the better way of looking at it is formal. Now, I don't want to get that wrapped
up with all methods, because that's clearly an important subset, but it's formalized. And I was,
it's, there's, there's elements. And that's something I've always found fascinating is that
you got the human element, which is definitely hugely informal, sloppy, we are not, we are not
formalized creatures with very, very associative. And then what developers have to do is bridge the
gap between this incredibly sloppy world that somehow has form and shape, but is not necessarily
rigid and prescriptive and with well-defined boundaries. And then you kind of shift into
the world of programs, which have exactly opposite nature. They are highly formalized, you know,
it's, it's a programming language is a formal structure. There's no kind of like, well, maybe
today I'll compile it, or maybe you don't. And if it looks like that, you know, you have a problem.
You know, but it, there's kind of, there's a, and what you've got to try and do is build a system
for the kind of the soft squishy thinking and soft squishy beings out of stuff that is really
quite different. And the nature of these two, bringing them together, I think that for me,
that's one of the things I find fascinating, but it's probably also for many people without them
realizing is what's interesting about software development is there is the rewarding aspect
of some things that are solved and elegant. And it's just like, that's done. But then there's
the other element of like, and how does it fit with the world, which is also quite exciting.
And also the discoveries that you make is just like, well, I thought this was a really good
abstraction, but now I truly understand what's being built. I don't think that's the right
abstraction. That doesn't mean it's a bad abstraction. It's just not the right abstraction
for this system. It's just now I understand how it's evolving through time and the kind of the
nature of changes that the client wants from it, or the things we've discovered from, from sprint
to sprint. It's just like, oh, okay, I keep touching this, keep changing it with that
optimism. Oh, I'll get it right this time. But actually, actually, maybe I'm learning
something deeper, the fact that this is not the idea that I thought it was. And I need a different
point of view. And that's, that's not a side effect to an accident. That's the nature of
of the game. It's this exercise, it's this continual exercise in learning in which
we enhance our understanding of the problem that we're trying to address,
and the nature of our solution, a solution that we're trying to apply to it. And, and, and it seems
to, that's one of the things that I very, very strongly come to believe that that's a complete
cornerstone of our discipline. And we optimize to be able to maintain our ability to make changes
when we learn new stuff. So, so I refer to it as this kind of one of the ways of kind of
pragmatically, informally adopting the philosophy of science to software. So I want to, I want to
consciously start out assuming that I'm going to make a mistake and I'm going to be wrong.
And then I'm going to look at ways in which I can falsify my, my guesses along the way.
And that's a much stronger way of learning than assuming that my design's perfect and it's going
to be right. And I'm never going to have to correct it again. I've found the one true way.
You know, I'm always reminded, you know, this, this is years ago, but I had a client where I had,
I, I become a success of visits, I become familiar with the nature of their system and what they
were doing. And they one day asked me, we love you to design this kind of like subsystem. And it's
got these performance constraints and stuff like that. And we've got the suggestion for the basic
idea of the design. And I kind of said, I don't think the memory manager is going to like that. I
don't think that's, that's, I don't think that's going to work. I don't think it's going to meet
the performance requirements that you need. I think it's going to be issues with it. And then I
made a suggestion and I said, are you sure? And I said, well, I think this is going to
work better. I think this will work better with memory allocation on this platform. I think
that for the, you know, you're dealing with peak, they basically wanted to deal with peak demand
in some way. You know, we can't handle the data, but all we need to do is spool it off so we can
handle it later. And I said, I think the way I'm proposing will work this way. And then I tossed
in another idea, because I'm not really happy until I've got three ideas. So they gave me one. I,
I had, I had a preferred one. I didn't think there's a worker and I had a preferred one.
And, you know, and, and then I had a third one. And I thought that one was okay. I thought it was
better than their suggestion, but I didn't think it was great. And, you know, they gave me a couple
of days, you know, they fed me coffee, you know, gave me a meeting room, all the rest of it. But
my favorite thing is one, one of the guys came in one day, you know, the first or second day,
and he came in and he saw I had an idea on my screen. I had code, there were curly brackets
happening. I said, Oh, we didn't expect you to cut it. And it's just, I kind of looked at it.
It was just like, well, how do you think I was going to do this sit here?
You know, and come up with the pure design, you know, I have designed it.
I have the architecture. Here is the solution. And it's just like, no, I'm trying each one of
these out. I want to see what it feels like in code. And also, I'm going to do some basic,
basic performance analysis, not too big, just to get a kind of order of magnitude feel for the
stuff. And I wrote it up. And the funny thing is, I wrote it up. And it's only in hindsight that
I realized I'd written it up like an experimental report. Here's the situation. Here's what we've
got. Here's the various proposals. Here's how we've run it. Here's the results and recommendations
for future work. But what I found is that I was right and I was wrong. I was right. Their approach
wouldn't meet their requirements. I was also right that my preferred approach would meet their
requirements. But I was wrong in that my kind of like third throwaway option, that was outstanding.
It was way ahead of me. You know, and I would not have known that by meditating upon it. That had
to be made real. It had to be brought into the world. And to actually, you also have to kind of
mess about with it. In other words, the very active, and you mentioned kind of like
solving mathematical problems for fun. And that's one of those interesting things is that I'm guilty
of having done similar things in the past. And it's kind of fun. But the thing is, until you've
done it, you don't know how you're going to do it. You've got some ideas, and you're going to crack
away at it. And in that sense, there is a sort of a creativity. You know, mathematics is not
necessarily empirical, but it is certainly creative. I'm going to try this. And what about
this one? What about this? And you've got that. And software just pushes it a little bit further
to bring it into the world and say, Well, yeah, but how does that work in the world as opposed to
this abstract space? And that is the really important. And that idea, I think it's a really
interesting one, because what we're doing is we're bringing together the idea of problem solving and
creativity. But with something that somebody else is going to experience, and they're going to work
with it, that somebody else is either going to be another developer experiencing the code,
or it's going to be an end user experiencing what is this system like. And so there's a kind of a
feedback, you don't necessarily get that quite the same from something that is mathematical.
There's a kind of a sense there of, is this appropriate for the world that we want, as opposed
to, you know, yeah, this is this is fine. It's a nice idea. But it's a case of like, what is its
context? You know, I can give you a picture of a house. And I could ask you, is this a good house?
And, and you could say, Yeah, that looks good. And then I say, Well, here's the hill that I got to
put it on. You said, Well, you didn't say that. The context absolutely matters. And I think that
sometimes we kind of, there's this kind of sort of maths envy that sometimes takes over people.
And sometimes there's that idealism, because software does, you know, as I said, there's
these two different spaces, that the sloppy human one that is filled with economics and
ill form thoughts, and the fact that the realization that no matter what we do with any
development process, people always talk about prioritizing requirements, stuff like that.
Humans don't walk around with a list of priorities that we don't actually that's
that's not a thing that happens in the brain. We don't have lists like that.
And so my wife, I have very organized, but I'm going to say that when people,
but that's a thinking tool, that a list becomes a thinking tool. Yes, when you provoke a human,
just randomly, they don't have a, they have to create a list. And it's going to be drawn from
whatever is available. It's an availability bias there, whatever is available at that particular
point in time. And unless they've already really thought through, I'm going to use lists like
this, unless they've actually structured that in there, then that's not the naturally the way
they think. Most people don't sit there thinking like, we want a product, and I'm going to think in
terms of these requirements. No, you're probably thinking in terms of other things that are your
skill space. And so when we provoke humans into, oh, I need a formal structure, give me a priority
list. That's not how they actually think, but they can learn to move towards it. But that doesn't
mean they're thinking genuinely like that. And then we have this associative mess, which is also
where all the creativity comes from. And then we have this kind of hard edge stuff, which is very
uncompromising. You know, there's no negotiation with the compiler. It's not a matter of opinion
whether or not this works or not. And then we're trying to do all of this. So we've got all these
different strands of creativity yet bounded by a particular formalism. And so it's kind of like
you need lots of different points of view. And so although ultimately, I believe that it is all
underpinned by a perspective of engineering. And I think with software engineering, it's not,
it's, I put it, I did a very long time ago at the GoTo conference. It was nearly 20 years ago,
and it was an end note. And it was, oh, no, it was the Java conference at that time. I hadn't
called themselves GoTo yet. And it was entitled Beyond Metaphore, where I looked at a bunch of
metaphors that we use in software development. And the whole value of them, I sort of said,
yeah, it gives you different points of view. But I said, one of them is actually what we do is
engineering, but it's not engineering that has to worry mostly about physics and logistics. We
don't really worry about logistics. It turns out that what happened? What does engineering look
like when you take all of that away? You're still making trade-offs and you're still doing a whole
load of things. It's just that you don't have to worry about the bridge materials. You don't have
to worry about, you know, all the materials, you don't have to worry about it the same way.
There's a whole load of other things that just disappear, but that doesn't stop it from being
a discipline that is learning-based, that is in some sense pragmatic, but is also very trade-offs
driven. That's a really, really important part. You know, mathematically, we know
when people talk about maths, the trade-offs are not quite there in the same way. When somebody
comes up with a proof, and they can't quite prove it, it's like, well, you know, close enough.
No, that does not pass the mathematical test. It's like the Fermat's Last Theorem,
which is kind of fresh in my mind because I interviewed Simon Singh a few months ago
on some of his things, and he wrote a wonderful book on Fermat's Last Theorem, the history
of that, and Andrew Wiles's proof. Everybody kind of suspected, no, an engineer would have said,
yeah, you know what, Fermat's right. It's close enough. We can't find anything that's good enough.
We can't find an N for, you know, that there is other than Pythagoras, you know, other than A
squared plus B squared equals C squared for A, B, and C being integers. You know,
you're not going to find any other powers. You know, there is no N that is going to fit that.
An engineer would have given up a long time ago. They would have moved on to the next problem,
because they said, you know, actually, we've done a plausibility analysis, and really,
it doesn't look like there's anything there. And given the time and effort, this is good enough.
And that's, in other words, there's a kind of a, there's a stopping point and a trade-off discussion
that happens there. Glenn Vanderberg did a great talk about software engineering a few years ago,
and he says engineering, in other disciplines, engineering is just the stuff that works.
And that's it. It's that mix between adopting a scientific style of rational thinking to solve
the problems, where that's practical. But it doesn't have to be definitive. There's also this
empirical little add-on that, you know, you know, yeah, that's good enough. You know, it's, and that
seems important to me as part, you know, as part of the discipline is to not expect kind of quantum
physics levels of precision in engineering, you know, in engineering, unless you're building
something that's using quantum physics, you know, you don't expect, you don't do that if you're
built in a car, you know, you're more pragmatic than that. And I think, I think that's one of the,
it's interesting that the way that you kind of couch that in talking about your talk is,
is that it's engineering without those kind of the logistics. I talk about it in terms of,
I think our mistake is assuming that engineering, because it's so popular in the real world,
is production engineering that we're talking about. There are synonym and they're not.
There's also design engineering, which I think is much, much closer to what it is that we do.
We're much less interested in those, the logistics of production, because production's free for us.
Yes, yes. Yeah, I think that's a, I think that is a really important distinction because it's,
it's one of those things when you zoom in, and I think it's, it's the, as you start zooming in,
you start realizing distinctions that are not necessarily, and that's, and I think that's,
that's both the strength and weakness of any, of any word when we throw a word out there to say,
this is like this, this is this. We probably have a fairly clear idea in our heads, based on
whatever our experience is, but we've got no guarantee that the other person, the receiver,
has the same mental model. I mean, it's, and that I think is, is a really,
a really important one. It's sometimes when, when you kind of like push the edges of those
definitions. So I think for me, one of the really interesting ones, actually funny enough,
and I wonder whether, and this is tied, I know this is just like a, you know, I can't, I can't
tell you, but basically the late 90s, I read Two Engineers Human by Henry Piotroski.
Wonderful book. Now he's a civil engineer and historian, and really wonderful book, but,
but the subtitle is The Role of Failure, a successful engineer. And it's that idea of
understanding things through failure, which I wonder if that ties into me, my fascination
with taking pictures and how other people send me pictures of failure, but that idea that actually
we can learn a lot by nudging a system to, nudging a system beyond what we actually understood,
nudging it beyond our preconceptions, revealing our own assumptions. It's just like, ah,
and, and, and, and it's occasionally doing that on purpose. But one of the things I'm, I'm, I'm
currently obsessed watching SpaceX build their Starships in, in, in Texas. And, and I've been
following it for a, for a while now, a little while ago, they decided that they, they made
an unusual decision of building their, their, their Spaceships out of stainless steel rather
than aluminium, which is what Spaceships were. Originally they thought they were going to do
carbon fiber. Then they showed, they looked at the alternatives. They came up with stainless steel
because it got a better temperature range and for strength to weight ratio and all that kind of
stuff. But at one point they'd built these things. They'd, they'd flown some of them.
They decided they were going to move from four millimetre stainless steel to three millimetre
stainless steel. Same stuff, same, same type of steel, but just a thickness change. You'd think
that'd be the kind of thing that you could just do your slide roll in the olden days, but running
through, running through a computer and, and understand. But no, they built, they built the
system and then they, they, they pressure tested it to destruction to see how their welds held up,
how their designs stood up under that real and, you know, empirical load in, you know, life-like
circumstances. See what happens at the point when it screws up. And, you know, that, that's what real
world engineering looks like. It seems, seems to me. But there's that idea of like, you're going to,
we're going to do this and try this thing out and then see what happens. And there's a, for me,
that, that's this idea that time is a really important ingredient to what we do, which I think
is really missing from a lot of, a lot of formalisms of what is software, that it's the
time full aspect. It's not the timeless aspect, but the time full aspect. And I was, honestly,
I couldn't tell you the answer to this until I built it and we've seen it for a bit. I know it's
not quite right, but it's plausibly in the right space. And, but I don't know what my assumptions
are, you know, and as by definition, you don't know what your assumptions are. Because, and,
you know, I always like to point out that assumptions are really weird pieces of knowledge.
They are, they are only ever discovered, they are normally only discovered in contradiction.
You know, somebody says something, you go, Oh, but I had assumed that at that moment,
yeah, you discover you had an assumption, you've had it for a long time. But if anybody had asked
you prior to that, what are your assumptions, you just said, I have none. Only when it is
contradicted, you go, Oh, that's an assumption. So this is very curious from an epistemological
point of view. This is really weird kind of thing. Yeah. And it's, it's the fact that you know, if
you know, you know, it's, you know, it's kind of Lego bricks, Lego bricks in the dark, you know,
there is a dark room, I know there are Lego bricks on the floor. The problem is, although I know that
I have assumptions, although I know there are Lego bricks, I can't tell you where they are until
I've stepped on them. I have assumptions, but until, but I'm not going to do that by just standing
at the door, I can, I have to walk into the room, I have to tread through, tread through, there's
one, there's another one, there's another one. You know, it's one of those things, you have
to be deliberate about this, you've got to put that stuff out there. And of course, prior knowledge
can, can give you a real kind of a real leg up, that's the standing on shoulders of giants, that's
the cumulative experience. Now, why, why is it that we are recreating the errors that previous
projects have done? Yeah, we got, we got all this experience. And it's, it's, we, we see this
repeatedly at the level of individuals, companies, and discipline as a whole. It's a case of
one of those interesting things is like, yeah, we all make mistakes. That's absolutely fine. We,
we, we are always operating within complete knowledge by definition. We're operating within
complete knowledge. Software, as you said, production is free. We've got that was a solved
problem in the 1950s. We basically solved the elements of that, and we've just been getting
better at it ever since. But that whole idea of that leaves us with the hard problem of,
and what is it we're trying to build and why? And how do we, how do we do that? Yes. Which
turns out to be surprisingly challenging, but it's by definition, open-ended, because we're
not producing identical artifacts. I've got, I don't know how many of these pens lying around
my office. And they are all equivalent to one another. They are all, except for the in-content,
substitutable for one another. They are identical. That's because they have a production. Yeah,
they have a production process that is designed to eliminate variation. Yeah, we, we've done that
far. Software is never like that. Yes. Yeah. The software challenge is that if somebody comes
along and says, I want something that, I want that system over there running over here,
well, that's a solved problem. If I see one of my sons, if they show me an app on the phone,
I don't have to say, oh, I need to build that. It's just like, I'm going to go to the store
and get it for myself. It's downloadable. If somebody shows me a piece of code and say, oh,
that's really good. We now need to write that code over here. No. Yeah. It's just a case of,
we've solved all of these issues, but we're left with that, we're left with the challenging issue,
which is not the production of the elimination variation, but the production of variation.
That's our job. When somebody says, I want this system, but I want it slightly different.
I want to, you know, I want that thing our competitor has done. Well, that's different
because we don't have their code. That's for us. This is new. It's new to us. I want the old version.
I want a new version of the system. And so whenever anybody asks for a feature extension,
they're not just asking for a feature extension. They're actually asking for a new system.
It's the old system plus the new behavior. That's a new system by definition.
And that's one of the key facets of doing a good job is to be able to make that move
from the previous version to the new version easy. Yes. I am increasingly of the mind that
if you can't change your software, then the software is low quality. That is the practical,
pragmatic realization of quality in your software. I don't care about anything else.
You know, I don't care what language it's in. I don't care what, you know,
if I can change it easily and safely, then it's good quality. Yeah. And and pretty much,
and there was, and again, there were some of these sort of deep tools that, you know,
things like modularity, cohesion, separation of concerns, encapsulation, abstraction,
those sorts of tools that allow us the freedom to make those kinds of moves when we realize,
oh, shit, we got it wrong. Yeah. The freedom to make the change.
And I think that's really important because I think for me, one of those insights or,
you know, an emerging wave of insights over the years has come from this idea,
okay, we're always operating with incomplete knowledge. So that means that whatever I'm
building is, is in some sense wrong, although I think wrong is sometimes there's too much,
there's too much attached to that word. Your best guess so far.
That's my best, yeah, based on what I knew. It's, you know, we did our best job.
This is what we've got. But now we've learned something from it, either because the world
told us, or because we learned as a result, our own awareness of this. But what's interesting
is you can derive a lot of the ideas that we value, modularity, loose coupling and all the rest of it,
from an understanding of like, well, how would you build something if you didn't know everything?
Yes. Yes. Here's the thing I'm not sure about. Here's the thing I'm very sure about. You know,
this thing I'm not sure about, I'm going to really ram it in there and couple tightly to it.
No, you were loosely coupled to it because this is probably going to change. Yeah, exactly. You
know, I'm going to isolate myself from that. This is, I'm not totally sure about it. I've kind
of got an idea, but I want to put a little bit of distance between this and this. And that distance
is our dependencies, that distance is our interfaces, that distance is, the idea is that
all of this falls out naturally when you start saying, well, you can actually, and this is,
I think, is fascinating because it runs along kind of an alternative axis. Sometimes they
arrive at the same conclusion, but sometimes they don't, to the traditional language of abstraction
and things like that. How would you modularise? Modularise according to abstraction? Well,
there's multiple ways of abstract. We have different paradigms for that. But what is interesting is
going, well, how sure are you about this? And it's just like, well, you know, we, oh, I'm pretty
sure we've built, we've done something almost identical. It's not identical, but it's almost
identical. Well, that gives us maybe high confidence and this worked out well. That's the
second bit. People don't even forget that. Doing it, sometimes we get stuck in a rut. You know,
it's just like, we did this before and how did that work out for you? Yeah, you know, not very
well, but we're going to do it again this way. It's just that, no, no, no, there's no opportunity here.
And that's that idea of, you know, you know, it's good to have a few ideas
that you can trade off against one another. Yes. But then you've got that other idea of like saying,
well, let's, let's go through this in terms of certainty. And I've done this a couple of times
with people and they're always kind of slightly freaked out because you kind of come up with a
rough kind of like sketch of what you're going to do. And so, well, hang on, but we haven't actually
talked about all of the design detail that they normally talk about. It's just like, well, yeah,
what we did is we've just drawn a bunch of boxes and lines and things
based on your confidence. In other words, when I've asked a question,
and you've said, oh, yeah, we're not really sure how we can do that. Right, I've drawn a line.
There's a boundary there of knowledge, because we're sure about one side, but we're not sure about
the other. I don't care what it is. I don't care what paradigm we're talking. It's clearly something
we're not sure about. So maybe we shouldn't hug it too closely, a little bit of, a little bit of
looseness. A little bit of a wall would be good. Yeah. And likewise, when somebody says, oh, yeah,
we're going to do it this way. And a colleague says, oh, I thought we're going to do it this way.
You know what? There's a line there as well, because it's clearly this is not settled. And it may
turn out that one of them is right one year and the other one is right the next year. In other
words, things may change whether it's performance characteristics or whatever, but favor one and
then the other. And again, that's the time for rather than timeless quality. But that idea there
that we can get a heads up just by actually almost constructively using our uncertainty
as a positive aspect to sort of see, well, how does this work? And then we've got the empirical
side of things, which is, okay, here's what kept changing every release. You know, what are the
hotspots? What do we keep going back and saying, oh, no, no, this time it'll be right. We'll just
add this here. And that kind of, I think one of the first times I ever really noticed that question
of stability properly feeding back was a Java system. It was a company that was doing a Java
system. And they had the debate about they were having the debate and are not yet resolved it
about checked exceptions. And for those tuning in who are not aware of this feature in Java,
checked exceptions basically allow, basically allow you to make exceptions part of the
signature of a method, sort of checkable aspect of the signature. And it's one of those things
that in theory is a good idea. But that actually turns out that in practice, if you don't know
exactly what you're doing, in other words, you don't have perfect knowledge, and you're building
a large system, they have a really nasty impact because they they introduce an element that is
unstable or rather needs to be stable, but is not yet stable. And I was, how does this fail?
I don't know yet, because we haven't fully understood this goes right back to where we
started. What are the failure modes of this? Yeah, short of saying something trivial, like
there is an error. And that's often what these checked exceptions tend towards, which is
throws framework error. In other words, actually, that's almost no use to anybody whatsoever.
Bad things may happen. Well, we knew that because bad things may happen. You end up either saying
nothing at all, or you say it so precisely that unless you've actually had this out in the field
for a long time and converged on that, the chances are somebody's going to come up with a new failure
mode. And it's just like, Oh, well, so the curious thing is what you've done is that your happy day
scenario, what you want from the method, why the reason you called it, you don't call it,
I'm not calling this method, except perhaps in a test, I'm not calling this method in order for
it to fail. That's not my goal here. I'm calling this because I wanted a result. I'm expecting
that it's all going to work out. So therefore, right at the edge of my vision, and edge of my
awareness, is all of these failures, all these possible failures, which we've not yet explored.
And as time goes by, we get more refined understanding. Oh, this could be distributed. Oh,
well, our distributed bit throws different exceptions to the ones that are those. So we now
made this thing, which was local. Now it's a set. Oh, okay. So we're going to have to change the
signature again. So we've suddenly made something we've caused churn in the in the interface. And
this is one of those interesting things that comes out of the kind of the more empirical side
of things. It's not that nobody who put that feature in Java did so maliciously or without
thought. I certainly, you know, I certainly my understanding at the time, and all everything
being out there, no, there's some good thinking, solid thinking that goes right back to the 1970s
in exploring all of this. But it's the scale, what happens when you actually create open systems
that are large, and with all manner of developers, it suddenly turns out that there's a fundamental
problem here. And it's to do with rate of change and stability of knowledge, what we do and don't
know. And that was a, that was a revelation. Anyway, for this team, it was a real revelation
because they were split down the middle, you know, half or pro, half or anti, and I was just like,
I can't come in and just sort of say, you know, it's going to go this way or that way.
So actually came up with an empirical approach. And I basically said, you know, don't make anything
checked. You know, look at, you know, before you decide to make something checked as it were,
and seal it in, look at how it's behaved over the last few iterations, write a test to simulate
the failures as well. Oh, well, yeah, not just, yeah, but the failures do all of those. And they
were actually quite good, though, but your understanding of the failures. Yeah, how you
respond to it. Absolutely. Yeah. And it turns out that some of these kept changing on a frequent
basis. And I said, look, that idea is not yet stabilized. That idea is still young, you know,
don't nail it in place. And which is my polite way of saying, like, actually, probably, you know,
so don't make it checked until you're sure, which is a fancy way of saying, don't make it checked,
because the chances are they weren't going to go back and review stuff. But what we had is at least
a maturity model. And it's this idea of that it doesn't matter what you think today, you're going
to overvalue your confidence. And, and so it's this idea that time will give you the answer. I
can't tell you how this is going to evolve. I can't tell you how other, you know, this goes
across API's, it's not just about failure loads. Yeah, this is going on. How's it going to be used?
What are the things that are frequently going to change? And then go back to your point about
what we want to do is align the structures of our software with what are the frequent changes we
actually experience, as opposed to, you know, people often pad their design or add complexity,
because they're saying, oh, well, maybe this will change. It turns out that the better your
imagination, the worse this gets. So if you're an imagine, if you are creative and imaginative
developer, you can imagine all kinds of possibilities and the gold plating and the extra hooks and
bits and pieces. And so the more imagination, the less imagination you have, the quicker you'll get
the job done. The more imagination you have, actually, in that sense, it works against you
because you think, well, what about this? What about all of these are possible, but most of them
are not likely. And probably what you want to do is see, well, what actually happens with this?
And that gets you to ask the more meaningful questions like, well, you know, should we release
this API yet? Or should we release it and put a caution on it? It's just like, okay, this is a
beta release. No, we're not planning to support this. This is a beta release. This is for you to
try. It makes us a little more aware. It makes us look up from the keyboard and go, how are people
going to use this? That gets to one of the things that I think is really important is just in software
development is always thinking in that broader context of, you know, how do people consume this?
Whether it's other developers or whether it's end users, how do people consume it? It's that
stuff that you were talking about earlier about the interaction between the relatively rigid
forms of software and the relatively fuzzy forms of people. But ultimately always comes down to
that, whether it's an API or some complex system that people interact with. It's so much about
being pragmatic and learning that. And it's one of the things that drives me nuts, working with
big organisations when they silo up the development process to the extent where you get development
teams who have no idea the context in which they're pieces of software use. They have some kind of
people giving them requirements in the form of programming by remote control, which they're
supposed to be able to churn out these widgets and they don't have any context. And you get to use
software that you can tell sometimes just by using the software. Nobody's ever thought about
actually using this bloody piece of software. It's so bad. And that idea of usability, it's
turtles all the way down. You have the end user, but then also as software developers, we are
clients of our own products. We are clients. It's the classic consumer and supplier metaphor
for understanding components and interfaces and so on. How is somebody going to consume this?
But also there's that contractual idea of like, what am I going to say about how they should
use it? Because everything has a boundary, everything has a limit. And it's that idea,
because the over-engineering issue, I've seen that where, in fact, again, I can pick on Java,
but I've seen it certainly in other cases. So when Java arrived, it basically said,
hey, everything is synchronisable so you can make it thread safe. And I remember thinking at the time,
that's a really bad idea. That's a terrible idea. That's not how you do this. Because I remember
at the time, somebody showed me this C++ and said, oh, this isn't thread safe. And I said,
no, it's perfectly thread safe. You can pass one thread through it. That's it. If you do anything
else on your head, be it. But I've just told you the circumstances under which this will work.
I have given you a context under which this will work. And that's not me being picky. It's actually
a genuine answer. Because otherwise, people do go around and they start goal-plating everything.
And they do so very badly. And it's just like, no, I don't actually... The question is, this is
thread safe. Oh, yeah, but you can't share it between threads. No, I didn't say you could. It's
perfectly thread safe. I can run it in one thread on its own. And that is its safety level. Whereas
there are some code that has a safety level of zero. In other words, this is thread safe if you
pass zero threads through it. It's buggy. I can give you a real-world example of that,
the danger of synchronization blocks in Java. Martin Thompson, my friend,
years ago worked on... At the time, we thought it was the first ever internet bank. So he was
called in to consult on this because they had a serious performance problem with their Java
implementations. It was one of the early big Java implementations in the sort of mid to late 90s.
And he went in, and several people had been to look at that. And it turned out to cut a
long story short. All of the tests ran, and it all looked fine. They put it to production,
and the performance absolutely tanked. It turned out somebody put a synchronization
block around some piece of code in the critical path. So this internet bank could support one
concurrent user, and everybody else queued up. And that's the thing is that for people
oh, yeah, but this needs to be thread safe. Well, what do you mean by that term? Yeah.
And it's like, why are you doing it? There was, again, the context. So it's a case of like,
not everything wants to be shared between threads. And there are other ways. And everybody was a lot
happy in the 90s. And you finally got... It's one of the reasons I keep certain old books around
is that you can kind of see the shifts in style and approach. But I also remember with the client,
this was a C++ system that we went through and looked at their problem. And they just
it wasn't they weren't highlighting a performance problem. But I remember looking through,
they've got this huge stuff in memory, lots of data, lots of rows of data memory. And there's
30,000 locks. And I'm sitting there going like, I'm pretty sure this is all the ways that you could
do this. This is probably not the right way. And it was one of those kind of like, take a step back
and look at it and go, well, actually, what you've got, you've done it as a kind of a data
centered problem with lots of threads operating on the data. And I said, but if you understand
what the threads do, I said, they actually follow a life cycle. And that life cycle,
we could do that as a data flow. In other words, it worked out basically, it was a data flow. We
basically, you know, yeah, you don't need anywhere near as many threads, you're stealing from yourself,
it turns out. And with a lot of this, and it turns out, if you do it as a pipeline,
then we ended up with only needing six locks. And that was in the bits that connected.
That's in the pipes. And in other words, the point is the data, but they said, but the data
itself is not thread safe. And I said, yes, it is because it's environment guarantees that it's
thread safe. That data will only ever be accessed by zero or one threads at any one particular
point in time. And that's the game. And when we start looking at this, and again, this is this
engineering to context idea, is when we talk about when we talk about car safety and road
safety and all the rest of it, we understand that there are conventions, rules of the road,
and contexts in which we evaluate that. And the outside of that, we make no, there's no guarantee.
And that's, again, for my own, for me, that light bulb moment I had when reading
in the early days of patents, actually reading outside the kind of the software space going,
ah, right, this is idea of context. Where does this idea apply? Beyond which we make no guarantees.
And that doesn't mean that it's a bad solution. It just means that it is no longer appropriate or,
you know, outside that context. It's a perfectly fine solution for the thing that it was intended
for. And that, I think, rubs up against a different trend that sometimes we see
developers, architects, and so on, is overgeneralization. The idea that everything must be
general. I think that one of the traps of our disciplines, that it seems to me inherent in
the nature of software, is that we are often inches away from some quite deeply complicated
problems. Whatever the level of abstraction that we're working at almost, as soon as you have,
it seems to me, fundamental, that as soon as you have two copies of information
in separate places that are changing independently, you've got a world-class, first-class,
quantum physics level problem. You know, this is hard stuff. However, you know, whatever the
nature of the information, however is that you deal with it, working on high-performance systems,
along with Martin building exchanges and trading systems and stuff. You know, we measured the costs
of concurrency, locks, comparing swap operations in processes and all those kinds of things to
try and optimize the performance of our systems. And one of the things that I've observed is that
the more that people know about building concurrent systems, the more their advice is,
don't do it unless you can possibly avoid it. This is incredibly difficult stuff. So things
like adding synchronization blocks and saying everybody can now in Java can write threads or
having thousands of locks in your C++ program are all, seems to me, symptoms of not realizing
that this is a point to stop and think hard because this requires hard thinking. This is a
difficult part. It seems to me that concurrency and coupling are the kind of the really hard parts
of our discipline. Yeah, I think so. Because again, coupling is, and what's interesting,
it's interesting you draw those two together because I think the interesting thing about
coupling is that it's, concurrency is hard for us to reason about and conceptualize.
Coupling suffers a different problem. But interesting, but both of both, which I find
fascinating, both of them manifest themselves physically in terms of concurrency is about
structuring things in time. But if somebody says, well, how tightly coupled is this code base,
I'll tell you what, let me do a build. In other words, you can actually measure the energy of
a build. And it's one of those things that I remember turning up at a particular,
it was an engineering project, electricity company, multiple companies were subcontracting.
It was a political nightmare. It was just pure Conway all the way through. But in the failure
mode. And it was a political nightmare. And all kinds of fascinating things. But one of the most
interesting things was, as a development team, the team that I was working with, what we were
working on was very much back end stuff. It was much more towards the hardware, it was the real-time
stuff. But it was kind of like we felt like we came out of our cave to go and speak to these
other people. It's just, oh my goodness, this is absolute enterprise chaos. And then what was
funny is that because we were building for multiple environments, we were building for
32 and 64 bit environments, we were building for slow environments, as well as environments where
we had high-powered CPUs and as we had a framework supposed to work everywhere. But the embedded
environment, oh my goodness, the builds on that were so incredibly slow that we really, we cared
about dependencies at such a level, so that we had fast builds, which meant when we ran out on a
64 bit platform, it was practically instantaneous. It was a beautiful side effect. But then we
encountered all these other people who were just doing all kinds of stuff with their code. And this
was like, I'm going to call it C plus most of what they were writing, because it was clearly
using a C plus plus compiler, but I don't think it got much beyond C. But the way they managed
their dependencies or didn't, everything depended on everything else. And the build times were
staggering and shocking. We ended up building an isolation layer between our company and the rest
as a result, because it's just like we got so used to fast build times on these platforms. And
it's just that idea of like, yeah, I can, you know, how good is your coupling? I can either measure
it in joules or I can actually time it. And you see, you know, it's kind of like, you know,
our builds take a lot less time than your builds, because we've got really, we've stripped it right
down. What is essential? So there's a physical aspect there. But it's not, you know, and again,
concurrency is this physical one, but concurrency would find difficult to reason about because
having so many things in motion is not a, it's not a strength of human beings.
But coupling is, is more a sense of scale is once we've, you know, it's that idea of like,
there's so a large system is like really understanding what a tangle looks like,
really understanding that this dependency means that, and they are both limits, we are limited
by what goes on up here, but in slightly different ways with those two. But I think you're right,
that they are fundamental. They are, they remind us, they remind us of some of the physics that
we do encounter in the universe. And coupling certainly entropy in the build. But concurrency
is, is that point where your idealization lands in the real world. And sometimes it reveals
assumptions. I've certainly had that with one client there. I remember the one, one client,
we did this kind of surgery style thing, you know, I had a couple of days there, I ran some
training, then I had a couple of days, and people would book a morning or an afternoon and kind of,
I'd go with the tea. And I had one, one team say, Oh, well, yeah, an hour of your time this morning
would be great. I said, well, no, you can have the full three hours. They said, no, no, we won't
need that. They walked in. And I remember asking a particular question. I said, you know, looking
at the code, they were going through. And I said, Oh, so how many threads run through this piece of
code? Because I was aware they were using threading. How many threads run through this piece of code?
And there are a number of correct answers to this. Zero is a valid answer, which means this code is
dead. One is also a valid answer. And many is also a valid answer. I didn't get that. What I got is
usually one. And I said, That's interesting. What do you mean by usually one? Why would you not say
that's many? As far as I can say, that's many. Well, what we have is we have a single threaded,
except, you know, this is single threaded code. Except occasionally, another thread will just
sneak into this bit here. Threads don't sneak. And they had this mental model of threading that
was not actually how threading works. They had kind of thought that threading respected the
natural boundaries of the language and statements and blocks and things like that. And they had,
and they said, Well, it only happens occasionally. I said, Well, you know, you only need to fail
occasionally. You know, there's, I said, there's a race condition waiting to happen here, because
you see you load this and then you validate this here. What if something else sneaks in at this
point and you've got an unvalidated, it's just like, and it's kind of like one person looks at
another said, You know, that might explain this intermittent bug we've been having.
And they said, What should we do? Should we add locks everywhere? And I said, No, no, no, no.
Actually, what you need to do is take, take a step back here. Your, the problem is not to add,
but actually to sort of say, Well, why are you doing this? What they were doing was a lazy load.
Yeah. And, and, and it's a case of like, why are you doing the lazy load here? And they said,
Well, we don't know the reasons are lost to time. But I said, because the problem goes away, if you
do an eager load, if you do an eager load before it goes multi threaded, then the data you're looking
at is actually immutable. It's reference data. It's the load that is the state change. And I said,
let's do, let's do the opposite way rather than add locks. Let's take a step back. I mean, honestly,
given enough time, I would have removed all the threads from this application. It was not a
threat application. But, but it was a case of, you know, actually take the opposite view.
This is a question of time, you're doing the load at the wrong time, you're doing, you should be
doing the load before you go multi threaded. And if you do that, then the problem solves itself.
But it's that shift in time and perspective. But my favorite bit, again, to do with time was when,
when the, when the lead in the room said, you know, we might need more than that one hour,
Kevlin now. Because when I said, do you have code like this? Again, it's because it's not,
and it's not to criticize because that's the whole point. It goes back to say, I said earlier,
we are always operating with incomplete knowledge, and we are built filled with assumptions.
Until you've actually run into those, you don't realize what you're missing.
And you, and you don't realize the magnitude of either how well you've done something or
actually how wrong you've understood something. So, oh, actually, no, I'm using completely
the wrong mental model for thinking about this. And that mental model
has informed how I've structured the software, you know, the software is kind of like applied
thought. And that mental model, it's off. And we, so that's the squishy human bit,
that's the learning bit, but it's also the bit we need to be more, we need to sort of say,
yeah, we need to have a bigger process that is tolerant of the fact that we are imperfect.
And we can't know everything. And that's the whole point. This team had not really interacted
with that and had not accommodated that idea at that level. And most teams, I don't think have,
I think it's a very difficult thing for us to do. It's almost against the culture and the nature
of software development in many companies. I think you're absolutely right. And to give me,
bringing it back around to my stuff. But I think that's one of the things that
treating this more like an engineering discipline ought to be able to give us.
It's just those disciplines of just being able to just recognizing that we don't know the answers
when we're starting out, recognizing that we're probably not going to be right. Therefore,
working more experimentally, therefore, working to control, manage the complexity of the systems
that we build and to measure things and to try stuff out. And all of those sorts of things,
test-driven development is certainly part of that for me deeply. But I think that mindset
is so important. One of the other kind of deep properties, it seems to me of software,
is that unlike lots of other things, it's actually very easy to start. You can learn to
write your first simple lines of code in a few minutes. If you've done a little bit of algebra,
at least, it's trivial to just do your first easy, trivial bits of code. But it's deceptive because
you don't go very far before you get into some of these more complicated things that we've been
talking about. And as soon as you start thinking about things like concurrency, that's really hard
for the best people in the world. It's one of those things about Martin Thompson. They're world
class experts at some of these stuff. But they're still, they think really hard and worry about
shared data at any point and all these kinds of things. So to be able to manage this sort of
stuff, it's an interest. I think that's one of the beauties of it. It's a challenge of it that's
delightful. But also, very risky. I've been talking to a few people recently about
low code solutions. And I think that my take is that many of those sorts of systems
suffer from that kind of failing because they assume that it's the almost, that it's the typing
of the code that's the hard part, where it's these broader design concepts and how we organize
the information in ways that we can make a mistake and come back to it in future and correct it.
Identify the mistake. The identification, the recognition of how things, as you say,
is deceptive. Things can get very messy very quickly. And we see that. So I've given a few
partly as a result of failure screens. I've given a few talks on software failures and the
natures of failures and what contribute to them. But one area of enduring fascination for me is
spreadsheets, which I find absolutely fascinating because it takes a good idea and implements it
incredibly badly. In the sense of vision, people find grid forms, it's very, very intuitive.
There's, you know, I mean, what kid doesn't like square paper, you know, this kind of stuff. It's
like, we like laying things out in grids and tables and all the rest of it. This is incredibly
intuitive. And it's, it's a very, to be fair, that might just be you and me.
Yeah, this might be a conversation. Obviously, anybody else in the comments is free to add in.
But, you know, this whole thing is incredibly intuitive at that level. But the problem is
there are two very fundamental issues that scupper spreadsheets and make them massively error prone.
One is, well, three, let me raise that to three. Okay. One is the fact that you end up with a lot
of interdependencies very, very quickly. If you're doing anything that is reasonable.
And then the next bit, and they're all invisible. This is the one, and that this is,
this is the one thing that as a software developer, you kind of look at spreadsheet and go, well,
that's a nice start. But where's the button that I pressed that shows me all the dependencies
between everything you've hidden all of the code, you've hidden a bit that actually makes it
that shows me the structure and therefore reveals my assumptions. You've actually, and I understand.
And there's no real mechanism for me to step back to safety when I screw it up.
Yeah, well, yeah. But there's that idea that we've lost the bit. In other words, what we've done is
we've presented the veneer and treated that as the whole was no software spreadsheets have a deep
structure. But I, you know, honestly, even just using something like even a word document has
better structure, structure tools than Excel. In Excel, I can go in and I've got the grid. That's
it. I'm done. That is my abstraction. That's the level at which I'm invited to think.
The code is fragmented and scattered around the relationships are thrown to the winds.
It's a matter of detective work and archaeology to recover them. Whereas a whereas when I work with
a document, and this is, you know, this, this is true of many different editors and word processes,
I can get a high level structure, I can do an outline structure, I can say show me the dependencies,
show me the cross references to this kind of stuff. In other words, it's spreadsheets are
absolutely, you know, they are an absolute mess. And I always say this and somebody says,
oh, but users find them intuitive. They find them intuitive to use. But they, it's like walking,
it's like walking to a minefield. You can walk into a minefield incredibly easily.
The problem is we have been depriving and I think this is, you know, actually, I'm going to,
I'm going to push this one right back to the profession here. We've been depriving people
of the things that we know. Because we know that a spreadsheet is a, it's got a terrible
type system. It's astonishingly bad. And if anybody ever throws up the argument of backward
compatibility, that is an absolute nonsense. We've actually seen formats for documents change
on a five to 10 year cycle. There's no backward compatibility issue here at all with things
like Excel. That's a myth. You know, you're looking at a five-year window at most. Explain to me why
it is that my, my Excel looks like it was developed 30 years ago, but everything else that I'm using
looks like it was developed in at least the last decade. Why, why is Excel failed to take on board
all of these other tools that we know to show dependencies? And the third thing to come back
to is that people don't realize that there are these issues. So they therefore, they, they, they
scale up very, very, very poorly. They, they embed many mistakes. Now, the point here is what
the reason I'm riffing on this is because you talked about the low code stuff.
Spreadsheets are by far and away the world's most successful low code to low code solution.
And, and anybody who hopes to rival that is just kidding themselves. I'm going to say that right
now, you know, I'm not predicting the future. It's just like those here are very particular
strives. They have embedded themselves in a particular way, you know, in a particular world
and they've, and they're very current. They are ubiquitous. But if anybody wants to learn how
to do and how not to do, learn from its successes and learn from its failures,
and what are the things that it's good at and not good at, and then also go back through the
history of 4GLs. And what you'll discover is that you, what there's a, there's a, there's a line
that you're looking to draw. And you need to understand that it's not universal. There's
a line here. You need to work out where it is. It's like, this is the bit that allows people
the convenience they want. And then there's this other bit that's incredibly hard. If you're, if
you're assuming that they can do, you're going to, you're, they're going to be in for a big and
nasty shock. And we're just going to be throwing more stuff over the wall at other people for,
oh, we need to customize this all. Oh, this is something that we knocked up. And, you know,
but we're having a couple of problems with it. And then kind of suddenly, yes, that it, you know,
it's just like, at this point, they suddenly discover that they are, in fact, an Olympic
runner, because that's the only way they can get away from it faster. That we are, if we're not
doing this one right. So I don't have any grievance with low code. It's just that when people talk
about it as a general solution, no, what's value. It's a highly specific solution. That's its value.
That's the value. Yeah. A narrow, narrow, narrow constrained focus. Yeah. That's really good. That
thing. Yeah. Again, it goes back to this question, this quest for generality that we sometimes have,
we over generalize, make things, either we end up over generalizing, make things ridiculously
complex for ourselves, or we end up not over generalizing and forever working around taking
something that really wasn't. It's, it's, it's, it's that, it's that really, really, you know,
shades of gray kind of slippery slope over a snake pit,
mixing my metaphors horribly. But, but, but you, you can't, you kind of go from, you know,
I'm adding up a column of numbers in my spreadsheet. Cool. That's really nice. It's really good for
that to, you know, I've built this thing. And if I change that, it goes and recalculates all of
these other things. And there's all this, you know, which is an un-maintainable big ball of mud.
And, you know, there's, there's no, there's no easy way to define the line between, between where
you step over and it becomes, you know, it's just like you tread carefully, push, push your foot out
just in front of you, just to, and I think that that is the, again, that feeds back into this
idea that what we're looking for is, is an approach, a philosophical approach, but a practical
approach. And, you know, I'm definitely of the school of thought that, I guess, old school
philosophy, in the sense that philosophy was intended to be a practical thing to help you
understand life. It was not intended to be abstract and disconnected from life. It was
intended to be quite the opposite. So for me, this idea of actually what we want from how we
think about software, we need, we need to understand is like, yeah, it's a bit experimental.
There's, there's, there's things that are unknown. And actually, not only is that okay,
but that's actually part of the job. It's not just acceptable. It is the job. It is the job.
It is the job. So, so, so I've, I've, I've just become aware of the time we've, the amount of time
we were talking about time and now we're aware of it. It's been so, so much fun having the
conversation, but let's try and, let's try and run this off. So, so, so if you, if you could,
if you could summarize this, what's, what's the advice, what do you think is the advice
that we should give people to help walk this, walk this tightrope, explore, you know,
walk into the, into your room of Lego with bare feet.
Yeah. Well, I guess, I guess the simple one is runaway.
That's the only, I think it's really to, to understand that what the challenges that you are,
the challenge and the joy. Let's, let's, let's, I think we're all, we're always
putting up challenges. I think we all need to accept that sometimes the challenge is the fun bit.
You know, it has two, two aspects to it. The challenge and the fun is that you are working
with incomplete knowledge. There is a joy to learning something. And there can also be a joy
in discovering better solutions, alternative solutions, penny dropping moments where you go,
you know what, I've been thinking about this wrong. And, you know, yes, I've over-abstracted.
This really is just a string. The abstractions I wanted to do are over here.
Those are the points that I love those run. Oh, shit. I was thinking of it wrong. And now I can
see a new, new path. That's the thing. And it's the case of like, or I've been devoting my effort
to this, but actually the fun is over here. If I reframe the problem, in other words,
it's that idea of take a step back. And I think a little bit, a little bit
something, I keep reading to do a talk called slow agile. I think I'm probably going to do it at some
point. But it's this idea that although we often use the language of fast in connection with many
of our practices, I think sometimes there's a different emphasis I might want to give, which is
that some of what we're trying to do is to do things sooner, as opposed to faster, which is not
quite the same thing. Yes, I'll use the phrase. I think that might what you might be saying is
the phrase that I use is small steps. Yeah. And small steps are a way of achieving it.
And I was just talking to a group today about refactoring. I was trying to
emphasise to them, you know, in terms of all the design practices and so on. I said, there's a
difference between running and walking. And there's a very simple idea that running is defined by
the fact that at various points, you have zero points of contact with the ground. If you watch
somebody running, it's 0101. If you watch somebody walking, it's 1212. There is always at least one
point of contact with the ground. And now what does this mean? It means you move from moment to
moment more slowly. But you are also moving with more certainty and more sureness and the ability
to change your direction. If you try changing your direction when you are running, or when you
stumble when you're running, and I have relatively recent personal experience of this, and I can
say it hurt an awful lot and I was not able to walk for a few weeks. Whereas had I had that fall
when I was walking, I would have just got up and walked off. The point there is that
software development is not a race. Although we use the language of fast, the time scales
we're thinking of, sometimes it encourages the wrong behaviour, I think. It's again one of those
things when what I use a word and somebody else picks up, oh, they're talking about raw speed.
We're not trying to optimise for speed of development. What we observe is the speed of
development or the speed of deployment. But the thing, it's not the pedal to the metal.
You're just going to exhaust all your developers and exhaust their capacity to think creatively.
It's the idea that actually what we need to be doing is walking. It is the idea of stability.
It's the idea of like, oh, that's not right. Let me just pull back a moment. It's the small steps.
Let's roll back and take a different path. Whether that rollback is a version control rollback,
whether that rollback is a conceptual rollback, whatever it means is the idea that we have given
ourselves the opportunity to pay attention to what we're doing. When you are moving at high
speed, you're not paying attention to your surroundings. All of this talk of feedback just
disappears in the wind. The whole point is you are sensing your way and that you are adjusting
according to that. And it's your feet on the carpet type thing. It's the sensing
your way. How are we doing? I originally intended this, but now I see this. Why? Because I can,
because I've taken the time to do that. I'm not individually, as a human being, remember it's
all about people ultimately, me to appreciate which formal approach I'm going to take, which
structure, which choices, which modules I am going to select, or my criteria for modularity,
which ones I'm going to do here as opposed to there. That's going to take a deliberation that
doesn't happen when you're exhausted and running at speed. It's not a productivity conversation.
So I think for me, most of the advice I give people is just like, honestly,
go a little bit slower because you'll go faster. That's great. I hadn't thought of it in those
words, which is always interesting, and always an interesting thing, but still was reinforcing
my prejudices. So good both ways around. He gave me some new things to think about
and reinforce my prejudices, which is great. I've really enjoyed the talk today. Thank you
so much for taking part. Thank you, Dave. It was a fun exploration. Please do check out
Kevlin. Check his Google Unique name. You'll find his stuff, and lots of good books that Kevlin's
written as well. So thanks, Kevlin, very much indeed. And I'll let you know when the video is
available. Thank you. Thank you very much, Dave. That's brilliant.
