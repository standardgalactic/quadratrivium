start	end	text
0	10000	For a long time, most people whose opinion I respected were somewhat skeptical of metrics and in particular of metrics used to measure software development performance.
10000	16000	Martin Fowler wrote an excellent piece called Cannot Measure Productivity some years ago.
16000	25000	Then some smart people came up with a better way and discovered all sorts of interesting and useful things based on their metrics.
25000	34000	These were not necessarily new ideas, but the combination was, and the model that they built from them was certainly new.
34000	38000	And is, in my opinion, an important step forward for our discipline.
38000	46000	So, how do you measure software delivery and what are the important lessons that we can learn from it when we do?
46000	59000	Hi, I'm Dave Farley of Continuous Delivery. Welcome to my channel.
59000	62000	And if you haven't been here before, please do hit subscribe.
62000	66000	And if you enjoy the content today, hit like as well.
66000	72000	I'd like to begin by thanking our sponsors, Equal Experts, Octopus and Speckflow.
72000	78000	They're great supporters of our channel, so please support them in turn by checking their links in the description below.
78000	87000	If you'd like to learn more about the engineering approach that I describe a little later in this video, check out my new book, Modern Software Engineering,
87000	95000	which is currently a bestseller in several categories on Amazon and also at the time of recording on sale.
95000	99000	In 2010, the book that I wrote with Jess Humble was published.
99000	104000	It introduced lots of ideas to people who hadn't seen this way of working before.
104000	108000	Its popularity surprised both of us.
108000	112000	We certainly believe that the ideas in it were important and valuable.
112000	116000	We'd already seen them work in every place that we'd tried them over several years.
116000	122000	But Continuous Delivery came to be seen as a significant book, and we didn't really anticipate that.
122000	127000	As we gained more experience with the practices in a wider variety of settings though,
127000	130000	our understanding of this approach grew significantly.
130000	135000	So now, I no longer feel embarrassed to say things like,
135000	139000	this is the current state of the art in software development.
139000	145000	And this represents a genuine engineering discipline for software in that if you practice it,
145000	151000	you will substantially increase your chances of success, whatever it is that you're trying to build.
151000	158000	One of the reasons that I feel confident to make such claims is because we have the best data on software delivery practice
158000	165000	that's been collected so far, and it tells us that this stuff works better than anything else that we know of.
165000	171000	A few years after our book was published, Jess met Dr. Nicole Fosgren,
171000	178000	and together with some sponsors, they started work on something called the State of DevOps Report.
178000	183000	Nicole brought an academic approach to the collection and interpretation of the data.
183000	188000	It's scientifically justifiable in terms of how that works.
188000	195000	The results are, as far as I can tell, the most thorough examination of software development practice
195000	198000	than our industry's seen so far.
198000	205000	Later, Jess, Nicole and Jean Kim formed Dora and released the Accelerate book,
205000	211000	which describes some of their findings over the years and the science behind their approach.
211000	221000	This is sociology, not physics, so research like this is complex and always open to interpretation and question to some degree.
221000	225000	Human beings are messy things compared to atoms and electrons.
225000	230000	Nevertheless, their findings are profound. They can make predictions.
230000	234000	These predictions have been born out in thousands of organizations now.
234000	239000	They say things like, there's no trade-off between speed and quality.
239000	245000	Excellence in software delivery and operational performance drives organizational performance.
245000	254000	And the data also tells us that continuous delivery is being successfully adopted by more and more teams year on year.
254000	260000	At the heart of the State of DevOps research and the approach are a collection of metrics,
260000	264000	now commonly referred to as the Dora metrics.
264000	269000	For decades, people had been trying to figure out how to measure software development performance.
269000	275000	They counted lines of code, man hours and later velocity in the form of story points.
275000	278000	None of these things really matter.
278000	282000	Why would you want more code if you could do a better job with less?
282000	286000	You don't go faster by adding more people, you go slower.
286000	293000	And velocity only tells us what we did last week and it's not really transferable between teams.
293000	296000	Velocity is also really easy to game.
296000	298000	Want to increase your velocity for your team?
298000	304000	Okay, this story, which was five points last week, is ten points this week.
304000	310000	I've seen all of these games being played out in real teams. They don't help us to do a better job.
310000	315000	Metrics are extremely useful, but not when they're treated as a goal.
315000	320000	Our goal is always to produce good software that people find useful efficiently.
320000	325000	The difficulty of measuring software projects left lots of people, including me,
325000	329000	thinking that you can't measure software productivity in any way that's useful.
329000	335000	And that is still probably correct, but we were thinking about the problem in the wrong way.
335000	338000	Productivity is really the wrong measure.
338000	343000	It's not that productivity doesn't matter, but it's not enough on its own.
343000	349000	If we produce loads of things, but they're all useless, then we aren't really productive.
349000	354000	So if we count lines of code and optimize to create lots of code,
354000	358000	then we're actively encouraging people to write nasty, complex systems
358000	362000	that over time will become harder and harder to work on.
362000	369000	So even by the mistaken measure of productivity, we have to be really careful about what we measure and incentivize.
370000	376000	All metrics suffer from this problem. They're all subject to being misinterpreted and misapplied,
376000	381000	particularly when we use them as goals in their own right.
381000	385000	The people behind the Dora metrics made some good choices.
385000	392000	These metrics are not easy to cheat or misinterpret, but somehow people still manage to.
392000	396000	So you still have to be cautious how you use them.
396000	402000	The Dora company was later unbought by Google, who now run the ongoing research
402000	407000	and have lots of interesting stuff online in addition to that.
407000	412000	You can get a quick assessment of where you stand on the Dora metrics, for example,
412000	416000	and targeted advice for next steps.
416000	422000	You can also explore the relationships between different behaviors that have been unearthed by their research.
422000	428000	There are four Dora metrics divided into two groups, stability and throughput.
428000	434000	The Dora research categorized teams into groups based on their scores on these metrics,
434000	437000	ranging from low performers to elite.
437000	442000	The difference in performance levels based on these metrics is sometimes extreme.
442000	448000	The research correlates behaviors and outcomes based on the scores against these metrics too,
448000	452000	and these too are often quite extreme in their range.
452000	457000	For example, an elite team will deploy changes to production
457000	462000	thousands of times more frequently than a low-performing team.
462000	467000	At the same time, they'll produce code of significantly higher quality,
467000	474000	and they will spend 44% more of their time on new features than low-performing teams.
474000	477000	The measure themselves are interesting.
477000	480000	They don't attempt to measure productivity directly,
480000	485000	and it's important to consider them as a whole, not independently of one another.
485000	490000	They're focused on measuring the flow of changes and their quality.
490000	495000	Stability is a measure of the quality of the software that we produce,
495000	500000	so you can't mistakenly optimize to produce loads of rubbish and get a good score
500000	503000	as you can with most more naive productivity metrics.
503000	509000	Stability is measured by change failure rate and mean time to recovery.
509000	516000	That is, what proportion of the time does a change result in a failure of some kind
516000	518000	when it reaches production?
518000	523000	And how long does it take your team to recover from a failure once it finds one?
523000	529000	Elite teams have changed failure rates in the 0 to 15% range.
529000	533000	Everybody else is in the 16 to 30% range.
533000	538000	Elite teams restore service in under one hour once a defect is found.
538000	543000	Low performers often take over six months to achieve the same result.
543000	549000	Throughput is measured by lead time for changes and deployment frequency.
549000	556000	Lead time for changes is measured by how long does it take from commit to code running in production.
556000	562000	And deployment frequency is how often do you release a change to end users?
562000	566000	These are measurements of the efficiency with which we can create changes
566000	569000	of the quality measured by our stability metrics.
569000	575000	They measure the cost of our production processing time and the rate of flow of change into production.
575000	582000	Elite teams have lead times of under one hour compared to over six months for low performers.
582000	586000	And deployment frequency is on demand multiple times per day usually
586000	591000	compared to less than once every six months for low performers again.
591000	596000	The combination of measuring stability and throughput together is important.
596000	601000	There's no value in optimizing for throughput if the stability goes down
601000	606000	because over time and it's a relatively short period of time
606000	611000	will incur more work as a result of the low quality of our work as measured by its stability.
611000	618000	This means that we'll end up going a lot slower and so our throughput will suffer after all.
618000	623000	This is why elite teams outperform poor teams quite so significantly.
623000	627000	They produce higher quality work and they do it more efficiently.
627000	630000	These metrics are inextricably linked.
630000	637000	Optimizing for a naive version of throughput is an extremely common anti-pattern.
637000	640000	I see this in dev teams all of the time.
640000	646000	The organization promotes the idea that it's vital to deliver features at some high rate.
646000	650000	So they cut corners all over the place to do so.
650000	654000	The result is that quality goes down.
654000	660000	Now dev teams are either drowning bug reports and the effort to prioritize and fix them
660000	667000	or they produce such tactical crap that their systems rapidly become so difficult to change
667000	671000	that they grind to a halt and make little or no forward progress at all.
671000	676000	In both of these cases productivity slows down dramatically
676000	680000	and better teams that don't cut corners like this steam pass them
680000	684000	and produce more better features sooner overall.
684000	688000	You need good scores on throughput and stability
688000	692000	and you need to take a longer term view of what counts as success.
692000	698000	Being able to work at a sustained pace matters a lot in software development.
698000	705000	The Dora metrics are still metrics so it's still more subtle than simply striving for good scores.
705000	708000	There's a very good presentation by Brian Finster.
708000	713000	There's a link in the description below called How to Misuse the Dora Metrics
713000	717000	where he points out several common misinterpretations and misuses.
717000	725000	Throughput is not about lowering quality to make developers cogs in some feature factory machine.
725000	728000	It's about reducing the back size of change.
728000	734000	We want to work in smaller steps that give us the chance to gather more feedback,
734000	738000	work more experimentally and so learn faster.
738000	742000	These are ideas at the heart of my modern software engineering approach.
742000	748000	So the Dora metrics can be used to help us amplify our engineering discipline.
748000	753000	The goal is long term success, not short term gratification here.
753000	758000	So if we want to maintain our ability to have a good scores in throughput
758000	764000	and good scores in stability too, we need to work in ways that are sustainable over time.
764000	770000	We can't afford to create rubbish now that doesn't result in us going faster over the long haul.
770000	774000	That means we need to manage the complexity in our systems
774000	778000	in a way that allows us to establish and maintain a fast pace.
778000	784000	So we need to employ the second group of principles from my engineering book too.
784000	788000	These are mutually reinforcing ideas.
788000	791000	This relationship is important.
791000	796000	In Brian's talk he correctly points out that in the Accelerate book
796000	800000	there are lots more things than only these metrics to consider.
800000	804000	I think that the Dora metrics are a great tool.
804000	810000	We can use them to select choices that work better than others, but they aren't the goal.
810000	816000	The goal is to produce great software efficiently and repeatedly that our users get value from.
816000	822000	I've seen teams misuse the Dora metrics, but I don't think that devalues them.
822000	829000	The problem with all metrics is that human beings are both smart and dumb at the same time.
829000	836000	Smart in that they will attempt to gain whatever metrics you choose if it's in their interest to do so.
836000	841000	I once saw a company incentivize their teams to compete with Dora scores.
841000	847000	Some of the teams redefined what the stability metrics meant so that they could get better scores.
847000	853000	Instead of change failure rate, they interpreted this to mean bugcat.
853000	856000	A subtle difference, but an important one.
856000	860000	They interpreted throughput as the number of features delivered,
860000	865000	and no measure of time involved and they ignored deployment frequency altogether.
865000	870000	Then they optimized for fixing bugs over delivering features.
870000	876000	Now, because they'd lost focus on feature delivery, they cut more corners to try and keep feature delivery up,
876000	880000	but the real change failure rate went up, not down.
880000	886000	They still declared success though, because overall the bug count did go down.
886000	893000	That's because the team, unsurprisingly, prioritized fixing easy bugs, even if they weren't important.
893000	896000	After all, they were being incentivized to reduce the bug count.
896000	899000	Nothing said which bugs they should fix.
899000	903000	Overall, the bug count numbers went down, but the real stability was worse than before.
903000	907000	Humans are dumb in the sense that they love a simple target.
907000	912000	So even in complex situations, we were often focused only on the target,
912000	917000	ignoring everything else, even when other things are more important.
917000	923000	So if you think throughput means speed of feature delivery rather than rate of change,
923000	927000	you'll ignore the longer term consequences of cutting corners.
927000	932000	The data is in. Working with high throughput and high stability
932000	937000	is the most efficient way to sustainably deliver software that we know.
937000	942000	The problem with this is that there is a point at which the lines cross.
942000	946000	What if you don't care about being sustainable?
946000	953000	If I need to deliver a feature next week, and that is my only way of measuring success,
953000	957000	then it may be in my interest to cut the corners.
957000	961000	If I don't test, if I just implement the first thought that springs to mind,
961000	965000	if I don't worry about the maintainability of the code that I leave behind,
965000	967000	I may save some time.
967000	973000	So my feature is delivered within a week, but the chances are it won't be working the following week.
973000	978000	The following week, I'll get bug reports, and now when I need to add a new feature,
978000	980000	my code is a mess.
980000	984000	So as well as taking the time to fix the last week's bugs,
984000	987000	my new feature will also take longer to create.
987000	990000	This is always a stupid trade.
990000	995000	The problem is in identifying the point at which the cost of doing a bad job
995000	999000	is more than the cost of doing a better job.
999000	1001000	It's always difficult.
1001000	1007000	In my experience though, the cost of bad work hit us a lot sooner than most of us seem to think.
1008000	1011000	If you have a six month deadline to meet,
1011000	1017000	there's no doubt at all that spending the extra time to do a good job will pay off.
1017000	1022000	You'll produce more, better code over that time.
1022000	1028000	If the deadline is only a couple of weeks and you're about to leave the project and have no conscience,
1028000	1033000	then cutting corners may get you to a low quality delivery sooner.
1033000	1036000	If your deadline is somewhere between these limits though,
1036000	1042000	it's quite tricky to tell if the investment in doing a good job will get you to the finish line sooner.
1042000	1045000	There is no silver bullet.
1045000	1049000	Software development is complicated and difficult to do well.
1049000	1052000	The Dora metrics are trailing indicators.
1052000	1055000	They tell you how you did, not how you're going to do.
1055000	1060000	Approaches like continuous integration and test driven development and continuous delivery
1060000	1064000	predict good scores on those trailing indicators.
1064000	1068000	So represent other tools that we can use along with them.
1068000	1073000	Think of the Dora metrics as being rather like the instruments in your car.
1073000	1079000	They make no sense without context and correlation with the other instruments.
1079000	1086000	If our speedometer says we're traveling at 70 miles an hour, is that good or bad?
1086000	1087000	Neither.
1087000	1089000	It depends on the context.
1089000	1095000	It may be good if we're within the speed limit and hoping to make good time to our destination.
1095000	1100000	It's bad if we're driving through a school district or want to maximise our range.
1100000	1105000	What does it mean if we are averaging 60 miles an hour towards our destination?
1105000	1108000	Again, it depends on other things.
1108000	1113000	If we're about to run out of charge or petrol, then our average speed doesn't tell us.
1113000	1119000	We need to correlate that with those other instruments to figure out when we'll arrive.
1119000	1126000	If our engine is overheating and about to break down, then driving this fast is just hastening the point at which our engine fails.
1126000	1130000	We have to treat the Dora metrics for what they are.
1130000	1134000	Useful gauges that can give us information about our progress.
1134000	1139000	They aren't targets and how we interpret them will depend on context.
1139000	1145000	But for the first time, we have some instruments that can help us to see how we're doing.
1145000	1149000	As long as we're thoughtful about what the results mean.
1149000	1153000	This is a very big step forward.
1153000	1155000	Thank you very much for watching.
