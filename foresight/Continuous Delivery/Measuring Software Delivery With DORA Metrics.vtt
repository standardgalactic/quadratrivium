WEBVTT

00:00.000 --> 00:10.000
For a long time, most people whose opinion I respected were somewhat skeptical of metrics and in particular of metrics used to measure software development performance.

00:10.000 --> 00:16.000
Martin Fowler wrote an excellent piece called Cannot Measure Productivity some years ago.

00:16.000 --> 00:25.000
Then some smart people came up with a better way and discovered all sorts of interesting and useful things based on their metrics.

00:25.000 --> 00:34.000
These were not necessarily new ideas, but the combination was, and the model that they built from them was certainly new.

00:34.000 --> 00:38.000
And is, in my opinion, an important step forward for our discipline.

00:38.000 --> 00:46.000
So, how do you measure software delivery and what are the important lessons that we can learn from it when we do?

00:46.000 --> 00:59.000
Hi, I'm Dave Farley of Continuous Delivery. Welcome to my channel.

00:59.000 --> 01:02.000
And if you haven't been here before, please do hit subscribe.

01:02.000 --> 01:06.000
And if you enjoy the content today, hit like as well.

01:06.000 --> 01:12.000
I'd like to begin by thanking our sponsors, Equal Experts, Octopus and Speckflow.

01:12.000 --> 01:18.000
They're great supporters of our channel, so please support them in turn by checking their links in the description below.

01:18.000 --> 01:27.000
If you'd like to learn more about the engineering approach that I describe a little later in this video, check out my new book, Modern Software Engineering,

01:27.000 --> 01:35.000
which is currently a bestseller in several categories on Amazon and also at the time of recording on sale.

01:35.000 --> 01:39.000
In 2010, the book that I wrote with Jess Humble was published.

01:39.000 --> 01:44.000
It introduced lots of ideas to people who hadn't seen this way of working before.

01:44.000 --> 01:48.000
Its popularity surprised both of us.

01:48.000 --> 01:52.000
We certainly believe that the ideas in it were important and valuable.

01:52.000 --> 01:56.000
We'd already seen them work in every place that we'd tried them over several years.

01:56.000 --> 02:02.000
But Continuous Delivery came to be seen as a significant book, and we didn't really anticipate that.

02:02.000 --> 02:07.000
As we gained more experience with the practices in a wider variety of settings though,

02:07.000 --> 02:10.000
our understanding of this approach grew significantly.

02:10.000 --> 02:15.000
So now, I no longer feel embarrassed to say things like,

02:15.000 --> 02:19.000
this is the current state of the art in software development.

02:19.000 --> 02:25.000
And this represents a genuine engineering discipline for software in that if you practice it,

02:25.000 --> 02:31.000
you will substantially increase your chances of success, whatever it is that you're trying to build.

02:31.000 --> 02:38.000
One of the reasons that I feel confident to make such claims is because we have the best data on software delivery practice

02:38.000 --> 02:45.000
that's been collected so far, and it tells us that this stuff works better than anything else that we know of.

02:45.000 --> 02:51.000
A few years after our book was published, Jess met Dr. Nicole Fosgren,

02:51.000 --> 02:58.000
and together with some sponsors, they started work on something called the State of DevOps Report.

02:58.000 --> 03:03.000
Nicole brought an academic approach to the collection and interpretation of the data.

03:03.000 --> 03:08.000
It's scientifically justifiable in terms of how that works.

03:08.000 --> 03:15.000
The results are, as far as I can tell, the most thorough examination of software development practice

03:15.000 --> 03:18.000
than our industry's seen so far.

03:18.000 --> 03:25.000
Later, Jess, Nicole and Jean Kim formed Dora and released the Accelerate book,

03:25.000 --> 03:31.000
which describes some of their findings over the years and the science behind their approach.

03:31.000 --> 03:41.000
This is sociology, not physics, so research like this is complex and always open to interpretation and question to some degree.

03:41.000 --> 03:45.000
Human beings are messy things compared to atoms and electrons.

03:45.000 --> 03:50.000
Nevertheless, their findings are profound. They can make predictions.

03:50.000 --> 03:54.000
These predictions have been born out in thousands of organizations now.

03:54.000 --> 03:59.000
They say things like, there's no trade-off between speed and quality.

03:59.000 --> 04:05.000
Excellence in software delivery and operational performance drives organizational performance.

04:05.000 --> 04:14.000
And the data also tells us that continuous delivery is being successfully adopted by more and more teams year on year.

04:14.000 --> 04:20.000
At the heart of the State of DevOps research and the approach are a collection of metrics,

04:20.000 --> 04:24.000
now commonly referred to as the Dora metrics.

04:24.000 --> 04:29.000
For decades, people had been trying to figure out how to measure software development performance.

04:29.000 --> 04:35.000
They counted lines of code, man hours and later velocity in the form of story points.

04:35.000 --> 04:38.000
None of these things really matter.

04:38.000 --> 04:42.000
Why would you want more code if you could do a better job with less?

04:42.000 --> 04:46.000
You don't go faster by adding more people, you go slower.

04:46.000 --> 04:53.000
And velocity only tells us what we did last week and it's not really transferable between teams.

04:53.000 --> 04:56.000
Velocity is also really easy to game.

04:56.000 --> 04:58.000
Want to increase your velocity for your team?

04:58.000 --> 05:04.000
Okay, this story, which was five points last week, is ten points this week.

05:04.000 --> 05:10.000
I've seen all of these games being played out in real teams. They don't help us to do a better job.

05:10.000 --> 05:15.000
Metrics are extremely useful, but not when they're treated as a goal.

05:15.000 --> 05:20.000
Our goal is always to produce good software that people find useful efficiently.

05:20.000 --> 05:25.000
The difficulty of measuring software projects left lots of people, including me,

05:25.000 --> 05:29.000
thinking that you can't measure software productivity in any way that's useful.

05:29.000 --> 05:35.000
And that is still probably correct, but we were thinking about the problem in the wrong way.

05:35.000 --> 05:38.000
Productivity is really the wrong measure.

05:38.000 --> 05:43.000
It's not that productivity doesn't matter, but it's not enough on its own.

05:43.000 --> 05:49.000
If we produce loads of things, but they're all useless, then we aren't really productive.

05:49.000 --> 05:54.000
So if we count lines of code and optimize to create lots of code,

05:54.000 --> 05:58.000
then we're actively encouraging people to write nasty, complex systems

05:58.000 --> 06:02.000
that over time will become harder and harder to work on.

06:02.000 --> 06:09.000
So even by the mistaken measure of productivity, we have to be really careful about what we measure and incentivize.

06:10.000 --> 06:16.000
All metrics suffer from this problem. They're all subject to being misinterpreted and misapplied,

06:16.000 --> 06:21.000
particularly when we use them as goals in their own right.

06:21.000 --> 06:25.000
The people behind the Dora metrics made some good choices.

06:25.000 --> 06:32.000
These metrics are not easy to cheat or misinterpret, but somehow people still manage to.

06:32.000 --> 06:36.000
So you still have to be cautious how you use them.

06:36.000 --> 06:42.000
The Dora company was later unbought by Google, who now run the ongoing research

06:42.000 --> 06:47.000
and have lots of interesting stuff online in addition to that.

06:47.000 --> 06:52.000
You can get a quick assessment of where you stand on the Dora metrics, for example,

06:52.000 --> 06:56.000
and targeted advice for next steps.

06:56.000 --> 07:02.000
You can also explore the relationships between different behaviors that have been unearthed by their research.

07:02.000 --> 07:08.000
There are four Dora metrics divided into two groups, stability and throughput.

07:08.000 --> 07:14.000
The Dora research categorized teams into groups based on their scores on these metrics,

07:14.000 --> 07:17.000
ranging from low performers to elite.

07:17.000 --> 07:22.000
The difference in performance levels based on these metrics is sometimes extreme.

07:22.000 --> 07:28.000
The research correlates behaviors and outcomes based on the scores against these metrics too,

07:28.000 --> 07:32.000
and these too are often quite extreme in their range.

07:32.000 --> 07:37.000
For example, an elite team will deploy changes to production

07:37.000 --> 07:42.000
thousands of times more frequently than a low-performing team.

07:42.000 --> 07:47.000
At the same time, they'll produce code of significantly higher quality,

07:47.000 --> 07:54.000
and they will spend 44% more of their time on new features than low-performing teams.

07:54.000 --> 07:57.000
The measure themselves are interesting.

07:57.000 --> 08:00.000
They don't attempt to measure productivity directly,

08:00.000 --> 08:05.000
and it's important to consider them as a whole, not independently of one another.

08:05.000 --> 08:10.000
They're focused on measuring the flow of changes and their quality.

08:10.000 --> 08:15.000
Stability is a measure of the quality of the software that we produce,

08:15.000 --> 08:20.000
so you can't mistakenly optimize to produce loads of rubbish and get a good score

08:20.000 --> 08:23.000
as you can with most more naive productivity metrics.

08:23.000 --> 08:29.000
Stability is measured by change failure rate and mean time to recovery.

08:29.000 --> 08:36.000
That is, what proportion of the time does a change result in a failure of some kind

08:36.000 --> 08:38.000
when it reaches production?

08:38.000 --> 08:43.000
And how long does it take your team to recover from a failure once it finds one?

08:43.000 --> 08:49.000
Elite teams have changed failure rates in the 0 to 15% range.

08:49.000 --> 08:53.000
Everybody else is in the 16 to 30% range.

08:53.000 --> 08:58.000
Elite teams restore service in under one hour once a defect is found.

08:58.000 --> 09:03.000
Low performers often take over six months to achieve the same result.

09:03.000 --> 09:09.000
Throughput is measured by lead time for changes and deployment frequency.

09:09.000 --> 09:16.000
Lead time for changes is measured by how long does it take from commit to code running in production.

09:16.000 --> 09:22.000
And deployment frequency is how often do you release a change to end users?

09:22.000 --> 09:26.000
These are measurements of the efficiency with which we can create changes

09:26.000 --> 09:29.000
of the quality measured by our stability metrics.

09:29.000 --> 09:35.000
They measure the cost of our production processing time and the rate of flow of change into production.

09:35.000 --> 09:42.000
Elite teams have lead times of under one hour compared to over six months for low performers.

09:42.000 --> 09:46.000
And deployment frequency is on demand multiple times per day usually

09:46.000 --> 09:51.000
compared to less than once every six months for low performers again.

09:51.000 --> 09:56.000
The combination of measuring stability and throughput together is important.

09:56.000 --> 10:01.000
There's no value in optimizing for throughput if the stability goes down

10:01.000 --> 10:06.000
because over time and it's a relatively short period of time

10:06.000 --> 10:11.000
will incur more work as a result of the low quality of our work as measured by its stability.

10:11.000 --> 10:18.000
This means that we'll end up going a lot slower and so our throughput will suffer after all.

10:18.000 --> 10:23.000
This is why elite teams outperform poor teams quite so significantly.

10:23.000 --> 10:27.000
They produce higher quality work and they do it more efficiently.

10:27.000 --> 10:30.000
These metrics are inextricably linked.

10:30.000 --> 10:37.000
Optimizing for a naive version of throughput is an extremely common anti-pattern.

10:37.000 --> 10:40.000
I see this in dev teams all of the time.

10:40.000 --> 10:46.000
The organization promotes the idea that it's vital to deliver features at some high rate.

10:46.000 --> 10:50.000
So they cut corners all over the place to do so.

10:50.000 --> 10:54.000
The result is that quality goes down.

10:54.000 --> 11:00.000
Now dev teams are either drowning bug reports and the effort to prioritize and fix them

11:00.000 --> 11:07.000
or they produce such tactical crap that their systems rapidly become so difficult to change

11:07.000 --> 11:11.000
that they grind to a halt and make little or no forward progress at all.

11:11.000 --> 11:16.000
In both of these cases productivity slows down dramatically

11:16.000 --> 11:20.000
and better teams that don't cut corners like this steam pass them

11:20.000 --> 11:24.000
and produce more better features sooner overall.

11:24.000 --> 11:28.000
You need good scores on throughput and stability

11:28.000 --> 11:32.000
and you need to take a longer term view of what counts as success.

11:32.000 --> 11:38.000
Being able to work at a sustained pace matters a lot in software development.

11:38.000 --> 11:45.000
The Dora metrics are still metrics so it's still more subtle than simply striving for good scores.

11:45.000 --> 11:48.000
There's a very good presentation by Brian Finster.

11:48.000 --> 11:53.000
There's a link in the description below called How to Misuse the Dora Metrics

11:53.000 --> 11:57.000
where he points out several common misinterpretations and misuses.

11:57.000 --> 12:05.000
Throughput is not about lowering quality to make developers cogs in some feature factory machine.

12:05.000 --> 12:08.000
It's about reducing the back size of change.

12:08.000 --> 12:14.000
We want to work in smaller steps that give us the chance to gather more feedback,

12:14.000 --> 12:18.000
work more experimentally and so learn faster.

12:18.000 --> 12:22.000
These are ideas at the heart of my modern software engineering approach.

12:22.000 --> 12:28.000
So the Dora metrics can be used to help us amplify our engineering discipline.

12:28.000 --> 12:33.000
The goal is long term success, not short term gratification here.

12:33.000 --> 12:38.000
So if we want to maintain our ability to have a good scores in throughput

12:38.000 --> 12:44.000
and good scores in stability too, we need to work in ways that are sustainable over time.

12:44.000 --> 12:50.000
We can't afford to create rubbish now that doesn't result in us going faster over the long haul.

12:50.000 --> 12:54.000
That means we need to manage the complexity in our systems

12:54.000 --> 12:58.000
in a way that allows us to establish and maintain a fast pace.

12:58.000 --> 13:04.000
So we need to employ the second group of principles from my engineering book too.

13:04.000 --> 13:08.000
These are mutually reinforcing ideas.

13:08.000 --> 13:11.000
This relationship is important.

13:11.000 --> 13:16.000
In Brian's talk he correctly points out that in the Accelerate book

13:16.000 --> 13:20.000
there are lots more things than only these metrics to consider.

13:20.000 --> 13:24.000
I think that the Dora metrics are a great tool.

13:24.000 --> 13:30.000
We can use them to select choices that work better than others, but they aren't the goal.

13:30.000 --> 13:36.000
The goal is to produce great software efficiently and repeatedly that our users get value from.

13:36.000 --> 13:42.000
I've seen teams misuse the Dora metrics, but I don't think that devalues them.

13:42.000 --> 13:49.000
The problem with all metrics is that human beings are both smart and dumb at the same time.

13:49.000 --> 13:56.000
Smart in that they will attempt to gain whatever metrics you choose if it's in their interest to do so.

13:56.000 --> 14:01.000
I once saw a company incentivize their teams to compete with Dora scores.

14:01.000 --> 14:07.000
Some of the teams redefined what the stability metrics meant so that they could get better scores.

14:07.000 --> 14:13.000
Instead of change failure rate, they interpreted this to mean bugcat.

14:13.000 --> 14:16.000
A subtle difference, but an important one.

14:16.000 --> 14:20.000
They interpreted throughput as the number of features delivered,

14:20.000 --> 14:25.000
and no measure of time involved and they ignored deployment frequency altogether.

14:25.000 --> 14:30.000
Then they optimized for fixing bugs over delivering features.

14:30.000 --> 14:36.000
Now, because they'd lost focus on feature delivery, they cut more corners to try and keep feature delivery up,

14:36.000 --> 14:40.000
but the real change failure rate went up, not down.

14:40.000 --> 14:46.000
They still declared success though, because overall the bug count did go down.

14:46.000 --> 14:53.000
That's because the team, unsurprisingly, prioritized fixing easy bugs, even if they weren't important.

14:53.000 --> 14:56.000
After all, they were being incentivized to reduce the bug count.

14:56.000 --> 14:59.000
Nothing said which bugs they should fix.

14:59.000 --> 15:03.000
Overall, the bug count numbers went down, but the real stability was worse than before.

15:03.000 --> 15:07.000
Humans are dumb in the sense that they love a simple target.

15:07.000 --> 15:12.000
So even in complex situations, we were often focused only on the target,

15:12.000 --> 15:17.000
ignoring everything else, even when other things are more important.

15:17.000 --> 15:23.000
So if you think throughput means speed of feature delivery rather than rate of change,

15:23.000 --> 15:27.000
you'll ignore the longer term consequences of cutting corners.

15:27.000 --> 15:32.000
The data is in. Working with high throughput and high stability

15:32.000 --> 15:37.000
is the most efficient way to sustainably deliver software that we know.

15:37.000 --> 15:42.000
The problem with this is that there is a point at which the lines cross.

15:42.000 --> 15:46.000
What if you don't care about being sustainable?

15:46.000 --> 15:53.000
If I need to deliver a feature next week, and that is my only way of measuring success,

15:53.000 --> 15:57.000
then it may be in my interest to cut the corners.

15:57.000 --> 16:01.000
If I don't test, if I just implement the first thought that springs to mind,

16:01.000 --> 16:05.000
if I don't worry about the maintainability of the code that I leave behind,

16:05.000 --> 16:07.000
I may save some time.

16:07.000 --> 16:13.000
So my feature is delivered within a week, but the chances are it won't be working the following week.

16:13.000 --> 16:18.000
The following week, I'll get bug reports, and now when I need to add a new feature,

16:18.000 --> 16:20.000
my code is a mess.

16:20.000 --> 16:24.000
So as well as taking the time to fix the last week's bugs,

16:24.000 --> 16:27.000
my new feature will also take longer to create.

16:27.000 --> 16:30.000
This is always a stupid trade.

16:30.000 --> 16:35.000
The problem is in identifying the point at which the cost of doing a bad job

16:35.000 --> 16:39.000
is more than the cost of doing a better job.

16:39.000 --> 16:41.000
It's always difficult.

16:41.000 --> 16:47.000
In my experience though, the cost of bad work hit us a lot sooner than most of us seem to think.

16:48.000 --> 16:51.000
If you have a six month deadline to meet,

16:51.000 --> 16:57.000
there's no doubt at all that spending the extra time to do a good job will pay off.

16:57.000 --> 17:02.000
You'll produce more, better code over that time.

17:02.000 --> 17:08.000
If the deadline is only a couple of weeks and you're about to leave the project and have no conscience,

17:08.000 --> 17:13.000
then cutting corners may get you to a low quality delivery sooner.

17:13.000 --> 17:16.000
If your deadline is somewhere between these limits though,

17:16.000 --> 17:22.000
it's quite tricky to tell if the investment in doing a good job will get you to the finish line sooner.

17:22.000 --> 17:25.000
There is no silver bullet.

17:25.000 --> 17:29.000
Software development is complicated and difficult to do well.

17:29.000 --> 17:32.000
The Dora metrics are trailing indicators.

17:32.000 --> 17:35.000
They tell you how you did, not how you're going to do.

17:35.000 --> 17:40.000
Approaches like continuous integration and test driven development and continuous delivery

17:40.000 --> 17:44.000
predict good scores on those trailing indicators.

17:44.000 --> 17:48.000
So represent other tools that we can use along with them.

17:48.000 --> 17:53.000
Think of the Dora metrics as being rather like the instruments in your car.

17:53.000 --> 17:59.000
They make no sense without context and correlation with the other instruments.

17:59.000 --> 18:06.000
If our speedometer says we're traveling at 70 miles an hour, is that good or bad?

18:06.000 --> 18:07.000
Neither.

18:07.000 --> 18:09.000
It depends on the context.

18:09.000 --> 18:15.000
It may be good if we're within the speed limit and hoping to make good time to our destination.

18:15.000 --> 18:20.000
It's bad if we're driving through a school district or want to maximise our range.

18:20.000 --> 18:25.000
What does it mean if we are averaging 60 miles an hour towards our destination?

18:25.000 --> 18:28.000
Again, it depends on other things.

18:28.000 --> 18:33.000
If we're about to run out of charge or petrol, then our average speed doesn't tell us.

18:33.000 --> 18:39.000
We need to correlate that with those other instruments to figure out when we'll arrive.

18:39.000 --> 18:46.000
If our engine is overheating and about to break down, then driving this fast is just hastening the point at which our engine fails.

18:46.000 --> 18:50.000
We have to treat the Dora metrics for what they are.

18:50.000 --> 18:54.000
Useful gauges that can give us information about our progress.

18:54.000 --> 18:59.000
They aren't targets and how we interpret them will depend on context.

18:59.000 --> 19:05.000
But for the first time, we have some instruments that can help us to see how we're doing.

19:05.000 --> 19:09.000
As long as we're thoughtful about what the results mean.

19:09.000 --> 19:13.000
This is a very big step forward.

19:13.000 --> 19:15.000
Thank you very much for watching.

