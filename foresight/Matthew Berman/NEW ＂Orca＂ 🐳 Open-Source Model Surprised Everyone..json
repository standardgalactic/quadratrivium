{"text": " There's a battle being waged right now in the world of artificial intelligence between large foundational models and smaller open source models. And just this week, a new research paper was dropped that promises to append the conversation completely. Now, if you remember a few weeks ago, I made a video about the letter called We Have No Mote, which was a leaked internal memo from Google that really highlighted how open source models, smaller ones specifically, are iterating so quickly that these large foundational models that Google and OpenAI have are truly at risk. And I found that to be a very compelling paper. And then just two weeks ago, another research paper was released that claimed to disprove a lot of the value that these open source smaller models have. Today, we're gonna take a look at all of this and we're gonna figure out what's the truth. We're gonna take a look at the new Orca paper that was just dropped this week. We're gonna look at the We Have No Mote document again and we're gonna take a look at the research paper that came out a couple of weeks ago talking about the false promise of imitating proprietary large language models like GPT-4. Let's go. So this is Orca, progressive learning from complex explanation traces of GPT-4. This is a new research paper dropped by Microsoft Research of all companies. Of course, they made a substantial investment in OpenAI and own a significant portion of that company. So for them to release a new research paper illustrating a new technique to make open source smaller models extremely powerful is really fascinating. Microsoft as a company has embraced open source in the years since Satya Nadella took over and I'm all for it. This paper is absolutely fascinating and it makes a ton of sense. But before we get into this paper, let's take a look at those previous documents that I mentioned. Now a little over a month ago, this internal memo from Google was released called We Have No Mote. And the main point of this memo is that open source models are proliferating and iterating so quickly that the gap between models like GPT-4 and Palm 2 are shrinking very quickly. The fact that any developer can get their hands on these models and new techniques to train and fine tune these models are coming out every day. And we're seeing that from Laura to Q Laura to now having a ton of different options of how to train and fine tune these models in really efficient ways and run them on any consumer grade hardware. And I agreed with a lot that was in this paper. Of course, a business mode is not just the technical limitations. There's much more to it than that. But a lot of the points made in this paper are very valid. And I've seen more innovation in the open source community over these last few weeks than I've seen on these proprietary large models. But then a research paper out of UC Berkeley was dropped a couple of weeks ago that really challenged the assertions of the We Have No Mote leak document. In this research paper, the false promise of imitating proprietary LLMs, they spell out that these open source models are simply just imitating the outputs of these larger models without actually understanding the logic to reach certain outputs. The gist of this paper and what Orca looks to correct is that these open source models are simply being trained on prompts and responses which is good for pattern matching. So for example, if you're a student in college and you're taking a class, you could probably do pretty well on a lot of tests simply by pattern matching the question to an answer. But that student is gonna have a lot of limitations. If one of the questions varies from their pattern matching ability, by even just a little bit, their ability to reason and figure out what the answer might be becomes highly limited. Whereas the student who fundamentally and deeply understands a topic won't be thrown off by any variation of the question. They'll be able to reason and step by step get to the answer because they do truly understand the topic. And that's really the difference between these large foundational models and the open source imitations of them as per this paper. And that brings us to Orca. Orca challenges the idea that open source models can only really imitate answers and will get thrown off by any variation in the prompts themselves. And the way they do it seems very obvious in hindsight. Before we get into the details, Orca outperforms every other open source model and even outperforms ChatGPT, which is GPT 3.5, in a lot of different benchmarks. Now, of course, it still lags behind GPT4, but the gap continues to close. So let's take a look at this paper now. They start off the abstract by addressing this imitation concept. Recent research has focused on enhancing the capability of smaller models through imitation learning, drawing on the outputs generated by large foundational models. Again, LFMs are referring to ChatGPT and GPT4. And they start to outline the limitations of these imitation techniques. Some that they point out are limited imitation signals from shallow LFM outputs, small scale homogenous training data, and most notably a lack of rigorous evaluation resulting in overestimating the small models capability as they tend to learn to imitate the style but not the reasoning process of LFMs. That is really the crux of this paper. How do we start getting these open source models to not just mimic the question answer pairs, but actually understand how they get from a question to an answer. And only with that is true intelligence created. To address these challenges, we develop ORCA, a 13 billion parameter model that learns to imitate the reasoning process of LFMs. Let's pause there for a second. This model, the ORCA model is only 13 billion parameters, which means it can run on pretty much any modern hardware. Whereas some of the other models that I've been reviewing recently, like the Guinaco model, require me to rent out a cloud GPU, like an A6000 that has 48 gigabytes of VRAM, because it's so large, 65 billion parameters. And this performs better than that. Now here's the key to the paper. Here's the key technique. ORCA learns from rich signals from GPT-4, including explanation traces, step-by-step thought processes, and other complex instructions guided by teacher assistance from chat GPT. Now I'll explain what teacher assistance is in a little bit, but looking at this sentence, what it's really saying is, rather than learning from the prompt and response pairs, we're going to ask these large foundational models to explain their reasoning step-by-step, and the smaller open source models will learn from that. Truly fascinating. Now I wanna briefly touch on this guided by teacher assistance from chat GPT. They have a two-tier teaching process. One, they take chat GPT, which is GPT-3.5, and they have a large number of examples to learn from, five million. Then they take those five million, boil it down to the most important one million examples, and then use GPT-4 to continue to train on more complex examples. So how does it actually perform? ORCA surpasses conventional state-of-the-art instruction-tuned models, such as Vecunia 13B by more than 100% in complex zero-shot reasoning benchmarks like Big Bench Hard and 42% on AGI Eval. Big Bench Hard and AGI Eval are just sets of tests that they give to these large language models to test their performance. ORCA reaches parity with chat GPT on the BBH benchmark and shows competitive performance in professional and academic examinations like SAT, LSAT, GRE, and GMAT, both in zero-shot setting without chain of thought while trailing behind GPT-4. And again, this last sentence is everything. Our research indicates that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills. And just like humans, large language models understanding how something works is much more effective than just being able to pattern match questions and answers. So large language models are typically tuned by something called instruction tuning. You have a set of prompts and you have a set of responses and those pairs are passed to the open source model and it learns from that. This technique is called explanation tuning where it's not just the prompt and the answer but an explanation of the reasoning and the logic for how chat GPT and GPT-4 arrived at an answer. And so we can see here when evaluated by GPT-4 and that's called auto-evaluation, ORCA 13B actually beats chat GPT. It beats Bard and it certainly beats the open source models based on Lama. And then for zero shot problems on academic exams, chat GPT definitely performs better but ORCA 13B is really closing the gap in performance and performs much better than Virginia 13B. And for complex zero shot reasoning tasks and big bench hard, ORCA achieves parity with chat GPT. And here again, they specifically call out that imitation paper. Authors assert that model imitation is a false promise since broadly matching chat GPT using purely imitation would require one, a concerted effort to collect enormous imitation data sets and far more diverse and higher quality imitation data than is currently available. So one of the biggest problems is these open source models can't get enough data to use the imitation technique and perform at the same rate as these large foundational models. Contrary to this assertion, we demonstrate that both conditions one and two are attainable and that it is possible to reduce the gap with proprietary LLMs on multiple zero shot benchmarks that require sophisticated reasoning. And here they touch on what the existing open source models are doing currently to train themselves. Both Alpaca and Wizard LM employ a variant of self-instructs. So that's what we've been talking about. Wizard LM introduces the concept of Eval Instruct which gradually rewrites the initial set of instructions into more complex versions attempting to overcome some of the methods inherent shortcomings. But with Vakunya and Kuala, they demonstrate remarkable performance due to the more human-like conversations and natural instructions in the community contributed conversations like those in shared GPT. So basically what they're saying is as more people are using these open source models and sharing their data, sharing their instructions, their prompts and the output, they'll continue to train on those pairs and get better and better. But there's a limitation with that as well. And it's the same thing that we keep coming back to. Models trained on such natural conversations may capture the style but not the reasoning process of the LLFM. So again, they'll be able to pattern match but they're not gonna truly understand the logic and the reasoning behind arriving at the solutions. Now the Orca Paper puts forth three key contributions. Number one is explanation tuning. And again, this is fine tuning models based on the step-by-step explanation of the reasoning and the logic of how to arrive at a solution. Let's read this a little bit. We augment the query response pairs with detailed responses from GPT-4 that explain the reasoning process of the teacher as it generates the response. And to get the step-by-step reasoning, they're using some of these more modern prompting techniques that we've been learning about, such as explain like I'm five, think step-by-step and justify your response. This forces GPT-4 to put forth its reasoning and its logic in the response itself and that is used to train. And that's what explanation tuning is. Another issue is scaling the amount of tasks and instructions. As you'll see in a graph that I'll show in a second, a lot of these open source models are using a highly limited data set, but that's where Orca really excels. We utilize the Flaan 2020 collection and that's a data set of tasks and instructions put forth by Google that has tens of millions of instructions. So let's quickly take a look at the data sizes for these open source models. All of them have in the thousands. So you can see here that Alpaca has 52,000, Vakunya has 70,000 and WizardLM with the most has 250,000 based on the teacher of chat GPT. And some of these other ones like Dolly are human instructed. So they're even more limited because of the limitations of humans. However, as you could see here, Orca has five million, many times more than all of the other open source models. And it's based on chat GPT initially, so that's the initial five million pass and then GPT-4 with a second pass of much more complex tasks and instructions. So not only are they getting full explanations of query and responses and how they actually reach those responses, but they're getting so many more of them and they're solving the data scaling issue. Last is evaluation. There are a lot of issues with current evaluation techniques for open source models, but Orca claims to solve these in a few ways. They use auto evaluation with GPT-4. So basically asking GPT-4 between two potential responses, which one is best. They also use academic benchmarks like Big Bench Hard and Truthful QA and professional and academic exams like the SAT, LSAT, et cetera. And last, they use safety evaluation from ToxicGen, basically do these responses contain toxic language. So in figure four, they illustrate what the previous techniques do with queries and responses. So here's a user instruction. Use the data to calculate the median. Here it says, first, we need to arrange the data in ascending order. Since there are five numbers, the median is in the middle, which is seven. And so this is very basic. It's a prompt, a query, and then the response. Whereas with their new method, they ask GPT-4 to explain. So the system instruction seems to be the main tool that they use to get chat GPT and GPT-4 to explain their reasoning. You are an AI assistant. User will give you a task. Your goal is to complete the task as faithfully as you can while performing the task. Think step by step and justify your steps. So again, use the given data to calculate the median. Same prompt. To calculate the median, I will follow these steps. And GPT-4 actually outlines step by step how it will figure out what the median is. That data is then used to train the open source model. I find it so fascinating that we're using some of these modern prompting techniques like chain of thought, like explain like on five, that people have been figuring out over the last few months to get better answers from chat GPT and GPT-4. And we're using those to get better data to train the open source models with. And as I mentioned, system messages seem to be the main tool to get chat GPT and GPT-4 to provide the step by step explanations. And if you play around with the chat GPT playground or even the API, you'll know that the system messages are a requirement for using either of these tools. So here are a few examples. You will be given a task, you must generate a detailed and long answer. Think like you are answering to a five year old, help as much as you can. So it's really just coaxing chat GPT-4 to explain its reasoning and to be as verbose as possible. So let's actually take a look at the difference between these two prompting techniques. So for the system message, you are an AI assistant that helps people find information. User will give you a question. Your task is to answer as faithfully as you can while answering things step by step and justify your answer. So the prompt is pick which sentence is not logical. Sentence A, people in the desert often look forward to flood. Sentence B, people in the desert often look forward to rain. Now it might seem obvious to us why these sentences are different, but there's really just a subtle difference between them. So here a typical response might be, there is no reason to look forward to a flood because floods cause damage. The answer is sentence A. Chat GPT says, okay, let's break it down. Read the sentences carefully. So it's talking about what steps am I going to take to answer this? Identify the differences between them and determine which sentence is not logical. Then they say, sentence A, people in the desert often look forward to the flood. This sentence suggests that people in the desert are eagerly anticipating a flood. However, floods are typically destructive and dangerous. So it seems unlikely that people would look forward to them. Sentence B, they're looking forward to the rain. And it says this sentence suggests that people in the desert are eagerly anticipating rain while the desert is a dry environment. Rain can be beneficial in providing water for plants and animals. Then it finishes with, based on the above analysis, it can be concluded that sentence A is not logical. Then we look at GPT-4's answer. And as you can see, I won't read all of it. It's a much more detailed and verbose answer. Now in this section, they talk about why Chat GPT as a teaching assistant, assistant to GPT-4, is such a powerful method. And there's really two reasons for it. One is a capacity gap because there's such a large gap between the ORCA model and GPT-4. Being able to take data from GPT-4 and passing it directly into ORCA, it's gonna struggle with imitation. Whereas if they progressively teach it to get to the GPT-4 level by the intermediate step of Chat GPT, it really performs much better. This can be viewed as a form of progressive learning or curriculum learning, where the student first learns from easier examples followed by harder ones. Again, more and more human-like behavior. Human doesn't go from learning the basics of addition all the way to calculus in one step. They learn many incrementally more difficult steps of mathematics between addition and calculus. Next is a simple pragmatic reason, cost and time. Chat GPT, specifically GPT-3.5 turbo, is much faster and much less expensive than GPT-4. So that's why they use five million examples with Chat GPT and one million examples for GPT-4. So this graphic shows the performance of these large foundational models, Vecunia and ORCA. And as we can clearly see from questions from the LSAT and the SAT, ORCA performs significantly better than Vecunia. And if we look at the ORCA column, compared to the Chat GPT column, overall it performs quite similarly, but it still does lag behind GPT-4. And they've actually shown that this progressive learning technique really works. As we can see here, using only GPT-4, they were able to achieve a score of 37.18, whereas if they used that intermediate step of Chat GPT, they were able to achieve 41.7. That might seem small, but that is a significant improvement. And for the big bench hard results, ORCA performs marginally better than Chat GPT on aggregate across all tasks, significantly lags GPT-4 and outperforms Vecunia by 113%. And here they give a graphic of the zero-shot performance against all of these different tasks. And you can see that ORCA performs substantially better than Vecunia. And even across all of them, like it said, it performs better than Chat GPT. So what does all this mean? I find it fascinating for two reasons. One, open-source models continue to get better at a rapid clip. New techniques for fine-tuning, training are coming out every single day. So I think back to that we have no mode paper and it makes a lot of sense still. I also find it fascinating that GPT-4 still seems to have some secret sauce and performs much better than any other model. So OpenAI seems to have plenty of mode. This mode seems to be incrementally getting decreased, but they still do have it. The last thing that I find fascinating is that this paper is by Microsoft Research. Microsoft Research owns a significant portion of OpenAI. So the fact that they're making research gains in open-source is awesome. And OpenAI really must feel that they have a significant mode to work with. And OpenAI also mentioned a week ago that they're releasing their own open-source model. So I think what all of this means is that these large language models will continue to get better and cheaper over time. Now Orca's code and dataset are not yet released, but as soon as it is, we're gonna review it. I'm gonna show you how to use it and we'll see how it performs. If you liked this video, please consider giving me a like and subscribe and I'll see you in the next one.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.44, "text": " There's a battle being waged right now", "tokens": [50364, 821, 311, 257, 4635, 885, 261, 2980, 558, 586, 50536], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 1, "seek": 0, "start": 3.44, "end": 5.44, "text": " in the world of artificial intelligence", "tokens": [50536, 294, 264, 1002, 295, 11677, 7599, 50636], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 2, "seek": 0, "start": 5.44, "end": 7.96, "text": " between large foundational models", "tokens": [50636, 1296, 2416, 32195, 5245, 50762], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 3, "seek": 0, "start": 7.96, "end": 10.16, "text": " and smaller open source models.", "tokens": [50762, 293, 4356, 1269, 4009, 5245, 13, 50872], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 4, "seek": 0, "start": 10.16, "end": 13.4, "text": " And just this week, a new research paper was dropped", "tokens": [50872, 400, 445, 341, 1243, 11, 257, 777, 2132, 3035, 390, 8119, 51034], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 5, "seek": 0, "start": 13.4, "end": 16.68, "text": " that promises to append the conversation completely.", "tokens": [51034, 300, 16403, 281, 34116, 264, 3761, 2584, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 6, "seek": 0, "start": 16.68, "end": 18.2, "text": " Now, if you remember a few weeks ago,", "tokens": [51198, 823, 11, 498, 291, 1604, 257, 1326, 3259, 2057, 11, 51274], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 7, "seek": 0, "start": 18.2, "end": 21.16, "text": " I made a video about the letter called We Have No Mote,", "tokens": [51274, 286, 1027, 257, 960, 466, 264, 5063, 1219, 492, 3560, 883, 376, 1370, 11, 51422], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 8, "seek": 0, "start": 21.16, "end": 23.88, "text": " which was a leaked internal memo from Google", "tokens": [51422, 597, 390, 257, 31779, 6920, 35900, 490, 3329, 51558], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 9, "seek": 0, "start": 23.88, "end": 26.64, "text": " that really highlighted how open source models,", "tokens": [51558, 300, 534, 17173, 577, 1269, 4009, 5245, 11, 51696], "temperature": 0.0, "avg_logprob": -0.13535935024045548, "compression_ratio": 1.589090909090909, "no_speech_prob": 0.01743057370185852}, {"id": 10, "seek": 2664, "start": 26.64, "end": 30.28, "text": " smaller ones specifically, are iterating so quickly", "tokens": [50364, 4356, 2306, 4682, 11, 366, 17138, 990, 370, 2661, 50546], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 11, "seek": 2664, "start": 30.28, "end": 32.0, "text": " that these large foundational models", "tokens": [50546, 300, 613, 2416, 32195, 5245, 50632], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 12, "seek": 2664, "start": 32.0, "end": 36.0, "text": " that Google and OpenAI have are truly at risk.", "tokens": [50632, 300, 3329, 293, 7238, 48698, 362, 366, 4908, 412, 3148, 13, 50832], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 13, "seek": 2664, "start": 36.0, "end": 38.72, "text": " And I found that to be a very compelling paper.", "tokens": [50832, 400, 286, 1352, 300, 281, 312, 257, 588, 20050, 3035, 13, 50968], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 14, "seek": 2664, "start": 38.72, "end": 40.760000000000005, "text": " And then just two weeks ago,", "tokens": [50968, 400, 550, 445, 732, 3259, 2057, 11, 51070], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 15, "seek": 2664, "start": 40.760000000000005, "end": 42.8, "text": " another research paper was released", "tokens": [51070, 1071, 2132, 3035, 390, 4736, 51172], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 16, "seek": 2664, "start": 42.8, "end": 45.2, "text": " that claimed to disprove a lot of the value", "tokens": [51172, 300, 12941, 281, 717, 46955, 257, 688, 295, 264, 2158, 51292], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 17, "seek": 2664, "start": 45.2, "end": 47.519999999999996, "text": " that these open source smaller models have.", "tokens": [51292, 300, 613, 1269, 4009, 4356, 5245, 362, 13, 51408], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 18, "seek": 2664, "start": 47.519999999999996, "end": 49.92, "text": " Today, we're gonna take a look at all of this", "tokens": [51408, 2692, 11, 321, 434, 799, 747, 257, 574, 412, 439, 295, 341, 51528], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 19, "seek": 2664, "start": 49.92, "end": 51.68, "text": " and we're gonna figure out what's the truth.", "tokens": [51528, 293, 321, 434, 799, 2573, 484, 437, 311, 264, 3494, 13, 51616], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 20, "seek": 2664, "start": 51.68, "end": 53.519999999999996, "text": " We're gonna take a look at the new Orca paper", "tokens": [51616, 492, 434, 799, 747, 257, 574, 412, 264, 777, 1610, 496, 3035, 51708], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 21, "seek": 2664, "start": 53.519999999999996, "end": 55.0, "text": " that was just dropped this week.", "tokens": [51708, 300, 390, 445, 8119, 341, 1243, 13, 51782], "temperature": 0.0, "avg_logprob": -0.08545237156882216, "compression_ratio": 1.775438596491228, "no_speech_prob": 0.11273549497127533}, {"id": 22, "seek": 5500, "start": 55.04, "end": 57.88, "text": " We're gonna look at the We Have No Mote document again", "tokens": [50366, 492, 434, 799, 574, 412, 264, 492, 3560, 883, 376, 1370, 4166, 797, 50508], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 23, "seek": 5500, "start": 57.88, "end": 59.480000000000004, "text": " and we're gonna take a look at the research paper", "tokens": [50508, 293, 321, 434, 799, 747, 257, 574, 412, 264, 2132, 3035, 50588], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 24, "seek": 5500, "start": 59.480000000000004, "end": 60.6, "text": " that came out a couple of weeks ago", "tokens": [50588, 300, 1361, 484, 257, 1916, 295, 3259, 2057, 50644], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 25, "seek": 5500, "start": 60.6, "end": 62.04, "text": " talking about the false promise", "tokens": [50644, 1417, 466, 264, 7908, 6228, 50716], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 26, "seek": 5500, "start": 62.04, "end": 65.88, "text": " of imitating proprietary large language models like GPT-4.", "tokens": [50716, 295, 566, 16350, 38992, 2416, 2856, 5245, 411, 26039, 51, 12, 19, 13, 50908], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 27, "seek": 5500, "start": 65.88, "end": 66.72, "text": " Let's go.", "tokens": [50908, 961, 311, 352, 13, 50950], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 28, "seek": 5500, "start": 66.72, "end": 68.8, "text": " So this is Orca, progressive learning", "tokens": [50950, 407, 341, 307, 1610, 496, 11, 16131, 2539, 51054], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 29, "seek": 5500, "start": 68.8, "end": 72.03999999999999, "text": " from complex explanation traces of GPT-4.", "tokens": [51054, 490, 3997, 10835, 26076, 295, 26039, 51, 12, 19, 13, 51216], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 30, "seek": 5500, "start": 72.03999999999999, "end": 73.24000000000001, "text": " This is a new research paper", "tokens": [51216, 639, 307, 257, 777, 2132, 3035, 51276], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 31, "seek": 5500, "start": 73.24000000000001, "end": 76.08, "text": " dropped by Microsoft Research of all companies.", "tokens": [51276, 8119, 538, 8116, 10303, 295, 439, 3431, 13, 51418], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 32, "seek": 5500, "start": 76.08, "end": 79.16, "text": " Of course, they made a substantial investment in OpenAI", "tokens": [51418, 2720, 1164, 11, 436, 1027, 257, 16726, 6078, 294, 7238, 48698, 51572], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 33, "seek": 5500, "start": 79.16, "end": 82.16, "text": " and own a significant portion of that company.", "tokens": [51572, 293, 1065, 257, 4776, 8044, 295, 300, 2237, 13, 51722], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 34, "seek": 5500, "start": 82.16, "end": 84.36, "text": " So for them to release a new research paper", "tokens": [51722, 407, 337, 552, 281, 4374, 257, 777, 2132, 3035, 51832], "temperature": 0.0, "avg_logprob": -0.09506152893279816, "compression_ratio": 1.6820987654320987, "no_speech_prob": 0.00026117925881408155}, {"id": 35, "seek": 8436, "start": 84.4, "end": 85.67999999999999, "text": " illustrating a new technique", "tokens": [50366, 8490, 8754, 257, 777, 6532, 50430], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 36, "seek": 8436, "start": 85.67999999999999, "end": 87.76, "text": " to make open source smaller models", "tokens": [50430, 281, 652, 1269, 4009, 4356, 5245, 50534], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 37, "seek": 8436, "start": 87.76, "end": 90.6, "text": " extremely powerful is really fascinating.", "tokens": [50534, 4664, 4005, 307, 534, 10343, 13, 50676], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 38, "seek": 8436, "start": 90.6, "end": 93.68, "text": " Microsoft as a company has embraced open source", "tokens": [50676, 8116, 382, 257, 2237, 575, 28673, 1269, 4009, 50830], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 39, "seek": 8436, "start": 93.68, "end": 96.32, "text": " in the years since Satya Nadella took over", "tokens": [50830, 294, 264, 924, 1670, 5344, 3016, 426, 762, 3505, 1890, 670, 50962], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 40, "seek": 8436, "start": 96.32, "end": 97.48, "text": " and I'm all for it.", "tokens": [50962, 293, 286, 478, 439, 337, 309, 13, 51020], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 41, "seek": 8436, "start": 97.48, "end": 100.24, "text": " This paper is absolutely fascinating", "tokens": [51020, 639, 3035, 307, 3122, 10343, 51158], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 42, "seek": 8436, "start": 100.24, "end": 102.08, "text": " and it makes a ton of sense.", "tokens": [51158, 293, 309, 1669, 257, 2952, 295, 2020, 13, 51250], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 43, "seek": 8436, "start": 102.08, "end": 104.08, "text": " But before we get into this paper,", "tokens": [51250, 583, 949, 321, 483, 666, 341, 3035, 11, 51350], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 44, "seek": 8436, "start": 104.08, "end": 105.96000000000001, "text": " let's take a look at those previous documents", "tokens": [51350, 718, 311, 747, 257, 574, 412, 729, 3894, 8512, 51444], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 45, "seek": 8436, "start": 105.96000000000001, "end": 106.8, "text": " that I mentioned.", "tokens": [51444, 300, 286, 2835, 13, 51486], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 46, "seek": 8436, "start": 106.8, "end": 108.28, "text": " Now a little over a month ago,", "tokens": [51486, 823, 257, 707, 670, 257, 1618, 2057, 11, 51560], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 47, "seek": 8436, "start": 108.28, "end": 110.76, "text": " this internal memo from Google was released", "tokens": [51560, 341, 6920, 35900, 490, 3329, 390, 4736, 51684], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 48, "seek": 8436, "start": 110.76, "end": 111.92, "text": " called We Have No Mote.", "tokens": [51684, 1219, 492, 3560, 883, 376, 1370, 13, 51742], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 49, "seek": 8436, "start": 111.92, "end": 113.12, "text": " And the main point of this memo", "tokens": [51742, 400, 264, 2135, 935, 295, 341, 35900, 51802], "temperature": 0.0, "avg_logprob": -0.08822430043980695, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.001410224474966526}, {"id": 50, "seek": 11312, "start": 113.12, "end": 115.32000000000001, "text": " is that open source models are proliferating", "tokens": [50364, 307, 300, 1269, 4009, 5245, 366, 24398, 9361, 990, 50474], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 51, "seek": 11312, "start": 115.32000000000001, "end": 119.28, "text": " and iterating so quickly that the gap between models", "tokens": [50474, 293, 17138, 990, 370, 2661, 300, 264, 7417, 1296, 5245, 50672], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 52, "seek": 11312, "start": 119.28, "end": 124.08000000000001, "text": " like GPT-4 and Palm 2 are shrinking very quickly.", "tokens": [50672, 411, 26039, 51, 12, 19, 293, 32668, 568, 366, 41684, 588, 2661, 13, 50912], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 53, "seek": 11312, "start": 124.08000000000001, "end": 126.48, "text": " The fact that any developer can get their hands", "tokens": [50912, 440, 1186, 300, 604, 10754, 393, 483, 641, 2377, 51032], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 54, "seek": 11312, "start": 126.48, "end": 129.04, "text": " on these models and new techniques to train", "tokens": [51032, 322, 613, 5245, 293, 777, 7512, 281, 3847, 51160], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 55, "seek": 11312, "start": 129.04, "end": 131.08, "text": " and fine tune these models are coming out every day.", "tokens": [51160, 293, 2489, 10864, 613, 5245, 366, 1348, 484, 633, 786, 13, 51262], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 56, "seek": 11312, "start": 131.08, "end": 133.88, "text": " And we're seeing that from Laura to Q Laura", "tokens": [51262, 400, 321, 434, 2577, 300, 490, 13220, 281, 1249, 13220, 51402], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 57, "seek": 11312, "start": 133.88, "end": 136.08, "text": " to now having a ton of different options", "tokens": [51402, 281, 586, 1419, 257, 2952, 295, 819, 3956, 51512], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 58, "seek": 11312, "start": 136.08, "end": 138.56, "text": " of how to train and fine tune these models", "tokens": [51512, 295, 577, 281, 3847, 293, 2489, 10864, 613, 5245, 51636], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 59, "seek": 11312, "start": 138.56, "end": 139.8, "text": " in really efficient ways", "tokens": [51636, 294, 534, 7148, 2098, 51698], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 60, "seek": 11312, "start": 139.8, "end": 142.32, "text": " and run them on any consumer grade hardware.", "tokens": [51698, 293, 1190, 552, 322, 604, 9711, 7204, 8837, 13, 51824], "temperature": 0.0, "avg_logprob": -0.09847087244833669, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.0033761407248675823}, {"id": 61, "seek": 14232, "start": 142.32, "end": 144.64, "text": " And I agreed with a lot that was in this paper.", "tokens": [50364, 400, 286, 9166, 365, 257, 688, 300, 390, 294, 341, 3035, 13, 50480], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 62, "seek": 14232, "start": 144.64, "end": 146.2, "text": " Of course, a business mode", "tokens": [50480, 2720, 1164, 11, 257, 1606, 4391, 50558], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 63, "seek": 14232, "start": 146.2, "end": 148.6, "text": " is not just the technical limitations.", "tokens": [50558, 307, 406, 445, 264, 6191, 15705, 13, 50678], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 64, "seek": 14232, "start": 148.6, "end": 150.16, "text": " There's much more to it than that.", "tokens": [50678, 821, 311, 709, 544, 281, 309, 813, 300, 13, 50756], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 65, "seek": 14232, "start": 150.16, "end": 152.76, "text": " But a lot of the points made in this paper are very valid.", "tokens": [50756, 583, 257, 688, 295, 264, 2793, 1027, 294, 341, 3035, 366, 588, 7363, 13, 50886], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 66, "seek": 14232, "start": 152.76, "end": 155.28, "text": " And I've seen more innovation in the open source community", "tokens": [50886, 400, 286, 600, 1612, 544, 8504, 294, 264, 1269, 4009, 1768, 51012], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 67, "seek": 14232, "start": 155.28, "end": 156.32, "text": " over these last few weeks", "tokens": [51012, 670, 613, 1036, 1326, 3259, 51064], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 68, "seek": 14232, "start": 156.32, "end": 158.92, "text": " than I've seen on these proprietary large models.", "tokens": [51064, 813, 286, 600, 1612, 322, 613, 38992, 2416, 5245, 13, 51194], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 69, "seek": 14232, "start": 158.92, "end": 161.04, "text": " But then a research paper out of UC Berkeley", "tokens": [51194, 583, 550, 257, 2132, 3035, 484, 295, 14079, 23684, 51300], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 70, "seek": 14232, "start": 161.04, "end": 162.4, "text": " was dropped a couple of weeks ago", "tokens": [51300, 390, 8119, 257, 1916, 295, 3259, 2057, 51368], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 71, "seek": 14232, "start": 162.4, "end": 164.07999999999998, "text": " that really challenged the assertions", "tokens": [51368, 300, 534, 17737, 264, 19810, 626, 51452], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 72, "seek": 14232, "start": 164.07999999999998, "end": 165.95999999999998, "text": " of the We Have No Mote leak document.", "tokens": [51452, 295, 264, 492, 3560, 883, 376, 1370, 17143, 4166, 13, 51546], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 73, "seek": 14232, "start": 165.95999999999998, "end": 167.35999999999999, "text": " In this research paper,", "tokens": [51546, 682, 341, 2132, 3035, 11, 51616], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 74, "seek": 14232, "start": 167.35999999999999, "end": 170.68, "text": " the false promise of imitating proprietary LLMs,", "tokens": [51616, 264, 7908, 6228, 295, 566, 16350, 38992, 441, 43, 26386, 11, 51782], "temperature": 0.0, "avg_logprob": -0.07415401935577393, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.001284213736653328}, {"id": 75, "seek": 17068, "start": 170.68, "end": 172.4, "text": " they spell out that these open source models", "tokens": [50364, 436, 9827, 484, 300, 613, 1269, 4009, 5245, 50450], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 76, "seek": 17068, "start": 172.4, "end": 176.44, "text": " are simply just imitating the outputs of these larger models", "tokens": [50450, 366, 2935, 445, 566, 16350, 264, 23930, 295, 613, 4833, 5245, 50652], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 77, "seek": 17068, "start": 176.44, "end": 178.76000000000002, "text": " without actually understanding the logic", "tokens": [50652, 1553, 767, 3701, 264, 9952, 50768], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 78, "seek": 17068, "start": 178.76000000000002, "end": 180.56, "text": " to reach certain outputs.", "tokens": [50768, 281, 2524, 1629, 23930, 13, 50858], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 79, "seek": 17068, "start": 180.56, "end": 181.96, "text": " The gist of this paper", "tokens": [50858, 440, 290, 468, 295, 341, 3035, 50928], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 80, "seek": 17068, "start": 181.96, "end": 184.08, "text": " and what Orca looks to correct", "tokens": [50928, 293, 437, 1610, 496, 1542, 281, 3006, 51034], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 81, "seek": 17068, "start": 184.08, "end": 186.20000000000002, "text": " is that these open source models", "tokens": [51034, 307, 300, 613, 1269, 4009, 5245, 51140], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 82, "seek": 17068, "start": 186.20000000000002, "end": 189.32, "text": " are simply being trained on prompts and responses", "tokens": [51140, 366, 2935, 885, 8895, 322, 41095, 293, 13019, 51296], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 83, "seek": 17068, "start": 189.32, "end": 191.44, "text": " which is good for pattern matching.", "tokens": [51296, 597, 307, 665, 337, 5102, 14324, 13, 51402], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 84, "seek": 17068, "start": 191.44, "end": 193.88, "text": " So for example, if you're a student in college", "tokens": [51402, 407, 337, 1365, 11, 498, 291, 434, 257, 3107, 294, 3859, 51524], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 85, "seek": 17068, "start": 193.88, "end": 195.20000000000002, "text": " and you're taking a class,", "tokens": [51524, 293, 291, 434, 1940, 257, 1508, 11, 51590], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 86, "seek": 17068, "start": 195.20000000000002, "end": 198.04000000000002, "text": " you could probably do pretty well on a lot of tests", "tokens": [51590, 291, 727, 1391, 360, 1238, 731, 322, 257, 688, 295, 6921, 51732], "temperature": 0.0, "avg_logprob": -0.068271806684591, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.00023781493655405939}, {"id": 87, "seek": 19804, "start": 198.04, "end": 200.92, "text": " simply by pattern matching the question to an answer.", "tokens": [50364, 2935, 538, 5102, 14324, 264, 1168, 281, 364, 1867, 13, 50508], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 88, "seek": 19804, "start": 200.92, "end": 203.48, "text": " But that student is gonna have a lot of limitations.", "tokens": [50508, 583, 300, 3107, 307, 799, 362, 257, 688, 295, 15705, 13, 50636], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 89, "seek": 19804, "start": 203.48, "end": 205.04, "text": " If one of the questions varies", "tokens": [50636, 759, 472, 295, 264, 1651, 21716, 50714], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 90, "seek": 19804, "start": 205.04, "end": 206.92, "text": " from their pattern matching ability,", "tokens": [50714, 490, 641, 5102, 14324, 3485, 11, 50808], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 91, "seek": 19804, "start": 206.92, "end": 208.51999999999998, "text": " by even just a little bit,", "tokens": [50808, 538, 754, 445, 257, 707, 857, 11, 50888], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 92, "seek": 19804, "start": 208.51999999999998, "end": 210.68, "text": " their ability to reason and figure out", "tokens": [50888, 641, 3485, 281, 1778, 293, 2573, 484, 50996], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 93, "seek": 19804, "start": 210.68, "end": 213.32, "text": " what the answer might be becomes highly limited.", "tokens": [50996, 437, 264, 1867, 1062, 312, 3643, 5405, 5567, 13, 51128], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 94, "seek": 19804, "start": 213.32, "end": 216.0, "text": " Whereas the student who fundamentally", "tokens": [51128, 13813, 264, 3107, 567, 17879, 51262], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 95, "seek": 19804, "start": 216.0, "end": 217.95999999999998, "text": " and deeply understands a topic", "tokens": [51262, 293, 8760, 15146, 257, 4829, 51360], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 96, "seek": 19804, "start": 217.95999999999998, "end": 221.2, "text": " won't be thrown off by any variation of the question.", "tokens": [51360, 1582, 380, 312, 11732, 766, 538, 604, 12990, 295, 264, 1168, 13, 51522], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 97, "seek": 19804, "start": 221.2, "end": 225.12, "text": " They'll be able to reason and step by step get to the answer", "tokens": [51522, 814, 603, 312, 1075, 281, 1778, 293, 1823, 538, 1823, 483, 281, 264, 1867, 51718], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 98, "seek": 19804, "start": 225.12, "end": 227.32, "text": " because they do truly understand the topic.", "tokens": [51718, 570, 436, 360, 4908, 1223, 264, 4829, 13, 51828], "temperature": 0.0, "avg_logprob": -0.09653110352773515, "compression_ratio": 1.8398576512455516, "no_speech_prob": 0.005554084200412035}, {"id": 99, "seek": 22732, "start": 227.32, "end": 228.76, "text": " And that's really the difference", "tokens": [50364, 400, 300, 311, 534, 264, 2649, 50436], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 100, "seek": 22732, "start": 228.76, "end": 231.35999999999999, "text": " between these large foundational models", "tokens": [50436, 1296, 613, 2416, 32195, 5245, 50566], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 101, "seek": 22732, "start": 231.35999999999999, "end": 234.16, "text": " and the open source imitations of them", "tokens": [50566, 293, 264, 1269, 4009, 566, 31265, 295, 552, 50706], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 102, "seek": 22732, "start": 234.16, "end": 235.35999999999999, "text": " as per this paper.", "tokens": [50706, 382, 680, 341, 3035, 13, 50766], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 103, "seek": 22732, "start": 235.35999999999999, "end": 237.12, "text": " And that brings us to Orca.", "tokens": [50766, 400, 300, 5607, 505, 281, 1610, 496, 13, 50854], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 104, "seek": 22732, "start": 237.12, "end": 239.79999999999998, "text": " Orca challenges the idea that open source models", "tokens": [50854, 1610, 496, 4759, 264, 1558, 300, 1269, 4009, 5245, 50988], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 105, "seek": 22732, "start": 239.79999999999998, "end": 242.28, "text": " can only really imitate answers", "tokens": [50988, 393, 787, 534, 35556, 6338, 51112], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 106, "seek": 22732, "start": 242.28, "end": 244.51999999999998, "text": " and will get thrown off by any variation", "tokens": [51112, 293, 486, 483, 11732, 766, 538, 604, 12990, 51224], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 107, "seek": 22732, "start": 244.51999999999998, "end": 246.04, "text": " in the prompts themselves.", "tokens": [51224, 294, 264, 41095, 2969, 13, 51300], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 108, "seek": 22732, "start": 246.04, "end": 248.95999999999998, "text": " And the way they do it seems very obvious in hindsight.", "tokens": [51300, 400, 264, 636, 436, 360, 309, 2544, 588, 6322, 294, 44357, 13, 51446], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 109, "seek": 22732, "start": 248.95999999999998, "end": 250.79999999999998, "text": " Before we get into the details,", "tokens": [51446, 4546, 321, 483, 666, 264, 4365, 11, 51538], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 110, "seek": 22732, "start": 250.79999999999998, "end": 255.04, "text": " Orca outperforms every other open source model", "tokens": [51538, 1610, 496, 484, 26765, 82, 633, 661, 1269, 4009, 2316, 51750], "temperature": 0.0, "avg_logprob": -0.08840939036586828, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.000379960925783962}, {"id": 111, "seek": 25504, "start": 255.04, "end": 259.28, "text": " and even outperforms ChatGPT, which is GPT 3.5,", "tokens": [50364, 293, 754, 484, 26765, 82, 27503, 38, 47, 51, 11, 597, 307, 26039, 51, 805, 13, 20, 11, 50576], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 112, "seek": 25504, "start": 259.28, "end": 261.2, "text": " in a lot of different benchmarks.", "tokens": [50576, 294, 257, 688, 295, 819, 43751, 13, 50672], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 113, "seek": 25504, "start": 261.2, "end": 264.36, "text": " Now, of course, it still lags behind GPT4,", "tokens": [50672, 823, 11, 295, 1164, 11, 309, 920, 8953, 82, 2261, 26039, 51, 19, 11, 50830], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 114, "seek": 25504, "start": 264.36, "end": 266.48, "text": " but the gap continues to close.", "tokens": [50830, 457, 264, 7417, 6515, 281, 1998, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 115, "seek": 25504, "start": 266.48, "end": 268.08, "text": " So let's take a look at this paper now.", "tokens": [50936, 407, 718, 311, 747, 257, 574, 412, 341, 3035, 586, 13, 51016], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 116, "seek": 25504, "start": 268.08, "end": 269.52, "text": " They start off the abstract", "tokens": [51016, 814, 722, 766, 264, 12649, 51088], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 117, "seek": 25504, "start": 269.52, "end": 272.34, "text": " by addressing this imitation concept.", "tokens": [51088, 538, 14329, 341, 47624, 3410, 13, 51229], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 118, "seek": 25504, "start": 272.34, "end": 275.32, "text": " Recent research has focused on enhancing the capability", "tokens": [51229, 17553, 2132, 575, 5178, 322, 36579, 264, 13759, 51378], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 119, "seek": 25504, "start": 275.32, "end": 277.52, "text": " of smaller models through imitation learning,", "tokens": [51378, 295, 4356, 5245, 807, 47624, 2539, 11, 51488], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 120, "seek": 25504, "start": 277.52, "end": 279.28, "text": " drawing on the outputs generated", "tokens": [51488, 6316, 322, 264, 23930, 10833, 51576], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 121, "seek": 25504, "start": 279.28, "end": 280.92, "text": " by large foundational models.", "tokens": [51576, 538, 2416, 32195, 5245, 13, 51658], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 122, "seek": 25504, "start": 280.92, "end": 285.0, "text": " Again, LFMs are referring to ChatGPT and GPT4.", "tokens": [51658, 3764, 11, 441, 37, 26386, 366, 13761, 281, 27503, 38, 47, 51, 293, 26039, 51, 19, 13, 51862], "temperature": 0.0, "avg_logprob": -0.10005893426782944, "compression_ratio": 1.5695364238410596, "no_speech_prob": 0.0005883803823962808}, {"id": 123, "seek": 28500, "start": 285.0, "end": 286.88, "text": " And they start to outline the limitations", "tokens": [50364, 400, 436, 722, 281, 16387, 264, 15705, 50458], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 124, "seek": 28500, "start": 286.88, "end": 288.68, "text": " of these imitation techniques.", "tokens": [50458, 295, 613, 47624, 7512, 13, 50548], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 125, "seek": 28500, "start": 288.68, "end": 291.64, "text": " Some that they point out are limited imitation signals", "tokens": [50548, 2188, 300, 436, 935, 484, 366, 5567, 47624, 12354, 50696], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 126, "seek": 28500, "start": 291.64, "end": 293.88, "text": " from shallow LFM outputs,", "tokens": [50696, 490, 20488, 441, 37, 44, 23930, 11, 50808], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 127, "seek": 28500, "start": 293.88, "end": 296.28, "text": " small scale homogenous training data,", "tokens": [50808, 1359, 4373, 3655, 45519, 3097, 1412, 11, 50928], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 128, "seek": 28500, "start": 296.28, "end": 300.36, "text": " and most notably a lack of rigorous evaluation", "tokens": [50928, 293, 881, 31357, 257, 5011, 295, 29882, 13344, 51132], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 129, "seek": 28500, "start": 300.36, "end": 303.28, "text": " resulting in overestimating the small models capability", "tokens": [51132, 16505, 294, 670, 377, 332, 990, 264, 1359, 5245, 13759, 51278], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 130, "seek": 28500, "start": 303.28, "end": 305.8, "text": " as they tend to learn to imitate the style", "tokens": [51278, 382, 436, 3928, 281, 1466, 281, 35556, 264, 3758, 51404], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 131, "seek": 28500, "start": 305.8, "end": 308.76, "text": " but not the reasoning process of LFMs.", "tokens": [51404, 457, 406, 264, 21577, 1399, 295, 441, 37, 26386, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 132, "seek": 28500, "start": 308.76, "end": 311.64, "text": " That is really the crux of this paper.", "tokens": [51552, 663, 307, 534, 264, 5140, 87, 295, 341, 3035, 13, 51696], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 133, "seek": 28500, "start": 311.64, "end": 314.52, "text": " How do we start getting these open source models", "tokens": [51696, 1012, 360, 321, 722, 1242, 613, 1269, 4009, 5245, 51840], "temperature": 0.0, "avg_logprob": -0.09155606808869735, "compression_ratio": 1.7121771217712176, "no_speech_prob": 0.0002780143986456096}, {"id": 134, "seek": 31452, "start": 314.52, "end": 317.59999999999997, "text": " to not just mimic the question answer pairs,", "tokens": [50364, 281, 406, 445, 31075, 264, 1168, 1867, 15494, 11, 50518], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 135, "seek": 31452, "start": 317.59999999999997, "end": 320.03999999999996, "text": " but actually understand how they get", "tokens": [50518, 457, 767, 1223, 577, 436, 483, 50640], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 136, "seek": 31452, "start": 320.03999999999996, "end": 322.41999999999996, "text": " from a question to an answer.", "tokens": [50640, 490, 257, 1168, 281, 364, 1867, 13, 50759], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 137, "seek": 31452, "start": 322.41999999999996, "end": 325.56, "text": " And only with that is true intelligence created.", "tokens": [50759, 400, 787, 365, 300, 307, 2074, 7599, 2942, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 138, "seek": 31452, "start": 325.56, "end": 328.0, "text": " To address these challenges, we develop ORCA,", "tokens": [50916, 1407, 2985, 613, 4759, 11, 321, 1499, 19654, 15515, 11, 51038], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 139, "seek": 31452, "start": 328.0, "end": 329.68, "text": " a 13 billion parameter model", "tokens": [51038, 257, 3705, 5218, 13075, 2316, 51122], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 140, "seek": 31452, "start": 329.68, "end": 332.79999999999995, "text": " that learns to imitate the reasoning process of LFMs.", "tokens": [51122, 300, 27152, 281, 35556, 264, 21577, 1399, 295, 441, 37, 26386, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 141, "seek": 31452, "start": 332.79999999999995, "end": 334.56, "text": " Let's pause there for a second.", "tokens": [51278, 961, 311, 10465, 456, 337, 257, 1150, 13, 51366], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 142, "seek": 31452, "start": 334.56, "end": 339.08, "text": " This model, the ORCA model is only 13 billion parameters,", "tokens": [51366, 639, 2316, 11, 264, 19654, 15515, 2316, 307, 787, 3705, 5218, 9834, 11, 51592], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 143, "seek": 31452, "start": 339.08, "end": 343.12, "text": " which means it can run on pretty much any modern hardware.", "tokens": [51592, 597, 1355, 309, 393, 1190, 322, 1238, 709, 604, 4363, 8837, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1121153367304169, "compression_ratio": 1.6162361623616237, "no_speech_prob": 0.00045827520079910755}, {"id": 144, "seek": 34312, "start": 343.12, "end": 344.52, "text": " Whereas some of the other models", "tokens": [50364, 13813, 512, 295, 264, 661, 5245, 50434], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 145, "seek": 34312, "start": 344.52, "end": 345.84000000000003, "text": " that I've been reviewing recently,", "tokens": [50434, 300, 286, 600, 668, 19576, 3938, 11, 50500], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 146, "seek": 34312, "start": 345.84000000000003, "end": 347.08, "text": " like the Guinaco model,", "tokens": [50500, 411, 264, 2694, 259, 11428, 2316, 11, 50562], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 147, "seek": 34312, "start": 347.08, "end": 349.6, "text": " require me to rent out a cloud GPU,", "tokens": [50562, 3651, 385, 281, 6214, 484, 257, 4588, 18407, 11, 50688], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 148, "seek": 34312, "start": 349.6, "end": 353.08, "text": " like an A6000 that has 48 gigabytes of VRAM,", "tokens": [50688, 411, 364, 316, 21, 1360, 300, 575, 11174, 42741, 295, 13722, 2865, 11, 50862], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 149, "seek": 34312, "start": 353.08, "end": 355.82, "text": " because it's so large, 65 billion parameters.", "tokens": [50862, 570, 309, 311, 370, 2416, 11, 11624, 5218, 9834, 13, 50999], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 150, "seek": 34312, "start": 355.82, "end": 357.56, "text": " And this performs better than that.", "tokens": [50999, 400, 341, 26213, 1101, 813, 300, 13, 51086], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 151, "seek": 34312, "start": 357.56, "end": 359.06, "text": " Now here's the key to the paper.", "tokens": [51086, 823, 510, 311, 264, 2141, 281, 264, 3035, 13, 51161], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 152, "seek": 34312, "start": 359.06, "end": 360.36, "text": " Here's the key technique.", "tokens": [51161, 1692, 311, 264, 2141, 6532, 13, 51226], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 153, "seek": 34312, "start": 360.36, "end": 363.72, "text": " ORCA learns from rich signals from GPT-4,", "tokens": [51226, 19654, 15515, 27152, 490, 4593, 12354, 490, 26039, 51, 12, 19, 11, 51394], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 154, "seek": 34312, "start": 363.72, "end": 365.56, "text": " including explanation traces,", "tokens": [51394, 3009, 10835, 26076, 11, 51486], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 155, "seek": 34312, "start": 365.56, "end": 367.68, "text": " step-by-step thought processes,", "tokens": [51486, 1823, 12, 2322, 12, 16792, 1194, 7555, 11, 51592], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 156, "seek": 34312, "start": 367.68, "end": 369.64, "text": " and other complex instructions", "tokens": [51592, 293, 661, 3997, 9415, 51690], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 157, "seek": 34312, "start": 369.64, "end": 372.8, "text": " guided by teacher assistance from chat GPT.", "tokens": [51690, 19663, 538, 5027, 9683, 490, 5081, 26039, 51, 13, 51848], "temperature": 0.0, "avg_logprob": -0.13845466721987892, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0010986364213749766}, {"id": 158, "seek": 37280, "start": 372.8, "end": 374.72, "text": " Now I'll explain what teacher assistance is", "tokens": [50364, 823, 286, 603, 2903, 437, 5027, 9683, 307, 50460], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 159, "seek": 37280, "start": 374.72, "end": 377.04, "text": " in a little bit, but looking at this sentence,", "tokens": [50460, 294, 257, 707, 857, 11, 457, 1237, 412, 341, 8174, 11, 50576], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 160, "seek": 37280, "start": 377.04, "end": 378.52000000000004, "text": " what it's really saying is,", "tokens": [50576, 437, 309, 311, 534, 1566, 307, 11, 50650], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 161, "seek": 37280, "start": 378.52000000000004, "end": 381.12, "text": " rather than learning from the prompt and response pairs,", "tokens": [50650, 2831, 813, 2539, 490, 264, 12391, 293, 4134, 15494, 11, 50780], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 162, "seek": 37280, "start": 381.12, "end": 383.8, "text": " we're going to ask these large foundational models", "tokens": [50780, 321, 434, 516, 281, 1029, 613, 2416, 32195, 5245, 50914], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 163, "seek": 37280, "start": 383.8, "end": 387.16, "text": " to explain their reasoning step-by-step,", "tokens": [50914, 281, 2903, 641, 21577, 1823, 12, 2322, 12, 16792, 11, 51082], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 164, "seek": 37280, "start": 387.16, "end": 389.28000000000003, "text": " and the smaller open source models", "tokens": [51082, 293, 264, 4356, 1269, 4009, 5245, 51188], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 165, "seek": 37280, "start": 389.28000000000003, "end": 390.56, "text": " will learn from that.", "tokens": [51188, 486, 1466, 490, 300, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 166, "seek": 37280, "start": 390.56, "end": 392.28000000000003, "text": " Truly fascinating.", "tokens": [51252, 43548, 10343, 13, 51338], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 167, "seek": 37280, "start": 392.28000000000003, "end": 394.16, "text": " Now I wanna briefly touch on this guided", "tokens": [51338, 823, 286, 1948, 10515, 2557, 322, 341, 19663, 51432], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 168, "seek": 37280, "start": 394.16, "end": 396.84000000000003, "text": " by teacher assistance from chat GPT.", "tokens": [51432, 538, 5027, 9683, 490, 5081, 26039, 51, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 169, "seek": 37280, "start": 396.84000000000003, "end": 400.0, "text": " They have a two-tier teaching process.", "tokens": [51566, 814, 362, 257, 732, 12, 25402, 4571, 1399, 13, 51724], "temperature": 0.0, "avg_logprob": -0.1265803337097168, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.00019109263666905463}, {"id": 170, "seek": 40000, "start": 400.0, "end": 403.68, "text": " One, they take chat GPT, which is GPT-3.5,", "tokens": [50364, 1485, 11, 436, 747, 5081, 26039, 51, 11, 597, 307, 26039, 51, 12, 18, 13, 20, 11, 50548], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 171, "seek": 40000, "start": 403.68, "end": 405.48, "text": " and they have a large number of examples", "tokens": [50548, 293, 436, 362, 257, 2416, 1230, 295, 5110, 50638], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 172, "seek": 40000, "start": 405.48, "end": 406.88, "text": " to learn from, five million.", "tokens": [50638, 281, 1466, 490, 11, 1732, 2459, 13, 50708], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 173, "seek": 40000, "start": 406.88, "end": 408.68, "text": " Then they take those five million,", "tokens": [50708, 1396, 436, 747, 729, 1732, 2459, 11, 50798], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 174, "seek": 40000, "start": 408.68, "end": 412.26, "text": " boil it down to the most important one million examples,", "tokens": [50798, 13329, 309, 760, 281, 264, 881, 1021, 472, 2459, 5110, 11, 50977], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 175, "seek": 40000, "start": 412.26, "end": 415.76, "text": " and then use GPT-4 to continue to train", "tokens": [50977, 293, 550, 764, 26039, 51, 12, 19, 281, 2354, 281, 3847, 51152], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 176, "seek": 40000, "start": 415.76, "end": 417.52, "text": " on more complex examples.", "tokens": [51152, 322, 544, 3997, 5110, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 177, "seek": 40000, "start": 417.52, "end": 419.16, "text": " So how does it actually perform?", "tokens": [51240, 407, 577, 775, 309, 767, 2042, 30, 51322], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 178, "seek": 40000, "start": 419.16, "end": 421.48, "text": " ORCA surpasses conventional state-of-the-art", "tokens": [51322, 19654, 15515, 27650, 279, 16011, 1785, 12, 2670, 12, 3322, 12, 446, 51438], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 179, "seek": 40000, "start": 421.48, "end": 424.64, "text": " instruction-tuned models, such as Vecunia 13B", "tokens": [51438, 10951, 12, 83, 43703, 5245, 11, 1270, 382, 691, 3045, 409, 654, 3705, 33, 51596], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 180, "seek": 40000, "start": 424.64, "end": 429.0, "text": " by more than 100% in complex zero-shot reasoning benchmarks", "tokens": [51596, 538, 544, 813, 2319, 4, 294, 3997, 4018, 12, 18402, 21577, 43751, 51814], "temperature": 0.0, "avg_logprob": -0.1296921765362775, "compression_ratio": 1.5985915492957747, "no_speech_prob": 0.0014102343702688813}, {"id": 181, "seek": 42900, "start": 429.0, "end": 433.04, "text": " like Big Bench Hard and 42% on AGI Eval.", "tokens": [50364, 411, 5429, 3964, 339, 11817, 293, 14034, 4, 322, 316, 26252, 462, 3337, 13, 50566], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 182, "seek": 42900, "start": 433.04, "end": 436.14, "text": " Big Bench Hard and AGI Eval are just sets of tests", "tokens": [50566, 5429, 3964, 339, 11817, 293, 316, 26252, 462, 3337, 366, 445, 6352, 295, 6921, 50721], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 183, "seek": 42900, "start": 436.14, "end": 437.92, "text": " that they give to these large language models", "tokens": [50721, 300, 436, 976, 281, 613, 2416, 2856, 5245, 50810], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 184, "seek": 42900, "start": 437.92, "end": 439.36, "text": " to test their performance.", "tokens": [50810, 281, 1500, 641, 3389, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 185, "seek": 42900, "start": 439.36, "end": 443.8, "text": " ORCA reaches parity with chat GPT on the BBH benchmark", "tokens": [50882, 19654, 15515, 14235, 44747, 365, 5081, 26039, 51, 322, 264, 19168, 39, 18927, 51104], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 186, "seek": 42900, "start": 443.8, "end": 445.56, "text": " and shows competitive performance", "tokens": [51104, 293, 3110, 10043, 3389, 51192], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 187, "seek": 42900, "start": 445.56, "end": 448.12, "text": " in professional and academic examinations", "tokens": [51192, 294, 4843, 293, 7778, 1139, 10325, 51320], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 188, "seek": 42900, "start": 448.12, "end": 450.4, "text": " like SAT, LSAT, GRE, and GMAT,", "tokens": [51320, 411, 31536, 11, 36657, 2218, 11, 20830, 11, 293, 16609, 2218, 11, 51434], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 189, "seek": 42900, "start": 450.4, "end": 453.36, "text": " both in zero-shot setting without chain of thought", "tokens": [51434, 1293, 294, 4018, 12, 18402, 3287, 1553, 5021, 295, 1194, 51582], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 190, "seek": 42900, "start": 453.36, "end": 455.1, "text": " while trailing behind GPT-4.", "tokens": [51582, 1339, 944, 4883, 2261, 26039, 51, 12, 19, 13, 51669], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 191, "seek": 42900, "start": 455.1, "end": 458.08, "text": " And again, this last sentence is everything.", "tokens": [51669, 400, 797, 11, 341, 1036, 8174, 307, 1203, 13, 51818], "temperature": 0.0, "avg_logprob": -0.1166161373257637, "compression_ratio": 1.5659722222222223, "no_speech_prob": 0.001754460041411221}, {"id": 192, "seek": 45808, "start": 458.08, "end": 460.2, "text": " Our research indicates that learning", "tokens": [50364, 2621, 2132, 16203, 300, 2539, 50470], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 193, "seek": 45808, "start": 460.2, "end": 462.4, "text": " from step-by-step explanations,", "tokens": [50470, 490, 1823, 12, 2322, 12, 16792, 28708, 11, 50580], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 194, "seek": 45808, "start": 462.4, "end": 464.15999999999997, "text": " whether these are generated by humans", "tokens": [50580, 1968, 613, 366, 10833, 538, 6255, 50668], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 195, "seek": 45808, "start": 464.15999999999997, "end": 465.56, "text": " or more advanced AI models,", "tokens": [50668, 420, 544, 7339, 7318, 5245, 11, 50738], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 196, "seek": 45808, "start": 465.56, "end": 468.44, "text": " is a promising direction to improve model capabilities", "tokens": [50738, 307, 257, 20257, 3513, 281, 3470, 2316, 10862, 50882], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 197, "seek": 45808, "start": 468.44, "end": 469.28, "text": " and skills.", "tokens": [50882, 293, 3942, 13, 50924], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 198, "seek": 45808, "start": 469.28, "end": 470.71999999999997, "text": " And just like humans,", "tokens": [50924, 400, 445, 411, 6255, 11, 50996], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 199, "seek": 45808, "start": 470.71999999999997, "end": 473.84, "text": " large language models understanding how something works", "tokens": [50996, 2416, 2856, 5245, 3701, 577, 746, 1985, 51152], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 200, "seek": 45808, "start": 473.84, "end": 475.76, "text": " is much more effective", "tokens": [51152, 307, 709, 544, 4942, 51248], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 201, "seek": 45808, "start": 475.76, "end": 479.0, "text": " than just being able to pattern match questions and answers.", "tokens": [51248, 813, 445, 885, 1075, 281, 5102, 2995, 1651, 293, 6338, 13, 51410], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 202, "seek": 45808, "start": 479.0, "end": 481.64, "text": " So large language models are typically tuned", "tokens": [51410, 407, 2416, 2856, 5245, 366, 5850, 10870, 51542], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 203, "seek": 45808, "start": 481.64, "end": 483.65999999999997, "text": " by something called instruction tuning.", "tokens": [51542, 538, 746, 1219, 10951, 15164, 13, 51643], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 204, "seek": 45808, "start": 483.65999999999997, "end": 486.59999999999997, "text": " You have a set of prompts and you have a set of responses", "tokens": [51643, 509, 362, 257, 992, 295, 41095, 293, 291, 362, 257, 992, 295, 13019, 51790], "temperature": 0.0, "avg_logprob": -0.12031620384281518, "compression_ratio": 1.7210884353741496, "no_speech_prob": 0.015420014038681984}, {"id": 205, "seek": 48660, "start": 486.6, "end": 489.6, "text": " and those pairs are passed to the open source model", "tokens": [50364, 293, 729, 15494, 366, 4678, 281, 264, 1269, 4009, 2316, 50514], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 206, "seek": 48660, "start": 489.6, "end": 490.68, "text": " and it learns from that.", "tokens": [50514, 293, 309, 27152, 490, 300, 13, 50568], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 207, "seek": 48660, "start": 490.68, "end": 493.96000000000004, "text": " This technique is called explanation tuning", "tokens": [50568, 639, 6532, 307, 1219, 10835, 15164, 50732], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 208, "seek": 48660, "start": 493.96000000000004, "end": 496.16, "text": " where it's not just the prompt and the answer", "tokens": [50732, 689, 309, 311, 406, 445, 264, 12391, 293, 264, 1867, 50842], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 209, "seek": 48660, "start": 496.16, "end": 498.92, "text": " but an explanation of the reasoning and the logic", "tokens": [50842, 457, 364, 10835, 295, 264, 21577, 293, 264, 9952, 50980], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 210, "seek": 48660, "start": 498.92, "end": 502.12, "text": " for how chat GPT and GPT-4 arrived at an answer.", "tokens": [50980, 337, 577, 5081, 26039, 51, 293, 26039, 51, 12, 19, 6678, 412, 364, 1867, 13, 51140], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 211, "seek": 48660, "start": 502.12, "end": 505.88, "text": " And so we can see here when evaluated by GPT-4", "tokens": [51140, 400, 370, 321, 393, 536, 510, 562, 25509, 538, 26039, 51, 12, 19, 51328], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 212, "seek": 48660, "start": 505.88, "end": 507.6, "text": " and that's called auto-evaluation,", "tokens": [51328, 293, 300, 311, 1219, 8399, 12, 68, 46504, 11, 51414], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 213, "seek": 48660, "start": 507.6, "end": 510.82000000000005, "text": " ORCA 13B actually beats chat GPT.", "tokens": [51414, 19654, 15515, 3705, 33, 767, 16447, 5081, 26039, 51, 13, 51575], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 214, "seek": 48660, "start": 510.82000000000005, "end": 513.14, "text": " It beats Bard and it certainly beats", "tokens": [51575, 467, 16447, 26841, 293, 309, 3297, 16447, 51691], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 215, "seek": 48660, "start": 513.14, "end": 515.2, "text": " the open source models based on Lama.", "tokens": [51691, 264, 1269, 4009, 5245, 2361, 322, 441, 2404, 13, 51794], "temperature": 0.0, "avg_logprob": -0.11939682811498642, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0011334946611896157}, {"id": 216, "seek": 51520, "start": 515.2, "end": 518.6400000000001, "text": " And then for zero shot problems on academic exams,", "tokens": [50364, 400, 550, 337, 4018, 3347, 2740, 322, 7778, 20514, 11, 50536], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 217, "seek": 51520, "start": 518.6400000000001, "end": 521.4000000000001, "text": " chat GPT definitely performs better", "tokens": [50536, 5081, 26039, 51, 2138, 26213, 1101, 50674], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 218, "seek": 51520, "start": 521.4000000000001, "end": 525.6, "text": " but ORCA 13B is really closing the gap in performance", "tokens": [50674, 457, 19654, 15515, 3705, 33, 307, 534, 10377, 264, 7417, 294, 3389, 50884], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 219, "seek": 51520, "start": 525.6, "end": 528.84, "text": " and performs much better than Virginia 13B.", "tokens": [50884, 293, 26213, 709, 1101, 813, 10956, 3705, 33, 13, 51046], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 220, "seek": 51520, "start": 528.84, "end": 531.48, "text": " And for complex zero shot reasoning tasks", "tokens": [51046, 400, 337, 3997, 4018, 3347, 21577, 9608, 51178], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 221, "seek": 51520, "start": 531.48, "end": 535.12, "text": " and big bench hard, ORCA achieves parity with chat GPT.", "tokens": [51178, 293, 955, 10638, 1152, 11, 19654, 15515, 3538, 977, 44747, 365, 5081, 26039, 51, 13, 51360], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 222, "seek": 51520, "start": 535.12, "end": 536.08, "text": " And here again,", "tokens": [51360, 400, 510, 797, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 223, "seek": 51520, "start": 536.08, "end": 538.76, "text": " they specifically call out that imitation paper.", "tokens": [51408, 436, 4682, 818, 484, 300, 47624, 3035, 13, 51542], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 224, "seek": 51520, "start": 538.76, "end": 541.8000000000001, "text": " Authors assert that model imitation is a false promise", "tokens": [51542, 40231, 830, 19810, 300, 2316, 47624, 307, 257, 7908, 6228, 51694], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 225, "seek": 51520, "start": 541.8000000000001, "end": 544.96, "text": " since broadly matching chat GPT using purely imitation", "tokens": [51694, 1670, 19511, 14324, 5081, 26039, 51, 1228, 17491, 47624, 51852], "temperature": 0.0, "avg_logprob": -0.12372472456523351, "compression_ratio": 1.6801470588235294, "no_speech_prob": 0.0010986635461449623}, {"id": 226, "seek": 54496, "start": 544.96, "end": 547.32, "text": " would require one, a concerted effort", "tokens": [50364, 576, 3651, 472, 11, 257, 8543, 292, 4630, 50482], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 227, "seek": 54496, "start": 547.32, "end": 549.72, "text": " to collect enormous imitation data sets", "tokens": [50482, 281, 2500, 11322, 47624, 1412, 6352, 50602], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 228, "seek": 54496, "start": 549.72, "end": 552.72, "text": " and far more diverse and higher quality imitation data", "tokens": [50602, 293, 1400, 544, 9521, 293, 2946, 3125, 47624, 1412, 50752], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 229, "seek": 54496, "start": 552.72, "end": 554.14, "text": " than is currently available.", "tokens": [50752, 813, 307, 4362, 2435, 13, 50823], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 230, "seek": 54496, "start": 554.14, "end": 556.8000000000001, "text": " So one of the biggest problems is these open source models", "tokens": [50823, 407, 472, 295, 264, 3880, 2740, 307, 613, 1269, 4009, 5245, 50956], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 231, "seek": 54496, "start": 556.8000000000001, "end": 559.6800000000001, "text": " can't get enough data to use the imitation technique", "tokens": [50956, 393, 380, 483, 1547, 1412, 281, 764, 264, 47624, 6532, 51100], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 232, "seek": 54496, "start": 559.6800000000001, "end": 560.84, "text": " and perform at the same rate", "tokens": [51100, 293, 2042, 412, 264, 912, 3314, 51158], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 233, "seek": 54496, "start": 560.84, "end": 562.8000000000001, "text": " as these large foundational models.", "tokens": [51158, 382, 613, 2416, 32195, 5245, 13, 51256], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 234, "seek": 54496, "start": 562.8000000000001, "end": 564.2800000000001, "text": " Contrary to this assertion,", "tokens": [51256, 4839, 81, 822, 281, 341, 19810, 313, 11, 51330], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 235, "seek": 54496, "start": 564.2800000000001, "end": 567.0, "text": " we demonstrate that both conditions one and two", "tokens": [51330, 321, 11698, 300, 1293, 4487, 472, 293, 732, 51466], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 236, "seek": 54496, "start": 567.0, "end": 569.32, "text": " are attainable and that it is possible", "tokens": [51466, 366, 23766, 712, 293, 300, 309, 307, 1944, 51582], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 237, "seek": 54496, "start": 569.32, "end": 571.5600000000001, "text": " to reduce the gap with proprietary LLMs", "tokens": [51582, 281, 5407, 264, 7417, 365, 38992, 441, 43, 26386, 51694], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 238, "seek": 54496, "start": 571.5600000000001, "end": 573.5600000000001, "text": " on multiple zero shot benchmarks", "tokens": [51694, 322, 3866, 4018, 3347, 43751, 51794], "temperature": 0.0, "avg_logprob": -0.1077656483087014, "compression_ratio": 1.7077922077922079, "no_speech_prob": 0.0006070270319469273}, {"id": 239, "seek": 57356, "start": 573.56, "end": 575.28, "text": " that require sophisticated reasoning.", "tokens": [50364, 300, 3651, 16950, 21577, 13, 50450], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 240, "seek": 57356, "start": 575.28, "end": 578.1199999999999, "text": " And here they touch on what the existing open source models", "tokens": [50450, 400, 510, 436, 2557, 322, 437, 264, 6741, 1269, 4009, 5245, 50592], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 241, "seek": 57356, "start": 578.1199999999999, "end": 580.04, "text": " are doing currently to train themselves.", "tokens": [50592, 366, 884, 4362, 281, 3847, 2969, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 242, "seek": 57356, "start": 580.04, "end": 583.1999999999999, "text": " Both Alpaca and Wizard LM employ a variant", "tokens": [50688, 6767, 967, 79, 6628, 293, 37449, 46529, 3188, 257, 17501, 50846], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 243, "seek": 57356, "start": 583.1999999999999, "end": 584.16, "text": " of self-instructs.", "tokens": [50846, 295, 2698, 12, 13911, 1757, 82, 13, 50894], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 244, "seek": 57356, "start": 584.16, "end": 585.4399999999999, "text": " So that's what we've been talking about.", "tokens": [50894, 407, 300, 311, 437, 321, 600, 668, 1417, 466, 13, 50958], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 245, "seek": 57356, "start": 585.4399999999999, "end": 588.9599999999999, "text": " Wizard LM introduces the concept of Eval Instruct", "tokens": [50958, 37449, 46529, 31472, 264, 3410, 295, 462, 3337, 2730, 1757, 51134], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 246, "seek": 57356, "start": 588.9599999999999, "end": 591.8, "text": " which gradually rewrites the initial set of instructions", "tokens": [51134, 597, 13145, 319, 86, 30931, 264, 5883, 992, 295, 9415, 51276], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 247, "seek": 57356, "start": 591.8, "end": 593.4, "text": " into more complex versions", "tokens": [51276, 666, 544, 3997, 9606, 51356], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 248, "seek": 57356, "start": 593.4, "end": 595.0, "text": " attempting to overcome some of the methods", "tokens": [51356, 22001, 281, 10473, 512, 295, 264, 7150, 51436], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 249, "seek": 57356, "start": 595.0, "end": 596.2399999999999, "text": " inherent shortcomings.", "tokens": [51436, 26387, 2099, 49886, 13, 51498], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 250, "seek": 57356, "start": 596.2399999999999, "end": 597.92, "text": " But with Vakunya and Kuala,", "tokens": [51498, 583, 365, 691, 514, 409, 3016, 293, 591, 901, 64, 11, 51582], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 251, "seek": 57356, "start": 597.92, "end": 600.16, "text": " they demonstrate remarkable performance", "tokens": [51582, 436, 11698, 12802, 3389, 51694], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 252, "seek": 57356, "start": 600.16, "end": 601.9599999999999, "text": " due to the more human-like conversations", "tokens": [51694, 3462, 281, 264, 544, 1952, 12, 4092, 7315, 51784], "temperature": 0.0, "avg_logprob": -0.1701280206873797, "compression_ratio": 1.6516516516516517, "no_speech_prob": 0.006096013821661472}, {"id": 253, "seek": 60196, "start": 601.96, "end": 603.12, "text": " and natural instructions", "tokens": [50364, 293, 3303, 9415, 50422], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 254, "seek": 60196, "start": 603.12, "end": 605.08, "text": " in the community contributed conversations", "tokens": [50422, 294, 264, 1768, 18434, 7315, 50520], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 255, "seek": 60196, "start": 605.08, "end": 606.6800000000001, "text": " like those in shared GPT.", "tokens": [50520, 411, 729, 294, 5507, 26039, 51, 13, 50600], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 256, "seek": 60196, "start": 606.6800000000001, "end": 608.4000000000001, "text": " So basically what they're saying is", "tokens": [50600, 407, 1936, 437, 436, 434, 1566, 307, 50686], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 257, "seek": 60196, "start": 608.4000000000001, "end": 610.72, "text": " as more people are using these open source models", "tokens": [50686, 382, 544, 561, 366, 1228, 613, 1269, 4009, 5245, 50802], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 258, "seek": 60196, "start": 610.72, "end": 611.6800000000001, "text": " and sharing their data,", "tokens": [50802, 293, 5414, 641, 1412, 11, 50850], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 259, "seek": 60196, "start": 611.6800000000001, "end": 613.2800000000001, "text": " sharing their instructions,", "tokens": [50850, 5414, 641, 9415, 11, 50930], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 260, "seek": 60196, "start": 613.2800000000001, "end": 614.76, "text": " their prompts and the output,", "tokens": [50930, 641, 41095, 293, 264, 5598, 11, 51004], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 261, "seek": 60196, "start": 614.76, "end": 616.4000000000001, "text": " they'll continue to train on those pairs", "tokens": [51004, 436, 603, 2354, 281, 3847, 322, 729, 15494, 51086], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 262, "seek": 60196, "start": 616.4000000000001, "end": 617.5600000000001, "text": " and get better and better.", "tokens": [51086, 293, 483, 1101, 293, 1101, 13, 51144], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 263, "seek": 60196, "start": 617.5600000000001, "end": 619.44, "text": " But there's a limitation with that as well.", "tokens": [51144, 583, 456, 311, 257, 27432, 365, 300, 382, 731, 13, 51238], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 264, "seek": 60196, "start": 619.44, "end": 621.9200000000001, "text": " And it's the same thing that we keep coming back to.", "tokens": [51238, 400, 309, 311, 264, 912, 551, 300, 321, 1066, 1348, 646, 281, 13, 51362], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 265, "seek": 60196, "start": 621.9200000000001, "end": 624.6, "text": " Models trained on such natural conversations", "tokens": [51362, 6583, 1625, 8895, 322, 1270, 3303, 7315, 51496], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 266, "seek": 60196, "start": 624.6, "end": 626.08, "text": " may capture the style", "tokens": [51496, 815, 7983, 264, 3758, 51570], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 267, "seek": 60196, "start": 626.08, "end": 629.32, "text": " but not the reasoning process of the LLFM.", "tokens": [51570, 457, 406, 264, 21577, 1399, 295, 264, 441, 43, 37, 44, 13, 51732], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 268, "seek": 60196, "start": 629.32, "end": 631.88, "text": " So again, they'll be able to pattern match", "tokens": [51732, 407, 797, 11, 436, 603, 312, 1075, 281, 5102, 2995, 51860], "temperature": 0.0, "avg_logprob": -0.09269912083943685, "compression_ratio": 1.7981366459627328, "no_speech_prob": 0.0033757570199668407}, {"id": 269, "seek": 63188, "start": 631.88, "end": 634.72, "text": " but they're not gonna truly understand the logic", "tokens": [50364, 457, 436, 434, 406, 799, 4908, 1223, 264, 9952, 50506], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 270, "seek": 63188, "start": 634.72, "end": 637.56, "text": " and the reasoning behind arriving at the solutions.", "tokens": [50506, 293, 264, 21577, 2261, 22436, 412, 264, 6547, 13, 50648], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 271, "seek": 63188, "start": 637.56, "end": 641.68, "text": " Now the Orca Paper puts forth three key contributions.", "tokens": [50648, 823, 264, 1610, 496, 24990, 8137, 5220, 1045, 2141, 15725, 13, 50854], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 272, "seek": 63188, "start": 641.68, "end": 644.08, "text": " Number one is explanation tuning.", "tokens": [50854, 5118, 472, 307, 10835, 15164, 13, 50974], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 273, "seek": 63188, "start": 644.08, "end": 646.84, "text": " And again, this is fine tuning models", "tokens": [50974, 400, 797, 11, 341, 307, 2489, 15164, 5245, 51112], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 274, "seek": 63188, "start": 646.84, "end": 649.88, "text": " based on the step-by-step explanation", "tokens": [51112, 2361, 322, 264, 1823, 12, 2322, 12, 16792, 10835, 51264], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 275, "seek": 63188, "start": 649.88, "end": 651.24, "text": " of the reasoning and the logic", "tokens": [51264, 295, 264, 21577, 293, 264, 9952, 51332], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 276, "seek": 63188, "start": 651.24, "end": 652.76, "text": " of how to arrive at a solution.", "tokens": [51332, 295, 577, 281, 8881, 412, 257, 3827, 13, 51408], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 277, "seek": 63188, "start": 652.76, "end": 653.92, "text": " Let's read this a little bit.", "tokens": [51408, 961, 311, 1401, 341, 257, 707, 857, 13, 51466], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 278, "seek": 63188, "start": 653.92, "end": 656.4, "text": " We augment the query response pairs", "tokens": [51466, 492, 29919, 264, 14581, 4134, 15494, 51590], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 279, "seek": 63188, "start": 656.4, "end": 658.76, "text": " with detailed responses from GPT-4", "tokens": [51590, 365, 9942, 13019, 490, 26039, 51, 12, 19, 51708], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 280, "seek": 63188, "start": 658.76, "end": 661.32, "text": " that explain the reasoning process of the teacher", "tokens": [51708, 300, 2903, 264, 21577, 1399, 295, 264, 5027, 51836], "temperature": 0.0, "avg_logprob": -0.09625609976346375, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.00033532254747115076}, {"id": 281, "seek": 66132, "start": 661.32, "end": 663.0, "text": " as it generates the response.", "tokens": [50364, 382, 309, 23815, 264, 4134, 13, 50448], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 282, "seek": 66132, "start": 663.0, "end": 664.72, "text": " And to get the step-by-step reasoning,", "tokens": [50448, 400, 281, 483, 264, 1823, 12, 2322, 12, 16792, 21577, 11, 50534], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 283, "seek": 66132, "start": 664.72, "end": 667.24, "text": " they're using some of these more modern prompting techniques", "tokens": [50534, 436, 434, 1228, 512, 295, 613, 544, 4363, 12391, 278, 7512, 50660], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 284, "seek": 66132, "start": 667.24, "end": 668.6, "text": " that we've been learning about,", "tokens": [50660, 300, 321, 600, 668, 2539, 466, 11, 50728], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 285, "seek": 66132, "start": 668.6, "end": 670.72, "text": " such as explain like I'm five,", "tokens": [50728, 1270, 382, 2903, 411, 286, 478, 1732, 11, 50834], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 286, "seek": 66132, "start": 670.72, "end": 673.44, "text": " think step-by-step and justify your response.", "tokens": [50834, 519, 1823, 12, 2322, 12, 16792, 293, 20833, 428, 4134, 13, 50970], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 287, "seek": 66132, "start": 673.44, "end": 677.5200000000001, "text": " This forces GPT-4 to put forth its reasoning", "tokens": [50970, 639, 5874, 26039, 51, 12, 19, 281, 829, 5220, 1080, 21577, 51174], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 288, "seek": 66132, "start": 677.5200000000001, "end": 679.72, "text": " and its logic in the response itself", "tokens": [51174, 293, 1080, 9952, 294, 264, 4134, 2564, 51284], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 289, "seek": 66132, "start": 679.72, "end": 681.6800000000001, "text": " and that is used to train.", "tokens": [51284, 293, 300, 307, 1143, 281, 3847, 13, 51382], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 290, "seek": 66132, "start": 681.6800000000001, "end": 683.48, "text": " And that's what explanation tuning is.", "tokens": [51382, 400, 300, 311, 437, 10835, 15164, 307, 13, 51472], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 291, "seek": 66132, "start": 683.48, "end": 686.2800000000001, "text": " Another issue is scaling the amount of tasks and instructions.", "tokens": [51472, 3996, 2734, 307, 21589, 264, 2372, 295, 9608, 293, 9415, 13, 51612], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 292, "seek": 66132, "start": 686.2800000000001, "end": 688.96, "text": " As you'll see in a graph that I'll show in a second,", "tokens": [51612, 1018, 291, 603, 536, 294, 257, 4295, 300, 286, 603, 855, 294, 257, 1150, 11, 51746], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 293, "seek": 66132, "start": 688.96, "end": 690.44, "text": " a lot of these open source models", "tokens": [51746, 257, 688, 295, 613, 1269, 4009, 5245, 51820], "temperature": 0.0, "avg_logprob": -0.0900092769313503, "compression_ratio": 1.763157894736842, "no_speech_prob": 0.0023227857891470194}, {"id": 294, "seek": 69044, "start": 690.44, "end": 692.48, "text": " are using a highly limited data set,", "tokens": [50364, 366, 1228, 257, 5405, 5567, 1412, 992, 11, 50466], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 295, "seek": 69044, "start": 692.48, "end": 695.08, "text": " but that's where Orca really excels.", "tokens": [50466, 457, 300, 311, 689, 1610, 496, 534, 1624, 1625, 13, 50596], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 296, "seek": 69044, "start": 695.08, "end": 697.6, "text": " We utilize the Flaan 2020 collection", "tokens": [50596, 492, 16117, 264, 479, 875, 282, 4808, 5765, 50722], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 297, "seek": 69044, "start": 697.6, "end": 699.7600000000001, "text": " and that's a data set of tasks and instructions", "tokens": [50722, 293, 300, 311, 257, 1412, 992, 295, 9608, 293, 9415, 50830], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 298, "seek": 69044, "start": 699.7600000000001, "end": 700.8800000000001, "text": " put forth by Google", "tokens": [50830, 829, 5220, 538, 3329, 50886], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 299, "seek": 69044, "start": 700.8800000000001, "end": 703.7600000000001, "text": " that has tens of millions of instructions.", "tokens": [50886, 300, 575, 10688, 295, 6803, 295, 9415, 13, 51030], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 300, "seek": 69044, "start": 703.7600000000001, "end": 706.0200000000001, "text": " So let's quickly take a look at the data sizes", "tokens": [51030, 407, 718, 311, 2661, 747, 257, 574, 412, 264, 1412, 11602, 51143], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 301, "seek": 69044, "start": 706.0200000000001, "end": 707.36, "text": " for these open source models.", "tokens": [51143, 337, 613, 1269, 4009, 5245, 13, 51210], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 302, "seek": 69044, "start": 707.36, "end": 709.72, "text": " All of them have in the thousands.", "tokens": [51210, 1057, 295, 552, 362, 294, 264, 5383, 13, 51328], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 303, "seek": 69044, "start": 709.72, "end": 713.2800000000001, "text": " So you can see here that Alpaca has 52,000,", "tokens": [51328, 407, 291, 393, 536, 510, 300, 967, 79, 6628, 575, 18079, 11, 1360, 11, 51506], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 304, "seek": 69044, "start": 713.2800000000001, "end": 716.2800000000001, "text": " Vakunya has 70,000 and WizardLM with the most", "tokens": [51506, 691, 514, 409, 3016, 575, 5285, 11, 1360, 293, 37449, 43, 44, 365, 264, 881, 51656], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 305, "seek": 69044, "start": 716.2800000000001, "end": 719.2800000000001, "text": " has 250,000 based on the teacher of chat GPT.", "tokens": [51656, 575, 11650, 11, 1360, 2361, 322, 264, 5027, 295, 5081, 26039, 51, 13, 51806], "temperature": 0.0, "avg_logprob": -0.16303814556581755, "compression_ratio": 1.606164383561644, "no_speech_prob": 0.004330882802605629}, {"id": 306, "seek": 71928, "start": 719.28, "end": 720.68, "text": " And some of these other ones like Dolly", "tokens": [50364, 400, 512, 295, 613, 661, 2306, 411, 1144, 13020, 50434], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 307, "seek": 71928, "start": 720.68, "end": 721.76, "text": " are human instructed.", "tokens": [50434, 366, 1952, 36384, 13, 50488], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 308, "seek": 71928, "start": 721.76, "end": 723.28, "text": " So they're even more limited", "tokens": [50488, 407, 436, 434, 754, 544, 5567, 50564], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 309, "seek": 71928, "start": 723.28, "end": 724.92, "text": " because of the limitations of humans.", "tokens": [50564, 570, 295, 264, 15705, 295, 6255, 13, 50646], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 310, "seek": 71928, "start": 724.92, "end": 726.72, "text": " However, as you could see here,", "tokens": [50646, 2908, 11, 382, 291, 727, 536, 510, 11, 50736], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 311, "seek": 71928, "start": 726.72, "end": 730.1999999999999, "text": " Orca has five million, many times more", "tokens": [50736, 1610, 496, 575, 1732, 2459, 11, 867, 1413, 544, 50910], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 312, "seek": 71928, "start": 730.1999999999999, "end": 732.6, "text": " than all of the other open source models.", "tokens": [50910, 813, 439, 295, 264, 661, 1269, 4009, 5245, 13, 51030], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 313, "seek": 71928, "start": 732.6, "end": 735.16, "text": " And it's based on chat GPT initially,", "tokens": [51030, 400, 309, 311, 2361, 322, 5081, 26039, 51, 9105, 11, 51158], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 314, "seek": 71928, "start": 735.16, "end": 737.16, "text": " so that's the initial five million pass", "tokens": [51158, 370, 300, 311, 264, 5883, 1732, 2459, 1320, 51258], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 315, "seek": 71928, "start": 737.16, "end": 740.28, "text": " and then GPT-4 with a second pass", "tokens": [51258, 293, 550, 26039, 51, 12, 19, 365, 257, 1150, 1320, 51414], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 316, "seek": 71928, "start": 740.28, "end": 743.12, "text": " of much more complex tasks and instructions.", "tokens": [51414, 295, 709, 544, 3997, 9608, 293, 9415, 13, 51556], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 317, "seek": 71928, "start": 743.12, "end": 745.42, "text": " So not only are they getting full explanations", "tokens": [51556, 407, 406, 787, 366, 436, 1242, 1577, 28708, 51671], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 318, "seek": 71928, "start": 745.42, "end": 746.92, "text": " of query and responses", "tokens": [51671, 295, 14581, 293, 13019, 51746], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 319, "seek": 71928, "start": 746.92, "end": 749.16, "text": " and how they actually reach those responses,", "tokens": [51746, 293, 577, 436, 767, 2524, 729, 13019, 11, 51858], "temperature": 0.0, "avg_logprob": -0.10601660825204158, "compression_ratio": 1.71, "no_speech_prob": 0.0008558540139347315}, {"id": 320, "seek": 74916, "start": 749.16, "end": 751.04, "text": " but they're getting so many more of them", "tokens": [50364, 457, 436, 434, 1242, 370, 867, 544, 295, 552, 50458], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 321, "seek": 74916, "start": 751.04, "end": 752.9599999999999, "text": " and they're solving the data scaling issue.", "tokens": [50458, 293, 436, 434, 12606, 264, 1412, 21589, 2734, 13, 50554], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 322, "seek": 74916, "start": 752.9599999999999, "end": 754.4, "text": " Last is evaluation.", "tokens": [50554, 5264, 307, 13344, 13, 50626], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 323, "seek": 74916, "start": 754.4, "end": 757.7199999999999, "text": " There are a lot of issues with current evaluation techniques", "tokens": [50626, 821, 366, 257, 688, 295, 2663, 365, 2190, 13344, 7512, 50792], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 324, "seek": 74916, "start": 757.7199999999999, "end": 758.92, "text": " for open source models,", "tokens": [50792, 337, 1269, 4009, 5245, 11, 50852], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 325, "seek": 74916, "start": 758.92, "end": 761.3199999999999, "text": " but Orca claims to solve these in a few ways.", "tokens": [50852, 457, 1610, 496, 9441, 281, 5039, 613, 294, 257, 1326, 2098, 13, 50972], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 326, "seek": 74916, "start": 761.3199999999999, "end": 763.8, "text": " They use auto evaluation with GPT-4.", "tokens": [50972, 814, 764, 8399, 13344, 365, 26039, 51, 12, 19, 13, 51096], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 327, "seek": 74916, "start": 763.8, "end": 767.64, "text": " So basically asking GPT-4 between two potential responses,", "tokens": [51096, 407, 1936, 3365, 26039, 51, 12, 19, 1296, 732, 3995, 13019, 11, 51288], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 328, "seek": 74916, "start": 767.64, "end": 768.7199999999999, "text": " which one is best.", "tokens": [51288, 597, 472, 307, 1151, 13, 51342], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 329, "seek": 74916, "start": 768.7199999999999, "end": 771.56, "text": " They also use academic benchmarks like Big Bench Hard", "tokens": [51342, 814, 611, 764, 7778, 43751, 411, 5429, 3964, 339, 11817, 51484], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 330, "seek": 74916, "start": 771.56, "end": 774.56, "text": " and Truthful QA and professional and academic exams", "tokens": [51484, 293, 20522, 906, 1249, 32, 293, 4843, 293, 7778, 20514, 51634], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 331, "seek": 74916, "start": 774.56, "end": 776.48, "text": " like the SAT, LSAT, et cetera.", "tokens": [51634, 411, 264, 31536, 11, 36657, 2218, 11, 1030, 11458, 13, 51730], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 332, "seek": 74916, "start": 776.48, "end": 778.5799999999999, "text": " And last, they use safety evaluation", "tokens": [51730, 400, 1036, 11, 436, 764, 4514, 13344, 51835], "temperature": 0.0, "avg_logprob": -0.11849383090404754, "compression_ratio": 1.6687898089171975, "no_speech_prob": 0.00029592381906695664}, {"id": 333, "seek": 77858, "start": 778.58, "end": 781.14, "text": " from ToxicGen, basically do these responses", "tokens": [50364, 490, 1407, 47228, 26647, 11, 1936, 360, 613, 13019, 50492], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 334, "seek": 77858, "start": 781.14, "end": 782.7, "text": " contain toxic language.", "tokens": [50492, 5304, 12786, 2856, 13, 50570], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 335, "seek": 77858, "start": 782.7, "end": 784.22, "text": " So in figure four,", "tokens": [50570, 407, 294, 2573, 1451, 11, 50646], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 336, "seek": 77858, "start": 784.22, "end": 786.6600000000001, "text": " they illustrate what the previous techniques do", "tokens": [50646, 436, 23221, 437, 264, 3894, 7512, 360, 50768], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 337, "seek": 77858, "start": 786.6600000000001, "end": 788.58, "text": " with queries and responses.", "tokens": [50768, 365, 24109, 293, 13019, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 338, "seek": 77858, "start": 788.58, "end": 790.1800000000001, "text": " So here's a user instruction.", "tokens": [50864, 407, 510, 311, 257, 4195, 10951, 13, 50944], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 339, "seek": 77858, "start": 790.1800000000001, "end": 792.26, "text": " Use the data to calculate the median.", "tokens": [50944, 8278, 264, 1412, 281, 8873, 264, 26779, 13, 51048], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 340, "seek": 77858, "start": 792.26, "end": 794.34, "text": " Here it says, first, we need to arrange the data", "tokens": [51048, 1692, 309, 1619, 11, 700, 11, 321, 643, 281, 9424, 264, 1412, 51152], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 341, "seek": 77858, "start": 794.34, "end": 795.62, "text": " in ascending order.", "tokens": [51152, 294, 15526, 2029, 1668, 13, 51216], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 342, "seek": 77858, "start": 795.62, "end": 796.7, "text": " Since there are five numbers,", "tokens": [51216, 4162, 456, 366, 1732, 3547, 11, 51270], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 343, "seek": 77858, "start": 796.7, "end": 799.0600000000001, "text": " the median is in the middle, which is seven.", "tokens": [51270, 264, 26779, 307, 294, 264, 2808, 11, 597, 307, 3407, 13, 51388], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 344, "seek": 77858, "start": 799.0600000000001, "end": 800.22, "text": " And so this is very basic.", "tokens": [51388, 400, 370, 341, 307, 588, 3875, 13, 51446], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 345, "seek": 77858, "start": 800.22, "end": 802.7, "text": " It's a prompt, a query, and then the response.", "tokens": [51446, 467, 311, 257, 12391, 11, 257, 14581, 11, 293, 550, 264, 4134, 13, 51570], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 346, "seek": 77858, "start": 802.7, "end": 804.0600000000001, "text": " Whereas with their new method,", "tokens": [51570, 13813, 365, 641, 777, 3170, 11, 51638], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 347, "seek": 77858, "start": 804.0600000000001, "end": 806.38, "text": " they ask GPT-4 to explain.", "tokens": [51638, 436, 1029, 26039, 51, 12, 19, 281, 2903, 13, 51754], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 348, "seek": 77858, "start": 806.38, "end": 807.5400000000001, "text": " So the system instruction", "tokens": [51754, 407, 264, 1185, 10951, 51812], "temperature": 0.0, "avg_logprob": -0.11614948824832314, "compression_ratio": 1.694267515923567, "no_speech_prob": 0.0011333773145452142}, {"id": 349, "seek": 80754, "start": 807.54, "end": 809.42, "text": " seems to be the main tool that they use", "tokens": [50364, 2544, 281, 312, 264, 2135, 2290, 300, 436, 764, 50458], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 350, "seek": 80754, "start": 809.42, "end": 813.06, "text": " to get chat GPT and GPT-4 to explain their reasoning.", "tokens": [50458, 281, 483, 5081, 26039, 51, 293, 26039, 51, 12, 19, 281, 2903, 641, 21577, 13, 50640], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 351, "seek": 80754, "start": 813.06, "end": 814.5799999999999, "text": " You are an AI assistant.", "tokens": [50640, 509, 366, 364, 7318, 10994, 13, 50716], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 352, "seek": 80754, "start": 814.5799999999999, "end": 816.06, "text": " User will give you a task.", "tokens": [50716, 32127, 486, 976, 291, 257, 5633, 13, 50790], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 353, "seek": 80754, "start": 816.06, "end": 818.4599999999999, "text": " Your goal is to complete the task as faithfully", "tokens": [50790, 2260, 3387, 307, 281, 3566, 264, 5633, 382, 4522, 2277, 50910], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 354, "seek": 80754, "start": 818.4599999999999, "end": 819.98, "text": " as you can while performing the task.", "tokens": [50910, 382, 291, 393, 1339, 10205, 264, 5633, 13, 50986], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 355, "seek": 80754, "start": 819.98, "end": 822.42, "text": " Think step by step and justify your steps.", "tokens": [50986, 6557, 1823, 538, 1823, 293, 20833, 428, 4439, 13, 51108], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 356, "seek": 80754, "start": 822.42, "end": 825.8199999999999, "text": " So again, use the given data to calculate the median.", "tokens": [51108, 407, 797, 11, 764, 264, 2212, 1412, 281, 8873, 264, 26779, 13, 51278], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 357, "seek": 80754, "start": 825.8199999999999, "end": 826.78, "text": " Same prompt.", "tokens": [51278, 10635, 12391, 13, 51326], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 358, "seek": 80754, "start": 826.78, "end": 828.98, "text": " To calculate the median, I will follow these steps.", "tokens": [51326, 1407, 8873, 264, 26779, 11, 286, 486, 1524, 613, 4439, 13, 51436], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 359, "seek": 80754, "start": 828.98, "end": 831.6999999999999, "text": " And GPT-4 actually outlines step by step", "tokens": [51436, 400, 26039, 51, 12, 19, 767, 40125, 1823, 538, 1823, 51572], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 360, "seek": 80754, "start": 831.6999999999999, "end": 833.8199999999999, "text": " how it will figure out what the median is.", "tokens": [51572, 577, 309, 486, 2573, 484, 437, 264, 26779, 307, 13, 51678], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 361, "seek": 80754, "start": 833.8199999999999, "end": 836.42, "text": " That data is then used to train the open source model.", "tokens": [51678, 663, 1412, 307, 550, 1143, 281, 3847, 264, 1269, 4009, 2316, 13, 51808], "temperature": 0.0, "avg_logprob": -0.0818470573425293, "compression_ratio": 1.7674418604651163, "no_speech_prob": 0.0012064636684954166}, {"id": 362, "seek": 83642, "start": 836.42, "end": 838.02, "text": " I find it so fascinating", "tokens": [50364, 286, 915, 309, 370, 10343, 50444], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 363, "seek": 83642, "start": 838.02, "end": 840.42, "text": " that we're using some of these modern prompting techniques", "tokens": [50444, 300, 321, 434, 1228, 512, 295, 613, 4363, 12391, 278, 7512, 50564], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 364, "seek": 83642, "start": 840.42, "end": 843.3, "text": " like chain of thought, like explain like on five,", "tokens": [50564, 411, 5021, 295, 1194, 11, 411, 2903, 411, 322, 1732, 11, 50708], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 365, "seek": 83642, "start": 843.3, "end": 845.6999999999999, "text": " that people have been figuring out over the last few months", "tokens": [50708, 300, 561, 362, 668, 15213, 484, 670, 264, 1036, 1326, 2493, 50828], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 366, "seek": 83642, "start": 845.6999999999999, "end": 849.26, "text": " to get better answers from chat GPT and GPT-4.", "tokens": [50828, 281, 483, 1101, 6338, 490, 5081, 26039, 51, 293, 26039, 51, 12, 19, 13, 51006], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 367, "seek": 83642, "start": 849.26, "end": 851.26, "text": " And we're using those to get better data", "tokens": [51006, 400, 321, 434, 1228, 729, 281, 483, 1101, 1412, 51106], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 368, "seek": 83642, "start": 851.26, "end": 853.6999999999999, "text": " to train the open source models with.", "tokens": [51106, 281, 3847, 264, 1269, 4009, 5245, 365, 13, 51228], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 369, "seek": 83642, "start": 853.6999999999999, "end": 856.88, "text": " And as I mentioned, system messages seem to be the main tool", "tokens": [51228, 400, 382, 286, 2835, 11, 1185, 7897, 1643, 281, 312, 264, 2135, 2290, 51387], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 370, "seek": 83642, "start": 856.88, "end": 859.02, "text": " to get chat GPT and GPT-4", "tokens": [51387, 281, 483, 5081, 26039, 51, 293, 26039, 51, 12, 19, 51494], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 371, "seek": 83642, "start": 859.02, "end": 862.06, "text": " to provide the step by step explanations.", "tokens": [51494, 281, 2893, 264, 1823, 538, 1823, 28708, 13, 51646], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 372, "seek": 83642, "start": 862.06, "end": 865.06, "text": " And if you play around with the chat GPT playground", "tokens": [51646, 400, 498, 291, 862, 926, 365, 264, 5081, 26039, 51, 24646, 51796], "temperature": 0.0, "avg_logprob": -0.11069639700430411, "compression_ratio": 1.7921146953405018, "no_speech_prob": 0.00014425348490476608}, {"id": 373, "seek": 86506, "start": 865.06, "end": 867.76, "text": " or even the API, you'll know that the system messages", "tokens": [50364, 420, 754, 264, 9362, 11, 291, 603, 458, 300, 264, 1185, 7897, 50499], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 374, "seek": 86506, "start": 867.76, "end": 870.9399999999999, "text": " are a requirement for using either of these tools.", "tokens": [50499, 366, 257, 11695, 337, 1228, 2139, 295, 613, 3873, 13, 50658], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 375, "seek": 86506, "start": 870.9399999999999, "end": 872.14, "text": " So here are a few examples.", "tokens": [50658, 407, 510, 366, 257, 1326, 5110, 13, 50718], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 376, "seek": 86506, "start": 872.14, "end": 873.18, "text": " You will be given a task,", "tokens": [50718, 509, 486, 312, 2212, 257, 5633, 11, 50770], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 377, "seek": 86506, "start": 873.18, "end": 875.3399999999999, "text": " you must generate a detailed and long answer.", "tokens": [50770, 291, 1633, 8460, 257, 9942, 293, 938, 1867, 13, 50878], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 378, "seek": 86506, "start": 875.3399999999999, "end": 877.54, "text": " Think like you are answering to a five year old,", "tokens": [50878, 6557, 411, 291, 366, 13430, 281, 257, 1732, 1064, 1331, 11, 50988], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 379, "seek": 86506, "start": 877.54, "end": 878.66, "text": " help as much as you can.", "tokens": [50988, 854, 382, 709, 382, 291, 393, 13, 51044], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 380, "seek": 86506, "start": 878.66, "end": 880.5, "text": " So it's really just coaxing chat GPT-4", "tokens": [51044, 407, 309, 311, 534, 445, 598, 2797, 278, 5081, 26039, 51, 12, 19, 51136], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 381, "seek": 86506, "start": 880.5, "end": 883.38, "text": " to explain its reasoning and to be as verbose as possible.", "tokens": [51136, 281, 2903, 1080, 21577, 293, 281, 312, 382, 9595, 541, 382, 1944, 13, 51280], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 382, "seek": 86506, "start": 883.38, "end": 884.9799999999999, "text": " So let's actually take a look at the difference", "tokens": [51280, 407, 718, 311, 767, 747, 257, 574, 412, 264, 2649, 51360], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 383, "seek": 86506, "start": 884.9799999999999, "end": 886.7399999999999, "text": " between these two prompting techniques.", "tokens": [51360, 1296, 613, 732, 12391, 278, 7512, 13, 51448], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 384, "seek": 86506, "start": 886.7399999999999, "end": 888.06, "text": " So for the system message,", "tokens": [51448, 407, 337, 264, 1185, 3636, 11, 51514], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 385, "seek": 86506, "start": 888.06, "end": 891.38, "text": " you are an AI assistant that helps people find information.", "tokens": [51514, 291, 366, 364, 7318, 10994, 300, 3665, 561, 915, 1589, 13, 51680], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 386, "seek": 86506, "start": 891.38, "end": 892.6199999999999, "text": " User will give you a question.", "tokens": [51680, 32127, 486, 976, 291, 257, 1168, 13, 51742], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 387, "seek": 86506, "start": 892.6199999999999, "end": 894.6199999999999, "text": " Your task is to answer as faithfully as you can", "tokens": [51742, 2260, 5633, 307, 281, 1867, 382, 4522, 2277, 382, 291, 393, 51842], "temperature": 0.0, "avg_logprob": -0.07957995420246455, "compression_ratio": 1.7355371900826446, "no_speech_prob": 0.0038239052519202232}, {"id": 388, "seek": 89462, "start": 894.66, "end": 896.98, "text": " while answering things step by step and justify your answer.", "tokens": [50366, 1339, 13430, 721, 1823, 538, 1823, 293, 20833, 428, 1867, 13, 50482], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 389, "seek": 89462, "start": 896.98, "end": 900.88, "text": " So the prompt is pick which sentence is not logical.", "tokens": [50482, 407, 264, 12391, 307, 1888, 597, 8174, 307, 406, 14978, 13, 50677], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 390, "seek": 89462, "start": 900.88, "end": 904.0600000000001, "text": " Sentence A, people in the desert often look forward to flood.", "tokens": [50677, 23652, 655, 316, 11, 561, 294, 264, 11029, 2049, 574, 2128, 281, 10481, 13, 50836], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 391, "seek": 89462, "start": 904.0600000000001, "end": 907.14, "text": " Sentence B, people in the desert often look forward to rain.", "tokens": [50836, 23652, 655, 363, 11, 561, 294, 264, 11029, 2049, 574, 2128, 281, 4830, 13, 50990], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 392, "seek": 89462, "start": 907.14, "end": 908.86, "text": " Now it might seem obvious to us", "tokens": [50990, 823, 309, 1062, 1643, 6322, 281, 505, 51076], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 393, "seek": 89462, "start": 908.86, "end": 910.58, "text": " why these sentences are different,", "tokens": [51076, 983, 613, 16579, 366, 819, 11, 51162], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 394, "seek": 89462, "start": 910.58, "end": 913.3, "text": " but there's really just a subtle difference between them.", "tokens": [51162, 457, 456, 311, 534, 445, 257, 13743, 2649, 1296, 552, 13, 51298], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 395, "seek": 89462, "start": 913.3, "end": 915.02, "text": " So here a typical response might be,", "tokens": [51298, 407, 510, 257, 7476, 4134, 1062, 312, 11, 51384], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 396, "seek": 89462, "start": 915.02, "end": 916.74, "text": " there is no reason to look forward to a flood", "tokens": [51384, 456, 307, 572, 1778, 281, 574, 2128, 281, 257, 10481, 51470], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 397, "seek": 89462, "start": 916.74, "end": 917.98, "text": " because floods cause damage.", "tokens": [51470, 570, 35536, 3082, 4344, 13, 51532], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 398, "seek": 89462, "start": 917.98, "end": 919.26, "text": " The answer is sentence A.", "tokens": [51532, 440, 1867, 307, 8174, 316, 13, 51596], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 399, "seek": 89462, "start": 919.26, "end": 921.7, "text": " Chat GPT says, okay, let's break it down.", "tokens": [51596, 27503, 26039, 51, 1619, 11, 1392, 11, 718, 311, 1821, 309, 760, 13, 51718], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 400, "seek": 89462, "start": 921.7, "end": 923.1, "text": " Read the sentences carefully.", "tokens": [51718, 17604, 264, 16579, 7500, 13, 51788], "temperature": 0.0, "avg_logprob": -0.0982851630089267, "compression_ratio": 1.8721311475409836, "no_speech_prob": 0.001648247940465808}, {"id": 401, "seek": 92310, "start": 923.1, "end": 924.14, "text": " So it's talking about what steps", "tokens": [50364, 407, 309, 311, 1417, 466, 437, 4439, 50416], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 402, "seek": 92310, "start": 924.14, "end": 925.58, "text": " am I going to take to answer this?", "tokens": [50416, 669, 286, 516, 281, 747, 281, 1867, 341, 30, 50488], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 403, "seek": 92310, "start": 925.58, "end": 927.38, "text": " Identify the differences between them", "tokens": [50488, 25905, 2505, 264, 7300, 1296, 552, 50578], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 404, "seek": 92310, "start": 927.38, "end": 929.5, "text": " and determine which sentence is not logical.", "tokens": [50578, 293, 6997, 597, 8174, 307, 406, 14978, 13, 50684], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 405, "seek": 92310, "start": 929.5, "end": 932.02, "text": " Then they say, sentence A, people in the desert", "tokens": [50684, 1396, 436, 584, 11, 8174, 316, 11, 561, 294, 264, 11029, 50810], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 406, "seek": 92310, "start": 932.02, "end": 933.1, "text": " often look forward to the flood.", "tokens": [50810, 2049, 574, 2128, 281, 264, 10481, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 407, "seek": 92310, "start": 933.1, "end": 935.62, "text": " This sentence suggests that people in the desert", "tokens": [50864, 639, 8174, 13409, 300, 561, 294, 264, 11029, 50990], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 408, "seek": 92310, "start": 935.62, "end": 937.5, "text": " are eagerly anticipating a flood.", "tokens": [50990, 366, 18259, 356, 40568, 257, 10481, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 409, "seek": 92310, "start": 937.5, "end": 940.0600000000001, "text": " However, floods are typically destructive and dangerous.", "tokens": [51084, 2908, 11, 35536, 366, 5850, 26960, 293, 5795, 13, 51212], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 410, "seek": 92310, "start": 940.0600000000001, "end": 942.94, "text": " So it seems unlikely that people would look forward to them.", "tokens": [51212, 407, 309, 2544, 17518, 300, 561, 576, 574, 2128, 281, 552, 13, 51356], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 411, "seek": 92310, "start": 942.94, "end": 945.26, "text": " Sentence B, they're looking forward to the rain.", "tokens": [51356, 23652, 655, 363, 11, 436, 434, 1237, 2128, 281, 264, 4830, 13, 51472], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 412, "seek": 92310, "start": 945.26, "end": 948.5, "text": " And it says this sentence suggests that people in the desert", "tokens": [51472, 400, 309, 1619, 341, 8174, 13409, 300, 561, 294, 264, 11029, 51634], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 413, "seek": 92310, "start": 948.5, "end": 950.3000000000001, "text": " are eagerly anticipating rain", "tokens": [51634, 366, 18259, 356, 40568, 4830, 51724], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 414, "seek": 92310, "start": 950.3000000000001, "end": 951.86, "text": " while the desert is a dry environment.", "tokens": [51724, 1339, 264, 11029, 307, 257, 4016, 2823, 13, 51802], "temperature": 0.0, "avg_logprob": -0.09901571909586589, "compression_ratio": 2.057239057239057, "no_speech_prob": 0.00021653623844031245}, {"id": 415, "seek": 95186, "start": 951.86, "end": 953.62, "text": " Rain can be beneficial in providing water", "tokens": [50364, 14487, 393, 312, 14072, 294, 6530, 1281, 50452], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 416, "seek": 95186, "start": 953.62, "end": 954.58, "text": " for plants and animals.", "tokens": [50452, 337, 5972, 293, 4882, 13, 50500], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 417, "seek": 95186, "start": 954.58, "end": 957.1, "text": " Then it finishes with, based on the above analysis,", "tokens": [50500, 1396, 309, 23615, 365, 11, 2361, 322, 264, 3673, 5215, 11, 50626], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 418, "seek": 95186, "start": 957.1, "end": 959.58, "text": " it can be concluded that sentence A is not logical.", "tokens": [50626, 309, 393, 312, 22960, 300, 8174, 316, 307, 406, 14978, 13, 50750], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 419, "seek": 95186, "start": 959.58, "end": 961.34, "text": " Then we look at GPT-4's answer.", "tokens": [50750, 1396, 321, 574, 412, 26039, 51, 12, 19, 311, 1867, 13, 50838], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 420, "seek": 95186, "start": 961.34, "end": 963.5, "text": " And as you can see, I won't read all of it.", "tokens": [50838, 400, 382, 291, 393, 536, 11, 286, 1582, 380, 1401, 439, 295, 309, 13, 50946], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 421, "seek": 95186, "start": 963.5, "end": 966.86, "text": " It's a much more detailed and verbose answer.", "tokens": [50946, 467, 311, 257, 709, 544, 9942, 293, 9595, 541, 1867, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 422, "seek": 95186, "start": 966.86, "end": 968.66, "text": " Now in this section, they talk about", "tokens": [51114, 823, 294, 341, 3541, 11, 436, 751, 466, 51204], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 423, "seek": 95186, "start": 968.66, "end": 971.34, "text": " why Chat GPT as a teaching assistant,", "tokens": [51204, 983, 27503, 26039, 51, 382, 257, 4571, 10994, 11, 51338], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 424, "seek": 95186, "start": 971.34, "end": 974.66, "text": " assistant to GPT-4, is such a powerful method.", "tokens": [51338, 10994, 281, 26039, 51, 12, 19, 11, 307, 1270, 257, 4005, 3170, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 425, "seek": 95186, "start": 974.66, "end": 976.74, "text": " And there's really two reasons for it.", "tokens": [51504, 400, 456, 311, 534, 732, 4112, 337, 309, 13, 51608], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 426, "seek": 95186, "start": 976.74, "end": 979.84, "text": " One is a capacity gap because there's such a large gap", "tokens": [51608, 1485, 307, 257, 6042, 7417, 570, 456, 311, 1270, 257, 2416, 7417, 51763], "temperature": 0.0, "avg_logprob": -0.09421719012617254, "compression_ratio": 1.6198083067092652, "no_speech_prob": 0.0007793185068294406}, {"id": 427, "seek": 97984, "start": 979.84, "end": 982.48, "text": " between the ORCA model and GPT-4.", "tokens": [50364, 1296, 264, 19654, 15515, 2316, 293, 26039, 51, 12, 19, 13, 50496], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 428, "seek": 97984, "start": 982.48, "end": 984.24, "text": " Being able to take data from GPT-4", "tokens": [50496, 8891, 1075, 281, 747, 1412, 490, 26039, 51, 12, 19, 50584], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 429, "seek": 97984, "start": 984.24, "end": 986.08, "text": " and passing it directly into ORCA,", "tokens": [50584, 293, 8437, 309, 3838, 666, 19654, 15515, 11, 50676], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 430, "seek": 97984, "start": 986.08, "end": 987.76, "text": " it's gonna struggle with imitation.", "tokens": [50676, 309, 311, 799, 7799, 365, 47624, 13, 50760], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 431, "seek": 97984, "start": 987.76, "end": 989.52, "text": " Whereas if they progressively teach it", "tokens": [50760, 13813, 498, 436, 46667, 2924, 309, 50848], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 432, "seek": 97984, "start": 989.52, "end": 991.48, "text": " to get to the GPT-4 level", "tokens": [50848, 281, 483, 281, 264, 26039, 51, 12, 19, 1496, 50946], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 433, "seek": 97984, "start": 991.48, "end": 994.08, "text": " by the intermediate step of Chat GPT,", "tokens": [50946, 538, 264, 19376, 1823, 295, 27503, 26039, 51, 11, 51076], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 434, "seek": 97984, "start": 994.08, "end": 995.84, "text": " it really performs much better.", "tokens": [51076, 309, 534, 26213, 709, 1101, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 435, "seek": 97984, "start": 995.84, "end": 998.6, "text": " This can be viewed as a form of progressive learning", "tokens": [51164, 639, 393, 312, 19174, 382, 257, 1254, 295, 16131, 2539, 51302], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 436, "seek": 97984, "start": 998.6, "end": 1000.24, "text": " or curriculum learning,", "tokens": [51302, 420, 14302, 2539, 11, 51384], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 437, "seek": 97984, "start": 1000.24, "end": 1002.52, "text": " where the student first learns from easier examples", "tokens": [51384, 689, 264, 3107, 700, 27152, 490, 3571, 5110, 51498], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 438, "seek": 97984, "start": 1002.52, "end": 1003.9200000000001, "text": " followed by harder ones.", "tokens": [51498, 6263, 538, 6081, 2306, 13, 51568], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 439, "seek": 97984, "start": 1003.9200000000001, "end": 1007.4000000000001, "text": " Again, more and more human-like behavior.", "tokens": [51568, 3764, 11, 544, 293, 544, 1952, 12, 4092, 5223, 13, 51742], "temperature": 0.0, "avg_logprob": -0.0848149452500671, "compression_ratio": 1.587837837837838, "no_speech_prob": 0.01743706315755844}, {"id": 440, "seek": 100740, "start": 1007.4399999999999, "end": 1010.0799999999999, "text": " Human doesn't go from learning the basics of addition", "tokens": [50366, 10294, 1177, 380, 352, 490, 2539, 264, 14688, 295, 4500, 50498], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 441, "seek": 100740, "start": 1010.0799999999999, "end": 1011.92, "text": " all the way to calculus in one step.", "tokens": [50498, 439, 264, 636, 281, 33400, 294, 472, 1823, 13, 50590], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 442, "seek": 100740, "start": 1011.92, "end": 1014.4, "text": " They learn many incrementally more difficult steps", "tokens": [50590, 814, 1466, 867, 26200, 379, 544, 2252, 4439, 50714], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 443, "seek": 100740, "start": 1014.4, "end": 1017.88, "text": " of mathematics between addition and calculus.", "tokens": [50714, 295, 18666, 1296, 4500, 293, 33400, 13, 50888], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 444, "seek": 100740, "start": 1017.88, "end": 1021.4399999999999, "text": " Next is a simple pragmatic reason, cost and time.", "tokens": [50888, 3087, 307, 257, 2199, 46904, 1778, 11, 2063, 293, 565, 13, 51066], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 445, "seek": 100740, "start": 1021.4399999999999, "end": 1025.2, "text": " Chat GPT, specifically GPT-3.5 turbo,", "tokens": [51066, 27503, 26039, 51, 11, 4682, 26039, 51, 12, 18, 13, 20, 20902, 11, 51254], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 446, "seek": 100740, "start": 1025.2, "end": 1029.2, "text": " is much faster and much less expensive than GPT-4.", "tokens": [51254, 307, 709, 4663, 293, 709, 1570, 5124, 813, 26039, 51, 12, 19, 13, 51454], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 447, "seek": 100740, "start": 1029.2, "end": 1031.12, "text": " So that's why they use five million examples", "tokens": [51454, 407, 300, 311, 983, 436, 764, 1732, 2459, 5110, 51550], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 448, "seek": 100740, "start": 1031.12, "end": 1034.92, "text": " with Chat GPT and one million examples for GPT-4.", "tokens": [51550, 365, 27503, 26039, 51, 293, 472, 2459, 5110, 337, 26039, 51, 12, 19, 13, 51740], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 449, "seek": 100740, "start": 1034.92, "end": 1036.56, "text": " So this graphic shows the performance", "tokens": [51740, 407, 341, 14089, 3110, 264, 3389, 51822], "temperature": 0.0, "avg_logprob": -0.08904758828585266, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.00019109885033685714}, {"id": 450, "seek": 103656, "start": 1036.6, "end": 1038.36, "text": " of these large foundational models,", "tokens": [50366, 295, 613, 2416, 32195, 5245, 11, 50454], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 451, "seek": 103656, "start": 1038.36, "end": 1040.44, "text": " Vecunia and ORCA.", "tokens": [50454, 691, 3045, 409, 654, 293, 19654, 15515, 13, 50558], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 452, "seek": 103656, "start": 1040.44, "end": 1042.52, "text": " And as we can clearly see from questions", "tokens": [50558, 400, 382, 321, 393, 4448, 536, 490, 1651, 50662], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 453, "seek": 103656, "start": 1042.52, "end": 1044.28, "text": " from the LSAT and the SAT,", "tokens": [50662, 490, 264, 36657, 2218, 293, 264, 31536, 11, 50750], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 454, "seek": 103656, "start": 1044.28, "end": 1047.6, "text": " ORCA performs significantly better than Vecunia.", "tokens": [50750, 19654, 15515, 26213, 10591, 1101, 813, 691, 3045, 409, 654, 13, 50916], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 455, "seek": 103656, "start": 1047.6, "end": 1049.44, "text": " And if we look at the ORCA column,", "tokens": [50916, 400, 498, 321, 574, 412, 264, 19654, 15515, 7738, 11, 51008], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 456, "seek": 103656, "start": 1049.44, "end": 1051.24, "text": " compared to the Chat GPT column,", "tokens": [51008, 5347, 281, 264, 27503, 26039, 51, 7738, 11, 51098], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 457, "seek": 103656, "start": 1051.24, "end": 1053.72, "text": " overall it performs quite similarly,", "tokens": [51098, 4787, 309, 26213, 1596, 14138, 11, 51222], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 458, "seek": 103656, "start": 1053.72, "end": 1056.48, "text": " but it still does lag behind GPT-4.", "tokens": [51222, 457, 309, 920, 775, 8953, 2261, 26039, 51, 12, 19, 13, 51360], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 459, "seek": 103656, "start": 1056.48, "end": 1057.6, "text": " And they've actually shown", "tokens": [51360, 400, 436, 600, 767, 4898, 51416], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 460, "seek": 103656, "start": 1057.6, "end": 1060.24, "text": " that this progressive learning technique really works.", "tokens": [51416, 300, 341, 16131, 2539, 6532, 534, 1985, 13, 51548], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 461, "seek": 103656, "start": 1060.24, "end": 1063.2, "text": " As we can see here, using only GPT-4,", "tokens": [51548, 1018, 321, 393, 536, 510, 11, 1228, 787, 26039, 51, 12, 19, 11, 51696], "temperature": 0.0, "avg_logprob": -0.10708864033222198, "compression_ratio": 1.6022304832713754, "no_speech_prob": 0.005729091819375753}, {"id": 462, "seek": 106320, "start": 1063.24, "end": 1066.8400000000001, "text": " they were able to achieve a score of 37.18,", "tokens": [50366, 436, 645, 1075, 281, 4584, 257, 6175, 295, 13435, 13, 6494, 11, 50546], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 463, "seek": 106320, "start": 1066.8400000000001, "end": 1070.4, "text": " whereas if they used that intermediate step of Chat GPT,", "tokens": [50546, 9735, 498, 436, 1143, 300, 19376, 1823, 295, 27503, 26039, 51, 11, 50724], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 464, "seek": 106320, "start": 1070.4, "end": 1073.16, "text": " they were able to achieve 41.7.", "tokens": [50724, 436, 645, 1075, 281, 4584, 18173, 13, 22, 13, 50862], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 465, "seek": 106320, "start": 1073.16, "end": 1074.72, "text": " That might seem small,", "tokens": [50862, 663, 1062, 1643, 1359, 11, 50940], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 466, "seek": 106320, "start": 1074.72, "end": 1076.32, "text": " but that is a significant improvement.", "tokens": [50940, 457, 300, 307, 257, 4776, 10444, 13, 51020], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 467, "seek": 106320, "start": 1076.32, "end": 1078.1200000000001, "text": " And for the big bench hard results,", "tokens": [51020, 400, 337, 264, 955, 10638, 1152, 3542, 11, 51110], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 468, "seek": 106320, "start": 1078.1200000000001, "end": 1081.56, "text": " ORCA performs marginally better than Chat GPT on aggregate", "tokens": [51110, 19654, 15515, 26213, 10270, 379, 1101, 813, 27503, 26039, 51, 322, 26118, 51282], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 469, "seek": 106320, "start": 1081.56, "end": 1085.44, "text": " across all tasks, significantly lags GPT-4", "tokens": [51282, 2108, 439, 9608, 11, 10591, 8953, 82, 26039, 51, 12, 19, 51476], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 470, "seek": 106320, "start": 1085.44, "end": 1089.1200000000001, "text": " and outperforms Vecunia by 113%.", "tokens": [51476, 293, 484, 26765, 82, 691, 3045, 409, 654, 538, 2975, 18, 6856, 51660], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 471, "seek": 106320, "start": 1089.1200000000001, "end": 1092.16, "text": " And here they give a graphic of the zero-shot performance", "tokens": [51660, 400, 510, 436, 976, 257, 14089, 295, 264, 4018, 12, 18402, 3389, 51812], "temperature": 0.0, "avg_logprob": -0.10005633557429079, "compression_ratio": 1.5902255639097744, "no_speech_prob": 0.0005033189663663507}, {"id": 472, "seek": 109216, "start": 1092.2, "end": 1094.16, "text": " against all of these different tasks.", "tokens": [50366, 1970, 439, 295, 613, 819, 9608, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 473, "seek": 109216, "start": 1094.16, "end": 1097.28, "text": " And you can see that ORCA performs substantially", "tokens": [50464, 400, 291, 393, 536, 300, 19654, 15515, 26213, 30797, 50620], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 474, "seek": 109216, "start": 1097.28, "end": 1098.44, "text": " better than Vecunia.", "tokens": [50620, 1101, 813, 691, 3045, 409, 654, 13, 50678], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 475, "seek": 109216, "start": 1098.44, "end": 1101.24, "text": " And even across all of them, like it said,", "tokens": [50678, 400, 754, 2108, 439, 295, 552, 11, 411, 309, 848, 11, 50818], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 476, "seek": 109216, "start": 1101.24, "end": 1103.68, "text": " it performs better than Chat GPT.", "tokens": [50818, 309, 26213, 1101, 813, 27503, 26039, 51, 13, 50940], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 477, "seek": 109216, "start": 1103.68, "end": 1105.24, "text": " So what does all this mean?", "tokens": [50940, 407, 437, 775, 439, 341, 914, 30, 51018], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 478, "seek": 109216, "start": 1105.24, "end": 1107.8000000000002, "text": " I find it fascinating for two reasons.", "tokens": [51018, 286, 915, 309, 10343, 337, 732, 4112, 13, 51146], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 479, "seek": 109216, "start": 1107.8000000000002, "end": 1110.3600000000001, "text": " One, open-source models continue to get better", "tokens": [51146, 1485, 11, 1269, 12, 41676, 5245, 2354, 281, 483, 1101, 51274], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 480, "seek": 109216, "start": 1110.3600000000001, "end": 1112.0400000000002, "text": " at a rapid clip.", "tokens": [51274, 412, 257, 7558, 7353, 13, 51358], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 481, "seek": 109216, "start": 1112.0400000000002, "end": 1114.5600000000002, "text": " New techniques for fine-tuning, training", "tokens": [51358, 1873, 7512, 337, 2489, 12, 83, 37726, 11, 3097, 51484], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 482, "seek": 109216, "start": 1114.5600000000002, "end": 1116.88, "text": " are coming out every single day.", "tokens": [51484, 366, 1348, 484, 633, 2167, 786, 13, 51600], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 483, "seek": 109216, "start": 1116.88, "end": 1119.44, "text": " So I think back to that we have no mode paper", "tokens": [51600, 407, 286, 519, 646, 281, 300, 321, 362, 572, 4391, 3035, 51728], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 484, "seek": 109216, "start": 1119.44, "end": 1121.24, "text": " and it makes a lot of sense still.", "tokens": [51728, 293, 309, 1669, 257, 688, 295, 2020, 920, 13, 51818], "temperature": 0.0, "avg_logprob": -0.09482261713813334, "compression_ratio": 1.6095890410958904, "no_speech_prob": 0.003945016302168369}, {"id": 485, "seek": 112124, "start": 1121.24, "end": 1123.52, "text": " I also find it fascinating that GPT-4", "tokens": [50364, 286, 611, 915, 309, 10343, 300, 26039, 51, 12, 19, 50478], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 486, "seek": 112124, "start": 1123.52, "end": 1125.92, "text": " still seems to have some secret sauce", "tokens": [50478, 920, 2544, 281, 362, 512, 4054, 4880, 50598], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 487, "seek": 112124, "start": 1125.92, "end": 1129.08, "text": " and performs much better than any other model.", "tokens": [50598, 293, 26213, 709, 1101, 813, 604, 661, 2316, 13, 50756], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 488, "seek": 112124, "start": 1129.08, "end": 1131.84, "text": " So OpenAI seems to have plenty of mode.", "tokens": [50756, 407, 7238, 48698, 2544, 281, 362, 7140, 295, 4391, 13, 50894], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 489, "seek": 112124, "start": 1131.84, "end": 1135.08, "text": " This mode seems to be incrementally getting decreased,", "tokens": [50894, 639, 4391, 2544, 281, 312, 26200, 379, 1242, 24436, 11, 51056], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 490, "seek": 112124, "start": 1135.08, "end": 1136.4, "text": " but they still do have it.", "tokens": [51056, 457, 436, 920, 360, 362, 309, 13, 51122], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 491, "seek": 112124, "start": 1136.4, "end": 1138.16, "text": " The last thing that I find fascinating", "tokens": [51122, 440, 1036, 551, 300, 286, 915, 10343, 51210], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 492, "seek": 112124, "start": 1138.16, "end": 1141.36, "text": " is that this paper is by Microsoft Research.", "tokens": [51210, 307, 300, 341, 3035, 307, 538, 8116, 10303, 13, 51370], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 493, "seek": 112124, "start": 1141.36, "end": 1145.68, "text": " Microsoft Research owns a significant portion of OpenAI.", "tokens": [51370, 8116, 10303, 19143, 257, 4776, 8044, 295, 7238, 48698, 13, 51586], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 494, "seek": 112124, "start": 1145.68, "end": 1147.64, "text": " So the fact that they're making research gains", "tokens": [51586, 407, 264, 1186, 300, 436, 434, 1455, 2132, 16823, 51684], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 495, "seek": 112124, "start": 1147.64, "end": 1149.6200000000001, "text": " in open-source is awesome.", "tokens": [51684, 294, 1269, 12, 41676, 307, 3476, 13, 51783], "temperature": 0.0, "avg_logprob": -0.08579196449087448, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.0001535550254629925}, {"id": 496, "seek": 114962, "start": 1149.62, "end": 1151.86, "text": " And OpenAI really must feel", "tokens": [50364, 400, 7238, 48698, 534, 1633, 841, 50476], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 497, "seek": 114962, "start": 1151.86, "end": 1153.9399999999998, "text": " that they have a significant mode to work with.", "tokens": [50476, 300, 436, 362, 257, 4776, 4391, 281, 589, 365, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 498, "seek": 114962, "start": 1153.9399999999998, "end": 1156.9399999999998, "text": " And OpenAI also mentioned a week ago", "tokens": [50580, 400, 7238, 48698, 611, 2835, 257, 1243, 2057, 50730], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 499, "seek": 114962, "start": 1156.9399999999998, "end": 1159.1, "text": " that they're releasing their own open-source model.", "tokens": [50730, 300, 436, 434, 16327, 641, 1065, 1269, 12, 41676, 2316, 13, 50838], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 500, "seek": 114962, "start": 1159.1, "end": 1160.9399999999998, "text": " So I think what all of this means", "tokens": [50838, 407, 286, 519, 437, 439, 295, 341, 1355, 50930], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 501, "seek": 114962, "start": 1160.9399999999998, "end": 1162.78, "text": " is that these large language models", "tokens": [50930, 307, 300, 613, 2416, 2856, 5245, 51022], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 502, "seek": 114962, "start": 1162.78, "end": 1165.4399999999998, "text": " will continue to get better and cheaper over time.", "tokens": [51022, 486, 2354, 281, 483, 1101, 293, 12284, 670, 565, 13, 51155], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 503, "seek": 114962, "start": 1165.4399999999998, "end": 1168.1, "text": " Now Orca's code and dataset are not yet released,", "tokens": [51155, 823, 1610, 496, 311, 3089, 293, 28872, 366, 406, 1939, 4736, 11, 51288], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 504, "seek": 114962, "start": 1168.1, "end": 1170.1799999999998, "text": " but as soon as it is, we're gonna review it.", "tokens": [51288, 457, 382, 2321, 382, 309, 307, 11, 321, 434, 799, 3131, 309, 13, 51392], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 505, "seek": 114962, "start": 1170.1799999999998, "end": 1171.6599999999999, "text": " I'm gonna show you how to use it", "tokens": [51392, 286, 478, 799, 855, 291, 577, 281, 764, 309, 51466], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 506, "seek": 114962, "start": 1171.6599999999999, "end": 1173.26, "text": " and we'll see how it performs.", "tokens": [51466, 293, 321, 603, 536, 577, 309, 26213, 13, 51546], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 507, "seek": 114962, "start": 1173.26, "end": 1174.5, "text": " If you liked this video,", "tokens": [51546, 759, 291, 4501, 341, 960, 11, 51608], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 508, "seek": 114962, "start": 1174.5, "end": 1176.4199999999998, "text": " please consider giving me a like and subscribe", "tokens": [51608, 1767, 1949, 2902, 385, 257, 411, 293, 3022, 51704], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}, {"id": 509, "seek": 114962, "start": 1176.4199999999998, "end": 1178.02, "text": " and I'll see you in the next one.", "tokens": [51704, 293, 286, 603, 536, 291, 294, 264, 958, 472, 13, 51784], "temperature": 0.0, "avg_logprob": -0.09136976426647556, "compression_ratio": 1.7080745341614907, "no_speech_prob": 0.002672586590051651}], "language": "en"}