WEBVTT

00:00.000 --> 00:02.960
So I think we're going to live in a world where there are going to be hundreds of millions

00:02.960 --> 00:07.760
and billions of different AI agents, eventually probably more AI agents than there are people

00:07.760 --> 00:08.760
in the world.

00:08.760 --> 00:09.760
All right.

00:09.760 --> 00:12.440
So Lama 3.1 was just released today.

00:12.440 --> 00:17.120
And along with that, there has been a ton of additional information released from meta

00:17.120 --> 00:19.720
about how they think about open source.

00:19.720 --> 00:25.680
Mark Zuckerberg wrote a letter, which I will also be covering, but he also did a 30 plus

00:25.680 --> 00:27.800
minute interview with Rowan Chang.

00:27.800 --> 00:31.440
And so today we're going to watch the video together and I'm going to give you my thoughts

00:31.440 --> 00:32.440
on it.

00:32.440 --> 00:36.760
Can I also just say I am absolutely loving Mark Zuckerberg's current vibes.

00:36.760 --> 00:41.500
His transformation is going to be studied in business school for decades to come.

00:41.500 --> 00:42.500
So let's keep watching.

00:42.500 --> 00:43.500
Okay.

00:43.500 --> 00:48.040
In this first section, he is going to introduce the Lama 3.1 launch and it'll be in his

00:48.040 --> 00:49.640
own words and what he thinks about it.

00:49.640 --> 00:50.640
So let's watch.

00:50.640 --> 00:54.320
I mean, the big release today, first of all, happy to be doing this big fan of what you

00:54.320 --> 00:56.000
do.

00:56.000 --> 01:01.600
The big release today is Lama 3.1 and we're releasing three models.

01:01.600 --> 01:05.640
The first time we're releasing a 405 billion parameter model.

01:05.640 --> 01:11.000
So it's by far the most sophisticated open source model that I think anyone has put out.

01:11.000 --> 01:14.920
And it really kind of is competitive with some of the leading close models and in some

01:14.920 --> 01:15.920
areas is even ahead.

01:15.920 --> 01:21.440
So I'm really excited to see what people do with that, especially now that we're making

01:21.440 --> 01:26.680
it so that the community policies around Lama allow people to use it as a teacher model

01:26.680 --> 01:32.560
to distill and fine tune and basically create whatever other models they want with it.

01:32.560 --> 01:33.760
I already have to pause it.

01:33.760 --> 01:37.160
So he said a lot just in those few words.

01:37.160 --> 01:41.680
First of all, the 405 billion parameter model is outstanding.

01:41.680 --> 01:46.120
It is leaps and bounds more sophisticated than any other open source model out there.

01:46.120 --> 01:51.360
It is directly competitive with closed source frontier models and it is really the first

01:51.360 --> 01:54.480
open source model that can be considered frontier.

01:54.480 --> 02:00.920
Also all of this is a direct shot at open AI and other closed source companies.

02:00.920 --> 02:03.880
And he is taking a tried and true playbook.

02:03.880 --> 02:06.560
Microsoft did this for years.

02:06.560 --> 02:11.000
Basically when you're behind in a technology race, the strategy that you employ is called

02:11.000 --> 02:12.840
scorched earth.

02:12.840 --> 02:17.600
Essentially invest a ton of money into replicating whatever that technology is and then release

02:17.600 --> 02:18.800
it for free.

02:18.800 --> 02:23.680
Because at that point it becomes ubiquitous, it becomes a commodity and why would anybody

02:23.680 --> 02:28.680
go and pay premium prices for a closed source frontier model when you can have full control

02:28.680 --> 02:31.920
over the model and pay fractions of the price.

02:31.920 --> 02:35.160
Plus you are increasing competition greatly.

02:35.160 --> 02:39.440
And so you have companies like grok, GROQ who are able to take this large model and

02:39.440 --> 02:44.800
run it at inference speeds that are absolutely incredible orders of magnitude greater than

02:44.800 --> 02:46.280
what open AI can run at.

02:46.280 --> 02:50.080
And by the way, I don't think open AI is down for the count by any means.

02:50.080 --> 02:52.480
Sam Altman is a brilliant operator.

02:52.480 --> 02:58.520
And this is likely why they just released GPT-40 mini, a model that is as performant

02:58.520 --> 03:03.040
as GPT-40 but for a fraction of the price and it's much faster.

03:03.040 --> 03:04.840
So they're getting the message.

03:04.840 --> 03:06.600
They see the writing on the wall.

03:06.600 --> 03:10.200
Models are becoming commodities and I just read an article and I'm going to talk about

03:10.200 --> 03:14.680
this in another video that open AI is building their own AI chips.

03:14.680 --> 03:18.600
And again, that's really where a lot of the differentiation comes from.

03:18.600 --> 03:22.400
That is why GROQ is so special because of their unique chip design that allows them

03:22.400 --> 03:25.960
to run inference at blazing fast speeds.

03:25.960 --> 03:30.880
And open AI wants to accomplish the same thing because especially after this release, models

03:30.880 --> 03:31.880
are a commodity.

03:31.880 --> 03:32.880
Let's keep watching.

03:32.880 --> 03:38.160
In addition to that, we've distilled the 405 billion parameter model down to make newer

03:38.160 --> 03:44.360
and updated and now leading for their size 70 billion and 8 billion parameter models.

03:44.400 --> 03:48.640
So that's actually one thing I didn't quite realize even after reading the announcement,

03:48.640 --> 03:53.840
they took the 405 billion parameter model and distilled it down into the 3.1 versions of

03:53.840 --> 03:56.240
the 8 billion and 70 billion parameter models.

03:56.240 --> 03:57.240
I didn't know that.

03:57.240 --> 04:01.680
Does that mean that the 8 billion and 70 billion parameter models that first came out the three

04:01.680 --> 04:05.560
version were standalone models and now these are completely new models?

04:05.560 --> 04:06.720
I'm not actually sure.

04:06.720 --> 04:08.280
So if you know, let me know in the comments.

04:08.280 --> 04:12.520
Yeah, I mean, taking a step back, I think this is a pretty big moment for open source AI.

04:12.680 --> 04:16.640
Yeah, I've been reflecting on this and I kind of think it's, you know, I thought for a while

04:16.640 --> 04:20.160
that open source AI was going to become the industry standard.

04:20.160 --> 04:25.880
And I thought that it would basically follow the path that Linux did, where, you know,

04:25.880 --> 04:30.120
if you just go back to before Linux was popular, there, you know, there were all these companies

04:30.120 --> 04:34.520
that had their own closed versions of Unix and at the time, you know, there's nothing

04:34.520 --> 04:39.240
that was sort of that sophisticated that had ever been done as an open source project.

04:39.240 --> 04:42.600
And people thought, hey, no, this is like the closed model of development is the only

04:42.600 --> 04:44.120
way to do something that's this advanced.

04:44.120 --> 04:51.080
And at first Linux kind of got its foothold because it was cheaper because developers

04:51.080 --> 04:52.680
could customize it in different ways.

04:52.680 --> 04:57.240
And then over time as the ecosystem built out, it, you know, got more scrutiny.

04:57.240 --> 04:59.280
So it actually became the more secure one.

04:59.280 --> 05:00.760
It became the more advanced one.

05:00.760 --> 05:05.120
There were more partners that basically built more capabilities in the case of Linux, more

05:05.120 --> 05:09.800
developers, and things like that, that basically ended up making it have more capabilities as

05:09.800 --> 05:11.280
well than any closed source Unix.

05:11.280 --> 05:18.840
So I think that this moment with Lama 3.1 is kind of like that inflection point where

05:18.840 --> 05:24.880
I think Lama has the opportunity to become the open source AI standard for open source

05:24.880 --> 05:28.280
to become the standard, the industry standard for AI.

05:28.280 --> 05:33.720
And even in the places where it's not yet ahead on performance, it leads on, on kind

05:33.720 --> 05:38.600
of cost and on, on customized ability and on the ability to take the model and fine tune

05:38.600 --> 05:40.960
it and do all the things that you want with it.

05:40.960 --> 05:46.000
So he also mentioned, and I forgot to mention this in my last comments, but he also mentioned

05:46.000 --> 05:51.160
that they changed the license and now with the 405 billion perimeter model, they allow

05:51.160 --> 05:56.160
you to create synthetic data from that model to train smaller models.

05:56.160 --> 06:01.600
So that is a huge change and extremely valuable for the ecosystem.

06:01.600 --> 06:04.520
This is what NVIDIA did with their Nemetron model.

06:04.520 --> 06:11.200
They trained a massive model that generates data and has the permission to do so to train

06:11.200 --> 06:12.200
smaller models.

06:12.200 --> 06:17.320
And this is going to allow a lot of companies, a lot of AI model companies as well to make

06:17.320 --> 06:21.080
their own versions of these models, dependent on the use case.

06:21.080 --> 06:26.080
And that is a really cool strategy and something again, I really appreciate from meta.

06:26.080 --> 06:29.320
I think that those are just huge advantages that that we're going to see developers take

06:29.360 --> 06:33.160
and we're focusing on building out this partner ecosystem and there are going to be all these

06:33.160 --> 06:35.040
different capabilities that get built out around it.

06:35.040 --> 06:36.440
So yeah, and that's another thing.

06:36.440 --> 06:39.720
He's not doing this just to screw over the closed source companies.

06:39.720 --> 06:44.240
He actually believes in this as a true business strategy.

06:44.240 --> 06:49.560
If you build out the foundation for other companies to come build on top of you, then

06:49.560 --> 06:54.520
of course you get to set the standards and then you'll figure out ways to monetize over

06:54.520 --> 06:55.520
time.

06:55.520 --> 06:56.520
That's essentially what they did with Facebook.

06:56.520 --> 06:59.560
They built out a platform, other developers came and built on top of it.

06:59.560 --> 07:03.680
Now the counter example to that is Apple with the App Store.

07:03.680 --> 07:08.120
They built a completely closed system and then other developers because it was so popular

07:08.120 --> 07:10.040
came and built on top of it.

07:10.040 --> 07:13.840
And there's the alternative Android, which is the open version of the smartphone operating

07:13.840 --> 07:14.840
system.

07:14.840 --> 07:18.600
So we needed a strong open source player in AI and now we have it.

07:18.600 --> 07:19.600
All right.

07:19.600 --> 07:22.560
Next is something you all ask me about all the time whenever I put out a tutorial, whenever

07:22.560 --> 07:26.480
I put out some kind of news, you always ask me, okay, but what's the real world use case

07:26.480 --> 07:30.520
and that's something that I've been trying to include more and more in my videos.

07:30.520 --> 07:34.280
So now what Mark is going to talk about in this segment is what he sees as the real world

07:34.280 --> 07:35.280
use cases.

07:35.280 --> 07:38.960
He's probably going to start with the things that we all know are pretty obvious and are

07:38.960 --> 07:42.560
the basic intro use cases for AI.

07:42.560 --> 07:46.560
But I'm hoping he's also going to talk about the more sophisticated use cases and there's

07:46.560 --> 07:50.720
probably a ton of use cases we haven't even thought of yet or that the capabilities aren't

07:50.720 --> 07:52.320
quite there yet until today.

07:52.320 --> 07:53.320
So let's watch.

07:53.320 --> 08:00.320
The thing that I'm most excited about is seeing people use it to distill and fine tune their

08:00.320 --> 08:01.320
own models.

08:01.320 --> 08:02.320
Right.

08:02.320 --> 08:05.560
It's, I mean, like you're saying, I mean, this is the first open source frontier level

08:05.560 --> 08:08.400
model, but it's not the first frontier level model.

08:08.400 --> 08:12.240
So there have been other models that sort of have that capacity and yeah, people are going

08:12.240 --> 08:17.000
to want to do inference directly on the four or five because it's, you know, by our estimates,

08:17.000 --> 08:22.400
it's going to be at 50% cheaper, I think, than the GPT four or to do that directly.

08:22.480 --> 08:26.040
And so I think that that obviously makes a difference to a lot of people.

08:26.040 --> 08:30.480
But the thing that I think is really new in the world with this is the, because it's

08:30.480 --> 08:33.040
open weights, the ability to take the model.

08:33.040 --> 08:38.600
And by the way, it was rumored that Meta was not going to release the weights for the 405

08:38.600 --> 08:41.240
billion parameter model, but Mark Zuckerberg corrected them.

08:41.240 --> 08:44.000
And it seems like he held true to his promise.

08:44.000 --> 08:45.000
They did release it.

08:45.000 --> 08:46.720
It is completely open source.

08:46.720 --> 08:52.240
And I can truly say that completely open source and distill it down to whatever size

08:52.280 --> 08:56.840
that you want to use it for synthetic data generation, to use it as a teacher model.

08:57.640 --> 09:01.000
You know, so our vision for the future, it's not just, okay, it was never that there's

09:01.000 --> 09:02.280
going to be one singular thing.

09:02.280 --> 09:05.880
I think this is like open AI sort of as this vision that they're going to build kind of

09:05.880 --> 09:10.320
one big AI, Anthropic does to Google does to it's never been our vision.

09:10.320 --> 09:13.480
Our vision is that there should be lots of different models.

09:13.480 --> 09:19.160
I think every startup out there, every enterprise governments, they all kind of want to have

09:19.400 --> 09:21.280
their own custom models.

09:21.640 --> 09:26.000
And yeah, when the closed ecosystem was so much better than open source, it was just

09:26.000 --> 09:30.640
better to take the vanilla closed thing off the shelf, because even though you could

09:30.640 --> 09:33.960
customize open source, there was still some gap between the performance that you could

09:33.960 --> 09:35.600
get, but now we don't see that anymore.

09:35.600 --> 09:38.720
So this is actually something that I believe in really strongly.

09:38.720 --> 09:42.640
And if you've watched my videos as of the last few weeks and months, you've heard me

09:42.640 --> 09:48.960
say it, I truly believe that small vertical narrow use case models

09:49.160 --> 09:53.560
are going to be the future, especially, and again, something else I've talked about as

09:53.560 --> 09:57.560
AI compute continues to get pushed towards edge devices.

09:57.600 --> 10:02.880
The only way we're going to be able to have capable AI is by running small models, multiple

10:02.880 --> 10:05.160
small models on a device like this.

10:05.200 --> 10:10.360
And when we also have algorithmic innovations like route LLM and mixture of agents, all

10:10.360 --> 10:15.720
of a sudden, these small models become so much better and know when to offload to these

10:15.880 --> 10:19.320
giant kind of world knowledge models, if they have to.

10:19.360 --> 10:24.840
And funny, that is the approach that Apple intelligence took, but they just took the

10:24.840 --> 10:29.440
completely closed ecosystem approach, which is very Apple to do as open source basically

10:29.440 --> 10:30.320
closes the gap.

10:30.720 --> 10:37.000
I think you're just going to see this wide proliferation of models where people now have

10:37.000 --> 10:41.440
the incentive to basically customize and build and train exactly the right size model for

10:41.440 --> 10:45.120
what they're doing, train their data into it, they're going to have the tools to do it

10:45.120 --> 10:49.720
because of a lot of the partner integrations that the companies like Amazon are doing with

10:49.720 --> 10:54.440
AWS or Databricks or different folks like that, who are building these whole suites

10:54.440 --> 10:57.680
of services for distilling and fine tuning open models.

10:57.680 --> 10:59.920
So I think that that's going to be the thing that's new here.

10:59.920 --> 11:02.720
And that's really exciting is how far can that get pushed?

11:02.760 --> 11:06.800
And and that's a completely new capability in the world because there hasn't been an

11:06.800 --> 11:12.040
open source or open weight model of kind of this sophistication that's ever been released

11:12.040 --> 11:12.480
before.

11:12.520 --> 11:12.760
Yeah.

11:12.760 --> 11:14.520
And that's a really important point.

11:14.840 --> 11:20.360
The battle for unique and diverse data is really going to be the front lines of artificial

11:20.360 --> 11:21.000
intelligence.

11:21.000 --> 11:26.720
That is why, as I mentioned in a recent previous video, open AI has been building partnerships

11:26.720 --> 11:30.520
with numerous content companies like Time Magazine and so on.

11:30.600 --> 11:36.000
And so if you're a company and you have this very unique, very proprietary data set, you can

11:36.000 --> 11:43.840
now take these large frontier open source models and train them on your particular use case for

11:43.840 --> 11:48.320
your business, whether you want to use it internally or resell it to your industry.

11:48.320 --> 11:50.960
And I think that's a really interesting approach.

11:51.080 --> 11:51.320
All right.

11:51.320 --> 11:56.640
In this next section, Rowan asks Mark Zuckerberg about how they're going to teach the world

11:56.640 --> 12:00.160
how to use these AI models and specifically about open source and its benefits.

12:00.160 --> 12:01.480
So let's watch what he has to say.

12:01.560 --> 12:01.840
Yeah.

12:01.840 --> 12:06.680
So I'd say before a 3.1 hour approach, I mean, the reason that Meta fundamentally is

12:06.680 --> 12:11.480
investing in this is we basically want to know that we have access to, to a leading

12:11.480 --> 12:17.040
model, you know, because of some of our, our history of kind of how mobile worked and

12:17.040 --> 12:22.240
things like that, we didn't want to be in a position where we had to rely on some competitor.

12:22.280 --> 12:22.720
All right.

12:22.800 --> 12:26.080
So he kind of revealed why he's employing this strategy.

12:26.360 --> 12:31.360
During the mobile revolution, Facebook got caught flat footed.

12:31.360 --> 12:36.840
They basically were completely platform dependent on Apple or Android.

12:36.840 --> 12:41.800
And they even tried to make their own phone at a certain point, although it was a failed

12:41.800 --> 12:42.480
project.

12:42.480 --> 12:46.680
So he is obviously not wanting to make that same mistake again.

12:46.720 --> 12:47.920
We built it for ourselves.

12:48.280 --> 12:53.680
And before a 3.1, you know, we, we kind of had this instinct that if we made it open

12:53.680 --> 12:57.400
source, there would be a community that would grow around it and that would actually extend

12:57.400 --> 12:59.920
the capabilities and make it more valuable for everyone, including us.

13:00.120 --> 13:02.120
Because at the end of the day, this isn't just a technology.

13:02.120 --> 13:03.360
It's an ecosystem, right?

13:03.360 --> 13:04.680
That, that, that you're developing.

13:04.680 --> 13:08.280
So in order for this to end up being a useful thing for us, there also needs to be a broad

13:08.280 --> 13:08.880
ecosystem.

13:09.520 --> 13:13.400
One of the big changes that I think we see with llama 3.1 is instead of just building

13:13.400 --> 13:17.600
it for ourselves and throwing it over the wall and letting developers use it, this time

13:17.600 --> 13:23.840
we're really taking a much more proactive stance on building partnerships and making

13:23.840 --> 13:29.240
sure that there's this whole ecosystem of companies that can do interesting things with

13:29.240 --> 13:31.840
the model and conserve developers in ways that we're not going to.

13:31.960 --> 13:36.480
Okay, really, what that translates into is they want control of the ecosystem.

13:36.480 --> 13:38.440
They want to be able to define the standards.

13:38.680 --> 13:40.720
They want to build up the ecosystem.

13:40.720 --> 13:44.800
So there's obviously a financial motive for meta and it's not just purely out of the

13:44.800 --> 13:48.080
goodness of their heart that they release this model for free, but that's okay.

13:48.120 --> 13:49.400
Everybody can still win.

13:49.480 --> 13:53.360
And quickly, I want to show Mark Zuckerberg talking about GROC, which is, as you know,

13:53.360 --> 13:56.600
one of my favorite companies out there right now in the world of AI.

13:56.640 --> 14:00.720
The time I think that there are also going to be folks like GROC, right, who are doing

14:00.760 --> 14:06.840
really interesting work on really kind of ultra low latency inference.

14:06.840 --> 14:08.840
And I'm really excited to get this in their hands.

14:08.840 --> 14:11.400
And they're building something for launch that basically.

14:11.520 --> 14:15.040
Okay, so obviously this was filmed a few days before launch, if not more.

14:15.080 --> 14:18.120
And GROC on day one already has it available.

14:18.360 --> 14:21.160
I haven't been able to use it because there's been so many people trying.

14:21.160 --> 14:23.120
It basically says they're bandwidth limited.

14:23.120 --> 14:25.400
But as soon as I can, I'm definitely going to try it out.

14:25.440 --> 14:29.720
Next, Rowan asks Mark Zuckerberg what the implications are of open source AI.

14:29.720 --> 14:34.320
And this has been a fierce debate amongst obviously close source companies like

14:34.320 --> 14:38.960
open AI and open source companies like Meta AI, Mark Zuckerberg, Yann LeCun.

14:39.080 --> 14:44.960
And what I really appreciate about the open source approach is that it hopefully puts

14:45.000 --> 14:49.640
a stop to any regulatory capture that might be happening and likely is happening

14:49.640 --> 14:52.680
from the likes of Google and open AI.

14:52.920 --> 14:54.840
They want regulation.

14:54.840 --> 15:00.680
They want the hurdle to start a new AI company, a new innovative frontier AI model

15:00.680 --> 15:04.200
to be as high as possible because they're already on that side of the fence.

15:04.320 --> 15:06.360
They don't want anybody else at their party.

15:06.440 --> 15:14.440
My view is that open source is a really important ingredient to having a positive AI future.

15:14.600 --> 15:18.200
And that there are all these awesome things that AI is going to bring in terms

15:18.200 --> 15:22.200
of productivity gains and creativity enhancements for people.

15:22.200 --> 15:24.800
And hopefully it'll help us with research and things like that.

15:25.120 --> 15:29.440
But I think open source is an important part of how we make sure that this benefits

15:29.440 --> 15:31.480
everyone and is accessible to everyone.

15:31.760 --> 15:35.600
It isn't something that's just locked into a handful of big companies.

15:35.800 --> 15:40.760
At the same time, I actually think that open source is going to end up being the

15:40.760 --> 15:43.480
safer and more secure way to develop AI.

15:43.720 --> 15:47.360
I know that there's sort of a debate today about is open source safe?

15:47.760 --> 15:49.920
And I actually take the different position on it.

15:49.920 --> 15:51.000
It's not only do I think it's safe.

15:51.000 --> 15:53.360
I think it's safer than the alternative of closed development.

15:53.360 --> 15:56.720
And yeah, so I'm going to cut him off for a second because I kind of already know

15:56.720 --> 16:00.120
where he's going, something I've already talked about in previous videos.

16:00.160 --> 16:02.680
And he actually talked about it a little bit earlier in this video.

16:02.800 --> 16:07.320
Basically, when you open source something and everybody with diverse skills,

16:07.320 --> 16:12.960
diverse perspectives and a much larger pool of talent can look and examine

16:13.000 --> 16:16.320
every single line of code, every single piece of data, how it's behaving,

16:16.320 --> 16:19.880
why it's doing certain things, you're going to harden the system.

16:19.960 --> 16:23.280
Much more so than closed source systems.

16:23.320 --> 16:25.840
Now, there's some examples where that's not the case.

16:25.840 --> 16:29.560
There's some examples where that is the case, but I think generally speaking,

16:29.800 --> 16:33.160
open source tends to be more secure than closed source systems.

16:33.200 --> 16:35.200
If you agree, let me know in the comments.

16:35.200 --> 16:37.800
I'm not sure if that's even a controversial statement or not.

16:37.840 --> 16:41.240
You know, I sort of break it down into, you know, there are lots of different

16:41.240 --> 16:44.920
kinds of harm, so you can't just talk about one type of thing.

16:45.200 --> 16:48.920
But on this, I think that there's unintentional harms.

16:48.920 --> 16:53.000
So the system goes off the rails in some way that people didn't intend.

16:53.520 --> 16:56.440
And then there's intentional harms where you have like some bad actors

16:56.440 --> 16:59.800
trying to use the system to do something bad when it comes to unintentional harms,

16:59.800 --> 17:03.320
which I think, by the way, it's worth noting that like most of the sci-fi

17:03.320 --> 17:08.360
scenarios that people worry about of AI just going rogue are kind of unintentional.

17:08.640 --> 17:13.520
I actually think that open source should be safer on that because it's

17:14.080 --> 17:16.680
it will have more scrutiny, they'll have more transparency.

17:17.640 --> 17:21.640
And I think all the developers who use it with all the Lama Guard

17:21.640 --> 17:23.640
and the safety tools that it comes with,

17:25.080 --> 17:27.720
there's going to be so much scrutiny and testing and pressure on those

17:27.960 --> 17:32.480
that my guess is that it will have kind of just like traditional open

17:32.480 --> 17:36.280
source software, any kind of issues with it, I think will be ironed out

17:36.280 --> 17:39.840
and fixed a lot quicker than with the closed models.

17:39.840 --> 17:43.080
So I think you've got you've got that on kind of unintentional harm,

17:43.080 --> 17:46.360
which is why I think most of the discussion around safety for open

17:46.360 --> 17:49.320
source revolves around intentional harm. It's OK, it's open.

17:49.320 --> 17:52.280
It's out there. How are you going to stop bad actors from doing it?

17:52.680 --> 17:53.840
Doing bad things with it?

17:53.840 --> 17:57.040
There, I think you basically want to probably divide the problem

17:57.760 --> 18:00.960
into kind of smaller actors like an individual or

18:01.840 --> 18:06.040
or some kind of smaller group that's trying to create some some some mayhem

18:06.640 --> 18:10.600
and the larger actors who are more sophisticated and have huge amounts

18:10.600 --> 18:12.680
of resources like big nation states.

18:12.720 --> 18:15.240
I think it's kind of a different mix for the two of those.

18:15.400 --> 18:18.960
This reminds me of something that Jan LeCun talked about in his interview

18:18.960 --> 18:21.840
with Lex Friedman. I'm actually forgetting the name he called this,

18:21.840 --> 18:26.680
but he essentially said that if everybody has open source frontier models

18:26.840 --> 18:29.800
that are incredibly capable, as capable as closed source,

18:30.080 --> 18:33.800
then it's basically a battle of AI versus AI.

18:33.800 --> 18:37.760
If there's a bad actor and a good actor with AI, whoever has the better AI

18:37.760 --> 18:42.160
is going to win. And if both sides have equal AI, or if they're just off

18:42.160 --> 18:47.360
by, let's say, 5% in terms of capabilities, then it's pretty much null.

18:47.560 --> 18:51.240
And one AI is going to protect the good actors from the bad actors.

18:51.240 --> 18:55.080
AI. Now, the instance that we're not talking about,

18:55.080 --> 19:00.000
which I actually don't think is possible is if bad actors with huge resources

19:00.000 --> 19:05.720
suddenly get a massive jump in capabilities in AI

19:05.760 --> 19:07.760
that nobody else even thought was possible.

19:07.920 --> 19:15.320
So imagine all of a sudden a bad actors AI is 100% better than the good actors AI.

19:15.320 --> 19:17.640
At that point, we'd probably be in trouble.

19:17.640 --> 19:21.440
But as I said, I don't even believe that's possible just because of the iterative

19:21.440 --> 19:24.440
nature of artificial intelligence innovation.

19:24.480 --> 19:28.320
There just aren't these huge step functions in capability gain.

19:28.440 --> 19:32.160
You know, for the smaller actors, my view on this is that, you know,

19:32.160 --> 19:36.240
the way that we've, I think that having a balance of power on this is super important.

19:36.840 --> 19:39.400
You know, what we've done in managing our social networks is we have all these

19:39.400 --> 19:43.240
kind of bad actors who are trying to do kind of bad stuff on our networks.

19:43.240 --> 19:46.240
And a lot of times they deploy AI systems to do that.

19:46.240 --> 19:48.960
And the way that we stop them and identify them is by having more

19:48.960 --> 19:53.080
sophisticated AI systems that have more compute to go find what they're doing.

19:53.400 --> 19:57.160
So I think that this is actually pretty similar to the governments and law

19:57.160 --> 20:00.000
enforcement essentially maintain order in society.

20:00.200 --> 20:03.240
It's like, yeah, you have a bunch of rogue people who might be committing crimes.

20:03.480 --> 20:07.360
But, you know, generally the police forces and the militaries are much

20:07.360 --> 20:09.000
better funded, have more resources.

20:09.240 --> 20:10.880
And I think that that's basically going to be true here.

20:11.200 --> 20:15.000
As a matter of fact, I think what you want is for open source to be widely deployed,

20:15.000 --> 20:18.160
which I think that there's sort of a risk if it's closed, that that's not the case.

20:18.160 --> 20:21.000
But when it's open, you're going to have all these big institutions

20:21.320 --> 20:25.200
that have a ton of resources that they can basically deploy these systems

20:25.200 --> 20:27.720
in a way that I think will check bad actors.

20:27.720 --> 20:32.760
Then you get to the question of basically, you know, folks like

20:32.760 --> 20:35.360
China or like large, sophisticated actors.

20:35.680 --> 20:38.640
And one of the questions that you sometimes hear debated is like, OK,

20:38.800 --> 20:42.000
if you're open sourcing the really advanced models, how do you make it so

20:42.000 --> 20:46.360
that that it doesn't get to to China or they're not going to use that against us?

20:46.360 --> 20:49.920
And that's sometimes an argument that people have for, hey, you should lock down development.

20:49.920 --> 20:52.480
But I think that that's sort of missing a few things.

20:52.760 --> 20:56.360
One is that in order for this all to work, the US has to have an advantage

20:56.360 --> 20:58.480
in the first place or the West.

20:58.480 --> 21:03.600
And in kind of our advantage is basically open and decentralized innovation,

21:03.600 --> 21:06.880
where it's not just a small number of big companies or labs at startups

21:06.880 --> 21:11.200
and universities and individuals hacking on things who are in parts of companies.

21:11.200 --> 21:12.600
And that's a big part of it.

21:12.600 --> 21:13.960
And you don't want to shut that down.

21:13.960 --> 21:17.280
And I think if you do, you increase the chance that we don't even lead in the first place.

21:17.640 --> 21:22.840
But then I think you get to the the issue, which is, OK, China or not even China,

21:22.840 --> 21:27.480
any government, you know, there are all the risks of kind of stealing the models

21:27.480 --> 21:31.240
and an espionage, I mean, a lot of the models fit on, you know, a hard drive

21:31.240 --> 21:34.360
that you can quickly put in your backpack or whatever.

21:34.360 --> 21:40.920
And it's I just think we need to be realistic about how likely it is that we can secure

21:41.520 --> 21:45.000
and not just not us, but like any of the tech companies can secure any of these

21:45.680 --> 21:49.240
models long term against very sophisticated efforts to do that.

21:49.360 --> 21:54.040
So if you remember my video about the situational awareness paper by Leopold

21:54.080 --> 21:58.080
Aschenbrenner, he specifically calls out China, but let's just use any large state

21:58.080 --> 22:05.080
actor, the chances of being able to protect a closed source model, the weights

22:05.240 --> 22:08.800
indefinitely is essentially zero in my mind.

22:08.920 --> 22:13.600
How many times have US companies been hacked specifically by China and their IP stolen?

22:13.880 --> 22:16.640
And here's the thing, it only needs to happen once.

22:16.960 --> 22:23.120
You only need to have one minor lapse in security and then you lose the model weights.

22:23.240 --> 22:28.120
So with the likelihood of a private company like open AI getting hacked, having their

22:28.120 --> 22:33.600
model weights stolen, then let's just assume state actors are going to have these AI models.

22:33.600 --> 22:36.880
So let's just make sure everybody has them.

22:37.000 --> 22:41.280
Let's make sure all of the good guys, quote unquote, good guys, quote unquote, bad guys,

22:41.560 --> 22:44.320
they all have the same capabilities.

22:44.320 --> 22:49.560
And thus we're back to the original statement of it's one AI versus another AI.

22:49.720 --> 22:55.440
My own fear is that if we lock down development, we end up in a world where basically you have

22:55.440 --> 22:59.640
a small number of companies, plus all the adversaries who can steal the model are the

22:59.640 --> 23:00.720
only ones who have access.

23:01.000 --> 23:05.440
But all the startups, all the universities, all the individual hackers are kind of just

23:05.440 --> 23:07.720
left out and don't have the ability to do this.

23:07.720 --> 23:16.400
So my own view is that a realistic aim that we should hope for is that we use open

23:16.400 --> 23:20.920
source to basically develop the leading and most robust ecosystem in the world, in

23:20.920 --> 23:27.160
that we have an expectation that our companies work closely with our government and allied

23:27.160 --> 23:32.200
governments on national security so that way our governments can persistently just be

23:32.200 --> 23:36.240
integrating the latest technology and have, you know, whatever it is, a six month advantage,

23:36.240 --> 23:38.040
eight month advantage on our adversaries.

23:38.040 --> 23:41.280
And I think that that's, you know, I don't know that that in this world, you get a 10

23:41.280 --> 23:46.240
year permanent advantage, but I think a kind of perpetual lead actually will make us more

23:46.240 --> 23:50.960
safe in one where we're leading, then the model that others are advocating, which is,

23:50.960 --> 23:55.080
okay, you have a small number of closed labs, they lock down development, we probably risk

23:55.080 --> 23:59.520
being in the lead at all, like probably the other governments are getting access to it.

23:59.800 --> 24:01.160
It's that that's my view.

24:01.200 --> 24:06.680
I actually think on both these things, spreading prosperity for more evenly around the world,

24:06.680 --> 24:10.680
making it so that there can be more progress and on safety, I think we're basically just

24:10.680 --> 24:12.520
going to find over time that open source leads.

24:12.720 --> 24:14.320
Look, there are going to be issues, right?

24:14.320 --> 24:15.600
It's like, well, to mitigate the issues.

24:15.600 --> 24:16.880
We're going to test everything rigorously.

24:16.880 --> 24:18.800
We do, we work with governments and all the stuff.

24:19.080 --> 24:20.160
We'll continue doing that.

24:20.520 --> 24:24.440
Um, but that's my view of kind of where the equilibrium, I think we'll settle out, given

24:24.440 --> 24:25.160
what I know today.

24:25.240 --> 24:30.520
So next, Mark Zuckerberg is going to start talking about economic possibilities with

24:30.520 --> 24:31.560
the use of AI.

24:31.560 --> 24:35.600
And this is something that I'm extremely curious and excited about.

24:35.640 --> 24:37.400
I tend to be an optimist.

24:37.440 --> 24:41.920
I can also see the pessimist point of view here and open AI just came out with a bunch of

24:41.920 --> 24:45.880
research talking about UBI and economic effects of AI.

24:45.880 --> 24:48.800
And I'm going to be reading and covering that in a subsequent video.

24:48.960 --> 24:51.520
But for now, let's see what Mark Zuckerberg has to say about it.

24:51.600 --> 24:56.040
There's a version of this, which AI will do no matter how it's developed.

24:56.640 --> 25:00.600
Um, and then there's a version of this that I think benefits from open source specifically.

25:00.600 --> 25:05.040
So I think that AI has more potential than any other single technology that's

25:05.040 --> 25:10.040
being developed right now to increase productivity, accelerate the economy, um,

25:10.040 --> 25:14.200
make it set kind of every person has the ability to be more creative and, and,

25:14.440 --> 25:16.320
and produce more interesting things.

25:16.320 --> 25:17.800
And I think that that's all going to be great.

25:17.800 --> 25:21.880
I also think I hope that it'll help out with science and, um, medical research

25:21.880 --> 25:23.000
and things like that.

25:23.040 --> 25:27.920
There are a lot of folks today, though, who don't necessarily have access to the

25:27.920 --> 25:31.640
ability to fine tune or build their own state of the art models.

25:31.720 --> 25:35.120
So they're sort of limited to what these large labs do.

25:35.520 --> 25:39.760
Um, and like I just said, I think, um, you know, one of the defining

25:39.880 --> 25:44.680
aspects of our culture and innovation as a sort of a country or, or society is

25:44.680 --> 25:47.200
like, it's not just big companies that do it, right?

25:47.200 --> 25:50.680
There's all these startups and hackers and academics and people in university.

25:51.040 --> 25:55.800
And I think you want to give all of those folks access to state of the art models

25:55.800 --> 25:58.720
that they can build on top of, not just that they can run, which is what they have

25:58.720 --> 26:03.400
today with, with the closed vendors, but that they can build on top of and tweak

26:03.400 --> 26:07.480
and distill down to smaller models that they can run on their laptop or their

26:07.480 --> 26:09.840
phone or whatever other device they're building.

26:09.840 --> 26:12.560
And I think that that's just going to unlock a ton of progress.

26:12.920 --> 26:17.040
There's also a version of this where there are, you can look at it by, you know,

26:17.040 --> 26:18.160
nation too.

26:18.600 --> 26:21.760
Um, you know, so it's not just that startups might not have the resources

26:21.760 --> 26:25.180
or universities might not have the resources to go train their own, um,

26:25.200 --> 26:28.280
they know, large scale foundation models now or in the future.

26:28.560 --> 26:31.280
But, um, but there were a lot of countries that aren't going to have

26:31.280 --> 26:34.840
the ability to do that because I mean, you know, pretty soon these things

26:34.840 --> 26:37.440
are going to cost many billions of dollars to train.

26:37.480 --> 26:42.160
And I think that having the ability for different countries and entrepreneurs

26:42.160 --> 26:47.080
and different countries and businesses to use it to serve people better and just

26:47.080 --> 26:52.080
do better work is going to be something that, that basically like lifts all boats

26:52.080 --> 26:56.640
around the world and, um, which just has a massive kind of equalizing effect.

26:56.640 --> 26:58.640
So I know that that's really positive.

26:58.680 --> 27:02.760
Here, Rowan asked Mark about specifically in his letter, which I'll

27:02.800 --> 27:07.480
cover in another video, but he directly called out Apple in their closed approach

27:07.480 --> 27:11.040
and Rowan asked him to elaborate on it and what are his thoughts.

27:11.040 --> 27:11.760
So let's listen.

27:11.880 --> 27:17.360
I mean, my point in there is more, it's a little more philosophical on how

27:17.360 --> 27:23.760
it's affected my own kind of approach towards things and, and psychologically

27:23.760 --> 27:26.000
sort of affected how I think about building stuff.

27:26.440 --> 27:29.080
Um, I actually don't know how they're going to approach AI.

27:29.360 --> 27:31.360
Um, you know, they do some open development.

27:31.360 --> 27:32.600
I do some closed development.

27:32.920 --> 27:35.560
Um, you know, by the way, I think it's worth noting, like, I don't actually

27:35.560 --> 27:37.360
consider myself to be an open source as Ellen.

27:37.360 --> 27:40.520
I just think that in this case, um, I think that open models are going to be

27:40.520 --> 27:42.520
the standard and I think that that's going to be good for the world, but we

27:42.520 --> 27:44.240
do open development, we do close development.

27:44.240 --> 27:44.960
So I get it, right?

27:44.960 --> 27:47.680
And I'm not saying that Apple is necessarily going to be on the wrong place

27:47.680 --> 27:52.440
on this for AI, but if you look back over the last 10 or 15 years, it has

27:52.440 --> 27:58.240
been a formative experience for us is building our services on top of platforms

27:58.240 --> 28:03.120
that are controlled by our competitors and for a number of different incentives.

28:03.200 --> 28:06.800
Oh, I can just sense, I can just sense the anger here.

28:06.880 --> 28:12.160
And I can tell he's being very diplomatic about the way he's saying all of this.

28:12.480 --> 28:18.160
But I know he got burned and, and burned for years on the fact that he had to

28:18.160 --> 28:23.920
build his platforms during the mobile revolution, which really took over everything.

28:23.920 --> 28:29.040
He had to build Facebook and the rest of his app portfolio on top of his competitors.

28:29.040 --> 28:30.960
And he doesn't want to make that mistake again.

28:30.960 --> 28:36.800
That is also likely why they invested and he invested so much in the metaverse

28:36.800 --> 28:42.320
because he really foresaw the VR revolution as being the next computing platform.

28:42.320 --> 28:44.480
And so he wanted to be way ahead of that.

28:44.480 --> 28:45.600
And he was.

28:45.600 --> 28:50.960
And although it didn't come to fruition either as quickly or at all, the way he

28:51.040 --> 28:54.240
thought it would, AI seemingly is heading in that same direction.

28:54.240 --> 28:56.080
And he wants to be the platform.

28:56.080 --> 28:57.680
Meta wants to be the platform.

28:57.680 --> 29:03.680
They absolutely, from my perspective, apply different rules to kind of limit what we can do.

29:03.680 --> 29:06.000
And, and yeah, they have all these taxes.

29:06.000 --> 29:10.880
And, you know, at some point we did, we've done some analysis that we think we'd be

29:10.880 --> 29:14.880
way more profitable if it weren't for some of these arbitrary rules.

29:14.880 --> 29:17.280
And I think a lot of other businesses would be too.

29:17.280 --> 29:21.440
But, you know, honestly, the money part, I think it's annoying.

29:21.440 --> 29:22.880
But for me, it's not the biggest thing.

29:22.880 --> 29:28.160
It's, I think it's a little bit soul crushing when you go build features that are what you

29:28.160 --> 29:30.560
believe is good for your community.

29:30.560 --> 29:35.040
And then you're told that you can't ship them because some company wants to put you in a box

29:35.040 --> 29:36.960
so that they can better compete with you.

29:36.960 --> 29:38.640
Don't mess with the Zuck.

29:38.640 --> 29:41.120
He thinks in 4D chess.

29:41.120 --> 29:44.640
My concern for AI at this point isn't actually Apple.

29:44.640 --> 29:47.680
It's more the other companies and how that would evolve.

29:47.680 --> 29:50.160
And I think to some degree, it's not even that.

29:50.160 --> 29:52.800
I'm not even saying that they're like bad people.

29:52.800 --> 29:57.920
It's, it's, I think that there's just a physics and incentive structure to the system where,

29:57.920 --> 30:02.400
you know, if you build a closed system, then eventually there are all these forces on you

30:02.400 --> 30:06.960
that sort of kind of push you to, to kind of clamp down on things.

30:06.960 --> 30:15.280
And I think that it will be a healthier ecosystem if it's developed more like the web,

30:15.280 --> 30:16.240
but more capable.

30:16.240 --> 30:20.320
And I think that, you know, because of how mobile developed where the closed model won,

30:20.320 --> 30:20.560
right?

30:20.560 --> 30:24.640
And it's like Apple, I think, has really reaped most of the benefits in terms of,

30:24.640 --> 30:28.400
you know, they, there might be more Android phones out there, but like Apple gets like

30:28.400 --> 30:30.800
almost all the profits of, for mobile phones.

30:31.360 --> 30:35.360
I think there's a bit of recency bias because these are, these are long cycles.

30:35.360 --> 30:38.800
Right? I mean, the iPhone, I think it came out in 2007, right?

30:38.800 --> 30:40.560
So we're almost 20 years into this thing.

30:40.560 --> 30:45.600
It's a long cycle, but it's easy to forget the fact that the closed model doesn't always win.

30:46.560 --> 30:52.000
If you go back to PCs, now I know a lot of people have, especially if you're using the

30:52.000 --> 30:57.120
Linux analogy, people don't necessarily consider windows to be maximally open, but compared to

30:57.120 --> 31:02.480
the, the Apple approach of kind of coupling your operating system with, with the device,

31:03.200 --> 31:07.040
the windows approach was a more open ecosystem and it won.

31:07.600 --> 31:13.360
And part of my hope for the next generation of platforms, which includes both AI and the

31:13.360 --> 31:17.760
work that we're doing and augmented in virtual reality is to, you know, meta wants to be on the

31:17.760 --> 31:19.520
side of building the open ecosystems.

31:20.080 --> 31:23.600
And it's not just that we want to build something that's an alternative to the closed

31:23.600 --> 31:27.680
ecosystem. I want to restore the industry to the state where the open ecosystem is actually

31:27.680 --> 31:28.800
the one that is leading.

31:28.800 --> 31:34.320
As I said earlier, his motivations for doing this are clearly because he got burned.

31:34.320 --> 31:40.720
That is why he is trying to change the way that this ecosystem is going to be developed

31:40.720 --> 31:42.240
and play out in the long run.

31:42.240 --> 31:45.200
And I kind of love it. I love this approach.

31:45.200 --> 31:52.000
This is also why Elon Musk decided to open source grok because he was bitter at open AI

31:52.000 --> 31:56.560
being closed AI, taking a bunch of his money and then eventually converting into a for-profit

31:56.560 --> 32:02.400
company. And so there is nothing like a burned entrepreneur with a chip on their shoulder

32:02.400 --> 32:05.920
and a little bit of spite to really drive them to innovate.

32:05.920 --> 32:09.120
And let me tell you, I am here for it. I love it.

32:09.120 --> 32:12.480
All right. So Rowan now asks Mark Zuckerberg about Lama 4.

32:12.480 --> 32:17.200
And I'm sure he's already planning Lama 4, but he just released Lama 3.5.

32:17.200 --> 32:19.520
He just released the 405B model.

32:19.520 --> 32:23.680
And so I think all of their efforts, probably for the foreseeable future,

32:23.760 --> 32:29.920
are going to be on iterating and innovating on Lama 3, Lama 3.1, Lama 3.2.

32:29.920 --> 32:33.040
And they'll probably have Lama 4 cooking in the background,

32:33.040 --> 32:35.200
but it's going to be a while before we see that.

32:35.200 --> 32:40.880
And I'm going to guess that's probably going to come out a little bit after GPT-5 comes out.

32:40.880 --> 32:45.680
Oh man. I mean, it's, you know, we're just doing 3.1 for Lama now.

32:45.680 --> 32:52.080
I think it might be a little early to talk about Lama 4, but we've got the compute cluster set up.

32:52.400 --> 32:55.040
All right. I think that was hilarious.

32:55.040 --> 32:59.040
He's like, oh, it's probably a little bit too early to be thinking about Lama 4,

32:59.040 --> 33:04.160
but we have all the compute necessary to do it. I absolutely love this new version of Zuck.

33:04.160 --> 33:08.880
The data setup, we kind of have a sense of what the architecture is going to be,

33:08.880 --> 33:12.720
and have run a bunch of research experiments to kind of max that out.

33:12.720 --> 33:18.080
So I do think that Lama 4 is going to be another big leap on top of Lama 3.

33:18.080 --> 33:21.520
I think we have a bunch more progress that we can make.

33:21.520 --> 33:23.680
I mean, this is the first dot release for Lama.

33:23.680 --> 33:30.880
There's more that I'd like to do, including launching the multimodal models, which we...

33:30.880 --> 33:35.520
Yes. I can't wait for that. That is the one biggest missing capability between Lama 3,

33:35.520 --> 33:42.240
4.0.5.B, and GPT-4.0. GPT-4.0 can take many different file formats, interpret them,

33:42.240 --> 33:46.720
including images, and really that one feature I use all the time.

33:47.200 --> 33:51.760
Unfortunately, we have not had a Lama model that is really truly multimodal,

33:51.760 --> 33:55.920
and there have been a few fine-tuned versions that allow for multimodality,

33:55.920 --> 34:00.240
but they don't work super well if I'm being honest. You've seen me test them on this channel,

34:00.240 --> 34:03.840
so I can't wait for a native multimodal Lama 3.1.

34:03.840 --> 34:06.800
We kind of had an unfortunate setback on that,

34:08.080 --> 34:11.120
but I think we're going to be launching them probably everywhere outside of the EU.

34:11.840 --> 34:16.400
For those who are wondering what he's talking about, just recently it was reported that they

34:16.400 --> 34:21.360
are not going to be releasing multimodal AI in the EU strictly because of their regulations,

34:21.360 --> 34:27.200
and that is the setback that he's talking about. I can't wait till they release it here in the US

34:27.200 --> 34:32.080
and other countries that allow it, but not going to be in the EU, and that is one reason why they

34:32.080 --> 34:38.480
need to ease up on the regulation in the EU, and hopefully we don't over-regulate in the US,

34:38.480 --> 34:42.480
and from where I'm from, California. Yeah, probably a little early to talk about Lama 4,

34:42.960 --> 34:46.640
but it is going to be awesome, and it has been one of the interesting things in running the

34:46.640 --> 34:55.760
company is basically planning out the compute clusters and data trajectories for not just

34:55.760 --> 35:03.520
Lama 4, but the next probably four or five versions of Lama, because these are long lead

35:03.520 --> 35:10.480
time investments to build out these data centers and the power around them and the chip architectures

35:10.480 --> 35:16.160
and the networking architectures, so all this stuff. So yeah, I realize that's a bit of a non-answer

35:16.160 --> 35:23.920
for now other than just some general excitement, but I don't know, I think Lama 3 deserves at

35:23.920 --> 35:32.320
least a week of kind of just processing what we've put out there before we get into talking

35:32.320 --> 35:35.680
about the future. All right, I'm going to put this out into the world. I want to interview

35:35.680 --> 35:40.240
Mark Zuckerberg, and if I'm the one who gets to interview him or one of the ones who gets to

35:40.240 --> 35:45.280
interview him as part of the Lama 4 launch, I would love that. So if anybody from META is watching

35:45.280 --> 35:50.640
this, please consider me. I would love to do that. All right, next, he's going to be talking about AGI

35:50.640 --> 35:57.680
and agents, something that I've seen little bits of in the Lama 3.1 launch. They are defining their

35:57.680 --> 36:02.160
own agent architecture, it seems, or its own language. I still haven't dug into it too deeply

36:02.160 --> 36:07.200
yet, but I plan to, but let's see what Mark has to say about AGI and specifically agents.

36:07.200 --> 36:12.160
I'm happy to talk about it both from a technical perspective and a product perspective, but since

36:12.160 --> 36:18.640
we've mostly talked about the models so far, maybe I'll start with the products. So our vision

36:19.360 --> 36:26.000
is that there should be a lot of different AIs out there in AI services, not just kind of one

36:26.000 --> 36:31.360
singular AI. And that really informs the open source approach. It's, you know, it also informs

36:31.360 --> 36:37.280
the product roadmap. So yeah, we have META AI. META AI is doing quite well. My goal was for it

36:37.280 --> 36:42.960
to be the most used AI assistant in the world by the end of the year. I think we're well on track

36:42.960 --> 36:49.120
for that. We'll probably hit it, hit that milestone, you know, a few months before the end of the year.

36:49.120 --> 36:55.840
That's a huge statement, if true. That means that META AI has more usage than chat GPT,

36:55.840 --> 37:00.640
which would be surprising to me because anybody who knows about AI knows about chat GPT, but they

37:00.640 --> 37:06.640
don't necessarily know about Anthropics Clawd or other models. And many people have never even heard

37:06.640 --> 37:13.360
of Lama before. Obviously, META has the billions of built-in user base. So it's exciting to see

37:13.360 --> 37:19.120
that. But he's specifically talking about the META.AI product. And I believe when he's saying META AI,

37:19.120 --> 37:25.440
it's not just META.AI, which is kind of the chat GPT competitor, but it is also each of the

37:25.520 --> 37:31.040
implementations of META AI in each of their products, WhatsApp, Instagram, Facebook, etc.

37:31.040 --> 37:37.440
A lot of what we're focused on is giving every creator and every small business the ability to

37:37.440 --> 37:44.000
create AI agents for themselves, making it so that every person on our platforms can create their own

37:44.000 --> 37:48.080
AI agents that they want to interact with. And if you think about it, these are just

37:48.080 --> 37:52.640
huge spaces, right? So there are hundreds of millions of small businesses in the world.

37:52.640 --> 37:55.760
And one of the things I think is really important is basically making it

37:56.560 --> 38:02.240
so with a relatively small amount of work, a business can basically, you know, few taps,

38:03.040 --> 38:09.760
stand up an AI agent for themselves that can do customer support, sales, communicate with all

38:09.760 --> 38:14.720
their people, all their customers. That's going to be hundreds of millions, maybe billions of

38:14.720 --> 38:20.160
what kind of small business agents. Similar deal for creators. There are more than,

38:20.160 --> 38:23.920
there's more than 200 million people on our platforms who consider themselves creators,

38:23.920 --> 38:29.200
who basically use our platform in a way that is primarily for, you know, building a community,

38:30.560 --> 38:34.480
you know, putting out content feel like it's kind of like a part of their job is doing that.

38:35.040 --> 38:38.640
And they all have this basic issue, which is that there aren't enough hours in the day to engage

38:38.640 --> 38:42.480
with their community as much as they'd like. And likewise, I think that their communities would

38:42.480 --> 38:47.920
generally want more of their time, but, but again, not enough hours in the day. So I just think it's

38:48.480 --> 38:52.400
there's going to be a huge unlock where basically every creator can pull in all their information

38:52.400 --> 38:57.760
from social media, can train these systems to reflect their values and their business objectives

38:57.760 --> 39:03.120
and what they're trying to do. And then people can interact with that. It'll be almost like this,

39:03.120 --> 39:09.760
almost artistic artifact that creators create that people can kind of interact with in different

39:09.760 --> 39:13.680
ways. And then, and that's not even getting into all the different ways that I think people are

39:13.680 --> 39:17.040
going to be able to create, you know, different AI agents for themselves to do different things.

39:17.040 --> 39:20.320
So I think we're going to live in a world where there are going to be hundreds of millions and

39:20.320 --> 39:24.880
billions of different AI agents, eventually probably more AI agents than there are people in

39:24.880 --> 39:27.520
the world, and that people are just going to interact with them in all these different ways.

39:27.520 --> 39:31.840
So that's part of, you know, that's the product vision. Obviously, there's a lot of business

39:31.840 --> 39:35.760
opportunity in that. That's where we want to go make money. So we don't want to, we're not going

39:35.760 --> 39:40.080
to make money from selling access to the model itself. Because again, we're not a public cloud

39:40.080 --> 39:44.960
company. We will make money by building the best products. An important ingredient to the best

39:44.960 --> 39:50.000
products is building, is having the best models. So this is echoing exactly what Jan Lacoon told

39:50.000 --> 39:55.360
Lex Freeman a few months ago, where he said, they're not going to make money by developing and

39:55.360 --> 40:00.240
deploying open source models, but as being the company who can define the standards and thus

40:00.240 --> 40:04.240
make the best products around that AI, that's going to be meta. And that's how they're going

40:04.240 --> 40:09.120
to make money. All right, in this last section, he talks about fear of AI, why people worry about

40:09.120 --> 40:12.800
AI, why they should, why they shouldn't. So let's take a look. I mean, financially, one thing that

40:12.800 --> 40:20.160
I'm quite aware of is the internet had a big bubble burst before it succeeded. And it's all

40:20.160 --> 40:25.920
the people who were very long on the internet were eventually right. But sometimes things take

40:25.920 --> 40:29.360
a little longer to develop than you think. And you just need to have the commitment to see that

40:29.360 --> 40:34.400
through. And that's something that I'm aware of. Because yeah, I mean, I'm really excited about,

40:34.400 --> 40:37.280
you know, all the unlocks that we're going to get from llama three, and then llama four,

40:37.280 --> 40:40.320
and then llama five, and I think that's going to translate into better products. But

40:41.200 --> 40:46.080
realistically, it's hard to know in advance when something is good enough that you're going to

40:46.080 --> 40:51.040
have a product that billions of people use. And then when it's ready to kind of be a large business.

40:51.040 --> 40:56.240
And I mean, look, we're all spending, you know, a lot of capital and on basically training these

40:56.240 --> 40:59.680
models. So I think that people are going to be probably losing money for quite a while.

40:59.680 --> 41:03.840
Yeah. And that's what I've been hearing generally from the industry. Every single big tech company,

41:03.840 --> 41:08.880
even a lot of VC dollars going into startups, they're all spending their money buying the silicon.

41:08.880 --> 41:14.080
They are mostly buying it from Nvidia. That is why Nvidia stock price has skyrocketed over the last

41:14.080 --> 41:19.680
couple of years. However, all of that initial investment has not necessarily translated into

41:19.680 --> 41:23.520
revenue. In fact, I think the number is only like 30 billion in revenue, even though there's been a

41:23.520 --> 41:29.360
trillion dollars in spend. So that's not sustainable in the long term. There is likely a mini bubble

41:29.360 --> 41:35.760
that is going to burst eventually. But obviously I'm bullish on AI. And that's what I'm dedicating

41:35.760 --> 41:40.160
all of my time to right now. So I believe in it in the long run. And as he said,

41:40.160 --> 41:46.800
people who were early on the web and long on the web eventually were right. Now there was a bubble

41:46.800 --> 41:52.640
in between that we do have to think about and consider and worry about when it comes to AI.

41:52.640 --> 41:57.440
The other part of this that I think you are more getting at is people's concern about what it means

41:57.440 --> 42:03.520
for their livelihoods. And on that, this is one of the reasons why I think the open source approach,

42:03.520 --> 42:10.000
the approach of lots of different models out there that are kind of personalized and customized to

42:10.000 --> 42:17.680
every business and every creator and every person. I think that's important because if this develops

42:17.680 --> 42:24.960
in a way where it's just a small number of companies that build the products and benefit

42:24.960 --> 42:29.920
and people use the products and maybe they like talking to an AI assistant and that's valuable

42:29.920 --> 42:38.560
for them. But if this doesn't in some way help lift all boats, then I think you end up eventually

42:38.560 --> 42:45.360
getting a backlash. And part of what I've spent some time thinking about after just looking at how

42:45.920 --> 42:52.480
the kind of Web 2.0 stuff developed is in the next generation of technologies around AI,

42:52.560 --> 43:02.080
around AR and VR. How do we create not just a thriving set of products and economic productivity

43:02.080 --> 43:07.280
gains, but how do we have a better and more sustainable political economy around it where

43:07.280 --> 43:12.720
there's just way more people who feel like they're kind of bought in or benefiting from this

43:13.360 --> 43:19.760
in support of the system? And I thought we did that reasonably well with social media,

43:20.320 --> 43:25.040
but just looking at some of the feedback and some of the response from the world,

43:25.920 --> 43:29.360
I think that it's going to be important to do that even better with AI and some of the new

43:29.360 --> 43:34.480
technologies in order to mitigate some of the concerns that people are going to have about

43:34.480 --> 43:38.320
what this is going to mean for their livelihoods and jobs and their lives.

43:38.320 --> 43:42.080
So we're going to end it there. I think that is a great place to end it. Something for us to think

43:42.080 --> 43:48.800
about over the coming weeks, months and years, incredibly important stuff. I'm so excited to see

43:48.880 --> 43:53.760
all the different innovations that come from Lama 3.1, whether we're talking about an extremely

43:53.760 --> 44:00.320
capable model that can fit on your phone or on your laptop, or the massive 405B model that is

44:00.320 --> 44:05.360
as capable as any closed source frontier model. If you enjoyed this video, please consider giving

44:05.360 --> 44:15.360
a like and subscribe, and I'll see you in the next one.

