WEBVTT

00:00.000 --> 00:03.440
There's a battle being waged right now

00:03.440 --> 00:05.440
in the world of artificial intelligence

00:05.440 --> 00:07.960
between large foundational models

00:07.960 --> 00:10.160
and smaller open source models.

00:10.160 --> 00:13.400
And just this week, a new research paper was dropped

00:13.400 --> 00:16.680
that promises to append the conversation completely.

00:16.680 --> 00:18.200
Now, if you remember a few weeks ago,

00:18.200 --> 00:21.160
I made a video about the letter called We Have No Mote,

00:21.160 --> 00:23.880
which was a leaked internal memo from Google

00:23.880 --> 00:26.640
that really highlighted how open source models,

00:26.640 --> 00:30.280
smaller ones specifically, are iterating so quickly

00:30.280 --> 00:32.000
that these large foundational models

00:32.000 --> 00:36.000
that Google and OpenAI have are truly at risk.

00:36.000 --> 00:38.720
And I found that to be a very compelling paper.

00:38.720 --> 00:40.760
And then just two weeks ago,

00:40.760 --> 00:42.800
another research paper was released

00:42.800 --> 00:45.200
that claimed to disprove a lot of the value

00:45.200 --> 00:47.520
that these open source smaller models have.

00:47.520 --> 00:49.920
Today, we're gonna take a look at all of this

00:49.920 --> 00:51.680
and we're gonna figure out what's the truth.

00:51.680 --> 00:53.520
We're gonna take a look at the new Orca paper

00:53.520 --> 00:55.000
that was just dropped this week.

00:55.040 --> 00:57.880
We're gonna look at the We Have No Mote document again

00:57.880 --> 00:59.480
and we're gonna take a look at the research paper

00:59.480 --> 01:00.600
that came out a couple of weeks ago

01:00.600 --> 01:02.040
talking about the false promise

01:02.040 --> 01:05.880
of imitating proprietary large language models like GPT-4.

01:05.880 --> 01:06.720
Let's go.

01:06.720 --> 01:08.800
So this is Orca, progressive learning

01:08.800 --> 01:12.040
from complex explanation traces of GPT-4.

01:12.040 --> 01:13.240
This is a new research paper

01:13.240 --> 01:16.080
dropped by Microsoft Research of all companies.

01:16.080 --> 01:19.160
Of course, they made a substantial investment in OpenAI

01:19.160 --> 01:22.160
and own a significant portion of that company.

01:22.160 --> 01:24.360
So for them to release a new research paper

01:24.400 --> 01:25.680
illustrating a new technique

01:25.680 --> 01:27.760
to make open source smaller models

01:27.760 --> 01:30.600
extremely powerful is really fascinating.

01:30.600 --> 01:33.680
Microsoft as a company has embraced open source

01:33.680 --> 01:36.320
in the years since Satya Nadella took over

01:36.320 --> 01:37.480
and I'm all for it.

01:37.480 --> 01:40.240
This paper is absolutely fascinating

01:40.240 --> 01:42.080
and it makes a ton of sense.

01:42.080 --> 01:44.080
But before we get into this paper,

01:44.080 --> 01:45.960
let's take a look at those previous documents

01:45.960 --> 01:46.800
that I mentioned.

01:46.800 --> 01:48.280
Now a little over a month ago,

01:48.280 --> 01:50.760
this internal memo from Google was released

01:50.760 --> 01:51.920
called We Have No Mote.

01:51.920 --> 01:53.120
And the main point of this memo

01:53.120 --> 01:55.320
is that open source models are proliferating

01:55.320 --> 01:59.280
and iterating so quickly that the gap between models

01:59.280 --> 02:04.080
like GPT-4 and Palm 2 are shrinking very quickly.

02:04.080 --> 02:06.480
The fact that any developer can get their hands

02:06.480 --> 02:09.040
on these models and new techniques to train

02:09.040 --> 02:11.080
and fine tune these models are coming out every day.

02:11.080 --> 02:13.880
And we're seeing that from Laura to Q Laura

02:13.880 --> 02:16.080
to now having a ton of different options

02:16.080 --> 02:18.560
of how to train and fine tune these models

02:18.560 --> 02:19.800
in really efficient ways

02:19.800 --> 02:22.320
and run them on any consumer grade hardware.

02:22.320 --> 02:24.640
And I agreed with a lot that was in this paper.

02:24.640 --> 02:26.200
Of course, a business mode

02:26.200 --> 02:28.600
is not just the technical limitations.

02:28.600 --> 02:30.160
There's much more to it than that.

02:30.160 --> 02:32.760
But a lot of the points made in this paper are very valid.

02:32.760 --> 02:35.280
And I've seen more innovation in the open source community

02:35.280 --> 02:36.320
over these last few weeks

02:36.320 --> 02:38.920
than I've seen on these proprietary large models.

02:38.920 --> 02:41.040
But then a research paper out of UC Berkeley

02:41.040 --> 02:42.400
was dropped a couple of weeks ago

02:42.400 --> 02:44.080
that really challenged the assertions

02:44.080 --> 02:45.960
of the We Have No Mote leak document.

02:45.960 --> 02:47.360
In this research paper,

02:47.360 --> 02:50.680
the false promise of imitating proprietary LLMs,

02:50.680 --> 02:52.400
they spell out that these open source models

02:52.400 --> 02:56.440
are simply just imitating the outputs of these larger models

02:56.440 --> 02:58.760
without actually understanding the logic

02:58.760 --> 03:00.560
to reach certain outputs.

03:00.560 --> 03:01.960
The gist of this paper

03:01.960 --> 03:04.080
and what Orca looks to correct

03:04.080 --> 03:06.200
is that these open source models

03:06.200 --> 03:09.320
are simply being trained on prompts and responses

03:09.320 --> 03:11.440
which is good for pattern matching.

03:11.440 --> 03:13.880
So for example, if you're a student in college

03:13.880 --> 03:15.200
and you're taking a class,

03:15.200 --> 03:18.040
you could probably do pretty well on a lot of tests

03:18.040 --> 03:20.920
simply by pattern matching the question to an answer.

03:20.920 --> 03:23.480
But that student is gonna have a lot of limitations.

03:23.480 --> 03:25.040
If one of the questions varies

03:25.040 --> 03:26.920
from their pattern matching ability,

03:26.920 --> 03:28.520
by even just a little bit,

03:28.520 --> 03:30.680
their ability to reason and figure out

03:30.680 --> 03:33.320
what the answer might be becomes highly limited.

03:33.320 --> 03:36.000
Whereas the student who fundamentally

03:36.000 --> 03:37.960
and deeply understands a topic

03:37.960 --> 03:41.200
won't be thrown off by any variation of the question.

03:41.200 --> 03:45.120
They'll be able to reason and step by step get to the answer

03:45.120 --> 03:47.320
because they do truly understand the topic.

03:47.320 --> 03:48.760
And that's really the difference

03:48.760 --> 03:51.360
between these large foundational models

03:51.360 --> 03:54.160
and the open source imitations of them

03:54.160 --> 03:55.360
as per this paper.

03:55.360 --> 03:57.120
And that brings us to Orca.

03:57.120 --> 03:59.800
Orca challenges the idea that open source models

03:59.800 --> 04:02.280
can only really imitate answers

04:02.280 --> 04:04.520
and will get thrown off by any variation

04:04.520 --> 04:06.040
in the prompts themselves.

04:06.040 --> 04:08.960
And the way they do it seems very obvious in hindsight.

04:08.960 --> 04:10.800
Before we get into the details,

04:10.800 --> 04:15.040
Orca outperforms every other open source model

04:15.040 --> 04:19.280
and even outperforms ChatGPT, which is GPT 3.5,

04:19.280 --> 04:21.200
in a lot of different benchmarks.

04:21.200 --> 04:24.360
Now, of course, it still lags behind GPT4,

04:24.360 --> 04:26.480
but the gap continues to close.

04:26.480 --> 04:28.080
So let's take a look at this paper now.

04:28.080 --> 04:29.520
They start off the abstract

04:29.520 --> 04:32.340
by addressing this imitation concept.

04:32.340 --> 04:35.320
Recent research has focused on enhancing the capability

04:35.320 --> 04:37.520
of smaller models through imitation learning,

04:37.520 --> 04:39.280
drawing on the outputs generated

04:39.280 --> 04:40.920
by large foundational models.

04:40.920 --> 04:45.000
Again, LFMs are referring to ChatGPT and GPT4.

04:45.000 --> 04:46.880
And they start to outline the limitations

04:46.880 --> 04:48.680
of these imitation techniques.

04:48.680 --> 04:51.640
Some that they point out are limited imitation signals

04:51.640 --> 04:53.880
from shallow LFM outputs,

04:53.880 --> 04:56.280
small scale homogenous training data,

04:56.280 --> 05:00.360
and most notably a lack of rigorous evaluation

05:00.360 --> 05:03.280
resulting in overestimating the small models capability

05:03.280 --> 05:05.800
as they tend to learn to imitate the style

05:05.800 --> 05:08.760
but not the reasoning process of LFMs.

05:08.760 --> 05:11.640
That is really the crux of this paper.

05:11.640 --> 05:14.520
How do we start getting these open source models

05:14.520 --> 05:17.600
to not just mimic the question answer pairs,

05:17.600 --> 05:20.040
but actually understand how they get

05:20.040 --> 05:22.420
from a question to an answer.

05:22.420 --> 05:25.560
And only with that is true intelligence created.

05:25.560 --> 05:28.000
To address these challenges, we develop ORCA,

05:28.000 --> 05:29.680
a 13 billion parameter model

05:29.680 --> 05:32.800
that learns to imitate the reasoning process of LFMs.

05:32.800 --> 05:34.560
Let's pause there for a second.

05:34.560 --> 05:39.080
This model, the ORCA model is only 13 billion parameters,

05:39.080 --> 05:43.120
which means it can run on pretty much any modern hardware.

05:43.120 --> 05:44.520
Whereas some of the other models

05:44.520 --> 05:45.840
that I've been reviewing recently,

05:45.840 --> 05:47.080
like the Guinaco model,

05:47.080 --> 05:49.600
require me to rent out a cloud GPU,

05:49.600 --> 05:53.080
like an A6000 that has 48 gigabytes of VRAM,

05:53.080 --> 05:55.820
because it's so large, 65 billion parameters.

05:55.820 --> 05:57.560
And this performs better than that.

05:57.560 --> 05:59.060
Now here's the key to the paper.

05:59.060 --> 06:00.360
Here's the key technique.

06:00.360 --> 06:03.720
ORCA learns from rich signals from GPT-4,

06:03.720 --> 06:05.560
including explanation traces,

06:05.560 --> 06:07.680
step-by-step thought processes,

06:07.680 --> 06:09.640
and other complex instructions

06:09.640 --> 06:12.800
guided by teacher assistance from chat GPT.

06:12.800 --> 06:14.720
Now I'll explain what teacher assistance is

06:14.720 --> 06:17.040
in a little bit, but looking at this sentence,

06:17.040 --> 06:18.520
what it's really saying is,

06:18.520 --> 06:21.120
rather than learning from the prompt and response pairs,

06:21.120 --> 06:23.800
we're going to ask these large foundational models

06:23.800 --> 06:27.160
to explain their reasoning step-by-step,

06:27.160 --> 06:29.280
and the smaller open source models

06:29.280 --> 06:30.560
will learn from that.

06:30.560 --> 06:32.280
Truly fascinating.

06:32.280 --> 06:34.160
Now I wanna briefly touch on this guided

06:34.160 --> 06:36.840
by teacher assistance from chat GPT.

06:36.840 --> 06:40.000
They have a two-tier teaching process.

06:40.000 --> 06:43.680
One, they take chat GPT, which is GPT-3.5,

06:43.680 --> 06:45.480
and they have a large number of examples

06:45.480 --> 06:46.880
to learn from, five million.

06:46.880 --> 06:48.680
Then they take those five million,

06:48.680 --> 06:52.260
boil it down to the most important one million examples,

06:52.260 --> 06:55.760
and then use GPT-4 to continue to train

06:55.760 --> 06:57.520
on more complex examples.

06:57.520 --> 06:59.160
So how does it actually perform?

06:59.160 --> 07:01.480
ORCA surpasses conventional state-of-the-art

07:01.480 --> 07:04.640
instruction-tuned models, such as Vecunia 13B

07:04.640 --> 07:09.000
by more than 100% in complex zero-shot reasoning benchmarks

07:09.000 --> 07:13.040
like Big Bench Hard and 42% on AGI Eval.

07:13.040 --> 07:16.140
Big Bench Hard and AGI Eval are just sets of tests

07:16.140 --> 07:17.920
that they give to these large language models

07:17.920 --> 07:19.360
to test their performance.

07:19.360 --> 07:23.800
ORCA reaches parity with chat GPT on the BBH benchmark

07:23.800 --> 07:25.560
and shows competitive performance

07:25.560 --> 07:28.120
in professional and academic examinations

07:28.120 --> 07:30.400
like SAT, LSAT, GRE, and GMAT,

07:30.400 --> 07:33.360
both in zero-shot setting without chain of thought

07:33.360 --> 07:35.100
while trailing behind GPT-4.

07:35.100 --> 07:38.080
And again, this last sentence is everything.

07:38.080 --> 07:40.200
Our research indicates that learning

07:40.200 --> 07:42.400
from step-by-step explanations,

07:42.400 --> 07:44.160
whether these are generated by humans

07:44.160 --> 07:45.560
or more advanced AI models,

07:45.560 --> 07:48.440
is a promising direction to improve model capabilities

07:48.440 --> 07:49.280
and skills.

07:49.280 --> 07:50.720
And just like humans,

07:50.720 --> 07:53.840
large language models understanding how something works

07:53.840 --> 07:55.760
is much more effective

07:55.760 --> 07:59.000
than just being able to pattern match questions and answers.

07:59.000 --> 08:01.640
So large language models are typically tuned

08:01.640 --> 08:03.660
by something called instruction tuning.

08:03.660 --> 08:06.600
You have a set of prompts and you have a set of responses

08:06.600 --> 08:09.600
and those pairs are passed to the open source model

08:09.600 --> 08:10.680
and it learns from that.

08:10.680 --> 08:13.960
This technique is called explanation tuning

08:13.960 --> 08:16.160
where it's not just the prompt and the answer

08:16.160 --> 08:18.920
but an explanation of the reasoning and the logic

08:18.920 --> 08:22.120
for how chat GPT and GPT-4 arrived at an answer.

08:22.120 --> 08:25.880
And so we can see here when evaluated by GPT-4

08:25.880 --> 08:27.600
and that's called auto-evaluation,

08:27.600 --> 08:30.820
ORCA 13B actually beats chat GPT.

08:30.820 --> 08:33.140
It beats Bard and it certainly beats

08:33.140 --> 08:35.200
the open source models based on Lama.

08:35.200 --> 08:38.640
And then for zero shot problems on academic exams,

08:38.640 --> 08:41.400
chat GPT definitely performs better

08:41.400 --> 08:45.600
but ORCA 13B is really closing the gap in performance

08:45.600 --> 08:48.840
and performs much better than Virginia 13B.

08:48.840 --> 08:51.480
And for complex zero shot reasoning tasks

08:51.480 --> 08:55.120
and big bench hard, ORCA achieves parity with chat GPT.

08:55.120 --> 08:56.080
And here again,

08:56.080 --> 08:58.760
they specifically call out that imitation paper.

08:58.760 --> 09:01.800
Authors assert that model imitation is a false promise

09:01.800 --> 09:04.960
since broadly matching chat GPT using purely imitation

09:04.960 --> 09:07.320
would require one, a concerted effort

09:07.320 --> 09:09.720
to collect enormous imitation data sets

09:09.720 --> 09:12.720
and far more diverse and higher quality imitation data

09:12.720 --> 09:14.140
than is currently available.

09:14.140 --> 09:16.800
So one of the biggest problems is these open source models

09:16.800 --> 09:19.680
can't get enough data to use the imitation technique

09:19.680 --> 09:20.840
and perform at the same rate

09:20.840 --> 09:22.800
as these large foundational models.

09:22.800 --> 09:24.280
Contrary to this assertion,

09:24.280 --> 09:27.000
we demonstrate that both conditions one and two

09:27.000 --> 09:29.320
are attainable and that it is possible

09:29.320 --> 09:31.560
to reduce the gap with proprietary LLMs

09:31.560 --> 09:33.560
on multiple zero shot benchmarks

09:33.560 --> 09:35.280
that require sophisticated reasoning.

09:35.280 --> 09:38.120
And here they touch on what the existing open source models

09:38.120 --> 09:40.040
are doing currently to train themselves.

09:40.040 --> 09:43.200
Both Alpaca and Wizard LM employ a variant

09:43.200 --> 09:44.160
of self-instructs.

09:44.160 --> 09:45.440
So that's what we've been talking about.

09:45.440 --> 09:48.960
Wizard LM introduces the concept of Eval Instruct

09:48.960 --> 09:51.800
which gradually rewrites the initial set of instructions

09:51.800 --> 09:53.400
into more complex versions

09:53.400 --> 09:55.000
attempting to overcome some of the methods

09:55.000 --> 09:56.240
inherent shortcomings.

09:56.240 --> 09:57.920
But with Vakunya and Kuala,

09:57.920 --> 10:00.160
they demonstrate remarkable performance

10:00.160 --> 10:01.960
due to the more human-like conversations

10:01.960 --> 10:03.120
and natural instructions

10:03.120 --> 10:05.080
in the community contributed conversations

10:05.080 --> 10:06.680
like those in shared GPT.

10:06.680 --> 10:08.400
So basically what they're saying is

10:08.400 --> 10:10.720
as more people are using these open source models

10:10.720 --> 10:11.680
and sharing their data,

10:11.680 --> 10:13.280
sharing their instructions,

10:13.280 --> 10:14.760
their prompts and the output,

10:14.760 --> 10:16.400
they'll continue to train on those pairs

10:16.400 --> 10:17.560
and get better and better.

10:17.560 --> 10:19.440
But there's a limitation with that as well.

10:19.440 --> 10:21.920
And it's the same thing that we keep coming back to.

10:21.920 --> 10:24.600
Models trained on such natural conversations

10:24.600 --> 10:26.080
may capture the style

10:26.080 --> 10:29.320
but not the reasoning process of the LLFM.

10:29.320 --> 10:31.880
So again, they'll be able to pattern match

10:31.880 --> 10:34.720
but they're not gonna truly understand the logic

10:34.720 --> 10:37.560
and the reasoning behind arriving at the solutions.

10:37.560 --> 10:41.680
Now the Orca Paper puts forth three key contributions.

10:41.680 --> 10:44.080
Number one is explanation tuning.

10:44.080 --> 10:46.840
And again, this is fine tuning models

10:46.840 --> 10:49.880
based on the step-by-step explanation

10:49.880 --> 10:51.240
of the reasoning and the logic

10:51.240 --> 10:52.760
of how to arrive at a solution.

10:52.760 --> 10:53.920
Let's read this a little bit.

10:53.920 --> 10:56.400
We augment the query response pairs

10:56.400 --> 10:58.760
with detailed responses from GPT-4

10:58.760 --> 11:01.320
that explain the reasoning process of the teacher

11:01.320 --> 11:03.000
as it generates the response.

11:03.000 --> 11:04.720
And to get the step-by-step reasoning,

11:04.720 --> 11:07.240
they're using some of these more modern prompting techniques

11:07.240 --> 11:08.600
that we've been learning about,

11:08.600 --> 11:10.720
such as explain like I'm five,

11:10.720 --> 11:13.440
think step-by-step and justify your response.

11:13.440 --> 11:17.520
This forces GPT-4 to put forth its reasoning

11:17.520 --> 11:19.720
and its logic in the response itself

11:19.720 --> 11:21.680
and that is used to train.

11:21.680 --> 11:23.480
And that's what explanation tuning is.

11:23.480 --> 11:26.280
Another issue is scaling the amount of tasks and instructions.

11:26.280 --> 11:28.960
As you'll see in a graph that I'll show in a second,

11:28.960 --> 11:30.440
a lot of these open source models

11:30.440 --> 11:32.480
are using a highly limited data set,

11:32.480 --> 11:35.080
but that's where Orca really excels.

11:35.080 --> 11:37.600
We utilize the Flaan 2020 collection

11:37.600 --> 11:39.760
and that's a data set of tasks and instructions

11:39.760 --> 11:40.880
put forth by Google

11:40.880 --> 11:43.760
that has tens of millions of instructions.

11:43.760 --> 11:46.020
So let's quickly take a look at the data sizes

11:46.020 --> 11:47.360
for these open source models.

11:47.360 --> 11:49.720
All of them have in the thousands.

11:49.720 --> 11:53.280
So you can see here that Alpaca has 52,000,

11:53.280 --> 11:56.280
Vakunya has 70,000 and WizardLM with the most

11:56.280 --> 11:59.280
has 250,000 based on the teacher of chat GPT.

11:59.280 --> 12:00.680
And some of these other ones like Dolly

12:00.680 --> 12:01.760
are human instructed.

12:01.760 --> 12:03.280
So they're even more limited

12:03.280 --> 12:04.920
because of the limitations of humans.

12:04.920 --> 12:06.720
However, as you could see here,

12:06.720 --> 12:10.200
Orca has five million, many times more

12:10.200 --> 12:12.600
than all of the other open source models.

12:12.600 --> 12:15.160
And it's based on chat GPT initially,

12:15.160 --> 12:17.160
so that's the initial five million pass

12:17.160 --> 12:20.280
and then GPT-4 with a second pass

12:20.280 --> 12:23.120
of much more complex tasks and instructions.

12:23.120 --> 12:25.420
So not only are they getting full explanations

12:25.420 --> 12:26.920
of query and responses

12:26.920 --> 12:29.160
and how they actually reach those responses,

12:29.160 --> 12:31.040
but they're getting so many more of them

12:31.040 --> 12:32.960
and they're solving the data scaling issue.

12:32.960 --> 12:34.400
Last is evaluation.

12:34.400 --> 12:37.720
There are a lot of issues with current evaluation techniques

12:37.720 --> 12:38.920
for open source models,

12:38.920 --> 12:41.320
but Orca claims to solve these in a few ways.

12:41.320 --> 12:43.800
They use auto evaluation with GPT-4.

12:43.800 --> 12:47.640
So basically asking GPT-4 between two potential responses,

12:47.640 --> 12:48.720
which one is best.

12:48.720 --> 12:51.560
They also use academic benchmarks like Big Bench Hard

12:51.560 --> 12:54.560
and Truthful QA and professional and academic exams

12:54.560 --> 12:56.480
like the SAT, LSAT, et cetera.

12:56.480 --> 12:58.580
And last, they use safety evaluation

12:58.580 --> 13:01.140
from ToxicGen, basically do these responses

13:01.140 --> 13:02.700
contain toxic language.

13:02.700 --> 13:04.220
So in figure four,

13:04.220 --> 13:06.660
they illustrate what the previous techniques do

13:06.660 --> 13:08.580
with queries and responses.

13:08.580 --> 13:10.180
So here's a user instruction.

13:10.180 --> 13:12.260
Use the data to calculate the median.

13:12.260 --> 13:14.340
Here it says, first, we need to arrange the data

13:14.340 --> 13:15.620
in ascending order.

13:15.620 --> 13:16.700
Since there are five numbers,

13:16.700 --> 13:19.060
the median is in the middle, which is seven.

13:19.060 --> 13:20.220
And so this is very basic.

13:20.220 --> 13:22.700
It's a prompt, a query, and then the response.

13:22.700 --> 13:24.060
Whereas with their new method,

13:24.060 --> 13:26.380
they ask GPT-4 to explain.

13:26.380 --> 13:27.540
So the system instruction

13:27.540 --> 13:29.420
seems to be the main tool that they use

13:29.420 --> 13:33.060
to get chat GPT and GPT-4 to explain their reasoning.

13:33.060 --> 13:34.580
You are an AI assistant.

13:34.580 --> 13:36.060
User will give you a task.

13:36.060 --> 13:38.460
Your goal is to complete the task as faithfully

13:38.460 --> 13:39.980
as you can while performing the task.

13:39.980 --> 13:42.420
Think step by step and justify your steps.

13:42.420 --> 13:45.820
So again, use the given data to calculate the median.

13:45.820 --> 13:46.780
Same prompt.

13:46.780 --> 13:48.980
To calculate the median, I will follow these steps.

13:48.980 --> 13:51.700
And GPT-4 actually outlines step by step

13:51.700 --> 13:53.820
how it will figure out what the median is.

13:53.820 --> 13:56.420
That data is then used to train the open source model.

13:56.420 --> 13:58.020
I find it so fascinating

13:58.020 --> 14:00.420
that we're using some of these modern prompting techniques

14:00.420 --> 14:03.300
like chain of thought, like explain like on five,

14:03.300 --> 14:05.700
that people have been figuring out over the last few months

14:05.700 --> 14:09.260
to get better answers from chat GPT and GPT-4.

14:09.260 --> 14:11.260
And we're using those to get better data

14:11.260 --> 14:13.700
to train the open source models with.

14:13.700 --> 14:16.880
And as I mentioned, system messages seem to be the main tool

14:16.880 --> 14:19.020
to get chat GPT and GPT-4

14:19.020 --> 14:22.060
to provide the step by step explanations.

14:22.060 --> 14:25.060
And if you play around with the chat GPT playground

14:25.060 --> 14:27.760
or even the API, you'll know that the system messages

14:27.760 --> 14:30.940
are a requirement for using either of these tools.

14:30.940 --> 14:32.140
So here are a few examples.

14:32.140 --> 14:33.180
You will be given a task,

14:33.180 --> 14:35.340
you must generate a detailed and long answer.

14:35.340 --> 14:37.540
Think like you are answering to a five year old,

14:37.540 --> 14:38.660
help as much as you can.

14:38.660 --> 14:40.500
So it's really just coaxing chat GPT-4

14:40.500 --> 14:43.380
to explain its reasoning and to be as verbose as possible.

14:43.380 --> 14:44.980
So let's actually take a look at the difference

14:44.980 --> 14:46.740
between these two prompting techniques.

14:46.740 --> 14:48.060
So for the system message,

14:48.060 --> 14:51.380
you are an AI assistant that helps people find information.

14:51.380 --> 14:52.620
User will give you a question.

14:52.620 --> 14:54.620
Your task is to answer as faithfully as you can

14:54.660 --> 14:56.980
while answering things step by step and justify your answer.

14:56.980 --> 15:00.880
So the prompt is pick which sentence is not logical.

15:00.880 --> 15:04.060
Sentence A, people in the desert often look forward to flood.

15:04.060 --> 15:07.140
Sentence B, people in the desert often look forward to rain.

15:07.140 --> 15:08.860
Now it might seem obvious to us

15:08.860 --> 15:10.580
why these sentences are different,

15:10.580 --> 15:13.300
but there's really just a subtle difference between them.

15:13.300 --> 15:15.020
So here a typical response might be,

15:15.020 --> 15:16.740
there is no reason to look forward to a flood

15:16.740 --> 15:17.980
because floods cause damage.

15:17.980 --> 15:19.260
The answer is sentence A.

15:19.260 --> 15:21.700
Chat GPT says, okay, let's break it down.

15:21.700 --> 15:23.100
Read the sentences carefully.

15:23.100 --> 15:24.140
So it's talking about what steps

15:24.140 --> 15:25.580
am I going to take to answer this?

15:25.580 --> 15:27.380
Identify the differences between them

15:27.380 --> 15:29.500
and determine which sentence is not logical.

15:29.500 --> 15:32.020
Then they say, sentence A, people in the desert

15:32.020 --> 15:33.100
often look forward to the flood.

15:33.100 --> 15:35.620
This sentence suggests that people in the desert

15:35.620 --> 15:37.500
are eagerly anticipating a flood.

15:37.500 --> 15:40.060
However, floods are typically destructive and dangerous.

15:40.060 --> 15:42.940
So it seems unlikely that people would look forward to them.

15:42.940 --> 15:45.260
Sentence B, they're looking forward to the rain.

15:45.260 --> 15:48.500
And it says this sentence suggests that people in the desert

15:48.500 --> 15:50.300
are eagerly anticipating rain

15:50.300 --> 15:51.860
while the desert is a dry environment.

15:51.860 --> 15:53.620
Rain can be beneficial in providing water

15:53.620 --> 15:54.580
for plants and animals.

15:54.580 --> 15:57.100
Then it finishes with, based on the above analysis,

15:57.100 --> 15:59.580
it can be concluded that sentence A is not logical.

15:59.580 --> 16:01.340
Then we look at GPT-4's answer.

16:01.340 --> 16:03.500
And as you can see, I won't read all of it.

16:03.500 --> 16:06.860
It's a much more detailed and verbose answer.

16:06.860 --> 16:08.660
Now in this section, they talk about

16:08.660 --> 16:11.340
why Chat GPT as a teaching assistant,

16:11.340 --> 16:14.660
assistant to GPT-4, is such a powerful method.

16:14.660 --> 16:16.740
And there's really two reasons for it.

16:16.740 --> 16:19.840
One is a capacity gap because there's such a large gap

16:19.840 --> 16:22.480
between the ORCA model and GPT-4.

16:22.480 --> 16:24.240
Being able to take data from GPT-4

16:24.240 --> 16:26.080
and passing it directly into ORCA,

16:26.080 --> 16:27.760
it's gonna struggle with imitation.

16:27.760 --> 16:29.520
Whereas if they progressively teach it

16:29.520 --> 16:31.480
to get to the GPT-4 level

16:31.480 --> 16:34.080
by the intermediate step of Chat GPT,

16:34.080 --> 16:35.840
it really performs much better.

16:35.840 --> 16:38.600
This can be viewed as a form of progressive learning

16:38.600 --> 16:40.240
or curriculum learning,

16:40.240 --> 16:42.520
where the student first learns from easier examples

16:42.520 --> 16:43.920
followed by harder ones.

16:43.920 --> 16:47.400
Again, more and more human-like behavior.

16:47.440 --> 16:50.080
Human doesn't go from learning the basics of addition

16:50.080 --> 16:51.920
all the way to calculus in one step.

16:51.920 --> 16:54.400
They learn many incrementally more difficult steps

16:54.400 --> 16:57.880
of mathematics between addition and calculus.

16:57.880 --> 17:01.440
Next is a simple pragmatic reason, cost and time.

17:01.440 --> 17:05.200
Chat GPT, specifically GPT-3.5 turbo,

17:05.200 --> 17:09.200
is much faster and much less expensive than GPT-4.

17:09.200 --> 17:11.120
So that's why they use five million examples

17:11.120 --> 17:14.920
with Chat GPT and one million examples for GPT-4.

17:14.920 --> 17:16.560
So this graphic shows the performance

17:16.600 --> 17:18.360
of these large foundational models,

17:18.360 --> 17:20.440
Vecunia and ORCA.

17:20.440 --> 17:22.520
And as we can clearly see from questions

17:22.520 --> 17:24.280
from the LSAT and the SAT,

17:24.280 --> 17:27.600
ORCA performs significantly better than Vecunia.

17:27.600 --> 17:29.440
And if we look at the ORCA column,

17:29.440 --> 17:31.240
compared to the Chat GPT column,

17:31.240 --> 17:33.720
overall it performs quite similarly,

17:33.720 --> 17:36.480
but it still does lag behind GPT-4.

17:36.480 --> 17:37.600
And they've actually shown

17:37.600 --> 17:40.240
that this progressive learning technique really works.

17:40.240 --> 17:43.200
As we can see here, using only GPT-4,

17:43.240 --> 17:46.840
they were able to achieve a score of 37.18,

17:46.840 --> 17:50.400
whereas if they used that intermediate step of Chat GPT,

17:50.400 --> 17:53.160
they were able to achieve 41.7.

17:53.160 --> 17:54.720
That might seem small,

17:54.720 --> 17:56.320
but that is a significant improvement.

17:56.320 --> 17:58.120
And for the big bench hard results,

17:58.120 --> 18:01.560
ORCA performs marginally better than Chat GPT on aggregate

18:01.560 --> 18:05.440
across all tasks, significantly lags GPT-4

18:05.440 --> 18:09.120
and outperforms Vecunia by 113%.

18:09.120 --> 18:12.160
And here they give a graphic of the zero-shot performance

18:12.200 --> 18:14.160
against all of these different tasks.

18:14.160 --> 18:17.280
And you can see that ORCA performs substantially

18:17.280 --> 18:18.440
better than Vecunia.

18:18.440 --> 18:21.240
And even across all of them, like it said,

18:21.240 --> 18:23.680
it performs better than Chat GPT.

18:23.680 --> 18:25.240
So what does all this mean?

18:25.240 --> 18:27.800
I find it fascinating for two reasons.

18:27.800 --> 18:30.360
One, open-source models continue to get better

18:30.360 --> 18:32.040
at a rapid clip.

18:32.040 --> 18:34.560
New techniques for fine-tuning, training

18:34.560 --> 18:36.880
are coming out every single day.

18:36.880 --> 18:39.440
So I think back to that we have no mode paper

18:39.440 --> 18:41.240
and it makes a lot of sense still.

18:41.240 --> 18:43.520
I also find it fascinating that GPT-4

18:43.520 --> 18:45.920
still seems to have some secret sauce

18:45.920 --> 18:49.080
and performs much better than any other model.

18:49.080 --> 18:51.840
So OpenAI seems to have plenty of mode.

18:51.840 --> 18:55.080
This mode seems to be incrementally getting decreased,

18:55.080 --> 18:56.400
but they still do have it.

18:56.400 --> 18:58.160
The last thing that I find fascinating

18:58.160 --> 19:01.360
is that this paper is by Microsoft Research.

19:01.360 --> 19:05.680
Microsoft Research owns a significant portion of OpenAI.

19:05.680 --> 19:07.640
So the fact that they're making research gains

19:07.640 --> 19:09.620
in open-source is awesome.

19:09.620 --> 19:11.860
And OpenAI really must feel

19:11.860 --> 19:13.940
that they have a significant mode to work with.

19:13.940 --> 19:16.940
And OpenAI also mentioned a week ago

19:16.940 --> 19:19.100
that they're releasing their own open-source model.

19:19.100 --> 19:20.940
So I think what all of this means

19:20.940 --> 19:22.780
is that these large language models

19:22.780 --> 19:25.440
will continue to get better and cheaper over time.

19:25.440 --> 19:28.100
Now Orca's code and dataset are not yet released,

19:28.100 --> 19:30.180
but as soon as it is, we're gonna review it.

19:30.180 --> 19:31.660
I'm gonna show you how to use it

19:31.660 --> 19:33.260
and we'll see how it performs.

19:33.260 --> 19:34.500
If you liked this video,

19:34.500 --> 19:36.420
please consider giving me a like and subscribe

19:36.420 --> 19:38.020
and I'll see you in the next one.

