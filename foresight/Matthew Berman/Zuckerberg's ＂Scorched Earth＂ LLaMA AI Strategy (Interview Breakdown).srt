1
00:00:00,000 --> 00:00:02,960
So I think we're going to live in a world where there are going to be hundreds of millions

2
00:00:02,960 --> 00:00:07,760
and billions of different AI agents, eventually probably more AI agents than there are people

3
00:00:07,760 --> 00:00:08,760
in the world.

4
00:00:08,760 --> 00:00:09,760
All right.

5
00:00:09,760 --> 00:00:12,440
So Lama 3.1 was just released today.

6
00:00:12,440 --> 00:00:17,120
And along with that, there has been a ton of additional information released from meta

7
00:00:17,120 --> 00:00:19,720
about how they think about open source.

8
00:00:19,720 --> 00:00:25,680
Mark Zuckerberg wrote a letter, which I will also be covering, but he also did a 30 plus

9
00:00:25,680 --> 00:00:27,800
minute interview with Rowan Chang.

10
00:00:27,800 --> 00:00:31,440
And so today we're going to watch the video together and I'm going to give you my thoughts

11
00:00:31,440 --> 00:00:32,440
on it.

12
00:00:32,440 --> 00:00:36,760
Can I also just say I am absolutely loving Mark Zuckerberg's current vibes.

13
00:00:36,760 --> 00:00:41,500
His transformation is going to be studied in business school for decades to come.

14
00:00:41,500 --> 00:00:42,500
So let's keep watching.

15
00:00:42,500 --> 00:00:43,500
Okay.

16
00:00:43,500 --> 00:00:48,040
In this first section, he is going to introduce the Lama 3.1 launch and it'll be in his

17
00:00:48,040 --> 00:00:49,640
own words and what he thinks about it.

18
00:00:49,640 --> 00:00:50,640
So let's watch.

19
00:00:50,640 --> 00:00:54,320
I mean, the big release today, first of all, happy to be doing this big fan of what you

20
00:00:54,320 --> 00:00:56,000
do.

21
00:00:56,000 --> 00:01:01,600
The big release today is Lama 3.1 and we're releasing three models.

22
00:01:01,600 --> 00:01:05,640
The first time we're releasing a 405 billion parameter model.

23
00:01:05,640 --> 00:01:11,000
So it's by far the most sophisticated open source model that I think anyone has put out.

24
00:01:11,000 --> 00:01:14,920
And it really kind of is competitive with some of the leading close models and in some

25
00:01:14,920 --> 00:01:15,920
areas is even ahead.

26
00:01:15,920 --> 00:01:21,440
So I'm really excited to see what people do with that, especially now that we're making

27
00:01:21,440 --> 00:01:26,680
it so that the community policies around Lama allow people to use it as a teacher model

28
00:01:26,680 --> 00:01:32,560
to distill and fine tune and basically create whatever other models they want with it.

29
00:01:32,560 --> 00:01:33,760
I already have to pause it.

30
00:01:33,760 --> 00:01:37,160
So he said a lot just in those few words.

31
00:01:37,160 --> 00:01:41,680
First of all, the 405 billion parameter model is outstanding.

32
00:01:41,680 --> 00:01:46,120
It is leaps and bounds more sophisticated than any other open source model out there.

33
00:01:46,120 --> 00:01:51,360
It is directly competitive with closed source frontier models and it is really the first

34
00:01:51,360 --> 00:01:54,480
open source model that can be considered frontier.

35
00:01:54,480 --> 00:02:00,920
Also all of this is a direct shot at open AI and other closed source companies.

36
00:02:00,920 --> 00:02:03,880
And he is taking a tried and true playbook.

37
00:02:03,880 --> 00:02:06,560
Microsoft did this for years.

38
00:02:06,560 --> 00:02:11,000
Basically when you're behind in a technology race, the strategy that you employ is called

39
00:02:11,000 --> 00:02:12,840
scorched earth.

40
00:02:12,840 --> 00:02:17,600
Essentially invest a ton of money into replicating whatever that technology is and then release

41
00:02:17,600 --> 00:02:18,800
it for free.

42
00:02:18,800 --> 00:02:23,680
Because at that point it becomes ubiquitous, it becomes a commodity and why would anybody

43
00:02:23,680 --> 00:02:28,680
go and pay premium prices for a closed source frontier model when you can have full control

44
00:02:28,680 --> 00:02:31,920
over the model and pay fractions of the price.

45
00:02:31,920 --> 00:02:35,160
Plus you are increasing competition greatly.

46
00:02:35,160 --> 00:02:39,440
And so you have companies like grok, GROQ who are able to take this large model and

47
00:02:39,440 --> 00:02:44,800
run it at inference speeds that are absolutely incredible orders of magnitude greater than

48
00:02:44,800 --> 00:02:46,280
what open AI can run at.

49
00:02:46,280 --> 00:02:50,080
And by the way, I don't think open AI is down for the count by any means.

50
00:02:50,080 --> 00:02:52,480
Sam Altman is a brilliant operator.

51
00:02:52,480 --> 00:02:58,520
And this is likely why they just released GPT-40 mini, a model that is as performant

52
00:02:58,520 --> 00:03:03,040
as GPT-40 but for a fraction of the price and it's much faster.

53
00:03:03,040 --> 00:03:04,840
So they're getting the message.

54
00:03:04,840 --> 00:03:06,600
They see the writing on the wall.

55
00:03:06,600 --> 00:03:10,200
Models are becoming commodities and I just read an article and I'm going to talk about

56
00:03:10,200 --> 00:03:14,680
this in another video that open AI is building their own AI chips.

57
00:03:14,680 --> 00:03:18,600
And again, that's really where a lot of the differentiation comes from.

58
00:03:18,600 --> 00:03:22,400
That is why GROQ is so special because of their unique chip design that allows them

59
00:03:22,400 --> 00:03:25,960
to run inference at blazing fast speeds.

60
00:03:25,960 --> 00:03:30,880
And open AI wants to accomplish the same thing because especially after this release, models

61
00:03:30,880 --> 00:03:31,880
are a commodity.

62
00:03:31,880 --> 00:03:32,880
Let's keep watching.

63
00:03:32,880 --> 00:03:38,160
In addition to that, we've distilled the 405 billion parameter model down to make newer

64
00:03:38,160 --> 00:03:44,360
and updated and now leading for their size 70 billion and 8 billion parameter models.

65
00:03:44,400 --> 00:03:48,640
So that's actually one thing I didn't quite realize even after reading the announcement,

66
00:03:48,640 --> 00:03:53,840
they took the 405 billion parameter model and distilled it down into the 3.1 versions of

67
00:03:53,840 --> 00:03:56,240
the 8 billion and 70 billion parameter models.

68
00:03:56,240 --> 00:03:57,240
I didn't know that.

69
00:03:57,240 --> 00:04:01,680
Does that mean that the 8 billion and 70 billion parameter models that first came out the three

70
00:04:01,680 --> 00:04:05,560
version were standalone models and now these are completely new models?

71
00:04:05,560 --> 00:04:06,720
I'm not actually sure.

72
00:04:06,720 --> 00:04:08,280
So if you know, let me know in the comments.

73
00:04:08,280 --> 00:04:12,520
Yeah, I mean, taking a step back, I think this is a pretty big moment for open source AI.

74
00:04:12,680 --> 00:04:16,640
Yeah, I've been reflecting on this and I kind of think it's, you know, I thought for a while

75
00:04:16,640 --> 00:04:20,160
that open source AI was going to become the industry standard.

76
00:04:20,160 --> 00:04:25,880
And I thought that it would basically follow the path that Linux did, where, you know,

77
00:04:25,880 --> 00:04:30,120
if you just go back to before Linux was popular, there, you know, there were all these companies

78
00:04:30,120 --> 00:04:34,520
that had their own closed versions of Unix and at the time, you know, there's nothing

79
00:04:34,520 --> 00:04:39,240
that was sort of that sophisticated that had ever been done as an open source project.

80
00:04:39,240 --> 00:04:42,600
And people thought, hey, no, this is like the closed model of development is the only

81
00:04:42,600 --> 00:04:44,120
way to do something that's this advanced.

82
00:04:44,120 --> 00:04:51,080
And at first Linux kind of got its foothold because it was cheaper because developers

83
00:04:51,080 --> 00:04:52,680
could customize it in different ways.

84
00:04:52,680 --> 00:04:57,240
And then over time as the ecosystem built out, it, you know, got more scrutiny.

85
00:04:57,240 --> 00:04:59,280
So it actually became the more secure one.

86
00:04:59,280 --> 00:05:00,760
It became the more advanced one.

87
00:05:00,760 --> 00:05:05,120
There were more partners that basically built more capabilities in the case of Linux, more

88
00:05:05,120 --> 00:05:09,800
developers, and things like that, that basically ended up making it have more capabilities as

89
00:05:09,800 --> 00:05:11,280
well than any closed source Unix.

90
00:05:11,280 --> 00:05:18,840
So I think that this moment with Lama 3.1 is kind of like that inflection point where

91
00:05:18,840 --> 00:05:24,880
I think Lama has the opportunity to become the open source AI standard for open source

92
00:05:24,880 --> 00:05:28,280
to become the standard, the industry standard for AI.

93
00:05:28,280 --> 00:05:33,720
And even in the places where it's not yet ahead on performance, it leads on, on kind

94
00:05:33,720 --> 00:05:38,600
of cost and on, on customized ability and on the ability to take the model and fine tune

95
00:05:38,600 --> 00:05:40,960
it and do all the things that you want with it.

96
00:05:40,960 --> 00:05:46,000
So he also mentioned, and I forgot to mention this in my last comments, but he also mentioned

97
00:05:46,000 --> 00:05:51,160
that they changed the license and now with the 405 billion perimeter model, they allow

98
00:05:51,160 --> 00:05:56,160
you to create synthetic data from that model to train smaller models.

99
00:05:56,160 --> 00:06:01,600
So that is a huge change and extremely valuable for the ecosystem.

100
00:06:01,600 --> 00:06:04,520
This is what NVIDIA did with their Nemetron model.

101
00:06:04,520 --> 00:06:11,200
They trained a massive model that generates data and has the permission to do so to train

102
00:06:11,200 --> 00:06:12,200
smaller models.

103
00:06:12,200 --> 00:06:17,320
And this is going to allow a lot of companies, a lot of AI model companies as well to make

104
00:06:17,320 --> 00:06:21,080
their own versions of these models, dependent on the use case.

105
00:06:21,080 --> 00:06:26,080
And that is a really cool strategy and something again, I really appreciate from meta.

106
00:06:26,080 --> 00:06:29,320
I think that those are just huge advantages that that we're going to see developers take

107
00:06:29,360 --> 00:06:33,160
and we're focusing on building out this partner ecosystem and there are going to be all these

108
00:06:33,160 --> 00:06:35,040
different capabilities that get built out around it.

109
00:06:35,040 --> 00:06:36,440
So yeah, and that's another thing.

110
00:06:36,440 --> 00:06:39,720
He's not doing this just to screw over the closed source companies.

111
00:06:39,720 --> 00:06:44,240
He actually believes in this as a true business strategy.

112
00:06:44,240 --> 00:06:49,560
If you build out the foundation for other companies to come build on top of you, then

113
00:06:49,560 --> 00:06:54,520
of course you get to set the standards and then you'll figure out ways to monetize over

114
00:06:54,520 --> 00:06:55,520
time.

115
00:06:55,520 --> 00:06:56,520
That's essentially what they did with Facebook.

116
00:06:56,520 --> 00:06:59,560
They built out a platform, other developers came and built on top of it.

117
00:06:59,560 --> 00:07:03,680
Now the counter example to that is Apple with the App Store.

118
00:07:03,680 --> 00:07:08,120
They built a completely closed system and then other developers because it was so popular

119
00:07:08,120 --> 00:07:10,040
came and built on top of it.

120
00:07:10,040 --> 00:07:13,840
And there's the alternative Android, which is the open version of the smartphone operating

121
00:07:13,840 --> 00:07:14,840
system.

122
00:07:14,840 --> 00:07:18,600
So we needed a strong open source player in AI and now we have it.

123
00:07:18,600 --> 00:07:19,600
All right.

124
00:07:19,600 --> 00:07:22,560
Next is something you all ask me about all the time whenever I put out a tutorial, whenever

125
00:07:22,560 --> 00:07:26,480
I put out some kind of news, you always ask me, okay, but what's the real world use case

126
00:07:26,480 --> 00:07:30,520
and that's something that I've been trying to include more and more in my videos.

127
00:07:30,520 --> 00:07:34,280
So now what Mark is going to talk about in this segment is what he sees as the real world

128
00:07:34,280 --> 00:07:35,280
use cases.

129
00:07:35,280 --> 00:07:38,960
He's probably going to start with the things that we all know are pretty obvious and are

130
00:07:38,960 --> 00:07:42,560
the basic intro use cases for AI.

131
00:07:42,560 --> 00:07:46,560
But I'm hoping he's also going to talk about the more sophisticated use cases and there's

132
00:07:46,560 --> 00:07:50,720
probably a ton of use cases we haven't even thought of yet or that the capabilities aren't

133
00:07:50,720 --> 00:07:52,320
quite there yet until today.

134
00:07:52,320 --> 00:07:53,320
So let's watch.

135
00:07:53,320 --> 00:08:00,320
The thing that I'm most excited about is seeing people use it to distill and fine tune their

136
00:08:00,320 --> 00:08:01,320
own models.

137
00:08:01,320 --> 00:08:02,320
Right.

138
00:08:02,320 --> 00:08:05,560
It's, I mean, like you're saying, I mean, this is the first open source frontier level

139
00:08:05,560 --> 00:08:08,400
model, but it's not the first frontier level model.

140
00:08:08,400 --> 00:08:12,240
So there have been other models that sort of have that capacity and yeah, people are going

141
00:08:12,240 --> 00:08:17,000
to want to do inference directly on the four or five because it's, you know, by our estimates,

142
00:08:17,000 --> 00:08:22,400
it's going to be at 50% cheaper, I think, than the GPT four or to do that directly.

143
00:08:22,480 --> 00:08:26,040
And so I think that that obviously makes a difference to a lot of people.

144
00:08:26,040 --> 00:08:30,480
But the thing that I think is really new in the world with this is the, because it's

145
00:08:30,480 --> 00:08:33,040
open weights, the ability to take the model.

146
00:08:33,040 --> 00:08:38,600
And by the way, it was rumored that Meta was not going to release the weights for the 405

147
00:08:38,600 --> 00:08:41,240
billion parameter model, but Mark Zuckerberg corrected them.

148
00:08:41,240 --> 00:08:44,000
And it seems like he held true to his promise.

149
00:08:44,000 --> 00:08:45,000
They did release it.

150
00:08:45,000 --> 00:08:46,720
It is completely open source.

151
00:08:46,720 --> 00:08:52,240
And I can truly say that completely open source and distill it down to whatever size

152
00:08:52,280 --> 00:08:56,840
that you want to use it for synthetic data generation, to use it as a teacher model.

153
00:08:57,640 --> 00:09:01,000
You know, so our vision for the future, it's not just, okay, it was never that there's

154
00:09:01,000 --> 00:09:02,280
going to be one singular thing.

155
00:09:02,280 --> 00:09:05,880
I think this is like open AI sort of as this vision that they're going to build kind of

156
00:09:05,880 --> 00:09:10,320
one big AI, Anthropic does to Google does to it's never been our vision.

157
00:09:10,320 --> 00:09:13,480
Our vision is that there should be lots of different models.

158
00:09:13,480 --> 00:09:19,160
I think every startup out there, every enterprise governments, they all kind of want to have

159
00:09:19,400 --> 00:09:21,280
their own custom models.

160
00:09:21,640 --> 00:09:26,000
And yeah, when the closed ecosystem was so much better than open source, it was just

161
00:09:26,000 --> 00:09:30,640
better to take the vanilla closed thing off the shelf, because even though you could

162
00:09:30,640 --> 00:09:33,960
customize open source, there was still some gap between the performance that you could

163
00:09:33,960 --> 00:09:35,600
get, but now we don't see that anymore.

164
00:09:35,600 --> 00:09:38,720
So this is actually something that I believe in really strongly.

165
00:09:38,720 --> 00:09:42,640
And if you've watched my videos as of the last few weeks and months, you've heard me

166
00:09:42,640 --> 00:09:48,960
say it, I truly believe that small vertical narrow use case models

167
00:09:49,160 --> 00:09:53,560
are going to be the future, especially, and again, something else I've talked about as

168
00:09:53,560 --> 00:09:57,560
AI compute continues to get pushed towards edge devices.

169
00:09:57,600 --> 00:10:02,880
The only way we're going to be able to have capable AI is by running small models, multiple

170
00:10:02,880 --> 00:10:05,160
small models on a device like this.

171
00:10:05,200 --> 00:10:10,360
And when we also have algorithmic innovations like route LLM and mixture of agents, all

172
00:10:10,360 --> 00:10:15,720
of a sudden, these small models become so much better and know when to offload to these

173
00:10:15,880 --> 00:10:19,320
giant kind of world knowledge models, if they have to.

174
00:10:19,360 --> 00:10:24,840
And funny, that is the approach that Apple intelligence took, but they just took the

175
00:10:24,840 --> 00:10:29,440
completely closed ecosystem approach, which is very Apple to do as open source basically

176
00:10:29,440 --> 00:10:30,320
closes the gap.

177
00:10:30,720 --> 00:10:37,000
I think you're just going to see this wide proliferation of models where people now have

178
00:10:37,000 --> 00:10:41,440
the incentive to basically customize and build and train exactly the right size model for

179
00:10:41,440 --> 00:10:45,120
what they're doing, train their data into it, they're going to have the tools to do it

180
00:10:45,120 --> 00:10:49,720
because of a lot of the partner integrations that the companies like Amazon are doing with

181
00:10:49,720 --> 00:10:54,440
AWS or Databricks or different folks like that, who are building these whole suites

182
00:10:54,440 --> 00:10:57,680
of services for distilling and fine tuning open models.

183
00:10:57,680 --> 00:10:59,920
So I think that that's going to be the thing that's new here.

184
00:10:59,920 --> 00:11:02,720
And that's really exciting is how far can that get pushed?

185
00:11:02,760 --> 00:11:06,800
And and that's a completely new capability in the world because there hasn't been an

186
00:11:06,800 --> 00:11:12,040
open source or open weight model of kind of this sophistication that's ever been released

187
00:11:12,040 --> 00:11:12,480
before.

188
00:11:12,520 --> 00:11:12,760
Yeah.

189
00:11:12,760 --> 00:11:14,520
And that's a really important point.

190
00:11:14,840 --> 00:11:20,360
The battle for unique and diverse data is really going to be the front lines of artificial

191
00:11:20,360 --> 00:11:21,000
intelligence.

192
00:11:21,000 --> 00:11:26,720
That is why, as I mentioned in a recent previous video, open AI has been building partnerships

193
00:11:26,720 --> 00:11:30,520
with numerous content companies like Time Magazine and so on.

194
00:11:30,600 --> 00:11:36,000
And so if you're a company and you have this very unique, very proprietary data set, you can

195
00:11:36,000 --> 00:11:43,840
now take these large frontier open source models and train them on your particular use case for

196
00:11:43,840 --> 00:11:48,320
your business, whether you want to use it internally or resell it to your industry.

197
00:11:48,320 --> 00:11:50,960
And I think that's a really interesting approach.

198
00:11:51,080 --> 00:11:51,320
All right.

199
00:11:51,320 --> 00:11:56,640
In this next section, Rowan asks Mark Zuckerberg about how they're going to teach the world

200
00:11:56,640 --> 00:12:00,160
how to use these AI models and specifically about open source and its benefits.

201
00:12:00,160 --> 00:12:01,480
So let's watch what he has to say.

202
00:12:01,560 --> 00:12:01,840
Yeah.

203
00:12:01,840 --> 00:12:06,680
So I'd say before a 3.1 hour approach, I mean, the reason that Meta fundamentally is

204
00:12:06,680 --> 00:12:11,480
investing in this is we basically want to know that we have access to, to a leading

205
00:12:11,480 --> 00:12:17,040
model, you know, because of some of our, our history of kind of how mobile worked and

206
00:12:17,040 --> 00:12:22,240
things like that, we didn't want to be in a position where we had to rely on some competitor.

207
00:12:22,280 --> 00:12:22,720
All right.

208
00:12:22,800 --> 00:12:26,080
So he kind of revealed why he's employing this strategy.

209
00:12:26,360 --> 00:12:31,360
During the mobile revolution, Facebook got caught flat footed.

210
00:12:31,360 --> 00:12:36,840
They basically were completely platform dependent on Apple or Android.

211
00:12:36,840 --> 00:12:41,800
And they even tried to make their own phone at a certain point, although it was a failed

212
00:12:41,800 --> 00:12:42,480
project.

213
00:12:42,480 --> 00:12:46,680
So he is obviously not wanting to make that same mistake again.

214
00:12:46,720 --> 00:12:47,920
We built it for ourselves.

215
00:12:48,280 --> 00:12:53,680
And before a 3.1, you know, we, we kind of had this instinct that if we made it open

216
00:12:53,680 --> 00:12:57,400
source, there would be a community that would grow around it and that would actually extend

217
00:12:57,400 --> 00:12:59,920
the capabilities and make it more valuable for everyone, including us.

218
00:13:00,120 --> 00:13:02,120
Because at the end of the day, this isn't just a technology.

219
00:13:02,120 --> 00:13:03,360
It's an ecosystem, right?

220
00:13:03,360 --> 00:13:04,680
That, that, that you're developing.

221
00:13:04,680 --> 00:13:08,280
So in order for this to end up being a useful thing for us, there also needs to be a broad

222
00:13:08,280 --> 00:13:08,880
ecosystem.

223
00:13:09,520 --> 00:13:13,400
One of the big changes that I think we see with llama 3.1 is instead of just building

224
00:13:13,400 --> 00:13:17,600
it for ourselves and throwing it over the wall and letting developers use it, this time

225
00:13:17,600 --> 00:13:23,840
we're really taking a much more proactive stance on building partnerships and making

226
00:13:23,840 --> 00:13:29,240
sure that there's this whole ecosystem of companies that can do interesting things with

227
00:13:29,240 --> 00:13:31,840
the model and conserve developers in ways that we're not going to.

228
00:13:31,960 --> 00:13:36,480
Okay, really, what that translates into is they want control of the ecosystem.

229
00:13:36,480 --> 00:13:38,440
They want to be able to define the standards.

230
00:13:38,680 --> 00:13:40,720
They want to build up the ecosystem.

231
00:13:40,720 --> 00:13:44,800
So there's obviously a financial motive for meta and it's not just purely out of the

232
00:13:44,800 --> 00:13:48,080
goodness of their heart that they release this model for free, but that's okay.

233
00:13:48,120 --> 00:13:49,400
Everybody can still win.

234
00:13:49,480 --> 00:13:53,360
And quickly, I want to show Mark Zuckerberg talking about GROC, which is, as you know,

235
00:13:53,360 --> 00:13:56,600
one of my favorite companies out there right now in the world of AI.

236
00:13:56,640 --> 00:14:00,720
The time I think that there are also going to be folks like GROC, right, who are doing

237
00:14:00,760 --> 00:14:06,840
really interesting work on really kind of ultra low latency inference.

238
00:14:06,840 --> 00:14:08,840
And I'm really excited to get this in their hands.

239
00:14:08,840 --> 00:14:11,400
And they're building something for launch that basically.

240
00:14:11,520 --> 00:14:15,040
Okay, so obviously this was filmed a few days before launch, if not more.

241
00:14:15,080 --> 00:14:18,120
And GROC on day one already has it available.

242
00:14:18,360 --> 00:14:21,160
I haven't been able to use it because there's been so many people trying.

243
00:14:21,160 --> 00:14:23,120
It basically says they're bandwidth limited.

244
00:14:23,120 --> 00:14:25,400
But as soon as I can, I'm definitely going to try it out.

245
00:14:25,440 --> 00:14:29,720
Next, Rowan asks Mark Zuckerberg what the implications are of open source AI.

246
00:14:29,720 --> 00:14:34,320
And this has been a fierce debate amongst obviously close source companies like

247
00:14:34,320 --> 00:14:38,960
open AI and open source companies like Meta AI, Mark Zuckerberg, Yann LeCun.

248
00:14:39,080 --> 00:14:44,960
And what I really appreciate about the open source approach is that it hopefully puts

249
00:14:45,000 --> 00:14:49,640
a stop to any regulatory capture that might be happening and likely is happening

250
00:14:49,640 --> 00:14:52,680
from the likes of Google and open AI.

251
00:14:52,920 --> 00:14:54,840
They want regulation.

252
00:14:54,840 --> 00:15:00,680
They want the hurdle to start a new AI company, a new innovative frontier AI model

253
00:15:00,680 --> 00:15:04,200
to be as high as possible because they're already on that side of the fence.

254
00:15:04,320 --> 00:15:06,360
They don't want anybody else at their party.

255
00:15:06,440 --> 00:15:14,440
My view is that open source is a really important ingredient to having a positive AI future.

256
00:15:14,600 --> 00:15:18,200
And that there are all these awesome things that AI is going to bring in terms

257
00:15:18,200 --> 00:15:22,200
of productivity gains and creativity enhancements for people.

258
00:15:22,200 --> 00:15:24,800
And hopefully it'll help us with research and things like that.

259
00:15:25,120 --> 00:15:29,440
But I think open source is an important part of how we make sure that this benefits

260
00:15:29,440 --> 00:15:31,480
everyone and is accessible to everyone.

261
00:15:31,760 --> 00:15:35,600
It isn't something that's just locked into a handful of big companies.

262
00:15:35,800 --> 00:15:40,760
At the same time, I actually think that open source is going to end up being the

263
00:15:40,760 --> 00:15:43,480
safer and more secure way to develop AI.

264
00:15:43,720 --> 00:15:47,360
I know that there's sort of a debate today about is open source safe?

265
00:15:47,760 --> 00:15:49,920
And I actually take the different position on it.

266
00:15:49,920 --> 00:15:51,000
It's not only do I think it's safe.

267
00:15:51,000 --> 00:15:53,360
I think it's safer than the alternative of closed development.

268
00:15:53,360 --> 00:15:56,720
And yeah, so I'm going to cut him off for a second because I kind of already know

269
00:15:56,720 --> 00:16:00,120
where he's going, something I've already talked about in previous videos.

270
00:16:00,160 --> 00:16:02,680
And he actually talked about it a little bit earlier in this video.

271
00:16:02,800 --> 00:16:07,320
Basically, when you open source something and everybody with diverse skills,

272
00:16:07,320 --> 00:16:12,960
diverse perspectives and a much larger pool of talent can look and examine

273
00:16:13,000 --> 00:16:16,320
every single line of code, every single piece of data, how it's behaving,

274
00:16:16,320 --> 00:16:19,880
why it's doing certain things, you're going to harden the system.

275
00:16:19,960 --> 00:16:23,280
Much more so than closed source systems.

276
00:16:23,320 --> 00:16:25,840
Now, there's some examples where that's not the case.

277
00:16:25,840 --> 00:16:29,560
There's some examples where that is the case, but I think generally speaking,

278
00:16:29,800 --> 00:16:33,160
open source tends to be more secure than closed source systems.

279
00:16:33,200 --> 00:16:35,200
If you agree, let me know in the comments.

280
00:16:35,200 --> 00:16:37,800
I'm not sure if that's even a controversial statement or not.

281
00:16:37,840 --> 00:16:41,240
You know, I sort of break it down into, you know, there are lots of different

282
00:16:41,240 --> 00:16:44,920
kinds of harm, so you can't just talk about one type of thing.

283
00:16:45,200 --> 00:16:48,920
But on this, I think that there's unintentional harms.

284
00:16:48,920 --> 00:16:53,000
So the system goes off the rails in some way that people didn't intend.

285
00:16:53,520 --> 00:16:56,440
And then there's intentional harms where you have like some bad actors

286
00:16:56,440 --> 00:16:59,800
trying to use the system to do something bad when it comes to unintentional harms,

287
00:16:59,800 --> 00:17:03,320
which I think, by the way, it's worth noting that like most of the sci-fi

288
00:17:03,320 --> 00:17:08,360
scenarios that people worry about of AI just going rogue are kind of unintentional.

289
00:17:08,640 --> 00:17:13,520
I actually think that open source should be safer on that because it's

290
00:17:14,080 --> 00:17:16,680
it will have more scrutiny, they'll have more transparency.

291
00:17:17,640 --> 00:17:21,640
And I think all the developers who use it with all the Lama Guard

292
00:17:21,640 --> 00:17:23,640
and the safety tools that it comes with,

293
00:17:25,080 --> 00:17:27,720
there's going to be so much scrutiny and testing and pressure on those

294
00:17:27,960 --> 00:17:32,480
that my guess is that it will have kind of just like traditional open

295
00:17:32,480 --> 00:17:36,280
source software, any kind of issues with it, I think will be ironed out

296
00:17:36,280 --> 00:17:39,840
and fixed a lot quicker than with the closed models.

297
00:17:39,840 --> 00:17:43,080
So I think you've got you've got that on kind of unintentional harm,

298
00:17:43,080 --> 00:17:46,360
which is why I think most of the discussion around safety for open

299
00:17:46,360 --> 00:17:49,320
source revolves around intentional harm. It's OK, it's open.

300
00:17:49,320 --> 00:17:52,280
It's out there. How are you going to stop bad actors from doing it?

301
00:17:52,680 --> 00:17:53,840
Doing bad things with it?

302
00:17:53,840 --> 00:17:57,040
There, I think you basically want to probably divide the problem

303
00:17:57,760 --> 00:18:00,960
into kind of smaller actors like an individual or

304
00:18:01,840 --> 00:18:06,040
or some kind of smaller group that's trying to create some some some mayhem

305
00:18:06,640 --> 00:18:10,600
and the larger actors who are more sophisticated and have huge amounts

306
00:18:10,600 --> 00:18:12,680
of resources like big nation states.

307
00:18:12,720 --> 00:18:15,240
I think it's kind of a different mix for the two of those.

308
00:18:15,400 --> 00:18:18,960
This reminds me of something that Jan LeCun talked about in his interview

309
00:18:18,960 --> 00:18:21,840
with Lex Friedman. I'm actually forgetting the name he called this,

310
00:18:21,840 --> 00:18:26,680
but he essentially said that if everybody has open source frontier models

311
00:18:26,840 --> 00:18:29,800
that are incredibly capable, as capable as closed source,

312
00:18:30,080 --> 00:18:33,800
then it's basically a battle of AI versus AI.

313
00:18:33,800 --> 00:18:37,760
If there's a bad actor and a good actor with AI, whoever has the better AI

314
00:18:37,760 --> 00:18:42,160
is going to win. And if both sides have equal AI, or if they're just off

315
00:18:42,160 --> 00:18:47,360
by, let's say, 5% in terms of capabilities, then it's pretty much null.

316
00:18:47,560 --> 00:18:51,240
And one AI is going to protect the good actors from the bad actors.

317
00:18:51,240 --> 00:18:55,080
AI. Now, the instance that we're not talking about,

318
00:18:55,080 --> 00:19:00,000
which I actually don't think is possible is if bad actors with huge resources

319
00:19:00,000 --> 00:19:05,720
suddenly get a massive jump in capabilities in AI

320
00:19:05,760 --> 00:19:07,760
that nobody else even thought was possible.

321
00:19:07,920 --> 00:19:15,320
So imagine all of a sudden a bad actors AI is 100% better than the good actors AI.

322
00:19:15,320 --> 00:19:17,640
At that point, we'd probably be in trouble.

323
00:19:17,640 --> 00:19:21,440
But as I said, I don't even believe that's possible just because of the iterative

324
00:19:21,440 --> 00:19:24,440
nature of artificial intelligence innovation.

325
00:19:24,480 --> 00:19:28,320
There just aren't these huge step functions in capability gain.

326
00:19:28,440 --> 00:19:32,160
You know, for the smaller actors, my view on this is that, you know,

327
00:19:32,160 --> 00:19:36,240
the way that we've, I think that having a balance of power on this is super important.

328
00:19:36,840 --> 00:19:39,400
You know, what we've done in managing our social networks is we have all these

329
00:19:39,400 --> 00:19:43,240
kind of bad actors who are trying to do kind of bad stuff on our networks.

330
00:19:43,240 --> 00:19:46,240
And a lot of times they deploy AI systems to do that.

331
00:19:46,240 --> 00:19:48,960
And the way that we stop them and identify them is by having more

332
00:19:48,960 --> 00:19:53,080
sophisticated AI systems that have more compute to go find what they're doing.

333
00:19:53,400 --> 00:19:57,160
So I think that this is actually pretty similar to the governments and law

334
00:19:57,160 --> 00:20:00,000
enforcement essentially maintain order in society.

335
00:20:00,200 --> 00:20:03,240
It's like, yeah, you have a bunch of rogue people who might be committing crimes.

336
00:20:03,480 --> 00:20:07,360
But, you know, generally the police forces and the militaries are much

337
00:20:07,360 --> 00:20:09,000
better funded, have more resources.

338
00:20:09,240 --> 00:20:10,880
And I think that that's basically going to be true here.

339
00:20:11,200 --> 00:20:15,000
As a matter of fact, I think what you want is for open source to be widely deployed,

340
00:20:15,000 --> 00:20:18,160
which I think that there's sort of a risk if it's closed, that that's not the case.

341
00:20:18,160 --> 00:20:21,000
But when it's open, you're going to have all these big institutions

342
00:20:21,320 --> 00:20:25,200
that have a ton of resources that they can basically deploy these systems

343
00:20:25,200 --> 00:20:27,720
in a way that I think will check bad actors.

344
00:20:27,720 --> 00:20:32,760
Then you get to the question of basically, you know, folks like

345
00:20:32,760 --> 00:20:35,360
China or like large, sophisticated actors.

346
00:20:35,680 --> 00:20:38,640
And one of the questions that you sometimes hear debated is like, OK,

347
00:20:38,800 --> 00:20:42,000
if you're open sourcing the really advanced models, how do you make it so

348
00:20:42,000 --> 00:20:46,360
that that it doesn't get to to China or they're not going to use that against us?

349
00:20:46,360 --> 00:20:49,920
And that's sometimes an argument that people have for, hey, you should lock down development.

350
00:20:49,920 --> 00:20:52,480
But I think that that's sort of missing a few things.

351
00:20:52,760 --> 00:20:56,360
One is that in order for this all to work, the US has to have an advantage

352
00:20:56,360 --> 00:20:58,480
in the first place or the West.

353
00:20:58,480 --> 00:21:03,600
And in kind of our advantage is basically open and decentralized innovation,

354
00:21:03,600 --> 00:21:06,880
where it's not just a small number of big companies or labs at startups

355
00:21:06,880 --> 00:21:11,200
and universities and individuals hacking on things who are in parts of companies.

356
00:21:11,200 --> 00:21:12,600
And that's a big part of it.

357
00:21:12,600 --> 00:21:13,960
And you don't want to shut that down.

358
00:21:13,960 --> 00:21:17,280
And I think if you do, you increase the chance that we don't even lead in the first place.

359
00:21:17,640 --> 00:21:22,840
But then I think you get to the the issue, which is, OK, China or not even China,

360
00:21:22,840 --> 00:21:27,480
any government, you know, there are all the risks of kind of stealing the models

361
00:21:27,480 --> 00:21:31,240
and an espionage, I mean, a lot of the models fit on, you know, a hard drive

362
00:21:31,240 --> 00:21:34,360
that you can quickly put in your backpack or whatever.

363
00:21:34,360 --> 00:21:40,920
And it's I just think we need to be realistic about how likely it is that we can secure

364
00:21:41,520 --> 00:21:45,000
and not just not us, but like any of the tech companies can secure any of these

365
00:21:45,680 --> 00:21:49,240
models long term against very sophisticated efforts to do that.

366
00:21:49,360 --> 00:21:54,040
So if you remember my video about the situational awareness paper by Leopold

367
00:21:54,080 --> 00:21:58,080
Aschenbrenner, he specifically calls out China, but let's just use any large state

368
00:21:58,080 --> 00:22:05,080
actor, the chances of being able to protect a closed source model, the weights

369
00:22:05,240 --> 00:22:08,800
indefinitely is essentially zero in my mind.

370
00:22:08,920 --> 00:22:13,600
How many times have US companies been hacked specifically by China and their IP stolen?

371
00:22:13,880 --> 00:22:16,640
And here's the thing, it only needs to happen once.

372
00:22:16,960 --> 00:22:23,120
You only need to have one minor lapse in security and then you lose the model weights.

373
00:22:23,240 --> 00:22:28,120
So with the likelihood of a private company like open AI getting hacked, having their

374
00:22:28,120 --> 00:22:33,600
model weights stolen, then let's just assume state actors are going to have these AI models.

375
00:22:33,600 --> 00:22:36,880
So let's just make sure everybody has them.

376
00:22:37,000 --> 00:22:41,280
Let's make sure all of the good guys, quote unquote, good guys, quote unquote, bad guys,

377
00:22:41,560 --> 00:22:44,320
they all have the same capabilities.

378
00:22:44,320 --> 00:22:49,560
And thus we're back to the original statement of it's one AI versus another AI.

379
00:22:49,720 --> 00:22:55,440
My own fear is that if we lock down development, we end up in a world where basically you have

380
00:22:55,440 --> 00:22:59,640
a small number of companies, plus all the adversaries who can steal the model are the

381
00:22:59,640 --> 00:23:00,720
only ones who have access.

382
00:23:01,000 --> 00:23:05,440
But all the startups, all the universities, all the individual hackers are kind of just

383
00:23:05,440 --> 00:23:07,720
left out and don't have the ability to do this.

384
00:23:07,720 --> 00:23:16,400
So my own view is that a realistic aim that we should hope for is that we use open

385
00:23:16,400 --> 00:23:20,920
source to basically develop the leading and most robust ecosystem in the world, in

386
00:23:20,920 --> 00:23:27,160
that we have an expectation that our companies work closely with our government and allied

387
00:23:27,160 --> 00:23:32,200
governments on national security so that way our governments can persistently just be

388
00:23:32,200 --> 00:23:36,240
integrating the latest technology and have, you know, whatever it is, a six month advantage,

389
00:23:36,240 --> 00:23:38,040
eight month advantage on our adversaries.

390
00:23:38,040 --> 00:23:41,280
And I think that that's, you know, I don't know that that in this world, you get a 10

391
00:23:41,280 --> 00:23:46,240
year permanent advantage, but I think a kind of perpetual lead actually will make us more

392
00:23:46,240 --> 00:23:50,960
safe in one where we're leading, then the model that others are advocating, which is,

393
00:23:50,960 --> 00:23:55,080
okay, you have a small number of closed labs, they lock down development, we probably risk

394
00:23:55,080 --> 00:23:59,520
being in the lead at all, like probably the other governments are getting access to it.

395
00:23:59,800 --> 00:24:01,160
It's that that's my view.

396
00:24:01,200 --> 00:24:06,680
I actually think on both these things, spreading prosperity for more evenly around the world,

397
00:24:06,680 --> 00:24:10,680
making it so that there can be more progress and on safety, I think we're basically just

398
00:24:10,680 --> 00:24:12,520
going to find over time that open source leads.

399
00:24:12,720 --> 00:24:14,320
Look, there are going to be issues, right?

400
00:24:14,320 --> 00:24:15,600
It's like, well, to mitigate the issues.

401
00:24:15,600 --> 00:24:16,880
We're going to test everything rigorously.

402
00:24:16,880 --> 00:24:18,800
We do, we work with governments and all the stuff.

403
00:24:19,080 --> 00:24:20,160
We'll continue doing that.

404
00:24:20,520 --> 00:24:24,440
Um, but that's my view of kind of where the equilibrium, I think we'll settle out, given

405
00:24:24,440 --> 00:24:25,160
what I know today.

406
00:24:25,240 --> 00:24:30,520
So next, Mark Zuckerberg is going to start talking about economic possibilities with

407
00:24:30,520 --> 00:24:31,560
the use of AI.

408
00:24:31,560 --> 00:24:35,600
And this is something that I'm extremely curious and excited about.

409
00:24:35,640 --> 00:24:37,400
I tend to be an optimist.

410
00:24:37,440 --> 00:24:41,920
I can also see the pessimist point of view here and open AI just came out with a bunch of

411
00:24:41,920 --> 00:24:45,880
research talking about UBI and economic effects of AI.

412
00:24:45,880 --> 00:24:48,800
And I'm going to be reading and covering that in a subsequent video.

413
00:24:48,960 --> 00:24:51,520
But for now, let's see what Mark Zuckerberg has to say about it.

414
00:24:51,600 --> 00:24:56,040
There's a version of this, which AI will do no matter how it's developed.

415
00:24:56,640 --> 00:25:00,600
Um, and then there's a version of this that I think benefits from open source specifically.

416
00:25:00,600 --> 00:25:05,040
So I think that AI has more potential than any other single technology that's

417
00:25:05,040 --> 00:25:10,040
being developed right now to increase productivity, accelerate the economy, um,

418
00:25:10,040 --> 00:25:14,200
make it set kind of every person has the ability to be more creative and, and,

419
00:25:14,440 --> 00:25:16,320
and produce more interesting things.

420
00:25:16,320 --> 00:25:17,800
And I think that that's all going to be great.

421
00:25:17,800 --> 00:25:21,880
I also think I hope that it'll help out with science and, um, medical research

422
00:25:21,880 --> 00:25:23,000
and things like that.

423
00:25:23,040 --> 00:25:27,920
There are a lot of folks today, though, who don't necessarily have access to the

424
00:25:27,920 --> 00:25:31,640
ability to fine tune or build their own state of the art models.

425
00:25:31,720 --> 00:25:35,120
So they're sort of limited to what these large labs do.

426
00:25:35,520 --> 00:25:39,760
Um, and like I just said, I think, um, you know, one of the defining

427
00:25:39,880 --> 00:25:44,680
aspects of our culture and innovation as a sort of a country or, or society is

428
00:25:44,680 --> 00:25:47,200
like, it's not just big companies that do it, right?

429
00:25:47,200 --> 00:25:50,680
There's all these startups and hackers and academics and people in university.

430
00:25:51,040 --> 00:25:55,800
And I think you want to give all of those folks access to state of the art models

431
00:25:55,800 --> 00:25:58,720
that they can build on top of, not just that they can run, which is what they have

432
00:25:58,720 --> 00:26:03,400
today with, with the closed vendors, but that they can build on top of and tweak

433
00:26:03,400 --> 00:26:07,480
and distill down to smaller models that they can run on their laptop or their

434
00:26:07,480 --> 00:26:09,840
phone or whatever other device they're building.

435
00:26:09,840 --> 00:26:12,560
And I think that that's just going to unlock a ton of progress.

436
00:26:12,920 --> 00:26:17,040
There's also a version of this where there are, you can look at it by, you know,

437
00:26:17,040 --> 00:26:18,160
nation too.

438
00:26:18,600 --> 00:26:21,760
Um, you know, so it's not just that startups might not have the resources

439
00:26:21,760 --> 00:26:25,180
or universities might not have the resources to go train their own, um,

440
00:26:25,200 --> 00:26:28,280
they know, large scale foundation models now or in the future.

441
00:26:28,560 --> 00:26:31,280
But, um, but there were a lot of countries that aren't going to have

442
00:26:31,280 --> 00:26:34,840
the ability to do that because I mean, you know, pretty soon these things

443
00:26:34,840 --> 00:26:37,440
are going to cost many billions of dollars to train.

444
00:26:37,480 --> 00:26:42,160
And I think that having the ability for different countries and entrepreneurs

445
00:26:42,160 --> 00:26:47,080
and different countries and businesses to use it to serve people better and just

446
00:26:47,080 --> 00:26:52,080
do better work is going to be something that, that basically like lifts all boats

447
00:26:52,080 --> 00:26:56,640
around the world and, um, which just has a massive kind of equalizing effect.

448
00:26:56,640 --> 00:26:58,640
So I know that that's really positive.

449
00:26:58,680 --> 00:27:02,760
Here, Rowan asked Mark about specifically in his letter, which I'll

450
00:27:02,800 --> 00:27:07,480
cover in another video, but he directly called out Apple in their closed approach

451
00:27:07,480 --> 00:27:11,040
and Rowan asked him to elaborate on it and what are his thoughts.

452
00:27:11,040 --> 00:27:11,760
So let's listen.

453
00:27:11,880 --> 00:27:17,360
I mean, my point in there is more, it's a little more philosophical on how

454
00:27:17,360 --> 00:27:23,760
it's affected my own kind of approach towards things and, and psychologically

455
00:27:23,760 --> 00:27:26,000
sort of affected how I think about building stuff.

456
00:27:26,440 --> 00:27:29,080
Um, I actually don't know how they're going to approach AI.

457
00:27:29,360 --> 00:27:31,360
Um, you know, they do some open development.

458
00:27:31,360 --> 00:27:32,600
I do some closed development.

459
00:27:32,920 --> 00:27:35,560
Um, you know, by the way, I think it's worth noting, like, I don't actually

460
00:27:35,560 --> 00:27:37,360
consider myself to be an open source as Ellen.

461
00:27:37,360 --> 00:27:40,520
I just think that in this case, um, I think that open models are going to be

462
00:27:40,520 --> 00:27:42,520
the standard and I think that that's going to be good for the world, but we

463
00:27:42,520 --> 00:27:44,240
do open development, we do close development.

464
00:27:44,240 --> 00:27:44,960
So I get it, right?

465
00:27:44,960 --> 00:27:47,680
And I'm not saying that Apple is necessarily going to be on the wrong place

466
00:27:47,680 --> 00:27:52,440
on this for AI, but if you look back over the last 10 or 15 years, it has

467
00:27:52,440 --> 00:27:58,240
been a formative experience for us is building our services on top of platforms

468
00:27:58,240 --> 00:28:03,120
that are controlled by our competitors and for a number of different incentives.

469
00:28:03,200 --> 00:28:06,800
Oh, I can just sense, I can just sense the anger here.

470
00:28:06,880 --> 00:28:12,160
And I can tell he's being very diplomatic about the way he's saying all of this.

471
00:28:12,480 --> 00:28:18,160
But I know he got burned and, and burned for years on the fact that he had to

472
00:28:18,160 --> 00:28:23,920
build his platforms during the mobile revolution, which really took over everything.

473
00:28:23,920 --> 00:28:29,040
He had to build Facebook and the rest of his app portfolio on top of his competitors.

474
00:28:29,040 --> 00:28:30,960
And he doesn't want to make that mistake again.

475
00:28:30,960 --> 00:28:36,800
That is also likely why they invested and he invested so much in the metaverse

476
00:28:36,800 --> 00:28:42,320
because he really foresaw the VR revolution as being the next computing platform.

477
00:28:42,320 --> 00:28:44,480
And so he wanted to be way ahead of that.

478
00:28:44,480 --> 00:28:45,600
And he was.

479
00:28:45,600 --> 00:28:50,960
And although it didn't come to fruition either as quickly or at all, the way he

480
00:28:51,040 --> 00:28:54,240
thought it would, AI seemingly is heading in that same direction.

481
00:28:54,240 --> 00:28:56,080
And he wants to be the platform.

482
00:28:56,080 --> 00:28:57,680
Meta wants to be the platform.

483
00:28:57,680 --> 00:29:03,680
They absolutely, from my perspective, apply different rules to kind of limit what we can do.

484
00:29:03,680 --> 00:29:06,000
And, and yeah, they have all these taxes.

485
00:29:06,000 --> 00:29:10,880
And, you know, at some point we did, we've done some analysis that we think we'd be

486
00:29:10,880 --> 00:29:14,880
way more profitable if it weren't for some of these arbitrary rules.

487
00:29:14,880 --> 00:29:17,280
And I think a lot of other businesses would be too.

488
00:29:17,280 --> 00:29:21,440
But, you know, honestly, the money part, I think it's annoying.

489
00:29:21,440 --> 00:29:22,880
But for me, it's not the biggest thing.

490
00:29:22,880 --> 00:29:28,160
It's, I think it's a little bit soul crushing when you go build features that are what you

491
00:29:28,160 --> 00:29:30,560
believe is good for your community.

492
00:29:30,560 --> 00:29:35,040
And then you're told that you can't ship them because some company wants to put you in a box

493
00:29:35,040 --> 00:29:36,960
so that they can better compete with you.

494
00:29:36,960 --> 00:29:38,640
Don't mess with the Zuck.

495
00:29:38,640 --> 00:29:41,120
He thinks in 4D chess.

496
00:29:41,120 --> 00:29:44,640
My concern for AI at this point isn't actually Apple.

497
00:29:44,640 --> 00:29:47,680
It's more the other companies and how that would evolve.

498
00:29:47,680 --> 00:29:50,160
And I think to some degree, it's not even that.

499
00:29:50,160 --> 00:29:52,800
I'm not even saying that they're like bad people.

500
00:29:52,800 --> 00:29:57,920
It's, it's, I think that there's just a physics and incentive structure to the system where,

501
00:29:57,920 --> 00:30:02,400
you know, if you build a closed system, then eventually there are all these forces on you

502
00:30:02,400 --> 00:30:06,960
that sort of kind of push you to, to kind of clamp down on things.

503
00:30:06,960 --> 00:30:15,280
And I think that it will be a healthier ecosystem if it's developed more like the web,

504
00:30:15,280 --> 00:30:16,240
but more capable.

505
00:30:16,240 --> 00:30:20,320
And I think that, you know, because of how mobile developed where the closed model won,

506
00:30:20,320 --> 00:30:20,560
right?

507
00:30:20,560 --> 00:30:24,640
And it's like Apple, I think, has really reaped most of the benefits in terms of,

508
00:30:24,640 --> 00:30:28,400
you know, they, there might be more Android phones out there, but like Apple gets like

509
00:30:28,400 --> 00:30:30,800
almost all the profits of, for mobile phones.

510
00:30:31,360 --> 00:30:35,360
I think there's a bit of recency bias because these are, these are long cycles.

511
00:30:35,360 --> 00:30:38,800
Right? I mean, the iPhone, I think it came out in 2007, right?

512
00:30:38,800 --> 00:30:40,560
So we're almost 20 years into this thing.

513
00:30:40,560 --> 00:30:45,600
It's a long cycle, but it's easy to forget the fact that the closed model doesn't always win.

514
00:30:46,560 --> 00:30:52,000
If you go back to PCs, now I know a lot of people have, especially if you're using the

515
00:30:52,000 --> 00:30:57,120
Linux analogy, people don't necessarily consider windows to be maximally open, but compared to

516
00:30:57,120 --> 00:31:02,480
the, the Apple approach of kind of coupling your operating system with, with the device,

517
00:31:03,200 --> 00:31:07,040
the windows approach was a more open ecosystem and it won.

518
00:31:07,600 --> 00:31:13,360
And part of my hope for the next generation of platforms, which includes both AI and the

519
00:31:13,360 --> 00:31:17,760
work that we're doing and augmented in virtual reality is to, you know, meta wants to be on the

520
00:31:17,760 --> 00:31:19,520
side of building the open ecosystems.

521
00:31:20,080 --> 00:31:23,600
And it's not just that we want to build something that's an alternative to the closed

522
00:31:23,600 --> 00:31:27,680
ecosystem. I want to restore the industry to the state where the open ecosystem is actually

523
00:31:27,680 --> 00:31:28,800
the one that is leading.

524
00:31:28,800 --> 00:31:34,320
As I said earlier, his motivations for doing this are clearly because he got burned.

525
00:31:34,320 --> 00:31:40,720
That is why he is trying to change the way that this ecosystem is going to be developed

526
00:31:40,720 --> 00:31:42,240
and play out in the long run.

527
00:31:42,240 --> 00:31:45,200
And I kind of love it. I love this approach.

528
00:31:45,200 --> 00:31:52,000
This is also why Elon Musk decided to open source grok because he was bitter at open AI

529
00:31:52,000 --> 00:31:56,560
being closed AI, taking a bunch of his money and then eventually converting into a for-profit

530
00:31:56,560 --> 00:32:02,400
company. And so there is nothing like a burned entrepreneur with a chip on their shoulder

531
00:32:02,400 --> 00:32:05,920
and a little bit of spite to really drive them to innovate.

532
00:32:05,920 --> 00:32:09,120
And let me tell you, I am here for it. I love it.

533
00:32:09,120 --> 00:32:12,480
All right. So Rowan now asks Mark Zuckerberg about Lama 4.

534
00:32:12,480 --> 00:32:17,200
And I'm sure he's already planning Lama 4, but he just released Lama 3.5.

535
00:32:17,200 --> 00:32:19,520
He just released the 405B model.

536
00:32:19,520 --> 00:32:23,680
And so I think all of their efforts, probably for the foreseeable future,

537
00:32:23,760 --> 00:32:29,920
are going to be on iterating and innovating on Lama 3, Lama 3.1, Lama 3.2.

538
00:32:29,920 --> 00:32:33,040
And they'll probably have Lama 4 cooking in the background,

539
00:32:33,040 --> 00:32:35,200
but it's going to be a while before we see that.

540
00:32:35,200 --> 00:32:40,880
And I'm going to guess that's probably going to come out a little bit after GPT-5 comes out.

541
00:32:40,880 --> 00:32:45,680
Oh man. I mean, it's, you know, we're just doing 3.1 for Lama now.

542
00:32:45,680 --> 00:32:52,080
I think it might be a little early to talk about Lama 4, but we've got the compute cluster set up.

543
00:32:52,400 --> 00:32:55,040
All right. I think that was hilarious.

544
00:32:55,040 --> 00:32:59,040
He's like, oh, it's probably a little bit too early to be thinking about Lama 4,

545
00:32:59,040 --> 00:33:04,160
but we have all the compute necessary to do it. I absolutely love this new version of Zuck.

546
00:33:04,160 --> 00:33:08,880
The data setup, we kind of have a sense of what the architecture is going to be,

547
00:33:08,880 --> 00:33:12,720
and have run a bunch of research experiments to kind of max that out.

548
00:33:12,720 --> 00:33:18,080
So I do think that Lama 4 is going to be another big leap on top of Lama 3.

549
00:33:18,080 --> 00:33:21,520
I think we have a bunch more progress that we can make.

550
00:33:21,520 --> 00:33:23,680
I mean, this is the first dot release for Lama.

551
00:33:23,680 --> 00:33:30,880
There's more that I'd like to do, including launching the multimodal models, which we...

552
00:33:30,880 --> 00:33:35,520
Yes. I can't wait for that. That is the one biggest missing capability between Lama 3,

553
00:33:35,520 --> 00:33:42,240
4.0.5.B, and GPT-4.0. GPT-4.0 can take many different file formats, interpret them,

554
00:33:42,240 --> 00:33:46,720
including images, and really that one feature I use all the time.

555
00:33:47,200 --> 00:33:51,760
Unfortunately, we have not had a Lama model that is really truly multimodal,

556
00:33:51,760 --> 00:33:55,920
and there have been a few fine-tuned versions that allow for multimodality,

557
00:33:55,920 --> 00:34:00,240
but they don't work super well if I'm being honest. You've seen me test them on this channel,

558
00:34:00,240 --> 00:34:03,840
so I can't wait for a native multimodal Lama 3.1.

559
00:34:03,840 --> 00:34:06,800
We kind of had an unfortunate setback on that,

560
00:34:08,080 --> 00:34:11,120
but I think we're going to be launching them probably everywhere outside of the EU.

561
00:34:11,840 --> 00:34:16,400
For those who are wondering what he's talking about, just recently it was reported that they

562
00:34:16,400 --> 00:34:21,360
are not going to be releasing multimodal AI in the EU strictly because of their regulations,

563
00:34:21,360 --> 00:34:27,200
and that is the setback that he's talking about. I can't wait till they release it here in the US

564
00:34:27,200 --> 00:34:32,080
and other countries that allow it, but not going to be in the EU, and that is one reason why they

565
00:34:32,080 --> 00:34:38,480
need to ease up on the regulation in the EU, and hopefully we don't over-regulate in the US,

566
00:34:38,480 --> 00:34:42,480
and from where I'm from, California. Yeah, probably a little early to talk about Lama 4,

567
00:34:42,960 --> 00:34:46,640
but it is going to be awesome, and it has been one of the interesting things in running the

568
00:34:46,640 --> 00:34:55,760
company is basically planning out the compute clusters and data trajectories for not just

569
00:34:55,760 --> 00:35:03,520
Lama 4, but the next probably four or five versions of Lama, because these are long lead

570
00:35:03,520 --> 00:35:10,480
time investments to build out these data centers and the power around them and the chip architectures

571
00:35:10,480 --> 00:35:16,160
and the networking architectures, so all this stuff. So yeah, I realize that's a bit of a non-answer

572
00:35:16,160 --> 00:35:23,920
for now other than just some general excitement, but I don't know, I think Lama 3 deserves at

573
00:35:23,920 --> 00:35:32,320
least a week of kind of just processing what we've put out there before we get into talking

574
00:35:32,320 --> 00:35:35,680
about the future. All right, I'm going to put this out into the world. I want to interview

575
00:35:35,680 --> 00:35:40,240
Mark Zuckerberg, and if I'm the one who gets to interview him or one of the ones who gets to

576
00:35:40,240 --> 00:35:45,280
interview him as part of the Lama 4 launch, I would love that. So if anybody from META is watching

577
00:35:45,280 --> 00:35:50,640
this, please consider me. I would love to do that. All right, next, he's going to be talking about AGI

578
00:35:50,640 --> 00:35:57,680
and agents, something that I've seen little bits of in the Lama 3.1 launch. They are defining their

579
00:35:57,680 --> 00:36:02,160
own agent architecture, it seems, or its own language. I still haven't dug into it too deeply

580
00:36:02,160 --> 00:36:07,200
yet, but I plan to, but let's see what Mark has to say about AGI and specifically agents.

581
00:36:07,200 --> 00:36:12,160
I'm happy to talk about it both from a technical perspective and a product perspective, but since

582
00:36:12,160 --> 00:36:18,640
we've mostly talked about the models so far, maybe I'll start with the products. So our vision

583
00:36:19,360 --> 00:36:26,000
is that there should be a lot of different AIs out there in AI services, not just kind of one

584
00:36:26,000 --> 00:36:31,360
singular AI. And that really informs the open source approach. It's, you know, it also informs

585
00:36:31,360 --> 00:36:37,280
the product roadmap. So yeah, we have META AI. META AI is doing quite well. My goal was for it

586
00:36:37,280 --> 00:36:42,960
to be the most used AI assistant in the world by the end of the year. I think we're well on track

587
00:36:42,960 --> 00:36:49,120
for that. We'll probably hit it, hit that milestone, you know, a few months before the end of the year.

588
00:36:49,120 --> 00:36:55,840
That's a huge statement, if true. That means that META AI has more usage than chat GPT,

589
00:36:55,840 --> 00:37:00,640
which would be surprising to me because anybody who knows about AI knows about chat GPT, but they

590
00:37:00,640 --> 00:37:06,640
don't necessarily know about Anthropics Clawd or other models. And many people have never even heard

591
00:37:06,640 --> 00:37:13,360
of Lama before. Obviously, META has the billions of built-in user base. So it's exciting to see

592
00:37:13,360 --> 00:37:19,120
that. But he's specifically talking about the META.AI product. And I believe when he's saying META AI,

593
00:37:19,120 --> 00:37:25,440
it's not just META.AI, which is kind of the chat GPT competitor, but it is also each of the

594
00:37:25,520 --> 00:37:31,040
implementations of META AI in each of their products, WhatsApp, Instagram, Facebook, etc.

595
00:37:31,040 --> 00:37:37,440
A lot of what we're focused on is giving every creator and every small business the ability to

596
00:37:37,440 --> 00:37:44,000
create AI agents for themselves, making it so that every person on our platforms can create their own

597
00:37:44,000 --> 00:37:48,080
AI agents that they want to interact with. And if you think about it, these are just

598
00:37:48,080 --> 00:37:52,640
huge spaces, right? So there are hundreds of millions of small businesses in the world.

599
00:37:52,640 --> 00:37:55,760
And one of the things I think is really important is basically making it

600
00:37:56,560 --> 00:38:02,240
so with a relatively small amount of work, a business can basically, you know, few taps,

601
00:38:03,040 --> 00:38:09,760
stand up an AI agent for themselves that can do customer support, sales, communicate with all

602
00:38:09,760 --> 00:38:14,720
their people, all their customers. That's going to be hundreds of millions, maybe billions of

603
00:38:14,720 --> 00:38:20,160
what kind of small business agents. Similar deal for creators. There are more than,

604
00:38:20,160 --> 00:38:23,920
there's more than 200 million people on our platforms who consider themselves creators,

605
00:38:23,920 --> 00:38:29,200
who basically use our platform in a way that is primarily for, you know, building a community,

606
00:38:30,560 --> 00:38:34,480
you know, putting out content feel like it's kind of like a part of their job is doing that.

607
00:38:35,040 --> 00:38:38,640
And they all have this basic issue, which is that there aren't enough hours in the day to engage

608
00:38:38,640 --> 00:38:42,480
with their community as much as they'd like. And likewise, I think that their communities would

609
00:38:42,480 --> 00:38:47,920
generally want more of their time, but, but again, not enough hours in the day. So I just think it's

610
00:38:48,480 --> 00:38:52,400
there's going to be a huge unlock where basically every creator can pull in all their information

611
00:38:52,400 --> 00:38:57,760
from social media, can train these systems to reflect their values and their business objectives

612
00:38:57,760 --> 00:39:03,120
and what they're trying to do. And then people can interact with that. It'll be almost like this,

613
00:39:03,120 --> 00:39:09,760
almost artistic artifact that creators create that people can kind of interact with in different

614
00:39:09,760 --> 00:39:13,680
ways. And then, and that's not even getting into all the different ways that I think people are

615
00:39:13,680 --> 00:39:17,040
going to be able to create, you know, different AI agents for themselves to do different things.

616
00:39:17,040 --> 00:39:20,320
So I think we're going to live in a world where there are going to be hundreds of millions and

617
00:39:20,320 --> 00:39:24,880
billions of different AI agents, eventually probably more AI agents than there are people in

618
00:39:24,880 --> 00:39:27,520
the world, and that people are just going to interact with them in all these different ways.

619
00:39:27,520 --> 00:39:31,840
So that's part of, you know, that's the product vision. Obviously, there's a lot of business

620
00:39:31,840 --> 00:39:35,760
opportunity in that. That's where we want to go make money. So we don't want to, we're not going

621
00:39:35,760 --> 00:39:40,080
to make money from selling access to the model itself. Because again, we're not a public cloud

622
00:39:40,080 --> 00:39:44,960
company. We will make money by building the best products. An important ingredient to the best

623
00:39:44,960 --> 00:39:50,000
products is building, is having the best models. So this is echoing exactly what Jan Lacoon told

624
00:39:50,000 --> 00:39:55,360
Lex Freeman a few months ago, where he said, they're not going to make money by developing and

625
00:39:55,360 --> 00:40:00,240
deploying open source models, but as being the company who can define the standards and thus

626
00:40:00,240 --> 00:40:04,240
make the best products around that AI, that's going to be meta. And that's how they're going

627
00:40:04,240 --> 00:40:09,120
to make money. All right, in this last section, he talks about fear of AI, why people worry about

628
00:40:09,120 --> 00:40:12,800
AI, why they should, why they shouldn't. So let's take a look. I mean, financially, one thing that

629
00:40:12,800 --> 00:40:20,160
I'm quite aware of is the internet had a big bubble burst before it succeeded. And it's all

630
00:40:20,160 --> 00:40:25,920
the people who were very long on the internet were eventually right. But sometimes things take

631
00:40:25,920 --> 00:40:29,360
a little longer to develop than you think. And you just need to have the commitment to see that

632
00:40:29,360 --> 00:40:34,400
through. And that's something that I'm aware of. Because yeah, I mean, I'm really excited about,

633
00:40:34,400 --> 00:40:37,280
you know, all the unlocks that we're going to get from llama three, and then llama four,

634
00:40:37,280 --> 00:40:40,320
and then llama five, and I think that's going to translate into better products. But

635
00:40:41,200 --> 00:40:46,080
realistically, it's hard to know in advance when something is good enough that you're going to

636
00:40:46,080 --> 00:40:51,040
have a product that billions of people use. And then when it's ready to kind of be a large business.

637
00:40:51,040 --> 00:40:56,240
And I mean, look, we're all spending, you know, a lot of capital and on basically training these

638
00:40:56,240 --> 00:40:59,680
models. So I think that people are going to be probably losing money for quite a while.

639
00:40:59,680 --> 00:41:03,840
Yeah. And that's what I've been hearing generally from the industry. Every single big tech company,

640
00:41:03,840 --> 00:41:08,880
even a lot of VC dollars going into startups, they're all spending their money buying the silicon.

641
00:41:08,880 --> 00:41:14,080
They are mostly buying it from Nvidia. That is why Nvidia stock price has skyrocketed over the last

642
00:41:14,080 --> 00:41:19,680
couple of years. However, all of that initial investment has not necessarily translated into

643
00:41:19,680 --> 00:41:23,520
revenue. In fact, I think the number is only like 30 billion in revenue, even though there's been a

644
00:41:23,520 --> 00:41:29,360
trillion dollars in spend. So that's not sustainable in the long term. There is likely a mini bubble

645
00:41:29,360 --> 00:41:35,760
that is going to burst eventually. But obviously I'm bullish on AI. And that's what I'm dedicating

646
00:41:35,760 --> 00:41:40,160
all of my time to right now. So I believe in it in the long run. And as he said,

647
00:41:40,160 --> 00:41:46,800
people who were early on the web and long on the web eventually were right. Now there was a bubble

648
00:41:46,800 --> 00:41:52,640
in between that we do have to think about and consider and worry about when it comes to AI.

649
00:41:52,640 --> 00:41:57,440
The other part of this that I think you are more getting at is people's concern about what it means

650
00:41:57,440 --> 00:42:03,520
for their livelihoods. And on that, this is one of the reasons why I think the open source approach,

651
00:42:03,520 --> 00:42:10,000
the approach of lots of different models out there that are kind of personalized and customized to

652
00:42:10,000 --> 00:42:17,680
every business and every creator and every person. I think that's important because if this develops

653
00:42:17,680 --> 00:42:24,960
in a way where it's just a small number of companies that build the products and benefit

654
00:42:24,960 --> 00:42:29,920
and people use the products and maybe they like talking to an AI assistant and that's valuable

655
00:42:29,920 --> 00:42:38,560
for them. But if this doesn't in some way help lift all boats, then I think you end up eventually

656
00:42:38,560 --> 00:42:45,360
getting a backlash. And part of what I've spent some time thinking about after just looking at how

657
00:42:45,920 --> 00:42:52,480
the kind of Web 2.0 stuff developed is in the next generation of technologies around AI,

658
00:42:52,560 --> 00:43:02,080
around AR and VR. How do we create not just a thriving set of products and economic productivity

659
00:43:02,080 --> 00:43:07,280
gains, but how do we have a better and more sustainable political economy around it where

660
00:43:07,280 --> 00:43:12,720
there's just way more people who feel like they're kind of bought in or benefiting from this

661
00:43:13,360 --> 00:43:19,760
in support of the system? And I thought we did that reasonably well with social media,

662
00:43:20,320 --> 00:43:25,040
but just looking at some of the feedback and some of the response from the world,

663
00:43:25,920 --> 00:43:29,360
I think that it's going to be important to do that even better with AI and some of the new

664
00:43:29,360 --> 00:43:34,480
technologies in order to mitigate some of the concerns that people are going to have about

665
00:43:34,480 --> 00:43:38,320
what this is going to mean for their livelihoods and jobs and their lives.

666
00:43:38,320 --> 00:43:42,080
So we're going to end it there. I think that is a great place to end it. Something for us to think

667
00:43:42,080 --> 00:43:48,800
about over the coming weeks, months and years, incredibly important stuff. I'm so excited to see

668
00:43:48,880 --> 00:43:53,760
all the different innovations that come from Lama 3.1, whether we're talking about an extremely

669
00:43:53,760 --> 00:44:00,320
capable model that can fit on your phone or on your laptop, or the massive 405B model that is

670
00:44:00,320 --> 00:44:05,360
as capable as any closed source frontier model. If you enjoyed this video, please consider giving

671
00:44:05,360 --> 00:44:15,360
a like and subscribe, and I'll see you in the next one.

