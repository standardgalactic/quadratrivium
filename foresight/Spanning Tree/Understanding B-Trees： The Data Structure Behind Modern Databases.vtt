WEBVTT

00:00.000 --> 00:08.000
When deciding how to store large amounts of data, a key question is how to organize that data so we can operate on it efficiently.

00:08.000 --> 00:12.000
Let's start by looking at a popular data structure, the binary search tree.

00:12.000 --> 00:16.000
A binary search tree consists of connected nodes.

00:16.000 --> 00:23.000
Each node has a key, in this case a unique number, and can point to a left child node and a right child node.

00:23.000 --> 00:29.000
Binary search trees obey the property that any keys to the left of a node are less than that node's key

00:29.000 --> 00:33.000
and any keys to the right of a node are greater than that node's key.

00:33.000 --> 00:36.000
This allows for an efficient search process.

00:36.000 --> 00:42.000
If we want to search for a key, we start at the root of the tree and compare against the key there,

00:42.000 --> 00:48.000
going left or right depending on if the key we want is less than or greater than the key in the node.

00:48.000 --> 00:54.000
At each step, we pick only the half of the tree that might contain the key we're looking for,

00:54.000 --> 00:59.000
saving time by not bothering to search parts of the tree we know won't have our key.

00:59.000 --> 01:04.000
At this point, we might wonder if binary search trees work so well

01:04.000 --> 01:09.000
because at each step we focus in on only the relevant half of the search space,

01:09.000 --> 01:14.000
wouldn't it be better to have a tree that split into three instead of two

01:14.000 --> 01:19.000
so that at each step we could reduce the search space to a third the size?

01:19.000 --> 01:23.000
But depending on the circumstances, this might actually be less efficient.

01:23.000 --> 01:28.000
That's because each node, instead of having one key, might have two.

01:28.000 --> 01:34.000
Keys less than the first key would be to the left, keys greater than the second key would be to the right,

01:34.000 --> 01:37.000
and keys between the two would be in the middle.

01:37.000 --> 01:42.000
The tree is definitely shallower, meaning we have to look at fewer nodes to get to our answer.

01:42.000 --> 01:47.000
But we also might need to double the number of comparisons for each node.

01:47.000 --> 01:52.000
Rather than just compare against one key, we might need to compare against both keys.

01:52.000 --> 01:57.000
In the end, we'll often end up performing more comparisons than in binary search.

01:57.000 --> 02:02.000
But does more comparisons mean this approach is less efficient?

02:02.000 --> 02:05.000
That depends on what operations take up more time.

02:05.000 --> 02:09.000
If you have a lot of data, say in a database or file system,

02:09.000 --> 02:16.000
it's likely that the most expensive part of any algorithm will be the time it takes to go fetch a new node of data.

02:16.000 --> 02:19.000
Comparing keys, the processor can perform very quickly,

02:19.000 --> 02:26.000
but when it's time to search the next node, the processor needs to wait much longer for the data to be ready.

02:26.000 --> 02:29.000
It's in this context that the B tree shines.

02:29.000 --> 02:35.000
The idea behind a B tree is to store a tree with nodes that can have more than one key.

02:35.000 --> 02:40.000
Maybe they have up to two keys, maybe ten, maybe hundreds or more.

02:40.000 --> 02:47.000
To demonstrate the idea, we'll use a B tree with nodes that have a maximum of four keys.

02:47.000 --> 02:51.000
Once we have a B tree, search works just like you'd expect.

02:51.000 --> 02:56.000
We start with the root node and check our key against the keys in the node.

02:56.000 --> 03:01.000
Maybe one of the keys matches and we're done, but otherwise we need to search another node.

03:01.000 --> 03:08.000
However many keys a node has, it has a number of children equal to one more than the number of keys.

03:08.000 --> 03:11.000
This lets us know where to search.

03:11.000 --> 03:15.000
If our search key is less than all the keys, we'll look to the leftmost child.

03:15.000 --> 03:19.000
If our search key is greater than all of the keys, we'll look to the rightmost child.

03:19.000 --> 03:24.000
Otherwise, depending on which two keys our search key falls between,

03:24.000 --> 03:27.000
we'll look at the child in between those two keys.

03:27.000 --> 03:34.000
This process continues, one node at a time, until we either find or don't find the key we're looking for.

03:34.000 --> 03:40.000
Because each node branches in so many different directions, we cut down on the search space quickly,

03:40.000 --> 03:47.000
and we can find what we're looking for by checking far fewer nodes than a binary search tree would require.

03:47.000 --> 03:50.000
But how exactly do we create a B tree?

03:50.000 --> 03:56.000
And how do we make sure it doesn't get imbalanced, where the tree is deeper on one side compared to another,

03:56.000 --> 03:58.000
which might slow down operations?

03:58.000 --> 04:03.000
Let's start with some rules of B trees that will guide how we operate on them.

04:03.000 --> 04:08.000
First, the leaves of the B tree, the nodes that don't point to any other nodes,

04:08.000 --> 04:12.000
those all need to be at the same level of the tree.

04:12.000 --> 04:18.000
This makes sure everything stays balanced, we're not allowed to have some leaves deeper in the tree than others.

04:18.000 --> 04:23.000
Next, every node has a maximum and a minimum number of keys.

04:23.000 --> 04:31.000
We get to choose the maximum, and the minimum is always half of the maximum number of keys, rounding down if needed.

04:31.000 --> 04:36.000
So in our case, each node can have two, three, or four keys.

04:36.000 --> 04:40.000
What happens when we try to add our first number into a B tree?

04:40.000 --> 04:43.000
We'll place the key into the root node.

04:43.000 --> 04:45.000
But wait, you might say.

04:45.000 --> 04:49.000
We just said that nodes need at least two keys.

04:49.000 --> 04:52.000
For B trees, the root node is the exception to the rule.

04:52.000 --> 04:59.000
The root node can have fewer keys, but all other nodes in the tree need to have at least the minimum.

04:59.000 --> 05:03.000
So let's keep adding keys to the tree and see what happens.

05:03.000 --> 05:08.000
Each time we add a key, the key will get added to the root node in sorted order.

05:08.000 --> 05:15.000
But remember, all nodes can only store up to a maximum number of keys, in this case four.

05:15.000 --> 05:19.000
So what happens when we try to add our fifth key?

05:19.000 --> 05:21.000
It's not going to fit.

05:21.000 --> 05:26.000
So anytime we have too many keys in a node, we need to split that node up.

05:26.000 --> 05:30.000
Remember that each node requires a minimum of two keys.

05:30.000 --> 05:37.000
So we'll take the smallest two keys and make one node, and take the largest two keys and make a second node.

05:37.000 --> 05:40.000
What do we do with this leftover middle key?

05:40.000 --> 05:48.000
Well, all the keys in the left node are less than this key, and all the keys in the right node are greater than this key.

05:48.000 --> 05:54.000
So we can push this key up into a new root node that points to the left and right.

05:54.000 --> 05:57.000
Now this is starting to look more like a tree.

05:57.000 --> 06:02.000
When we add new elements, we're always going to add them at the bottom level of the B tree.

06:02.000 --> 06:08.000
So when a new element comes along, we use the root node to decide which node to visit next,

06:08.000 --> 06:13.000
and when we get to the bottom of the tree, we insert the key in the appropriate spot.

06:13.000 --> 06:18.000
That process will continue, with new nodes getting added to the bottom level.

06:18.000 --> 06:22.000
Notice that most of the time inserting is very straightforward.

06:22.000 --> 06:27.000
Just follow the B tree nodes down to the bottom and insert the value there.

06:27.000 --> 06:34.000
It's only when we completely fill up a node and then try to add a new key there that we need to perform a split.

06:34.000 --> 06:39.000
We'll split the node into two nodes and push the middle key up into the parent.

06:39.000 --> 06:47.000
As we add more and more data, you might start to wonder, what happens when the parent becomes full of keys too?

06:47.000 --> 06:51.000
It turns out this splitting algorithm is recursive.

06:51.000 --> 06:58.000
Let's look at this tree, for example, and try to insert a new key that belongs in a node that's already full.

06:58.000 --> 07:04.000
We start as usual by splitting the node up into two nodes and pushing the middle key into the parent.

07:04.000 --> 07:11.000
But the parent is already full, so when we push the middle key into the parent, it's overflowing.

07:11.000 --> 07:14.000
If that happens, we repeat the process again.

07:14.000 --> 07:20.000
We split the parent into two nodes of its own, pushing the middle key into a new root node.

07:20.000 --> 07:25.000
This elegant recursive algorithm preserves all the properties of the B tree.

07:25.000 --> 07:33.000
When a node gets full, we split it into new nodes, and the only time the tree gets taller is when we split the root node into two

07:33.000 --> 07:39.000
and create a new root node, keeping all leaves at the same level of the tree.

07:39.000 --> 07:45.000
But we don't just care about adding new data, we also want the ability to delete data.

07:45.000 --> 07:48.000
So how do you delete data from a B tree?

07:48.000 --> 07:51.000
Well, let's say there's a key we want to delete.

07:51.000 --> 07:56.000
We start by searching through the tree as usual, starting from the root, looking for the key.

07:56.000 --> 07:59.000
Once we find it, we can delete it from the tree.

07:59.000 --> 08:05.000
That worked well in this case, but there are a few trickier cases we need to think about.

08:05.000 --> 08:15.000
Take a moment and consider under what circumstances might this deletion algorithm run into problems with the properties we've set up around B trees.

08:15.000 --> 08:17.000
Here's one case to consider.

08:17.000 --> 08:23.000
What happens if we try to delete a key, but the node already has the minimum key count?

08:23.000 --> 08:28.000
If we deleted the key, the node would have fewer than the minimum, which isn't allowed.

08:28.000 --> 08:31.000
So how do we fix the problem?

08:31.000 --> 08:39.000
Well, if a node has too few keys, we'll take a key from a node next to it, one of its sibling nodes, so to speak.

08:39.000 --> 08:42.000
In this case, let's take a key from the right sibling.

08:42.000 --> 08:46.000
We'll take the smallest key, since we're going to be moving it to the left.

08:46.000 --> 08:53.000
If we were taking a key from the left sibling, we'd instead take the largest key, since we'd be moving it to the right.

08:53.000 --> 08:58.000
But we can't just take the key from the sibling and fill in the missing spot in our node,

08:58.000 --> 09:02.000
because that would make the parent key in between these two nodes wrong.

09:02.000 --> 09:06.000
Remember that this parent key acts as a separator.

09:06.000 --> 09:11.000
All keys to the left of it should be smaller, and all keys to the right of it should be bigger.

09:11.000 --> 09:15.000
So we can't just move a key from one sibling to another like this.

09:15.000 --> 09:19.000
With a slight modification, though, we can make this work.

09:19.000 --> 09:25.000
Instead of taking the key from the sibling directly, we'll let the key from the sibling become the new separator,

09:25.000 --> 09:29.000
and we'll use what used to be the separator in our node.

09:29.000 --> 09:32.000
This preserves the properties of the B tree.

09:32.000 --> 09:35.000
Values less than the separator are to the left of it.

09:35.000 --> 09:38.000
Values greater than the separator are to the right of it.

09:38.000 --> 09:42.000
So this works, as long as we have a sibling we can take from.

09:42.000 --> 09:47.000
But what happens if when we remove a key, we fall below the minimum,

09:47.000 --> 09:50.000
and our sibling nodes are also already at the minimum?

09:50.000 --> 09:53.000
Now we can't take from a sibling.

09:53.000 --> 09:57.000
Instead, just like when adding keys we could split nodes apart,

09:57.000 --> 10:01.000
when deleting keys we might merge nodes back together.

10:01.000 --> 10:06.000
We can take our node, our sibling node, and the separator between them,

10:06.000 --> 10:10.000
and merge them together into a single full node.

10:10.000 --> 10:18.000
In this case, this works out fine, though note that in some cases that might mean the parent node now ends up with having too few keys.

10:18.000 --> 10:22.000
In that case, we can recursively apply the algorithm we've just described,

10:22.000 --> 10:28.000
either taking from a sibling if possible, or merging with a sibling if that's not possible.

10:28.000 --> 10:35.000
The other scenario we need to think about is what happens if we delete a key from the middle of the tree, rather than from a leaf.

10:35.000 --> 10:39.000
In that case, it's not as simple as just deleting the key,

10:39.000 --> 10:44.000
since that key was also acting as a separator between two subtrees.

10:44.000 --> 10:47.000
In that case, we need a new separator.

10:47.000 --> 10:52.000
And that separator needs to still be greater than everything in the left subtree,

10:52.000 --> 10:55.000
and smaller than everything in the right subtree.

10:55.000 --> 10:59.000
That means we have two options for the new separator.

10:59.000 --> 11:03.000
We either need to take the largest value in the left subtree,

11:03.000 --> 11:06.000
or the smallest value in the right subtree.

11:06.000 --> 11:11.000
Either one works, and that becomes the new separator value.

11:11.000 --> 11:17.000
Of course, after we do this, it's possible that the node we took the key from will now have fewer than the minimum.

11:17.000 --> 11:19.000
But we know how to fix that already.

11:19.000 --> 11:25.000
We just take a key from a sibling, or merge two nodes together if that's not possible.

11:25.000 --> 11:30.000
So by taking care to always preserve these rules of B trees,

11:30.000 --> 11:35.000
we've constructed a data structure that's remarkably efficient for searching lots of data.

11:35.000 --> 11:42.000
Even though we have lots of nodes, we only need to access a small number of them any time we're working with data,

11:42.000 --> 11:47.000
resulting in substantial savings on time waiting for data to be retrieved.

11:47.000 --> 11:53.000
And the property that nodes can be anywhere between half full and completely full turns out to be especially helpful.

11:53.000 --> 11:59.000
It means we can split up nodes when they get too full, and merge them together when they get too empty.

11:59.000 --> 12:05.000
The fact that many nodes aren't completely full also makes inserting new data easy most of the time,

12:05.000 --> 12:13.000
since often we can just fill in an empty space in a node without needing to change anything about the rest of the tree.

12:13.000 --> 12:20.000
These properties make B trees and variance on B trees a popular and elegant choice for handling data,

12:20.000 --> 12:23.000
especially in databases and file systems.

12:23.000 --> 12:27.000
And I hope this tour of the B tree, its structure and how it operates,

12:27.000 --> 12:35.000
gives you a better understanding and appreciation for what's really going on inside your computer as it stores and navigates your data.

