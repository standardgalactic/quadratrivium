1
00:00:00,000 --> 00:00:05,680
Why transformer architecture does not have vanishing gradients problem as opposed to

2
00:00:05,680 --> 00:00:08,920
recurrent neural network or RNN?

3
00:00:08,920 --> 00:00:13,880
The simple answer is that in the transformer architecture at every layer, you still have

4
00:00:13,880 --> 00:00:20,800
access to all the input tokens, which is in stark contrast to any RNN where each token

5
00:00:20,800 --> 00:00:22,880
is processed one by one.

6
00:00:22,880 --> 00:00:25,280
Let's look at few more points.

7
00:00:25,280 --> 00:00:29,680
In the transformer, due to the self-attention mechanism, every token in a sequence has

8
00:00:29,680 --> 00:00:35,880
the potential to directly attend to every other token irrespective of their relative

9
00:00:35,880 --> 00:00:36,880
position.

10
00:00:36,880 --> 00:00:43,800
This means that the information flow between distant tokens is not constrained by the sequential

11
00:00:43,800 --> 00:00:47,000
processing nature seen in RNN.

12
00:00:47,000 --> 00:00:53,360
So RNNNs, that is recurrent neural network, inherently process sequences in a step-by-step

13
00:00:53,360 --> 00:00:54,360
manner.

14
00:00:54,360 --> 00:01:01,400
This means that to relay information from an early token to a later position, the information

15
00:01:01,400 --> 00:01:05,080
must be propagated through every intermediate step.

16
00:01:05,080 --> 00:01:10,120
This can potentially lead to vanishing or even exploding gradients, especially for long

17
00:01:10,120 --> 00:01:17,440
sequences, as a gradient signal might diminish or explode as it's back propagated through

18
00:01:17,440 --> 00:01:18,440
the time.

19
00:01:18,440 --> 00:01:24,240
Now, the direct connection between all tokens in the transformer ensured that there is

20
00:01:24,240 --> 00:01:31,680
no need to go through potentially many intermediate steps as with RNN for the gradient to flow

21
00:01:31,680 --> 00:01:34,520
from one token's position to another.

22
00:01:34,520 --> 00:01:41,320
This architecture design allows for more direct gradient pathways during back propagation.

23
00:01:41,320 --> 00:01:47,600
Additionally, residual connections in transformers further alleviate the vanishing gradient problem.

24
00:01:47,600 --> 00:01:53,560
These connections ensure that the gradient can flow unimpeded through the network by

25
00:01:53,560 --> 00:01:55,880
passing certain layers if necessary.

26
00:01:55,880 --> 00:02:01,680
It's also important to note that the normalization techniques like layer normalization employed

27
00:02:01,680 --> 00:02:06,120
in transformer models further stabilizes the training process.

28
00:02:06,120 --> 00:02:12,720
Stable activations reduce the risk of gradients becoming too small or too large.

29
00:02:12,720 --> 00:02:17,360
The sliding window attention in transformer networks have vanishing gradient problems.

30
00:02:17,360 --> 00:02:22,040
To answer simply, sliding window attention in transformers is designed to mitigate the

31
00:02:22,040 --> 00:02:28,920
vanishing gradient problem by constraining the scope of attention within each window.

32
00:02:28,920 --> 00:02:34,320
This approach limits the paths through which gradients must propagate, reducing the likelihood

33
00:02:34,320 --> 00:02:38,400
of vanishing gradients compared to full sequence attention mechanism.

34
00:02:38,400 --> 00:02:44,400
Now, sliding window attention is a mechanism designed to improve the efficiency of transformer

35
00:02:44,400 --> 00:02:50,240
models, particularly when dealing with long sequences, by restricting attention to a fixed

36
00:02:50,240 --> 00:02:53,480
size window around each position.

37
00:02:53,480 --> 00:02:59,880
It reduces the quadratic computational complexity associated with standard self-attention.

38
00:02:59,880 --> 00:03:05,920
The vanishing gradient problem is difficulty that arises during training of deep neural

39
00:03:05,920 --> 00:03:06,920
networks.

40
00:03:06,920 --> 00:03:13,440
It refers to gradients becoming too small for earlier layers during propagation, that

41
00:03:13,440 --> 00:03:17,280
is, back propagation, leading to insufficient learning.

42
00:03:17,280 --> 00:03:23,760
The consequence is that weights in the early layers of the network barely change, making

43
00:03:23,760 --> 00:03:31,420
it difficult or impossible for model to learn from its input data and thereby the weights

44
00:03:31,420 --> 00:03:35,000
update mechanism breaks down.

45
00:03:35,000 --> 00:03:39,760
Now let's quickly think about if sliding window attention has the vanishing gradient

46
00:03:39,760 --> 00:03:40,760
problem.

47
00:03:40,760 --> 00:03:47,240
Though the use of sliding window attention by itself does not inherently introduce the

48
00:03:47,240 --> 00:03:53,000
vanishing gradient problem, the primary purpose of SWA, that is, sliding window attention,

49
00:03:53,000 --> 00:03:55,000
is to reduce computational complexity.

50
00:03:55,000 --> 00:04:01,160
However, the depth of the network and the activation functions used are the primary

51
00:04:01,160 --> 00:04:05,200
factors influencing the vanishing gradient problem.

52
00:04:05,200 --> 00:04:09,840
Transformers, due to their architecture, are generally less prone to vanishing gradient

53
00:04:09,840 --> 00:04:16,480
problem compared to traditional deep RNN, that is, recurrent neural networks, or LSTM,

54
00:04:16,480 --> 00:04:19,920
that is, long and short term network.

55
00:04:19,920 --> 00:04:24,800
And this is mainly because transformers use multi-head attention and skip connections,

56
00:04:24,800 --> 00:04:26,680
that is, residual connections.

57
00:04:26,680 --> 00:04:31,160
And these connections allow gradients to flow more freely through the network.

