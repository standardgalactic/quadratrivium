WEBVTT

00:00.000 --> 00:05.680
Why transformer architecture does not have vanishing gradients problem as opposed to

00:05.680 --> 00:08.920
recurrent neural network or RNN?

00:08.920 --> 00:13.880
The simple answer is that in the transformer architecture at every layer, you still have

00:13.880 --> 00:20.800
access to all the input tokens, which is in stark contrast to any RNN where each token

00:20.800 --> 00:22.880
is processed one by one.

00:22.880 --> 00:25.280
Let's look at few more points.

00:25.280 --> 00:29.680
In the transformer, due to the self-attention mechanism, every token in a sequence has

00:29.680 --> 00:35.880
the potential to directly attend to every other token irrespective of their relative

00:35.880 --> 00:36.880
position.

00:36.880 --> 00:43.800
This means that the information flow between distant tokens is not constrained by the sequential

00:43.800 --> 00:47.000
processing nature seen in RNN.

00:47.000 --> 00:53.360
So RNNNs, that is recurrent neural network, inherently process sequences in a step-by-step

00:53.360 --> 00:54.360
manner.

00:54.360 --> 01:01.400
This means that to relay information from an early token to a later position, the information

01:01.400 --> 01:05.080
must be propagated through every intermediate step.

01:05.080 --> 01:10.120
This can potentially lead to vanishing or even exploding gradients, especially for long

01:10.120 --> 01:17.440
sequences, as a gradient signal might diminish or explode as it's back propagated through

01:17.440 --> 01:18.440
the time.

01:18.440 --> 01:24.240
Now, the direct connection between all tokens in the transformer ensured that there is

01:24.240 --> 01:31.680
no need to go through potentially many intermediate steps as with RNN for the gradient to flow

01:31.680 --> 01:34.520
from one token's position to another.

01:34.520 --> 01:41.320
This architecture design allows for more direct gradient pathways during back propagation.

01:41.320 --> 01:47.600
Additionally, residual connections in transformers further alleviate the vanishing gradient problem.

01:47.600 --> 01:53.560
These connections ensure that the gradient can flow unimpeded through the network by

01:53.560 --> 01:55.880
passing certain layers if necessary.

01:55.880 --> 02:01.680
It's also important to note that the normalization techniques like layer normalization employed

02:01.680 --> 02:06.120
in transformer models further stabilizes the training process.

02:06.120 --> 02:12.720
Stable activations reduce the risk of gradients becoming too small or too large.

02:12.720 --> 02:17.360
The sliding window attention in transformer networks have vanishing gradient problems.

02:17.360 --> 02:22.040
To answer simply, sliding window attention in transformers is designed to mitigate the

02:22.040 --> 02:28.920
vanishing gradient problem by constraining the scope of attention within each window.

02:28.920 --> 02:34.320
This approach limits the paths through which gradients must propagate, reducing the likelihood

02:34.320 --> 02:38.400
of vanishing gradients compared to full sequence attention mechanism.

02:38.400 --> 02:44.400
Now, sliding window attention is a mechanism designed to improve the efficiency of transformer

02:44.400 --> 02:50.240
models, particularly when dealing with long sequences, by restricting attention to a fixed

02:50.240 --> 02:53.480
size window around each position.

02:53.480 --> 02:59.880
It reduces the quadratic computational complexity associated with standard self-attention.

02:59.880 --> 03:05.920
The vanishing gradient problem is difficulty that arises during training of deep neural

03:05.920 --> 03:06.920
networks.

03:06.920 --> 03:13.440
It refers to gradients becoming too small for earlier layers during propagation, that

03:13.440 --> 03:17.280
is, back propagation, leading to insufficient learning.

03:17.280 --> 03:23.760
The consequence is that weights in the early layers of the network barely change, making

03:23.760 --> 03:31.420
it difficult or impossible for model to learn from its input data and thereby the weights

03:31.420 --> 03:35.000
update mechanism breaks down.

03:35.000 --> 03:39.760
Now let's quickly think about if sliding window attention has the vanishing gradient

03:39.760 --> 03:40.760
problem.

03:40.760 --> 03:47.240
Though the use of sliding window attention by itself does not inherently introduce the

03:47.240 --> 03:53.000
vanishing gradient problem, the primary purpose of SWA, that is, sliding window attention,

03:53.000 --> 03:55.000
is to reduce computational complexity.

03:55.000 --> 04:01.160
However, the depth of the network and the activation functions used are the primary

04:01.160 --> 04:05.200
factors influencing the vanishing gradient problem.

04:05.200 --> 04:09.840
Transformers, due to their architecture, are generally less prone to vanishing gradient

04:09.840 --> 04:16.480
problem compared to traditional deep RNN, that is, recurrent neural networks, or LSTM,

04:16.480 --> 04:19.920
that is, long and short term network.

04:19.920 --> 04:24.800
And this is mainly because transformers use multi-head attention and skip connections,

04:24.800 --> 04:26.680
that is, residual connections.

04:26.680 --> 04:31.160
And these connections allow gradients to flow more freely through the network.

