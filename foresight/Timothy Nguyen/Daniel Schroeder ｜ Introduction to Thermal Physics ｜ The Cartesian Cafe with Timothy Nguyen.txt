Welcome everyone to the Cartesian Cafe. Today we're very lucky to have Dan Schroeder here with us.
Dan is a particle and accelerator physicist and an editor for the American Journal of Physics.
Dan received his PhD from Stanford University, where he spent most of his time at the Stanford
Linear Accelerator, and he is currently a professor in the Department of Physics and
Astronomy at Weber State University. Dan is also the author of two revered physics textbooks,
the first with Michael Peskin, called An Introduction to Quantum Field Theory,
or simply Peskin and Schroeder for short within the physics community,
and the second, An Introduction to Thermophysics. Dan enjoys teaching physics courses at all levels
from elementary astronomy through quantum mechanics. Welcome Dan, how are you doing today?
Thank you so much, Jim. I'm doing great, and thanks for having me.
Yeah, no problem. You know, it's quite a rare honor to have a student connect with one of his
teachers. At some point in his education, in this case, I was your student in some sense,
because I own both of your textbooks. So I'll just flash them right here for the audience to see.
So I have your Quantum Field Theory textbook, which I believe I got at MIT when I was a graduate
student. I took Quantum Field Theory under Alan Gooth there. And before that, I obtained your
Thermophysics textbook as a sophomore at Caltech when I took thermodynamics. So
it's great to be able to connect with you. It's funny because I didn't realize
it was the same Schroeder until I actually reached out to you. I didn't make that connection.
So yeah, it's great to be able to, many years later, to look you up and now discuss with you
these topics that I learned long ago, and hope to continue learning more as we discuss
these matters. I thought maybe we'd start out by having you tell us more about yourself,
your story and how you became a physicist, and maybe a little bit about how you came to write
your books. Oh, well, the story of the Quantum Field Theory book is kind of easy to tell.
I need to explain that I was very much the junior author of that collaboration.
But when I was a graduate student at Stanford, I first took Quantum Field Theory from a
wonderful teacher named Savasthamopolis. Maybe some people have heard of him.
And he went nice and slow, and I was able to follow most of it. And then the next year,
Michael Peskin was teaching the course, and I knew he would go a lot faster and cover a lot
more. So I audited the class, and every once in a while, I would get a little annoyed with him
because he'd be talking so fast, he'd forget to say something, you know, the step between,
he would go straight from A to C without saying B first, and I'd raise my hand in the back of
the room and say, excuse me, but we need to assume this or something like that. And I think
he really appreciated those interruptions. And one day we were writing the shuttle van up to the
accelerator laboratory together, and he asked me if I'd be interested in helping him turn his
lecture notes into a textbook. And we were supposed to finish it in a couple of years,
and it ended up taking eight. And by that time, I had gone through two temporary jobs and finally
landed in a tenure-track job here at Weaver State. It was a very rewarding process and quite an
honor to work with Michael on that. I haven't had much opportunity to think about Quantum
Field Theory since 1995 when that book was published. Here I teach only undergraduates.
So that's that story. And then the thermal physics was a topic that I was awful at in
graduate school. I actually took the statistical mechanics course from Michael, and I was lost
through a lot of it. But fortunately, I didn't need it at all for my PhD thesis. So I got through
Stanford without knowing much thermal physics, and then a year of teaching at Pomona College.
And then when I got to Grinnell College in a leave replacement position there,
they needed someone to teach their senior-level thermal physics courses taught at the senior
level there, probably about the same as the sophomore level at Caltech. And I thought,
well, I got to learn it sometimes. So I voluntarily agreed to teach that class. And
fortunately, I had only three students, and they were all very good students. So we sat around a
seminar table and kind of taught each other the subject. We worked through together the textbook
by Catalan Cromer, which is a brilliant textbook full of insight. And Cromer has since gotten the
Nobel Prize and so on. There's a lot of wisdom in that book. But it's not an easy book for students.
And so I kind of learned it as I was teaching it there. And then the next year,
I got to teach the class again to a larger group. And by that time, I decided I wanted
to cover things in a different order. And I kind of reordered things and where they made sense to
me. And then I came here and so we were state and taught it two more times out of a different
textbook by Keith Stowe. And there were some issues with that book for the students as well.
And so my third time through here, which is my fifth time through all together, I started writing
up my lecture notes, checking my lecture notes, and handing them out serially. And by the end of
that term, I pretty much had a draft of a textbook and kind of stumbled into it. And that's what
happens, I think, with a lot of physics textbooks is people teach and get ideas for how to teach
a subject and start writing it down and then take it to a publisher.
And since I already had one successful book, I was very, very fortunate that publishers
took a serious look at this draft. And Addison Wesley published it initially. And it's now with
Oxford University Press. And it's been successful beyond my wildest dreams. I thought of 10% of
the classes in the US use my book, I'll be very, very fortunate to reach that many and
it's been substantially above that. I guess I stumbled into the right project at the right time
and I was kind of an opportunist about it. But I'm an amateur when it comes to thermal physics.
I've done no research in the subject. And I just kind of wrote it down, wrote down what
made sense to me. Okay, well, it's a very charming book and we'll get to it soon enough.
I just maybe one more question I wanted to ask you. I used to be an academic and I was
very much gunning for the research track and in academia, there are essentially two tracks to
basically do research and to do teaching. Most people do both, but it's sort of a matter of
emphasis. It appears that you went for the teaching track or can you explain your choice
or how that ended up happening? Sure. I attended a very good liberal arts college,
Carlton College in Minnesota, where teaching was the main emphasis by far. Not much research
was going on there, at least in physics when I was a student and it's probably changed a little
by now, but teaching is still the top priority at places like that. And so, and I had a wonderful
time there. I had a very good experience. I had wonderful professors and wonderful classmates and
it was just a very positive experience for me. And so, I came out of there into graduate school
thinking that that might be the career path for me. And then when I got to Stanford and
saw what things are like at a big university where, frankly, the people were less friendly and the
place was much more competitive. And I was not the strongest student in my class. And
you know, I kind of, that reinforced what I already thought, which was that
probably a place like this is not where I want to spend the rest of my life. And that wasn't,
that wasn't what I wanted. And so, I pretty much was determined even more strongly after a couple
of years there that I would learn what it was like to do some research. I did some research on
accelerator physics, applying quantum electrodynamics to future linear colliders, which are still in
the future 30 years later, 33 years later. And that was a wonderful experience. And it was quite
an honor to hang out at the Stanford Linear Accelerator Center, which is now called the Slack
Linear Accelerator Laboratory or something. It was a wonderful experience to hang out there for
four years and go to the talks in the colloquia and absorb what was going on and kind of learn about
the research process that way, not partly as a participant, but mostly just by being immersed
in the laboratory and what was going on there and talking to my classmates and other researchers.
And I think that gave me a lot of perspective on physics. But then I started looking for teaching
positions and I talked for a year at Pomona College and other liberal arts college and then
two years at Grinnell College. And then I came here to a department at a public
open enrollment regional university, which is a very different type of place from those liberal
arts colleges. But my life as a faculty member is actually very similar. We have no graduate programs
in the sciences at Weber State. And so I do essentially all teaching and try to find
many research projects for students to get involved with, but publishing is not a consideration
with those. Okay, great. Well, I'm glad that things worked out for you. Well, I guess I want to
acknowledge that I appreciate being at a place where you can write a couple of textbooks and
publish a few articles in a journal aimed at other college teachers and still get tenure and
have a have a rewarding career. So so this is a good place for me. And anyone else who's interested
in such a career, look around. There's lots of places where I think teaching is valued. And if
you put a good effort into it, you can have a good career. That's great. And I'm really excited by
the fact that now, with our conversation, we'll be able to showcase to the world you as a teacher
to an audience that maybe you wouldn't normally come across. Well, anyways, why don't we get started?
And I wanted to illustrate to our audience some of the charm that I think is unique to your book.
So I pasted two snippets from your book, one, which is this cartoon of a magician
bringing a rabbit into existence, which is which is your way of illustrating the concept of enthalpy.
And then this passage where you talk about small numbers, large numbers and very large numbers
in a sort of conical, definitional way. And, you know, the funny thing is, you know, so I'm
my PhDs in mathematics, but I took a lot of physics courses. And, you know, there's always kind
of a funny tension between the two when it comes to rigor. And I think in this case, your book is
not rigorous, certainly because it's, well, it's a physics book. But somehow in physics, there's
the lack of rigor is, in many cases, a feature, not a bug. And in this case, there's this level
of charm to it. And I thought this little, this passage where, you know, you, what, 10 to the
23 plus 23 is 10 to the 23, because, right, you could just sort of absorb, you could just kind
of throw away small numbers. So, you know, yeah, physics is at the heart of my place.
Well, I don't want to get into a fight between the math and physics way of doing things and the
physicist's way of doing things. I started out as an undergraduate thinking I would probably
major in mathematics and then absolutely fell in love with physics during my freshman year in
college. And it was things like relativity and quantum mechanics that really hooked me then.
I didn't fall in love with thermal physics until I was teaching it much later.
But while I've got the floor here, I need to say that both of these excerpts that you
put on the screen are stolen, as are most of my ideas. The idea that the enthalpy of an object is
its energy plus the work you need to do to make room for it, comes straight from Cattell and
Cromer's book. And it's written there, it's been ages since I looked at it, but it's written there
in approximately those words. And that was a tremendous insight to me when I was finally ready
to read that page of Cattell and Cromer and ready to understand what it was saying. Of course,
I immediately took that and used it in the classroom. And then it was just ad-living in
the classroom thinking, oh, well, what if I wanted to create some object? And then the cartoon, I
asked my friend Karen Thurber, who's an artist, if she could draw me a cartoon wizard and rabbit.
Very kindly, because I can't draw. So I have to acknowledge Cattell and Cromer and my friend
Karen for that excerpt in the book. And then the other one, small numbers, large numbers,
and very large numbers, that comes from one of my undergraduate professors who taught my
undergraduate statistical mechanics course, Bruce Thomas. And he's someone I've collaborated with
more recently as well, a wonderful teacher. And he made that distinction between large numbers
and very large numbers, which I then borrowed in my own teaching. It's fine to, of course,
recycle ideas because I mean, thermodynamics is a very old subject. Of course, we can't reinvent
the wheel every generation. So that's fine. But thermodynamics is also often a very abstract
subject. And it's traditionally discussed in such an abstract way that I think that somehow there was
a need for someone to try to make things a little more vivid and put in a cartoon and be a little
bit whimsical about it, I guess. Sure, sure. I thought I'd maybe plan this discussion. It really
is a more informal dialogue. Usually with my past subjects, I've usually outlined more of a
structure. But I think given how amorphous a subject thermodynamics is, it would probably
more natural to kind of let it go where it goes. Let's contrast this with a calculus, right? Calculus
has a very logical progression that basically everyone sticks to. You learn limits, derivatives,
integrals, it builds up. Whereas I feel like thermodynamics, there are concepts that we
learned at a very young age, temperature, energy, heat. And well before we took a physics course
on them, if we ever did take a physics course on them. And so it's sort of not something that is so
cumulative. And it just has all these kind of, it's more of a dialectical learning process. You
learn the words, you learn some formalism, you go back and you try to make the two harmonious,
and it's this more like an iterative process. So I think maybe, yeah, that's why I think maybe
kind of a discussion might be more suitable. And the way I wanted to organize this is there are
really two questions I want to ask. And I think sort of maybe two of the most important questions
would one is trying to learn thermodynamics. And so let me just write them down. So the first question
is what is temperature? Sort of, if you're going to, you know, learn anything from thermodynamics,
you kind of should know what the answer to that question is. And then the second one is what is
entropy? And I guess that's a more advanced topic is the word entropy isn't kind of a word, an
everyday English word. But nevertheless, it's very fundamental to thermodynamics. And it's one of
those words where it's described with a lot of poetry in popular science. Disorder tends to increase.
And I'd like to somehow try to maybe resolve some of that possible confusion that might come from
such a, you know, kind of course or poetic understanding. And so what I really hope for
this discussion is to sort of dive deeper and relate temperature to entropy and maybe work
out some examples. So we kind of get a feel for what the subject is about. How does that sound, Dan?
Sure. I'm tempted to say may I answer the second question first?
Oh, yeah, but answer it however you want to start with the first question. It's just
that we're not we're going to have to go to this in order to really answer the first question,
we're going to have to answer the second. Okay, great. So that's just a little warning.
So empirically, temperature is what you measure with a thermometer. And I think it's,
you talked about people learning about some of these concepts, temperature, energy, heat
at a younger age before you take a formal physics class, maybe in an elementary school or middle
school science class or something like that, you people learn about temperature. And of course,
we hear about temperature in the news and we experience temperature directly. But the temperature
is is what you measure with a thermometer. And that's an important thing to understand. It's
because it's not the same thing as energy, it's not the same thing as what we call heat. And
those ideas do get confused with each other. So if in doubt, fall back on the idea that temperature
is what you measure with a thermometer. And it's not those other things, even though those other
things are related, those are not what we measure with a thermometer, what we measure with a
thermometer is temperature. And when I teach in my classes, I get a beaker of ice water and stick
a thermometer in it. And then I get a beaker of hot water and stick a thermometer in and talk about
how you stick the thermometer in and you wait for the thermometer to come to thermal equilibrium with
the beaker of water. And once the reading on the thermometer stabilizes, which takes a little
while, which is kind of interesting, you get to wait a little while to get a good reading,
then you've measured the temperature of the water. And why does that work?
We say that there's energy spontaneously flowing from the boiling water into the mercury in the
thermometer. We don't use mercury anymore, do we? I need to revise my book, because we don't use
mercury anymore. Or if you stick it in the ice water, there's energy spontaneously flowing from
the mercury into the ice water, causing a little of the ice to melt or something. So
the concept of temperature is founded on this amazing fact of the universe, that when you
put two things at different temperatures together and wait a while, they will spontaneously come
to the same temperature. And one of them will lose energy, and the other one will gain energy.
And the one that loses energy is the one we say was at was at the higher temperature. And the one
that gains energy is the one we say is at the lower temperature. So temperature is somehow a measure
of the tendency of energy to spontaneously flow out of things. Otherwise, thermometers wouldn't
work. Okay, so that's one way of understanding what temperature is, but it raises a more
fundamental question. Why does this even happen? Why does energy spontaneously flow out of some
things and into others? Okay, and that's going to get us into entropy. But there's a first crack
at answering your first question. Temperature is what you measure with a thermometer, but that
wouldn't work unless it was a measure of the tendency of objects to spontaneously give up energy.
Can I maybe push back on that definition in the sense that you'll say something like
the temperature of the sun is this or the temperature of the early universe was that?
And you might not have a thermometer around with you to measure that. So I don't know if that's
like a flippant kind of response, but I just wonder if the- It's not flippant, but we can get around
those ideas. We can get around those problems. We can eventually learn how hot objects spontaneously
give off light with a certain mix of colors. And therefore, by looking at the mix of colors,
sun gives off, we can figure out what its temperature is, even without going there and
sticking a thermometer into it. But that's more advanced and to understand why all that happens,
you know, you've got to get to chapter seven or something like that. That's not going to be
that's not going to be the first section of the first chapter of the book. We're going to learn
how that works later on. Of course, it's very familiar to people now who have these, I have one
in my closet, these infrared thermometers, you pointed at anything you want, and it tells you
the temperature. It has a temperature readout. You get it at the hardware store or an infrared
camera, even fancier, if you have the money, you can, you know, measure temperatures of things,
make images that show temperatures of different things in different places.
And that's the same idea as how we measure the sub-temperature of the sun by looking at the
color of the infrared light or, in the case of the sun, visible light that they emit.
Yeah. So I guess, of course, we'll get to this, but there's a concept of temperature and there
are different ways of measuring it. I guess the thermometer is a common way of measuring temperature
in ordinary life, but as you pointed out, there are other ways of measuring temperature, which
aren't thermometer related, at least on the face of it, it sounds like.
I can't disagree with that. And if you're, are you trying to start a fight?
I just wanted to find temperature as the thing you measure with a thermometer.
If you want to state some other interesting things about temperature and talk about some
other ways to measure it, okay, but, but fundamentally that's what temperature is.
Let me give you an example of a bad temperature.
Oh, okay, sure, sure.
When I say it's bad, I, I'm not trying to start a fight either.
No, no, I'm eating that. I'm just, I'm just a curious guy.
In grade schools, high schools, I don't know, various levels of school, wherever they teach
about these things that work for that matter, a lot of chemistry classes and maybe even
college physics classes that are aimed at a non-science, you know, people who aren't
going to go on and become physicists. We often tell people, gosh, I've seen it in a few textbooks
that even are aimed at people who are going to become physicists. We tell people that
temperature is a measure of the average kinetic energy of the atoms in an object.
I'll use k for kinetic energy, a bar over it for average, k equals three halves.
So the average kinetic energy per particle is three halves, Boltzmann's constant times the
temperature. So you see, you could turn that around. You could define temperature as two-thirds
times the average kinetic energy over Boltzmann's constant. And so, so the little k is Boltzmann's
constant. Some people put a subscript b on it in my book. I don't because we use it so much, but
all right. So I don't like that definition of temperature because it's too restrictive. It
doesn't work for a lot of ordinary objects, in fact, a lot of solid objects. For a lot of solid
objects, that's not true. It's a theorem you can prove under certain assumptions for classical
systems, for particles that are behaving classically, but as soon as quantum effects come into play,
it breaks down. And then what if you want to apply it to a gas of photons, which aren't really
particles? You know, what about relativistic systems? Photons are a relativistic system.
Or what about magnetic systems where there's only a finite number of states that a particle can have,
some magnetic dipole that can point either up or down, and maybe there's only two possibilities
or three or four or something. For all those systems, the relations, those systems don't even have
translational kinetic energy. So what are you going to do? My definition, the temperature is a measure
of the of the tendency of an object to spontaneously give up energy to other objects, is much more
general than this formula, which is a version of the equal partition theorem. This is not the
definition of temperature. It's not a definition of anything else either. It's just a fact you can
prove for classical systems that the temperature happens to be directly proportional to the kinetic
energy. And in three dimensions, the constant proportionality is three halves, and one dimension
it would be one half. The better way to define temperature is qualitatively what I said earlier,
that a measure of the tendency of an object to spontaneously give up energy.
Now, I haven't made that precise yet. And if you want me to make it precise, I can give you an
equation for that too, but it's going to have entropy in it. Okay, so to make that precise,
we need to define entropy first. I have just a question on that. So if you take two objects,
here's the amazing thing, right? I take a two objects, temperature one, temperature two.
And it doesn't matter what those two things are made of, heat will flow from the hotter object,
the one with higher temperature to the colder object, the one with lower temperature.
If there's a mechanism that allows that. Yeah, right. I'm wondering to what extent
do the details matter in the sense of we sort of abstracted everything away about in terms of the
material composition of the object, their proximity, say, I mean, I guess in practice,
you did say one thing, which was you had to wait for a while to measure the temperature, right?
So you put your thermometer in the cup of ice, and it doesn't change temperature right away,
you have to wait. And then the question is, how long do you have to wait until you know
that's the actual temperature? So how do you think about in terms of this waiting time and when
you really know that it's the temperature, sort of, to further operationalize what you just said?
Usually it'll be an exponential process, you can measure the time constant, and then wait a few
more time constants, and then it's stabilized. I see. Okay. And I guess maybe that stabilization
time might be different for different materials, but nevertheless, there is some stabilization.
It's called the relaxation time. There is a relaxation time is how much time
some system takes to come to thermal equilibrium, that it starts out out of thermal equilibrium.
And for some systems, the relaxation time is a microsecond, and for others, it's
deans, billions of years. The universe is not thermal equilibrium. Thanks for that clarification.
So yeah, why don't we get to entropy then? Sure. So entropy is a statistical idea. It's
a measure of the number of microscopic arrangements there can be within an object or
within some system. And that's a kind of a hard idea, but if you think of
matter as being made out of atoms or molecules or particles, and these particles can have
different amounts of energy, we can ask how many arrangements are there for the atoms,
and then how many arrangements are there for the energy among the atoms, different atoms can
have different amounts of energy. And for large macroscopic objects, that turns out to be a pretty
well-defined question, even though it might seem like it's a little bit ambiguous, which
arrangements are you going to count, which arrangements are accessible and which ones
are inaccessible. But it turns out that for large systems, that hardly ever matters. And you can
you can count to enough precision that the imprecision doesn't matter. You can count
accurately enough the number of possible arrangements. And to actually come up with a
number, you need to use some quantum physics, because in classical mechanics, you can't actually
put an integer number on them. Things aren't discrete. There's no way to discretize things to
get an actual number. And that's why I have great admiration for Boltzmann and Gibbs and the other
physicists who invented statistical mechanics before we had quantum mechanics and could put a
number on the entropy. All they could define was changes in entropy. And you could still do that
in classical mechanics, you can talk about changes in entropy, but it kind of takes quantum mechanics
or some arbitrary convention, maybe, to come up with an actual number for entropy, which is,
again, a measure of how many arrangements are possible. And quantum mechanics gives us a way
of actually counting things. And the formulas for entropy always have Planck's constant in them,
because that's what gives us the dimensional factor to get an actual number out of it.
Anyway, and if you look in my book or any other book on statistical mechanics,
we'll start with examples of coin flipping. And if each particle can only be in two possible
states, column heads or tails, then how many possible ways are there for a system of 20 coins
to have 10 heads and 10 tails? And you can count how many different ways there are of doing that.
And then, if you want an example, it's a little bit more like most ordinary matter,
you could, you can take what we call an Einstein solid, which is a model system that I first
learned about from another mentor of mine, Tom Moore of Pomona College. He really emphasized
this Einstein solid example in his teaching, and I borrowed that. An Einstein solid is just a
collection of harmonic oscillators, but they're quantum harmonic oscillators, so each of them
can store an arbitrary number of energy units. But the reason it's simple is because all the
energy units are the same size for a quantum harmonic oscillator, the runs on the quantum
ladder are equally spaced. So each unit of energy has the same size count how many units of energy
it has above the ground state. And then so then we can talk about a collection of many harmonic
oscillators, sharing energy and how many ways are there for all the energy to be in this side and
how many ways are there for all the energy to be on that side, but we can count it because you
can just count how many ways there are for if you have a given total amount of energy, you can
count how many ways there are of it being shared among n harmonic oscillators. And this model of,
and this was Einstein's 1907 model of predicting the heat capacity of solids by counting how many
ways there are of storing the energy, and real solids aren't all identical harmonic oscillators.
They don't all have the same natural frequency, and so Einstein's model is actually not that accurate
at low temperatures, and the Bayes model was better, but Einstein had the fundamental insight
that the reason heat capacities go to zero at low temperature is because of these quantum effects
and the counting coming into play. So first let's understand multiplicity. I'm going to write that
word. Okay, and let's give it a symbol. And I like to use a capital Greek letter omega. Other people
use other symbols for multiplicity w. Boltzmann used w, I guess, and Kotlin-Kromer used g, which is
density of states, so they borrowed it from there of it. But I like to use capital omega, so that's
multiplicity, and that's a dimensionless number. That's how many arrangements are there. That's
how many possible, we call them microstates, could the system be in, consistent with what we know
about it at the large scale, at the macroscopic level. And so just as a simple example, if we
have a system with three boxes, and let's say two units of energy, so each of those boxes
represents a quantum harmonic oscillator or something, you can think of it as an atom or some
microscopic way, place that the system can store energy, and say we have two units of energy to
put in it. Okay, how can we distribute them? Well, we could put both units of energy in the first
box, and that I'll call two zero zero, or we could put both units of energy in the second box, zero
two zero, or we could put them both in the third box, zero zero two, or we could put, let me erase
a little bit. Okay, we could put one unit of energy in the first box, and one unit of energy in the
second, and that's what I'll call one one zero, or we could do one zero one, and I'm running out of
space here, so what should we do? I'll write the rest of them over here, one zero one, let's see,
one one zero one zero one, zero one one, I'm gonna miss some, zero. That's okay, I think we get the
idea. I want to get the counting right though. Oh, I see, okay. So we've got the one one zero,
one zero one zero one, no maybe that's all, sorry that's all of them, yeah, so there's six ways to
do this, for some reason I was thinking there'd be more than six, but that's a different example.
Okay, so there's there's six ways that we can put two units of energy in three quantum harmonic
oscillators. I'm assuming that the units of energy are not distinguishable from each other,
one of them isn't red, and the other one isn't green, they're they're just units of energy.
I'm assuming that the slots I'm putting them into the boxes I'm putting them into are distinguishable
from each other, there's this there's atom number one, atom number two, and atom number three,
okay, and they're distinguished by their locations in space or something like that, because
this is a very, very tiny, tiny example of what will eventually call a model of a solid crystal
in which the atoms are in fixed in particular places. So okay, so so here the multiplicity would
be six, okay, so that the multiplicity is six in this example, there's six possible arrangements
of two units of energy among three harmonic oscillators, identical harmonic oscillators.
You know, and then we can work our way up to larger systems, and I like to do exercises on a
spreadsheet where we work first we work out what the formula is for a certain number of harmonic
oscillators and for a certain number of energy units, what's the formula, and it's a nice common
torque formula, so for an Einstein solid,
right, if we have n oscillators,
so that was three in the example I just gave, and if we have q units of energy,
which was two in the example I just gave,
all right, then omega is q plus n minus one, choose q, if I recall correctly,
all right, so let's go ahead and apply it to the example I just gave, q is two,
n is three minus one, and if viewers don't know what this symbol means, I'll show you in a second,
choose two, okay, so what that means, so that's two plus three is five minus one, that's four,
choose two, okay, and so what that means is four factorial over two factorial
times four minus two factorial, which is also two factorial,
okay, all right, so what is that, four times three times two times one over two times one
times two times one, all right, cancel that, okay, so we get six, so we could also write
that as q plus n minus one, choose n minus one, like this, right, same answer, yeah, right, yeah,
okay, yeah, so once you have that formula, then you can plug in bigger numbers without having
actually count all the microstates, and if you plug in numbers that are on the order of 100 or so,
which a spreadsheet program can handle just fine, you get multiplicities that are numbers
with 100 digits, or numbers with 50 or 100 or 150 digits, depending on exactly what you do,
but I think Excel can go up to 10 to the 300 or something like that, so we're within what
what common computer software can handle, but it's nice to do exercises with those,
and of course there's quite a few of them in the book, to just get a feel for how overwhelmingly
huge some multiplicities are compared to others, all right, so if I have two Einstein solids and
I ask, well, what's the multiplicity for distributing the energy equally between them,
compared to the multiplicity for all the energy being in one and none of the energy in the other,
the ratio of those multiplicities is a number with 30 digits or something like that, okay,
all right, and therefore, now here comes the fundamental assumption of statistical mechanics,
if all microstates, if all accessible microstates are equally likely, if all accessible microstates
are equally probable, then the macro states, the large scale states that have high multiplicity
will be overwhelmingly more likely than the states that have lower multiplicity,
that have much lower multiplicity, not the ones that have just slightly lower multiplicity,
but the ones that have much lower multiplicity, and this gets us into the idea where if you
take two bricks and put them next to each other, and one of them is at a higher temp,
of course, I'm using the word temperature, but if one of them has more than its share of the energy
than the other, so the multiplicity is lower than it could be, then energy is going to spontaneously
flow from one of the bricks into the other until the combined system has
close to the maximum possible multiplicity. I think right now what we have written, it's not
clear this relationship between what you just said in terms of the most likely macro state is the one
with the most number of multiplicities, because right now we don't have two different systems
interacting, so why don't we flesh that out a little bit, because I don't think that's...
So now we have two Einstein solids, okay, so with nA and qA and another one, mB, qB,
so this one has, we can calculate omega for this one, and we can calculate omega for this one,
the omega for the combined system is the product.
In other words, if the block on the left, if block A has 100 possible micro states it can be in,
given how many particles it has and how much energy it has, and block B has 50 micro states
that it can be in, given how many particles it has and how much energy it has, then for the combined
system, again, keeping the numbers of particles and energy units on each side fixed, the total
number of micro states is the product, because for every one of the 100 states that the one on
the left could be in, there's 50 states that the one on the right could be in, so we would just
multiply those and get 5,000, I guess, for the total number of states for the whole system.
Okay, and so we could go through an example of this, but the idea is that if,
in a simple example, we might make the n's the same and then vary the q's, so now let's let
energy go back and forth between the two sides, let's let the two objects exchange energy.
Well, whenever a unit of energy goes from A to B, qA goes down by one and qB goes up by one,
and if it goes the other way, then qB goes down by one and qA goes up by one.
So now we can ask, well, for which values of qA and qB would the multiplicity be the highest
and by how much? And what we find is that there's always a value of qA that maximizes this total
multiplicity. So basically you have a total energy, right, so qA plus qB is fixed.
Right, and the point is this total multiplicity is a function of qA and qB itself subject to that
fixed value, the total energy, right, and the point is you want to now study this multiplicity
function as a function of the qA and qB. Correct. What we want to do is, so the assumption is that
all the microstates for any way of sharing the energy between the two sides are going to be
equally probable. Let's figure out then which value of qA is the most probable, which value of qA
maximizes the total. Right, right, so I should say we determine a value of qB because that's just
the total minus qA. Maybe one thing that's worth clarifying because this is something that I had
to re-clarify myself to when revisiting this. So we talked about microstates and macrostate, we didn't
quite clarify what we meant yet, so correct me if I'm wrong, but basically microstate will
correspond to the, let's say, the individual kind of distribution of energies
that you worked out on the previous slide. Whereas macrostate basically involves these
high-level thermodynamic variables, like total energy, temperature, and if we talk about a pressure
and volume, right, those are things that don't involve looking at the particular granularities,
they just involve these relatively few-in-number macroscopic variables. So macrostate basically
just, it's just basically the collection of macroscopic variables,
total energy, for example. Is that right? Did you want to add anything to what I just wrote?
Or do you think about it differently? Let me just take another crack at it to see if this
makes sense. A microstate is the state of a system described in microscopic detail,
where we specify this atom has this much energy, that atom has that much energy, for instance.
And we say where all the atoms are and so on, if it's a gas and they're allowed to move around.
Macrostate is a high-level description, as you said. So we might specify that this brick has this
much energy and that brick has that much energy, for instance. So you might say, well, can't I
divide each of the bricks in half and specify that as well? Sure, you can do that. Okay, now we have
this half of the brick has this much energy and that half of the brick has that much energy.
But I'm not going to let you divide it all the way down to Avogadro's scale, right,
to the level of individual particles. As long as you only divide things up a little bit,
we can be fuzzy about whether the macrostate is, you know, the whole system at once or
specifying some parts of it, specifying the energies or the number of particles or something in
various parts of it. Here I'm going to specify the number of particles and the energy in brick one
and the number of particles and the energy in brick two. And I'm saying that once we've specified
all those things, that defines a particular macrostate. But now I'm going to let, I'm going to
consider a whole list of those macrostates with different values of QA and correspondingly
different values of QB with the total health fixed. And then we're going to ask which one of those
macrostates has the largest multiplicity, has the largest number of microstates,
and by what factor? And what we'll find is that some macrostates where the energy is more
uniformly distributed in a certain sense, turn out to have much higher multiplicities
than others. And that leads us into much higher probabilities.
So just to, just to maybe wrap up this discussion a bit. So the macrostate in this particular
example is essentially, let's say QA, right? And it's really, right? I mean, there's, there's two
variables QA and QB, but since their total energy is fixed, that gives us one free variable. And
we'll just take it to PQA. So we're, the question, the analysis we're trying to do is
what are the number of microstates, this, this omega right here, as a function of our
macrostate QA is QA. Yep. If you plot that, so omega on the vertical axis and QA on the horizontal
axis, okay? For moderately large systems like a hundred particles and a hundred energy units,
so you get a graph with a very sharp peak. And if you ask
way out here on the tails, how small is it compared to up here at the peak,
again, for a system of a hundred particles and a hundred units of energy or something like that,
you find that it's 10 to the 30 times smaller or 10 to the 40 times smaller or something like that.
And that's a pretty big number, right? You could, if this system is randomly select,
randomly going from one microstate to another, right? If the energy is just randomly moving around,
and so the system is sampling the various microstates, all with equal probability,
you could sample it a billion times a second for the age of the universe. And I don't think you'd
be very likely to find it down there, down, you know, down on the tails of this distribution.
And that's for a system of only a hundred particles. What if it has 10 to the 23rd particles,
then this peak is so overwhelmingly sharp, so overwhelmingly high, so overwhelmingly narrow,
that you can bet your life on the equilibrium configuration of this system being very, very
close to that peak, never out in the tails. Right. And it's worth mentioning that basically
the width of this peak, I forget, is it like one over n or one over square root of n,
something like that, but it'll have a square root in it. And whether it's the square root of q
or the square root of n depends on which is bigger. And yeah, the relative width of the peak
compared to the full scale of the graph is typically one over the square root of whichever
is smaller. Very, very roughly. I see. Okay, so let's just, let's just, I don't know, o of
one over square root of, let's say, min of n and qa, let's say. The point is that when you have n,
an Avrogadro's number of particles and qa of the appropriate scale, basically that this width is
smaller than any resolution you can, you can probe with, right, into a bench. So the relative width
depends on, so Avrogadro's number is about 10 to the 24, the square root of that is 10 to the 12.
So the relative width is one part in 10 to the 12, one part in a trillion. Yeah.
Okay, well, one, okay, what's, okay, well, I guess. Well, you can measure fluctuations to one part
in a trillion, you might be able to detect some fluctuations there. Yeah, okay. That's
completely unthinkable that we might be able to measure, but the probability of getting a
fluctuation 10 times bigger than that, a 10 to the 11, one in 10 to the 11 fluctuation is
overwhelmingly minuscule. Right, right. That's right, because that's basically, I don't know,
what, 10, 10 standard deviations. That's 10 standard deviations, right? That's right. So that's
potentially suppressed, right? That's right. I guess there's several things we, many, many
different forks in the row we can go on here. But I still haven't told you what entropy is.
Yeah, okay, sure. Let's, let's get back to that. Yeah. Entropy is just the natural log of this
multiplicity. It's times a conventional constant to put it in conventional units, which is Boltzmann's
constant. So, so whether the multiplicity is higher, the entropy is higher, where the multiplicity is
lower, the entropy is lower. And therefore, when you put the two bricks together and wait for a while,
assuming that all microstates are equally likely, we're going to end up somewhere
very close to that peak. And that's going to be the state of the highest multiplicity, or very
near the state of the highest multiplicity, which is also the state of the highest entropy, because
all the, all the logarithm does it smooths out the graph. It doesn't change where the maximum is.
Okay. Actually, uh-huh. Okay. So actually, how do I say it? So we've talked about entropy in a
macroscopic setting in the sense of when you deal with ordinary everyday systems, they have
an Avogadro's number of elements. We are in this large and setting where everything will
concentrate on, uh, you know, the peak entropy say, but if you're in a more microscopic setting
where you have hundreds, thousands of atoms, right, very, you know, you, right, then, um,
the statement about entropy always increases or whatnot. There, you're in a situation where
now the full spectrum of probabilities, you have, they're all comparable now. They're not,
there's not like a delta and everything's negligible. So, so this notion of,
I'm going to push back on you saying they're all comparable. How many particles are we talking
about? A hundred or a thousand, or they're, they're more comparable than, than the situation we just
drew there. The fluctuations start to become very noticeable. That still doesn't mean that all
macro states are going to be equally likely. Oh yeah, they're not, they're, they're not equally
likely. You know, the sales distribution are still very, very, are still very, very unlikely,
even for a system of a hundred or a thousand particles. But what does change is the width of
the peak is now 10% of the width of the whole graph or 5% of the width of the whole graph or
something like that. So the fluctuations start to become very noticeable even though it's,
you're still not going to get all the way into the extreme tail distribution.
Um, yeah, I was trying to, you know, at some point when we get to the second law of thermodynamics
where people talk about entropy, uh, can't decrease or is likely not to decrease. I don't,
I don't know. I like to say entropy tends to increase. Tends to increase. Okay. I think that's
also the correct way of saying it as well. Nicely a little bit fuzzy, right? Yeah. Yes. Yes. Yeah.
I was just trying to say that things are much more guaranteed when you have more particles
because the fluctuations are, are less in a relative sense. Yeah. Okay. Still the second law
of thermodynamics is that multiplicity tends to increase. You, you, you let energy move around
and it's, and, and systems are going to try to find the macro states with higher multiplicity
than the ones they had before because the multiplicities aren't just a little bit higher.
They're a whole lot higher and that makes them more probable. When we take the logarithm and
talk about and restated in terms of entropy instead, the entropy might only increase what
looks like a little bit, but you see a small increase in the logarithm of a quantity
and lead to a, can, can represent a very, very big increase in the quantity you're taking the
logarithm of. So taking the logarithm to get the entropy, all right, Boltzmann's equation S equals
Boltzmann's constant times the natural log of the multiplicity. Okay. So that logarithm turns a
very large number into a number that's merely large, right? And, but it also changes that very
sharp peak into a nice broad, smooth curve. Okay. But that doesn't change the probabilities. You
know, the probability is still you're going to be very, very close to the maximum on that curve.
And so, so, so, you know, if we, if we graph for the same system qa on the horizontal axis
and now the total entropy on the vertical axis, we're going to get some kind of a broad curve
like this. And the top part of the curve is right where the peak of that multiplicity curve
was, it just doesn't look very sharply peaked anymore. But it's still overwhelmingly likely
that for a moderately large system that fluctuations aren't going to take you very far from that peak.
So entropy tends to, if you start out the system over here in an extreme state, it's going to,
let me just get another color, if you start out the system over here in an extreme state,
it's going to spontaneously evolve that way for the peak
by the laws of probability. And they're not, of course, if you look closely enough, there's
going to be tiny fluctuations and it's not going to be a monotonic change. But for a moderately
large system, you can be very certain to one part in a trillion or something like that, where it's
going to end up. Yeah, maybe it's worth stating a few things here. So one of the things you said
earlier was that one of the fundamental assumptions of statistical mechanics, I forget maybe with
the right phrases to that, all fundamental assumption of statistical mechanics. Yeah,
right. So I'll just write it. So the fundamental assumption
of statistical mechanics is that all microstates are equally likely.
All accessible microstates.
Okay, sure. Okay, you know, we put them in the two categories, we say, these are microstates,
we can't get into it all. That's not where you are. But let's count up the ones we can,
as long as it's a mechanism over a reasonable, over the time period we're willing to wait.
The issue of time scales is kind of a tricky one in thermodynamics. And you always have to kind
of have in your head, over what time scale am I talking here? You stir cream in your coffee,
and it mixes in a matter of a few seconds. But the coffee doesn't come to equilibrium with
the surrounding room for many minutes or an hour or something like that. So different states might
be accessible over a long time period, there are going to be states that might become accessible
that are not accessible over short time periods. And that's kind of slippery.
Yeah, I wanted to bring this up because there's many things one could say about this. I mean,
first of all, in some sense, this is both an insight and a simplification in the following sense.
If you have an Avogadro's number of particles, right, you can't apply Newtonian mechanics to
that system and expect to solve it exactly, right? That's impossible. So what you do is say, in my
ignorance of all this motion, I'm just going to maybe, let's say do the simplest or even the
dumbest thing possible, which is to say, well, everything's equally accessible. And that's maybe
a function of, in some sense, my ignorance, right? And that's maybe how this principle
is. I mean, I don't know if you think about it in a different way, but it's sort of like saying,
I don't know what's going on. So let's just say everything's equally accessible.
So in Newtonian mechanics, the number of microstates is literally infinity,
because positions are continued for tables. And so that introduces some issues. I don't
necessarily want to say that quantum mechanics is easier than Newtonian mechanics, because we're
kind of doing some violence to quantum mechanics here too, by only counting the number of quote
states and what we really mean is linearly independent states and quantum mechanics. So
we're really counting the dimension of the vector space that the quantum state lives in, right?
And so I'm really butchering quantum mechanics here.
But at least quantum mechanics gives us a way of thinking of things discreetly and counting them,
whereas in Newtonian mechanics, the multiplicity is always infinite. And then we have to talk
about the measure of some space and whether some infinities are bigger than others by some
measure and things. And that's conceptually, I think, harder for students. One thing you did in
your book, for example, when you were dealing with ideal gases, right, you started computing
entropy in terms of the volume of phase space they occupy. What you did in that analysis was to
essentially discretize it by saying, well, you have Heisenberg's uncertainty principle. And so
you can kind of chop up your space into boxes whose size is Planck's constant, because that's the
irreducible small length. And then you can kind of go between volumes and counting. I mean,
that's essentially what you did, right? And I should say that in mathematics, the way this
manifests itself also is you can think about entropy in terms of the counting problem log of
the number of microstates. But there's also a notion of differential entropy. There's entropy
of a measure. And so there's a way and there's sort of a way of going back between the two also
kind of taking a continuum limit or discretizing things. And those are all kind of related.
So now you're using words that are too big for me. So I'll comment on that.
Okay, okay. Actually, let me just write. Okay, you know, I could teach you, you can teach me,
but let me just say there's a notion of if you have a discrete probability distribution,
that's okay, right? Just a sequence of numbers that adds to one. Have you seen this formula?
This is an entropy of a probability distribution. So what I mean by that is the sum of the Pi's
equals one. Yeah, that formula appears in a problem in my book. And I've never found any use for it.
Okay, but actually, okay, here's the connection between what we've been doing because
ah, it's all written right here. So we look at all accessible microstates are equally likely.
Equally likely means you have the uniform distribution, right?
That would be a special case, right? Yeah, exactly. So in this case, if you have
So you can derive Boltzmann's formula at the top of the screen from the one you just wrote.
Yeah, exactly. Because now in this case, then in the uniform case,
then that means all the Pi's are one over N. And then you'll see that S is indeed,
when you just work that out, is just log of N. Okay, because then you just have all the terms
are equal. So you have a minus log of one over N. And that just is log of N, right? Yeah. So
anyways, this is just a way of unpackaging maybe that gobbledygook I said earlier about
entropies of measures related to kind of you can discretize things, which is what I just did here.
This is this is discretized version because in general, entropy is given by an integral
when you have a continuous object, a measure. And then when you go to discrete objects,
and then you're in this special case of everything being equally accessible or uniform,
then you derive this particular entropy, which is just a special case of a more
general notion of entropy. Anyways, I brought this up actually, well,
this is interesting in and of itself. But to relate it back to what we just said about
classical mechanics and quantum mechanics, you were worried about continuous versus
discrete. And this is just kind of part of that story. In your book, what you did was you
chopped up space based on the Heisenberg uncertainty relation, which says that delta x,
delta p is lower bounded by h bar. And so you can kind of say, okay, well, let me just
take a continuous thing, put boxes of size h bar, and then everything becomes discrete. And then
then you could kind of, yeah, I mean, the particles in wave packet states for which equality holds
in the Heisenberg relation, and then say how wide if they're this wide in position space,
they have to be this wide in momentum space. And, and, and then we can just
ask how many independent ones are there, what does independent mean? It means it's done most,
mostly doesn't overlap. All right, there's a cartoon in the book of that,
you know, a bunch of, bunch of wave packets that are kind of right next to each other. Count them,
one, two, three, four, five, so if you make them narrower, you'll get more in the same volume.
Okay, but if you make them narrower in position space, you got to make them wider in momentum
space, then you get fewer over there. By the way, I've probably gotten more complaints over that
section of the book than over any other. Oh, really? And there's other ways to do it.
My excuse, I guess, is that in chapter six, I'll come back and do it very carefully,
counting energy eigenstates, instead of putting them instead of putting the part of the gas particles
into wave packet states, which are, which are not eigenstates of anything,
put them into energy eigenstates, a one-bump wave, a two-bump wave, a three-bump wave,
in a box of a given size, and then the counting is pretty straightforward.
But I wait until chapter six to do it that way. Okay, okay. Anyways, this is a fun digression.
The reason I, let me get back to the original point I wanted to make about this fundamental
assumption of statistical mechanics. Yeah, because, how do I say, this assumption of all
microstates being equally likely, that really is a cornerstone of all these computations and the
analysis. And what I wanted to ask you, going back to this thing I said earlier about sort of entropy
being described at this very fuzzy heuristic level when people talk about it in normal
speaker and popular expositions of science. So you'll hear things like, oh, it's, you know,
the reason why you can't unscramble an egg is because that would be an entropy decreasing event.
Right. So let's just, let's just try to unpack that when you, when you hear a statement like that.
And as I'm thinking about it, I'm not sure, I mean, that's true. It's much harder to unscramble an egg
than it is scramble. We all, we all know what that means intuitively. But in terms of thinking about
it, in terms of thermodynamics, I kind of hesitated a bit because the setting of an egg and you
stirring it, that's not a setting in which all microstates are equally likely, I would say. I
think it seems to me more having to do with there are, I would say, let me take it like a silly
analogy. Like there's one of you and one of me, but there's a billion people. And that's not a
statement about thermodynamics. It's just that there's fewer atoms that are Dan Schroeder and
fewer atoms that are Tim Wynn than there are that are lots of other people. Every time you
talk about multiplicity, that doesn't mean thermodynamics is involved is all I'm trying to
say. So, so for me, I feel like when talking about thermodynamics, you're really in this situation
about this fundamental assumption of statistical mechanics. Give me your thoughts. Like specifically
the scrambling the egg. Okay, well, I don't know enough about protein molecules to calculate the
multiplicity of scrambles or unscrambled egg. I
can we do simpler examples in that? Yeah, or I'm just just if you know, somebody knows
you're a physicist unless you, you know, Professor Schroeder, why is it harder to
to or easier to scramble an egg than it is to unscramble an egg? Would you resort to
thermodynamics or would you just say, Oh, that's that's that's not that's not that's outside my
domain of expertise? No, no, no. It's just that there's a lot of steps to relate these very,
very simple Einstein solid toy systems to two complicated protein molecules. But I mean,
another issue with scrambling the egg is that you're adding energy to the system during the
process. And then as the egg cools off, it loses energy and so on. I'm pretty confident that the
multiplicity of the scrambled egg is higher than the multiplicity of the of the unbroken raw egg.
And I would assume that has to do with the denaturing of the protein molecules, there's more
ways for them to be stretched out and wiggling around than there are for them to be bolded in
a very precise way. That there's only a smaller way number of ways to do that. Yeah, I guess I
guess maybe the point I'm trying to get at is in this particular example that we had with the
Einstein solid and two different ones, they're put into contact. And then we make the assumption
that all microstates are equally accessible. And then we find that peak and say, ah,
we find that entropy increases because we allowed all the microstates to be equally accessible.
So first of all, that's, I think I described that situation correctly. It's not just it's not just
an assumption that comes out of the blue that all microstates are equally accessible. So
if you if we look at a Newtonian system or a quantum system, there's a time reversal symmetry
to things, which means that if you go from state X to state Y, that there's also a mechanism to get
you back from state Y to state X. I'm talking about microstates now. And so so the
the quantum system we would and I'm not very good with the using the right words here because it's
it's just not something that we discuss a lot at the undergraduate level. But there's something
called the principle of detailed balance. Are you familiar with that? I've heard of it, but you
have to remind me. Yeah, I mean, I'm not sure I can state it correctly. But it just has to do with
saying that any the rate of going from one state to the other is the same as the rate of going back
or the probability. It's you I think of it in terms when we do Monte Carlo simulations that
I was gonna say, yeah, in Markov chains, that's where I've heard it. Yeah, that's right. So
so if you have if you're writing a simulation of the universe and and you want to probabilistically
assign or calculate the chance of going from microstate X to microstate Y, that your system
has to always for modeling any classical system or any quantum system, it has to have the same
probability. If you're already in statewide, I'm going back to state X. Hey, that's the
principle of detailed that you have to have the same probability of going either direction.
And so, you know, at some fundamental level, I think maybe that's the better way of asking
whether it's plausible that all states would be would be all microstates would be equally likely.
Now you might say, well, okay, I can go from this one to this one, I can go from that one to
that one. But maybe there's no way I can get to these over here. Well, then those are inaccessible.
Right. So, but show me a show me a version, you know, show me a force law in Newtonian
mechanics or show me a Hamiltonian in quantum mechanics, it'll take you from one state to
another that can also take you back. I see. But I think the way you would say this in
mathematical analysis is that if you had a Markov matrix, that was symmetric. Right. So
a matrix of all the transition probabilities symmetric is just that detailed balance you just
said. And it's irreducible. So you don't have different subcomponents that don't talk to each
other. And basically, the only stationary measure, the only stationary distribution you can have is
the uniform distribution. I think that's the statement. So there's like a Markov. That's
correct. Okay, I have no idea. Okay. Okay, I think I just I'm just translating what you just said
into Markov chain language. Okay, sounds like you don't like the the fundamental assumptions
statistical mechanics. Maybe tomorrow someone will discover a system for which the fundamental
assumption is wrong. It's not something I'm going to prove. But I think it's consistent with our
experience. Yeah, no. I thought that I don't like it. It was kind of what I said earlier. It's
both kind of I don't know how much of it is an insight versus an approximation, but it seems to
work well. And that's that's ultimately what what matters. I was just trying to
clarify whether given that you have to make that assumption for this kind of analysis.
Is it really fair to talk to apply thermodynamic reasoning in all situations where one is tempted
to make statements about order disorder, multiplicity, yada, yada, when really since this
assumption is what's needed to drive the analysis, that's what one really should be focusing on more
versus like, oh, you know, all these sorts of loose ways in which people talk about order and
disorder. That's all I was trying to the point I was trying to make yet to be able to calculate
the entropy is to in order to in order to apply the second law of thermodynamics, which is a
statement about entropy, you need to be able to quantify the entropy is otherwise it's not useful.
So the second law of thermodynamics says entropy tends to increase and for a sufficiently large
system, you need to worry about small fluctuations away, small probabilistic fluctuations away from
that. Okay, but now if I give you some black box and ask you, you know, what's going to happen,
some chunk of matter that you've never seen before and ask you, how is it going to change in time,
you might not know enough about it to calculate the entropy of the state it's currently in or the
entropy is of the states that might evolve into so you still need to know something about how the
system is made up in order to know enough about the possible things that can happen and what their
entropies would be in order to know which way it's going to go. Maybe, okay, let's just maybe
one more thought on this because I think we might we might just be at an impasse here but I think
here's maybe one one way to nail this point. If you if you were an all powerful demon,
lost demons, if I had control over every molecule, I could arrange for all the velocities and positions
so that they miraculously move all the way into the corner, which would be a huge entropy decreasing
event. And that would be possible in that situation because you violated this all accessible microstates
are equally likely right because you're in that sitting where you actually know completely what's
going to happen to the point where you can't now access every microstate it just goes to the microstate
that is given by Newtonian determinism right so all I was trying to say is since we're not Laplace's
demon then we resign to us in some sense as a statistical ignorance we make this assumption
about all accessible microstates being equally likely and that's the setting in which we do
thermodynamics. Right I mean in Newtonian mechanics at least it's ridiculous to even talk about
likelihoods the system is in whatever state it's in it's right you know tomorrow it'll be in this
state the next day it'll be in that state next day it'll be in that state so yeah I mean thermodynamics
is I don't disagree with the idea that thermodynamics is largely a
exploiting our ignorance of that but remember the very large numbers just just remember the
very large numbers and actually that does take me to one question I wanted to ask you exactly
which exactly was this point about Newtonian mechanics has this feature of time reversal
symmetry nevertheless entropy the second law of thermodynamics that entropy tends to increase
um but if if uh thermodynamics is based on statistical mechanics and therefore Newtonian
mechanics how how how do you reconcile the two the fact that Newtonian mechanics is time reversible
whereas the second law suggests sort of an arrow of time where uh it's the direction which entropy
is increasing well the universe apparently has started out in a very unlikely state far from
thermal equilibrium so from there it's just the probability arguments that I've already made
now I don't know why the universe started out over here okay but you can see that we're starting
out over here and then overwhelmingly likely that we're going to end up here or maybe I should just
be working on the horizontal axis we start out over there and we end up there okay so are you
saying that the way you would answer it is not so it's kind of funny you it's almost like you're
giving a phenomenological answer not one of uh of the laws of physics in the sense that oh there's
there's a timey symmetry just because in the past we happen to be in lower entropy it could have been
the case that in the future that'd be lower entropy uh but that's just not what happens right because
you could have just switched time the laws of physics are the same and now future is past and
passes future so Dan and I got stuck on this puzzling question about the arrow of time so
I'll just insert some of my own post conversation comments here a short answer to our arrow of
time paradox is that the fundamental assumption of statistical mechanics introduces time asymmetry
it assumes that future microstates are all equally accessible with the past determined and the present
as an initial condition advanced listeners can think of it this way diffusion processes like
brownie motion add independent noise to future increments this causes the diffusion process to
spread out in the future rather than the past note that running the diffusion in reverse time
causes the increments to instead be correlated and not independent for listeners interested in
learning more about this puzzle and some additional perspectives they can look up lo schmidt's paradox
or take a look at landau and liftschitz statistical physics section eight so back my original uh two
questions about what is temperature what is entropy um i haven't told you what temperature is yet
what symbol should i use for energy e in my book use u capital u but let's wait whichever let's let's
use e for energy okay okay so here uh the inverse temperature is the rate of change of entropy
with this energy for most types of matter for most objects if we just plot the entropy as a
function of energy we get a graph that's increasing and concave down so what does it mean when the
slope of this graph is steep okay like down at lower energies it has a steeper slope
that's an object which if you give it a little more energy it'll gain a lot of entropy
okay if the slope is steep then when you give it a little bit of energy it gains a lot of
entropy in the process okay whereas when the slope is shallow up here let's let's call this
e sub a and e sub b so at e at e sub a the slope is steep it gains a lot of entropy for each unit
of energy you add for each little bit of energy you add when the slope is shallow it gains only a
little bit of entropy for each bit of energy you add okay so what does that mean
that means that if you have two objects and one of them has a steep slope and the other has a
shallower slope of on its entropy versus energy graph and you let them exchange energy in such a
way that the total entropy increases the one with the steeper slope is going to pull in energy
right because its entropy increases more than the other one's entropy decreases i should
maybe what we should do is is have two different objects right so so one object has this much
energy okay with the shallower slope okay and the other one is is over here so
all right so let's look at that and that okay and and
i should have been more careful with this so let's let's let the first one be object a
and the second one be object b okay okay so
so now we put these objects in thermal contact so they can exchange energy
energy is conserved any bit of energy that b loses a has to gain any energy that a loses b has to
gain okay and we can ask okay well the second law of thermodynamics says that the total
entropy tends to increase what's going to happen the one with the steeper slope is going to gain
the energy because its entropy goes up more by going a little bit this way okay meanwhile that
one goes a little bit that way so so so a's energy increases a little bit b's energy decreases a
little bit b loses some entropy in the process but a has gained even more entropy and therefore
that will happen right and going the other way won't happen so so a is the object that we would
say is at the lower temperature and b is the object that we would say is at the higher temperature
okay because b is the one that spontaneously giving up energy that a okay right and therefore we
define temperature as the reciprocal of the slope okay so so so that's that's that's the slope
okay if the slope is steep the temperature is low and if the slope is shallow the temperature is high
yeah I think it's that's how that works yeah this is a very nice visualizations let me just
rephrase that just to have a different way of parsing what you just said so these two objects
they're at their own energies e sub a and e sub b and they're therefore at their own temperatures
t sub a and t sub b and and by the definition you just gave t sub b is higher than t sub a because
its slope is lower and temperatures the reciprocal slope right so what you just what we just
illustrated here right so this has higher slope therefore lower temperature and this has lower
slope and therefore higher temperature and then sort of the picture you should have in mind is that
you can gain more from gain more entropy by
moving energy to the right let me draw the ingredient so here energy ea is going to increase
and energy eb is going to decrease because the amount of entropy you lose when you move when b
moves to the left is more than compensated by the entropy increased by a moving to the right
precisely because of this slope disparity and then the equilibrium temperature they're going to reach
and and the it will be the point at which these two slopes are equal which is precisely
them being at the same temperature and and the reason why the slopes are equal is because then
there's no net gain by moving by adjusting the energies right and that's precisely yeah right
yeah you so you've taken it one step farther and asked where does the process end in the
action is exactly where the two slopes are equal and the odds are at what we would call the same
temperature when the slopes are the same yep that's yeah this is a and that's so that's what
temperature is and that's what this formula is telling us that temperature is the reciprocal
of the slope of an object's entropy versus energy trap it's it's a and therefore it is a measure
of the tendency of an object to spontaneously give up energy to other objects the higher the
temperature the more the object will spontaneously give up energy that other things with lower
temperature yeah and this this basically is the formal definition of temperature correct
yeah it's the only definition of temperature yeah sure so you know there's an operational
definition sure what we measure one meter yeah i'm not sure what you mean so what did you mean
by the word formal is there an informal definition oh yeah by far on it like in terms of a mathematical
definition which is what this is yeah i was just wondering if if you had some other informal
definition in mind uh no i mean okay yeah i mean there was a potential formal definition in terms of
average kinetic energy because that is oh no no no yeah it's not it's not yeah yeah okay
find that one yeah yeah yeah so this is this is very nice as a mathematical definition now
one thing i genuinely curious about you you did mention that the the the founding fathers of
thermodynamics they they had to sort of do things the the hard way uh in terms of not having quantum
mechanics at their disposal now i think historically uh maybe actually maybe you can comment on this
because you know there was there's carno there's engines and they they they thought about reversible
irreversible processes processes and sort of the way it's it's often written is uh let's say d
let's say d e just to be consistent with your notation and i'm thinking of it as as heat energy
salt water as d q uh no i'm not going to let you do that oh what does q stand for
uh well at least in your book it's it represents the heat heat is and see heat is energy in transit
that so when i talk about energy spontaneously flowing from one object to another
that's what we call heat it's the energy transferred due to that spontaneous process
it's it's energy in transit or or an amount of energy that that moves from one place to another
spontaneously and driven by this entropy increase overall entropy increase okay what i don't like
is the little d that you put in term in front of q all right oh sorry i should have just written q
maybe is that what your is that what you were complaining about okay sorry yeah i see sorry okay
if it's an internet test we'll call all right yeah so okay when you put a d in front of it people
want to say the change in q ah okay sorry i just uh it was not a state function that can change got
it it's it's an amount of heat that flows got it fair enough fair enough okay uh yeah okay all right
thank you thank you and then q uh let's see if i you know is is tds all right okay um so i think
this is the way people historically thought about the the relationship between these quantities and
i'm curious to know a little bit more about the history and how like people we you know we have
the benefit of generations of work we could just write down these formulas and talk about it as if
everything's obvious but somebody had to figure this out right so how did they
how did they sort of think about this i don't know if you have any further comments
i don't know i'm not a historian and i've never studied the the history i've i've you know read a
little bit of carno in translation and maybe a little bit of clausius and a little bit of
bolstern but not enough to have um to have any insight into the history and of course history is
is incredibly complex when you get into it because they don't have the benefit of hindsight on
these ideas and they're wrestling with them and and often aren't expressing things the simplest way
that that they could so so i i'm i'm not in a position to say that you know and in textbooks
we always give a little bit of pseudo history and i do too and talk about how far no came first and
then clausius and then bolstern um so you know i know a little bit about the chronology and and
who roughly what was known at one time and by a certain person and what was roughly what was
known at another time by another person even if they wouldn't have said it the same way we do
um so so with all those caveats what do you want to ask me oh i i just i just wanted to
see if if there was a way of because i i i certainly as a mathematician i'm i'm very happy with the
story i just i was just wondering if there was any insight to be gained about how people
thought about it in such a way that it might give insight into how this was discovered
well everyone's seen
that when you put two things at different temperatures together the hot one cools off and
the cold one warms up i i i guess people like carno and clausius thought very deeply about that
now carno did not i i my understanding is that carno did not know that he was a form of energy
and and he didn't quite distinguish between entropy and heat and yet he had this deep insight
that led him to formulate a version of the second law of thermodynamics and put limits on how
efficient engines can be and things like that from that um you know and by clausius's time a few
decades later it was understood that that heat is a form of energy or a or a means of transferring
energy um even then it wasn't so clausius figured out that there has to be this thing called that
he called entropy there has to be this quantity you know every object has a certain amount of it
and and and it's related to temperature by this formula that you wrote and i wrote it's the same
formula right your formula is the same as mine okay so you just like d's and i like delton's
okay but it's the same you know and you don't need q right you can just you could say just
changing energy is t times yes um i i call that formula the definition of temperature that that
formula embodies the most fundamental thing about temperature which is that it's a measure of how
generous an object is with its energy giving it up to other objects um okay uh and you know and and
then it was bolstern who made the connection to statistics and microstates and understanding
what entropy actually is in a statistical sense have we answered everything is there anything else
you wanted to say there's a lot of things we have not answered okay right um but i you know i hope i've
given a basic textbook explanation of what entropy is how it relates to temperature and
what the second law of thermodynamics says do you have any final i don't know any final thoughts
about uh the subject i know of course thermodynamics is is a richer subject that
can be covered in in a single conversation much less even even a single textbook um yeah just any
any i don't know final thoughts on the subject or words of advice on maybe people trying to learn
the subject well those are two different questions let me let me give another thought
we've been speaking very abstractly and talking about some pretty abstract model systems and things
thermodynamics is also extremely important in the real world and and some people might be
less interested in that and some people might be more interested in that um but i think everyone
has a reason to care about thermodynamics thermodynamics does as carnault showed put
limits on the efficiencies of engines it tells us that we can make heat pumps that are more efficient
than an electric space heater um it's an incredibly important thing to understand thermodynamics to
if if we care about energy and the biman implications of energy and the energy transition
as people try to use more energy more efficiently and get it in better wet get it from better sources
thermodynamics is what tells us where all the energy ends up it all ends up as dispersed
waste heat that we can't make any use of and and what it means to use energy right we can't destroy
energy no but using energy means taking it from a low entropy form and turning it into a high
entropy form where it's very widely distributed so i i guess i think it's important that people
understand thermodynamics if they want to participate in making the world a better place through
coming up with better energy technologies and i think there's a lot of i see a lot of misconceptions
about thermodynamics and i guess i would encourage people to get away from those one of them this
that temperature is merely a measure of the average kinetic energy of the system
but another one a much more specialized one is is that when an engine doesn't achieve the
maximum efficiency that the second law of thermodynamics allows i see in a lot of textbooks
and a lot of people talk as if the reason is because well there's just friction and if there
were no friction maybe it would achieve the car no efficiency no that's not the reason
and it's much has to do with the reason why the heat closed in the first place and how a heat
engine works so um so i think that there are very very practical reasons to understand basic
thermodynamic all right now you also asked if i have some advice for students learning thermodynamics
don't just watch videos get out the pencil and paper and and and do some hard work and wrestle
with things oh absolutely yeah that's my advice yeah i mean this uh make no mistake this this the
videos i produce are not a substitute for the labor that is involved in any learning process
really not not you know particularly in science and math but in any field there's there's uh
a long pipeline and repetition of doing problems and thinking about it until you really understand
the material right great well uh thank you so much for your time Dan this was a very uh i think
enlightening to to get your thoughts certainly it's it's rare to hear it from the author himself
when when learning a subject well thank you very much for your interest Tim and thanks to all those
out there who are watching this
