WEBVTT

00:00.000 --> 00:05.000
Hi, and welcome to my course on the Jupyter Notebook for Data Science Teams.

00:05.000 --> 00:09.000
I'll just give you a brief overview of the course so you can know what you're getting into.

00:09.000 --> 00:14.000
So we'll start off by doing what you expect, getting Jupyter Notebook set up on your machines.

00:14.000 --> 00:18.000
The second thing I'll go over then is Jupyter Notebook features.

00:18.000 --> 00:21.000
So there's a lot of really interesting things going on with the Jupyter Notebook.

00:21.000 --> 00:25.000
Some of the most useful functionality comes from what's called Notebook Extensions.

00:25.000 --> 00:27.000
So I'll be going over a lot of those.

00:27.000 --> 00:30.000
I'll be showing you how you can use both Python and R in the same Notebook.

00:30.000 --> 00:33.000
So if you have some piece of your analysis that needs to be done on R,

00:33.000 --> 00:36.000
it's very easy to actually do part of it in R, send it back to Python,

00:36.000 --> 00:41.000
or even use R natively as the entire kernel that runs your Jupyter Notebook.

00:41.000 --> 00:45.000
Also, different things like using SQL in the Notebook to query databases,

00:45.000 --> 00:49.000
some really nice post-save hook functionality and widgets,

00:49.000 --> 00:51.000
which I'll just demonstrate right here.

00:51.000 --> 00:56.000
This is an example of a widget where I've created a function that generates data

00:56.000 --> 00:59.000
according to some line, and the green line shows you the actual function

00:59.000 --> 01:04.000
with some noise, and the blue dots are data that gets drawn from this distribution.

01:04.000 --> 01:08.000
So the total number of points is 10, and as I click and drag this off to the right,

01:08.000 --> 01:12.000
increase the number of points, and you can see that the fit dynamically moves around

01:12.000 --> 01:18.000
as I add data points and actually slowly but surely converges on the underlying distribution.

01:18.000 --> 01:22.000
So this is an example of a widget which is very easy to do

01:22.000 --> 01:26.000
and provides lots of functionality for all kinds of data exploration.

01:26.000 --> 01:31.000
Finally, I'll get into sharing the notebooks on a data science team,

01:31.000 --> 01:35.000
so there's a lot of questions you have to consider for your particular situation,

01:35.000 --> 01:39.000
so I'll try to give you a strategic framework so that you can actually identify

01:39.000 --> 01:42.000
what kind of workflow makes sense for your situation.

01:42.000 --> 01:46.000
There's various other things about conceptually breaking up your notebook structure

01:46.000 --> 01:50.000
into lab notebooks and deliverable notebooks, and a lot more that goes into that.

01:50.000 --> 01:53.000
Finally, I'll go through two different data science projects,

01:53.000 --> 01:56.000
which will just demonstrate the principles I talked about above,

01:56.000 --> 02:00.000
and you get to see it in an environment where I'm explaining and going through

02:00.000 --> 02:03.000
how to actually do a data science project from end to end

02:03.000 --> 02:07.000
using all the different techniques I was already talking about.

02:07.000 --> 02:11.000
In this lesson, we'll be installing the R version of Python,

02:11.000 --> 02:14.000
iPython, and Jupyter so that we can run the Jupyter notebook.

02:14.000 --> 02:18.000
The way we run this is by installing the Anaconda distribution.

02:18.000 --> 02:21.000
There are other ways of installing the iPython notebook,

02:21.000 --> 02:24.000
but I recommend the Anaconda for its ease of use.

02:24.000 --> 02:29.000
We'll first go over to any web browser and search for Anaconda Python.

02:29.000 --> 02:34.000
What you'll see here is that the top link is the continuum Anaconda distribution.

02:34.000 --> 02:36.000
Clicking on that takes you right to the downloads page,

02:36.000 --> 02:38.000
and you see that you have different options.

02:38.000 --> 02:43.000
You can get it with Windows, OSX, or Linux, whichever one you prefer.

02:43.000 --> 02:46.000
Since I'm using OSX, I'll click on OSX.

02:46.000 --> 02:50.000
The choice between using Python 2.7 and 3.4 is a tough one.

02:50.000 --> 02:55.000
I'll be using 2.7 just because many of the legacy code bases still use 2.7,

02:55.000 --> 02:59.000
but feel free to, if you're feeling experimental, to go to Python 3.4.

02:59.000 --> 03:02.000
I'll be using this Mac OSX 64-bit one.

03:02.000 --> 03:05.000
If you have a different system, please use that one.

03:05.000 --> 03:08.000
Okay, great. Now that that package has downloaded,

03:08.000 --> 03:12.000
install it by following the instructions on the screen.

03:12.000 --> 03:16.000
So, clicking through and agreeing to various licenses.

03:16.000 --> 03:20.000
And hopefully you get to this stage where it says the installation was successful.

03:20.000 --> 03:22.000
Click Close.

03:22.000 --> 03:24.000
We can close our browser as well.

03:24.000 --> 03:28.000
At this stage, if we typed IPython, it still won't work.

03:28.000 --> 03:32.000
One thing that the graphical interface does is actually adds a command to your bash profile.

03:32.000 --> 03:36.000
So if I were to actually go into vi.bash underscore profile,

03:36.000 --> 03:39.000
the first piece of the profile has been there from before,

03:39.000 --> 03:42.000
but this was added by the Anaconda 2.3 installer,

03:42.000 --> 03:47.000
which exports this path, shows you that the Anaconda folder has been created in my home directory,

03:47.000 --> 03:49.000
and adds that to my path.

03:49.000 --> 03:53.000
So if I type ls, you actually do see the Anaconda directory right here.

03:53.000 --> 03:56.000
This makes it really easy to uninstall Anaconda if you want.

03:56.000 --> 04:00.000
You can remove that line from that path in your bash profile,

04:00.000 --> 04:03.000
and you can delete this folder, and everything should be gone off your system,

04:03.000 --> 04:05.000
and you can use the old system defaults.

04:05.000 --> 04:09.000
But now that we have this, we have to source our bash profile,

04:09.000 --> 04:12.000
and we should now be able to type IPython.

04:12.000 --> 04:17.000
Now that we've run IPython, we see we are running Python 2.7, in this case 0.10.

04:17.000 --> 04:22.000
It's the Anaconda distribution, and this is IPython version 3.2.

04:22.000 --> 04:24.000
So a lot of different numbers here.

04:24.000 --> 04:27.000
The ones that are important are the Python version, which is 2.7,

04:27.000 --> 04:29.000
and the IPython version, which is 3.2.

04:29.000 --> 04:31.000
Now this is actually a bit behind.

04:31.000 --> 04:34.000
So what we're going to do is hit Ctrl-D, and it says, do you really want to exit?

04:34.000 --> 04:36.000
You can either type yes in return,

04:36.000 --> 04:39.000
or you can type Ctrl-D a second time to actually exit it.

04:39.000 --> 04:43.000
But what we'd like to do now is actually update to the most recent version of Anaconda,

04:43.000 --> 04:45.000
so that we have the most recent version.

04:45.000 --> 04:47.000
The way to do this is to type conda.

04:47.000 --> 04:49.000
This is a new command line argument that you have.

04:49.000 --> 04:54.000
The way that we update the Anaconda distribution is by typing conda in various packages.

04:54.000 --> 04:57.000
So in this case, we want to conda, install conda.

04:57.000 --> 05:00.000
What this should do is check to see various things.

05:00.000 --> 05:02.000
This tells us the following packages will be updated.

05:02.000 --> 05:07.000
Conda will go from this version 3.14 to 3.18, conda environment, and so on.

05:07.000 --> 05:11.000
And we would like to set this up, and we will hit yes to this.

05:11.000 --> 05:15.000
It actually takes quite a bit of time to install all of these things from source,

05:15.000 --> 05:19.000
but most of these things are actually pre-compiled, so everything's already completed.

05:19.000 --> 05:22.000
We'd like to also update a number of packages.

05:22.000 --> 05:28.000
So let's conda install Jupyter, and now you can actually chain which packages you'd like to see.

05:28.000 --> 05:35.000
So in this case, we'll install Jupyter, the pandas package, and scikit-learn.

05:35.000 --> 05:37.000
So these are the packages to be updated.

05:37.000 --> 05:41.000
We see that a number of things are going from ipython, which is important,

05:41.000 --> 05:43.000
going from version 3.2 to version 4.0.

05:43.000 --> 05:46.000
We'd like to proceed with that.

05:46.000 --> 05:51.000
I'd like to just say a few words about why I find the Anaconda package to be a useful thing to use.

05:51.000 --> 05:55.000
They make sure that all the packages you've installed will play nicely with each other.

05:55.000 --> 06:01.000
So sometimes if you're using pip by itself, you can actually install dependencies that overlap each other out of place,

06:01.000 --> 06:04.000
so they end up with a conflict when you're trying to import these libraries,

06:04.000 --> 06:08.000
and Anaconda does a really nice job of making sure, checking those dependencies really well.

06:08.000 --> 06:14.000
So now if we type ipython, we should see that we are running ipython 4.0, which we are.

06:14.000 --> 06:17.000
Now we would like to actually check out the ipython notebook,

06:17.000 --> 06:19.000
because that's the part where it really gets interesting.

06:19.000 --> 06:24.000
So let's create an example directory, and from here we can type Jupyter notebook.

06:25.000 --> 06:27.000
Just typing Jupyter notebook, a couple of things happened.

06:27.000 --> 06:29.000
First of all, go back to the terminal.

06:29.000 --> 06:32.000
I typed Jupyter notebook, and ran it.

06:32.000 --> 06:37.000
And a notebook server started from the directory we are in, so a user's jbw example in this case.

06:37.000 --> 06:41.000
So an ipython notebook, which has been started with a Jupyter, is now at this location.

06:41.000 --> 06:45.000
HTTP colon slash slash localhost, in this case, quadruple 8.

06:45.000 --> 06:49.000
And it says here this useful thing, control C to stop this server and shut down all kernels,

06:49.000 --> 06:51.000
and you have to do it twice to skip the confirmation.

06:51.000 --> 06:55.000
Now this starts the server running, and this terminal needs to stay open.

06:55.000 --> 07:01.000
If we go back to this, what it runs is a web server, and it automatically by default opens your default browser.

07:01.000 --> 07:05.000
So in this case, here we are at this location, localhost, quadruple 8, underscore 3.

07:05.000 --> 07:10.000
And if we'd like to start a new notebook, we can click new, Python 2 notebook.

07:10.000 --> 07:12.000
And again, this is referring to which version of Python you're running.

07:12.000 --> 07:15.000
This is a 2 version 3 versus 3.

07:15.000 --> 07:21.000
And we see now that we're running a Jupyter notebook, and we can start typing valid Python code,

07:21.000 --> 07:24.000
and see the output from it right there.

07:24.000 --> 07:26.000
Let's do something a little bit more interesting.

07:26.000 --> 07:31.000
So we import the NumPy library as np, and then print numpy.a range 10.

07:31.000 --> 07:40.000
So we see the first bit of Python code, and we know that we have the installation working just as we hoped.

07:40.000 --> 07:45.000
In this video, we're going to start a GitHub repo to house a data science project.

07:45.000 --> 07:48.000
First, we have to go to github.com.

07:48.000 --> 07:54.000
If you don't have GitHub or Git setup, I highly recommend starting out by picking a username,

07:54.000 --> 07:57.000
by giving your email and creating a GitHub account.

07:57.000 --> 08:02.000
Now, if you have Windows or Linux or Mac operating system,

08:02.000 --> 08:08.000
GitHub itself has a really nice tutorial for how to actually set up Git on your machine and for your setup.

08:08.000 --> 08:09.000
So I recommend doing that.

08:09.000 --> 08:16.000
So once you have a GitHub account, which is free, or if you already have one, click sign in, let's go to the next step.

08:16.000 --> 08:23.000
So you've signed into GitHub, click the plus next to your name in the upper right hand corner, and start a new repository.

08:23.000 --> 08:28.000
I prefer to start a new repository through the GitHub website itself, and then clone it to my local machine.

08:28.000 --> 08:31.000
So that way, the remote connection has already set up.

08:31.000 --> 08:37.000
And that's usually a stumbling block that can be a little bit annoying to overcome if you try to do it the other way around.

08:37.000 --> 08:40.000
In this case, I'm going to be looking at some cold data.

08:40.000 --> 08:43.000
So I'm going to call it cold exploration.

08:43.000 --> 08:46.000
I'm going to give it a quick description.

08:46.000 --> 08:49.000
I'm giving it the description a first look at the cold data.

08:49.000 --> 08:52.000
I'm going to let it be public so anyone can see this repository.

08:52.000 --> 08:55.000
So afterward, you can also see this if you'd like to go to it.

08:55.000 --> 09:00.000
I will initialize this repository with a readme and I will add a gitignore.

09:00.000 --> 09:07.000
A .gitignore file will let you ignore the machine generated code that comes along with various programming languages.

09:07.000 --> 09:13.000
Now Python doesn't have that many, but there is usually a .pyc if you're running a Python file.

09:13.000 --> 09:19.000
I also recommend having a license, especially if it's going to be public, so that you can share your repositories with others.

09:19.000 --> 09:22.000
If you work for a company, obviously you have different licensing concerns.

09:22.000 --> 09:24.000
So then click create repository.

09:24.000 --> 09:25.000
It's as easy as that.

09:25.000 --> 09:30.000
So now I have the cold exploration repository in my GitHub account.

09:30.000 --> 09:35.000
From here, we would like to actually tie this account to our local machine.

09:35.000 --> 09:40.000
So we can copy this text that's just to the right of this SSH tab.

09:40.000 --> 09:46.000
Now, if it doesn't say SSH, if it says HTTPS, I would recommend clicking it to SSH.

09:46.000 --> 09:50.000
And once you do that, copy the text that's in this text box.

09:50.000 --> 09:56.000
Navigate with your terminal to a place that you think is an appropriate spot for this repository.

09:56.000 --> 10:03.000
Type in git clone and paste the text that you just copied from the website itself.

10:03.000 --> 10:08.000
So now we see the license and the readme files that we created on the website itself.

10:08.000 --> 10:17.000
All right, so we have set up our GitHub repository and we've cloned it to our local machine and we're ready to start doing some data science.

10:17.000 --> 10:24.000
In this lesson, I'm going to give you some extra intuition so you can understand what's happening when the Jupyter Notebook is running.

10:24.000 --> 10:29.000
So in my terminal, if I type ls, I get to see the directories now underneath this current directory.

10:29.000 --> 10:31.000
I see deliver, dev and source.

10:31.000 --> 10:36.000
By typing Jupyter Notebook, I again start the Jupyter server.

10:36.000 --> 10:38.000
My default browser is Chrome.

10:38.000 --> 10:41.000
So again, we see those same three directories deliver, dev and source.

10:41.000 --> 10:45.000
If we toggle back to the terminal, we can see several messages.

10:45.000 --> 10:49.000
The first is the directory under which the notebook server has been started.

10:49.000 --> 10:51.000
The second message is the number of active kernels.

10:51.000 --> 10:56.000
The third message is the location that you can point your browser to to find this notebook.

10:56.000 --> 10:59.000
And finally a message to say how to stop this server.

10:59.000 --> 11:06.000
So going back to the notebook itself, if we click on the development branch, we see that there's no notebooks in here.

11:06.000 --> 11:10.000
We can start a notebook by clicking on new and then clicking on Python 2.

11:10.000 --> 11:13.000
So after clicking new, we see a new tab appear.

11:13.000 --> 11:18.000
It's currently named untitled and the last checkpoint comes from a few seconds ago.

11:18.000 --> 11:19.000
So let's type a few things.

11:19.000 --> 11:23.000
So let's just say first as a variable is equal to the 5.0.

11:23.000 --> 11:27.000
I execute that cell by holding down shift and hitting return.

11:27.000 --> 11:30.000
When I do that, a new cell appears beneath it.

11:30.000 --> 11:39.000
And as I type a second variable and label it say 23.0, again hitting return with the shift key produces another cell beneath it.

11:39.000 --> 11:43.000
So I now have two variables, one named first and one named second.

11:43.000 --> 11:49.000
And there's unsaved changes, which means if I lose this current browser, I will lose the changes that happened from the last time it was saved.

11:49.000 --> 11:51.000
In this case, there's nothing that's been saved.

11:51.000 --> 11:53.000
So let me go ahead and save this right now.

11:53.000 --> 11:54.000
There's two ways of doing this.

11:54.000 --> 11:59.000
One, typing command S if you're on the Mac or control S on Windows, which I just did.

11:59.000 --> 12:03.000
Or you can click this save disk and it will also save it.

12:03.000 --> 12:07.000
Now that it's been saved and there's no unsaved changes.

12:07.000 --> 12:18.000
If I close this tab, or if I even close the whole browser by quitting the Chrome browser, all of the actual information has been stored in the kernel itself.

12:18.000 --> 12:24.000
In other words, there's this kernel and everything that's happened with the kernel is being stored in state by this kernel.

12:24.000 --> 12:31.000
This means if I open up a brand new version of Chrome and I go to where the notebook is running from the previous message before.

12:31.000 --> 12:36.000
I copied that with control C, go back to Chrome browser and type it in here.

12:36.000 --> 12:39.000
I go back to the exact view we had before.

12:39.000 --> 12:41.000
Clicking on Dev, because that's where we were.

12:41.000 --> 12:45.000
We actually see that the untitled IPython notebook is actually still running.

12:45.000 --> 12:50.000
So if we click on this, we reattach the browser to the underlying kernel.

12:50.000 --> 12:56.000
So if you have saved your notebook as you work and you close the browser, the work still remains in memory.

12:56.000 --> 13:01.000
So if I say print first comma second, now we see the actual results is here.

13:01.000 --> 13:03.000
So this is all been saved.

13:03.000 --> 13:09.000
And that's one interesting thing that you should know is that the browser itself is a front end to what's really going on in the kernel.

13:09.000 --> 13:16.000
Now, the converse to this is what happens if I completely close and shut down the server.

13:16.000 --> 13:19.000
So I hit control C twice and shut down the kernels.

13:19.000 --> 13:21.000
So all the kernels have been shutting down.

13:21.000 --> 13:26.000
So going back to the browser, you see a message that says connection to the notebook server cannot be established.

13:26.000 --> 13:30.000
Let's continue to try to reconnect, but you won't be able to run any code.

13:30.000 --> 13:40.000
So in this case, if I try to do something, say I want to say first times second and execute this and shift enter, nothing happens.

13:40.000 --> 13:43.000
And this is what you see when it's trying to connect to the kernel and it's failing to.

13:43.000 --> 13:49.000
So this is the part where it actually needs to be running and needs to be continually talking to your browser.

13:49.000 --> 13:54.000
Unfortunately, restarting the kernel does not give us back to where we were before.

13:54.000 --> 14:04.000
So here I can try to reload this notebook and we still see what we had previously done, but watch what happens when I try to run this third cell.

14:04.000 --> 14:08.000
The name first is not defined and the input name of the cell one to one.

14:08.000 --> 14:14.000
So the kernel has completely restarted as you saw me do in the terminal, which means that now we have to start from the beginning.

14:14.000 --> 14:21.000
And now everything has been stored in state saving it keeps it so that the kernel is now running in the background.

14:21.000 --> 14:23.000
Hopefully that gave you a little bit of insight into what's happening.

14:23.000 --> 14:28.000
The browser acts as a front end to this process that's running in the back end on this terminal.

14:28.000 --> 14:41.000
The browser can be closed or blown away after you've saved all of the changes that you've made, but the kernel cannot be the kernel has to stay running if you want to keep the changes that you've done in memory.

14:41.000 --> 14:45.000
In this lesson, we'll be talking about Jupyter notebook extensions.

14:46.000 --> 14:53.000
Notebook extensions, as the name suggests, are extensions to the capabilities that the Jupyter notebook already comes with.

14:53.000 --> 14:57.000
Now there's many different ways that you can actually extend the behavior of a Jupyter notebook.

14:57.000 --> 14:59.000
I'm going to show you just two.

14:59.000 --> 15:03.000
The first extension that I'll show you is called Jupyter Pivot Tables.

15:03.000 --> 15:07.000
And if you click on this link here, you'll see that you go to this website.

15:07.000 --> 15:13.000
Nicholas.crucian.com slash content 2015-09 Jupyter Pivot Tables.

15:13.000 --> 15:17.000
And this allows for drag and drop pivot tables and charts.

15:17.000 --> 15:20.000
And this write-up he has is actually a really nice write-up.

15:20.000 --> 15:27.000
I recommend you reading and watching this video as well because he explains in some detail how you can actually use his extension.

15:27.000 --> 15:33.000
To install this, all you have to do is go to this pip install command.

15:33.000 --> 15:40.000
So copy pip install pivot table JS and run that command in your terminal.

15:40.000 --> 15:43.000
So it's successfully installed the pivot table JS.

15:43.000 --> 15:45.000
We go back to our notebook.

15:45.000 --> 15:55.000
We can now run the cells that import both pandas and numpy and this command, which is from pivot table JS import pivot UI.

15:55.000 --> 15:58.000
So that loaded correctly without any errors.

15:58.000 --> 16:01.000
So we have now loaded this extension.

16:01.000 --> 16:09.000
As of Jupyter 4.0, the preferred way of installing notebook extensions is through a pip install of the extension.

16:09.000 --> 16:13.000
There are other ways of doing it as well and I'll show you a second way at the end of this video.

16:13.000 --> 16:18.000
So let's actually take a look at some data with this pivot table extension.

16:18.000 --> 16:24.000
Go to HTTPS colon slash slash data dot austintexas.gov.

16:24.000 --> 16:30.000
In this website, we're going to go down and look at the restaurant inspection scores.

16:30.000 --> 16:33.000
From this, we will export data.

16:33.000 --> 16:35.000
The format we want is CSV.

16:35.000 --> 16:40.000
We do want it to go into our data folder and it's called restaurant inspection scores.

16:40.000 --> 16:42.000
Return to save that.

16:42.000 --> 16:45.000
You can now close this tab and go back to our notebook.

16:45.000 --> 16:50.000
Now that we've downloaded the CSV file, let's read it into pandas data frame.

16:50.000 --> 16:59.000
I'm going to split the cell at the current place where it's blinking by typing control shift minus because I want to run this on just one cell by itself.

16:59.000 --> 17:05.000
So reselecting that cell, I now hit shift and return and it correctly loads in the data frame.

17:05.000 --> 17:10.000
So ways to check that is actually look at what the top of this data frame looks like.

17:10.000 --> 17:19.000
We see that the restaurant name, the zip code, inspection date, the score, the address, facility ID and the process description actually looks like it's been read in correctly.

17:19.000 --> 17:25.000
One thing you will notice is that the address has return characters in it because standard address has multiple lines.

17:25.000 --> 17:27.000
And I'm actually going to be okay with that.

17:27.000 --> 17:32.000
I'm going to say I would like to keep the address on one line in the data frame, not have that split up in different ways.

17:32.000 --> 17:39.000
So let's take a look at what we get when we look at just the data frame itself is pivot underscore UI.

17:39.000 --> 17:43.000
So we've imported pivot underscore UI up here in the first cell.

17:43.000 --> 17:45.000
Let's execute the cell here.

17:45.000 --> 17:47.000
Now a number of things happened in the background.

17:47.000 --> 17:52.000
But what you end up seeing, we close this window down here that shows what we downloaded.

17:52.000 --> 17:56.000
And I will actually toggle this toolbar for now.

17:56.000 --> 17:58.000
So we can actually see a bit more.

17:58.000 --> 18:02.000
We have the various columns of the data frame available on the top here.

18:02.000 --> 18:04.000
So zip code, inspection date, score.

18:04.000 --> 18:07.000
They are now dragable into these two places.

18:07.000 --> 18:08.000
So let's do that.

18:08.000 --> 18:12.000
Let's actually drag score along the top.

18:12.000 --> 18:17.000
Let's see if there's a relationship between the zip code of a restaurant and the score.

18:17.000 --> 18:24.000
So just by dragging those two columns in, we see that there are, for each of these zip codes, different scores that have been given to the restaurant.

18:24.000 --> 18:28.000
Of course, a really good score is a 100 for the health score.

18:28.000 --> 18:33.000
And we can actually scroll down and take a look at this data in a really intuitive way.

18:33.000 --> 18:37.000
This looks pretty neat, but there's a lot of numbers going on.

18:37.000 --> 18:39.000
It's actually kind of hard to read.

18:39.000 --> 18:44.000
So one thing we can do is actually change the output type from table to something else like a heat map.

18:44.000 --> 18:51.000
So this does the same data as we saw before, but it actually highlights the outlying points that are large with a darker color.

18:51.000 --> 18:56.000
So now by eye, you can visually see the different relationships between these two variables.

18:56.000 --> 18:58.000
I still think this is actually a little bit too big.

18:58.000 --> 19:05.000
So I'll give one extra hint of taking data that actually has a lot of different granular pieces.

19:05.000 --> 19:14.000
So let's take this very granular number across the top and bin it by something, let's say five to give us a little bit less granularity.

19:14.000 --> 19:16.000
So here's some code that will actually do that.

19:16.000 --> 19:21.000
So we're going to create a new column in this data frame called bin score for bin to score.

19:21.000 --> 19:31.000
I'm going to use a pandas function called cut, which will now cut up these column df.score into bins that go from 30 to 100,

19:31.000 --> 19:37.000
because no b.a range is not inclusive of the last data point and stepping by five.

19:37.000 --> 19:39.000
So I'm going to run this cell.

19:39.000 --> 19:43.000
It's going to create a data frame column named bin score.

19:43.000 --> 19:46.000
And let's see what this one looks like.

19:46.000 --> 19:51.000
We can drag bin score along the x-axis here and zip code along the y-axis.

19:51.000 --> 20:00.000
We now see that the binned scores are now counting everything that has a zip code off to the left and any score within a certain range.

20:00.000 --> 20:02.000
In a range of five.

20:02.000 --> 20:04.000
We can then also take a look at this.

20:04.000 --> 20:06.000
Instead of a table, we can look at it as a heat map.

20:06.000 --> 20:11.000
You can also see if it looks okay in terms of a bar chart, for example.

20:11.000 --> 20:15.000
And this doesn't quite make sense, but there's many different things that are different here.

20:15.000 --> 20:19.000
You can actually look at tree map, for example.

20:19.000 --> 20:25.000
So the various visualizations that are available to you may or may not make sense to the data that you're looking at.

20:25.000 --> 20:34.000
But the availability of this is actually a really nice extension to the notebook capability that Jupyter already comes with.

20:34.000 --> 20:39.000
Alright, so picking up on where the last video left off, notebook extensions.

20:39.000 --> 20:41.000
We've already installed one extension.

20:41.000 --> 20:43.000
This is the pivot table extension.

20:43.000 --> 20:47.000
It's one of the extensions that I'd like to highlight for this video.

20:47.000 --> 20:51.000
And it actually comes from this URL here.

20:51.000 --> 20:55.000
I want to turn this into, let me just show you this real quick.

20:55.000 --> 20:59.000
This code block is currently set as code.

20:59.000 --> 21:01.000
I'd like to actually change it to mark down.

21:01.000 --> 21:02.000
There's two ways to do that.

21:02.000 --> 21:10.000
By clicking on the toolbar like I just did, or by typing M when this cell is selected in this gray circle right now.

21:10.000 --> 21:13.000
If I type Y, it would turn back to code.

21:13.000 --> 21:17.000
So I just typed Y, you saw the drop down menu turned to code.

21:17.000 --> 21:23.000
And since it's still selected with a gray box, I can type M and it goes to mark down.

21:23.000 --> 21:29.000
So I want it marked down so that when I click this, I can actually open a new tab.

21:29.000 --> 21:33.000
So the Jupyter slideshow extension is this GitHub repo right here.

21:33.000 --> 21:39.000
It has a lot of really interesting capabilities that I will be showing you at the very end of this course.

21:39.000 --> 21:49.000
I'll be using the rise Jupyter slideshow extension to help us make a final slideshow presentation out of some of our data science projects.

21:49.000 --> 21:56.000
To install this notebook extension, it says to simply run python setup.py install from the rise repository.

21:56.000 --> 22:00.000
Now this means we actually have to first download this extension code.

22:00.000 --> 22:03.000
So this isn't done in the usual PIP install way.

22:03.000 --> 22:07.000
This is done by choosing the SSH version here at the top of the page.

22:07.000 --> 22:09.000
Selecting this by clicking once.

22:09.000 --> 22:14.000
GitHub actually makes it so that the entire thing is highlighted so you can now command C to copy this.

22:14.000 --> 22:17.000
Go to your terminal.

22:17.000 --> 22:29.000
And at this point, if you don't have a folder for your GitHub repositories that you just grabbed from wild, basically, I would recommend creating one.

22:29.000 --> 22:35.000
So we type git clone and then paste in the code we had copied from GitHub web page.

22:35.000 --> 22:38.000
So it clones into this thing called rise.

22:38.000 --> 22:40.000
Let's CD into this.

22:40.000 --> 22:43.000
We see various things here, the live reveal package, Jason and so on.

22:43.000 --> 22:46.000
Let's go back to the GitHub page.

22:46.000 --> 22:49.000
This is we simply run python setup.py install.

22:49.000 --> 22:52.000
So I'll copy that code and paste.

22:52.000 --> 22:58.000
Okay, so we have now installed this live reveal.js notebook extension.

22:58.000 --> 23:00.000
So we go back to our notebook.

23:00.000 --> 23:07.000
We see that there's an extra toolbar cell here, which has something different than we normally see, including a slideshow option.

23:07.000 --> 23:13.000
And we actually need to restart this notebook to actually get the ability to make this look like a slideshow.

23:13.000 --> 23:16.000
So let me go ahead and do that.

23:16.000 --> 23:21.000
I'll do save and checkpoint and then close and halt.

23:21.000 --> 23:26.000
I'll go back to where it's running in the terminal and hit control C once.

23:26.000 --> 23:29.000
It says it's currently running, shut down the server, yes or no.

23:29.000 --> 23:32.000
If you wait too long, it'll actually say I didn't see an answer.

23:32.000 --> 23:34.000
So I'm just going to assume you did that by mistake.

23:34.000 --> 23:37.000
We actually do want to quit this.

23:37.000 --> 23:39.000
So we'll do control C twice.

23:39.000 --> 23:41.000
You can have also selected why.

23:41.000 --> 23:43.000
So we shut down all the kernels.

23:43.000 --> 23:46.000
And this thing, if I reload this should not be available.

23:46.000 --> 23:51.000
Let's rerun Jupyter notebook and it will give us a new version of this exact thing.

23:51.000 --> 23:53.000
Click notebook extensions.

23:53.000 --> 23:58.000
And now you still see this toolbar here with the currently being non the slideshow option,

23:58.000 --> 24:01.000
but you also have a new button off to the right.

24:01.000 --> 24:05.000
So let's actually click this and click the slideshow option.

24:05.000 --> 24:12.000
If you'd actually like to turn one of your notebooks into a slideshow, the functionality is now at your fingertips.

24:12.000 --> 24:17.000
And if you don't want to see all these extra cell toolbars, you can always put this back to none.

24:17.000 --> 24:18.000
They should be saved.

24:18.000 --> 24:22.000
So any clicking slideshow again, the fact that these are all slides has been preserved.

24:22.000 --> 24:29.000
To look at the slideshow itself, we just click this button and type into the right gives you the different slides.

24:29.000 --> 24:36.000
And one interesting thing about this or one thing that I think is very, very useful is that this is not just a rendered notebook of this.

24:36.000 --> 24:41.000
This is actually a live cell that we can actually import and actually run new code.

24:41.000 --> 24:46.000
So I just ran that piece of Python code during the slideshow while it's up.

24:46.000 --> 24:51.000
So this is very nice for interactive demonstrations.

24:52.000 --> 24:59.000
In this video, I'll be showing you how to actually query SQL databases from the Jupyter notebook itself.

24:59.000 --> 25:04.000
A lot of enterprise data is stored in databases, so dealing with them will be part of your everyday job.

25:04.000 --> 25:11.000
The Jupyter notebook makes it really nice to be able to document very clearly the SQL queries that you are creating.

25:11.000 --> 25:19.000
So I recommend if you're going to be using SQL connections using a Jupyter notebook extension called ipython SQL.

25:19.000 --> 25:24.000
It's installed by typing pip install ipython dash SQL.

25:24.000 --> 25:33.000
Once you install that, you then have access to an extension that you can load by simply typing percent load extension space SQL.

25:33.000 --> 25:37.000
When you run this cell, it actually loads in this magic extension.

25:37.000 --> 25:42.000
It gives you a number of warning signs, but these are just warnings. The package will still work just fine.

25:42.000 --> 25:48.000
This next line percent config will actually configure our ipython SQL extension.

25:48.000 --> 25:57.000
And what this configuration does, we say SQL magic, we would like to automatically return results that are a table as a pandas data frame.

25:57.000 --> 26:07.000
You don't have to do this, but I recommend it because most of the time you'd actually like to take the data you've queried the database from and transform it and use it in the standard data science tools.

26:07.000 --> 26:09.000
So I'll run that command as well.

26:09.000 --> 26:15.000
Next import pandas and for this demonstration, I'll be using SQL lite.

26:15.000 --> 26:19.000
You can use any of the standard SQL engine connections.

26:19.000 --> 26:24.000
I'm just using SQL lite because it's a simple and easy database to run with for an example.

26:24.000 --> 26:29.000
This next cell, I'm actually going to create a table and put some data into it.

26:29.000 --> 26:38.000
So if you're familiar with SQL, you'll notice that everything below the first line of this cell is SQL commands that leaves this top line to be explained.

26:38.000 --> 26:42.000
So what we have here is a double percent sign and then SQL.

26:42.000 --> 26:45.000
This is how you call what's called a cell magic.

26:45.000 --> 26:56.000
If I hit tab while I'm at the end of these double percent sign, I will see a little pop up that tells us of all the different options we can have to change this into a cell magic.

26:56.000 --> 27:03.000
When I say cell magic, what this means is that this is a special flag that tells ipython that something different is going to happen for this entire cell.

27:03.000 --> 27:07.000
In this case, we're telling it everything after this first line is going to be a SQL query.

27:07.000 --> 27:09.000
As you can tell, there's other ways you can do this as well.

27:09.000 --> 27:11.000
You can have HTML, you can have bash.

27:11.000 --> 27:15.000
There's various other options as well, but I'm just showing you right now the SQL one.

27:15.000 --> 27:19.000
Now this is how you connect to a SQL database that's just stored in memory.

27:19.000 --> 27:27.000
If you have a different package, a different engine, then you can use the various documentation to tell you which connection you should use.

27:27.000 --> 27:30.000
So we're going to create a very simple small table called presidents.

27:30.000 --> 27:35.000
We're going to have first and last name and we're going to include the year that they were born.

27:35.000 --> 27:40.000
And I just have a random sampling of about 10 US presidents here.

27:40.000 --> 27:44.000
So running this cell, we get some output here that says one row is affected.

27:44.000 --> 27:46.000
We've inserted values into this table.

27:46.000 --> 27:52.000
And now we can actually run a SQL command that's in inline again with a single percent.

27:52.000 --> 27:55.000
When you have this command here, it says everything after it will be SQL.

27:55.000 --> 28:02.000
So we're going to store an object called later presidents, the SQL command and the results that come from the SQL query.

28:02.000 --> 28:09.000
The SQL query being select everything from the presidents table where the year of birth was later than 1825.

28:09.000 --> 28:12.000
And then I'm going to show you what that looks like by typing it there.

28:12.000 --> 28:18.000
So we see that there were three presidents that were born in that table after 1825.

28:18.000 --> 28:24.000
And if we took a look at the type of this return, we will see that it is actually a pandas core data frame.

28:24.000 --> 28:28.000
So we have returned a SQL query into a pandas data frame.

28:28.000 --> 28:33.000
And now we can use all of the normal tools and functionality of pandas directly.

28:33.000 --> 28:39.000
If we would like to write out this into a file, we can do that by doing this SQL three command here.

28:39.000 --> 28:41.000
So we make a connection to a new file.

28:41.000 --> 28:49.000
And then you run the pandas data frame method to SQL and say, we'll write out the presidents table to the connection.

28:49.000 --> 28:57.000
Now, if you don't want to use cell magic in this way, you can also use pandas directly to query our SQL database.

28:57.000 --> 29:01.000
So I'll show you how to do that from reading in that file that we just wrote out.

29:01.000 --> 29:05.000
So we're going to connect it out to this presidents SQL output.

29:05.000 --> 29:08.000
We're going to now create a cursor that connects to that connection.

29:08.000 --> 29:14.000
And we will create a new data frame by doing the pandas function read SQL.

29:14.000 --> 29:19.000
If you hit shift tab while your cursor is inside the parentheses, you get to see the various calls here.

29:19.000 --> 29:25.000
So we have the SQL command, you're giving it the SQL, then you're following it with the connection.

29:25.000 --> 29:30.000
And then everything else can be these many other options that you have to really customize it.

29:30.000 --> 29:36.000
And once you've done that, be sure to remember to close the connection by doing com.close.

29:36.000 --> 29:41.000
So the new data frame should have everything that we stored in the previous query.

29:41.000 --> 29:45.000
So the three presidents that we saved from above.

29:45.000 --> 29:49.000
And again, this is a data frame that was returned from that.

29:49.000 --> 29:54.000
So I just showed you two different ways that you can query databases.

29:54.000 --> 29:59.000
You can query them with an inline magic, or you can query them through pandas directly.

29:59.000 --> 30:10.000
And either one will return to a pandas data frame so that you can actually use the output in some exploratory data analysis or your full-fledged project.

30:10.000 --> 30:16.000
In this video, we'll be talking about how to actually use R in the Jupyter Notebook ecosystem.

30:16.000 --> 30:22.000
Previously, we talked about how we can actually set up different Python and R environments.

30:22.000 --> 30:26.000
To set up a unique conda environment for Python 2, for example,

30:26.000 --> 30:33.000
we can do conda create minus n for name pi2, for example, just as a descriptive name that you could use.

30:33.000 --> 30:38.000
We set the Python version to be equal to 2, and then the other packages that we would like to install.

30:38.000 --> 30:41.000
So anaconda, Jupyter itself, notebook.

30:41.000 --> 30:44.000
We do the same thing for the Python 3 environment.

30:44.000 --> 30:50.000
So conda create with a different name, Python 3, for example, and setting the Python version equaling to 3.

30:50.000 --> 30:53.000
We also do the same thing when we want to do an R environment.

30:53.000 --> 30:57.000
So in this case, conda create minus n, and we're going to call this Jupyter underscore R.

30:57.000 --> 31:06.000
And with creating the channel by minus C, R tells Jupyter and tells conda that you're actually creating an R kernel as well as the default other ones.

31:06.000 --> 31:12.000
And this creates the R kernel so that the Jupyter Notebook can actually run R natively,

31:12.000 --> 31:17.000
as well as installing a number of different packages that it thinks are both recommended and essential.

31:17.000 --> 31:20.000
And finally, a Python package called rpi2.

31:20.000 --> 31:27.000
The way to activate these commands is you say source activate and then the name of the environment that you created.

31:27.000 --> 31:30.000
And when you're done with it, source deactivate.

31:30.000 --> 31:36.000
And if you ever forget which environments you've actually installed or what the names you used were, you can do conda environment list.

31:36.000 --> 31:40.000
Let's do that to start with conda env list.

31:40.000 --> 31:44.000
And we see that there are four different environments installed.

31:44.000 --> 31:49.000
There's the root one, which doesn't really qualify as an environment, but then we have pi2, pi3, and Jupyter R.

31:49.000 --> 31:56.000
So let's source activate pi3 and say Jupyter Notebook.

31:56.000 --> 31:59.000
Once we start that, we can start a Python Notebook.

31:59.000 --> 32:06.000
And we see in the upper right-hand corner, not only a blue flag that says using the pi3 kernel just for a second before it flashed away,

32:06.000 --> 32:10.000
you actually see that it types Python 3 in the upper right-hand corner.

32:10.000 --> 32:15.000
Let's verify that by doing a print 5 plus 5 as a statement and as we can do in Python 2.

32:15.000 --> 32:17.000
And this doesn't work in Python 3.

32:17.000 --> 32:20.000
The syntax for Python 3 is with parentheses.

32:20.000 --> 32:23.000
All right, so we are using Python 3.

32:23.000 --> 32:28.000
Let's close and halt this and shut down the server by hitting control C twice.

32:28.000 --> 32:33.000
We can tell that we're using Python 3 because pi3 is at the beginning of our terminal screen right there.

32:33.000 --> 32:36.000
So I have to say source deactivate.

32:36.000 --> 32:38.000
Again, conda env list.

32:38.000 --> 32:43.000
Let's switch to Jupyter.

32:43.000 --> 32:46.000
The command is the same Jupyter Notebook.

32:46.000 --> 32:50.000
Now we can click this pure R example and it loads up R.

32:50.000 --> 32:56.000
Just in case you're curious, we can go back to this home directory and create a new, in this case, R.

32:56.000 --> 33:00.000
And this is an R kernel running natively.

33:00.000 --> 33:04.000
So you can tell again, look in the upper right-hand corner, not only is it not using Python,

33:04.000 --> 33:08.000
it's actually using the R kernel natively for this entire notebook.

33:08.000 --> 33:10.000
Let's go back to this pure R example.

33:10.000 --> 33:12.000
So what is it that R can do?

33:12.000 --> 33:16.000
R is a language that has some design choices that are slightly different than Python,

33:16.000 --> 33:20.000
but it does have a huge statistics library packages.

33:20.000 --> 33:25.000
So you load them in and everything you'll be done in this notebook will be actual R code itself.

33:25.000 --> 33:28.000
And again, just looking in the upper-hand corner, this is now R code.

33:28.000 --> 33:30.000
I loaded a few libraries here.

33:30.000 --> 33:36.000
These are some standard, actually really nice libraries in R, the plier package and ggplot2.

33:36.000 --> 33:43.000
This economics data comes when you load in the plier library and you see the head of this economics data.

33:43.000 --> 33:48.000
You can create a ggplot command by doing this R code here.

33:48.000 --> 33:58.000
And just like with the Jupyter Notebook, we're using Python, we see inline plotting so that all of the workflow is in the same really nice way

33:58.000 --> 34:05.000
where you can do this piecemeal exploring by looking at a single piece of R code in the output.

34:05.000 --> 34:07.000
Let's close and save this.

34:07.000 --> 34:11.000
And now let's open up this Rpy2 example.

34:11.000 --> 34:18.000
We are now running again a Python 2 kernel and we're actually using the Jupyter R environment.

34:18.000 --> 34:22.000
So Jupyter R environment can run Python and it can run R itself.

34:22.000 --> 34:25.000
It's running either one depending on what you started the notebook as.

34:25.000 --> 34:27.000
So we're running this one as a Python notebook.

34:27.000 --> 34:31.000
But here's a really nice feature of the Jupyter Notebook.

34:31.000 --> 34:36.000
You can intermingle Python code and R code in the same notebook.

34:36.000 --> 34:38.000
I'll show you how this works.

34:38.000 --> 34:40.000
So the top here importing numpy as NP.

34:40.000 --> 34:46.000
So again, just Python code, we're creating X and Y where X is this a range.

34:46.000 --> 34:53.000
Let's just look at what X is an array from zero to nine and Y is some random number plus the X variable.

34:53.000 --> 34:59.000
We import this library Rpy2 and load this extension Rpy2.ipython.

34:59.000 --> 35:05.000
So we load it by doing this percent magic percent load extension Rpy2.ipython.

35:05.000 --> 35:08.000
And you can do this in a cell that has other code.

35:08.000 --> 35:10.000
You don't have to make this a single cell.

35:10.000 --> 35:17.000
Just wrote it five plus five just so you can see that we've loaded in an extension and we have this other code running as well.

35:17.000 --> 35:22.000
So we have these two numpy arrays, a capital X and a capital Y.

35:22.000 --> 35:28.000
If we would actually like to do some analysis in R and then push something back into Python,

35:28.000 --> 35:31.000
we do that by now doing a thing called a cell magic.

35:31.000 --> 35:36.000
So cell magics are known by having a double percent sign at the very beginning of a cell.

35:36.000 --> 35:41.000
That means that this top line is a special thing that in this case we're having it.

35:41.000 --> 35:43.000
There's HTML and bash and various other options.

35:43.000 --> 35:50.000
We are using the R option and we are sending in with this input X and Y from the Python environment.

35:50.000 --> 35:58.000
So we are sending to R the two numpy arrays and we would like to get back from R this thing called XY coefficient.

35:58.000 --> 36:01.000
Everything else in this cell is R code.

36:01.000 --> 36:06.000
So XYLM is equal to linear model of Y goes as X.

36:06.000 --> 36:14.000
XY coefficient which we will be returning back to Python after this cell completes is the coefficients of this model.

36:14.000 --> 36:17.000
We're going to print the summary and we're going to make a plot.

36:17.000 --> 36:25.000
So run that cell and we see the formula call here, the residual, some intercept and X coefficients.

36:25.000 --> 36:30.000
And we have some plots that are displayed in our Python notebook.

36:30.000 --> 36:37.000
And again, we actually get our XY coefficient out back into our Python environment.

36:37.000 --> 36:45.000
So if you're a person who actually likes to use R just as much as you like to use Python or you like to use R for particular tasks

36:45.000 --> 36:49.000
or you like to use Python for lots of it, the Jupyter notebook is very, very flexible.

36:49.000 --> 36:57.000
It lets you work in whichever environment you prefer while dropping into the alternate Python or R environment to do just even a few pieces of it.

36:57.000 --> 37:02.000
So if you're in the middle of a long piece of data science analysis and you need one functionality from R,

37:02.000 --> 37:11.000
you can keep that not only in the notebook but passing it back and forth through native types.

37:11.000 --> 37:16.000
In this video, we'll be doing a somewhat more advanced topic and it's definitely 100% optional.

37:16.000 --> 37:23.000
We'll be talking about how to get into the guts of the Jupyter notebook system itself and create a post save hook,

37:23.000 --> 37:31.000
which will, for our purposes, save a script and an HTML file version of our Jupyter notebooks themselves.

37:31.000 --> 37:32.000
So how do we do this?

37:32.000 --> 37:37.000
The first step is to actually create a notebook configuration file.

37:37.000 --> 37:41.000
Now you can do that if you're interested in doing it in just your root environment

37:41.000 --> 37:47.000
or having this behavior be copied everywhere you are actually working on anything to do with the Jupyter notebook.

37:47.000 --> 37:53.000
Just go ahead and run Jupyter notebook generate config and I will copy and paste this into the terminal.

37:53.000 --> 37:55.000
So you can see what it looks like when you run this.

37:55.000 --> 38:03.000
The key takeaway here is this writing default config to now this should be your home directory dot Jupyter slash

38:03.000 --> 38:06.000
and then it's going to be this file called Jupyter notebook config.

38:06.000 --> 38:11.000
There's another way you can do this if you want to make this for a specific type of analysis.

38:11.000 --> 38:14.000
So maybe only the analysis you do involving housing data.

38:14.000 --> 38:17.000
Do you want to have a special behavior happen?

38:17.000 --> 38:19.000
You can do that in a somewhat roundabout way.

38:19.000 --> 38:22.000
You set this Jupyter config directory.

38:22.000 --> 38:26.000
It's an environment variable and set that to be a thing that doesn't exist yet.

38:26.000 --> 38:31.000
A home directory so tilde slash dot Jupyter save.

38:31.000 --> 38:35.000
You run a command that starts like this and then you generate the config.

38:35.000 --> 38:37.000
So I will show you what this looks like.

38:37.000 --> 38:41.000
So it wrote the default configuration file to dot Jupyter underscore save,

38:41.000 --> 38:47.000
which is the name of this profile and then the same Jupyter notebook config file.

38:47.000 --> 38:55.000
Now, running it in this way, you have Jupyter and configure before you do the actual command sets it as a temporary environment variable,

38:55.000 --> 38:58.000
meaning it's only set for that one command.

38:58.000 --> 39:01.000
If I try to echo this, I won't have anything stored in it.

39:01.000 --> 39:04.000
So I'm not exporting this as an environment variable.

39:04.000 --> 39:12.000
Now, I have a bit of code here that I'm going to actually toggle this header and this toolbar just to give us a little bit of extra space.

39:12.000 --> 39:19.000
I have some code here that I would like you to add to your Jupyter config profile file.

39:19.000 --> 39:24.000
So this Jupyter notebook config dot pi and instead of trying to type it off the screen,

39:24.000 --> 39:34.000
you can actually access it by typing HTTP colon slash slash b i t dot l y so bit dot l y and then Jupyter underscore profile.

39:34.000 --> 39:40.000
You click on that, you will go to the same exact code I have that I typed out here.

39:40.000 --> 39:47.000
In this case, I will actually copy this code and we're going to open up the file that we would like to modify.

39:47.000 --> 39:53.000
So in this case, we're going to be modifying this Jupyter save underscore Jupyter notebook config file.

39:53.000 --> 39:58.000
You can do with any text editor, I'm going to use sublime text, so sublime text open it up.

39:58.000 --> 40:00.000
Now, here's what the file looks like.

40:00.000 --> 40:07.000
It's actually a whole lot of things you can do to modify the behavior of your Jupyter notebook and they're almost all commented out.

40:07.000 --> 40:11.000
So you can read through this if you want to actually make different changes than what I'm going to recommend.

40:11.000 --> 40:17.000
But this is where we post just at the top this code, just a brief overview what's happening.

40:17.000 --> 40:24.000
It defines a function called post save, and it basically grabs the path of the notebook that's currently running,

40:24.000 --> 40:29.000
and it actually tries to run this command ipython nb convert to script,

40:29.000 --> 40:35.000
which means it's going to be a .py file if it's a Python file or a .r file if it's a r notebook,

40:35.000 --> 40:40.000
and an HTML file, which means that it'll just be the rendered HTML version of it.

40:40.000 --> 40:45.000
And the C dot file contents manager post save hook equals post save.

40:45.000 --> 40:52.000
So this is a way that Jupyter developers have allowed a person to make changes after every save that they do.

40:52.000 --> 40:57.000
So let's save that, and let's go back to our notebook.

40:57.000 --> 40:59.000
So let's list what's in this directory.

40:59.000 --> 41:05.000
We see the name of this current notebook is autosave other formats.

41:05.000 --> 41:07.000
I'm going to toggle that away again.

41:07.000 --> 41:09.000
So we see it here when I type ls.

41:09.000 --> 41:14.000
We can also do exclamation mark ls to do a command like this.

41:14.000 --> 41:18.000
And we see that when we save this, we see a checkpoint is created,

41:18.000 --> 41:21.000
but no other new files are being created.

41:21.000 --> 41:28.000
If we would like to see what happens when we run Jupyter notebook with this new Jupyter save configuration file,

41:28.000 --> 41:30.000
we'll have to run a command that looks like this.

41:30.000 --> 41:34.000
Jupyterconfigure equals this with Jupyter notebook.

41:34.000 --> 41:40.000
And in this case, I would actually like to save this entire thing as an alias,

41:40.000 --> 41:45.000
and then you can add this to your bash RC, or you can simply run this in a single line on your terminal.

41:45.000 --> 41:49.000
If you just want it in your terminal, however, it will not set it as a thing.

41:49.000 --> 41:53.000
So if you restart your computer or open up a new terminal, typing Jupyter save won't work.

41:53.000 --> 41:59.000
If you add this to your dot bash RC, then this special way of opening Jupyter notebook will be saved.

41:59.000 --> 42:07.000
So let's close this current notebook and let's type Jupyter save.

42:08.000 --> 42:11.000
And let's reopen it again in this new way.

42:11.000 --> 42:13.000
So we just opened it up.

42:13.000 --> 42:16.000
The list function down here should show us what we saw before.

42:16.000 --> 42:19.000
So we see the same files in this directory.

42:19.000 --> 42:23.000
When I click save, if our post save hook worked correctly,

42:23.000 --> 42:29.000
we will see autosaveotherformats.py and autosaveotherformats.html.

42:29.000 --> 42:34.000
So I'm going to do that after I click save type ls again.

42:34.000 --> 42:37.000
And we see that we do have two other forms.

42:37.000 --> 42:39.000
I have html and .py.

42:39.000 --> 42:43.000
Just to show you what those html and py versions look like, let's open that up.

42:43.000 --> 42:45.000
Oh, one last note.

42:45.000 --> 42:48.000
Every time you hit save, it will overwrite the same file a bunch of times.

42:48.000 --> 42:50.000
So it's not going to create new versions of this.

42:50.000 --> 42:56.000
It's going to just continually overwrite this and always keep the .html and the .py files completely up to date.

42:56.000 --> 42:58.000
Let's look at one of these html files actually looks like.

42:58.000 --> 43:01.000
So let's go back to the terminal to open a new one.

43:01.000 --> 43:08.000
So by typing open autosaveotherformats.html, we actually have the fully rendered notebook here.

43:08.000 --> 43:11.000
So what we see here is what we saw on the other page.

43:11.000 --> 43:13.000
And this is now the html version of this.

43:13.000 --> 43:15.000
This can be emailed somewhere.

43:15.000 --> 43:18.000
This can be posted online somewhere and people can see this.

43:18.000 --> 43:26.000
Now the links work like you'd expect and the code is all formatted and looks like it looks in the notebook.

43:26.000 --> 43:29.000
But since it's just an html file and it's not an actual notebook running,

43:29.000 --> 43:32.000
none of these cells are actually computable.

43:32.000 --> 43:34.000
I can't actually rerun these cells.

43:34.000 --> 43:47.000
So now we have a way of creating a post-save hook that lets us save out automatically html and script versions of any notebook that you're saving.

43:47.000 --> 43:53.000
If you would like to commit this to your GitHub repository for fellow members of the team to review in different ways,

43:53.000 --> 43:58.000
then having a post-save hook like this can save you tons of time and keep everything up to date.

43:59.000 --> 44:04.000
In this video, we'll be talking about a really fun topic called widgets.

44:04.000 --> 44:11.000
Widgets is an entire aspect of the Jupyter Notebook ecosystem that lets you do interactive things with the notebook.

44:11.000 --> 44:13.000
Let's go over to the notebook.

44:13.000 --> 44:18.000
This top cell has various imports, matplotlib, numpy, and so forth.

44:18.000 --> 44:22.000
This last line in this cell actually imports the ipython widgets.

44:22.000 --> 44:30.000
And we're going to import a number of sliders, a float slider, the integer slider, a toggle button, and this interactive thing as well.

44:30.000 --> 44:33.000
So let's execute that by typing shift enter.

44:33.000 --> 44:35.000
Now this next cell contains a simple formula.

44:35.000 --> 44:38.000
We define a Python function named polynomial.

44:38.000 --> 44:46.000
It takes three arguments, which has default values, so slope of 2.0 and intercept of 5 and show points, which can be either true or false.

44:46.000 --> 44:52.000
We're going to create some x values, which is just a linear spacing from negative 10 to 10 using 50 points.

44:52.000 --> 44:57.000
We're having a y value, which is just the slope times x plus the intercept.

44:57.000 --> 45:01.000
Everything else in this function is actually going to be plotting something.

45:01.000 --> 45:04.000
So this tells us the figure size we're going to use.

45:04.000 --> 45:07.000
The next line tells you that we're going to actually use a figure.

45:07.000 --> 45:12.000
The next two lines actually talk about whether or not the show points is true or false.

45:12.000 --> 45:21.000
If it says show points, we'll see what this actually does in a second, but it'll add the actual data points we are plotting up when we define this x at the top line here.

45:21.000 --> 45:27.000
Finally, we plot x and y and we set some window parameters and give ourselves some axes.

45:27.000 --> 45:30.000
The last thing we do is add this tight layout call at the very bottom.

45:30.000 --> 45:35.000
This just helps clean up the map plotlib plots before they're finally ready.

45:35.000 --> 45:39.000
So after executing this cell, we now have defined polynomial.

45:39.000 --> 45:41.000
Let's scroll down to this next cell.

45:41.000 --> 45:44.000
I'm defining a thing called a slope slider.

45:44.000 --> 45:48.000
This slope slider is called a float slider, which means it can actually take float values.

45:48.000 --> 45:50.000
That's what it's actually sweeping across.

45:50.000 --> 45:54.000
The value is 2.0, meaning that that's the starting value for the slope.

45:54.000 --> 46:01.000
Let's actually start this at minus 10 at the maximum of plus 10 and step size of, oh, let's say 1.0.

46:01.000 --> 46:06.000
The next line defines this object called w, which is interactive.

46:06.000 --> 46:11.000
The first argument you give interactive is actually the function that you want to be interacting with.

46:11.000 --> 46:17.000
In this case, the function we just defined polynomial and any other widgets that need to be connected to it.

46:17.000 --> 46:25.000
So in this case, we're going to connect the slope parameter that's given to the polynomial function to the slope slider.

46:25.000 --> 46:28.000
Now we call it slope slider, which is because we want to have a descriptive name.

46:28.000 --> 46:30.000
You can name it anything you want.

46:30.000 --> 46:34.000
The last thing we do is actually execute this w. Let's see what we see.

46:34.000 --> 46:41.000
We see three widgets that we can interact with, the slope, the intercept, and show points, which is toggle.

46:41.000 --> 46:47.000
Let's scroll down and I'm going to actually hide this toolbar so we have a little extra space.

46:47.000 --> 46:54.000
We have the slope, which is 2, and now you can actually click and drag this to different values.

46:54.000 --> 47:03.000
As you drag it to the right, you're increasing the slope and we can see that it's actually correspondingly increasing the slope of that line in the plot.

47:03.000 --> 47:06.000
We can also move the intercept point up and down.

47:06.000 --> 47:12.000
And as we know, an intercept just changes the y-positioning, shifting these things linearly up and down.

47:12.000 --> 47:17.000
And of course, the last thing is to toggle on and off show points.

47:17.000 --> 47:24.000
If you want to change this, we can actually make this much more sensitive by saying let's make the minimum minus 100, the maximum plus 100,

47:24.000 --> 47:27.000
step size of, oh, let's say five.

47:27.000 --> 47:35.000
Now, as we change the slope, it should be much more sensitive than it is because we're now at slope of 75.

47:35.000 --> 47:38.000
And as we go negative, we can see that as well.

47:38.000 --> 47:45.000
So as you can tell, having this kind of functionality at your fingertips is actually incredibly useful during all phases of doing a data science project,

47:45.000 --> 47:48.000
especially during the exploratory data analysis stage.

47:48.000 --> 47:53.000
So you can imagine if you did something like k-means to look at your data.

47:53.000 --> 47:57.000
You can set k, the number of clusters you're fitting for, the number of centroids.

47:57.000 --> 48:07.000
And as you can move that back and forth with the integer slider, for example, you can see how well the algorithm is actually clustering on that number of centroids.

48:07.000 --> 48:12.000
So being able to do that in an interactive way can speed things up quite a bit, and it's really nice.

48:12.000 --> 48:15.000
So this is a somewhat simple example that I just showed.

48:15.000 --> 48:22.000
Here is a much more complicated example, but just to give you a sense of what is possible with this kind of a thing.

48:22.000 --> 48:28.000
So I'm not expecting you to actually read this and understand the code that goes behind it, but let's just execute this real fast.

48:28.000 --> 48:35.000
This is one of the projects I was working on just on my own, where I want to actually have some random points in a small area,

48:35.000 --> 48:42.000
and I would like to interpolate with a spline interpolate those random points, and I wanted to see what that looked like at the end.

48:42.000 --> 48:47.000
So I can say the number of points that I'm randomly generating and splining between.

48:47.000 --> 48:51.000
And as I slide this to the right, you can see the pattern becomes more and more complicated.

48:51.000 --> 48:55.000
And as I slide this to the left, we get much simpler shapes.

48:55.000 --> 49:01.000
We also have a smoothing parameter here, which can give you a smoothing factor to these kind of more complicated shapes.

49:01.000 --> 49:09.000
It sort of unwinds the and rewinds up the knots and the alpha value, for example, like how dark this is.

49:09.000 --> 49:17.000
Or if I want to have a slight jitter to each of these strokes, I can add the number of brush strokes and then increase or decrease the jitter for this.

49:17.000 --> 49:26.000
So obviously there's a lot going on here, but this is one aspect that shows you just how, first of all, how quickly this can refresh, but also how useful it is.

49:26.000 --> 49:39.000
In this video, we saw how we can use interactive capabilities of the Jupyter Notebook to help us plot and look at data and change the values by sliding sliders around.

49:39.000 --> 49:44.000
In this video, I'll be talking about some bleeding edge developments in the Jupyter project.

49:44.000 --> 49:47.000
A specific thing called Jupyter Hub.

49:47.000 --> 49:59.000
If we were to go to Google, let's just search for it by saying Jupyter, then HUB, the top link will be this GitHub repository, which is Jupyter slash Jupyter Hub.

49:59.000 --> 50:04.000
And this allows, as it says, multi-user servers for Jupyter Notebooks.

50:04.000 --> 50:13.000
In other words, if you have a server where there's data being held for a data science team, you can run a single instance of this thing called Jupyter Hub.

50:13.000 --> 50:21.000
And it allows many different data scientists to log in and start a Jupyter Notebook on that server co-located with the data.

50:21.000 --> 50:25.000
Now, this is an active development. It's changing on weekly time scales.

50:25.000 --> 50:32.000
So if I were to actually show you how to set it up today, by the time you saw this video, it would probably be different from how you're supposed to be setting it up then.

50:32.000 --> 50:41.000
So for right now, I'll point you to this documentation and mention that it's actually very much bleeding edge, but I think it will be the future for data science teams.

50:41.000 --> 50:50.000
Just to give you a sense of what it looks like when you were to use Jupyter Hub, you can go to try.jupyter.org and hit Return.

50:50.000 --> 50:59.000
And what you're actually interfacing with here is a Jupyter Hub server somewhere in the back end, currently being hosted by Rackspace, apparently.

50:59.000 --> 51:09.000
And you can start a new notebook in any of these different styles, so Bash, Haskell, Julia, Python 2, or Python 3, R, Ruby, and Scala.

51:09.000 --> 51:16.000
So you can start a notebook here, and this is just letting you run a temporary quick one. You can also start one of these notebooks, like this Python one.

51:16.000 --> 51:22.000
And it starts with this warning. Don't rely on this server for anything you want to last. The server will be deleted after 10 minutes of inactivity.

51:22.000 --> 51:35.000
So that's important. This is just a demonstration area, so it's not for long-term storage of some sort of data science analysis, but it gives you a flavor of what the Jupyter Hub will be doing if you were to install this for your own sake.

51:35.000 --> 51:42.000
Now you can actually run this Jupyter Python 3 notebook, and you can actually see the fun results that come out from this.

51:42.000 --> 51:52.000
So we see some plots here, and everything works just like you expected to when you're running the Jupyter server locally, which is how all the videos I'm doing in this lesson are.

51:52.000 --> 52:00.000
Separating the server from the notebook aspect is that you can do something like this in the future, have the server being hosted on some server somewhere,

52:00.000 --> 52:07.000
and being able to access it just through the browser, and having the same exact functionality that I've been showing you for the entire course so far.

52:07.000 --> 52:14.000
And one last thing just to show you how fun this is, let's navigate back to our initials-try.jupyter thing.

52:14.000 --> 52:24.000
There's a couple other things you can do besides notebooks. This is true for the local server as well, but just to give you a sense of this, you can add a new folder, which is kind of unexciting.

52:24.000 --> 52:34.000
You just have a new unentitled folder here. Then you can do this new text file. So if you click text file, instead of starting a new notebook, you're starting a new file.

52:34.000 --> 52:40.000
And this is a lightweight in-browser text editor that has various options. You can choose what kind of key mapping you'd like.

52:40.000 --> 52:47.000
So I prefer sublime text these days as ways of interfacing with your text editor.

52:47.000 --> 52:58.000
So from here, you can do your standard Python, and you can both create and edit Python scripts or any kind of text file that you want to that's located on the server.

52:58.000 --> 53:05.000
And of course, renaming the file is as simple as clicking this top thing, calling it startup.py, for example.

53:05.000 --> 53:14.000
And once you do that, syntax highlighting gets turned on. You can save this and rename it, and then navigate back to the main server page.

53:14.000 --> 53:19.000
And the last thing to show you is that you can also start a terminal.

53:19.000 --> 53:25.000
And here you actually have the terminal for your tri-Jupiter. And this is the same case for if you're running this on a server.

53:25.000 --> 53:31.000
So you can actually have access to the terminal with all the functionality of a standard bash terminal there.

53:31.000 --> 53:36.000
So we see the startup thing we can copy that startup.py folder and call it something else.

53:36.000 --> 53:39.000
And going back, we should be able to see this.

53:39.000 --> 53:51.000
It's a very cool thing, and it will definitely be the way of the future if you have data science teams working and needing access to a single server somewhere that has the data in some database, for example.

53:51.000 --> 54:02.000
So Jupyter Hub, it will be the future. It is bleeding edge. So try it out. It should be pretty usable, but the exact instructions will be different from what I would say today.

54:02.000 --> 54:09.000
In this lesson, we'll be taking a look at organizing the overall structure for a data science team to be working on their various projects.

54:09.000 --> 54:14.000
So in this notebook, I'm going to use the slideshow button that we installed in a different video.

54:14.000 --> 54:18.000
And I make this full screen by clicking shift command F.

54:18.000 --> 54:24.000
The initial topic is questions to ask to organize the workflow of a data science team.

54:24.000 --> 54:29.000
So the first question is how many data scientists will be working on a single problem?

54:29.000 --> 54:36.000
And the high level view of this is to basically break this up into thinking about this in terms of Git repositories.

54:36.000 --> 54:45.000
What I mean by that is, if you have different data sources and different problems working in a single company, let's say, then you should definitely use different Git repositories.

54:45.000 --> 54:54.000
If you have fewer than 10 data scientists working on the same data, but working on different problems, it also probably makes sense to keep everything in a single Git repository, although it doesn't have to.

54:54.000 --> 54:57.000
If you have different concerns, feel free to break that up.

54:57.000 --> 55:08.000
And if you have more than 10 data scientists and they're working on the same data, but they're working on different problems, fundamentally addressing different data science issues, then I recommend using different Git repositories.

55:08.000 --> 55:13.000
And all of my recommendations will be within context of a single Git repository.

55:13.000 --> 55:17.000
The second main question to be asked is where is the data actually hosted?

55:17.000 --> 55:26.000
If it's small enough data to be loaded onto a data scientist's personal laptop, then it's very simple to actually just use the data on the laptop locally.

55:26.000 --> 55:36.000
So I would recommend just running the Jupyter Notebook as I'm doing in most of the videos for this course, where you just open up a terminal on your local laptop or local desktop and just run Jupyter Notebook.

55:36.000 --> 55:39.000
However, many data science projects actually use big data.

55:39.000 --> 55:42.000
They access the data on some other server or something like this.

55:42.000 --> 55:45.000
And in this case, you have a couple of options.

55:45.000 --> 56:00.000
The obvious one is to say, if you can access this server data via SSH and you can actually do work in a server, then you can actually run a Jupyter server on that server and you can SSH tunnel and forward your connection to that server.

56:00.000 --> 56:05.000
That way, both the data and the Jupyter server are on the same machine.

56:05.000 --> 56:08.000
Another option is to consider using a thing called Jupyter Hub.

56:08.000 --> 56:13.000
The Jupyter Hub would have to be installed on the server where the data is actually being held.

56:13.000 --> 56:16.000
And if I click this link, you go to this GitHub page here.

56:16.000 --> 56:21.000
So it can be found at github.com slash Jupyter slash Jupyter Hub.

56:21.000 --> 56:24.000
And you can see it's a bit more work than we can go into.

56:24.000 --> 56:26.000
It's a bit outside the scope of this class.

56:26.000 --> 56:30.000
But Jupyter Hub is a multi-user server for Jupyter Notebooks.

56:30.000 --> 56:37.000
And there's actually some really nice documentation to explain how this can be set up on a server or some AWS instance, for example.

56:37.000 --> 56:41.000
There's lots of installation instructions and things to work on here.

56:41.000 --> 56:43.000
So those are the main questions to be asking.

56:43.000 --> 56:46.000
At what level do you set the Git repository?

56:46.000 --> 56:48.000
And where are you going to be running this server?

56:48.000 --> 56:50.000
Are you going to be running it on a server somewhere?

56:50.000 --> 56:54.000
Or will you be running it locally on your local laptop or something else?

56:54.000 --> 57:04.000
Once you have those two questions settled, then the mechanics of actually how do you work on a Jupyter Notebook in a single repository or what we'll deal with next?

57:07.000 --> 57:12.000
In this lesson, we'll be organizing our work into two different types of notebooks.

57:12.000 --> 57:16.000
Conceptually, there are two types of notebooks I'd like to introduce.

57:16.000 --> 57:20.000
One called a laboratory notebook and one called a deliverable notebook.

57:20.000 --> 57:28.000
The difference here, a laboratory notebook is in the same style as lab notebooks that are actually in science labs throughout the world.

57:28.000 --> 57:34.000
And by that, a lab notebook keeps a historical record of the analysis that's been explored.

57:34.000 --> 57:43.000
So each day, a person goes to a lab bench, writes down the date at the top of the page, writes down what happened in lab that day for that particular experiment.

57:43.000 --> 57:46.000
And this record just continually gets amended to.

57:46.000 --> 57:55.000
It is also meant to be a place where there's development or scratch ideas or initial analyses, and it's very much not a polished piece of work.

57:55.000 --> 57:59.000
It is meant for record keeping of scratch pad type nature.

57:59.000 --> 58:02.000
And each notebook is controlled by a single data scientist.

58:02.000 --> 58:12.000
And by this, I'm talking about a Jupyter notebook where it is a single person single data scientists record of what they were doing that day and it is not shared by anyone else.

58:12.000 --> 58:20.000
Now, it's not secret people can look at it and you can upload it as well, but it's not meant to be viewed by other people necessarily.

58:20.000 --> 58:22.000
A few more final points on lab notebooks.

58:22.000 --> 58:27.000
Split the notebook when it gets too long and too long is just sort of a personal preference.

58:27.000 --> 58:35.000
As you start scrolling down the page as a point when a lab notebook or any notebook gets to the point where, okay, this is too much of a document to look at at one time.

58:35.000 --> 58:36.000
So then split it.

58:36.000 --> 58:37.000
There's no cost in splitting it.

58:37.000 --> 58:40.000
And you can think of this as just turning the page in a lab notebook.

58:40.000 --> 58:45.000
And finally, if you're working on a single day, you can actually split notebooks into different topics.

58:45.000 --> 58:49.000
So for the same day, you can actually have two different or more notebooks.

58:49.000 --> 58:52.000
And if you're splitting by topic, that makes sense as well.

58:52.000 --> 58:56.000
On contrast to a lab notebook, there's another idea of a deliverable notebook.

58:56.000 --> 59:02.000
As I work as a consultant, most of my work is actually going to be delivered either to a project manager or to a client.

59:02.000 --> 59:10.000
And these notebooks are different from lab notebooks in the sense that these will be delivered to someone to consume besides myself.

59:10.000 --> 59:15.000
Now candidates for deliverable notebooks can be any notebook that will be referenced in the future.

59:15.000 --> 59:21.000
By this, I mean, if I expect someone else to also use the same data cleaning notebook, for example,

59:21.000 --> 59:29.000
so I might have a notebook that explains how I took raw data and transformed it into the clean data that I use for the rest of the analysis.

59:29.000 --> 59:35.000
And I might provide a single link to a deliverable notebook, which is simply the data cleaning of the raw data.

59:35.000 --> 59:42.000
And in that notebook, I'll have things like what the actual transformations were, but also reasoning behind it and some documentation around it.

59:42.000 --> 59:45.000
So this is for anyone who wants to know how is this data actually cleaned?

59:45.000 --> 59:47.000
There's a single spot for it to look at.

59:47.000 --> 59:57.000
And obviously, of course, the final fully polished and final analysis of a data science piece of work will also be considered a deliverable notebook.

59:57.000 --> 01:00:02.000
I also recommend that deliverable notebooks should be peer reviewed via pull requests,

01:00:02.000 --> 01:00:06.000
which means other members will actually review the notebook before it's accepted.

01:00:06.000 --> 01:00:10.000
Other members can be other data scientists or it can be a manager or something else.

01:00:10.000 --> 01:00:13.000
And these notebooks are controlled by the whole data science team.

01:00:13.000 --> 01:00:18.000
If we think about these notebooks as living in a certain repository, for example,

01:00:18.000 --> 01:00:21.000
then the whole data science team will have these deliverable notebooks,

01:00:21.000 --> 01:00:26.000
which are in the same topic scope as the problem that they're all together trying to solve.

01:00:26.000 --> 01:00:33.000
So how do we organize the directories so that the lab notebooks and deliverable notebooks all are in their proper place?

01:00:33.000 --> 01:00:38.000
So these are the minimum directories, and I think it can be expanded by a few or taken away by a few.

01:00:38.000 --> 01:00:45.000
So I have listed here the directories I think belong at the top level of a data science git repository.

01:00:45.000 --> 01:00:47.000
The first one is data. This is optional.

01:00:47.000 --> 01:00:53.000
If you have very small data and you want to have it locally, it's possible to include it in a git repository.

01:00:53.000 --> 01:00:57.000
Generally, though, data science data is actually backed up outside of version control.

01:00:57.000 --> 01:00:59.000
It's in a different environment.

01:00:59.000 --> 01:01:02.000
So this is definitely an optional directory to have.

01:01:02.000 --> 01:01:04.000
The second one is the deliver directory.

01:01:04.000 --> 01:01:08.000
This is where the final polished notebooks for consumption.

01:01:08.000 --> 01:01:16.000
If a new data scientist is coming onto the project, they will look in the deliver directory to see what has been done before.

01:01:16.000 --> 01:01:22.000
In the develop directory, we store the lab notebooks, and I will explain the naming convention in a further video,

01:01:22.000 --> 01:01:28.000
but this will say all the scratch work that has been done by each of the data scientists working on this problem.

01:01:28.000 --> 01:01:36.000
The directory called figures will contain the figures that have been the output from both to develop and the deliver notebooks.

01:01:36.000 --> 01:01:39.000
I will be expressing a bit more on that in the future.

01:01:39.000 --> 01:01:45.000
And finally, a source directory where as you come up with various scripts or modules or anything else that needs to be,

01:01:45.000 --> 01:01:50.000
that's actual computer code that doesn't belong in a notebook directory, goes in a source directory.

01:01:50.000 --> 01:01:53.000
Again, you can add to this or you can modify this as you want to,

01:01:53.000 --> 01:02:00.000
but I think this is a good starting structure to work from and modify it as your needs evolve.

01:02:00.000 --> 01:02:06.000
In this video, I'll be telling you about my recommended convention for naming lab notebooks.

01:02:06.000 --> 01:02:10.000
So naming a lab notebook can be a more difficult problem than you might expect,

01:02:10.000 --> 01:02:14.000
especially if there's many different data scientists working on a similar problem.

01:02:14.000 --> 01:02:17.000
So to help with that, the following convention is what I recommend.

01:02:17.000 --> 01:02:20.000
You can obviously change this to fit your own needs.

01:02:20.000 --> 01:02:26.000
I recommend prepending each file name with the current date that you started the work on that notebook.

01:02:26.000 --> 01:02:31.000
So in this case, it was started 2015 dash 11 dash 21.

01:02:31.000 --> 01:02:36.000
I also recommend it in that format where it's the year dash the two digit month,

01:02:36.000 --> 01:02:43.000
meaning if it's three, it'd be dash zero three dash the two digit day like the month in zero four and so on.

01:02:43.000 --> 01:02:51.000
This is called an ISO 8601 formatted date, and it just helps with keeping everything so that it's sortable in a nice way.

01:02:51.000 --> 01:02:57.000
So the initial part of the name is the date that you started working on that particular notebook.

01:02:57.000 --> 01:03:01.000
The second piece immediately after that is the data scientists initials.

01:03:01.000 --> 01:03:03.000
So in my case, my initials are JBW.

01:03:03.000 --> 01:03:09.000
So I put dash after the date my initials, or you can put it if you have a data scientists with the same initials,

01:03:09.000 --> 01:03:12.000
you can just put some unique signifier that's the same every time.

01:03:12.000 --> 01:03:16.000
So that if you want to look at a directory that has many different data scientists notebooks,

01:03:16.000 --> 01:03:21.000
you can do an LS for that person's initials and find their notebooks.

01:03:21.000 --> 01:03:27.000
And finally, I recommend putting a two to forward description that describes what goes in that notebook.

01:03:27.000 --> 01:03:32.000
So in this case, Cole predict RF for random forest regression.

01:03:32.000 --> 01:03:36.000
So looking through this later on, I can think back, okay, what was I doing two months ago,

01:03:36.000 --> 01:03:38.000
something with random forest, and it was a regression.

01:03:38.000 --> 01:03:44.000
And on a classifier, seeing this in the title helps pick this out.

01:03:44.000 --> 01:03:47.000
In this video, we'll be talking about version control.

01:03:47.000 --> 01:03:53.000
One of the key questions you have when dealing with a data science team is how do you peer review code?

01:03:53.000 --> 01:03:57.000
How do you store analysis in version control like get?

01:03:57.000 --> 01:03:59.000
And I'm going to assume a number of further constraints.

01:03:59.000 --> 01:04:03.000
And I think this is probably the most restrictive constraints I can think of.

01:04:03.000 --> 01:04:05.000
This might not apply to you.

01:04:05.000 --> 01:04:10.000
But I think if it does apply to you, I have reasonable work rounds for each of the possible concerns.

01:04:10.000 --> 01:04:15.000
For example, imagine you have a project manager who would like to see the notebooks you're working on,

01:04:15.000 --> 01:04:18.000
but they don't want to install Python or I Python or anything like this,

01:04:18.000 --> 01:04:22.000
or consider that you might not be using GitHub for whatever reason.

01:04:22.000 --> 01:04:27.000
And some of the nice tools that GitHub has for showing diffs aren't available to you.

01:04:27.000 --> 01:04:33.000
Or if you would want to review the Python code itself and don't want to have to look at it in a notebook environment.

01:04:33.000 --> 01:04:40.000
How do I recommend dealing with these kinds of constraints while also maintaining a peer review of the code stored in the version control?

01:04:40.000 --> 01:04:46.000
The standard practice for my answer is that each data scientist who's working on the same problem in the same repo

01:04:46.000 --> 01:04:48.000
should have their own development branch.

01:04:48.000 --> 01:04:52.000
And each day or even more frequently than each day,

01:04:52.000 --> 01:04:57.000
but at minimum work is saved and pushed to the dev branch that they have daily,

01:04:57.000 --> 01:05:02.000
which means that anyone can then check out another data scientist development branch.

01:05:02.000 --> 01:05:05.000
When ready to merge to master, you have to do a pull request.

01:05:05.000 --> 01:05:14.000
So a data scientist says, OK, I think the deliverable notebooks as well as my laboratory notebooks are ready to be reviewed and pulled into master.

01:05:14.000 --> 01:05:17.000
Now the question of what exactly to commit.

01:05:17.000 --> 01:05:24.000
This is a question that people who come from a more software engineering background might start to recoil at my suggestions here.

01:05:24.000 --> 01:05:29.000
I say this after a lot of thought and there might be a better way of doing it, but this is the best way that I can come up with.

01:05:29.000 --> 01:05:40.000
So I recommend committing the .ipynb files, which are the notebook files, the .py and the .html of all notebooks, both develop and deliver.

01:05:40.000 --> 01:05:44.000
And I'll also say any of the figures that are saved should also be committed.

01:05:44.000 --> 01:05:48.000
Now, when I say the .py and the .html, what am I referring to?

01:05:48.000 --> 01:05:51.000
So I'll go to an open notebook right now.

01:05:51.000 --> 01:05:55.000
This is a notebook for making a prediction about call production.

01:05:55.000 --> 01:05:58.000
And this is in the develop folder of a certain directory.

01:05:58.000 --> 01:06:05.000
And I have this notebook that's currently running and you can tell it's running by the green symbol here and the words running green all the way to the right.

01:06:05.000 --> 01:06:14.000
So let's go to this running notebook and actually save it, save in checkpoint and download as a Python file.

01:06:14.000 --> 01:06:24.000
Let's download it to the same directory of develop, save that, and let's download this as an .html file and save it in the same spot.

01:06:24.000 --> 01:06:31.000
So if we take a look at what this is, it has taken all of the Python code and none of the output,

01:06:31.000 --> 01:06:35.000
but it's shipped out everything else in this file that's not Python code.

01:06:35.000 --> 01:06:38.000
And so you see this input three, input four, and so on.

01:06:38.000 --> 01:06:44.000
This is delineating the cells in the notebook, but everything you see here is actually Python code.

01:06:44.000 --> 01:06:49.000
So this can actually run as a .py or you can run it as Python, this file name.

01:06:49.000 --> 01:06:56.000
And this .html file, if we open up this file, we actually see the HTML representation of the notebook.

01:06:56.000 --> 01:06:58.000
So this is not executable.

01:06:58.000 --> 01:07:00.000
This is just a .html file.

01:07:00.000 --> 01:07:06.000
And so this can be copied into an email and read by anyone who opens this with a web browser.

01:07:06.000 --> 01:07:10.000
You don't need to run Python or IPython to actually see the output here.

01:07:10.000 --> 01:07:14.000
Again, the limitation, though, is you cannot actually edit this code and make a new plot,

01:07:14.000 --> 01:07:18.000
but this is great for being able to share a particular notebook.

01:07:18.000 --> 01:07:24.000
So I recommend saving both of those file types to your Git repository.

01:07:24.000 --> 01:07:28.000
And of course, all of the figures as well if you create separate figures.

01:07:28.000 --> 01:07:36.000
The reasoning behind that is that the .py files allows a person to make easy changes to the actual Python code itself,

01:07:36.000 --> 01:07:38.000
as well as to track those changes.

01:07:38.000 --> 01:07:44.000
The .html file allows a person to see the fully rendered notebook without having to run a notebook themselves.

01:07:44.000 --> 01:07:48.000
So the benefits of structuring your repository this way are several fold.

01:07:48.000 --> 01:07:52.000
First of all, you have a complete record of the analysis that includes dead ends.

01:07:52.000 --> 01:07:57.000
So if one day you worked down a single hypothesis and turned out that it wasn't very useful,

01:07:57.000 --> 01:08:00.000
that is still saved in the lab notebook directory.

01:08:00.000 --> 01:08:05.000
It also allows for easy peer review of the analysis and of the dead ends.

01:08:05.000 --> 01:08:11.000
If in the future, a different team member has an idea to try to do a random forest regression on the data,

01:08:11.000 --> 01:08:14.000
they can actually see if someone else has done the same type of analysis,

01:08:14.000 --> 01:08:17.000
and if so, what led to a dead end, for example.

01:08:17.000 --> 01:08:22.000
And finally, project managers can easily see and read the analysis with GitHub

01:08:22.000 --> 01:08:25.000
because GitHub itself renders IP, UI, and Bs natively.

01:08:25.000 --> 01:08:29.000
Or if you don't have GitHub access or not rendering it for whatever reason,

01:08:29.000 --> 01:08:35.000
if you save the .html files, anyone can actually see the rendered notebook without having to run any code themselves,

01:08:35.000 --> 01:08:38.000
or installing IPython or anything else.

01:08:38.000 --> 01:08:41.000
Some final organization thoughts of this whole structure.

01:08:41.000 --> 01:08:44.000
So organizing the workflow for teams is actually a difficult problem,

01:08:44.000 --> 01:08:48.000
and I think this is a very good framework for having some standards.

01:08:48.000 --> 01:08:51.000
And this bullet point about the wrong thing solves the problem.

01:08:51.000 --> 01:08:56.000
Often with version control, software engineering types think we need the source that's version control

01:08:56.000 --> 01:09:00.000
and we don't need to track the output, or that output is something that's blown away.

01:09:00.000 --> 01:09:04.000
In data science work, the output is often the thing we need to look at.

01:09:04.000 --> 01:09:07.000
For example, if there is a plot that shows some deviation,

01:09:07.000 --> 01:09:13.000
that plot is best viewed in the peer review process, actually in the notebook itself,

01:09:13.000 --> 01:09:19.000
or in an .html rendering of that notebook, because that gives rise to any sort of correction

01:09:19.000 --> 01:09:21.000
or reinterpretation that needs to happen.

01:09:21.000 --> 01:09:25.000
So the output actually is the thing that matters in a lot of data science work.

01:09:25.000 --> 01:09:28.000
So storing that in version control is actually the right thing to do,

01:09:28.000 --> 01:09:32.000
even though in typical practice it's the wrong to actually store the output.

01:09:32.000 --> 01:09:36.000
Finally, I am open to new ideas if you have a better way of solving these problems,

01:09:36.000 --> 01:09:40.000
or if your situation is completely different so that such that you will always be using GitHub,

01:09:40.000 --> 01:09:44.000
you never have to worry about seeing a rendered .html file.

01:09:44.000 --> 01:09:49.000
You can make these modifications by doing your own version of this kind of organization.

01:09:49.000 --> 01:09:54.000
So hopefully this gave you some structure to organize how a team of data scientists

01:09:54.000 --> 01:09:57.000
would work in a Git environment.

01:09:59.000 --> 01:10:03.000
In this lesson, we'll be getting some data that we can actually do some data science with.

01:10:03.000 --> 01:10:08.000
I recommend having a data folder in your projects directory that actually is at the same level

01:10:08.000 --> 01:10:12.000
as your deliver directory, your development directory, and your source directory.

01:10:12.000 --> 01:10:17.000
In my case, I have about 10 files in here that are coal data from the U.S. government.

01:10:17.000 --> 01:10:24.000
If you'd like to grab this same data set so you can follow along, go to www.eia.gov.

01:10:24.000 --> 01:10:28.000
This is the government's energy information administration website,

01:10:28.000 --> 01:10:34.000
and if you go to the data tab, you can scroll down to where it says production, give that a click.

01:10:34.000 --> 01:10:37.000
And there's lots of different data available here,

01:10:37.000 --> 01:10:43.000
but we're looking at the historical detailed coal production data available from 1983 to 2013.

01:10:43.000 --> 01:10:47.000
Select which year you'd like to do, and in case I picked 10 of them,

01:10:47.000 --> 01:10:50.000
click the arrow here and save it into that data directory.

01:10:50.000 --> 01:10:55.000
Once you do that, you'll then have the data that we'll need for this upcoming lessons.

01:10:57.000 --> 01:11:00.000
In this lesson, we're going to take our very first look at the data.

01:11:00.000 --> 01:11:03.000
We might even do some initial data cleaning.

01:11:03.000 --> 01:11:09.000
I'm currently in this directory where you can see we have data, deliver, development, and source directories.

01:11:09.000 --> 01:11:13.000
I'm going to start the Jupyter Notebook by, again, typing Jupyter Notebook.

01:11:13.000 --> 01:11:16.000
From here, we see the same directories I just saw in that directory.

01:11:16.000 --> 01:11:23.000
Let's open up the development list and start a new notebook by going over to new Python 2 notebook.

01:11:23.000 --> 01:11:26.000
From here, we see the familiar text box where you can type in code.

01:11:26.000 --> 01:11:29.000
In here, we see the code box is actually surrounded by green,

01:11:29.000 --> 01:11:33.000
which means as we type, it should be typing in text into that cell.

01:11:33.000 --> 01:11:38.000
We're going to need the pandas library, and we're going to import it as import pandas as pd.

01:11:38.000 --> 01:11:43.000
This can create alias for the pandas library to actually be called pd.

01:11:43.000 --> 01:11:48.000
This is a standard way of calling pandas, and I recommend you following the standards as often as possible.

01:11:48.000 --> 01:11:52.000
This lets you share your code with other people in the most seamless way possible.

01:11:52.000 --> 01:11:56.000
To run the cell, I can click the run cell button in the toolbar,

01:11:56.000 --> 01:12:03.000
or I can have done the shift enter technique, which, as you can see, increments which input number it is by one.

01:12:03.000 --> 01:12:10.000
The pandas version that I'm actually running is done by doing a print double underscore and then hitting the tab button.

01:12:10.000 --> 01:12:14.000
Hitting tab is a thing you should be thinking about doing quite often,

01:12:14.000 --> 01:12:17.000
because it often lets you make sure you don't have to type everything out.

01:12:17.000 --> 01:12:22.000
It's faster, but also make sure you are in the right vicinity of what you're hoping to do.

01:12:22.000 --> 01:12:25.000
There's a version, I'm going to hit return here, and then shift return,

01:12:25.000 --> 01:12:30.000
and it prints the pandas version that we're using, which is 0.17.0.

01:12:30.000 --> 01:12:34.000
From here, let's actually take a look at our very first data file.

01:12:34.000 --> 01:12:38.000
The way we can read this in, we happen to know, and here's an interesting side note,

01:12:38.000 --> 01:12:43.000
if you type ls and execute that, you actually see all the folders in the directory you're currently in.

01:12:43.000 --> 01:12:47.000
If you type ls up one, we see the parent directory,

01:12:47.000 --> 01:12:50.000
and if you'd like to look at what's in data,

01:12:50.000 --> 01:12:54.000
we see the files that we just downloaded in the previous video.

01:12:54.000 --> 01:12:59.000
Let's load in one of these Excel files and take a look at what's actually in them.

01:12:59.000 --> 01:13:05.000
I'm going to create a variable called df for data frame, and I'm going to df1 for the first one.

01:13:05.000 --> 01:13:09.000
I'm going to do pd.read, and I think it's going to be Excel,

01:13:09.000 --> 01:13:12.000
but I type the tab and I see an option pull up, and it is.

01:13:12.000 --> 01:13:16.000
It is pd.read underscore Excel, open parentheses.

01:13:16.000 --> 01:13:21.000
At this point, if you're not sure what a function does, there's a function called tooltip,

01:13:21.000 --> 01:13:24.000
which is generated by holding down shift and hitting tab once.

01:13:24.000 --> 01:13:30.000
Here it tells you the signature for this function, which has an input output, a sheet name, header, and so on.

01:13:30.000 --> 01:13:34.000
A lot of different options available for reading in Excel files.

01:13:34.000 --> 01:13:39.000
There's actually a longer version of this, where if you do shift tab tab in rapid succession,

01:13:39.000 --> 01:13:44.000
so it's a double tab, then you have the full doc string and the examples that go along with it.

01:13:44.000 --> 01:13:49.000
This is a very useful feature, so you can actually look up documentation on the fly, and it's very useful.

01:13:49.000 --> 01:13:53.000
In this case, we're going to try to load in the data from above.

01:13:53.000 --> 01:13:57.000
Again, tab completing commands will make your life much easier.

01:13:57.000 --> 01:14:03.000
As I start typing out this, I can hit tab and it actually produces again a list of possible data sources.

01:14:03.000 --> 01:14:05.000
Let's just see if this works.

01:14:05.000 --> 01:14:10.000
Head is a function on a data frame, and it lets you show the various options.

01:14:10.000 --> 01:14:12.000
We see that a number of things have happened here.

01:14:12.000 --> 01:14:17.000
First, we have the year, the MSHA ID, the mine name, the mine state.

01:14:17.000 --> 01:14:21.000
We actually see some of the data, and this is just the first few rows by doing head.

01:14:21.000 --> 01:14:26.000
I recommend doing head because it actually stores the full output of this.

01:14:26.000 --> 01:14:28.000
It's a separate thing that you can actually call.

01:14:28.000 --> 01:14:33.000
In future lessons, I'll explain exactly why using .head as best practices,

01:14:33.000 --> 01:14:39.000
but for now, let's just use .head to look into the contents of our pandas data frames.

01:14:39.000 --> 01:14:44.000
At this point, we've taken a first look at loading in some Excel data files,

01:14:44.000 --> 01:14:47.000
and we're going to start looking at this and playing around with it.

01:14:49.000 --> 01:14:54.000
In this lesson, we're going to take a look at how we can start to manipulate the data that we've read in

01:14:54.000 --> 01:14:56.000
in ways that are useful for analysis.

01:14:56.000 --> 01:15:01.000
Last time, we read in the CoalPublic2013 file and took a look at the header.

01:15:01.000 --> 01:15:04.000
The heading had an interesting, well, let's call it a problem.

01:15:04.000 --> 01:15:07.000
The historical Coal production data is the title here.

01:15:07.000 --> 01:15:12.000
There's a source function. There's also a bunch of nans, and all the columns are unnamed.

01:15:12.000 --> 01:15:20.000
This is most useful when this line, line 2, which is our row 2, is actually year MSHA ID, mine name.

01:15:20.000 --> 01:15:26.000
This is supposed to be the headers or the column names, and all the rest of it should be the actual rows of data.

01:15:26.000 --> 01:15:30.000
We're going to put the second row here up to the columns at the top.

01:15:30.000 --> 01:15:33.000
We'd also like to make this ID the index for the pandas data frame.

01:15:33.000 --> 01:15:38.000
We'll go into exactly why in the future, but for now, let's merge the reading in of the data frame

01:15:38.000 --> 01:15:42.000
with the printing out of what the head of that data frame looks like.

01:15:42.000 --> 01:15:48.000
We're going up here and clicking Edit, Merge Cell Below, because we've actually selected the above cell.

01:15:48.000 --> 01:15:50.000
So merge the cell below into one.

01:15:50.000 --> 01:15:58.000
So now that I execute this cell, we see that there is in one cell both reading the file and looking at the head of the file.

01:15:58.000 --> 01:16:02.000
Now, this is, again, wrong. We would like to remove this top part.

01:16:02.000 --> 01:16:07.000
So the way to remove this is we're actually going to use a thing called header and start giving it a number.

01:16:07.000 --> 01:16:12.000
Because if we look at this, we can see that it actually takes a header equals zero as the default value.

01:16:12.000 --> 01:16:16.000
So if we do header equals one, it actually deletes that top row.

01:16:16.000 --> 01:16:21.000
And so this is a way of telling the pandas that, hey, you don't have to modify that Excel file.

01:16:21.000 --> 01:16:25.000
You can just, when you read it in, know that there's two lines of header files.

01:16:25.000 --> 01:16:30.000
Now, there was two lines that had data in it, and there was a third NAN line that just, it knew it could not possibly be the header.

01:16:30.000 --> 01:16:31.000
So it removed that.

01:16:31.000 --> 01:16:35.000
So now the column names are these bolded ones are at the top.

01:16:35.000 --> 01:16:37.000
We're getting very close to what we actually want.

01:16:37.000 --> 01:16:40.000
Another thing we'd like to actually do is set the index.

01:16:40.000 --> 01:16:46.000
So we set the index by typing index and hitting tab because we think it's going to be something like set index or index set.

01:16:46.000 --> 01:16:50.000
And it's index columns equals, if this type, we like the name of it.

01:16:50.000 --> 01:16:54.000
So we would like to do the MSHA ID as the index column.

01:16:54.000 --> 01:17:04.000
And doing that, we see that the MSHA ID is indeed the index for this data and the columns are all appropriately named.

01:17:04.000 --> 01:17:07.000
This is one way to interact with the pandas library.

01:17:07.000 --> 01:17:12.000
But it actually applies to all Python libraries that have any sort of documentation strings.

01:17:12.000 --> 01:17:18.000
Just to give you an example of that, I'm going to save this currently and just show you example function, right?

01:17:18.000 --> 01:17:20.000
We define a function by typing def.

01:17:20.000 --> 01:17:22.000
We'll do it test function.

01:17:22.000 --> 01:17:31.000
Let's say it takes two values first equals five and second equals 10 and it will return first plus second.

01:17:31.000 --> 01:17:35.000
Let's give it a doc string and we execute that line.

01:17:35.000 --> 01:17:44.000
If we start typing test underscore f and then hit tab, it will automatically complete that because we have a defined function here called def function.

01:17:44.000 --> 01:17:47.000
We do the initial parentheses and hit shift tab.

01:17:47.000 --> 01:17:50.000
You actually see the doc string that we wrote just above.

01:17:50.000 --> 01:17:56.000
This is an example and it has the signature of it to the first equals five second equals 10.

01:17:56.000 --> 01:18:02.000
If you want to redefine what actually we give it, we can say first equals three and the test function gives us 13, which is what we'd expect.

01:18:02.000 --> 01:18:16.000
So that's just a fun side note on how the interaction with Jupiter notebook lets you look into the doc strings of functions that you define yourself as well as any of the libraries that you'll be using your data science day to day.

01:18:16.000 --> 01:18:21.000
In this lesson, we'll be making a new GitHub repository for a new data science project.

01:18:21.000 --> 01:18:28.000
So let's go over to GitHub and from GitHub, if you go all the way over to the right, you can create new repository.

01:18:28.000 --> 01:18:31.000
Give the repository some name that you think makes sense.

01:18:31.000 --> 01:18:33.000
So we'll do some coal exploration.

01:18:33.000 --> 01:18:37.000
So let's make a coal exploration repository name.

01:18:37.000 --> 01:18:39.000
You can give it a description if you'd like to.

01:18:39.000 --> 01:18:42.000
You don't need to decide whether it will be public or private.

01:18:42.000 --> 01:18:45.000
I'll let it be public so that you can see this as well.

01:18:45.000 --> 01:18:48.000
And generally, I like to initialize the repository with a read me.

01:18:48.000 --> 01:18:52.000
It get ignore file that's Python because I use a lot of Python code.

01:18:52.000 --> 01:18:55.000
And I add an MIT license.

01:18:55.000 --> 01:18:58.000
After doing all this, click create repository.

01:18:58.000 --> 01:19:04.000
Once you click create repository, you can go over to this place here where you can click SSH.

01:19:04.000 --> 01:19:07.000
You can have HTTPS or SSH.

01:19:07.000 --> 01:19:09.000
I just use SSH most of the time.

01:19:09.000 --> 01:19:11.000
Clicking once in here highlights everything.

01:19:11.000 --> 01:19:13.000
Command C will copy this.

01:19:13.000 --> 01:19:21.000
And going back into a terminal, type git clone and then command V to paste the required link.

01:19:21.000 --> 01:19:22.000
Hit return.

01:19:22.000 --> 01:19:27.000
And you will now clone the GitHub repository to your local machine.

01:19:27.000 --> 01:19:32.000
And from here, we see a new coal exploration folder being created.

01:19:32.000 --> 01:19:42.000
And if we CD into coal exploration, we see that it has a license and a read me file that we've made previously.

01:19:42.000 --> 01:19:45.000
In this lesson, we'll be taking our GitHub repository that we've just started.

01:19:45.000 --> 01:19:47.000
We'll first look at the data.

01:19:47.000 --> 01:19:52.000
So the directory as we last left, it has two files in it, a license and a read me file.

01:19:52.000 --> 01:19:55.000
We're going to create some extra directories and some structure around here.

01:19:55.000 --> 01:19:58.000
And I'll go through the reasoning behind this in other videos.

01:19:58.000 --> 01:20:05.000
But we're going to create using the make directory command, a data directory, a deliver directory,

01:20:05.000 --> 01:20:12.000
which is going to house the final deliverable important Jupyter notebooks, a develop directory,

01:20:12.000 --> 01:20:15.000
which is where we're going to mostly do our development place,

01:20:15.000 --> 01:20:19.000
place to put our source code if we have any scripts that we'll end up using.

01:20:19.000 --> 01:20:23.000
So separate from ipython notebooks, usually Python files or other kinds of scripts,

01:20:23.000 --> 01:20:28.000
we'll go in a source directory and a figures directory running that command.

01:20:28.000 --> 01:20:33.000
The folder structure that we have now has a data deliver develop figures and source directories.

01:20:33.000 --> 01:20:36.000
So let's actually get that data and put it into this directory.

01:20:36.000 --> 01:20:37.000
You might have already downloaded it.

01:20:37.000 --> 01:20:42.000
If not, again, the way to get this is to go to eia.gov slash coal.

01:20:42.000 --> 01:20:46.000
Go to the data tab down to production.

01:20:46.000 --> 01:20:49.000
And we go to the historical detailed coal production data.

01:20:49.000 --> 01:20:52.000
And let's just use the year 2013 for now.

01:20:52.000 --> 01:20:57.000
We're going to go into this coal exploration, navigate to the data folder and save.

01:20:57.000 --> 01:21:00.000
That is done downloading.

01:21:00.000 --> 01:21:04.000
You can see it in this folder as coal public 2013.

01:21:04.000 --> 01:21:05.000
Great.

01:21:05.000 --> 01:21:06.000
So let's take a look at this.

01:21:06.000 --> 01:21:08.000
We'll open up a Jupyter notebook and take a look.

01:21:08.000 --> 01:21:12.000
So from this top level directory, I will start Jupyter notebook.

01:21:12.000 --> 01:21:15.000
You can now close this download file.

01:21:15.000 --> 01:21:19.000
And you can navigate this structure similarly to the terminal itself.

01:21:19.000 --> 01:21:22.000
So you can actually click data and you see the coal public data that we had before.

01:21:22.000 --> 01:21:29.000
We can navigate back and let's go into the develop and start a new Python to notebook.

01:21:29.000 --> 01:21:31.000
It starts off being called untitled.

01:21:31.000 --> 01:21:33.000
And that is a not very helpful name.

01:21:33.000 --> 01:21:38.000
So I recommend using the date in ISO 8601 format.

01:21:38.000 --> 01:21:41.000
And the reason for that is that it helps with sorting.

01:21:41.000 --> 01:21:46.000
But basically it goes year dash month dash dates today is the 21st.

01:21:46.000 --> 01:21:55.000
After you do the date, I recommend, especially if you're working in teams to have your initials or some other identifier that creates it so that people know it's your notebook.

01:21:55.000 --> 01:21:57.000
And so I'm going to type my initials here.

01:21:57.000 --> 01:22:03.000
And then I recommend having a couple words that describe what you think you're doing in this notebook.

01:22:03.000 --> 01:22:06.000
So I think I'll just say a first look.

01:22:06.000 --> 01:22:12.000
So now I've renamed that notebook and it helpfully tells us when it lasted the last checkpoint.

01:22:12.000 --> 01:22:15.000
This means when it's been saved auto saves every once in a while.

01:22:15.000 --> 01:22:20.000
You can also click this button, but you just see that the last checkpoint saved.

01:22:20.000 --> 01:22:22.000
And you can also do command s, which is how I normally do it.

01:22:22.000 --> 01:22:27.000
So this means that it's keeping auto saved versions of this as we go along.

01:22:27.000 --> 01:22:28.000
All right.

01:22:28.000 --> 01:22:31.000
So there's a number of libraries that we'd like to import.

01:22:31.000 --> 01:22:35.000
And I import these almost every time and it starts off with matplotlib inline.

01:22:35.000 --> 01:22:39.000
So this percent sign at the top of the line means it's a magic import.

01:22:39.000 --> 01:22:48.000
And we also have to import matplotlib like so importing it as PLT is the standard best practice for doing that next we import pandas.

01:22:48.000 --> 01:22:52.000
And we should also import seaborne, which is a package that wraps matplotlib.

01:22:52.000 --> 01:22:56.000
Interestingly, you're supposed to import seaborne as SNS.

01:22:56.000 --> 01:23:00.000
I don't know exactly why, but importing it as SNS is the standard way of doing it.

01:23:00.000 --> 01:23:04.000
Also, if you do SNS dot set, it actually sets a number of the default parameters for matplotlib.

01:23:04.000 --> 01:23:07.000
So it already looks nicer if you just use it from there.

01:23:07.000 --> 01:23:09.000
So let's go ahead and start with that.

01:23:09.000 --> 01:23:14.000
And now let's read into a data frame, the data file that we just downloaded.

01:23:14.000 --> 01:23:21.000
So we say df equals pandas library dot read hit tab to see the options go to Excel.

01:23:21.000 --> 01:23:26.000
And we navigate to the directory by going up one directory by doing dot dot slash.

01:23:26.000 --> 01:23:30.000
If we hit tab, we also get the possible navigation options.

01:23:30.000 --> 01:23:34.000
It's in the data and if you tap again, it will have complete to say cold public 2013.

01:23:34.000 --> 01:23:40.000
If we actually execute that and take a look at the head, we notice that we again have this unnamed part at the top.

01:23:40.000 --> 01:23:43.000
So we actually wouldn't like to remember that it has a header.

01:23:43.000 --> 01:23:48.000
Set the headers equal to two and that correctly gets the column types labeled in there.

01:23:48.000 --> 01:23:52.000
And we want to set the index to the MSH ID.

01:23:52.000 --> 01:24:01.000
So if it's annoying, you set index by doing index something hit tab and its index column equals MSHA space ID.

01:24:01.000 --> 01:24:07.000
Excelling those two cells, you have the ID of the mine setting as the index of this data frame

01:24:07.000 --> 01:24:11.000
and all the data in here correctly parsed from that Excel file.

01:24:11.000 --> 01:24:20.000
Okay, so I'm going to stop it here and we'll begin to actually start to plot this and take a look at what this data actually looks like.

01:24:20.000 --> 01:24:25.000
In this lesson, we'll take a look at the data and do some data cleaning and maybe do some visualizations.

01:24:25.000 --> 01:24:32.000
Let's go back into this notebook and rerun the first cell here, load everything in that warning that we've seen before.

01:24:32.000 --> 01:24:36.000
Load in the data and take a look at the data dot head.

01:24:36.000 --> 01:24:38.000
So everything here looks normal.

01:24:38.000 --> 01:24:44.000
And the day to day data science work, you often take a look at what's in each of these columns.

01:24:44.000 --> 01:24:50.000
So we can just do a very quick look, for example, at a data frame and take a look at the company type.

01:24:50.000 --> 01:24:57.000
Now, if we have thousands of rows, we don't want to look at all of them, but we do want to look at the unique ones.

01:24:57.000 --> 01:25:02.000
In here, we see that there's three types of unique companies according into our file right now.

01:25:02.000 --> 01:25:06.000
We have what I think the word is supposed to be independent producer operator.

01:25:06.000 --> 01:25:10.000
The next one is operating subsidiary and contractor.

01:25:10.000 --> 01:25:16.000
Now, obviously, this first piece of information is that the data has some data quality issues.

01:25:16.000 --> 01:25:19.000
So let's go ahead and actually make a correction here for this data.

01:25:19.000 --> 01:25:26.000
We'd actually like to replace all of the independent producer operators with independent producer operators.

01:25:26.000 --> 01:25:33.000
So the way to do this in place is to actually do a company type to replace it.

01:25:33.000 --> 01:25:38.000
And if you don't remember the syntax for replacing, if you do a shift tab, you can actually see the tool tip come up.

01:25:38.000 --> 01:25:40.000
There's two ways to do this.

01:25:40.000 --> 01:25:44.000
You can say to replace equals x, the value in place, everything else.

01:25:44.000 --> 01:25:47.000
And we can also do it by giving it a dictionary.

01:25:47.000 --> 01:25:49.000
I'm actually going to do it the standard way.

01:25:49.000 --> 01:25:56.000
So to replace should be equal to, we'll just copy the words from above.

01:25:56.000 --> 01:26:02.000
And the value I would like to replace it with is going to be the independent producer operator.

01:26:02.000 --> 01:26:05.000
This cell is already becoming wider than the screen.

01:26:05.000 --> 01:26:09.000
So I'm going to actually hit return here so that it's lined up with the beginning of this.

01:26:09.000 --> 01:26:12.000
So you can say one later on can actually read this in a much nicer way.

01:26:12.000 --> 01:26:17.000
Suppose a DF company replace this thing and then do head on this.

01:26:17.000 --> 01:26:24.000
It should show us that it is indeed replacing the independent producer, but it hasn't replaced it in the actual data frame itself.

01:26:24.000 --> 01:26:30.000
To do that, we have to add an extra command here, which is in place equals true.

01:26:30.000 --> 01:26:34.000
One extra interesting, let's call it a quirk of the Jupiter system.

01:26:34.000 --> 01:26:40.000
If you're in line with the beginning of this command, if you do a tool tip by doing a shift tab, it appears.

01:26:40.000 --> 01:26:45.000
If you're not on that first line and it's broken up across multiple lines, then doing the shift tab in the middle here will not work.

01:26:45.000 --> 01:26:48.000
If you're thinking, is it in place one word or is it in underscore place?

01:26:48.000 --> 01:26:50.000
You have to do it up here to get the tool tip help.

01:26:50.000 --> 01:26:52.000
So it's in place one word.

01:26:52.000 --> 01:26:53.000
So I typed it down here.

01:26:53.000 --> 01:26:59.000
This will in place change the DF company type to be independent.

01:26:59.000 --> 01:27:02.000
So this has now been replaced in place.

01:27:02.000 --> 01:27:10.000
Now we also see that even though I could actually hit tab, which is a very useful thing to be able to call the column heading by just typing the beginning of it.

01:27:11.000 --> 01:27:17.000
Having these spaces is going to just make life a little bit more difficult than it should be.

01:27:17.000 --> 01:27:24.000
So what I'd like to do is actually go through all of the columns in this data frame and replace every single space with an underscore.

01:27:24.000 --> 01:27:27.000
So it's still readable, but I'd just like to actually do that.

01:27:27.000 --> 01:27:30.000
So to do that, we actually would like to do a name of the columns.

01:27:30.000 --> 01:27:35.000
So we DF dot rename index columns equals and keyword arguments.

01:27:35.000 --> 01:27:37.000
So you can say columns equals.

01:27:37.000 --> 01:27:41.000
Now this is a really fun trick because you actually pass a Lambda function.

01:27:41.000 --> 01:27:46.000
Lambda function says for everything in that columns, I'd like to do X dot replace.

01:27:46.000 --> 01:27:53.000
So similar syntax as above, but I replace all of the spaces with underscores.

01:27:53.000 --> 01:28:00.000
So the thing that's being quoted is the thing that's being found single space replacing that space with is the underscore.

01:28:00.000 --> 01:28:06.000
So I'd like to rename the data frame where every column space will be turned into an underscore.

01:28:06.000 --> 01:28:10.000
And of course, I would also like to actually make this happen to the data frame in place.

01:28:10.000 --> 01:28:16.000
So I say in place equals true now to check if that actually worked as we hope we can look at the DF dot head.

01:28:16.000 --> 01:28:21.000
And we see that underscore name mine underscore state mine underscore county and so on.

01:28:21.000 --> 01:28:27.000
So this with one line and very quickly typing it out replaced all of the spaces here with underscores.

01:28:27.000 --> 01:28:31.000
And this will just make life much easier as we go on from here.

01:28:31.000 --> 01:28:33.000
Let's also take a look at how big is this data frame.

01:28:33.000 --> 01:28:35.000
We have 1400 data points.

01:28:35.000 --> 01:28:38.000
And let's take a first look at just what's in here.

01:28:38.000 --> 01:28:45.000
So we just read this off as my name all the way through regions and average employees and labor hours.

01:28:45.000 --> 01:28:51.000
Let's see what the relationship between the number of employees for a mine and the number of labor hours looks like.

01:28:51.000 --> 01:28:53.000
There's a couple of ways we can do this.

01:28:53.000 --> 01:28:55.000
Let's see the simplest way I can think of is to do a scatter plot.

01:28:55.000 --> 01:29:00.000
So we can do PLT dot scatter and DF dot average employees.

01:29:00.000 --> 01:29:08.000
So now I've indexed the data frames column by simply doing a dot before it because it has a space in it.

01:29:08.000 --> 01:29:13.000
I would have to have done it the DF bracket space labor hours, for example.

01:29:13.000 --> 01:29:15.000
So this will actually work.

01:29:15.000 --> 01:29:17.000
You see that the plot actually works as expected.

01:29:17.000 --> 01:29:24.000
But now instead of having to type out labor hours previously with a space there, I can actually do dot labor hours.

01:29:24.000 --> 01:29:28.000
And that just makes my life just ever so slightly bit better.

01:29:28.000 --> 01:29:32.000
Let's label this.

01:29:32.000 --> 01:29:35.000
Okay, so just as we expect, number of employees goes up.

01:29:35.000 --> 01:29:39.000
The total number of hours worked at that mine goes up in a pretty linear fashion.

01:29:39.000 --> 01:29:43.000
Another way of doing this would actually be linear regression plot on this.

01:29:43.000 --> 01:29:45.000
And you can use Seaborn for that.

01:29:45.000 --> 01:29:47.000
So SNS dot regression plot.

01:29:47.000 --> 01:29:51.000
And I'll do, I'll pass it the X and Y this way.

01:29:51.000 --> 01:29:55.000
And so when you can see here, the regression plot does the same thing as above,

01:29:55.000 --> 01:30:00.000
but it actually fits aligned in the data and gives it a bootstrapping in the middle of it.

01:30:00.000 --> 01:30:03.000
This bootstrap is done by a confidence interval of 95%.

01:30:03.000 --> 01:30:09.000
And it bootstraps a thousand times the underlying data to actually figure out what the variance is.

01:30:09.000 --> 01:30:15.000
So this is a kind of neat, very quick way of getting an initial look at two variables that you think might have a relationship and they clearly do.

01:30:15.000 --> 01:30:21.000
Now, if you'd like to actually save this figure as in this isn't just to look at and have it for later on,

01:30:21.000 --> 01:30:25.000
you should actually save this figure into the figures directory.

01:30:25.000 --> 01:30:29.000
So I would do PLT dot save fig figures.

01:30:29.000 --> 01:30:38.000
And I like to actually have the same beginning date structure for these figures so that if I am looking through the figures directory later on

01:30:38.000 --> 01:30:45.000
across all the different notebooks that I'll be looking at, I can easily re-correspond which figure came from which notebook.

01:30:45.000 --> 01:30:50.000
So this is just a little bit of mental accounting to get this straightforward.

01:30:50.000 --> 01:30:55.000
And let's do employees versus hours.

01:30:55.000 --> 01:30:58.000
Let's keep our underscores and spaces being the same.

01:30:58.000 --> 01:31:05.000
All right, so that's our first look at the data and it is a quick linear regression plot against two of the features that we found inside,

01:31:05.000 --> 01:31:11.000
as well as a bit of data frame manipulation using pandas.

01:31:11.000 --> 01:31:15.000
We've seen a very first look at this and we see that there's at least some trends in this data.

01:31:15.000 --> 01:31:17.000
There's probably something pretty interesting in here.

01:31:17.000 --> 01:31:21.000
So I'll keep going with this data set and seeing what I can come out with this.

01:31:21.000 --> 01:31:28.000
Now I will actually remove this header and I will toggle the toolbar as well as I need space.

01:31:28.000 --> 01:31:30.000
So let me go ahead and do that.

01:31:30.000 --> 01:31:36.000
So previously we saw with seaborne a really nice regression of the average number of employees versus labor hours.

01:31:36.000 --> 01:31:38.000
Let's keep seeing what's in this data set.

01:31:38.000 --> 01:31:41.000
Let's take a look at the columns for column in.

01:31:41.000 --> 01:31:43.000
So these are the columns in the data frame.

01:31:43.000 --> 01:31:51.000
We have year and then various things about the mind itself, the name, the state, the county, its status and its type, the company type, union code.

01:31:51.000 --> 01:31:57.000
There's a coal supply region, the production in short tons and the number of employees in labor hours.

01:31:57.000 --> 01:32:04.000
So see if the amount people work, like the labor hours total is very predictive of the production in short tons.

01:32:04.000 --> 01:32:06.000
Let's take a look at that scatter plot.

01:32:08.000 --> 01:32:09.000
Let's take a look here.

01:32:09.000 --> 01:32:13.000
So it doesn't appear to be a fantastic relationship here.

01:32:13.000 --> 01:32:15.000
Let's take a look at the actual histogram of this.

01:32:15.000 --> 01:32:21.000
So I'll do df production short tons dot hist, which is a function on pandas.

01:32:21.000 --> 01:32:25.000
And we see a very bad looking histogram.

01:32:25.000 --> 01:32:31.000
So it looks like a lot of things in this first one, which is either typical of a power law or some other kind of problem.

01:32:31.000 --> 01:32:33.000
Let's do a few transformations on this production.

01:32:33.000 --> 01:32:40.000
Let's see if we can find the minimum value or yeah, let's take a look at the minimum value zero.

01:32:40.000 --> 01:32:44.000
Let's take the length of the data frame where this is equal to zero.

01:32:44.000 --> 01:32:49.000
So if we did first, let's just look at this where the production short tons is equal to zero.

01:32:49.000 --> 01:32:54.000
We have what's returned as a series that tells us false false true true false and so forth.

01:32:54.000 --> 01:32:56.000
So this tells us whether or not the production is equal to zero.

01:32:56.000 --> 01:33:03.000
So we say df where you actually give this as an argument to data frame saying where this is equal to zero.

01:33:03.000 --> 01:33:07.000
We get the full data frame where all of the production values are equal to zero.

01:33:07.000 --> 01:33:11.000
And it looks to be like quite a few of these things produced zero tons of coal.

01:33:11.000 --> 01:33:17.000
In the interest of how much a coal mine is producing, let's take the ones that have produced at least one ton.

01:33:17.000 --> 01:33:22.000
We will say the data frame where the production of short tons is greater than zero.

01:33:22.000 --> 01:33:26.000
This has, okay, values that are not zero. This is good.

01:33:26.000 --> 01:33:28.000
From here, we will now set the data frame equal to this.

01:33:28.000 --> 01:33:30.000
Now we are at this point doing a slice.

01:33:30.000 --> 01:33:31.000
So I will make a note here.

01:33:31.000 --> 01:33:34.000
We are removing data here.

01:33:34.000 --> 01:33:37.000
That's okay as long as you're keeping track of what you're doing and why.

01:33:37.000 --> 01:33:41.000
So the reasoning behind this is if we're going to try to predict, let's say the production of mines

01:33:41.000 --> 01:33:45.000
and use things like what state the mine is in as a predictive indicator.

01:33:45.000 --> 01:33:49.000
Let's actually restrict ourselves to mines that produced something more than zero.

01:33:49.000 --> 01:33:52.000
And that's the reasoning behind how I choose something like this.

01:33:52.000 --> 01:33:58.000
So now data frame is equal to where the data frame production values is over zero.

01:33:58.000 --> 01:34:00.000
So let's see what the length of data frame is now.

01:34:00.000 --> 01:34:03.000
Okay, so we have 1061 data points.

01:34:03.000 --> 01:34:05.000
Let's redo this one.

01:34:05.000 --> 01:34:08.000
I'm going to copy this and place it down here just so that we can do a comparison.

01:34:08.000 --> 01:34:13.000
And it appears to still have quite the skew distribution.

01:34:13.000 --> 01:34:18.000
So I will try to do something now where I will actually take the log of this.

01:34:18.000 --> 01:34:20.000
So let's create a new column.

01:34:20.000 --> 01:34:26.000
And the way to create a new column in pandas is to actually just create a column as though it exists

01:34:26.000 --> 01:34:29.000
and set it equal to a function of this.

01:34:29.000 --> 01:34:32.000
So I don't know if I have NumPy installed just yet.

01:34:32.000 --> 01:34:34.000
So I'll give this a try.

01:34:34.000 --> 01:34:36.000
So let's go to the top of the page.

01:34:36.000 --> 01:34:40.000
And in all of our imports at the top of the notebook, I recommend keeping them together

01:34:40.000 --> 01:34:45.000
so that if and everyone later on can see where things were imported, import NumPy as NPs.

01:34:45.000 --> 01:34:47.000
Now this input is 30.

01:34:47.000 --> 01:34:50.000
I've imported it and I should be able to rerun this one all the way to the bottom here

01:34:50.000 --> 01:34:51.000
and create a new one.

01:34:51.000 --> 01:34:55.000
So let's look at df.logproduction.hist.

01:34:55.000 --> 01:34:59.000
So what we see here is a very close to a log normal distribution.

01:34:59.000 --> 01:35:03.000
So the production of coal mines follows a log normal distribution,

01:35:03.000 --> 01:35:06.000
which is reasonable from first guesses.

01:35:06.000 --> 01:35:07.000
All right, great.

01:35:07.000 --> 01:35:11.000
So I think I'm going to stick with this as a thing we're going to be interested in predicting.

01:35:11.000 --> 01:35:13.000
So we have our prediction variable.

01:35:13.000 --> 01:35:17.000
Now at this point, we've done quite a few things to the data frame itself.

01:35:17.000 --> 01:35:20.000
So we loaded it in, we renamed the columns.

01:35:20.000 --> 01:35:24.000
We actually created what's going to be my target variable is going to be the production of these mines

01:35:24.000 --> 01:35:27.000
and did a transformation, which is the log of this value.

01:35:27.000 --> 01:35:31.000
So after doing all this, I think I would like to actually save out this data frame

01:35:31.000 --> 01:35:34.000
that I can load it into any future analysis.

01:35:34.000 --> 01:35:36.000
So I'll do df.2.

01:35:36.000 --> 01:35:38.000
Let's save it as a CSV.

01:35:38.000 --> 01:35:42.000
So I'll call it, let's find it in the data directory, coal public this thing.

01:35:42.000 --> 01:35:44.000
We'll do cleaned version of this.

01:35:44.000 --> 01:35:46.000
And it's a CSV.

01:35:46.000 --> 01:35:51.000
So now that I've done this exploratory analysis, I would have this first look that I've taken at

01:35:51.000 --> 01:35:53.000
and I've saved the data out into this CSV file.

01:35:53.000 --> 01:35:56.000
I'm going to copy this into a new one that's going to be called data cleaning.

01:35:56.000 --> 01:36:00.000
And in the future, all I'll have to do is load in this CSV file

01:36:00.000 --> 01:36:02.000
and all the transformations will have already been done.

01:36:02.000 --> 01:36:09.000
And I'll have a link back to the reasoning behind it as well as the actual code that does this process.

01:36:09.000 --> 01:36:13.000
In this video, I'll be cleaning up the data cleaning notebook

01:36:13.000 --> 01:36:19.000
and I'll be doing our first commits to a new branch to keep everything organized.

01:36:19.000 --> 01:36:23.000
I last laughed off with this first look and their develop directory.

01:36:23.000 --> 01:36:27.000
So what we're going to do now is actually make a copy of this

01:36:27.000 --> 01:36:29.000
and I will toggle the header for this.

01:36:29.000 --> 01:36:31.000
Make a copy.

01:36:31.000 --> 01:36:34.000
And the first thing it does is it opens a new tab with everything copied in the previous one.

01:36:34.000 --> 01:36:38.000
And none of the code has been run here even though all of the inputs have been copied.

01:36:38.000 --> 01:36:41.000
What we're going to do here is actually call this something completely different

01:36:41.000 --> 01:36:43.000
which is data cleaning.

01:36:43.000 --> 01:36:46.000
I didn't put a date in front of it because this is the notebook

01:36:46.000 --> 01:36:51.000
that's going to be the one that people look at if they actually want to see how we changed the data.

01:36:51.000 --> 01:36:55.000
So I'm going to actually close this from this directory,

01:36:55.000 --> 01:36:58.000
go over to my actual terminal here

01:36:58.000 --> 01:37:02.000
and move from the develop the data cleaning ipython nb

01:37:02.000 --> 01:37:04.000
which we just created into the deliver.

01:37:04.000 --> 01:37:07.000
So we move the file from develop into deliver

01:37:07.000 --> 01:37:10.000
because deliver is the directory that people should be looking at

01:37:10.000 --> 01:37:14.000
if they're actually interested in seeing the final analysis that matters.

01:37:14.000 --> 01:37:18.000
In this case, we don't want to hide data cleaning in this development directory

01:37:18.000 --> 01:37:20.000
which has many, many files.

01:37:20.000 --> 01:37:24.000
So we've moved it into deliver and if we go back to our browser here

01:37:24.000 --> 01:37:27.000
go up into deliver and open up the data cleaning.

01:37:27.000 --> 01:37:31.000
Now we should actually start to do things like actually creating the markdown file

01:37:31.000 --> 01:37:34.000
changing the code from code to markdown

01:37:34.000 --> 01:37:37.000
giving it a nice title and continuing on with this.

01:37:37.000 --> 01:37:41.000
So we can say Jonathan by Jonathan to say like who actually did this

01:37:41.000 --> 01:37:45.000
and then you can look it up in the get repo cleaned up the data

01:37:45.000 --> 01:37:49.000
removed zero production coal mines.

01:37:49.000 --> 01:37:51.000
You can actually do a bit more of that in the end

01:37:51.000 --> 01:37:53.000
but for now that should suffice.

01:37:53.000 --> 01:37:55.000
We don't need to actually have any of these plots in here.

01:37:55.000 --> 01:37:58.000
So I'm going to be cleaning this up as quickly as I can.

01:37:58.000 --> 01:38:01.000
So numpy as NP pandas as PD.

01:38:01.000 --> 01:38:03.000
We need to read in the file still.

01:38:03.000 --> 01:38:04.000
We don't need to see the head.

01:38:04.000 --> 01:38:05.000
We know what that looks like.

01:38:05.000 --> 01:38:09.000
This can be left in because it tells us the transformation we made and why

01:38:09.000 --> 01:38:12.000
the head part doesn't need to be here for the second one

01:38:12.000 --> 01:38:15.000
but we can add a note above it that says mistake

01:38:15.000 --> 01:38:18.000
renaming indipedent to independent.

01:38:18.000 --> 01:38:24.000
Now we're in here changing spaces to underscores

01:38:24.000 --> 01:38:26.000
double check that still looks right.

01:38:26.000 --> 01:38:29.000
Okay, it does and we will now delete this head,

01:38:29.000 --> 01:38:35.000
delete the different plots here and give an extra sentence here.

01:38:35.000 --> 01:38:41.000
Coal mines without any coal production are removed.

01:38:41.000 --> 01:38:45.000
The length is 1061 and we are now creating a new column called log

01:38:45.000 --> 01:38:50.000
production which is the log of the production of the data frame.

01:38:50.000 --> 01:38:53.000
And we can put we don't have any histograms here.

01:38:53.000 --> 01:38:57.000
We need that out and now the output file is this guy and I will

01:38:57.000 --> 01:39:02.000
actually move this to the top here to the output file.

01:39:02.000 --> 01:39:05.000
The very first thing you see here will be the name of the output file

01:39:05.000 --> 01:39:09.000
and the last thing we'll do is actually write that CSV to that output file.

01:39:09.000 --> 01:39:13.000
So now when I load in this cleaned coal public 2013 and notice

01:39:13.000 --> 01:39:15.000
that I did not overwrite the old file.

01:39:15.000 --> 01:39:18.000
So I strongly recommend keeping the raw files and creating a new file.

01:39:18.000 --> 01:39:21.000
That's the cleaned version of it so that if you ever made a mistake

01:39:21.000 --> 01:39:23.000
in your cleaning which has happened before,

01:39:23.000 --> 01:39:25.000
you can easily revert and change that back.

01:39:25.000 --> 01:39:28.000
And if someone says, oh, something happened in the cleaning process,

01:39:28.000 --> 01:39:30.000
you have a full documentation of what happened here.

01:39:30.000 --> 01:39:34.000
So we've created the final document that went through and cleaned up

01:39:34.000 --> 01:39:37.000
what actually happened in the cleaning process.

01:39:37.000 --> 01:39:40.000
So anyone looking in the future can easily follow what happened.

01:39:40.000 --> 01:39:44.000
So I will now close and halt this directory and I'm going to actually

01:39:44.000 --> 01:39:48.000
do our first commit and we are in the master branch as it sits.

01:39:48.000 --> 01:39:51.000
So I will actually check out a new branch.

01:39:51.000 --> 01:39:58.000
The branch will be called JBW underscore predict production.

01:39:58.000 --> 01:39:59.000
And so we're here.

01:39:59.000 --> 01:40:02.000
There's two theories here on adding the data.

01:40:02.000 --> 01:40:04.000
So the data here is actually pretty small.

01:40:04.000 --> 01:40:06.000
So I'm going to add it to this.

01:40:06.000 --> 01:40:09.000
This is also so that you can actually get the data as well.

01:40:09.000 --> 01:40:10.000
Generally in a production environment,

01:40:10.000 --> 01:40:12.000
you don't add the data to your Git repository.

01:40:12.000 --> 01:40:14.000
This is stored and tracked in some other way.

01:40:14.000 --> 01:40:16.000
So I'll add the data cleaning.

01:40:16.000 --> 01:40:21.000
I'll add develop and not going to add the figures just yet.

01:40:21.000 --> 01:40:25.000
I usually will only add this when I actually have something interesting there.

01:40:25.000 --> 01:40:28.000
So this figures is going to be kept on my own directory for now,

01:40:28.000 --> 01:40:30.000
not put into the branch just yet.

01:40:30.000 --> 01:40:32.000
Let's look at the status one more time.

01:40:32.000 --> 01:40:36.000
So we have a number of new files, the actual data file, the cleaned data file,

01:40:36.000 --> 01:40:40.000
the data cleaning that is the official way of actually making this file

01:40:40.000 --> 01:40:42.000
and this develop one.

01:40:42.000 --> 01:40:45.000
So let's commit this.

01:40:45.000 --> 01:40:47.000
Let's not call it that then.

01:40:47.000 --> 01:40:50.000
And I have to actually configure this.

01:40:50.000 --> 01:40:52.000
So I will configure my Git.

01:40:52.000 --> 01:40:58.000
Do this commit and continue this on in just a second.

01:40:58.000 --> 01:41:06.000
So commit the data and I will be pushing it to GitHub.

01:41:06.000 --> 01:41:10.000
So the final command I ran was git push origin JBW predict production.

01:41:10.000 --> 01:41:13.000
And this means that I have now sent this off to GitHub.

01:41:13.000 --> 01:41:17.000
Go back to the GitHub of the coal exploration, reload this.

01:41:17.000 --> 01:41:21.000
What we see here is the master branch where you can actually go to the

01:41:21.000 --> 01:41:25.000
JBW production branch and see the various things we've done here.

01:41:25.000 --> 01:41:29.000
Let's actually look at the deliver and click this IPYNB.

01:41:29.000 --> 01:41:33.000
And we'll notice that GitHub does a fantastically nice job of actually rendering

01:41:33.000 --> 01:41:36.000
the notebook as it looks correctly.

01:41:36.000 --> 01:41:40.000
And this is even more dramatic when you actually look at the develop one.

01:41:40.000 --> 01:41:44.000
So we can see this and you can see in here if you're browsing with GitHub,

01:41:44.000 --> 01:41:46.000
the figures are faithfully reproduced here.

01:41:46.000 --> 01:41:50.000
And this is a very useful thing to be able to look at the files being used

01:41:50.000 --> 01:41:54.000
and especially when we do a pull request in the future.

01:41:54.000 --> 01:41:56.000
Okay, so we've cleaned the data.

01:41:56.000 --> 01:41:59.000
We have the way that we cleaned it separated out so that anyone else

01:41:59.000 --> 01:42:01.000
can look at it in a reproducible way.

01:42:01.000 --> 01:42:04.000
And so let's actually try to predict something.

01:42:04.000 --> 01:42:08.000
So I'll go back into this develop directory and it will make a copy

01:42:08.000 --> 01:42:11.000
of the first look notebook that we had.

01:42:11.000 --> 01:42:15.000
So I'll make a copy of this and I'm going to call it CoalPredict.

01:42:15.000 --> 01:42:19.000
I'm going to go back to the previous tab and actually finish closing this

01:42:19.000 --> 01:42:20.000
and halting it.

01:42:20.000 --> 01:42:24.000
And just to give you a sense of how everything is standing,

01:42:24.000 --> 01:42:27.000
I'm now back at the home of this develop thing.

01:42:27.000 --> 01:42:31.000
You can see the first look notebook and it's currently black because it's not running.

01:42:31.000 --> 01:42:34.000
This one is green because you can see on the right here it says running.

01:42:34.000 --> 01:42:37.000
So this is a notebook that's currently being run.

01:42:37.000 --> 01:42:40.000
There's a couple of things I want to do different here since this is now the prediction one.

01:42:40.000 --> 01:42:46.000
When I start off by saying what the goal of this notebook is going to be

01:42:46.000 --> 01:42:50.000
and because everything that's here is a direct copy of the previous notebook,

01:42:50.000 --> 01:42:52.000
most of this stuff I'll just be able to delete.

01:42:52.000 --> 01:42:56.000
So I'm going to toggle the header, give us a little bit more space

01:42:56.000 --> 01:43:01.000
and the changes I'm going to make are basically going to drive me toward being able to make this new prediction.

01:43:01.000 --> 01:43:05.000
So first of all, I don't want to reproduce all this cleaning I did before.

01:43:05.000 --> 01:43:08.000
So I will actually instead of reading in the previous raw data,

01:43:08.000 --> 01:43:11.000
I'll actually go into and read the CSV that we saved.

01:43:11.000 --> 01:43:17.000
And this is up into the data directory and it's the cleaned public CSV.

01:43:17.000 --> 01:43:22.000
And we still need to set the index column to be the MSHA ID.

01:43:22.000 --> 01:43:27.000
So that's loaded in and actually one thing I like to do is look at the head of the data frame

01:43:27.000 --> 01:43:31.000
and read it in at the same time in case I need to make any changes.

01:43:31.000 --> 01:43:35.000
So the way to do this is since the four is selected with a gray box,

01:43:35.000 --> 01:43:41.000
if I hold down shift and type K, I'm selecting both the second and third cell which are index three and four.

01:43:41.000 --> 01:43:46.000
If I type shift M, they are now combined into a single merged cell.

01:43:46.000 --> 01:43:53.000
So let me just run this one cell and I read in the CSV and then you are seeing the head of that data frame as well.

01:43:53.000 --> 01:43:58.000
So we can see that we're loading in the cleaned CSV and the head is looking nice.

01:43:58.000 --> 01:44:02.000
I'm going to now delete a number of these things because we don't need them.

01:44:02.000 --> 01:44:07.000
One thing I will remain is that we initially did this LEN of the data frame before.

01:44:07.000 --> 01:44:10.000
This was on the first one that you saw on the raw data.

01:44:10.000 --> 01:44:13.000
So since this is the clean data, I expect this to be just over a thousand.

01:44:13.000 --> 01:44:16.000
Yep, it went to 1061.

01:44:16.000 --> 01:44:18.000
Simply delete these.

01:44:18.000 --> 01:44:24.000
I'll leave the number of columns in here so we can actually think about what's in each of these columns a bit.

01:44:24.000 --> 01:44:27.000
Alright, so as we see, this is the production.

01:44:27.000 --> 01:44:31.000
Longer the production is the thing that we're going to be trying to predict.

01:44:31.000 --> 01:44:36.000
And let's take a look at just a high level view of the different categories that might be able to help us.

01:44:36.000 --> 01:44:38.000
So let me get the columns here.

01:44:38.000 --> 01:44:42.000
I think that the mine status might be a predictive variable.

01:44:42.000 --> 01:44:45.000
So I do df.mine status.

01:44:45.000 --> 01:44:51.000
You see that there's an active men working, not producing, permanently abandoned, active,

01:44:51.000 --> 01:44:55.000
temporarily closed and new under construction of the different status types.

01:44:55.000 --> 01:45:00.000
I suspect this will give me a pretty good predictor into how productive the mine actually is.

01:45:00.000 --> 01:45:04.000
So I will actually do a group by on this to see what is in here.

01:45:04.000 --> 01:45:08.000
So df.mine status.

01:45:08.000 --> 01:45:16.000
Let's do production.

01:45:16.000 --> 01:45:25.000
What I did here was I said, take all the ones that have the same status of active and take the average or the mean of the production in short tons.

01:45:25.000 --> 01:45:31.000
And we can see that the active ones are much more productive than the temporarily closed ones or the permanently abandoned ones.

01:45:31.000 --> 01:45:36.000
It's interesting to me that permanently abandoned has on average 60,000 tons.

01:45:36.000 --> 01:45:40.000
Let's look at it in terms of the log of the production though.

01:45:40.000 --> 01:45:43.000
This will be what I think we're going to be going against.

01:45:43.000 --> 01:45:50.000
So huge difference in the overall production capabilities, but we'll see how good this is at making a final prediction.

01:45:50.000 --> 01:45:56.000
So from here is we would like to predict the log of coal mines.

01:45:56.000 --> 01:46:02.000
And we'd also like to know what actually leads to the production, higher production and lower production.

01:46:02.000 --> 01:46:08.000
If we look again at all the columns in our data frame, the data that we have year is the same for all of them.

01:46:08.000 --> 01:46:11.000
And various things that shouldn't matter at all.

01:46:11.000 --> 01:46:20.000
Like the union code is just going to be a code that's given to the mine from a, let's just look at that.

01:46:20.000 --> 01:46:22.000
Actually, that might be predictive.

01:46:22.000 --> 01:46:27.000
So I'm going to try to throw as many of these things as we can into a predictive model.

01:46:27.000 --> 01:46:29.000
So I'll call these features.

01:46:29.000 --> 01:46:32.000
And let's start with this as our list of features.

01:46:32.000 --> 01:46:36.000
We'll have our target be log production.

01:46:36.000 --> 01:46:42.000
So year is going to be entirely unpredictable because it's a single thing.

01:46:42.000 --> 01:46:46.000
Mine name, I suspect will not be predictive because it's simply the mine.

01:46:46.000 --> 01:46:51.000
The state might be what state is it in, what county is it in that could be useful.

01:46:51.000 --> 01:46:54.000
The mine status, I'm sure will be predictive.

01:46:54.000 --> 01:47:05.000
Mine type will probably be it's possible that the operating type, the address of the operating company probably isn't because we already have the geographic things done with the county and the state.

01:47:05.000 --> 01:47:09.000
Though it's interesting, we'll definitely have some collinearity between the state and the county.

01:47:09.000 --> 01:47:11.000
So it's possible that particular county and the state's good.

01:47:11.000 --> 01:47:12.000
We'll leave those in.

01:47:12.000 --> 01:47:16.000
Leave in the union code, the coal supply region.

01:47:16.000 --> 01:47:22.000
We can't give it the production of short tons as a prediction of the log of the production because that's cheating.

01:47:22.000 --> 01:47:26.000
The number of employees that are employed and the number of labor hours.

01:47:26.000 --> 01:47:27.000
Just to clean this up.

01:47:27.000 --> 01:47:32.000
So I hold down shift and push the down arrow key and I've highlighted everything to indent.

01:47:32.000 --> 01:47:36.000
I'm going to hold down command and hit the right bracket key, which is the square brackets.

01:47:36.000 --> 01:47:38.000
So the parentheses are curved all the way around.

01:47:38.000 --> 01:47:42.000
There's curly braces, which have a lot of curls in the square brackets.

01:47:42.000 --> 01:47:46.000
So holding down command and typing the right one will indent an entire block of text.

01:47:46.000 --> 01:47:49.000
If you do the left bracket, it unindents.

01:47:49.000 --> 01:47:52.000
This is a quick way of formatting lists.

01:47:52.000 --> 01:47:57.000
So the features that we're going to be giving our model are going to be all of these features here.

01:47:57.000 --> 01:48:00.000
The target's going to be the log of the production.

01:48:00.000 --> 01:48:06.000
Now of these, I think only two of these are actually numbers to start with.

01:48:06.000 --> 01:48:11.000
So I think average employees and labor hours are the only ones that are proper features.

01:48:11.000 --> 01:48:14.000
And the rest of them are what I'm going to call categorical.

01:48:14.000 --> 01:48:23.000
So the categoricals are these minus the average employees in the labor hours.

01:48:23.000 --> 01:48:26.000
And having a trailing comma here is actually okay.

01:48:26.000 --> 01:48:28.000
We need commas between all the rest of them otherwise.

01:48:28.000 --> 01:48:30.000
But this is one of my favorite features of Python.

01:48:30.000 --> 01:48:32.000
And I don't know why it makes me so happy.

01:48:32.000 --> 01:48:36.000
But having a trailing comma and having it not have a problem just makes me really happy.

01:48:36.000 --> 01:48:40.000
So the features, which I'm going to just call the ones that are numeric,

01:48:40.000 --> 01:48:43.000
are the average employees and labor hours.

01:48:43.000 --> 01:48:46.000
The categoricals are the ones that are category variables.

01:48:46.000 --> 01:48:51.000
So mine state, county, status, type, company type, operating type, union code,

01:48:51.000 --> 01:48:54.000
and coal supply region are all categoricals.

01:48:54.000 --> 01:48:58.000
One thing that we'll have to do is create, because we'll be using scikit-learn,

01:48:58.000 --> 01:49:03.000
we'll have to turn these categoricals into numbers or into some sort of numerical thing.

01:49:03.000 --> 01:49:06.000
And we'll be doing that with what's called a one-hot encoding.

01:49:06.000 --> 01:49:09.000
Also called dummy variables. There's probably a few other names as well.

01:49:09.000 --> 01:49:11.000
So we split this up into numeric features.

01:49:11.000 --> 01:49:14.000
So things that have numbers representing how long people worked,

01:49:14.000 --> 01:49:18.000
how many employees a mine has, categorical, which is what state

01:49:18.000 --> 01:49:20.000
or some other thing that actually has a category,

01:49:20.000 --> 01:49:23.000
and the target variable, which is log of the production.

01:49:23.000 --> 01:49:26.000
From here, we need to do a bit more data munging after it's all been cleaned.

01:49:26.000 --> 01:49:34.000
We now have to do some munging to make this into a form that scikit-learn can actually predict with.

01:49:34.000 --> 01:49:40.000
In this lesson, we'll be looking at the final data munging and the final prediction for this data.

01:49:40.000 --> 01:49:42.000
So I've actually changed up this slightly.

01:49:42.000 --> 01:49:45.000
So the features that we'll be looking at, these are numeric features to start with.

01:49:45.000 --> 01:49:50.000
The average number of employees per mine and the number of labor hours total worked for that mine.

01:49:50.000 --> 01:49:54.000
And also a categorical list. This categorical list contains features

01:49:54.000 --> 01:50:00.000
which have a small number of string representations instead of actual numbers.

01:50:00.000 --> 01:50:06.000
And again, the target we're looking at is the log value of the production in tons.

01:50:06.000 --> 01:50:10.000
So one thing that I recommend you doing is taking a look at the interplay

01:50:10.000 --> 01:50:13.000
between each of the variables and the target variable.

01:50:13.000 --> 01:50:16.000
So I'll do a quick example of this.

01:50:16.000 --> 01:50:20.000
So let's take a look at the relationship between mine status,

01:50:20.000 --> 01:50:23.000
which is a categorical variable, and the log of the production.

01:50:23.000 --> 01:50:28.000
I'll be doing that with this Seaborn code here, which I just executed.

01:50:28.000 --> 01:50:32.000
And the set context has to be run twice the first time.

01:50:32.000 --> 01:50:35.000
What this is doing is doing a violin plot.

01:50:35.000 --> 01:50:38.000
So this is the Seaborn library SNS, and it's creating this.

01:50:38.000 --> 01:50:40.000
It's using the violin plot function.

01:50:40.000 --> 01:50:45.000
And what we see here on the y-axis is the mine status, the five possible values,

01:50:45.000 --> 01:50:49.000
active with men working but not producing, permanently abandoned, active,

01:50:49.000 --> 01:50:52.000
temporarily closed, and new under construction.

01:50:52.000 --> 01:50:56.000
And on the x-axis, we see the log of the production.

01:50:56.000 --> 01:51:01.000
So you see that each of these mine status types corresponds to a different log

01:51:01.000 --> 01:51:03.000
of the production value of that mine.

01:51:03.000 --> 01:51:06.000
But also the distribution has this interesting shape,

01:51:06.000 --> 01:51:08.000
and it changes between these categories.

01:51:08.000 --> 01:51:11.000
This kind of a plot is a very nice high level view of what these variables

01:51:11.000 --> 01:51:13.000
interactions look like.

01:51:13.000 --> 01:51:14.000
I'll do just one more.

01:51:14.000 --> 01:51:17.000
How does company type corresponds to the production?

01:51:17.000 --> 01:51:20.000
So we see that there are three company types here, independent producer,

01:51:20.000 --> 01:51:23.000
operating subsidiary and contractor, and each of those corresponds

01:51:23.000 --> 01:51:25.000
to a very different distribution.

01:51:25.000 --> 01:51:28.000
So you can do this for all of the variables, and I recommend doing that,

01:51:28.000 --> 01:51:31.000
especially before and getting a sense of what the data actually looks like.

01:51:31.000 --> 01:51:35.000
But for us, we just look at this company type a little bit more closely.

01:51:35.000 --> 01:51:40.000
So if we do a DF company type dot unique, we return all the unique values.

01:51:40.000 --> 01:51:42.000
Of course, we see the three that we see in the plot above.

01:51:42.000 --> 01:51:46.000
An independent producer operator operating subsidiary and contractor.

01:51:46.000 --> 01:51:51.000
The scikit-learn functions don't take in these strings as separate

01:51:51.000 --> 01:51:53.000
category variables.

01:51:53.000 --> 01:51:55.000
We actually have to encode this ourselves.

01:51:55.000 --> 01:51:58.000
Now one way to encode this would be to do something like assign

01:51:58.000 --> 01:52:02.000
independent producer to be one, operating subsidiary to be two,

01:52:02.000 --> 01:52:03.000
and contractor to be three.

01:52:03.000 --> 01:52:06.000
And that would work except that we are then implicitly telling,

01:52:06.000 --> 01:52:10.000
let's say a scikit-learn random forest function that three is greater than two,

01:52:10.000 --> 01:52:11.000
which is also greater than one.

01:52:11.000 --> 01:52:13.000
And there's an implicit ordering there.

01:52:13.000 --> 01:52:17.000
And it might start to try to cut the features in a way that doesn't make sense.

01:52:17.000 --> 01:52:21.000
A more safe way to do this is to actually create what's called dummy variables.

01:52:21.000 --> 01:52:24.000
Pandas has a built-in dummy variable function.

01:52:24.000 --> 01:52:28.000
So we do PD dot get dummies on the data frame with just,

01:52:28.000 --> 01:52:31.000
we're looking at the single column of company type.

01:52:31.000 --> 01:52:33.000
And I'm taking a sample of 50 so that we get a mix of types,

01:52:33.000 --> 01:52:35.000
because it's actually ordered in this data set,

01:52:35.000 --> 01:52:37.000
and just taking a look at the top 10.

01:52:37.000 --> 01:52:39.000
So I'm going to run this a couple of times.

01:52:39.000 --> 01:52:41.000
This sample will actually re-sample every time I run it.

01:52:41.000 --> 01:52:45.000
So what we see here is the contractor independent and operating subsidiary,

01:52:45.000 --> 01:52:48.000
this MSHA ID corresponds to an independent producer operator,

01:52:48.000 --> 01:52:50.000
because it has a one in that column,

01:52:50.000 --> 01:52:52.000
and zeros in each of the other columns.

01:52:52.000 --> 01:52:55.000
And if you go down to this 4407123 ID,

01:52:55.000 --> 01:52:57.000
it is an operating subsidiary company,

01:52:57.000 --> 01:52:59.000
and it has zeros in the rest of the column.

01:52:59.000 --> 01:53:02.000
So this is what the get dummies function does with pandas.

01:53:02.000 --> 01:53:05.000
Now what we want to do is actually turn each of the categorical variables

01:53:05.000 --> 01:53:07.000
that we're looking at into dummy variables.

01:53:07.000 --> 01:53:10.000
And then we'll actually learn to drop one of the variables

01:53:10.000 --> 01:53:12.000
to avoid the dummy variable trap.

01:53:12.000 --> 01:53:15.000
We're then going to concat the data frames together.

01:53:15.000 --> 01:53:18.000
So we're taking the data frame and the temporary data frame together.

01:53:18.000 --> 01:53:23.000
And axis equals 1 means it will add it as columns to the existing data frames.

01:53:23.000 --> 01:53:27.000
And we will then drop the drop variable from the data frame

01:53:27.000 --> 01:53:33.000
and call that to list function on the columns of the temporary data frame

01:53:33.000 --> 01:53:37.000
so that we have a final list of what the dummy categories look like.

01:53:37.000 --> 01:53:39.000
Let's run that real fast.

01:53:39.000 --> 01:53:40.000
It completes very quickly.

01:53:40.000 --> 01:53:44.000
We see that there are 29 mine states, 164 mine counties.

01:53:44.000 --> 01:53:45.000
So this might be a little bit high.

01:53:45.000 --> 01:53:47.000
We might have to come back and look at that.

01:53:47.000 --> 01:53:51.000
The mine status, there's five, mine type 3, company type 3, and so on.

01:53:51.000 --> 01:53:53.000
And the actual value of the dummy variables themselves,

01:53:53.000 --> 01:53:55.000
let's take a look at say the first 10.

01:53:55.000 --> 01:53:59.000
We see mine state, Alabama, mine state, Alaska, and so on.

01:53:59.000 --> 01:54:01.000
So these are the different state variables

01:54:01.000 --> 01:54:03.000
that have been created.

01:54:03.000 --> 01:54:05.000
Let's actually start to build a model.

01:54:05.000 --> 01:54:09.000
So we'll say, so I created this as a markdown

01:54:09.000 --> 01:54:13.000
by typing escape to make me into select mode instead of insert mode

01:54:13.000 --> 01:54:15.000
and typing m, m for markdown.

01:54:15.000 --> 01:54:17.000
You can also go up here and click it.

01:54:17.000 --> 01:54:20.000
So if I could go back to code, this is simply commented out Python code

01:54:20.000 --> 01:54:21.000
as far as the notebook is concerned.

01:54:21.000 --> 01:54:23.000
We actually want this to be markdown.

01:54:23.000 --> 01:54:26.000
So we click markdown and you can see it pre-rendered

01:54:26.000 --> 01:54:27.000
before we actually execute the cell.

01:54:27.000 --> 01:54:29.000
And it looks like this nice bold font.

01:54:29.000 --> 01:54:32.000
We're going to need to import a couple of things from scikit-learn itself.

01:54:32.000 --> 01:54:35.000
So we're going to say from scikit-learn dot cross validation.

01:54:35.000 --> 01:54:38.000
So this is the sub module of scikit-learn.

01:54:38.000 --> 01:54:42.000
We're going to import the test train split function, which is labeled here.

01:54:42.000 --> 01:54:46.000
And we're also going to use a random force regressor as our algorithm.

01:54:46.000 --> 01:54:51.000
Loading that in, you look at total length of the dummy categoricals is 213.

01:54:51.000 --> 01:54:55.000
The train and test is going to be the names of the data frames

01:54:55.000 --> 01:54:59.000
that's going to be split by this test train split function.

01:54:59.000 --> 01:55:02.000
The function takes in our data frame.

01:55:02.000 --> 01:55:05.000
And you tell it how large you'd like the test size to be.

01:55:05.000 --> 01:55:09.000
So in this case, we're going to have a 30% of the data frame is going to be the holdout set.

01:55:09.000 --> 01:55:13.000
And the nice thing about this function is that we actually retain

01:55:13.000 --> 01:55:16.000
the data frame structure of these variables.

01:55:16.000 --> 01:55:19.000
Scikit-learn likes to think in terms of native numpy arrays,

01:55:19.000 --> 01:55:23.000
but many of the features can actually read in a pandas data frame as well.

01:55:23.000 --> 01:55:27.000
And the utility of having a pandas data frame around just makes it really nice to keep it,

01:55:27.000 --> 01:55:29.000
to stay in data frames as long as you can.

01:55:29.000 --> 01:55:32.000
So we can actually do it the whole way through. So that's really nice.

01:55:32.000 --> 01:55:35.000
Our train is a data frame. Our test is a data frame.

01:55:35.000 --> 01:55:39.000
And they've been split from the data frame that contains all of our data.

01:55:39.000 --> 01:55:41.000
So now we're going to create a random forest.

01:55:41.000 --> 01:55:43.000
And I would like to run these separately.

01:55:43.000 --> 01:55:49.000
So I'm going to split this cell here by typing control shift minus splits the cells into two.

01:55:49.000 --> 01:55:51.000
And I will execute this one.

01:55:51.000 --> 01:55:54.000
This says RF is an instantiation of this random forest regressor,

01:55:54.000 --> 01:55:56.000
which we imported above.

01:55:56.000 --> 01:55:58.000
And there's two things we're going to initialize it with.

01:55:58.000 --> 01:56:00.000
Number of estimators is 100.

01:56:00.000 --> 01:56:03.000
This is a number of trees that we're going to be building a random forest out of.

01:56:03.000 --> 01:56:05.000
And whether or not we're going to be using the out of bag score,

01:56:05.000 --> 01:56:07.000
which we are in this case.

01:56:07.000 --> 01:56:11.000
So we have an RF model and we'd like to fit on this by giving it

01:56:11.000 --> 01:56:15.000
x comma y and sample equals non as default.

01:56:15.000 --> 01:56:19.000
So the x value is the design matrix.

01:56:19.000 --> 01:56:21.000
The y is the target variable.

01:56:21.000 --> 01:56:24.000
So in our case, we're going to do the train data frame.

01:56:24.000 --> 01:56:26.000
And we're going to give it all the features,

01:56:26.000 --> 01:56:30.000
which is just those two average employees and the total laborer,

01:56:30.000 --> 01:56:32.000
as well as the dummy categoricals.

01:56:32.000 --> 01:56:35.000
Now, these two things together is just adding them together.

01:56:35.000 --> 01:56:37.000
It creates a large Python list.

01:56:37.000 --> 01:56:40.000
We can see the top two things up here at the top,

01:56:40.000 --> 01:56:42.000
average employees and labor hours,

01:56:42.000 --> 01:56:44.000
and then everything else is dummy categoricals.

01:56:44.000 --> 01:56:47.000
We then run the fit method on the random forest

01:56:47.000 --> 01:56:50.000
and we can get the design matrix of train features

01:56:50.000 --> 01:56:53.000
plus dummy categoricals and the target,

01:56:53.000 --> 01:56:56.000
which is train just selected on the target variable,

01:56:56.000 --> 01:56:58.000
which we defined above as log production.

01:56:58.000 --> 01:57:02.000
So it tells us some features or it gives us a little summary

01:57:02.000 --> 01:57:05.000
where it talks about the bootstrap, the criterion as mean squared error,

01:57:05.000 --> 01:57:06.000
various other things here.

01:57:06.000 --> 01:57:08.000
So this is all the variables that you can change very easily.

01:57:08.000 --> 01:57:11.000
If you'd like to actually tweak this for your own problems.

01:57:11.000 --> 01:57:13.000
So let's take a look at how well this does.

01:57:13.000 --> 01:57:16.000
And we're going to do this by giving a seaborne plot again,

01:57:16.000 --> 01:57:18.000
a regression plot, but except the train,

01:57:18.000 --> 01:57:21.000
we're going to be using the test data frame.

01:57:21.000 --> 01:57:24.000
So I test the target and the regression plot here is going to be

01:57:24.000 --> 01:57:29.000
in target versus what we actually predict this to be.

01:57:29.000 --> 01:57:31.000
So the actual is along the x-axis here.

01:57:31.000 --> 01:57:33.000
This is what the actual production is.

01:57:33.000 --> 01:57:37.000
And the y-axis is the predicted value.

01:57:37.000 --> 01:57:39.000
I can actually add that in.

01:57:39.000 --> 01:57:40.000
I think it should be there.

01:57:40.000 --> 01:57:43.000
So we say predicted production.

01:57:43.000 --> 01:57:45.000
So predicted production is on the y-axis

01:57:45.000 --> 01:57:47.000
and the actual production is along the x-axis.

01:57:47.000 --> 01:57:50.000
So perfectly calibrated and perfectly predictive thing.

01:57:50.000 --> 01:57:54.000
Everything would line along this one to one ratio line here.

01:57:54.000 --> 01:57:55.000
We see that there's some scatter around it,

01:57:55.000 --> 01:57:58.000
but actually it looks like it's a pretty good overall predictor

01:57:58.000 --> 01:58:00.000
of the actual production.

01:58:00.000 --> 01:58:02.000
We'd like to actually see how good is this fit

01:58:02.000 --> 01:58:03.000
rather than just look at the plot and say,

01:58:03.000 --> 01:58:04.000
oh, it looks pretty good.

01:58:04.000 --> 01:58:08.000
So let's import a few of the test metrics that we can actually look at.

01:58:08.000 --> 01:58:12.000
So we can say we can import explained variance score,

01:58:12.000 --> 01:58:15.000
the R2 scored, and the mean squared error.

01:58:15.000 --> 01:58:17.000
So the way these functions work,

01:58:17.000 --> 01:58:21.000
they always take in the true and then they take in the predicted.

01:58:21.000 --> 01:58:23.000
So this is going to be test target

01:58:23.000 --> 01:58:25.000
and then the predicted test target.

01:58:25.000 --> 01:58:29.000
And actually I think this way of writing it is a little bit too verbose.

01:58:29.000 --> 01:58:32.000
So I'm going to call it predicted equals this.

01:58:32.000 --> 01:58:35.000
And I'm going to say predicted here.

01:58:35.000 --> 01:58:38.000
So the R squared score is 0.88.

01:58:38.000 --> 01:58:42.000
Explained variance score is 0.88 as well.

01:58:42.000 --> 01:58:45.000
The mean squared error is 0.54.

01:58:45.000 --> 01:58:47.000
And now because this is a random forest,

01:58:47.000 --> 01:58:50.000
we actually have the feature importance of the model.

01:58:50.000 --> 01:58:54.000
And I don't know of a good way that's naturally given by scikit-learn

01:58:54.000 --> 01:58:55.000
to actually report this,

01:58:55.000 --> 01:58:57.000
but here's a little bit of code that I have written to make it

01:58:57.000 --> 01:59:00.000
so that I can actually read this in a way that I actually think is useful.

01:59:00.000 --> 01:59:02.000
So I'm going to create a new pandas data frame

01:59:02.000 --> 01:59:04.000
called rf underscore importances,

01:59:04.000 --> 01:59:09.000
which actually takes out the features and the importances from the fit model.

01:59:09.000 --> 01:59:12.000
And I'm going to look at that at the top 20 here.

01:59:12.000 --> 01:59:16.000
All of the importances of every variable we give it to in total adds up to one.

01:59:16.000 --> 01:59:19.000
So we can think of this as fractional importance

01:59:19.000 --> 01:59:21.000
in terms of what the random forest has decided

01:59:21.000 --> 01:59:25.000
is going to be discriminative in giving us a final regression score.

01:59:25.000 --> 01:59:28.000
So of utmost importance is the labor hours

01:59:28.000 --> 01:59:30.000
and then average employees is down from there.

01:59:30.000 --> 01:59:33.000
The mine type being surface is predictive.

01:59:33.000 --> 01:59:35.000
The mine county being campel

01:59:35.000 --> 01:59:39.000
and coal supply region powder river basin is apparently moderately predictive.

01:59:39.000 --> 01:59:41.000
And then it goes down from there.

01:59:41.000 --> 01:59:42.000
So this is just the first 20.

01:59:42.000 --> 01:59:46.000
And we have not only a final fit with a nice plot,

01:59:46.000 --> 01:59:48.000
we also have some diagnostics and metrics,

01:59:48.000 --> 01:59:51.000
as well as a list of what's important.

01:59:53.000 --> 01:59:54.000
In this video,

01:59:54.000 --> 01:59:58.000
I'll be showing you how to take a development lab notebook

01:59:58.000 --> 02:00:01.000
and turn it into a deliverable notebook.

02:00:01.000 --> 02:00:06.000
So let's go into our directory and we go to the develop folder.

02:00:06.000 --> 02:00:08.000
Clicking that we navigate into that folder

02:00:08.000 --> 02:00:10.000
and we see we had our first look notebook

02:00:10.000 --> 02:00:12.000
and then this coal prediction notebook.

02:00:12.000 --> 02:00:15.000
And what we'd like to do is make a copy of this notebook.

02:00:15.000 --> 02:00:19.000
So you can select it by clicking this checkbox here

02:00:19.000 --> 02:00:21.000
and clicking duplicate.

02:00:21.000 --> 02:00:24.000
When we do that, we have a second copy of this,

02:00:24.000 --> 02:00:27.000
which is added to the end of the name copy one.

02:00:27.000 --> 02:00:31.000
Now this file should exist in this directory and we see it here.

02:00:31.000 --> 02:00:32.000
Copy one.

02:00:32.000 --> 02:00:34.000
Because it's going to be a deliverable notebook,

02:00:34.000 --> 02:00:37.000
we should actually move this into the delivery folder.

02:00:37.000 --> 02:00:44.000
So let's move 2015 coal predict copy into the deliver directory.

02:00:44.000 --> 02:00:46.000
Go over to the deliver directory

02:00:46.000 --> 02:00:50.000
and let's navigate there with the notebook server itself.

02:00:50.000 --> 02:00:52.000
Let's open this up.

02:00:52.000 --> 02:00:54.000
Okay, so let's first give this a title

02:00:54.000 --> 02:00:56.000
that we think is an appropriate title.

02:00:56.000 --> 02:00:58.000
And because it's going to be a deliverable notebook,

02:00:58.000 --> 02:01:00.000
it shouldn't start with a date.

02:01:00.000 --> 02:01:06.000
So it should start with something like coal prediction of production.

02:01:06.000 --> 02:01:08.000
So we have a new name for this.

02:01:08.000 --> 02:01:11.000
You can save this and I'm going to toggle that header bar.

02:01:11.000 --> 02:01:13.000
So I have a little bit more space

02:01:13.000 --> 02:01:16.000
and I'm going to toggle this toolbar as well

02:01:16.000 --> 02:01:19.000
because I'll mostly be using keyboard shortcuts.

02:01:19.000 --> 02:01:23.000
So at this stage, we have this long notebook that went through

02:01:23.000 --> 02:01:26.000
and it's a complete copy of our lab notebook style.

02:01:26.000 --> 02:01:29.000
So we can delete things here pretty freely

02:01:29.000 --> 02:01:32.000
and just focus on the main story that you'd like to tell

02:01:32.000 --> 02:01:34.000
to either your teammates or your manager

02:01:34.000 --> 02:01:36.000
or whoever is going to be consuming this.

02:01:36.000 --> 02:01:39.000
So keep in mind with your audience what you think they would like to see

02:01:39.000 --> 02:01:41.000
and cut out the extraneous stuff

02:01:41.000 --> 02:01:43.000
and adding in as much text as you think is useful.

02:01:43.000 --> 02:01:45.000
And in that keyboard shortcuts,

02:01:45.000 --> 02:01:48.000
especially are going to be make your life a lot easier

02:01:48.000 --> 02:01:50.000
and make this whole process really fast.

02:01:50.000 --> 02:01:52.000
All right, so let's just go through this.

02:01:52.000 --> 02:01:55.000
And initially what I'd like to do is give a good title

02:01:55.000 --> 02:02:00.000
and you can just call it coal production in mines 2013, let's say.

02:02:00.000 --> 02:02:03.000
And so we have our first setup here

02:02:03.000 --> 02:02:05.000
and you can also give a little abstracts.

02:02:05.000 --> 02:02:07.000
You can say we did a lot of analysis,

02:02:07.000 --> 02:02:09.000
came to some interesting conclusions.

02:02:09.000 --> 02:02:13.000
Now, of course, fill that out with more verbiage as you see fit.

02:02:13.000 --> 02:02:15.000
Keeping the code in this notebook is useful

02:02:15.000 --> 02:02:17.000
so that someone else looking down the road

02:02:17.000 --> 02:02:19.000
can actually reproduce all the key results

02:02:19.000 --> 02:02:21.000
that you think you can find.

02:02:21.000 --> 02:02:23.000
Now, this isn't always possible, but as far as it is possible,

02:02:23.000 --> 02:02:25.000
I recommend trying to do it.

02:02:25.000 --> 02:02:27.000
So try to keep the imports neat and tidy.

02:02:27.000 --> 02:02:29.000
Keep only the imports that are required

02:02:29.000 --> 02:02:31.000
and remove the ones that are extraneous.

02:02:31.000 --> 02:02:33.000
I think we actually use all of these.

02:02:33.000 --> 02:02:35.000
I would recommend keeping these magic imports

02:02:35.000 --> 02:02:37.000
on their own line at the top.

02:02:37.000 --> 02:02:40.000
So having matplotlib inline at the top, that is good.

02:02:40.000 --> 02:02:42.000
Put a space between that.

02:02:42.000 --> 02:02:44.000
The Pepe convention is to have one of the standard libraries,

02:02:44.000 --> 02:02:46.000
like import string, let's say.

02:02:46.000 --> 02:02:48.000
That would be next and any of the other ones here,

02:02:48.000 --> 02:02:51.000
and then another blank line before third-party libraries,

02:02:51.000 --> 02:02:53.000
which is what these are.

02:02:53.000 --> 02:02:56.000
And finally, we have an actual plotting change

02:02:56.000 --> 02:02:59.000
that we make with this SNS command here.

02:02:59.000 --> 02:03:02.000
So we execute that cell and make sure everything is making sense.

02:03:02.000 --> 02:03:04.000
Yes, we see this warning, we've seen this before,

02:03:04.000 --> 02:03:06.000
so we're not too worried about it.

02:03:06.000 --> 02:03:08.000
Now, from here on out, we should be making decisions

02:03:08.000 --> 02:03:10.000
about whether this actually improves the story

02:03:10.000 --> 02:03:13.000
for the person reading this or if it becomes just tedious.

02:03:13.000 --> 02:03:16.000
And when you have data that's being imported

02:03:16.000 --> 02:03:18.000
and it's changed from the raw data,

02:03:18.000 --> 02:03:20.000
there's this clean data set here.

02:03:20.000 --> 02:03:23.000
I think it needs to have some extra commentary around it

02:03:23.000 --> 02:03:25.000
so that people know what's going on.

02:03:25.000 --> 02:03:27.000
So I might say...

02:03:30.000 --> 02:03:32.000
I might give it a description about where exactly it is

02:03:32.000 --> 02:03:35.000
in this repo, and let's just type an ls here.

02:03:35.000 --> 02:03:38.000
The name of the notebook is data underscore cleaning.

02:03:38.000 --> 02:03:41.000
So we will say the same thing.

02:03:41.000 --> 02:03:44.000
Double click, drag over, command C to copy that,

02:03:44.000 --> 02:03:46.000
command V to paste.

02:03:46.000 --> 02:03:49.000
And this ls command, which is handy, we can be deleted.

02:03:49.000 --> 02:03:52.000
So typing escape to get out of the insert mode

02:03:52.000 --> 02:03:55.000
so that the cell is now surrounded by a gray box.

02:03:55.000 --> 02:03:58.000
And then typing D twice, it deletes that cell.

02:03:58.000 --> 02:04:01.000
And in this cell, we are starting to write some markdown.

02:04:01.000 --> 02:04:03.000
We can tell it's markdown because it's just a text

02:04:03.000 --> 02:04:05.000
for people to look at.

02:04:05.000 --> 02:04:07.000
But also, we've put a double header marking too.

02:04:07.000 --> 02:04:10.000
So let's just change this cell type to be markdown.

02:04:10.000 --> 02:04:12.000
So we're currently in a code cell.

02:04:12.000 --> 02:04:14.000
We can change it to be markdown by typing M.

02:04:14.000 --> 02:04:17.000
And as soon as you type M, it switches into markdown

02:04:17.000 --> 02:04:19.000
and gives you a preview of what this will look like

02:04:19.000 --> 02:04:20.000
when you render it.

02:04:20.000 --> 02:04:22.000
So let's render it real fast, shift, enter.

02:04:22.000 --> 02:04:24.000
And we see that this is indeed bolded.

02:04:24.000 --> 02:04:27.000
This two pound signs, or hash signs,

02:04:27.000 --> 02:04:29.000
means it's a H2 heading.

02:04:29.000 --> 02:04:32.000
So this is H1, this is H2,

02:04:32.000 --> 02:04:35.000
and it keeps getting smaller as you go down.

02:04:35.000 --> 02:04:37.000
So in this case, I think clean data just deserves

02:04:37.000 --> 02:04:39.000
a second level heading.

02:04:39.000 --> 02:04:41.000
We said we clean this data in the notebook stored in this.

02:04:41.000 --> 02:04:44.000
So deliver slash data cleaning IPYB.

02:04:44.000 --> 02:04:46.000
So we've told people where this cleaned data file

02:04:46.000 --> 02:04:48.000
actually sits.

02:04:48.000 --> 02:04:51.000
And we actually know the exact steps that went through

02:04:51.000 --> 02:04:53.000
to take it from the raw data into this cleaned data,

02:04:53.000 --> 02:04:55.000
which we've pointed to here.

02:04:55.000 --> 02:04:58.000
This head is actually quite a bit of text,

02:04:58.000 --> 02:05:00.000
even though it should be the top five lines.

02:05:00.000 --> 02:05:03.000
So if we're going to include something here to make sure

02:05:03.000 --> 02:05:05.000
that the data is read in correctly,

02:05:05.000 --> 02:05:08.000
we might select a few columns that we think are useful.

02:05:08.000 --> 02:05:13.000
So in this case, maybe we have year and maybe mine name.

02:05:13.000 --> 02:05:16.000
And so we read in just the heading with those two columns.

02:05:16.000 --> 02:05:19.000
Okay, just to give people a flavor of what's in that data frame.

02:05:19.000 --> 02:05:21.000
This length we don't need to worry about.

02:05:21.000 --> 02:05:23.000
This column thing we don't need to worry about.

02:05:23.000 --> 02:05:25.000
So we delete with 2Ds.

02:05:25.000 --> 02:05:27.000
Now, consider the different plots that you have included.

02:05:27.000 --> 02:05:29.000
And is this something that tells a story?

02:05:29.000 --> 02:05:31.000
If so, leave it in and clean it up so that the axes

02:05:31.000 --> 02:05:33.000
and the colors all look right.

02:05:33.000 --> 02:05:35.000
If not, you can go ahead and just delete it.

02:05:35.000 --> 02:05:37.000
So I think this is deleteable, also deleteable,

02:05:37.000 --> 02:05:39.000
and finally deleteable.

02:05:39.000 --> 02:05:41.000
Okay, so we get to the point where we're predicting

02:05:41.000 --> 02:05:42.000
the production of coal mines.

02:05:42.000 --> 02:05:44.000
Again, we're just looking at what the columns are.

02:05:44.000 --> 02:05:45.000
We don't need this.

02:05:45.000 --> 02:05:47.000
Don't need to know what unique year it is.

02:05:47.000 --> 02:05:50.000
So this is required code, so we need to leave this in.

02:05:50.000 --> 02:05:53.000
Again, clean it up if it needs to be broken up into different cells

02:05:53.000 --> 02:05:56.000
or if you think it needs to be changed in some other way.

02:05:56.000 --> 02:05:58.000
So let's delete a few of these empty ones.

02:05:58.000 --> 02:06:00.000
And let's say we want to like to keep this.

02:06:00.000 --> 02:06:04.000
Let's decide one of these violin plots to keep.

02:06:04.000 --> 02:06:06.000
So let's keep the second one.

02:06:06.000 --> 02:06:08.000
So I'm going to delete this one.

02:06:08.000 --> 02:06:11.000
And to save this, I will say plt.savefig

02:06:11.000 --> 02:06:14.000
and using tab complete and it'll help us know

02:06:14.000 --> 02:06:17.000
where proper structure to put this in here.

02:06:17.000 --> 02:06:20.000
And as I said before, I like to give the same name,

02:06:20.000 --> 02:06:23.000
beginning of the figure that the notebook itself has.

02:06:23.000 --> 02:06:27.000
So in this case, it starts with coal prediction

02:06:27.000 --> 02:06:29.000
as the starting of this notebook name.

02:06:29.000 --> 02:06:32.000
So that looking at this figures folder separately later on,

02:06:32.000 --> 02:06:34.000
someone knows which notebook it came from.

02:06:34.000 --> 02:06:37.000
And then what it's actually being plotted here.

02:06:37.000 --> 02:06:40.000
So we have company type versus log of production.

02:06:40.000 --> 02:06:43.000
So company type versus log production.

02:06:43.000 --> 02:06:46.000
Again, we get a warning, but this should work out just fine.

02:06:46.000 --> 02:06:49.000
Let's run this a second time to make sure everything.

02:06:49.000 --> 02:06:51.000
Okay, so that looks better.

02:06:51.000 --> 02:06:53.000
Running at the second time with the set context

02:06:53.000 --> 02:06:57.000
actually lets the font sizes get to a nice reasonable size.

02:06:57.000 --> 02:06:59.000
Okay, so we are saving this output.

02:06:59.000 --> 02:07:01.000
We think it's useful for our story.

02:07:01.000 --> 02:07:04.000
We again don't need this or looking at this.

02:07:04.000 --> 02:07:06.000
So just typing DD to delete these cells.

02:07:06.000 --> 02:07:08.000
We need to create the dummy categoricals.

02:07:08.000 --> 02:07:10.000
This is required for our analysis.

02:07:10.000 --> 02:07:14.000
We don't necessarily need to actually print the categoricals each time.

02:07:14.000 --> 02:07:16.000
So let's run this comment out that line

02:07:16.000 --> 02:07:19.000
and just double check that that is the same answer as before.

02:07:19.000 --> 02:07:21.000
Okay, let's delete that.

02:07:21.000 --> 02:07:24.000
And we've made a note here about avoiding dummy variable trap.

02:07:24.000 --> 02:07:27.000
You might decide that that needs to be elevated from a comment

02:07:27.000 --> 02:07:29.000
and to mark down cell above it.

02:07:29.000 --> 02:07:31.000
Okay, so let's leave it as a comment here.

02:07:31.000 --> 02:07:34.000
And we don't need to actually look at the categoricals for the final report.

02:07:34.000 --> 02:07:36.000
So let's just delete that.

02:07:36.000 --> 02:07:37.000
Build our model.

02:07:37.000 --> 02:07:39.000
Let's call it a little bit something more descriptive.

02:07:39.000 --> 02:07:42.000
So it's going to be a random forest regressor.

02:07:42.000 --> 02:07:45.000
And we should always put all of the imports

02:07:45.000 --> 02:07:47.000
all the way at the top of the notebook.

02:07:47.000 --> 02:07:50.000
And so let's move this to the top.

02:07:50.000 --> 02:07:53.000
But first let's combine a few of the other imports.

02:07:53.000 --> 02:07:55.000
I think I have a few more imports down here I do.

02:07:55.000 --> 02:07:58.000
So let's move this, I'm going to turn on the toolbar

02:07:58.000 --> 02:08:02.000
and move this up so that it's next to the previous one.

02:08:02.000 --> 02:08:07.000
I'll scroll back down and see if I can find another import.

02:08:07.000 --> 02:08:10.000
It looks like it should be everything.

02:08:10.000 --> 02:08:13.000
A keyboard shortcut that I find that I'm using all the time

02:08:13.000 --> 02:08:16.000
and really saves me time is knowing how to merge and split cells

02:08:16.000 --> 02:08:18.000
with keyboard shortcuts.

02:08:18.000 --> 02:08:21.000
Knowing this will save you tons of time with moving your mouse around.

02:08:21.000 --> 02:08:24.000
So we currently have input cell 30 selected.

02:08:24.000 --> 02:08:27.000
We can type shift and hold it down and then type K.

02:08:27.000 --> 02:08:29.000
We will now select the cell above it.

02:08:29.000 --> 02:08:32.000
We can select as many cells like this as we'd like

02:08:32.000 --> 02:08:35.000
or unselected by typing J to go back down.

02:08:35.000 --> 02:08:37.000
Also, if we go J from here,

02:08:37.000 --> 02:08:40.000
we can select down from the current cell that's selected.

02:08:40.000 --> 02:08:42.000
But let's go up shift K.

02:08:42.000 --> 02:08:46.000
We have selected two cells to merge this type shift M.

02:08:46.000 --> 02:08:48.000
So we've now merged those two cells together.

02:08:48.000 --> 02:08:50.000
Again, you can do that for 10, 20 cells,

02:08:50.000 --> 02:08:52.000
or you can easily split them again.

02:08:52.000 --> 02:08:55.000
I've said multiple times, control shift minus, splits in part,

02:08:55.000 --> 02:08:59.000
escape, shift K, shift M, merges them back together again.

02:08:59.000 --> 02:09:02.000
So this needs to go at the top of the notebook.

02:09:02.000 --> 02:09:05.000
So I will put this to the top by typing this up arrow.

02:09:05.000 --> 02:09:07.000
So bear with me for a second.

02:09:07.000 --> 02:09:09.000
And we need to merge these two cells

02:09:09.000 --> 02:09:11.000
and then do some recombination.

02:09:11.000 --> 02:09:15.000
So shift K, shift M, type return to get a cursor in the cell.

02:09:15.000 --> 02:09:17.000
And we're importing things from sklearn,

02:09:17.000 --> 02:09:19.000
which should go after C-Born,

02:09:19.000 --> 02:09:21.000
for this set command.

02:09:21.000 --> 02:09:24.000
Execute that, and everything looks good again.

02:09:24.000 --> 02:09:27.000
Let's scroll back down to where we've made our progress.

02:09:27.000 --> 02:09:30.000
Down to here, we don't need the length of our demi-categoricals.

02:09:30.000 --> 02:09:33.000
We do need to test train, split our data.

02:09:33.000 --> 02:09:35.000
Let's merge these two cells by typing shift J,

02:09:35.000 --> 02:09:38.000
shift M, and let's just leave that middle line.

02:09:38.000 --> 02:09:40.000
Execute, shift enter.

02:09:40.000 --> 02:09:43.000
And look at our final plot here.

02:09:43.000 --> 02:09:45.000
And this looks like a reasonable good plot.

02:09:45.000 --> 02:09:46.000
Thing looks nice.

02:09:46.000 --> 02:09:49.000
Let's save it out again into the figures directory.

02:09:49.000 --> 02:09:52.000
Let's call this coal production RF prediction.

02:09:52.000 --> 02:09:55.000
Great. So we've now saved this out.

02:09:55.000 --> 02:09:58.000
And we can do our various scores that we'd like to do.

02:09:58.000 --> 02:10:01.000
If we're going to be printing out this output for consumption,

02:10:01.000 --> 02:10:03.000
we should make this look a little bit prettier.

02:10:03.000 --> 02:10:06.000
So let's just do this first one.

02:10:06.000 --> 02:10:10.000
And let's combine these two cells.

02:10:10.000 --> 02:10:12.000
So now we have the R-squared score

02:10:12.000 --> 02:10:14.000
and our mean-squared error scores.

02:10:14.000 --> 02:10:16.000
And finally, our random forest importances.

02:10:16.000 --> 02:10:18.000
And let's just look at the top five.

02:10:18.000 --> 02:10:20.000
So the top five are labor hours all the way down.

02:10:20.000 --> 02:10:21.000
Cool.

02:10:21.000 --> 02:10:23.000
So we've done a lot of rearranging of the code.

02:10:23.000 --> 02:10:27.000
So at this point, I think it's crucial to restart the kernel

02:10:27.000 --> 02:10:29.000
and try to run the entire notebook again.

02:10:29.000 --> 02:10:32.000
If you have some process that actually takes a very long time,

02:10:32.000 --> 02:10:33.000
you can decide not to do that.

02:10:33.000 --> 02:10:35.000
But this, you'd have to take a little bit more care

02:10:35.000 --> 02:10:38.000
into making sure that each piece runs correctly.

02:10:38.000 --> 02:10:41.000
But in this case, this entire analysis runs very quickly.

02:10:41.000 --> 02:10:45.000
So we have no problem clearing all outputs and restarting.

02:10:45.000 --> 02:10:49.000
And clicking cell run all should run every single cell.

02:10:49.000 --> 02:10:52.000
If we've deleted some piece of code that was necessary,

02:10:52.000 --> 02:10:55.000
we'll have an error and we have to go back and correct that.

02:10:55.000 --> 02:10:59.000
Let's go through all the way down to the bottom thing was actually done.

02:10:59.000 --> 02:11:03.000
If there was an error, so let's say it would stop at this fifth cell here.

02:11:03.000 --> 02:11:06.000
It would have an error printout here and nothing else would be executed

02:11:06.000 --> 02:11:08.000
below that when you do this run all cells.

02:11:08.000 --> 02:11:11.000
That's a good way of identifying where the error happened.

02:11:11.000 --> 02:11:13.000
We don't have an error, thankfully, so that's good.

02:11:13.000 --> 02:11:17.000
We do have something that is somewhat annoying to me that this has to be run twice.

02:11:17.000 --> 02:11:21.000
As we can tell, we run this a second time we get our font gets bigger.

02:11:21.000 --> 02:11:23.000
So I think I know what happened.

02:11:23.000 --> 02:11:26.000
I set the context after I set the figure.

02:11:26.000 --> 02:11:29.000
So I'm going to re-align the order of these two pieces of code.

02:11:29.000 --> 02:11:35.000
Save this, restart the kernel, clear all outputs, cell run all.

02:11:35.000 --> 02:11:38.000
And now we see that the font size is the correct size

02:11:38.000 --> 02:11:40.000
and we've run all the way to the bottom.

02:11:40.000 --> 02:11:43.000
And each time we run this, do note that we are overwriting these figure files,

02:11:43.000 --> 02:11:45.000
which is what we were hoping to do,

02:11:45.000 --> 02:11:49.000
but also keep track that is what you indeed want to do when you're running this.

02:11:49.000 --> 02:11:52.000
I guess a good thing to add at the end, of course, would be some sort of conclusion,

02:11:52.000 --> 02:11:56.000
so we can just add a conclusion statement.

02:11:56.000 --> 02:11:59.000
Okay, so a detailed and amazing conclusion goes there.

02:11:59.000 --> 02:12:00.000
So we're done with this.

02:12:00.000 --> 02:12:04.000
We will close and halt and we need to submit this to GitHub.

02:12:04.000 --> 02:12:07.000
So we can do a get status back at our terminal.

02:12:07.000 --> 02:12:09.000
We've modified a figure.

02:12:09.000 --> 02:12:12.000
We have added a figure and we've created a new file.

02:12:12.000 --> 02:12:19.000
So let's add those type get status to make sure we know what we're doing.

02:12:19.000 --> 02:12:21.000
We are adding two new files or modifying another file.

02:12:21.000 --> 02:12:22.000
This looks good.

02:12:22.000 --> 02:12:30.000
So get commit, get push, origin, Jonathan prediction production.

02:12:30.000 --> 02:12:33.000
And this should be sent up to GitHub and everything is now

02:12:33.000 --> 02:12:34.000
up to date.

02:12:34.000 --> 02:12:36.000
So let's go to our get repository.

02:12:36.000 --> 02:12:39.000
So in my case, JBWit Coal Exploration.

02:12:39.000 --> 02:12:42.000
And there is a new branch which we can click on.

02:12:42.000 --> 02:12:48.000
And if we click on the deliver, we should be able to see our Coal Prediction Production Notebook,

02:12:48.000 --> 02:12:54.000
including all of the code and everything else in here.

02:12:54.000 --> 02:13:01.000
In this video, we'll be talking about how to do a pull request and how to merge this back into a final branch

02:13:01.000 --> 02:13:04.000
so the team members can review it and check off on it.

02:13:04.000 --> 02:13:05.000
All right.

02:13:05.000 --> 02:13:07.000
So we last left us.

02:13:07.000 --> 02:13:12.000
We had just put in our deliverable notebook that talks about the Coal Prediction Production.

02:13:12.000 --> 02:13:17.000
And so at this point, after pushing it to master, we have this branch.

02:13:17.000 --> 02:13:23.000
If you go back to the home directory under your username and whatever you've named your data science project,

02:13:23.000 --> 02:13:26.000
you can actually see this button here called new pull request.

02:13:26.000 --> 02:13:31.000
And I like to switch to the branch that I'm going to generate the pull request from.

02:13:31.000 --> 02:13:36.000
So this is all predicated on using GitHub as your repository of choice.

02:13:36.000 --> 02:13:43.000
So after you click new pull request, you'll ask you to do one last step here where it'll say you're creating a new pull request

02:13:43.000 --> 02:13:45.000
by comparing changes across two branches.

02:13:45.000 --> 02:13:51.000
You're going to be taking stuff from this Jonathan Predict Production branch and putting it into master.

02:13:51.000 --> 02:13:54.000
And GitHub does this nice thing which says it's able to be merged,

02:13:54.000 --> 02:13:58.000
which means that if it's approved, it can just be approved at the single button click.

02:13:58.000 --> 02:13:59.000
That's always nice.

02:13:59.000 --> 02:14:03.000
So give your commit an extra bit of detail here.

02:14:03.000 --> 02:14:06.000
So say something like final review.

02:14:06.000 --> 02:14:11.000
And then if you want to leave a few more comments, create pull request.

02:14:11.000 --> 02:14:18.000
So now what this does is it creates a pull request and lets you see the various commits that have happened in this branch

02:14:18.000 --> 02:14:23.000
and allows a person who can possibly merge this to review the pull request.

02:14:23.000 --> 02:14:31.000
So a person coming into this would see who's not me, for example, would look at the pull request and see that there's one open pull request.

02:14:31.000 --> 02:14:34.000
And it was open 25 seconds ago by me.

02:14:34.000 --> 02:14:37.000
So if you click on this, then you'll see the comment here.

02:14:37.000 --> 02:14:38.000
Please check the figures especially.

02:14:38.000 --> 02:14:40.000
They're going to be put into a slideshow.

02:14:40.000 --> 02:14:41.000
Okay, so this must be pretty important.

02:14:41.000 --> 02:14:45.000
And so I'll take a look at the different files that were committed.

02:14:45.000 --> 02:14:47.000
So I'll click to the files changed.

02:14:47.000 --> 02:14:52.000
I see that we have a couple of notebooks and we have a couple of figures.

02:14:52.000 --> 02:14:56.000
So let's take a look at, let's say this current figure here.

02:14:56.000 --> 02:14:57.000
This one was added.

02:14:57.000 --> 02:15:00.000
Let's say we want to change that color.

02:15:00.000 --> 02:15:03.000
So in the pull request, you can actually make changes.

02:15:03.000 --> 02:15:05.000
And this is where you actually wanted to be doing this.

02:15:05.000 --> 02:15:13.000
You click in the conversation part of the pull request, say, I need a few changes.

02:15:13.000 --> 02:15:14.000
Add a comment.

02:15:14.000 --> 02:15:17.000
Now, of course, I'm commenting on my own pull request.

02:15:17.000 --> 02:15:21.000
Normally what happens is you make a pull request and your team members or your

02:15:21.000 --> 02:15:24.000
manager will be actually the one reviewing the pull request.

02:15:24.000 --> 02:15:28.000
But in this case, just for demonstration purposes, I'm both the submitter and the

02:15:28.000 --> 02:15:31.000
reviewer just so that it's easy to see what needs to happen.

02:15:31.000 --> 02:15:32.000
So added a comment.

02:15:32.000 --> 02:15:33.000
I need a few changes.

02:15:33.000 --> 02:15:35.000
Please change the figure to be green.

02:15:35.000 --> 02:15:36.000
Okay.

02:15:36.000 --> 02:15:41.000
Now that we go back to our terminal, we see that still on the Jonathan prediction

02:15:41.000 --> 02:15:46.000
production branch, so we'll need to make some changes to the pull request.

02:15:46.000 --> 02:15:48.000
So this is actually pretty simple.

02:15:48.000 --> 02:15:53.000
So I'm going to switch tabs back to our deliver directory that is running under the

02:15:53.000 --> 02:15:55.000
Jupyter Notebook server.

02:15:55.000 --> 02:15:59.000
And so let's go into this cold predict production and make the requisite changes.

02:15:59.000 --> 02:16:03.000
Now we'll have to actually shift return and work our way through this so that

02:16:03.000 --> 02:16:05.000
everything is loaded into the namespace.

02:16:05.000 --> 02:16:09.000
So that one is probably the one that should stay the same.

02:16:09.000 --> 02:16:14.000
We get down to this one here where sure enough, the figure itself is printing

02:16:15.000 --> 02:16:16.000
something that's blue.

02:16:16.000 --> 02:16:19.000
We want to change this color to be green.

02:16:19.000 --> 02:16:20.000
Okay.

02:16:20.000 --> 02:16:24.000
In this plot, we will actually make the color equal to green.

02:16:24.000 --> 02:16:27.000
C is not what it takes as a thing.

02:16:27.000 --> 02:16:30.000
So we'll see if color works and color is indeed the keyword.

02:16:30.000 --> 02:16:31.000
Okay.

02:16:31.000 --> 02:16:35.000
So changing the color to be green, the figure is now green and we have

02:16:35.000 --> 02:16:37.000
overwritten that figure file.

02:16:37.000 --> 02:16:42.000
So cold production RF prediction is now a green plot rather than blue.

02:16:42.000 --> 02:16:47.000
And so we can want to redo everything just to make sure that you haven't made

02:16:47.000 --> 02:16:48.000
any catastrophic changes.

02:16:48.000 --> 02:16:50.000
You can do this one more time.

02:16:50.000 --> 02:16:57.000
Takes just a few seconds to go through the entire pipeline and save this file

02:16:57.000 --> 02:16:58.000
close and halt.

02:16:58.000 --> 02:17:01.000
Go back to your terminal, get status.

02:17:01.000 --> 02:17:02.000
Two things have been changed.

02:17:02.000 --> 02:17:03.000
And that's as we expect.

02:17:03.000 --> 02:17:07.000
They changed the notebook itself that created this figure and the figure

02:17:07.000 --> 02:17:08.000
itself.

02:17:08.000 --> 02:17:10.000
So let's add those two files.

02:17:10.000 --> 02:17:14.000
Those two files have been modified.

02:17:14.000 --> 02:17:20.000
So we then get push origin, your branch name, and it's now updated on GitHub.

02:17:20.000 --> 02:17:24.000
The nice thing about how GitHub handles these pull requests as a tab back to this

02:17:24.000 --> 02:17:27.000
Chrome tab, this commit is already added now.

02:17:27.000 --> 02:17:29.000
You actually can see the commit that was done here.

02:17:29.000 --> 02:17:32.000
And if you click on that commit, you get to see that things that were changed.

02:17:32.000 --> 02:17:36.000
So the few things were changed in the IPYNB, which is not shown partly because

02:17:36.000 --> 02:17:39.000
the actual changes in the notebook don't look so great.

02:17:39.000 --> 02:17:43.000
But the change in the figures has been changed.

02:17:43.000 --> 02:17:45.000
So this figure, the blue one was deleted.

02:17:45.000 --> 02:17:48.000
And the one on the right, the green one was added.

02:17:48.000 --> 02:17:51.000
So this is one of the reasons that changing it in the notebook, which it

02:17:51.000 --> 02:17:52.000
actually did.

02:17:52.000 --> 02:17:54.000
So it changed the embedded figure in the notebook.

02:17:54.000 --> 02:17:56.000
It's hard to see the differences there.

02:17:56.000 --> 02:18:00.000
This is why I advocate creating these figures in a separate folder and a separate

02:18:00.000 --> 02:18:02.000
PNG file for each of them.

02:18:02.000 --> 02:18:06.000
So you see the diffs in the figures if you have feedback on the output.

02:18:06.000 --> 02:18:10.000
Now, as an extra piece of sugar or something nice that GitHub has given us,

02:18:10.000 --> 02:18:14.000
there's this to side by side approach where you can see what was deleted and

02:18:14.000 --> 02:18:15.000
see what was added.

02:18:15.000 --> 02:18:20.000
You can also choose the swipe option where as you swipe this thing across the

02:18:20.000 --> 02:18:23.000
figure that you've just done, you can actually see the changes that have been

02:18:23.000 --> 02:18:25.000
made, which is turning the figure green.

02:18:25.000 --> 02:18:29.000
Last one is onion skin where it fades from the entire thing from behind.

02:18:29.000 --> 02:18:31.000
So this is what it currently is.

02:18:31.000 --> 02:18:33.000
And previously it was blue.

02:18:33.000 --> 02:18:34.000
You can see this.

02:18:34.000 --> 02:18:37.000
So having this functionality is actually really nice.

02:18:37.000 --> 02:18:43.000
And another reason why I advocate for this figures being submitted separately.

02:18:43.000 --> 02:18:47.000
Just a final note, you saw that the points are slightly different in this swipe.

02:18:47.000 --> 02:18:51.000
And that's because during our test train split, we were taking a random selection

02:18:51.000 --> 02:18:55.000
of points that were going to be the testing set and the training set.

02:18:55.000 --> 02:18:59.000
So those differences, well, shouldn't matter much and they don't change the

02:18:59.000 --> 02:19:02.000
actual scatter points, but the fit itself, as you can tell is almost completely

02:19:02.000 --> 02:19:03.000
unchanged.

02:19:03.000 --> 02:19:06.000
It's actually a nice robustness check to look at this as well.

02:19:06.000 --> 02:19:12.000
So once I've looked at these changes, I can now go back to this pull request

02:19:12.000 --> 02:19:13.000
branch.

02:19:13.000 --> 02:19:14.000
So I need a few changes.

02:19:14.000 --> 02:19:15.000
Please make the figure green.

02:19:15.000 --> 02:19:17.000
I committed made the figure green.

02:19:17.000 --> 02:19:22.000
The only thing I need to do to update this whole threat of changes was to just

02:19:22.000 --> 02:19:25.000
say get push origin branch title.

02:19:25.000 --> 02:19:29.000
So I'll say it looks good to me plus one.

02:19:30.000 --> 02:19:34.000
And then clicking merge pull request will take everything from this branch

02:19:34.000 --> 02:19:36.000
and pull it into the master branch.

02:19:36.000 --> 02:19:41.000
So I'll say figures ready to be put into a slideshow.

02:19:41.000 --> 02:19:46.000
So once you pull request is successfully merged and accepted, then you should

02:19:46.000 --> 02:19:49.000
delete the branch to keep these branches from floating around.

02:19:49.000 --> 02:19:53.000
So I just deleted the branch on GitHub and should now do the same thing

02:19:53.000 --> 02:19:55.000
in your local environment.

02:19:55.000 --> 02:19:59.000
So first I'm going to check out master and I'll say get pull origin master

02:19:59.000 --> 02:20:03.000
to pull everything down from GitHub and all these changes have been made

02:20:03.000 --> 02:20:07.000
and say get branch minus D Jonathan predict.

02:20:07.000 --> 02:20:11.000
So I've deleted the prediction branch and get does a final check to make sure

02:20:11.000 --> 02:20:14.000
that any of the changes that have been made on that prediction branch have

02:20:14.000 --> 02:20:15.000
been already pulled into master.

02:20:15.000 --> 02:20:18.000
So if you just try to do this and it doesn't think it's been fully merged,

02:20:18.000 --> 02:20:21.000
you get an error at that point and you have to figure out what happened at

02:20:21.000 --> 02:20:22.000
that stage.

02:20:22.000 --> 02:20:26.000
In this video, we just over reviewed the basic process of going through a

02:20:26.000 --> 02:20:30.000
pull request and how the peer review process works in a pull request.

02:20:30.000 --> 02:20:38.000
So we saw how to merge our development branch into master after doing a pull request.

02:20:38.000 --> 02:20:43.000
In this video, we'll start our data science project number two.

02:20:43.000 --> 02:20:47.000
And in this project, our main focus will be to focus on various plotting

02:20:47.000 --> 02:20:50.000
and statistical libraries that I think you should know about.

02:20:50.000 --> 02:20:55.000
All right, so to start a new data science project, let's start out by

02:20:55.000 --> 02:21:01.000
going to GitHub and signing in going up to the plus by our little icon and

02:21:01.000 --> 02:21:03.000
clicking on a new repository.

02:21:03.000 --> 02:21:11.000
So we can call this data vis project to in this case and give it a

02:21:11.000 --> 02:21:15.000
description that says I will make it public.

02:21:15.000 --> 02:21:18.000
So you can see this project as you go forward.

02:21:18.000 --> 02:21:20.000
We'll initialize with a read me.

02:21:20.000 --> 02:21:23.000
We will include a Python dot get ignore.

02:21:23.000 --> 02:21:28.000
We'll add an MIT license and create the repository.

02:21:28.000 --> 02:21:33.000
Once we've created it, go to this SSH option, click in this box.

02:21:33.000 --> 02:21:37.000
It'll select all the text by default command C copies it, go back to our

02:21:37.000 --> 02:21:42.000
terminal, say get clone and then command V to paste that URL.

02:21:42.000 --> 02:21:47.000
All right, so let's CD into data vis projects and look at what we have here.

02:21:47.000 --> 02:21:49.000
And we're currently on the master branch.

02:21:49.000 --> 02:21:54.000
So first step, let's create a development branch and we'll call it

02:21:54.000 --> 02:21:59.000
Jonathan vis and let's create our normal directory structure.

02:21:59.000 --> 02:22:05.000
So we have data deliver develop figures start with and I happen to

02:22:05.000 --> 02:22:09.000
know that I've already started a few of these notebooks.

02:22:09.000 --> 02:22:14.000
So I'll move them from a previous location into our develop folder.

02:22:14.000 --> 02:22:16.000
So let's look at our develop folder.

02:22:16.000 --> 02:22:17.000
Okay, we got some stuff there.

02:22:17.000 --> 02:22:21.000
And now that we have a new branch and we have a new directory structure

02:22:21.000 --> 02:22:22.000
and some stuff to look at.

02:22:22.000 --> 02:22:25.000
Let's start up the Jupyter notebook server.

02:22:25.000 --> 02:22:28.000
All right, so we see the same directories we were just looking at in the

02:22:28.000 --> 02:22:29.000
terminal.

02:22:29.000 --> 02:22:33.000
I will now right click on this tab and pin this tab so that it goes all the

02:22:33.000 --> 02:22:36.000
way to the left and stays in place so that if I have a lot of tabs because

02:22:36.000 --> 02:22:39.000
I'm searching for a bunch of different things, I always know where to go

02:22:39.000 --> 02:22:41.000
back to find the home server directory.

02:22:41.000 --> 02:22:44.000
And I just find that useful to pin that tab all the way to the left.

02:22:44.000 --> 02:22:47.000
All right, so let's take a look at some of the notebooks.

02:22:47.000 --> 02:22:53.000
I've already pre populated what I'll do here is I only have my usual date

02:22:53.000 --> 02:22:57.000
and then my initials at the top of the page from the actual name of my

02:22:57.000 --> 02:23:00.000
notebook, just including your short description, which is exploratory

02:23:00.000 --> 02:23:02.000
data analysis, which is pretty long title.

02:23:02.000 --> 02:23:06.000
So I'll do all caps EDA and that is a standard way of talking about that.

02:23:06.000 --> 02:23:11.000
So I'll view and toggle the header and toggle the toolbar just so that we

02:23:11.000 --> 02:23:12.000
have some extra space.

02:23:12.000 --> 02:23:15.000
Remember, if you want to save it when you're in this kind of configuration,

02:23:15.000 --> 02:23:17.000
you just command S to save it.

02:23:17.000 --> 02:23:20.000
So one more time, I'll just give you a brief overview of what I'm hoping to

02:23:20.000 --> 02:23:21.000
do here.

02:23:21.000 --> 02:23:23.000
So this isn't to teach you how to do data science.

02:23:23.000 --> 02:23:27.000
It's more of an exposure to the tools that I think most people haven't seen

02:23:27.000 --> 02:23:29.000
all of them or haven't seen enough of them.

02:23:29.000 --> 02:23:34.000
And I just think these tools will allow you to do your data science much

02:23:34.000 --> 02:23:37.000
more efficiently and usefully.

02:23:37.000 --> 02:23:40.000
I'll go over a few of these plotting and statistical packages that you might

02:23:40.000 --> 02:23:41.000
not know about.

02:23:41.000 --> 02:23:44.000
So the first thing we have is importing map plot lib inline.

02:23:44.000 --> 02:23:47.000
Almost all these plotting libraries uses map plot lib.

02:23:47.000 --> 02:23:48.000
So I'll be using that for now.

02:23:48.000 --> 02:23:52.000
And I'm importing map plot lib dot pie plot as PLT, which is the

02:23:52.000 --> 02:23:53.000
standard way of doing that.

02:23:53.000 --> 02:23:57.000
Seaborn as SNS, which is the standard way of importing Seaborn,

02:23:57.000 --> 02:24:00.000
importing pandas as PD, NumPy as NP.

02:24:00.000 --> 02:24:04.000
I'll also load in some data sets from scikit-learn and importing some

02:24:04.000 --> 02:24:08.000
stats models, which I'll be talking about at length in a later video.

02:24:08.000 --> 02:24:09.000
Execute this cell.

02:24:09.000 --> 02:24:12.000
Now, if I do shift return, it will execute it and go to the next cell.

02:24:12.000 --> 02:24:16.000
If I hold down control and hit return, it will execute the cell in place,

02:24:16.000 --> 02:24:18.000
and it won't go to the next cell.

02:24:18.000 --> 02:24:22.000
So I can continue to stay in the same cell if I hit control and return.

02:24:22.000 --> 02:24:26.000
I've used Seaborn in other videos, but I would really like to just double

02:24:26.000 --> 02:24:28.000
emphasize how useful this is.

02:24:28.000 --> 02:24:32.000
You can find the main library for this by Google searching Seaborn,

02:24:32.000 --> 02:24:35.000
and Seaborn Python should do it.

02:24:35.000 --> 02:24:39.000
And the top result is the statistical data visualization library here.

02:24:39.000 --> 02:24:42.000
This is what you should see, something like this, unless he's updated the page.

02:24:42.000 --> 02:24:45.000
And this website has a lot of really good information on it.

02:24:45.000 --> 02:24:46.000
The documentation is excellent.

02:24:46.000 --> 02:24:49.000
The features with these different tutorials is also excellent.

02:24:49.000 --> 02:24:53.000
These images that you can click on here will show you different capability,

02:24:53.000 --> 02:24:55.000
the tutorial and the gallery.

02:24:55.000 --> 02:24:58.000
If you click on gallery, you get to see many different visualization types

02:24:58.000 --> 02:25:01.000
that Seaborn makes really easy, especially like heat map.

02:25:01.000 --> 02:25:03.000
That's a nice one.

02:25:03.000 --> 02:25:05.000
Look through the example gallery.

02:25:05.000 --> 02:25:08.000
If you have some data and you have some sense that you should be able to visualize it in a way,

02:25:08.000 --> 02:25:11.000
see if Seaborn has a response to that.

02:25:11.000 --> 02:25:14.000
So let's go back to our notebook and load in some data.

02:25:14.000 --> 02:25:18.000
So Seaborn SNS has data sets that you can load in by default.

02:25:18.000 --> 02:25:21.000
We will load in the Titanic data set.

02:25:21.000 --> 02:25:25.000
This is actually the data of passengers on the ill-fated Titanic.

02:25:25.000 --> 02:25:31.000
And it has various information about them, their age, their sex, their class of ticket.

02:25:31.000 --> 02:25:33.000
So first class, second class, third class.

02:25:33.000 --> 02:25:36.000
And it talks about whether or not they survived the crash.

02:25:36.000 --> 02:25:42.000
So doing a factor plot like this where you set this G object to be equal to this factor plot

02:25:42.000 --> 02:25:45.000
and then modify the G label like this.

02:25:45.000 --> 02:25:50.000
This is modified from a Seaborn example, commenting out this hue equals sex line.

02:25:50.000 --> 02:25:52.000
And I'll talk about that in a second.

02:25:52.000 --> 02:25:55.000
But I will shift return and execute this cell.

02:25:55.000 --> 02:26:02.000
What you see here is the survival probability against the class of passengers on Titanic held.

02:26:02.000 --> 02:26:05.000
You can see that first class had by far the best survival probability,

02:26:05.000 --> 02:26:08.000
followed by second, followed finally by third class.

02:26:08.000 --> 02:26:13.000
So this is a very nice high level summary of the data that underlies this.

02:26:13.000 --> 02:26:19.000
Some of the nice things about Seaborn is that you can actually give it dimensions to also give you the same plot.

02:26:19.000 --> 02:26:22.000
So let's uncomment this hue equals sex line and see what that does.

02:26:22.000 --> 02:26:26.000
So what you see here is each of these classes is now been split out by sex.

02:26:26.000 --> 02:26:29.000
So male and female, survivability for first class.

02:26:29.000 --> 02:26:33.000
You can tell the very high difference in probability for surviving in each of those,

02:26:33.000 --> 02:26:36.000
whether you're male or female in each of the classes.

02:26:36.000 --> 02:26:41.000
So this tells you a more rich and deeper story of the underlying data set than the previous plot.

02:26:41.000 --> 02:26:45.000
And you can see the first, second, third class, all of the different responses here.

02:26:45.000 --> 02:26:48.000
So this is just one aspect of Seaborn.

02:26:48.000 --> 02:26:51.000
I recommend getting to know it and use it as much as you can.

02:26:51.000 --> 02:26:54.000
And that's going to be all for this video.

02:26:54.000 --> 02:26:57.000
We've set up in this video a new Git repository.

02:26:57.000 --> 02:26:59.000
We've started a new development branch.

02:26:59.000 --> 02:27:03.000
We have our directory structure set up as we like to do it for our data science projects.

02:27:03.000 --> 02:27:09.000
And we've taken a look at the Seaborn visualization library.

02:27:09.000 --> 02:27:14.000
In this video, we'll continue to look at some visualization methods and techniques.

02:27:14.000 --> 02:27:17.000
So let's go on to exploratory data analysis two.

02:27:17.000 --> 02:27:24.000
Again, it starts off the same way with Matplotlib inline and the various other things being imported.

02:27:24.000 --> 02:27:26.000
This warning message, which we can ignore for now.

02:27:26.000 --> 02:27:30.000
So we will load in this Boston data from the scikit-learn data sets.

02:27:30.000 --> 02:27:35.000
And we will first of all print what the data dictionary describes it as.

02:27:35.000 --> 02:27:40.000
The way this load Boston gets imported, I'm calling it a data frame dictionary

02:27:40.000 --> 02:27:43.000
and just calling this description key.

02:27:43.000 --> 02:27:49.000
So let's toggle the top header and the top toolbar to give us some extra space.

02:27:49.000 --> 02:27:53.000
And we see that this is the Boston house prices data set.

02:27:53.000 --> 02:27:58.000
Now, it's worth reading through this data set and knowing what each of these attributes actually means

02:27:58.000 --> 02:28:02.000
because if we're doing a deep data science project, it's really important to know the attributes,

02:28:02.000 --> 02:28:04.000
especially if there's only 13 of them.

02:28:04.000 --> 02:28:09.000
But what the main takeaway will be trying to predict the median value of the house

02:28:09.000 --> 02:28:14.000
and by looking at the 13 categories that predict this house,

02:28:14.000 --> 02:28:17.000
we have 506 total instances of this data set.

02:28:17.000 --> 02:28:22.000
The different attributes are crime, we've written as CRIM, all caps.

02:28:22.000 --> 02:28:27.000
Zone or the proportion of residential land zone for lots over 25,000 square feet.

02:28:27.000 --> 02:28:32.000
Indus, which is a proportion of non-retail business acres per town.

02:28:32.000 --> 02:28:38.000
A dummy variable where if you're next to the Charles River, then you're equaling to one, otherwise you're zero.

02:28:38.000 --> 02:28:41.000
The nitric oxides concentration in parts per 10 million.

02:28:41.000 --> 02:28:43.000
The average number of rooms per dwelling.

02:28:43.000 --> 02:28:48.000
The proportion of owner occupied units built prior to 1940, which is age,

02:28:48.000 --> 02:28:53.000
weighted distances to five Boston employment centers, distance.

02:28:53.000 --> 02:28:56.000
Rad is index of accessibility to radial highways.

02:28:56.000 --> 02:29:01.000
Tax, the full value property tax per $10,000.

02:29:01.000 --> 02:29:04.000
People to teacher ratio by town.

02:29:04.000 --> 02:29:09.000
The B, which is the formula that says the BK is the proportion of blacks by town.

02:29:09.000 --> 02:29:12.000
L stat, which is percentage of lower status of the population.

02:29:12.000 --> 02:29:16.000
And median value, the thing we are tending to be predicting,

02:29:16.000 --> 02:29:20.000
which is median value of the owner occupied home in terms of 1000s.

02:29:20.000 --> 02:29:22.000
This is the information that the data comes from.

02:29:22.000 --> 02:29:24.000
So it's from Harrison and Rubenfeld.

02:29:24.000 --> 02:29:31.000
And this is all the information about exactly where it was taken from the stat lab library maintained at Carnegie Mellon University.

02:29:31.000 --> 02:29:36.000
So this data dictionary as it comes from scikit-learn is not in my favorite format.

02:29:36.000 --> 02:29:38.000
It's this weird data dictionary.

02:29:38.000 --> 02:29:41.000
If we actually say type on this, it'll be this weird like data set bunch.

02:29:41.000 --> 02:29:44.000
So instead of using it in the form that it's given to us,

02:29:44.000 --> 02:29:49.000
I like to convert this into a panda's data frame because those in my view are much easier to use.

02:29:49.000 --> 02:29:52.000
So we'll create a data frame called features.

02:29:52.000 --> 02:29:54.000
I'll create a data frame called target.

02:29:54.000 --> 02:29:59.000
Now features will take the DF underscore dict, which is the the scikit-learn bunch thing.

02:29:59.000 --> 02:30:05.000
And the dot data element and assign the columns to this data frame to be the feature names.

02:30:05.000 --> 02:30:07.000
We'll also do this with target.

02:30:07.000 --> 02:30:11.000
So we'll do this with another create another pandas dot data frame to create the data frame.

02:30:11.000 --> 02:30:14.000
And then it'll be this DF dict dot target.

02:30:14.000 --> 02:30:18.000
So run this and we can look at the head of the features by doing dot head on it.

02:30:18.000 --> 02:30:24.000
So here are the different values of the different features for the first five elements of our data set.

02:30:24.000 --> 02:30:28.000
We have the crime number here, zone, the industry.

02:30:28.000 --> 02:30:33.000
Are you close to the Charles River, the nitrous oxide, average number of rooms, the age,

02:30:33.000 --> 02:30:36.000
all the different features that we're reading about before.

02:30:36.000 --> 02:30:42.000
If we look at the target, we would see that it's a single element or a single column data frame.

02:30:42.000 --> 02:30:45.000
So what we'll like to do is actually for most of our visualization,

02:30:45.000 --> 02:30:48.000
we will like to put these two things together side by side.

02:30:48.000 --> 02:30:51.000
Well, we can use concat for that pandas dot concat.

02:30:51.000 --> 02:30:54.000
We give it a list of the data frames you'd like to concatenate together.

02:30:54.000 --> 02:30:57.000
And we have to tell it which axis that we would like to use.

02:30:57.000 --> 02:31:02.000
Now, I'm sure there's some very useful mnemonic that will tell us the right way to do it every time,

02:31:02.000 --> 02:31:07.000
but I prefer to not trust that I remembered it correctly, but always test that I have it right.

02:31:07.000 --> 02:31:10.000
So if we start out with axis equals zero and look at the head,

02:31:10.000 --> 02:31:15.000
we will see that it's trying to combine it in a way that they're stacked on top of each other.

02:31:15.000 --> 02:31:16.000
And there's two ways to know this.

02:31:16.000 --> 02:31:21.000
One is that everything has a value except for medv, which is the target data frame.

02:31:21.000 --> 02:31:22.000
All of them have nans.

02:31:22.000 --> 02:31:29.000
And if we were to look at the tail, we will see that everything else has nans and medv has values.

02:31:29.000 --> 02:31:31.000
That's one way to know that we've done it wrong.

02:31:31.000 --> 02:31:35.000
So this is trying to do some sort of concatenating the two data frames vertically.

02:31:35.000 --> 02:31:40.000
And if we do it axis equals one, we will see that we've put them side by side,

02:31:40.000 --> 02:31:41.000
which is what we actually want.

02:31:41.000 --> 02:31:42.000
And let's look at the head.

02:31:42.000 --> 02:31:47.000
We will see that all of them are here, including medv being the very final column in this data frame.

02:31:47.000 --> 02:31:49.000
So we now have a new data frame called df.

02:31:49.000 --> 02:31:53.000
It contains a target and the feature variables underneath it.

02:31:53.000 --> 02:31:58.000
Now to give you a sense of the data underneath it, there's many different ways you can slice and dice this.

02:31:58.000 --> 02:32:03.000
One very simple quick way to start with is to iterate over all of the columns of the data frame

02:32:03.000 --> 02:32:08.000
and to print both the column name and the number of unique values in that column.

02:32:08.000 --> 02:32:16.000
For column in df, the data frame columns, print the column name and df of the column, the number of unique values.

02:32:16.000 --> 02:32:20.000
This n unique is a method you can call on a data frame.

02:32:20.000 --> 02:32:27.000
So there are 504 unique values in crime and there's two totally unique values in chance, which is a boolean value.

02:32:27.000 --> 02:32:29.000
Makes sense, we'd expect that.

02:32:29.000 --> 02:32:30.000
Some of them are pretty low.

02:32:30.000 --> 02:32:32.000
So our ad, for example, is at nine.

02:32:32.000 --> 02:32:35.000
Some of these have many values and they're continuous values.

02:32:35.000 --> 02:32:38.000
Other of them have smaller numbers of possible values.

02:32:38.000 --> 02:32:41.000
You can see rad here is this kind of numbers here.

02:32:41.000 --> 02:32:48.000
One thing you might not know is that pandas not only has fantastic data frame support, but also has some very useful plotting tools.

02:32:48.000 --> 02:32:54.000
So in this case, we will be importing a thing called scatter matrix from pandas.

02:32:54.000 --> 02:32:58.000
And this can be done in a couple of libraries as well, but let's just look at the pandas version of this.

02:32:58.000 --> 02:33:04.000
Recreating a figure with some plots in pi plot, making a large figure 12 by 12 fig size.

02:33:04.000 --> 02:33:15.000
And we're going to call it on this data frame with some see-through value of alpha and the diagonal will be KDE, which is this kernel density estimation plot that we see here.

02:33:15.000 --> 02:33:22.000
Again, we see a warning that we can safely ignore, but this is a very information dense plot.

02:33:22.000 --> 02:33:25.000
There's no way to go over all of it in this video as we look at it.

02:33:25.000 --> 02:33:29.000
But this, if you have your own data set, will give you a lot of things to look at.

02:33:29.000 --> 02:33:37.000
What is being plotted here on the x-axis and the y-axis is every possible pair of the two columns in this data frame, which is why it took a while to actually plot this.

02:33:37.000 --> 02:33:45.000
Along the diagonal, this KDE plot, it's showing interactions with itself or basically the histogram of that variable itself.

02:33:45.000 --> 02:33:49.000
So this is what medv looks like. It's just this histogram here.

02:33:49.000 --> 02:33:53.000
Along the diagonal, it's just a histogram of the values of that variable.

02:33:53.000 --> 02:34:01.000
Everything else is going to be what the response from this variable looks like with every other variable on the x-axis.

02:34:01.000 --> 02:34:03.000
So you can see a number of really nice trends here.

02:34:03.000 --> 02:34:05.000
You can see some kind of this U-shaped trend here.

02:34:05.000 --> 02:34:09.000
We see something that's basically a straight line, which means there's not much information there at all.

02:34:09.000 --> 02:34:11.000
That's from the Boolean value.

02:34:11.000 --> 02:34:17.000
We can see some of these have very fuzzy relationships where it's not really showing anything very interesting.

02:34:17.000 --> 02:34:23.000
But spending some time looking at plots like this, getting to know your data set is a vital part of data science.

02:34:23.000 --> 02:34:25.000
And I highly recommend looking at this.

02:34:25.000 --> 02:34:29.000
If you have far too many columns to look at it in one, I would say this is probably too many.

02:34:29.000 --> 02:34:34.000
If you have even more than this, though, you can take subsets of this and plot this with the same command,

02:34:34.000 --> 02:34:41.000
but you would be giving it a list inside of double brackets of feature one, feature two, and so on.

02:34:41.000 --> 02:34:43.000
And this will plot just those features against each other.

02:34:43.000 --> 02:34:46.000
So there's a downside of that is that you're not getting all of the interaction terms,

02:34:46.000 --> 02:34:53.000
but if it's a trade-off between possible to view in one screen or not look at it at all, I recommend that.

02:34:55.000 --> 02:34:59.000
Okay, in the last video, we last looked at this scatterplot functionality within Pandas.

02:34:59.000 --> 02:35:07.000
In this video, we're going to continue taking a look at this data and some of the plotting functionality that's built into the Pandas library itself.

02:35:07.000 --> 02:35:16.000
Just as a brief overview, again, this scatterplot gives you a very nice, fast way of looking at all of the interactions between the terms in your data frame.

02:35:16.000 --> 02:35:22.000
If you suspect that there might be something interesting going on with, let's say, rad, we see something happening here,

02:35:22.000 --> 02:35:26.000
or this diagonal term for rad, the intersection of rad and rad on the X and Y axis.

02:35:26.000 --> 02:35:32.000
You see a histogram plot or a KDE plot that shows a very bimodal distribution.

02:35:32.000 --> 02:35:40.000
So you can take a deeper look into that and see what it looks like by selecting that column by saying df of rad.hist.

02:35:40.000 --> 02:35:44.000
And we will see this bimodal shape really appear again.

02:35:44.000 --> 02:35:49.000
So it's really values that are greater than 20 and then a bunch of different values that are around 10 and lower.

02:35:49.000 --> 02:35:53.000
You can also, of course, select it if you have a nice column name.

02:35:53.000 --> 02:35:56.000
In other words, there's no spaces or any other characters in that column name.

02:35:56.000 --> 02:36:00.000
You do the same exact thing by doing df.rad.hist.

02:36:00.000 --> 02:36:02.000
See the same exact plot.

02:36:02.000 --> 02:36:07.000
When you see a feature like this, in this case, it might not make sense, but if you have the thought that,

02:36:07.000 --> 02:36:10.000
you know what, let's actually consider this as two separate groups.

02:36:10.000 --> 02:36:15.000
This bimodal characteristic should actually be characterized as really a high group and a low group.

02:36:15.000 --> 02:36:21.000
One way to do this is to apply a lambda function, which will create a Boolean value of these values.

02:36:21.000 --> 02:36:25.000
So everything down here gets one flag of the low group and everything up here gets the high group.

02:36:25.000 --> 02:36:31.000
And so we will build up this command below by getting some intuition here.

02:36:31.000 --> 02:36:34.000
So let's grab our data frame like this.

02:36:34.000 --> 02:36:39.000
This apply function is a method that goes to the column that you've selected in your data frame.

02:36:39.000 --> 02:36:42.000
And there's a number of ways you can actually call this apply.

02:36:42.000 --> 02:36:45.000
You give it a function and the default axis is zero.

02:36:45.000 --> 02:36:48.000
You can do it in various other ways, so you can access equals one.

02:36:48.000 --> 02:36:53.000
But in this case, most of the time you'll end up doing a lambda function, which is an anonymous function.

02:36:53.000 --> 02:36:58.000
It's like you define a function in Python, but you don't give it a name.

02:36:58.000 --> 02:37:03.000
You're giving it via this apply method every value in the RAD column.

02:37:03.000 --> 02:37:08.000
And you're saying for each of those values in that column, is it greater than say 15?

02:37:08.000 --> 02:37:12.000
So 15 is clearly going to split us into the low and high group.

02:37:12.000 --> 02:37:14.000
And let's just take a look at the first few values of that.

02:37:14.000 --> 02:37:20.000
So I did not head on that to give us the first values and we see that is this X value greater than 15?

02:37:20.000 --> 02:37:22.000
It was false, false, false, false, false.

02:37:22.000 --> 02:37:29.000
And if we want to look at just what that head value looks like without the Boolean, we see that it's 1, 1, 2, 2, 3, 3.

02:37:29.000 --> 02:37:32.000
So everything here is indeed less than 15.

02:37:32.000 --> 02:37:39.000
So we have this function call, which will return a Boolean series false.

02:37:39.000 --> 02:37:43.000
And what we'd like to do is say we want a new column in this data frame.

02:37:43.000 --> 02:37:49.000
We're not going to overwrite this column, but we're going to give a new data frame that we're going to call radian underscore bool,

02:37:49.000 --> 02:37:52.000
because we want to have a nice descriptive name of where it came from.

02:37:52.000 --> 02:37:59.000
And the way you create a new column in a pandas data frame is you give a column that doesn't quite exist yet or doesn't exist yet in the data frame

02:37:59.000 --> 02:38:02.000
and assign it equaling to something else.

02:38:02.000 --> 02:38:07.000
So in this case, we have this rad dot apply lambda greater than or equal to 15.

02:38:07.000 --> 02:38:09.000
And I'm just adding this as type bool.

02:38:09.000 --> 02:38:16.000
Just to give you a sense that if it doesn't automatically get incurred into a type of bool, which we see right here, the d type is boolean.

02:38:16.000 --> 02:38:19.000
You can force it by doing this as type.

02:38:19.000 --> 02:38:21.000
There's other times when this is useful as well.

02:38:21.000 --> 02:38:28.000
So I'll just leave it in here as kind of a best practices or a hint for future ways if you're trying to do something similar and having some problems with it.

02:38:28.000 --> 02:38:33.000
So we've just created a new column in the data frame of rad underscore bool.

02:38:33.000 --> 02:38:38.000
And if we look at what the type of this single value is this I location of zero is a boolean.

02:38:38.000 --> 02:38:42.000
Let's take a look at the histogram on that now that we've created this new column.

02:38:42.000 --> 02:38:45.000
And we see this perfect bimolality of 0 and 1.

02:38:45.000 --> 02:38:48.000
That's one way if you have different features that you want to create.

02:38:48.000 --> 02:38:52.000
It's very flexible to say if it's greater than 15, give it as bool.

02:38:52.000 --> 02:38:57.000
You can also do something if you had a trimol tool or so three different groups or various other ways of slicing this.

02:38:57.000 --> 02:39:00.000
Any function you can think of that can be written down in Python.

02:39:00.000 --> 02:39:08.000
You can then use to filter out the columns and I recommend creating new columns, but sometimes you can overwrite columns if that makes more sense.

02:39:08.000 --> 02:39:17.000
So after doing this we have another seaborne plot that's called a pair plot and let's execute this and then explain what's happening here.

02:39:17.000 --> 02:39:26.000
This is very similar to what's happening above in the scatter plot where we're having the same x value versus y value and the where they intersect.

02:39:26.000 --> 02:39:31.000
So this medium value here is the same intersection of medium value on the x and y axis.

02:39:31.000 --> 02:39:39.000
Instead of a KDE or a kernel density estimation, which is that line, we did it with a histogram and that is a flag given right here.

02:39:39.000 --> 02:39:44.000
I guess it's just the default value. It's under dyag kind equals hist.

02:39:44.000 --> 02:39:49.000
And if we did KDE it would give us the KDE plot as before.

02:39:49.000 --> 02:39:54.000
There's one difference here though where we've given an extra character of hue.

02:39:54.000 --> 02:40:02.000
So there is a Boolean value which is are you near the Charles River and this is similar to splitting the Titanic data set into male and female for each class.

02:40:02.000 --> 02:40:08.000
So it says give us the pairwise interactions between these variables. I just picked four of them.

02:40:08.000 --> 02:40:16.000
And for each of these though I would like to see the differences whether you're close to this river split up by a different color.

02:40:16.000 --> 02:40:25.000
So we have this hue value can take a zero or a one and we see if there's possibly different distributions behavior conditioned on whether it's actually close to the river.

02:40:25.000 --> 02:40:40.000
So this gives you an extra dimension of interaction and interpretability so you can see like oh I see that there's a behavior but it only exists if there's let's say the green dots had a nice tight relationship here and the blue dots were all kind of all vague and all over the place.

02:40:40.000 --> 02:40:50.000
And so if you looked at this without splitting by this Boolean value you might say oh there's not much of a relationship here but turns out that this underlying feature could have been the really important thing.

02:40:50.000 --> 02:40:57.000
Now I don't actually see anything that jumps out at me in this case but having this availability is something that's worth noting.

02:40:57.000 --> 02:41:02.000
So we'll do sms.kde plot and it'll be df.nox.

02:41:02.000 --> 02:41:09.000
So we're seeing here it's not a histogram it's a kernel density estimation of the distribution of this underlying feature here.

02:41:09.000 --> 02:41:13.000
So this is like a histogram but it's more of a smoothed out version of that.

02:41:13.000 --> 02:41:18.000
This is what happens if you give this kde plot method in seaborne single column of values.

02:41:18.000 --> 02:41:26.000
If you gave it two values let's give kde plot the Boolean value of rad versus the Knox value which we just plotted above.

02:41:26.000 --> 02:41:32.000
And we'll get a two dimensional plot which shows the distribution of these two values together.

02:41:32.000 --> 02:41:40.000
So radian is a Boolean value when split it's in the x and y and you can see that if Boolean is true then the Knox values are actually conditioned higher.

02:41:40.000 --> 02:41:45.000
If it's zero then it's conditioned lower with a little bit of data points up here in the upper one.

02:41:45.000 --> 02:41:52.000
So giving two dimensions to a kde plot you get this 2d map which shows you some contour plots some really nice things.

02:41:52.000 --> 02:41:57.000
One final thing for the pandas plotting thing is a thing called Andrew's curves.

02:41:57.000 --> 02:42:04.000
Now I haven't used them much myself but in Wikipedia it has this as their answer of what Andrew's plots are.

02:42:04.000 --> 02:42:07.000
It's a way apparently to visualize structured high dimensional data.

02:42:07.000 --> 02:42:14.000
They show it with the iris data set and the iris data set is the ubiquitous data set from Fisher way back in the day.

02:42:15.000 --> 02:42:20.000
And if we import this we can take a look at a specific value of a data frame.

02:42:20.000 --> 02:42:28.000
So let's look at this whether this Boolean value has much structure to it and it doesn't look like it but perhaps this Knox value does.

02:42:28.000 --> 02:42:34.000
And looks like there's too much to that one. Let's go with rad which is only a nine values for that.

02:42:34.000 --> 02:42:37.000
So you can try to see if there's clustering of behavior.

02:42:37.000 --> 02:42:43.000
Now the actual numbers here I think aren't so easy to read but the fact that this should give you a sense of

02:42:43.000 --> 02:42:48.000
if there's different behaviors going on. Too many data points overlying each other I will do a sample like this

02:42:48.000 --> 02:42:54.000
and a sample is another built-in function of data frames where you can say give me only a hundred values and then do the same exact plot.

02:42:54.000 --> 02:43:01.000
And it'll pick out randomly a hundred values from this data frame and then you're doing the same kind of estimation here.

02:43:01.000 --> 02:43:06.000
And so at this point you might say hey this value of 24 for the data framework is rad.

02:43:06.000 --> 02:43:11.000
That looks like it's having fundamentally different behavior than the other values which seem to be clustered together.

02:43:11.000 --> 02:43:17.000
I don't know that this actually tells us much in this case but it's another piece of functionality that I think is worth knowing about.

02:43:17.000 --> 02:43:22.000
I'll go through the last few bits here and just talk about them really quickly.

02:43:22.000 --> 02:43:29.000
So here's another KDE plot of this median value for the houses which is what we've seen before but this is going to be the target

02:43:29.000 --> 02:43:33.000
like how much the price of the house is actually going to be sold for.

02:43:33.000 --> 02:43:38.000
And we can add to that by saying we want to also see what's called this rug being true.

02:43:38.000 --> 02:43:45.000
So instead of doing a KDE plot we can give it a distribution plot and add the fringe kind of rug thing at the bottom

02:43:45.000 --> 02:43:49.000
which adds the actual density of points at these different values.

02:43:49.000 --> 02:43:53.000
So you can see that it is actually very dense here as we go across these values.

02:43:53.000 --> 02:43:58.000
So sometimes if you've chosen a kernel that's too wide or too narrow for your underlying dataset

02:43:58.000 --> 02:44:03.000
seeing the rug along the bottom here gives you extra clues into what's going on.

02:44:03.000 --> 02:44:09.000
And one last look here at two variables that might actually be more useful for looking at relationships.

02:44:09.000 --> 02:44:17.000
The median value versus the L stat and you can see that there's this kind of banana shaped curve here going on in the relationships.

02:44:17.000 --> 02:44:20.000
Okay so that's going to be it for this video.

02:44:20.000 --> 02:44:24.000
What we did in this video is we showed a number of different visualization techniques.

02:44:24.000 --> 02:44:31.000
We took a value that had a clear bimodality of a low and a high group and created a new data frame column to encode that.

02:44:31.000 --> 02:44:41.000
Went through also and saw various methods of doing kernel density estimations, scatter plots and various other features.

02:44:41.000 --> 02:44:44.000
In this video we'll be talking about stats models.

02:44:44.000 --> 02:44:52.000
Stats models is a library that you can use that allows for a lot of statistical machinery that can help you with your data science work.

02:44:52.000 --> 02:44:59.000
So we'll continue with the same Boston housing dataset as before which we were just looking at in the last video.

02:44:59.000 --> 02:45:03.000
And take a look at some of these Boston housing prices.

02:45:03.000 --> 02:45:07.000
Let me toggle this header in this toolbar real fast.

02:45:07.000 --> 02:45:10.000
Make this full screen so we have a little extra room to look at.

02:45:10.000 --> 02:45:17.000
So we will load in the scikit-learn dataset load Boston which again has the same attributes as we saw in the previous videos.

02:45:17.000 --> 02:45:26.000
We will construct our pandas data frame from this scikit-learn dataset so that we can use the standard tools we've learned over the years.

02:45:26.000 --> 02:45:34.000
We've combined the features and the target into one data frame and here's that scatter matrix plot we made in the previous video as well.

02:45:34.000 --> 02:45:36.000
This is a pandas call.

02:45:36.000 --> 02:45:41.000
So with this function call we get all of the pairwise interaction terms for this dataset.

02:45:41.000 --> 02:45:49.000
And from this we see a number of features that look like they have some strong trends with the thing we would try to predict which is the median value of the house.

02:45:49.000 --> 02:45:51.000
There's a trend here with this RM.

02:45:51.000 --> 02:45:56.000
It looks like there's this kind of banana shaped L-stat curve that we talked about at the end of the previous video.

02:45:56.000 --> 02:46:03.000
So we have a few things that we think you might be able to combine into some sort of model that will predict our median value.

02:46:03.000 --> 02:46:07.000
Again, let's look at the columns and the number of unique values for each of these.

02:46:07.000 --> 02:46:09.000
In particular, Rad has nine values.

02:46:09.000 --> 02:46:10.000
We previously made that a Boolean.

02:46:10.000 --> 02:46:13.000
Let's actually take a look at what the values comprise it.

02:46:13.000 --> 02:46:17.000
So there are nine values and these are the values.

02:46:17.000 --> 02:46:20.000
And then 24 is obviously the outlier here.

02:46:20.000 --> 02:46:23.000
And we previously made a Boolean variable, which we can do again right now.

02:46:23.000 --> 02:46:35.000
So we'll split everything from less than 15, which means everything up to here, 1, 2, 3, 4, 5, 6, 7, 8 will be labeled as 0 and 24 will be labeled as 1.

02:46:35.000 --> 02:46:42.000
Let's look at the target variable, which is this median value plot just done as a distribution plot with a rug at the bottom.

02:46:42.000 --> 02:46:47.000
And so this will be the target variable and we see some interesting structure going on here.

02:46:47.000 --> 02:46:53.000
So plot again L-stat, which we identified just a second ago, versus median value.

02:46:53.000 --> 02:46:56.000
We have again this kind of weird shaped banana plot.

02:46:56.000 --> 02:46:59.000
This is sort of a tapering off effect of this thing.

02:46:59.000 --> 02:47:01.000
So stats models.

02:47:01.000 --> 02:47:06.000
Let's actually go and take a look at this as a Google search.

02:47:06.000 --> 02:47:09.000
So stats models for Python.

02:47:09.000 --> 02:47:15.000
The current documentation for this sits at statsmodels.sourceforge.net.

02:47:15.000 --> 02:47:25.000
And it has, as it says, it's a Python module that allows users to look at data to estimate statistical models and perform statistical tests as many different modeling choices.

02:47:25.000 --> 02:47:34.000
So our options, we have linear regression, generalized linear models and all the things listed here, and also some nice examples that explain more.

02:47:34.000 --> 02:47:43.000
This is definitely a package that's geared more toward the statistical side of data science than the machine learning side, which is how I'd classify scikit-learn.

02:47:43.000 --> 02:47:49.000
So with that comes a number of useful tools that if you haven't used them, it can be very powerful.

02:47:49.000 --> 02:47:52.000
So this is where the documentation resides.

02:47:52.000 --> 02:47:54.000
I recommend looking at that.

02:47:54.000 --> 02:47:56.000
We imported this at the top.

02:47:56.000 --> 02:47:58.000
So I'll scroll up to the top real fast.

02:47:58.000 --> 02:48:04.000
We imported statsmodels.api as SM, which is not a typical way of importing Python modules.

02:48:04.000 --> 02:48:07.000
This is one of the standard ways of doing stats models.

02:48:07.000 --> 02:48:14.000
And then there's this formula.api, from which we're going to import ordinary least squares, which is just OLS in this case.

02:48:14.000 --> 02:48:16.000
So let's scroll back down.

02:48:16.000 --> 02:48:19.000
The formulas work in a way that's very similar to R.

02:48:19.000 --> 02:48:27.000
So if you've used R before, or if you've used the Python package Patsy or various other ones, what you end up writing is the dependent variables.

02:48:28.000 --> 02:48:30.000
Or the thing you're trying to predict.

02:48:30.000 --> 02:48:36.000
So in this case, the median value, this tilde, which goes as Lstat, which is this thing that we're just plotting up here.

02:48:36.000 --> 02:48:39.000
So Lstat versus medv median value.

02:48:39.000 --> 02:48:43.000
So we've given the formula in terms of the relationship between these different variables.

02:48:43.000 --> 02:48:46.000
I have to tell the model where the data comes from.

02:48:46.000 --> 02:48:47.000
So we say this data frame.

02:48:47.000 --> 02:48:56.000
When you give it this data frame, it says, okay, I'm going to look in this data source, df, four columns that are named in the same way that you've written it out in this formula here.

02:48:56.000 --> 02:49:02.000
So we've said, okay, we've rewritten out medv and Lstat are actual columns.

02:49:02.000 --> 02:49:05.000
And at the end, we will fit this with the dot fit function.

02:49:05.000 --> 02:49:09.000
And the end, you have a model, which we've written down as mod.

02:49:09.000 --> 02:49:15.000
And running the method dot summary tells us the output of trying to fit this data.

02:49:15.000 --> 02:49:17.000
So we have the results from that.

02:49:17.000 --> 02:49:19.000
So the dependent variable is median value.

02:49:19.000 --> 02:49:25.000
The model is ordinarily squares method least squares tells you a bunch of different pieces of information that are pretty good.

02:49:25.000 --> 02:49:27.000
These pieces of information that are pretty useful here.

02:49:27.000 --> 02:49:35.000
So we have r squared, adjusted r squared, f statistics, log likelihood, AIC, the Ikeke information criteria, or however you say that.

02:49:35.000 --> 02:49:41.000
Of course, the values of the coefficients and the intercepts, standard error, the 95% confidence intervals and so on.

02:49:41.000 --> 02:49:48.000
If you're looking at this and wanting to evaluate this model statistically, you have all kinds of things at your fingertips here to look at.

02:49:48.000 --> 02:49:54.000
Now, the relationship between Lstat and median value of the houses does not look linear to me.

02:49:54.000 --> 02:49:58.000
This looks like a weird shape here and we can actually plot this with the river.

02:49:58.000 --> 02:50:08.000
We can reverse this and see how this kind of tapering off of the median value versus Lstat can be a combination of features.

02:50:08.000 --> 02:50:12.000
So I'm going to add an extra term here.

02:50:12.000 --> 02:50:18.000
I'll actually take the log of the value and you can actually write it in this way in this string.

02:50:18.000 --> 02:50:23.000
So you say numpy.log or np.log of the variable that you want to look at.

02:50:23.000 --> 02:50:29.000
And you have to wrap it in this extra I for wrapping up because this doesn't actually exist as a column.

02:50:29.000 --> 02:50:32.000
You have to wrap it in this I. There's other ways you can wrap this as well.

02:50:32.000 --> 02:50:40.000
But I think having this and this both be in this linear model is likely to give a much better fit than just the Lstat by itself.

02:50:40.000 --> 02:50:47.000
Or even Lstat squared, which we could also do simply by just instead of it numpy.log, we do Lstat star star squared.

02:50:47.000 --> 02:50:56.000
So let's run this and we see our summary comes out and we have our R squared and AIC and all these different various intercepts and log values.

02:50:56.000 --> 02:50:58.000
So let's actually compare the two.

02:50:58.000 --> 02:51:00.000
So one way to compare it is to look at the AIC.

02:51:00.000 --> 02:51:07.000
So this one from 3200 down to 3100, which is a pretty substantial decrease in the AIC.

02:51:07.000 --> 02:51:14.000
So we think this is actually a better fit statistically, although we have to look at the residuals and do many other tests to make sure that this is actually is a viable model.

02:51:14.000 --> 02:51:21.000
So that we're nowhere near done and like to double emphasize that what I'm showing you here is not a final rigorous data science result.

02:51:21.000 --> 02:51:26.000
This is more of a sketch of what's possible with the tools that I think are useful.

02:51:26.000 --> 02:51:29.000
Don't be taking directly from this lessons on how to do data science.

02:51:29.000 --> 02:51:32.000
This is more of a sketch of how the tools should work.

02:51:32.000 --> 02:51:34.000
Let's make this a little bit bigger so we have more room again.

02:51:34.000 --> 02:51:41.000
One way to start to evaluate how good this fit is to actually look at this graphics from the stats models.

02:51:41.000 --> 02:51:45.000
So statsmodels.graphics has a lot of different plotting options.

02:51:45.000 --> 02:51:56.000
And there's these component and component plus residual plots, which is the CCPR plots, which you feed it the model itself contained within this model object is the underlying formula.

02:51:56.000 --> 02:52:02.000
And so you can tell it, I want to know this one term here, the term that went with the log of the LSTAT score.

02:52:02.000 --> 02:52:07.000
How does that look versus the residuals of this plus the I squared?

02:52:07.000 --> 02:52:13.000
So we can see that the component actually does a decent job at this log stat versus residuals plus log stat.

02:52:13.000 --> 02:52:17.000
So this line actually does a pretty good job of fitting this.

02:52:17.000 --> 02:52:24.000
And for some reason that I don't quite understand, it actually plots it twice to the same exact plot, but was just LSTAT by itself.

02:52:24.000 --> 02:52:34.000
So the first term in that model, we can see a little bit wonky behavior where it's not quite as good as the previous one where the residuals has some extra structure here in the low end, especially.

02:52:34.000 --> 02:52:41.000
But we can start to have various goodness of fits and start to model out how good our model is at capturing the underlying data.

02:52:41.000 --> 02:52:44.000
Again, it shows it twice. And again, I don't know why.

02:52:44.000 --> 02:52:47.000
We can also add more terms to this model.

02:52:47.000 --> 02:52:52.000
So previously we had LSTAT and this the log of LSTAT plus one for the intercept.

02:52:52.000 --> 02:52:55.000
We can also add the RM category.

02:52:55.000 --> 02:53:01.000
We can also add the Boolean value, which is whether it's in the higher low of the RAD variable.

02:53:01.000 --> 02:53:13.000
And because it's a categorical, you can feed it to the model with this C value, and it will properly take into account the fact that what's in this column should be considered a category.

02:53:13.000 --> 02:53:17.000
And it won't get you in trouble with the dummy variable trap that I had to mention at the last time.

02:53:17.000 --> 02:53:30.000
So if we look at this, we see a number of things, including the fact that it starts off with the categorical variable radian bool, the categorical value where the default value is false.

02:53:30.000 --> 02:53:36.000
And if it's true, what the change in the coefficient is for that value and the various other values as well.

02:53:36.000 --> 02:53:42.000
Another thing to look at, depending on what you prefer to look at the BIC or the AIC or log likelihood F statistics.

02:53:42.000 --> 02:53:53.000
To compare this to the previous models, this again has lowered the AIC substantially so that in terms of is it a better fit or not, it has some statistical basis for saying that this is a better fit.

02:53:53.000 --> 02:54:01.000
We still have to do a lot of work still before we decide this is actually a reasonable fit and all the assumptions behind an ordinarily squares fit are holding true.

02:54:01.000 --> 02:54:08.000
But just as a first pass, we have a lot of really nice information here.

02:54:08.000 --> 02:54:18.000
In this video, I would like to continue from the previous video where we had just run a model to find the intercept and the categorical RAD value.

02:54:18.000 --> 02:54:31.000
We can also run the same exact model as before deciding that this RAD values that are doing the boolean version of this, we can actually run on the entire column itself and telling stats models that we're actually using a categorical variable here as well.

02:54:31.000 --> 02:54:40.000
So now we're trying to predict the median value of the house using all of these possible variables where these each have a coefficient in front of it.

02:54:40.000 --> 02:54:49.000
We run a dot fit method on that and save it as a model as MOD and we're going to output the summary of that fit ran just then.

02:54:49.000 --> 02:54:53.000
And again, dependent variable is this MED V variable.

02:54:53.000 --> 02:54:59.000
And we see in the output here various goodness of fit and metrics about how the fit actually worked out.

02:54:59.000 --> 02:55:12.000
We see an R squared of 0.73 and ASC of 3040, which is a slight improvement from the previous one, meaning that encoding the RAD variable where each value is independently stored.

02:55:12.000 --> 02:55:17.000
So since it's a categorical, this is all with a baseline of one, which is why it doesn't appear here.

02:55:17.000 --> 02:55:24.000
These coefficients are all based off of comparing each of these terms with the baseline of RAD equals to one.

02:55:24.000 --> 02:55:26.000
If that doesn't make sense to you, don't worry about it.

02:55:26.000 --> 02:55:28.000
Don't worry about this kind of statistical model.

02:55:28.000 --> 02:55:31.000
And if it does make sense to you, then you understand what I just said.

02:55:31.000 --> 02:55:38.000
So anyway, you get the output from this, but we actually want to see some plots to see how good this fit actually is.

02:55:38.000 --> 02:55:45.000
Because just looking at the diagnostics and the metrics that come out from these fits isn't enough to tell us whether we're making a good model here.

02:55:45.000 --> 02:55:48.000
So let's start to look at how we can assess fit quality.

02:55:48.000 --> 02:55:51.000
One of the easy things you can do is to look at a thing called leverage.

02:55:51.000 --> 02:56:02.000
StatsModels gives us a nice way to see this and visualize this by using the sm.graphics.influence plot and the plot leverage residual squared plots.

02:56:02.000 --> 02:56:04.000
So let's take a look at these two plots.

02:56:04.000 --> 02:56:06.000
I will first do this one.

02:56:06.000 --> 02:56:15.000
So what we see here is on the y-axis, studentized residuals versus the x-axis, the h-leverage is using the cooks method for influence.

02:56:15.000 --> 02:56:23.000
What you see on the leverage corresponds to an outside influence on the overall fit for its values.

02:56:23.000 --> 02:56:30.000
So if you see something with high residuals and high leverage, that's something that we should possibly consider looking at that point and figuring out what's going on at that exact point.

02:56:30.000 --> 02:56:37.000
So like 368, for example, would be a candidate to be looking at here because it has high residuals and high leverage.

02:56:37.000 --> 02:56:38.000
That's one way of looking at it.

02:56:38.000 --> 02:56:42.000
Another way is to look at it through this leverage residual squared.

02:56:42.000 --> 02:56:46.000
And you give it simply the model object that you just fit above.

02:56:46.000 --> 02:56:51.000
You just give it mod and it will give normalized residuals squared versus the leverage.

02:56:51.000 --> 02:56:59.000
Again, 368, 365, 372, 371 are all outliers in terms of points that we should possibly take another look at again.

02:56:59.000 --> 02:57:02.000
That corresponds to those four points up here.

02:57:02.000 --> 02:57:09.000
So this leverage plots is one way of assessing the fit and the data points to make sure something isn't going crazy.

02:57:09.000 --> 02:57:15.000
There's also a way of doing partial regression and I've quoted a bit from the documentation stats models here.

02:57:15.000 --> 02:57:21.000
It says the slope of the fitted line is that of the exogenous in the full multiple regressions.

02:57:21.000 --> 02:57:22.000
That's what's going on here.

02:57:22.000 --> 02:57:27.000
The individual points can be used to assess the influence of points on the estimated coefficient.

02:57:27.000 --> 02:57:29.000
So let's take a look at what this means visually.

02:57:29.000 --> 02:57:32.000
I think it's easier to see what's happening this way.

02:57:32.000 --> 02:57:39.000
So we have a partial regression plot and we're evaluating the expectation value of L-stat given the values that we have

02:57:39.000 --> 02:57:43.000
and same plotting against the dependent variable, the median value that we're trying to predict.

02:57:43.000 --> 02:57:52.000
And in this, we see that a lot of the points are kind of in a mass right here and the outliers are sitting here at this very low end.

02:57:52.000 --> 02:57:55.000
And the same culprits appear again and you can actually see the effect that it's having on this.

02:57:55.000 --> 02:57:57.000
It's pulling the slope up a bit.

02:57:57.000 --> 02:58:03.000
So that's with a plot partial regression and giving various features as you're holding constant.

02:58:03.000 --> 02:58:06.000
You can give it the entire model and see what that looks like.

02:58:06.000 --> 02:58:12.000
When we get an entire grid, you're going to have a lot more involved to look at this grid of plots.

02:58:12.000 --> 02:58:21.000
But it's the various features here so that the RAD variable is a feature three given X versus the median value on the X and the Y axis.

02:58:21.000 --> 02:58:26.000
And so you can look at the various categorical variables and how they are being fit with the lines

02:58:26.000 --> 02:58:34.000
and how they are interacting with the overall fit as well as the values that clearly are more continuous and having a nicer time of it.

02:58:34.000 --> 02:58:39.000
So there's two ways to do this partial regression plot and both give you different ways of looking at this data.

02:58:39.000 --> 02:58:42.000
Again, this is plotted twice for reasons unknown.

02:58:42.000 --> 02:58:44.000
Finally, we have regression.

02:58:44.000 --> 02:58:47.000
We can do this as a plot regress exogenous.

02:58:47.000 --> 02:58:54.000
It gives you this four panel plot of median value versus L-stat and residuals versus L-stat.

02:58:54.000 --> 02:58:56.000
So this is the data minus the fit itself.

02:58:56.000 --> 02:59:04.000
And what you're hoping to see is noise pretty symmetrically about this axis here, the estimated variables and the CCPR plots.

02:59:04.000 --> 02:59:09.000
So we see fit versus the actual values in this plot here.

02:59:09.000 --> 02:59:15.000
And then we can also do it versus any other term in that model, which is, in this case, the natural log of the L-stat.

02:59:15.000 --> 02:59:21.000
And then we get this plot here, which shows much tighter fit to this instance.

02:59:21.000 --> 02:59:26.000
If you've built up a model, then again, I'm not saying I've built up some amazing model at this point.

02:59:26.000 --> 02:59:29.000
This is definitely more descriptive of how this kind of process can work.

02:59:29.000 --> 02:59:39.000
But if you would like to build up a model and look through a lot of diagnostic plots and have a true statistics, robust package manager behind you,

02:59:39.000 --> 02:59:44.000
look into stats models and really try to dive into this because there's a lot of really good stuff in this.

02:59:44.000 --> 02:59:49.000
So with that, I am concluding the second data science project.

02:59:49.000 --> 02:59:58.000
And what I really try to focus on this time was a little bit of some more advanced features of using the plotting features of pandas,

02:59:58.000 --> 03:00:04.000
really taking a deep dive into how one aspect of the stats models library and there's many aspects of it.

03:00:04.000 --> 03:00:12.000
So I highlighted the ordinarily squares and how fitting linear model there with the statistical analysis and output that comes out of every fit,

03:00:12.000 --> 03:00:16.000
as well as fitting the diagnostics and doing a quality of fit.

03:00:16.000 --> 03:00:24.000
I also spent a lot of time on the visuals of this diving a little bit deeper into Seaborn and a few of the other options there.

03:00:24.000 --> 03:00:31.000
So just as a kind of a wrap up of this, using map plot live and Seaborn stats models and pandas,

03:00:31.000 --> 03:00:35.000
these data sets can be explored and manipulated and fit.

03:00:35.000 --> 03:00:44.000
And these tools give a lot of flexibility and exploring and analyzing data in a notebook lets someone else take a look at what you did through your analysis.

03:00:44.000 --> 03:00:51.000
So if you've made some horrendous error as you went through, that is something that's easy to point out and point to the plot.

03:00:51.000 --> 03:00:57.000
As I said, this was a decent fit, for example. This is clearly bad because of reason X and you can point to it and circle it.

03:00:57.000 --> 03:01:02.000
And it's not just a bunch of random files sitting in a directory somewhere.

03:01:02.000 --> 03:01:13.000
To close off this project, the last thing that remains to do is to save this and to close it and push it back to GitHub so that you guys can also look at the same data sets and follow along yourself.

03:01:13.000 --> 03:01:18.000
So I'm going to file, close and halt, go back to the terminal.

03:01:18.000 --> 03:01:22.000
Git status has only this develop directory that has any changes in it.

03:01:22.000 --> 03:01:27.000
So git add, develop, git status, we have three new files.

03:01:27.000 --> 03:01:31.000
Okay. And I've closed down all of them. Just double check the server here.

03:01:31.000 --> 03:01:38.000
Yep. Everything looks to be closed and say git commit.

03:01:38.000 --> 03:01:40.000
Give a commit message that makes sense.

03:01:40.000 --> 03:01:45.000
Git push origin Jonathan viz, which is the name of this branch.

03:01:45.000 --> 03:01:54.000
Go back to GitHub, see that we've already made a change this we can compare and pull request and create a pull request.

03:01:54.000 --> 03:02:02.000
I'll go ahead and actually merge this pull request because I've demonstrated how to do the full pull request and peer review aspect of it before.

03:02:02.000 --> 03:02:07.000
And going back to the data viz project, what we have here is don't save that.

03:02:07.000 --> 03:02:15.000
So the data viz project to will have the notebooks that I went through during this project available right there.

03:02:15.000 --> 03:02:27.000
Just to recap what happened in this video, finished up looking at the plots from stats models and finished up the second data science project for this course.

03:02:27.000 --> 03:02:33.000
Let's talk about some of the security issues with using the Jupiter notebook as is out of the box.

03:02:33.000 --> 03:02:36.000
The notebook only listens to requests on local host.

03:02:36.000 --> 03:02:39.000
This means that it ignores requests from the Internet.

03:02:39.000 --> 03:02:44.000
People connecting from the Internet can't see your server and they won't be able to connect.

03:02:44.000 --> 03:02:50.000
In order to allow them to connect, you have to explicitly configure the notebook to listen to the correct IP.

03:02:50.000 --> 03:02:53.000
Once you do, anybody can access your notebook server.

03:02:53.000 --> 03:03:02.000
The notebook server has no password by default and permissions of the users that are connecting are the same as the permissions of the user who had launched the server.

03:03:02.000 --> 03:03:09.000
So this means if you launch the server, everybody who connects to the notebook will be executing things as if they were you.

03:03:09.000 --> 03:03:13.000
The second main problem with using the notebook is it's using an insecure line.

03:03:13.000 --> 03:03:19.000
So typically, the notebook is broken into three pieces, the kernel, the web server, and the client.

03:03:19.000 --> 03:03:22.000
The client is what you see in the web browser.

03:03:22.000 --> 03:03:24.000
It's the notebook as you know it.

03:03:24.000 --> 03:03:29.000
And the web server is the thing that relays messages from the kernel to the client.

03:03:29.000 --> 03:03:32.000
The web server communicates with the kernel using ZMQ.

03:03:32.000 --> 03:03:35.000
Usually, the kernel and the web server exist on the same machine.

03:03:35.000 --> 03:03:40.000
The kernel is the server that executes code and runs requests.

03:03:40.000 --> 03:03:45.000
The line between the kernel and the web server you don't have to worry about usually because it's on the same machine.

03:03:45.000 --> 03:03:51.000
However, the line between the web server and the client you have to worry about because it's over the open Internet.

03:03:51.000 --> 03:03:57.000
This means that it's available for people to listen to and inject messages.

03:03:57.000 --> 03:04:03.000
However, there are some setups where it makes sense to separate the kernel onto its own machine.

03:04:03.000 --> 03:04:09.000
For example, you may have a cluster of computers running kernels, one computer running the web server.

03:04:09.000 --> 03:04:15.000
In this case, you also have to worry about the ZMQ communication between the kernel and the web server

03:04:15.000 --> 03:04:20.000
if the kernel and the web server are not on a VPN or in a secured network.

03:04:20.000 --> 03:04:27.000
I'd just like to note, we aren't security experts, but we do have experts in the community and they do help us.

03:04:27.000 --> 03:04:32.000
If you spot a problem, I ask you, please email us at our security mailing list.

03:04:32.000 --> 03:04:35.000
The address is security at ipython.org.

03:04:35.000 --> 03:04:38.000
Once you do, we'll work quickly to open a CVE.

03:04:38.000 --> 03:04:49.000
In the next set of slides, I'll talk about how you can mitigate some of these problems and rest assured that your notebook deployment is as secure as can be.

03:04:49.000 --> 03:04:54.000
In the last video, we talked about some of the limitations of running the notebook server publicly.

03:04:54.000 --> 03:04:58.000
Specifically, we talked about security vulnerabilities.

03:04:58.000 --> 03:05:05.000
In this video, I'll describe to you some of the solutions provided by the notebook software and some of the limitations of the notebook software.

03:05:05.000 --> 03:05:13.000
First, in the last video, I showed you this diagram and told you that the communication between the web server and client was insecure by default.

03:05:13.000 --> 03:05:20.000
The notebook actually provides support for HTTPS, industry grade encryption, for this communication line.

03:05:20.000 --> 03:05:22.000
I'll show you how to configure this.

03:05:22.000 --> 03:05:28.000
However, the notebook does not provide support out of the box for encrypting the line between the kernel and the web server.

03:05:28.000 --> 03:05:37.000
Therefore, I recommend you either run the kernel and the web server on the same machine, if possible, or run them within a VPN.

03:05:37.000 --> 03:05:40.000
The latest version of ZMQ does support encryption.

03:05:40.000 --> 03:05:44.000
However, the notebook is not using that version of ZMQ currently.

03:05:44.000 --> 03:05:50.000
Before we secure the notebook server, we need to be able to launch it so that people on the internet can connect to it.

03:05:50.000 --> 03:05:53.000
In the previous chapter, you learned about Tralits.

03:05:53.000 --> 03:05:58.000
We can configure the notebook to listen to all IP addresses using Tralits.

03:05:58.000 --> 03:06:07.000
If I do jupiter, notebook, double dash help, I can list all the configuration options of the notebook.

03:06:07.000 --> 03:06:12.000
The third to last configuration option is double dash IP.

03:06:12.000 --> 03:06:16.000
That allows me to change the IP that the notebook server is listening on.

03:06:16.000 --> 03:06:22.000
Just to cement the idea that this is a Tralit, I'll show you in the notebook source where this Tralit can be found.

03:06:22.000 --> 03:06:28.000
In parentheses next to the configuration value, you see that notebook app.ip is listed.

03:06:28.000 --> 03:06:33.000
This means that IP is a Tralit inside the notebook app class.

03:06:33.000 --> 03:06:43.000
So opening up the notebook subfolder of the notebook repository and then the notebook app module inside that, we should be able to find the IP trait.

03:06:43.000 --> 03:06:46.000
I'll use the search function of Adam to find IP.

03:06:46.000 --> 03:06:49.000
Here's the definition of the IP trait.

03:06:49.000 --> 03:06:54.000
If you want to configure something of the application and you don't see the option in the help string,

03:06:54.000 --> 03:07:00.000
it's a good skill to be able to look through the source code and see if there's a Tralit that isn't being listed.

03:07:00.000 --> 03:07:03.000
So we have two ways to set this IP trait.

03:07:03.000 --> 03:07:12.000
We can either pass it in at the command line, like so, or we can specify it via config, so it's the new default.

03:07:12.000 --> 03:07:19.000
By specifying IP to asterisk, we're telling the server to listen to requests on all IP addresses.

03:07:19.000 --> 03:07:27.000
You may get two warnings, one from your system firewall prompting for Python to have the ability to accept incoming network connection.

03:07:27.000 --> 03:07:30.000
This is because the notebook server is written in Python.

03:07:30.000 --> 03:07:35.000
The other warning you'll see is in your terminal output from the notebook server itself,

03:07:35.000 --> 03:07:42.000
warning you that the server is listening on all IP addresses and is not using encryption or authentication.

03:07:42.000 --> 03:07:45.000
Don't worry, I'll show you how to set these up.

03:07:45.000 --> 03:07:49.000
But first, let's try setting IP equals asterisk in the config.

03:07:49.000 --> 03:07:57.000
If you recall from the earlier Tralits video, the config is stored inside the .jupiter folder inside my home directory.

03:07:57.000 --> 03:08:05.000
Opening the folder up in Adam, we see that the config files from the earlier weekend and weekday demonstration still exist.

03:08:05.000 --> 03:08:10.000
We'll go ahead and erase that here inside the jupiter notebook config.py file.

03:08:10.000 --> 03:08:21.000
Now, recalling what the help text said in the terminal, we'll set notebook app.ip equal to asterisk.

03:08:21.000 --> 03:08:27.000
Go ahead and save the file and we'll try launching the notebook server again.

03:08:27.000 --> 03:08:35.000
This time, however, we won't specify the double-dash IP equals asterisk on the command line because it's already specified inside our config.

03:08:35.000 --> 03:08:37.000
It looks like the launch was a success.

03:08:37.000 --> 03:08:46.000
We still received the warnings about the server listening on all IP addresses, even though we didn't specify the IP equals asterisk flag in the command line.

03:08:46.000 --> 03:08:52.000
This means that the line that we added to the config file worked as expected.

03:08:52.000 --> 03:08:56.000
In the last video, we added password security to the notebook.

03:08:56.000 --> 03:09:01.000
However, we did not encrypt the line between the web browser and the notebook web server.

03:09:01.000 --> 03:09:10.000
This means that the notebook is vulnerable to people eavesdropping on the communication between it and you or any other users of your server.

03:09:10.000 --> 03:09:15.000
In this video, we'll add HTTPS encryption to your notebook web server.

03:09:15.000 --> 03:09:22.000
To get the notebook to start using HTTPS, all you have to do is point it to your key file and cert file.

03:09:22.000 --> 03:09:27.000
If you don't have a key file and cert file, you can generate one yourself.

03:09:27.000 --> 03:09:35.000
Before I show you how to tell the notebook to use your key file and cert file, I'll show you how to generate one using OpenSSL.

03:09:35.000 --> 03:09:38.000
If you already have a key, you can skip this step.

03:09:38.000 --> 03:09:41.000
Anaconda already comes with OpenSSL installed.

03:09:41.000 --> 03:09:49.000
However, OpenSSL frequently releases security updates, so I highly recommend that you update to the latest version.

03:09:49.000 --> 03:09:54.000
To do so, you can run conda space update OpenSSL.

03:09:54.000 --> 03:09:57.000
I'm currently inside my Jupyter config directory.

03:09:57.000 --> 03:10:03.000
I'm going to run OpenSSL to generate the key insert file.

03:10:03.000 --> 03:10:06.000
I'm going to generate the cert so it lasts for one year.

03:10:06.000 --> 03:10:14.000
To do so, I'm going to pass in 365 days into the days argument.

03:10:14.000 --> 03:10:22.000
I'm going to output both the key and the cert file into the same file.

03:10:22.000 --> 03:10:25.000
Once I run the command, an interactive wizard will start.

03:10:25.000 --> 03:10:27.000
I'll answer some of these questions.

03:10:27.000 --> 03:10:35.000
However, if you want, you can skip any of the questions just by hitting return to accept the default value.

03:10:35.000 --> 03:10:40.000
Once that is done, we'll have to configure the notebook to use this key insert file.

03:10:40.000 --> 03:10:46.000
To do so, I'm going to open up Adam inside the Jupyter configuration directory.

03:10:46.000 --> 03:10:50.000
After the shaw from the password trait, I'm going to create a new line.

03:10:50.000 --> 03:10:52.000
I'm going to specify the cert file first.

03:10:52.000 --> 03:10:55.000
The cert file is a trait of the notebook app.

03:10:55.000 --> 03:11:02.000
It's important that I pass the full path to the cert file.

03:11:02.000 --> 03:11:04.000
Next, I'm going to specify the key file.

03:11:04.000 --> 03:11:09.000
The key file is a trait of the session class.

03:11:09.000 --> 03:11:14.000
Since we output it the key into the cert file, we can just specify the same file here.

03:11:14.000 --> 03:11:17.000
Now I'm going to save the config.

03:11:17.000 --> 03:11:22.000
Back in the terminal, I'm going to try launching the notebook.

03:11:22.000 --> 03:11:26.000
When the notebook launches, you'll probably see this security error from your web browser,

03:11:26.000 --> 03:11:30.000
saying that your connection is not private and that the authority is invalid.

03:11:30.000 --> 03:11:33.000
This is because you self-generated the cert.

03:11:33.000 --> 03:11:36.000
You can get around this by having a third party generate your cert.

03:11:36.000 --> 03:11:41.000
For now, let's just click Advanced and proceed the local host.

03:11:41.000 --> 03:11:43.000
Now our connection is being encrypted.

03:11:43.000 --> 03:11:47.000
If you are interested in getting a cert that's verified by a third party,

03:11:47.000 --> 03:11:49.000
I recommend using StartSSL.

03:11:49.000 --> 03:11:51.000
They'll do it for free.

03:11:51.000 --> 03:11:56.000
You can visit their website at www.startssl.com.

03:11:56.000 --> 03:12:00.000
The StartSSL free cert should be fine for basic setups.

03:12:00.000 --> 03:12:04.000
The other two offer slightly more features that are verified,

03:12:04.000 --> 03:12:10.000
whereas the most expensive gives your site a green bar inside the address bar when the user is connected.

03:12:10.000 --> 03:12:17.000
You can see that in the screenshot in the side column of their website.

03:12:17.000 --> 03:12:22.000
In the last chapter, we talked about how you could deploy the notebook securely.

03:12:22.000 --> 03:12:24.000
In this chapter, we'll change gears.

03:12:24.000 --> 03:12:26.000
We'll start looking at NB Viewer.

03:12:26.000 --> 03:12:32.000
Before I discuss installing NB Viewer, I'm going to show you what NB Viewer looks like in the wild.

03:12:32.000 --> 03:12:36.000
I'm currently on the Jupyter public deployment of NB Viewer,

03:12:36.000 --> 03:12:41.000
which is accessible at nbviewer.jupyter.org.

03:12:41.000 --> 03:12:48.000
NB Viewer is a web application that is used to render static views of notebooks online.

03:12:48.000 --> 03:12:51.000
In the back end, NB Viewer uses NB Convert,

03:12:51.000 --> 03:12:58.000
the application that I showed you in chapter one, which can be used to convert notebooks to various static formats.

03:12:58.000 --> 03:13:04.000
NB Viewer just uses NB Convert to convert notebooks to static HTML representations.

03:13:04.000 --> 03:13:10.000
NB Viewer itself is a simple website that has a title and then an address bar

03:13:10.000 --> 03:13:13.000
where you can paste the link to your notebook file.

03:13:13.000 --> 03:13:17.000
After pasting the link, you click Go and it will render that notebook file.

03:13:17.000 --> 03:13:21.000
Below that, there's a showcase of notebooks for various categories.

03:13:21.000 --> 03:13:27.000
Here, for example, we can click on this iRuby notebook to see what iRuby is.

03:13:27.000 --> 03:13:29.000
This is what a rendered notebook looks like.

03:13:29.000 --> 03:13:33.000
You can see it looks quite different than the notebook client that you're used to.

03:13:33.000 --> 03:13:39.000
It's quite a bit more bare, but it still bears some resemblance to pieces of the interactive notebook,

03:13:39.000 --> 03:13:43.000
such as these prompts and cell formatting.

03:13:43.000 --> 03:13:51.000
At the top, there are links to download the notebook, view the notebook on GitHub if it is a GitHub hosted file,

03:13:51.000 --> 03:13:53.000
and a link to go to the top of the file.

03:13:53.000 --> 03:13:58.000
At the bottom of the page, you can see the version of NB Viewer that we're running,

03:13:58.000 --> 03:14:03.000
the notebooks version, and the version of NB Convert that NB Viewer is running against.

03:14:03.000 --> 03:14:11.000
NB Viewer tries to be aggressive about caching notebooks, so you also get a status of when the notebook was last rendered.

03:14:11.000 --> 03:14:19.000
Because NB Viewer is not a user application and it's actually a web application, it's not included with Anaconda.

03:14:19.000 --> 03:14:21.000
Therefore, I'll have to show you how to install it.

03:14:21.000 --> 03:14:25.000
The easiest way to install NB Viewer is using Docker.

03:14:25.000 --> 03:14:31.000
Docker is not included with Anaconda either, so I'll also have to show you how to install that.

03:14:31.000 --> 03:14:33.000
Docker is an emulation platform.

03:14:33.000 --> 03:14:39.000
It allows you to run applications inside an isolated environment called containers.

03:14:39.000 --> 03:14:45.000
Docker containers differ from virtual machines in that the containers share the host OS.

03:14:45.000 --> 03:14:49.000
Containers can also share dependencies with each other.

03:14:49.000 --> 03:14:58.000
This minimizes the distance between the container and the system hardware, which makes containers faster and smaller to install.

03:14:58.000 --> 03:15:05.000
To install Docker, first go to Docker's website at www.docker.com.

03:15:05.000 --> 03:15:09.000
Then click on the Get Started link in the top right hand corner.

03:15:09.000 --> 03:15:13.000
The instructions for getting started are operating system dependent.

03:15:13.000 --> 03:15:17.000
Because I'm running a Mac, I'll show you how to get started with Docker on a Mac.

03:15:17.000 --> 03:15:22.000
If you're running Linux or Windows, this page will look a little different for you.

03:15:22.000 --> 03:15:25.000
The first step is to install Docker tools.

03:15:25.000 --> 03:15:28.000
You can click on Install Docker on OS X.

03:15:28.000 --> 03:15:34.000
Scroll down to step two, where you'll see Install Docker Toolbox.

03:15:34.000 --> 03:15:36.000
Click on that and then scroll down.

03:15:36.000 --> 03:15:40.000
Click the Download button for Mac if you're on OS X.

03:15:40.000 --> 03:15:43.000
Once you have the Toolbox installer, run it.

03:15:43.000 --> 03:15:45.000
Follow the prompts in the wizard.

03:15:45.000 --> 03:15:48.000
Select a hard drive to install to.

03:15:48.000 --> 03:15:51.000
Enter your password when prompted.

03:15:51.000 --> 03:15:53.000
When done, click Continue.

03:15:53.000 --> 03:15:55.000
Then click Close.

03:15:55.000 --> 03:15:59.000
Now launch the Docker Quick Start Terminal.

03:15:59.000 --> 03:16:02.000
It takes a little while for it to start the machine.

03:16:02.000 --> 03:16:13.000
Once the process finishes, you can run Docker space run space hello dash world.

03:16:13.000 --> 03:16:21.000
You should see a Hello from Docker message, which confirms that your installation is working.

03:16:21.000 --> 03:16:25.000
In the last video, I introduced you to NBViewer and Docker.

03:16:25.000 --> 03:16:28.000
We then installed Docker on your machine.

03:16:28.000 --> 03:16:31.000
In this video, we'll install the NBViewer Docker image.

03:16:31.000 --> 03:16:35.000
To get started, open the Docker Quick Terminal.

03:16:35.000 --> 03:16:37.000
Your terminal may take a while to start.

03:16:37.000 --> 03:16:42.000
Once the terminal has started, pay attention to the IP address listed in green.

03:16:42.000 --> 03:16:46.000
Mine's 192.168.99.100.

03:16:46.000 --> 03:16:49.000
That is the IP address of the Docker image.

03:16:49.000 --> 03:16:55.000
You'll use that IP address to access your NBViewer server once it's started.

03:16:55.000 --> 03:16:57.000
The first step is to download NBViewer.

03:16:57.000 --> 03:16:59.000
Now, I've already done this ahead of time.

03:16:59.000 --> 03:17:04.000
So mine will download fairly quick because it will just be verifying that I have the latest version.

03:17:04.000 --> 03:17:10.000
But the first time you run this command, it may take a while.

03:17:10.000 --> 03:17:18.000
Next, let's try launching NBViewer.

03:17:18.000 --> 03:17:22.000
Once the server starts, it should tell you the port it's listening on.

03:17:22.000 --> 03:17:27.000
In a new web browser, go ahead and try accessing that IP address that you remember that was in green,

03:17:27.000 --> 03:17:32.000
followed by colon 8080.

03:17:32.000 --> 03:17:35.000
If all worked well, you should see NBViewer.

03:17:35.000 --> 03:17:38.000
Go ahead and try to open up a notebook.

03:17:38.000 --> 03:17:41.000
Once the notebook opens, go back to your terminal.

03:17:41.000 --> 03:17:46.000
You should see output from the NBViewer server verifying your request.

03:17:46.000 --> 03:17:49.000
Without this, it would be hard to tell if you were actually running the server or not,

03:17:49.000 --> 03:17:53.000
or if you were just accessing the public NBViewer deployment by Jupyter.

03:17:53.000 --> 03:17:59.000
NBViewer has this wonderful feature that allows you to access notebooks on GitHub using short URLs.

03:17:59.000 --> 03:18:05.000
To demonstrate this, I'll access a notebook that's stored as a gist under my GitHub account.

03:18:05.000 --> 03:18:08.000
So here's a simple notebook I created for PyData.

03:18:08.000 --> 03:18:12.000
It's stored under my account as this gist.

03:18:12.000 --> 03:18:14.000
I'm going to just copy this URL.

03:18:14.000 --> 03:18:20.000
Because NBViewer has support for gist, I can just paste it directly in and click go.

03:18:20.000 --> 03:18:26.000
Alternatively, I can use an even shorter form, which is just the gist ID.

03:18:26.000 --> 03:18:30.000
To do so, I'll remove all the stuff before the last forward slash.

03:18:30.000 --> 03:18:32.000
This is my gist ID.

03:18:32.000 --> 03:18:34.000
You can see NBViewer still renders it.

03:18:34.000 --> 03:18:37.000
The GitHub public APIs have rate limiting.

03:18:37.000 --> 03:18:43.000
So if you plan on supporting this feature, it's a good idea to generate an access token for NBViewer.

03:18:43.000 --> 03:18:45.000
Doing so is relatively painless.

03:18:45.000 --> 03:18:48.000
Log on to github.com using your account.

03:18:48.000 --> 03:18:52.000
Then in the top right hand corner, click view profile and more.

03:18:52.000 --> 03:18:56.000
Next, select your profile.

03:18:56.000 --> 03:18:58.000
Click edit profile.

03:18:58.000 --> 03:19:03.000
Then click personal access tokens in the left hand column.

03:19:03.000 --> 03:19:06.000
Next, click generate new token.

03:19:06.000 --> 03:19:08.000
Give the token a name.

03:19:08.000 --> 03:19:12.000
And then change the scopes that you want to use to restrict the token.

03:19:12.000 --> 03:19:14.000
When you're done, click generate token.

03:19:14.000 --> 03:19:17.000
Your token will be displayed in the green bar.

03:19:17.000 --> 03:19:20.000
I've blurred a couple of the numbers of my token for security.

03:19:20.000 --> 03:19:24.000
Click the copy button to copy the token to your clipboard.

03:19:24.000 --> 03:19:29.000
Now in the terminal that's running NBViewer, hit ctrl C to stop NBViewer.

03:19:29.000 --> 03:19:37.000
Now let's relaunch NBViewer, adding our new access token to the command line.

03:19:37.000 --> 03:19:44.000
Because we're running NBViewer as a Docker image, we can't specify arguments directly to NBViewer.

03:19:44.000 --> 03:19:49.000
Instead, we have to set environment variables to cause NBViewer to change its behavior.

03:19:49.000 --> 03:19:59.000
Here, I'm telling Docker to set the github underscore api underscore token variable to the token that I just copied from github.

03:19:59.000 --> 03:20:03.000
Now when I try accessing NBViewer, it should be using that token.

03:20:03.000 --> 03:20:06.000
Let's paste the same gist id from earlier.

03:20:06.000 --> 03:20:12.000
Now let's go to github.com to see if the api token was used.

03:20:12.000 --> 03:20:16.000
We can see that it was just used because github says it was used within the last day.

03:20:16.000 --> 03:20:21.000
Using this token should help lift some of the rate limits for github access.

03:20:21.000 --> 03:20:28.000
And it's also nice because it allows github to control who's accessing their APIs.

03:20:28.000 --> 03:20:32.000
In the last video, we installed NBViewer using Docker.

03:20:32.000 --> 03:20:34.000
This is great for most use cases.

03:20:34.000 --> 03:20:39.000
However, sometimes it's necessary to maintain more control over the distribution.

03:20:39.000 --> 03:20:43.000
To do this, you can install NBViewer from source.

03:20:43.000 --> 03:20:45.000
This will allow you to do two things.

03:20:45.000 --> 03:20:50.000
One, it will allow you to control what dependencies NBViewer is using.

03:20:50.000 --> 03:21:00.000
And two, it will allow you to modify NBViewer's source code directly, including installing additional extensions without having to recompile the Docker image.

03:21:00.000 --> 03:21:03.000
The first step is to clone the NBViewer repository.

03:21:03.000 --> 03:21:13.000
You can either clone the upstream fork, like I will do here, or you can clone your own fork.

03:21:13.000 --> 03:21:17.000
Once NBViewer has finished cloning, cd into that directory.

03:21:17.000 --> 03:21:25.000
Now run pip install-r requirements dev.txt.

03:21:25.000 --> 03:21:32.000
Next run npm space install, then run invoke bower.

03:21:32.000 --> 03:21:34.000
This is installing the static assets.

03:21:34.000 --> 03:21:39.000
Next run invoke less, which will compile the less into CSS.

03:21:39.000 --> 03:21:41.000
CSS is what styles NBViewer.

03:21:41.000 --> 03:21:43.000
I've cleared my console.

03:21:43.000 --> 03:21:46.000
Now I'm going to run pip install markdown.

03:21:46.000 --> 03:21:51.000
Once that finishes, I should be able to launch NBViewer.

03:21:51.000 --> 03:21:55.000
Now I can access NBViewer using localhost.

03:21:55.000 --> 03:21:59.000
To verify that this is actually running locally, let's try changing some of the code.

03:21:59.000 --> 03:22:01.000
Let's change the title.

03:22:01.000 --> 03:22:04.000
I'm going to hit ctrl-c to stop the server.

03:22:04.000 --> 03:22:07.000
I'm going to open up Adam in the NBViewer repository.

03:22:07.000 --> 03:22:16.000
Once Adam opens, I'm going to open the NBViewer sub directory, the template sub folder, and then the index.html file.

03:22:16.000 --> 03:22:18.000
Let's change the title of the website.

03:22:18.000 --> 03:22:21.000
We'll change NBViewer to myNBViewer.

03:22:21.000 --> 03:22:23.000
We'll go ahead and save.

03:22:23.000 --> 03:22:27.000
Editing these templates directly is actually not the best way to modify NBViewer,

03:22:27.000 --> 03:22:31.000
but we'll do it for now just to verify that we've installed from source.

03:22:31.000 --> 03:22:35.000
Now back at the terminal, go ahead and relaunch the server.

03:22:35.000 --> 03:22:39.000
Back in your web browser, refresh the page.

03:22:39.000 --> 03:22:46.000
When you see the title update to myNBViewer, you know that the changes that we made to the template file were loaded.

03:22:46.000 --> 03:22:51.000
If when you refresh the page, the title doesn't change, try emptying your web browser's cache.

03:22:51.000 --> 03:22:58.000
If you want a quicker way to see if this is the problem, open an incognito tab and then navigate to the NBViewer web page.

03:22:58.000 --> 03:23:01.000
The incognito tab should prevent the web browser from caching.

03:23:01.000 --> 03:23:09.000
Often when you do web app development, caching causes problems because it doesn't let you see your most recent changes to the code.

03:23:09.000 --> 03:23:16.000
Earlier, I had mentioned that modifying the template directly in NBViewer's source was not the right way to modify the template.

03:23:16.000 --> 03:23:22.000
A better way would be to configure NBViewer's template directory to a different directory,

03:23:22.000 --> 03:23:29.000
have it load from one of your own custom templates, which inherit it from the template included with NBViewer.

03:23:29.000 --> 03:23:37.000
In the following videos, we'll look at how we can do that in addition to customizing NBViewer different ways.

03:23:37.000 --> 03:23:43.000
In this video, we'll look at what we can do just by extending the NBViewer templates.

03:23:43.000 --> 03:23:47.000
Before we get started, we need to remove the hack that we added in the last video.

03:23:47.000 --> 03:23:51.000
I'm going to go ahead and launch Adam from within the NBViewer repository.

03:23:51.000 --> 03:23:59.000
Once Adam's launched, I'll open the NBViewer subfolder, then the template subfolder, and then index.html.

03:23:59.000 --> 03:24:02.000
In there, I'll remove my space.

03:24:02.000 --> 03:24:04.000
Now, I'll save the file.

03:24:04.000 --> 03:24:07.000
Let's see who loads this index.html file.

03:24:07.000 --> 03:24:16.000
I'm going to open the find in project dialog by hitting command shift F, which is control shift F on Linux and Windows.

03:24:16.000 --> 03:24:21.000
Looks like the template is rendered here in the index handler method.

03:24:21.000 --> 03:24:27.000
Let's see where the render template method searches for index.html.

03:24:27.000 --> 03:24:33.000
Looks like the definition of render template is in the NBViewer provider's base.py class.

03:24:33.000 --> 03:24:36.000
The get template method is used to load the template.

03:24:36.000 --> 03:24:40.000
Inside the get template method, which is above the render template method,

03:24:40.000 --> 03:24:46.000
we can see that the Jinja2 environment has another get template method defined, which we call out to.

03:24:46.000 --> 03:24:50.000
Let's see where this Jinja2 environment comes from.

03:24:50.000 --> 03:24:53.000
Looks like it's defined in app.py.

03:24:53.000 --> 03:25:02.000
Scrolling up to see where nv is defined, we see nv is an instance of environment, which is imported from Jinja2.

03:25:03.000 --> 03:25:08.000
The template loader is a file system loader, which loads from template paths.

03:25:08.000 --> 03:25:13.000
Template paths is hard coded to the repository directory template subdirectory.

03:25:13.000 --> 03:25:21.000
However, if you specify a custom template path using the nbViewer underscore template underscore path environment variable,

03:25:21.000 --> 03:25:28.000
it gets propended to a list of paths, which then is used as the higher priority path.

03:25:28.000 --> 03:25:33.000
So we can set a custom template search path just by setting that environment variable.

03:25:33.000 --> 03:25:37.000
Knowing this, we can set the nbViewer template path.

03:25:37.000 --> 03:25:44.000
I'm going to set it to the nbViewer underscore templates subfolder of my home directory.

03:25:44.000 --> 03:25:47.000
Now I'm going to create that directory.

03:25:47.000 --> 03:25:51.000
I'll cd into it and open Adam.

03:25:51.000 --> 03:25:55.000
In Adam, I'll create an index.html file.

03:25:55.000 --> 03:26:02.000
This file will override the index.html file in the nbViewer templates folder.

03:26:02.000 --> 03:26:05.000
For now, I'll just write hello world and save the file.

03:26:05.000 --> 03:26:11.000
Now, switching back to the terminal, I'll cd back into the nbViewer repository.

03:26:11.000 --> 03:26:15.000
I'll launch nbViewer using the same command from earlier.

03:26:15.000 --> 03:26:19.000
Now when I try to access nbViewer, the page just says hello world.

03:26:19.000 --> 03:26:21.000
This means that our template was loaded successfully.

03:26:21.000 --> 03:26:23.000
Let's try to complicate things.

03:26:23.000 --> 03:26:28.000
Back inside the Adam that is opened in the nbViewer repository,

03:26:28.000 --> 03:26:33.000
I'm going to go to the templates folder and open index.html again.

03:26:33.000 --> 03:26:38.000
nbViewer uses the Jinja templating library to render its HTML pages.

03:26:38.000 --> 03:26:41.000
This funky syntax extends and block body.

03:26:41.000 --> 03:26:44.000
Those are Jinja 2 specific keywords.

03:26:44.000 --> 03:26:47.000
The rest of the code that you see is vanilla HTML.

03:26:47.000 --> 03:26:50.000
Let's go ahead and copy all the contents of this file.

03:26:50.000 --> 03:26:57.000
Back into our index.html file inside the nbViewer templates folder.

03:26:57.000 --> 03:27:01.000
Now let's change the title here and save.

03:27:01.000 --> 03:27:06.000
If we've done this correctly, we'll have changed the look of the nbViewer landing page

03:27:06.000 --> 03:27:09.000
without actually modifying nbViewer's source code.

03:27:09.000 --> 03:27:16.000
I refreshed the nbViewer page and it looks like our custom template was loaded.

03:27:16.000 --> 03:27:21.000
To give ourselves a target, let's try to set up an O'Reilly themed nbViewer.

03:27:21.000 --> 03:27:25.000
Our O'Reilly nbViewer should look like its O'Reilly's nbViewer,

03:27:25.000 --> 03:27:28.000
but also host O'Reilly content.

03:27:28.000 --> 03:27:33.000
First, let's change the basic index template that we created in the last video.

03:27:33.000 --> 03:27:39.000
To do so, I'll open up the nbViewer templates folder that we created in my home directory.

03:27:39.000 --> 03:27:43.000
Now I'll open Adam in that directory.

03:27:43.000 --> 03:27:47.000
I'll change the title to O'Reilly notebooks.

03:27:47.000 --> 03:27:51.000
We'll also change the descriptive paragraph below.

03:27:51.000 --> 03:27:54.000
Eventually, we won't want to be hosting notebooks from GitHub,

03:27:54.000 --> 03:27:58.000
so let's change the placeholder text to reflect that.

03:27:58.000 --> 03:28:00.000
Now I'll save and see how it looks.

03:28:00.000 --> 03:28:04.000
We can launch nbViewer using the same command that we used in the previous video.

03:28:04.000 --> 03:28:06.000
I'll create a new tab of my terminal.

03:28:06.000 --> 03:28:12.000
Before starting nbViewer, I need to set the environment variable again for the custom templates.

03:28:13.000 --> 03:28:16.000
Now I can launch the server.

03:28:16.000 --> 03:28:18.000
It looks like our change is rendered.

03:28:18.000 --> 03:28:21.000
However, we should probably change this logo in the top left,

03:28:21.000 --> 03:28:24.000
and also remove this link to Jupyter.

03:28:24.000 --> 03:28:27.000
Let's scroll down to see if there's anything else we need to change.

03:28:27.000 --> 03:28:32.000
We'll have to change this section of showcased notebooks.

03:28:32.000 --> 03:28:36.000
And at the very bottom, it looks like we'll want to change the footer.

03:28:36.000 --> 03:28:39.000
Lastly, we should probably change the styling

03:28:39.000 --> 03:28:42.000
and maybe use JavaScript to spiff up the page a bit.

03:28:42.000 --> 03:28:45.000
First, let's see if we can change the header and footer.

03:28:45.000 --> 03:28:49.000
Let's go back to the index.html file in our custom template folder.

03:28:49.000 --> 03:28:54.000
Looking at the index.html file, it looks like layout.html is extended

03:28:54.000 --> 03:28:56.000
for the basic layout of the page.

03:28:56.000 --> 03:28:57.000
Let's open that.

03:28:57.000 --> 03:29:00.000
It should be inside the nbViewer directory.

03:29:00.000 --> 03:29:04.000
Inside the nbViewer repository in the nbViewer subfolder,

03:29:05.000 --> 03:29:09.000
under templates, we can find layout.html.

03:29:09.000 --> 03:29:12.000
Like we did with index, let's copy everything in here.

03:29:12.000 --> 03:29:16.000
Then, back inside our custom nbViewer templates folder,

03:29:16.000 --> 03:29:19.000
let's create a layout.html.

03:29:19.000 --> 03:29:23.000
Here, I'll paste all the contents from the other layout.html.

03:29:23.000 --> 03:29:25.000
Let's remove this link to Google Analytics,

03:29:25.000 --> 03:29:31.000
because this is the Google Analytics for the Jupyter deployment of nbViewer.

03:29:31.000 --> 03:29:34.000
Also, we'll want to get rid of these links to Fastly

03:29:34.000 --> 03:29:39.000
and change the Rackspace link to O'Reilly.

03:29:39.000 --> 03:29:43.000
Scrolling up, let's get rid of the text that says this website does not host notebooks.

03:29:43.000 --> 03:29:46.000
It only renders notebooks available on other websites,

03:29:46.000 --> 03:29:51.000
because we're going to be using this pseudo-website to host O'Reilly notebooks.

03:29:51.000 --> 03:29:55.000
Here's the link to Jupyter that we wanted to remove.

03:29:55.000 --> 03:29:59.000
Lastly, we'll want to change the nav logo to O'Reilly's logo.

03:29:59.000 --> 03:30:03.000
Let's go to O'Reilly's website to see if we can get the link to their logo.

03:30:03.000 --> 03:30:07.000
I'm on O'Reilly's website now at www.oreilly.com.

03:30:07.000 --> 03:30:09.000
I like this logo in the top left-hand corner.

03:30:09.000 --> 03:30:13.000
I'm going to right-click on it and click Copy Image URL.

03:30:13.000 --> 03:30:16.000
Back inside the layout.html file,

03:30:16.000 --> 03:30:23.000
I'm then going to paste that URL over the image URL for the existing nav logo.

03:30:23.000 --> 03:30:26.000
We'll also get rid of the New Relic reference.

03:30:26.000 --> 03:30:31.000
Let's save what we have and go back to the browser to see how it renders.

03:30:31.000 --> 03:30:35.000
Awesome! This is already looking a little more O'Reilly-like.

03:30:35.000 --> 03:30:37.000
We'll probably still want to change the color scheme,

03:30:37.000 --> 03:30:40.000
because I noticed when I roll over FAQ, it highlights orange,

03:30:40.000 --> 03:30:45.000
which doesn't match O'Reilly's red.

03:30:45.000 --> 03:30:50.000
Looking at the bottom of the page, it looks like our footer updated it correctly.

03:30:50.000 --> 03:30:53.000
Let's check out the FAQ page.

03:30:53.000 --> 03:30:55.000
It looks like there's some questions that shouldn't be here.

03:30:55.000 --> 03:30:57.000
Let's remove them.

03:30:57.000 --> 03:30:59.000
Back inside the nbviewer repository,

03:30:59.000 --> 03:31:03.000
it looks like the FAQ.md file might be the file that's getting rendered.

03:31:03.000 --> 03:31:04.000
Let's open that.

03:31:04.000 --> 03:31:08.000
It looks like this file does indeed extend the layout.html file

03:31:08.000 --> 03:31:12.000
and uses a special markdown filter to convert itself from markdown to html.

03:31:12.000 --> 03:31:16.000
In the process, it automatically generates its table of contents.

03:31:16.000 --> 03:31:22.000
Let's do what we did for index.html and layout.html in our custom templates folder.

03:31:22.000 --> 03:31:32.000
Let's create an FAQ.md file and copy the contents from the FAQ.md file in nbviewer.

03:31:32.000 --> 03:31:34.000
Let's get rid of the first two questions,

03:31:34.000 --> 03:31:38.000
because they are completely specific to Jupyter's nbviewer.

03:31:38.000 --> 03:31:50.000
We'll defer them to nbviewer for this information.

03:31:50.000 --> 03:31:53.000
This paragraph doesn't relate at all to our viewer,

03:31:53.000 --> 03:31:56.000
nor does the one below or the one below that.

03:31:56.000 --> 03:31:58.000
This paragraph also doesn't relate.

03:31:58.000 --> 03:31:59.000
This is related, though.

03:31:59.000 --> 03:32:03.000
We just need to update it to point to our email address.

03:32:03.000 --> 03:32:06.000
The last few before the final one also don't relate,

03:32:06.000 --> 03:32:12.000
and we'll replace the text of the final one with an email link to the O'Reilly administrator.

03:32:12.000 --> 03:32:16.000
Now let's save and see if the FAQ page renders how we want.

03:32:16.000 --> 03:32:19.000
Back in the web browser, I'm going to refresh the page.

03:32:19.000 --> 03:32:26.000
Looks like we should remove the first question as well.

03:32:26.000 --> 03:32:29.000
Let's refresh the page again.

03:32:29.000 --> 03:32:31.000
Ah, much better.

03:32:31.000 --> 03:32:34.000
Let's try clicking on the O'Reilly image to go back to the homepage.

03:32:34.000 --> 03:32:36.000
Sweet, it worked.

03:32:36.000 --> 03:32:44.000
In the next tutorial, we'll look at adding custom CSS to style it more like O'Reilly's main website.

03:32:44.000 --> 03:32:49.000
In this video, we'll talk about how nbviewer compiles its less into CSS.

03:32:49.000 --> 03:32:54.000
We'll then look at adding our own CSS to our custom nbviewer templates.

03:32:54.000 --> 03:32:58.000
I've still left the nbviewer server running from the last video.

03:32:58.000 --> 03:33:01.000
This is because I do not need to restart it.

03:33:01.000 --> 03:33:08.000
As long as I'm only changing static files, all I have to do is refresh the webpage to update the contents.

03:33:08.000 --> 03:33:12.000
If I were working on server-side files, for example the Python files,

03:33:12.000 --> 03:33:15.000
then I would have to restart the server.

03:33:15.000 --> 03:33:20.000
Let's go ahead and open up Adam inside the nbviewer repository.

03:33:20.000 --> 03:33:26.000
When you installed nbviewer from source code, you had to run a command called invoke less.

03:33:26.000 --> 03:33:34.000
When you ran that command, what it did was run a function called less inside the tasks.py file.

03:33:34.000 --> 03:33:36.000
Here's that function.

03:33:36.000 --> 03:33:42.000
What this function does is compile the less into CSS using the less compiler.

03:33:42.000 --> 03:33:46.000
It outputs the compiled CSS into a build subdirectory.

03:33:46.000 --> 03:33:52.000
It outputs a styles.css, notebook.css, and slides.css.

03:33:52.000 --> 03:33:59.000
Likewise, the source files used are styles, notebook, and slides.less.

03:33:59.000 --> 03:34:03.000
Let's open the nbviewer static directory.

03:34:03.000 --> 03:34:07.000
In here, you see the folder less and the build folder.

03:34:07.000 --> 03:34:12.000
The build folder is grayed out here because it's not included in the git repository.

03:34:12.000 --> 03:34:15.000
That's because we don't want to check in the built files.

03:34:15.000 --> 03:34:17.000
That would just be including changes twice.

03:34:17.000 --> 03:34:20.000
The less folder is where the less is stored.

03:34:20.000 --> 03:34:24.000
We can open up the notebook.less to get an idea of how notebooks are styled.

03:34:24.000 --> 03:34:29.000
A major difference between less and CSS is that less allows you to import.

03:34:29.000 --> 03:34:34.000
Here you can see that bootstrap is imported and styling from ipython.

03:34:34.000 --> 03:34:38.000
Let's go ahead and see where the build files are referenced.

03:34:38.000 --> 03:34:46.000
In the layout template, inside the header, we see that styles.css is referenced.

03:34:46.000 --> 03:34:52.000
Inside notebook.html, we can see where notebook.css is referenced.

03:34:52.000 --> 03:34:56.000
Let's go ahead and add our own styling to our custom templates.

03:34:56.000 --> 03:35:01.000
Going back to the terminal, I'm going to cd into our custom templates directory.

03:35:01.000 --> 03:35:04.000
Here I'll open Adam.

03:35:04.000 --> 03:35:10.000
Inside our layout.html, below the existing CSS import, let's add our own.

03:35:10.000 --> 03:35:16.000
It's important that you do this below the existing because this will cause your style to override the existing.

03:35:16.000 --> 03:35:22.000
Unfortunately, nbviewer doesn't support pulling files from directories outside of its own, so we have two options.

03:35:22.000 --> 03:35:29.000
We could either place our custom style inside the nbviewer repository, which I'd rather not do,

03:35:29.000 --> 03:35:36.000
or we can use the ginga templating to load it from our nbviewer templates directory and then inline it directly into the html.

03:35:36.000 --> 03:35:42.000
First, let me show you what it would look like if you were to put the CSS inside the nbviewer repository.

03:35:42.000 --> 03:35:48.000
You would change build to CSS and then give your CSS file a name, like custom.

03:35:48.000 --> 03:35:56.000
You would then save this file and inside the nbviewer repository under the static directory in CSS,

03:35:56.000 --> 03:36:01.000
you would right-click, create a new file called custom.css.

03:36:01.000 --> 03:36:06.000
And then inside here, you would put whatever custom CSS you want.

03:36:06.000 --> 03:36:14.000
Moving back into our nbviewer templates directory, the alternative, I think, makes more sense because then you can keep your CSS next to your templates.

03:36:14.000 --> 03:36:19.000
For this, instead of using a link tag, you'll use a style tag.

03:36:19.000 --> 03:36:26.000
Then inside style tags, use the ginga include to include your style file.

03:36:26.000 --> 03:36:32.000
The only downside to using this method is that you're disabling the browser's ability to cache your style,

03:36:32.000 --> 03:36:36.000
which means that every time a page is requested, client will have to download the CSS again.

03:36:36.000 --> 03:36:43.000
That's usually not a problem with small CSS files, and if it is a problem, you can use the other method that I just showed you.

03:36:43.000 --> 03:36:48.000
So now let's save this file and create our own custom CSS.

03:36:48.000 --> 03:36:55.000
To test to see if our custom CSS is working, let's try setting the body background color.

03:36:55.000 --> 03:37:00.000
I'm going to use important just to make sure it overrides any other values.

03:37:00.000 --> 03:37:06.000
However, it's important to note that important isn't the best practice.

03:37:06.000 --> 03:37:10.000
Using important disables you from later overriding styles.

03:37:10.000 --> 03:37:15.000
In a new browser window, let's navigate to our NB viewer page to see if our style gets loaded.

03:37:15.000 --> 03:37:21.000
Awesome, it looks like the style loaded successfully.

03:37:21.000 --> 03:37:28.000
Now, instead of applying such a hideous style, let's try to override the orange highlight color that's applied to buttons.

03:37:28.000 --> 03:37:33.000
Let's inspect the FAQ button to see how we can select it using CSS.

03:37:33.000 --> 03:37:40.000
Looks like a good selector would be to use the navbar right class and then the anchor tag.

03:37:40.000 --> 03:37:44.000
Back inside our custom CSS, let's do that.

03:37:44.000 --> 03:37:49.000
To specify that we want to change the styling when it is hovered over, add the hover sudo selector.

03:37:49.000 --> 03:37:52.000
For now, let's just try changing the background color.

03:37:52.000 --> 03:37:58.000
Again, let's use the important tag just to make sure that what we're doing gets applied.

03:37:58.000 --> 03:38:00.000
Looks like that worked.

03:38:00.000 --> 03:38:07.000
So now, let's change the font color instead of changing the background color and let's actually use O'Reilly's red.

03:38:07.000 --> 03:38:14.000
Let's go to O'Reilly's website and we'll right click on the home link to look at its color.

03:38:14.000 --> 03:38:17.000
Now, I'll just double click this and copy it.

03:38:17.000 --> 03:38:23.000
Back inside our custom CSS, I'm going to change background color to color and paste this new color.

03:38:23.000 --> 03:38:30.000
Let's save the file, then go back to the web browser where I'll open our MB Viewer tab and refresh the page.

03:38:30.000 --> 03:38:32.000
Looks like that works.

03:38:32.000 --> 03:38:37.000
Now back inside the custom CSS, let's try to move the important flag.

03:38:37.000 --> 03:38:42.000
Like I said earlier, it's better to not use important when you can get away with it.

03:38:42.000 --> 03:38:46.000
Back in the browser, let's refresh the page and see if it still works.

03:38:46.000 --> 03:38:48.000
Looks like it's no longer working.

03:38:48.000 --> 03:38:55.000
We have two options. We can either stick with the important flag or we can try to make our selector more specific.

03:38:55.000 --> 03:39:03.000
Because I know that I'm applying the top most level styling and nobody's going to come in and inherit from the O'Reilly page and add their own styling,

03:39:03.000 --> 03:39:05.000
it's okay for me to use important.

03:39:05.000 --> 03:39:11.000
If, however, you were writing something that would later be styled by somebody else, you'd want to make the selector more specific.

03:39:11.000 --> 03:39:19.000
To do so, you could inspect the element and either A, add more levels of elements to your selector,

03:39:19.000 --> 03:39:26.000
or B, in the templates, actually add an ID to this anchor tag and then address the anchor tag by ID.

03:39:26.000 --> 03:39:32.000
Addressing an element by ID has a higher specificity than addressing it otherwise.

03:39:32.000 --> 03:39:36.000
Back inside the custom CSS, let's re-add the important.

03:39:36.000 --> 03:39:39.000
I'm going to refresh the browser page.

03:39:39.000 --> 03:39:41.000
Looks like that's still working.

03:39:41.000 --> 03:39:43.000
Let's scroll down to the bottom of the page.

03:39:43.000 --> 03:39:47.000
Maybe we should use one of O'Reilly's grays for this bottom.

03:39:47.000 --> 03:39:51.000
We could also use O'Reilly's red for the links.

03:39:51.000 --> 03:39:53.000
This gray looks nice.

03:39:53.000 --> 03:39:56.000
We'll copy the background color.

03:39:56.000 --> 03:40:02.000
Now back on the Jupyter NB viewer tab, let's try styling this footer.

03:40:02.000 --> 03:40:07.000
The font doesn't have enough contrast now. Let's change it to black.

03:40:07.000 --> 03:40:13.000
That seems like it has too much contrast. Let's see what O'Reilly does.

03:40:13.000 --> 03:40:18.000
Looks like they use an off black. We'll use that too.

03:40:18.000 --> 03:40:24.000
I'd also like to add a top border.

03:40:24.000 --> 03:40:28.000
Let's copy the border color that O'Reilly uses.

03:40:28.000 --> 03:40:33.000
Looks like they use this off-shaded gray.

03:40:33.000 --> 03:40:38.000
Now we can just copy this CSS that we've designed in the browser

03:40:38.000 --> 03:40:42.000
and paste it into our custom CSS in a footer selector.

03:40:42.000 --> 03:40:44.000
Now let's refresh the page.

03:40:44.000 --> 03:40:48.000
Scrolling to the bottom, we see that our new styling has been applied.

03:40:48.000 --> 03:40:52.000
Lastly, we need to change the default link color to that red.

03:40:52.000 --> 03:40:58.000
Back on our custom CSS, let's define an anchor selector.

03:40:58.000 --> 03:41:03.000
We have four C problems with this anchor tag and this anchor tag.

03:41:03.000 --> 03:41:10.000
Let's define a color for when the FAQ anchor tag is not hovered on.

03:41:10.000 --> 03:41:16.000
We'll use the color that we used for text.

03:41:16.000 --> 03:41:22.000
I'm going to save and then go back to the browser and refresh the page one more time.

03:41:22.000 --> 03:41:25.000
The FAQ button is still working. Scroll to the bottom.

03:41:25.000 --> 03:41:30.000
And it looks like our links are formatted correctly now.

03:41:30.000 --> 03:41:36.000
In the last video, we looked at customizing our MbViewer deployments CSS.

03:41:36.000 --> 03:41:40.000
In this video, we used JavaScript to spiff up the website a little bit.

03:41:40.000 --> 03:41:44.000
I found this really cool carousel on Bootstrap's website.

03:41:44.000 --> 03:41:45.000
Here it is.

03:41:45.000 --> 03:41:48.000
Bootstrap is a component that MbViewer already uses,

03:41:48.000 --> 03:41:51.000
so we should be able to just drag and drop this code into place.

03:41:51.000 --> 03:41:56.000
What I want to do is replace the notebook listing in the showcase

03:41:56.000 --> 03:41:59.000
on our MbViewer with a carousel.

03:41:59.000 --> 03:42:06.000
So I'm going to go back to the Bootstrap website and copy and paste the code here.

03:42:06.000 --> 03:42:10.000
Inside the index template in our custom templates folder,

03:42:10.000 --> 03:42:15.000
scrolling down towards the bottom, you can see where the showcase is built.

03:42:15.000 --> 03:42:19.000
The JINJA templating for loop is used to iterate over each section

03:42:19.000 --> 03:42:23.000
and then it's used again to iterate over each link in each section.

03:42:23.000 --> 03:42:27.000
We'll use this logic to compile the different slides for our carousel.

03:42:27.000 --> 03:42:32.000
For now, I'm going to insert the carousel code above this existing code

03:42:32.000 --> 03:42:35.000
in between the header and the showcase,

03:42:35.000 --> 03:42:38.000
pasting what we copied from Bootstrap's website.

03:42:38.000 --> 03:42:41.000
I'm going to remove the indicator dots on the carousel.

03:42:41.000 --> 03:42:47.000
Also, from experience, I know that we're not loading glyph icon on MbViewer by default,

03:42:47.000 --> 03:42:49.000
and I don't feel like adding that dependency.

03:42:49.000 --> 03:42:51.000
Instead, we're using font awesome.

03:42:51.000 --> 03:43:00.000
Equivalent icons would be icon-prev and icon-next.

03:43:00.000 --> 03:43:05.000
Now, what we need to do is use that JINJA code that iterates through each item

03:43:05.000 --> 03:43:07.000
to construct our carousel slides.

03:43:07.000 --> 03:43:10.000
It looks like each individual unit is an item.

03:43:10.000 --> 03:43:12.000
The first item is active.

03:43:12.000 --> 03:43:15.000
Let's go ahead and delete the ellipses.

03:43:15.000 --> 03:43:20.000
Now, let's move the JINJA templating loop logic below this first item

03:43:20.000 --> 03:43:25.000
to create the latter items.

03:43:25.000 --> 03:43:29.000
We're going to just ignore the notion of sections,

03:43:29.000 --> 03:43:35.000
so we'll group both the loops next to each other.

03:43:35.000 --> 03:43:39.000
Now, let's copy the item template into the loop.

03:43:39.000 --> 03:43:43.000
Then we'll copy the image source into the item's image.

03:43:43.000 --> 03:43:48.000
We'll also copy the link text as the alternative text

03:43:48.000 --> 03:43:50.000
and use it as the caption.

03:43:50.000 --> 03:43:54.000
Then we'll take the anchor tag and put it around the caption.

03:43:54.000 --> 03:43:57.000
This will make the caption clickable.

03:43:57.000 --> 03:44:01.000
Now, finally, we'll remove the original code from the gallery.

03:44:01.000 --> 03:44:07.000
We'll save our changes and refresh the page to see how it renders.

03:44:07.000 --> 03:44:08.000
So here's the page.

03:44:08.000 --> 03:44:11.000
You can see it doesn't have the gallery below anymore.

03:44:11.000 --> 03:44:14.000
Now it just has this carousel that rotates through images.

03:44:14.000 --> 03:44:19.000
And each image has a link that we can click to open that notebook.

03:44:19.000 --> 03:44:24.000
However, you may notice the size is constantly changing.

03:44:24.000 --> 03:44:27.000
It must depend on the image height.

03:44:27.000 --> 03:44:29.000
Let's fix the size of the carousel.

03:44:29.000 --> 03:44:31.000
We'll do so using CSS.

03:44:31.000 --> 03:44:34.000
First, let's get the ID of the carousel.

03:44:34.000 --> 03:44:35.000
Copy that.

03:44:35.000 --> 03:44:39.000
Then in your custom CSS, add a selector for the carousel.

03:44:39.000 --> 03:44:42.000
To select an ID, prefix with the hashtag.

03:44:42.000 --> 03:44:48.000
Now set the height to 300 pixels and the width to 300 pixels.

03:44:48.000 --> 03:44:50.000
Save.

03:44:50.000 --> 03:44:53.000
And let's go back to the web browser to see how that renders.

03:44:53.000 --> 03:44:55.000
We're going to refresh the page.

03:44:55.000 --> 03:44:58.000
Here's what our smaller carousel looks like.

03:44:58.000 --> 03:45:02.000
We should probably center it in the page and add a margin.

03:45:02.000 --> 03:45:07.000
It looks kind of weird hugging the bottom so closely in the top.

03:45:07.000 --> 03:45:12.000
Let's try centering it in the web browser.

03:45:12.000 --> 03:45:16.000
By setting margin left and right to auto, the element will center.

03:45:16.000 --> 03:45:22.000
Now let's add a top margin to give it some distance from this horizontal line.

03:45:22.000 --> 03:45:24.000
40 pixels looks good.

03:45:24.000 --> 03:45:27.000
Let's do the same with the bottom.

03:45:27.000 --> 03:45:29.000
Now take one last look.

03:45:29.000 --> 03:45:31.000
That looks good.

03:45:31.000 --> 03:45:35.000
Let's copy and paste this style back to our CSS.

03:45:35.000 --> 03:45:42.000
Oops, looks like I forgot to copy margin left and right auto.

03:45:42.000 --> 03:45:46.000
Now we need to get rid of that placeholder for the first active item.

03:45:46.000 --> 03:45:51.000
In index.html, in the carousel code, you can see that item here.

03:45:51.000 --> 03:45:53.000
Go ahead and remove that.

03:45:53.000 --> 03:45:57.000
What we need to do is only add active to the first class.

03:45:57.000 --> 03:46:00.000
To do that, let's create a flag.

03:46:00.000 --> 03:46:03.000
Once that flag is used once, we'll set it the false.

03:46:03.000 --> 03:46:11.000
We can use the ginger set command to set this flag.

03:46:11.000 --> 03:46:17.000
Then we'll test for that in class.

03:46:17.000 --> 03:46:23.000
Lastly, let's make sure we set first the false.

03:46:23.000 --> 03:46:27.000
When we set first the false here, we're actually declaring a new variable first

03:46:27.000 --> 03:46:33.000
within the scope of this for loop that overrides the first declared in the outer scope.

03:46:33.000 --> 03:46:37.000
This means when we get to the next for loop, first will be set to true again.

03:46:37.000 --> 03:46:40.000
So we have to set first the false twice.

03:46:40.000 --> 03:46:42.000
Let's refresh the page.

03:46:42.000 --> 03:46:44.000
Ah, looks like that worked.

03:46:44.000 --> 03:46:47.000
Awesome.

03:46:47.000 --> 03:46:55.000
In the last video, we talked about adding custom CSS and custom JavaScript to your NB viewer deployment.

03:46:55.000 --> 03:47:00.000
In this video, we'll talk about changing what NB viewer is hosting to the user.

03:47:00.000 --> 03:47:07.000
NB viewer has a notion of providers, which are the things that dictate what NB viewer can host.

03:47:07.000 --> 03:47:13.000
There are two types of providers, URI rewrites and handlers.

03:47:13.000 --> 03:47:18.000
URI rewrites take textual content that's entered into the go bar of NB viewer

03:47:18.000 --> 03:47:22.000
and translate it to a canonical NB viewer URL,

03:47:22.000 --> 03:47:26.000
a URL that NB viewer understands and is capable of rendering.

03:47:26.000 --> 03:47:32.000
Handlers are things that are designed to interpret and load from NB viewer URLs.

03:47:32.000 --> 03:47:39.000
The handler is the thing that actually fetches the resources from the local or remote location.

03:47:39.000 --> 03:47:49.000
For example, the GitHub handler accesses notebook content directly from GitHub using GitHub's API instead of standard HTTP.

03:47:49.000 --> 03:47:53.000
Let's start by configuring NB viewer to host local files.

03:47:53.000 --> 03:48:01.000
Sticking to our O'Reilly themed example, let's pretend that O'Reilly wants to host files from a network-attached storage device.

03:48:01.000 --> 03:48:05.000
Let's say that that storage device is SimLink into the home directory.

03:48:05.000 --> 03:48:08.000
We'll pretend that that SimLink is called network.

03:48:08.000 --> 03:48:16.000
I'm going to create this folder just as an example that we can use to demonstrate this feature of NB viewer.

03:48:16.000 --> 03:48:22.000
Let's pretend that in the network-attached storage drive, there's a subfolder called notebooks.

03:48:22.000 --> 03:48:26.000
And then inside the notebooks folder, there are author folders.

03:48:26.000 --> 03:48:31.000
For now, I'll just create an authored folder for myself.

03:48:31.000 --> 03:48:35.000
I have some example notebooks that are sitting inside my home folder.

03:48:35.000 --> 03:48:43.000
I'm going to copy those over to here.

03:48:43.000 --> 03:48:45.000
Now let's take a look at the NB viewer source code.

03:48:45.000 --> 03:48:51.000
I'm going to CD into the NB viewer repository and open Adam.

03:48:51.000 --> 03:48:56.000
Inside the NB viewer subfolder, I'm going to open app.py.

03:48:56.000 --> 03:49:02.000
Scrolling down to the very bottom of app.py, we see all the command line arguments that we can pass to NB viewer.

03:49:02.000 --> 03:49:05.000
One of the command line arguments is local files.

03:49:05.000 --> 03:49:09.000
This tells NB viewer to host files from the local file system.

03:49:09.000 --> 03:49:11.000
Let's use this.

03:49:11.000 --> 03:49:13.000
I've closed the NB viewer server.

03:49:13.000 --> 03:49:15.000
I'll relaunch it with this new command.

03:49:15.000 --> 03:49:26.000
But before I launch, remember that we need to set the correct environment variable in order for our custom templates to be loaded.

03:49:26.000 --> 03:49:33.000
Now let's launch NB viewer.

03:49:33.000 --> 03:49:38.000
Let's switch to the web browser to see if we can load files from the local files system.

03:49:38.000 --> 03:49:41.000
I'm going to try accessing the notebook using the go bar.

03:49:41.000 --> 03:49:48.000
I'll type in the subpath to the notebook from its location inside network.

03:49:48.000 --> 03:49:50.000
Doing that didn't work.

03:49:50.000 --> 03:49:55.000
This would make you want to jump to the conclusion that the local files setting isn't working.

03:49:55.000 --> 03:49:58.000
However, this is an invalid conclusion.

03:49:58.000 --> 03:50:06.000
If you pay attention to the URL, you'll see that URL for slash was prefixed to what we tried to access.

03:50:06.000 --> 03:50:12.000
This is telling NB viewer to use the URL handler to load the following content.

03:50:12.000 --> 03:50:21.000
Of course, notebooks for slash jd frederick four slash one dot ipynb is not a domain name and is not located within a public top level domain name.

03:50:21.000 --> 03:50:26.000
So it makes sense that URL would fail to load this content.

03:50:26.000 --> 03:50:31.000
Instead, what we need to do is change the URL prefix to local file.

03:50:31.000 --> 03:50:33.000
And that will get the notebook to load.

03:50:33.000 --> 03:50:35.000
We want to automate this though.

03:50:35.000 --> 03:50:46.000
We don't want the go bar to not work and we would like the go bar to automatically translate to this canonical and be viewer local file format.

03:50:46.000 --> 03:50:50.000
In the last video, we got the NB viewer local files provider working.

03:50:50.000 --> 03:50:54.000
However, we weren't able to access it via the go bar.

03:50:54.000 --> 03:51:02.000
In this video, we'll write a URI rewrite provider that will allow us to access local files easily from the go bar.

03:51:02.000 --> 03:51:07.000
The first step is to open up Adam inside your NB viewer repository.

03:51:07.000 --> 03:51:11.000
Next, open the NB viewer sub folder and inside that open providers.

03:51:11.000 --> 03:51:15.000
Here, you'll see a list of the providers that are default with NB viewer.

03:51:15.000 --> 03:51:21.000
The Dropbox provider has a URI rewrite, which is a good example for the rewrite that we're going to do.

03:51:21.000 --> 03:51:31.000
Let's copy the handlers dot py file and create a sub folder inside the providers folder called X for X for is going to be the name of our plugin.

03:51:31.000 --> 03:51:33.000
Paste the file inside there.

03:51:33.000 --> 03:51:38.000
You can also copy the init file.

03:51:38.000 --> 03:51:41.000
Now open the handlers dot py file that you copied.

03:51:41.000 --> 03:51:44.000
Go ahead and remove the ipython header.

03:51:44.000 --> 03:51:51.000
We want this URI rewrite to accept URIs of the form author forward slash notebook name.

03:51:51.000 --> 03:51:56.000
We'll accept the notebook name either with or without an IPYNB extension.

03:51:56.000 --> 03:51:59.000
The first step is to replace the first string in the tuple.

03:51:59.000 --> 03:52:02.000
This string is the string that is used to search.

03:52:02.000 --> 03:52:06.000
The second string is the string that replaces the search string.

03:52:06.000 --> 03:52:15.000
Each group of the regular expression where a group is defined by parentheses can be accessed in the replacement string by using curly brackets.

03:52:15.000 --> 03:52:24.000
So this zero refers to this first item here, whereas the one refers to this second group here.

03:52:24.000 --> 03:52:30.000
Without explaining too much of regular expressions, I'll tell you that this matches a set of characters of variable length.

03:52:30.000 --> 03:52:35.000
I'll remove this text here where this first group will match the author.

03:52:35.000 --> 03:52:37.000
Add a forward slash.

03:52:37.000 --> 03:52:38.000
Copy this first group.

03:52:38.000 --> 03:52:41.000
This second group will match the notebook name.

03:52:41.000 --> 03:52:44.000
And at the end, I'll add dot ipynb.

03:52:44.000 --> 03:52:49.000
And I have to escape the dot because dot has a special meaning in regular expressions.

03:52:49.000 --> 03:52:54.000
And add a question mark because we don't know if the user is going to write dot ipynb or not.

03:52:54.000 --> 03:53:05.000
Now in the replacement string, I'll replace the URL with local file because local file is the canonical form of the URI accepted by the local file provider.

03:53:05.000 --> 03:53:12.000
I'll also add notebooks because notebooks is the subfolder that sits inside the network folder.

03:53:12.000 --> 03:53:21.000
The first value will be the author name, followed by the notebook name, and then we'll append a dot ipynb file extension.

03:53:21.000 --> 03:53:27.000
Now let's save this and we'll go back to the terminal and try launching nbviewer.

03:53:27.000 --> 03:53:36.000
But first make sure to set the environment variable that uses the custom templates that we created earlier.

03:53:36.000 --> 03:53:38.000
Now let's try launching nbviewer.

03:53:42.000 --> 03:53:52.000
To get nbviewer to use our URI rewrite, we use the double dash provider underscore rewrites.

03:53:52.000 --> 03:53:58.000
The provider rewrites flag takes a full Python namespace to a rewrite provider.

03:53:58.000 --> 03:54:02.000
You may be wondering why we had to edit nbviewer directly.

03:54:02.000 --> 03:54:04.000
Well, we actually didn't have to.

03:54:04.000 --> 03:54:09.000
We could have wrote our own Python package and then reference that Python namespace here.

03:54:09.000 --> 03:54:14.000
However, writing a Python package is outside of the scope of this video series.

03:54:14.000 --> 03:54:18.000
So for simplicity, we edit it nbviewer directly.

03:54:18.000 --> 03:54:21.000
That allows us to piggyback on nbviewer's namespace here.

03:54:21.000 --> 03:54:29.000
So to access our rewrite, we can use nbviewer.providers.exfer.

03:54:29.000 --> 03:54:33.000
Lastly, we'll want to disable github and gis providers.

03:54:33.000 --> 03:54:39.000
To do so, we'll set the URL provider as the only provider used by nbviewer.

03:54:39.000 --> 03:54:49.000
We can do that using the double dash providers flag and setting that to nbviewer.providers.url.

03:54:49.000 --> 03:54:52.000
Now that the server is launched, let's go to our web browser.

03:54:52.000 --> 03:54:58.000
Let's try accessing the first notebook under my name here.

03:54:58.000 --> 03:55:00.000
Looks like that worked correctly.

03:55:00.000 --> 03:55:07.000
Let's go back to the homepage and try accessing it without the ipynb to make sure it still works.

03:55:07.000 --> 03:55:09.000
Looks like that worked too.

03:55:09.000 --> 03:55:16.000
The last thing we'll want to do is change the showcase so it shows notebooks that are actually hosted by us.

03:55:16.000 --> 03:55:20.000
To understand how this is done, let's look at the source code of nbviewer.

03:55:20.000 --> 03:55:25.000
Back inside Adam in the nbviewer repository, open up app.py.

03:55:25.000 --> 03:55:30.000
If you scroll towards the bottom, you'll see where all the command line arguments are defined.

03:55:30.000 --> 03:55:34.000
The command line argument that we're interested in is this front page argument.

03:55:34.000 --> 03:55:41.000
This argument points to a JSON file which defines the content that will be used on the front page to render the showcase.

03:55:41.000 --> 03:55:48.000
The default used by nbviewer sits inside the nbviewer repository under frontpage.json.

03:55:48.000 --> 03:55:49.000
Let's open that.

03:55:49.000 --> 03:55:52.000
Here you can see the links that we see when nbviewer runs.

03:55:52.000 --> 03:55:56.000
Let's copy all the contents of this file.

03:55:56.000 --> 03:56:05.000
And then in a new terminal window, let's cd into our custom nbviewer templates directory.

03:56:05.000 --> 03:56:10.000
The reason why I had you open this directory is because it's where we're storing a lot of other custom things for our server.

03:56:10.000 --> 03:56:15.000
We might as well store other content in here just to keep it all grouped in one place.

03:56:15.000 --> 03:56:18.000
Create a new file called gallery.json.

03:56:18.000 --> 03:56:25.000
Inside that file, paste the contents from the front page.json that we copied out of the nbviewer repository.

03:56:25.000 --> 03:56:31.000
Now, looking at this file, we see that it has groups defined by this header attribute.

03:56:31.000 --> 03:56:36.000
Since we're ignoring the notion of groups, let's get rid of all the other groups below.

03:56:36.000 --> 03:56:41.000
When we set up the dummy directory, I only copied two files into my author directory.

03:56:41.000 --> 03:56:43.000
So let's get rid of the third entry.

03:56:43.000 --> 03:56:48.000
We'll give the first two names.

03:56:48.000 --> 03:56:55.000
And then change the target to the canonical URL that points to the correct notebook.

03:56:55.000 --> 03:57:00.000
The URL for the second notebook is almost the same, just the notebook file is different.

03:57:00.000 --> 03:57:06.000
Now we could change the image as well, but I don't have any nice images for my test notebooks.

03:57:06.000 --> 03:57:09.000
So I'm just going to leave the images as is.

03:57:09.000 --> 03:57:13.000
I'm going to save this file and go back to the terminal.

03:57:13.000 --> 03:57:18.000
Opening the tab of the terminal that's running nbviewer, I'm going to stop nbviewer by hitting Ctrl C.

03:57:18.000 --> 03:57:29.000
I'm going to rerun the same command except this time I'll change front page to the full path of the JSON that specifies our gallery.

03:57:29.000 --> 03:57:32.000
Now let's open the web browser to see if that worked.

03:57:32.000 --> 03:57:36.000
Refreshing the home page, we see that my JSON was loaded.

03:57:36.000 --> 03:57:41.000
Does this URL now points to the Jons notebook, even though it's still using the old screenshot?

03:57:41.000 --> 03:57:45.000
Jons notebook 2 is also available, even though it's using the old screenshot.

03:57:45.000 --> 03:57:48.000
Let's click on the link to see if it works.

03:57:48.000 --> 03:57:50.000
Awesome, it looks like that worked.

03:57:50.000 --> 03:58:03.000
If you want to find out more about nbviewer, visit the nbviewer repository at www.github.com forward slash Jupiter forward slash nbviewer.

03:58:03.000 --> 03:58:06.000
In this chapter, I'm going to talk about temp nb.

03:58:06.000 --> 03:58:09.000
It stands for Temporary Notebook Server.

03:58:09.000 --> 03:58:15.000
Temp nb is a service that launches sandboxed ephemeral notebook servers on demand,

03:58:15.000 --> 03:58:19.000
where ephemeral is defined as something lasting for a short time.

03:58:19.000 --> 03:58:22.000
It's kind of like an interactive version of nbviewer.

03:58:22.000 --> 03:58:29.000
Temp nb is useful for cases where you need to share notebooks that lose importance if they're not interactive.

03:58:29.000 --> 03:58:34.000
Temp nb users can interact with your notebooks to see what they have to provide.

03:58:34.000 --> 03:58:38.000
They can explore the data sets and write their own code inside the notebooks.

03:58:38.000 --> 03:58:45.000
The changes that they make won't be persistent anywhere, so it's okay to open a Temp nb service to the public.

03:58:45.000 --> 03:58:52.000
In my web browser, I'm going to navigate to Temp nb's website at www.github.com forward slash Jupiter Temp nb.

03:58:52.000 --> 03:58:55.000
I'm now going to scroll down to the readme.

03:58:55.000 --> 03:59:00.000
At the top of the readme, there's this very useful diagram for describing how Temp nb works.

03:59:00.000 --> 03:59:03.000
Temp nb can be broken into a few pieces.

03:59:03.000 --> 03:59:07.000
The user-facing piece is the configurable HTTP proxy.

03:59:07.000 --> 03:59:10.000
This piece routes traffic to the correct sub-pieces.

03:59:10.000 --> 03:59:16.000
The Temp nb orchestrator is what is used to launch the temporary notebook servers.

03:59:16.000 --> 03:59:19.000
Docker is the technology that is used to containerize them.

03:59:19.000 --> 03:59:25.000
Once a server is launched, the Temp nb orchestrator communicates to the configurable HTTP proxy,

03:59:25.000 --> 03:59:31.000
telling it to route a certain subset of addresses to the correct Temp nb container.

03:59:31.000 --> 03:59:35.000
Jupiter runs and maintains its own instance of Temp nb.

03:59:35.000 --> 03:59:38.000
You can access it at try.jupiter.org.

03:59:38.000 --> 03:59:43.000
The notebook itself is the same notebook that you're used to running on your local machine.

03:59:43.000 --> 03:59:47.000
You can see that this notebook comes pre-populated with example notebook files.

03:59:47.000 --> 03:59:50.000
In this video chapter, I'll show you how to do this.

03:59:50.000 --> 03:59:56.000
I'll also show you how to customize your notebook server image so that it reflects your organization's needs.

03:59:59.000 --> 04:00:02.000
In this video, I'll talk about installing Temp nb.

04:00:02.000 --> 04:00:10.000
Temp nb, like nbViewer, can be installed either using a Docker image or in development mode from source code.

04:00:11.000 --> 04:00:19.000
However, unlike nbViewer, it doesn't really make sense to install Temp nb from source code unless you're planning on developing Temp nb.

04:00:19.000 --> 04:00:25.000
That's because all the common configuration that one would want to do can be done through custom Docker images,

04:00:25.000 --> 04:00:30.000
the images that are launched by Temp nb as temporary servers.

04:00:30.000 --> 04:00:33.000
First, let's open up the Docker quick terminal.

04:00:33.000 --> 04:00:38.000
As we did in the last chapter, remember the IP address that's printed by Docker in green.

04:00:38.000 --> 04:00:42.000
This is the IP address to use to access your server later.

04:00:42.000 --> 04:00:46.000
The first step is to tell Docker to download Temp nb.

04:00:46.000 --> 04:00:51.000
You can do that by running Docker pull Jupyter minimal.

04:00:51.000 --> 04:00:56.000
Once that is finished downloading, you should have a full copy of the Jupyter minimal image.

04:00:56.000 --> 04:00:59.000
Now you'll need to generate a random token.

04:00:59.000 --> 04:01:04.000
This token will be used to authenticate with configurable HTTP proxy.

04:01:04.000 --> 04:01:10.000
This command works on Linux and Mac operating systems to generate a random string of 30 characters.

04:01:10.000 --> 04:01:14.000
However, you can use any random string you'd like for your token.

04:01:14.000 --> 04:01:19.000
So on a Windows machine, you can use the equivalent command provided by that operating system.

04:01:19.000 --> 04:01:21.000
Copy the random token.

04:01:21.000 --> 04:01:24.000
Now we'll launch the configurable HTTP proxy.

04:01:24.000 --> 04:01:27.000
To do so, I'll start with Docker run.

04:01:27.000 --> 04:01:32.000
And then I'm going to tell Docker to use the network adapter of the host.

04:01:32.000 --> 04:01:36.000
To do that, I'll use double dash net equals host.

04:01:36.000 --> 04:01:42.000
Then I'll tell Docker to run in the background and print its ID using the dash D flag.

04:01:42.000 --> 04:01:48.000
Next, I'll pass in the proxy token as an environment variable within the image.

04:01:48.000 --> 04:01:53.000
To do that, I'll use the dash E flag, specify the environment variable,

04:01:53.000 --> 04:01:56.000
and I'll paste the token that I generated in the last step.

04:01:56.000 --> 04:01:59.000
I'll set the name of this container to proxy.

04:01:59.000 --> 04:02:03.000
Then I'll specify the name of the container I want to launch.

04:02:03.000 --> 04:02:07.000
And I'll specify default target.

04:02:07.000 --> 04:02:13.000
Since this is the first time I've ran the command, Docker will load the image from its repository.

04:02:13.000 --> 04:02:18.000
Once that is finished downloading and has launched, we'll launch the tempnb orchestrator.

04:02:18.000 --> 04:02:24.000
To do so, we'll use the same type of command except we'll change the last couple pieces of it.

04:02:24.000 --> 04:02:27.000
The name will change to tempnb.

04:02:27.000 --> 04:02:35.000
And then we'll use the special dash V flag to tell the Docker image to bind the Docker client within itself.

04:02:35.000 --> 04:02:39.000
This will allow the Docker image to spawn other Docker images.

04:02:39.000 --> 04:02:42.000
Specifically, we'll bind the Docker sock.

04:02:42.000 --> 04:02:45.000
And lastly, we'll specify the name of the image.

04:02:45.000 --> 04:02:48.000
The orchestrator's name is tempnb.

04:02:48.000 --> 04:02:53.000
Since this is the first time I've ran this command too, Docker will download the image.

04:02:53.000 --> 04:02:57.000
Once that finishes, you should be able to visit your tempnb service.

04:02:57.000 --> 04:03:01.000
In the web browser, navigate to the IP address you remembered from earlier.

04:03:01.000 --> 04:03:07.000
At the end, append colon 8000 to visit port 8000.

04:03:07.000 --> 04:03:10.000
This is the port that tempnb is listening on by default.

04:03:10.000 --> 04:03:13.000
If all is well, tempnb should just work.

04:03:13.000 --> 04:03:18.000
And accessing that address will spawn a notebook server for you in a Docker image.

04:03:18.000 --> 04:03:22.000
In the top right hand corner, you'll see a hosted by Rackspace logo.

04:03:22.000 --> 04:03:24.000
This is not actually being hosted by Rackspace.

04:03:24.000 --> 04:03:26.000
This is being hosted on your machine.

04:03:26.000 --> 04:03:30.000
It's just that the image that you downloaded, Jupyter 4 slash minimal,

04:03:30.000 --> 04:03:35.000
is based on the same image that we use in the Jupyter deployment.

04:03:35.000 --> 04:03:42.000
In this video, we'll look at how we can use custom Docker notebook images with tempnb.

04:03:42.000 --> 04:03:47.000
Jupyter has a bunch of notebook images predefined in the Jupyter organization.

04:03:47.000 --> 04:03:54.000
In your web browser, open up the Jupyter organization GitHub page at github.com forward slash Jupyter.

04:03:54.000 --> 04:04:00.000
Once the page loads, scroll down and you'll see a repository called Docker stacks.

04:04:00.000 --> 04:04:01.000
Open that.

04:04:01.000 --> 04:04:05.000
This repository contains a bunch of Docker images for various tasks.

04:04:05.000 --> 04:04:08.000
Let's go ahead and clone this repository.

04:04:08.000 --> 04:04:12.000
To do so, copy the clone URL in the right hand column.

04:04:12.000 --> 04:04:16.000
Now, in a terminal, navigate to your home directory.

04:04:16.000 --> 04:04:21.000
Run, get, space, clone, and then paste the URL.

04:04:21.000 --> 04:04:25.000
Once the cloning is finished, CD into that directory.

04:04:25.000 --> 04:04:28.000
And let's open Adam.

04:04:28.000 --> 04:04:31.000
Once Adam opens, open the minimal notebook directory.

04:04:31.000 --> 04:04:37.000
This minimal notebook image is actually different than the minimal notebook image you used in the last video,

04:04:37.000 --> 04:04:40.000
but the one that we used in the last video is actually deprecated.

04:04:40.000 --> 04:04:42.000
And this is the modern replacement.

04:04:42.000 --> 04:04:46.000
This image doesn't have a racks based logo in the top right hand corner.

04:04:46.000 --> 04:04:48.000
Let's open up the Docker file.

04:04:48.000 --> 04:04:51.000
This is the file that tells Docker how to build the image.

04:04:51.000 --> 04:04:56.000
This from line is how Docker knows what this image inherits from.

04:04:56.000 --> 04:04:59.000
The Debbie and Jesse image is used as a base.

04:04:59.000 --> 04:05:04.000
You can see the list of Docker commands used to build this image.

04:05:04.000 --> 04:05:10.000
At the end, we specify that the start notebook dot shell file should be executed.

04:05:10.000 --> 04:05:11.000
Let's open that.

04:05:11.000 --> 04:05:14.000
Here you can see how the notebook is launched.

04:05:14.000 --> 04:05:19.000
The config file used for the notebook is stored under jupiter underscore notebook underscore config.

04:05:19.000 --> 04:05:23.000
This is the same kind of config file that we looked at in the second chapter.

04:05:23.000 --> 04:05:26.000
The files as they are in this repository are not a Docker image.

04:05:26.000 --> 04:05:28.000
We have to first build them.

04:05:28.000 --> 04:05:31.000
The build process is described in the make file.

04:05:31.000 --> 04:05:33.000
Let's open that.

04:05:33.000 --> 04:05:37.000
The help section describes how the build make file is used.

04:05:37.000 --> 04:05:43.000
To build the minimal notebook, we just need to run build forward slash minimal dash notebook.

04:05:43.000 --> 04:05:46.000
Let's try that within this directory.

04:05:46.000 --> 04:05:49.000
First, Docker will download the base image.

04:05:49.000 --> 04:05:56.000
It will take a while, but once it's done, your image will be built.

04:05:56.000 --> 04:05:59.000
Now let's try using this image with Tempenby.

04:05:59.000 --> 04:06:02.000
Start a Docker quick terminal.

04:06:02.000 --> 04:06:06.000
Once the terminal starts, pay attention to the IP address like you did before.

04:06:06.000 --> 04:06:10.000
We're going to run the same commands that we did in the video before the last video,

04:06:10.000 --> 04:06:15.000
skipping the Docker pull command and changing some of the contents of the last command.

04:06:15.000 --> 04:06:18.000
If you're continuing on from the last video,

04:06:18.000 --> 04:06:23.000
make sure that you close all the existing Docker containers before trying to do this.

04:06:23.000 --> 04:06:26.000
To do so, you can run the following command.

04:06:26.000 --> 04:06:36.000
Docker space stop, dollar sign, and then in parentheses Docker space PS space dash a space dash Q.

04:06:36.000 --> 04:06:41.000
I don't have any Docker containers running right now, so I get the help output.

04:06:41.000 --> 04:06:47.000
After running that command, you want to run almost the same command, but replacing stop with RM.

04:06:56.000 --> 04:07:08.000
The last command is almost identical, just changing from the name forward.

04:07:08.000 --> 04:07:13.000
Once again, we'll tell it to connect to itself, so it's capable of launching other Docker images.

04:07:13.000 --> 04:07:17.000
And here's where the command will start to change significantly from the last video,

04:07:17.000 --> 04:07:20.000
in addition to the omitted name flag.

04:07:20.000 --> 04:07:24.000
We'll start specifying the Python command that launches the orchestrator.

04:07:24.000 --> 04:07:27.000
We'll specify the image that we just built.

04:07:27.000 --> 04:07:32.000
Now the tricky part is that we'll have to tell the image how to launch the notebook server.

04:07:32.000 --> 04:07:35.000
We do so using the double dash command flag.

04:07:35.000 --> 04:07:39.000
We have to tell the notebook app what its base URL is.

04:07:39.000 --> 04:07:45.000
The image will format the string and you can insert special variables using curly brackets.

04:07:45.000 --> 04:07:48.000
Base path is one of those special variables that you can insert.

04:07:48.000 --> 04:07:54.000
We'll tell it to listen to IP0.0.0.0, which will allow it to listen to anything.

04:07:54.000 --> 04:07:57.000
Lastly, we'll specify the port that it's listening on.

04:07:57.000 --> 04:08:03.000
Once you run that command, in your web browser, try accessing the Docker image.

04:08:03.000 --> 04:08:07.000
If everything works, you should see a new notebook server.

04:08:07.000 --> 04:08:12.000
This notebook server won't have a Rackspace logo in the top right-hand corner.

04:08:12.000 --> 04:08:15.000
If you have troubles, most likely you mistyped something.

04:08:15.000 --> 04:08:21.000
If you need to debug why it's not working, open up another Docker quick terminal.

04:08:21.000 --> 04:08:26.000
When the Docker quick terminal launches, you can run docker ps-a.

04:08:26.000 --> 04:08:29.000
This will list all the Docker processes that are running.

04:08:29.000 --> 04:08:34.000
If you see one that says exit it with an exit code in parentheses,

04:08:34.000 --> 04:08:36.000
you can look at the logs of that Docker image.

04:08:36.000 --> 04:08:46.000
To do so, run docker logs and then copy the container ID, which is in the far left column, and paste it.

04:08:46.000 --> 04:08:52.000
In one of the attempts I made earlier to run this long command, I misspelled orchestrate.

04:08:52.000 --> 04:08:56.000
This caused the server to not run and me to receive gateway errors.

04:08:56.000 --> 04:09:03.000
By looking at the logs, I could tell that that was the problem and was able to correct it quickly.

04:09:03.000 --> 04:09:09.000
In the last couple of videos, we looked at launching TempNB using custom notebook image.

04:09:09.000 --> 04:09:16.000
In the following videos, including this one, we'll look at creating our own custom notebook image for use with TempNB.

04:09:16.000 --> 04:09:19.000
To get started, launch the Docker quick start terminal.

04:09:19.000 --> 04:09:23.000
Once the terminal launches, pay attention to the IP address like you did before.

04:09:23.000 --> 04:09:27.000
We'll be using that IP address to access TempNB.

04:09:27.000 --> 04:09:32.000
In the last couple of videos, we used the Jupyter Docker stacks minimal notebook image.

04:09:32.000 --> 04:09:36.000
We'll use that image as a base for our new custom image.

04:09:36.000 --> 04:09:40.000
To do so, let's copy the image out of the repository.

04:09:40.000 --> 04:09:43.000
I'll copy it into a directory called custom notebook.

04:09:43.000 --> 04:09:46.000
This will be the name of the custom image that I'm going to create.

04:09:46.000 --> 04:09:50.000
I'll then cd into custom notebook and I'll open Adam.

04:09:50.000 --> 04:09:58.000
Once Adam opens, I'll open the config file inside custom notebook Jupyter notebook underscore config.py.

04:09:58.000 --> 04:10:03.000
This is the configuration file that will be loaded by the Jupyter notebook inside the notebook image.

04:10:03.000 --> 04:10:07.000
Recalling from an earlier chapter, I'm going to set the untitled notebook name.

04:10:07.000 --> 04:10:13.000
This is an easy variable to set that we can use to quickly judge whether or not our config file is being loaded.

04:10:13.000 --> 04:10:20.000
The variable is c.contentsManager.untitled notebook.

04:10:20.000 --> 04:10:23.000
I'll set that to test. Now I'll save the file.

04:10:23.000 --> 04:10:28.000
Next, I'm going to create a shell file that we'll use to build this image.

04:10:28.000 --> 04:10:32.000
I'm going to copy the shebang from the start notebook file.

04:10:32.000 --> 04:10:35.000
We'll call the new file build.sh.

04:10:35.000 --> 04:10:38.000
I'm going to go back to my Docker quick start terminal.

04:10:38.000 --> 04:10:43.000
I'm going to open Adam up inside the Docker stacks repository.

04:10:43.000 --> 04:10:46.000
When Adam opens, I'm going to open the make file.

04:10:46.000 --> 04:10:50.000
I'm going to scroll down to the build line so I can see how images are built.

04:10:50.000 --> 04:10:52.000
I'll go ahead and copy this line.

04:10:52.000 --> 04:10:57.000
I'm going to go back to the Adam that we opened up inside the custom notebook directory.

04:10:57.000 --> 04:11:01.000
I'm going to paste this line inside the build.sh file.

04:11:01.000 --> 04:11:07.000
I'm going to remove drgs and replace owner with JD Fredder.

04:11:07.000 --> 04:11:11.000
You can use whatever you want here to identify yourself.

04:11:11.000 --> 04:11:16.000
And I'm going to replace this notdir $at with the name of my notebook image.

04:11:16.000 --> 04:11:21.000
I'll also get rid of the notdir $at at the end and the forward slash.

04:11:21.000 --> 04:11:27.000
This tells Docker to build the contents inside the current directory.

04:11:27.000 --> 04:11:31.000
Now I'm going to copy this shebang again.

04:11:31.000 --> 04:11:35.000
And create a new file for testing this image with tempnb.

04:11:35.000 --> 04:11:37.000
I'll call this file test.sh.

04:11:37.000 --> 04:11:39.000
I'll paste the shebang.

04:11:39.000 --> 04:11:44.000
And then I'll enter a command that causes all the images that are currently running in Docker to close.

04:11:44.000 --> 04:11:48.000
It's important to note that this command is inside this file.

04:11:48.000 --> 04:11:52.000
We don't want to run this file if there are Docker images on our system that we don't want to close.

04:11:52.000 --> 04:12:00.000
The reason I'm adding this line is because it becomes tedious to constantly close Docker images each time you want to run your test.

04:12:00.000 --> 04:12:10.000
To close all the images that are currently running, I'll use docker stop and dollar parentheses docker ps-a-q.

04:12:10.000 --> 04:12:16.000
What this does is runs docker stop on every docker image that's currently running.

04:12:16.000 --> 04:12:21.000
I'm going to copy this line, paste it below, and replace stop with rm.

04:12:21.000 --> 04:12:25.000
This will do the same thing but remove the images instead of stopping them.

04:12:25.000 --> 04:12:29.000
Next, I'm going to create a token for use with the HTTP config proxy.

04:12:29.000 --> 04:12:42.000
I'll use export to define the variable token as head 30 characters long of dev urandom piped with xxd-p.

04:12:42.000 --> 04:12:46.000
Next, I'll run the configurable HTTP proxy image.

04:12:46.000 --> 04:12:49.000
To do so, I'll use docker run.

04:12:49.000 --> 04:12:59.000
Double dash net equals host dash d dash e config proxy auth token equal the token variable.

04:12:59.000 --> 04:13:09.000
Double dash name equals proxy image name jupiter configurable HTTP proxy space.

04:13:09.000 --> 04:13:20.000
Double dash default dash target 127.0.0.1 port 9999.

04:13:20.000 --> 04:13:24.000
I'm going to turn on word wrap so you can see the whole command.

04:13:24.000 --> 04:13:29.000
Next, I'm going to launch the tempnb orchestrator image.

04:13:29.000 --> 04:13:33.000
I'll start with the same command but deviate once I get to the name.

04:13:33.000 --> 04:13:44.000
I'll use dash v bar run docker dot sock colon four slash docker dot sock to cause the image to connect to the docker client.

04:13:44.000 --> 04:13:53.000
Next, I'll specify the jupiter tempnb image and the command python orchestrate dot pi.

04:13:53.000 --> 04:14:04.000
I'll specify the image to jd fredder custom notebook and the command to start dash notebook dot sh.

04:14:04.000 --> 04:14:06.000
This part's really important.

04:14:06.000 --> 04:14:12.000
The minimal notebook image requires you to start the notebook server using start dash notebook dot sh

04:14:12.000 --> 04:14:17.000
instead of running ipython space notebook or jupiter space notebook.

04:14:17.000 --> 04:14:22.000
That's because if you run either of those, the notebook will be launched as root

04:14:22.000 --> 04:14:27.000
and the notebook will be looking for the configuration file inside the root home directory.

04:14:27.000 --> 04:14:32.000
However, the configuration file is installed into the jovian user's home directory.

04:14:32.000 --> 04:14:40.000
So running start dash notebook dot sh does some special things that causes the notebook to launch the server as the jovian user.

04:14:40.000 --> 04:14:43.000
I'll have to pass some commands into the start notebook shell script.

04:14:43.000 --> 04:14:46.000
To do so, I'll escape quotes.

04:14:46.000 --> 04:14:53.000
Inside those quotes, I'll set the base URL,

04:14:53.000 --> 04:14:56.000
allow origin,

04:14:56.000 --> 04:14:57.000
and the port.

04:14:57.000 --> 04:15:03.000
I'll save this file and go back to the docker terminal.

04:15:03.000 --> 04:15:08.000
Now I'll navigate to the custom notebook directory that I created earlier

04:15:08.000 --> 04:15:11.000
and I'll try running the build dot sh file I just created.

04:15:11.000 --> 04:15:15.000
If you get a permission denied, it's probably because permissions aren't set correctly on the file.

04:15:15.000 --> 04:15:22.000
You can do so by running chmod plus x build dot sh.

04:15:22.000 --> 04:15:24.000
Looks like the image built successfully.

04:15:24.000 --> 04:15:27.000
Now let's try running the test shell file.

04:15:27.000 --> 04:15:33.000
We'll have to change the permissions of that as well.

04:15:33.000 --> 04:15:34.000
Looks like that worked.

04:15:34.000 --> 04:15:38.000
We get these help outputs because no images were running at the time.

04:15:38.000 --> 04:15:43.000
The last two outputs are the grids for the images that were launched.

04:15:43.000 --> 04:15:45.000
Let's go to the web browser.

04:15:45.000 --> 04:15:50.000
Try accessing the tempnb server via the ip address that docker printed.

04:15:50.000 --> 04:15:52.000
Looks like the server launched successfully.

04:15:52.000 --> 04:15:55.000
Now let's see if the config worked.

04:15:55.000 --> 04:15:56.000
Awesome.

04:15:56.000 --> 04:16:00.000
It looks like the default notebook name is no longer untitled, but is test,

04:16:00.000 --> 04:16:05.000
which implies that our config is being loaded.

04:16:05.000 --> 04:16:11.000
In this video, we'll add custom content to our tempnb notebook custom image.

04:16:11.000 --> 04:16:17.000
This process is very similar to the process that you use for adding custom content to your nb viewer deployment.

04:16:17.000 --> 04:16:23.000
That's because the notebook itself uses ginga2, like nb viewer, to do its templating.

04:16:23.000 --> 04:16:26.000
First, let's start the docker quick start terminal.

04:16:26.000 --> 04:16:31.000
Pay attention to the ip address that is listed, for that's the ip you'll use to access docker.

04:16:31.000 --> 04:16:37.000
Let's go ahead and navigate into our custom notebook directory and open atom.

04:16:37.000 --> 04:16:41.000
The first thing we'll do is create a page.html template.

04:16:41.000 --> 04:16:46.000
This template will override the page.html template of the notebook.

04:16:46.000 --> 04:16:52.000
Inside the page.html template, we'll extend the base template of the notebook.

04:16:52.000 --> 04:16:56.000
Next, we'll override the header underscore buttons block.

04:16:56.000 --> 04:17:00.000
This block exists at the top of the notebook pages.

04:17:00.000 --> 04:17:02.000
We can use this to add our own logo.

04:17:02.000 --> 04:17:05.000
We'll go ahead and add an O'Reilly logo here.

04:17:05.000 --> 04:17:07.000
We have two options to do this.

04:17:07.000 --> 04:17:12.000
We could either add the O'Reilly picture to our custom notebook image,

04:17:12.000 --> 04:17:16.000
or we could host it externally and reference it here.

04:17:16.000 --> 04:17:25.000
For tempnb, it's better to host your images and other static content externally to the images that are launched by the orchestrator.

04:17:25.000 --> 04:17:29.000
That's because the notebook server uses tornado to host its files,

04:17:29.000 --> 04:17:34.000
and tornado isn't as fast as other servers like engine x or Apache,

04:17:34.000 --> 04:17:38.000
which are even slower than services like CDNs.

04:17:38.000 --> 04:17:43.000
So what we'll do is open our web browser and get the link for the O'Reilly image.

04:17:43.000 --> 04:17:46.000
www.oreilly.com

04:17:46.000 --> 04:17:52.000
Once the page loads, right-click on the image and say copy image URL.

04:17:52.000 --> 04:17:54.000
Then go back to atom.

04:17:54.000 --> 04:17:57.000
Now on the header buttons block, add an image tag.

04:17:57.000 --> 04:18:01.000
Set the source of that image tag to the link that you copied from O'Reilly.

04:18:01.000 --> 04:18:03.000
Save the page.

04:18:03.000 --> 04:18:06.000
Now we'll need to copy this template into our image.

04:18:06.000 --> 04:18:08.000
To do so, open your docker file.

04:18:08.000 --> 04:18:10.000
Scroll down to the bottom.

04:18:10.000 --> 04:18:14.000
The first thing you'll need to do is create a directory that contains templates.

04:18:14.000 --> 04:18:20.000
To do so, we're going to copy this line that creates the dot Jupyter directory inside the user directory.

04:18:20.000 --> 04:18:23.000
We'll put our template directory inside that.

04:18:23.000 --> 04:18:25.000
We'll call it custom.

04:18:25.000 --> 04:18:28.000
We'll then need to copy the file into that directory.

04:18:28.000 --> 04:18:31.000
Go ahead and copy the line that does the notebook config.

04:18:31.000 --> 04:18:38.000
Change notebook config.py to page.html and update the path to custom.

04:18:38.000 --> 04:18:40.000
Save the file.

04:18:40.000 --> 04:18:44.000
Lastly, you'll need to go into your Jupyter notebook config.py file.

04:18:44.000 --> 04:18:51.000
Inside here, below the untitled notebook line, set the extra template paths variable of the notebook app.

04:18:51.000 --> 04:18:54.000
This variable accepts a list, a path.

04:18:54.000 --> 04:18:57.000
Give it the path to your custom template folder.

04:18:57.000 --> 04:18:59.000
And then save the file.

04:18:59.000 --> 04:19:01.000
Now go back to your docker terminal.

04:19:01.000 --> 04:19:05.000
And inside here, run the build script again.

04:19:05.000 --> 04:19:10.000
Once the build script finishes, you can run the test script.

04:19:10.000 --> 04:19:12.000
Now go back to your web browser.

04:19:12.000 --> 04:19:14.000
Try accessing tempnb.

04:19:14.000 --> 04:19:18.000
If all goes well, you should see the O'Reilly logo on the top of the header bar.

04:19:18.000 --> 04:19:22.000
You could style this better by using css in your template page.

04:19:22.000 --> 04:19:25.000
But the point here is not to make something that looks good.

04:19:25.000 --> 04:19:31.000
It's just to show you how to get static content into your tempnb images.

04:19:31.000 --> 04:19:36.000
In this video, I'm going to talk to you about setting limits on your tempnb service.

04:19:36.000 --> 04:19:39.000
And then briefly, I'll talk about security.

04:19:39.000 --> 04:19:41.000
To get started, open up a terminal.

04:19:41.000 --> 04:19:45.000
Then navigate into the custom notebook directory.

04:19:45.000 --> 04:19:49.000
This is the directory that contains the custom image we've created.

04:19:49.000 --> 04:19:51.000
Now open Adam.

04:19:51.000 --> 04:19:55.000
Once Adam opens, open the test.sh file.

04:19:55.000 --> 04:20:00.000
This is the file that contains the lines that can launch this image in tempnb.

04:20:00.000 --> 04:20:02.000
In a real deployment, you could use these same lines.

04:20:02.000 --> 04:20:06.000
Just remove the two docker stop and docker rn lines.

04:20:06.000 --> 04:20:11.000
I'm going to enable word wrap so you can see the whole commands.

04:20:11.000 --> 04:20:14.000
The last command is the command that launches the orchestrator.

04:20:14.000 --> 04:20:19.000
We pass in a command into the image using the double dash command flag.

04:20:19.000 --> 04:20:23.000
You can tell the orchestrator what image to use using the double dash image flag.

04:20:23.000 --> 04:20:26.000
There are also additional flags.

04:20:26.000 --> 04:20:32.000
For example, if you need to limit the number of CPUs any particular container can use,

04:20:32.000 --> 04:20:36.000
you can use the double dash CPU underscore shares flag.

04:20:36.000 --> 04:20:41.000
And this accepts an integer value for how many CPUs are allowed.

04:20:41.000 --> 04:20:47.000
For example, we could limit each image to using two CPUs at most by doing equals two.

04:20:47.000 --> 04:20:52.000
The next useful flag for limiting is the coal period flag.

04:20:52.000 --> 04:21:00.000
This flag accepts an integer in seconds that determines how often containers are examined for their age

04:21:00.000 --> 04:21:02.000
and then collect it if old enough.

04:21:02.000 --> 04:21:05.000
The default for this is 600 seconds.

04:21:05.000 --> 04:21:07.000
This is 10 minutes.

04:21:07.000 --> 04:21:11.000
We could make this faster, for example, by doing 300 seconds.

04:21:11.000 --> 04:21:19.000
Cold timeout is the variable that sets how long it takes for a container to be sitting idle that it will get cold.

04:21:19.000 --> 04:21:23.000
The default for this is 3600 seconds.

04:21:23.000 --> 04:21:27.000
This variable is also an integer specified in seconds.

04:21:27.000 --> 04:21:32.000
We can half that time by setting it to 1800 seconds.

04:21:32.000 --> 04:21:40.000
We can also set a limit on the amount of memory each container is allowed to use by setting mem underscore limit.

04:21:40.000 --> 04:21:46.000
This accepts a string specifying the amount of memory that each container is allowed to use.

04:21:46.000 --> 04:21:51.000
It defaults to 512M for our 512 megabytes.

04:21:51.000 --> 04:21:56.000
We can half this by setting it to 256M.

04:21:56.000 --> 04:22:01.000
The last important flag I would like to mention is the pool size flag.

04:22:01.000 --> 04:22:08.000
This flag accepts an integer which specifies how many child docker containers can be launched by the orchestrator.

04:22:08.000 --> 04:22:13.000
We can think of this as a limit as how many users can use Temp Add B at any given time.

04:22:13.000 --> 04:22:15.000
The default for this is 10.

04:22:15.000 --> 04:22:18.000
We can limit it to half that by setting it to 5.

04:22:18.000 --> 04:22:26.000
Note that these flags are all set outside of the double dash command because they're not actually getting passed into the image but to the orchestrator itself.

04:22:26.000 --> 04:22:29.000
Lastly, let's talk a little bit about security.

04:22:29.000 --> 04:22:33.000
Go ahead and open up your Jupyter underscore notebook underscore config.

04:22:33.000 --> 04:22:40.000
You see here in this configuration file that there's a flag for HTTPS encryption and password.

04:22:40.000 --> 04:22:47.000
This is the same HTTPS encryption and password that you used in the earlier chapter where you learned how to deploy the Jupyter notebook.

04:22:47.000 --> 04:22:54.000
This may be useful to you but take note that this is not affecting the orchestrator itself.

04:22:54.000 --> 04:23:00.000
So any random user can still access your deployment of Temp Add B and launch containers.

04:23:00.000 --> 04:23:06.000
They just may not be able to take advantage of those containers if they don't have the appropriate credentials to log on to them.

04:23:06.000 --> 04:23:15.000
This means that those people could still spawn up a bunch of containers and use your entire pool even if they're not authenticated.

04:23:15.000 --> 04:23:22.000
This is a limitation of Temp Add B as the Temp Add B orchestrator does not yet have a password mechanism.

04:23:22.000 --> 04:23:28.000
You could, however, wrap the orchestrator in your own password at Proxy.

04:23:28.000 --> 04:23:32.000
In this chapter, I'll teach you about Jupyter Hub.

04:23:32.000 --> 04:23:43.000
The technical definition of Jupyter Hub is that it's a multi-user server that manages in Proxy's multiple instances of the single user Jupyter notebook server.

04:23:43.000 --> 04:23:50.000
A less technical definition is that Jupyter Hub is a multi-user version of the Jupyter notebook.

04:23:50.000 --> 04:24:01.000
Jupyter Hub is a Python 3 only application, but that doesn't mean that the kernels that are ran by the notebook servers launched by Jupyter Hub are restricted to Python 3 only.

04:24:01.000 --> 04:24:05.000
In other words, the user isn't restricted to Python 3.

04:24:05.000 --> 04:24:18.000
Jupyter Hub is comprised of three main pieces, the multi-user hub, the configurable HTTP proxy, and the multiple single user notebook servers that are launched by the hub.

04:24:18.000 --> 04:24:22.000
When you start Jupyter Hub, you're actually starting the hub application.

04:24:22.000 --> 04:24:26.000
The hub application then spawns the configurable proxy.

04:24:26.000 --> 04:24:31.000
The proxy forwards all requests on the root domain to the hub.

04:24:31.000 --> 04:24:34.000
The proxy is what's exposed to the internet.

04:24:34.000 --> 04:24:42.000
The hub then authenticates the user when the user connects, and the hub will launch a single user notebook server for that user.

04:24:42.000 --> 04:24:52.000
It then configures the proxy to route all requests on the root domain forward slash the username to that new single user notebook server that it launched.

04:24:52.000 --> 04:24:55.000
Jupyter Hub is highly configurable.

04:24:55.000 --> 04:24:57.000
The authentication is configurable.

04:24:57.000 --> 04:25:07.000
We're going to look specifically at the O authenticator extension, which allows you to use GitHub authentication with Jupyter Hub, but you could write your own authenticator.

04:25:07.000 --> 04:25:13.000
This is useful if your organization uses a specialized authentication scheme.

04:25:13.000 --> 04:25:15.000
Second, you can configure the spawning.

04:25:15.000 --> 04:25:19.000
In other words, you can configure how single user notebook servers are launched.

04:25:19.000 --> 04:25:28.000
We're going to look specifically at the Docker spawner, which is a tool that allows Jupyter Hub to spawn the single user notebook servers using Docker.

04:25:28.000 --> 04:25:32.000
And lastly, you can configure the spawn notebook itself.

04:25:32.000 --> 04:25:36.000
By default, Jupyter Hub launches the notebook that's installed on the local machine.

04:25:36.000 --> 04:25:49.000
If you're using something like the Docker spawner, you can customize the notebook by using the techniques described in the last chapter where we created a custom Jupyter notebook Docker image.

04:25:49.000 --> 04:25:54.000
In the following videos, we'll look at three ways to install Jupyter Hub.

04:25:54.000 --> 04:25:59.000
The first is a completely vanilla installed directly from package managers.

04:25:59.000 --> 04:26:03.000
The second is a vanilla install with the Docker launcher extension.

04:26:03.000 --> 04:26:18.000
And the last is a more complex install that uses a combination of the Docker launcher extension and Docker swarm to handle more users to redistribute the demand across multiple machines in order to handle a higher user load.

04:26:18.000 --> 04:26:27.000
First, let's remove the dot Jupyter folder that we created in the earlier chapter where we examined installing the vanilla notebook.

04:26:27.000 --> 04:26:32.000
We need to do this because Jupyter Hub relies on the local notebook install.

04:26:32.000 --> 04:26:37.000
We don't want to dirty our new Jupyter Hub install with the config options that we set earlier.

04:26:37.000 --> 04:26:52.000
On the other hand, later you'll find configuration of Jupyter Hub to be easy because configuring the notebook servers that get launched by Jupyter Hub is the exact same procedure that we examined earlier using traitlets in the config machinery to config the vanilla notebook.

04:26:52.000 --> 04:26:58.000
All the configuration that you have for the vanilla notebook will apply to the vanilla notebook that's launched by Jupyter Hub.

04:26:58.000 --> 04:27:01.000
You'll want to verify that you have Python 3 on your machine.

04:27:01.000 --> 04:27:05.000
You can do so by running Python double-dash version.

04:27:05.000 --> 04:27:10.000
If your system does not print Python 3, try Python 3 double-dash version.

04:27:10.000 --> 04:27:23.000
If that too does not work or does not print version 3, then you'll want to revisit chapter 1 video 3 where we talk about prerequisites and you'll want to make sure that you have Python 3 installed on your machine.

04:27:23.000 --> 04:27:26.000
Next, let's look at the version of Node that we have installed.

04:27:26.000 --> 04:27:30.000
You can do so by running npm-v.

04:27:30.000 --> 04:27:33.000
I have version 3.4.1 installed on my machine.

04:27:33.000 --> 04:27:41.000
If your version is lesser than that, you can update it by running sudo npm install-g npm.

04:27:41.000 --> 04:27:49.000
What this will do is cause npm to uninstall itself and then install the latest version of itself in its place.

04:27:49.000 --> 04:27:54.000
If this command fails partway through, you'll find that you need to reinstall Node and npm.

04:27:54.000 --> 04:27:58.000
The first thing we'll install is the configurable HTTP proxy.

04:27:58.000 --> 04:28:03.000
You'll recognize that name from the earlier chapter where we looked at deploying tempnb.

04:28:03.000 --> 04:28:08.000
However, in that chapter, we used a configurable HTTP proxy docker image.

04:28:08.000 --> 04:28:13.000
So we didn't actually install the configurable HTTP proxy on the local machine.

04:28:13.000 --> 04:28:17.000
Because we're installing Jupyter Hub on the local machine, we'll need to do that here.

04:28:17.000 --> 04:28:30.000
Go ahead and run sudo npm install-g where this dash-g flag installs the software globally configurable HTTP proxy.

04:28:30.000 --> 04:28:33.000
Once that is finished, you'll want to install Jupyter Hub.

04:28:33.000 --> 04:28:38.000
You can do so by running pip3 install Jupyter Hub.

04:28:38.000 --> 04:28:42.000
By running pip3, we force the python3 pip to be used.

04:28:42.000 --> 04:28:47.000
If you receive a permission denied error, go ahead and prepend the command with sudo.

04:28:47.000 --> 04:28:50.000
Now you can try launching Jupyter Hub.

04:28:50.000 --> 04:29:01.000
If you have an error like this, go ahead and uninstall Jupyter Hub and then reinstall it.

04:29:01.000 --> 04:29:05.000
When you first run the hub, you may get an error that there's a bad configuration file.

04:29:05.000 --> 04:29:09.000
You can fix this by running the command that is recommended.

04:29:09.000 --> 04:29:13.000
This command will generate a configuration file for you.

04:29:13.000 --> 04:29:16.000
Say yes when asked if you want to override the file.

04:29:16.000 --> 04:29:19.000
Now try launching the hub.

04:29:19.000 --> 04:29:25.000
If everything is successful, you should get a message saying that the hub is now running at localhost 8000.

04:29:25.000 --> 04:29:29.000
In your web browser, try accessing that.

04:29:29.000 --> 04:29:31.000
Awesome, looks like that worked.

04:29:31.000 --> 04:29:37.000
Now you should be able to log on using your local system credentials.

04:29:37.000 --> 04:29:41.000
Now that Jupyter Hub is installed, let's see how it works.

04:29:41.000 --> 04:29:44.000
You can launch Jupyter Hub by running Jupyter Hub.

04:29:44.000 --> 04:29:48.000
When Jupyter Hub launches, you'll notice a couple warnings.

04:29:48.000 --> 04:29:54.000
The first warning is that the config proxy auth token had to be generated by Jupyter Hub.

04:29:54.000 --> 04:29:57.000
You can bypass this warning by setting that variable explicitly.

04:29:57.000 --> 04:30:04.000
In the future, when you decide to use extensions with Jupyter Hub, such as NBGrader, you'll need to set this token.

04:30:04.000 --> 04:30:10.000
This token is how applications can communicate with the configurable HTTP proxy.

04:30:10.000 --> 04:30:19.000
NBGrader, for example, adds a handle to the configurable HTTP proxy that allows graders to access notebooks with a special interface.

04:30:19.000 --> 04:30:25.000
The second warning you'll see is that no admin users are defined, so the admin interface will not be accessible.

04:30:25.000 --> 04:30:27.000
We'll go ahead and ignore that for now.

04:30:27.000 --> 04:30:29.000
Switch to your web browser.

04:30:29.000 --> 04:30:31.000
We'll access the address listed here.

04:30:31.000 --> 04:30:34.000
It should be available at localhost8000.

04:30:34.000 --> 04:30:38.000
When you access that address, you'll be presented with a login screen.

04:30:38.000 --> 04:30:42.000
Jupyter Hub uses PAM as a default authentication method.

04:30:42.000 --> 04:30:47.000
This means that to access Jupyter Hub, you use credentials on the host machine.

04:30:47.000 --> 04:30:51.000
In other words, you use your current account name if you're running it locally.

04:30:51.000 --> 04:30:56.000
The password is the same password for the account on the host operating system.

04:30:56.000 --> 04:31:00.000
When you sign in, you'll be presented with your own notebook server.

04:31:00.000 --> 04:31:05.000
In the top right-hand corner, you'll see a button for a control panel and a button to log out.

04:31:05.000 --> 04:31:07.000
Go ahead and click on control panel.

04:31:07.000 --> 04:31:12.000
In the control panel, you'll see an option to stop your server or access your server.

04:31:12.000 --> 04:31:14.000
Go ahead and stop your server.

04:31:14.000 --> 04:31:17.000
You'll also see an option to administrate Jupyter Hub.

04:31:17.000 --> 04:31:18.000
Click on that.

04:31:18.000 --> 04:31:24.000
Here, you'll see a screen that allows you to define new users and remove users.

04:31:24.000 --> 04:31:27.000
Here, I'm going to remove JD Fredder.

04:31:27.000 --> 04:31:33.000
You can also change users from admin to normal users.

04:31:33.000 --> 04:31:37.000
Go ahead and log out.

04:31:37.000 --> 04:31:42.000
In this video, I'll show you how to install the Jupyter Hub Docker Launcher extension.

04:31:42.000 --> 04:31:45.000
Jupyter Hub is a highly configurable application.

04:31:45.000 --> 04:31:50.000
Even the way that Jupyter Hub launches single-user notebook servers is configurable.

04:31:50.000 --> 04:31:57.000
The Docker Launcher extension allows you to force Jupyter Hub to launch the single-user notebook servers as Docker images.

04:31:57.000 --> 04:32:03.000
With this extension, you can launch any custom Docker image that you have that contains a Jupyter notebook server.

04:32:03.000 --> 04:32:08.000
If you want Jupyter Hub to launch the single-user notebook servers using something other than Docker,

04:32:08.000 --> 04:32:10.000
you can write your own extension to do so.

04:32:10.000 --> 04:32:13.000
To get started, open up a Docker Quick Term.

04:32:13.000 --> 04:32:17.000
Once the Docker Quick Terminal launches, pay attention to the IP address.

04:32:17.000 --> 04:32:20.000
You'll need that IP address for later during configuration.

04:32:20.000 --> 04:32:24.000
Before we get started, we should close all existing Docker images,

04:32:24.000 --> 04:32:28.000
just to make sure that none are running that will conflict with what we're trying to do.

04:32:28.000 --> 04:32:37.000
To do so, you can run Docker, stop, and then dollar parentheses, Docker PS-A-Q,

04:32:37.000 --> 04:32:46.000
semicolon, Docker RM, dollar parentheses, Docker PS-A-Q.

04:32:46.000 --> 04:32:51.000
Now, you can get the Docker Spawner extension source code by cloning it from GitHub.

04:32:51.000 --> 04:33:01.000
To do so, run git clone, HTTPS, github.com, Jupyter Docker Spawner.git.

04:33:01.000 --> 04:33:05.000
You want to run this inside the directory that you want to install the source code to.

04:33:05.000 --> 04:33:08.000
I'm doing it inside my home directory.

04:33:08.000 --> 04:33:11.000
Now, CD into that repository.

04:33:11.000 --> 04:33:17.000
Run pip3 install-r requirements.txt.

04:33:17.000 --> 04:33:20.000
This will install the requirements of the Docker Spawner.

04:33:20.000 --> 04:33:24.000
Don't forget to add a sudo in front if your permissions require it.

04:33:24.000 --> 04:33:31.000
Next, run python3 setup.py install.

04:33:31.000 --> 04:33:37.000
Lastly, run sudo pip3 install-e.

04:33:37.000 --> 04:33:43.000
Now, we'll need to change our Jupyter Hub config file so it launches using the Docker Spawner.

04:33:43.000 --> 04:33:49.000
CD back out into your home directory or whatever directory that you launched Jupyter Hub from.

04:33:49.000 --> 04:33:51.000
I launched Jupyter Hub from my home directory.

04:33:51.000 --> 04:33:57.000
Once there, open up Jupyter Hub underscore config.py file in your text editor.

04:33:57.000 --> 04:34:00.000
I'm going to open it in Adam.

04:34:00.000 --> 04:34:11.000
Below the first line, add c.jupyterhub.spawner underscore class equals dockerspawner.dockerspawner.

04:34:11.000 --> 04:34:13.000
Pay attention to the capitalization.

04:34:13.000 --> 04:34:17.000
This tells Jupyter Hub to use the Docker Spawner.

04:34:17.000 --> 04:34:29.000
Next, add c.dockerspawner.use underscore docker underscore client underscore env equal to true.

04:34:29.000 --> 04:34:33.000
This allows the Docker Spawner to work with the Docker Quick Terminal.

04:34:33.000 --> 04:34:42.000
Next, add c.dockerspawner.tls assert underscore hostname equal to false.

04:34:42.000 --> 04:34:48.000
This is also required to use the Docker Quick Term in your custom image with Docker Spawner.

04:34:48.000 --> 04:34:57.000
Next, add c.dockerspawner.container underscore image equal the name of your custom image.

04:34:57.000 --> 04:35:01.000
I'm going to use the image that I created earlier in the tempnb chapter.

04:35:01.000 --> 04:35:03.000
Use your custom image here too.

04:35:03.000 --> 04:35:07.000
Now go ahead and save the file.

04:35:07.000 --> 04:35:11.000
Now cd into your custom notebook image directory.

04:35:11.000 --> 04:35:15.000
This is the same directory from the chapter where we investigate at tempnb.

04:35:15.000 --> 04:35:17.000
Open Adam.

04:35:17.000 --> 04:35:20.000
Open up your Jupyter underscore notebook config file.

04:35:20.000 --> 04:35:34.000
Inside here, add c.notebookapp.baseurl equals os.environ.jpy underscore base underscore url.

04:35:34.000 --> 04:35:39.000
This configures the notebook server to listen to the URL that's a subset of Jupyter Hub.

04:35:39.000 --> 04:35:42.000
Go ahead and click save and then close Adam.

04:35:42.000 --> 04:35:45.000
Now you should be able to launch Jupyter Hub.

04:35:45.000 --> 04:35:48.000
Navigate back to the directory that you launched Jupyter Hub from.

04:35:48.000 --> 04:35:50.000
Mine is the home directory.

04:35:50.000 --> 04:36:02.000
Type Jupyter Hub double dash Docker Spawner.container underscore ip equals 192.168.99.100.

04:36:02.000 --> 04:36:08.000
Replace this IP address with the IP address that was listed by Docker when you launched the Quick Term.

04:36:08.000 --> 04:36:10.000
Click return.

04:36:10.000 --> 04:36:12.000
Once the server launches, go to your web browser.

04:36:12.000 --> 04:36:14.000
You should be prompted with a login.

04:36:14.000 --> 04:36:17.000
Login using your local credentials.

04:36:17.000 --> 04:36:21.000
Once you log in, you should see your custom notebook image running.

04:36:21.000 --> 04:36:25.000
This means that everything we did worked.

04:36:25.000 --> 04:36:30.000
In the last video, we set up Jupyter Hub with the Docker Spawner extension.

04:36:30.000 --> 04:36:34.000
This made Jupyter Hub spawn notebook servers inside Docker images.

04:36:34.000 --> 04:36:39.000
In this video, we'll take it a step further and customize how Jupyter Hub does authentication.

04:36:39.000 --> 04:36:46.000
Jupyter Hub has a notion of authenticators, which allow you to change how users authenticate with Jupyter Hub.

04:36:46.000 --> 04:36:52.000
You can use authentication methods ranging from traditional, used in academia and in the industry,

04:36:52.000 --> 04:36:58.000
to more specialized methods, like using social networking or social media authentication.

04:36:58.000 --> 04:37:02.000
In this video, we'll look at using GitHub's authentication system.

04:37:02.000 --> 04:37:08.000
There's an extension called the O authenticator, which was written for Jupyter Hub to allow us to do this.

04:37:08.000 --> 04:37:10.000
First, open up a Docker Quick Terminal.

04:37:10.000 --> 04:37:15.000
Once the Quick Terminal launches, make sure to close all images that are already running on the machine.

04:37:15.000 --> 04:37:25.000
Include it in the Docker Spawner extension repository is an example of how they use the Docker Spawner with the O authenticator.

04:37:25.000 --> 04:37:27.000
We'll use that as a starting point.

04:37:27.000 --> 04:37:31.000
First, you want to copy your Jupyter Hub config into that directory.

04:37:31.000 --> 04:37:36.000
My Jupyter Hub config is located in my home directory because that's where I launched Jupyter Hub.

04:37:36.000 --> 04:37:41.000
So I'm going to copy that from my home directory into that repository example folder.

04:37:41.000 --> 04:37:44.000
Next, cd into that directory.

04:37:44.000 --> 04:37:50.000
Now run sudo pip3 install get plus HTTPS

04:37:50.000 --> 04:37:58.000
forward slash forward slash github.com forward slash Jupyter forward slash O authenticator dot get.

04:37:58.000 --> 04:38:02.000
When that finishes, you want to create a user list file.

04:38:02.000 --> 04:38:04.000
Let's open up Adam inside this directory.

04:38:04.000 --> 04:38:09.000
Once Adam opens, go ahead and right click and create a user list file.

04:38:09.000 --> 04:38:14.000
Inside the user list, add GitHub user names that you want to have access to your server.

04:38:14.000 --> 04:38:16.000
Don't forget to add your own.

04:38:16.000 --> 04:38:19.000
I'm going to add Brian and Kyle, my colleagues.

04:38:19.000 --> 04:38:24.000
Make yourself an admin by adding a space and admin after your account name.

04:38:24.000 --> 04:38:30.000
Save the file and go ahead and close Adam for now.

04:38:30.000 --> 04:38:39.000
In your web browser, go to github.com forward slash settings forward slash applications forward slash new.

04:38:39.000 --> 04:38:42.000
When that page loads, give your application a name.

04:38:42.000 --> 04:38:44.000
I'm going to call mine Jupyter Hub.

04:38:44.000 --> 04:38:49.000
This is the name that users will see when authenticating while connecting to your Jupyter Hub instance.

04:38:49.000 --> 04:38:53.000
Set the homepage URL to the Jupyter Hub URL.

04:38:53.000 --> 04:38:57.000
This should be for now local host 8000.

04:38:57.000 --> 04:38:59.000
Go ahead and copy that URL.

04:38:59.000 --> 04:39:03.000
Paste it below where it says authorization callback URL.

04:39:03.000 --> 04:39:08.000
Then append hub forward slash OAuth underscore callback.

04:39:08.000 --> 04:39:10.000
Now click register application.

04:39:10.000 --> 04:39:12.000
Go back to your desktop.

04:39:12.000 --> 04:39:15.000
Launch a Docker quick start terminal.

04:39:15.000 --> 04:39:19.000
Once the quick start terminal launches, pay attention to the IP address that's listed.

04:39:19.000 --> 04:39:20.000
You'll need this later.

04:39:20.000 --> 04:39:23.000
Now let's CD into the Docker Spanner directory.

04:39:23.000 --> 04:39:27.000
Inside that, CD into the OAuth examples directory.

04:39:27.000 --> 04:39:29.000
Now open Adam.

04:39:29.000 --> 04:39:34.000
Once Adam opens in that directory, open the Jupyter Hub config file.

04:39:34.000 --> 04:39:38.000
Below the container image line, you're going to need to add a new line.

04:39:38.000 --> 04:39:49.000
Add C dot Jupyter Hub dot authenticator underscore class equal to in quotes OAuth.github OAuth.

04:39:50.000 --> 04:39:59.000
Now below that line, add C dot GitHub OAuth authenticator dot OAuth underscore callback underscore URL

04:39:59.000 --> 04:40:05.000
equal to the URL that you provided for the authentication callback while creating the application on github.com.

04:40:05.000 --> 04:40:09.000
I'm going to go back to my web browser to show you what that URL is.

04:40:09.000 --> 04:40:11.000
At the bottom of the page, you'll see it.

04:40:11.000 --> 04:40:14.000
Go ahead and copy that.

04:40:14.000 --> 04:40:21.000
Now below that line, add C dot GitHub OAuth authenticator dot client underscore ID

04:40:21.000 --> 04:40:25.000
equal to the client ID provided to you by github.

04:40:25.000 --> 04:40:29.000
It's located at the top of the page.

04:40:29.000 --> 04:40:35.000
Now below that line, add C dot GitHub OAuth authenticator dot client underscore secret

04:40:35.000 --> 04:40:41.000
equal to the secret provided to you by github.

04:40:41.000 --> 04:40:46.000
Lastly, on the line below that, you'll need to set yourself as an administrator.

04:40:46.000 --> 04:40:54.000
To do so, set C dot authenticator dot admin underscore users equal to

04:40:54.000 --> 04:40:58.000
and then in square brackets and quotes your account name.

04:40:58.000 --> 04:41:00.000
This is your GitHub account name.

04:41:00.000 --> 04:41:02.000
Now save the file.

04:41:02.000 --> 04:41:10.000
Back in the terminal run dash run dot sh double dash Docker spawner dot container IP

04:41:10.000 --> 04:41:14.000
equal to the IP address listed in green.

04:41:14.000 --> 04:41:18.000
Now in your web browser, navigate to the Jupyter Hub instance.

04:41:18.000 --> 04:41:21.000
It should be at local host colon 8000.

04:41:21.000 --> 04:41:25.000
Once you arrive on that page, click the sign in with GitHub button.

04:41:25.000 --> 04:41:27.000
You should be asked to authorize the application.

04:41:27.000 --> 04:41:29.000
Click authorize.

04:41:29.000 --> 04:41:32.000
You'll then be redirected back to your Jupyter Hub instance.

04:41:32.000 --> 04:41:37.000
You can click my server to access your server or admin to administrate Jupyter Hub.

04:41:37.000 --> 04:41:39.000
I'm going to click on my server.

04:41:39.000 --> 04:41:44.000
Note that our custom image is still being loaded.

04:41:44.000 --> 04:41:49.000
In the previous videos, we were able to get Jupyter Hub working with GitHub OAuth

04:41:49.000 --> 04:41:52.000
and a custom Docker image.

04:41:52.000 --> 04:41:58.000
In this video, we'll look at how we can enable our users to share files across their different accounts

04:41:58.000 --> 04:42:00.000
inside the Jupyter Hub instance.

04:42:00.000 --> 04:42:05.000
To do so, we'll mount a shared directory on the host operating system.

04:42:05.000 --> 04:42:07.000
We'll do this two ways.

04:42:07.000 --> 04:42:12.000
One, we'll mount it as read only for content that all users should be able to see,

04:42:12.000 --> 04:42:14.000
but not necessarily edit.

04:42:14.000 --> 04:42:18.000
Two, we'll mount it as read write so users can have a shared directory

04:42:18.000 --> 04:42:21.000
from which they can save files and fetch files.

04:42:21.000 --> 04:42:24.000
To get started, open up a Docker quick terminal.

04:42:24.000 --> 04:42:31.000
Once your Docker quick terminal launches, go ahead and make sure no Docker images are currently running.

04:42:31.000 --> 04:42:35.000
Inside my home directory, I'm going to create two shared folders.

04:42:35.000 --> 04:42:40.000
One will be called shared underscore RW for shared read write,

04:42:40.000 --> 04:42:44.000
and the other shared underscore R for read only shared.

04:42:44.000 --> 04:42:47.000
You can use any directory that's accessible on your file system.

04:42:47.000 --> 04:42:50.000
I'm using my home directory as a convenience.

04:42:50.000 --> 04:42:54.000
Now, I'm going to copy two example notebooks into each of those folders.

04:42:54.000 --> 04:43:00.000
I'm going to CD into the shared read write folder and launch a normal Jupyter notebook server.

04:43:00.000 --> 04:43:05.000
When the notebook server launches, I'm going to go ahead and create a new Python notebook.

04:43:05.000 --> 04:43:08.000
First, I'm going to change the name of this notebook.

04:43:08.000 --> 04:43:11.000
I'll change it to test one.

04:43:11.000 --> 04:43:13.000
Now, I'll give the notebook some content.

04:43:13.000 --> 04:43:16.000
I'll make the first cell a markdown cell.

04:43:16.000 --> 04:43:19.000
In the second cell, I'll add some code.

04:43:19.000 --> 04:43:21.000
Now, I'm going to save this notebook.

04:43:21.000 --> 04:43:24.000
Now, close the web browser and go back to the terminal.

04:43:24.000 --> 04:43:28.000
In the terminal, I'll hit Ctrl C twice to close the server.

04:43:28.000 --> 04:43:31.000
Now, I'll CD into the read only directory.

04:43:31.000 --> 04:43:36.000
I'll launch the notebook server here too.

04:43:36.000 --> 04:43:43.000
Once the notebook server launches, I'm going to create a new notebook.

04:43:43.000 --> 04:43:46.000
I'll call this notebook test two.

04:43:47.000 --> 04:43:52.000
I'll make the first cell a markdown cell.

04:43:52.000 --> 04:43:55.000
In the second cell, I'll add some code.

04:43:55.000 --> 04:43:58.000
Now, I'll save the file and close the web browser.

04:43:58.000 --> 04:44:03.000
Next, I'll close the Jupyter notebook server by hitting Ctrl C twice.

04:44:03.000 --> 04:44:07.000
Now, CD into the Docker spawner directory.

04:44:07.000 --> 04:44:13.000
Inside there, I'll CD into the example's OAuth directory and open Adam.

04:44:13.000 --> 04:44:18.000
Once Adam opens, I'll make sure my Jupyter Hub underscore config file is opened.

04:44:18.000 --> 04:44:27.000
Then, below the admin users line, I'll add c.dockersponer.volumes equals a mapping of volumes.

04:44:27.000 --> 04:44:32.000
The volume mapping is path on the local machine as the key

04:44:32.000 --> 04:44:37.000
and as the value path that it should be mounted inside the Docker image.

04:44:37.000 --> 04:44:40.000
I'll mount the read-write directory.

04:44:40.000 --> 04:44:47.000
I'll have it mounted to home jovian for slash work for slash shared.

04:44:47.000 --> 04:44:54.000
That's because home jovian work is the directory that's loaded by default inside the Docker image.

04:44:54.000 --> 04:44:58.000
To mount read-only directories, the syntax is almost the same.

04:44:58.000 --> 04:45:02.000
Go ahead and copy that line and paste a copy of it below.

04:45:02.000 --> 04:45:05.000
On this line, we'll change the name of the path that's mounted.

04:45:05.000 --> 04:45:07.000
Let's change it to read-only.

04:45:07.000 --> 04:45:12.000
Likewise, we'll change the path on the parent system to the read-only directory.

04:45:12.000 --> 04:45:21.000
The important part is that the key is not docersponer.volumes, it's actually dot read underscore only underscore volumes.

04:45:21.000 --> 04:45:24.000
Once you make that change, go ahead and save the file.

04:45:24.000 --> 04:45:27.000
Go back to the terminal.

04:45:27.000 --> 04:45:30.000
Now, launch the server like you did before.

04:45:30.000 --> 04:45:34.000
Don't forget to set the Docker spawner container IP trait.

04:45:34.000 --> 04:45:40.000
The IP address is the IP listed by the Docker quick terminal in green when you launched it.

04:45:40.000 --> 04:45:43.000
Once your server is launched, go back to your web browser.

04:45:43.000 --> 04:45:47.000
In your web browser, navigate to your Jupyter Hub instance.

04:45:47.000 --> 04:45:52.000
You may still be logged on to your other session from the earlier videos. That's okay.

04:45:52.000 --> 04:45:54.000
Go ahead and click on my server.

04:45:54.000 --> 04:46:00.000
When my server loads, you should see two folders, read-only and shared.

04:46:00.000 --> 04:46:02.000
Go ahead and open shared.

04:46:02.000 --> 04:46:05.000
Inside shared, you should see the test one notebook.

04:46:05.000 --> 04:46:07.000
Go ahead and open that.

04:46:07.000 --> 04:46:09.000
Make a change to this notebook.

04:46:09.000 --> 04:46:13.000
It doesn't matter what change, just a change that you can see.

04:46:13.000 --> 04:46:15.000
Then go ahead and try saving the notebook.

04:46:15.000 --> 04:46:20.000
When you save, you should have seen the checkpoint flash up to the left of the kernel name.

04:46:20.000 --> 04:46:22.000
Go ahead and close the notebook.

04:46:22.000 --> 04:46:24.000
And try reopening it.

04:46:24.000 --> 04:46:26.000
Looks like that worked.

04:46:26.000 --> 04:46:28.000
Go ahead and close the notebook.

04:46:28.000 --> 04:46:30.000
Go back to your home directory.

04:46:30.000 --> 04:46:33.000
Then go inside the read-only directory.

04:46:33.000 --> 04:46:36.000
Open up the test to that notebook.

04:46:36.000 --> 04:46:43.000
When you open this notebook, you should see a notification that flashes quickly to the left of the kernel that says auto-save disabled.

04:46:43.000 --> 04:46:50.000
You should also see an icon of a floppy with a red circle above it indicating that saving is disabled.

04:46:50.000 --> 04:46:52.000
Try making changes to this file.

04:46:52.000 --> 04:46:54.000
Any changes, it doesn't matter.

04:46:54.000 --> 04:46:56.000
I'm going to remove this read-only.

04:46:56.000 --> 04:46:58.000
Now I'm going to try saving.

04:46:58.000 --> 04:47:03.000
When I save, I should see another notification in yellow that says the notebook is read-only.

04:47:03.000 --> 04:47:06.000
Go ahead and close the notebook.

04:47:06.000 --> 04:47:08.000
Reopen the notebook.

04:47:08.000 --> 04:47:11.000
And you should notice your changes weren't saved.

04:47:11.000 --> 04:47:16.000
This means that the read-only is working correctly.

04:47:16.000 --> 04:47:23.000
In this video, we'll talk about how you can increase the performance of your Jupyter Hub deployment using EngineX.

04:47:24.000 --> 04:47:29.000
EngineX will be used to host the static files of the Jupyter notebook.

04:47:29.000 --> 04:47:32.000
The Jupyter notebook uses Tornado to host its web content.

04:47:32.000 --> 04:47:36.000
Tornado is great for templating and hosting dynamic content.

04:47:36.000 --> 04:47:42.000
However, it's slower than things like EngineX or Apache to host static files.

04:47:42.000 --> 04:47:49.000
The methods described in this video can also be extended to redirect and host the static content on CDNs.

04:47:49.000 --> 04:47:52.000
First, we're going to launch Jupyter Hub.

04:47:52.000 --> 04:47:55.000
Go ahead and open up a Docker quick terminal.

04:47:55.000 --> 04:47:57.000
Pay attention to the IP in green.

04:47:57.000 --> 04:48:01.000
Then make sure that all Docker images are closed.

04:48:01.000 --> 04:48:08.000
Next, navigate into the OAuth example folder inside the Docker spawner directory.

04:48:08.000 --> 04:48:15.000
Launch Jupyter Hub by running the run.sh script.

04:48:15.000 --> 04:48:21.000
Once Jupyter Hub launches, open up your web browser and verify that Jupyter Hub is running.

04:48:21.000 --> 04:48:25.000
This should be available at localhost colon 8000.

04:48:25.000 --> 04:48:27.000
Now, go back to the terminal.

04:48:27.000 --> 04:48:31.000
Open up a new tab by hitting command T.

04:48:31.000 --> 04:48:35.000
If you're on a machine that doesn't support tabs in your terminal, open up a new terminal.

04:48:35.000 --> 04:48:38.000
Now, we'll install EngineX.

04:48:38.000 --> 04:48:41.000
On OS X, you can do this using brew.

04:48:41.000 --> 04:48:46.000
On Linux operating systems, you'll want to use the package manager of that system.

04:48:46.000 --> 04:48:50.000
Typically, this is apt-get or yum.

04:48:50.000 --> 04:48:53.000
If you're on OS X, go to your web browser.

04:48:53.000 --> 04:48:56.000
Go to brew.sh.

04:48:56.000 --> 04:48:58.000
This is the home page for brew.

04:48:58.000 --> 04:49:05.000
If you don't have brew installed already, copy the line under the install home brew section inside the text box.

04:49:05.000 --> 04:49:09.000
Paste that line in your terminal and execute it to install home brew.

04:49:09.000 --> 04:49:14.000
I've already installed home brew on my machine, so I'm not going to demonstrate this for you.

04:49:14.000 --> 04:49:16.000
Go back to your terminal.

04:49:16.000 --> 04:49:19.000
Now, make sure that brew is up to date.

04:49:19.000 --> 04:49:22.000
To do so, you're going to run brew update.

04:49:22.000 --> 04:49:25.000
Now, we'll use brew to install EngineX.

04:49:25.000 --> 04:49:30.000
Once brew is finished installing EngineX, run EngineX.

04:49:30.000 --> 04:49:32.000
Now, go back to your web browser.

04:49:32.000 --> 04:49:38.000
Access localhost 8080 to see if EngineX is running.

04:49:38.000 --> 04:49:42.000
If EngineX is running, you should see a welcome to EngineX page.

04:49:42.000 --> 04:49:44.000
Now, go back to your terminal.

04:49:44.000 --> 04:49:51.000
Run EngineX-S to stop the EngineX service.

04:49:51.000 --> 04:49:53.000
Now, go back to your web browser.

04:49:53.000 --> 04:49:56.000
Go to github.com.

04:49:56.000 --> 04:50:03.000
You should see the Jupyter Hub application that you registered earlier.

04:50:03.000 --> 04:50:05.000
Click on that.

04:50:05.000 --> 04:50:13.000
Now, change the port on the home page and the authentication callback URL to 8080.

04:50:13.000 --> 04:50:17.000
Go back to your terminal.

04:50:17.000 --> 04:50:23.000
Now, change the EngineX configuration file so that it proxies all requests to Jupyter Hub.

04:50:23.000 --> 04:50:27.000
We'll also proxy the web socket connections to Jupyter Hub.

04:50:27.000 --> 04:50:33.000
However, we'll intercept requests to static assets and host those directly using EngineX.

04:50:33.000 --> 04:50:39.000
To edit the EngineX configuration file on OS X, run atom or open up

04:50:39.000 --> 04:50:47.000
forward slash usr forward slash local forward slash Etsy forward slash EngineX forward slash EngineX.conf.

04:50:47.000 --> 04:50:50.000
This is the path to the configuration file for EngineX.

04:50:50.000 --> 04:50:55.000
If you're running EngineX on a machine other than OS X, this path may be different.

04:50:55.000 --> 04:51:02.000
You'll have to refer to your installation method to figure out where the configuration file lives by default.

04:51:02.000 --> 04:51:06.000
I'm going to open this file in atom.

04:51:06.000 --> 04:51:10.000
The first thing we'll do is trim a lot of the comments and access lines.

04:51:10.000 --> 04:51:16.000
This will allow us to focus better on what the contents of the configuration file should be.

04:51:16.000 --> 04:51:19.000
I'm going to go ahead and remove this userNobody comment.

04:51:19.000 --> 04:51:23.000
And also the log comments and process ID comment below.

04:51:23.000 --> 04:51:29.000
I'll leave the events block and remove the log format comment, access log comment,

04:51:29.000 --> 04:51:37.000
send file, TCP push, keep a live time out, gzip, all the way down to the server block.

04:51:37.000 --> 04:51:44.000
Inside the server block, I'll leave the listen to port 8080 and server name local host lines.

04:51:44.000 --> 04:51:48.000
I'll remove the lines down to the location forward slash.

04:51:48.000 --> 04:51:55.000
Everything from here on out, I'll remove.

04:51:55.000 --> 04:52:00.000
Now we'll configure all requests on root to forward to Jupyter Hub.

04:52:00.000 --> 04:52:04.000
To do so, remove the lines inside the root block.

04:52:04.000 --> 04:52:12.000
The first line you'll need is proxy underscore pass space, the address to Jupyter Hub.

04:52:12.000 --> 04:52:26.000
Next, proxy underscore set underscore header, capital X dash capital R real dash all caps IP space dollar remote underscore add semicolon.

04:52:26.000 --> 04:52:36.000
Next, you'll want proxy underscore set underscore header host with the capital H dollar HTTP underscore host semicolon.

04:52:36.000 --> 04:52:56.000
In the last line you'll want in the root section proxy underscore set underscore header space capital X dash capital F forward it dash capital F four space dollar proxy underscore add underscore X underscore forward it underscore four.

04:52:56.000 --> 04:53:00.000
Now copy these four lines that you just wrote.

04:53:00.000 --> 04:53:08.000
Below the location root block, we'll need to add another location block, which will only intercept attempts to connect to WebSockets.

04:53:08.000 --> 04:53:15.000
We have to handle WebSocket forwarding specially. This is a detail of engine X configuration.

04:53:15.000 --> 04:53:19.000
To do so, write location space till day asterisk.

04:53:19.000 --> 04:53:24.000
Then we're going to add a long regular expression that will look kind of funky.

04:53:24.000 --> 04:53:30.000
This regular expression will be used to match the request path for WebSocket connections.

04:53:30.000 --> 04:53:37.000
This first group is matching the user forward slash account name section of the URL.

04:53:37.000 --> 04:53:42.000
The second group matches the WebSocket request specific to the notebook server.

04:53:42.000 --> 04:53:50.000
Then suffix with forward slash question mark, and that's all you need for the regular expression that identifies WebSocket requests.

04:53:50.000 --> 04:53:55.000
I'll turn on word wrap so you can see this whole line.

04:53:55.000 --> 04:53:59.000
Inside that group, paste the four lines that you copied earlier.

04:53:59.000 --> 04:54:04.000
You'll need to add some additional lines to get WebSocket forwarding the work.

04:54:04.000 --> 04:54:11.000
First, you'll want to add proxy underscore HTTP underscore version space one point one.

04:54:11.000 --> 04:54:22.000
Next, you'll want to add proxy underscore set underscore header space capital U upgrade space dollar HTTP underscore upgrade semicolon.

04:54:22.000 --> 04:54:32.000
Next, add proxy underscore set underscore header space capital C connection space upgrade in quotes semicolon.

04:54:32.000 --> 04:54:40.000
Last, you'll want to add proxy underscore read underscore timeout space 86,400 semicolon.

04:54:40.000 --> 04:54:46.000
This is all you need to get content to forward to Jupyter Hub using engine X.

04:54:46.000 --> 04:54:51.000
The last piece we'll want to add is to intercept request for static assets.

04:54:51.000 --> 04:54:55.000
We'll want to host directly from the notebook directory.

04:54:55.000 --> 04:54:58.000
But first, let's make sure that this is working.

04:54:58.000 --> 04:55:00.000
Save the file.

04:55:00.000 --> 04:55:04.000
Go back to your terminal.

04:55:04.000 --> 04:55:06.000
Launch engine X.

04:55:06.000 --> 04:55:11.000
If you get a message like this, it means there's something wrong with your configuration file.

04:55:11.000 --> 04:55:13.000
It looks like mine has a typo.

04:55:13.000 --> 04:55:17.000
Remote underscore add was supposed to be remote underscore adder.

04:55:17.000 --> 04:55:20.000
I'm going to add an R and then save the file.

04:55:20.000 --> 04:55:22.000
Now I'm going to go back to the terminal.

04:55:22.000 --> 04:55:25.000
I'm going to try launching engine X again.

04:55:25.000 --> 04:55:29.000
It looks like I missed another instance of remote add.

04:55:29.000 --> 04:55:33.000
Also down here where I copied that content from the root.

04:55:33.000 --> 04:55:35.000
I'm going to save the file.

04:55:35.000 --> 04:55:38.000
I'll try launching engine X again.

04:55:38.000 --> 04:55:40.000
Looks like it launched successfully.

04:55:40.000 --> 04:55:43.000
Now I'm going to go to my web browser to verify that it launched.

04:55:43.000 --> 04:55:46.000
I'm going to try accessing engine X.

04:55:46.000 --> 04:55:50.000
If you recall correctly, it's at localhost 8080.

04:55:50.000 --> 04:55:55.000
When I first access it, it looks as if what I did had no effect on engine X.

04:55:55.000 --> 04:56:00.000
However, this is because my web browser is caching the contents of the last request.

04:56:00.000 --> 04:56:04.000
By refreshing the page, I should see the right contents.

04:56:04.000 --> 04:56:09.000
If refreshing the page doesn't fix the problem for you, you may need to clear your web browser's cache.

04:56:09.000 --> 04:56:13.000
To do so, you'll have to follow steps specific to your web browser.

04:56:13.000 --> 04:56:16.000
I'm going to go ahead and click on my server.

04:56:16.000 --> 04:56:21.000
I need to validate that the proxy for the web sockets is working.

04:56:21.000 --> 04:56:24.000
I'm going to open up the shared folder.

04:56:24.000 --> 04:56:26.000
And then the test one notebook.

04:56:26.000 --> 04:56:30.000
I'm going to try to run the cell with a change.

04:56:30.000 --> 04:56:36.000
If it works, I know that the web sockets are forwarding correctly because the notebook is able to execute code.

04:56:36.000 --> 04:56:40.000
I'm going to save and close this notebook.

04:56:40.000 --> 04:56:44.000
Now I want to try to speed up this Jupyter Hub instance.

04:56:44.000 --> 04:56:47.000
To do so, I'll have to intercept request the static.

04:56:47.000 --> 04:56:49.000
I'm going to go back to my terminal.

04:56:49.000 --> 04:56:55.000
The first thing I need to do is make sure that I have the static notebook files somewhere on my computer.

04:56:55.000 --> 04:56:57.000
That way, Nginx can host them.

04:56:57.000 --> 04:57:00.000
I'm going to navigate to my root directory.

04:57:00.000 --> 04:57:12.000
Here, to clone the notebook, I'm going to run getClone, space, HTTPS, github.com, forward slash Jupyter, forward slash notebook.

04:57:12.000 --> 04:57:20.000
Once the notebook clones successfully, I'm going to go back to the atom instance that I used to open the Nginx configuration.

04:57:20.000 --> 04:57:24.000
Above the location root block, I'm going to add a new block.

04:57:24.000 --> 04:57:27.000
This block will recognize requests for static assets.

04:57:27.000 --> 04:57:30.000
To do so, I'll have to use a regular expression again.

04:57:30.000 --> 04:57:34.000
This time, just use tilde, no asterisk.

04:57:34.000 --> 04:57:36.000
The regular expression is as follows.

04:57:36.000 --> 04:57:37.000
Forward slash.

04:57:37.000 --> 04:57:41.000
And the first group is the user block, just like we did earlier.

04:57:41.000 --> 04:57:46.000
And then the next block is forward slash static forward slash.

04:57:46.000 --> 04:57:59.000
Lastly, parentheses dot asterisk to match all characters, forward slash, question, v equals, and then parentheses dot asterisk to match all characters.

04:57:59.000 --> 04:58:06.000
Now, you're going to specify the root directory to the directory that we clone the notebook repository to.

04:58:06.000 --> 04:58:10.000
When that is finished, save the file and return to your terminal.

04:58:10.000 --> 04:58:17.000
Make sure to stop Nginx if it's already running by running Nginx, dash s, stop.

04:58:17.000 --> 04:58:21.000
Then run Nginx to launch Nginx again.

04:58:21.000 --> 04:58:23.000
Now let's go back to the web browser.

04:58:23.000 --> 04:58:27.000
Navigate back to the root page, refresh the page.

04:58:27.000 --> 04:58:31.000
If everything worked, the page shouldn't look any different.

04:58:31.000 --> 04:58:36.000
However, this Jupyter logo, for example, is being hosted by Nginx.

