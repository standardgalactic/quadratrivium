start	end	text
0	5000	Hi, and welcome to my course on the Jupyter Notebook for Data Science Teams.
5000	9000	I'll just give you a brief overview of the course so you can know what you're getting into.
9000	14000	So we'll start off by doing what you expect, getting Jupyter Notebook set up on your machines.
14000	18000	The second thing I'll go over then is Jupyter Notebook features.
18000	21000	So there's a lot of really interesting things going on with the Jupyter Notebook.
21000	25000	Some of the most useful functionality comes from what's called Notebook Extensions.
25000	27000	So I'll be going over a lot of those.
27000	30000	I'll be showing you how you can use both Python and R in the same Notebook.
30000	33000	So if you have some piece of your analysis that needs to be done on R,
33000	36000	it's very easy to actually do part of it in R, send it back to Python,
36000	41000	or even use R natively as the entire kernel that runs your Jupyter Notebook.
41000	45000	Also, different things like using SQL in the Notebook to query databases,
45000	49000	some really nice post-save hook functionality and widgets,
49000	51000	which I'll just demonstrate right here.
51000	56000	This is an example of a widget where I've created a function that generates data
56000	59000	according to some line, and the green line shows you the actual function
59000	64000	with some noise, and the blue dots are data that gets drawn from this distribution.
64000	68000	So the total number of points is 10, and as I click and drag this off to the right,
68000	72000	increase the number of points, and you can see that the fit dynamically moves around
72000	78000	as I add data points and actually slowly but surely converges on the underlying distribution.
78000	82000	So this is an example of a widget which is very easy to do
82000	86000	and provides lots of functionality for all kinds of data exploration.
86000	91000	Finally, I'll get into sharing the notebooks on a data science team,
91000	95000	so there's a lot of questions you have to consider for your particular situation,
95000	99000	so I'll try to give you a strategic framework so that you can actually identify
99000	102000	what kind of workflow makes sense for your situation.
102000	106000	There's various other things about conceptually breaking up your notebook structure
106000	110000	into lab notebooks and deliverable notebooks, and a lot more that goes into that.
110000	113000	Finally, I'll go through two different data science projects,
113000	116000	which will just demonstrate the principles I talked about above,
116000	120000	and you get to see it in an environment where I'm explaining and going through
120000	123000	how to actually do a data science project from end to end
123000	127000	using all the different techniques I was already talking about.
127000	131000	In this lesson, we'll be installing the R version of Python,
131000	134000	iPython, and Jupyter so that we can run the Jupyter notebook.
134000	138000	The way we run this is by installing the Anaconda distribution.
138000	141000	There are other ways of installing the iPython notebook,
141000	144000	but I recommend the Anaconda for its ease of use.
144000	149000	We'll first go over to any web browser and search for Anaconda Python.
149000	154000	What you'll see here is that the top link is the continuum Anaconda distribution.
154000	156000	Clicking on that takes you right to the downloads page,
156000	158000	and you see that you have different options.
158000	163000	You can get it with Windows, OSX, or Linux, whichever one you prefer.
163000	166000	Since I'm using OSX, I'll click on OSX.
166000	170000	The choice between using Python 2.7 and 3.4 is a tough one.
170000	175000	I'll be using 2.7 just because many of the legacy code bases still use 2.7,
175000	179000	but feel free to, if you're feeling experimental, to go to Python 3.4.
179000	182000	I'll be using this Mac OSX 64-bit one.
182000	185000	If you have a different system, please use that one.
185000	188000	Okay, great. Now that that package has downloaded,
188000	192000	install it by following the instructions on the screen.
192000	196000	So, clicking through and agreeing to various licenses.
196000	200000	And hopefully you get to this stage where it says the installation was successful.
200000	202000	Click Close.
202000	204000	We can close our browser as well.
204000	208000	At this stage, if we typed IPython, it still won't work.
208000	212000	One thing that the graphical interface does is actually adds a command to your bash profile.
212000	216000	So if I were to actually go into vi.bash underscore profile,
216000	219000	the first piece of the profile has been there from before,
219000	222000	but this was added by the Anaconda 2.3 installer,
222000	227000	which exports this path, shows you that the Anaconda folder has been created in my home directory,
227000	229000	and adds that to my path.
229000	233000	So if I type ls, you actually do see the Anaconda directory right here.
233000	236000	This makes it really easy to uninstall Anaconda if you want.
236000	240000	You can remove that line from that path in your bash profile,
240000	243000	and you can delete this folder, and everything should be gone off your system,
243000	245000	and you can use the old system defaults.
245000	249000	But now that we have this, we have to source our bash profile,
249000	252000	and we should now be able to type IPython.
252000	257000	Now that we've run IPython, we see we are running Python 2.7, in this case 0.10.
257000	262000	It's the Anaconda distribution, and this is IPython version 3.2.
262000	264000	So a lot of different numbers here.
264000	267000	The ones that are important are the Python version, which is 2.7,
267000	269000	and the IPython version, which is 3.2.
269000	271000	Now this is actually a bit behind.
271000	274000	So what we're going to do is hit Ctrl-D, and it says, do you really want to exit?
274000	276000	You can either type yes in return,
276000	279000	or you can type Ctrl-D a second time to actually exit it.
279000	283000	But what we'd like to do now is actually update to the most recent version of Anaconda,
283000	285000	so that we have the most recent version.
285000	287000	The way to do this is to type conda.
287000	289000	This is a new command line argument that you have.
289000	294000	The way that we update the Anaconda distribution is by typing conda in various packages.
294000	297000	So in this case, we want to conda, install conda.
297000	300000	What this should do is check to see various things.
300000	302000	This tells us the following packages will be updated.
302000	307000	Conda will go from this version 3.14 to 3.18, conda environment, and so on.
307000	311000	And we would like to set this up, and we will hit yes to this.
311000	315000	It actually takes quite a bit of time to install all of these things from source,
315000	319000	but most of these things are actually pre-compiled, so everything's already completed.
319000	322000	We'd like to also update a number of packages.
322000	328000	So let's conda install Jupyter, and now you can actually chain which packages you'd like to see.
328000	335000	So in this case, we'll install Jupyter, the pandas package, and scikit-learn.
335000	337000	So these are the packages to be updated.
337000	341000	We see that a number of things are going from ipython, which is important,
341000	343000	going from version 3.2 to version 4.0.
343000	346000	We'd like to proceed with that.
346000	351000	I'd like to just say a few words about why I find the Anaconda package to be a useful thing to use.
351000	355000	They make sure that all the packages you've installed will play nicely with each other.
355000	361000	So sometimes if you're using pip by itself, you can actually install dependencies that overlap each other out of place,
361000	364000	so they end up with a conflict when you're trying to import these libraries,
364000	368000	and Anaconda does a really nice job of making sure, checking those dependencies really well.
368000	374000	So now if we type ipython, we should see that we are running ipython 4.0, which we are.
374000	377000	Now we would like to actually check out the ipython notebook,
377000	379000	because that's the part where it really gets interesting.
379000	384000	So let's create an example directory, and from here we can type Jupyter notebook.
385000	387000	Just typing Jupyter notebook, a couple of things happened.
387000	389000	First of all, go back to the terminal.
389000	392000	I typed Jupyter notebook, and ran it.
392000	397000	And a notebook server started from the directory we are in, so a user's jbw example in this case.
397000	401000	So an ipython notebook, which has been started with a Jupyter, is now at this location.
401000	405000	HTTP colon slash slash localhost, in this case, quadruple 8.
405000	409000	And it says here this useful thing, control C to stop this server and shut down all kernels,
409000	411000	and you have to do it twice to skip the confirmation.
411000	415000	Now this starts the server running, and this terminal needs to stay open.
415000	421000	If we go back to this, what it runs is a web server, and it automatically by default opens your default browser.
421000	425000	So in this case, here we are at this location, localhost, quadruple 8, underscore 3.
425000	430000	And if we'd like to start a new notebook, we can click new, Python 2 notebook.
430000	432000	And again, this is referring to which version of Python you're running.
432000	435000	This is a 2 version 3 versus 3.
435000	441000	And we see now that we're running a Jupyter notebook, and we can start typing valid Python code,
441000	444000	and see the output from it right there.
444000	446000	Let's do something a little bit more interesting.
446000	451000	So we import the NumPy library as np, and then print numpy.a range 10.
451000	460000	So we see the first bit of Python code, and we know that we have the installation working just as we hoped.
460000	465000	In this video, we're going to start a GitHub repo to house a data science project.
465000	468000	First, we have to go to github.com.
468000	474000	If you don't have GitHub or Git setup, I highly recommend starting out by picking a username,
474000	477000	by giving your email and creating a GitHub account.
477000	482000	Now, if you have Windows or Linux or Mac operating system,
482000	488000	GitHub itself has a really nice tutorial for how to actually set up Git on your machine and for your setup.
488000	489000	So I recommend doing that.
489000	496000	So once you have a GitHub account, which is free, or if you already have one, click sign in, let's go to the next step.
496000	503000	So you've signed into GitHub, click the plus next to your name in the upper right hand corner, and start a new repository.
503000	508000	I prefer to start a new repository through the GitHub website itself, and then clone it to my local machine.
508000	511000	So that way, the remote connection has already set up.
511000	517000	And that's usually a stumbling block that can be a little bit annoying to overcome if you try to do it the other way around.
517000	520000	In this case, I'm going to be looking at some cold data.
520000	523000	So I'm going to call it cold exploration.
523000	526000	I'm going to give it a quick description.
526000	529000	I'm giving it the description a first look at the cold data.
529000	532000	I'm going to let it be public so anyone can see this repository.
532000	535000	So afterward, you can also see this if you'd like to go to it.
535000	540000	I will initialize this repository with a readme and I will add a gitignore.
540000	547000	A .gitignore file will let you ignore the machine generated code that comes along with various programming languages.
547000	553000	Now Python doesn't have that many, but there is usually a .pyc if you're running a Python file.
553000	559000	I also recommend having a license, especially if it's going to be public, so that you can share your repositories with others.
559000	562000	If you work for a company, obviously you have different licensing concerns.
562000	564000	So then click create repository.
564000	565000	It's as easy as that.
565000	570000	So now I have the cold exploration repository in my GitHub account.
570000	575000	From here, we would like to actually tie this account to our local machine.
575000	580000	So we can copy this text that's just to the right of this SSH tab.
580000	586000	Now, if it doesn't say SSH, if it says HTTPS, I would recommend clicking it to SSH.
586000	590000	And once you do that, copy the text that's in this text box.
590000	596000	Navigate with your terminal to a place that you think is an appropriate spot for this repository.
596000	603000	Type in git clone and paste the text that you just copied from the website itself.
603000	608000	So now we see the license and the readme files that we created on the website itself.
608000	617000	All right, so we have set up our GitHub repository and we've cloned it to our local machine and we're ready to start doing some data science.
617000	624000	In this lesson, I'm going to give you some extra intuition so you can understand what's happening when the Jupyter Notebook is running.
624000	629000	So in my terminal, if I type ls, I get to see the directories now underneath this current directory.
629000	631000	I see deliver, dev and source.
631000	636000	By typing Jupyter Notebook, I again start the Jupyter server.
636000	638000	My default browser is Chrome.
638000	641000	So again, we see those same three directories deliver, dev and source.
641000	645000	If we toggle back to the terminal, we can see several messages.
645000	649000	The first is the directory under which the notebook server has been started.
649000	651000	The second message is the number of active kernels.
651000	656000	The third message is the location that you can point your browser to to find this notebook.
656000	659000	And finally a message to say how to stop this server.
659000	666000	So going back to the notebook itself, if we click on the development branch, we see that there's no notebooks in here.
666000	670000	We can start a notebook by clicking on new and then clicking on Python 2.
670000	673000	So after clicking new, we see a new tab appear.
673000	678000	It's currently named untitled and the last checkpoint comes from a few seconds ago.
678000	679000	So let's type a few things.
679000	683000	So let's just say first as a variable is equal to the 5.0.
683000	687000	I execute that cell by holding down shift and hitting return.
687000	690000	When I do that, a new cell appears beneath it.
690000	699000	And as I type a second variable and label it say 23.0, again hitting return with the shift key produces another cell beneath it.
699000	703000	So I now have two variables, one named first and one named second.
703000	709000	And there's unsaved changes, which means if I lose this current browser, I will lose the changes that happened from the last time it was saved.
709000	711000	In this case, there's nothing that's been saved.
711000	713000	So let me go ahead and save this right now.
713000	714000	There's two ways of doing this.
714000	719000	One, typing command S if you're on the Mac or control S on Windows, which I just did.
719000	723000	Or you can click this save disk and it will also save it.
723000	727000	Now that it's been saved and there's no unsaved changes.
727000	738000	If I close this tab, or if I even close the whole browser by quitting the Chrome browser, all of the actual information has been stored in the kernel itself.
738000	744000	In other words, there's this kernel and everything that's happened with the kernel is being stored in state by this kernel.
744000	751000	This means if I open up a brand new version of Chrome and I go to where the notebook is running from the previous message before.
751000	756000	I copied that with control C, go back to Chrome browser and type it in here.
756000	759000	I go back to the exact view we had before.
759000	761000	Clicking on Dev, because that's where we were.
761000	765000	We actually see that the untitled IPython notebook is actually still running.
765000	770000	So if we click on this, we reattach the browser to the underlying kernel.
770000	776000	So if you have saved your notebook as you work and you close the browser, the work still remains in memory.
776000	781000	So if I say print first comma second, now we see the actual results is here.
781000	783000	So this is all been saved.
783000	789000	And that's one interesting thing that you should know is that the browser itself is a front end to what's really going on in the kernel.
789000	796000	Now, the converse to this is what happens if I completely close and shut down the server.
796000	799000	So I hit control C twice and shut down the kernels.
799000	801000	So all the kernels have been shutting down.
801000	806000	So going back to the browser, you see a message that says connection to the notebook server cannot be established.
806000	810000	Let's continue to try to reconnect, but you won't be able to run any code.
810000	820000	So in this case, if I try to do something, say I want to say first times second and execute this and shift enter, nothing happens.
820000	823000	And this is what you see when it's trying to connect to the kernel and it's failing to.
823000	829000	So this is the part where it actually needs to be running and needs to be continually talking to your browser.
829000	834000	Unfortunately, restarting the kernel does not give us back to where we were before.
834000	844000	So here I can try to reload this notebook and we still see what we had previously done, but watch what happens when I try to run this third cell.
844000	848000	The name first is not defined and the input name of the cell one to one.
848000	854000	So the kernel has completely restarted as you saw me do in the terminal, which means that now we have to start from the beginning.
854000	861000	And now everything has been stored in state saving it keeps it so that the kernel is now running in the background.
861000	863000	Hopefully that gave you a little bit of insight into what's happening.
863000	868000	The browser acts as a front end to this process that's running in the back end on this terminal.
868000	881000	The browser can be closed or blown away after you've saved all of the changes that you've made, but the kernel cannot be the kernel has to stay running if you want to keep the changes that you've done in memory.
881000	885000	In this lesson, we'll be talking about Jupyter notebook extensions.
886000	893000	Notebook extensions, as the name suggests, are extensions to the capabilities that the Jupyter notebook already comes with.
893000	897000	Now there's many different ways that you can actually extend the behavior of a Jupyter notebook.
897000	899000	I'm going to show you just two.
899000	903000	The first extension that I'll show you is called Jupyter Pivot Tables.
903000	907000	And if you click on this link here, you'll see that you go to this website.
907000	913000	Nicholas.crucian.com slash content 2015-09 Jupyter Pivot Tables.
913000	917000	And this allows for drag and drop pivot tables and charts.
917000	920000	And this write-up he has is actually a really nice write-up.
920000	927000	I recommend you reading and watching this video as well because he explains in some detail how you can actually use his extension.
927000	933000	To install this, all you have to do is go to this pip install command.
933000	940000	So copy pip install pivot table JS and run that command in your terminal.
940000	943000	So it's successfully installed the pivot table JS.
943000	945000	We go back to our notebook.
945000	955000	We can now run the cells that import both pandas and numpy and this command, which is from pivot table JS import pivot UI.
955000	958000	So that loaded correctly without any errors.
958000	961000	So we have now loaded this extension.
961000	969000	As of Jupyter 4.0, the preferred way of installing notebook extensions is through a pip install of the extension.
969000	973000	There are other ways of doing it as well and I'll show you a second way at the end of this video.
973000	978000	So let's actually take a look at some data with this pivot table extension.
978000	984000	Go to HTTPS colon slash slash data dot austintexas.gov.
984000	990000	In this website, we're going to go down and look at the restaurant inspection scores.
990000	993000	From this, we will export data.
993000	995000	The format we want is CSV.
995000	1000000	We do want it to go into our data folder and it's called restaurant inspection scores.
1000000	1002000	Return to save that.
1002000	1005000	You can now close this tab and go back to our notebook.
1005000	1010000	Now that we've downloaded the CSV file, let's read it into pandas data frame.
1010000	1019000	I'm going to split the cell at the current place where it's blinking by typing control shift minus because I want to run this on just one cell by itself.
1019000	1025000	So reselecting that cell, I now hit shift and return and it correctly loads in the data frame.
1025000	1030000	So ways to check that is actually look at what the top of this data frame looks like.
1030000	1039000	We see that the restaurant name, the zip code, inspection date, the score, the address, facility ID and the process description actually looks like it's been read in correctly.
1039000	1045000	One thing you will notice is that the address has return characters in it because standard address has multiple lines.
1045000	1047000	And I'm actually going to be okay with that.
1047000	1052000	I'm going to say I would like to keep the address on one line in the data frame, not have that split up in different ways.
1052000	1059000	So let's take a look at what we get when we look at just the data frame itself is pivot underscore UI.
1059000	1063000	So we've imported pivot underscore UI up here in the first cell.
1063000	1065000	Let's execute the cell here.
1065000	1067000	Now a number of things happened in the background.
1067000	1072000	But what you end up seeing, we close this window down here that shows what we downloaded.
1072000	1076000	And I will actually toggle this toolbar for now.
1076000	1078000	So we can actually see a bit more.
1078000	1082000	We have the various columns of the data frame available on the top here.
1082000	1084000	So zip code, inspection date, score.
1084000	1087000	They are now dragable into these two places.
1087000	1088000	So let's do that.
1088000	1092000	Let's actually drag score along the top.
1092000	1097000	Let's see if there's a relationship between the zip code of a restaurant and the score.
1097000	1104000	So just by dragging those two columns in, we see that there are, for each of these zip codes, different scores that have been given to the restaurant.
1104000	1108000	Of course, a really good score is a 100 for the health score.
1108000	1113000	And we can actually scroll down and take a look at this data in a really intuitive way.
1113000	1117000	This looks pretty neat, but there's a lot of numbers going on.
1117000	1119000	It's actually kind of hard to read.
1119000	1124000	So one thing we can do is actually change the output type from table to something else like a heat map.
1124000	1131000	So this does the same data as we saw before, but it actually highlights the outlying points that are large with a darker color.
1131000	1136000	So now by eye, you can visually see the different relationships between these two variables.
1136000	1138000	I still think this is actually a little bit too big.
1138000	1145000	So I'll give one extra hint of taking data that actually has a lot of different granular pieces.
1145000	1154000	So let's take this very granular number across the top and bin it by something, let's say five to give us a little bit less granularity.
1154000	1156000	So here's some code that will actually do that.
1156000	1161000	So we're going to create a new column in this data frame called bin score for bin to score.
1161000	1171000	I'm going to use a pandas function called cut, which will now cut up these column df.score into bins that go from 30 to 100,
1171000	1177000	because no b.a range is not inclusive of the last data point and stepping by five.
1177000	1179000	So I'm going to run this cell.
1179000	1183000	It's going to create a data frame column named bin score.
1183000	1186000	And let's see what this one looks like.
1186000	1191000	We can drag bin score along the x-axis here and zip code along the y-axis.
1191000	1200000	We now see that the binned scores are now counting everything that has a zip code off to the left and any score within a certain range.
1200000	1202000	In a range of five.
1202000	1204000	We can then also take a look at this.
1204000	1206000	Instead of a table, we can look at it as a heat map.
1206000	1211000	You can also see if it looks okay in terms of a bar chart, for example.
1211000	1215000	And this doesn't quite make sense, but there's many different things that are different here.
1215000	1219000	You can actually look at tree map, for example.
1219000	1225000	So the various visualizations that are available to you may or may not make sense to the data that you're looking at.
1225000	1234000	But the availability of this is actually a really nice extension to the notebook capability that Jupyter already comes with.
1234000	1239000	Alright, so picking up on where the last video left off, notebook extensions.
1239000	1241000	We've already installed one extension.
1241000	1243000	This is the pivot table extension.
1243000	1247000	It's one of the extensions that I'd like to highlight for this video.
1247000	1251000	And it actually comes from this URL here.
1251000	1255000	I want to turn this into, let me just show you this real quick.
1255000	1259000	This code block is currently set as code.
1259000	1261000	I'd like to actually change it to mark down.
1261000	1262000	There's two ways to do that.
1262000	1270000	By clicking on the toolbar like I just did, or by typing M when this cell is selected in this gray circle right now.
1270000	1273000	If I type Y, it would turn back to code.
1273000	1277000	So I just typed Y, you saw the drop down menu turned to code.
1277000	1283000	And since it's still selected with a gray box, I can type M and it goes to mark down.
1283000	1289000	So I want it marked down so that when I click this, I can actually open a new tab.
1289000	1293000	So the Jupyter slideshow extension is this GitHub repo right here.
1293000	1299000	It has a lot of really interesting capabilities that I will be showing you at the very end of this course.
1299000	1309000	I'll be using the rise Jupyter slideshow extension to help us make a final slideshow presentation out of some of our data science projects.
1309000	1316000	To install this notebook extension, it says to simply run python setup.py install from the rise repository.
1316000	1320000	Now this means we actually have to first download this extension code.
1320000	1323000	So this isn't done in the usual PIP install way.
1323000	1327000	This is done by choosing the SSH version here at the top of the page.
1327000	1329000	Selecting this by clicking once.
1329000	1334000	GitHub actually makes it so that the entire thing is highlighted so you can now command C to copy this.
1334000	1337000	Go to your terminal.
1337000	1349000	And at this point, if you don't have a folder for your GitHub repositories that you just grabbed from wild, basically, I would recommend creating one.
1349000	1355000	So we type git clone and then paste in the code we had copied from GitHub web page.
1355000	1358000	So it clones into this thing called rise.
1358000	1360000	Let's CD into this.
1360000	1363000	We see various things here, the live reveal package, Jason and so on.
1363000	1366000	Let's go back to the GitHub page.
1366000	1369000	This is we simply run python setup.py install.
1369000	1372000	So I'll copy that code and paste.
1372000	1378000	Okay, so we have now installed this live reveal.js notebook extension.
1378000	1380000	So we go back to our notebook.
1380000	1387000	We see that there's an extra toolbar cell here, which has something different than we normally see, including a slideshow option.
1387000	1393000	And we actually need to restart this notebook to actually get the ability to make this look like a slideshow.
1393000	1396000	So let me go ahead and do that.
1396000	1401000	I'll do save and checkpoint and then close and halt.
1401000	1406000	I'll go back to where it's running in the terminal and hit control C once.
1406000	1409000	It says it's currently running, shut down the server, yes or no.
1409000	1412000	If you wait too long, it'll actually say I didn't see an answer.
1412000	1414000	So I'm just going to assume you did that by mistake.
1414000	1417000	We actually do want to quit this.
1417000	1419000	So we'll do control C twice.
1419000	1421000	You can have also selected why.
1421000	1423000	So we shut down all the kernels.
1423000	1426000	And this thing, if I reload this should not be available.
1426000	1431000	Let's rerun Jupyter notebook and it will give us a new version of this exact thing.
1431000	1433000	Click notebook extensions.
1433000	1438000	And now you still see this toolbar here with the currently being non the slideshow option,
1438000	1441000	but you also have a new button off to the right.
1441000	1445000	So let's actually click this and click the slideshow option.
1445000	1452000	If you'd actually like to turn one of your notebooks into a slideshow, the functionality is now at your fingertips.
1452000	1457000	And if you don't want to see all these extra cell toolbars, you can always put this back to none.
1457000	1458000	They should be saved.
1458000	1462000	So any clicking slideshow again, the fact that these are all slides has been preserved.
1462000	1469000	To look at the slideshow itself, we just click this button and type into the right gives you the different slides.
1469000	1476000	And one interesting thing about this or one thing that I think is very, very useful is that this is not just a rendered notebook of this.
1476000	1481000	This is actually a live cell that we can actually import and actually run new code.
1481000	1486000	So I just ran that piece of Python code during the slideshow while it's up.
1486000	1491000	So this is very nice for interactive demonstrations.
1492000	1499000	In this video, I'll be showing you how to actually query SQL databases from the Jupyter notebook itself.
1499000	1504000	A lot of enterprise data is stored in databases, so dealing with them will be part of your everyday job.
1504000	1511000	The Jupyter notebook makes it really nice to be able to document very clearly the SQL queries that you are creating.
1511000	1519000	So I recommend if you're going to be using SQL connections using a Jupyter notebook extension called ipython SQL.
1519000	1524000	It's installed by typing pip install ipython dash SQL.
1524000	1533000	Once you install that, you then have access to an extension that you can load by simply typing percent load extension space SQL.
1533000	1537000	When you run this cell, it actually loads in this magic extension.
1537000	1542000	It gives you a number of warning signs, but these are just warnings. The package will still work just fine.
1542000	1548000	This next line percent config will actually configure our ipython SQL extension.
1548000	1557000	And what this configuration does, we say SQL magic, we would like to automatically return results that are a table as a pandas data frame.
1557000	1567000	You don't have to do this, but I recommend it because most of the time you'd actually like to take the data you've queried the database from and transform it and use it in the standard data science tools.
1567000	1569000	So I'll run that command as well.
1569000	1575000	Next import pandas and for this demonstration, I'll be using SQL lite.
1575000	1579000	You can use any of the standard SQL engine connections.
1579000	1584000	I'm just using SQL lite because it's a simple and easy database to run with for an example.
1584000	1589000	This next cell, I'm actually going to create a table and put some data into it.
1589000	1598000	So if you're familiar with SQL, you'll notice that everything below the first line of this cell is SQL commands that leaves this top line to be explained.
1598000	1602000	So what we have here is a double percent sign and then SQL.
1602000	1605000	This is how you call what's called a cell magic.
1605000	1616000	If I hit tab while I'm at the end of these double percent sign, I will see a little pop up that tells us of all the different options we can have to change this into a cell magic.
1616000	1623000	When I say cell magic, what this means is that this is a special flag that tells ipython that something different is going to happen for this entire cell.
1623000	1627000	In this case, we're telling it everything after this first line is going to be a SQL query.
1627000	1629000	As you can tell, there's other ways you can do this as well.
1629000	1631000	You can have HTML, you can have bash.
1631000	1635000	There's various other options as well, but I'm just showing you right now the SQL one.
1635000	1639000	Now this is how you connect to a SQL database that's just stored in memory.
1639000	1647000	If you have a different package, a different engine, then you can use the various documentation to tell you which connection you should use.
1647000	1650000	So we're going to create a very simple small table called presidents.
1650000	1655000	We're going to have first and last name and we're going to include the year that they were born.
1655000	1660000	And I just have a random sampling of about 10 US presidents here.
1660000	1664000	So running this cell, we get some output here that says one row is affected.
1664000	1666000	We've inserted values into this table.
1666000	1672000	And now we can actually run a SQL command that's in inline again with a single percent.
1672000	1675000	When you have this command here, it says everything after it will be SQL.
1675000	1682000	So we're going to store an object called later presidents, the SQL command and the results that come from the SQL query.
1682000	1689000	The SQL query being select everything from the presidents table where the year of birth was later than 1825.
1689000	1692000	And then I'm going to show you what that looks like by typing it there.
1692000	1698000	So we see that there were three presidents that were born in that table after 1825.
1698000	1704000	And if we took a look at the type of this return, we will see that it is actually a pandas core data frame.
1704000	1708000	So we have returned a SQL query into a pandas data frame.
1708000	1713000	And now we can use all of the normal tools and functionality of pandas directly.
1713000	1719000	If we would like to write out this into a file, we can do that by doing this SQL three command here.
1719000	1721000	So we make a connection to a new file.
1721000	1729000	And then you run the pandas data frame method to SQL and say, we'll write out the presidents table to the connection.
1729000	1737000	Now, if you don't want to use cell magic in this way, you can also use pandas directly to query our SQL database.
1737000	1741000	So I'll show you how to do that from reading in that file that we just wrote out.
1741000	1745000	So we're going to connect it out to this presidents SQL output.
1745000	1748000	We're going to now create a cursor that connects to that connection.
1748000	1754000	And we will create a new data frame by doing the pandas function read SQL.
1754000	1759000	If you hit shift tab while your cursor is inside the parentheses, you get to see the various calls here.
1759000	1765000	So we have the SQL command, you're giving it the SQL, then you're following it with the connection.
1765000	1770000	And then everything else can be these many other options that you have to really customize it.
1770000	1776000	And once you've done that, be sure to remember to close the connection by doing com.close.
1776000	1781000	So the new data frame should have everything that we stored in the previous query.
1781000	1785000	So the three presidents that we saved from above.
1785000	1789000	And again, this is a data frame that was returned from that.
1789000	1794000	So I just showed you two different ways that you can query databases.
1794000	1799000	You can query them with an inline magic, or you can query them through pandas directly.
1799000	1810000	And either one will return to a pandas data frame so that you can actually use the output in some exploratory data analysis or your full-fledged project.
1810000	1816000	In this video, we'll be talking about how to actually use R in the Jupyter Notebook ecosystem.
1816000	1822000	Previously, we talked about how we can actually set up different Python and R environments.
1822000	1826000	To set up a unique conda environment for Python 2, for example,
1826000	1833000	we can do conda create minus n for name pi2, for example, just as a descriptive name that you could use.
1833000	1838000	We set the Python version to be equal to 2, and then the other packages that we would like to install.
1838000	1841000	So anaconda, Jupyter itself, notebook.
1841000	1844000	We do the same thing for the Python 3 environment.
1844000	1850000	So conda create with a different name, Python 3, for example, and setting the Python version equaling to 3.
1850000	1853000	We also do the same thing when we want to do an R environment.
1853000	1857000	So in this case, conda create minus n, and we're going to call this Jupyter underscore R.
1857000	1866000	And with creating the channel by minus C, R tells Jupyter and tells conda that you're actually creating an R kernel as well as the default other ones.
1866000	1872000	And this creates the R kernel so that the Jupyter Notebook can actually run R natively,
1872000	1877000	as well as installing a number of different packages that it thinks are both recommended and essential.
1877000	1880000	And finally, a Python package called rpi2.
1880000	1887000	The way to activate these commands is you say source activate and then the name of the environment that you created.
1887000	1890000	And when you're done with it, source deactivate.
1890000	1896000	And if you ever forget which environments you've actually installed or what the names you used were, you can do conda environment list.
1896000	1900000	Let's do that to start with conda env list.
1900000	1904000	And we see that there are four different environments installed.
1904000	1909000	There's the root one, which doesn't really qualify as an environment, but then we have pi2, pi3, and Jupyter R.
1909000	1916000	So let's source activate pi3 and say Jupyter Notebook.
1916000	1919000	Once we start that, we can start a Python Notebook.
1919000	1926000	And we see in the upper right-hand corner, not only a blue flag that says using the pi3 kernel just for a second before it flashed away,
1926000	1930000	you actually see that it types Python 3 in the upper right-hand corner.
1930000	1935000	Let's verify that by doing a print 5 plus 5 as a statement and as we can do in Python 2.
1935000	1937000	And this doesn't work in Python 3.
1937000	1940000	The syntax for Python 3 is with parentheses.
1940000	1943000	All right, so we are using Python 3.
1943000	1948000	Let's close and halt this and shut down the server by hitting control C twice.
1948000	1953000	We can tell that we're using Python 3 because pi3 is at the beginning of our terminal screen right there.
1953000	1956000	So I have to say source deactivate.
1956000	1958000	Again, conda env list.
1958000	1963000	Let's switch to Jupyter.
1963000	1966000	The command is the same Jupyter Notebook.
1966000	1970000	Now we can click this pure R example and it loads up R.
1970000	1976000	Just in case you're curious, we can go back to this home directory and create a new, in this case, R.
1976000	1980000	And this is an R kernel running natively.
1980000	1984000	So you can tell again, look in the upper right-hand corner, not only is it not using Python,
1984000	1988000	it's actually using the R kernel natively for this entire notebook.
1988000	1990000	Let's go back to this pure R example.
1990000	1992000	So what is it that R can do?
1992000	1996000	R is a language that has some design choices that are slightly different than Python,
1996000	2000000	but it does have a huge statistics library packages.
2000000	2005000	So you load them in and everything you'll be done in this notebook will be actual R code itself.
2005000	2008000	And again, just looking in the upper-hand corner, this is now R code.
2008000	2010000	I loaded a few libraries here.
2010000	2016000	These are some standard, actually really nice libraries in R, the plier package and ggplot2.
2016000	2023000	This economics data comes when you load in the plier library and you see the head of this economics data.
2023000	2028000	You can create a ggplot command by doing this R code here.
2028000	2038000	And just like with the Jupyter Notebook, we're using Python, we see inline plotting so that all of the workflow is in the same really nice way
2038000	2045000	where you can do this piecemeal exploring by looking at a single piece of R code in the output.
2045000	2047000	Let's close and save this.
2047000	2051000	And now let's open up this Rpy2 example.
2051000	2058000	We are now running again a Python 2 kernel and we're actually using the Jupyter R environment.
2058000	2062000	So Jupyter R environment can run Python and it can run R itself.
2062000	2065000	It's running either one depending on what you started the notebook as.
2065000	2067000	So we're running this one as a Python notebook.
2067000	2071000	But here's a really nice feature of the Jupyter Notebook.
2071000	2076000	You can intermingle Python code and R code in the same notebook.
2076000	2078000	I'll show you how this works.
2078000	2080000	So the top here importing numpy as NP.
2080000	2086000	So again, just Python code, we're creating X and Y where X is this a range.
2086000	2093000	Let's just look at what X is an array from zero to nine and Y is some random number plus the X variable.
2093000	2099000	We import this library Rpy2 and load this extension Rpy2.ipython.
2099000	2105000	So we load it by doing this percent magic percent load extension Rpy2.ipython.
2105000	2108000	And you can do this in a cell that has other code.
2108000	2110000	You don't have to make this a single cell.
2110000	2117000	Just wrote it five plus five just so you can see that we've loaded in an extension and we have this other code running as well.
2117000	2122000	So we have these two numpy arrays, a capital X and a capital Y.
2122000	2128000	If we would actually like to do some analysis in R and then push something back into Python,
2128000	2131000	we do that by now doing a thing called a cell magic.
2131000	2136000	So cell magics are known by having a double percent sign at the very beginning of a cell.
2136000	2141000	That means that this top line is a special thing that in this case we're having it.
2141000	2143000	There's HTML and bash and various other options.
2143000	2150000	We are using the R option and we are sending in with this input X and Y from the Python environment.
2150000	2158000	So we are sending to R the two numpy arrays and we would like to get back from R this thing called XY coefficient.
2158000	2161000	Everything else in this cell is R code.
2161000	2166000	So XYLM is equal to linear model of Y goes as X.
2166000	2174000	XY coefficient which we will be returning back to Python after this cell completes is the coefficients of this model.
2174000	2177000	We're going to print the summary and we're going to make a plot.
2177000	2185000	So run that cell and we see the formula call here, the residual, some intercept and X coefficients.
2185000	2190000	And we have some plots that are displayed in our Python notebook.
2190000	2197000	And again, we actually get our XY coefficient out back into our Python environment.
2197000	2205000	So if you're a person who actually likes to use R just as much as you like to use Python or you like to use R for particular tasks
2205000	2209000	or you like to use Python for lots of it, the Jupyter notebook is very, very flexible.
2209000	2217000	It lets you work in whichever environment you prefer while dropping into the alternate Python or R environment to do just even a few pieces of it.
2217000	2222000	So if you're in the middle of a long piece of data science analysis and you need one functionality from R,
2222000	2231000	you can keep that not only in the notebook but passing it back and forth through native types.
2231000	2236000	In this video, we'll be doing a somewhat more advanced topic and it's definitely 100% optional.
2236000	2243000	We'll be talking about how to get into the guts of the Jupyter notebook system itself and create a post save hook,
2243000	2251000	which will, for our purposes, save a script and an HTML file version of our Jupyter notebooks themselves.
2251000	2252000	So how do we do this?
2252000	2257000	The first step is to actually create a notebook configuration file.
2257000	2261000	Now you can do that if you're interested in doing it in just your root environment
2261000	2267000	or having this behavior be copied everywhere you are actually working on anything to do with the Jupyter notebook.
2267000	2273000	Just go ahead and run Jupyter notebook generate config and I will copy and paste this into the terminal.
2273000	2275000	So you can see what it looks like when you run this.
2275000	2283000	The key takeaway here is this writing default config to now this should be your home directory dot Jupyter slash
2283000	2286000	and then it's going to be this file called Jupyter notebook config.
2286000	2291000	There's another way you can do this if you want to make this for a specific type of analysis.
2291000	2294000	So maybe only the analysis you do involving housing data.
2294000	2297000	Do you want to have a special behavior happen?
2297000	2299000	You can do that in a somewhat roundabout way.
2299000	2302000	You set this Jupyter config directory.
2302000	2306000	It's an environment variable and set that to be a thing that doesn't exist yet.
2306000	2311000	A home directory so tilde slash dot Jupyter save.
2311000	2315000	You run a command that starts like this and then you generate the config.
2315000	2317000	So I will show you what this looks like.
2317000	2321000	So it wrote the default configuration file to dot Jupyter underscore save,
2321000	2327000	which is the name of this profile and then the same Jupyter notebook config file.
2327000	2335000	Now, running it in this way, you have Jupyter and configure before you do the actual command sets it as a temporary environment variable,
2335000	2338000	meaning it's only set for that one command.
2338000	2341000	If I try to echo this, I won't have anything stored in it.
2341000	2344000	So I'm not exporting this as an environment variable.
2344000	2352000	Now, I have a bit of code here that I'm going to actually toggle this header and this toolbar just to give us a little bit of extra space.
2352000	2359000	I have some code here that I would like you to add to your Jupyter config profile file.
2359000	2364000	So this Jupyter notebook config dot pi and instead of trying to type it off the screen,
2364000	2374000	you can actually access it by typing HTTP colon slash slash b i t dot l y so bit dot l y and then Jupyter underscore profile.
2374000	2380000	You click on that, you will go to the same exact code I have that I typed out here.
2380000	2387000	In this case, I will actually copy this code and we're going to open up the file that we would like to modify.
2387000	2393000	So in this case, we're going to be modifying this Jupyter save underscore Jupyter notebook config file.
2393000	2398000	You can do with any text editor, I'm going to use sublime text, so sublime text open it up.
2398000	2400000	Now, here's what the file looks like.
2400000	2407000	It's actually a whole lot of things you can do to modify the behavior of your Jupyter notebook and they're almost all commented out.
2407000	2411000	So you can read through this if you want to actually make different changes than what I'm going to recommend.
2411000	2417000	But this is where we post just at the top this code, just a brief overview what's happening.
2417000	2424000	It defines a function called post save, and it basically grabs the path of the notebook that's currently running,
2424000	2429000	and it actually tries to run this command ipython nb convert to script,
2429000	2435000	which means it's going to be a .py file if it's a Python file or a .r file if it's a r notebook,
2435000	2440000	and an HTML file, which means that it'll just be the rendered HTML version of it.
2440000	2445000	And the C dot file contents manager post save hook equals post save.
2445000	2452000	So this is a way that Jupyter developers have allowed a person to make changes after every save that they do.
2452000	2457000	So let's save that, and let's go back to our notebook.
2457000	2459000	So let's list what's in this directory.
2459000	2465000	We see the name of this current notebook is autosave other formats.
2465000	2467000	I'm going to toggle that away again.
2467000	2469000	So we see it here when I type ls.
2469000	2474000	We can also do exclamation mark ls to do a command like this.
2474000	2478000	And we see that when we save this, we see a checkpoint is created,
2478000	2481000	but no other new files are being created.
2481000	2488000	If we would like to see what happens when we run Jupyter notebook with this new Jupyter save configuration file,
2488000	2490000	we'll have to run a command that looks like this.
2490000	2494000	Jupyterconfigure equals this with Jupyter notebook.
2494000	2500000	And in this case, I would actually like to save this entire thing as an alias,
2500000	2505000	and then you can add this to your bash RC, or you can simply run this in a single line on your terminal.
2505000	2509000	If you just want it in your terminal, however, it will not set it as a thing.
2509000	2513000	So if you restart your computer or open up a new terminal, typing Jupyter save won't work.
2513000	2519000	If you add this to your dot bash RC, then this special way of opening Jupyter notebook will be saved.
2519000	2527000	So let's close this current notebook and let's type Jupyter save.
2528000	2531000	And let's reopen it again in this new way.
2531000	2533000	So we just opened it up.
2533000	2536000	The list function down here should show us what we saw before.
2536000	2539000	So we see the same files in this directory.
2539000	2543000	When I click save, if our post save hook worked correctly,
2543000	2549000	we will see autosaveotherformats.py and autosaveotherformats.html.
2549000	2554000	So I'm going to do that after I click save type ls again.
2554000	2557000	And we see that we do have two other forms.
2557000	2559000	I have html and .py.
2559000	2563000	Just to show you what those html and py versions look like, let's open that up.
2563000	2565000	Oh, one last note.
2565000	2568000	Every time you hit save, it will overwrite the same file a bunch of times.
2568000	2570000	So it's not going to create new versions of this.
2570000	2576000	It's going to just continually overwrite this and always keep the .html and the .py files completely up to date.
2576000	2578000	Let's look at one of these html files actually looks like.
2578000	2581000	So let's go back to the terminal to open a new one.
2581000	2588000	So by typing open autosaveotherformats.html, we actually have the fully rendered notebook here.
2588000	2591000	So what we see here is what we saw on the other page.
2591000	2593000	And this is now the html version of this.
2593000	2595000	This can be emailed somewhere.
2595000	2598000	This can be posted online somewhere and people can see this.
2598000	2606000	Now the links work like you'd expect and the code is all formatted and looks like it looks in the notebook.
2606000	2609000	But since it's just an html file and it's not an actual notebook running,
2609000	2612000	none of these cells are actually computable.
2612000	2614000	I can't actually rerun these cells.
2614000	2627000	So now we have a way of creating a post-save hook that lets us save out automatically html and script versions of any notebook that you're saving.
2627000	2633000	If you would like to commit this to your GitHub repository for fellow members of the team to review in different ways,
2633000	2638000	then having a post-save hook like this can save you tons of time and keep everything up to date.
2639000	2644000	In this video, we'll be talking about a really fun topic called widgets.
2644000	2651000	Widgets is an entire aspect of the Jupyter Notebook ecosystem that lets you do interactive things with the notebook.
2651000	2653000	Let's go over to the notebook.
2653000	2658000	This top cell has various imports, matplotlib, numpy, and so forth.
2658000	2662000	This last line in this cell actually imports the ipython widgets.
2662000	2670000	And we're going to import a number of sliders, a float slider, the integer slider, a toggle button, and this interactive thing as well.
2670000	2673000	So let's execute that by typing shift enter.
2673000	2675000	Now this next cell contains a simple formula.
2675000	2678000	We define a Python function named polynomial.
2678000	2686000	It takes three arguments, which has default values, so slope of 2.0 and intercept of 5 and show points, which can be either true or false.
2686000	2692000	We're going to create some x values, which is just a linear spacing from negative 10 to 10 using 50 points.
2692000	2697000	We're having a y value, which is just the slope times x plus the intercept.
2697000	2701000	Everything else in this function is actually going to be plotting something.
2701000	2704000	So this tells us the figure size we're going to use.
2704000	2707000	The next line tells you that we're going to actually use a figure.
2707000	2712000	The next two lines actually talk about whether or not the show points is true or false.
2712000	2721000	If it says show points, we'll see what this actually does in a second, but it'll add the actual data points we are plotting up when we define this x at the top line here.
2721000	2727000	Finally, we plot x and y and we set some window parameters and give ourselves some axes.
2727000	2730000	The last thing we do is add this tight layout call at the very bottom.
2730000	2735000	This just helps clean up the map plotlib plots before they're finally ready.
2735000	2739000	So after executing this cell, we now have defined polynomial.
2739000	2741000	Let's scroll down to this next cell.
2741000	2744000	I'm defining a thing called a slope slider.
2744000	2748000	This slope slider is called a float slider, which means it can actually take float values.
2748000	2750000	That's what it's actually sweeping across.
2750000	2754000	The value is 2.0, meaning that that's the starting value for the slope.
2754000	2761000	Let's actually start this at minus 10 at the maximum of plus 10 and step size of, oh, let's say 1.0.
2761000	2766000	The next line defines this object called w, which is interactive.
2766000	2771000	The first argument you give interactive is actually the function that you want to be interacting with.
2771000	2777000	In this case, the function we just defined polynomial and any other widgets that need to be connected to it.
2777000	2785000	So in this case, we're going to connect the slope parameter that's given to the polynomial function to the slope slider.
2785000	2788000	Now we call it slope slider, which is because we want to have a descriptive name.
2788000	2790000	You can name it anything you want.
2790000	2794000	The last thing we do is actually execute this w. Let's see what we see.
2794000	2801000	We see three widgets that we can interact with, the slope, the intercept, and show points, which is toggle.
2801000	2807000	Let's scroll down and I'm going to actually hide this toolbar so we have a little extra space.
2807000	2814000	We have the slope, which is 2, and now you can actually click and drag this to different values.
2814000	2823000	As you drag it to the right, you're increasing the slope and we can see that it's actually correspondingly increasing the slope of that line in the plot.
2823000	2826000	We can also move the intercept point up and down.
2826000	2832000	And as we know, an intercept just changes the y-positioning, shifting these things linearly up and down.
2832000	2837000	And of course, the last thing is to toggle on and off show points.
2837000	2844000	If you want to change this, we can actually make this much more sensitive by saying let's make the minimum minus 100, the maximum plus 100,
2844000	2847000	step size of, oh, let's say five.
2847000	2855000	Now, as we change the slope, it should be much more sensitive than it is because we're now at slope of 75.
2855000	2858000	And as we go negative, we can see that as well.
2858000	2865000	So as you can tell, having this kind of functionality at your fingertips is actually incredibly useful during all phases of doing a data science project,
2865000	2868000	especially during the exploratory data analysis stage.
2868000	2873000	So you can imagine if you did something like k-means to look at your data.
2873000	2877000	You can set k, the number of clusters you're fitting for, the number of centroids.
2877000	2887000	And as you can move that back and forth with the integer slider, for example, you can see how well the algorithm is actually clustering on that number of centroids.
2887000	2892000	So being able to do that in an interactive way can speed things up quite a bit, and it's really nice.
2892000	2895000	So this is a somewhat simple example that I just showed.
2895000	2902000	Here is a much more complicated example, but just to give you a sense of what is possible with this kind of a thing.
2902000	2908000	So I'm not expecting you to actually read this and understand the code that goes behind it, but let's just execute this real fast.
2908000	2915000	This is one of the projects I was working on just on my own, where I want to actually have some random points in a small area,
2915000	2922000	and I would like to interpolate with a spline interpolate those random points, and I wanted to see what that looked like at the end.
2922000	2927000	So I can say the number of points that I'm randomly generating and splining between.
2927000	2931000	And as I slide this to the right, you can see the pattern becomes more and more complicated.
2931000	2935000	And as I slide this to the left, we get much simpler shapes.
2935000	2941000	We also have a smoothing parameter here, which can give you a smoothing factor to these kind of more complicated shapes.
2941000	2949000	It sort of unwinds the and rewinds up the knots and the alpha value, for example, like how dark this is.
2949000	2957000	Or if I want to have a slight jitter to each of these strokes, I can add the number of brush strokes and then increase or decrease the jitter for this.
2957000	2966000	So obviously there's a lot going on here, but this is one aspect that shows you just how, first of all, how quickly this can refresh, but also how useful it is.
2966000	2979000	In this video, we saw how we can use interactive capabilities of the Jupyter Notebook to help us plot and look at data and change the values by sliding sliders around.
2979000	2984000	In this video, I'll be talking about some bleeding edge developments in the Jupyter project.
2984000	2987000	A specific thing called Jupyter Hub.
2987000	2999000	If we were to go to Google, let's just search for it by saying Jupyter, then HUB, the top link will be this GitHub repository, which is Jupyter slash Jupyter Hub.
2999000	3004000	And this allows, as it says, multi-user servers for Jupyter Notebooks.
3004000	3013000	In other words, if you have a server where there's data being held for a data science team, you can run a single instance of this thing called Jupyter Hub.
3013000	3021000	And it allows many different data scientists to log in and start a Jupyter Notebook on that server co-located with the data.
3021000	3025000	Now, this is an active development. It's changing on weekly time scales.
3025000	3032000	So if I were to actually show you how to set it up today, by the time you saw this video, it would probably be different from how you're supposed to be setting it up then.
3032000	3041000	So for right now, I'll point you to this documentation and mention that it's actually very much bleeding edge, but I think it will be the future for data science teams.
3041000	3050000	Just to give you a sense of what it looks like when you were to use Jupyter Hub, you can go to try.jupyter.org and hit Return.
3050000	3059000	And what you're actually interfacing with here is a Jupyter Hub server somewhere in the back end, currently being hosted by Rackspace, apparently.
3059000	3069000	And you can start a new notebook in any of these different styles, so Bash, Haskell, Julia, Python 2, or Python 3, R, Ruby, and Scala.
3069000	3076000	So you can start a notebook here, and this is just letting you run a temporary quick one. You can also start one of these notebooks, like this Python one.
3076000	3082000	And it starts with this warning. Don't rely on this server for anything you want to last. The server will be deleted after 10 minutes of inactivity.
3082000	3095000	So that's important. This is just a demonstration area, so it's not for long-term storage of some sort of data science analysis, but it gives you a flavor of what the Jupyter Hub will be doing if you were to install this for your own sake.
3095000	3102000	Now you can actually run this Jupyter Python 3 notebook, and you can actually see the fun results that come out from this.
3102000	3112000	So we see some plots here, and everything works just like you expected to when you're running the Jupyter server locally, which is how all the videos I'm doing in this lesson are.
3112000	3120000	Separating the server from the notebook aspect is that you can do something like this in the future, have the server being hosted on some server somewhere,
3120000	3127000	and being able to access it just through the browser, and having the same exact functionality that I've been showing you for the entire course so far.
3127000	3134000	And one last thing just to show you how fun this is, let's navigate back to our initials-try.jupyter thing.
3134000	3144000	There's a couple other things you can do besides notebooks. This is true for the local server as well, but just to give you a sense of this, you can add a new folder, which is kind of unexciting.
3144000	3154000	You just have a new unentitled folder here. Then you can do this new text file. So if you click text file, instead of starting a new notebook, you're starting a new file.
3154000	3160000	And this is a lightweight in-browser text editor that has various options. You can choose what kind of key mapping you'd like.
3160000	3167000	So I prefer sublime text these days as ways of interfacing with your text editor.
3167000	3178000	So from here, you can do your standard Python, and you can both create and edit Python scripts or any kind of text file that you want to that's located on the server.
3178000	3185000	And of course, renaming the file is as simple as clicking this top thing, calling it startup.py, for example.
3185000	3194000	And once you do that, syntax highlighting gets turned on. You can save this and rename it, and then navigate back to the main server page.
3194000	3199000	And the last thing to show you is that you can also start a terminal.
3199000	3205000	And here you actually have the terminal for your tri-Jupiter. And this is the same case for if you're running this on a server.
3205000	3211000	So you can actually have access to the terminal with all the functionality of a standard bash terminal there.
3211000	3216000	So we see the startup thing we can copy that startup.py folder and call it something else.
3216000	3219000	And going back, we should be able to see this.
3219000	3231000	It's a very cool thing, and it will definitely be the way of the future if you have data science teams working and needing access to a single server somewhere that has the data in some database, for example.
3231000	3242000	So Jupyter Hub, it will be the future. It is bleeding edge. So try it out. It should be pretty usable, but the exact instructions will be different from what I would say today.
3242000	3249000	In this lesson, we'll be taking a look at organizing the overall structure for a data science team to be working on their various projects.
3249000	3254000	So in this notebook, I'm going to use the slideshow button that we installed in a different video.
3254000	3258000	And I make this full screen by clicking shift command F.
3258000	3264000	The initial topic is questions to ask to organize the workflow of a data science team.
3264000	3269000	So the first question is how many data scientists will be working on a single problem?
3269000	3276000	And the high level view of this is to basically break this up into thinking about this in terms of Git repositories.
3276000	3285000	What I mean by that is, if you have different data sources and different problems working in a single company, let's say, then you should definitely use different Git repositories.
3285000	3294000	If you have fewer than 10 data scientists working on the same data, but working on different problems, it also probably makes sense to keep everything in a single Git repository, although it doesn't have to.
3294000	3297000	If you have different concerns, feel free to break that up.
3297000	3308000	And if you have more than 10 data scientists and they're working on the same data, but they're working on different problems, fundamentally addressing different data science issues, then I recommend using different Git repositories.
3308000	3313000	And all of my recommendations will be within context of a single Git repository.
3313000	3317000	The second main question to be asked is where is the data actually hosted?
3317000	3326000	If it's small enough data to be loaded onto a data scientist's personal laptop, then it's very simple to actually just use the data on the laptop locally.
3326000	3336000	So I would recommend just running the Jupyter Notebook as I'm doing in most of the videos for this course, where you just open up a terminal on your local laptop or local desktop and just run Jupyter Notebook.
3336000	3339000	However, many data science projects actually use big data.
3339000	3342000	They access the data on some other server or something like this.
3342000	3345000	And in this case, you have a couple of options.
3345000	3360000	The obvious one is to say, if you can access this server data via SSH and you can actually do work in a server, then you can actually run a Jupyter server on that server and you can SSH tunnel and forward your connection to that server.
3360000	3365000	That way, both the data and the Jupyter server are on the same machine.
3365000	3368000	Another option is to consider using a thing called Jupyter Hub.
3368000	3373000	The Jupyter Hub would have to be installed on the server where the data is actually being held.
3373000	3376000	And if I click this link, you go to this GitHub page here.
3376000	3381000	So it can be found at github.com slash Jupyter slash Jupyter Hub.
3381000	3384000	And you can see it's a bit more work than we can go into.
3384000	3386000	It's a bit outside the scope of this class.
3386000	3390000	But Jupyter Hub is a multi-user server for Jupyter Notebooks.
3390000	3397000	And there's actually some really nice documentation to explain how this can be set up on a server or some AWS instance, for example.
3397000	3401000	There's lots of installation instructions and things to work on here.
3401000	3403000	So those are the main questions to be asking.
3403000	3406000	At what level do you set the Git repository?
3406000	3408000	And where are you going to be running this server?
3408000	3410000	Are you going to be running it on a server somewhere?
3410000	3414000	Or will you be running it locally on your local laptop or something else?
3414000	3424000	Once you have those two questions settled, then the mechanics of actually how do you work on a Jupyter Notebook in a single repository or what we'll deal with next?
3427000	3432000	In this lesson, we'll be organizing our work into two different types of notebooks.
3432000	3436000	Conceptually, there are two types of notebooks I'd like to introduce.
3436000	3440000	One called a laboratory notebook and one called a deliverable notebook.
3440000	3448000	The difference here, a laboratory notebook is in the same style as lab notebooks that are actually in science labs throughout the world.
3448000	3454000	And by that, a lab notebook keeps a historical record of the analysis that's been explored.
3454000	3463000	So each day, a person goes to a lab bench, writes down the date at the top of the page, writes down what happened in lab that day for that particular experiment.
3463000	3466000	And this record just continually gets amended to.
3466000	3475000	It is also meant to be a place where there's development or scratch ideas or initial analyses, and it's very much not a polished piece of work.
3475000	3479000	It is meant for record keeping of scratch pad type nature.
3479000	3482000	And each notebook is controlled by a single data scientist.
3482000	3492000	And by this, I'm talking about a Jupyter notebook where it is a single person single data scientists record of what they were doing that day and it is not shared by anyone else.
3492000	3500000	Now, it's not secret people can look at it and you can upload it as well, but it's not meant to be viewed by other people necessarily.
3500000	3502000	A few more final points on lab notebooks.
3502000	3507000	Split the notebook when it gets too long and too long is just sort of a personal preference.
3507000	3515000	As you start scrolling down the page as a point when a lab notebook or any notebook gets to the point where, okay, this is too much of a document to look at at one time.
3515000	3516000	So then split it.
3516000	3517000	There's no cost in splitting it.
3517000	3520000	And you can think of this as just turning the page in a lab notebook.
3520000	3525000	And finally, if you're working on a single day, you can actually split notebooks into different topics.
3525000	3529000	So for the same day, you can actually have two different or more notebooks.
3529000	3532000	And if you're splitting by topic, that makes sense as well.
3532000	3536000	On contrast to a lab notebook, there's another idea of a deliverable notebook.
3536000	3542000	As I work as a consultant, most of my work is actually going to be delivered either to a project manager or to a client.
3542000	3550000	And these notebooks are different from lab notebooks in the sense that these will be delivered to someone to consume besides myself.
3550000	3555000	Now candidates for deliverable notebooks can be any notebook that will be referenced in the future.
3555000	3561000	By this, I mean, if I expect someone else to also use the same data cleaning notebook, for example,
3561000	3569000	so I might have a notebook that explains how I took raw data and transformed it into the clean data that I use for the rest of the analysis.
3569000	3575000	And I might provide a single link to a deliverable notebook, which is simply the data cleaning of the raw data.
3575000	3582000	And in that notebook, I'll have things like what the actual transformations were, but also reasoning behind it and some documentation around it.
3582000	3585000	So this is for anyone who wants to know how is this data actually cleaned?
3585000	3587000	There's a single spot for it to look at.
3587000	3597000	And obviously, of course, the final fully polished and final analysis of a data science piece of work will also be considered a deliverable notebook.
3597000	3602000	I also recommend that deliverable notebooks should be peer reviewed via pull requests,
3602000	3606000	which means other members will actually review the notebook before it's accepted.
3606000	3610000	Other members can be other data scientists or it can be a manager or something else.
3610000	3613000	And these notebooks are controlled by the whole data science team.
3613000	3618000	If we think about these notebooks as living in a certain repository, for example,
3618000	3621000	then the whole data science team will have these deliverable notebooks,
3621000	3626000	which are in the same topic scope as the problem that they're all together trying to solve.
3626000	3633000	So how do we organize the directories so that the lab notebooks and deliverable notebooks all are in their proper place?
3633000	3638000	So these are the minimum directories, and I think it can be expanded by a few or taken away by a few.
3638000	3645000	So I have listed here the directories I think belong at the top level of a data science git repository.
3645000	3647000	The first one is data. This is optional.
3647000	3653000	If you have very small data and you want to have it locally, it's possible to include it in a git repository.
3653000	3657000	Generally, though, data science data is actually backed up outside of version control.
3657000	3659000	It's in a different environment.
3659000	3662000	So this is definitely an optional directory to have.
3662000	3664000	The second one is the deliver directory.
3664000	3668000	This is where the final polished notebooks for consumption.
3668000	3676000	If a new data scientist is coming onto the project, they will look in the deliver directory to see what has been done before.
3676000	3682000	In the develop directory, we store the lab notebooks, and I will explain the naming convention in a further video,
3682000	3688000	but this will say all the scratch work that has been done by each of the data scientists working on this problem.
3688000	3696000	The directory called figures will contain the figures that have been the output from both to develop and the deliver notebooks.
3696000	3699000	I will be expressing a bit more on that in the future.
3699000	3705000	And finally, a source directory where as you come up with various scripts or modules or anything else that needs to be,
3705000	3710000	that's actual computer code that doesn't belong in a notebook directory, goes in a source directory.
3710000	3713000	Again, you can add to this or you can modify this as you want to,
3713000	3720000	but I think this is a good starting structure to work from and modify it as your needs evolve.
3720000	3726000	In this video, I'll be telling you about my recommended convention for naming lab notebooks.
3726000	3730000	So naming a lab notebook can be a more difficult problem than you might expect,
3730000	3734000	especially if there's many different data scientists working on a similar problem.
3734000	3737000	So to help with that, the following convention is what I recommend.
3737000	3740000	You can obviously change this to fit your own needs.
3740000	3746000	I recommend prepending each file name with the current date that you started the work on that notebook.
3746000	3751000	So in this case, it was started 2015 dash 11 dash 21.
3751000	3756000	I also recommend it in that format where it's the year dash the two digit month,
3756000	3763000	meaning if it's three, it'd be dash zero three dash the two digit day like the month in zero four and so on.
3763000	3771000	This is called an ISO 8601 formatted date, and it just helps with keeping everything so that it's sortable in a nice way.
3771000	3777000	So the initial part of the name is the date that you started working on that particular notebook.
3777000	3781000	The second piece immediately after that is the data scientists initials.
3781000	3783000	So in my case, my initials are JBW.
3783000	3789000	So I put dash after the date my initials, or you can put it if you have a data scientists with the same initials,
3789000	3792000	you can just put some unique signifier that's the same every time.
3792000	3796000	So that if you want to look at a directory that has many different data scientists notebooks,
3796000	3801000	you can do an LS for that person's initials and find their notebooks.
3801000	3807000	And finally, I recommend putting a two to forward description that describes what goes in that notebook.
3807000	3812000	So in this case, Cole predict RF for random forest regression.
3812000	3816000	So looking through this later on, I can think back, okay, what was I doing two months ago,
3816000	3818000	something with random forest, and it was a regression.
3818000	3824000	And on a classifier, seeing this in the title helps pick this out.
3824000	3827000	In this video, we'll be talking about version control.
3827000	3833000	One of the key questions you have when dealing with a data science team is how do you peer review code?
3833000	3837000	How do you store analysis in version control like get?
3837000	3839000	And I'm going to assume a number of further constraints.
3839000	3843000	And I think this is probably the most restrictive constraints I can think of.
3843000	3845000	This might not apply to you.
3845000	3850000	But I think if it does apply to you, I have reasonable work rounds for each of the possible concerns.
3850000	3855000	For example, imagine you have a project manager who would like to see the notebooks you're working on,
3855000	3858000	but they don't want to install Python or I Python or anything like this,
3858000	3862000	or consider that you might not be using GitHub for whatever reason.
3862000	3867000	And some of the nice tools that GitHub has for showing diffs aren't available to you.
3867000	3873000	Or if you would want to review the Python code itself and don't want to have to look at it in a notebook environment.
3873000	3880000	How do I recommend dealing with these kinds of constraints while also maintaining a peer review of the code stored in the version control?
3880000	3886000	The standard practice for my answer is that each data scientist who's working on the same problem in the same repo
3886000	3888000	should have their own development branch.
3888000	3892000	And each day or even more frequently than each day,
3892000	3897000	but at minimum work is saved and pushed to the dev branch that they have daily,
3897000	3902000	which means that anyone can then check out another data scientist development branch.
3902000	3905000	When ready to merge to master, you have to do a pull request.
3905000	3914000	So a data scientist says, OK, I think the deliverable notebooks as well as my laboratory notebooks are ready to be reviewed and pulled into master.
3914000	3917000	Now the question of what exactly to commit.
3917000	3924000	This is a question that people who come from a more software engineering background might start to recoil at my suggestions here.
3924000	3929000	I say this after a lot of thought and there might be a better way of doing it, but this is the best way that I can come up with.
3929000	3940000	So I recommend committing the .ipynb files, which are the notebook files, the .py and the .html of all notebooks, both develop and deliver.
3940000	3944000	And I'll also say any of the figures that are saved should also be committed.
3944000	3948000	Now, when I say the .py and the .html, what am I referring to?
3948000	3951000	So I'll go to an open notebook right now.
3951000	3955000	This is a notebook for making a prediction about call production.
3955000	3958000	And this is in the develop folder of a certain directory.
3958000	3965000	And I have this notebook that's currently running and you can tell it's running by the green symbol here and the words running green all the way to the right.
3965000	3974000	So let's go to this running notebook and actually save it, save in checkpoint and download as a Python file.
3974000	3984000	Let's download it to the same directory of develop, save that, and let's download this as an .html file and save it in the same spot.
3984000	3991000	So if we take a look at what this is, it has taken all of the Python code and none of the output,
3991000	3995000	but it's shipped out everything else in this file that's not Python code.
3995000	3998000	And so you see this input three, input four, and so on.
3998000	4004000	This is delineating the cells in the notebook, but everything you see here is actually Python code.
4004000	4009000	So this can actually run as a .py or you can run it as Python, this file name.
4009000	4016000	And this .html file, if we open up this file, we actually see the HTML representation of the notebook.
4016000	4018000	So this is not executable.
4018000	4020000	This is just a .html file.
4020000	4026000	And so this can be copied into an email and read by anyone who opens this with a web browser.
4026000	4030000	You don't need to run Python or IPython to actually see the output here.
4030000	4034000	Again, the limitation, though, is you cannot actually edit this code and make a new plot,
4034000	4038000	but this is great for being able to share a particular notebook.
4038000	4044000	So I recommend saving both of those file types to your Git repository.
4044000	4048000	And of course, all of the figures as well if you create separate figures.
4048000	4056000	The reasoning behind that is that the .py files allows a person to make easy changes to the actual Python code itself,
4056000	4058000	as well as to track those changes.
4058000	4064000	The .html file allows a person to see the fully rendered notebook without having to run a notebook themselves.
4064000	4068000	So the benefits of structuring your repository this way are several fold.
4068000	4072000	First of all, you have a complete record of the analysis that includes dead ends.
4072000	4077000	So if one day you worked down a single hypothesis and turned out that it wasn't very useful,
4077000	4080000	that is still saved in the lab notebook directory.
4080000	4085000	It also allows for easy peer review of the analysis and of the dead ends.
4085000	4091000	If in the future, a different team member has an idea to try to do a random forest regression on the data,
4091000	4094000	they can actually see if someone else has done the same type of analysis,
4094000	4097000	and if so, what led to a dead end, for example.
4097000	4102000	And finally, project managers can easily see and read the analysis with GitHub
4102000	4105000	because GitHub itself renders IP, UI, and Bs natively.
4105000	4109000	Or if you don't have GitHub access or not rendering it for whatever reason,
4109000	4115000	if you save the .html files, anyone can actually see the rendered notebook without having to run any code themselves,
4115000	4118000	or installing IPython or anything else.
4118000	4121000	Some final organization thoughts of this whole structure.
4121000	4124000	So organizing the workflow for teams is actually a difficult problem,
4124000	4128000	and I think this is a very good framework for having some standards.
4128000	4131000	And this bullet point about the wrong thing solves the problem.
4131000	4136000	Often with version control, software engineering types think we need the source that's version control
4136000	4140000	and we don't need to track the output, or that output is something that's blown away.
4140000	4144000	In data science work, the output is often the thing we need to look at.
4144000	4147000	For example, if there is a plot that shows some deviation,
4147000	4153000	that plot is best viewed in the peer review process, actually in the notebook itself,
4153000	4159000	or in an .html rendering of that notebook, because that gives rise to any sort of correction
4159000	4161000	or reinterpretation that needs to happen.
4161000	4165000	So the output actually is the thing that matters in a lot of data science work.
4165000	4168000	So storing that in version control is actually the right thing to do,
4168000	4172000	even though in typical practice it's the wrong to actually store the output.
4172000	4176000	Finally, I am open to new ideas if you have a better way of solving these problems,
4176000	4180000	or if your situation is completely different so that such that you will always be using GitHub,
4180000	4184000	you never have to worry about seeing a rendered .html file.
4184000	4189000	You can make these modifications by doing your own version of this kind of organization.
4189000	4194000	So hopefully this gave you some structure to organize how a team of data scientists
4194000	4197000	would work in a Git environment.
4199000	4203000	In this lesson, we'll be getting some data that we can actually do some data science with.
4203000	4208000	I recommend having a data folder in your projects directory that actually is at the same level
4208000	4212000	as your deliver directory, your development directory, and your source directory.
4212000	4217000	In my case, I have about 10 files in here that are coal data from the U.S. government.
4217000	4224000	If you'd like to grab this same data set so you can follow along, go to www.eia.gov.
4224000	4228000	This is the government's energy information administration website,
4228000	4234000	and if you go to the data tab, you can scroll down to where it says production, give that a click.
4234000	4237000	And there's lots of different data available here,
4237000	4243000	but we're looking at the historical detailed coal production data available from 1983 to 2013.
4243000	4247000	Select which year you'd like to do, and in case I picked 10 of them,
4247000	4250000	click the arrow here and save it into that data directory.
4250000	4255000	Once you do that, you'll then have the data that we'll need for this upcoming lessons.
4257000	4260000	In this lesson, we're going to take our very first look at the data.
4260000	4263000	We might even do some initial data cleaning.
4263000	4269000	I'm currently in this directory where you can see we have data, deliver, development, and source directories.
4269000	4273000	I'm going to start the Jupyter Notebook by, again, typing Jupyter Notebook.
4273000	4276000	From here, we see the same directories I just saw in that directory.
4276000	4283000	Let's open up the development list and start a new notebook by going over to new Python 2 notebook.
4283000	4286000	From here, we see the familiar text box where you can type in code.
4286000	4289000	In here, we see the code box is actually surrounded by green,
4289000	4293000	which means as we type, it should be typing in text into that cell.
4293000	4298000	We're going to need the pandas library, and we're going to import it as import pandas as pd.
4298000	4303000	This can create alias for the pandas library to actually be called pd.
4303000	4308000	This is a standard way of calling pandas, and I recommend you following the standards as often as possible.
4308000	4312000	This lets you share your code with other people in the most seamless way possible.
4312000	4316000	To run the cell, I can click the run cell button in the toolbar,
4316000	4323000	or I can have done the shift enter technique, which, as you can see, increments which input number it is by one.
4323000	4330000	The pandas version that I'm actually running is done by doing a print double underscore and then hitting the tab button.
4330000	4334000	Hitting tab is a thing you should be thinking about doing quite often,
4334000	4337000	because it often lets you make sure you don't have to type everything out.
4337000	4342000	It's faster, but also make sure you are in the right vicinity of what you're hoping to do.
4342000	4345000	There's a version, I'm going to hit return here, and then shift return,
4345000	4350000	and it prints the pandas version that we're using, which is 0.17.0.
4350000	4354000	From here, let's actually take a look at our very first data file.
4354000	4358000	The way we can read this in, we happen to know, and here's an interesting side note,
4358000	4363000	if you type ls and execute that, you actually see all the folders in the directory you're currently in.
4363000	4367000	If you type ls up one, we see the parent directory,
4367000	4370000	and if you'd like to look at what's in data,
4370000	4374000	we see the files that we just downloaded in the previous video.
4374000	4379000	Let's load in one of these Excel files and take a look at what's actually in them.
4379000	4385000	I'm going to create a variable called df for data frame, and I'm going to df1 for the first one.
4385000	4389000	I'm going to do pd.read, and I think it's going to be Excel,
4389000	4392000	but I type the tab and I see an option pull up, and it is.
4392000	4396000	It is pd.read underscore Excel, open parentheses.
4396000	4401000	At this point, if you're not sure what a function does, there's a function called tooltip,
4401000	4404000	which is generated by holding down shift and hitting tab once.
4404000	4410000	Here it tells you the signature for this function, which has an input output, a sheet name, header, and so on.
4410000	4414000	A lot of different options available for reading in Excel files.
4414000	4419000	There's actually a longer version of this, where if you do shift tab tab in rapid succession,
4419000	4424000	so it's a double tab, then you have the full doc string and the examples that go along with it.
4424000	4429000	This is a very useful feature, so you can actually look up documentation on the fly, and it's very useful.
4429000	4433000	In this case, we're going to try to load in the data from above.
4433000	4437000	Again, tab completing commands will make your life much easier.
4437000	4443000	As I start typing out this, I can hit tab and it actually produces again a list of possible data sources.
4443000	4445000	Let's just see if this works.
4445000	4450000	Head is a function on a data frame, and it lets you show the various options.
4450000	4452000	We see that a number of things have happened here.
4452000	4457000	First, we have the year, the MSHA ID, the mine name, the mine state.
4457000	4461000	We actually see some of the data, and this is just the first few rows by doing head.
4461000	4466000	I recommend doing head because it actually stores the full output of this.
4466000	4468000	It's a separate thing that you can actually call.
4468000	4473000	In future lessons, I'll explain exactly why using .head as best practices,
4473000	4479000	but for now, let's just use .head to look into the contents of our pandas data frames.
4479000	4484000	At this point, we've taken a first look at loading in some Excel data files,
4484000	4487000	and we're going to start looking at this and playing around with it.
4489000	4494000	In this lesson, we're going to take a look at how we can start to manipulate the data that we've read in
4494000	4496000	in ways that are useful for analysis.
4496000	4501000	Last time, we read in the CoalPublic2013 file and took a look at the header.
4501000	4504000	The heading had an interesting, well, let's call it a problem.
4504000	4507000	The historical Coal production data is the title here.
4507000	4512000	There's a source function. There's also a bunch of nans, and all the columns are unnamed.
4512000	4520000	This is most useful when this line, line 2, which is our row 2, is actually year MSHA ID, mine name.
4520000	4526000	This is supposed to be the headers or the column names, and all the rest of it should be the actual rows of data.
4526000	4530000	We're going to put the second row here up to the columns at the top.
4530000	4533000	We'd also like to make this ID the index for the pandas data frame.
4533000	4538000	We'll go into exactly why in the future, but for now, let's merge the reading in of the data frame
4538000	4542000	with the printing out of what the head of that data frame looks like.
4542000	4548000	We're going up here and clicking Edit, Merge Cell Below, because we've actually selected the above cell.
4548000	4550000	So merge the cell below into one.
4550000	4558000	So now that I execute this cell, we see that there is in one cell both reading the file and looking at the head of the file.
4558000	4562000	Now, this is, again, wrong. We would like to remove this top part.
4562000	4567000	So the way to remove this is we're actually going to use a thing called header and start giving it a number.
4567000	4572000	Because if we look at this, we can see that it actually takes a header equals zero as the default value.
4572000	4576000	So if we do header equals one, it actually deletes that top row.
4576000	4581000	And so this is a way of telling the pandas that, hey, you don't have to modify that Excel file.
4581000	4585000	You can just, when you read it in, know that there's two lines of header files.
4585000	4590000	Now, there was two lines that had data in it, and there was a third NAN line that just, it knew it could not possibly be the header.
4590000	4591000	So it removed that.
4591000	4595000	So now the column names are these bolded ones are at the top.
4595000	4597000	We're getting very close to what we actually want.
4597000	4600000	Another thing we'd like to actually do is set the index.
4600000	4606000	So we set the index by typing index and hitting tab because we think it's going to be something like set index or index set.
4606000	4610000	And it's index columns equals, if this type, we like the name of it.
4610000	4614000	So we would like to do the MSHA ID as the index column.
4614000	4624000	And doing that, we see that the MSHA ID is indeed the index for this data and the columns are all appropriately named.
4624000	4627000	This is one way to interact with the pandas library.
4627000	4632000	But it actually applies to all Python libraries that have any sort of documentation strings.
4632000	4638000	Just to give you an example of that, I'm going to save this currently and just show you example function, right?
4638000	4640000	We define a function by typing def.
4640000	4642000	We'll do it test function.
4642000	4651000	Let's say it takes two values first equals five and second equals 10 and it will return first plus second.
4651000	4655000	Let's give it a doc string and we execute that line.
4655000	4664000	If we start typing test underscore f and then hit tab, it will automatically complete that because we have a defined function here called def function.
4664000	4667000	We do the initial parentheses and hit shift tab.
4667000	4670000	You actually see the doc string that we wrote just above.
4670000	4676000	This is an example and it has the signature of it to the first equals five second equals 10.
4676000	4682000	If you want to redefine what actually we give it, we can say first equals three and the test function gives us 13, which is what we'd expect.
4682000	4696000	So that's just a fun side note on how the interaction with Jupiter notebook lets you look into the doc strings of functions that you define yourself as well as any of the libraries that you'll be using your data science day to day.
4696000	4701000	In this lesson, we'll be making a new GitHub repository for a new data science project.
4701000	4708000	So let's go over to GitHub and from GitHub, if you go all the way over to the right, you can create new repository.
4708000	4711000	Give the repository some name that you think makes sense.
4711000	4713000	So we'll do some coal exploration.
4713000	4717000	So let's make a coal exploration repository name.
4717000	4719000	You can give it a description if you'd like to.
4719000	4722000	You don't need to decide whether it will be public or private.
4722000	4725000	I'll let it be public so that you can see this as well.
4725000	4728000	And generally, I like to initialize the repository with a read me.
4728000	4732000	It get ignore file that's Python because I use a lot of Python code.
4732000	4735000	And I add an MIT license.
4735000	4738000	After doing all this, click create repository.
4738000	4744000	Once you click create repository, you can go over to this place here where you can click SSH.
4744000	4747000	You can have HTTPS or SSH.
4747000	4749000	I just use SSH most of the time.
4749000	4751000	Clicking once in here highlights everything.
4751000	4753000	Command C will copy this.
4753000	4761000	And going back into a terminal, type git clone and then command V to paste the required link.
4761000	4762000	Hit return.
4762000	4767000	And you will now clone the GitHub repository to your local machine.
4767000	4772000	And from here, we see a new coal exploration folder being created.
4772000	4782000	And if we CD into coal exploration, we see that it has a license and a read me file that we've made previously.
4782000	4785000	In this lesson, we'll be taking our GitHub repository that we've just started.
4785000	4787000	We'll first look at the data.
4787000	4792000	So the directory as we last left, it has two files in it, a license and a read me file.
4792000	4795000	We're going to create some extra directories and some structure around here.
4795000	4798000	And I'll go through the reasoning behind this in other videos.
4798000	4805000	But we're going to create using the make directory command, a data directory, a deliver directory,
4805000	4812000	which is going to house the final deliverable important Jupyter notebooks, a develop directory,
4812000	4815000	which is where we're going to mostly do our development place,
4815000	4819000	place to put our source code if we have any scripts that we'll end up using.
4819000	4823000	So separate from ipython notebooks, usually Python files or other kinds of scripts,
4823000	4828000	we'll go in a source directory and a figures directory running that command.
4828000	4833000	The folder structure that we have now has a data deliver develop figures and source directories.
4833000	4836000	So let's actually get that data and put it into this directory.
4836000	4837000	You might have already downloaded it.
4837000	4842000	If not, again, the way to get this is to go to eia.gov slash coal.
4842000	4846000	Go to the data tab down to production.
4846000	4849000	And we go to the historical detailed coal production data.
4849000	4852000	And let's just use the year 2013 for now.
4852000	4857000	We're going to go into this coal exploration, navigate to the data folder and save.
4857000	4860000	That is done downloading.
4860000	4864000	You can see it in this folder as coal public 2013.
4864000	4865000	Great.
4865000	4866000	So let's take a look at this.
4866000	4868000	We'll open up a Jupyter notebook and take a look.
4868000	4872000	So from this top level directory, I will start Jupyter notebook.
4872000	4875000	You can now close this download file.
4875000	4879000	And you can navigate this structure similarly to the terminal itself.
4879000	4882000	So you can actually click data and you see the coal public data that we had before.
4882000	4889000	We can navigate back and let's go into the develop and start a new Python to notebook.
4889000	4891000	It starts off being called untitled.
4891000	4893000	And that is a not very helpful name.
4893000	4898000	So I recommend using the date in ISO 8601 format.
4898000	4901000	And the reason for that is that it helps with sorting.
4901000	4906000	But basically it goes year dash month dash dates today is the 21st.
4906000	4915000	After you do the date, I recommend, especially if you're working in teams to have your initials or some other identifier that creates it so that people know it's your notebook.
4915000	4917000	And so I'm going to type my initials here.
4917000	4923000	And then I recommend having a couple words that describe what you think you're doing in this notebook.
4923000	4926000	So I think I'll just say a first look.
4926000	4932000	So now I've renamed that notebook and it helpfully tells us when it lasted the last checkpoint.
4932000	4935000	This means when it's been saved auto saves every once in a while.
4935000	4940000	You can also click this button, but you just see that the last checkpoint saved.
4940000	4942000	And you can also do command s, which is how I normally do it.
4942000	4947000	So this means that it's keeping auto saved versions of this as we go along.
4947000	4948000	All right.
4948000	4951000	So there's a number of libraries that we'd like to import.
4951000	4955000	And I import these almost every time and it starts off with matplotlib inline.
4955000	4959000	So this percent sign at the top of the line means it's a magic import.
4959000	4968000	And we also have to import matplotlib like so importing it as PLT is the standard best practice for doing that next we import pandas.
4968000	4972000	And we should also import seaborne, which is a package that wraps matplotlib.
4972000	4976000	Interestingly, you're supposed to import seaborne as SNS.
4976000	4980000	I don't know exactly why, but importing it as SNS is the standard way of doing it.
4980000	4984000	Also, if you do SNS dot set, it actually sets a number of the default parameters for matplotlib.
4984000	4987000	So it already looks nicer if you just use it from there.
4987000	4989000	So let's go ahead and start with that.
4989000	4994000	And now let's read into a data frame, the data file that we just downloaded.
4994000	5001000	So we say df equals pandas library dot read hit tab to see the options go to Excel.
5001000	5006000	And we navigate to the directory by going up one directory by doing dot dot slash.
5006000	5010000	If we hit tab, we also get the possible navigation options.
5010000	5014000	It's in the data and if you tap again, it will have complete to say cold public 2013.
5014000	5020000	If we actually execute that and take a look at the head, we notice that we again have this unnamed part at the top.
5020000	5023000	So we actually wouldn't like to remember that it has a header.
5023000	5028000	Set the headers equal to two and that correctly gets the column types labeled in there.
5028000	5032000	And we want to set the index to the MSH ID.
5032000	5041000	So if it's annoying, you set index by doing index something hit tab and its index column equals MSHA space ID.
5041000	5047000	Excelling those two cells, you have the ID of the mine setting as the index of this data frame
5047000	5051000	and all the data in here correctly parsed from that Excel file.
5051000	5060000	Okay, so I'm going to stop it here and we'll begin to actually start to plot this and take a look at what this data actually looks like.
5060000	5065000	In this lesson, we'll take a look at the data and do some data cleaning and maybe do some visualizations.
5065000	5072000	Let's go back into this notebook and rerun the first cell here, load everything in that warning that we've seen before.
5072000	5076000	Load in the data and take a look at the data dot head.
5076000	5078000	So everything here looks normal.
5078000	5084000	And the day to day data science work, you often take a look at what's in each of these columns.
5084000	5090000	So we can just do a very quick look, for example, at a data frame and take a look at the company type.
5090000	5097000	Now, if we have thousands of rows, we don't want to look at all of them, but we do want to look at the unique ones.
5097000	5102000	In here, we see that there's three types of unique companies according into our file right now.
5102000	5106000	We have what I think the word is supposed to be independent producer operator.
5106000	5110000	The next one is operating subsidiary and contractor.
5110000	5116000	Now, obviously, this first piece of information is that the data has some data quality issues.
5116000	5119000	So let's go ahead and actually make a correction here for this data.
5119000	5126000	We'd actually like to replace all of the independent producer operators with independent producer operators.
5126000	5133000	So the way to do this in place is to actually do a company type to replace it.
5133000	5138000	And if you don't remember the syntax for replacing, if you do a shift tab, you can actually see the tool tip come up.
5138000	5140000	There's two ways to do this.
5140000	5144000	You can say to replace equals x, the value in place, everything else.
5144000	5147000	And we can also do it by giving it a dictionary.
5147000	5149000	I'm actually going to do it the standard way.
5149000	5156000	So to replace should be equal to, we'll just copy the words from above.
5156000	5162000	And the value I would like to replace it with is going to be the independent producer operator.
5162000	5165000	This cell is already becoming wider than the screen.
5165000	5169000	So I'm going to actually hit return here so that it's lined up with the beginning of this.
5169000	5172000	So you can say one later on can actually read this in a much nicer way.
5172000	5177000	Suppose a DF company replace this thing and then do head on this.
5177000	5184000	It should show us that it is indeed replacing the independent producer, but it hasn't replaced it in the actual data frame itself.
5184000	5190000	To do that, we have to add an extra command here, which is in place equals true.
5190000	5194000	One extra interesting, let's call it a quirk of the Jupiter system.
5194000	5200000	If you're in line with the beginning of this command, if you do a tool tip by doing a shift tab, it appears.
5200000	5205000	If you're not on that first line and it's broken up across multiple lines, then doing the shift tab in the middle here will not work.
5205000	5208000	If you're thinking, is it in place one word or is it in underscore place?
5208000	5210000	You have to do it up here to get the tool tip help.
5210000	5212000	So it's in place one word.
5212000	5213000	So I typed it down here.
5213000	5219000	This will in place change the DF company type to be independent.
5219000	5222000	So this has now been replaced in place.
5222000	5230000	Now we also see that even though I could actually hit tab, which is a very useful thing to be able to call the column heading by just typing the beginning of it.
5231000	5237000	Having these spaces is going to just make life a little bit more difficult than it should be.
5237000	5244000	So what I'd like to do is actually go through all of the columns in this data frame and replace every single space with an underscore.
5244000	5247000	So it's still readable, but I'd just like to actually do that.
5247000	5250000	So to do that, we actually would like to do a name of the columns.
5250000	5255000	So we DF dot rename index columns equals and keyword arguments.
5255000	5257000	So you can say columns equals.
5257000	5261000	Now this is a really fun trick because you actually pass a Lambda function.
5261000	5266000	Lambda function says for everything in that columns, I'd like to do X dot replace.
5266000	5273000	So similar syntax as above, but I replace all of the spaces with underscores.
5273000	5280000	So the thing that's being quoted is the thing that's being found single space replacing that space with is the underscore.
5280000	5286000	So I'd like to rename the data frame where every column space will be turned into an underscore.
5286000	5290000	And of course, I would also like to actually make this happen to the data frame in place.
5290000	5296000	So I say in place equals true now to check if that actually worked as we hope we can look at the DF dot head.
5296000	5301000	And we see that underscore name mine underscore state mine underscore county and so on.
5301000	5307000	So this with one line and very quickly typing it out replaced all of the spaces here with underscores.
5307000	5311000	And this will just make life much easier as we go on from here.
5311000	5313000	Let's also take a look at how big is this data frame.
5313000	5315000	We have 1400 data points.
5315000	5318000	And let's take a first look at just what's in here.
5318000	5325000	So we just read this off as my name all the way through regions and average employees and labor hours.
5325000	5331000	Let's see what the relationship between the number of employees for a mine and the number of labor hours looks like.
5331000	5333000	There's a couple of ways we can do this.
5333000	5335000	Let's see the simplest way I can think of is to do a scatter plot.
5335000	5340000	So we can do PLT dot scatter and DF dot average employees.
5340000	5348000	So now I've indexed the data frames column by simply doing a dot before it because it has a space in it.
5348000	5353000	I would have to have done it the DF bracket space labor hours, for example.
5353000	5355000	So this will actually work.
5355000	5357000	You see that the plot actually works as expected.
5357000	5364000	But now instead of having to type out labor hours previously with a space there, I can actually do dot labor hours.
5364000	5368000	And that just makes my life just ever so slightly bit better.
5368000	5372000	Let's label this.
5372000	5375000	Okay, so just as we expect, number of employees goes up.
5375000	5379000	The total number of hours worked at that mine goes up in a pretty linear fashion.
5379000	5383000	Another way of doing this would actually be linear regression plot on this.
5383000	5385000	And you can use Seaborn for that.
5385000	5387000	So SNS dot regression plot.
5387000	5391000	And I'll do, I'll pass it the X and Y this way.
5391000	5395000	And so when you can see here, the regression plot does the same thing as above,
5395000	5400000	but it actually fits aligned in the data and gives it a bootstrapping in the middle of it.
5400000	5403000	This bootstrap is done by a confidence interval of 95%.
5403000	5409000	And it bootstraps a thousand times the underlying data to actually figure out what the variance is.
5409000	5415000	So this is a kind of neat, very quick way of getting an initial look at two variables that you think might have a relationship and they clearly do.
5415000	5421000	Now, if you'd like to actually save this figure as in this isn't just to look at and have it for later on,
5421000	5425000	you should actually save this figure into the figures directory.
5425000	5429000	So I would do PLT dot save fig figures.
5429000	5438000	And I like to actually have the same beginning date structure for these figures so that if I am looking through the figures directory later on
5438000	5445000	across all the different notebooks that I'll be looking at, I can easily re-correspond which figure came from which notebook.
5445000	5450000	So this is just a little bit of mental accounting to get this straightforward.
5450000	5455000	And let's do employees versus hours.
5455000	5458000	Let's keep our underscores and spaces being the same.
5458000	5465000	All right, so that's our first look at the data and it is a quick linear regression plot against two of the features that we found inside,
5465000	5471000	as well as a bit of data frame manipulation using pandas.
5471000	5475000	We've seen a very first look at this and we see that there's at least some trends in this data.
5475000	5477000	There's probably something pretty interesting in here.
5477000	5481000	So I'll keep going with this data set and seeing what I can come out with this.
5481000	5488000	Now I will actually remove this header and I will toggle the toolbar as well as I need space.
5488000	5490000	So let me go ahead and do that.
5490000	5496000	So previously we saw with seaborne a really nice regression of the average number of employees versus labor hours.
5496000	5498000	Let's keep seeing what's in this data set.
5498000	5501000	Let's take a look at the columns for column in.
5501000	5503000	So these are the columns in the data frame.
5503000	5511000	We have year and then various things about the mind itself, the name, the state, the county, its status and its type, the company type, union code.
5511000	5517000	There's a coal supply region, the production in short tons and the number of employees in labor hours.
5517000	5524000	So see if the amount people work, like the labor hours total is very predictive of the production in short tons.
5524000	5526000	Let's take a look at that scatter plot.
5528000	5529000	Let's take a look here.
5529000	5533000	So it doesn't appear to be a fantastic relationship here.
5533000	5535000	Let's take a look at the actual histogram of this.
5535000	5541000	So I'll do df production short tons dot hist, which is a function on pandas.
5541000	5545000	And we see a very bad looking histogram.
5545000	5551000	So it looks like a lot of things in this first one, which is either typical of a power law or some other kind of problem.
5551000	5553000	Let's do a few transformations on this production.
5553000	5560000	Let's see if we can find the minimum value or yeah, let's take a look at the minimum value zero.
5560000	5564000	Let's take the length of the data frame where this is equal to zero.
5564000	5569000	So if we did first, let's just look at this where the production short tons is equal to zero.
5569000	5574000	We have what's returned as a series that tells us false false true true false and so forth.
5574000	5576000	So this tells us whether or not the production is equal to zero.
5576000	5583000	So we say df where you actually give this as an argument to data frame saying where this is equal to zero.
5583000	5587000	We get the full data frame where all of the production values are equal to zero.
5587000	5591000	And it looks to be like quite a few of these things produced zero tons of coal.
5591000	5597000	In the interest of how much a coal mine is producing, let's take the ones that have produced at least one ton.
5597000	5602000	We will say the data frame where the production of short tons is greater than zero.
5602000	5606000	This has, okay, values that are not zero. This is good.
5606000	5608000	From here, we will now set the data frame equal to this.
5608000	5610000	Now we are at this point doing a slice.
5610000	5611000	So I will make a note here.
5611000	5614000	We are removing data here.
5614000	5617000	That's okay as long as you're keeping track of what you're doing and why.
5617000	5621000	So the reasoning behind this is if we're going to try to predict, let's say the production of mines
5621000	5625000	and use things like what state the mine is in as a predictive indicator.
5625000	5629000	Let's actually restrict ourselves to mines that produced something more than zero.
5629000	5632000	And that's the reasoning behind how I choose something like this.
5632000	5638000	So now data frame is equal to where the data frame production values is over zero.
5638000	5640000	So let's see what the length of data frame is now.
5640000	5643000	Okay, so we have 1061 data points.
5643000	5645000	Let's redo this one.
5645000	5648000	I'm going to copy this and place it down here just so that we can do a comparison.
5648000	5653000	And it appears to still have quite the skew distribution.
5653000	5658000	So I will try to do something now where I will actually take the log of this.
5658000	5660000	So let's create a new column.
5660000	5666000	And the way to create a new column in pandas is to actually just create a column as though it exists
5666000	5669000	and set it equal to a function of this.
5669000	5672000	So I don't know if I have NumPy installed just yet.
5672000	5674000	So I'll give this a try.
5674000	5676000	So let's go to the top of the page.
5676000	5680000	And in all of our imports at the top of the notebook, I recommend keeping them together
5680000	5685000	so that if and everyone later on can see where things were imported, import NumPy as NPs.
5685000	5687000	Now this input is 30.
5687000	5690000	I've imported it and I should be able to rerun this one all the way to the bottom here
5690000	5691000	and create a new one.
5691000	5695000	So let's look at df.logproduction.hist.
5695000	5699000	So what we see here is a very close to a log normal distribution.
5699000	5703000	So the production of coal mines follows a log normal distribution,
5703000	5706000	which is reasonable from first guesses.
5706000	5707000	All right, great.
5707000	5711000	So I think I'm going to stick with this as a thing we're going to be interested in predicting.
5711000	5713000	So we have our prediction variable.
5713000	5717000	Now at this point, we've done quite a few things to the data frame itself.
5717000	5720000	So we loaded it in, we renamed the columns.
5720000	5724000	We actually created what's going to be my target variable is going to be the production of these mines
5724000	5727000	and did a transformation, which is the log of this value.
5727000	5731000	So after doing all this, I think I would like to actually save out this data frame
5731000	5734000	that I can load it into any future analysis.
5734000	5736000	So I'll do df.2.
5736000	5738000	Let's save it as a CSV.
5738000	5742000	So I'll call it, let's find it in the data directory, coal public this thing.
5742000	5744000	We'll do cleaned version of this.
5744000	5746000	And it's a CSV.
5746000	5751000	So now that I've done this exploratory analysis, I would have this first look that I've taken at
5751000	5753000	and I've saved the data out into this CSV file.
5753000	5756000	I'm going to copy this into a new one that's going to be called data cleaning.
5756000	5760000	And in the future, all I'll have to do is load in this CSV file
5760000	5762000	and all the transformations will have already been done.
5762000	5769000	And I'll have a link back to the reasoning behind it as well as the actual code that does this process.
5769000	5773000	In this video, I'll be cleaning up the data cleaning notebook
5773000	5779000	and I'll be doing our first commits to a new branch to keep everything organized.
5779000	5783000	I last laughed off with this first look and their develop directory.
5783000	5787000	So what we're going to do now is actually make a copy of this
5787000	5789000	and I will toggle the header for this.
5789000	5791000	Make a copy.
5791000	5794000	And the first thing it does is it opens a new tab with everything copied in the previous one.
5794000	5798000	And none of the code has been run here even though all of the inputs have been copied.
5798000	5801000	What we're going to do here is actually call this something completely different
5801000	5803000	which is data cleaning.
5803000	5806000	I didn't put a date in front of it because this is the notebook
5806000	5811000	that's going to be the one that people look at if they actually want to see how we changed the data.
5811000	5815000	So I'm going to actually close this from this directory,
5815000	5818000	go over to my actual terminal here
5818000	5822000	and move from the develop the data cleaning ipython nb
5822000	5824000	which we just created into the deliver.
5824000	5827000	So we move the file from develop into deliver
5827000	5830000	because deliver is the directory that people should be looking at
5830000	5834000	if they're actually interested in seeing the final analysis that matters.
5834000	5838000	In this case, we don't want to hide data cleaning in this development directory
5838000	5840000	which has many, many files.
5840000	5844000	So we've moved it into deliver and if we go back to our browser here
5844000	5847000	go up into deliver and open up the data cleaning.
5847000	5851000	Now we should actually start to do things like actually creating the markdown file
5851000	5854000	changing the code from code to markdown
5854000	5857000	giving it a nice title and continuing on with this.
5857000	5861000	So we can say Jonathan by Jonathan to say like who actually did this
5861000	5865000	and then you can look it up in the get repo cleaned up the data
5865000	5869000	removed zero production coal mines.
5869000	5871000	You can actually do a bit more of that in the end
5871000	5873000	but for now that should suffice.
5873000	5875000	We don't need to actually have any of these plots in here.
5875000	5878000	So I'm going to be cleaning this up as quickly as I can.
5878000	5881000	So numpy as NP pandas as PD.
5881000	5883000	We need to read in the file still.
5883000	5884000	We don't need to see the head.
5884000	5885000	We know what that looks like.
5885000	5889000	This can be left in because it tells us the transformation we made and why
5889000	5892000	the head part doesn't need to be here for the second one
5892000	5895000	but we can add a note above it that says mistake
5895000	5898000	renaming indipedent to independent.
5898000	5904000	Now we're in here changing spaces to underscores
5904000	5906000	double check that still looks right.
5906000	5909000	Okay, it does and we will now delete this head,
5909000	5915000	delete the different plots here and give an extra sentence here.
5915000	5921000	Coal mines without any coal production are removed.
5921000	5925000	The length is 1061 and we are now creating a new column called log
5925000	5930000	production which is the log of the production of the data frame.
5930000	5933000	And we can put we don't have any histograms here.
5933000	5937000	We need that out and now the output file is this guy and I will
5937000	5942000	actually move this to the top here to the output file.
5942000	5945000	The very first thing you see here will be the name of the output file
5945000	5949000	and the last thing we'll do is actually write that CSV to that output file.
5949000	5953000	So now when I load in this cleaned coal public 2013 and notice
5953000	5955000	that I did not overwrite the old file.
5955000	5958000	So I strongly recommend keeping the raw files and creating a new file.
5958000	5961000	That's the cleaned version of it so that if you ever made a mistake
5961000	5963000	in your cleaning which has happened before,
5963000	5965000	you can easily revert and change that back.
5965000	5968000	And if someone says, oh, something happened in the cleaning process,
5968000	5970000	you have a full documentation of what happened here.
5970000	5974000	So we've created the final document that went through and cleaned up
5974000	5977000	what actually happened in the cleaning process.
5977000	5980000	So anyone looking in the future can easily follow what happened.
5980000	5984000	So I will now close and halt this directory and I'm going to actually
5984000	5988000	do our first commit and we are in the master branch as it sits.
5988000	5991000	So I will actually check out a new branch.
5991000	5998000	The branch will be called JBW underscore predict production.
5998000	5999000	And so we're here.
5999000	6002000	There's two theories here on adding the data.
6002000	6004000	So the data here is actually pretty small.
6004000	6006000	So I'm going to add it to this.
6006000	6009000	This is also so that you can actually get the data as well.
6009000	6010000	Generally in a production environment,
6010000	6012000	you don't add the data to your Git repository.
6012000	6014000	This is stored and tracked in some other way.
6014000	6016000	So I'll add the data cleaning.
6016000	6021000	I'll add develop and not going to add the figures just yet.
6021000	6025000	I usually will only add this when I actually have something interesting there.
6025000	6028000	So this figures is going to be kept on my own directory for now,
6028000	6030000	not put into the branch just yet.
6030000	6032000	Let's look at the status one more time.
6032000	6036000	So we have a number of new files, the actual data file, the cleaned data file,
6036000	6040000	the data cleaning that is the official way of actually making this file
6040000	6042000	and this develop one.
6042000	6045000	So let's commit this.
6045000	6047000	Let's not call it that then.
6047000	6050000	And I have to actually configure this.
6050000	6052000	So I will configure my Git.
6052000	6058000	Do this commit and continue this on in just a second.
6058000	6066000	So commit the data and I will be pushing it to GitHub.
6066000	6070000	So the final command I ran was git push origin JBW predict production.
6070000	6073000	And this means that I have now sent this off to GitHub.
6073000	6077000	Go back to the GitHub of the coal exploration, reload this.
6077000	6081000	What we see here is the master branch where you can actually go to the
6081000	6085000	JBW production branch and see the various things we've done here.
6085000	6089000	Let's actually look at the deliver and click this IPYNB.
6089000	6093000	And we'll notice that GitHub does a fantastically nice job of actually rendering
6093000	6096000	the notebook as it looks correctly.
6096000	6100000	And this is even more dramatic when you actually look at the develop one.
6100000	6104000	So we can see this and you can see in here if you're browsing with GitHub,
6104000	6106000	the figures are faithfully reproduced here.
6106000	6110000	And this is a very useful thing to be able to look at the files being used
6110000	6114000	and especially when we do a pull request in the future.
6114000	6116000	Okay, so we've cleaned the data.
6116000	6119000	We have the way that we cleaned it separated out so that anyone else
6119000	6121000	can look at it in a reproducible way.
6121000	6124000	And so let's actually try to predict something.
6124000	6128000	So I'll go back into this develop directory and it will make a copy
6128000	6131000	of the first look notebook that we had.
6131000	6135000	So I'll make a copy of this and I'm going to call it CoalPredict.
6135000	6139000	I'm going to go back to the previous tab and actually finish closing this
6139000	6140000	and halting it.
6140000	6144000	And just to give you a sense of how everything is standing,
6144000	6147000	I'm now back at the home of this develop thing.
6147000	6151000	You can see the first look notebook and it's currently black because it's not running.
6151000	6154000	This one is green because you can see on the right here it says running.
6154000	6157000	So this is a notebook that's currently being run.
6157000	6160000	There's a couple of things I want to do different here since this is now the prediction one.
6160000	6166000	When I start off by saying what the goal of this notebook is going to be
6166000	6170000	and because everything that's here is a direct copy of the previous notebook,
6170000	6172000	most of this stuff I'll just be able to delete.
6172000	6176000	So I'm going to toggle the header, give us a little bit more space
6176000	6181000	and the changes I'm going to make are basically going to drive me toward being able to make this new prediction.
6181000	6185000	So first of all, I don't want to reproduce all this cleaning I did before.
6185000	6188000	So I will actually instead of reading in the previous raw data,
6188000	6191000	I'll actually go into and read the CSV that we saved.
6191000	6197000	And this is up into the data directory and it's the cleaned public CSV.
6197000	6202000	And we still need to set the index column to be the MSHA ID.
6202000	6207000	So that's loaded in and actually one thing I like to do is look at the head of the data frame
6207000	6211000	and read it in at the same time in case I need to make any changes.
6211000	6215000	So the way to do this is since the four is selected with a gray box,
6215000	6221000	if I hold down shift and type K, I'm selecting both the second and third cell which are index three and four.
6221000	6226000	If I type shift M, they are now combined into a single merged cell.
6226000	6233000	So let me just run this one cell and I read in the CSV and then you are seeing the head of that data frame as well.
6233000	6238000	So we can see that we're loading in the cleaned CSV and the head is looking nice.
6238000	6242000	I'm going to now delete a number of these things because we don't need them.
6242000	6247000	One thing I will remain is that we initially did this LEN of the data frame before.
6247000	6250000	This was on the first one that you saw on the raw data.
6250000	6253000	So since this is the clean data, I expect this to be just over a thousand.
6253000	6256000	Yep, it went to 1061.
6256000	6258000	Simply delete these.
6258000	6264000	I'll leave the number of columns in here so we can actually think about what's in each of these columns a bit.
6264000	6267000	Alright, so as we see, this is the production.
6267000	6271000	Longer the production is the thing that we're going to be trying to predict.
6271000	6276000	And let's take a look at just a high level view of the different categories that might be able to help us.
6276000	6278000	So let me get the columns here.
6278000	6282000	I think that the mine status might be a predictive variable.
6282000	6285000	So I do df.mine status.
6285000	6291000	You see that there's an active men working, not producing, permanently abandoned, active,
6291000	6295000	temporarily closed and new under construction of the different status types.
6295000	6300000	I suspect this will give me a pretty good predictor into how productive the mine actually is.
6300000	6304000	So I will actually do a group by on this to see what is in here.
6304000	6308000	So df.mine status.
6308000	6316000	Let's do production.
6316000	6325000	What I did here was I said, take all the ones that have the same status of active and take the average or the mean of the production in short tons.
6325000	6331000	And we can see that the active ones are much more productive than the temporarily closed ones or the permanently abandoned ones.
6331000	6336000	It's interesting to me that permanently abandoned has on average 60,000 tons.
6336000	6340000	Let's look at it in terms of the log of the production though.
6340000	6343000	This will be what I think we're going to be going against.
6343000	6350000	So huge difference in the overall production capabilities, but we'll see how good this is at making a final prediction.
6350000	6356000	So from here is we would like to predict the log of coal mines.
6356000	6362000	And we'd also like to know what actually leads to the production, higher production and lower production.
6362000	6368000	If we look again at all the columns in our data frame, the data that we have year is the same for all of them.
6368000	6371000	And various things that shouldn't matter at all.
6371000	6380000	Like the union code is just going to be a code that's given to the mine from a, let's just look at that.
6380000	6382000	Actually, that might be predictive.
6382000	6387000	So I'm going to try to throw as many of these things as we can into a predictive model.
6387000	6389000	So I'll call these features.
6389000	6392000	And let's start with this as our list of features.
6392000	6396000	We'll have our target be log production.
6396000	6402000	So year is going to be entirely unpredictable because it's a single thing.
6402000	6406000	Mine name, I suspect will not be predictive because it's simply the mine.
6406000	6411000	The state might be what state is it in, what county is it in that could be useful.
6411000	6414000	The mine status, I'm sure will be predictive.
6414000	6425000	Mine type will probably be it's possible that the operating type, the address of the operating company probably isn't because we already have the geographic things done with the county and the state.
6425000	6429000	Though it's interesting, we'll definitely have some collinearity between the state and the county.
6429000	6431000	So it's possible that particular county and the state's good.
6431000	6432000	We'll leave those in.
6432000	6436000	Leave in the union code, the coal supply region.
6436000	6442000	We can't give it the production of short tons as a prediction of the log of the production because that's cheating.
6442000	6446000	The number of employees that are employed and the number of labor hours.
6446000	6447000	Just to clean this up.
6447000	6452000	So I hold down shift and push the down arrow key and I've highlighted everything to indent.
6452000	6456000	I'm going to hold down command and hit the right bracket key, which is the square brackets.
6456000	6458000	So the parentheses are curved all the way around.
6458000	6462000	There's curly braces, which have a lot of curls in the square brackets.
6462000	6466000	So holding down command and typing the right one will indent an entire block of text.
6466000	6469000	If you do the left bracket, it unindents.
6469000	6472000	This is a quick way of formatting lists.
6472000	6477000	So the features that we're going to be giving our model are going to be all of these features here.
6477000	6480000	The target's going to be the log of the production.
6480000	6486000	Now of these, I think only two of these are actually numbers to start with.
6486000	6491000	So I think average employees and labor hours are the only ones that are proper features.
6491000	6494000	And the rest of them are what I'm going to call categorical.
6494000	6503000	So the categoricals are these minus the average employees in the labor hours.
6503000	6506000	And having a trailing comma here is actually okay.
6506000	6508000	We need commas between all the rest of them otherwise.
6508000	6510000	But this is one of my favorite features of Python.
6510000	6512000	And I don't know why it makes me so happy.
6512000	6516000	But having a trailing comma and having it not have a problem just makes me really happy.
6516000	6520000	So the features, which I'm going to just call the ones that are numeric,
6520000	6523000	are the average employees and labor hours.
6523000	6526000	The categoricals are the ones that are category variables.
6526000	6531000	So mine state, county, status, type, company type, operating type, union code,
6531000	6534000	and coal supply region are all categoricals.
6534000	6538000	One thing that we'll have to do is create, because we'll be using scikit-learn,
6538000	6543000	we'll have to turn these categoricals into numbers or into some sort of numerical thing.
6543000	6546000	And we'll be doing that with what's called a one-hot encoding.
6546000	6549000	Also called dummy variables. There's probably a few other names as well.
6549000	6551000	So we split this up into numeric features.
6551000	6554000	So things that have numbers representing how long people worked,
6554000	6558000	how many employees a mine has, categorical, which is what state
6558000	6560000	or some other thing that actually has a category,
6560000	6563000	and the target variable, which is log of the production.
6563000	6566000	From here, we need to do a bit more data munging after it's all been cleaned.
6566000	6574000	We now have to do some munging to make this into a form that scikit-learn can actually predict with.
6574000	6580000	In this lesson, we'll be looking at the final data munging and the final prediction for this data.
6580000	6582000	So I've actually changed up this slightly.
6582000	6585000	So the features that we'll be looking at, these are numeric features to start with.
6585000	6590000	The average number of employees per mine and the number of labor hours total worked for that mine.
6590000	6594000	And also a categorical list. This categorical list contains features
6594000	6600000	which have a small number of string representations instead of actual numbers.
6600000	6606000	And again, the target we're looking at is the log value of the production in tons.
6606000	6610000	So one thing that I recommend you doing is taking a look at the interplay
6610000	6613000	between each of the variables and the target variable.
6613000	6616000	So I'll do a quick example of this.
6616000	6620000	So let's take a look at the relationship between mine status,
6620000	6623000	which is a categorical variable, and the log of the production.
6623000	6628000	I'll be doing that with this Seaborn code here, which I just executed.
6628000	6632000	And the set context has to be run twice the first time.
6632000	6635000	What this is doing is doing a violin plot.
6635000	6638000	So this is the Seaborn library SNS, and it's creating this.
6638000	6640000	It's using the violin plot function.
6640000	6645000	And what we see here on the y-axis is the mine status, the five possible values,
6645000	6649000	active with men working but not producing, permanently abandoned, active,
6649000	6652000	temporarily closed, and new under construction.
6652000	6656000	And on the x-axis, we see the log of the production.
6656000	6661000	So you see that each of these mine status types corresponds to a different log
6661000	6663000	of the production value of that mine.
6663000	6666000	But also the distribution has this interesting shape,
6666000	6668000	and it changes between these categories.
6668000	6671000	This kind of a plot is a very nice high level view of what these variables
6671000	6673000	interactions look like.
6673000	6674000	I'll do just one more.
6674000	6677000	How does company type corresponds to the production?
6677000	6680000	So we see that there are three company types here, independent producer,
6680000	6683000	operating subsidiary and contractor, and each of those corresponds
6683000	6685000	to a very different distribution.
6685000	6688000	So you can do this for all of the variables, and I recommend doing that,
6688000	6691000	especially before and getting a sense of what the data actually looks like.
6691000	6695000	But for us, we just look at this company type a little bit more closely.
6695000	6700000	So if we do a DF company type dot unique, we return all the unique values.
6700000	6702000	Of course, we see the three that we see in the plot above.
6702000	6706000	An independent producer operator operating subsidiary and contractor.
6706000	6711000	The scikit-learn functions don't take in these strings as separate
6711000	6713000	category variables.
6713000	6715000	We actually have to encode this ourselves.
6715000	6718000	Now one way to encode this would be to do something like assign
6718000	6722000	independent producer to be one, operating subsidiary to be two,
6722000	6723000	and contractor to be three.
6723000	6726000	And that would work except that we are then implicitly telling,
6726000	6730000	let's say a scikit-learn random forest function that three is greater than two,
6730000	6731000	which is also greater than one.
6731000	6733000	And there's an implicit ordering there.
6733000	6737000	And it might start to try to cut the features in a way that doesn't make sense.
6737000	6741000	A more safe way to do this is to actually create what's called dummy variables.
6741000	6744000	Pandas has a built-in dummy variable function.
6744000	6748000	So we do PD dot get dummies on the data frame with just,
6748000	6751000	we're looking at the single column of company type.
6751000	6753000	And I'm taking a sample of 50 so that we get a mix of types,
6753000	6755000	because it's actually ordered in this data set,
6755000	6757000	and just taking a look at the top 10.
6757000	6759000	So I'm going to run this a couple of times.
6759000	6761000	This sample will actually re-sample every time I run it.
6761000	6765000	So what we see here is the contractor independent and operating subsidiary,
6765000	6768000	this MSHA ID corresponds to an independent producer operator,
6768000	6770000	because it has a one in that column,
6770000	6772000	and zeros in each of the other columns.
6772000	6775000	And if you go down to this 4407123 ID,
6775000	6777000	it is an operating subsidiary company,
6777000	6779000	and it has zeros in the rest of the column.
6779000	6782000	So this is what the get dummies function does with pandas.
6782000	6785000	Now what we want to do is actually turn each of the categorical variables
6785000	6787000	that we're looking at into dummy variables.
6787000	6790000	And then we'll actually learn to drop one of the variables
6790000	6792000	to avoid the dummy variable trap.
6792000	6795000	We're then going to concat the data frames together.
6795000	6798000	So we're taking the data frame and the temporary data frame together.
6798000	6803000	And axis equals 1 means it will add it as columns to the existing data frames.
6803000	6807000	And we will then drop the drop variable from the data frame
6807000	6813000	and call that to list function on the columns of the temporary data frame
6813000	6817000	so that we have a final list of what the dummy categories look like.
6817000	6819000	Let's run that real fast.
6819000	6820000	It completes very quickly.
6820000	6824000	We see that there are 29 mine states, 164 mine counties.
6824000	6825000	So this might be a little bit high.
6825000	6827000	We might have to come back and look at that.
6827000	6831000	The mine status, there's five, mine type 3, company type 3, and so on.
6831000	6833000	And the actual value of the dummy variables themselves,
6833000	6835000	let's take a look at say the first 10.
6835000	6839000	We see mine state, Alabama, mine state, Alaska, and so on.
6839000	6841000	So these are the different state variables
6841000	6843000	that have been created.
6843000	6845000	Let's actually start to build a model.
6845000	6849000	So we'll say, so I created this as a markdown
6849000	6853000	by typing escape to make me into select mode instead of insert mode
6853000	6855000	and typing m, m for markdown.
6855000	6857000	You can also go up here and click it.
6857000	6860000	So if I could go back to code, this is simply commented out Python code
6860000	6861000	as far as the notebook is concerned.
6861000	6863000	We actually want this to be markdown.
6863000	6866000	So we click markdown and you can see it pre-rendered
6866000	6867000	before we actually execute the cell.
6867000	6869000	And it looks like this nice bold font.
6869000	6872000	We're going to need to import a couple of things from scikit-learn itself.
6872000	6875000	So we're going to say from scikit-learn dot cross validation.
6875000	6878000	So this is the sub module of scikit-learn.
6878000	6882000	We're going to import the test train split function, which is labeled here.
6882000	6886000	And we're also going to use a random force regressor as our algorithm.
6886000	6891000	Loading that in, you look at total length of the dummy categoricals is 213.
6891000	6895000	The train and test is going to be the names of the data frames
6895000	6899000	that's going to be split by this test train split function.
6899000	6902000	The function takes in our data frame.
6902000	6905000	And you tell it how large you'd like the test size to be.
6905000	6909000	So in this case, we're going to have a 30% of the data frame is going to be the holdout set.
6909000	6913000	And the nice thing about this function is that we actually retain
6913000	6916000	the data frame structure of these variables.
6916000	6919000	Scikit-learn likes to think in terms of native numpy arrays,
6919000	6923000	but many of the features can actually read in a pandas data frame as well.
6923000	6927000	And the utility of having a pandas data frame around just makes it really nice to keep it,
6927000	6929000	to stay in data frames as long as you can.
6929000	6932000	So we can actually do it the whole way through. So that's really nice.
6932000	6935000	Our train is a data frame. Our test is a data frame.
6935000	6939000	And they've been split from the data frame that contains all of our data.
6939000	6941000	So now we're going to create a random forest.
6941000	6943000	And I would like to run these separately.
6943000	6949000	So I'm going to split this cell here by typing control shift minus splits the cells into two.
6949000	6951000	And I will execute this one.
6951000	6954000	This says RF is an instantiation of this random forest regressor,
6954000	6956000	which we imported above.
6956000	6958000	And there's two things we're going to initialize it with.
6958000	6960000	Number of estimators is 100.
6960000	6963000	This is a number of trees that we're going to be building a random forest out of.
6963000	6965000	And whether or not we're going to be using the out of bag score,
6965000	6967000	which we are in this case.
6967000	6971000	So we have an RF model and we'd like to fit on this by giving it
6971000	6975000	x comma y and sample equals non as default.
6975000	6979000	So the x value is the design matrix.
6979000	6981000	The y is the target variable.
6981000	6984000	So in our case, we're going to do the train data frame.
6984000	6986000	And we're going to give it all the features,
6986000	6990000	which is just those two average employees and the total laborer,
6990000	6992000	as well as the dummy categoricals.
6992000	6995000	Now, these two things together is just adding them together.
6995000	6997000	It creates a large Python list.
6997000	7000000	We can see the top two things up here at the top,
7000000	7002000	average employees and labor hours,
7002000	7004000	and then everything else is dummy categoricals.
7004000	7007000	We then run the fit method on the random forest
7007000	7010000	and we can get the design matrix of train features
7010000	7013000	plus dummy categoricals and the target,
7013000	7016000	which is train just selected on the target variable,
7016000	7018000	which we defined above as log production.
7018000	7022000	So it tells us some features or it gives us a little summary
7022000	7025000	where it talks about the bootstrap, the criterion as mean squared error,
7025000	7026000	various other things here.
7026000	7028000	So this is all the variables that you can change very easily.
7028000	7031000	If you'd like to actually tweak this for your own problems.
7031000	7033000	So let's take a look at how well this does.
7033000	7036000	And we're going to do this by giving a seaborne plot again,
7036000	7038000	a regression plot, but except the train,
7038000	7041000	we're going to be using the test data frame.
7041000	7044000	So I test the target and the regression plot here is going to be
7044000	7049000	in target versus what we actually predict this to be.
7049000	7051000	So the actual is along the x-axis here.
7051000	7053000	This is what the actual production is.
7053000	7057000	And the y-axis is the predicted value.
7057000	7059000	I can actually add that in.
7059000	7060000	I think it should be there.
7060000	7063000	So we say predicted production.
7063000	7065000	So predicted production is on the y-axis
7065000	7067000	and the actual production is along the x-axis.
7067000	7070000	So perfectly calibrated and perfectly predictive thing.
7070000	7074000	Everything would line along this one to one ratio line here.
7074000	7075000	We see that there's some scatter around it,
7075000	7078000	but actually it looks like it's a pretty good overall predictor
7078000	7080000	of the actual production.
7080000	7082000	We'd like to actually see how good is this fit
7082000	7083000	rather than just look at the plot and say,
7083000	7084000	oh, it looks pretty good.
7084000	7088000	So let's import a few of the test metrics that we can actually look at.
7088000	7092000	So we can say we can import explained variance score,
7092000	7095000	the R2 scored, and the mean squared error.
7095000	7097000	So the way these functions work,
7097000	7101000	they always take in the true and then they take in the predicted.
7101000	7103000	So this is going to be test target
7103000	7105000	and then the predicted test target.
7105000	7109000	And actually I think this way of writing it is a little bit too verbose.
7109000	7112000	So I'm going to call it predicted equals this.
7112000	7115000	And I'm going to say predicted here.
7115000	7118000	So the R squared score is 0.88.
7118000	7122000	Explained variance score is 0.88 as well.
7122000	7125000	The mean squared error is 0.54.
7125000	7127000	And now because this is a random forest,
7127000	7130000	we actually have the feature importance of the model.
7130000	7134000	And I don't know of a good way that's naturally given by scikit-learn
7134000	7135000	to actually report this,
7135000	7137000	but here's a little bit of code that I have written to make it
7137000	7140000	so that I can actually read this in a way that I actually think is useful.
7140000	7142000	So I'm going to create a new pandas data frame
7142000	7144000	called rf underscore importances,
7144000	7149000	which actually takes out the features and the importances from the fit model.
7149000	7152000	And I'm going to look at that at the top 20 here.
7152000	7156000	All of the importances of every variable we give it to in total adds up to one.
7156000	7159000	So we can think of this as fractional importance
7159000	7161000	in terms of what the random forest has decided
7161000	7165000	is going to be discriminative in giving us a final regression score.
7165000	7168000	So of utmost importance is the labor hours
7168000	7170000	and then average employees is down from there.
7170000	7173000	The mine type being surface is predictive.
7173000	7175000	The mine county being campel
7175000	7179000	and coal supply region powder river basin is apparently moderately predictive.
7179000	7181000	And then it goes down from there.
7181000	7182000	So this is just the first 20.
7182000	7186000	And we have not only a final fit with a nice plot,
7186000	7188000	we also have some diagnostics and metrics,
7188000	7191000	as well as a list of what's important.
7193000	7194000	In this video,
7194000	7198000	I'll be showing you how to take a development lab notebook
7198000	7201000	and turn it into a deliverable notebook.
7201000	7206000	So let's go into our directory and we go to the develop folder.
7206000	7208000	Clicking that we navigate into that folder
7208000	7210000	and we see we had our first look notebook
7210000	7212000	and then this coal prediction notebook.
7212000	7215000	And what we'd like to do is make a copy of this notebook.
7215000	7219000	So you can select it by clicking this checkbox here
7219000	7221000	and clicking duplicate.
7221000	7224000	When we do that, we have a second copy of this,
7224000	7227000	which is added to the end of the name copy one.
7227000	7231000	Now this file should exist in this directory and we see it here.
7231000	7232000	Copy one.
7232000	7234000	Because it's going to be a deliverable notebook,
7234000	7237000	we should actually move this into the delivery folder.
7237000	7244000	So let's move 2015 coal predict copy into the deliver directory.
7244000	7246000	Go over to the deliver directory
7246000	7250000	and let's navigate there with the notebook server itself.
7250000	7252000	Let's open this up.
7252000	7254000	Okay, so let's first give this a title
7254000	7256000	that we think is an appropriate title.
7256000	7258000	And because it's going to be a deliverable notebook,
7258000	7260000	it shouldn't start with a date.
7260000	7266000	So it should start with something like coal prediction of production.
7266000	7268000	So we have a new name for this.
7268000	7271000	You can save this and I'm going to toggle that header bar.
7271000	7273000	So I have a little bit more space
7273000	7276000	and I'm going to toggle this toolbar as well
7276000	7279000	because I'll mostly be using keyboard shortcuts.
7279000	7283000	So at this stage, we have this long notebook that went through
7283000	7286000	and it's a complete copy of our lab notebook style.
7286000	7289000	So we can delete things here pretty freely
7289000	7292000	and just focus on the main story that you'd like to tell
7292000	7294000	to either your teammates or your manager
7294000	7296000	or whoever is going to be consuming this.
7296000	7299000	So keep in mind with your audience what you think they would like to see
7299000	7301000	and cut out the extraneous stuff
7301000	7303000	and adding in as much text as you think is useful.
7303000	7305000	And in that keyboard shortcuts,
7305000	7308000	especially are going to be make your life a lot easier
7308000	7310000	and make this whole process really fast.
7310000	7312000	All right, so let's just go through this.
7312000	7315000	And initially what I'd like to do is give a good title
7315000	7320000	and you can just call it coal production in mines 2013, let's say.
7320000	7323000	And so we have our first setup here
7323000	7325000	and you can also give a little abstracts.
7325000	7327000	You can say we did a lot of analysis,
7327000	7329000	came to some interesting conclusions.
7329000	7333000	Now, of course, fill that out with more verbiage as you see fit.
7333000	7335000	Keeping the code in this notebook is useful
7335000	7337000	so that someone else looking down the road
7337000	7339000	can actually reproduce all the key results
7339000	7341000	that you think you can find.
7341000	7343000	Now, this isn't always possible, but as far as it is possible,
7343000	7345000	I recommend trying to do it.
7345000	7347000	So try to keep the imports neat and tidy.
7347000	7349000	Keep only the imports that are required
7349000	7351000	and remove the ones that are extraneous.
7351000	7353000	I think we actually use all of these.
7353000	7355000	I would recommend keeping these magic imports
7355000	7357000	on their own line at the top.
7357000	7360000	So having matplotlib inline at the top, that is good.
7360000	7362000	Put a space between that.
7362000	7364000	The Pepe convention is to have one of the standard libraries,
7364000	7366000	like import string, let's say.
7366000	7368000	That would be next and any of the other ones here,
7368000	7371000	and then another blank line before third-party libraries,
7371000	7373000	which is what these are.
7373000	7376000	And finally, we have an actual plotting change
7376000	7379000	that we make with this SNS command here.
7379000	7382000	So we execute that cell and make sure everything is making sense.
7382000	7384000	Yes, we see this warning, we've seen this before,
7384000	7386000	so we're not too worried about it.
7386000	7388000	Now, from here on out, we should be making decisions
7388000	7390000	about whether this actually improves the story
7390000	7393000	for the person reading this or if it becomes just tedious.
7393000	7396000	And when you have data that's being imported
7396000	7398000	and it's changed from the raw data,
7398000	7400000	there's this clean data set here.
7400000	7403000	I think it needs to have some extra commentary around it
7403000	7405000	so that people know what's going on.
7405000	7407000	So I might say...
7410000	7412000	I might give it a description about where exactly it is
7412000	7415000	in this repo, and let's just type an ls here.
7415000	7418000	The name of the notebook is data underscore cleaning.
7418000	7421000	So we will say the same thing.
7421000	7424000	Double click, drag over, command C to copy that,
7424000	7426000	command V to paste.
7426000	7429000	And this ls command, which is handy, we can be deleted.
7429000	7432000	So typing escape to get out of the insert mode
7432000	7435000	so that the cell is now surrounded by a gray box.
7435000	7438000	And then typing D twice, it deletes that cell.
7438000	7441000	And in this cell, we are starting to write some markdown.
7441000	7443000	We can tell it's markdown because it's just a text
7443000	7445000	for people to look at.
7445000	7447000	But also, we've put a double header marking too.
7447000	7450000	So let's just change this cell type to be markdown.
7450000	7452000	So we're currently in a code cell.
7452000	7454000	We can change it to be markdown by typing M.
7454000	7457000	And as soon as you type M, it switches into markdown
7457000	7459000	and gives you a preview of what this will look like
7459000	7460000	when you render it.
7460000	7462000	So let's render it real fast, shift, enter.
7462000	7464000	And we see that this is indeed bolded.
7464000	7467000	This two pound signs, or hash signs,
7467000	7469000	means it's a H2 heading.
7469000	7472000	So this is H1, this is H2,
7472000	7475000	and it keeps getting smaller as you go down.
7475000	7477000	So in this case, I think clean data just deserves
7477000	7479000	a second level heading.
7479000	7481000	We said we clean this data in the notebook stored in this.
7481000	7484000	So deliver slash data cleaning IPYB.
7484000	7486000	So we've told people where this cleaned data file
7486000	7488000	actually sits.
7488000	7491000	And we actually know the exact steps that went through
7491000	7493000	to take it from the raw data into this cleaned data,
7493000	7495000	which we've pointed to here.
7495000	7498000	This head is actually quite a bit of text,
7498000	7500000	even though it should be the top five lines.
7500000	7503000	So if we're going to include something here to make sure
7503000	7505000	that the data is read in correctly,
7505000	7508000	we might select a few columns that we think are useful.
7508000	7513000	So in this case, maybe we have year and maybe mine name.
7513000	7516000	And so we read in just the heading with those two columns.
7516000	7519000	Okay, just to give people a flavor of what's in that data frame.
7519000	7521000	This length we don't need to worry about.
7521000	7523000	This column thing we don't need to worry about.
7523000	7525000	So we delete with 2Ds.
7525000	7527000	Now, consider the different plots that you have included.
7527000	7529000	And is this something that tells a story?
7529000	7531000	If so, leave it in and clean it up so that the axes
7531000	7533000	and the colors all look right.
7533000	7535000	If not, you can go ahead and just delete it.
7535000	7537000	So I think this is deleteable, also deleteable,
7537000	7539000	and finally deleteable.
7539000	7541000	Okay, so we get to the point where we're predicting
7541000	7542000	the production of coal mines.
7542000	7544000	Again, we're just looking at what the columns are.
7544000	7545000	We don't need this.
7545000	7547000	Don't need to know what unique year it is.
7547000	7550000	So this is required code, so we need to leave this in.
7550000	7553000	Again, clean it up if it needs to be broken up into different cells
7553000	7556000	or if you think it needs to be changed in some other way.
7556000	7558000	So let's delete a few of these empty ones.
7558000	7560000	And let's say we want to like to keep this.
7560000	7564000	Let's decide one of these violin plots to keep.
7564000	7566000	So let's keep the second one.
7566000	7568000	So I'm going to delete this one.
7568000	7571000	And to save this, I will say plt.savefig
7571000	7574000	and using tab complete and it'll help us know
7574000	7577000	where proper structure to put this in here.
7577000	7580000	And as I said before, I like to give the same name,
7580000	7583000	beginning of the figure that the notebook itself has.
7583000	7587000	So in this case, it starts with coal prediction
7587000	7589000	as the starting of this notebook name.
7589000	7592000	So that looking at this figures folder separately later on,
7592000	7594000	someone knows which notebook it came from.
7594000	7597000	And then what it's actually being plotted here.
7597000	7600000	So we have company type versus log of production.
7600000	7603000	So company type versus log production.
7603000	7606000	Again, we get a warning, but this should work out just fine.
7606000	7609000	Let's run this a second time to make sure everything.
7609000	7611000	Okay, so that looks better.
7611000	7613000	Running at the second time with the set context
7613000	7617000	actually lets the font sizes get to a nice reasonable size.
7617000	7619000	Okay, so we are saving this output.
7619000	7621000	We think it's useful for our story.
7621000	7624000	We again don't need this or looking at this.
7624000	7626000	So just typing DD to delete these cells.
7626000	7628000	We need to create the dummy categoricals.
7628000	7630000	This is required for our analysis.
7630000	7634000	We don't necessarily need to actually print the categoricals each time.
7634000	7636000	So let's run this comment out that line
7636000	7639000	and just double check that that is the same answer as before.
7639000	7641000	Okay, let's delete that.
7641000	7644000	And we've made a note here about avoiding dummy variable trap.
7644000	7647000	You might decide that that needs to be elevated from a comment
7647000	7649000	and to mark down cell above it.
7649000	7651000	Okay, so let's leave it as a comment here.
7651000	7654000	And we don't need to actually look at the categoricals for the final report.
7654000	7656000	So let's just delete that.
7656000	7657000	Build our model.
7657000	7659000	Let's call it a little bit something more descriptive.
7659000	7662000	So it's going to be a random forest regressor.
7662000	7665000	And we should always put all of the imports
7665000	7667000	all the way at the top of the notebook.
7667000	7670000	And so let's move this to the top.
7670000	7673000	But first let's combine a few of the other imports.
7673000	7675000	I think I have a few more imports down here I do.
7675000	7678000	So let's move this, I'm going to turn on the toolbar
7678000	7682000	and move this up so that it's next to the previous one.
7682000	7687000	I'll scroll back down and see if I can find another import.
7687000	7690000	It looks like it should be everything.
7690000	7693000	A keyboard shortcut that I find that I'm using all the time
7693000	7696000	and really saves me time is knowing how to merge and split cells
7696000	7698000	with keyboard shortcuts.
7698000	7701000	Knowing this will save you tons of time with moving your mouse around.
7701000	7704000	So we currently have input cell 30 selected.
7704000	7707000	We can type shift and hold it down and then type K.
7707000	7709000	We will now select the cell above it.
7709000	7712000	We can select as many cells like this as we'd like
7712000	7715000	or unselected by typing J to go back down.
7715000	7717000	Also, if we go J from here,
7717000	7720000	we can select down from the current cell that's selected.
7720000	7722000	But let's go up shift K.
7722000	7726000	We have selected two cells to merge this type shift M.
7726000	7728000	So we've now merged those two cells together.
7728000	7730000	Again, you can do that for 10, 20 cells,
7730000	7732000	or you can easily split them again.
7732000	7735000	I've said multiple times, control shift minus, splits in part,
7735000	7739000	escape, shift K, shift M, merges them back together again.
7739000	7742000	So this needs to go at the top of the notebook.
7742000	7745000	So I will put this to the top by typing this up arrow.
7745000	7747000	So bear with me for a second.
7747000	7749000	And we need to merge these two cells
7749000	7751000	and then do some recombination.
7751000	7755000	So shift K, shift M, type return to get a cursor in the cell.
7755000	7757000	And we're importing things from sklearn,
7757000	7759000	which should go after C-Born,
7759000	7761000	for this set command.
7761000	7764000	Execute that, and everything looks good again.
7764000	7767000	Let's scroll back down to where we've made our progress.
7767000	7770000	Down to here, we don't need the length of our demi-categoricals.
7770000	7773000	We do need to test train, split our data.
7773000	7775000	Let's merge these two cells by typing shift J,
7775000	7778000	shift M, and let's just leave that middle line.
7778000	7780000	Execute, shift enter.
7780000	7783000	And look at our final plot here.
7783000	7785000	And this looks like a reasonable good plot.
7785000	7786000	Thing looks nice.
7786000	7789000	Let's save it out again into the figures directory.
7789000	7792000	Let's call this coal production RF prediction.
7792000	7795000	Great. So we've now saved this out.
7795000	7798000	And we can do our various scores that we'd like to do.
7798000	7801000	If we're going to be printing out this output for consumption,
7801000	7803000	we should make this look a little bit prettier.
7803000	7806000	So let's just do this first one.
7806000	7810000	And let's combine these two cells.
7810000	7812000	So now we have the R-squared score
7812000	7814000	and our mean-squared error scores.
7814000	7816000	And finally, our random forest importances.
7816000	7818000	And let's just look at the top five.
7818000	7820000	So the top five are labor hours all the way down.
7820000	7821000	Cool.
7821000	7823000	So we've done a lot of rearranging of the code.
7823000	7827000	So at this point, I think it's crucial to restart the kernel
7827000	7829000	and try to run the entire notebook again.
7829000	7832000	If you have some process that actually takes a very long time,
7832000	7833000	you can decide not to do that.
7833000	7835000	But this, you'd have to take a little bit more care
7835000	7838000	into making sure that each piece runs correctly.
7838000	7841000	But in this case, this entire analysis runs very quickly.
7841000	7845000	So we have no problem clearing all outputs and restarting.
7845000	7849000	And clicking cell run all should run every single cell.
7849000	7852000	If we've deleted some piece of code that was necessary,
7852000	7855000	we'll have an error and we have to go back and correct that.
7855000	7859000	Let's go through all the way down to the bottom thing was actually done.
7859000	7863000	If there was an error, so let's say it would stop at this fifth cell here.
7863000	7866000	It would have an error printout here and nothing else would be executed
7866000	7868000	below that when you do this run all cells.
7868000	7871000	That's a good way of identifying where the error happened.
7871000	7873000	We don't have an error, thankfully, so that's good.
7873000	7877000	We do have something that is somewhat annoying to me that this has to be run twice.
7877000	7881000	As we can tell, we run this a second time we get our font gets bigger.
7881000	7883000	So I think I know what happened.
7883000	7886000	I set the context after I set the figure.
7886000	7889000	So I'm going to re-align the order of these two pieces of code.
7889000	7895000	Save this, restart the kernel, clear all outputs, cell run all.
7895000	7898000	And now we see that the font size is the correct size
7898000	7900000	and we've run all the way to the bottom.
7900000	7903000	And each time we run this, do note that we are overwriting these figure files,
7903000	7905000	which is what we were hoping to do,
7905000	7909000	but also keep track that is what you indeed want to do when you're running this.
7909000	7912000	I guess a good thing to add at the end, of course, would be some sort of conclusion,
7912000	7916000	so we can just add a conclusion statement.
7916000	7919000	Okay, so a detailed and amazing conclusion goes there.
7919000	7920000	So we're done with this.
7920000	7924000	We will close and halt and we need to submit this to GitHub.
7924000	7927000	So we can do a get status back at our terminal.
7927000	7929000	We've modified a figure.
7929000	7932000	We have added a figure and we've created a new file.
7932000	7939000	So let's add those type get status to make sure we know what we're doing.
7939000	7941000	We are adding two new files or modifying another file.
7941000	7942000	This looks good.
7942000	7950000	So get commit, get push, origin, Jonathan prediction production.
7950000	7953000	And this should be sent up to GitHub and everything is now
7953000	7954000	up to date.
7954000	7956000	So let's go to our get repository.
7956000	7959000	So in my case, JBWit Coal Exploration.
7959000	7962000	And there is a new branch which we can click on.
7962000	7968000	And if we click on the deliver, we should be able to see our Coal Prediction Production Notebook,
7968000	7974000	including all of the code and everything else in here.
7974000	7981000	In this video, we'll be talking about how to do a pull request and how to merge this back into a final branch
7981000	7984000	so the team members can review it and check off on it.
7984000	7985000	All right.
7985000	7987000	So we last left us.
7987000	7992000	We had just put in our deliverable notebook that talks about the Coal Prediction Production.
7992000	7997000	And so at this point, after pushing it to master, we have this branch.
7997000	8003000	If you go back to the home directory under your username and whatever you've named your data science project,
8003000	8006000	you can actually see this button here called new pull request.
8006000	8011000	And I like to switch to the branch that I'm going to generate the pull request from.
8011000	8016000	So this is all predicated on using GitHub as your repository of choice.
8016000	8023000	So after you click new pull request, you'll ask you to do one last step here where it'll say you're creating a new pull request
8023000	8025000	by comparing changes across two branches.
8025000	8031000	You're going to be taking stuff from this Jonathan Predict Production branch and putting it into master.
8031000	8034000	And GitHub does this nice thing which says it's able to be merged,
8034000	8038000	which means that if it's approved, it can just be approved at the single button click.
8038000	8039000	That's always nice.
8039000	8043000	So give your commit an extra bit of detail here.
8043000	8046000	So say something like final review.
8046000	8051000	And then if you want to leave a few more comments, create pull request.
8051000	8058000	So now what this does is it creates a pull request and lets you see the various commits that have happened in this branch
8058000	8063000	and allows a person who can possibly merge this to review the pull request.
8063000	8071000	So a person coming into this would see who's not me, for example, would look at the pull request and see that there's one open pull request.
8071000	8074000	And it was open 25 seconds ago by me.
8074000	8077000	So if you click on this, then you'll see the comment here.
8077000	8078000	Please check the figures especially.
8078000	8080000	They're going to be put into a slideshow.
8080000	8081000	Okay, so this must be pretty important.
8081000	8085000	And so I'll take a look at the different files that were committed.
8085000	8087000	So I'll click to the files changed.
8087000	8092000	I see that we have a couple of notebooks and we have a couple of figures.
8092000	8096000	So let's take a look at, let's say this current figure here.
8096000	8097000	This one was added.
8097000	8100000	Let's say we want to change that color.
8100000	8103000	So in the pull request, you can actually make changes.
8103000	8105000	And this is where you actually wanted to be doing this.
8105000	8113000	You click in the conversation part of the pull request, say, I need a few changes.
8113000	8114000	Add a comment.
8114000	8117000	Now, of course, I'm commenting on my own pull request.
8117000	8121000	Normally what happens is you make a pull request and your team members or your
8121000	8124000	manager will be actually the one reviewing the pull request.
8124000	8128000	But in this case, just for demonstration purposes, I'm both the submitter and the
8128000	8131000	reviewer just so that it's easy to see what needs to happen.
8131000	8132000	So added a comment.
8132000	8133000	I need a few changes.
8133000	8135000	Please change the figure to be green.
8135000	8136000	Okay.
8136000	8141000	Now that we go back to our terminal, we see that still on the Jonathan prediction
8141000	8146000	production branch, so we'll need to make some changes to the pull request.
8146000	8148000	So this is actually pretty simple.
8148000	8153000	So I'm going to switch tabs back to our deliver directory that is running under the
8153000	8155000	Jupyter Notebook server.
8155000	8159000	And so let's go into this cold predict production and make the requisite changes.
8159000	8163000	Now we'll have to actually shift return and work our way through this so that
8163000	8165000	everything is loaded into the namespace.
8165000	8169000	So that one is probably the one that should stay the same.
8169000	8174000	We get down to this one here where sure enough, the figure itself is printing
8175000	8176000	something that's blue.
8176000	8179000	We want to change this color to be green.
8179000	8180000	Okay.
8180000	8184000	In this plot, we will actually make the color equal to green.
8184000	8187000	C is not what it takes as a thing.
8187000	8190000	So we'll see if color works and color is indeed the keyword.
8190000	8191000	Okay.
8191000	8195000	So changing the color to be green, the figure is now green and we have
8195000	8197000	overwritten that figure file.
8197000	8202000	So cold production RF prediction is now a green plot rather than blue.
8202000	8207000	And so we can want to redo everything just to make sure that you haven't made
8207000	8208000	any catastrophic changes.
8208000	8210000	You can do this one more time.
8210000	8217000	Takes just a few seconds to go through the entire pipeline and save this file
8217000	8218000	close and halt.
8218000	8221000	Go back to your terminal, get status.
8221000	8222000	Two things have been changed.
8222000	8223000	And that's as we expect.
8223000	8227000	They changed the notebook itself that created this figure and the figure
8227000	8228000	itself.
8228000	8230000	So let's add those two files.
8230000	8234000	Those two files have been modified.
8234000	8240000	So we then get push origin, your branch name, and it's now updated on GitHub.
8240000	8244000	The nice thing about how GitHub handles these pull requests as a tab back to this
8244000	8247000	Chrome tab, this commit is already added now.
8247000	8249000	You actually can see the commit that was done here.
8249000	8252000	And if you click on that commit, you get to see that things that were changed.
8252000	8256000	So the few things were changed in the IPYNB, which is not shown partly because
8256000	8259000	the actual changes in the notebook don't look so great.
8259000	8263000	But the change in the figures has been changed.
8263000	8265000	So this figure, the blue one was deleted.
8265000	8268000	And the one on the right, the green one was added.
8268000	8271000	So this is one of the reasons that changing it in the notebook, which it
8271000	8272000	actually did.
8272000	8274000	So it changed the embedded figure in the notebook.
8274000	8276000	It's hard to see the differences there.
8276000	8280000	This is why I advocate creating these figures in a separate folder and a separate
8280000	8282000	PNG file for each of them.
8282000	8286000	So you see the diffs in the figures if you have feedback on the output.
8286000	8290000	Now, as an extra piece of sugar or something nice that GitHub has given us,
8290000	8294000	there's this to side by side approach where you can see what was deleted and
8294000	8295000	see what was added.
8295000	8300000	You can also choose the swipe option where as you swipe this thing across the
8300000	8303000	figure that you've just done, you can actually see the changes that have been
8303000	8305000	made, which is turning the figure green.
8305000	8309000	Last one is onion skin where it fades from the entire thing from behind.
8309000	8311000	So this is what it currently is.
8311000	8313000	And previously it was blue.
8313000	8314000	You can see this.
8314000	8317000	So having this functionality is actually really nice.
8317000	8323000	And another reason why I advocate for this figures being submitted separately.
8323000	8327000	Just a final note, you saw that the points are slightly different in this swipe.
8327000	8331000	And that's because during our test train split, we were taking a random selection
8331000	8335000	of points that were going to be the testing set and the training set.
8335000	8339000	So those differences, well, shouldn't matter much and they don't change the
8339000	8342000	actual scatter points, but the fit itself, as you can tell is almost completely
8342000	8343000	unchanged.
8343000	8346000	It's actually a nice robustness check to look at this as well.
8346000	8352000	So once I've looked at these changes, I can now go back to this pull request
8352000	8353000	branch.
8353000	8354000	So I need a few changes.
8354000	8355000	Please make the figure green.
8355000	8357000	I committed made the figure green.
8357000	8362000	The only thing I need to do to update this whole threat of changes was to just
8362000	8365000	say get push origin branch title.
8365000	8369000	So I'll say it looks good to me plus one.
8370000	8374000	And then clicking merge pull request will take everything from this branch
8374000	8376000	and pull it into the master branch.
8376000	8381000	So I'll say figures ready to be put into a slideshow.
8381000	8386000	So once you pull request is successfully merged and accepted, then you should
8386000	8389000	delete the branch to keep these branches from floating around.
8389000	8393000	So I just deleted the branch on GitHub and should now do the same thing
8393000	8395000	in your local environment.
8395000	8399000	So first I'm going to check out master and I'll say get pull origin master
8399000	8403000	to pull everything down from GitHub and all these changes have been made
8403000	8407000	and say get branch minus D Jonathan predict.
8407000	8411000	So I've deleted the prediction branch and get does a final check to make sure
8411000	8414000	that any of the changes that have been made on that prediction branch have
8414000	8415000	been already pulled into master.
8415000	8418000	So if you just try to do this and it doesn't think it's been fully merged,
8418000	8421000	you get an error at that point and you have to figure out what happened at
8421000	8422000	that stage.
8422000	8426000	In this video, we just over reviewed the basic process of going through a
8426000	8430000	pull request and how the peer review process works in a pull request.
8430000	8438000	So we saw how to merge our development branch into master after doing a pull request.
8438000	8443000	In this video, we'll start our data science project number two.
8443000	8447000	And in this project, our main focus will be to focus on various plotting
8447000	8450000	and statistical libraries that I think you should know about.
8450000	8455000	All right, so to start a new data science project, let's start out by
8455000	8461000	going to GitHub and signing in going up to the plus by our little icon and
8461000	8463000	clicking on a new repository.
8463000	8471000	So we can call this data vis project to in this case and give it a
8471000	8475000	description that says I will make it public.
8475000	8478000	So you can see this project as you go forward.
8478000	8480000	We'll initialize with a read me.
8480000	8483000	We will include a Python dot get ignore.
8483000	8488000	We'll add an MIT license and create the repository.
8488000	8493000	Once we've created it, go to this SSH option, click in this box.
8493000	8497000	It'll select all the text by default command C copies it, go back to our
8497000	8502000	terminal, say get clone and then command V to paste that URL.
8502000	8507000	All right, so let's CD into data vis projects and look at what we have here.
8507000	8509000	And we're currently on the master branch.
8509000	8514000	So first step, let's create a development branch and we'll call it
8514000	8519000	Jonathan vis and let's create our normal directory structure.
8519000	8525000	So we have data deliver develop figures start with and I happen to
8525000	8529000	know that I've already started a few of these notebooks.
8529000	8534000	So I'll move them from a previous location into our develop folder.
8534000	8536000	So let's look at our develop folder.
8536000	8537000	Okay, we got some stuff there.
8537000	8541000	And now that we have a new branch and we have a new directory structure
8541000	8542000	and some stuff to look at.
8542000	8545000	Let's start up the Jupyter notebook server.
8545000	8548000	All right, so we see the same directories we were just looking at in the
8548000	8549000	terminal.
8549000	8553000	I will now right click on this tab and pin this tab so that it goes all the
8553000	8556000	way to the left and stays in place so that if I have a lot of tabs because
8556000	8559000	I'm searching for a bunch of different things, I always know where to go
8559000	8561000	back to find the home server directory.
8561000	8564000	And I just find that useful to pin that tab all the way to the left.
8564000	8567000	All right, so let's take a look at some of the notebooks.
8567000	8573000	I've already pre populated what I'll do here is I only have my usual date
8573000	8577000	and then my initials at the top of the page from the actual name of my
8577000	8580000	notebook, just including your short description, which is exploratory
8580000	8582000	data analysis, which is pretty long title.
8582000	8586000	So I'll do all caps EDA and that is a standard way of talking about that.
8586000	8591000	So I'll view and toggle the header and toggle the toolbar just so that we
8591000	8592000	have some extra space.
8592000	8595000	Remember, if you want to save it when you're in this kind of configuration,
8595000	8597000	you just command S to save it.
8597000	8600000	So one more time, I'll just give you a brief overview of what I'm hoping to
8600000	8601000	do here.
8601000	8603000	So this isn't to teach you how to do data science.
8603000	8607000	It's more of an exposure to the tools that I think most people haven't seen
8607000	8609000	all of them or haven't seen enough of them.
8609000	8614000	And I just think these tools will allow you to do your data science much
8614000	8617000	more efficiently and usefully.
8617000	8620000	I'll go over a few of these plotting and statistical packages that you might
8620000	8621000	not know about.
8621000	8624000	So the first thing we have is importing map plot lib inline.
8624000	8627000	Almost all these plotting libraries uses map plot lib.
8627000	8628000	So I'll be using that for now.
8628000	8632000	And I'm importing map plot lib dot pie plot as PLT, which is the
8632000	8633000	standard way of doing that.
8633000	8637000	Seaborn as SNS, which is the standard way of importing Seaborn,
8637000	8640000	importing pandas as PD, NumPy as NP.
8640000	8644000	I'll also load in some data sets from scikit-learn and importing some
8644000	8648000	stats models, which I'll be talking about at length in a later video.
8648000	8649000	Execute this cell.
8649000	8652000	Now, if I do shift return, it will execute it and go to the next cell.
8652000	8656000	If I hold down control and hit return, it will execute the cell in place,
8656000	8658000	and it won't go to the next cell.
8658000	8662000	So I can continue to stay in the same cell if I hit control and return.
8662000	8666000	I've used Seaborn in other videos, but I would really like to just double
8666000	8668000	emphasize how useful this is.
8668000	8672000	You can find the main library for this by Google searching Seaborn,
8672000	8675000	and Seaborn Python should do it.
8675000	8679000	And the top result is the statistical data visualization library here.
8679000	8682000	This is what you should see, something like this, unless he's updated the page.
8682000	8685000	And this website has a lot of really good information on it.
8685000	8686000	The documentation is excellent.
8686000	8689000	The features with these different tutorials is also excellent.
8689000	8693000	These images that you can click on here will show you different capability,
8693000	8695000	the tutorial and the gallery.
8695000	8698000	If you click on gallery, you get to see many different visualization types
8698000	8701000	that Seaborn makes really easy, especially like heat map.
8701000	8703000	That's a nice one.
8703000	8705000	Look through the example gallery.
8705000	8708000	If you have some data and you have some sense that you should be able to visualize it in a way,
8708000	8711000	see if Seaborn has a response to that.
8711000	8714000	So let's go back to our notebook and load in some data.
8714000	8718000	So Seaborn SNS has data sets that you can load in by default.
8718000	8721000	We will load in the Titanic data set.
8721000	8725000	This is actually the data of passengers on the ill-fated Titanic.
8725000	8731000	And it has various information about them, their age, their sex, their class of ticket.
8731000	8733000	So first class, second class, third class.
8733000	8736000	And it talks about whether or not they survived the crash.
8736000	8742000	So doing a factor plot like this where you set this G object to be equal to this factor plot
8742000	8745000	and then modify the G label like this.
8745000	8750000	This is modified from a Seaborn example, commenting out this hue equals sex line.
8750000	8752000	And I'll talk about that in a second.
8752000	8755000	But I will shift return and execute this cell.
8755000	8762000	What you see here is the survival probability against the class of passengers on Titanic held.
8762000	8765000	You can see that first class had by far the best survival probability,
8765000	8768000	followed by second, followed finally by third class.
8768000	8773000	So this is a very nice high level summary of the data that underlies this.
8773000	8779000	Some of the nice things about Seaborn is that you can actually give it dimensions to also give you the same plot.
8779000	8782000	So let's uncomment this hue equals sex line and see what that does.
8782000	8786000	So what you see here is each of these classes is now been split out by sex.
8786000	8789000	So male and female, survivability for first class.
8789000	8793000	You can tell the very high difference in probability for surviving in each of those,
8793000	8796000	whether you're male or female in each of the classes.
8796000	8801000	So this tells you a more rich and deeper story of the underlying data set than the previous plot.
8801000	8805000	And you can see the first, second, third class, all of the different responses here.
8805000	8808000	So this is just one aspect of Seaborn.
8808000	8811000	I recommend getting to know it and use it as much as you can.
8811000	8814000	And that's going to be all for this video.
8814000	8817000	We've set up in this video a new Git repository.
8817000	8819000	We've started a new development branch.
8819000	8823000	We have our directory structure set up as we like to do it for our data science projects.
8823000	8829000	And we've taken a look at the Seaborn visualization library.
8829000	8834000	In this video, we'll continue to look at some visualization methods and techniques.
8834000	8837000	So let's go on to exploratory data analysis two.
8837000	8844000	Again, it starts off the same way with Matplotlib inline and the various other things being imported.
8844000	8846000	This warning message, which we can ignore for now.
8846000	8850000	So we will load in this Boston data from the scikit-learn data sets.
8850000	8855000	And we will first of all print what the data dictionary describes it as.
8855000	8860000	The way this load Boston gets imported, I'm calling it a data frame dictionary
8860000	8863000	and just calling this description key.
8863000	8869000	So let's toggle the top header and the top toolbar to give us some extra space.
8869000	8873000	And we see that this is the Boston house prices data set.
8873000	8878000	Now, it's worth reading through this data set and knowing what each of these attributes actually means
8878000	8882000	because if we're doing a deep data science project, it's really important to know the attributes,
8882000	8884000	especially if there's only 13 of them.
8884000	8889000	But what the main takeaway will be trying to predict the median value of the house
8889000	8894000	and by looking at the 13 categories that predict this house,
8894000	8897000	we have 506 total instances of this data set.
8897000	8902000	The different attributes are crime, we've written as CRIM, all caps.
8902000	8907000	Zone or the proportion of residential land zone for lots over 25,000 square feet.
8907000	8912000	Indus, which is a proportion of non-retail business acres per town.
8912000	8918000	A dummy variable where if you're next to the Charles River, then you're equaling to one, otherwise you're zero.
8918000	8921000	The nitric oxides concentration in parts per 10 million.
8921000	8923000	The average number of rooms per dwelling.
8923000	8928000	The proportion of owner occupied units built prior to 1940, which is age,
8928000	8933000	weighted distances to five Boston employment centers, distance.
8933000	8936000	Rad is index of accessibility to radial highways.
8936000	8941000	Tax, the full value property tax per $10,000.
8941000	8944000	People to teacher ratio by town.
8944000	8949000	The B, which is the formula that says the BK is the proportion of blacks by town.
8949000	8952000	L stat, which is percentage of lower status of the population.
8952000	8956000	And median value, the thing we are tending to be predicting,
8956000	8960000	which is median value of the owner occupied home in terms of 1000s.
8960000	8962000	This is the information that the data comes from.
8962000	8964000	So it's from Harrison and Rubenfeld.
8964000	8971000	And this is all the information about exactly where it was taken from the stat lab library maintained at Carnegie Mellon University.
8971000	8976000	So this data dictionary as it comes from scikit-learn is not in my favorite format.
8976000	8978000	It's this weird data dictionary.
8978000	8981000	If we actually say type on this, it'll be this weird like data set bunch.
8981000	8984000	So instead of using it in the form that it's given to us,
8984000	8989000	I like to convert this into a panda's data frame because those in my view are much easier to use.
8989000	8992000	So we'll create a data frame called features.
8992000	8994000	I'll create a data frame called target.
8994000	8999000	Now features will take the DF underscore dict, which is the the scikit-learn bunch thing.
8999000	9005000	And the dot data element and assign the columns to this data frame to be the feature names.
9005000	9007000	We'll also do this with target.
9007000	9011000	So we'll do this with another create another pandas dot data frame to create the data frame.
9011000	9014000	And then it'll be this DF dict dot target.
9014000	9018000	So run this and we can look at the head of the features by doing dot head on it.
9018000	9024000	So here are the different values of the different features for the first five elements of our data set.
9024000	9028000	We have the crime number here, zone, the industry.
9028000	9033000	Are you close to the Charles River, the nitrous oxide, average number of rooms, the age,
9033000	9036000	all the different features that we're reading about before.
9036000	9042000	If we look at the target, we would see that it's a single element or a single column data frame.
9042000	9045000	So what we'll like to do is actually for most of our visualization,
9045000	9048000	we will like to put these two things together side by side.
9048000	9051000	Well, we can use concat for that pandas dot concat.
9051000	9054000	We give it a list of the data frames you'd like to concatenate together.
9054000	9057000	And we have to tell it which axis that we would like to use.
9057000	9062000	Now, I'm sure there's some very useful mnemonic that will tell us the right way to do it every time,
9062000	9067000	but I prefer to not trust that I remembered it correctly, but always test that I have it right.
9067000	9070000	So if we start out with axis equals zero and look at the head,
9070000	9075000	we will see that it's trying to combine it in a way that they're stacked on top of each other.
9075000	9076000	And there's two ways to know this.
9076000	9081000	One is that everything has a value except for medv, which is the target data frame.
9081000	9082000	All of them have nans.
9082000	9089000	And if we were to look at the tail, we will see that everything else has nans and medv has values.
9089000	9091000	That's one way to know that we've done it wrong.
9091000	9095000	So this is trying to do some sort of concatenating the two data frames vertically.
9095000	9100000	And if we do it axis equals one, we will see that we've put them side by side,
9100000	9101000	which is what we actually want.
9101000	9102000	And let's look at the head.
9102000	9107000	We will see that all of them are here, including medv being the very final column in this data frame.
9107000	9109000	So we now have a new data frame called df.
9109000	9113000	It contains a target and the feature variables underneath it.
9113000	9118000	Now to give you a sense of the data underneath it, there's many different ways you can slice and dice this.
9118000	9123000	One very simple quick way to start with is to iterate over all of the columns of the data frame
9123000	9128000	and to print both the column name and the number of unique values in that column.
9128000	9136000	For column in df, the data frame columns, print the column name and df of the column, the number of unique values.
9136000	9140000	This n unique is a method you can call on a data frame.
9140000	9147000	So there are 504 unique values in crime and there's two totally unique values in chance, which is a boolean value.
9147000	9149000	Makes sense, we'd expect that.
9149000	9150000	Some of them are pretty low.
9150000	9152000	So our ad, for example, is at nine.
9152000	9155000	Some of these have many values and they're continuous values.
9155000	9158000	Other of them have smaller numbers of possible values.
9158000	9161000	You can see rad here is this kind of numbers here.
9161000	9168000	One thing you might not know is that pandas not only has fantastic data frame support, but also has some very useful plotting tools.
9168000	9174000	So in this case, we will be importing a thing called scatter matrix from pandas.
9174000	9178000	And this can be done in a couple of libraries as well, but let's just look at the pandas version of this.
9178000	9184000	Recreating a figure with some plots in pi plot, making a large figure 12 by 12 fig size.
9184000	9195000	And we're going to call it on this data frame with some see-through value of alpha and the diagonal will be KDE, which is this kernel density estimation plot that we see here.
9195000	9202000	Again, we see a warning that we can safely ignore, but this is a very information dense plot.
9202000	9205000	There's no way to go over all of it in this video as we look at it.
9205000	9209000	But this, if you have your own data set, will give you a lot of things to look at.
9209000	9217000	What is being plotted here on the x-axis and the y-axis is every possible pair of the two columns in this data frame, which is why it took a while to actually plot this.
9217000	9225000	Along the diagonal, this KDE plot, it's showing interactions with itself or basically the histogram of that variable itself.
9225000	9229000	So this is what medv looks like. It's just this histogram here.
9229000	9233000	Along the diagonal, it's just a histogram of the values of that variable.
9233000	9241000	Everything else is going to be what the response from this variable looks like with every other variable on the x-axis.
9241000	9243000	So you can see a number of really nice trends here.
9243000	9245000	You can see some kind of this U-shaped trend here.
9245000	9249000	We see something that's basically a straight line, which means there's not much information there at all.
9249000	9251000	That's from the Boolean value.
9251000	9257000	We can see some of these have very fuzzy relationships where it's not really showing anything very interesting.
9257000	9263000	But spending some time looking at plots like this, getting to know your data set is a vital part of data science.
9263000	9265000	And I highly recommend looking at this.
9265000	9269000	If you have far too many columns to look at it in one, I would say this is probably too many.
9269000	9274000	If you have even more than this, though, you can take subsets of this and plot this with the same command,
9274000	9281000	but you would be giving it a list inside of double brackets of feature one, feature two, and so on.
9281000	9283000	And this will plot just those features against each other.
9283000	9286000	So there's a downside of that is that you're not getting all of the interaction terms,
9286000	9293000	but if it's a trade-off between possible to view in one screen or not look at it at all, I recommend that.
9295000	9299000	Okay, in the last video, we last looked at this scatterplot functionality within Pandas.
9299000	9307000	In this video, we're going to continue taking a look at this data and some of the plotting functionality that's built into the Pandas library itself.
9307000	9316000	Just as a brief overview, again, this scatterplot gives you a very nice, fast way of looking at all of the interactions between the terms in your data frame.
9316000	9322000	If you suspect that there might be something interesting going on with, let's say, rad, we see something happening here,
9322000	9326000	or this diagonal term for rad, the intersection of rad and rad on the X and Y axis.
9326000	9332000	You see a histogram plot or a KDE plot that shows a very bimodal distribution.
9332000	9340000	So you can take a deeper look into that and see what it looks like by selecting that column by saying df of rad.hist.
9340000	9344000	And we will see this bimodal shape really appear again.
9344000	9349000	So it's really values that are greater than 20 and then a bunch of different values that are around 10 and lower.
9349000	9353000	You can also, of course, select it if you have a nice column name.
9353000	9356000	In other words, there's no spaces or any other characters in that column name.
9356000	9360000	You do the same exact thing by doing df.rad.hist.
9360000	9362000	See the same exact plot.
9362000	9367000	When you see a feature like this, in this case, it might not make sense, but if you have the thought that,
9367000	9370000	you know what, let's actually consider this as two separate groups.
9370000	9375000	This bimodal characteristic should actually be characterized as really a high group and a low group.
9375000	9381000	One way to do this is to apply a lambda function, which will create a Boolean value of these values.
9381000	9385000	So everything down here gets one flag of the low group and everything up here gets the high group.
9385000	9391000	And so we will build up this command below by getting some intuition here.
9391000	9394000	So let's grab our data frame like this.
9394000	9399000	This apply function is a method that goes to the column that you've selected in your data frame.
9399000	9402000	And there's a number of ways you can actually call this apply.
9402000	9405000	You give it a function and the default axis is zero.
9405000	9408000	You can do it in various other ways, so you can access equals one.
9408000	9413000	But in this case, most of the time you'll end up doing a lambda function, which is an anonymous function.
9413000	9418000	It's like you define a function in Python, but you don't give it a name.
9418000	9423000	You're giving it via this apply method every value in the RAD column.
9423000	9428000	And you're saying for each of those values in that column, is it greater than say 15?
9428000	9432000	So 15 is clearly going to split us into the low and high group.
9432000	9434000	And let's just take a look at the first few values of that.
9434000	9440000	So I did not head on that to give us the first values and we see that is this X value greater than 15?
9440000	9442000	It was false, false, false, false, false.
9442000	9449000	And if we want to look at just what that head value looks like without the Boolean, we see that it's 1, 1, 2, 2, 3, 3.
9449000	9452000	So everything here is indeed less than 15.
9452000	9459000	So we have this function call, which will return a Boolean series false.
9459000	9463000	And what we'd like to do is say we want a new column in this data frame.
9463000	9469000	We're not going to overwrite this column, but we're going to give a new data frame that we're going to call radian underscore bool,
9469000	9472000	because we want to have a nice descriptive name of where it came from.
9472000	9479000	And the way you create a new column in a pandas data frame is you give a column that doesn't quite exist yet or doesn't exist yet in the data frame
9479000	9482000	and assign it equaling to something else.
9482000	9487000	So in this case, we have this rad dot apply lambda greater than or equal to 15.
9487000	9489000	And I'm just adding this as type bool.
9489000	9496000	Just to give you a sense that if it doesn't automatically get incurred into a type of bool, which we see right here, the d type is boolean.
9496000	9499000	You can force it by doing this as type.
9499000	9501000	There's other times when this is useful as well.
9501000	9508000	So I'll just leave it in here as kind of a best practices or a hint for future ways if you're trying to do something similar and having some problems with it.
9508000	9513000	So we've just created a new column in the data frame of rad underscore bool.
9513000	9518000	And if we look at what the type of this single value is this I location of zero is a boolean.
9518000	9522000	Let's take a look at the histogram on that now that we've created this new column.
9522000	9525000	And we see this perfect bimolality of 0 and 1.
9525000	9528000	That's one way if you have different features that you want to create.
9528000	9532000	It's very flexible to say if it's greater than 15, give it as bool.
9532000	9537000	You can also do something if you had a trimol tool or so three different groups or various other ways of slicing this.
9537000	9540000	Any function you can think of that can be written down in Python.
9540000	9548000	You can then use to filter out the columns and I recommend creating new columns, but sometimes you can overwrite columns if that makes more sense.
9548000	9557000	So after doing this we have another seaborne plot that's called a pair plot and let's execute this and then explain what's happening here.
9557000	9566000	This is very similar to what's happening above in the scatter plot where we're having the same x value versus y value and the where they intersect.
9566000	9571000	So this medium value here is the same intersection of medium value on the x and y axis.
9571000	9579000	Instead of a KDE or a kernel density estimation, which is that line, we did it with a histogram and that is a flag given right here.
9579000	9584000	I guess it's just the default value. It's under dyag kind equals hist.
9584000	9589000	And if we did KDE it would give us the KDE plot as before.
9589000	9594000	There's one difference here though where we've given an extra character of hue.
9594000	9602000	So there is a Boolean value which is are you near the Charles River and this is similar to splitting the Titanic data set into male and female for each class.
9602000	9608000	So it says give us the pairwise interactions between these variables. I just picked four of them.
9608000	9616000	And for each of these though I would like to see the differences whether you're close to this river split up by a different color.
9616000	9625000	So we have this hue value can take a zero or a one and we see if there's possibly different distributions behavior conditioned on whether it's actually close to the river.
9625000	9640000	So this gives you an extra dimension of interaction and interpretability so you can see like oh I see that there's a behavior but it only exists if there's let's say the green dots had a nice tight relationship here and the blue dots were all kind of all vague and all over the place.
9640000	9650000	And so if you looked at this without splitting by this Boolean value you might say oh there's not much of a relationship here but turns out that this underlying feature could have been the really important thing.
9650000	9657000	Now I don't actually see anything that jumps out at me in this case but having this availability is something that's worth noting.
9657000	9662000	So we'll do sms.kde plot and it'll be df.nox.
9662000	9669000	So we're seeing here it's not a histogram it's a kernel density estimation of the distribution of this underlying feature here.
9669000	9673000	So this is like a histogram but it's more of a smoothed out version of that.
9673000	9678000	This is what happens if you give this kde plot method in seaborne single column of values.
9678000	9686000	If you gave it two values let's give kde plot the Boolean value of rad versus the Knox value which we just plotted above.
9686000	9692000	And we'll get a two dimensional plot which shows the distribution of these two values together.
9692000	9700000	So radian is a Boolean value when split it's in the x and y and you can see that if Boolean is true then the Knox values are actually conditioned higher.
9700000	9705000	If it's zero then it's conditioned lower with a little bit of data points up here in the upper one.
9705000	9712000	So giving two dimensions to a kde plot you get this 2d map which shows you some contour plots some really nice things.
9712000	9717000	One final thing for the pandas plotting thing is a thing called Andrew's curves.
9717000	9724000	Now I haven't used them much myself but in Wikipedia it has this as their answer of what Andrew's plots are.
9724000	9727000	It's a way apparently to visualize structured high dimensional data.
9727000	9734000	They show it with the iris data set and the iris data set is the ubiquitous data set from Fisher way back in the day.
9735000	9740000	And if we import this we can take a look at a specific value of a data frame.
9740000	9748000	So let's look at this whether this Boolean value has much structure to it and it doesn't look like it but perhaps this Knox value does.
9748000	9754000	And looks like there's too much to that one. Let's go with rad which is only a nine values for that.
9754000	9757000	So you can try to see if there's clustering of behavior.
9757000	9763000	Now the actual numbers here I think aren't so easy to read but the fact that this should give you a sense of
9763000	9768000	if there's different behaviors going on. Too many data points overlying each other I will do a sample like this
9768000	9774000	and a sample is another built-in function of data frames where you can say give me only a hundred values and then do the same exact plot.
9774000	9781000	And it'll pick out randomly a hundred values from this data frame and then you're doing the same kind of estimation here.
9781000	9786000	And so at this point you might say hey this value of 24 for the data framework is rad.
9786000	9791000	That looks like it's having fundamentally different behavior than the other values which seem to be clustered together.
9791000	9797000	I don't know that this actually tells us much in this case but it's another piece of functionality that I think is worth knowing about.
9797000	9802000	I'll go through the last few bits here and just talk about them really quickly.
9802000	9809000	So here's another KDE plot of this median value for the houses which is what we've seen before but this is going to be the target
9809000	9813000	like how much the price of the house is actually going to be sold for.
9813000	9818000	And we can add to that by saying we want to also see what's called this rug being true.
9818000	9825000	So instead of doing a KDE plot we can give it a distribution plot and add the fringe kind of rug thing at the bottom
9825000	9829000	which adds the actual density of points at these different values.
9829000	9833000	So you can see that it is actually very dense here as we go across these values.
9833000	9838000	So sometimes if you've chosen a kernel that's too wide or too narrow for your underlying dataset
9838000	9843000	seeing the rug along the bottom here gives you extra clues into what's going on.
9843000	9849000	And one last look here at two variables that might actually be more useful for looking at relationships.
9849000	9857000	The median value versus the L stat and you can see that there's this kind of banana shaped curve here going on in the relationships.
9857000	9860000	Okay so that's going to be it for this video.
9860000	9864000	What we did in this video is we showed a number of different visualization techniques.
9864000	9871000	We took a value that had a clear bimodality of a low and a high group and created a new data frame column to encode that.
9871000	9881000	Went through also and saw various methods of doing kernel density estimations, scatter plots and various other features.
9881000	9884000	In this video we'll be talking about stats models.
9884000	9892000	Stats models is a library that you can use that allows for a lot of statistical machinery that can help you with your data science work.
9892000	9899000	So we'll continue with the same Boston housing dataset as before which we were just looking at in the last video.
9899000	9903000	And take a look at some of these Boston housing prices.
9903000	9907000	Let me toggle this header in this toolbar real fast.
9907000	9910000	Make this full screen so we have a little extra room to look at.
9910000	9917000	So we will load in the scikit-learn dataset load Boston which again has the same attributes as we saw in the previous videos.
9917000	9926000	We will construct our pandas data frame from this scikit-learn dataset so that we can use the standard tools we've learned over the years.
9926000	9934000	We've combined the features and the target into one data frame and here's that scatter matrix plot we made in the previous video as well.
9934000	9936000	This is a pandas call.
9936000	9941000	So with this function call we get all of the pairwise interaction terms for this dataset.
9941000	9949000	And from this we see a number of features that look like they have some strong trends with the thing we would try to predict which is the median value of the house.
9949000	9951000	There's a trend here with this RM.
9951000	9956000	It looks like there's this kind of banana shaped L-stat curve that we talked about at the end of the previous video.
9956000	9963000	So we have a few things that we think you might be able to combine into some sort of model that will predict our median value.
9963000	9967000	Again, let's look at the columns and the number of unique values for each of these.
9967000	9969000	In particular, Rad has nine values.
9969000	9970000	We previously made that a Boolean.
9970000	9973000	Let's actually take a look at what the values comprise it.
9973000	9977000	So there are nine values and these are the values.
9977000	9980000	And then 24 is obviously the outlier here.
9980000	9983000	And we previously made a Boolean variable, which we can do again right now.
9983000	9995000	So we'll split everything from less than 15, which means everything up to here, 1, 2, 3, 4, 5, 6, 7, 8 will be labeled as 0 and 24 will be labeled as 1.
9995000	10002000	Let's look at the target variable, which is this median value plot just done as a distribution plot with a rug at the bottom.
10002000	10007000	And so this will be the target variable and we see some interesting structure going on here.
10007000	10013000	So plot again L-stat, which we identified just a second ago, versus median value.
10013000	10016000	We have again this kind of weird shaped banana plot.
10016000	10019000	This is sort of a tapering off effect of this thing.
10019000	10021000	So stats models.
10021000	10026000	Let's actually go and take a look at this as a Google search.
10026000	10029000	So stats models for Python.
10029000	10035000	The current documentation for this sits at statsmodels.sourceforge.net.
10035000	10045000	And it has, as it says, it's a Python module that allows users to look at data to estimate statistical models and perform statistical tests as many different modeling choices.
10045000	10054000	So our options, we have linear regression, generalized linear models and all the things listed here, and also some nice examples that explain more.
10054000	10063000	This is definitely a package that's geared more toward the statistical side of data science than the machine learning side, which is how I'd classify scikit-learn.
10063000	10069000	So with that comes a number of useful tools that if you haven't used them, it can be very powerful.
10069000	10072000	So this is where the documentation resides.
10072000	10074000	I recommend looking at that.
10074000	10076000	We imported this at the top.
10076000	10078000	So I'll scroll up to the top real fast.
10078000	10084000	We imported statsmodels.api as SM, which is not a typical way of importing Python modules.
10084000	10087000	This is one of the standard ways of doing stats models.
10087000	10094000	And then there's this formula.api, from which we're going to import ordinary least squares, which is just OLS in this case.
10094000	10096000	So let's scroll back down.
10096000	10099000	The formulas work in a way that's very similar to R.
10099000	10107000	So if you've used R before, or if you've used the Python package Patsy or various other ones, what you end up writing is the dependent variables.
10108000	10110000	Or the thing you're trying to predict.
10110000	10116000	So in this case, the median value, this tilde, which goes as Lstat, which is this thing that we're just plotting up here.
10116000	10119000	So Lstat versus medv median value.
10119000	10123000	So we've given the formula in terms of the relationship between these different variables.
10123000	10126000	I have to tell the model where the data comes from.
10126000	10127000	So we say this data frame.
10127000	10136000	When you give it this data frame, it says, okay, I'm going to look in this data source, df, four columns that are named in the same way that you've written it out in this formula here.
10136000	10142000	So we've said, okay, we've rewritten out medv and Lstat are actual columns.
10142000	10145000	And at the end, we will fit this with the dot fit function.
10145000	10149000	And the end, you have a model, which we've written down as mod.
10149000	10155000	And running the method dot summary tells us the output of trying to fit this data.
10155000	10157000	So we have the results from that.
10157000	10159000	So the dependent variable is median value.
10159000	10165000	The model is ordinarily squares method least squares tells you a bunch of different pieces of information that are pretty good.
10165000	10167000	These pieces of information that are pretty useful here.
10167000	10175000	So we have r squared, adjusted r squared, f statistics, log likelihood, AIC, the Ikeke information criteria, or however you say that.
10175000	10181000	Of course, the values of the coefficients and the intercepts, standard error, the 95% confidence intervals and so on.
10181000	10188000	If you're looking at this and wanting to evaluate this model statistically, you have all kinds of things at your fingertips here to look at.
10188000	10194000	Now, the relationship between Lstat and median value of the houses does not look linear to me.
10194000	10198000	This looks like a weird shape here and we can actually plot this with the river.
10198000	10208000	We can reverse this and see how this kind of tapering off of the median value versus Lstat can be a combination of features.
10208000	10212000	So I'm going to add an extra term here.
10212000	10218000	I'll actually take the log of the value and you can actually write it in this way in this string.
10218000	10223000	So you say numpy.log or np.log of the variable that you want to look at.
10223000	10229000	And you have to wrap it in this extra I for wrapping up because this doesn't actually exist as a column.
10229000	10232000	You have to wrap it in this I. There's other ways you can wrap this as well.
10232000	10240000	But I think having this and this both be in this linear model is likely to give a much better fit than just the Lstat by itself.
10240000	10247000	Or even Lstat squared, which we could also do simply by just instead of it numpy.log, we do Lstat star star squared.
10247000	10256000	So let's run this and we see our summary comes out and we have our R squared and AIC and all these different various intercepts and log values.
10256000	10258000	So let's actually compare the two.
10258000	10260000	So one way to compare it is to look at the AIC.
10260000	10267000	So this one from 3200 down to 3100, which is a pretty substantial decrease in the AIC.
10267000	10274000	So we think this is actually a better fit statistically, although we have to look at the residuals and do many other tests to make sure that this is actually is a viable model.
10274000	10281000	So that we're nowhere near done and like to double emphasize that what I'm showing you here is not a final rigorous data science result.
10281000	10286000	This is more of a sketch of what's possible with the tools that I think are useful.
10286000	10289000	Don't be taking directly from this lessons on how to do data science.
10289000	10292000	This is more of a sketch of how the tools should work.
10292000	10294000	Let's make this a little bit bigger so we have more room again.
10294000	10301000	One way to start to evaluate how good this fit is to actually look at this graphics from the stats models.
10301000	10305000	So statsmodels.graphics has a lot of different plotting options.
10305000	10316000	And there's these component and component plus residual plots, which is the CCPR plots, which you feed it the model itself contained within this model object is the underlying formula.
10316000	10322000	And so you can tell it, I want to know this one term here, the term that went with the log of the LSTAT score.
10322000	10327000	How does that look versus the residuals of this plus the I squared?
10327000	10333000	So we can see that the component actually does a decent job at this log stat versus residuals plus log stat.
10333000	10337000	So this line actually does a pretty good job of fitting this.
10337000	10344000	And for some reason that I don't quite understand, it actually plots it twice to the same exact plot, but was just LSTAT by itself.
10344000	10354000	So the first term in that model, we can see a little bit wonky behavior where it's not quite as good as the previous one where the residuals has some extra structure here in the low end, especially.
10354000	10361000	But we can start to have various goodness of fits and start to model out how good our model is at capturing the underlying data.
10361000	10364000	Again, it shows it twice. And again, I don't know why.
10364000	10367000	We can also add more terms to this model.
10367000	10372000	So previously we had LSTAT and this the log of LSTAT plus one for the intercept.
10372000	10375000	We can also add the RM category.
10375000	10381000	We can also add the Boolean value, which is whether it's in the higher low of the RAD variable.
10381000	10393000	And because it's a categorical, you can feed it to the model with this C value, and it will properly take into account the fact that what's in this column should be considered a category.
10393000	10397000	And it won't get you in trouble with the dummy variable trap that I had to mention at the last time.
10397000	10410000	So if we look at this, we see a number of things, including the fact that it starts off with the categorical variable radian bool, the categorical value where the default value is false.
10410000	10416000	And if it's true, what the change in the coefficient is for that value and the various other values as well.
10416000	10422000	Another thing to look at, depending on what you prefer to look at the BIC or the AIC or log likelihood F statistics.
10422000	10433000	To compare this to the previous models, this again has lowered the AIC substantially so that in terms of is it a better fit or not, it has some statistical basis for saying that this is a better fit.
10433000	10441000	We still have to do a lot of work still before we decide this is actually a reasonable fit and all the assumptions behind an ordinarily squares fit are holding true.
10441000	10448000	But just as a first pass, we have a lot of really nice information here.
10448000	10458000	In this video, I would like to continue from the previous video where we had just run a model to find the intercept and the categorical RAD value.
10458000	10471000	We can also run the same exact model as before deciding that this RAD values that are doing the boolean version of this, we can actually run on the entire column itself and telling stats models that we're actually using a categorical variable here as well.
10471000	10480000	So now we're trying to predict the median value of the house using all of these possible variables where these each have a coefficient in front of it.
10480000	10489000	We run a dot fit method on that and save it as a model as MOD and we're going to output the summary of that fit ran just then.
10489000	10493000	And again, dependent variable is this MED V variable.
10493000	10499000	And we see in the output here various goodness of fit and metrics about how the fit actually worked out.
10499000	10512000	We see an R squared of 0.73 and ASC of 3040, which is a slight improvement from the previous one, meaning that encoding the RAD variable where each value is independently stored.
10512000	10517000	So since it's a categorical, this is all with a baseline of one, which is why it doesn't appear here.
10517000	10524000	These coefficients are all based off of comparing each of these terms with the baseline of RAD equals to one.
10524000	10526000	If that doesn't make sense to you, don't worry about it.
10526000	10528000	Don't worry about this kind of statistical model.
10528000	10531000	And if it does make sense to you, then you understand what I just said.
10531000	10538000	So anyway, you get the output from this, but we actually want to see some plots to see how good this fit actually is.
10538000	10545000	Because just looking at the diagnostics and the metrics that come out from these fits isn't enough to tell us whether we're making a good model here.
10545000	10548000	So let's start to look at how we can assess fit quality.
10548000	10551000	One of the easy things you can do is to look at a thing called leverage.
10551000	10562000	StatsModels gives us a nice way to see this and visualize this by using the sm.graphics.influence plot and the plot leverage residual squared plots.
10562000	10564000	So let's take a look at these two plots.
10564000	10566000	I will first do this one.
10566000	10575000	So what we see here is on the y-axis, studentized residuals versus the x-axis, the h-leverage is using the cooks method for influence.
10575000	10583000	What you see on the leverage corresponds to an outside influence on the overall fit for its values.
10583000	10590000	So if you see something with high residuals and high leverage, that's something that we should possibly consider looking at that point and figuring out what's going on at that exact point.
10590000	10597000	So like 368, for example, would be a candidate to be looking at here because it has high residuals and high leverage.
10597000	10598000	That's one way of looking at it.
10598000	10602000	Another way is to look at it through this leverage residual squared.
10602000	10606000	And you give it simply the model object that you just fit above.
10606000	10611000	You just give it mod and it will give normalized residuals squared versus the leverage.
10611000	10619000	Again, 368, 365, 372, 371 are all outliers in terms of points that we should possibly take another look at again.
10619000	10622000	That corresponds to those four points up here.
10622000	10629000	So this leverage plots is one way of assessing the fit and the data points to make sure something isn't going crazy.
10629000	10635000	There's also a way of doing partial regression and I've quoted a bit from the documentation stats models here.
10635000	10641000	It says the slope of the fitted line is that of the exogenous in the full multiple regressions.
10641000	10642000	That's what's going on here.
10642000	10647000	The individual points can be used to assess the influence of points on the estimated coefficient.
10647000	10649000	So let's take a look at what this means visually.
10649000	10652000	I think it's easier to see what's happening this way.
10652000	10659000	So we have a partial regression plot and we're evaluating the expectation value of L-stat given the values that we have
10659000	10663000	and same plotting against the dependent variable, the median value that we're trying to predict.
10663000	10672000	And in this, we see that a lot of the points are kind of in a mass right here and the outliers are sitting here at this very low end.
10672000	10675000	And the same culprits appear again and you can actually see the effect that it's having on this.
10675000	10677000	It's pulling the slope up a bit.
10677000	10683000	So that's with a plot partial regression and giving various features as you're holding constant.
10683000	10686000	You can give it the entire model and see what that looks like.
10686000	10692000	When we get an entire grid, you're going to have a lot more involved to look at this grid of plots.
10692000	10701000	But it's the various features here so that the RAD variable is a feature three given X versus the median value on the X and the Y axis.
10701000	10706000	And so you can look at the various categorical variables and how they are being fit with the lines
10706000	10714000	and how they are interacting with the overall fit as well as the values that clearly are more continuous and having a nicer time of it.
10714000	10719000	So there's two ways to do this partial regression plot and both give you different ways of looking at this data.
10719000	10722000	Again, this is plotted twice for reasons unknown.
10722000	10724000	Finally, we have regression.
10724000	10727000	We can do this as a plot regress exogenous.
10727000	10734000	It gives you this four panel plot of median value versus L-stat and residuals versus L-stat.
10734000	10736000	So this is the data minus the fit itself.
10736000	10744000	And what you're hoping to see is noise pretty symmetrically about this axis here, the estimated variables and the CCPR plots.
10744000	10749000	So we see fit versus the actual values in this plot here.
10749000	10755000	And then we can also do it versus any other term in that model, which is, in this case, the natural log of the L-stat.
10755000	10761000	And then we get this plot here, which shows much tighter fit to this instance.
10761000	10766000	If you've built up a model, then again, I'm not saying I've built up some amazing model at this point.
10766000	10769000	This is definitely more descriptive of how this kind of process can work.
10769000	10779000	But if you would like to build up a model and look through a lot of diagnostic plots and have a true statistics, robust package manager behind you,
10779000	10784000	look into stats models and really try to dive into this because there's a lot of really good stuff in this.
10784000	10789000	So with that, I am concluding the second data science project.
10789000	10798000	And what I really try to focus on this time was a little bit of some more advanced features of using the plotting features of pandas,
10798000	10804000	really taking a deep dive into how one aspect of the stats models library and there's many aspects of it.
10804000	10812000	So I highlighted the ordinarily squares and how fitting linear model there with the statistical analysis and output that comes out of every fit,
10812000	10816000	as well as fitting the diagnostics and doing a quality of fit.
10816000	10824000	I also spent a lot of time on the visuals of this diving a little bit deeper into Seaborn and a few of the other options there.
10824000	10831000	So just as a kind of a wrap up of this, using map plot live and Seaborn stats models and pandas,
10831000	10835000	these data sets can be explored and manipulated and fit.
10835000	10844000	And these tools give a lot of flexibility and exploring and analyzing data in a notebook lets someone else take a look at what you did through your analysis.
10844000	10851000	So if you've made some horrendous error as you went through, that is something that's easy to point out and point to the plot.
10851000	10857000	As I said, this was a decent fit, for example. This is clearly bad because of reason X and you can point to it and circle it.
10857000	10862000	And it's not just a bunch of random files sitting in a directory somewhere.
10862000	10873000	To close off this project, the last thing that remains to do is to save this and to close it and push it back to GitHub so that you guys can also look at the same data sets and follow along yourself.
10873000	10878000	So I'm going to file, close and halt, go back to the terminal.
10878000	10882000	Git status has only this develop directory that has any changes in it.
10882000	10887000	So git add, develop, git status, we have three new files.
10887000	10891000	Okay. And I've closed down all of them. Just double check the server here.
10891000	10898000	Yep. Everything looks to be closed and say git commit.
10898000	10900000	Give a commit message that makes sense.
10900000	10905000	Git push origin Jonathan viz, which is the name of this branch.
10905000	10914000	Go back to GitHub, see that we've already made a change this we can compare and pull request and create a pull request.
10914000	10922000	I'll go ahead and actually merge this pull request because I've demonstrated how to do the full pull request and peer review aspect of it before.
10922000	10927000	And going back to the data viz project, what we have here is don't save that.
10927000	10935000	So the data viz project to will have the notebooks that I went through during this project available right there.
10935000	10947000	Just to recap what happened in this video, finished up looking at the plots from stats models and finished up the second data science project for this course.
10947000	10953000	Let's talk about some of the security issues with using the Jupiter notebook as is out of the box.
10953000	10956000	The notebook only listens to requests on local host.
10956000	10959000	This means that it ignores requests from the Internet.
10959000	10964000	People connecting from the Internet can't see your server and they won't be able to connect.
10964000	10970000	In order to allow them to connect, you have to explicitly configure the notebook to listen to the correct IP.
10970000	10973000	Once you do, anybody can access your notebook server.
10973000	10982000	The notebook server has no password by default and permissions of the users that are connecting are the same as the permissions of the user who had launched the server.
10982000	10989000	So this means if you launch the server, everybody who connects to the notebook will be executing things as if they were you.
10989000	10993000	The second main problem with using the notebook is it's using an insecure line.
10993000	10999000	So typically, the notebook is broken into three pieces, the kernel, the web server, and the client.
10999000	11002000	The client is what you see in the web browser.
11002000	11004000	It's the notebook as you know it.
11004000	11009000	And the web server is the thing that relays messages from the kernel to the client.
11009000	11012000	The web server communicates with the kernel using ZMQ.
11012000	11015000	Usually, the kernel and the web server exist on the same machine.
11015000	11020000	The kernel is the server that executes code and runs requests.
11020000	11025000	The line between the kernel and the web server you don't have to worry about usually because it's on the same machine.
11025000	11031000	However, the line between the web server and the client you have to worry about because it's over the open Internet.
11031000	11037000	This means that it's available for people to listen to and inject messages.
11037000	11043000	However, there are some setups where it makes sense to separate the kernel onto its own machine.
11043000	11049000	For example, you may have a cluster of computers running kernels, one computer running the web server.
11049000	11055000	In this case, you also have to worry about the ZMQ communication between the kernel and the web server
11055000	11060000	if the kernel and the web server are not on a VPN or in a secured network.
11060000	11067000	I'd just like to note, we aren't security experts, but we do have experts in the community and they do help us.
11067000	11072000	If you spot a problem, I ask you, please email us at our security mailing list.
11072000	11075000	The address is security at ipython.org.
11075000	11078000	Once you do, we'll work quickly to open a CVE.
11078000	11089000	In the next set of slides, I'll talk about how you can mitigate some of these problems and rest assured that your notebook deployment is as secure as can be.
11089000	11094000	In the last video, we talked about some of the limitations of running the notebook server publicly.
11094000	11098000	Specifically, we talked about security vulnerabilities.
11098000	11105000	In this video, I'll describe to you some of the solutions provided by the notebook software and some of the limitations of the notebook software.
11105000	11113000	First, in the last video, I showed you this diagram and told you that the communication between the web server and client was insecure by default.
11113000	11120000	The notebook actually provides support for HTTPS, industry grade encryption, for this communication line.
11120000	11122000	I'll show you how to configure this.
11122000	11128000	However, the notebook does not provide support out of the box for encrypting the line between the kernel and the web server.
11128000	11137000	Therefore, I recommend you either run the kernel and the web server on the same machine, if possible, or run them within a VPN.
11137000	11140000	The latest version of ZMQ does support encryption.
11140000	11144000	However, the notebook is not using that version of ZMQ currently.
11144000	11150000	Before we secure the notebook server, we need to be able to launch it so that people on the internet can connect to it.
11150000	11153000	In the previous chapter, you learned about Tralits.
11153000	11158000	We can configure the notebook to listen to all IP addresses using Tralits.
11158000	11167000	If I do jupiter, notebook, double dash help, I can list all the configuration options of the notebook.
11167000	11172000	The third to last configuration option is double dash IP.
11172000	11176000	That allows me to change the IP that the notebook server is listening on.
11176000	11182000	Just to cement the idea that this is a Tralit, I'll show you in the notebook source where this Tralit can be found.
11182000	11188000	In parentheses next to the configuration value, you see that notebook app.ip is listed.
11188000	11193000	This means that IP is a Tralit inside the notebook app class.
11193000	11203000	So opening up the notebook subfolder of the notebook repository and then the notebook app module inside that, we should be able to find the IP trait.
11203000	11206000	I'll use the search function of Adam to find IP.
11206000	11209000	Here's the definition of the IP trait.
11209000	11214000	If you want to configure something of the application and you don't see the option in the help string,
11214000	11220000	it's a good skill to be able to look through the source code and see if there's a Tralit that isn't being listed.
11220000	11223000	So we have two ways to set this IP trait.
11223000	11232000	We can either pass it in at the command line, like so, or we can specify it via config, so it's the new default.
11232000	11239000	By specifying IP to asterisk, we're telling the server to listen to requests on all IP addresses.
11239000	11247000	You may get two warnings, one from your system firewall prompting for Python to have the ability to accept incoming network connection.
11247000	11250000	This is because the notebook server is written in Python.
11250000	11255000	The other warning you'll see is in your terminal output from the notebook server itself,
11255000	11262000	warning you that the server is listening on all IP addresses and is not using encryption or authentication.
11262000	11265000	Don't worry, I'll show you how to set these up.
11265000	11269000	But first, let's try setting IP equals asterisk in the config.
11269000	11277000	If you recall from the earlier Tralits video, the config is stored inside the .jupiter folder inside my home directory.
11277000	11285000	Opening the folder up in Adam, we see that the config files from the earlier weekend and weekday demonstration still exist.
11285000	11290000	We'll go ahead and erase that here inside the jupiter notebook config.py file.
11290000	11301000	Now, recalling what the help text said in the terminal, we'll set notebook app.ip equal to asterisk.
11301000	11307000	Go ahead and save the file and we'll try launching the notebook server again.
11307000	11315000	This time, however, we won't specify the double-dash IP equals asterisk on the command line because it's already specified inside our config.
11315000	11317000	It looks like the launch was a success.
11317000	11326000	We still received the warnings about the server listening on all IP addresses, even though we didn't specify the IP equals asterisk flag in the command line.
11326000	11332000	This means that the line that we added to the config file worked as expected.
11332000	11336000	In the last video, we added password security to the notebook.
11336000	11341000	However, we did not encrypt the line between the web browser and the notebook web server.
11341000	11350000	This means that the notebook is vulnerable to people eavesdropping on the communication between it and you or any other users of your server.
11350000	11355000	In this video, we'll add HTTPS encryption to your notebook web server.
11355000	11362000	To get the notebook to start using HTTPS, all you have to do is point it to your key file and cert file.
11362000	11367000	If you don't have a key file and cert file, you can generate one yourself.
11367000	11375000	Before I show you how to tell the notebook to use your key file and cert file, I'll show you how to generate one using OpenSSL.
11375000	11378000	If you already have a key, you can skip this step.
11378000	11381000	Anaconda already comes with OpenSSL installed.
11381000	11389000	However, OpenSSL frequently releases security updates, so I highly recommend that you update to the latest version.
11389000	11394000	To do so, you can run conda space update OpenSSL.
11394000	11397000	I'm currently inside my Jupyter config directory.
11397000	11403000	I'm going to run OpenSSL to generate the key insert file.
11403000	11406000	I'm going to generate the cert so it lasts for one year.
11406000	11414000	To do so, I'm going to pass in 365 days into the days argument.
11414000	11422000	I'm going to output both the key and the cert file into the same file.
11422000	11425000	Once I run the command, an interactive wizard will start.
11425000	11427000	I'll answer some of these questions.
11427000	11435000	However, if you want, you can skip any of the questions just by hitting return to accept the default value.
11435000	11440000	Once that is done, we'll have to configure the notebook to use this key insert file.
11440000	11446000	To do so, I'm going to open up Adam inside the Jupyter configuration directory.
11446000	11450000	After the shaw from the password trait, I'm going to create a new line.
11450000	11452000	I'm going to specify the cert file first.
11452000	11455000	The cert file is a trait of the notebook app.
11455000	11462000	It's important that I pass the full path to the cert file.
11462000	11464000	Next, I'm going to specify the key file.
11464000	11469000	The key file is a trait of the session class.
11469000	11474000	Since we output it the key into the cert file, we can just specify the same file here.
11474000	11477000	Now I'm going to save the config.
11477000	11482000	Back in the terminal, I'm going to try launching the notebook.
11482000	11486000	When the notebook launches, you'll probably see this security error from your web browser,
11486000	11490000	saying that your connection is not private and that the authority is invalid.
11490000	11493000	This is because you self-generated the cert.
11493000	11496000	You can get around this by having a third party generate your cert.
11496000	11501000	For now, let's just click Advanced and proceed the local host.
11501000	11503000	Now our connection is being encrypted.
11503000	11507000	If you are interested in getting a cert that's verified by a third party,
11507000	11509000	I recommend using StartSSL.
11509000	11511000	They'll do it for free.
11511000	11516000	You can visit their website at www.startssl.com.
11516000	11520000	The StartSSL free cert should be fine for basic setups.
11520000	11524000	The other two offer slightly more features that are verified,
11524000	11530000	whereas the most expensive gives your site a green bar inside the address bar when the user is connected.
11530000	11537000	You can see that in the screenshot in the side column of their website.
11537000	11542000	In the last chapter, we talked about how you could deploy the notebook securely.
11542000	11544000	In this chapter, we'll change gears.
11544000	11546000	We'll start looking at NB Viewer.
11546000	11552000	Before I discuss installing NB Viewer, I'm going to show you what NB Viewer looks like in the wild.
11552000	11556000	I'm currently on the Jupyter public deployment of NB Viewer,
11556000	11561000	which is accessible at nbviewer.jupyter.org.
11561000	11568000	NB Viewer is a web application that is used to render static views of notebooks online.
11568000	11571000	In the back end, NB Viewer uses NB Convert,
11571000	11578000	the application that I showed you in chapter one, which can be used to convert notebooks to various static formats.
11578000	11584000	NB Viewer just uses NB Convert to convert notebooks to static HTML representations.
11584000	11590000	NB Viewer itself is a simple website that has a title and then an address bar
11590000	11593000	where you can paste the link to your notebook file.
11593000	11597000	After pasting the link, you click Go and it will render that notebook file.
11597000	11601000	Below that, there's a showcase of notebooks for various categories.
11601000	11607000	Here, for example, we can click on this iRuby notebook to see what iRuby is.
11607000	11609000	This is what a rendered notebook looks like.
11609000	11613000	You can see it looks quite different than the notebook client that you're used to.
11613000	11619000	It's quite a bit more bare, but it still bears some resemblance to pieces of the interactive notebook,
11619000	11623000	such as these prompts and cell formatting.
11623000	11631000	At the top, there are links to download the notebook, view the notebook on GitHub if it is a GitHub hosted file,
11631000	11633000	and a link to go to the top of the file.
11633000	11638000	At the bottom of the page, you can see the version of NB Viewer that we're running,
11638000	11643000	the notebooks version, and the version of NB Convert that NB Viewer is running against.
11643000	11651000	NB Viewer tries to be aggressive about caching notebooks, so you also get a status of when the notebook was last rendered.
11651000	11659000	Because NB Viewer is not a user application and it's actually a web application, it's not included with Anaconda.
11659000	11661000	Therefore, I'll have to show you how to install it.
11661000	11665000	The easiest way to install NB Viewer is using Docker.
11665000	11671000	Docker is not included with Anaconda either, so I'll also have to show you how to install that.
11671000	11673000	Docker is an emulation platform.
11673000	11679000	It allows you to run applications inside an isolated environment called containers.
11679000	11685000	Docker containers differ from virtual machines in that the containers share the host OS.
11685000	11689000	Containers can also share dependencies with each other.
11689000	11698000	This minimizes the distance between the container and the system hardware, which makes containers faster and smaller to install.
11698000	11705000	To install Docker, first go to Docker's website at www.docker.com.
11705000	11709000	Then click on the Get Started link in the top right hand corner.
11709000	11713000	The instructions for getting started are operating system dependent.
11713000	11717000	Because I'm running a Mac, I'll show you how to get started with Docker on a Mac.
11717000	11722000	If you're running Linux or Windows, this page will look a little different for you.
11722000	11725000	The first step is to install Docker tools.
11725000	11728000	You can click on Install Docker on OS X.
11728000	11734000	Scroll down to step two, where you'll see Install Docker Toolbox.
11734000	11736000	Click on that and then scroll down.
11736000	11740000	Click the Download button for Mac if you're on OS X.
11740000	11743000	Once you have the Toolbox installer, run it.
11743000	11745000	Follow the prompts in the wizard.
11745000	11748000	Select a hard drive to install to.
11748000	11751000	Enter your password when prompted.
11751000	11753000	When done, click Continue.
11753000	11755000	Then click Close.
11755000	11759000	Now launch the Docker Quick Start Terminal.
11759000	11762000	It takes a little while for it to start the machine.
11762000	11773000	Once the process finishes, you can run Docker space run space hello dash world.
11773000	11781000	You should see a Hello from Docker message, which confirms that your installation is working.
11781000	11785000	In the last video, I introduced you to NBViewer and Docker.
11785000	11788000	We then installed Docker on your machine.
11788000	11791000	In this video, we'll install the NBViewer Docker image.
11791000	11795000	To get started, open the Docker Quick Terminal.
11795000	11797000	Your terminal may take a while to start.
11797000	11802000	Once the terminal has started, pay attention to the IP address listed in green.
11802000	11806000	Mine's 192.168.99.100.
11806000	11809000	That is the IP address of the Docker image.
11809000	11815000	You'll use that IP address to access your NBViewer server once it's started.
11815000	11817000	The first step is to download NBViewer.
11817000	11819000	Now, I've already done this ahead of time.
11819000	11824000	So mine will download fairly quick because it will just be verifying that I have the latest version.
11824000	11830000	But the first time you run this command, it may take a while.
11830000	11838000	Next, let's try launching NBViewer.
11838000	11842000	Once the server starts, it should tell you the port it's listening on.
11842000	11847000	In a new web browser, go ahead and try accessing that IP address that you remember that was in green,
11847000	11852000	followed by colon 8080.
11852000	11855000	If all worked well, you should see NBViewer.
11855000	11858000	Go ahead and try to open up a notebook.
11858000	11861000	Once the notebook opens, go back to your terminal.
11861000	11866000	You should see output from the NBViewer server verifying your request.
11866000	11869000	Without this, it would be hard to tell if you were actually running the server or not,
11869000	11873000	or if you were just accessing the public NBViewer deployment by Jupyter.
11873000	11879000	NBViewer has this wonderful feature that allows you to access notebooks on GitHub using short URLs.
11879000	11885000	To demonstrate this, I'll access a notebook that's stored as a gist under my GitHub account.
11885000	11888000	So here's a simple notebook I created for PyData.
11888000	11892000	It's stored under my account as this gist.
11892000	11894000	I'm going to just copy this URL.
11894000	11900000	Because NBViewer has support for gist, I can just paste it directly in and click go.
11900000	11906000	Alternatively, I can use an even shorter form, which is just the gist ID.
11906000	11910000	To do so, I'll remove all the stuff before the last forward slash.
11910000	11912000	This is my gist ID.
11912000	11914000	You can see NBViewer still renders it.
11914000	11917000	The GitHub public APIs have rate limiting.
11917000	11923000	So if you plan on supporting this feature, it's a good idea to generate an access token for NBViewer.
11923000	11925000	Doing so is relatively painless.
11925000	11928000	Log on to github.com using your account.
11928000	11932000	Then in the top right hand corner, click view profile and more.
11932000	11936000	Next, select your profile.
11936000	11938000	Click edit profile.
11938000	11943000	Then click personal access tokens in the left hand column.
11943000	11946000	Next, click generate new token.
11946000	11948000	Give the token a name.
11948000	11952000	And then change the scopes that you want to use to restrict the token.
11952000	11954000	When you're done, click generate token.
11954000	11957000	Your token will be displayed in the green bar.
11957000	11960000	I've blurred a couple of the numbers of my token for security.
11960000	11964000	Click the copy button to copy the token to your clipboard.
11964000	11969000	Now in the terminal that's running NBViewer, hit ctrl C to stop NBViewer.
11969000	11977000	Now let's relaunch NBViewer, adding our new access token to the command line.
11977000	11984000	Because we're running NBViewer as a Docker image, we can't specify arguments directly to NBViewer.
11984000	11989000	Instead, we have to set environment variables to cause NBViewer to change its behavior.
11989000	11999000	Here, I'm telling Docker to set the github underscore api underscore token variable to the token that I just copied from github.
11999000	12003000	Now when I try accessing NBViewer, it should be using that token.
12003000	12006000	Let's paste the same gist id from earlier.
12006000	12012000	Now let's go to github.com to see if the api token was used.
12012000	12016000	We can see that it was just used because github says it was used within the last day.
12016000	12021000	Using this token should help lift some of the rate limits for github access.
12021000	12028000	And it's also nice because it allows github to control who's accessing their APIs.
12028000	12032000	In the last video, we installed NBViewer using Docker.
12032000	12034000	This is great for most use cases.
12034000	12039000	However, sometimes it's necessary to maintain more control over the distribution.
12039000	12043000	To do this, you can install NBViewer from source.
12043000	12045000	This will allow you to do two things.
12045000	12050000	One, it will allow you to control what dependencies NBViewer is using.
12050000	12060000	And two, it will allow you to modify NBViewer's source code directly, including installing additional extensions without having to recompile the Docker image.
12060000	12063000	The first step is to clone the NBViewer repository.
12063000	12073000	You can either clone the upstream fork, like I will do here, or you can clone your own fork.
12073000	12077000	Once NBViewer has finished cloning, cd into that directory.
12077000	12085000	Now run pip install-r requirements dev.txt.
12085000	12092000	Next run npm space install, then run invoke bower.
12092000	12094000	This is installing the static assets.
12094000	12099000	Next run invoke less, which will compile the less into CSS.
12099000	12101000	CSS is what styles NBViewer.
12101000	12103000	I've cleared my console.
12103000	12106000	Now I'm going to run pip install markdown.
12106000	12111000	Once that finishes, I should be able to launch NBViewer.
12111000	12115000	Now I can access NBViewer using localhost.
12115000	12119000	To verify that this is actually running locally, let's try changing some of the code.
12119000	12121000	Let's change the title.
12121000	12124000	I'm going to hit ctrl-c to stop the server.
12124000	12127000	I'm going to open up Adam in the NBViewer repository.
12127000	12136000	Once Adam opens, I'm going to open the NBViewer sub directory, the template sub folder, and then the index.html file.
12136000	12138000	Let's change the title of the website.
12138000	12141000	We'll change NBViewer to myNBViewer.
12141000	12143000	We'll go ahead and save.
12143000	12147000	Editing these templates directly is actually not the best way to modify NBViewer,
12147000	12151000	but we'll do it for now just to verify that we've installed from source.
12151000	12155000	Now back at the terminal, go ahead and relaunch the server.
12155000	12159000	Back in your web browser, refresh the page.
12159000	12166000	When you see the title update to myNBViewer, you know that the changes that we made to the template file were loaded.
12166000	12171000	If when you refresh the page, the title doesn't change, try emptying your web browser's cache.
12171000	12178000	If you want a quicker way to see if this is the problem, open an incognito tab and then navigate to the NBViewer web page.
12178000	12181000	The incognito tab should prevent the web browser from caching.
12181000	12189000	Often when you do web app development, caching causes problems because it doesn't let you see your most recent changes to the code.
12189000	12196000	Earlier, I had mentioned that modifying the template directly in NBViewer's source was not the right way to modify the template.
12196000	12202000	A better way would be to configure NBViewer's template directory to a different directory,
12202000	12209000	have it load from one of your own custom templates, which inherit it from the template included with NBViewer.
12209000	12217000	In the following videos, we'll look at how we can do that in addition to customizing NBViewer different ways.
12217000	12223000	In this video, we'll look at what we can do just by extending the NBViewer templates.
12223000	12227000	Before we get started, we need to remove the hack that we added in the last video.
12227000	12231000	I'm going to go ahead and launch Adam from within the NBViewer repository.
12231000	12239000	Once Adam's launched, I'll open the NBViewer subfolder, then the template subfolder, and then index.html.
12239000	12242000	In there, I'll remove my space.
12242000	12244000	Now, I'll save the file.
12244000	12247000	Let's see who loads this index.html file.
12247000	12256000	I'm going to open the find in project dialog by hitting command shift F, which is control shift F on Linux and Windows.
12256000	12261000	Looks like the template is rendered here in the index handler method.
12261000	12267000	Let's see where the render template method searches for index.html.
12267000	12273000	Looks like the definition of render template is in the NBViewer provider's base.py class.
12273000	12276000	The get template method is used to load the template.
12276000	12280000	Inside the get template method, which is above the render template method,
12280000	12286000	we can see that the Jinja2 environment has another get template method defined, which we call out to.
12286000	12290000	Let's see where this Jinja2 environment comes from.
12290000	12293000	Looks like it's defined in app.py.
12293000	12302000	Scrolling up to see where nv is defined, we see nv is an instance of environment, which is imported from Jinja2.
12303000	12308000	The template loader is a file system loader, which loads from template paths.
12308000	12313000	Template paths is hard coded to the repository directory template subdirectory.
12313000	12321000	However, if you specify a custom template path using the nbViewer underscore template underscore path environment variable,
12321000	12328000	it gets propended to a list of paths, which then is used as the higher priority path.
12328000	12333000	So we can set a custom template search path just by setting that environment variable.
12333000	12337000	Knowing this, we can set the nbViewer template path.
12337000	12344000	I'm going to set it to the nbViewer underscore templates subfolder of my home directory.
12344000	12347000	Now I'm going to create that directory.
12347000	12351000	I'll cd into it and open Adam.
12351000	12355000	In Adam, I'll create an index.html file.
12355000	12362000	This file will override the index.html file in the nbViewer templates folder.
12362000	12365000	For now, I'll just write hello world and save the file.
12365000	12371000	Now, switching back to the terminal, I'll cd back into the nbViewer repository.
12371000	12375000	I'll launch nbViewer using the same command from earlier.
12375000	12379000	Now when I try to access nbViewer, the page just says hello world.
12379000	12381000	This means that our template was loaded successfully.
12381000	12383000	Let's try to complicate things.
12383000	12388000	Back inside the Adam that is opened in the nbViewer repository,
12388000	12393000	I'm going to go to the templates folder and open index.html again.
12393000	12398000	nbViewer uses the Jinja templating library to render its HTML pages.
12398000	12401000	This funky syntax extends and block body.
12401000	12404000	Those are Jinja 2 specific keywords.
12404000	12407000	The rest of the code that you see is vanilla HTML.
12407000	12410000	Let's go ahead and copy all the contents of this file.
12410000	12417000	Back into our index.html file inside the nbViewer templates folder.
12417000	12421000	Now let's change the title here and save.
12421000	12426000	If we've done this correctly, we'll have changed the look of the nbViewer landing page
12426000	12429000	without actually modifying nbViewer's source code.
12429000	12436000	I refreshed the nbViewer page and it looks like our custom template was loaded.
12436000	12441000	To give ourselves a target, let's try to set up an O'Reilly themed nbViewer.
12441000	12445000	Our O'Reilly nbViewer should look like its O'Reilly's nbViewer,
12445000	12448000	but also host O'Reilly content.
12448000	12453000	First, let's change the basic index template that we created in the last video.
12453000	12459000	To do so, I'll open up the nbViewer templates folder that we created in my home directory.
12459000	12463000	Now I'll open Adam in that directory.
12463000	12467000	I'll change the title to O'Reilly notebooks.
12467000	12471000	We'll also change the descriptive paragraph below.
12471000	12474000	Eventually, we won't want to be hosting notebooks from GitHub,
12474000	12478000	so let's change the placeholder text to reflect that.
12478000	12480000	Now I'll save and see how it looks.
12480000	12484000	We can launch nbViewer using the same command that we used in the previous video.
12484000	12486000	I'll create a new tab of my terminal.
12486000	12492000	Before starting nbViewer, I need to set the environment variable again for the custom templates.
12493000	12496000	Now I can launch the server.
12496000	12498000	It looks like our change is rendered.
12498000	12501000	However, we should probably change this logo in the top left,
12501000	12504000	and also remove this link to Jupyter.
12504000	12507000	Let's scroll down to see if there's anything else we need to change.
12507000	12512000	We'll have to change this section of showcased notebooks.
12512000	12516000	And at the very bottom, it looks like we'll want to change the footer.
12516000	12519000	Lastly, we should probably change the styling
12519000	12522000	and maybe use JavaScript to spiff up the page a bit.
12522000	12525000	First, let's see if we can change the header and footer.
12525000	12529000	Let's go back to the index.html file in our custom template folder.
12529000	12534000	Looking at the index.html file, it looks like layout.html is extended
12534000	12536000	for the basic layout of the page.
12536000	12537000	Let's open that.
12537000	12540000	It should be inside the nbViewer directory.
12540000	12544000	Inside the nbViewer repository in the nbViewer subfolder,
12545000	12549000	under templates, we can find layout.html.
12549000	12552000	Like we did with index, let's copy everything in here.
12552000	12556000	Then, back inside our custom nbViewer templates folder,
12556000	12559000	let's create a layout.html.
12559000	12563000	Here, I'll paste all the contents from the other layout.html.
12563000	12565000	Let's remove this link to Google Analytics,
12565000	12571000	because this is the Google Analytics for the Jupyter deployment of nbViewer.
12571000	12574000	Also, we'll want to get rid of these links to Fastly
12574000	12579000	and change the Rackspace link to O'Reilly.
12579000	12583000	Scrolling up, let's get rid of the text that says this website does not host notebooks.
12583000	12586000	It only renders notebooks available on other websites,
12586000	12591000	because we're going to be using this pseudo-website to host O'Reilly notebooks.
12591000	12595000	Here's the link to Jupyter that we wanted to remove.
12595000	12599000	Lastly, we'll want to change the nav logo to O'Reilly's logo.
12599000	12603000	Let's go to O'Reilly's website to see if we can get the link to their logo.
12603000	12607000	I'm on O'Reilly's website now at www.oreilly.com.
12607000	12609000	I like this logo in the top left-hand corner.
12609000	12613000	I'm going to right-click on it and click Copy Image URL.
12613000	12616000	Back inside the layout.html file,
12616000	12623000	I'm then going to paste that URL over the image URL for the existing nav logo.
12623000	12626000	We'll also get rid of the New Relic reference.
12626000	12631000	Let's save what we have and go back to the browser to see how it renders.
12631000	12635000	Awesome! This is already looking a little more O'Reilly-like.
12635000	12637000	We'll probably still want to change the color scheme,
12637000	12640000	because I noticed when I roll over FAQ, it highlights orange,
12640000	12645000	which doesn't match O'Reilly's red.
12645000	12650000	Looking at the bottom of the page, it looks like our footer updated it correctly.
12650000	12653000	Let's check out the FAQ page.
12653000	12655000	It looks like there's some questions that shouldn't be here.
12655000	12657000	Let's remove them.
12657000	12659000	Back inside the nbviewer repository,
12659000	12663000	it looks like the FAQ.md file might be the file that's getting rendered.
12663000	12664000	Let's open that.
12664000	12668000	It looks like this file does indeed extend the layout.html file
12668000	12672000	and uses a special markdown filter to convert itself from markdown to html.
12672000	12676000	In the process, it automatically generates its table of contents.
12676000	12682000	Let's do what we did for index.html and layout.html in our custom templates folder.
12682000	12692000	Let's create an FAQ.md file and copy the contents from the FAQ.md file in nbviewer.
12692000	12694000	Let's get rid of the first two questions,
12694000	12698000	because they are completely specific to Jupyter's nbviewer.
12698000	12710000	We'll defer them to nbviewer for this information.
12710000	12713000	This paragraph doesn't relate at all to our viewer,
12713000	12716000	nor does the one below or the one below that.
12716000	12718000	This paragraph also doesn't relate.
12718000	12719000	This is related, though.
12719000	12723000	We just need to update it to point to our email address.
12723000	12726000	The last few before the final one also don't relate,
12726000	12732000	and we'll replace the text of the final one with an email link to the O'Reilly administrator.
12732000	12736000	Now let's save and see if the FAQ page renders how we want.
12736000	12739000	Back in the web browser, I'm going to refresh the page.
12739000	12746000	Looks like we should remove the first question as well.
12746000	12749000	Let's refresh the page again.
12749000	12751000	Ah, much better.
12751000	12754000	Let's try clicking on the O'Reilly image to go back to the homepage.
12754000	12756000	Sweet, it worked.
12756000	12764000	In the next tutorial, we'll look at adding custom CSS to style it more like O'Reilly's main website.
12764000	12769000	In this video, we'll talk about how nbviewer compiles its less into CSS.
12769000	12774000	We'll then look at adding our own CSS to our custom nbviewer templates.
12774000	12778000	I've still left the nbviewer server running from the last video.
12778000	12781000	This is because I do not need to restart it.
12781000	12788000	As long as I'm only changing static files, all I have to do is refresh the webpage to update the contents.
12788000	12792000	If I were working on server-side files, for example the Python files,
12792000	12795000	then I would have to restart the server.
12795000	12800000	Let's go ahead and open up Adam inside the nbviewer repository.
12800000	12806000	When you installed nbviewer from source code, you had to run a command called invoke less.
12806000	12814000	When you ran that command, what it did was run a function called less inside the tasks.py file.
12814000	12816000	Here's that function.
12816000	12822000	What this function does is compile the less into CSS using the less compiler.
12822000	12826000	It outputs the compiled CSS into a build subdirectory.
12826000	12832000	It outputs a styles.css, notebook.css, and slides.css.
12832000	12839000	Likewise, the source files used are styles, notebook, and slides.less.
12839000	12843000	Let's open the nbviewer static directory.
12843000	12847000	In here, you see the folder less and the build folder.
12847000	12852000	The build folder is grayed out here because it's not included in the git repository.
12852000	12855000	That's because we don't want to check in the built files.
12855000	12857000	That would just be including changes twice.
12857000	12860000	The less folder is where the less is stored.
12860000	12864000	We can open up the notebook.less to get an idea of how notebooks are styled.
12864000	12869000	A major difference between less and CSS is that less allows you to import.
12869000	12874000	Here you can see that bootstrap is imported and styling from ipython.
12874000	12878000	Let's go ahead and see where the build files are referenced.
12878000	12886000	In the layout template, inside the header, we see that styles.css is referenced.
12886000	12892000	Inside notebook.html, we can see where notebook.css is referenced.
12892000	12896000	Let's go ahead and add our own styling to our custom templates.
12896000	12901000	Going back to the terminal, I'm going to cd into our custom templates directory.
12901000	12904000	Here I'll open Adam.
12904000	12910000	Inside our layout.html, below the existing CSS import, let's add our own.
12910000	12916000	It's important that you do this below the existing because this will cause your style to override the existing.
12916000	12922000	Unfortunately, nbviewer doesn't support pulling files from directories outside of its own, so we have two options.
12922000	12929000	We could either place our custom style inside the nbviewer repository, which I'd rather not do,
12929000	12936000	or we can use the ginga templating to load it from our nbviewer templates directory and then inline it directly into the html.
12936000	12942000	First, let me show you what it would look like if you were to put the CSS inside the nbviewer repository.
12942000	12948000	You would change build to CSS and then give your CSS file a name, like custom.
12948000	12956000	You would then save this file and inside the nbviewer repository under the static directory in CSS,
12956000	12961000	you would right-click, create a new file called custom.css.
12961000	12966000	And then inside here, you would put whatever custom CSS you want.
12966000	12974000	Moving back into our nbviewer templates directory, the alternative, I think, makes more sense because then you can keep your CSS next to your templates.
12974000	12979000	For this, instead of using a link tag, you'll use a style tag.
12979000	12986000	Then inside style tags, use the ginga include to include your style file.
12986000	12992000	The only downside to using this method is that you're disabling the browser's ability to cache your style,
12992000	12996000	which means that every time a page is requested, client will have to download the CSS again.
12996000	13003000	That's usually not a problem with small CSS files, and if it is a problem, you can use the other method that I just showed you.
13003000	13008000	So now let's save this file and create our own custom CSS.
13008000	13015000	To test to see if our custom CSS is working, let's try setting the body background color.
13015000	13020000	I'm going to use important just to make sure it overrides any other values.
13020000	13026000	However, it's important to note that important isn't the best practice.
13026000	13030000	Using important disables you from later overriding styles.
13030000	13035000	In a new browser window, let's navigate to our NB viewer page to see if our style gets loaded.
13035000	13041000	Awesome, it looks like the style loaded successfully.
13041000	13048000	Now, instead of applying such a hideous style, let's try to override the orange highlight color that's applied to buttons.
13048000	13053000	Let's inspect the FAQ button to see how we can select it using CSS.
13053000	13060000	Looks like a good selector would be to use the navbar right class and then the anchor tag.
13060000	13064000	Back inside our custom CSS, let's do that.
13064000	13069000	To specify that we want to change the styling when it is hovered over, add the hover sudo selector.
13069000	13072000	For now, let's just try changing the background color.
13072000	13078000	Again, let's use the important tag just to make sure that what we're doing gets applied.
13078000	13080000	Looks like that worked.
13080000	13087000	So now, let's change the font color instead of changing the background color and let's actually use O'Reilly's red.
13087000	13094000	Let's go to O'Reilly's website and we'll right click on the home link to look at its color.
13094000	13097000	Now, I'll just double click this and copy it.
13097000	13103000	Back inside our custom CSS, I'm going to change background color to color and paste this new color.
13103000	13110000	Let's save the file, then go back to the web browser where I'll open our MB Viewer tab and refresh the page.
13110000	13112000	Looks like that works.
13112000	13117000	Now back inside the custom CSS, let's try to move the important flag.
13117000	13122000	Like I said earlier, it's better to not use important when you can get away with it.
13122000	13126000	Back in the browser, let's refresh the page and see if it still works.
13126000	13128000	Looks like it's no longer working.
13128000	13135000	We have two options. We can either stick with the important flag or we can try to make our selector more specific.
13135000	13143000	Because I know that I'm applying the top most level styling and nobody's going to come in and inherit from the O'Reilly page and add their own styling,
13143000	13145000	it's okay for me to use important.
13145000	13151000	If, however, you were writing something that would later be styled by somebody else, you'd want to make the selector more specific.
13151000	13159000	To do so, you could inspect the element and either A, add more levels of elements to your selector,
13159000	13166000	or B, in the templates, actually add an ID to this anchor tag and then address the anchor tag by ID.
13166000	13172000	Addressing an element by ID has a higher specificity than addressing it otherwise.
13172000	13176000	Back inside the custom CSS, let's re-add the important.
13176000	13179000	I'm going to refresh the browser page.
13179000	13181000	Looks like that's still working.
13181000	13183000	Let's scroll down to the bottom of the page.
13183000	13187000	Maybe we should use one of O'Reilly's grays for this bottom.
13187000	13191000	We could also use O'Reilly's red for the links.
13191000	13193000	This gray looks nice.
13193000	13196000	We'll copy the background color.
13196000	13202000	Now back on the Jupyter NB viewer tab, let's try styling this footer.
13202000	13207000	The font doesn't have enough contrast now. Let's change it to black.
13207000	13213000	That seems like it has too much contrast. Let's see what O'Reilly does.
13213000	13218000	Looks like they use an off black. We'll use that too.
13218000	13224000	I'd also like to add a top border.
13224000	13228000	Let's copy the border color that O'Reilly uses.
13228000	13233000	Looks like they use this off-shaded gray.
13233000	13238000	Now we can just copy this CSS that we've designed in the browser
13238000	13242000	and paste it into our custom CSS in a footer selector.
13242000	13244000	Now let's refresh the page.
13244000	13248000	Scrolling to the bottom, we see that our new styling has been applied.
13248000	13252000	Lastly, we need to change the default link color to that red.
13252000	13258000	Back on our custom CSS, let's define an anchor selector.
13258000	13263000	We have four C problems with this anchor tag and this anchor tag.
13263000	13270000	Let's define a color for when the FAQ anchor tag is not hovered on.
13270000	13276000	We'll use the color that we used for text.
13276000	13282000	I'm going to save and then go back to the browser and refresh the page one more time.
13282000	13285000	The FAQ button is still working. Scroll to the bottom.
13285000	13290000	And it looks like our links are formatted correctly now.
13290000	13296000	In the last video, we looked at customizing our MbViewer deployments CSS.
13296000	13300000	In this video, we used JavaScript to spiff up the website a little bit.
13300000	13304000	I found this really cool carousel on Bootstrap's website.
13304000	13305000	Here it is.
13305000	13308000	Bootstrap is a component that MbViewer already uses,
13308000	13311000	so we should be able to just drag and drop this code into place.
13311000	13316000	What I want to do is replace the notebook listing in the showcase
13316000	13319000	on our MbViewer with a carousel.
13319000	13326000	So I'm going to go back to the Bootstrap website and copy and paste the code here.
13326000	13330000	Inside the index template in our custom templates folder,
13330000	13335000	scrolling down towards the bottom, you can see where the showcase is built.
13335000	13339000	The JINJA templating for loop is used to iterate over each section
13339000	13343000	and then it's used again to iterate over each link in each section.
13343000	13347000	We'll use this logic to compile the different slides for our carousel.
13347000	13352000	For now, I'm going to insert the carousel code above this existing code
13352000	13355000	in between the header and the showcase,
13355000	13358000	pasting what we copied from Bootstrap's website.
13358000	13361000	I'm going to remove the indicator dots on the carousel.
13361000	13367000	Also, from experience, I know that we're not loading glyph icon on MbViewer by default,
13367000	13369000	and I don't feel like adding that dependency.
13369000	13371000	Instead, we're using font awesome.
13371000	13380000	Equivalent icons would be icon-prev and icon-next.
13380000	13385000	Now, what we need to do is use that JINJA code that iterates through each item
13385000	13387000	to construct our carousel slides.
13387000	13390000	It looks like each individual unit is an item.
13390000	13392000	The first item is active.
13392000	13395000	Let's go ahead and delete the ellipses.
13395000	13400000	Now, let's move the JINJA templating loop logic below this first item
13400000	13405000	to create the latter items.
13405000	13409000	We're going to just ignore the notion of sections,
13409000	13415000	so we'll group both the loops next to each other.
13415000	13419000	Now, let's copy the item template into the loop.
13419000	13423000	Then we'll copy the image source into the item's image.
13423000	13428000	We'll also copy the link text as the alternative text
13428000	13430000	and use it as the caption.
13430000	13434000	Then we'll take the anchor tag and put it around the caption.
13434000	13437000	This will make the caption clickable.
13437000	13441000	Now, finally, we'll remove the original code from the gallery.
13441000	13447000	We'll save our changes and refresh the page to see how it renders.
13447000	13448000	So here's the page.
13448000	13451000	You can see it doesn't have the gallery below anymore.
13451000	13454000	Now it just has this carousel that rotates through images.
13454000	13459000	And each image has a link that we can click to open that notebook.
13459000	13464000	However, you may notice the size is constantly changing.
13464000	13467000	It must depend on the image height.
13467000	13469000	Let's fix the size of the carousel.
13469000	13471000	We'll do so using CSS.
13471000	13474000	First, let's get the ID of the carousel.
13474000	13475000	Copy that.
13475000	13479000	Then in your custom CSS, add a selector for the carousel.
13479000	13482000	To select an ID, prefix with the hashtag.
13482000	13488000	Now set the height to 300 pixels and the width to 300 pixels.
13488000	13490000	Save.
13490000	13493000	And let's go back to the web browser to see how that renders.
13493000	13495000	We're going to refresh the page.
13495000	13498000	Here's what our smaller carousel looks like.
13498000	13502000	We should probably center it in the page and add a margin.
13502000	13507000	It looks kind of weird hugging the bottom so closely in the top.
13507000	13512000	Let's try centering it in the web browser.
13512000	13516000	By setting margin left and right to auto, the element will center.
13516000	13522000	Now let's add a top margin to give it some distance from this horizontal line.
13522000	13524000	40 pixels looks good.
13524000	13527000	Let's do the same with the bottom.
13527000	13529000	Now take one last look.
13529000	13531000	That looks good.
13531000	13535000	Let's copy and paste this style back to our CSS.
13535000	13542000	Oops, looks like I forgot to copy margin left and right auto.
13542000	13546000	Now we need to get rid of that placeholder for the first active item.
13546000	13551000	In index.html, in the carousel code, you can see that item here.
13551000	13553000	Go ahead and remove that.
13553000	13557000	What we need to do is only add active to the first class.
13557000	13560000	To do that, let's create a flag.
13560000	13563000	Once that flag is used once, we'll set it the false.
13563000	13571000	We can use the ginger set command to set this flag.
13571000	13577000	Then we'll test for that in class.
13577000	13583000	Lastly, let's make sure we set first the false.
13583000	13587000	When we set first the false here, we're actually declaring a new variable first
13587000	13593000	within the scope of this for loop that overrides the first declared in the outer scope.
13593000	13597000	This means when we get to the next for loop, first will be set to true again.
13597000	13600000	So we have to set first the false twice.
13600000	13602000	Let's refresh the page.
13602000	13604000	Ah, looks like that worked.
13604000	13607000	Awesome.
13607000	13615000	In the last video, we talked about adding custom CSS and custom JavaScript to your NB viewer deployment.
13615000	13620000	In this video, we'll talk about changing what NB viewer is hosting to the user.
13620000	13627000	NB viewer has a notion of providers, which are the things that dictate what NB viewer can host.
13627000	13633000	There are two types of providers, URI rewrites and handlers.
13633000	13638000	URI rewrites take textual content that's entered into the go bar of NB viewer
13638000	13642000	and translate it to a canonical NB viewer URL,
13642000	13646000	a URL that NB viewer understands and is capable of rendering.
13646000	13652000	Handlers are things that are designed to interpret and load from NB viewer URLs.
13652000	13659000	The handler is the thing that actually fetches the resources from the local or remote location.
13659000	13669000	For example, the GitHub handler accesses notebook content directly from GitHub using GitHub's API instead of standard HTTP.
13669000	13673000	Let's start by configuring NB viewer to host local files.
13673000	13681000	Sticking to our O'Reilly themed example, let's pretend that O'Reilly wants to host files from a network-attached storage device.
13681000	13685000	Let's say that that storage device is SimLink into the home directory.
13685000	13688000	We'll pretend that that SimLink is called network.
13688000	13696000	I'm going to create this folder just as an example that we can use to demonstrate this feature of NB viewer.
13696000	13702000	Let's pretend that in the network-attached storage drive, there's a subfolder called notebooks.
13702000	13706000	And then inside the notebooks folder, there are author folders.
13706000	13711000	For now, I'll just create an authored folder for myself.
13711000	13715000	I have some example notebooks that are sitting inside my home folder.
13715000	13723000	I'm going to copy those over to here.
13723000	13725000	Now let's take a look at the NB viewer source code.
13725000	13731000	I'm going to CD into the NB viewer repository and open Adam.
13731000	13736000	Inside the NB viewer subfolder, I'm going to open app.py.
13736000	13742000	Scrolling down to the very bottom of app.py, we see all the command line arguments that we can pass to NB viewer.
13742000	13745000	One of the command line arguments is local files.
13745000	13749000	This tells NB viewer to host files from the local file system.
13749000	13751000	Let's use this.
13751000	13753000	I've closed the NB viewer server.
13753000	13755000	I'll relaunch it with this new command.
13755000	13766000	But before I launch, remember that we need to set the correct environment variable in order for our custom templates to be loaded.
13766000	13773000	Now let's launch NB viewer.
13773000	13778000	Let's switch to the web browser to see if we can load files from the local files system.
13778000	13781000	I'm going to try accessing the notebook using the go bar.
13781000	13788000	I'll type in the subpath to the notebook from its location inside network.
13788000	13790000	Doing that didn't work.
13790000	13795000	This would make you want to jump to the conclusion that the local files setting isn't working.
13795000	13798000	However, this is an invalid conclusion.
13798000	13806000	If you pay attention to the URL, you'll see that URL for slash was prefixed to what we tried to access.
13806000	13812000	This is telling NB viewer to use the URL handler to load the following content.
13812000	13821000	Of course, notebooks for slash jd frederick four slash one dot ipynb is not a domain name and is not located within a public top level domain name.
13821000	13826000	So it makes sense that URL would fail to load this content.
13826000	13831000	Instead, what we need to do is change the URL prefix to local file.
13831000	13833000	And that will get the notebook to load.
13833000	13835000	We want to automate this though.
13835000	13846000	We don't want the go bar to not work and we would like the go bar to automatically translate to this canonical and be viewer local file format.
13846000	13850000	In the last video, we got the NB viewer local files provider working.
13850000	13854000	However, we weren't able to access it via the go bar.
13854000	13862000	In this video, we'll write a URI rewrite provider that will allow us to access local files easily from the go bar.
13862000	13867000	The first step is to open up Adam inside your NB viewer repository.
13867000	13871000	Next, open the NB viewer sub folder and inside that open providers.
13871000	13875000	Here, you'll see a list of the providers that are default with NB viewer.
13875000	13881000	The Dropbox provider has a URI rewrite, which is a good example for the rewrite that we're going to do.
13881000	13891000	Let's copy the handlers dot py file and create a sub folder inside the providers folder called X for X for is going to be the name of our plugin.
13891000	13893000	Paste the file inside there.
13893000	13898000	You can also copy the init file.
13898000	13901000	Now open the handlers dot py file that you copied.
13901000	13904000	Go ahead and remove the ipython header.
13904000	13911000	We want this URI rewrite to accept URIs of the form author forward slash notebook name.
13911000	13916000	We'll accept the notebook name either with or without an IPYNB extension.
13916000	13919000	The first step is to replace the first string in the tuple.
13919000	13922000	This string is the string that is used to search.
13922000	13926000	The second string is the string that replaces the search string.
13926000	13935000	Each group of the regular expression where a group is defined by parentheses can be accessed in the replacement string by using curly brackets.
13935000	13944000	So this zero refers to this first item here, whereas the one refers to this second group here.
13944000	13950000	Without explaining too much of regular expressions, I'll tell you that this matches a set of characters of variable length.
13950000	13955000	I'll remove this text here where this first group will match the author.
13955000	13957000	Add a forward slash.
13957000	13958000	Copy this first group.
13958000	13961000	This second group will match the notebook name.
13961000	13964000	And at the end, I'll add dot ipynb.
13964000	13969000	And I have to escape the dot because dot has a special meaning in regular expressions.
13969000	13974000	And add a question mark because we don't know if the user is going to write dot ipynb or not.
13974000	13985000	Now in the replacement string, I'll replace the URL with local file because local file is the canonical form of the URI accepted by the local file provider.
13985000	13992000	I'll also add notebooks because notebooks is the subfolder that sits inside the network folder.
13992000	14001000	The first value will be the author name, followed by the notebook name, and then we'll append a dot ipynb file extension.
14001000	14007000	Now let's save this and we'll go back to the terminal and try launching nbviewer.
14007000	14016000	But first make sure to set the environment variable that uses the custom templates that we created earlier.
14016000	14018000	Now let's try launching nbviewer.
14022000	14032000	To get nbviewer to use our URI rewrite, we use the double dash provider underscore rewrites.
14032000	14038000	The provider rewrites flag takes a full Python namespace to a rewrite provider.
14038000	14042000	You may be wondering why we had to edit nbviewer directly.
14042000	14044000	Well, we actually didn't have to.
14044000	14049000	We could have wrote our own Python package and then reference that Python namespace here.
14049000	14054000	However, writing a Python package is outside of the scope of this video series.
14054000	14058000	So for simplicity, we edit it nbviewer directly.
14058000	14061000	That allows us to piggyback on nbviewer's namespace here.
14061000	14069000	So to access our rewrite, we can use nbviewer.providers.exfer.
14069000	14073000	Lastly, we'll want to disable github and gis providers.
14073000	14079000	To do so, we'll set the URL provider as the only provider used by nbviewer.
14079000	14089000	We can do that using the double dash providers flag and setting that to nbviewer.providers.url.
14089000	14092000	Now that the server is launched, let's go to our web browser.
14092000	14098000	Let's try accessing the first notebook under my name here.
14098000	14100000	Looks like that worked correctly.
14100000	14107000	Let's go back to the homepage and try accessing it without the ipynb to make sure it still works.
14107000	14109000	Looks like that worked too.
14109000	14116000	The last thing we'll want to do is change the showcase so it shows notebooks that are actually hosted by us.
14116000	14120000	To understand how this is done, let's look at the source code of nbviewer.
14120000	14125000	Back inside Adam in the nbviewer repository, open up app.py.
14125000	14130000	If you scroll towards the bottom, you'll see where all the command line arguments are defined.
14130000	14134000	The command line argument that we're interested in is this front page argument.
14134000	14141000	This argument points to a JSON file which defines the content that will be used on the front page to render the showcase.
14141000	14148000	The default used by nbviewer sits inside the nbviewer repository under frontpage.json.
14148000	14149000	Let's open that.
14149000	14152000	Here you can see the links that we see when nbviewer runs.
14152000	14156000	Let's copy all the contents of this file.
14156000	14165000	And then in a new terminal window, let's cd into our custom nbviewer templates directory.
14165000	14170000	The reason why I had you open this directory is because it's where we're storing a lot of other custom things for our server.
14170000	14175000	We might as well store other content in here just to keep it all grouped in one place.
14175000	14178000	Create a new file called gallery.json.
14178000	14185000	Inside that file, paste the contents from the front page.json that we copied out of the nbviewer repository.
14185000	14191000	Now, looking at this file, we see that it has groups defined by this header attribute.
14191000	14196000	Since we're ignoring the notion of groups, let's get rid of all the other groups below.
14196000	14201000	When we set up the dummy directory, I only copied two files into my author directory.
14201000	14203000	So let's get rid of the third entry.
14203000	14208000	We'll give the first two names.
14208000	14215000	And then change the target to the canonical URL that points to the correct notebook.
14215000	14220000	The URL for the second notebook is almost the same, just the notebook file is different.
14220000	14226000	Now we could change the image as well, but I don't have any nice images for my test notebooks.
14226000	14229000	So I'm just going to leave the images as is.
14229000	14233000	I'm going to save this file and go back to the terminal.
14233000	14238000	Opening the tab of the terminal that's running nbviewer, I'm going to stop nbviewer by hitting Ctrl C.
14238000	14249000	I'm going to rerun the same command except this time I'll change front page to the full path of the JSON that specifies our gallery.
14249000	14252000	Now let's open the web browser to see if that worked.
14252000	14256000	Refreshing the home page, we see that my JSON was loaded.
14256000	14261000	Does this URL now points to the Jons notebook, even though it's still using the old screenshot?
14261000	14265000	Jons notebook 2 is also available, even though it's using the old screenshot.
14265000	14268000	Let's click on the link to see if it works.
14268000	14270000	Awesome, it looks like that worked.
14270000	14283000	If you want to find out more about nbviewer, visit the nbviewer repository at www.github.com forward slash Jupiter forward slash nbviewer.
14283000	14286000	In this chapter, I'm going to talk about temp nb.
14286000	14289000	It stands for Temporary Notebook Server.
14289000	14295000	Temp nb is a service that launches sandboxed ephemeral notebook servers on demand,
14295000	14299000	where ephemeral is defined as something lasting for a short time.
14299000	14302000	It's kind of like an interactive version of nbviewer.
14302000	14309000	Temp nb is useful for cases where you need to share notebooks that lose importance if they're not interactive.
14309000	14314000	Temp nb users can interact with your notebooks to see what they have to provide.
14314000	14318000	They can explore the data sets and write their own code inside the notebooks.
14318000	14325000	The changes that they make won't be persistent anywhere, so it's okay to open a Temp nb service to the public.
14325000	14332000	In my web browser, I'm going to navigate to Temp nb's website at www.github.com forward slash Jupiter Temp nb.
14332000	14335000	I'm now going to scroll down to the readme.
14335000	14340000	At the top of the readme, there's this very useful diagram for describing how Temp nb works.
14340000	14343000	Temp nb can be broken into a few pieces.
14343000	14347000	The user-facing piece is the configurable HTTP proxy.
14347000	14350000	This piece routes traffic to the correct sub-pieces.
14350000	14356000	The Temp nb orchestrator is what is used to launch the temporary notebook servers.
14356000	14359000	Docker is the technology that is used to containerize them.
14359000	14365000	Once a server is launched, the Temp nb orchestrator communicates to the configurable HTTP proxy,
14365000	14371000	telling it to route a certain subset of addresses to the correct Temp nb container.
14371000	14375000	Jupiter runs and maintains its own instance of Temp nb.
14375000	14378000	You can access it at try.jupiter.org.
14378000	14383000	The notebook itself is the same notebook that you're used to running on your local machine.
14383000	14387000	You can see that this notebook comes pre-populated with example notebook files.
14387000	14390000	In this video chapter, I'll show you how to do this.
14390000	14396000	I'll also show you how to customize your notebook server image so that it reflects your organization's needs.
14399000	14402000	In this video, I'll talk about installing Temp nb.
14402000	14410000	Temp nb, like nbViewer, can be installed either using a Docker image or in development mode from source code.
14411000	14419000	However, unlike nbViewer, it doesn't really make sense to install Temp nb from source code unless you're planning on developing Temp nb.
14419000	14425000	That's because all the common configuration that one would want to do can be done through custom Docker images,
14425000	14430000	the images that are launched by Temp nb as temporary servers.
14430000	14433000	First, let's open up the Docker quick terminal.
14433000	14438000	As we did in the last chapter, remember the IP address that's printed by Docker in green.
14438000	14442000	This is the IP address to use to access your server later.
14442000	14446000	The first step is to tell Docker to download Temp nb.
14446000	14451000	You can do that by running Docker pull Jupyter minimal.
14451000	14456000	Once that is finished downloading, you should have a full copy of the Jupyter minimal image.
14456000	14459000	Now you'll need to generate a random token.
14459000	14464000	This token will be used to authenticate with configurable HTTP proxy.
14464000	14470000	This command works on Linux and Mac operating systems to generate a random string of 30 characters.
14470000	14474000	However, you can use any random string you'd like for your token.
14474000	14479000	So on a Windows machine, you can use the equivalent command provided by that operating system.
14479000	14481000	Copy the random token.
14481000	14484000	Now we'll launch the configurable HTTP proxy.
14484000	14487000	To do so, I'll start with Docker run.
14487000	14492000	And then I'm going to tell Docker to use the network adapter of the host.
14492000	14496000	To do that, I'll use double dash net equals host.
14496000	14502000	Then I'll tell Docker to run in the background and print its ID using the dash D flag.
14502000	14508000	Next, I'll pass in the proxy token as an environment variable within the image.
14508000	14513000	To do that, I'll use the dash E flag, specify the environment variable,
14513000	14516000	and I'll paste the token that I generated in the last step.
14516000	14519000	I'll set the name of this container to proxy.
14519000	14523000	Then I'll specify the name of the container I want to launch.
14523000	14527000	And I'll specify default target.
14527000	14533000	Since this is the first time I've ran the command, Docker will load the image from its repository.
14533000	14538000	Once that is finished downloading and has launched, we'll launch the tempnb orchestrator.
14538000	14544000	To do so, we'll use the same type of command except we'll change the last couple pieces of it.
14544000	14547000	The name will change to tempnb.
14547000	14555000	And then we'll use the special dash V flag to tell the Docker image to bind the Docker client within itself.
14555000	14559000	This will allow the Docker image to spawn other Docker images.
14559000	14562000	Specifically, we'll bind the Docker sock.
14562000	14565000	And lastly, we'll specify the name of the image.
14565000	14568000	The orchestrator's name is tempnb.
14568000	14573000	Since this is the first time I've ran this command too, Docker will download the image.
14573000	14577000	Once that finishes, you should be able to visit your tempnb service.
14577000	14581000	In the web browser, navigate to the IP address you remembered from earlier.
14581000	14587000	At the end, append colon 8000 to visit port 8000.
14587000	14590000	This is the port that tempnb is listening on by default.
14590000	14593000	If all is well, tempnb should just work.
14593000	14598000	And accessing that address will spawn a notebook server for you in a Docker image.
14598000	14602000	In the top right hand corner, you'll see a hosted by Rackspace logo.
14602000	14604000	This is not actually being hosted by Rackspace.
14604000	14606000	This is being hosted on your machine.
14606000	14610000	It's just that the image that you downloaded, Jupyter 4 slash minimal,
14610000	14615000	is based on the same image that we use in the Jupyter deployment.
14615000	14622000	In this video, we'll look at how we can use custom Docker notebook images with tempnb.
14622000	14627000	Jupyter has a bunch of notebook images predefined in the Jupyter organization.
14627000	14634000	In your web browser, open up the Jupyter organization GitHub page at github.com forward slash Jupyter.
14634000	14640000	Once the page loads, scroll down and you'll see a repository called Docker stacks.
14640000	14641000	Open that.
14641000	14645000	This repository contains a bunch of Docker images for various tasks.
14645000	14648000	Let's go ahead and clone this repository.
14648000	14652000	To do so, copy the clone URL in the right hand column.
14652000	14656000	Now, in a terminal, navigate to your home directory.
14656000	14661000	Run, get, space, clone, and then paste the URL.
14661000	14665000	Once the cloning is finished, CD into that directory.
14665000	14668000	And let's open Adam.
14668000	14671000	Once Adam opens, open the minimal notebook directory.
14671000	14677000	This minimal notebook image is actually different than the minimal notebook image you used in the last video,
14677000	14680000	but the one that we used in the last video is actually deprecated.
14680000	14682000	And this is the modern replacement.
14682000	14686000	This image doesn't have a racks based logo in the top right hand corner.
14686000	14688000	Let's open up the Docker file.
14688000	14691000	This is the file that tells Docker how to build the image.
14691000	14696000	This from line is how Docker knows what this image inherits from.
14696000	14699000	The Debbie and Jesse image is used as a base.
14699000	14704000	You can see the list of Docker commands used to build this image.
14704000	14710000	At the end, we specify that the start notebook dot shell file should be executed.
14710000	14711000	Let's open that.
14711000	14714000	Here you can see how the notebook is launched.
14714000	14719000	The config file used for the notebook is stored under jupiter underscore notebook underscore config.
14719000	14723000	This is the same kind of config file that we looked at in the second chapter.
14723000	14726000	The files as they are in this repository are not a Docker image.
14726000	14728000	We have to first build them.
14728000	14731000	The build process is described in the make file.
14731000	14733000	Let's open that.
14733000	14737000	The help section describes how the build make file is used.
14737000	14743000	To build the minimal notebook, we just need to run build forward slash minimal dash notebook.
14743000	14746000	Let's try that within this directory.
14746000	14749000	First, Docker will download the base image.
14749000	14756000	It will take a while, but once it's done, your image will be built.
14756000	14759000	Now let's try using this image with Tempenby.
14759000	14762000	Start a Docker quick terminal.
14762000	14766000	Once the terminal starts, pay attention to the IP address like you did before.
14766000	14770000	We're going to run the same commands that we did in the video before the last video,
14770000	14775000	skipping the Docker pull command and changing some of the contents of the last command.
14775000	14778000	If you're continuing on from the last video,
14778000	14783000	make sure that you close all the existing Docker containers before trying to do this.
14783000	14786000	To do so, you can run the following command.
14786000	14796000	Docker space stop, dollar sign, and then in parentheses Docker space PS space dash a space dash Q.
14796000	14801000	I don't have any Docker containers running right now, so I get the help output.
14801000	14807000	After running that command, you want to run almost the same command, but replacing stop with RM.
14816000	14828000	The last command is almost identical, just changing from the name forward.
14828000	14833000	Once again, we'll tell it to connect to itself, so it's capable of launching other Docker images.
14833000	14837000	And here's where the command will start to change significantly from the last video,
14837000	14840000	in addition to the omitted name flag.
14840000	14844000	We'll start specifying the Python command that launches the orchestrator.
14844000	14847000	We'll specify the image that we just built.
14847000	14852000	Now the tricky part is that we'll have to tell the image how to launch the notebook server.
14852000	14855000	We do so using the double dash command flag.
14855000	14859000	We have to tell the notebook app what its base URL is.
14859000	14865000	The image will format the string and you can insert special variables using curly brackets.
14865000	14868000	Base path is one of those special variables that you can insert.
14868000	14874000	We'll tell it to listen to IP0.0.0.0, which will allow it to listen to anything.
14874000	14877000	Lastly, we'll specify the port that it's listening on.
14877000	14883000	Once you run that command, in your web browser, try accessing the Docker image.
14883000	14887000	If everything works, you should see a new notebook server.
14887000	14892000	This notebook server won't have a Rackspace logo in the top right-hand corner.
14892000	14895000	If you have troubles, most likely you mistyped something.
14895000	14901000	If you need to debug why it's not working, open up another Docker quick terminal.
14901000	14906000	When the Docker quick terminal launches, you can run docker ps-a.
14906000	14909000	This will list all the Docker processes that are running.
14909000	14914000	If you see one that says exit it with an exit code in parentheses,
14914000	14916000	you can look at the logs of that Docker image.
14916000	14926000	To do so, run docker logs and then copy the container ID, which is in the far left column, and paste it.
14926000	14932000	In one of the attempts I made earlier to run this long command, I misspelled orchestrate.
14932000	14936000	This caused the server to not run and me to receive gateway errors.
14936000	14943000	By looking at the logs, I could tell that that was the problem and was able to correct it quickly.
14943000	14949000	In the last couple of videos, we looked at launching TempNB using custom notebook image.
14949000	14956000	In the following videos, including this one, we'll look at creating our own custom notebook image for use with TempNB.
14956000	14959000	To get started, launch the Docker quick start terminal.
14959000	14963000	Once the terminal launches, pay attention to the IP address like you did before.
14963000	14967000	We'll be using that IP address to access TempNB.
14967000	14972000	In the last couple of videos, we used the Jupyter Docker stacks minimal notebook image.
14972000	14976000	We'll use that image as a base for our new custom image.
14976000	14980000	To do so, let's copy the image out of the repository.
14980000	14983000	I'll copy it into a directory called custom notebook.
14983000	14986000	This will be the name of the custom image that I'm going to create.
14986000	14990000	I'll then cd into custom notebook and I'll open Adam.
14990000	14998000	Once Adam opens, I'll open the config file inside custom notebook Jupyter notebook underscore config.py.
14998000	15003000	This is the configuration file that will be loaded by the Jupyter notebook inside the notebook image.
15003000	15007000	Recalling from an earlier chapter, I'm going to set the untitled notebook name.
15007000	15013000	This is an easy variable to set that we can use to quickly judge whether or not our config file is being loaded.
15013000	15020000	The variable is c.contentsManager.untitled notebook.
15020000	15023000	I'll set that to test. Now I'll save the file.
15023000	15028000	Next, I'm going to create a shell file that we'll use to build this image.
15028000	15032000	I'm going to copy the shebang from the start notebook file.
15032000	15035000	We'll call the new file build.sh.
15035000	15038000	I'm going to go back to my Docker quick start terminal.
15038000	15043000	I'm going to open Adam up inside the Docker stacks repository.
15043000	15046000	When Adam opens, I'm going to open the make file.
15046000	15050000	I'm going to scroll down to the build line so I can see how images are built.
15050000	15052000	I'll go ahead and copy this line.
15052000	15057000	I'm going to go back to the Adam that we opened up inside the custom notebook directory.
15057000	15061000	I'm going to paste this line inside the build.sh file.
15061000	15067000	I'm going to remove drgs and replace owner with JD Fredder.
15067000	15071000	You can use whatever you want here to identify yourself.
15071000	15076000	And I'm going to replace this notdir $at with the name of my notebook image.
15076000	15081000	I'll also get rid of the notdir $at at the end and the forward slash.
15081000	15087000	This tells Docker to build the contents inside the current directory.
15087000	15091000	Now I'm going to copy this shebang again.
15091000	15095000	And create a new file for testing this image with tempnb.
15095000	15097000	I'll call this file test.sh.
15097000	15099000	I'll paste the shebang.
15099000	15104000	And then I'll enter a command that causes all the images that are currently running in Docker to close.
15104000	15108000	It's important to note that this command is inside this file.
15108000	15112000	We don't want to run this file if there are Docker images on our system that we don't want to close.
15112000	15120000	The reason I'm adding this line is because it becomes tedious to constantly close Docker images each time you want to run your test.
15120000	15130000	To close all the images that are currently running, I'll use docker stop and dollar parentheses docker ps-a-q.
15130000	15136000	What this does is runs docker stop on every docker image that's currently running.
15136000	15141000	I'm going to copy this line, paste it below, and replace stop with rm.
15141000	15145000	This will do the same thing but remove the images instead of stopping them.
15145000	15149000	Next, I'm going to create a token for use with the HTTP config proxy.
15149000	15162000	I'll use export to define the variable token as head 30 characters long of dev urandom piped with xxd-p.
15162000	15166000	Next, I'll run the configurable HTTP proxy image.
15166000	15169000	To do so, I'll use docker run.
15169000	15179000	Double dash net equals host dash d dash e config proxy auth token equal the token variable.
15179000	15189000	Double dash name equals proxy image name jupiter configurable HTTP proxy space.
15189000	15200000	Double dash default dash target 127.0.0.1 port 9999.
15200000	15204000	I'm going to turn on word wrap so you can see the whole command.
15204000	15209000	Next, I'm going to launch the tempnb orchestrator image.
15209000	15213000	I'll start with the same command but deviate once I get to the name.
15213000	15224000	I'll use dash v bar run docker dot sock colon four slash docker dot sock to cause the image to connect to the docker client.
15224000	15233000	Next, I'll specify the jupiter tempnb image and the command python orchestrate dot pi.
15233000	15244000	I'll specify the image to jd fredder custom notebook and the command to start dash notebook dot sh.
15244000	15246000	This part's really important.
15246000	15252000	The minimal notebook image requires you to start the notebook server using start dash notebook dot sh
15252000	15257000	instead of running ipython space notebook or jupiter space notebook.
15257000	15262000	That's because if you run either of those, the notebook will be launched as root
15262000	15267000	and the notebook will be looking for the configuration file inside the root home directory.
15267000	15272000	However, the configuration file is installed into the jovian user's home directory.
15272000	15280000	So running start dash notebook dot sh does some special things that causes the notebook to launch the server as the jovian user.
15280000	15283000	I'll have to pass some commands into the start notebook shell script.
15283000	15286000	To do so, I'll escape quotes.
15286000	15293000	Inside those quotes, I'll set the base URL,
15293000	15296000	allow origin,
15296000	15297000	and the port.
15297000	15303000	I'll save this file and go back to the docker terminal.
15303000	15308000	Now I'll navigate to the custom notebook directory that I created earlier
15308000	15311000	and I'll try running the build dot sh file I just created.
15311000	15315000	If you get a permission denied, it's probably because permissions aren't set correctly on the file.
15315000	15322000	You can do so by running chmod plus x build dot sh.
15322000	15324000	Looks like the image built successfully.
15324000	15327000	Now let's try running the test shell file.
15327000	15333000	We'll have to change the permissions of that as well.
15333000	15334000	Looks like that worked.
15334000	15338000	We get these help outputs because no images were running at the time.
15338000	15343000	The last two outputs are the grids for the images that were launched.
15343000	15345000	Let's go to the web browser.
15345000	15350000	Try accessing the tempnb server via the ip address that docker printed.
15350000	15352000	Looks like the server launched successfully.
15352000	15355000	Now let's see if the config worked.
15355000	15356000	Awesome.
15356000	15360000	It looks like the default notebook name is no longer untitled, but is test,
15360000	15365000	which implies that our config is being loaded.
15365000	15371000	In this video, we'll add custom content to our tempnb notebook custom image.
15371000	15377000	This process is very similar to the process that you use for adding custom content to your nb viewer deployment.
15377000	15383000	That's because the notebook itself uses ginga2, like nb viewer, to do its templating.
15383000	15386000	First, let's start the docker quick start terminal.
15386000	15391000	Pay attention to the ip address that is listed, for that's the ip you'll use to access docker.
15391000	15397000	Let's go ahead and navigate into our custom notebook directory and open atom.
15397000	15401000	The first thing we'll do is create a page.html template.
15401000	15406000	This template will override the page.html template of the notebook.
15406000	15412000	Inside the page.html template, we'll extend the base template of the notebook.
15412000	15416000	Next, we'll override the header underscore buttons block.
15416000	15420000	This block exists at the top of the notebook pages.
15420000	15422000	We can use this to add our own logo.
15422000	15425000	We'll go ahead and add an O'Reilly logo here.
15425000	15427000	We have two options to do this.
15427000	15432000	We could either add the O'Reilly picture to our custom notebook image,
15432000	15436000	or we could host it externally and reference it here.
15436000	15445000	For tempnb, it's better to host your images and other static content externally to the images that are launched by the orchestrator.
15445000	15449000	That's because the notebook server uses tornado to host its files,
15449000	15454000	and tornado isn't as fast as other servers like engine x or Apache,
15454000	15458000	which are even slower than services like CDNs.
15458000	15463000	So what we'll do is open our web browser and get the link for the O'Reilly image.
15463000	15466000	www.oreilly.com
15466000	15472000	Once the page loads, right-click on the image and say copy image URL.
15472000	15474000	Then go back to atom.
15474000	15477000	Now on the header buttons block, add an image tag.
15477000	15481000	Set the source of that image tag to the link that you copied from O'Reilly.
15481000	15483000	Save the page.
15483000	15486000	Now we'll need to copy this template into our image.
15486000	15488000	To do so, open your docker file.
15488000	15490000	Scroll down to the bottom.
15490000	15494000	The first thing you'll need to do is create a directory that contains templates.
15494000	15500000	To do so, we're going to copy this line that creates the dot Jupyter directory inside the user directory.
15500000	15503000	We'll put our template directory inside that.
15503000	15505000	We'll call it custom.
15505000	15508000	We'll then need to copy the file into that directory.
15508000	15511000	Go ahead and copy the line that does the notebook config.
15511000	15518000	Change notebook config.py to page.html and update the path to custom.
15518000	15520000	Save the file.
15520000	15524000	Lastly, you'll need to go into your Jupyter notebook config.py file.
15524000	15531000	Inside here, below the untitled notebook line, set the extra template paths variable of the notebook app.
15531000	15534000	This variable accepts a list, a path.
15534000	15537000	Give it the path to your custom template folder.
15537000	15539000	And then save the file.
15539000	15541000	Now go back to your docker terminal.
15541000	15545000	And inside here, run the build script again.
15545000	15550000	Once the build script finishes, you can run the test script.
15550000	15552000	Now go back to your web browser.
15552000	15554000	Try accessing tempnb.
15554000	15558000	If all goes well, you should see the O'Reilly logo on the top of the header bar.
15558000	15562000	You could style this better by using css in your template page.
15562000	15565000	But the point here is not to make something that looks good.
15565000	15571000	It's just to show you how to get static content into your tempnb images.
15571000	15576000	In this video, I'm going to talk to you about setting limits on your tempnb service.
15576000	15579000	And then briefly, I'll talk about security.
15579000	15581000	To get started, open up a terminal.
15581000	15585000	Then navigate into the custom notebook directory.
15585000	15589000	This is the directory that contains the custom image we've created.
15589000	15591000	Now open Adam.
15591000	15595000	Once Adam opens, open the test.sh file.
15595000	15600000	This is the file that contains the lines that can launch this image in tempnb.
15600000	15602000	In a real deployment, you could use these same lines.
15602000	15606000	Just remove the two docker stop and docker rn lines.
15606000	15611000	I'm going to enable word wrap so you can see the whole commands.
15611000	15614000	The last command is the command that launches the orchestrator.
15614000	15619000	We pass in a command into the image using the double dash command flag.
15619000	15623000	You can tell the orchestrator what image to use using the double dash image flag.
15623000	15626000	There are also additional flags.
15626000	15632000	For example, if you need to limit the number of CPUs any particular container can use,
15632000	15636000	you can use the double dash CPU underscore shares flag.
15636000	15641000	And this accepts an integer value for how many CPUs are allowed.
15641000	15647000	For example, we could limit each image to using two CPUs at most by doing equals two.
15647000	15652000	The next useful flag for limiting is the coal period flag.
15652000	15660000	This flag accepts an integer in seconds that determines how often containers are examined for their age
15660000	15662000	and then collect it if old enough.
15662000	15665000	The default for this is 600 seconds.
15665000	15667000	This is 10 minutes.
15667000	15671000	We could make this faster, for example, by doing 300 seconds.
15671000	15679000	Cold timeout is the variable that sets how long it takes for a container to be sitting idle that it will get cold.
15679000	15683000	The default for this is 3600 seconds.
15683000	15687000	This variable is also an integer specified in seconds.
15687000	15692000	We can half that time by setting it to 1800 seconds.
15692000	15700000	We can also set a limit on the amount of memory each container is allowed to use by setting mem underscore limit.
15700000	15706000	This accepts a string specifying the amount of memory that each container is allowed to use.
15706000	15711000	It defaults to 512M for our 512 megabytes.
15711000	15716000	We can half this by setting it to 256M.
15716000	15721000	The last important flag I would like to mention is the pool size flag.
15721000	15728000	This flag accepts an integer which specifies how many child docker containers can be launched by the orchestrator.
15728000	15733000	We can think of this as a limit as how many users can use Temp Add B at any given time.
15733000	15735000	The default for this is 10.
15735000	15738000	We can limit it to half that by setting it to 5.
15738000	15746000	Note that these flags are all set outside of the double dash command because they're not actually getting passed into the image but to the orchestrator itself.
15746000	15749000	Lastly, let's talk a little bit about security.
15749000	15753000	Go ahead and open up your Jupyter underscore notebook underscore config.
15753000	15760000	You see here in this configuration file that there's a flag for HTTPS encryption and password.
15760000	15767000	This is the same HTTPS encryption and password that you used in the earlier chapter where you learned how to deploy the Jupyter notebook.
15767000	15774000	This may be useful to you but take note that this is not affecting the orchestrator itself.
15774000	15780000	So any random user can still access your deployment of Temp Add B and launch containers.
15780000	15786000	They just may not be able to take advantage of those containers if they don't have the appropriate credentials to log on to them.
15786000	15795000	This means that those people could still spawn up a bunch of containers and use your entire pool even if they're not authenticated.
15795000	15802000	This is a limitation of Temp Add B as the Temp Add B orchestrator does not yet have a password mechanism.
15802000	15808000	You could, however, wrap the orchestrator in your own password at Proxy.
15808000	15812000	In this chapter, I'll teach you about Jupyter Hub.
15812000	15823000	The technical definition of Jupyter Hub is that it's a multi-user server that manages in Proxy's multiple instances of the single user Jupyter notebook server.
15823000	15830000	A less technical definition is that Jupyter Hub is a multi-user version of the Jupyter notebook.
15830000	15841000	Jupyter Hub is a Python 3 only application, but that doesn't mean that the kernels that are ran by the notebook servers launched by Jupyter Hub are restricted to Python 3 only.
15841000	15845000	In other words, the user isn't restricted to Python 3.
15845000	15858000	Jupyter Hub is comprised of three main pieces, the multi-user hub, the configurable HTTP proxy, and the multiple single user notebook servers that are launched by the hub.
15858000	15862000	When you start Jupyter Hub, you're actually starting the hub application.
15862000	15866000	The hub application then spawns the configurable proxy.
15866000	15871000	The proxy forwards all requests on the root domain to the hub.
15871000	15874000	The proxy is what's exposed to the internet.
15874000	15882000	The hub then authenticates the user when the user connects, and the hub will launch a single user notebook server for that user.
15882000	15892000	It then configures the proxy to route all requests on the root domain forward slash the username to that new single user notebook server that it launched.
15892000	15895000	Jupyter Hub is highly configurable.
15895000	15897000	The authentication is configurable.
15897000	15907000	We're going to look specifically at the O authenticator extension, which allows you to use GitHub authentication with Jupyter Hub, but you could write your own authenticator.
15907000	15913000	This is useful if your organization uses a specialized authentication scheme.
15913000	15915000	Second, you can configure the spawning.
15915000	15919000	In other words, you can configure how single user notebook servers are launched.
15919000	15928000	We're going to look specifically at the Docker spawner, which is a tool that allows Jupyter Hub to spawn the single user notebook servers using Docker.
15928000	15932000	And lastly, you can configure the spawn notebook itself.
15932000	15936000	By default, Jupyter Hub launches the notebook that's installed on the local machine.
15936000	15949000	If you're using something like the Docker spawner, you can customize the notebook by using the techniques described in the last chapter where we created a custom Jupyter notebook Docker image.
15949000	15954000	In the following videos, we'll look at three ways to install Jupyter Hub.
15954000	15959000	The first is a completely vanilla installed directly from package managers.
15959000	15963000	The second is a vanilla install with the Docker launcher extension.
15963000	15978000	And the last is a more complex install that uses a combination of the Docker launcher extension and Docker swarm to handle more users to redistribute the demand across multiple machines in order to handle a higher user load.
15978000	15987000	First, let's remove the dot Jupyter folder that we created in the earlier chapter where we examined installing the vanilla notebook.
15987000	15992000	We need to do this because Jupyter Hub relies on the local notebook install.
15992000	15997000	We don't want to dirty our new Jupyter Hub install with the config options that we set earlier.
15997000	16012000	On the other hand, later you'll find configuration of Jupyter Hub to be easy because configuring the notebook servers that get launched by Jupyter Hub is the exact same procedure that we examined earlier using traitlets in the config machinery to config the vanilla notebook.
16012000	16018000	All the configuration that you have for the vanilla notebook will apply to the vanilla notebook that's launched by Jupyter Hub.
16018000	16021000	You'll want to verify that you have Python 3 on your machine.
16021000	16025000	You can do so by running Python double-dash version.
16025000	16030000	If your system does not print Python 3, try Python 3 double-dash version.
16030000	16043000	If that too does not work or does not print version 3, then you'll want to revisit chapter 1 video 3 where we talk about prerequisites and you'll want to make sure that you have Python 3 installed on your machine.
16043000	16046000	Next, let's look at the version of Node that we have installed.
16046000	16050000	You can do so by running npm-v.
16050000	16053000	I have version 3.4.1 installed on my machine.
16053000	16061000	If your version is lesser than that, you can update it by running sudo npm install-g npm.
16061000	16069000	What this will do is cause npm to uninstall itself and then install the latest version of itself in its place.
16069000	16074000	If this command fails partway through, you'll find that you need to reinstall Node and npm.
16074000	16078000	The first thing we'll install is the configurable HTTP proxy.
16078000	16083000	You'll recognize that name from the earlier chapter where we looked at deploying tempnb.
16083000	16088000	However, in that chapter, we used a configurable HTTP proxy docker image.
16088000	16093000	So we didn't actually install the configurable HTTP proxy on the local machine.
16093000	16097000	Because we're installing Jupyter Hub on the local machine, we'll need to do that here.
16097000	16110000	Go ahead and run sudo npm install-g where this dash-g flag installs the software globally configurable HTTP proxy.
16110000	16113000	Once that is finished, you'll want to install Jupyter Hub.
16113000	16118000	You can do so by running pip3 install Jupyter Hub.
16118000	16122000	By running pip3, we force the python3 pip to be used.
16122000	16127000	If you receive a permission denied error, go ahead and prepend the command with sudo.
16127000	16130000	Now you can try launching Jupyter Hub.
16130000	16141000	If you have an error like this, go ahead and uninstall Jupyter Hub and then reinstall it.
16141000	16145000	When you first run the hub, you may get an error that there's a bad configuration file.
16145000	16149000	You can fix this by running the command that is recommended.
16149000	16153000	This command will generate a configuration file for you.
16153000	16156000	Say yes when asked if you want to override the file.
16156000	16159000	Now try launching the hub.
16159000	16165000	If everything is successful, you should get a message saying that the hub is now running at localhost 8000.
16165000	16169000	In your web browser, try accessing that.
16169000	16171000	Awesome, looks like that worked.
16171000	16177000	Now you should be able to log on using your local system credentials.
16177000	16181000	Now that Jupyter Hub is installed, let's see how it works.
16181000	16184000	You can launch Jupyter Hub by running Jupyter Hub.
16184000	16188000	When Jupyter Hub launches, you'll notice a couple warnings.
16188000	16194000	The first warning is that the config proxy auth token had to be generated by Jupyter Hub.
16194000	16197000	You can bypass this warning by setting that variable explicitly.
16197000	16204000	In the future, when you decide to use extensions with Jupyter Hub, such as NBGrader, you'll need to set this token.
16204000	16210000	This token is how applications can communicate with the configurable HTTP proxy.
16210000	16219000	NBGrader, for example, adds a handle to the configurable HTTP proxy that allows graders to access notebooks with a special interface.
16219000	16225000	The second warning you'll see is that no admin users are defined, so the admin interface will not be accessible.
16225000	16227000	We'll go ahead and ignore that for now.
16227000	16229000	Switch to your web browser.
16229000	16231000	We'll access the address listed here.
16231000	16234000	It should be available at localhost8000.
16234000	16238000	When you access that address, you'll be presented with a login screen.
16238000	16242000	Jupyter Hub uses PAM as a default authentication method.
16242000	16247000	This means that to access Jupyter Hub, you use credentials on the host machine.
16247000	16251000	In other words, you use your current account name if you're running it locally.
16251000	16256000	The password is the same password for the account on the host operating system.
16256000	16260000	When you sign in, you'll be presented with your own notebook server.
16260000	16265000	In the top right-hand corner, you'll see a button for a control panel and a button to log out.
16265000	16267000	Go ahead and click on control panel.
16267000	16272000	In the control panel, you'll see an option to stop your server or access your server.
16272000	16274000	Go ahead and stop your server.
16274000	16277000	You'll also see an option to administrate Jupyter Hub.
16277000	16278000	Click on that.
16278000	16284000	Here, you'll see a screen that allows you to define new users and remove users.
16284000	16287000	Here, I'm going to remove JD Fredder.
16287000	16293000	You can also change users from admin to normal users.
16293000	16297000	Go ahead and log out.
16297000	16302000	In this video, I'll show you how to install the Jupyter Hub Docker Launcher extension.
16302000	16305000	Jupyter Hub is a highly configurable application.
16305000	16310000	Even the way that Jupyter Hub launches single-user notebook servers is configurable.
16310000	16317000	The Docker Launcher extension allows you to force Jupyter Hub to launch the single-user notebook servers as Docker images.
16317000	16323000	With this extension, you can launch any custom Docker image that you have that contains a Jupyter notebook server.
16323000	16328000	If you want Jupyter Hub to launch the single-user notebook servers using something other than Docker,
16328000	16330000	you can write your own extension to do so.
16330000	16333000	To get started, open up a Docker Quick Term.
16333000	16337000	Once the Docker Quick Terminal launches, pay attention to the IP address.
16337000	16340000	You'll need that IP address for later during configuration.
16340000	16344000	Before we get started, we should close all existing Docker images,
16344000	16348000	just to make sure that none are running that will conflict with what we're trying to do.
16348000	16357000	To do so, you can run Docker, stop, and then dollar parentheses, Docker PS-A-Q,
16357000	16366000	semicolon, Docker RM, dollar parentheses, Docker PS-A-Q.
16366000	16371000	Now, you can get the Docker Spawner extension source code by cloning it from GitHub.
16371000	16381000	To do so, run git clone, HTTPS, github.com, Jupyter Docker Spawner.git.
16381000	16385000	You want to run this inside the directory that you want to install the source code to.
16385000	16388000	I'm doing it inside my home directory.
16388000	16391000	Now, CD into that repository.
16391000	16397000	Run pip3 install-r requirements.txt.
16397000	16400000	This will install the requirements of the Docker Spawner.
16400000	16404000	Don't forget to add a sudo in front if your permissions require it.
16404000	16411000	Next, run python3 setup.py install.
16411000	16417000	Lastly, run sudo pip3 install-e.
16417000	16423000	Now, we'll need to change our Jupyter Hub config file so it launches using the Docker Spawner.
16423000	16429000	CD back out into your home directory or whatever directory that you launched Jupyter Hub from.
16429000	16431000	I launched Jupyter Hub from my home directory.
16431000	16437000	Once there, open up Jupyter Hub underscore config.py file in your text editor.
16437000	16440000	I'm going to open it in Adam.
16440000	16451000	Below the first line, add c.jupyterhub.spawner underscore class equals dockerspawner.dockerspawner.
16451000	16453000	Pay attention to the capitalization.
16453000	16457000	This tells Jupyter Hub to use the Docker Spawner.
16457000	16469000	Next, add c.dockerspawner.use underscore docker underscore client underscore env equal to true.
16469000	16473000	This allows the Docker Spawner to work with the Docker Quick Terminal.
16473000	16482000	Next, add c.dockerspawner.tls assert underscore hostname equal to false.
16482000	16488000	This is also required to use the Docker Quick Term in your custom image with Docker Spawner.
16488000	16497000	Next, add c.dockerspawner.container underscore image equal the name of your custom image.
16497000	16501000	I'm going to use the image that I created earlier in the tempnb chapter.
16501000	16503000	Use your custom image here too.
16503000	16507000	Now go ahead and save the file.
16507000	16511000	Now cd into your custom notebook image directory.
16511000	16515000	This is the same directory from the chapter where we investigate at tempnb.
16515000	16517000	Open Adam.
16517000	16520000	Open up your Jupyter underscore notebook config file.
16520000	16534000	Inside here, add c.notebookapp.baseurl equals os.environ.jpy underscore base underscore url.
16534000	16539000	This configures the notebook server to listen to the URL that's a subset of Jupyter Hub.
16539000	16542000	Go ahead and click save and then close Adam.
16542000	16545000	Now you should be able to launch Jupyter Hub.
16545000	16548000	Navigate back to the directory that you launched Jupyter Hub from.
16548000	16550000	Mine is the home directory.
16550000	16562000	Type Jupyter Hub double dash Docker Spawner.container underscore ip equals 192.168.99.100.
16562000	16568000	Replace this IP address with the IP address that was listed by Docker when you launched the Quick Term.
16568000	16570000	Click return.
16570000	16572000	Once the server launches, go to your web browser.
16572000	16574000	You should be prompted with a login.
16574000	16577000	Login using your local credentials.
16577000	16581000	Once you log in, you should see your custom notebook image running.
16581000	16585000	This means that everything we did worked.
16585000	16590000	In the last video, we set up Jupyter Hub with the Docker Spawner extension.
16590000	16594000	This made Jupyter Hub spawn notebook servers inside Docker images.
16594000	16599000	In this video, we'll take it a step further and customize how Jupyter Hub does authentication.
16599000	16606000	Jupyter Hub has a notion of authenticators, which allow you to change how users authenticate with Jupyter Hub.
16606000	16612000	You can use authentication methods ranging from traditional, used in academia and in the industry,
16612000	16618000	to more specialized methods, like using social networking or social media authentication.
16618000	16622000	In this video, we'll look at using GitHub's authentication system.
16622000	16628000	There's an extension called the O authenticator, which was written for Jupyter Hub to allow us to do this.
16628000	16630000	First, open up a Docker Quick Terminal.
16630000	16635000	Once the Quick Terminal launches, make sure to close all images that are already running on the machine.
16635000	16645000	Include it in the Docker Spawner extension repository is an example of how they use the Docker Spawner with the O authenticator.
16645000	16647000	We'll use that as a starting point.
16647000	16651000	First, you want to copy your Jupyter Hub config into that directory.
16651000	16656000	My Jupyter Hub config is located in my home directory because that's where I launched Jupyter Hub.
16656000	16661000	So I'm going to copy that from my home directory into that repository example folder.
16661000	16664000	Next, cd into that directory.
16664000	16670000	Now run sudo pip3 install get plus HTTPS
16670000	16678000	forward slash forward slash github.com forward slash Jupyter forward slash O authenticator dot get.
16678000	16682000	When that finishes, you want to create a user list file.
16682000	16684000	Let's open up Adam inside this directory.
16684000	16689000	Once Adam opens, go ahead and right click and create a user list file.
16689000	16694000	Inside the user list, add GitHub user names that you want to have access to your server.
16694000	16696000	Don't forget to add your own.
16696000	16699000	I'm going to add Brian and Kyle, my colleagues.
16699000	16704000	Make yourself an admin by adding a space and admin after your account name.
16704000	16710000	Save the file and go ahead and close Adam for now.
16710000	16719000	In your web browser, go to github.com forward slash settings forward slash applications forward slash new.
16719000	16722000	When that page loads, give your application a name.
16722000	16724000	I'm going to call mine Jupyter Hub.
16724000	16729000	This is the name that users will see when authenticating while connecting to your Jupyter Hub instance.
16729000	16733000	Set the homepage URL to the Jupyter Hub URL.
16733000	16737000	This should be for now local host 8000.
16737000	16739000	Go ahead and copy that URL.
16739000	16743000	Paste it below where it says authorization callback URL.
16743000	16748000	Then append hub forward slash OAuth underscore callback.
16748000	16750000	Now click register application.
16750000	16752000	Go back to your desktop.
16752000	16755000	Launch a Docker quick start terminal.
16755000	16759000	Once the quick start terminal launches, pay attention to the IP address that's listed.
16759000	16760000	You'll need this later.
16760000	16763000	Now let's CD into the Docker Spanner directory.
16763000	16767000	Inside that, CD into the OAuth examples directory.
16767000	16769000	Now open Adam.
16769000	16774000	Once Adam opens in that directory, open the Jupyter Hub config file.
16774000	16778000	Below the container image line, you're going to need to add a new line.
16778000	16789000	Add C dot Jupyter Hub dot authenticator underscore class equal to in quotes OAuth.github OAuth.
16790000	16799000	Now below that line, add C dot GitHub OAuth authenticator dot OAuth underscore callback underscore URL
16799000	16805000	equal to the URL that you provided for the authentication callback while creating the application on github.com.
16805000	16809000	I'm going to go back to my web browser to show you what that URL is.
16809000	16811000	At the bottom of the page, you'll see it.
16811000	16814000	Go ahead and copy that.
16814000	16821000	Now below that line, add C dot GitHub OAuth authenticator dot client underscore ID
16821000	16825000	equal to the client ID provided to you by github.
16825000	16829000	It's located at the top of the page.
16829000	16835000	Now below that line, add C dot GitHub OAuth authenticator dot client underscore secret
16835000	16841000	equal to the secret provided to you by github.
16841000	16846000	Lastly, on the line below that, you'll need to set yourself as an administrator.
16846000	16854000	To do so, set C dot authenticator dot admin underscore users equal to
16854000	16858000	and then in square brackets and quotes your account name.
16858000	16860000	This is your GitHub account name.
16860000	16862000	Now save the file.
16862000	16870000	Back in the terminal run dash run dot sh double dash Docker spawner dot container IP
16870000	16874000	equal to the IP address listed in green.
16874000	16878000	Now in your web browser, navigate to the Jupyter Hub instance.
16878000	16881000	It should be at local host colon 8000.
16881000	16885000	Once you arrive on that page, click the sign in with GitHub button.
16885000	16887000	You should be asked to authorize the application.
16887000	16889000	Click authorize.
16889000	16892000	You'll then be redirected back to your Jupyter Hub instance.
16892000	16897000	You can click my server to access your server or admin to administrate Jupyter Hub.
16897000	16899000	I'm going to click on my server.
16899000	16904000	Note that our custom image is still being loaded.
16904000	16909000	In the previous videos, we were able to get Jupyter Hub working with GitHub OAuth
16909000	16912000	and a custom Docker image.
16912000	16918000	In this video, we'll look at how we can enable our users to share files across their different accounts
16918000	16920000	inside the Jupyter Hub instance.
16920000	16925000	To do so, we'll mount a shared directory on the host operating system.
16925000	16927000	We'll do this two ways.
16927000	16932000	One, we'll mount it as read only for content that all users should be able to see,
16932000	16934000	but not necessarily edit.
16934000	16938000	Two, we'll mount it as read write so users can have a shared directory
16938000	16941000	from which they can save files and fetch files.
16941000	16944000	To get started, open up a Docker quick terminal.
16944000	16951000	Once your Docker quick terminal launches, go ahead and make sure no Docker images are currently running.
16951000	16955000	Inside my home directory, I'm going to create two shared folders.
16955000	16960000	One will be called shared underscore RW for shared read write,
16960000	16964000	and the other shared underscore R for read only shared.
16964000	16967000	You can use any directory that's accessible on your file system.
16967000	16970000	I'm using my home directory as a convenience.
16970000	16974000	Now, I'm going to copy two example notebooks into each of those folders.
16974000	16980000	I'm going to CD into the shared read write folder and launch a normal Jupyter notebook server.
16980000	16985000	When the notebook server launches, I'm going to go ahead and create a new Python notebook.
16985000	16988000	First, I'm going to change the name of this notebook.
16988000	16991000	I'll change it to test one.
16991000	16993000	Now, I'll give the notebook some content.
16993000	16996000	I'll make the first cell a markdown cell.
16996000	16999000	In the second cell, I'll add some code.
16999000	17001000	Now, I'm going to save this notebook.
17001000	17004000	Now, close the web browser and go back to the terminal.
17004000	17008000	In the terminal, I'll hit Ctrl C twice to close the server.
17008000	17011000	Now, I'll CD into the read only directory.
17011000	17016000	I'll launch the notebook server here too.
17016000	17023000	Once the notebook server launches, I'm going to create a new notebook.
17023000	17026000	I'll call this notebook test two.
17027000	17032000	I'll make the first cell a markdown cell.
17032000	17035000	In the second cell, I'll add some code.
17035000	17038000	Now, I'll save the file and close the web browser.
17038000	17043000	Next, I'll close the Jupyter notebook server by hitting Ctrl C twice.
17043000	17047000	Now, CD into the Docker spawner directory.
17047000	17053000	Inside there, I'll CD into the example's OAuth directory and open Adam.
17053000	17058000	Once Adam opens, I'll make sure my Jupyter Hub underscore config file is opened.
17058000	17067000	Then, below the admin users line, I'll add c.dockersponer.volumes equals a mapping of volumes.
17067000	17072000	The volume mapping is path on the local machine as the key
17072000	17077000	and as the value path that it should be mounted inside the Docker image.
17077000	17080000	I'll mount the read-write directory.
17080000	17087000	I'll have it mounted to home jovian for slash work for slash shared.
17087000	17094000	That's because home jovian work is the directory that's loaded by default inside the Docker image.
17094000	17098000	To mount read-only directories, the syntax is almost the same.
17098000	17102000	Go ahead and copy that line and paste a copy of it below.
17102000	17105000	On this line, we'll change the name of the path that's mounted.
17105000	17107000	Let's change it to read-only.
17107000	17112000	Likewise, we'll change the path on the parent system to the read-only directory.
17112000	17121000	The important part is that the key is not docersponer.volumes, it's actually dot read underscore only underscore volumes.
17121000	17124000	Once you make that change, go ahead and save the file.
17124000	17127000	Go back to the terminal.
17127000	17130000	Now, launch the server like you did before.
17130000	17134000	Don't forget to set the Docker spawner container IP trait.
17134000	17140000	The IP address is the IP listed by the Docker quick terminal in green when you launched it.
17140000	17143000	Once your server is launched, go back to your web browser.
17143000	17147000	In your web browser, navigate to your Jupyter Hub instance.
17147000	17152000	You may still be logged on to your other session from the earlier videos. That's okay.
17152000	17154000	Go ahead and click on my server.
17154000	17160000	When my server loads, you should see two folders, read-only and shared.
17160000	17162000	Go ahead and open shared.
17162000	17165000	Inside shared, you should see the test one notebook.
17165000	17167000	Go ahead and open that.
17167000	17169000	Make a change to this notebook.
17169000	17173000	It doesn't matter what change, just a change that you can see.
17173000	17175000	Then go ahead and try saving the notebook.
17175000	17180000	When you save, you should have seen the checkpoint flash up to the left of the kernel name.
17180000	17182000	Go ahead and close the notebook.
17182000	17184000	And try reopening it.
17184000	17186000	Looks like that worked.
17186000	17188000	Go ahead and close the notebook.
17188000	17190000	Go back to your home directory.
17190000	17193000	Then go inside the read-only directory.
17193000	17196000	Open up the test to that notebook.
17196000	17203000	When you open this notebook, you should see a notification that flashes quickly to the left of the kernel that says auto-save disabled.
17203000	17210000	You should also see an icon of a floppy with a red circle above it indicating that saving is disabled.
17210000	17212000	Try making changes to this file.
17212000	17214000	Any changes, it doesn't matter.
17214000	17216000	I'm going to remove this read-only.
17216000	17218000	Now I'm going to try saving.
17218000	17223000	When I save, I should see another notification in yellow that says the notebook is read-only.
17223000	17226000	Go ahead and close the notebook.
17226000	17228000	Reopen the notebook.
17228000	17231000	And you should notice your changes weren't saved.
17231000	17236000	This means that the read-only is working correctly.
17236000	17243000	In this video, we'll talk about how you can increase the performance of your Jupyter Hub deployment using EngineX.
17244000	17249000	EngineX will be used to host the static files of the Jupyter notebook.
17249000	17252000	The Jupyter notebook uses Tornado to host its web content.
17252000	17256000	Tornado is great for templating and hosting dynamic content.
17256000	17262000	However, it's slower than things like EngineX or Apache to host static files.
17262000	17269000	The methods described in this video can also be extended to redirect and host the static content on CDNs.
17269000	17272000	First, we're going to launch Jupyter Hub.
17272000	17275000	Go ahead and open up a Docker quick terminal.
17275000	17277000	Pay attention to the IP in green.
17277000	17281000	Then make sure that all Docker images are closed.
17281000	17288000	Next, navigate into the OAuth example folder inside the Docker spawner directory.
17288000	17295000	Launch Jupyter Hub by running the run.sh script.
17295000	17301000	Once Jupyter Hub launches, open up your web browser and verify that Jupyter Hub is running.
17301000	17305000	This should be available at localhost colon 8000.
17305000	17307000	Now, go back to the terminal.
17307000	17311000	Open up a new tab by hitting command T.
17311000	17315000	If you're on a machine that doesn't support tabs in your terminal, open up a new terminal.
17315000	17318000	Now, we'll install EngineX.
17318000	17321000	On OS X, you can do this using brew.
17321000	17326000	On Linux operating systems, you'll want to use the package manager of that system.
17326000	17330000	Typically, this is apt-get or yum.
17330000	17333000	If you're on OS X, go to your web browser.
17333000	17336000	Go to brew.sh.
17336000	17338000	This is the home page for brew.
17338000	17345000	If you don't have brew installed already, copy the line under the install home brew section inside the text box.
17345000	17349000	Paste that line in your terminal and execute it to install home brew.
17349000	17354000	I've already installed home brew on my machine, so I'm not going to demonstrate this for you.
17354000	17356000	Go back to your terminal.
17356000	17359000	Now, make sure that brew is up to date.
17359000	17362000	To do so, you're going to run brew update.
17362000	17365000	Now, we'll use brew to install EngineX.
17365000	17370000	Once brew is finished installing EngineX, run EngineX.
17370000	17372000	Now, go back to your web browser.
17372000	17378000	Access localhost 8080 to see if EngineX is running.
17378000	17382000	If EngineX is running, you should see a welcome to EngineX page.
17382000	17384000	Now, go back to your terminal.
17384000	17391000	Run EngineX-S to stop the EngineX service.
17391000	17393000	Now, go back to your web browser.
17393000	17396000	Go to github.com.
17396000	17403000	You should see the Jupyter Hub application that you registered earlier.
17403000	17405000	Click on that.
17405000	17413000	Now, change the port on the home page and the authentication callback URL to 8080.
17413000	17417000	Go back to your terminal.
17417000	17423000	Now, change the EngineX configuration file so that it proxies all requests to Jupyter Hub.
17423000	17427000	We'll also proxy the web socket connections to Jupyter Hub.
17427000	17433000	However, we'll intercept requests to static assets and host those directly using EngineX.
17433000	17439000	To edit the EngineX configuration file on OS X, run atom or open up
17439000	17447000	forward slash usr forward slash local forward slash Etsy forward slash EngineX forward slash EngineX.conf.
17447000	17450000	This is the path to the configuration file for EngineX.
17450000	17455000	If you're running EngineX on a machine other than OS X, this path may be different.
17455000	17462000	You'll have to refer to your installation method to figure out where the configuration file lives by default.
17462000	17466000	I'm going to open this file in atom.
17466000	17470000	The first thing we'll do is trim a lot of the comments and access lines.
17470000	17476000	This will allow us to focus better on what the contents of the configuration file should be.
17476000	17479000	I'm going to go ahead and remove this userNobody comment.
17479000	17483000	And also the log comments and process ID comment below.
17483000	17489000	I'll leave the events block and remove the log format comment, access log comment,
17489000	17497000	send file, TCP push, keep a live time out, gzip, all the way down to the server block.
17497000	17504000	Inside the server block, I'll leave the listen to port 8080 and server name local host lines.
17504000	17508000	I'll remove the lines down to the location forward slash.
17508000	17515000	Everything from here on out, I'll remove.
17515000	17520000	Now we'll configure all requests on root to forward to Jupyter Hub.
17520000	17524000	To do so, remove the lines inside the root block.
17524000	17532000	The first line you'll need is proxy underscore pass space, the address to Jupyter Hub.
17532000	17546000	Next, proxy underscore set underscore header, capital X dash capital R real dash all caps IP space dollar remote underscore add semicolon.
17546000	17556000	Next, you'll want proxy underscore set underscore header host with the capital H dollar HTTP underscore host semicolon.
17556000	17576000	In the last line you'll want in the root section proxy underscore set underscore header space capital X dash capital F forward it dash capital F four space dollar proxy underscore add underscore X underscore forward it underscore four.
17576000	17580000	Now copy these four lines that you just wrote.
17580000	17588000	Below the location root block, we'll need to add another location block, which will only intercept attempts to connect to WebSockets.
17588000	17595000	We have to handle WebSocket forwarding specially. This is a detail of engine X configuration.
17595000	17599000	To do so, write location space till day asterisk.
17599000	17604000	Then we're going to add a long regular expression that will look kind of funky.
17604000	17610000	This regular expression will be used to match the request path for WebSocket connections.
17610000	17617000	This first group is matching the user forward slash account name section of the URL.
17617000	17622000	The second group matches the WebSocket request specific to the notebook server.
17622000	17630000	Then suffix with forward slash question mark, and that's all you need for the regular expression that identifies WebSocket requests.
17630000	17635000	I'll turn on word wrap so you can see this whole line.
17635000	17639000	Inside that group, paste the four lines that you copied earlier.
17639000	17644000	You'll need to add some additional lines to get WebSocket forwarding the work.
17644000	17651000	First, you'll want to add proxy underscore HTTP underscore version space one point one.
17651000	17662000	Next, you'll want to add proxy underscore set underscore header space capital U upgrade space dollar HTTP underscore upgrade semicolon.
17662000	17672000	Next, add proxy underscore set underscore header space capital C connection space upgrade in quotes semicolon.
17672000	17680000	Last, you'll want to add proxy underscore read underscore timeout space 86,400 semicolon.
17680000	17686000	This is all you need to get content to forward to Jupyter Hub using engine X.
17686000	17691000	The last piece we'll want to add is to intercept request for static assets.
17691000	17695000	We'll want to host directly from the notebook directory.
17695000	17698000	But first, let's make sure that this is working.
17698000	17700000	Save the file.
17700000	17704000	Go back to your terminal.
17704000	17706000	Launch engine X.
17706000	17711000	If you get a message like this, it means there's something wrong with your configuration file.
17711000	17713000	It looks like mine has a typo.
17713000	17717000	Remote underscore add was supposed to be remote underscore adder.
17717000	17720000	I'm going to add an R and then save the file.
17720000	17722000	Now I'm going to go back to the terminal.
17722000	17725000	I'm going to try launching engine X again.
17725000	17729000	It looks like I missed another instance of remote add.
17729000	17733000	Also down here where I copied that content from the root.
17733000	17735000	I'm going to save the file.
17735000	17738000	I'll try launching engine X again.
17738000	17740000	Looks like it launched successfully.
17740000	17743000	Now I'm going to go to my web browser to verify that it launched.
17743000	17746000	I'm going to try accessing engine X.
17746000	17750000	If you recall correctly, it's at localhost 8080.
17750000	17755000	When I first access it, it looks as if what I did had no effect on engine X.
17755000	17760000	However, this is because my web browser is caching the contents of the last request.
17760000	17764000	By refreshing the page, I should see the right contents.
17764000	17769000	If refreshing the page doesn't fix the problem for you, you may need to clear your web browser's cache.
17769000	17773000	To do so, you'll have to follow steps specific to your web browser.
17773000	17776000	I'm going to go ahead and click on my server.
17776000	17781000	I need to validate that the proxy for the web sockets is working.
17781000	17784000	I'm going to open up the shared folder.
17784000	17786000	And then the test one notebook.
17786000	17790000	I'm going to try to run the cell with a change.
17790000	17796000	If it works, I know that the web sockets are forwarding correctly because the notebook is able to execute code.
17796000	17800000	I'm going to save and close this notebook.
17800000	17804000	Now I want to try to speed up this Jupyter Hub instance.
17804000	17807000	To do so, I'll have to intercept request the static.
17807000	17809000	I'm going to go back to my terminal.
17809000	17815000	The first thing I need to do is make sure that I have the static notebook files somewhere on my computer.
17815000	17817000	That way, Nginx can host them.
17817000	17820000	I'm going to navigate to my root directory.
17820000	17832000	Here, to clone the notebook, I'm going to run getClone, space, HTTPS, github.com, forward slash Jupyter, forward slash notebook.
17832000	17840000	Once the notebook clones successfully, I'm going to go back to the atom instance that I used to open the Nginx configuration.
17840000	17844000	Above the location root block, I'm going to add a new block.
17844000	17847000	This block will recognize requests for static assets.
17847000	17850000	To do so, I'll have to use a regular expression again.
17850000	17854000	This time, just use tilde, no asterisk.
17854000	17856000	The regular expression is as follows.
17856000	17857000	Forward slash.
17857000	17861000	And the first group is the user block, just like we did earlier.
17861000	17866000	And then the next block is forward slash static forward slash.
17866000	17879000	Lastly, parentheses dot asterisk to match all characters, forward slash, question, v equals, and then parentheses dot asterisk to match all characters.
17879000	17886000	Now, you're going to specify the root directory to the directory that we clone the notebook repository to.
17886000	17890000	When that is finished, save the file and return to your terminal.
17890000	17897000	Make sure to stop Nginx if it's already running by running Nginx, dash s, stop.
17897000	17901000	Then run Nginx to launch Nginx again.
17901000	17903000	Now let's go back to the web browser.
17903000	17907000	Navigate back to the root page, refresh the page.
17907000	17911000	If everything worked, the page shouldn't look any different.
17911000	17916000	However, this Jupyter logo, for example, is being hosted by Nginx.
