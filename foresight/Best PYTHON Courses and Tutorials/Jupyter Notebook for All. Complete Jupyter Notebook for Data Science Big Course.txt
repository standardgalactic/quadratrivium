Hi, and welcome to my course on the Jupyter Notebook for Data Science Teams.
I'll just give you a brief overview of the course so you can know what you're getting into.
So we'll start off by doing what you expect, getting Jupyter Notebook set up on your machines.
The second thing I'll go over then is Jupyter Notebook features.
So there's a lot of really interesting things going on with the Jupyter Notebook.
Some of the most useful functionality comes from what's called Notebook Extensions.
So I'll be going over a lot of those.
I'll be showing you how you can use both Python and R in the same Notebook.
So if you have some piece of your analysis that needs to be done on R,
it's very easy to actually do part of it in R, send it back to Python,
or even use R natively as the entire kernel that runs your Jupyter Notebook.
Also, different things like using SQL in the Notebook to query databases,
some really nice post-save hook functionality and widgets,
which I'll just demonstrate right here.
This is an example of a widget where I've created a function that generates data
according to some line, and the green line shows you the actual function
with some noise, and the blue dots are data that gets drawn from this distribution.
So the total number of points is 10, and as I click and drag this off to the right,
increase the number of points, and you can see that the fit dynamically moves around
as I add data points and actually slowly but surely converges on the underlying distribution.
So this is an example of a widget which is very easy to do
and provides lots of functionality for all kinds of data exploration.
Finally, I'll get into sharing the notebooks on a data science team,
so there's a lot of questions you have to consider for your particular situation,
so I'll try to give you a strategic framework so that you can actually identify
what kind of workflow makes sense for your situation.
There's various other things about conceptually breaking up your notebook structure
into lab notebooks and deliverable notebooks, and a lot more that goes into that.
Finally, I'll go through two different data science projects,
which will just demonstrate the principles I talked about above,
and you get to see it in an environment where I'm explaining and going through
how to actually do a data science project from end to end
using all the different techniques I was already talking about.
In this lesson, we'll be installing the R version of Python,
iPython, and Jupyter so that we can run the Jupyter notebook.
The way we run this is by installing the Anaconda distribution.
There are other ways of installing the iPython notebook,
but I recommend the Anaconda for its ease of use.
We'll first go over to any web browser and search for Anaconda Python.
What you'll see here is that the top link is the continuum Anaconda distribution.
Clicking on that takes you right to the downloads page,
and you see that you have different options.
You can get it with Windows, OSX, or Linux, whichever one you prefer.
Since I'm using OSX, I'll click on OSX.
The choice between using Python 2.7 and 3.4 is a tough one.
I'll be using 2.7 just because many of the legacy code bases still use 2.7,
but feel free to, if you're feeling experimental, to go to Python 3.4.
I'll be using this Mac OSX 64-bit one.
If you have a different system, please use that one.
Okay, great. Now that that package has downloaded,
install it by following the instructions on the screen.
So, clicking through and agreeing to various licenses.
And hopefully you get to this stage where it says the installation was successful.
Click Close.
We can close our browser as well.
At this stage, if we typed IPython, it still won't work.
One thing that the graphical interface does is actually adds a command to your bash profile.
So if I were to actually go into vi.bash underscore profile,
the first piece of the profile has been there from before,
but this was added by the Anaconda 2.3 installer,
which exports this path, shows you that the Anaconda folder has been created in my home directory,
and adds that to my path.
So if I type ls, you actually do see the Anaconda directory right here.
This makes it really easy to uninstall Anaconda if you want.
You can remove that line from that path in your bash profile,
and you can delete this folder, and everything should be gone off your system,
and you can use the old system defaults.
But now that we have this, we have to source our bash profile,
and we should now be able to type IPython.
Now that we've run IPython, we see we are running Python 2.7, in this case 0.10.
It's the Anaconda distribution, and this is IPython version 3.2.
So a lot of different numbers here.
The ones that are important are the Python version, which is 2.7,
and the IPython version, which is 3.2.
Now this is actually a bit behind.
So what we're going to do is hit Ctrl-D, and it says, do you really want to exit?
You can either type yes in return,
or you can type Ctrl-D a second time to actually exit it.
But what we'd like to do now is actually update to the most recent version of Anaconda,
so that we have the most recent version.
The way to do this is to type conda.
This is a new command line argument that you have.
The way that we update the Anaconda distribution is by typing conda in various packages.
So in this case, we want to conda, install conda.
What this should do is check to see various things.
This tells us the following packages will be updated.
Conda will go from this version 3.14 to 3.18, conda environment, and so on.
And we would like to set this up, and we will hit yes to this.
It actually takes quite a bit of time to install all of these things from source,
but most of these things are actually pre-compiled, so everything's already completed.
We'd like to also update a number of packages.
So let's conda install Jupyter, and now you can actually chain which packages you'd like to see.
So in this case, we'll install Jupyter, the pandas package, and scikit-learn.
So these are the packages to be updated.
We see that a number of things are going from ipython, which is important,
going from version 3.2 to version 4.0.
We'd like to proceed with that.
I'd like to just say a few words about why I find the Anaconda package to be a useful thing to use.
They make sure that all the packages you've installed will play nicely with each other.
So sometimes if you're using pip by itself, you can actually install dependencies that overlap each other out of place,
so they end up with a conflict when you're trying to import these libraries,
and Anaconda does a really nice job of making sure, checking those dependencies really well.
So now if we type ipython, we should see that we are running ipython 4.0, which we are.
Now we would like to actually check out the ipython notebook,
because that's the part where it really gets interesting.
So let's create an example directory, and from here we can type Jupyter notebook.
Just typing Jupyter notebook, a couple of things happened.
First of all, go back to the terminal.
I typed Jupyter notebook, and ran it.
And a notebook server started from the directory we are in, so a user's jbw example in this case.
So an ipython notebook, which has been started with a Jupyter, is now at this location.
HTTP colon slash slash localhost, in this case, quadruple 8.
And it says here this useful thing, control C to stop this server and shut down all kernels,
and you have to do it twice to skip the confirmation.
Now this starts the server running, and this terminal needs to stay open.
If we go back to this, what it runs is a web server, and it automatically by default opens your default browser.
So in this case, here we are at this location, localhost, quadruple 8, underscore 3.
And if we'd like to start a new notebook, we can click new, Python 2 notebook.
And again, this is referring to which version of Python you're running.
This is a 2 version 3 versus 3.
And we see now that we're running a Jupyter notebook, and we can start typing valid Python code,
and see the output from it right there.
Let's do something a little bit more interesting.
So we import the NumPy library as np, and then print numpy.a range 10.
So we see the first bit of Python code, and we know that we have the installation working just as we hoped.
In this video, we're going to start a GitHub repo to house a data science project.
First, we have to go to github.com.
If you don't have GitHub or Git setup, I highly recommend starting out by picking a username,
by giving your email and creating a GitHub account.
Now, if you have Windows or Linux or Mac operating system,
GitHub itself has a really nice tutorial for how to actually set up Git on your machine and for your setup.
So I recommend doing that.
So once you have a GitHub account, which is free, or if you already have one, click sign in, let's go to the next step.
So you've signed into GitHub, click the plus next to your name in the upper right hand corner, and start a new repository.
I prefer to start a new repository through the GitHub website itself, and then clone it to my local machine.
So that way, the remote connection has already set up.
And that's usually a stumbling block that can be a little bit annoying to overcome if you try to do it the other way around.
In this case, I'm going to be looking at some cold data.
So I'm going to call it cold exploration.
I'm going to give it a quick description.
I'm giving it the description a first look at the cold data.
I'm going to let it be public so anyone can see this repository.
So afterward, you can also see this if you'd like to go to it.
I will initialize this repository with a readme and I will add a gitignore.
A .gitignore file will let you ignore the machine generated code that comes along with various programming languages.
Now Python doesn't have that many, but there is usually a .pyc if you're running a Python file.
I also recommend having a license, especially if it's going to be public, so that you can share your repositories with others.
If you work for a company, obviously you have different licensing concerns.
So then click create repository.
It's as easy as that.
So now I have the cold exploration repository in my GitHub account.
From here, we would like to actually tie this account to our local machine.
So we can copy this text that's just to the right of this SSH tab.
Now, if it doesn't say SSH, if it says HTTPS, I would recommend clicking it to SSH.
And once you do that, copy the text that's in this text box.
Navigate with your terminal to a place that you think is an appropriate spot for this repository.
Type in git clone and paste the text that you just copied from the website itself.
So now we see the license and the readme files that we created on the website itself.
All right, so we have set up our GitHub repository and we've cloned it to our local machine and we're ready to start doing some data science.
In this lesson, I'm going to give you some extra intuition so you can understand what's happening when the Jupyter Notebook is running.
So in my terminal, if I type ls, I get to see the directories now underneath this current directory.
I see deliver, dev and source.
By typing Jupyter Notebook, I again start the Jupyter server.
My default browser is Chrome.
So again, we see those same three directories deliver, dev and source.
If we toggle back to the terminal, we can see several messages.
The first is the directory under which the notebook server has been started.
The second message is the number of active kernels.
The third message is the location that you can point your browser to to find this notebook.
And finally a message to say how to stop this server.
So going back to the notebook itself, if we click on the development branch, we see that there's no notebooks in here.
We can start a notebook by clicking on new and then clicking on Python 2.
So after clicking new, we see a new tab appear.
It's currently named untitled and the last checkpoint comes from a few seconds ago.
So let's type a few things.
So let's just say first as a variable is equal to the 5.0.
I execute that cell by holding down shift and hitting return.
When I do that, a new cell appears beneath it.
And as I type a second variable and label it say 23.0, again hitting return with the shift key produces another cell beneath it.
So I now have two variables, one named first and one named second.
And there's unsaved changes, which means if I lose this current browser, I will lose the changes that happened from the last time it was saved.
In this case, there's nothing that's been saved.
So let me go ahead and save this right now.
There's two ways of doing this.
One, typing command S if you're on the Mac or control S on Windows, which I just did.
Or you can click this save disk and it will also save it.
Now that it's been saved and there's no unsaved changes.
If I close this tab, or if I even close the whole browser by quitting the Chrome browser, all of the actual information has been stored in the kernel itself.
In other words, there's this kernel and everything that's happened with the kernel is being stored in state by this kernel.
This means if I open up a brand new version of Chrome and I go to where the notebook is running from the previous message before.
I copied that with control C, go back to Chrome browser and type it in here.
I go back to the exact view we had before.
Clicking on Dev, because that's where we were.
We actually see that the untitled IPython notebook is actually still running.
So if we click on this, we reattach the browser to the underlying kernel.
So if you have saved your notebook as you work and you close the browser, the work still remains in memory.
So if I say print first comma second, now we see the actual results is here.
So this is all been saved.
And that's one interesting thing that you should know is that the browser itself is a front end to what's really going on in the kernel.
Now, the converse to this is what happens if I completely close and shut down the server.
So I hit control C twice and shut down the kernels.
So all the kernels have been shutting down.
So going back to the browser, you see a message that says connection to the notebook server cannot be established.
Let's continue to try to reconnect, but you won't be able to run any code.
So in this case, if I try to do something, say I want to say first times second and execute this and shift enter, nothing happens.
And this is what you see when it's trying to connect to the kernel and it's failing to.
So this is the part where it actually needs to be running and needs to be continually talking to your browser.
Unfortunately, restarting the kernel does not give us back to where we were before.
So here I can try to reload this notebook and we still see what we had previously done, but watch what happens when I try to run this third cell.
The name first is not defined and the input name of the cell one to one.
So the kernel has completely restarted as you saw me do in the terminal, which means that now we have to start from the beginning.
And now everything has been stored in state saving it keeps it so that the kernel is now running in the background.
Hopefully that gave you a little bit of insight into what's happening.
The browser acts as a front end to this process that's running in the back end on this terminal.
The browser can be closed or blown away after you've saved all of the changes that you've made, but the kernel cannot be the kernel has to stay running if you want to keep the changes that you've done in memory.
In this lesson, we'll be talking about Jupyter notebook extensions.
Notebook extensions, as the name suggests, are extensions to the capabilities that the Jupyter notebook already comes with.
Now there's many different ways that you can actually extend the behavior of a Jupyter notebook.
I'm going to show you just two.
The first extension that I'll show you is called Jupyter Pivot Tables.
And if you click on this link here, you'll see that you go to this website.
Nicholas.crucian.com slash content 2015-09 Jupyter Pivot Tables.
And this allows for drag and drop pivot tables and charts.
And this write-up he has is actually a really nice write-up.
I recommend you reading and watching this video as well because he explains in some detail how you can actually use his extension.
To install this, all you have to do is go to this pip install command.
So copy pip install pivot table JS and run that command in your terminal.
So it's successfully installed the pivot table JS.
We go back to our notebook.
We can now run the cells that import both pandas and numpy and this command, which is from pivot table JS import pivot UI.
So that loaded correctly without any errors.
So we have now loaded this extension.
As of Jupyter 4.0, the preferred way of installing notebook extensions is through a pip install of the extension.
There are other ways of doing it as well and I'll show you a second way at the end of this video.
So let's actually take a look at some data with this pivot table extension.
Go to HTTPS colon slash slash data dot austintexas.gov.
In this website, we're going to go down and look at the restaurant inspection scores.
From this, we will export data.
The format we want is CSV.
We do want it to go into our data folder and it's called restaurant inspection scores.
Return to save that.
You can now close this tab and go back to our notebook.
Now that we've downloaded the CSV file, let's read it into pandas data frame.
I'm going to split the cell at the current place where it's blinking by typing control shift minus because I want to run this on just one cell by itself.
So reselecting that cell, I now hit shift and return and it correctly loads in the data frame.
So ways to check that is actually look at what the top of this data frame looks like.
We see that the restaurant name, the zip code, inspection date, the score, the address, facility ID and the process description actually looks like it's been read in correctly.
One thing you will notice is that the address has return characters in it because standard address has multiple lines.
And I'm actually going to be okay with that.
I'm going to say I would like to keep the address on one line in the data frame, not have that split up in different ways.
So let's take a look at what we get when we look at just the data frame itself is pivot underscore UI.
So we've imported pivot underscore UI up here in the first cell.
Let's execute the cell here.
Now a number of things happened in the background.
But what you end up seeing, we close this window down here that shows what we downloaded.
And I will actually toggle this toolbar for now.
So we can actually see a bit more.
We have the various columns of the data frame available on the top here.
So zip code, inspection date, score.
They are now dragable into these two places.
So let's do that.
Let's actually drag score along the top.
Let's see if there's a relationship between the zip code of a restaurant and the score.
So just by dragging those two columns in, we see that there are, for each of these zip codes, different scores that have been given to the restaurant.
Of course, a really good score is a 100 for the health score.
And we can actually scroll down and take a look at this data in a really intuitive way.
This looks pretty neat, but there's a lot of numbers going on.
It's actually kind of hard to read.
So one thing we can do is actually change the output type from table to something else like a heat map.
So this does the same data as we saw before, but it actually highlights the outlying points that are large with a darker color.
So now by eye, you can visually see the different relationships between these two variables.
I still think this is actually a little bit too big.
So I'll give one extra hint of taking data that actually has a lot of different granular pieces.
So let's take this very granular number across the top and bin it by something, let's say five to give us a little bit less granularity.
So here's some code that will actually do that.
So we're going to create a new column in this data frame called bin score for bin to score.
I'm going to use a pandas function called cut, which will now cut up these column df.score into bins that go from 30 to 100,
because no b.a range is not inclusive of the last data point and stepping by five.
So I'm going to run this cell.
It's going to create a data frame column named bin score.
And let's see what this one looks like.
We can drag bin score along the x-axis here and zip code along the y-axis.
We now see that the binned scores are now counting everything that has a zip code off to the left and any score within a certain range.
In a range of five.
We can then also take a look at this.
Instead of a table, we can look at it as a heat map.
You can also see if it looks okay in terms of a bar chart, for example.
And this doesn't quite make sense, but there's many different things that are different here.
You can actually look at tree map, for example.
So the various visualizations that are available to you may or may not make sense to the data that you're looking at.
But the availability of this is actually a really nice extension to the notebook capability that Jupyter already comes with.
Alright, so picking up on where the last video left off, notebook extensions.
We've already installed one extension.
This is the pivot table extension.
It's one of the extensions that I'd like to highlight for this video.
And it actually comes from this URL here.
I want to turn this into, let me just show you this real quick.
This code block is currently set as code.
I'd like to actually change it to mark down.
There's two ways to do that.
By clicking on the toolbar like I just did, or by typing M when this cell is selected in this gray circle right now.
If I type Y, it would turn back to code.
So I just typed Y, you saw the drop down menu turned to code.
And since it's still selected with a gray box, I can type M and it goes to mark down.
So I want it marked down so that when I click this, I can actually open a new tab.
So the Jupyter slideshow extension is this GitHub repo right here.
It has a lot of really interesting capabilities that I will be showing you at the very end of this course.
I'll be using the rise Jupyter slideshow extension to help us make a final slideshow presentation out of some of our data science projects.
To install this notebook extension, it says to simply run python setup.py install from the rise repository.
Now this means we actually have to first download this extension code.
So this isn't done in the usual PIP install way.
This is done by choosing the SSH version here at the top of the page.
Selecting this by clicking once.
GitHub actually makes it so that the entire thing is highlighted so you can now command C to copy this.
Go to your terminal.
And at this point, if you don't have a folder for your GitHub repositories that you just grabbed from wild, basically, I would recommend creating one.
So we type git clone and then paste in the code we had copied from GitHub web page.
So it clones into this thing called rise.
Let's CD into this.
We see various things here, the live reveal package, Jason and so on.
Let's go back to the GitHub page.
This is we simply run python setup.py install.
So I'll copy that code and paste.
Okay, so we have now installed this live reveal.js notebook extension.
So we go back to our notebook.
We see that there's an extra toolbar cell here, which has something different than we normally see, including a slideshow option.
And we actually need to restart this notebook to actually get the ability to make this look like a slideshow.
So let me go ahead and do that.
I'll do save and checkpoint and then close and halt.
I'll go back to where it's running in the terminal and hit control C once.
It says it's currently running, shut down the server, yes or no.
If you wait too long, it'll actually say I didn't see an answer.
So I'm just going to assume you did that by mistake.
We actually do want to quit this.
So we'll do control C twice.
You can have also selected why.
So we shut down all the kernels.
And this thing, if I reload this should not be available.
Let's rerun Jupyter notebook and it will give us a new version of this exact thing.
Click notebook extensions.
And now you still see this toolbar here with the currently being non the slideshow option,
but you also have a new button off to the right.
So let's actually click this and click the slideshow option.
If you'd actually like to turn one of your notebooks into a slideshow, the functionality is now at your fingertips.
And if you don't want to see all these extra cell toolbars, you can always put this back to none.
They should be saved.
So any clicking slideshow again, the fact that these are all slides has been preserved.
To look at the slideshow itself, we just click this button and type into the right gives you the different slides.
And one interesting thing about this or one thing that I think is very, very useful is that this is not just a rendered notebook of this.
This is actually a live cell that we can actually import and actually run new code.
So I just ran that piece of Python code during the slideshow while it's up.
So this is very nice for interactive demonstrations.
In this video, I'll be showing you how to actually query SQL databases from the Jupyter notebook itself.
A lot of enterprise data is stored in databases, so dealing with them will be part of your everyday job.
The Jupyter notebook makes it really nice to be able to document very clearly the SQL queries that you are creating.
So I recommend if you're going to be using SQL connections using a Jupyter notebook extension called ipython SQL.
It's installed by typing pip install ipython dash SQL.
Once you install that, you then have access to an extension that you can load by simply typing percent load extension space SQL.
When you run this cell, it actually loads in this magic extension.
It gives you a number of warning signs, but these are just warnings. The package will still work just fine.
This next line percent config will actually configure our ipython SQL extension.
And what this configuration does, we say SQL magic, we would like to automatically return results that are a table as a pandas data frame.
You don't have to do this, but I recommend it because most of the time you'd actually like to take the data you've queried the database from and transform it and use it in the standard data science tools.
So I'll run that command as well.
Next import pandas and for this demonstration, I'll be using SQL lite.
You can use any of the standard SQL engine connections.
I'm just using SQL lite because it's a simple and easy database to run with for an example.
This next cell, I'm actually going to create a table and put some data into it.
So if you're familiar with SQL, you'll notice that everything below the first line of this cell is SQL commands that leaves this top line to be explained.
So what we have here is a double percent sign and then SQL.
This is how you call what's called a cell magic.
If I hit tab while I'm at the end of these double percent sign, I will see a little pop up that tells us of all the different options we can have to change this into a cell magic.
When I say cell magic, what this means is that this is a special flag that tells ipython that something different is going to happen for this entire cell.
In this case, we're telling it everything after this first line is going to be a SQL query.
As you can tell, there's other ways you can do this as well.
You can have HTML, you can have bash.
There's various other options as well, but I'm just showing you right now the SQL one.
Now this is how you connect to a SQL database that's just stored in memory.
If you have a different package, a different engine, then you can use the various documentation to tell you which connection you should use.
So we're going to create a very simple small table called presidents.
We're going to have first and last name and we're going to include the year that they were born.
And I just have a random sampling of about 10 US presidents here.
So running this cell, we get some output here that says one row is affected.
We've inserted values into this table.
And now we can actually run a SQL command that's in inline again with a single percent.
When you have this command here, it says everything after it will be SQL.
So we're going to store an object called later presidents, the SQL command and the results that come from the SQL query.
The SQL query being select everything from the presidents table where the year of birth was later than 1825.
And then I'm going to show you what that looks like by typing it there.
So we see that there were three presidents that were born in that table after 1825.
And if we took a look at the type of this return, we will see that it is actually a pandas core data frame.
So we have returned a SQL query into a pandas data frame.
And now we can use all of the normal tools and functionality of pandas directly.
If we would like to write out this into a file, we can do that by doing this SQL three command here.
So we make a connection to a new file.
And then you run the pandas data frame method to SQL and say, we'll write out the presidents table to the connection.
Now, if you don't want to use cell magic in this way, you can also use pandas directly to query our SQL database.
So I'll show you how to do that from reading in that file that we just wrote out.
So we're going to connect it out to this presidents SQL output.
We're going to now create a cursor that connects to that connection.
And we will create a new data frame by doing the pandas function read SQL.
If you hit shift tab while your cursor is inside the parentheses, you get to see the various calls here.
So we have the SQL command, you're giving it the SQL, then you're following it with the connection.
And then everything else can be these many other options that you have to really customize it.
And once you've done that, be sure to remember to close the connection by doing com.close.
So the new data frame should have everything that we stored in the previous query.
So the three presidents that we saved from above.
And again, this is a data frame that was returned from that.
So I just showed you two different ways that you can query databases.
You can query them with an inline magic, or you can query them through pandas directly.
And either one will return to a pandas data frame so that you can actually use the output in some exploratory data analysis or your full-fledged project.
In this video, we'll be talking about how to actually use R in the Jupyter Notebook ecosystem.
Previously, we talked about how we can actually set up different Python and R environments.
To set up a unique conda environment for Python 2, for example,
we can do conda create minus n for name pi2, for example, just as a descriptive name that you could use.
We set the Python version to be equal to 2, and then the other packages that we would like to install.
So anaconda, Jupyter itself, notebook.
We do the same thing for the Python 3 environment.
So conda create with a different name, Python 3, for example, and setting the Python version equaling to 3.
We also do the same thing when we want to do an R environment.
So in this case, conda create minus n, and we're going to call this Jupyter underscore R.
And with creating the channel by minus C, R tells Jupyter and tells conda that you're actually creating an R kernel as well as the default other ones.
And this creates the R kernel so that the Jupyter Notebook can actually run R natively,
as well as installing a number of different packages that it thinks are both recommended and essential.
And finally, a Python package called rpi2.
The way to activate these commands is you say source activate and then the name of the environment that you created.
And when you're done with it, source deactivate.
And if you ever forget which environments you've actually installed or what the names you used were, you can do conda environment list.
Let's do that to start with conda env list.
And we see that there are four different environments installed.
There's the root one, which doesn't really qualify as an environment, but then we have pi2, pi3, and Jupyter R.
So let's source activate pi3 and say Jupyter Notebook.
Once we start that, we can start a Python Notebook.
And we see in the upper right-hand corner, not only a blue flag that says using the pi3 kernel just for a second before it flashed away,
you actually see that it types Python 3 in the upper right-hand corner.
Let's verify that by doing a print 5 plus 5 as a statement and as we can do in Python 2.
And this doesn't work in Python 3.
The syntax for Python 3 is with parentheses.
All right, so we are using Python 3.
Let's close and halt this and shut down the server by hitting control C twice.
We can tell that we're using Python 3 because pi3 is at the beginning of our terminal screen right there.
So I have to say source deactivate.
Again, conda env list.
Let's switch to Jupyter.
The command is the same Jupyter Notebook.
Now we can click this pure R example and it loads up R.
Just in case you're curious, we can go back to this home directory and create a new, in this case, R.
And this is an R kernel running natively.
So you can tell again, look in the upper right-hand corner, not only is it not using Python,
it's actually using the R kernel natively for this entire notebook.
Let's go back to this pure R example.
So what is it that R can do?
R is a language that has some design choices that are slightly different than Python,
but it does have a huge statistics library packages.
So you load them in and everything you'll be done in this notebook will be actual R code itself.
And again, just looking in the upper-hand corner, this is now R code.
I loaded a few libraries here.
These are some standard, actually really nice libraries in R, the plier package and ggplot2.
This economics data comes when you load in the plier library and you see the head of this economics data.
You can create a ggplot command by doing this R code here.
And just like with the Jupyter Notebook, we're using Python, we see inline plotting so that all of the workflow is in the same really nice way
where you can do this piecemeal exploring by looking at a single piece of R code in the output.
Let's close and save this.
And now let's open up this Rpy2 example.
We are now running again a Python 2 kernel and we're actually using the Jupyter R environment.
So Jupyter R environment can run Python and it can run R itself.
It's running either one depending on what you started the notebook as.
So we're running this one as a Python notebook.
But here's a really nice feature of the Jupyter Notebook.
You can intermingle Python code and R code in the same notebook.
I'll show you how this works.
So the top here importing numpy as NP.
So again, just Python code, we're creating X and Y where X is this a range.
Let's just look at what X is an array from zero to nine and Y is some random number plus the X variable.
We import this library Rpy2 and load this extension Rpy2.ipython.
So we load it by doing this percent magic percent load extension Rpy2.ipython.
And you can do this in a cell that has other code.
You don't have to make this a single cell.
Just wrote it five plus five just so you can see that we've loaded in an extension and we have this other code running as well.
So we have these two numpy arrays, a capital X and a capital Y.
If we would actually like to do some analysis in R and then push something back into Python,
we do that by now doing a thing called a cell magic.
So cell magics are known by having a double percent sign at the very beginning of a cell.
That means that this top line is a special thing that in this case we're having it.
There's HTML and bash and various other options.
We are using the R option and we are sending in with this input X and Y from the Python environment.
So we are sending to R the two numpy arrays and we would like to get back from R this thing called XY coefficient.
Everything else in this cell is R code.
So XYLM is equal to linear model of Y goes as X.
XY coefficient which we will be returning back to Python after this cell completes is the coefficients of this model.
We're going to print the summary and we're going to make a plot.
So run that cell and we see the formula call here, the residual, some intercept and X coefficients.
And we have some plots that are displayed in our Python notebook.
And again, we actually get our XY coefficient out back into our Python environment.
So if you're a person who actually likes to use R just as much as you like to use Python or you like to use R for particular tasks
or you like to use Python for lots of it, the Jupyter notebook is very, very flexible.
It lets you work in whichever environment you prefer while dropping into the alternate Python or R environment to do just even a few pieces of it.
So if you're in the middle of a long piece of data science analysis and you need one functionality from R,
you can keep that not only in the notebook but passing it back and forth through native types.
In this video, we'll be doing a somewhat more advanced topic and it's definitely 100% optional.
We'll be talking about how to get into the guts of the Jupyter notebook system itself and create a post save hook,
which will, for our purposes, save a script and an HTML file version of our Jupyter notebooks themselves.
So how do we do this?
The first step is to actually create a notebook configuration file.
Now you can do that if you're interested in doing it in just your root environment
or having this behavior be copied everywhere you are actually working on anything to do with the Jupyter notebook.
Just go ahead and run Jupyter notebook generate config and I will copy and paste this into the terminal.
So you can see what it looks like when you run this.
The key takeaway here is this writing default config to now this should be your home directory dot Jupyter slash
and then it's going to be this file called Jupyter notebook config.
There's another way you can do this if you want to make this for a specific type of analysis.
So maybe only the analysis you do involving housing data.
Do you want to have a special behavior happen?
You can do that in a somewhat roundabout way.
You set this Jupyter config directory.
It's an environment variable and set that to be a thing that doesn't exist yet.
A home directory so tilde slash dot Jupyter save.
You run a command that starts like this and then you generate the config.
So I will show you what this looks like.
So it wrote the default configuration file to dot Jupyter underscore save,
which is the name of this profile and then the same Jupyter notebook config file.
Now, running it in this way, you have Jupyter and configure before you do the actual command sets it as a temporary environment variable,
meaning it's only set for that one command.
If I try to echo this, I won't have anything stored in it.
So I'm not exporting this as an environment variable.
Now, I have a bit of code here that I'm going to actually toggle this header and this toolbar just to give us a little bit of extra space.
I have some code here that I would like you to add to your Jupyter config profile file.
So this Jupyter notebook config dot pi and instead of trying to type it off the screen,
you can actually access it by typing HTTP colon slash slash b i t dot l y so bit dot l y and then Jupyter underscore profile.
You click on that, you will go to the same exact code I have that I typed out here.
In this case, I will actually copy this code and we're going to open up the file that we would like to modify.
So in this case, we're going to be modifying this Jupyter save underscore Jupyter notebook config file.
You can do with any text editor, I'm going to use sublime text, so sublime text open it up.
Now, here's what the file looks like.
It's actually a whole lot of things you can do to modify the behavior of your Jupyter notebook and they're almost all commented out.
So you can read through this if you want to actually make different changes than what I'm going to recommend.
But this is where we post just at the top this code, just a brief overview what's happening.
It defines a function called post save, and it basically grabs the path of the notebook that's currently running,
and it actually tries to run this command ipython nb convert to script,
which means it's going to be a .py file if it's a Python file or a .r file if it's a r notebook,
and an HTML file, which means that it'll just be the rendered HTML version of it.
And the C dot file contents manager post save hook equals post save.
So this is a way that Jupyter developers have allowed a person to make changes after every save that they do.
So let's save that, and let's go back to our notebook.
So let's list what's in this directory.
We see the name of this current notebook is autosave other formats.
I'm going to toggle that away again.
So we see it here when I type ls.
We can also do exclamation mark ls to do a command like this.
And we see that when we save this, we see a checkpoint is created,
but no other new files are being created.
If we would like to see what happens when we run Jupyter notebook with this new Jupyter save configuration file,
we'll have to run a command that looks like this.
Jupyterconfigure equals this with Jupyter notebook.
And in this case, I would actually like to save this entire thing as an alias,
and then you can add this to your bash RC, or you can simply run this in a single line on your terminal.
If you just want it in your terminal, however, it will not set it as a thing.
So if you restart your computer or open up a new terminal, typing Jupyter save won't work.
If you add this to your dot bash RC, then this special way of opening Jupyter notebook will be saved.
So let's close this current notebook and let's type Jupyter save.
And let's reopen it again in this new way.
So we just opened it up.
The list function down here should show us what we saw before.
So we see the same files in this directory.
When I click save, if our post save hook worked correctly,
we will see autosaveotherformats.py and autosaveotherformats.html.
So I'm going to do that after I click save type ls again.
And we see that we do have two other forms.
I have html and .py.
Just to show you what those html and py versions look like, let's open that up.
Oh, one last note.
Every time you hit save, it will overwrite the same file a bunch of times.
So it's not going to create new versions of this.
It's going to just continually overwrite this and always keep the .html and the .py files completely up to date.
Let's look at one of these html files actually looks like.
So let's go back to the terminal to open a new one.
So by typing open autosaveotherformats.html, we actually have the fully rendered notebook here.
So what we see here is what we saw on the other page.
And this is now the html version of this.
This can be emailed somewhere.
This can be posted online somewhere and people can see this.
Now the links work like you'd expect and the code is all formatted and looks like it looks in the notebook.
But since it's just an html file and it's not an actual notebook running,
none of these cells are actually computable.
I can't actually rerun these cells.
So now we have a way of creating a post-save hook that lets us save out automatically html and script versions of any notebook that you're saving.
If you would like to commit this to your GitHub repository for fellow members of the team to review in different ways,
then having a post-save hook like this can save you tons of time and keep everything up to date.
In this video, we'll be talking about a really fun topic called widgets.
Widgets is an entire aspect of the Jupyter Notebook ecosystem that lets you do interactive things with the notebook.
Let's go over to the notebook.
This top cell has various imports, matplotlib, numpy, and so forth.
This last line in this cell actually imports the ipython widgets.
And we're going to import a number of sliders, a float slider, the integer slider, a toggle button, and this interactive thing as well.
So let's execute that by typing shift enter.
Now this next cell contains a simple formula.
We define a Python function named polynomial.
It takes three arguments, which has default values, so slope of 2.0 and intercept of 5 and show points, which can be either true or false.
We're going to create some x values, which is just a linear spacing from negative 10 to 10 using 50 points.
We're having a y value, which is just the slope times x plus the intercept.
Everything else in this function is actually going to be plotting something.
So this tells us the figure size we're going to use.
The next line tells you that we're going to actually use a figure.
The next two lines actually talk about whether or not the show points is true or false.
If it says show points, we'll see what this actually does in a second, but it'll add the actual data points we are plotting up when we define this x at the top line here.
Finally, we plot x and y and we set some window parameters and give ourselves some axes.
The last thing we do is add this tight layout call at the very bottom.
This just helps clean up the map plotlib plots before they're finally ready.
So after executing this cell, we now have defined polynomial.
Let's scroll down to this next cell.
I'm defining a thing called a slope slider.
This slope slider is called a float slider, which means it can actually take float values.
That's what it's actually sweeping across.
The value is 2.0, meaning that that's the starting value for the slope.
Let's actually start this at minus 10 at the maximum of plus 10 and step size of, oh, let's say 1.0.
The next line defines this object called w, which is interactive.
The first argument you give interactive is actually the function that you want to be interacting with.
In this case, the function we just defined polynomial and any other widgets that need to be connected to it.
So in this case, we're going to connect the slope parameter that's given to the polynomial function to the slope slider.
Now we call it slope slider, which is because we want to have a descriptive name.
You can name it anything you want.
The last thing we do is actually execute this w. Let's see what we see.
We see three widgets that we can interact with, the slope, the intercept, and show points, which is toggle.
Let's scroll down and I'm going to actually hide this toolbar so we have a little extra space.
We have the slope, which is 2, and now you can actually click and drag this to different values.
As you drag it to the right, you're increasing the slope and we can see that it's actually correspondingly increasing the slope of that line in the plot.
We can also move the intercept point up and down.
And as we know, an intercept just changes the y-positioning, shifting these things linearly up and down.
And of course, the last thing is to toggle on and off show points.
If you want to change this, we can actually make this much more sensitive by saying let's make the minimum minus 100, the maximum plus 100,
step size of, oh, let's say five.
Now, as we change the slope, it should be much more sensitive than it is because we're now at slope of 75.
And as we go negative, we can see that as well.
So as you can tell, having this kind of functionality at your fingertips is actually incredibly useful during all phases of doing a data science project,
especially during the exploratory data analysis stage.
So you can imagine if you did something like k-means to look at your data.
You can set k, the number of clusters you're fitting for, the number of centroids.
And as you can move that back and forth with the integer slider, for example, you can see how well the algorithm is actually clustering on that number of centroids.
So being able to do that in an interactive way can speed things up quite a bit, and it's really nice.
So this is a somewhat simple example that I just showed.
Here is a much more complicated example, but just to give you a sense of what is possible with this kind of a thing.
So I'm not expecting you to actually read this and understand the code that goes behind it, but let's just execute this real fast.
This is one of the projects I was working on just on my own, where I want to actually have some random points in a small area,
and I would like to interpolate with a spline interpolate those random points, and I wanted to see what that looked like at the end.
So I can say the number of points that I'm randomly generating and splining between.
And as I slide this to the right, you can see the pattern becomes more and more complicated.
And as I slide this to the left, we get much simpler shapes.
We also have a smoothing parameter here, which can give you a smoothing factor to these kind of more complicated shapes.
It sort of unwinds the and rewinds up the knots and the alpha value, for example, like how dark this is.
Or if I want to have a slight jitter to each of these strokes, I can add the number of brush strokes and then increase or decrease the jitter for this.
So obviously there's a lot going on here, but this is one aspect that shows you just how, first of all, how quickly this can refresh, but also how useful it is.
In this video, we saw how we can use interactive capabilities of the Jupyter Notebook to help us plot and look at data and change the values by sliding sliders around.
In this video, I'll be talking about some bleeding edge developments in the Jupyter project.
A specific thing called Jupyter Hub.
If we were to go to Google, let's just search for it by saying Jupyter, then HUB, the top link will be this GitHub repository, which is Jupyter slash Jupyter Hub.
And this allows, as it says, multi-user servers for Jupyter Notebooks.
In other words, if you have a server where there's data being held for a data science team, you can run a single instance of this thing called Jupyter Hub.
And it allows many different data scientists to log in and start a Jupyter Notebook on that server co-located with the data.
Now, this is an active development. It's changing on weekly time scales.
So if I were to actually show you how to set it up today, by the time you saw this video, it would probably be different from how you're supposed to be setting it up then.
So for right now, I'll point you to this documentation and mention that it's actually very much bleeding edge, but I think it will be the future for data science teams.
Just to give you a sense of what it looks like when you were to use Jupyter Hub, you can go to try.jupyter.org and hit Return.
And what you're actually interfacing with here is a Jupyter Hub server somewhere in the back end, currently being hosted by Rackspace, apparently.
And you can start a new notebook in any of these different styles, so Bash, Haskell, Julia, Python 2, or Python 3, R, Ruby, and Scala.
So you can start a notebook here, and this is just letting you run a temporary quick one. You can also start one of these notebooks, like this Python one.
And it starts with this warning. Don't rely on this server for anything you want to last. The server will be deleted after 10 minutes of inactivity.
So that's important. This is just a demonstration area, so it's not for long-term storage of some sort of data science analysis, but it gives you a flavor of what the Jupyter Hub will be doing if you were to install this for your own sake.
Now you can actually run this Jupyter Python 3 notebook, and you can actually see the fun results that come out from this.
So we see some plots here, and everything works just like you expected to when you're running the Jupyter server locally, which is how all the videos I'm doing in this lesson are.
Separating the server from the notebook aspect is that you can do something like this in the future, have the server being hosted on some server somewhere,
and being able to access it just through the browser, and having the same exact functionality that I've been showing you for the entire course so far.
And one last thing just to show you how fun this is, let's navigate back to our initials-try.jupyter thing.
There's a couple other things you can do besides notebooks. This is true for the local server as well, but just to give you a sense of this, you can add a new folder, which is kind of unexciting.
You just have a new unentitled folder here. Then you can do this new text file. So if you click text file, instead of starting a new notebook, you're starting a new file.
And this is a lightweight in-browser text editor that has various options. You can choose what kind of key mapping you'd like.
So I prefer sublime text these days as ways of interfacing with your text editor.
So from here, you can do your standard Python, and you can both create and edit Python scripts or any kind of text file that you want to that's located on the server.
And of course, renaming the file is as simple as clicking this top thing, calling it startup.py, for example.
And once you do that, syntax highlighting gets turned on. You can save this and rename it, and then navigate back to the main server page.
And the last thing to show you is that you can also start a terminal.
And here you actually have the terminal for your tri-Jupiter. And this is the same case for if you're running this on a server.
So you can actually have access to the terminal with all the functionality of a standard bash terminal there.
So we see the startup thing we can copy that startup.py folder and call it something else.
And going back, we should be able to see this.
It's a very cool thing, and it will definitely be the way of the future if you have data science teams working and needing access to a single server somewhere that has the data in some database, for example.
So Jupyter Hub, it will be the future. It is bleeding edge. So try it out. It should be pretty usable, but the exact instructions will be different from what I would say today.
In this lesson, we'll be taking a look at organizing the overall structure for a data science team to be working on their various projects.
So in this notebook, I'm going to use the slideshow button that we installed in a different video.
And I make this full screen by clicking shift command F.
The initial topic is questions to ask to organize the workflow of a data science team.
So the first question is how many data scientists will be working on a single problem?
And the high level view of this is to basically break this up into thinking about this in terms of Git repositories.
What I mean by that is, if you have different data sources and different problems working in a single company, let's say, then you should definitely use different Git repositories.
If you have fewer than 10 data scientists working on the same data, but working on different problems, it also probably makes sense to keep everything in a single Git repository, although it doesn't have to.
If you have different concerns, feel free to break that up.
And if you have more than 10 data scientists and they're working on the same data, but they're working on different problems, fundamentally addressing different data science issues, then I recommend using different Git repositories.
And all of my recommendations will be within context of a single Git repository.
The second main question to be asked is where is the data actually hosted?
If it's small enough data to be loaded onto a data scientist's personal laptop, then it's very simple to actually just use the data on the laptop locally.
So I would recommend just running the Jupyter Notebook as I'm doing in most of the videos for this course, where you just open up a terminal on your local laptop or local desktop and just run Jupyter Notebook.
However, many data science projects actually use big data.
They access the data on some other server or something like this.
And in this case, you have a couple of options.
The obvious one is to say, if you can access this server data via SSH and you can actually do work in a server, then you can actually run a Jupyter server on that server and you can SSH tunnel and forward your connection to that server.
That way, both the data and the Jupyter server are on the same machine.
Another option is to consider using a thing called Jupyter Hub.
The Jupyter Hub would have to be installed on the server where the data is actually being held.
And if I click this link, you go to this GitHub page here.
So it can be found at github.com slash Jupyter slash Jupyter Hub.
And you can see it's a bit more work than we can go into.
It's a bit outside the scope of this class.
But Jupyter Hub is a multi-user server for Jupyter Notebooks.
And there's actually some really nice documentation to explain how this can be set up on a server or some AWS instance, for example.
There's lots of installation instructions and things to work on here.
So those are the main questions to be asking.
At what level do you set the Git repository?
And where are you going to be running this server?
Are you going to be running it on a server somewhere?
Or will you be running it locally on your local laptop or something else?
Once you have those two questions settled, then the mechanics of actually how do you work on a Jupyter Notebook in a single repository or what we'll deal with next?
In this lesson, we'll be organizing our work into two different types of notebooks.
Conceptually, there are two types of notebooks I'd like to introduce.
One called a laboratory notebook and one called a deliverable notebook.
The difference here, a laboratory notebook is in the same style as lab notebooks that are actually in science labs throughout the world.
And by that, a lab notebook keeps a historical record of the analysis that's been explored.
So each day, a person goes to a lab bench, writes down the date at the top of the page, writes down what happened in lab that day for that particular experiment.
And this record just continually gets amended to.
It is also meant to be a place where there's development or scratch ideas or initial analyses, and it's very much not a polished piece of work.
It is meant for record keeping of scratch pad type nature.
And each notebook is controlled by a single data scientist.
And by this, I'm talking about a Jupyter notebook where it is a single person single data scientists record of what they were doing that day and it is not shared by anyone else.
Now, it's not secret people can look at it and you can upload it as well, but it's not meant to be viewed by other people necessarily.
A few more final points on lab notebooks.
Split the notebook when it gets too long and too long is just sort of a personal preference.
As you start scrolling down the page as a point when a lab notebook or any notebook gets to the point where, okay, this is too much of a document to look at at one time.
So then split it.
There's no cost in splitting it.
And you can think of this as just turning the page in a lab notebook.
And finally, if you're working on a single day, you can actually split notebooks into different topics.
So for the same day, you can actually have two different or more notebooks.
And if you're splitting by topic, that makes sense as well.
On contrast to a lab notebook, there's another idea of a deliverable notebook.
As I work as a consultant, most of my work is actually going to be delivered either to a project manager or to a client.
And these notebooks are different from lab notebooks in the sense that these will be delivered to someone to consume besides myself.
Now candidates for deliverable notebooks can be any notebook that will be referenced in the future.
By this, I mean, if I expect someone else to also use the same data cleaning notebook, for example,
so I might have a notebook that explains how I took raw data and transformed it into the clean data that I use for the rest of the analysis.
And I might provide a single link to a deliverable notebook, which is simply the data cleaning of the raw data.
And in that notebook, I'll have things like what the actual transformations were, but also reasoning behind it and some documentation around it.
So this is for anyone who wants to know how is this data actually cleaned?
There's a single spot for it to look at.
And obviously, of course, the final fully polished and final analysis of a data science piece of work will also be considered a deliverable notebook.
I also recommend that deliverable notebooks should be peer reviewed via pull requests,
which means other members will actually review the notebook before it's accepted.
Other members can be other data scientists or it can be a manager or something else.
And these notebooks are controlled by the whole data science team.
If we think about these notebooks as living in a certain repository, for example,
then the whole data science team will have these deliverable notebooks,
which are in the same topic scope as the problem that they're all together trying to solve.
So how do we organize the directories so that the lab notebooks and deliverable notebooks all are in their proper place?
So these are the minimum directories, and I think it can be expanded by a few or taken away by a few.
So I have listed here the directories I think belong at the top level of a data science git repository.
The first one is data. This is optional.
If you have very small data and you want to have it locally, it's possible to include it in a git repository.
Generally, though, data science data is actually backed up outside of version control.
It's in a different environment.
So this is definitely an optional directory to have.
The second one is the deliver directory.
This is where the final polished notebooks for consumption.
If a new data scientist is coming onto the project, they will look in the deliver directory to see what has been done before.
In the develop directory, we store the lab notebooks, and I will explain the naming convention in a further video,
but this will say all the scratch work that has been done by each of the data scientists working on this problem.
The directory called figures will contain the figures that have been the output from both to develop and the deliver notebooks.
I will be expressing a bit more on that in the future.
And finally, a source directory where as you come up with various scripts or modules or anything else that needs to be,
that's actual computer code that doesn't belong in a notebook directory, goes in a source directory.
Again, you can add to this or you can modify this as you want to,
but I think this is a good starting structure to work from and modify it as your needs evolve.
In this video, I'll be telling you about my recommended convention for naming lab notebooks.
So naming a lab notebook can be a more difficult problem than you might expect,
especially if there's many different data scientists working on a similar problem.
So to help with that, the following convention is what I recommend.
You can obviously change this to fit your own needs.
I recommend prepending each file name with the current date that you started the work on that notebook.
So in this case, it was started 2015 dash 11 dash 21.
I also recommend it in that format where it's the year dash the two digit month,
meaning if it's three, it'd be dash zero three dash the two digit day like the month in zero four and so on.
This is called an ISO 8601 formatted date, and it just helps with keeping everything so that it's sortable in a nice way.
So the initial part of the name is the date that you started working on that particular notebook.
The second piece immediately after that is the data scientists initials.
So in my case, my initials are JBW.
So I put dash after the date my initials, or you can put it if you have a data scientists with the same initials,
you can just put some unique signifier that's the same every time.
So that if you want to look at a directory that has many different data scientists notebooks,
you can do an LS for that person's initials and find their notebooks.
And finally, I recommend putting a two to forward description that describes what goes in that notebook.
So in this case, Cole predict RF for random forest regression.
So looking through this later on, I can think back, okay, what was I doing two months ago,
something with random forest, and it was a regression.
And on a classifier, seeing this in the title helps pick this out.
In this video, we'll be talking about version control.
One of the key questions you have when dealing with a data science team is how do you peer review code?
How do you store analysis in version control like get?
And I'm going to assume a number of further constraints.
And I think this is probably the most restrictive constraints I can think of.
This might not apply to you.
But I think if it does apply to you, I have reasonable work rounds for each of the possible concerns.
For example, imagine you have a project manager who would like to see the notebooks you're working on,
but they don't want to install Python or I Python or anything like this,
or consider that you might not be using GitHub for whatever reason.
And some of the nice tools that GitHub has for showing diffs aren't available to you.
Or if you would want to review the Python code itself and don't want to have to look at it in a notebook environment.
How do I recommend dealing with these kinds of constraints while also maintaining a peer review of the code stored in the version control?
The standard practice for my answer is that each data scientist who's working on the same problem in the same repo
should have their own development branch.
And each day or even more frequently than each day,
but at minimum work is saved and pushed to the dev branch that they have daily,
which means that anyone can then check out another data scientist development branch.
When ready to merge to master, you have to do a pull request.
So a data scientist says, OK, I think the deliverable notebooks as well as my laboratory notebooks are ready to be reviewed and pulled into master.
Now the question of what exactly to commit.
This is a question that people who come from a more software engineering background might start to recoil at my suggestions here.
I say this after a lot of thought and there might be a better way of doing it, but this is the best way that I can come up with.
So I recommend committing the .ipynb files, which are the notebook files, the .py and the .html of all notebooks, both develop and deliver.
And I'll also say any of the figures that are saved should also be committed.
Now, when I say the .py and the .html, what am I referring to?
So I'll go to an open notebook right now.
This is a notebook for making a prediction about call production.
And this is in the develop folder of a certain directory.
And I have this notebook that's currently running and you can tell it's running by the green symbol here and the words running green all the way to the right.
So let's go to this running notebook and actually save it, save in checkpoint and download as a Python file.
Let's download it to the same directory of develop, save that, and let's download this as an .html file and save it in the same spot.
So if we take a look at what this is, it has taken all of the Python code and none of the output,
but it's shipped out everything else in this file that's not Python code.
And so you see this input three, input four, and so on.
This is delineating the cells in the notebook, but everything you see here is actually Python code.
So this can actually run as a .py or you can run it as Python, this file name.
And this .html file, if we open up this file, we actually see the HTML representation of the notebook.
So this is not executable.
This is just a .html file.
And so this can be copied into an email and read by anyone who opens this with a web browser.
You don't need to run Python or IPython to actually see the output here.
Again, the limitation, though, is you cannot actually edit this code and make a new plot,
but this is great for being able to share a particular notebook.
So I recommend saving both of those file types to your Git repository.
And of course, all of the figures as well if you create separate figures.
The reasoning behind that is that the .py files allows a person to make easy changes to the actual Python code itself,
as well as to track those changes.
The .html file allows a person to see the fully rendered notebook without having to run a notebook themselves.
So the benefits of structuring your repository this way are several fold.
First of all, you have a complete record of the analysis that includes dead ends.
So if one day you worked down a single hypothesis and turned out that it wasn't very useful,
that is still saved in the lab notebook directory.
It also allows for easy peer review of the analysis and of the dead ends.
If in the future, a different team member has an idea to try to do a random forest regression on the data,
they can actually see if someone else has done the same type of analysis,
and if so, what led to a dead end, for example.
And finally, project managers can easily see and read the analysis with GitHub
because GitHub itself renders IP, UI, and Bs natively.
Or if you don't have GitHub access or not rendering it for whatever reason,
if you save the .html files, anyone can actually see the rendered notebook without having to run any code themselves,
or installing IPython or anything else.
Some final organization thoughts of this whole structure.
So organizing the workflow for teams is actually a difficult problem,
and I think this is a very good framework for having some standards.
And this bullet point about the wrong thing solves the problem.
Often with version control, software engineering types think we need the source that's version control
and we don't need to track the output, or that output is something that's blown away.
In data science work, the output is often the thing we need to look at.
For example, if there is a plot that shows some deviation,
that plot is best viewed in the peer review process, actually in the notebook itself,
or in an .html rendering of that notebook, because that gives rise to any sort of correction
or reinterpretation that needs to happen.
So the output actually is the thing that matters in a lot of data science work.
So storing that in version control is actually the right thing to do,
even though in typical practice it's the wrong to actually store the output.
Finally, I am open to new ideas if you have a better way of solving these problems,
or if your situation is completely different so that such that you will always be using GitHub,
you never have to worry about seeing a rendered .html file.
You can make these modifications by doing your own version of this kind of organization.
So hopefully this gave you some structure to organize how a team of data scientists
would work in a Git environment.
In this lesson, we'll be getting some data that we can actually do some data science with.
I recommend having a data folder in your projects directory that actually is at the same level
as your deliver directory, your development directory, and your source directory.
In my case, I have about 10 files in here that are coal data from the U.S. government.
If you'd like to grab this same data set so you can follow along, go to www.eia.gov.
This is the government's energy information administration website,
and if you go to the data tab, you can scroll down to where it says production, give that a click.
And there's lots of different data available here,
but we're looking at the historical detailed coal production data available from 1983 to 2013.
Select which year you'd like to do, and in case I picked 10 of them,
click the arrow here and save it into that data directory.
Once you do that, you'll then have the data that we'll need for this upcoming lessons.
In this lesson, we're going to take our very first look at the data.
We might even do some initial data cleaning.
I'm currently in this directory where you can see we have data, deliver, development, and source directories.
I'm going to start the Jupyter Notebook by, again, typing Jupyter Notebook.
From here, we see the same directories I just saw in that directory.
Let's open up the development list and start a new notebook by going over to new Python 2 notebook.
From here, we see the familiar text box where you can type in code.
In here, we see the code box is actually surrounded by green,
which means as we type, it should be typing in text into that cell.
We're going to need the pandas library, and we're going to import it as import pandas as pd.
This can create alias for the pandas library to actually be called pd.
This is a standard way of calling pandas, and I recommend you following the standards as often as possible.
This lets you share your code with other people in the most seamless way possible.
To run the cell, I can click the run cell button in the toolbar,
or I can have done the shift enter technique, which, as you can see, increments which input number it is by one.
The pandas version that I'm actually running is done by doing a print double underscore and then hitting the tab button.
Hitting tab is a thing you should be thinking about doing quite often,
because it often lets you make sure you don't have to type everything out.
It's faster, but also make sure you are in the right vicinity of what you're hoping to do.
There's a version, I'm going to hit return here, and then shift return,
and it prints the pandas version that we're using, which is 0.17.0.
From here, let's actually take a look at our very first data file.
The way we can read this in, we happen to know, and here's an interesting side note,
if you type ls and execute that, you actually see all the folders in the directory you're currently in.
If you type ls up one, we see the parent directory,
and if you'd like to look at what's in data,
we see the files that we just downloaded in the previous video.
Let's load in one of these Excel files and take a look at what's actually in them.
I'm going to create a variable called df for data frame, and I'm going to df1 for the first one.
I'm going to do pd.read, and I think it's going to be Excel,
but I type the tab and I see an option pull up, and it is.
It is pd.read underscore Excel, open parentheses.
At this point, if you're not sure what a function does, there's a function called tooltip,
which is generated by holding down shift and hitting tab once.
Here it tells you the signature for this function, which has an input output, a sheet name, header, and so on.
A lot of different options available for reading in Excel files.
There's actually a longer version of this, where if you do shift tab tab in rapid succession,
so it's a double tab, then you have the full doc string and the examples that go along with it.
This is a very useful feature, so you can actually look up documentation on the fly, and it's very useful.
In this case, we're going to try to load in the data from above.
Again, tab completing commands will make your life much easier.
As I start typing out this, I can hit tab and it actually produces again a list of possible data sources.
Let's just see if this works.
Head is a function on a data frame, and it lets you show the various options.
We see that a number of things have happened here.
First, we have the year, the MSHA ID, the mine name, the mine state.
We actually see some of the data, and this is just the first few rows by doing head.
I recommend doing head because it actually stores the full output of this.
It's a separate thing that you can actually call.
In future lessons, I'll explain exactly why using .head as best practices,
but for now, let's just use .head to look into the contents of our pandas data frames.
At this point, we've taken a first look at loading in some Excel data files,
and we're going to start looking at this and playing around with it.
In this lesson, we're going to take a look at how we can start to manipulate the data that we've read in
in ways that are useful for analysis.
Last time, we read in the CoalPublic2013 file and took a look at the header.
The heading had an interesting, well, let's call it a problem.
The historical Coal production data is the title here.
There's a source function. There's also a bunch of nans, and all the columns are unnamed.
This is most useful when this line, line 2, which is our row 2, is actually year MSHA ID, mine name.
This is supposed to be the headers or the column names, and all the rest of it should be the actual rows of data.
We're going to put the second row here up to the columns at the top.
We'd also like to make this ID the index for the pandas data frame.
We'll go into exactly why in the future, but for now, let's merge the reading in of the data frame
with the printing out of what the head of that data frame looks like.
We're going up here and clicking Edit, Merge Cell Below, because we've actually selected the above cell.
So merge the cell below into one.
So now that I execute this cell, we see that there is in one cell both reading the file and looking at the head of the file.
Now, this is, again, wrong. We would like to remove this top part.
So the way to remove this is we're actually going to use a thing called header and start giving it a number.
Because if we look at this, we can see that it actually takes a header equals zero as the default value.
So if we do header equals one, it actually deletes that top row.
And so this is a way of telling the pandas that, hey, you don't have to modify that Excel file.
You can just, when you read it in, know that there's two lines of header files.
Now, there was two lines that had data in it, and there was a third NAN line that just, it knew it could not possibly be the header.
So it removed that.
So now the column names are these bolded ones are at the top.
We're getting very close to what we actually want.
Another thing we'd like to actually do is set the index.
So we set the index by typing index and hitting tab because we think it's going to be something like set index or index set.
And it's index columns equals, if this type, we like the name of it.
So we would like to do the MSHA ID as the index column.
And doing that, we see that the MSHA ID is indeed the index for this data and the columns are all appropriately named.
This is one way to interact with the pandas library.
But it actually applies to all Python libraries that have any sort of documentation strings.
Just to give you an example of that, I'm going to save this currently and just show you example function, right?
We define a function by typing def.
We'll do it test function.
Let's say it takes two values first equals five and second equals 10 and it will return first plus second.
Let's give it a doc string and we execute that line.
If we start typing test underscore f and then hit tab, it will automatically complete that because we have a defined function here called def function.
We do the initial parentheses and hit shift tab.
You actually see the doc string that we wrote just above.
This is an example and it has the signature of it to the first equals five second equals 10.
If you want to redefine what actually we give it, we can say first equals three and the test function gives us 13, which is what we'd expect.
So that's just a fun side note on how the interaction with Jupiter notebook lets you look into the doc strings of functions that you define yourself as well as any of the libraries that you'll be using your data science day to day.
In this lesson, we'll be making a new GitHub repository for a new data science project.
So let's go over to GitHub and from GitHub, if you go all the way over to the right, you can create new repository.
Give the repository some name that you think makes sense.
So we'll do some coal exploration.
So let's make a coal exploration repository name.
You can give it a description if you'd like to.
You don't need to decide whether it will be public or private.
I'll let it be public so that you can see this as well.
And generally, I like to initialize the repository with a read me.
It get ignore file that's Python because I use a lot of Python code.
And I add an MIT license.
After doing all this, click create repository.
Once you click create repository, you can go over to this place here where you can click SSH.
You can have HTTPS or SSH.
I just use SSH most of the time.
Clicking once in here highlights everything.
Command C will copy this.
And going back into a terminal, type git clone and then command V to paste the required link.
Hit return.
And you will now clone the GitHub repository to your local machine.
And from here, we see a new coal exploration folder being created.
And if we CD into coal exploration, we see that it has a license and a read me file that we've made previously.
In this lesson, we'll be taking our GitHub repository that we've just started.
We'll first look at the data.
So the directory as we last left, it has two files in it, a license and a read me file.
We're going to create some extra directories and some structure around here.
And I'll go through the reasoning behind this in other videos.
But we're going to create using the make directory command, a data directory, a deliver directory,
which is going to house the final deliverable important Jupyter notebooks, a develop directory,
which is where we're going to mostly do our development place,
place to put our source code if we have any scripts that we'll end up using.
So separate from ipython notebooks, usually Python files or other kinds of scripts,
we'll go in a source directory and a figures directory running that command.
The folder structure that we have now has a data deliver develop figures and source directories.
So let's actually get that data and put it into this directory.
You might have already downloaded it.
If not, again, the way to get this is to go to eia.gov slash coal.
Go to the data tab down to production.
And we go to the historical detailed coal production data.
And let's just use the year 2013 for now.
We're going to go into this coal exploration, navigate to the data folder and save.
That is done downloading.
You can see it in this folder as coal public 2013.
Great.
So let's take a look at this.
We'll open up a Jupyter notebook and take a look.
So from this top level directory, I will start Jupyter notebook.
You can now close this download file.
And you can navigate this structure similarly to the terminal itself.
So you can actually click data and you see the coal public data that we had before.
We can navigate back and let's go into the develop and start a new Python to notebook.
It starts off being called untitled.
And that is a not very helpful name.
So I recommend using the date in ISO 8601 format.
And the reason for that is that it helps with sorting.
But basically it goes year dash month dash dates today is the 21st.
After you do the date, I recommend, especially if you're working in teams to have your initials or some other identifier that creates it so that people know it's your notebook.
And so I'm going to type my initials here.
And then I recommend having a couple words that describe what you think you're doing in this notebook.
So I think I'll just say a first look.
So now I've renamed that notebook and it helpfully tells us when it lasted the last checkpoint.
This means when it's been saved auto saves every once in a while.
You can also click this button, but you just see that the last checkpoint saved.
And you can also do command s, which is how I normally do it.
So this means that it's keeping auto saved versions of this as we go along.
All right.
So there's a number of libraries that we'd like to import.
And I import these almost every time and it starts off with matplotlib inline.
So this percent sign at the top of the line means it's a magic import.
And we also have to import matplotlib like so importing it as PLT is the standard best practice for doing that next we import pandas.
And we should also import seaborne, which is a package that wraps matplotlib.
Interestingly, you're supposed to import seaborne as SNS.
I don't know exactly why, but importing it as SNS is the standard way of doing it.
Also, if you do SNS dot set, it actually sets a number of the default parameters for matplotlib.
So it already looks nicer if you just use it from there.
So let's go ahead and start with that.
And now let's read into a data frame, the data file that we just downloaded.
So we say df equals pandas library dot read hit tab to see the options go to Excel.
And we navigate to the directory by going up one directory by doing dot dot slash.
If we hit tab, we also get the possible navigation options.
It's in the data and if you tap again, it will have complete to say cold public 2013.
If we actually execute that and take a look at the head, we notice that we again have this unnamed part at the top.
So we actually wouldn't like to remember that it has a header.
Set the headers equal to two and that correctly gets the column types labeled in there.
And we want to set the index to the MSH ID.
So if it's annoying, you set index by doing index something hit tab and its index column equals MSHA space ID.
Excelling those two cells, you have the ID of the mine setting as the index of this data frame
and all the data in here correctly parsed from that Excel file.
Okay, so I'm going to stop it here and we'll begin to actually start to plot this and take a look at what this data actually looks like.
In this lesson, we'll take a look at the data and do some data cleaning and maybe do some visualizations.
Let's go back into this notebook and rerun the first cell here, load everything in that warning that we've seen before.
Load in the data and take a look at the data dot head.
So everything here looks normal.
And the day to day data science work, you often take a look at what's in each of these columns.
So we can just do a very quick look, for example, at a data frame and take a look at the company type.
Now, if we have thousands of rows, we don't want to look at all of them, but we do want to look at the unique ones.
In here, we see that there's three types of unique companies according into our file right now.
We have what I think the word is supposed to be independent producer operator.
The next one is operating subsidiary and contractor.
Now, obviously, this first piece of information is that the data has some data quality issues.
So let's go ahead and actually make a correction here for this data.
We'd actually like to replace all of the independent producer operators with independent producer operators.
So the way to do this in place is to actually do a company type to replace it.
And if you don't remember the syntax for replacing, if you do a shift tab, you can actually see the tool tip come up.
There's two ways to do this.
You can say to replace equals x, the value in place, everything else.
And we can also do it by giving it a dictionary.
I'm actually going to do it the standard way.
So to replace should be equal to, we'll just copy the words from above.
And the value I would like to replace it with is going to be the independent producer operator.
This cell is already becoming wider than the screen.
So I'm going to actually hit return here so that it's lined up with the beginning of this.
So you can say one later on can actually read this in a much nicer way.
Suppose a DF company replace this thing and then do head on this.
It should show us that it is indeed replacing the independent producer, but it hasn't replaced it in the actual data frame itself.
To do that, we have to add an extra command here, which is in place equals true.
One extra interesting, let's call it a quirk of the Jupiter system.
If you're in line with the beginning of this command, if you do a tool tip by doing a shift tab, it appears.
If you're not on that first line and it's broken up across multiple lines, then doing the shift tab in the middle here will not work.
If you're thinking, is it in place one word or is it in underscore place?
You have to do it up here to get the tool tip help.
So it's in place one word.
So I typed it down here.
This will in place change the DF company type to be independent.
So this has now been replaced in place.
Now we also see that even though I could actually hit tab, which is a very useful thing to be able to call the column heading by just typing the beginning of it.
Having these spaces is going to just make life a little bit more difficult than it should be.
So what I'd like to do is actually go through all of the columns in this data frame and replace every single space with an underscore.
So it's still readable, but I'd just like to actually do that.
So to do that, we actually would like to do a name of the columns.
So we DF dot rename index columns equals and keyword arguments.
So you can say columns equals.
Now this is a really fun trick because you actually pass a Lambda function.
Lambda function says for everything in that columns, I'd like to do X dot replace.
So similar syntax as above, but I replace all of the spaces with underscores.
So the thing that's being quoted is the thing that's being found single space replacing that space with is the underscore.
So I'd like to rename the data frame where every column space will be turned into an underscore.
And of course, I would also like to actually make this happen to the data frame in place.
So I say in place equals true now to check if that actually worked as we hope we can look at the DF dot head.
And we see that underscore name mine underscore state mine underscore county and so on.
So this with one line and very quickly typing it out replaced all of the spaces here with underscores.
And this will just make life much easier as we go on from here.
Let's also take a look at how big is this data frame.
We have 1400 data points.
And let's take a first look at just what's in here.
So we just read this off as my name all the way through regions and average employees and labor hours.
Let's see what the relationship between the number of employees for a mine and the number of labor hours looks like.
There's a couple of ways we can do this.
Let's see the simplest way I can think of is to do a scatter plot.
So we can do PLT dot scatter and DF dot average employees.
So now I've indexed the data frames column by simply doing a dot before it because it has a space in it.
I would have to have done it the DF bracket space labor hours, for example.
So this will actually work.
You see that the plot actually works as expected.
But now instead of having to type out labor hours previously with a space there, I can actually do dot labor hours.
And that just makes my life just ever so slightly bit better.
Let's label this.
Okay, so just as we expect, number of employees goes up.
The total number of hours worked at that mine goes up in a pretty linear fashion.
Another way of doing this would actually be linear regression plot on this.
And you can use Seaborn for that.
So SNS dot regression plot.
And I'll do, I'll pass it the X and Y this way.
And so when you can see here, the regression plot does the same thing as above,
but it actually fits aligned in the data and gives it a bootstrapping in the middle of it.
This bootstrap is done by a confidence interval of 95%.
And it bootstraps a thousand times the underlying data to actually figure out what the variance is.
So this is a kind of neat, very quick way of getting an initial look at two variables that you think might have a relationship and they clearly do.
Now, if you'd like to actually save this figure as in this isn't just to look at and have it for later on,
you should actually save this figure into the figures directory.
So I would do PLT dot save fig figures.
And I like to actually have the same beginning date structure for these figures so that if I am looking through the figures directory later on
across all the different notebooks that I'll be looking at, I can easily re-correspond which figure came from which notebook.
So this is just a little bit of mental accounting to get this straightforward.
And let's do employees versus hours.
Let's keep our underscores and spaces being the same.
All right, so that's our first look at the data and it is a quick linear regression plot against two of the features that we found inside,
as well as a bit of data frame manipulation using pandas.
We've seen a very first look at this and we see that there's at least some trends in this data.
There's probably something pretty interesting in here.
So I'll keep going with this data set and seeing what I can come out with this.
Now I will actually remove this header and I will toggle the toolbar as well as I need space.
So let me go ahead and do that.
So previously we saw with seaborne a really nice regression of the average number of employees versus labor hours.
Let's keep seeing what's in this data set.
Let's take a look at the columns for column in.
So these are the columns in the data frame.
We have year and then various things about the mind itself, the name, the state, the county, its status and its type, the company type, union code.
There's a coal supply region, the production in short tons and the number of employees in labor hours.
So see if the amount people work, like the labor hours total is very predictive of the production in short tons.
Let's take a look at that scatter plot.
Let's take a look here.
So it doesn't appear to be a fantastic relationship here.
Let's take a look at the actual histogram of this.
So I'll do df production short tons dot hist, which is a function on pandas.
And we see a very bad looking histogram.
So it looks like a lot of things in this first one, which is either typical of a power law or some other kind of problem.
Let's do a few transformations on this production.
Let's see if we can find the minimum value or yeah, let's take a look at the minimum value zero.
Let's take the length of the data frame where this is equal to zero.
So if we did first, let's just look at this where the production short tons is equal to zero.
We have what's returned as a series that tells us false false true true false and so forth.
So this tells us whether or not the production is equal to zero.
So we say df where you actually give this as an argument to data frame saying where this is equal to zero.
We get the full data frame where all of the production values are equal to zero.
And it looks to be like quite a few of these things produced zero tons of coal.
In the interest of how much a coal mine is producing, let's take the ones that have produced at least one ton.
We will say the data frame where the production of short tons is greater than zero.
This has, okay, values that are not zero. This is good.
From here, we will now set the data frame equal to this.
Now we are at this point doing a slice.
So I will make a note here.
We are removing data here.
That's okay as long as you're keeping track of what you're doing and why.
So the reasoning behind this is if we're going to try to predict, let's say the production of mines
and use things like what state the mine is in as a predictive indicator.
Let's actually restrict ourselves to mines that produced something more than zero.
And that's the reasoning behind how I choose something like this.
So now data frame is equal to where the data frame production values is over zero.
So let's see what the length of data frame is now.
Okay, so we have 1061 data points.
Let's redo this one.
I'm going to copy this and place it down here just so that we can do a comparison.
And it appears to still have quite the skew distribution.
So I will try to do something now where I will actually take the log of this.
So let's create a new column.
And the way to create a new column in pandas is to actually just create a column as though it exists
and set it equal to a function of this.
So I don't know if I have NumPy installed just yet.
So I'll give this a try.
So let's go to the top of the page.
And in all of our imports at the top of the notebook, I recommend keeping them together
so that if and everyone later on can see where things were imported, import NumPy as NPs.
Now this input is 30.
I've imported it and I should be able to rerun this one all the way to the bottom here
and create a new one.
So let's look at df.logproduction.hist.
So what we see here is a very close to a log normal distribution.
So the production of coal mines follows a log normal distribution,
which is reasonable from first guesses.
All right, great.
So I think I'm going to stick with this as a thing we're going to be interested in predicting.
So we have our prediction variable.
Now at this point, we've done quite a few things to the data frame itself.
So we loaded it in, we renamed the columns.
We actually created what's going to be my target variable is going to be the production of these mines
and did a transformation, which is the log of this value.
So after doing all this, I think I would like to actually save out this data frame
that I can load it into any future analysis.
So I'll do df.2.
Let's save it as a CSV.
So I'll call it, let's find it in the data directory, coal public this thing.
We'll do cleaned version of this.
And it's a CSV.
So now that I've done this exploratory analysis, I would have this first look that I've taken at
and I've saved the data out into this CSV file.
I'm going to copy this into a new one that's going to be called data cleaning.
And in the future, all I'll have to do is load in this CSV file
and all the transformations will have already been done.
And I'll have a link back to the reasoning behind it as well as the actual code that does this process.
In this video, I'll be cleaning up the data cleaning notebook
and I'll be doing our first commits to a new branch to keep everything organized.
I last laughed off with this first look and their develop directory.
So what we're going to do now is actually make a copy of this
and I will toggle the header for this.
Make a copy.
And the first thing it does is it opens a new tab with everything copied in the previous one.
And none of the code has been run here even though all of the inputs have been copied.
What we're going to do here is actually call this something completely different
which is data cleaning.
I didn't put a date in front of it because this is the notebook
that's going to be the one that people look at if they actually want to see how we changed the data.
So I'm going to actually close this from this directory,
go over to my actual terminal here
and move from the develop the data cleaning ipython nb
which we just created into the deliver.
So we move the file from develop into deliver
because deliver is the directory that people should be looking at
if they're actually interested in seeing the final analysis that matters.
In this case, we don't want to hide data cleaning in this development directory
which has many, many files.
So we've moved it into deliver and if we go back to our browser here
go up into deliver and open up the data cleaning.
Now we should actually start to do things like actually creating the markdown file
changing the code from code to markdown
giving it a nice title and continuing on with this.
So we can say Jonathan by Jonathan to say like who actually did this
and then you can look it up in the get repo cleaned up the data
removed zero production coal mines.
You can actually do a bit more of that in the end
but for now that should suffice.
We don't need to actually have any of these plots in here.
So I'm going to be cleaning this up as quickly as I can.
So numpy as NP pandas as PD.
We need to read in the file still.
We don't need to see the head.
We know what that looks like.
This can be left in because it tells us the transformation we made and why
the head part doesn't need to be here for the second one
but we can add a note above it that says mistake
renaming indipedent to independent.
Now we're in here changing spaces to underscores
double check that still looks right.
Okay, it does and we will now delete this head,
delete the different plots here and give an extra sentence here.
Coal mines without any coal production are removed.
The length is 1061 and we are now creating a new column called log
production which is the log of the production of the data frame.
And we can put we don't have any histograms here.
We need that out and now the output file is this guy and I will
actually move this to the top here to the output file.
The very first thing you see here will be the name of the output file
and the last thing we'll do is actually write that CSV to that output file.
So now when I load in this cleaned coal public 2013 and notice
that I did not overwrite the old file.
So I strongly recommend keeping the raw files and creating a new file.
That's the cleaned version of it so that if you ever made a mistake
in your cleaning which has happened before,
you can easily revert and change that back.
And if someone says, oh, something happened in the cleaning process,
you have a full documentation of what happened here.
So we've created the final document that went through and cleaned up
what actually happened in the cleaning process.
So anyone looking in the future can easily follow what happened.
So I will now close and halt this directory and I'm going to actually
do our first commit and we are in the master branch as it sits.
So I will actually check out a new branch.
The branch will be called JBW underscore predict production.
And so we're here.
There's two theories here on adding the data.
So the data here is actually pretty small.
So I'm going to add it to this.
This is also so that you can actually get the data as well.
Generally in a production environment,
you don't add the data to your Git repository.
This is stored and tracked in some other way.
So I'll add the data cleaning.
I'll add develop and not going to add the figures just yet.
I usually will only add this when I actually have something interesting there.
So this figures is going to be kept on my own directory for now,
not put into the branch just yet.
Let's look at the status one more time.
So we have a number of new files, the actual data file, the cleaned data file,
the data cleaning that is the official way of actually making this file
and this develop one.
So let's commit this.
Let's not call it that then.
And I have to actually configure this.
So I will configure my Git.
Do this commit and continue this on in just a second.
So commit the data and I will be pushing it to GitHub.
So the final command I ran was git push origin JBW predict production.
And this means that I have now sent this off to GitHub.
Go back to the GitHub of the coal exploration, reload this.
What we see here is the master branch where you can actually go to the
JBW production branch and see the various things we've done here.
Let's actually look at the deliver and click this IPYNB.
And we'll notice that GitHub does a fantastically nice job of actually rendering
the notebook as it looks correctly.
And this is even more dramatic when you actually look at the develop one.
So we can see this and you can see in here if you're browsing with GitHub,
the figures are faithfully reproduced here.
And this is a very useful thing to be able to look at the files being used
and especially when we do a pull request in the future.
Okay, so we've cleaned the data.
We have the way that we cleaned it separated out so that anyone else
can look at it in a reproducible way.
And so let's actually try to predict something.
So I'll go back into this develop directory and it will make a copy
of the first look notebook that we had.
So I'll make a copy of this and I'm going to call it CoalPredict.
I'm going to go back to the previous tab and actually finish closing this
and halting it.
And just to give you a sense of how everything is standing,
I'm now back at the home of this develop thing.
You can see the first look notebook and it's currently black because it's not running.
This one is green because you can see on the right here it says running.
So this is a notebook that's currently being run.
There's a couple of things I want to do different here since this is now the prediction one.
When I start off by saying what the goal of this notebook is going to be
and because everything that's here is a direct copy of the previous notebook,
most of this stuff I'll just be able to delete.
So I'm going to toggle the header, give us a little bit more space
and the changes I'm going to make are basically going to drive me toward being able to make this new prediction.
So first of all, I don't want to reproduce all this cleaning I did before.
So I will actually instead of reading in the previous raw data,
I'll actually go into and read the CSV that we saved.
And this is up into the data directory and it's the cleaned public CSV.
And we still need to set the index column to be the MSHA ID.
So that's loaded in and actually one thing I like to do is look at the head of the data frame
and read it in at the same time in case I need to make any changes.
So the way to do this is since the four is selected with a gray box,
if I hold down shift and type K, I'm selecting both the second and third cell which are index three and four.
If I type shift M, they are now combined into a single merged cell.
So let me just run this one cell and I read in the CSV and then you are seeing the head of that data frame as well.
So we can see that we're loading in the cleaned CSV and the head is looking nice.
I'm going to now delete a number of these things because we don't need them.
One thing I will remain is that we initially did this LEN of the data frame before.
This was on the first one that you saw on the raw data.
So since this is the clean data, I expect this to be just over a thousand.
Yep, it went to 1061.
Simply delete these.
I'll leave the number of columns in here so we can actually think about what's in each of these columns a bit.
Alright, so as we see, this is the production.
Longer the production is the thing that we're going to be trying to predict.
And let's take a look at just a high level view of the different categories that might be able to help us.
So let me get the columns here.
I think that the mine status might be a predictive variable.
So I do df.mine status.
You see that there's an active men working, not producing, permanently abandoned, active,
temporarily closed and new under construction of the different status types.
I suspect this will give me a pretty good predictor into how productive the mine actually is.
So I will actually do a group by on this to see what is in here.
So df.mine status.
Let's do production.
What I did here was I said, take all the ones that have the same status of active and take the average or the mean of the production in short tons.
And we can see that the active ones are much more productive than the temporarily closed ones or the permanently abandoned ones.
It's interesting to me that permanently abandoned has on average 60,000 tons.
Let's look at it in terms of the log of the production though.
This will be what I think we're going to be going against.
So huge difference in the overall production capabilities, but we'll see how good this is at making a final prediction.
So from here is we would like to predict the log of coal mines.
And we'd also like to know what actually leads to the production, higher production and lower production.
If we look again at all the columns in our data frame, the data that we have year is the same for all of them.
And various things that shouldn't matter at all.
Like the union code is just going to be a code that's given to the mine from a, let's just look at that.
Actually, that might be predictive.
So I'm going to try to throw as many of these things as we can into a predictive model.
So I'll call these features.
And let's start with this as our list of features.
We'll have our target be log production.
So year is going to be entirely unpredictable because it's a single thing.
Mine name, I suspect will not be predictive because it's simply the mine.
The state might be what state is it in, what county is it in that could be useful.
The mine status, I'm sure will be predictive.
Mine type will probably be it's possible that the operating type, the address of the operating company probably isn't because we already have the geographic things done with the county and the state.
Though it's interesting, we'll definitely have some collinearity between the state and the county.
So it's possible that particular county and the state's good.
We'll leave those in.
Leave in the union code, the coal supply region.
We can't give it the production of short tons as a prediction of the log of the production because that's cheating.
The number of employees that are employed and the number of labor hours.
Just to clean this up.
So I hold down shift and push the down arrow key and I've highlighted everything to indent.
I'm going to hold down command and hit the right bracket key, which is the square brackets.
So the parentheses are curved all the way around.
There's curly braces, which have a lot of curls in the square brackets.
So holding down command and typing the right one will indent an entire block of text.
If you do the left bracket, it unindents.
This is a quick way of formatting lists.
So the features that we're going to be giving our model are going to be all of these features here.
The target's going to be the log of the production.
Now of these, I think only two of these are actually numbers to start with.
So I think average employees and labor hours are the only ones that are proper features.
And the rest of them are what I'm going to call categorical.
So the categoricals are these minus the average employees in the labor hours.
And having a trailing comma here is actually okay.
We need commas between all the rest of them otherwise.
But this is one of my favorite features of Python.
And I don't know why it makes me so happy.
But having a trailing comma and having it not have a problem just makes me really happy.
So the features, which I'm going to just call the ones that are numeric,
are the average employees and labor hours.
The categoricals are the ones that are category variables.
So mine state, county, status, type, company type, operating type, union code,
and coal supply region are all categoricals.
One thing that we'll have to do is create, because we'll be using scikit-learn,
we'll have to turn these categoricals into numbers or into some sort of numerical thing.
And we'll be doing that with what's called a one-hot encoding.
Also called dummy variables. There's probably a few other names as well.
So we split this up into numeric features.
So things that have numbers representing how long people worked,
how many employees a mine has, categorical, which is what state
or some other thing that actually has a category,
and the target variable, which is log of the production.
From here, we need to do a bit more data munging after it's all been cleaned.
We now have to do some munging to make this into a form that scikit-learn can actually predict with.
In this lesson, we'll be looking at the final data munging and the final prediction for this data.
So I've actually changed up this slightly.
So the features that we'll be looking at, these are numeric features to start with.
The average number of employees per mine and the number of labor hours total worked for that mine.
And also a categorical list. This categorical list contains features
which have a small number of string representations instead of actual numbers.
And again, the target we're looking at is the log value of the production in tons.
So one thing that I recommend you doing is taking a look at the interplay
between each of the variables and the target variable.
So I'll do a quick example of this.
So let's take a look at the relationship between mine status,
which is a categorical variable, and the log of the production.
I'll be doing that with this Seaborn code here, which I just executed.
And the set context has to be run twice the first time.
What this is doing is doing a violin plot.
So this is the Seaborn library SNS, and it's creating this.
It's using the violin plot function.
And what we see here on the y-axis is the mine status, the five possible values,
active with men working but not producing, permanently abandoned, active,
temporarily closed, and new under construction.
And on the x-axis, we see the log of the production.
So you see that each of these mine status types corresponds to a different log
of the production value of that mine.
But also the distribution has this interesting shape,
and it changes between these categories.
This kind of a plot is a very nice high level view of what these variables
interactions look like.
I'll do just one more.
How does company type corresponds to the production?
So we see that there are three company types here, independent producer,
operating subsidiary and contractor, and each of those corresponds
to a very different distribution.
So you can do this for all of the variables, and I recommend doing that,
especially before and getting a sense of what the data actually looks like.
But for us, we just look at this company type a little bit more closely.
So if we do a DF company type dot unique, we return all the unique values.
Of course, we see the three that we see in the plot above.
An independent producer operator operating subsidiary and contractor.
The scikit-learn functions don't take in these strings as separate
category variables.
We actually have to encode this ourselves.
Now one way to encode this would be to do something like assign
independent producer to be one, operating subsidiary to be two,
and contractor to be three.
And that would work except that we are then implicitly telling,
let's say a scikit-learn random forest function that three is greater than two,
which is also greater than one.
And there's an implicit ordering there.
And it might start to try to cut the features in a way that doesn't make sense.
A more safe way to do this is to actually create what's called dummy variables.
Pandas has a built-in dummy variable function.
So we do PD dot get dummies on the data frame with just,
we're looking at the single column of company type.
And I'm taking a sample of 50 so that we get a mix of types,
because it's actually ordered in this data set,
and just taking a look at the top 10.
So I'm going to run this a couple of times.
This sample will actually re-sample every time I run it.
So what we see here is the contractor independent and operating subsidiary,
this MSHA ID corresponds to an independent producer operator,
because it has a one in that column,
and zeros in each of the other columns.
And if you go down to this 4407123 ID,
it is an operating subsidiary company,
and it has zeros in the rest of the column.
So this is what the get dummies function does with pandas.
Now what we want to do is actually turn each of the categorical variables
that we're looking at into dummy variables.
And then we'll actually learn to drop one of the variables
to avoid the dummy variable trap.
We're then going to concat the data frames together.
So we're taking the data frame and the temporary data frame together.
And axis equals 1 means it will add it as columns to the existing data frames.
And we will then drop the drop variable from the data frame
and call that to list function on the columns of the temporary data frame
so that we have a final list of what the dummy categories look like.
Let's run that real fast.
It completes very quickly.
We see that there are 29 mine states, 164 mine counties.
So this might be a little bit high.
We might have to come back and look at that.
The mine status, there's five, mine type 3, company type 3, and so on.
And the actual value of the dummy variables themselves,
let's take a look at say the first 10.
We see mine state, Alabama, mine state, Alaska, and so on.
So these are the different state variables
that have been created.
Let's actually start to build a model.
So we'll say, so I created this as a markdown
by typing escape to make me into select mode instead of insert mode
and typing m, m for markdown.
You can also go up here and click it.
So if I could go back to code, this is simply commented out Python code
as far as the notebook is concerned.
We actually want this to be markdown.
So we click markdown and you can see it pre-rendered
before we actually execute the cell.
And it looks like this nice bold font.
We're going to need to import a couple of things from scikit-learn itself.
So we're going to say from scikit-learn dot cross validation.
So this is the sub module of scikit-learn.
We're going to import the test train split function, which is labeled here.
And we're also going to use a random force regressor as our algorithm.
Loading that in, you look at total length of the dummy categoricals is 213.
The train and test is going to be the names of the data frames
that's going to be split by this test train split function.
The function takes in our data frame.
And you tell it how large you'd like the test size to be.
So in this case, we're going to have a 30% of the data frame is going to be the holdout set.
And the nice thing about this function is that we actually retain
the data frame structure of these variables.
Scikit-learn likes to think in terms of native numpy arrays,
but many of the features can actually read in a pandas data frame as well.
And the utility of having a pandas data frame around just makes it really nice to keep it,
to stay in data frames as long as you can.
So we can actually do it the whole way through. So that's really nice.
Our train is a data frame. Our test is a data frame.
And they've been split from the data frame that contains all of our data.
So now we're going to create a random forest.
And I would like to run these separately.
So I'm going to split this cell here by typing control shift minus splits the cells into two.
And I will execute this one.
This says RF is an instantiation of this random forest regressor,
which we imported above.
And there's two things we're going to initialize it with.
Number of estimators is 100.
This is a number of trees that we're going to be building a random forest out of.
And whether or not we're going to be using the out of bag score,
which we are in this case.
So we have an RF model and we'd like to fit on this by giving it
x comma y and sample equals non as default.
So the x value is the design matrix.
The y is the target variable.
So in our case, we're going to do the train data frame.
And we're going to give it all the features,
which is just those two average employees and the total laborer,
as well as the dummy categoricals.
Now, these two things together is just adding them together.
It creates a large Python list.
We can see the top two things up here at the top,
average employees and labor hours,
and then everything else is dummy categoricals.
We then run the fit method on the random forest
and we can get the design matrix of train features
plus dummy categoricals and the target,
which is train just selected on the target variable,
which we defined above as log production.
So it tells us some features or it gives us a little summary
where it talks about the bootstrap, the criterion as mean squared error,
various other things here.
So this is all the variables that you can change very easily.
If you'd like to actually tweak this for your own problems.
So let's take a look at how well this does.
And we're going to do this by giving a seaborne plot again,
a regression plot, but except the train,
we're going to be using the test data frame.
So I test the target and the regression plot here is going to be
in target versus what we actually predict this to be.
So the actual is along the x-axis here.
This is what the actual production is.
And the y-axis is the predicted value.
I can actually add that in.
I think it should be there.
So we say predicted production.
So predicted production is on the y-axis
and the actual production is along the x-axis.
So perfectly calibrated and perfectly predictive thing.
Everything would line along this one to one ratio line here.
We see that there's some scatter around it,
but actually it looks like it's a pretty good overall predictor
of the actual production.
We'd like to actually see how good is this fit
rather than just look at the plot and say,
oh, it looks pretty good.
So let's import a few of the test metrics that we can actually look at.
So we can say we can import explained variance score,
the R2 scored, and the mean squared error.
So the way these functions work,
they always take in the true and then they take in the predicted.
So this is going to be test target
and then the predicted test target.
And actually I think this way of writing it is a little bit too verbose.
So I'm going to call it predicted equals this.
And I'm going to say predicted here.
So the R squared score is 0.88.
Explained variance score is 0.88 as well.
The mean squared error is 0.54.
And now because this is a random forest,
we actually have the feature importance of the model.
And I don't know of a good way that's naturally given by scikit-learn
to actually report this,
but here's a little bit of code that I have written to make it
so that I can actually read this in a way that I actually think is useful.
So I'm going to create a new pandas data frame
called rf underscore importances,
which actually takes out the features and the importances from the fit model.
And I'm going to look at that at the top 20 here.
All of the importances of every variable we give it to in total adds up to one.
So we can think of this as fractional importance
in terms of what the random forest has decided
is going to be discriminative in giving us a final regression score.
So of utmost importance is the labor hours
and then average employees is down from there.
The mine type being surface is predictive.
The mine county being campel
and coal supply region powder river basin is apparently moderately predictive.
And then it goes down from there.
So this is just the first 20.
And we have not only a final fit with a nice plot,
we also have some diagnostics and metrics,
as well as a list of what's important.
In this video,
I'll be showing you how to take a development lab notebook
and turn it into a deliverable notebook.
So let's go into our directory and we go to the develop folder.
Clicking that we navigate into that folder
and we see we had our first look notebook
and then this coal prediction notebook.
And what we'd like to do is make a copy of this notebook.
So you can select it by clicking this checkbox here
and clicking duplicate.
When we do that, we have a second copy of this,
which is added to the end of the name copy one.
Now this file should exist in this directory and we see it here.
Copy one.
Because it's going to be a deliverable notebook,
we should actually move this into the delivery folder.
So let's move 2015 coal predict copy into the deliver directory.
Go over to the deliver directory
and let's navigate there with the notebook server itself.
Let's open this up.
Okay, so let's first give this a title
that we think is an appropriate title.
And because it's going to be a deliverable notebook,
it shouldn't start with a date.
So it should start with something like coal prediction of production.
So we have a new name for this.
You can save this and I'm going to toggle that header bar.
So I have a little bit more space
and I'm going to toggle this toolbar as well
because I'll mostly be using keyboard shortcuts.
So at this stage, we have this long notebook that went through
and it's a complete copy of our lab notebook style.
So we can delete things here pretty freely
and just focus on the main story that you'd like to tell
to either your teammates or your manager
or whoever is going to be consuming this.
So keep in mind with your audience what you think they would like to see
and cut out the extraneous stuff
and adding in as much text as you think is useful.
And in that keyboard shortcuts,
especially are going to be make your life a lot easier
and make this whole process really fast.
All right, so let's just go through this.
And initially what I'd like to do is give a good title
and you can just call it coal production in mines 2013, let's say.
And so we have our first setup here
and you can also give a little abstracts.
You can say we did a lot of analysis,
came to some interesting conclusions.
Now, of course, fill that out with more verbiage as you see fit.
Keeping the code in this notebook is useful
so that someone else looking down the road
can actually reproduce all the key results
that you think you can find.
Now, this isn't always possible, but as far as it is possible,
I recommend trying to do it.
So try to keep the imports neat and tidy.
Keep only the imports that are required
and remove the ones that are extraneous.
I think we actually use all of these.
I would recommend keeping these magic imports
on their own line at the top.
So having matplotlib inline at the top, that is good.
Put a space between that.
The Pepe convention is to have one of the standard libraries,
like import string, let's say.
That would be next and any of the other ones here,
and then another blank line before third-party libraries,
which is what these are.
And finally, we have an actual plotting change
that we make with this SNS command here.
So we execute that cell and make sure everything is making sense.
Yes, we see this warning, we've seen this before,
so we're not too worried about it.
Now, from here on out, we should be making decisions
about whether this actually improves the story
for the person reading this or if it becomes just tedious.
And when you have data that's being imported
and it's changed from the raw data,
there's this clean data set here.
I think it needs to have some extra commentary around it
so that people know what's going on.
So I might say...
I might give it a description about where exactly it is
in this repo, and let's just type an ls here.
The name of the notebook is data underscore cleaning.
So we will say the same thing.
Double click, drag over, command C to copy that,
command V to paste.
And this ls command, which is handy, we can be deleted.
So typing escape to get out of the insert mode
so that the cell is now surrounded by a gray box.
And then typing D twice, it deletes that cell.
And in this cell, we are starting to write some markdown.
We can tell it's markdown because it's just a text
for people to look at.
But also, we've put a double header marking too.
So let's just change this cell type to be markdown.
So we're currently in a code cell.
We can change it to be markdown by typing M.
And as soon as you type M, it switches into markdown
and gives you a preview of what this will look like
when you render it.
So let's render it real fast, shift, enter.
And we see that this is indeed bolded.
This two pound signs, or hash signs,
means it's a H2 heading.
So this is H1, this is H2,
and it keeps getting smaller as you go down.
So in this case, I think clean data just deserves
a second level heading.
We said we clean this data in the notebook stored in this.
So deliver slash data cleaning IPYB.
So we've told people where this cleaned data file
actually sits.
And we actually know the exact steps that went through
to take it from the raw data into this cleaned data,
which we've pointed to here.
This head is actually quite a bit of text,
even though it should be the top five lines.
So if we're going to include something here to make sure
that the data is read in correctly,
we might select a few columns that we think are useful.
So in this case, maybe we have year and maybe mine name.
And so we read in just the heading with those two columns.
Okay, just to give people a flavor of what's in that data frame.
This length we don't need to worry about.
This column thing we don't need to worry about.
So we delete with 2Ds.
Now, consider the different plots that you have included.
And is this something that tells a story?
If so, leave it in and clean it up so that the axes
and the colors all look right.
If not, you can go ahead and just delete it.
So I think this is deleteable, also deleteable,
and finally deleteable.
Okay, so we get to the point where we're predicting
the production of coal mines.
Again, we're just looking at what the columns are.
We don't need this.
Don't need to know what unique year it is.
So this is required code, so we need to leave this in.
Again, clean it up if it needs to be broken up into different cells
or if you think it needs to be changed in some other way.
So let's delete a few of these empty ones.
And let's say we want to like to keep this.
Let's decide one of these violin plots to keep.
So let's keep the second one.
So I'm going to delete this one.
And to save this, I will say plt.savefig
and using tab complete and it'll help us know
where proper structure to put this in here.
And as I said before, I like to give the same name,
beginning of the figure that the notebook itself has.
So in this case, it starts with coal prediction
as the starting of this notebook name.
So that looking at this figures folder separately later on,
someone knows which notebook it came from.
And then what it's actually being plotted here.
So we have company type versus log of production.
So company type versus log production.
Again, we get a warning, but this should work out just fine.
Let's run this a second time to make sure everything.
Okay, so that looks better.
Running at the second time with the set context
actually lets the font sizes get to a nice reasonable size.
Okay, so we are saving this output.
We think it's useful for our story.
We again don't need this or looking at this.
So just typing DD to delete these cells.
We need to create the dummy categoricals.
This is required for our analysis.
We don't necessarily need to actually print the categoricals each time.
So let's run this comment out that line
and just double check that that is the same answer as before.
Okay, let's delete that.
And we've made a note here about avoiding dummy variable trap.
You might decide that that needs to be elevated from a comment
and to mark down cell above it.
Okay, so let's leave it as a comment here.
And we don't need to actually look at the categoricals for the final report.
So let's just delete that.
Build our model.
Let's call it a little bit something more descriptive.
So it's going to be a random forest regressor.
And we should always put all of the imports
all the way at the top of the notebook.
And so let's move this to the top.
But first let's combine a few of the other imports.
I think I have a few more imports down here I do.
So let's move this, I'm going to turn on the toolbar
and move this up so that it's next to the previous one.
I'll scroll back down and see if I can find another import.
It looks like it should be everything.
A keyboard shortcut that I find that I'm using all the time
and really saves me time is knowing how to merge and split cells
with keyboard shortcuts.
Knowing this will save you tons of time with moving your mouse around.
So we currently have input cell 30 selected.
We can type shift and hold it down and then type K.
We will now select the cell above it.
We can select as many cells like this as we'd like
or unselected by typing J to go back down.
Also, if we go J from here,
we can select down from the current cell that's selected.
But let's go up shift K.
We have selected two cells to merge this type shift M.
So we've now merged those two cells together.
Again, you can do that for 10, 20 cells,
or you can easily split them again.
I've said multiple times, control shift minus, splits in part,
escape, shift K, shift M, merges them back together again.
So this needs to go at the top of the notebook.
So I will put this to the top by typing this up arrow.
So bear with me for a second.
And we need to merge these two cells
and then do some recombination.
So shift K, shift M, type return to get a cursor in the cell.
And we're importing things from sklearn,
which should go after C-Born,
for this set command.
Execute that, and everything looks good again.
Let's scroll back down to where we've made our progress.
Down to here, we don't need the length of our demi-categoricals.
We do need to test train, split our data.
Let's merge these two cells by typing shift J,
shift M, and let's just leave that middle line.
Execute, shift enter.
And look at our final plot here.
And this looks like a reasonable good plot.
Thing looks nice.
Let's save it out again into the figures directory.
Let's call this coal production RF prediction.
Great. So we've now saved this out.
And we can do our various scores that we'd like to do.
If we're going to be printing out this output for consumption,
we should make this look a little bit prettier.
So let's just do this first one.
And let's combine these two cells.
So now we have the R-squared score
and our mean-squared error scores.
And finally, our random forest importances.
And let's just look at the top five.
So the top five are labor hours all the way down.
Cool.
So we've done a lot of rearranging of the code.
So at this point, I think it's crucial to restart the kernel
and try to run the entire notebook again.
If you have some process that actually takes a very long time,
you can decide not to do that.
But this, you'd have to take a little bit more care
into making sure that each piece runs correctly.
But in this case, this entire analysis runs very quickly.
So we have no problem clearing all outputs and restarting.
And clicking cell run all should run every single cell.
If we've deleted some piece of code that was necessary,
we'll have an error and we have to go back and correct that.
Let's go through all the way down to the bottom thing was actually done.
If there was an error, so let's say it would stop at this fifth cell here.
It would have an error printout here and nothing else would be executed
below that when you do this run all cells.
That's a good way of identifying where the error happened.
We don't have an error, thankfully, so that's good.
We do have something that is somewhat annoying to me that this has to be run twice.
As we can tell, we run this a second time we get our font gets bigger.
So I think I know what happened.
I set the context after I set the figure.
So I'm going to re-align the order of these two pieces of code.
Save this, restart the kernel, clear all outputs, cell run all.
And now we see that the font size is the correct size
and we've run all the way to the bottom.
And each time we run this, do note that we are overwriting these figure files,
which is what we were hoping to do,
but also keep track that is what you indeed want to do when you're running this.
I guess a good thing to add at the end, of course, would be some sort of conclusion,
so we can just add a conclusion statement.
Okay, so a detailed and amazing conclusion goes there.
So we're done with this.
We will close and halt and we need to submit this to GitHub.
So we can do a get status back at our terminal.
We've modified a figure.
We have added a figure and we've created a new file.
So let's add those type get status to make sure we know what we're doing.
We are adding two new files or modifying another file.
This looks good.
So get commit, get push, origin, Jonathan prediction production.
And this should be sent up to GitHub and everything is now
up to date.
So let's go to our get repository.
So in my case, JBWit Coal Exploration.
And there is a new branch which we can click on.
And if we click on the deliver, we should be able to see our Coal Prediction Production Notebook,
including all of the code and everything else in here.
In this video, we'll be talking about how to do a pull request and how to merge this back into a final branch
so the team members can review it and check off on it.
All right.
So we last left us.
We had just put in our deliverable notebook that talks about the Coal Prediction Production.
And so at this point, after pushing it to master, we have this branch.
If you go back to the home directory under your username and whatever you've named your data science project,
you can actually see this button here called new pull request.
And I like to switch to the branch that I'm going to generate the pull request from.
So this is all predicated on using GitHub as your repository of choice.
So after you click new pull request, you'll ask you to do one last step here where it'll say you're creating a new pull request
by comparing changes across two branches.
You're going to be taking stuff from this Jonathan Predict Production branch and putting it into master.
And GitHub does this nice thing which says it's able to be merged,
which means that if it's approved, it can just be approved at the single button click.
That's always nice.
So give your commit an extra bit of detail here.
So say something like final review.
And then if you want to leave a few more comments, create pull request.
So now what this does is it creates a pull request and lets you see the various commits that have happened in this branch
and allows a person who can possibly merge this to review the pull request.
So a person coming into this would see who's not me, for example, would look at the pull request and see that there's one open pull request.
And it was open 25 seconds ago by me.
So if you click on this, then you'll see the comment here.
Please check the figures especially.
They're going to be put into a slideshow.
Okay, so this must be pretty important.
And so I'll take a look at the different files that were committed.
So I'll click to the files changed.
I see that we have a couple of notebooks and we have a couple of figures.
So let's take a look at, let's say this current figure here.
This one was added.
Let's say we want to change that color.
So in the pull request, you can actually make changes.
And this is where you actually wanted to be doing this.
You click in the conversation part of the pull request, say, I need a few changes.
Add a comment.
Now, of course, I'm commenting on my own pull request.
Normally what happens is you make a pull request and your team members or your
manager will be actually the one reviewing the pull request.
But in this case, just for demonstration purposes, I'm both the submitter and the
reviewer just so that it's easy to see what needs to happen.
So added a comment.
I need a few changes.
Please change the figure to be green.
Okay.
Now that we go back to our terminal, we see that still on the Jonathan prediction
production branch, so we'll need to make some changes to the pull request.
So this is actually pretty simple.
So I'm going to switch tabs back to our deliver directory that is running under the
Jupyter Notebook server.
And so let's go into this cold predict production and make the requisite changes.
Now we'll have to actually shift return and work our way through this so that
everything is loaded into the namespace.
So that one is probably the one that should stay the same.
We get down to this one here where sure enough, the figure itself is printing
something that's blue.
We want to change this color to be green.
Okay.
In this plot, we will actually make the color equal to green.
C is not what it takes as a thing.
So we'll see if color works and color is indeed the keyword.
Okay.
So changing the color to be green, the figure is now green and we have
overwritten that figure file.
So cold production RF prediction is now a green plot rather than blue.
And so we can want to redo everything just to make sure that you haven't made
any catastrophic changes.
You can do this one more time.
Takes just a few seconds to go through the entire pipeline and save this file
close and halt.
Go back to your terminal, get status.
Two things have been changed.
And that's as we expect.
They changed the notebook itself that created this figure and the figure
itself.
So let's add those two files.
Those two files have been modified.
So we then get push origin, your branch name, and it's now updated on GitHub.
The nice thing about how GitHub handles these pull requests as a tab back to this
Chrome tab, this commit is already added now.
You actually can see the commit that was done here.
And if you click on that commit, you get to see that things that were changed.
So the few things were changed in the IPYNB, which is not shown partly because
the actual changes in the notebook don't look so great.
But the change in the figures has been changed.
So this figure, the blue one was deleted.
And the one on the right, the green one was added.
So this is one of the reasons that changing it in the notebook, which it
actually did.
So it changed the embedded figure in the notebook.
It's hard to see the differences there.
This is why I advocate creating these figures in a separate folder and a separate
PNG file for each of them.
So you see the diffs in the figures if you have feedback on the output.
Now, as an extra piece of sugar or something nice that GitHub has given us,
there's this to side by side approach where you can see what was deleted and
see what was added.
You can also choose the swipe option where as you swipe this thing across the
figure that you've just done, you can actually see the changes that have been
made, which is turning the figure green.
Last one is onion skin where it fades from the entire thing from behind.
So this is what it currently is.
And previously it was blue.
You can see this.
So having this functionality is actually really nice.
And another reason why I advocate for this figures being submitted separately.
Just a final note, you saw that the points are slightly different in this swipe.
And that's because during our test train split, we were taking a random selection
of points that were going to be the testing set and the training set.
So those differences, well, shouldn't matter much and they don't change the
actual scatter points, but the fit itself, as you can tell is almost completely
unchanged.
It's actually a nice robustness check to look at this as well.
So once I've looked at these changes, I can now go back to this pull request
branch.
So I need a few changes.
Please make the figure green.
I committed made the figure green.
The only thing I need to do to update this whole threat of changes was to just
say get push origin branch title.
So I'll say it looks good to me plus one.
And then clicking merge pull request will take everything from this branch
and pull it into the master branch.
So I'll say figures ready to be put into a slideshow.
So once you pull request is successfully merged and accepted, then you should
delete the branch to keep these branches from floating around.
So I just deleted the branch on GitHub and should now do the same thing
in your local environment.
So first I'm going to check out master and I'll say get pull origin master
to pull everything down from GitHub and all these changes have been made
and say get branch minus D Jonathan predict.
So I've deleted the prediction branch and get does a final check to make sure
that any of the changes that have been made on that prediction branch have
been already pulled into master.
So if you just try to do this and it doesn't think it's been fully merged,
you get an error at that point and you have to figure out what happened at
that stage.
In this video, we just over reviewed the basic process of going through a
pull request and how the peer review process works in a pull request.
So we saw how to merge our development branch into master after doing a pull request.
In this video, we'll start our data science project number two.
And in this project, our main focus will be to focus on various plotting
and statistical libraries that I think you should know about.
All right, so to start a new data science project, let's start out by
going to GitHub and signing in going up to the plus by our little icon and
clicking on a new repository.
So we can call this data vis project to in this case and give it a
description that says I will make it public.
So you can see this project as you go forward.
We'll initialize with a read me.
We will include a Python dot get ignore.
We'll add an MIT license and create the repository.
Once we've created it, go to this SSH option, click in this box.
It'll select all the text by default command C copies it, go back to our
terminal, say get clone and then command V to paste that URL.
All right, so let's CD into data vis projects and look at what we have here.
And we're currently on the master branch.
So first step, let's create a development branch and we'll call it
Jonathan vis and let's create our normal directory structure.
So we have data deliver develop figures start with and I happen to
know that I've already started a few of these notebooks.
So I'll move them from a previous location into our develop folder.
So let's look at our develop folder.
Okay, we got some stuff there.
And now that we have a new branch and we have a new directory structure
and some stuff to look at.
Let's start up the Jupyter notebook server.
All right, so we see the same directories we were just looking at in the
terminal.
I will now right click on this tab and pin this tab so that it goes all the
way to the left and stays in place so that if I have a lot of tabs because
I'm searching for a bunch of different things, I always know where to go
back to find the home server directory.
And I just find that useful to pin that tab all the way to the left.
All right, so let's take a look at some of the notebooks.
I've already pre populated what I'll do here is I only have my usual date
and then my initials at the top of the page from the actual name of my
notebook, just including your short description, which is exploratory
data analysis, which is pretty long title.
So I'll do all caps EDA and that is a standard way of talking about that.
So I'll view and toggle the header and toggle the toolbar just so that we
have some extra space.
Remember, if you want to save it when you're in this kind of configuration,
you just command S to save it.
So one more time, I'll just give you a brief overview of what I'm hoping to
do here.
So this isn't to teach you how to do data science.
It's more of an exposure to the tools that I think most people haven't seen
all of them or haven't seen enough of them.
And I just think these tools will allow you to do your data science much
more efficiently and usefully.
I'll go over a few of these plotting and statistical packages that you might
not know about.
So the first thing we have is importing map plot lib inline.
Almost all these plotting libraries uses map plot lib.
So I'll be using that for now.
And I'm importing map plot lib dot pie plot as PLT, which is the
standard way of doing that.
Seaborn as SNS, which is the standard way of importing Seaborn,
importing pandas as PD, NumPy as NP.
I'll also load in some data sets from scikit-learn and importing some
stats models, which I'll be talking about at length in a later video.
Execute this cell.
Now, if I do shift return, it will execute it and go to the next cell.
If I hold down control and hit return, it will execute the cell in place,
and it won't go to the next cell.
So I can continue to stay in the same cell if I hit control and return.
I've used Seaborn in other videos, but I would really like to just double
emphasize how useful this is.
You can find the main library for this by Google searching Seaborn,
and Seaborn Python should do it.
And the top result is the statistical data visualization library here.
This is what you should see, something like this, unless he's updated the page.
And this website has a lot of really good information on it.
The documentation is excellent.
The features with these different tutorials is also excellent.
These images that you can click on here will show you different capability,
the tutorial and the gallery.
If you click on gallery, you get to see many different visualization types
that Seaborn makes really easy, especially like heat map.
That's a nice one.
Look through the example gallery.
If you have some data and you have some sense that you should be able to visualize it in a way,
see if Seaborn has a response to that.
So let's go back to our notebook and load in some data.
So Seaborn SNS has data sets that you can load in by default.
We will load in the Titanic data set.
This is actually the data of passengers on the ill-fated Titanic.
And it has various information about them, their age, their sex, their class of ticket.
So first class, second class, third class.
And it talks about whether or not they survived the crash.
So doing a factor plot like this where you set this G object to be equal to this factor plot
and then modify the G label like this.
This is modified from a Seaborn example, commenting out this hue equals sex line.
And I'll talk about that in a second.
But I will shift return and execute this cell.
What you see here is the survival probability against the class of passengers on Titanic held.
You can see that first class had by far the best survival probability,
followed by second, followed finally by third class.
So this is a very nice high level summary of the data that underlies this.
Some of the nice things about Seaborn is that you can actually give it dimensions to also give you the same plot.
So let's uncomment this hue equals sex line and see what that does.
So what you see here is each of these classes is now been split out by sex.
So male and female, survivability for first class.
You can tell the very high difference in probability for surviving in each of those,
whether you're male or female in each of the classes.
So this tells you a more rich and deeper story of the underlying data set than the previous plot.
And you can see the first, second, third class, all of the different responses here.
So this is just one aspect of Seaborn.
I recommend getting to know it and use it as much as you can.
And that's going to be all for this video.
We've set up in this video a new Git repository.
We've started a new development branch.
We have our directory structure set up as we like to do it for our data science projects.
And we've taken a look at the Seaborn visualization library.
In this video, we'll continue to look at some visualization methods and techniques.
So let's go on to exploratory data analysis two.
Again, it starts off the same way with Matplotlib inline and the various other things being imported.
This warning message, which we can ignore for now.
So we will load in this Boston data from the scikit-learn data sets.
And we will first of all print what the data dictionary describes it as.
The way this load Boston gets imported, I'm calling it a data frame dictionary
and just calling this description key.
So let's toggle the top header and the top toolbar to give us some extra space.
And we see that this is the Boston house prices data set.
Now, it's worth reading through this data set and knowing what each of these attributes actually means
because if we're doing a deep data science project, it's really important to know the attributes,
especially if there's only 13 of them.
But what the main takeaway will be trying to predict the median value of the house
and by looking at the 13 categories that predict this house,
we have 506 total instances of this data set.
The different attributes are crime, we've written as CRIM, all caps.
Zone or the proportion of residential land zone for lots over 25,000 square feet.
Indus, which is a proportion of non-retail business acres per town.
A dummy variable where if you're next to the Charles River, then you're equaling to one, otherwise you're zero.
The nitric oxides concentration in parts per 10 million.
The average number of rooms per dwelling.
The proportion of owner occupied units built prior to 1940, which is age,
weighted distances to five Boston employment centers, distance.
Rad is index of accessibility to radial highways.
Tax, the full value property tax per $10,000.
People to teacher ratio by town.
The B, which is the formula that says the BK is the proportion of blacks by town.
L stat, which is percentage of lower status of the population.
And median value, the thing we are tending to be predicting,
which is median value of the owner occupied home in terms of 1000s.
This is the information that the data comes from.
So it's from Harrison and Rubenfeld.
And this is all the information about exactly where it was taken from the stat lab library maintained at Carnegie Mellon University.
So this data dictionary as it comes from scikit-learn is not in my favorite format.
It's this weird data dictionary.
If we actually say type on this, it'll be this weird like data set bunch.
So instead of using it in the form that it's given to us,
I like to convert this into a panda's data frame because those in my view are much easier to use.
So we'll create a data frame called features.
I'll create a data frame called target.
Now features will take the DF underscore dict, which is the the scikit-learn bunch thing.
And the dot data element and assign the columns to this data frame to be the feature names.
We'll also do this with target.
So we'll do this with another create another pandas dot data frame to create the data frame.
And then it'll be this DF dict dot target.
So run this and we can look at the head of the features by doing dot head on it.
So here are the different values of the different features for the first five elements of our data set.
We have the crime number here, zone, the industry.
Are you close to the Charles River, the nitrous oxide, average number of rooms, the age,
all the different features that we're reading about before.
If we look at the target, we would see that it's a single element or a single column data frame.
So what we'll like to do is actually for most of our visualization,
we will like to put these two things together side by side.
Well, we can use concat for that pandas dot concat.
We give it a list of the data frames you'd like to concatenate together.
And we have to tell it which axis that we would like to use.
Now, I'm sure there's some very useful mnemonic that will tell us the right way to do it every time,
but I prefer to not trust that I remembered it correctly, but always test that I have it right.
So if we start out with axis equals zero and look at the head,
we will see that it's trying to combine it in a way that they're stacked on top of each other.
And there's two ways to know this.
One is that everything has a value except for medv, which is the target data frame.
All of them have nans.
And if we were to look at the tail, we will see that everything else has nans and medv has values.
That's one way to know that we've done it wrong.
So this is trying to do some sort of concatenating the two data frames vertically.
And if we do it axis equals one, we will see that we've put them side by side,
which is what we actually want.
And let's look at the head.
We will see that all of them are here, including medv being the very final column in this data frame.
So we now have a new data frame called df.
It contains a target and the feature variables underneath it.
Now to give you a sense of the data underneath it, there's many different ways you can slice and dice this.
One very simple quick way to start with is to iterate over all of the columns of the data frame
and to print both the column name and the number of unique values in that column.
For column in df, the data frame columns, print the column name and df of the column, the number of unique values.
This n unique is a method you can call on a data frame.
So there are 504 unique values in crime and there's two totally unique values in chance, which is a boolean value.
Makes sense, we'd expect that.
Some of them are pretty low.
So our ad, for example, is at nine.
Some of these have many values and they're continuous values.
Other of them have smaller numbers of possible values.
You can see rad here is this kind of numbers here.
One thing you might not know is that pandas not only has fantastic data frame support, but also has some very useful plotting tools.
So in this case, we will be importing a thing called scatter matrix from pandas.
And this can be done in a couple of libraries as well, but let's just look at the pandas version of this.
Recreating a figure with some plots in pi plot, making a large figure 12 by 12 fig size.
And we're going to call it on this data frame with some see-through value of alpha and the diagonal will be KDE, which is this kernel density estimation plot that we see here.
Again, we see a warning that we can safely ignore, but this is a very information dense plot.
There's no way to go over all of it in this video as we look at it.
But this, if you have your own data set, will give you a lot of things to look at.
What is being plotted here on the x-axis and the y-axis is every possible pair of the two columns in this data frame, which is why it took a while to actually plot this.
Along the diagonal, this KDE plot, it's showing interactions with itself or basically the histogram of that variable itself.
So this is what medv looks like. It's just this histogram here.
Along the diagonal, it's just a histogram of the values of that variable.
Everything else is going to be what the response from this variable looks like with every other variable on the x-axis.
So you can see a number of really nice trends here.
You can see some kind of this U-shaped trend here.
We see something that's basically a straight line, which means there's not much information there at all.
That's from the Boolean value.
We can see some of these have very fuzzy relationships where it's not really showing anything very interesting.
But spending some time looking at plots like this, getting to know your data set is a vital part of data science.
And I highly recommend looking at this.
If you have far too many columns to look at it in one, I would say this is probably too many.
If you have even more than this, though, you can take subsets of this and plot this with the same command,
but you would be giving it a list inside of double brackets of feature one, feature two, and so on.
And this will plot just those features against each other.
So there's a downside of that is that you're not getting all of the interaction terms,
but if it's a trade-off between possible to view in one screen or not look at it at all, I recommend that.
Okay, in the last video, we last looked at this scatterplot functionality within Pandas.
In this video, we're going to continue taking a look at this data and some of the plotting functionality that's built into the Pandas library itself.
Just as a brief overview, again, this scatterplot gives you a very nice, fast way of looking at all of the interactions between the terms in your data frame.
If you suspect that there might be something interesting going on with, let's say, rad, we see something happening here,
or this diagonal term for rad, the intersection of rad and rad on the X and Y axis.
You see a histogram plot or a KDE plot that shows a very bimodal distribution.
So you can take a deeper look into that and see what it looks like by selecting that column by saying df of rad.hist.
And we will see this bimodal shape really appear again.
So it's really values that are greater than 20 and then a bunch of different values that are around 10 and lower.
You can also, of course, select it if you have a nice column name.
In other words, there's no spaces or any other characters in that column name.
You do the same exact thing by doing df.rad.hist.
See the same exact plot.
When you see a feature like this, in this case, it might not make sense, but if you have the thought that,
you know what, let's actually consider this as two separate groups.
This bimodal characteristic should actually be characterized as really a high group and a low group.
One way to do this is to apply a lambda function, which will create a Boolean value of these values.
So everything down here gets one flag of the low group and everything up here gets the high group.
And so we will build up this command below by getting some intuition here.
So let's grab our data frame like this.
This apply function is a method that goes to the column that you've selected in your data frame.
And there's a number of ways you can actually call this apply.
You give it a function and the default axis is zero.
You can do it in various other ways, so you can access equals one.
But in this case, most of the time you'll end up doing a lambda function, which is an anonymous function.
It's like you define a function in Python, but you don't give it a name.
You're giving it via this apply method every value in the RAD column.
And you're saying for each of those values in that column, is it greater than say 15?
So 15 is clearly going to split us into the low and high group.
And let's just take a look at the first few values of that.
So I did not head on that to give us the first values and we see that is this X value greater than 15?
It was false, false, false, false, false.
And if we want to look at just what that head value looks like without the Boolean, we see that it's 1, 1, 2, 2, 3, 3.
So everything here is indeed less than 15.
So we have this function call, which will return a Boolean series false.
And what we'd like to do is say we want a new column in this data frame.
We're not going to overwrite this column, but we're going to give a new data frame that we're going to call radian underscore bool,
because we want to have a nice descriptive name of where it came from.
And the way you create a new column in a pandas data frame is you give a column that doesn't quite exist yet or doesn't exist yet in the data frame
and assign it equaling to something else.
So in this case, we have this rad dot apply lambda greater than or equal to 15.
And I'm just adding this as type bool.
Just to give you a sense that if it doesn't automatically get incurred into a type of bool, which we see right here, the d type is boolean.
You can force it by doing this as type.
There's other times when this is useful as well.
So I'll just leave it in here as kind of a best practices or a hint for future ways if you're trying to do something similar and having some problems with it.
So we've just created a new column in the data frame of rad underscore bool.
And if we look at what the type of this single value is this I location of zero is a boolean.
Let's take a look at the histogram on that now that we've created this new column.
And we see this perfect bimolality of 0 and 1.
That's one way if you have different features that you want to create.
It's very flexible to say if it's greater than 15, give it as bool.
You can also do something if you had a trimol tool or so three different groups or various other ways of slicing this.
Any function you can think of that can be written down in Python.
You can then use to filter out the columns and I recommend creating new columns, but sometimes you can overwrite columns if that makes more sense.
So after doing this we have another seaborne plot that's called a pair plot and let's execute this and then explain what's happening here.
This is very similar to what's happening above in the scatter plot where we're having the same x value versus y value and the where they intersect.
So this medium value here is the same intersection of medium value on the x and y axis.
Instead of a KDE or a kernel density estimation, which is that line, we did it with a histogram and that is a flag given right here.
I guess it's just the default value. It's under dyag kind equals hist.
And if we did KDE it would give us the KDE plot as before.
There's one difference here though where we've given an extra character of hue.
So there is a Boolean value which is are you near the Charles River and this is similar to splitting the Titanic data set into male and female for each class.
So it says give us the pairwise interactions between these variables. I just picked four of them.
And for each of these though I would like to see the differences whether you're close to this river split up by a different color.
So we have this hue value can take a zero or a one and we see if there's possibly different distributions behavior conditioned on whether it's actually close to the river.
So this gives you an extra dimension of interaction and interpretability so you can see like oh I see that there's a behavior but it only exists if there's let's say the green dots had a nice tight relationship here and the blue dots were all kind of all vague and all over the place.
And so if you looked at this without splitting by this Boolean value you might say oh there's not much of a relationship here but turns out that this underlying feature could have been the really important thing.
Now I don't actually see anything that jumps out at me in this case but having this availability is something that's worth noting.
So we'll do sms.kde plot and it'll be df.nox.
So we're seeing here it's not a histogram it's a kernel density estimation of the distribution of this underlying feature here.
So this is like a histogram but it's more of a smoothed out version of that.
This is what happens if you give this kde plot method in seaborne single column of values.
If you gave it two values let's give kde plot the Boolean value of rad versus the Knox value which we just plotted above.
And we'll get a two dimensional plot which shows the distribution of these two values together.
So radian is a Boolean value when split it's in the x and y and you can see that if Boolean is true then the Knox values are actually conditioned higher.
If it's zero then it's conditioned lower with a little bit of data points up here in the upper one.
So giving two dimensions to a kde plot you get this 2d map which shows you some contour plots some really nice things.
One final thing for the pandas plotting thing is a thing called Andrew's curves.
Now I haven't used them much myself but in Wikipedia it has this as their answer of what Andrew's plots are.
It's a way apparently to visualize structured high dimensional data.
They show it with the iris data set and the iris data set is the ubiquitous data set from Fisher way back in the day.
And if we import this we can take a look at a specific value of a data frame.
So let's look at this whether this Boolean value has much structure to it and it doesn't look like it but perhaps this Knox value does.
And looks like there's too much to that one. Let's go with rad which is only a nine values for that.
So you can try to see if there's clustering of behavior.
Now the actual numbers here I think aren't so easy to read but the fact that this should give you a sense of
if there's different behaviors going on. Too many data points overlying each other I will do a sample like this
and a sample is another built-in function of data frames where you can say give me only a hundred values and then do the same exact plot.
And it'll pick out randomly a hundred values from this data frame and then you're doing the same kind of estimation here.
And so at this point you might say hey this value of 24 for the data framework is rad.
That looks like it's having fundamentally different behavior than the other values which seem to be clustered together.
I don't know that this actually tells us much in this case but it's another piece of functionality that I think is worth knowing about.
I'll go through the last few bits here and just talk about them really quickly.
So here's another KDE plot of this median value for the houses which is what we've seen before but this is going to be the target
like how much the price of the house is actually going to be sold for.
And we can add to that by saying we want to also see what's called this rug being true.
So instead of doing a KDE plot we can give it a distribution plot and add the fringe kind of rug thing at the bottom
which adds the actual density of points at these different values.
So you can see that it is actually very dense here as we go across these values.
So sometimes if you've chosen a kernel that's too wide or too narrow for your underlying dataset
seeing the rug along the bottom here gives you extra clues into what's going on.
And one last look here at two variables that might actually be more useful for looking at relationships.
The median value versus the L stat and you can see that there's this kind of banana shaped curve here going on in the relationships.
Okay so that's going to be it for this video.
What we did in this video is we showed a number of different visualization techniques.
We took a value that had a clear bimodality of a low and a high group and created a new data frame column to encode that.
Went through also and saw various methods of doing kernel density estimations, scatter plots and various other features.
In this video we'll be talking about stats models.
Stats models is a library that you can use that allows for a lot of statistical machinery that can help you with your data science work.
So we'll continue with the same Boston housing dataset as before which we were just looking at in the last video.
And take a look at some of these Boston housing prices.
Let me toggle this header in this toolbar real fast.
Make this full screen so we have a little extra room to look at.
So we will load in the scikit-learn dataset load Boston which again has the same attributes as we saw in the previous videos.
We will construct our pandas data frame from this scikit-learn dataset so that we can use the standard tools we've learned over the years.
We've combined the features and the target into one data frame and here's that scatter matrix plot we made in the previous video as well.
This is a pandas call.
So with this function call we get all of the pairwise interaction terms for this dataset.
And from this we see a number of features that look like they have some strong trends with the thing we would try to predict which is the median value of the house.
There's a trend here with this RM.
It looks like there's this kind of banana shaped L-stat curve that we talked about at the end of the previous video.
So we have a few things that we think you might be able to combine into some sort of model that will predict our median value.
Again, let's look at the columns and the number of unique values for each of these.
In particular, Rad has nine values.
We previously made that a Boolean.
Let's actually take a look at what the values comprise it.
So there are nine values and these are the values.
And then 24 is obviously the outlier here.
And we previously made a Boolean variable, which we can do again right now.
So we'll split everything from less than 15, which means everything up to here, 1, 2, 3, 4, 5, 6, 7, 8 will be labeled as 0 and 24 will be labeled as 1.
Let's look at the target variable, which is this median value plot just done as a distribution plot with a rug at the bottom.
And so this will be the target variable and we see some interesting structure going on here.
So plot again L-stat, which we identified just a second ago, versus median value.
We have again this kind of weird shaped banana plot.
This is sort of a tapering off effect of this thing.
So stats models.
Let's actually go and take a look at this as a Google search.
So stats models for Python.
The current documentation for this sits at statsmodels.sourceforge.net.
And it has, as it says, it's a Python module that allows users to look at data to estimate statistical models and perform statistical tests as many different modeling choices.
So our options, we have linear regression, generalized linear models and all the things listed here, and also some nice examples that explain more.
This is definitely a package that's geared more toward the statistical side of data science than the machine learning side, which is how I'd classify scikit-learn.
So with that comes a number of useful tools that if you haven't used them, it can be very powerful.
So this is where the documentation resides.
I recommend looking at that.
We imported this at the top.
So I'll scroll up to the top real fast.
We imported statsmodels.api as SM, which is not a typical way of importing Python modules.
This is one of the standard ways of doing stats models.
And then there's this formula.api, from which we're going to import ordinary least squares, which is just OLS in this case.
So let's scroll back down.
The formulas work in a way that's very similar to R.
So if you've used R before, or if you've used the Python package Patsy or various other ones, what you end up writing is the dependent variables.
Or the thing you're trying to predict.
So in this case, the median value, this tilde, which goes as Lstat, which is this thing that we're just plotting up here.
So Lstat versus medv median value.
So we've given the formula in terms of the relationship between these different variables.
I have to tell the model where the data comes from.
So we say this data frame.
When you give it this data frame, it says, okay, I'm going to look in this data source, df, four columns that are named in the same way that you've written it out in this formula here.
So we've said, okay, we've rewritten out medv and Lstat are actual columns.
And at the end, we will fit this with the dot fit function.
And the end, you have a model, which we've written down as mod.
And running the method dot summary tells us the output of trying to fit this data.
So we have the results from that.
So the dependent variable is median value.
The model is ordinarily squares method least squares tells you a bunch of different pieces of information that are pretty good.
These pieces of information that are pretty useful here.
So we have r squared, adjusted r squared, f statistics, log likelihood, AIC, the Ikeke information criteria, or however you say that.
Of course, the values of the coefficients and the intercepts, standard error, the 95% confidence intervals and so on.
If you're looking at this and wanting to evaluate this model statistically, you have all kinds of things at your fingertips here to look at.
Now, the relationship between Lstat and median value of the houses does not look linear to me.
This looks like a weird shape here and we can actually plot this with the river.
We can reverse this and see how this kind of tapering off of the median value versus Lstat can be a combination of features.
So I'm going to add an extra term here.
I'll actually take the log of the value and you can actually write it in this way in this string.
So you say numpy.log or np.log of the variable that you want to look at.
And you have to wrap it in this extra I for wrapping up because this doesn't actually exist as a column.
You have to wrap it in this I. There's other ways you can wrap this as well.
But I think having this and this both be in this linear model is likely to give a much better fit than just the Lstat by itself.
Or even Lstat squared, which we could also do simply by just instead of it numpy.log, we do Lstat star star squared.
So let's run this and we see our summary comes out and we have our R squared and AIC and all these different various intercepts and log values.
So let's actually compare the two.
So one way to compare it is to look at the AIC.
So this one from 3200 down to 3100, which is a pretty substantial decrease in the AIC.
So we think this is actually a better fit statistically, although we have to look at the residuals and do many other tests to make sure that this is actually is a viable model.
So that we're nowhere near done and like to double emphasize that what I'm showing you here is not a final rigorous data science result.
This is more of a sketch of what's possible with the tools that I think are useful.
Don't be taking directly from this lessons on how to do data science.
This is more of a sketch of how the tools should work.
Let's make this a little bit bigger so we have more room again.
One way to start to evaluate how good this fit is to actually look at this graphics from the stats models.
So statsmodels.graphics has a lot of different plotting options.
And there's these component and component plus residual plots, which is the CCPR plots, which you feed it the model itself contained within this model object is the underlying formula.
And so you can tell it, I want to know this one term here, the term that went with the log of the LSTAT score.
How does that look versus the residuals of this plus the I squared?
So we can see that the component actually does a decent job at this log stat versus residuals plus log stat.
So this line actually does a pretty good job of fitting this.
And for some reason that I don't quite understand, it actually plots it twice to the same exact plot, but was just LSTAT by itself.
So the first term in that model, we can see a little bit wonky behavior where it's not quite as good as the previous one where the residuals has some extra structure here in the low end, especially.
But we can start to have various goodness of fits and start to model out how good our model is at capturing the underlying data.
Again, it shows it twice. And again, I don't know why.
We can also add more terms to this model.
So previously we had LSTAT and this the log of LSTAT plus one for the intercept.
We can also add the RM category.
We can also add the Boolean value, which is whether it's in the higher low of the RAD variable.
And because it's a categorical, you can feed it to the model with this C value, and it will properly take into account the fact that what's in this column should be considered a category.
And it won't get you in trouble with the dummy variable trap that I had to mention at the last time.
So if we look at this, we see a number of things, including the fact that it starts off with the categorical variable radian bool, the categorical value where the default value is false.
And if it's true, what the change in the coefficient is for that value and the various other values as well.
Another thing to look at, depending on what you prefer to look at the BIC or the AIC or log likelihood F statistics.
To compare this to the previous models, this again has lowered the AIC substantially so that in terms of is it a better fit or not, it has some statistical basis for saying that this is a better fit.
We still have to do a lot of work still before we decide this is actually a reasonable fit and all the assumptions behind an ordinarily squares fit are holding true.
But just as a first pass, we have a lot of really nice information here.
In this video, I would like to continue from the previous video where we had just run a model to find the intercept and the categorical RAD value.
We can also run the same exact model as before deciding that this RAD values that are doing the boolean version of this, we can actually run on the entire column itself and telling stats models that we're actually using a categorical variable here as well.
So now we're trying to predict the median value of the house using all of these possible variables where these each have a coefficient in front of it.
We run a dot fit method on that and save it as a model as MOD and we're going to output the summary of that fit ran just then.
And again, dependent variable is this MED V variable.
And we see in the output here various goodness of fit and metrics about how the fit actually worked out.
We see an R squared of 0.73 and ASC of 3040, which is a slight improvement from the previous one, meaning that encoding the RAD variable where each value is independently stored.
So since it's a categorical, this is all with a baseline of one, which is why it doesn't appear here.
These coefficients are all based off of comparing each of these terms with the baseline of RAD equals to one.
If that doesn't make sense to you, don't worry about it.
Don't worry about this kind of statistical model.
And if it does make sense to you, then you understand what I just said.
So anyway, you get the output from this, but we actually want to see some plots to see how good this fit actually is.
Because just looking at the diagnostics and the metrics that come out from these fits isn't enough to tell us whether we're making a good model here.
So let's start to look at how we can assess fit quality.
One of the easy things you can do is to look at a thing called leverage.
StatsModels gives us a nice way to see this and visualize this by using the sm.graphics.influence plot and the plot leverage residual squared plots.
So let's take a look at these two plots.
I will first do this one.
So what we see here is on the y-axis, studentized residuals versus the x-axis, the h-leverage is using the cooks method for influence.
What you see on the leverage corresponds to an outside influence on the overall fit for its values.
So if you see something with high residuals and high leverage, that's something that we should possibly consider looking at that point and figuring out what's going on at that exact point.
So like 368, for example, would be a candidate to be looking at here because it has high residuals and high leverage.
That's one way of looking at it.
Another way is to look at it through this leverage residual squared.
And you give it simply the model object that you just fit above.
You just give it mod and it will give normalized residuals squared versus the leverage.
Again, 368, 365, 372, 371 are all outliers in terms of points that we should possibly take another look at again.
That corresponds to those four points up here.
So this leverage plots is one way of assessing the fit and the data points to make sure something isn't going crazy.
There's also a way of doing partial regression and I've quoted a bit from the documentation stats models here.
It says the slope of the fitted line is that of the exogenous in the full multiple regressions.
That's what's going on here.
The individual points can be used to assess the influence of points on the estimated coefficient.
So let's take a look at what this means visually.
I think it's easier to see what's happening this way.
So we have a partial regression plot and we're evaluating the expectation value of L-stat given the values that we have
and same plotting against the dependent variable, the median value that we're trying to predict.
And in this, we see that a lot of the points are kind of in a mass right here and the outliers are sitting here at this very low end.
And the same culprits appear again and you can actually see the effect that it's having on this.
It's pulling the slope up a bit.
So that's with a plot partial regression and giving various features as you're holding constant.
You can give it the entire model and see what that looks like.
When we get an entire grid, you're going to have a lot more involved to look at this grid of plots.
But it's the various features here so that the RAD variable is a feature three given X versus the median value on the X and the Y axis.
And so you can look at the various categorical variables and how they are being fit with the lines
and how they are interacting with the overall fit as well as the values that clearly are more continuous and having a nicer time of it.
So there's two ways to do this partial regression plot and both give you different ways of looking at this data.
Again, this is plotted twice for reasons unknown.
Finally, we have regression.
We can do this as a plot regress exogenous.
It gives you this four panel plot of median value versus L-stat and residuals versus L-stat.
So this is the data minus the fit itself.
And what you're hoping to see is noise pretty symmetrically about this axis here, the estimated variables and the CCPR plots.
So we see fit versus the actual values in this plot here.
And then we can also do it versus any other term in that model, which is, in this case, the natural log of the L-stat.
And then we get this plot here, which shows much tighter fit to this instance.
If you've built up a model, then again, I'm not saying I've built up some amazing model at this point.
This is definitely more descriptive of how this kind of process can work.
But if you would like to build up a model and look through a lot of diagnostic plots and have a true statistics, robust package manager behind you,
look into stats models and really try to dive into this because there's a lot of really good stuff in this.
So with that, I am concluding the second data science project.
And what I really try to focus on this time was a little bit of some more advanced features of using the plotting features of pandas,
really taking a deep dive into how one aspect of the stats models library and there's many aspects of it.
So I highlighted the ordinarily squares and how fitting linear model there with the statistical analysis and output that comes out of every fit,
as well as fitting the diagnostics and doing a quality of fit.
I also spent a lot of time on the visuals of this diving a little bit deeper into Seaborn and a few of the other options there.
So just as a kind of a wrap up of this, using map plot live and Seaborn stats models and pandas,
these data sets can be explored and manipulated and fit.
And these tools give a lot of flexibility and exploring and analyzing data in a notebook lets someone else take a look at what you did through your analysis.
So if you've made some horrendous error as you went through, that is something that's easy to point out and point to the plot.
As I said, this was a decent fit, for example. This is clearly bad because of reason X and you can point to it and circle it.
And it's not just a bunch of random files sitting in a directory somewhere.
To close off this project, the last thing that remains to do is to save this and to close it and push it back to GitHub so that you guys can also look at the same data sets and follow along yourself.
So I'm going to file, close and halt, go back to the terminal.
Git status has only this develop directory that has any changes in it.
So git add, develop, git status, we have three new files.
Okay. And I've closed down all of them. Just double check the server here.
Yep. Everything looks to be closed and say git commit.
Give a commit message that makes sense.
Git push origin Jonathan viz, which is the name of this branch.
Go back to GitHub, see that we've already made a change this we can compare and pull request and create a pull request.
I'll go ahead and actually merge this pull request because I've demonstrated how to do the full pull request and peer review aspect of it before.
And going back to the data viz project, what we have here is don't save that.
So the data viz project to will have the notebooks that I went through during this project available right there.
Just to recap what happened in this video, finished up looking at the plots from stats models and finished up the second data science project for this course.
Let's talk about some of the security issues with using the Jupiter notebook as is out of the box.
The notebook only listens to requests on local host.
This means that it ignores requests from the Internet.
People connecting from the Internet can't see your server and they won't be able to connect.
In order to allow them to connect, you have to explicitly configure the notebook to listen to the correct IP.
Once you do, anybody can access your notebook server.
The notebook server has no password by default and permissions of the users that are connecting are the same as the permissions of the user who had launched the server.
So this means if you launch the server, everybody who connects to the notebook will be executing things as if they were you.
The second main problem with using the notebook is it's using an insecure line.
So typically, the notebook is broken into three pieces, the kernel, the web server, and the client.
The client is what you see in the web browser.
It's the notebook as you know it.
And the web server is the thing that relays messages from the kernel to the client.
The web server communicates with the kernel using ZMQ.
Usually, the kernel and the web server exist on the same machine.
The kernel is the server that executes code and runs requests.
The line between the kernel and the web server you don't have to worry about usually because it's on the same machine.
However, the line between the web server and the client you have to worry about because it's over the open Internet.
This means that it's available for people to listen to and inject messages.
However, there are some setups where it makes sense to separate the kernel onto its own machine.
For example, you may have a cluster of computers running kernels, one computer running the web server.
In this case, you also have to worry about the ZMQ communication between the kernel and the web server
if the kernel and the web server are not on a VPN or in a secured network.
I'd just like to note, we aren't security experts, but we do have experts in the community and they do help us.
If you spot a problem, I ask you, please email us at our security mailing list.
The address is security at ipython.org.
Once you do, we'll work quickly to open a CVE.
In the next set of slides, I'll talk about how you can mitigate some of these problems and rest assured that your notebook deployment is as secure as can be.
In the last video, we talked about some of the limitations of running the notebook server publicly.
Specifically, we talked about security vulnerabilities.
In this video, I'll describe to you some of the solutions provided by the notebook software and some of the limitations of the notebook software.
First, in the last video, I showed you this diagram and told you that the communication between the web server and client was insecure by default.
The notebook actually provides support for HTTPS, industry grade encryption, for this communication line.
I'll show you how to configure this.
However, the notebook does not provide support out of the box for encrypting the line between the kernel and the web server.
Therefore, I recommend you either run the kernel and the web server on the same machine, if possible, or run them within a VPN.
The latest version of ZMQ does support encryption.
However, the notebook is not using that version of ZMQ currently.
Before we secure the notebook server, we need to be able to launch it so that people on the internet can connect to it.
In the previous chapter, you learned about Tralits.
We can configure the notebook to listen to all IP addresses using Tralits.
If I do jupiter, notebook, double dash help, I can list all the configuration options of the notebook.
The third to last configuration option is double dash IP.
That allows me to change the IP that the notebook server is listening on.
Just to cement the idea that this is a Tralit, I'll show you in the notebook source where this Tralit can be found.
In parentheses next to the configuration value, you see that notebook app.ip is listed.
This means that IP is a Tralit inside the notebook app class.
So opening up the notebook subfolder of the notebook repository and then the notebook app module inside that, we should be able to find the IP trait.
I'll use the search function of Adam to find IP.
Here's the definition of the IP trait.
If you want to configure something of the application and you don't see the option in the help string,
it's a good skill to be able to look through the source code and see if there's a Tralit that isn't being listed.
So we have two ways to set this IP trait.
We can either pass it in at the command line, like so, or we can specify it via config, so it's the new default.
By specifying IP to asterisk, we're telling the server to listen to requests on all IP addresses.
You may get two warnings, one from your system firewall prompting for Python to have the ability to accept incoming network connection.
This is because the notebook server is written in Python.
The other warning you'll see is in your terminal output from the notebook server itself,
warning you that the server is listening on all IP addresses and is not using encryption or authentication.
Don't worry, I'll show you how to set these up.
But first, let's try setting IP equals asterisk in the config.
If you recall from the earlier Tralits video, the config is stored inside the .jupiter folder inside my home directory.
Opening the folder up in Adam, we see that the config files from the earlier weekend and weekday demonstration still exist.
We'll go ahead and erase that here inside the jupiter notebook config.py file.
Now, recalling what the help text said in the terminal, we'll set notebook app.ip equal to asterisk.
Go ahead and save the file and we'll try launching the notebook server again.
This time, however, we won't specify the double-dash IP equals asterisk on the command line because it's already specified inside our config.
It looks like the launch was a success.
We still received the warnings about the server listening on all IP addresses, even though we didn't specify the IP equals asterisk flag in the command line.
This means that the line that we added to the config file worked as expected.
In the last video, we added password security to the notebook.
However, we did not encrypt the line between the web browser and the notebook web server.
This means that the notebook is vulnerable to people eavesdropping on the communication between it and you or any other users of your server.
In this video, we'll add HTTPS encryption to your notebook web server.
To get the notebook to start using HTTPS, all you have to do is point it to your key file and cert file.
If you don't have a key file and cert file, you can generate one yourself.
Before I show you how to tell the notebook to use your key file and cert file, I'll show you how to generate one using OpenSSL.
If you already have a key, you can skip this step.
Anaconda already comes with OpenSSL installed.
However, OpenSSL frequently releases security updates, so I highly recommend that you update to the latest version.
To do so, you can run conda space update OpenSSL.
I'm currently inside my Jupyter config directory.
I'm going to run OpenSSL to generate the key insert file.
I'm going to generate the cert so it lasts for one year.
To do so, I'm going to pass in 365 days into the days argument.
I'm going to output both the key and the cert file into the same file.
Once I run the command, an interactive wizard will start.
I'll answer some of these questions.
However, if you want, you can skip any of the questions just by hitting return to accept the default value.
Once that is done, we'll have to configure the notebook to use this key insert file.
To do so, I'm going to open up Adam inside the Jupyter configuration directory.
After the shaw from the password trait, I'm going to create a new line.
I'm going to specify the cert file first.
The cert file is a trait of the notebook app.
It's important that I pass the full path to the cert file.
Next, I'm going to specify the key file.
The key file is a trait of the session class.
Since we output it the key into the cert file, we can just specify the same file here.
Now I'm going to save the config.
Back in the terminal, I'm going to try launching the notebook.
When the notebook launches, you'll probably see this security error from your web browser,
saying that your connection is not private and that the authority is invalid.
This is because you self-generated the cert.
You can get around this by having a third party generate your cert.
For now, let's just click Advanced and proceed the local host.
Now our connection is being encrypted.
If you are interested in getting a cert that's verified by a third party,
I recommend using StartSSL.
They'll do it for free.
You can visit their website at www.startssl.com.
The StartSSL free cert should be fine for basic setups.
The other two offer slightly more features that are verified,
whereas the most expensive gives your site a green bar inside the address bar when the user is connected.
You can see that in the screenshot in the side column of their website.
In the last chapter, we talked about how you could deploy the notebook securely.
In this chapter, we'll change gears.
We'll start looking at NB Viewer.
Before I discuss installing NB Viewer, I'm going to show you what NB Viewer looks like in the wild.
I'm currently on the Jupyter public deployment of NB Viewer,
which is accessible at nbviewer.jupyter.org.
NB Viewer is a web application that is used to render static views of notebooks online.
In the back end, NB Viewer uses NB Convert,
the application that I showed you in chapter one, which can be used to convert notebooks to various static formats.
NB Viewer just uses NB Convert to convert notebooks to static HTML representations.
NB Viewer itself is a simple website that has a title and then an address bar
where you can paste the link to your notebook file.
After pasting the link, you click Go and it will render that notebook file.
Below that, there's a showcase of notebooks for various categories.
Here, for example, we can click on this iRuby notebook to see what iRuby is.
This is what a rendered notebook looks like.
You can see it looks quite different than the notebook client that you're used to.
It's quite a bit more bare, but it still bears some resemblance to pieces of the interactive notebook,
such as these prompts and cell formatting.
At the top, there are links to download the notebook, view the notebook on GitHub if it is a GitHub hosted file,
and a link to go to the top of the file.
At the bottom of the page, you can see the version of NB Viewer that we're running,
the notebooks version, and the version of NB Convert that NB Viewer is running against.
NB Viewer tries to be aggressive about caching notebooks, so you also get a status of when the notebook was last rendered.
Because NB Viewer is not a user application and it's actually a web application, it's not included with Anaconda.
Therefore, I'll have to show you how to install it.
The easiest way to install NB Viewer is using Docker.
Docker is not included with Anaconda either, so I'll also have to show you how to install that.
Docker is an emulation platform.
It allows you to run applications inside an isolated environment called containers.
Docker containers differ from virtual machines in that the containers share the host OS.
Containers can also share dependencies with each other.
This minimizes the distance between the container and the system hardware, which makes containers faster and smaller to install.
To install Docker, first go to Docker's website at www.docker.com.
Then click on the Get Started link in the top right hand corner.
The instructions for getting started are operating system dependent.
Because I'm running a Mac, I'll show you how to get started with Docker on a Mac.
If you're running Linux or Windows, this page will look a little different for you.
The first step is to install Docker tools.
You can click on Install Docker on OS X.
Scroll down to step two, where you'll see Install Docker Toolbox.
Click on that and then scroll down.
Click the Download button for Mac if you're on OS X.
Once you have the Toolbox installer, run it.
Follow the prompts in the wizard.
Select a hard drive to install to.
Enter your password when prompted.
When done, click Continue.
Then click Close.
Now launch the Docker Quick Start Terminal.
It takes a little while for it to start the machine.
Once the process finishes, you can run Docker space run space hello dash world.
You should see a Hello from Docker message, which confirms that your installation is working.
In the last video, I introduced you to NBViewer and Docker.
We then installed Docker on your machine.
In this video, we'll install the NBViewer Docker image.
To get started, open the Docker Quick Terminal.
Your terminal may take a while to start.
Once the terminal has started, pay attention to the IP address listed in green.
Mine's 192.168.99.100.
That is the IP address of the Docker image.
You'll use that IP address to access your NBViewer server once it's started.
The first step is to download NBViewer.
Now, I've already done this ahead of time.
So mine will download fairly quick because it will just be verifying that I have the latest version.
But the first time you run this command, it may take a while.
Next, let's try launching NBViewer.
Once the server starts, it should tell you the port it's listening on.
In a new web browser, go ahead and try accessing that IP address that you remember that was in green,
followed by colon 8080.
If all worked well, you should see NBViewer.
Go ahead and try to open up a notebook.
Once the notebook opens, go back to your terminal.
You should see output from the NBViewer server verifying your request.
Without this, it would be hard to tell if you were actually running the server or not,
or if you were just accessing the public NBViewer deployment by Jupyter.
NBViewer has this wonderful feature that allows you to access notebooks on GitHub using short URLs.
To demonstrate this, I'll access a notebook that's stored as a gist under my GitHub account.
So here's a simple notebook I created for PyData.
It's stored under my account as this gist.
I'm going to just copy this URL.
Because NBViewer has support for gist, I can just paste it directly in and click go.
Alternatively, I can use an even shorter form, which is just the gist ID.
To do so, I'll remove all the stuff before the last forward slash.
This is my gist ID.
You can see NBViewer still renders it.
The GitHub public APIs have rate limiting.
So if you plan on supporting this feature, it's a good idea to generate an access token for NBViewer.
Doing so is relatively painless.
Log on to github.com using your account.
Then in the top right hand corner, click view profile and more.
Next, select your profile.
Click edit profile.
Then click personal access tokens in the left hand column.
Next, click generate new token.
Give the token a name.
And then change the scopes that you want to use to restrict the token.
When you're done, click generate token.
Your token will be displayed in the green bar.
I've blurred a couple of the numbers of my token for security.
Click the copy button to copy the token to your clipboard.
Now in the terminal that's running NBViewer, hit ctrl C to stop NBViewer.
Now let's relaunch NBViewer, adding our new access token to the command line.
Because we're running NBViewer as a Docker image, we can't specify arguments directly to NBViewer.
Instead, we have to set environment variables to cause NBViewer to change its behavior.
Here, I'm telling Docker to set the github underscore api underscore token variable to the token that I just copied from github.
Now when I try accessing NBViewer, it should be using that token.
Let's paste the same gist id from earlier.
Now let's go to github.com to see if the api token was used.
We can see that it was just used because github says it was used within the last day.
Using this token should help lift some of the rate limits for github access.
And it's also nice because it allows github to control who's accessing their APIs.
In the last video, we installed NBViewer using Docker.
This is great for most use cases.
However, sometimes it's necessary to maintain more control over the distribution.
To do this, you can install NBViewer from source.
This will allow you to do two things.
One, it will allow you to control what dependencies NBViewer is using.
And two, it will allow you to modify NBViewer's source code directly, including installing additional extensions without having to recompile the Docker image.
The first step is to clone the NBViewer repository.
You can either clone the upstream fork, like I will do here, or you can clone your own fork.
Once NBViewer has finished cloning, cd into that directory.
Now run pip install-r requirements dev.txt.
Next run npm space install, then run invoke bower.
This is installing the static assets.
Next run invoke less, which will compile the less into CSS.
CSS is what styles NBViewer.
I've cleared my console.
Now I'm going to run pip install markdown.
Once that finishes, I should be able to launch NBViewer.
Now I can access NBViewer using localhost.
To verify that this is actually running locally, let's try changing some of the code.
Let's change the title.
I'm going to hit ctrl-c to stop the server.
I'm going to open up Adam in the NBViewer repository.
Once Adam opens, I'm going to open the NBViewer sub directory, the template sub folder, and then the index.html file.
Let's change the title of the website.
We'll change NBViewer to myNBViewer.
We'll go ahead and save.
Editing these templates directly is actually not the best way to modify NBViewer,
but we'll do it for now just to verify that we've installed from source.
Now back at the terminal, go ahead and relaunch the server.
Back in your web browser, refresh the page.
When you see the title update to myNBViewer, you know that the changes that we made to the template file were loaded.
If when you refresh the page, the title doesn't change, try emptying your web browser's cache.
If you want a quicker way to see if this is the problem, open an incognito tab and then navigate to the NBViewer web page.
The incognito tab should prevent the web browser from caching.
Often when you do web app development, caching causes problems because it doesn't let you see your most recent changes to the code.
Earlier, I had mentioned that modifying the template directly in NBViewer's source was not the right way to modify the template.
A better way would be to configure NBViewer's template directory to a different directory,
have it load from one of your own custom templates, which inherit it from the template included with NBViewer.
In the following videos, we'll look at how we can do that in addition to customizing NBViewer different ways.
In this video, we'll look at what we can do just by extending the NBViewer templates.
Before we get started, we need to remove the hack that we added in the last video.
I'm going to go ahead and launch Adam from within the NBViewer repository.
Once Adam's launched, I'll open the NBViewer subfolder, then the template subfolder, and then index.html.
In there, I'll remove my space.
Now, I'll save the file.
Let's see who loads this index.html file.
I'm going to open the find in project dialog by hitting command shift F, which is control shift F on Linux and Windows.
Looks like the template is rendered here in the index handler method.
Let's see where the render template method searches for index.html.
Looks like the definition of render template is in the NBViewer provider's base.py class.
The get template method is used to load the template.
Inside the get template method, which is above the render template method,
we can see that the Jinja2 environment has another get template method defined, which we call out to.
Let's see where this Jinja2 environment comes from.
Looks like it's defined in app.py.
Scrolling up to see where nv is defined, we see nv is an instance of environment, which is imported from Jinja2.
The template loader is a file system loader, which loads from template paths.
Template paths is hard coded to the repository directory template subdirectory.
However, if you specify a custom template path using the nbViewer underscore template underscore path environment variable,
it gets propended to a list of paths, which then is used as the higher priority path.
So we can set a custom template search path just by setting that environment variable.
Knowing this, we can set the nbViewer template path.
I'm going to set it to the nbViewer underscore templates subfolder of my home directory.
Now I'm going to create that directory.
I'll cd into it and open Adam.
In Adam, I'll create an index.html file.
This file will override the index.html file in the nbViewer templates folder.
For now, I'll just write hello world and save the file.
Now, switching back to the terminal, I'll cd back into the nbViewer repository.
I'll launch nbViewer using the same command from earlier.
Now when I try to access nbViewer, the page just says hello world.
This means that our template was loaded successfully.
Let's try to complicate things.
Back inside the Adam that is opened in the nbViewer repository,
I'm going to go to the templates folder and open index.html again.
nbViewer uses the Jinja templating library to render its HTML pages.
This funky syntax extends and block body.
Those are Jinja 2 specific keywords.
The rest of the code that you see is vanilla HTML.
Let's go ahead and copy all the contents of this file.
Back into our index.html file inside the nbViewer templates folder.
Now let's change the title here and save.
If we've done this correctly, we'll have changed the look of the nbViewer landing page
without actually modifying nbViewer's source code.
I refreshed the nbViewer page and it looks like our custom template was loaded.
To give ourselves a target, let's try to set up an O'Reilly themed nbViewer.
Our O'Reilly nbViewer should look like its O'Reilly's nbViewer,
but also host O'Reilly content.
First, let's change the basic index template that we created in the last video.
To do so, I'll open up the nbViewer templates folder that we created in my home directory.
Now I'll open Adam in that directory.
I'll change the title to O'Reilly notebooks.
We'll also change the descriptive paragraph below.
Eventually, we won't want to be hosting notebooks from GitHub,
so let's change the placeholder text to reflect that.
Now I'll save and see how it looks.
We can launch nbViewer using the same command that we used in the previous video.
I'll create a new tab of my terminal.
Before starting nbViewer, I need to set the environment variable again for the custom templates.
Now I can launch the server.
It looks like our change is rendered.
However, we should probably change this logo in the top left,
and also remove this link to Jupyter.
Let's scroll down to see if there's anything else we need to change.
We'll have to change this section of showcased notebooks.
And at the very bottom, it looks like we'll want to change the footer.
Lastly, we should probably change the styling
and maybe use JavaScript to spiff up the page a bit.
First, let's see if we can change the header and footer.
Let's go back to the index.html file in our custom template folder.
Looking at the index.html file, it looks like layout.html is extended
for the basic layout of the page.
Let's open that.
It should be inside the nbViewer directory.
Inside the nbViewer repository in the nbViewer subfolder,
under templates, we can find layout.html.
Like we did with index, let's copy everything in here.
Then, back inside our custom nbViewer templates folder,
let's create a layout.html.
Here, I'll paste all the contents from the other layout.html.
Let's remove this link to Google Analytics,
because this is the Google Analytics for the Jupyter deployment of nbViewer.
Also, we'll want to get rid of these links to Fastly
and change the Rackspace link to O'Reilly.
Scrolling up, let's get rid of the text that says this website does not host notebooks.
It only renders notebooks available on other websites,
because we're going to be using this pseudo-website to host O'Reilly notebooks.
Here's the link to Jupyter that we wanted to remove.
Lastly, we'll want to change the nav logo to O'Reilly's logo.
Let's go to O'Reilly's website to see if we can get the link to their logo.
I'm on O'Reilly's website now at www.oreilly.com.
I like this logo in the top left-hand corner.
I'm going to right-click on it and click Copy Image URL.
Back inside the layout.html file,
I'm then going to paste that URL over the image URL for the existing nav logo.
We'll also get rid of the New Relic reference.
Let's save what we have and go back to the browser to see how it renders.
Awesome! This is already looking a little more O'Reilly-like.
We'll probably still want to change the color scheme,
because I noticed when I roll over FAQ, it highlights orange,
which doesn't match O'Reilly's red.
Looking at the bottom of the page, it looks like our footer updated it correctly.
Let's check out the FAQ page.
It looks like there's some questions that shouldn't be here.
Let's remove them.
Back inside the nbviewer repository,
it looks like the FAQ.md file might be the file that's getting rendered.
Let's open that.
It looks like this file does indeed extend the layout.html file
and uses a special markdown filter to convert itself from markdown to html.
In the process, it automatically generates its table of contents.
Let's do what we did for index.html and layout.html in our custom templates folder.
Let's create an FAQ.md file and copy the contents from the FAQ.md file in nbviewer.
Let's get rid of the first two questions,
because they are completely specific to Jupyter's nbviewer.
We'll defer them to nbviewer for this information.
This paragraph doesn't relate at all to our viewer,
nor does the one below or the one below that.
This paragraph also doesn't relate.
This is related, though.
We just need to update it to point to our email address.
The last few before the final one also don't relate,
and we'll replace the text of the final one with an email link to the O'Reilly administrator.
Now let's save and see if the FAQ page renders how we want.
Back in the web browser, I'm going to refresh the page.
Looks like we should remove the first question as well.
Let's refresh the page again.
Ah, much better.
Let's try clicking on the O'Reilly image to go back to the homepage.
Sweet, it worked.
In the next tutorial, we'll look at adding custom CSS to style it more like O'Reilly's main website.
In this video, we'll talk about how nbviewer compiles its less into CSS.
We'll then look at adding our own CSS to our custom nbviewer templates.
I've still left the nbviewer server running from the last video.
This is because I do not need to restart it.
As long as I'm only changing static files, all I have to do is refresh the webpage to update the contents.
If I were working on server-side files, for example the Python files,
then I would have to restart the server.
Let's go ahead and open up Adam inside the nbviewer repository.
When you installed nbviewer from source code, you had to run a command called invoke less.
When you ran that command, what it did was run a function called less inside the tasks.py file.
Here's that function.
What this function does is compile the less into CSS using the less compiler.
It outputs the compiled CSS into a build subdirectory.
It outputs a styles.css, notebook.css, and slides.css.
Likewise, the source files used are styles, notebook, and slides.less.
Let's open the nbviewer static directory.
In here, you see the folder less and the build folder.
The build folder is grayed out here because it's not included in the git repository.
That's because we don't want to check in the built files.
That would just be including changes twice.
The less folder is where the less is stored.
We can open up the notebook.less to get an idea of how notebooks are styled.
A major difference between less and CSS is that less allows you to import.
Here you can see that bootstrap is imported and styling from ipython.
Let's go ahead and see where the build files are referenced.
In the layout template, inside the header, we see that styles.css is referenced.
Inside notebook.html, we can see where notebook.css is referenced.
Let's go ahead and add our own styling to our custom templates.
Going back to the terminal, I'm going to cd into our custom templates directory.
Here I'll open Adam.
Inside our layout.html, below the existing CSS import, let's add our own.
It's important that you do this below the existing because this will cause your style to override the existing.
Unfortunately, nbviewer doesn't support pulling files from directories outside of its own, so we have two options.
We could either place our custom style inside the nbviewer repository, which I'd rather not do,
or we can use the ginga templating to load it from our nbviewer templates directory and then inline it directly into the html.
First, let me show you what it would look like if you were to put the CSS inside the nbviewer repository.
You would change build to CSS and then give your CSS file a name, like custom.
You would then save this file and inside the nbviewer repository under the static directory in CSS,
you would right-click, create a new file called custom.css.
And then inside here, you would put whatever custom CSS you want.
Moving back into our nbviewer templates directory, the alternative, I think, makes more sense because then you can keep your CSS next to your templates.
For this, instead of using a link tag, you'll use a style tag.
Then inside style tags, use the ginga include to include your style file.
The only downside to using this method is that you're disabling the browser's ability to cache your style,
which means that every time a page is requested, client will have to download the CSS again.
That's usually not a problem with small CSS files, and if it is a problem, you can use the other method that I just showed you.
So now let's save this file and create our own custom CSS.
To test to see if our custom CSS is working, let's try setting the body background color.
I'm going to use important just to make sure it overrides any other values.
However, it's important to note that important isn't the best practice.
Using important disables you from later overriding styles.
In a new browser window, let's navigate to our NB viewer page to see if our style gets loaded.
Awesome, it looks like the style loaded successfully.
Now, instead of applying such a hideous style, let's try to override the orange highlight color that's applied to buttons.
Let's inspect the FAQ button to see how we can select it using CSS.
Looks like a good selector would be to use the navbar right class and then the anchor tag.
Back inside our custom CSS, let's do that.
To specify that we want to change the styling when it is hovered over, add the hover sudo selector.
For now, let's just try changing the background color.
Again, let's use the important tag just to make sure that what we're doing gets applied.
Looks like that worked.
So now, let's change the font color instead of changing the background color and let's actually use O'Reilly's red.
Let's go to O'Reilly's website and we'll right click on the home link to look at its color.
Now, I'll just double click this and copy it.
Back inside our custom CSS, I'm going to change background color to color and paste this new color.
Let's save the file, then go back to the web browser where I'll open our MB Viewer tab and refresh the page.
Looks like that works.
Now back inside the custom CSS, let's try to move the important flag.
Like I said earlier, it's better to not use important when you can get away with it.
Back in the browser, let's refresh the page and see if it still works.
Looks like it's no longer working.
We have two options. We can either stick with the important flag or we can try to make our selector more specific.
Because I know that I'm applying the top most level styling and nobody's going to come in and inherit from the O'Reilly page and add their own styling,
it's okay for me to use important.
If, however, you were writing something that would later be styled by somebody else, you'd want to make the selector more specific.
To do so, you could inspect the element and either A, add more levels of elements to your selector,
or B, in the templates, actually add an ID to this anchor tag and then address the anchor tag by ID.
Addressing an element by ID has a higher specificity than addressing it otherwise.
Back inside the custom CSS, let's re-add the important.
I'm going to refresh the browser page.
Looks like that's still working.
Let's scroll down to the bottom of the page.
Maybe we should use one of O'Reilly's grays for this bottom.
We could also use O'Reilly's red for the links.
This gray looks nice.
We'll copy the background color.
Now back on the Jupyter NB viewer tab, let's try styling this footer.
The font doesn't have enough contrast now. Let's change it to black.
That seems like it has too much contrast. Let's see what O'Reilly does.
Looks like they use an off black. We'll use that too.
I'd also like to add a top border.
Let's copy the border color that O'Reilly uses.
Looks like they use this off-shaded gray.
Now we can just copy this CSS that we've designed in the browser
and paste it into our custom CSS in a footer selector.
Now let's refresh the page.
Scrolling to the bottom, we see that our new styling has been applied.
Lastly, we need to change the default link color to that red.
Back on our custom CSS, let's define an anchor selector.
We have four C problems with this anchor tag and this anchor tag.
Let's define a color for when the FAQ anchor tag is not hovered on.
We'll use the color that we used for text.
I'm going to save and then go back to the browser and refresh the page one more time.
The FAQ button is still working. Scroll to the bottom.
And it looks like our links are formatted correctly now.
In the last video, we looked at customizing our MbViewer deployments CSS.
In this video, we used JavaScript to spiff up the website a little bit.
I found this really cool carousel on Bootstrap's website.
Here it is.
Bootstrap is a component that MbViewer already uses,
so we should be able to just drag and drop this code into place.
What I want to do is replace the notebook listing in the showcase
on our MbViewer with a carousel.
So I'm going to go back to the Bootstrap website and copy and paste the code here.
Inside the index template in our custom templates folder,
scrolling down towards the bottom, you can see where the showcase is built.
The JINJA templating for loop is used to iterate over each section
and then it's used again to iterate over each link in each section.
We'll use this logic to compile the different slides for our carousel.
For now, I'm going to insert the carousel code above this existing code
in between the header and the showcase,
pasting what we copied from Bootstrap's website.
I'm going to remove the indicator dots on the carousel.
Also, from experience, I know that we're not loading glyph icon on MbViewer by default,
and I don't feel like adding that dependency.
Instead, we're using font awesome.
Equivalent icons would be icon-prev and icon-next.
Now, what we need to do is use that JINJA code that iterates through each item
to construct our carousel slides.
It looks like each individual unit is an item.
The first item is active.
Let's go ahead and delete the ellipses.
Now, let's move the JINJA templating loop logic below this first item
to create the latter items.
We're going to just ignore the notion of sections,
so we'll group both the loops next to each other.
Now, let's copy the item template into the loop.
Then we'll copy the image source into the item's image.
We'll also copy the link text as the alternative text
and use it as the caption.
Then we'll take the anchor tag and put it around the caption.
This will make the caption clickable.
Now, finally, we'll remove the original code from the gallery.
We'll save our changes and refresh the page to see how it renders.
So here's the page.
You can see it doesn't have the gallery below anymore.
Now it just has this carousel that rotates through images.
And each image has a link that we can click to open that notebook.
However, you may notice the size is constantly changing.
It must depend on the image height.
Let's fix the size of the carousel.
We'll do so using CSS.
First, let's get the ID of the carousel.
Copy that.
Then in your custom CSS, add a selector for the carousel.
To select an ID, prefix with the hashtag.
Now set the height to 300 pixels and the width to 300 pixels.
Save.
And let's go back to the web browser to see how that renders.
We're going to refresh the page.
Here's what our smaller carousel looks like.
We should probably center it in the page and add a margin.
It looks kind of weird hugging the bottom so closely in the top.
Let's try centering it in the web browser.
By setting margin left and right to auto, the element will center.
Now let's add a top margin to give it some distance from this horizontal line.
40 pixels looks good.
Let's do the same with the bottom.
Now take one last look.
That looks good.
Let's copy and paste this style back to our CSS.
Oops, looks like I forgot to copy margin left and right auto.
Now we need to get rid of that placeholder for the first active item.
In index.html, in the carousel code, you can see that item here.
Go ahead and remove that.
What we need to do is only add active to the first class.
To do that, let's create a flag.
Once that flag is used once, we'll set it the false.
We can use the ginger set command to set this flag.
Then we'll test for that in class.
Lastly, let's make sure we set first the false.
When we set first the false here, we're actually declaring a new variable first
within the scope of this for loop that overrides the first declared in the outer scope.
This means when we get to the next for loop, first will be set to true again.
So we have to set first the false twice.
Let's refresh the page.
Ah, looks like that worked.
Awesome.
In the last video, we talked about adding custom CSS and custom JavaScript to your NB viewer deployment.
In this video, we'll talk about changing what NB viewer is hosting to the user.
NB viewer has a notion of providers, which are the things that dictate what NB viewer can host.
There are two types of providers, URI rewrites and handlers.
URI rewrites take textual content that's entered into the go bar of NB viewer
and translate it to a canonical NB viewer URL,
a URL that NB viewer understands and is capable of rendering.
Handlers are things that are designed to interpret and load from NB viewer URLs.
The handler is the thing that actually fetches the resources from the local or remote location.
For example, the GitHub handler accesses notebook content directly from GitHub using GitHub's API instead of standard HTTP.
Let's start by configuring NB viewer to host local files.
Sticking to our O'Reilly themed example, let's pretend that O'Reilly wants to host files from a network-attached storage device.
Let's say that that storage device is SimLink into the home directory.
We'll pretend that that SimLink is called network.
I'm going to create this folder just as an example that we can use to demonstrate this feature of NB viewer.
Let's pretend that in the network-attached storage drive, there's a subfolder called notebooks.
And then inside the notebooks folder, there are author folders.
For now, I'll just create an authored folder for myself.
I have some example notebooks that are sitting inside my home folder.
I'm going to copy those over to here.
Now let's take a look at the NB viewer source code.
I'm going to CD into the NB viewer repository and open Adam.
Inside the NB viewer subfolder, I'm going to open app.py.
Scrolling down to the very bottom of app.py, we see all the command line arguments that we can pass to NB viewer.
One of the command line arguments is local files.
This tells NB viewer to host files from the local file system.
Let's use this.
I've closed the NB viewer server.
I'll relaunch it with this new command.
But before I launch, remember that we need to set the correct environment variable in order for our custom templates to be loaded.
Now let's launch NB viewer.
Let's switch to the web browser to see if we can load files from the local files system.
I'm going to try accessing the notebook using the go bar.
I'll type in the subpath to the notebook from its location inside network.
Doing that didn't work.
This would make you want to jump to the conclusion that the local files setting isn't working.
However, this is an invalid conclusion.
If you pay attention to the URL, you'll see that URL for slash was prefixed to what we tried to access.
This is telling NB viewer to use the URL handler to load the following content.
Of course, notebooks for slash jd frederick four slash one dot ipynb is not a domain name and is not located within a public top level domain name.
So it makes sense that URL would fail to load this content.
Instead, what we need to do is change the URL prefix to local file.
And that will get the notebook to load.
We want to automate this though.
We don't want the go bar to not work and we would like the go bar to automatically translate to this canonical and be viewer local file format.
In the last video, we got the NB viewer local files provider working.
However, we weren't able to access it via the go bar.
In this video, we'll write a URI rewrite provider that will allow us to access local files easily from the go bar.
The first step is to open up Adam inside your NB viewer repository.
Next, open the NB viewer sub folder and inside that open providers.
Here, you'll see a list of the providers that are default with NB viewer.
The Dropbox provider has a URI rewrite, which is a good example for the rewrite that we're going to do.
Let's copy the handlers dot py file and create a sub folder inside the providers folder called X for X for is going to be the name of our plugin.
Paste the file inside there.
You can also copy the init file.
Now open the handlers dot py file that you copied.
Go ahead and remove the ipython header.
We want this URI rewrite to accept URIs of the form author forward slash notebook name.
We'll accept the notebook name either with or without an IPYNB extension.
The first step is to replace the first string in the tuple.
This string is the string that is used to search.
The second string is the string that replaces the search string.
Each group of the regular expression where a group is defined by parentheses can be accessed in the replacement string by using curly brackets.
So this zero refers to this first item here, whereas the one refers to this second group here.
Without explaining too much of regular expressions, I'll tell you that this matches a set of characters of variable length.
I'll remove this text here where this first group will match the author.
Add a forward slash.
Copy this first group.
This second group will match the notebook name.
And at the end, I'll add dot ipynb.
And I have to escape the dot because dot has a special meaning in regular expressions.
And add a question mark because we don't know if the user is going to write dot ipynb or not.
Now in the replacement string, I'll replace the URL with local file because local file is the canonical form of the URI accepted by the local file provider.
I'll also add notebooks because notebooks is the subfolder that sits inside the network folder.
The first value will be the author name, followed by the notebook name, and then we'll append a dot ipynb file extension.
Now let's save this and we'll go back to the terminal and try launching nbviewer.
But first make sure to set the environment variable that uses the custom templates that we created earlier.
Now let's try launching nbviewer.
To get nbviewer to use our URI rewrite, we use the double dash provider underscore rewrites.
The provider rewrites flag takes a full Python namespace to a rewrite provider.
You may be wondering why we had to edit nbviewer directly.
Well, we actually didn't have to.
We could have wrote our own Python package and then reference that Python namespace here.
However, writing a Python package is outside of the scope of this video series.
So for simplicity, we edit it nbviewer directly.
That allows us to piggyback on nbviewer's namespace here.
So to access our rewrite, we can use nbviewer.providers.exfer.
Lastly, we'll want to disable github and gis providers.
To do so, we'll set the URL provider as the only provider used by nbviewer.
We can do that using the double dash providers flag and setting that to nbviewer.providers.url.
Now that the server is launched, let's go to our web browser.
Let's try accessing the first notebook under my name here.
Looks like that worked correctly.
Let's go back to the homepage and try accessing it without the ipynb to make sure it still works.
Looks like that worked too.
The last thing we'll want to do is change the showcase so it shows notebooks that are actually hosted by us.
To understand how this is done, let's look at the source code of nbviewer.
Back inside Adam in the nbviewer repository, open up app.py.
If you scroll towards the bottom, you'll see where all the command line arguments are defined.
The command line argument that we're interested in is this front page argument.
This argument points to a JSON file which defines the content that will be used on the front page to render the showcase.
The default used by nbviewer sits inside the nbviewer repository under frontpage.json.
Let's open that.
Here you can see the links that we see when nbviewer runs.
Let's copy all the contents of this file.
And then in a new terminal window, let's cd into our custom nbviewer templates directory.
The reason why I had you open this directory is because it's where we're storing a lot of other custom things for our server.
We might as well store other content in here just to keep it all grouped in one place.
Create a new file called gallery.json.
Inside that file, paste the contents from the front page.json that we copied out of the nbviewer repository.
Now, looking at this file, we see that it has groups defined by this header attribute.
Since we're ignoring the notion of groups, let's get rid of all the other groups below.
When we set up the dummy directory, I only copied two files into my author directory.
So let's get rid of the third entry.
We'll give the first two names.
And then change the target to the canonical URL that points to the correct notebook.
The URL for the second notebook is almost the same, just the notebook file is different.
Now we could change the image as well, but I don't have any nice images for my test notebooks.
So I'm just going to leave the images as is.
I'm going to save this file and go back to the terminal.
Opening the tab of the terminal that's running nbviewer, I'm going to stop nbviewer by hitting Ctrl C.
I'm going to rerun the same command except this time I'll change front page to the full path of the JSON that specifies our gallery.
Now let's open the web browser to see if that worked.
Refreshing the home page, we see that my JSON was loaded.
Does this URL now points to the Jons notebook, even though it's still using the old screenshot?
Jons notebook 2 is also available, even though it's using the old screenshot.
Let's click on the link to see if it works.
Awesome, it looks like that worked.
If you want to find out more about nbviewer, visit the nbviewer repository at www.github.com forward slash Jupiter forward slash nbviewer.
In this chapter, I'm going to talk about temp nb.
It stands for Temporary Notebook Server.
Temp nb is a service that launches sandboxed ephemeral notebook servers on demand,
where ephemeral is defined as something lasting for a short time.
It's kind of like an interactive version of nbviewer.
Temp nb is useful for cases where you need to share notebooks that lose importance if they're not interactive.
Temp nb users can interact with your notebooks to see what they have to provide.
They can explore the data sets and write their own code inside the notebooks.
The changes that they make won't be persistent anywhere, so it's okay to open a Temp nb service to the public.
In my web browser, I'm going to navigate to Temp nb's website at www.github.com forward slash Jupiter Temp nb.
I'm now going to scroll down to the readme.
At the top of the readme, there's this very useful diagram for describing how Temp nb works.
Temp nb can be broken into a few pieces.
The user-facing piece is the configurable HTTP proxy.
This piece routes traffic to the correct sub-pieces.
The Temp nb orchestrator is what is used to launch the temporary notebook servers.
Docker is the technology that is used to containerize them.
Once a server is launched, the Temp nb orchestrator communicates to the configurable HTTP proxy,
telling it to route a certain subset of addresses to the correct Temp nb container.
Jupiter runs and maintains its own instance of Temp nb.
You can access it at try.jupiter.org.
The notebook itself is the same notebook that you're used to running on your local machine.
You can see that this notebook comes pre-populated with example notebook files.
In this video chapter, I'll show you how to do this.
I'll also show you how to customize your notebook server image so that it reflects your organization's needs.
In this video, I'll talk about installing Temp nb.
Temp nb, like nbViewer, can be installed either using a Docker image or in development mode from source code.
However, unlike nbViewer, it doesn't really make sense to install Temp nb from source code unless you're planning on developing Temp nb.
That's because all the common configuration that one would want to do can be done through custom Docker images,
the images that are launched by Temp nb as temporary servers.
First, let's open up the Docker quick terminal.
As we did in the last chapter, remember the IP address that's printed by Docker in green.
This is the IP address to use to access your server later.
The first step is to tell Docker to download Temp nb.
You can do that by running Docker pull Jupyter minimal.
Once that is finished downloading, you should have a full copy of the Jupyter minimal image.
Now you'll need to generate a random token.
This token will be used to authenticate with configurable HTTP proxy.
This command works on Linux and Mac operating systems to generate a random string of 30 characters.
However, you can use any random string you'd like for your token.
So on a Windows machine, you can use the equivalent command provided by that operating system.
Copy the random token.
Now we'll launch the configurable HTTP proxy.
To do so, I'll start with Docker run.
And then I'm going to tell Docker to use the network adapter of the host.
To do that, I'll use double dash net equals host.
Then I'll tell Docker to run in the background and print its ID using the dash D flag.
Next, I'll pass in the proxy token as an environment variable within the image.
To do that, I'll use the dash E flag, specify the environment variable,
and I'll paste the token that I generated in the last step.
I'll set the name of this container to proxy.
Then I'll specify the name of the container I want to launch.
And I'll specify default target.
Since this is the first time I've ran the command, Docker will load the image from its repository.
Once that is finished downloading and has launched, we'll launch the tempnb orchestrator.
To do so, we'll use the same type of command except we'll change the last couple pieces of it.
The name will change to tempnb.
And then we'll use the special dash V flag to tell the Docker image to bind the Docker client within itself.
This will allow the Docker image to spawn other Docker images.
Specifically, we'll bind the Docker sock.
And lastly, we'll specify the name of the image.
The orchestrator's name is tempnb.
Since this is the first time I've ran this command too, Docker will download the image.
Once that finishes, you should be able to visit your tempnb service.
In the web browser, navigate to the IP address you remembered from earlier.
At the end, append colon 8000 to visit port 8000.
This is the port that tempnb is listening on by default.
If all is well, tempnb should just work.
And accessing that address will spawn a notebook server for you in a Docker image.
In the top right hand corner, you'll see a hosted by Rackspace logo.
This is not actually being hosted by Rackspace.
This is being hosted on your machine.
It's just that the image that you downloaded, Jupyter 4 slash minimal,
is based on the same image that we use in the Jupyter deployment.
In this video, we'll look at how we can use custom Docker notebook images with tempnb.
Jupyter has a bunch of notebook images predefined in the Jupyter organization.
In your web browser, open up the Jupyter organization GitHub page at github.com forward slash Jupyter.
Once the page loads, scroll down and you'll see a repository called Docker stacks.
Open that.
This repository contains a bunch of Docker images for various tasks.
Let's go ahead and clone this repository.
To do so, copy the clone URL in the right hand column.
Now, in a terminal, navigate to your home directory.
Run, get, space, clone, and then paste the URL.
Once the cloning is finished, CD into that directory.
And let's open Adam.
Once Adam opens, open the minimal notebook directory.
This minimal notebook image is actually different than the minimal notebook image you used in the last video,
but the one that we used in the last video is actually deprecated.
And this is the modern replacement.
This image doesn't have a racks based logo in the top right hand corner.
Let's open up the Docker file.
This is the file that tells Docker how to build the image.
This from line is how Docker knows what this image inherits from.
The Debbie and Jesse image is used as a base.
You can see the list of Docker commands used to build this image.
At the end, we specify that the start notebook dot shell file should be executed.
Let's open that.
Here you can see how the notebook is launched.
The config file used for the notebook is stored under jupiter underscore notebook underscore config.
This is the same kind of config file that we looked at in the second chapter.
The files as they are in this repository are not a Docker image.
We have to first build them.
The build process is described in the make file.
Let's open that.
The help section describes how the build make file is used.
To build the minimal notebook, we just need to run build forward slash minimal dash notebook.
Let's try that within this directory.
First, Docker will download the base image.
It will take a while, but once it's done, your image will be built.
Now let's try using this image with Tempenby.
Start a Docker quick terminal.
Once the terminal starts, pay attention to the IP address like you did before.
We're going to run the same commands that we did in the video before the last video,
skipping the Docker pull command and changing some of the contents of the last command.
If you're continuing on from the last video,
make sure that you close all the existing Docker containers before trying to do this.
To do so, you can run the following command.
Docker space stop, dollar sign, and then in parentheses Docker space PS space dash a space dash Q.
I don't have any Docker containers running right now, so I get the help output.
After running that command, you want to run almost the same command, but replacing stop with RM.
The last command is almost identical, just changing from the name forward.
Once again, we'll tell it to connect to itself, so it's capable of launching other Docker images.
And here's where the command will start to change significantly from the last video,
in addition to the omitted name flag.
We'll start specifying the Python command that launches the orchestrator.
We'll specify the image that we just built.
Now the tricky part is that we'll have to tell the image how to launch the notebook server.
We do so using the double dash command flag.
We have to tell the notebook app what its base URL is.
The image will format the string and you can insert special variables using curly brackets.
Base path is one of those special variables that you can insert.
We'll tell it to listen to IP0.0.0.0, which will allow it to listen to anything.
Lastly, we'll specify the port that it's listening on.
Once you run that command, in your web browser, try accessing the Docker image.
If everything works, you should see a new notebook server.
This notebook server won't have a Rackspace logo in the top right-hand corner.
If you have troubles, most likely you mistyped something.
If you need to debug why it's not working, open up another Docker quick terminal.
When the Docker quick terminal launches, you can run docker ps-a.
This will list all the Docker processes that are running.
If you see one that says exit it with an exit code in parentheses,
you can look at the logs of that Docker image.
To do so, run docker logs and then copy the container ID, which is in the far left column, and paste it.
In one of the attempts I made earlier to run this long command, I misspelled orchestrate.
This caused the server to not run and me to receive gateway errors.
By looking at the logs, I could tell that that was the problem and was able to correct it quickly.
In the last couple of videos, we looked at launching TempNB using custom notebook image.
In the following videos, including this one, we'll look at creating our own custom notebook image for use with TempNB.
To get started, launch the Docker quick start terminal.
Once the terminal launches, pay attention to the IP address like you did before.
We'll be using that IP address to access TempNB.
In the last couple of videos, we used the Jupyter Docker stacks minimal notebook image.
We'll use that image as a base for our new custom image.
To do so, let's copy the image out of the repository.
I'll copy it into a directory called custom notebook.
This will be the name of the custom image that I'm going to create.
I'll then cd into custom notebook and I'll open Adam.
Once Adam opens, I'll open the config file inside custom notebook Jupyter notebook underscore config.py.
This is the configuration file that will be loaded by the Jupyter notebook inside the notebook image.
Recalling from an earlier chapter, I'm going to set the untitled notebook name.
This is an easy variable to set that we can use to quickly judge whether or not our config file is being loaded.
The variable is c.contentsManager.untitled notebook.
I'll set that to test. Now I'll save the file.
Next, I'm going to create a shell file that we'll use to build this image.
I'm going to copy the shebang from the start notebook file.
We'll call the new file build.sh.
I'm going to go back to my Docker quick start terminal.
I'm going to open Adam up inside the Docker stacks repository.
When Adam opens, I'm going to open the make file.
I'm going to scroll down to the build line so I can see how images are built.
I'll go ahead and copy this line.
I'm going to go back to the Adam that we opened up inside the custom notebook directory.
I'm going to paste this line inside the build.sh file.
I'm going to remove drgs and replace owner with JD Fredder.
You can use whatever you want here to identify yourself.
And I'm going to replace this notdir $at with the name of my notebook image.
I'll also get rid of the notdir $at at the end and the forward slash.
This tells Docker to build the contents inside the current directory.
Now I'm going to copy this shebang again.
And create a new file for testing this image with tempnb.
I'll call this file test.sh.
I'll paste the shebang.
And then I'll enter a command that causes all the images that are currently running in Docker to close.
It's important to note that this command is inside this file.
We don't want to run this file if there are Docker images on our system that we don't want to close.
The reason I'm adding this line is because it becomes tedious to constantly close Docker images each time you want to run your test.
To close all the images that are currently running, I'll use docker stop and dollar parentheses docker ps-a-q.
What this does is runs docker stop on every docker image that's currently running.
I'm going to copy this line, paste it below, and replace stop with rm.
This will do the same thing but remove the images instead of stopping them.
Next, I'm going to create a token for use with the HTTP config proxy.
I'll use export to define the variable token as head 30 characters long of dev urandom piped with xxd-p.
Next, I'll run the configurable HTTP proxy image.
To do so, I'll use docker run.
Double dash net equals host dash d dash e config proxy auth token equal the token variable.
Double dash name equals proxy image name jupiter configurable HTTP proxy space.
Double dash default dash target 127.0.0.1 port 9999.
I'm going to turn on word wrap so you can see the whole command.
Next, I'm going to launch the tempnb orchestrator image.
I'll start with the same command but deviate once I get to the name.
I'll use dash v bar run docker dot sock colon four slash docker dot sock to cause the image to connect to the docker client.
Next, I'll specify the jupiter tempnb image and the command python orchestrate dot pi.
I'll specify the image to jd fredder custom notebook and the command to start dash notebook dot sh.
This part's really important.
The minimal notebook image requires you to start the notebook server using start dash notebook dot sh
instead of running ipython space notebook or jupiter space notebook.
That's because if you run either of those, the notebook will be launched as root
and the notebook will be looking for the configuration file inside the root home directory.
However, the configuration file is installed into the jovian user's home directory.
So running start dash notebook dot sh does some special things that causes the notebook to launch the server as the jovian user.
I'll have to pass some commands into the start notebook shell script.
To do so, I'll escape quotes.
Inside those quotes, I'll set the base URL,
allow origin,
and the port.
I'll save this file and go back to the docker terminal.
Now I'll navigate to the custom notebook directory that I created earlier
and I'll try running the build dot sh file I just created.
If you get a permission denied, it's probably because permissions aren't set correctly on the file.
You can do so by running chmod plus x build dot sh.
Looks like the image built successfully.
Now let's try running the test shell file.
We'll have to change the permissions of that as well.
Looks like that worked.
We get these help outputs because no images were running at the time.
The last two outputs are the grids for the images that were launched.
Let's go to the web browser.
Try accessing the tempnb server via the ip address that docker printed.
Looks like the server launched successfully.
Now let's see if the config worked.
Awesome.
It looks like the default notebook name is no longer untitled, but is test,
which implies that our config is being loaded.
In this video, we'll add custom content to our tempnb notebook custom image.
This process is very similar to the process that you use for adding custom content to your nb viewer deployment.
That's because the notebook itself uses ginga2, like nb viewer, to do its templating.
First, let's start the docker quick start terminal.
Pay attention to the ip address that is listed, for that's the ip you'll use to access docker.
Let's go ahead and navigate into our custom notebook directory and open atom.
The first thing we'll do is create a page.html template.
This template will override the page.html template of the notebook.
Inside the page.html template, we'll extend the base template of the notebook.
Next, we'll override the header underscore buttons block.
This block exists at the top of the notebook pages.
We can use this to add our own logo.
We'll go ahead and add an O'Reilly logo here.
We have two options to do this.
We could either add the O'Reilly picture to our custom notebook image,
or we could host it externally and reference it here.
For tempnb, it's better to host your images and other static content externally to the images that are launched by the orchestrator.
That's because the notebook server uses tornado to host its files,
and tornado isn't as fast as other servers like engine x or Apache,
which are even slower than services like CDNs.
So what we'll do is open our web browser and get the link for the O'Reilly image.
www.oreilly.com
Once the page loads, right-click on the image and say copy image URL.
Then go back to atom.
Now on the header buttons block, add an image tag.
Set the source of that image tag to the link that you copied from O'Reilly.
Save the page.
Now we'll need to copy this template into our image.
To do so, open your docker file.
Scroll down to the bottom.
The first thing you'll need to do is create a directory that contains templates.
To do so, we're going to copy this line that creates the dot Jupyter directory inside the user directory.
We'll put our template directory inside that.
We'll call it custom.
We'll then need to copy the file into that directory.
Go ahead and copy the line that does the notebook config.
Change notebook config.py to page.html and update the path to custom.
Save the file.
Lastly, you'll need to go into your Jupyter notebook config.py file.
Inside here, below the untitled notebook line, set the extra template paths variable of the notebook app.
This variable accepts a list, a path.
Give it the path to your custom template folder.
And then save the file.
Now go back to your docker terminal.
And inside here, run the build script again.
Once the build script finishes, you can run the test script.
Now go back to your web browser.
Try accessing tempnb.
If all goes well, you should see the O'Reilly logo on the top of the header bar.
You could style this better by using css in your template page.
But the point here is not to make something that looks good.
It's just to show you how to get static content into your tempnb images.
In this video, I'm going to talk to you about setting limits on your tempnb service.
And then briefly, I'll talk about security.
To get started, open up a terminal.
Then navigate into the custom notebook directory.
This is the directory that contains the custom image we've created.
Now open Adam.
Once Adam opens, open the test.sh file.
This is the file that contains the lines that can launch this image in tempnb.
In a real deployment, you could use these same lines.
Just remove the two docker stop and docker rn lines.
I'm going to enable word wrap so you can see the whole commands.
The last command is the command that launches the orchestrator.
We pass in a command into the image using the double dash command flag.
You can tell the orchestrator what image to use using the double dash image flag.
There are also additional flags.
For example, if you need to limit the number of CPUs any particular container can use,
you can use the double dash CPU underscore shares flag.
And this accepts an integer value for how many CPUs are allowed.
For example, we could limit each image to using two CPUs at most by doing equals two.
The next useful flag for limiting is the coal period flag.
This flag accepts an integer in seconds that determines how often containers are examined for their age
and then collect it if old enough.
The default for this is 600 seconds.
This is 10 minutes.
We could make this faster, for example, by doing 300 seconds.
Cold timeout is the variable that sets how long it takes for a container to be sitting idle that it will get cold.
The default for this is 3600 seconds.
This variable is also an integer specified in seconds.
We can half that time by setting it to 1800 seconds.
We can also set a limit on the amount of memory each container is allowed to use by setting mem underscore limit.
This accepts a string specifying the amount of memory that each container is allowed to use.
It defaults to 512M for our 512 megabytes.
We can half this by setting it to 256M.
The last important flag I would like to mention is the pool size flag.
This flag accepts an integer which specifies how many child docker containers can be launched by the orchestrator.
We can think of this as a limit as how many users can use Temp Add B at any given time.
The default for this is 10.
We can limit it to half that by setting it to 5.
Note that these flags are all set outside of the double dash command because they're not actually getting passed into the image but to the orchestrator itself.
Lastly, let's talk a little bit about security.
Go ahead and open up your Jupyter underscore notebook underscore config.
You see here in this configuration file that there's a flag for HTTPS encryption and password.
This is the same HTTPS encryption and password that you used in the earlier chapter where you learned how to deploy the Jupyter notebook.
This may be useful to you but take note that this is not affecting the orchestrator itself.
So any random user can still access your deployment of Temp Add B and launch containers.
They just may not be able to take advantage of those containers if they don't have the appropriate credentials to log on to them.
This means that those people could still spawn up a bunch of containers and use your entire pool even if they're not authenticated.
This is a limitation of Temp Add B as the Temp Add B orchestrator does not yet have a password mechanism.
You could, however, wrap the orchestrator in your own password at Proxy.
In this chapter, I'll teach you about Jupyter Hub.
The technical definition of Jupyter Hub is that it's a multi-user server that manages in Proxy's multiple instances of the single user Jupyter notebook server.
A less technical definition is that Jupyter Hub is a multi-user version of the Jupyter notebook.
Jupyter Hub is a Python 3 only application, but that doesn't mean that the kernels that are ran by the notebook servers launched by Jupyter Hub are restricted to Python 3 only.
In other words, the user isn't restricted to Python 3.
Jupyter Hub is comprised of three main pieces, the multi-user hub, the configurable HTTP proxy, and the multiple single user notebook servers that are launched by the hub.
When you start Jupyter Hub, you're actually starting the hub application.
The hub application then spawns the configurable proxy.
The proxy forwards all requests on the root domain to the hub.
The proxy is what's exposed to the internet.
The hub then authenticates the user when the user connects, and the hub will launch a single user notebook server for that user.
It then configures the proxy to route all requests on the root domain forward slash the username to that new single user notebook server that it launched.
Jupyter Hub is highly configurable.
The authentication is configurable.
We're going to look specifically at the O authenticator extension, which allows you to use GitHub authentication with Jupyter Hub, but you could write your own authenticator.
This is useful if your organization uses a specialized authentication scheme.
Second, you can configure the spawning.
In other words, you can configure how single user notebook servers are launched.
We're going to look specifically at the Docker spawner, which is a tool that allows Jupyter Hub to spawn the single user notebook servers using Docker.
And lastly, you can configure the spawn notebook itself.
By default, Jupyter Hub launches the notebook that's installed on the local machine.
If you're using something like the Docker spawner, you can customize the notebook by using the techniques described in the last chapter where we created a custom Jupyter notebook Docker image.
In the following videos, we'll look at three ways to install Jupyter Hub.
The first is a completely vanilla installed directly from package managers.
The second is a vanilla install with the Docker launcher extension.
And the last is a more complex install that uses a combination of the Docker launcher extension and Docker swarm to handle more users to redistribute the demand across multiple machines in order to handle a higher user load.
First, let's remove the dot Jupyter folder that we created in the earlier chapter where we examined installing the vanilla notebook.
We need to do this because Jupyter Hub relies on the local notebook install.
We don't want to dirty our new Jupyter Hub install with the config options that we set earlier.
On the other hand, later you'll find configuration of Jupyter Hub to be easy because configuring the notebook servers that get launched by Jupyter Hub is the exact same procedure that we examined earlier using traitlets in the config machinery to config the vanilla notebook.
All the configuration that you have for the vanilla notebook will apply to the vanilla notebook that's launched by Jupyter Hub.
You'll want to verify that you have Python 3 on your machine.
You can do so by running Python double-dash version.
If your system does not print Python 3, try Python 3 double-dash version.
If that too does not work or does not print version 3, then you'll want to revisit chapter 1 video 3 where we talk about prerequisites and you'll want to make sure that you have Python 3 installed on your machine.
Next, let's look at the version of Node that we have installed.
You can do so by running npm-v.
I have version 3.4.1 installed on my machine.
If your version is lesser than that, you can update it by running sudo npm install-g npm.
What this will do is cause npm to uninstall itself and then install the latest version of itself in its place.
If this command fails partway through, you'll find that you need to reinstall Node and npm.
The first thing we'll install is the configurable HTTP proxy.
You'll recognize that name from the earlier chapter where we looked at deploying tempnb.
However, in that chapter, we used a configurable HTTP proxy docker image.
So we didn't actually install the configurable HTTP proxy on the local machine.
Because we're installing Jupyter Hub on the local machine, we'll need to do that here.
Go ahead and run sudo npm install-g where this dash-g flag installs the software globally configurable HTTP proxy.
Once that is finished, you'll want to install Jupyter Hub.
You can do so by running pip3 install Jupyter Hub.
By running pip3, we force the python3 pip to be used.
If you receive a permission denied error, go ahead and prepend the command with sudo.
Now you can try launching Jupyter Hub.
If you have an error like this, go ahead and uninstall Jupyter Hub and then reinstall it.
When you first run the hub, you may get an error that there's a bad configuration file.
You can fix this by running the command that is recommended.
This command will generate a configuration file for you.
Say yes when asked if you want to override the file.
Now try launching the hub.
If everything is successful, you should get a message saying that the hub is now running at localhost 8000.
In your web browser, try accessing that.
Awesome, looks like that worked.
Now you should be able to log on using your local system credentials.
Now that Jupyter Hub is installed, let's see how it works.
You can launch Jupyter Hub by running Jupyter Hub.
When Jupyter Hub launches, you'll notice a couple warnings.
The first warning is that the config proxy auth token had to be generated by Jupyter Hub.
You can bypass this warning by setting that variable explicitly.
In the future, when you decide to use extensions with Jupyter Hub, such as NBGrader, you'll need to set this token.
This token is how applications can communicate with the configurable HTTP proxy.
NBGrader, for example, adds a handle to the configurable HTTP proxy that allows graders to access notebooks with a special interface.
The second warning you'll see is that no admin users are defined, so the admin interface will not be accessible.
We'll go ahead and ignore that for now.
Switch to your web browser.
We'll access the address listed here.
It should be available at localhost8000.
When you access that address, you'll be presented with a login screen.
Jupyter Hub uses PAM as a default authentication method.
This means that to access Jupyter Hub, you use credentials on the host machine.
In other words, you use your current account name if you're running it locally.
The password is the same password for the account on the host operating system.
When you sign in, you'll be presented with your own notebook server.
In the top right-hand corner, you'll see a button for a control panel and a button to log out.
Go ahead and click on control panel.
In the control panel, you'll see an option to stop your server or access your server.
Go ahead and stop your server.
You'll also see an option to administrate Jupyter Hub.
Click on that.
Here, you'll see a screen that allows you to define new users and remove users.
Here, I'm going to remove JD Fredder.
You can also change users from admin to normal users.
Go ahead and log out.
In this video, I'll show you how to install the Jupyter Hub Docker Launcher extension.
Jupyter Hub is a highly configurable application.
Even the way that Jupyter Hub launches single-user notebook servers is configurable.
The Docker Launcher extension allows you to force Jupyter Hub to launch the single-user notebook servers as Docker images.
With this extension, you can launch any custom Docker image that you have that contains a Jupyter notebook server.
If you want Jupyter Hub to launch the single-user notebook servers using something other than Docker,
you can write your own extension to do so.
To get started, open up a Docker Quick Term.
Once the Docker Quick Terminal launches, pay attention to the IP address.
You'll need that IP address for later during configuration.
Before we get started, we should close all existing Docker images,
just to make sure that none are running that will conflict with what we're trying to do.
To do so, you can run Docker, stop, and then dollar parentheses, Docker PS-A-Q,
semicolon, Docker RM, dollar parentheses, Docker PS-A-Q.
Now, you can get the Docker Spawner extension source code by cloning it from GitHub.
To do so, run git clone, HTTPS, github.com, Jupyter Docker Spawner.git.
You want to run this inside the directory that you want to install the source code to.
I'm doing it inside my home directory.
Now, CD into that repository.
Run pip3 install-r requirements.txt.
This will install the requirements of the Docker Spawner.
Don't forget to add a sudo in front if your permissions require it.
Next, run python3 setup.py install.
Lastly, run sudo pip3 install-e.
Now, we'll need to change our Jupyter Hub config file so it launches using the Docker Spawner.
CD back out into your home directory or whatever directory that you launched Jupyter Hub from.
I launched Jupyter Hub from my home directory.
Once there, open up Jupyter Hub underscore config.py file in your text editor.
I'm going to open it in Adam.
Below the first line, add c.jupyterhub.spawner underscore class equals dockerspawner.dockerspawner.
Pay attention to the capitalization.
This tells Jupyter Hub to use the Docker Spawner.
Next, add c.dockerspawner.use underscore docker underscore client underscore env equal to true.
This allows the Docker Spawner to work with the Docker Quick Terminal.
Next, add c.dockerspawner.tls assert underscore hostname equal to false.
This is also required to use the Docker Quick Term in your custom image with Docker Spawner.
Next, add c.dockerspawner.container underscore image equal the name of your custom image.
I'm going to use the image that I created earlier in the tempnb chapter.
Use your custom image here too.
Now go ahead and save the file.
Now cd into your custom notebook image directory.
This is the same directory from the chapter where we investigate at tempnb.
Open Adam.
Open up your Jupyter underscore notebook config file.
Inside here, add c.notebookapp.baseurl equals os.environ.jpy underscore base underscore url.
This configures the notebook server to listen to the URL that's a subset of Jupyter Hub.
Go ahead and click save and then close Adam.
Now you should be able to launch Jupyter Hub.
Navigate back to the directory that you launched Jupyter Hub from.
Mine is the home directory.
Type Jupyter Hub double dash Docker Spawner.container underscore ip equals 192.168.99.100.
Replace this IP address with the IP address that was listed by Docker when you launched the Quick Term.
Click return.
Once the server launches, go to your web browser.
You should be prompted with a login.
Login using your local credentials.
Once you log in, you should see your custom notebook image running.
This means that everything we did worked.
In the last video, we set up Jupyter Hub with the Docker Spawner extension.
This made Jupyter Hub spawn notebook servers inside Docker images.
In this video, we'll take it a step further and customize how Jupyter Hub does authentication.
Jupyter Hub has a notion of authenticators, which allow you to change how users authenticate with Jupyter Hub.
You can use authentication methods ranging from traditional, used in academia and in the industry,
to more specialized methods, like using social networking or social media authentication.
In this video, we'll look at using GitHub's authentication system.
There's an extension called the O authenticator, which was written for Jupyter Hub to allow us to do this.
First, open up a Docker Quick Terminal.
Once the Quick Terminal launches, make sure to close all images that are already running on the machine.
Include it in the Docker Spawner extension repository is an example of how they use the Docker Spawner with the O authenticator.
We'll use that as a starting point.
First, you want to copy your Jupyter Hub config into that directory.
My Jupyter Hub config is located in my home directory because that's where I launched Jupyter Hub.
So I'm going to copy that from my home directory into that repository example folder.
Next, cd into that directory.
Now run sudo pip3 install get plus HTTPS
forward slash forward slash github.com forward slash Jupyter forward slash O authenticator dot get.
When that finishes, you want to create a user list file.
Let's open up Adam inside this directory.
Once Adam opens, go ahead and right click and create a user list file.
Inside the user list, add GitHub user names that you want to have access to your server.
Don't forget to add your own.
I'm going to add Brian and Kyle, my colleagues.
Make yourself an admin by adding a space and admin after your account name.
Save the file and go ahead and close Adam for now.
In your web browser, go to github.com forward slash settings forward slash applications forward slash new.
When that page loads, give your application a name.
I'm going to call mine Jupyter Hub.
This is the name that users will see when authenticating while connecting to your Jupyter Hub instance.
Set the homepage URL to the Jupyter Hub URL.
This should be for now local host 8000.
Go ahead and copy that URL.
Paste it below where it says authorization callback URL.
Then append hub forward slash OAuth underscore callback.
Now click register application.
Go back to your desktop.
Launch a Docker quick start terminal.
Once the quick start terminal launches, pay attention to the IP address that's listed.
You'll need this later.
Now let's CD into the Docker Spanner directory.
Inside that, CD into the OAuth examples directory.
Now open Adam.
Once Adam opens in that directory, open the Jupyter Hub config file.
Below the container image line, you're going to need to add a new line.
Add C dot Jupyter Hub dot authenticator underscore class equal to in quotes OAuth.github OAuth.
Now below that line, add C dot GitHub OAuth authenticator dot OAuth underscore callback underscore URL
equal to the URL that you provided for the authentication callback while creating the application on github.com.
I'm going to go back to my web browser to show you what that URL is.
At the bottom of the page, you'll see it.
Go ahead and copy that.
Now below that line, add C dot GitHub OAuth authenticator dot client underscore ID
equal to the client ID provided to you by github.
It's located at the top of the page.
Now below that line, add C dot GitHub OAuth authenticator dot client underscore secret
equal to the secret provided to you by github.
Lastly, on the line below that, you'll need to set yourself as an administrator.
To do so, set C dot authenticator dot admin underscore users equal to
and then in square brackets and quotes your account name.
This is your GitHub account name.
Now save the file.
Back in the terminal run dash run dot sh double dash Docker spawner dot container IP
equal to the IP address listed in green.
Now in your web browser, navigate to the Jupyter Hub instance.
It should be at local host colon 8000.
Once you arrive on that page, click the sign in with GitHub button.
You should be asked to authorize the application.
Click authorize.
You'll then be redirected back to your Jupyter Hub instance.
You can click my server to access your server or admin to administrate Jupyter Hub.
I'm going to click on my server.
Note that our custom image is still being loaded.
In the previous videos, we were able to get Jupyter Hub working with GitHub OAuth
and a custom Docker image.
In this video, we'll look at how we can enable our users to share files across their different accounts
inside the Jupyter Hub instance.
To do so, we'll mount a shared directory on the host operating system.
We'll do this two ways.
One, we'll mount it as read only for content that all users should be able to see,
but not necessarily edit.
Two, we'll mount it as read write so users can have a shared directory
from which they can save files and fetch files.
To get started, open up a Docker quick terminal.
Once your Docker quick terminal launches, go ahead and make sure no Docker images are currently running.
Inside my home directory, I'm going to create two shared folders.
One will be called shared underscore RW for shared read write,
and the other shared underscore R for read only shared.
You can use any directory that's accessible on your file system.
I'm using my home directory as a convenience.
Now, I'm going to copy two example notebooks into each of those folders.
I'm going to CD into the shared read write folder and launch a normal Jupyter notebook server.
When the notebook server launches, I'm going to go ahead and create a new Python notebook.
First, I'm going to change the name of this notebook.
I'll change it to test one.
Now, I'll give the notebook some content.
I'll make the first cell a markdown cell.
In the second cell, I'll add some code.
Now, I'm going to save this notebook.
Now, close the web browser and go back to the terminal.
In the terminal, I'll hit Ctrl C twice to close the server.
Now, I'll CD into the read only directory.
I'll launch the notebook server here too.
Once the notebook server launches, I'm going to create a new notebook.
I'll call this notebook test two.
I'll make the first cell a markdown cell.
In the second cell, I'll add some code.
Now, I'll save the file and close the web browser.
Next, I'll close the Jupyter notebook server by hitting Ctrl C twice.
Now, CD into the Docker spawner directory.
Inside there, I'll CD into the example's OAuth directory and open Adam.
Once Adam opens, I'll make sure my Jupyter Hub underscore config file is opened.
Then, below the admin users line, I'll add c.dockersponer.volumes equals a mapping of volumes.
The volume mapping is path on the local machine as the key
and as the value path that it should be mounted inside the Docker image.
I'll mount the read-write directory.
I'll have it mounted to home jovian for slash work for slash shared.
That's because home jovian work is the directory that's loaded by default inside the Docker image.
To mount read-only directories, the syntax is almost the same.
Go ahead and copy that line and paste a copy of it below.
On this line, we'll change the name of the path that's mounted.
Let's change it to read-only.
Likewise, we'll change the path on the parent system to the read-only directory.
The important part is that the key is not docersponer.volumes, it's actually dot read underscore only underscore volumes.
Once you make that change, go ahead and save the file.
Go back to the terminal.
Now, launch the server like you did before.
Don't forget to set the Docker spawner container IP trait.
The IP address is the IP listed by the Docker quick terminal in green when you launched it.
Once your server is launched, go back to your web browser.
In your web browser, navigate to your Jupyter Hub instance.
You may still be logged on to your other session from the earlier videos. That's okay.
Go ahead and click on my server.
When my server loads, you should see two folders, read-only and shared.
Go ahead and open shared.
Inside shared, you should see the test one notebook.
Go ahead and open that.
Make a change to this notebook.
It doesn't matter what change, just a change that you can see.
Then go ahead and try saving the notebook.
When you save, you should have seen the checkpoint flash up to the left of the kernel name.
Go ahead and close the notebook.
And try reopening it.
Looks like that worked.
Go ahead and close the notebook.
Go back to your home directory.
Then go inside the read-only directory.
Open up the test to that notebook.
When you open this notebook, you should see a notification that flashes quickly to the left of the kernel that says auto-save disabled.
You should also see an icon of a floppy with a red circle above it indicating that saving is disabled.
Try making changes to this file.
Any changes, it doesn't matter.
I'm going to remove this read-only.
Now I'm going to try saving.
When I save, I should see another notification in yellow that says the notebook is read-only.
Go ahead and close the notebook.
Reopen the notebook.
And you should notice your changes weren't saved.
This means that the read-only is working correctly.
In this video, we'll talk about how you can increase the performance of your Jupyter Hub deployment using EngineX.
EngineX will be used to host the static files of the Jupyter notebook.
The Jupyter notebook uses Tornado to host its web content.
Tornado is great for templating and hosting dynamic content.
However, it's slower than things like EngineX or Apache to host static files.
The methods described in this video can also be extended to redirect and host the static content on CDNs.
First, we're going to launch Jupyter Hub.
Go ahead and open up a Docker quick terminal.
Pay attention to the IP in green.
Then make sure that all Docker images are closed.
Next, navigate into the OAuth example folder inside the Docker spawner directory.
Launch Jupyter Hub by running the run.sh script.
Once Jupyter Hub launches, open up your web browser and verify that Jupyter Hub is running.
This should be available at localhost colon 8000.
Now, go back to the terminal.
Open up a new tab by hitting command T.
If you're on a machine that doesn't support tabs in your terminal, open up a new terminal.
Now, we'll install EngineX.
On OS X, you can do this using brew.
On Linux operating systems, you'll want to use the package manager of that system.
Typically, this is apt-get or yum.
If you're on OS X, go to your web browser.
Go to brew.sh.
This is the home page for brew.
If you don't have brew installed already, copy the line under the install home brew section inside the text box.
Paste that line in your terminal and execute it to install home brew.
I've already installed home brew on my machine, so I'm not going to demonstrate this for you.
Go back to your terminal.
Now, make sure that brew is up to date.
To do so, you're going to run brew update.
Now, we'll use brew to install EngineX.
Once brew is finished installing EngineX, run EngineX.
Now, go back to your web browser.
Access localhost 8080 to see if EngineX is running.
If EngineX is running, you should see a welcome to EngineX page.
Now, go back to your terminal.
Run EngineX-S to stop the EngineX service.
Now, go back to your web browser.
Go to github.com.
You should see the Jupyter Hub application that you registered earlier.
Click on that.
Now, change the port on the home page and the authentication callback URL to 8080.
Go back to your terminal.
Now, change the EngineX configuration file so that it proxies all requests to Jupyter Hub.
We'll also proxy the web socket connections to Jupyter Hub.
However, we'll intercept requests to static assets and host those directly using EngineX.
To edit the EngineX configuration file on OS X, run atom or open up
forward slash usr forward slash local forward slash Etsy forward slash EngineX forward slash EngineX.conf.
This is the path to the configuration file for EngineX.
If you're running EngineX on a machine other than OS X, this path may be different.
You'll have to refer to your installation method to figure out where the configuration file lives by default.
I'm going to open this file in atom.
The first thing we'll do is trim a lot of the comments and access lines.
This will allow us to focus better on what the contents of the configuration file should be.
I'm going to go ahead and remove this userNobody comment.
And also the log comments and process ID comment below.
I'll leave the events block and remove the log format comment, access log comment,
send file, TCP push, keep a live time out, gzip, all the way down to the server block.
Inside the server block, I'll leave the listen to port 8080 and server name local host lines.
I'll remove the lines down to the location forward slash.
Everything from here on out, I'll remove.
Now we'll configure all requests on root to forward to Jupyter Hub.
To do so, remove the lines inside the root block.
The first line you'll need is proxy underscore pass space, the address to Jupyter Hub.
Next, proxy underscore set underscore header, capital X dash capital R real dash all caps IP space dollar remote underscore add semicolon.
Next, you'll want proxy underscore set underscore header host with the capital H dollar HTTP underscore host semicolon.
In the last line you'll want in the root section proxy underscore set underscore header space capital X dash capital F forward it dash capital F four space dollar proxy underscore add underscore X underscore forward it underscore four.
Now copy these four lines that you just wrote.
Below the location root block, we'll need to add another location block, which will only intercept attempts to connect to WebSockets.
We have to handle WebSocket forwarding specially. This is a detail of engine X configuration.
To do so, write location space till day asterisk.
Then we're going to add a long regular expression that will look kind of funky.
This regular expression will be used to match the request path for WebSocket connections.
This first group is matching the user forward slash account name section of the URL.
The second group matches the WebSocket request specific to the notebook server.
Then suffix with forward slash question mark, and that's all you need for the regular expression that identifies WebSocket requests.
I'll turn on word wrap so you can see this whole line.
Inside that group, paste the four lines that you copied earlier.
You'll need to add some additional lines to get WebSocket forwarding the work.
First, you'll want to add proxy underscore HTTP underscore version space one point one.
Next, you'll want to add proxy underscore set underscore header space capital U upgrade space dollar HTTP underscore upgrade semicolon.
Next, add proxy underscore set underscore header space capital C connection space upgrade in quotes semicolon.
Last, you'll want to add proxy underscore read underscore timeout space 86,400 semicolon.
This is all you need to get content to forward to Jupyter Hub using engine X.
The last piece we'll want to add is to intercept request for static assets.
We'll want to host directly from the notebook directory.
But first, let's make sure that this is working.
Save the file.
Go back to your terminal.
Launch engine X.
If you get a message like this, it means there's something wrong with your configuration file.
It looks like mine has a typo.
Remote underscore add was supposed to be remote underscore adder.
I'm going to add an R and then save the file.
Now I'm going to go back to the terminal.
I'm going to try launching engine X again.
It looks like I missed another instance of remote add.
Also down here where I copied that content from the root.
I'm going to save the file.
I'll try launching engine X again.
Looks like it launched successfully.
Now I'm going to go to my web browser to verify that it launched.
I'm going to try accessing engine X.
If you recall correctly, it's at localhost 8080.
When I first access it, it looks as if what I did had no effect on engine X.
However, this is because my web browser is caching the contents of the last request.
By refreshing the page, I should see the right contents.
If refreshing the page doesn't fix the problem for you, you may need to clear your web browser's cache.
To do so, you'll have to follow steps specific to your web browser.
I'm going to go ahead and click on my server.
I need to validate that the proxy for the web sockets is working.
I'm going to open up the shared folder.
And then the test one notebook.
I'm going to try to run the cell with a change.
If it works, I know that the web sockets are forwarding correctly because the notebook is able to execute code.
I'm going to save and close this notebook.
Now I want to try to speed up this Jupyter Hub instance.
To do so, I'll have to intercept request the static.
I'm going to go back to my terminal.
The first thing I need to do is make sure that I have the static notebook files somewhere on my computer.
That way, Nginx can host them.
I'm going to navigate to my root directory.
Here, to clone the notebook, I'm going to run getClone, space, HTTPS, github.com, forward slash Jupyter, forward slash notebook.
Once the notebook clones successfully, I'm going to go back to the atom instance that I used to open the Nginx configuration.
Above the location root block, I'm going to add a new block.
This block will recognize requests for static assets.
To do so, I'll have to use a regular expression again.
This time, just use tilde, no asterisk.
The regular expression is as follows.
Forward slash.
And the first group is the user block, just like we did earlier.
And then the next block is forward slash static forward slash.
Lastly, parentheses dot asterisk to match all characters, forward slash, question, v equals, and then parentheses dot asterisk to match all characters.
Now, you're going to specify the root directory to the directory that we clone the notebook repository to.
When that is finished, save the file and return to your terminal.
Make sure to stop Nginx if it's already running by running Nginx, dash s, stop.
Then run Nginx to launch Nginx again.
Now let's go back to the web browser.
Navigate back to the root page, refresh the page.
If everything worked, the page shouldn't look any different.
However, this Jupyter logo, for example, is being hosted by Nginx.
