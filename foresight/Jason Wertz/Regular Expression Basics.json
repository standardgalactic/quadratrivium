{"text": " In this video lecture we're going to talk about regular expressions. We'll also take a look at the command line utilities grep and sed. So what are regular expressions? Well regular expressions are patterns that we can build to match against specific series of characters, you know, words, in a programming sense we can say strings. We can use these patterns to pull out specific pieces of information from a larger set of information that we're interested in. So if we're looking at something like a very large log file, we can use a regular expression to just isolate those aspects of the log file that we're interested in. This is really great when you've got a lot of data to pick through on the command line. We can also use regular expressions for finding data and then acting upon it and we'll take a brief look at sed which is the stream editor later on in this video lecture. So for now we're going to look at mostly a tool called grep which allows us to show or not show specific aspects of a file that conform to one of our regular expression patterns. So the command line grep utility is used to apply patterns to either standard input or to a specific file. And so we might have a very large file and what we're interested in doing is just taking a look at certain aspects of it. There's actually two forms of grep and grep stands for get regular expression and there's also grep and egrep. And what you're going to find is that because Unix is so old, it's had a lot of time to evolve and sometimes people have gone on and extended aspects of Unix and in their sense of making it better. So the idea of grep and egrep is that grep is the original grep and egrep is extended grep. And extended grep actually supports extended regular expressions. So anytime a language has been around for a long time, people try to start to add to it when they start to notice failings in that language. And what I'll mention is that we'll look at both extended and regular, regular expressions. There's also pearl regular expressions, which I'm most familiar with. So I also throw that out there when I'm talking about regular expressions. Anytime I make an error, I will probably be thinking in terms of pearl regular expressions. So enough about what regular expressions are, what utilities we're going to use, what does a regular expression do and look like? Let's think about a typical pattern that we might want to apply in this case. Let's say that I have asked someone to look through the phone book or a directory of employees and show me everybody whose last name starts with the letter s. So I've given you a pattern. I've given you a few bits of information. I've said, okay, I want you to look at last name, oops, and I want that last name to start with the letter s. So if I have somebody whose name is, right, like John Smith, this would match my pattern because I look at the last name and the first letter of that last name is an s. Their name was Steve, John, I guess we'll stick with people with two first names. Well, the first name starts with the letter s, but the last name doesn't, so it doesn't match my pattern. So we define patterns and we do so programmatically and we use those patterns to filter or match against information that we're most interested in. Let's look at some really basic ways that we can apply patterns on the command line. I'm currently in my home directory and there's a number of folders and at least two files in my home directory. So there's actually some basic pattern matching capabilities built into the Bash shell. So for example, if I want to see all of the folders that start with the capital letter d, notice that if I just put the letter d, I get an error because it's going to say, I can't find a folder or file named d, but if I add a star after the word d, I get a lot of information. In fact, I wanted to look just for folders, I would do dash d, which would give me information about the directories as opposed to items inside the directories. And this star character on the shell is called the file glob operator and basically you can think of it like a wild card match. And the pattern that I've essentially given the ls command here is that the ls command should display all folders that start with the capital d and then folders that have any number of characters following it. So when I mean wild card, I mean any number from zero to an infinite amount and I mean any type of character. So basically this would match any folder that starts with d and goes ahead and has a bunch of stuff in front of it. You can also use a wild card at the front of a string, so I'm going to see any folder that starts with the letter s and notice it shows me documents, downloads, pictures, templates and videos because all of those end with a lower case s. And remember that the command line is case sensitive, so if I do capital s, I get nothing. You can also do something along the lines of putting two stars on the command line. So I could say something like show me all directories that have any number of characters, the letter l in them and then any number of characters on the other side. And in this case it shows me downloads, examples.desktop, public and templates because all of these have a letter l in the middle that we can see in each one of these and on either side there is any given number of characters that match this wild card pattern. So this is built into the shell and this can be really useful if you wanted to show say all files in a directory and so if we look back up into the shell it will notice that in my desktop directory I have a couple .txt files. Let's clear this and let's say I wanted to do ls-l desktop star .txt and what this would do is say okay go into my desktop directory, show me files that have any name and then have a .txt extension and if I hit enter I see those two files sitting in my home directory. So pretty cool that those exist and that this capability exists. But what we want to do is we want to actually look at how we can utilize grep to get more detail information about things like file directory listings and actual files themselves. So ls is great when you're looking for just information about some listings and the wild card is helpful for getting some basic pattern matching going. But to learn regular expressions what we're going to do is we're going to look at a very specific file on the Unix system and that file is the American English dictionary used for spell checking and other things by the system. So I'm going to pass the American English dictionary file which is found in slash user slash share slash dict slash American English into the less command so we can see what's in there and what you'll notice is this is a very large file that goes on and on and on and on and keeps going and there's lots of words in it. So this is really useful for our pattern matching exercises so I'm going to hit Q and we're going to start to talk about some really basic regular expressions. And the way we're going to do this is we're going to actually use the command grep. Remember that I had mentioned there are actually grep and egrep. So what we'll do is we'll use grep until it breaks. In other words we'll use grep until we run into a feature that is an extended regular expression and then we will jump over to using the egrep command. Actually it's a better habit to just get into using egrep all the time and that way you don't have to worry about what is an extended and what isn't an extended regular expression. But I kind of want to mention, show you that at some point some of these features will not work. So I am going to actually set this up so that we can grep this file and the way greps work is that we give the grep command what we'll do is right in here we'll put a pattern and then we will actually go ahead and match it against a file. And so the easiest pattern that I can put in here is I want to look for say words that have cat in it. So let's use cat and see what we get. What you'll notice is a lot of words scroll by. I gave the pattern cat and maybe you were thinking well I'll only see the word cat but notice that it shows me every word in the file that has the letters cat somewhere in the word. So that's what the color coded command line is we can actually see the matches. So that's kind of a nice feature for learning how to use regular expressions. And you'll also notice that this is a really big file so at some point we'll look at we're always going to see letters at the end of the alphabet just because of the way we're doing this. So let's clear this and take a look at that pattern again. My pattern is called cat and for the rest of the exercise is the other things or is cat. And I'm going to put these in double quotes because it'll make it a little bit easier to see where our pattern is within these lectures and it'll also help us deal with any non-standard characters. In other words any characters that we want to match in our pattern but have special meaning to the shell we want to make sure that those are not processed by the shell in a way that the shell wants to process them putting them in double quotes kind of fixes that. So what does this pattern say? It says well look for a C followed by an A followed by a T and what you found out again is when I run that command it doesn't really care if there's any characters before the cat or after the word cat it just looks for those three characters in order. So that's pretty interesting. What if we wanted to look for not just cat but you were wondering like well I wonder if there's a word that looks like cat that has another word in the middle. And maybe you were thinking of like cut or cot and you wanted to find all of these. Well regular expressions come with a special character which is the dot character and the dot character will match one of any character. So if there's actually a word called C2T then putting a dot there would match it. So what this will match is a C followed by any one character followed by the letter T. So you can just see in this little bit of information that we got I didn't even think about words like yachts which have Cht or watchtower. So we matched Cht, Cat, Cut, Cot and that's pretty cool. Why? Because the pattern used a dot. So let's talk about why this is and let's look at some other patterns that we might have available to us. So what we just looked at was this idea of basic elements. You can literally type a string and that will match the string or the series of characters and in this case we can also then use dots in our series of characters to represent any given one instance of any given character. So notice my first example matches bat, cat, mat. Finally if I just give the letters DOG it would match the literal string dog but found anywhere in a word. What's cool with regular expressions is we can start to get tricky. And so in our previous example we found we used the dot to find any character but let's say we only wanted to limit ourselves to finding a subset of any character and we can actually create what are called classes of characters. So we can have classes of characters by putting them into hard brackets and A-C will search for an A or a B or a C followed by an A and a T and this would match bat and cat but not rat. And let's take a look at this in the command line in a second. You can also use character classes to find uppercase, lowercase as well as numbers within your strings. Now again this is not looking for the number nine, this is looking for the character nine, the symbol that represents the value nine. So this is really just looking for a character zero through nine followed by another character zero through nine and this would match forty two, thirty seven, ninety nine, zero one. So basically match any two characters next to each other anywhere in a word. So let's take a look at that. So previously when we used the C dot T we matched cut, cat, cut. So the O, the A and the U. So let's say I don't want these watchtowers, CHT, I just want like C followed by a vowel followed by a T. So we can do that instead of using the dot I can create a character class. And if I want I can try to match any vowel, I don't know what we'll get but we'll see. And so what this says is look for the letter C followed by A, E, I, O, or U, just one of those but it can choose amongst all of the values in that set followed by the letter T. And now if I look at my output I get veracity, I get wainscot, I get vocatives, I get wildcat so I'm matching all the vowels between the letters C and T. All because of this character set that I've made. If you want to try and find any words in the dictionary, so that's character sets. And by the way you can use these and we'll see these in a second, if you want to match all lowercase characters you would do A through Z. You can also say find all lowercase and all uppercase characters which maybe you're looking for usernames or something along those lines. And then if you wanted to you could also add 0 through 9 and say like the underbar. And what that would do is match for all of those characters so you can have these sets of characters be as small or as large as you want them to be. I'm going to do is changes, let's see if there's any words that have 0 through 9, I doubt it. And nothing matches that pattern. That's what happens when you put in a pattern that doesn't match. Let's see if there's anything in the dictionary that matches against a number. Nope, so there are no numbers in the American English dictionary on the Unix system. So it's kind of helpful. There's a couple other things that we could actually use if we wanted to grep numbers. What I'll do right now is just to look at what we could possibly grep for a number would be the Etsy password file. And if I grep that I know there's a lot of numbers in there so I'm going to get a lot of matches and you'll notice that it matched all of these lines because all of the lines have a number in them. Again, the Etsy password files just shows you all the user accounts on the system and these numbers represent their user and group IDs. So I know that there were a lot of numeric matches in this file. So just to kind of demonstrate that you can match against numbers as well as words. So kind of helpful. Let's go back to an example where we can start to look at the English dictionary. So let's look at some other options for this feature, this ability to use character classes. Built into Unix is this idea of these really what I think are pretty ugly character classes. You'll see these sometimes built into the system. So bracket, bracket, colon, alpha, colon, bracket, bracket is the same as building your own character class that uses bracket A through Z, capital A through Z. And basically what these things on the left are are just ways of writing these out using words as opposed to using the categories A through Z. Pretty much if you haven't figured this out at this point in Unix, there's 50 ways to do everything. So these character class abbreviations can be helpful. They make things a little more readable in scripts. But the other thing if you haven't noticed is that regular expressions can get pretty big and pretty scary pretty fast. I would say the most useful character class that we're looking at right here is the one called space because that will match any white space characters. That includes tabs and spaces. That can be really helpful when trying to match words or sentences that have spaces. Or in my first example, we talked about last names and that would really be helpful for processing, finding somebody's last name. Because I don't like these, I'm mostly familiar with these character class abbreviations. So notice there's two character class abbreviations. The previous ones with the words like alpha, upper, and lower. But then there's also these character class abbreviations, slash D, slash W, slash S, slash S that make things a little bit more readable when we start to use these in larger regular expressions. I'll also mention that there are actually ways to negate character classes. If you notice the carrot inside of the bracket-based classes at the bottom here, you'll notice that that means does not match this character. And if you do slash capital D, slash capital W, slash capital S, that means that you do not want to match that character in that given position. I don't usually teach those when I introduce regular expressions because it's hard enough getting your head around regular expressions, let alone getting your head around negative logic at the same time. So we'll just kind of mention that they're there and just ignore that. So let's look at these two character class sets in operation. So let's say I want to find a word that has any letter in it. Well notice I just did that and I said that I wanted to find a slash D, but notice that it actually went ahead and found all of these words with the character D. Well that's a problem. You're thinking, well Jason, wait, you just showed us that slash D is a character class. But here's the thing. Remember earlier on I said there's grep and there's extended grep? Well these slash D character sets are actually part of extended grep. So if I switch to eGrep, it still doesn't work, which is a problem because I'm trying to explain this. Once you'll notice is that the slash D doesn't work, it actually goes ahead and finds the letter D within here. Slash D is actually a part of Perl regular expression. So if I actually add dash P, you'll note that it returns nothing. So that actually says u slash D, like it's a Perl regular expression and that'll pass numbers. So that doesn't actually work in grep or eGrep. But if I get rid of the dash P and just go with regular extended grep, what you'll find is I can actually utilize slash W. And that matches any character in any word that we might be looking at. So that one does work. And if we go ahead and we look at slash S, what you'll notice is that that also returns nothing. So it looks like slash D is one of those cases where that's more of a Perl thing than it is an extended grep thing. So I'll apologize for that and keep going. Or slash D is the same as typing 0 through 9. So you don't need to worry about the fact that slash D is not supported in eGrep. Let's take a look at some other ways that we can build patterns. We'll start out by taking a look at what are called anchors or positional anchors. In other words, we can say that a certain character has to match at a certain point on a line, either at the beginning of the line or at the end of the line. And you'll see that the characters that we're going to use for these beginning and end of line matches are a carrot and a dollar sign. And you'll notice that in the example in front of you, putting a carrot means to match only the word car, where car is the first line character on the line. So it would match car, cattle, and canine, where those are the first words on a line. And the second example, floating and sailing, would match because the G is at the end of the line if those are the only words on the line. And if you'd like to match situations where a word is the only word on a given line, such as cat, you would want a line that starts with the letter C, a line that ends with the letter T. We'll take a look at these in one second. Let's look at another possible anchor that we can use, which is a word boundary. So remember, the carrot character and the dollar sign character have to do with the beginning and the end of line. And word boundaries give us the ability to match on spaces, tabs, and basically these positions where there might be breaks between words. So in the example that I've got at the bottom, if I wanted to match Jason the prof, S-O-N is anchored on a word boundary. So there's a space between my name and the word the. So S-O-N slash B would match that line. Actually, there's no other instance of S-O-N on that line, but let's say I was in a file that had a lot of people named Jason, but I was the only one with the surname the prof. It would match. So let's take a look at how word boundaries and anchors work in real life. So to match items in word boundaries, we really need a file to do this. So what I'm going to do is take another look at the Etsy password file. I've also bumped up the font, so hopefully things are a little bit easier to see. So what you'll notice is in the Etsy password file, there's a number of lines here. So I've got root, demon, bin, sys. So let's say I only want to see lines that start with the letter S. There's a couple of them. So I'm going to use eGREP, and I'm going to say that I want to look at lines that start with the letter S, and I want to look at the Etsy password file. And what you'll notice is that shows me lines that just start with the letter S. Notice that the letter S might be in other parts of the line. If I take out that position qualifier, what you'll notice is that the letter S appears on many more lines, but I only want to be interested in those lines where the letter S is at the beginning. If I want to see which lines have at the end, the letter N, I can put a dollar sign, and it'll show me only the lines that end in the letter N. Let's try H. I was thinking bin, but I think I meant bash. And so what it'll show you is that many of these user accounts use the bash shell. And let's say, notice in this case I'm seeing bash and SH. Let's say I'm only concerned about those users in this file that have the login shell bash. So let's see how we can fix that. Instead of just saying H, I could say bash, and what this will say is the last letter on the line must be an H, but before it must come a B, A, and an S. And now I get exactly the information I want, lines that end with bash. So that's the concept behind positional values within regular expressions. What I have here is a file called demo that contains two lines. One is JSON space, the space prof, and the other line is JSONium. So maybe I have my own element. And if I grab this file for my name, what you're going to notice is that it shows me both lines. Maybe I only want to show those lines where it's actually my name and not my name embedded in another word. And the way I can do this is to add a word boundary to my regular expression. And remember slash B means word boundary. And so now it will only show me the JSON that is up against a word boundary. So it's pretty helpful when you know you have spaces in a file and you want to find words that you know are on a space. The last thing for us to review is the concept of quantifiers in a regular expression. Sometimes you want to look for zero or one or two or three of a certain character to appear in a certain spot. Other times you want to know exactly a specific number of items that you want to appear in a given spot in your regular expression. And so regular expressions come with the plus question mark and star operator. And while the star operator seems to work like it does on the shell, it's important to note that there is a slight difference between the file glob operator on the bash command line as opposed to the star operator within a regular expression. So how would we actually utilize these within some regular expressions? Well let's take a look at some sample regular expressions and then we'll go to the command line. So here's a sample regular expression that matches a phone number in the order of 555-555-5555. So typical US phone number, three digits dash, three digits dash, four digits. And you'll notice that I've got some positional notations here. So the line starts with a carrot and ends with a dollar sign. And notice that I've got a character class zero through nine. And since I know I want to match three characters, I could actually type bracket zero dash nine bracket, bracket zero dash nine bracket, bracket zero dash nine bracket dash. Or notice in this case I can use curly bracket three curly bracket, which says match exactly three of whatever you find before you. So we can actually use these curly bracket number, curly bracket notations to kind of make our regular expressions a little more condensed. So we'll look at this on the command line in a second and discuss. Also talked a little bit about the slash w slash b kind of type word boundary commands and the slash w character class commands. And you'll notice that I've written a really poor regular expression here that matches an email address. Regular expression email address matching can be helpful, but it's not something I would rely on because there's a number of email address formats and it's too complicated to really catch all of them. If you do a Google search for email address regular expression, take a look at some of the answers that you get. They're pretty long and detailed. But what you'll notice is this email address says it's also positionally situated. So it's got a carat and a dollar sign. So the only thing on this line should be an email address. It says find any number of word characters. So you'll notice that the star in this case says, and you'll notice the same thing with the number, the curly bracket three notation is it affects whatever is to the left of it. So this is look for any letter or number or valid email address character and then star after it means any number of valid email address characters, followed by an at, followed any number of valid characters, letters or numbers, followed by a dot, followed by any other number of valid letters or characters. Notice also that because I actually wanted a dot here, I didn't want to look for, remember that a dot has a special meaning with regular expressions. Anytime you want to tell a regular expression to use the actual character and not interpret it as a special regular expression character, you just put a slash in front of it. So let's take a look at how some of these components can work on the command line. I have a file that might be the file you're using for your lab. And if we look at the file lab3test.txt, what you're going to notice is it's a file full of names and phone numbers. So we're just going to look at matching the phone numbers since I've already given you a regular expression that does that. Let's see how that works. I want to build a regular expression to match just valid phone numbers in this file. So I'm going to use egrep and I'm going to put my pattern in here and I'm going to use lab3test.txt as my file. So before I said if I really wanted to match a phone number, I could write a regular expression that looks like this. And if I just run that, I'm not going to type, notice it matches any line that has three numbers in a row, or any instance of three numbers in a row. But notice it also gets these kind of user IDs over here. So then I could say as well, what if I match a dash, let me clear this, and then what if I add a dash? Well now we're getting better, we're getting closer, because it's matching, it's no longer matching these IDs on the left. But it is matching these two multi-formatted phone numbers and by the way, if anybody's dealt with large amounts of data that was input by humans over a period of time, you've probably run into situations like this where you have inconsistent data entry. So again you say, well I kind of just want these dash formats, because these are the only ones I really want to identify. So let's clear this and go back and look at how to fix a regular expression. So what I could do is just add one more number and I'm getting better. But still notice it's still matching out on these other numbers. So you notice that these are greedy matches, in other words, it keeps trying to match things. So I can keep adding these until I get the total number of characters that I want, but this is getting hard to read and getting long. So we just talked about characters. So I know my phone number is going to have three characters. So what this says is, look for a value that could be any value between zero and nine, look for three of them in a row. And they don't need to be the same number, like 222, it could be 215 or whatever. Then follow that with a dash. Then look for another series of numbers, zero through nine, and look for three of those. Let's run it. And it's still being a little too greedy, so we're going to have to extend it out here to get these four numbers. Also notice this file is a broken phone number in it. That's there on purpose, but we'll skip that for this exercise. Now I'm going to put another dash in, zero through nine, and I'm going to put the number four. And so what this says is, find three numbers, followed by a dash, followed by three numbers, followed by a dash, followed by four numbers. By the way, notice I'm using eGrep for this, and notice that it gave me the valid answer. If I use plain grep, I just want to point out that you'll get an error because in these curly brackets, there is a way to make them work in grep, but it's just easier to use eGrep because these curly brackets are part of the extended grep syntax. So just be sure that when you utilize numeric quantifiers that you utilize eGrep to do that. Now let's look at the plus question mark and star operators, and we'll go back to searching through files in the dictionary file. So let's say I wanted to find some words that have, start with a B, have an O, and then have the letter T, like bot. So I get a lot of things, saboteurs, robots, lobotomy, but what's interesting about that is it's definitely more information than I want. So let's say I'm curious about if there are words that have zero or one O's in them. And if I put a question mark, a question mark will say in this specific case that the O, there could be zero O's or there could be one O's. Let's see if we can make any changes. And I do. Notice that it matches subtropical and it matches turbot. It says, well, match any characters where there is an O or not an O. There could be zero O's or there could be one O. The plus sign will match one or more of a character. So in this case, it brings in toll booth as well as sabotage and robot. So notice there's two O's there, one O there. And then finally, we could use the star, which means zero to infinity. And so notice that star actually gives us the two O's, zero O's, and one O. Up to this point, we've been using grep to filter information so we can target stuff that we want in more detail. But we can also use regular expressions to find things and modify them. The set command is the stream editor. And while it can be a very powerful tool, what I want to do here is just kind of introduce how it works very basically so we have an idea how we can use regular expressions in a new and different way. So what this allows us to do is we can basically use the set editor to take stream of data and match against a pattern and then modify that information that matches that pattern in some way. One obvious way you could use this is to go through a bunch of files and find the old boss's name and replace it with the new boss's name. It's one way that I've actually used this. And there's a couple other things that are useful, so it's a great way to find something and replace it. So you can use set to find and replace stuff. It's a really powerful tool and we're going to only look at one aspect of it. So we're going to look at a command very similar to this one that's in this example. Let's jump over to the command line. Let's take a look at how set works. For this to work, we're going to actually need a file to edit. And so we're not going to actually edit the file in place today. We're just going to dump data out to standard output and what we're going to do is just modify what's printed to standard output. We could obviously redirect that back to another file. We could actually edit the file in place, but these are all some more advanced things. We just want to look at using regular expressions in a new way. So I am going to actually utilize the Etsy password file for this. And actually, if we just look at that file real quick, which you'll notice is that everything in here is separated by colons. So let's say I want to get rid of those colons. So let's use set. We'll get rid of this. So set dash E and dash E means I'm going to give set an expression. In this case, I'm going to use single quotes and a couple of things. I'm going to set this up and talk a little bit about what it means. Notice I've got single quote S slash slash slash G single quote. So whatever I want to find goes between the first two slashes. So in this case, I'm saying find the colon. And now I'm going to say replace it with three dashes. And what this will do is, said we'll go through the file, find every instance, and by the way, G stands for global. And that means it will find every instance of a colon in the file and replace it with three dashes. And if you look at my output, now instead of colons, you see a bunch of three dashes. So this can be really helpful for a bunch of reasons. One of the things that we can do too is we can use all of our new found regular expression powers in this file. So if we take another look at that password file, one of the things you'll notice at the top is, let's say there's a user called bin. But then you'll also notice that the word bin shows up a lot of times in this file. Let's say I just want to change that user bin. But I don't want to change any other instance of bin. So what I could do is, go back up to my previous command, and what I'm going to do is, instead of just match bin, which would match everything, I'm going to actually go ahead and what am I going to replace bin with? I'm going to replace it with JSON, really big. So if I just do that, notice it replaces every instance of bin in this file with the word JSON. It replaced the one I want, but then it replaced all these other instances. So what I want to do is, use an anchor. So I'm going to say, only match the bin that's at the start of the line. And now you'll notice that all the other bins are left in place, but if I scroll back up and look, it only replaced the one that was at the start of the line. So said it's a really powerful tool and we'll take a look at it more as the semester progresses. But in this case, I just want to show you that you can use your new found regular expression powers, not just a filter data for viewing, but filter data for editing.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.2, "text": " In this video lecture we're going to talk about regular expressions.", "tokens": [50364, 682, 341, 960, 7991, 321, 434, 516, 281, 751, 466, 3890, 15277, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 1, "seek": 0, "start": 7.2, "end": 11.36, "text": " We'll also take a look at the command line utilities grep and sed.", "tokens": [50724, 492, 603, 611, 747, 257, 574, 412, 264, 5622, 1622, 30482, 6066, 79, 293, 9643, 13, 50932], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 2, "seek": 0, "start": 11.36, "end": 13.08, "text": " So what are regular expressions?", "tokens": [50932, 407, 437, 366, 3890, 15277, 30, 51018], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 3, "seek": 0, "start": 13.08, "end": 19.400000000000002, "text": " Well regular expressions are patterns that we can build to match against specific series", "tokens": [51018, 1042, 3890, 15277, 366, 8294, 300, 321, 393, 1322, 281, 2995, 1970, 2685, 2638, 51334], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 4, "seek": 0, "start": 19.400000000000002, "end": 25.02, "text": " of characters, you know, words, in a programming sense we can say strings.", "tokens": [51334, 295, 4342, 11, 291, 458, 11, 2283, 11, 294, 257, 9410, 2020, 321, 393, 584, 13985, 13, 51615], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 5, "seek": 0, "start": 25.02, "end": 29.8, "text": " We can use these patterns to pull out specific pieces of information from a larger set of", "tokens": [51615, 492, 393, 764, 613, 8294, 281, 2235, 484, 2685, 3755, 295, 1589, 490, 257, 4833, 992, 295, 51854], "temperature": 0.0, "avg_logprob": -0.16659073977126287, "compression_ratio": 1.6947791164658634, "no_speech_prob": 0.168340265750885}, {"id": 6, "seek": 2980, "start": 29.8, "end": 31.72, "text": " information that we're interested in.", "tokens": [50364, 1589, 300, 321, 434, 3102, 294, 13, 50460], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 7, "seek": 2980, "start": 31.72, "end": 37.4, "text": " So if we're looking at something like a very large log file, we can use a regular expression", "tokens": [50460, 407, 498, 321, 434, 1237, 412, 746, 411, 257, 588, 2416, 3565, 3991, 11, 321, 393, 764, 257, 3890, 6114, 50744], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 8, "seek": 2980, "start": 37.4, "end": 41.56, "text": " to just isolate those aspects of the log file that we're interested in.", "tokens": [50744, 281, 445, 25660, 729, 7270, 295, 264, 3565, 3991, 300, 321, 434, 3102, 294, 13, 50952], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 9, "seek": 2980, "start": 41.56, "end": 47.400000000000006, "text": " This is really great when you've got a lot of data to pick through on the command line.", "tokens": [50952, 639, 307, 534, 869, 562, 291, 600, 658, 257, 688, 295, 1412, 281, 1888, 807, 322, 264, 5622, 1622, 13, 51244], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 10, "seek": 2980, "start": 47.400000000000006, "end": 51.92, "text": " We can also use regular expressions for finding data and then acting upon it and we'll take", "tokens": [51244, 492, 393, 611, 764, 3890, 15277, 337, 5006, 1412, 293, 550, 6577, 3564, 309, 293, 321, 603, 747, 51470], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 11, "seek": 2980, "start": 51.92, "end": 56.480000000000004, "text": " a brief look at sed which is the stream editor later on in this video lecture.", "tokens": [51470, 257, 5353, 574, 412, 9643, 597, 307, 264, 4309, 9839, 1780, 322, 294, 341, 960, 7991, 13, 51698], "temperature": 0.0, "avg_logprob": -0.151496629457216, "compression_ratio": 1.7595419847328244, "no_speech_prob": 0.0023220358416438103}, {"id": 12, "seek": 5648, "start": 56.48, "end": 62.04, "text": " So for now we're going to look at mostly a tool called grep which allows us to show", "tokens": [50364, 407, 337, 586, 321, 434, 516, 281, 574, 412, 5240, 257, 2290, 1219, 6066, 79, 597, 4045, 505, 281, 855, 50642], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 13, "seek": 5648, "start": 62.04, "end": 69.08, "text": " or not show specific aspects of a file that conform to one of our regular expression patterns.", "tokens": [50642, 420, 406, 855, 2685, 7270, 295, 257, 3991, 300, 18975, 281, 472, 295, 527, 3890, 6114, 8294, 13, 50994], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 14, "seek": 5648, "start": 69.08, "end": 76.6, "text": " So the command line grep utility is used to apply patterns to either standard input or", "tokens": [50994, 407, 264, 5622, 1622, 6066, 79, 14877, 307, 1143, 281, 3079, 8294, 281, 2139, 3832, 4846, 420, 51370], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 15, "seek": 5648, "start": 76.6, "end": 78.8, "text": " to a specific file.", "tokens": [51370, 281, 257, 2685, 3991, 13, 51480], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 16, "seek": 5648, "start": 78.8, "end": 83.0, "text": " And so we might have a very large file and what we're interested in doing is just taking", "tokens": [51480, 400, 370, 321, 1062, 362, 257, 588, 2416, 3991, 293, 437, 321, 434, 3102, 294, 884, 307, 445, 1940, 51690], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 17, "seek": 5648, "start": 83.0, "end": 85.36, "text": " a look at certain aspects of it.", "tokens": [51690, 257, 574, 412, 1629, 7270, 295, 309, 13, 51808], "temperature": 0.0, "avg_logprob": -0.11918864863933903, "compression_ratio": 1.6887966804979253, "no_speech_prob": 0.0003053362888749689}, {"id": 18, "seek": 8536, "start": 85.36, "end": 90.48, "text": " There's actually two forms of grep and grep stands for get regular expression and there's", "tokens": [50364, 821, 311, 767, 732, 6422, 295, 6066, 79, 293, 6066, 79, 7382, 337, 483, 3890, 6114, 293, 456, 311, 50620], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 19, "seek": 8536, "start": 90.48, "end": 91.48, "text": " also grep and egrep.", "tokens": [50620, 611, 6066, 79, 293, 24263, 19919, 13, 50670], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 20, "seek": 8536, "start": 91.48, "end": 97.12, "text": " And what you're going to find is that because Unix is so old, it's had a lot of time to evolve", "tokens": [50670, 400, 437, 291, 434, 516, 281, 915, 307, 300, 570, 1156, 970, 307, 370, 1331, 11, 309, 311, 632, 257, 688, 295, 565, 281, 16693, 50952], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 21, "seek": 8536, "start": 97.12, "end": 102.12, "text": " and sometimes people have gone on and extended aspects of Unix and in their sense of making", "tokens": [50952, 293, 2171, 561, 362, 2780, 322, 293, 10913, 7270, 295, 1156, 970, 293, 294, 641, 2020, 295, 1455, 51202], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 22, "seek": 8536, "start": 102.12, "end": 103.12, "text": " it better.", "tokens": [51202, 309, 1101, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 23, "seek": 8536, "start": 103.12, "end": 107.92, "text": " So the idea of grep and egrep is that grep is the original grep and egrep is extended", "tokens": [51252, 407, 264, 1558, 295, 6066, 79, 293, 24263, 19919, 307, 300, 6066, 79, 307, 264, 3380, 6066, 79, 293, 24263, 19919, 307, 10913, 51492], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 24, "seek": 8536, "start": 107.92, "end": 108.92, "text": " grep.", "tokens": [51492, 6066, 79, 13, 51542], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 25, "seek": 8536, "start": 108.92, "end": 112.12, "text": " And extended grep actually supports extended regular expressions.", "tokens": [51542, 400, 10913, 6066, 79, 767, 9346, 10913, 3890, 15277, 13, 51702], "temperature": 0.0, "avg_logprob": -0.13810845026894222, "compression_ratio": 1.9579831932773109, "no_speech_prob": 0.16005799174308777}, {"id": 26, "seek": 11212, "start": 112.12, "end": 116.36, "text": " So anytime a language has been around for a long time, people try to start to add to", "tokens": [50364, 407, 13038, 257, 2856, 575, 668, 926, 337, 257, 938, 565, 11, 561, 853, 281, 722, 281, 909, 281, 50576], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 27, "seek": 11212, "start": 116.36, "end": 120.2, "text": " it when they start to notice failings in that language.", "tokens": [50576, 309, 562, 436, 722, 281, 3449, 3061, 1109, 294, 300, 2856, 13, 50768], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 28, "seek": 11212, "start": 120.2, "end": 126.24000000000001, "text": " And what I'll mention is that we'll look at both extended and regular, regular expressions.", "tokens": [50768, 400, 437, 286, 603, 2152, 307, 300, 321, 603, 574, 412, 1293, 10913, 293, 3890, 11, 3890, 15277, 13, 51070], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 29, "seek": 11212, "start": 126.24000000000001, "end": 128.88, "text": " There's also pearl regular expressions, which I'm most familiar with.", "tokens": [51070, 821, 311, 611, 20287, 3890, 15277, 11, 597, 286, 478, 881, 4963, 365, 13, 51202], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 30, "seek": 11212, "start": 128.88, "end": 132.38, "text": " So I also throw that out there when I'm talking about regular expressions.", "tokens": [51202, 407, 286, 611, 3507, 300, 484, 456, 562, 286, 478, 1417, 466, 3890, 15277, 13, 51377], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 31, "seek": 11212, "start": 132.38, "end": 136.9, "text": " Anytime I make an error, I will probably be thinking in terms of pearl regular expressions.", "tokens": [51377, 39401, 286, 652, 364, 6713, 11, 286, 486, 1391, 312, 1953, 294, 2115, 295, 20287, 3890, 15277, 13, 51603], "temperature": 0.0, "avg_logprob": -0.12653648960697758, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.020954392850399017}, {"id": 32, "seek": 13690, "start": 136.9, "end": 142.56, "text": " So enough about what regular expressions are, what utilities we're going to use, what", "tokens": [50364, 407, 1547, 466, 437, 3890, 15277, 366, 11, 437, 30482, 321, 434, 516, 281, 764, 11, 437, 50647], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 33, "seek": 13690, "start": 142.56, "end": 148.14000000000001, "text": " does a regular expression do and look like?", "tokens": [50647, 775, 257, 3890, 6114, 360, 293, 574, 411, 30, 50926], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 34, "seek": 13690, "start": 148.14000000000001, "end": 155.04000000000002, "text": " Let's think about a typical pattern that we might want to apply in this case.", "tokens": [50926, 961, 311, 519, 466, 257, 7476, 5102, 300, 321, 1062, 528, 281, 3079, 294, 341, 1389, 13, 51271], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 35, "seek": 13690, "start": 155.04000000000002, "end": 160.14000000000001, "text": " Let's say that I have asked someone to look through the phone book or a directory of employees", "tokens": [51271, 961, 311, 584, 300, 286, 362, 2351, 1580, 281, 574, 807, 264, 2593, 1446, 420, 257, 21120, 295, 6619, 51526], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 36, "seek": 13690, "start": 160.14000000000001, "end": 164.86, "text": " and show me everybody whose last name starts with the letter s.", "tokens": [51526, 293, 855, 385, 2201, 6104, 1036, 1315, 3719, 365, 264, 5063, 262, 13, 51762], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 37, "seek": 13690, "start": 164.86, "end": 166.02, "text": " So I've given you a pattern.", "tokens": [51762, 407, 286, 600, 2212, 291, 257, 5102, 13, 51820], "temperature": 0.0, "avg_logprob": -0.11633415812069607, "compression_ratio": 1.6390041493775933, "no_speech_prob": 0.002980555407702923}, {"id": 38, "seek": 16602, "start": 166.02, "end": 168.3, "text": " I've given you a few bits of information.", "tokens": [50364, 286, 600, 2212, 291, 257, 1326, 9239, 295, 1589, 13, 50478], "temperature": 0.0, "avg_logprob": -0.11961174011230469, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008040209650062025}, {"id": 39, "seek": 16602, "start": 168.3, "end": 177.82000000000002, "text": " I've said, okay, I want you to look at last name, oops, and I want that last name to start", "tokens": [50478, 286, 600, 848, 11, 1392, 11, 286, 528, 291, 281, 574, 412, 1036, 1315, 11, 34166, 11, 293, 286, 528, 300, 1036, 1315, 281, 722, 50954], "temperature": 0.0, "avg_logprob": -0.11961174011230469, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008040209650062025}, {"id": 40, "seek": 16602, "start": 177.82000000000002, "end": 179.58, "text": " with the letter s.", "tokens": [50954, 365, 264, 5063, 262, 13, 51042], "temperature": 0.0, "avg_logprob": -0.11961174011230469, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008040209650062025}, {"id": 41, "seek": 16602, "start": 179.58, "end": 190.42000000000002, "text": " So if I have somebody whose name is, right, like John Smith, this would match my pattern", "tokens": [51042, 407, 498, 286, 362, 2618, 6104, 1315, 307, 11, 558, 11, 411, 2619, 8538, 11, 341, 576, 2995, 452, 5102, 51584], "temperature": 0.0, "avg_logprob": -0.11961174011230469, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008040209650062025}, {"id": 42, "seek": 16602, "start": 190.42000000000002, "end": 195.70000000000002, "text": " because I look at the last name and the first letter of that last name is an s.", "tokens": [51584, 570, 286, 574, 412, 264, 1036, 1315, 293, 264, 700, 5063, 295, 300, 1036, 1315, 307, 364, 262, 13, 51848], "temperature": 0.0, "avg_logprob": -0.11961174011230469, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0008040209650062025}, {"id": 43, "seek": 19570, "start": 195.7, "end": 203.26, "text": " Their name was Steve, John, I guess we'll stick with people with two first names.", "tokens": [50364, 6710, 1315, 390, 7466, 11, 2619, 11, 286, 2041, 321, 603, 2897, 365, 561, 365, 732, 700, 5288, 13, 50742], "temperature": 0.0, "avg_logprob": -0.16748411515179804, "compression_ratio": 1.675, "no_speech_prob": 0.006487010978162289}, {"id": 44, "seek": 19570, "start": 203.26, "end": 208.85999999999999, "text": " Well, the first name starts with the letter s, but the last name doesn't, so it doesn't", "tokens": [50742, 1042, 11, 264, 700, 1315, 3719, 365, 264, 5063, 262, 11, 457, 264, 1036, 1315, 1177, 380, 11, 370, 309, 1177, 380, 51022], "temperature": 0.0, "avg_logprob": -0.16748411515179804, "compression_ratio": 1.675, "no_speech_prob": 0.006487010978162289}, {"id": 45, "seek": 19570, "start": 208.85999999999999, "end": 210.22, "text": " match my pattern.", "tokens": [51022, 2995, 452, 5102, 13, 51090], "temperature": 0.0, "avg_logprob": -0.16748411515179804, "compression_ratio": 1.675, "no_speech_prob": 0.006487010978162289}, {"id": 46, "seek": 19570, "start": 210.22, "end": 217.66, "text": " So we define patterns and we do so programmatically and we use those patterns to filter or match", "tokens": [51090, 407, 321, 6964, 8294, 293, 321, 360, 370, 37648, 5030, 293, 321, 764, 729, 8294, 281, 6608, 420, 2995, 51462], "temperature": 0.0, "avg_logprob": -0.16748411515179804, "compression_ratio": 1.675, "no_speech_prob": 0.006487010978162289}, {"id": 47, "seek": 19570, "start": 217.66, "end": 224.33999999999997, "text": " against information that we're most interested in.", "tokens": [51462, 1970, 1589, 300, 321, 434, 881, 3102, 294, 13, 51796], "temperature": 0.0, "avg_logprob": -0.16748411515179804, "compression_ratio": 1.675, "no_speech_prob": 0.006487010978162289}, {"id": 48, "seek": 22434, "start": 225.06, "end": 230.18, "text": " Let's look at some really basic ways that we can apply patterns on the command line.", "tokens": [50400, 961, 311, 574, 412, 512, 534, 3875, 2098, 300, 321, 393, 3079, 8294, 322, 264, 5622, 1622, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 49, "seek": 22434, "start": 230.18, "end": 235.82, "text": " I'm currently in my home directory and there's a number of folders and at least two files", "tokens": [50656, 286, 478, 4362, 294, 452, 1280, 21120, 293, 456, 311, 257, 1230, 295, 31082, 293, 412, 1935, 732, 7098, 50938], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 50, "seek": 22434, "start": 235.82, "end": 237.42000000000002, "text": " in my home directory.", "tokens": [50938, 294, 452, 1280, 21120, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 51, "seek": 22434, "start": 237.42000000000002, "end": 243.98000000000002, "text": " So there's actually some basic pattern matching capabilities built into the Bash shell.", "tokens": [51018, 407, 456, 311, 767, 512, 3875, 5102, 14324, 10862, 3094, 666, 264, 43068, 8720, 13, 51346], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 52, "seek": 22434, "start": 243.98000000000002, "end": 247.02, "text": " So for example, if I want to see all of the folders that start with the capital letter", "tokens": [51346, 407, 337, 1365, 11, 498, 286, 528, 281, 536, 439, 295, 264, 31082, 300, 722, 365, 264, 4238, 5063, 51498], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 53, "seek": 22434, "start": 247.02, "end": 252.82, "text": " d, notice that if I just put the letter d, I get an error because it's going to say,", "tokens": [51498, 274, 11, 3449, 300, 498, 286, 445, 829, 264, 5063, 274, 11, 286, 483, 364, 6713, 570, 309, 311, 516, 281, 584, 11, 51788], "temperature": 0.0, "avg_logprob": -0.1639593681402966, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.27481305599212646}, {"id": 54, "seek": 25282, "start": 252.82, "end": 260.9, "text": " I can't find a folder or file named d, but if I add a star after the word d, I get a", "tokens": [50364, 286, 393, 380, 915, 257, 10820, 420, 3991, 4926, 274, 11, 457, 498, 286, 909, 257, 3543, 934, 264, 1349, 274, 11, 286, 483, 257, 50768], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 55, "seek": 25282, "start": 260.9, "end": 262.58, "text": " lot of information.", "tokens": [50768, 688, 295, 1589, 13, 50852], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 56, "seek": 25282, "start": 262.58, "end": 267.46, "text": " In fact, I wanted to look just for folders, I would do dash d, which would give me information", "tokens": [50852, 682, 1186, 11, 286, 1415, 281, 574, 445, 337, 31082, 11, 286, 576, 360, 8240, 274, 11, 597, 576, 976, 385, 1589, 51096], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 57, "seek": 25282, "start": 267.46, "end": 272.1, "text": " about the directories as opposed to items inside the directories.", "tokens": [51096, 466, 264, 5391, 530, 382, 8851, 281, 4754, 1854, 264, 5391, 530, 13, 51328], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 58, "seek": 25282, "start": 272.1, "end": 278.1, "text": " And this star character on the shell is called the file glob operator and basically you can", "tokens": [51328, 400, 341, 3543, 2517, 322, 264, 8720, 307, 1219, 264, 3991, 16125, 12973, 293, 1936, 291, 393, 51628], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 59, "seek": 25282, "start": 278.1, "end": 280.46, "text": " think of it like a wild card match.", "tokens": [51628, 519, 295, 309, 411, 257, 4868, 2920, 2995, 13, 51746], "temperature": 0.0, "avg_logprob": -0.16281668956463152, "compression_ratio": 1.6794871794871795, "no_speech_prob": 0.005553902592509985}, {"id": 60, "seek": 28046, "start": 280.46, "end": 285.14, "text": " And the pattern that I've essentially given the ls command here is that the ls command", "tokens": [50364, 400, 264, 5102, 300, 286, 600, 4476, 2212, 264, 287, 82, 5622, 510, 307, 300, 264, 287, 82, 5622, 50598], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 61, "seek": 28046, "start": 285.14, "end": 291.14, "text": " should display all folders that start with the capital d and then folders that have any", "tokens": [50598, 820, 4674, 439, 31082, 300, 722, 365, 264, 4238, 274, 293, 550, 31082, 300, 362, 604, 50898], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 62, "seek": 28046, "start": 291.14, "end": 293.58, "text": " number of characters following it.", "tokens": [50898, 1230, 295, 4342, 3480, 309, 13, 51020], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 63, "seek": 28046, "start": 293.58, "end": 298.97999999999996, "text": " So when I mean wild card, I mean any number from zero to an infinite amount and I mean", "tokens": [51020, 407, 562, 286, 914, 4868, 2920, 11, 286, 914, 604, 1230, 490, 4018, 281, 364, 13785, 2372, 293, 286, 914, 51290], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 64, "seek": 28046, "start": 298.97999999999996, "end": 300.34, "text": " any type of character.", "tokens": [51290, 604, 2010, 295, 2517, 13, 51358], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 65, "seek": 28046, "start": 300.34, "end": 305.58, "text": " So basically this would match any folder that starts with d and goes ahead and has a bunch", "tokens": [51358, 407, 1936, 341, 576, 2995, 604, 10820, 300, 3719, 365, 274, 293, 1709, 2286, 293, 575, 257, 3840, 51620], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 66, "seek": 28046, "start": 305.58, "end": 307.34, "text": " of stuff in front of it.", "tokens": [51620, 295, 1507, 294, 1868, 295, 309, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13441852320020445, "compression_ratio": 1.851063829787234, "no_speech_prob": 0.00609635841101408}, {"id": 67, "seek": 30734, "start": 307.34, "end": 311.5, "text": " You can also use a wild card at the front of a string, so I'm going to see any folder", "tokens": [50364, 509, 393, 611, 764, 257, 4868, 2920, 412, 264, 1868, 295, 257, 6798, 11, 370, 286, 478, 516, 281, 536, 604, 10820, 50572], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 68, "seek": 30734, "start": 311.5, "end": 316.9, "text": " that starts with the letter s and notice it shows me documents, downloads, pictures, templates", "tokens": [50572, 300, 3719, 365, 264, 5063, 262, 293, 3449, 309, 3110, 385, 8512, 11, 36553, 11, 5242, 11, 21165, 50842], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 69, "seek": 30734, "start": 316.9, "end": 319.65999999999997, "text": " and videos because all of those end with a lower case s.", "tokens": [50842, 293, 2145, 570, 439, 295, 729, 917, 365, 257, 3126, 1389, 262, 13, 50980], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 70, "seek": 30734, "start": 319.65999999999997, "end": 324.65999999999997, "text": " And remember that the command line is case sensitive, so if I do capital s, I get nothing.", "tokens": [50980, 400, 1604, 300, 264, 5622, 1622, 307, 1389, 9477, 11, 370, 498, 286, 360, 4238, 262, 11, 286, 483, 1825, 13, 51230], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 71, "seek": 30734, "start": 324.65999999999997, "end": 328.97999999999996, "text": " You can also do something along the lines of putting two stars on the command line.", "tokens": [51230, 509, 393, 611, 360, 746, 2051, 264, 3876, 295, 3372, 732, 6105, 322, 264, 5622, 1622, 13, 51446], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 72, "seek": 30734, "start": 328.97999999999996, "end": 333.7, "text": " So I could say something like show me all directories that have any number of characters,", "tokens": [51446, 407, 286, 727, 584, 746, 411, 855, 385, 439, 5391, 530, 300, 362, 604, 1230, 295, 4342, 11, 51682], "temperature": 0.0, "avg_logprob": -0.13444044531845464, "compression_ratio": 1.7430555555555556, "no_speech_prob": 0.010650972835719585}, {"id": 73, "seek": 33370, "start": 333.7, "end": 338.38, "text": " the letter l in them and then any number of characters on the other side.", "tokens": [50364, 264, 5063, 287, 294, 552, 293, 550, 604, 1230, 295, 4342, 322, 264, 661, 1252, 13, 50598], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 74, "seek": 33370, "start": 338.38, "end": 342.82, "text": " And in this case it shows me downloads, examples.desktop, public and templates because all of these", "tokens": [50598, 400, 294, 341, 1389, 309, 3110, 385, 36553, 11, 5110, 13, 14792, 13031, 11, 1908, 293, 21165, 570, 439, 295, 613, 50820], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 75, "seek": 33370, "start": 342.82, "end": 347.53999999999996, "text": " have a letter l in the middle that we can see in each one of these and on either side", "tokens": [50820, 362, 257, 5063, 287, 294, 264, 2808, 300, 321, 393, 536, 294, 1184, 472, 295, 613, 293, 322, 2139, 1252, 51056], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 76, "seek": 33370, "start": 347.53999999999996, "end": 354.02, "text": " there is any given number of characters that match this wild card pattern.", "tokens": [51056, 456, 307, 604, 2212, 1230, 295, 4342, 300, 2995, 341, 4868, 2920, 5102, 13, 51380], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 77, "seek": 33370, "start": 354.02, "end": 358.02, "text": " So this is built into the shell and this can be really useful if you wanted to show say", "tokens": [51380, 407, 341, 307, 3094, 666, 264, 8720, 293, 341, 393, 312, 534, 4420, 498, 291, 1415, 281, 855, 584, 51580], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 78, "seek": 33370, "start": 358.02, "end": 361.46, "text": " all files in a directory and so if we look back up into the shell it will notice that", "tokens": [51580, 439, 7098, 294, 257, 21120, 293, 370, 498, 321, 574, 646, 493, 666, 264, 8720, 309, 486, 3449, 300, 51752], "temperature": 0.0, "avg_logprob": -0.15930634785473832, "compression_ratio": 1.8339350180505416, "no_speech_prob": 0.2749345302581787}, {"id": 79, "seek": 36146, "start": 361.46, "end": 365.14, "text": " in my desktop directory I have a couple .txt files.", "tokens": [50364, 294, 452, 14502, 21120, 286, 362, 257, 1916, 2411, 83, 734, 7098, 13, 50548], "temperature": 0.0, "avg_logprob": -0.15591519602229087, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.4107906222343445}, {"id": 80, "seek": 36146, "start": 365.14, "end": 373.82, "text": " Let's clear this and let's say I wanted to do ls-l desktop star .txt and what this would", "tokens": [50548, 961, 311, 1850, 341, 293, 718, 311, 584, 286, 1415, 281, 360, 287, 82, 12, 75, 14502, 3543, 2411, 83, 734, 293, 437, 341, 576, 50982], "temperature": 0.0, "avg_logprob": -0.15591519602229087, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.4107906222343445}, {"id": 81, "seek": 36146, "start": 373.82, "end": 379.21999999999997, "text": " do is say okay go into my desktop directory, show me files that have any name and then", "tokens": [50982, 360, 307, 584, 1392, 352, 666, 452, 14502, 21120, 11, 855, 385, 7098, 300, 362, 604, 1315, 293, 550, 51252], "temperature": 0.0, "avg_logprob": -0.15591519602229087, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.4107906222343445}, {"id": 82, "seek": 36146, "start": 379.21999999999997, "end": 387.97999999999996, "text": " have a .txt extension and if I hit enter I see those two files sitting in my home directory.", "tokens": [51252, 362, 257, 2411, 83, 734, 10320, 293, 498, 286, 2045, 3242, 286, 536, 729, 732, 7098, 3798, 294, 452, 1280, 21120, 13, 51690], "temperature": 0.0, "avg_logprob": -0.15591519602229087, "compression_ratio": 1.702127659574468, "no_speech_prob": 0.4107906222343445}, {"id": 83, "seek": 38798, "start": 387.98, "end": 393.74, "text": " So pretty cool that those exist and that this capability exists.", "tokens": [50364, 407, 1238, 1627, 300, 729, 2514, 293, 300, 341, 13759, 8198, 13, 50652], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 84, "seek": 38798, "start": 393.74, "end": 399.14000000000004, "text": " But what we want to do is we want to actually look at how we can utilize grep to get more", "tokens": [50652, 583, 437, 321, 528, 281, 360, 307, 321, 528, 281, 767, 574, 412, 577, 321, 393, 16117, 6066, 79, 281, 483, 544, 50922], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 85, "seek": 38798, "start": 399.14000000000004, "end": 406.26, "text": " detail information about things like file directory listings and actual files themselves.", "tokens": [50922, 2607, 1589, 466, 721, 411, 3991, 21120, 45615, 293, 3539, 7098, 2969, 13, 51278], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 86, "seek": 38798, "start": 406.26, "end": 409.86, "text": " So ls is great when you're looking for just information about some listings and the wild", "tokens": [51278, 407, 287, 82, 307, 869, 562, 291, 434, 1237, 337, 445, 1589, 466, 512, 45615, 293, 264, 4868, 51458], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 87, "seek": 38798, "start": 409.86, "end": 414.18, "text": " card is helpful for getting some basic pattern matching going.", "tokens": [51458, 2920, 307, 4961, 337, 1242, 512, 3875, 5102, 14324, 516, 13, 51674], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 88, "seek": 38798, "start": 414.18, "end": 416.86, "text": " But to learn regular expressions what we're going to do is we're going to look at a very", "tokens": [51674, 583, 281, 1466, 3890, 15277, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 574, 412, 257, 588, 51808], "temperature": 0.0, "avg_logprob": -0.08894325603138317, "compression_ratio": 1.844106463878327, "no_speech_prob": 0.07472054660320282}, {"id": 89, "seek": 41686, "start": 416.86, "end": 425.02000000000004, "text": " specific file on the Unix system and that file is the American English dictionary used", "tokens": [50364, 2685, 3991, 322, 264, 1156, 970, 1185, 293, 300, 3991, 307, 264, 2665, 3669, 25890, 1143, 50772], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 90, "seek": 41686, "start": 425.02000000000004, "end": 427.62, "text": " for spell checking and other things by the system.", "tokens": [50772, 337, 9827, 8568, 293, 661, 721, 538, 264, 1185, 13, 50902], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 91, "seek": 41686, "start": 427.62, "end": 431.5, "text": " So I'm going to pass the American English dictionary file which is found in slash user", "tokens": [50902, 407, 286, 478, 516, 281, 1320, 264, 2665, 3669, 25890, 3991, 597, 307, 1352, 294, 17330, 4195, 51096], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 92, "seek": 41686, "start": 431.5, "end": 435.94, "text": " slash share slash dict slash American English into the less command so we can see what's", "tokens": [51096, 17330, 2073, 17330, 12569, 17330, 2665, 3669, 666, 264, 1570, 5622, 370, 321, 393, 536, 437, 311, 51318], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 93, "seek": 41686, "start": 435.94, "end": 442.02000000000004, "text": " in there and what you'll notice is this is a very large file that goes on and on and", "tokens": [51318, 294, 456, 293, 437, 291, 603, 3449, 307, 341, 307, 257, 588, 2416, 3991, 300, 1709, 322, 293, 322, 293, 51622], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 94, "seek": 41686, "start": 442.02000000000004, "end": 446.06, "text": " on and on and keeps going and there's lots of words in it.", "tokens": [51622, 322, 293, 322, 293, 5965, 516, 293, 456, 311, 3195, 295, 2283, 294, 309, 13, 51824], "temperature": 0.0, "avg_logprob": -0.129127939906689, "compression_ratio": 1.8884297520661157, "no_speech_prob": 0.3069814145565033}, {"id": 95, "seek": 44606, "start": 446.06, "end": 452.82, "text": " So this is really useful for our pattern matching exercises so I'm going to hit Q and we're", "tokens": [50364, 407, 341, 307, 534, 4420, 337, 527, 5102, 14324, 11900, 370, 286, 478, 516, 281, 2045, 1249, 293, 321, 434, 50702], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 96, "seek": 44606, "start": 452.82, "end": 458.82, "text": " going to start to talk about some really basic regular expressions.", "tokens": [50702, 516, 281, 722, 281, 751, 466, 512, 534, 3875, 3890, 15277, 13, 51002], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 97, "seek": 44606, "start": 458.82, "end": 462.74, "text": " And the way we're going to do this is we're going to actually use the command grep.", "tokens": [51002, 400, 264, 636, 321, 434, 516, 281, 360, 341, 307, 321, 434, 516, 281, 767, 764, 264, 5622, 6066, 79, 13, 51198], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 98, "seek": 44606, "start": 462.74, "end": 465.64, "text": " Remember that I had mentioned there are actually grep and egrep.", "tokens": [51198, 5459, 300, 286, 632, 2835, 456, 366, 767, 6066, 79, 293, 24263, 19919, 13, 51343], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 99, "seek": 44606, "start": 465.64, "end": 467.9, "text": " So what we'll do is we'll use grep until it breaks.", "tokens": [51343, 407, 437, 321, 603, 360, 307, 321, 603, 764, 6066, 79, 1826, 309, 9857, 13, 51456], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 100, "seek": 44606, "start": 467.9, "end": 471.14, "text": " In other words we'll use grep until we run into a feature that is an extended regular", "tokens": [51456, 682, 661, 2283, 321, 603, 764, 6066, 79, 1826, 321, 1190, 666, 257, 4111, 300, 307, 364, 10913, 3890, 51618], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 101, "seek": 44606, "start": 471.14, "end": 475.9, "text": " expression and then we will jump over to using the egrep command.", "tokens": [51618, 6114, 293, 550, 321, 486, 3012, 670, 281, 1228, 264, 24263, 19919, 5622, 13, 51856], "temperature": 0.0, "avg_logprob": -0.13786857546740816, "compression_ratio": 1.8892988929889298, "no_speech_prob": 0.009706278331577778}, {"id": 102, "seek": 47590, "start": 475.9, "end": 480.09999999999997, "text": " Actually it's a better habit to just get into using egrep all the time and that way", "tokens": [50364, 5135, 309, 311, 257, 1101, 7164, 281, 445, 483, 666, 1228, 24263, 19919, 439, 264, 565, 293, 300, 636, 50574], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 103, "seek": 47590, "start": 480.09999999999997, "end": 484.94, "text": " you don't have to worry about what is an extended and what isn't an extended regular expression.", "tokens": [50574, 291, 500, 380, 362, 281, 3292, 466, 437, 307, 364, 10913, 293, 437, 1943, 380, 364, 10913, 3890, 6114, 13, 50816], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 104, "seek": 47590, "start": 484.94, "end": 488.65999999999997, "text": " But I kind of want to mention, show you that at some point some of these features will", "tokens": [50816, 583, 286, 733, 295, 528, 281, 2152, 11, 855, 291, 300, 412, 512, 935, 512, 295, 613, 4122, 486, 51002], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 105, "seek": 47590, "start": 488.65999999999997, "end": 489.65999999999997, "text": " not work.", "tokens": [51002, 406, 589, 13, 51052], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 106, "seek": 47590, "start": 489.65999999999997, "end": 499.38, "text": " So I am going to actually set this up so that we can grep this file and the way greps work", "tokens": [51052, 407, 286, 669, 516, 281, 767, 992, 341, 493, 370, 300, 321, 393, 6066, 79, 341, 3991, 293, 264, 636, 6066, 1878, 589, 51538], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 107, "seek": 47590, "start": 499.38, "end": 505.65999999999997, "text": " is that we give the grep command what we'll do is right in here we'll put a pattern and", "tokens": [51538, 307, 300, 321, 976, 264, 6066, 79, 5622, 437, 321, 603, 360, 307, 558, 294, 510, 321, 603, 829, 257, 5102, 293, 51852], "temperature": 0.0, "avg_logprob": -0.16705613136291503, "compression_ratio": 1.7338403041825095, "no_speech_prob": 0.004754301160573959}, {"id": 108, "seek": 50566, "start": 505.66, "end": 509.70000000000005, "text": " then we will actually go ahead and match it against a file.", "tokens": [50364, 550, 321, 486, 767, 352, 2286, 293, 2995, 309, 1970, 257, 3991, 13, 50566], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 109, "seek": 50566, "start": 509.70000000000005, "end": 513.7, "text": " And so the easiest pattern that I can put in here is I want to look for say words that", "tokens": [50566, 400, 370, 264, 12889, 5102, 300, 286, 393, 829, 294, 510, 307, 286, 528, 281, 574, 337, 584, 2283, 300, 50766], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 110, "seek": 50566, "start": 513.7, "end": 514.7, "text": " have cat in it.", "tokens": [50766, 362, 3857, 294, 309, 13, 50816], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 111, "seek": 50566, "start": 514.7, "end": 518.46, "text": " So let's use cat and see what we get.", "tokens": [50816, 407, 718, 311, 764, 3857, 293, 536, 437, 321, 483, 13, 51004], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 112, "seek": 50566, "start": 518.46, "end": 521.9, "text": " What you'll notice is a lot of words scroll by.", "tokens": [51004, 708, 291, 603, 3449, 307, 257, 688, 295, 2283, 11369, 538, 13, 51176], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 113, "seek": 50566, "start": 521.9, "end": 526.4200000000001, "text": " I gave the pattern cat and maybe you were thinking well I'll only see the word cat but", "tokens": [51176, 286, 2729, 264, 5102, 3857, 293, 1310, 291, 645, 1953, 731, 286, 603, 787, 536, 264, 1349, 3857, 457, 51402], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 114, "seek": 50566, "start": 526.4200000000001, "end": 532.7, "text": " notice that it shows me every word in the file that has the letters cat somewhere in", "tokens": [51402, 3449, 300, 309, 3110, 385, 633, 1349, 294, 264, 3991, 300, 575, 264, 7825, 3857, 4079, 294, 51716], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 115, "seek": 50566, "start": 532.7, "end": 533.7, "text": " the word.", "tokens": [51716, 264, 1349, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14214409003823492, "compression_ratio": 1.7408906882591093, "no_speech_prob": 0.016396882012486458}, {"id": 116, "seek": 53370, "start": 533.74, "end": 538.1400000000001, "text": " So that's what the color coded command line is we can actually see the matches.", "tokens": [50366, 407, 300, 311, 437, 264, 2017, 34874, 5622, 1622, 307, 321, 393, 767, 536, 264, 10676, 13, 50586], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 117, "seek": 53370, "start": 538.1400000000001, "end": 543.34, "text": " So that's kind of a nice feature for learning how to use regular expressions.", "tokens": [50586, 407, 300, 311, 733, 295, 257, 1481, 4111, 337, 2539, 577, 281, 764, 3890, 15277, 13, 50846], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 118, "seek": 53370, "start": 543.34, "end": 547.3000000000001, "text": " And you'll also notice that this is a really big file so at some point we'll look at we're", "tokens": [50846, 400, 291, 603, 611, 3449, 300, 341, 307, 257, 534, 955, 3991, 370, 412, 512, 935, 321, 603, 574, 412, 321, 434, 51044], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 119, "seek": 53370, "start": 547.3000000000001, "end": 550.0200000000001, "text": " always going to see letters at the end of the alphabet just because of the way we're", "tokens": [51044, 1009, 516, 281, 536, 7825, 412, 264, 917, 295, 264, 23339, 445, 570, 295, 264, 636, 321, 434, 51180], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 120, "seek": 53370, "start": 550.0200000000001, "end": 551.0200000000001, "text": " doing this.", "tokens": [51180, 884, 341, 13, 51230], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 121, "seek": 53370, "start": 551.0200000000001, "end": 555.0200000000001, "text": " So let's clear this and take a look at that pattern again.", "tokens": [51230, 407, 718, 311, 1850, 341, 293, 747, 257, 574, 412, 300, 5102, 797, 13, 51430], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 122, "seek": 53370, "start": 555.0200000000001, "end": 561.5, "text": " My pattern is called cat and for the rest of the exercise is the other things or is cat.", "tokens": [51430, 1222, 5102, 307, 1219, 3857, 293, 337, 264, 1472, 295, 264, 5380, 307, 264, 661, 721, 420, 307, 3857, 13, 51754], "temperature": 0.0, "avg_logprob": -0.17865321749732607, "compression_ratio": 1.7607142857142857, "no_speech_prob": 0.0055536855943500996}, {"id": 123, "seek": 56150, "start": 561.5, "end": 564.1, "text": " And I'm going to put these in double quotes because it'll make it a little bit easier to", "tokens": [50364, 400, 286, 478, 516, 281, 829, 613, 294, 3834, 19963, 570, 309, 603, 652, 309, 257, 707, 857, 3571, 281, 50494], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 124, "seek": 56150, "start": 564.1, "end": 570.66, "text": " see where our pattern is within these lectures and it'll also help us deal with any non-standard", "tokens": [50494, 536, 689, 527, 5102, 307, 1951, 613, 16564, 293, 309, 603, 611, 854, 505, 2028, 365, 604, 2107, 12, 1115, 515, 50822], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 125, "seek": 56150, "start": 570.66, "end": 571.66, "text": " characters.", "tokens": [50822, 4342, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 126, "seek": 56150, "start": 571.66, "end": 575.34, "text": " In other words any characters that we want to match in our pattern but have special meaning", "tokens": [50872, 682, 661, 2283, 604, 4342, 300, 321, 528, 281, 2995, 294, 527, 5102, 457, 362, 2121, 3620, 51056], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 127, "seek": 56150, "start": 575.34, "end": 579.98, "text": " to the shell we want to make sure that those are not processed by the shell in a way that", "tokens": [51056, 281, 264, 8720, 321, 528, 281, 652, 988, 300, 729, 366, 406, 18846, 538, 264, 8720, 294, 257, 636, 300, 51288], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 128, "seek": 56150, "start": 579.98, "end": 584.1, "text": " the shell wants to process them putting them in double quotes kind of fixes that.", "tokens": [51288, 264, 8720, 2738, 281, 1399, 552, 3372, 552, 294, 3834, 19963, 733, 295, 32539, 300, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 129, "seek": 56150, "start": 584.1, "end": 585.62, "text": " So what does this pattern say?", "tokens": [51494, 407, 437, 775, 341, 5102, 584, 30, 51570], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 130, "seek": 56150, "start": 585.62, "end": 590.66, "text": " It says well look for a C followed by an A followed by a T and what you found out again", "tokens": [51570, 467, 1619, 731, 574, 337, 257, 383, 6263, 538, 364, 316, 6263, 538, 257, 314, 293, 437, 291, 1352, 484, 797, 51822], "temperature": 0.0, "avg_logprob": -0.1296179328166263, "compression_ratio": 1.8530351437699681, "no_speech_prob": 0.01542035210877657}, {"id": 131, "seek": 59066, "start": 590.66, "end": 594.9399999999999, "text": " is when I run that command it doesn't really care if there's any characters before the", "tokens": [50364, 307, 562, 286, 1190, 300, 5622, 309, 1177, 380, 534, 1127, 498, 456, 311, 604, 4342, 949, 264, 50578], "temperature": 0.0, "avg_logprob": -0.09964816871730761, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.06950294226408005}, {"id": 132, "seek": 59066, "start": 594.9399999999999, "end": 602.6999999999999, "text": " cat or after the word cat it just looks for those three characters in order.", "tokens": [50578, 3857, 420, 934, 264, 1349, 3857, 309, 445, 1542, 337, 729, 1045, 4342, 294, 1668, 13, 50966], "temperature": 0.0, "avg_logprob": -0.09964816871730761, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.06950294226408005}, {"id": 133, "seek": 59066, "start": 602.6999999999999, "end": 605.54, "text": " So that's pretty interesting.", "tokens": [50966, 407, 300, 311, 1238, 1880, 13, 51108], "temperature": 0.0, "avg_logprob": -0.09964816871730761, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.06950294226408005}, {"id": 134, "seek": 59066, "start": 605.54, "end": 611.4599999999999, "text": " What if we wanted to look for not just cat but you were wondering like well I wonder", "tokens": [51108, 708, 498, 321, 1415, 281, 574, 337, 406, 445, 3857, 457, 291, 645, 6359, 411, 731, 286, 2441, 51404], "temperature": 0.0, "avg_logprob": -0.09964816871730761, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.06950294226408005}, {"id": 135, "seek": 59066, "start": 611.4599999999999, "end": 615.9399999999999, "text": " if there's a word that looks like cat that has another word in the middle.", "tokens": [51404, 498, 456, 311, 257, 1349, 300, 1542, 411, 3857, 300, 575, 1071, 1349, 294, 264, 2808, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09964816871730761, "compression_ratio": 1.721951219512195, "no_speech_prob": 0.06950294226408005}, {"id": 136, "seek": 61594, "start": 615.94, "end": 622.3800000000001, "text": " And maybe you were thinking of like cut or cot and you wanted to find all of these.", "tokens": [50364, 400, 1310, 291, 645, 1953, 295, 411, 1723, 420, 26529, 293, 291, 1415, 281, 915, 439, 295, 613, 13, 50686], "temperature": 0.0, "avg_logprob": -0.17021684443697016, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.19427257776260376}, {"id": 137, "seek": 61594, "start": 622.3800000000001, "end": 626.1800000000001, "text": " Well regular expressions come with a special character which is the dot character and the", "tokens": [50686, 1042, 3890, 15277, 808, 365, 257, 2121, 2517, 597, 307, 264, 5893, 2517, 293, 264, 50876], "temperature": 0.0, "avg_logprob": -0.17021684443697016, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.19427257776260376}, {"id": 138, "seek": 61594, "start": 626.1800000000001, "end": 630.4200000000001, "text": " dot character will match one of any character.", "tokens": [50876, 5893, 2517, 486, 2995, 472, 295, 604, 2517, 13, 51088], "temperature": 0.0, "avg_logprob": -0.17021684443697016, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.19427257776260376}, {"id": 139, "seek": 61594, "start": 630.4200000000001, "end": 639.9200000000001, "text": " So if there's actually a word called C2T then putting a dot there would match it.", "tokens": [51088, 407, 498, 456, 311, 767, 257, 1349, 1219, 383, 17, 51, 550, 3372, 257, 5893, 456, 576, 2995, 309, 13, 51563], "temperature": 0.0, "avg_logprob": -0.17021684443697016, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.19427257776260376}, {"id": 140, "seek": 61594, "start": 639.9200000000001, "end": 645.0600000000001, "text": " So what this will match is a C followed by any one character followed by the letter T.", "tokens": [51563, 407, 437, 341, 486, 2995, 307, 257, 383, 6263, 538, 604, 472, 2517, 6263, 538, 264, 5063, 314, 13, 51820], "temperature": 0.0, "avg_logprob": -0.17021684443697016, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.19427257776260376}, {"id": 141, "seek": 64506, "start": 646.06, "end": 650.7399999999999, "text": " So you can just see in this little bit of information that we got I didn't even think", "tokens": [50414, 407, 291, 393, 445, 536, 294, 341, 707, 857, 295, 1589, 300, 321, 658, 286, 994, 380, 754, 519, 50648], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 142, "seek": 64506, "start": 650.7399999999999, "end": 655.06, "text": " about words like yachts which have Cht or watchtower.", "tokens": [50648, 466, 2283, 411, 39629, 82, 597, 362, 383, 357, 420, 1159, 83, 968, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 143, "seek": 64506, "start": 655.06, "end": 663.42, "text": " So we matched Cht, Cat, Cut, Cot and that's pretty cool.", "tokens": [50864, 407, 321, 21447, 383, 357, 11, 383, 267, 11, 383, 325, 11, 383, 310, 293, 300, 311, 1238, 1627, 13, 51282], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 144, "seek": 64506, "start": 663.42, "end": 664.42, "text": " Why?", "tokens": [51282, 1545, 30, 51332], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 145, "seek": 64506, "start": 664.42, "end": 667.5799999999999, "text": " Because the pattern used a dot.", "tokens": [51332, 1436, 264, 5102, 1143, 257, 5893, 13, 51490], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 146, "seek": 64506, "start": 667.5799999999999, "end": 671.18, "text": " So let's talk about why this is and let's look at some other patterns that we might have", "tokens": [51490, 407, 718, 311, 751, 466, 983, 341, 307, 293, 718, 311, 574, 412, 512, 661, 8294, 300, 321, 1062, 362, 51670], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 147, "seek": 64506, "start": 671.18, "end": 673.02, "text": " available to us.", "tokens": [51670, 2435, 281, 505, 13, 51762], "temperature": 0.0, "avg_logprob": -0.18470034879796646, "compression_ratio": 1.5622119815668203, "no_speech_prob": 0.03409167379140854}, {"id": 148, "seek": 67302, "start": 673.02, "end": 676.22, "text": " So what we just looked at was this idea of basic elements.", "tokens": [50364, 407, 437, 321, 445, 2956, 412, 390, 341, 1558, 295, 3875, 4959, 13, 50524], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 149, "seek": 67302, "start": 676.22, "end": 680.5799999999999, "text": " You can literally type a string and that will match the string or the series of characters", "tokens": [50524, 509, 393, 3736, 2010, 257, 6798, 293, 300, 486, 2995, 264, 6798, 420, 264, 2638, 295, 4342, 50742], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 150, "seek": 67302, "start": 680.5799999999999, "end": 686.02, "text": " and in this case we can also then use dots in our series of characters to represent any", "tokens": [50742, 293, 294, 341, 1389, 321, 393, 611, 550, 764, 15026, 294, 527, 2638, 295, 4342, 281, 2906, 604, 51014], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 151, "seek": 67302, "start": 686.02, "end": 689.42, "text": " given one instance of any given character.", "tokens": [51014, 2212, 472, 5197, 295, 604, 2212, 2517, 13, 51184], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 152, "seek": 67302, "start": 689.42, "end": 692.5, "text": " So notice my first example matches bat, cat, mat.", "tokens": [51184, 407, 3449, 452, 700, 1365, 10676, 7362, 11, 3857, 11, 3803, 13, 51338], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 153, "seek": 67302, "start": 692.5, "end": 697.42, "text": " Finally if I just give the letters DOG it would match the literal string dog but found", "tokens": [51338, 6288, 498, 286, 445, 976, 264, 7825, 10699, 38, 309, 576, 2995, 264, 20411, 6798, 3000, 457, 1352, 51584], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 154, "seek": 67302, "start": 697.42, "end": 700.9399999999999, "text": " anywhere in a word.", "tokens": [51584, 4992, 294, 257, 1349, 13, 51760], "temperature": 0.0, "avg_logprob": -0.15182331120856454, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005910132545977831}, {"id": 155, "seek": 70094, "start": 701.0200000000001, "end": 705.1, "text": " What's cool with regular expressions is we can start to get tricky.", "tokens": [50368, 708, 311, 1627, 365, 3890, 15277, 307, 321, 393, 722, 281, 483, 12414, 13, 50572], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 156, "seek": 70094, "start": 705.1, "end": 709.3800000000001, "text": " And so in our previous example we found we used the dot to find any character but let's", "tokens": [50572, 400, 370, 294, 527, 3894, 1365, 321, 1352, 321, 1143, 264, 5893, 281, 915, 604, 2517, 457, 718, 311, 50786], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 157, "seek": 70094, "start": 709.3800000000001, "end": 714.62, "text": " say we only wanted to limit ourselves to finding a subset of any character and we can actually", "tokens": [50786, 584, 321, 787, 1415, 281, 4948, 4175, 281, 5006, 257, 25993, 295, 604, 2517, 293, 321, 393, 767, 51048], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 158, "seek": 70094, "start": 714.62, "end": 717.3800000000001, "text": " create what are called classes of characters.", "tokens": [51048, 1884, 437, 366, 1219, 5359, 295, 4342, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 159, "seek": 70094, "start": 717.3800000000001, "end": 722.1800000000001, "text": " So we can have classes of characters by putting them into hard brackets and A-C will search", "tokens": [51186, 407, 321, 393, 362, 5359, 295, 4342, 538, 3372, 552, 666, 1152, 26179, 293, 316, 12, 34, 486, 3164, 51426], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 160, "seek": 70094, "start": 722.1800000000001, "end": 730.5400000000001, "text": " for an A or a B or a C followed by an A and a T and this would match bat and cat but not", "tokens": [51426, 337, 364, 316, 420, 257, 363, 420, 257, 383, 6263, 538, 364, 316, 293, 257, 314, 293, 341, 576, 2995, 7362, 293, 3857, 457, 406, 51844], "temperature": 0.0, "avg_logprob": -0.12171664604773888, "compression_ratio": 1.7472527472527473, "no_speech_prob": 0.0029806415550410748}, {"id": 161, "seek": 73054, "start": 730.62, "end": 731.62, "text": " rat.", "tokens": [50368, 5937, 13, 50418], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 162, "seek": 73054, "start": 731.62, "end": 734.26, "text": " And let's take a look at this in the command line in a second.", "tokens": [50418, 400, 718, 311, 747, 257, 574, 412, 341, 294, 264, 5622, 1622, 294, 257, 1150, 13, 50550], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 163, "seek": 73054, "start": 734.26, "end": 741.62, "text": " You can also use character classes to find uppercase, lowercase as well as numbers within", "tokens": [50550, 509, 393, 611, 764, 2517, 5359, 281, 915, 11775, 2869, 651, 11, 3126, 9765, 382, 731, 382, 3547, 1951, 50918], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 164, "seek": 73054, "start": 741.62, "end": 742.62, "text": " your strings.", "tokens": [50918, 428, 13985, 13, 50968], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 165, "seek": 73054, "start": 742.62, "end": 746.5, "text": " Now again this is not looking for the number nine, this is looking for the character nine,", "tokens": [50968, 823, 797, 341, 307, 406, 1237, 337, 264, 1230, 4949, 11, 341, 307, 1237, 337, 264, 2517, 4949, 11, 51162], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 166, "seek": 73054, "start": 746.5, "end": 748.62, "text": " the symbol that represents the value nine.", "tokens": [51162, 264, 5986, 300, 8855, 264, 2158, 4949, 13, 51268], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 167, "seek": 73054, "start": 748.62, "end": 755.18, "text": " So this is really just looking for a character zero through nine followed by another character", "tokens": [51268, 407, 341, 307, 534, 445, 1237, 337, 257, 2517, 4018, 807, 4949, 6263, 538, 1071, 2517, 51596], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 168, "seek": 73054, "start": 755.18, "end": 759.3399999999999, "text": " zero through nine and this would match forty two, thirty seven, ninety nine, zero one.", "tokens": [51596, 4018, 807, 4949, 293, 341, 576, 2995, 15815, 732, 11, 11790, 3407, 11, 25063, 4949, 11, 4018, 472, 13, 51804], "temperature": 0.0, "avg_logprob": -0.16184040705362956, "compression_ratio": 1.873076923076923, "no_speech_prob": 0.012427566573023796}, {"id": 169, "seek": 75934, "start": 759.34, "end": 765.22, "text": " So basically match any two characters next to each other anywhere in a word.", "tokens": [50364, 407, 1936, 2995, 604, 732, 4342, 958, 281, 1184, 661, 4992, 294, 257, 1349, 13, 50658], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 170, "seek": 75934, "start": 765.22, "end": 767.1800000000001, "text": " So let's take a look at that.", "tokens": [50658, 407, 718, 311, 747, 257, 574, 412, 300, 13, 50756], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 171, "seek": 75934, "start": 767.1800000000001, "end": 775.1, "text": " So previously when we used the C dot T we matched cut, cat, cut.", "tokens": [50756, 407, 8046, 562, 321, 1143, 264, 383, 5893, 314, 321, 21447, 1723, 11, 3857, 11, 1723, 13, 51152], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 172, "seek": 75934, "start": 775.1, "end": 777.1800000000001, "text": " So the O, the A and the U.", "tokens": [51152, 407, 264, 422, 11, 264, 316, 293, 264, 624, 13, 51256], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 173, "seek": 75934, "start": 777.1800000000001, "end": 782.5400000000001, "text": " So let's say I don't want these watchtowers, CHT, I just want like C followed by a vowel", "tokens": [51256, 407, 718, 311, 584, 286, 500, 380, 528, 613, 1159, 83, 23054, 11, 5995, 51, 11, 286, 445, 528, 411, 383, 6263, 538, 257, 29410, 51524], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 174, "seek": 75934, "start": 782.5400000000001, "end": 785.82, "text": " followed by a T.", "tokens": [51524, 6263, 538, 257, 314, 13, 51688], "temperature": 0.0, "avg_logprob": -0.17138613650673315, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.0006878342828713357}, {"id": 175, "seek": 78582, "start": 785.82, "end": 791.38, "text": " So we can do that instead of using the dot I can create a character class.", "tokens": [50364, 407, 321, 393, 360, 300, 2602, 295, 1228, 264, 5893, 286, 393, 1884, 257, 2517, 1508, 13, 50642], "temperature": 0.0, "avg_logprob": -0.14112470626831056, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.0028893756680190563}, {"id": 176, "seek": 78582, "start": 791.38, "end": 799.3000000000001, "text": " And if I want I can try to match any vowel, I don't know what we'll get but we'll see.", "tokens": [50642, 400, 498, 286, 528, 286, 393, 853, 281, 2995, 604, 29410, 11, 286, 500, 380, 458, 437, 321, 603, 483, 457, 321, 603, 536, 13, 51038], "temperature": 0.0, "avg_logprob": -0.14112470626831056, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.0028893756680190563}, {"id": 177, "seek": 78582, "start": 799.3000000000001, "end": 804.62, "text": " And so what this says is look for the letter C followed by A, E, I, O, or U, just one of", "tokens": [51038, 400, 370, 437, 341, 1619, 307, 574, 337, 264, 5063, 383, 6263, 538, 316, 11, 462, 11, 286, 11, 422, 11, 420, 624, 11, 445, 472, 295, 51304], "temperature": 0.0, "avg_logprob": -0.14112470626831056, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.0028893756680190563}, {"id": 178, "seek": 78582, "start": 804.62, "end": 808.3000000000001, "text": " those but it can choose amongst all of the values in that set followed by the letter", "tokens": [51304, 729, 457, 309, 393, 2826, 12918, 439, 295, 264, 4190, 294, 300, 992, 6263, 538, 264, 5063, 51488], "temperature": 0.0, "avg_logprob": -0.14112470626831056, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.0028893756680190563}, {"id": 179, "seek": 78582, "start": 808.3000000000001, "end": 810.1, "text": " T.", "tokens": [51488, 314, 13, 51578], "temperature": 0.0, "avg_logprob": -0.14112470626831056, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.0028893756680190563}, {"id": 180, "seek": 81010, "start": 810.1, "end": 819.34, "text": " And now if I look at my output I get veracity, I get wainscot, I get vocatives, I get wildcat", "tokens": [50364, 400, 586, 498, 286, 574, 412, 452, 5598, 286, 483, 1306, 19008, 11, 286, 483, 261, 2315, 48477, 11, 286, 483, 2329, 4884, 11, 286, 483, 4868, 18035, 50826], "temperature": 0.0, "avg_logprob": -0.20703430895535452, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.020327122882008553}, {"id": 181, "seek": 81010, "start": 819.34, "end": 827.5400000000001, "text": " so I'm matching all the vowels between the letters C and T. All because of this character", "tokens": [50826, 370, 286, 478, 14324, 439, 264, 44972, 1296, 264, 7825, 383, 293, 314, 13, 1057, 570, 295, 341, 2517, 51236], "temperature": 0.0, "avg_logprob": -0.20703430895535452, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.020327122882008553}, {"id": 182, "seek": 81010, "start": 827.5400000000001, "end": 830.14, "text": " set that I've made.", "tokens": [51236, 992, 300, 286, 600, 1027, 13, 51366], "temperature": 0.0, "avg_logprob": -0.20703430895535452, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.020327122882008553}, {"id": 183, "seek": 81010, "start": 830.14, "end": 833.3000000000001, "text": " If you want to try and find any words in the dictionary, so that's character sets.", "tokens": [51366, 759, 291, 528, 281, 853, 293, 915, 604, 2283, 294, 264, 25890, 11, 370, 300, 311, 2517, 6352, 13, 51524], "temperature": 0.0, "avg_logprob": -0.20703430895535452, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.020327122882008553}, {"id": 184, "seek": 81010, "start": 833.3000000000001, "end": 836.82, "text": " And by the way you can use these and we'll see these in a second, if you want to match", "tokens": [51524, 400, 538, 264, 636, 291, 393, 764, 613, 293, 321, 603, 536, 613, 294, 257, 1150, 11, 498, 291, 528, 281, 2995, 51700], "temperature": 0.0, "avg_logprob": -0.20703430895535452, "compression_ratio": 1.6504424778761062, "no_speech_prob": 0.020327122882008553}, {"id": 185, "seek": 83682, "start": 836.82, "end": 841.86, "text": " all lowercase characters you would do A through Z. You can also say find all lowercase and", "tokens": [50364, 439, 3126, 9765, 4342, 291, 576, 360, 316, 807, 1176, 13, 509, 393, 611, 584, 915, 439, 3126, 9765, 293, 50616], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 186, "seek": 83682, "start": 841.86, "end": 847.1, "text": " all uppercase characters which maybe you're looking for usernames or something along those", "tokens": [50616, 439, 11775, 2869, 651, 4342, 597, 1310, 291, 434, 1237, 337, 505, 1248, 1632, 420, 746, 2051, 729, 50878], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 187, "seek": 83682, "start": 847.1, "end": 848.1, "text": " lines.", "tokens": [50878, 3876, 13, 50928], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 188, "seek": 83682, "start": 848.1, "end": 852.2600000000001, "text": " And then if you wanted to you could also add 0 through 9 and say like the underbar.", "tokens": [50928, 400, 550, 498, 291, 1415, 281, 291, 727, 611, 909, 1958, 807, 1722, 293, 584, 411, 264, 833, 5356, 13, 51136], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 189, "seek": 83682, "start": 852.2600000000001, "end": 855.74, "text": " And what that would do is match for all of those characters so you can have these sets", "tokens": [51136, 400, 437, 300, 576, 360, 307, 2995, 337, 439, 295, 729, 4342, 370, 291, 393, 362, 613, 6352, 51310], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 190, "seek": 83682, "start": 855.74, "end": 859.82, "text": " of characters be as small or as large as you want them to be.", "tokens": [51310, 295, 4342, 312, 382, 1359, 420, 382, 2416, 382, 291, 528, 552, 281, 312, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 191, "seek": 83682, "start": 859.82, "end": 864.1800000000001, "text": " I'm going to do is changes, let's see if there's any words that have 0 through 9, I doubt it.", "tokens": [51514, 286, 478, 516, 281, 360, 307, 2962, 11, 718, 311, 536, 498, 456, 311, 604, 2283, 300, 362, 1958, 807, 1722, 11, 286, 6385, 309, 13, 51732], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 192, "seek": 83682, "start": 864.1800000000001, "end": 865.74, "text": " And nothing matches that pattern.", "tokens": [51732, 400, 1825, 10676, 300, 5102, 13, 51810], "temperature": 0.0, "avg_logprob": -0.17431663299773956, "compression_ratio": 1.8737201365187712, "no_speech_prob": 0.05661777779459953}, {"id": 193, "seek": 86574, "start": 865.74, "end": 869.26, "text": " That's what happens when you put in a pattern that doesn't match.", "tokens": [50364, 663, 311, 437, 2314, 562, 291, 829, 294, 257, 5102, 300, 1177, 380, 2995, 13, 50540], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 194, "seek": 86574, "start": 869.26, "end": 872.5, "text": " Let's see if there's anything in the dictionary that matches against a number.", "tokens": [50540, 961, 311, 536, 498, 456, 311, 1340, 294, 264, 25890, 300, 10676, 1970, 257, 1230, 13, 50702], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 195, "seek": 86574, "start": 872.5, "end": 878.02, "text": " Nope, so there are no numbers in the American English dictionary on the Unix system.", "tokens": [50702, 12172, 11, 370, 456, 366, 572, 3547, 294, 264, 2665, 3669, 25890, 322, 264, 1156, 970, 1185, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 196, "seek": 86574, "start": 878.02, "end": 880.02, "text": " So it's kind of helpful.", "tokens": [50978, 407, 309, 311, 733, 295, 4961, 13, 51078], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 197, "seek": 86574, "start": 880.02, "end": 885.34, "text": " There's a couple other things that we could actually use if we wanted to grep numbers.", "tokens": [51078, 821, 311, 257, 1916, 661, 721, 300, 321, 727, 767, 764, 498, 321, 1415, 281, 6066, 79, 3547, 13, 51344], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 198, "seek": 86574, "start": 885.34, "end": 889.42, "text": " What I'll do right now is just to look at what we could possibly grep for a number would", "tokens": [51344, 708, 286, 603, 360, 558, 586, 307, 445, 281, 574, 412, 437, 321, 727, 6264, 6066, 79, 337, 257, 1230, 576, 51548], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 199, "seek": 86574, "start": 889.42, "end": 891.78, "text": " be the Etsy password file.", "tokens": [51548, 312, 264, 47170, 88, 11524, 3991, 13, 51666], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 200, "seek": 86574, "start": 891.78, "end": 894.22, "text": " And if I grep that I know there's a lot of numbers in there so I'm going to get a lot", "tokens": [51666, 400, 498, 286, 6066, 79, 300, 286, 458, 456, 311, 257, 688, 295, 3547, 294, 456, 370, 286, 478, 516, 281, 483, 257, 688, 51788], "temperature": 0.0, "avg_logprob": -0.1525916395516231, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.007343457546085119}, {"id": 201, "seek": 89422, "start": 894.22, "end": 900.38, "text": " of matches and you'll notice that it matched all of these lines because all of the lines", "tokens": [50364, 295, 10676, 293, 291, 603, 3449, 300, 309, 21447, 439, 295, 613, 3876, 570, 439, 295, 264, 3876, 50672], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 202, "seek": 89422, "start": 900.38, "end": 901.7, "text": " have a number in them.", "tokens": [50672, 362, 257, 1230, 294, 552, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 203, "seek": 89422, "start": 901.7, "end": 905.86, "text": " Again, the Etsy password files just shows you all the user accounts on the system and", "tokens": [50738, 3764, 11, 264, 47170, 88, 11524, 7098, 445, 3110, 291, 439, 264, 4195, 9402, 322, 264, 1185, 293, 50946], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 204, "seek": 89422, "start": 905.86, "end": 909.1, "text": " these numbers represent their user and group IDs.", "tokens": [50946, 613, 3547, 2906, 641, 4195, 293, 1594, 48212, 13, 51108], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 205, "seek": 89422, "start": 909.1, "end": 911.58, "text": " So I know that there were a lot of numeric matches in this file.", "tokens": [51108, 407, 286, 458, 300, 456, 645, 257, 688, 295, 7866, 299, 10676, 294, 341, 3991, 13, 51232], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 206, "seek": 89422, "start": 911.58, "end": 917.5400000000001, "text": " So just to kind of demonstrate that you can match against numbers as well as words.", "tokens": [51232, 407, 445, 281, 733, 295, 11698, 300, 291, 393, 2995, 1970, 3547, 382, 731, 382, 2283, 13, 51530], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 207, "seek": 89422, "start": 917.5400000000001, "end": 920.9, "text": " So kind of helpful.", "tokens": [51530, 407, 733, 295, 4961, 13, 51698], "temperature": 0.0, "avg_logprob": -0.17401602608816966, "compression_ratio": 1.7405857740585775, "no_speech_prob": 0.009122070856392384}, {"id": 208, "seek": 92090, "start": 920.9, "end": 925.34, "text": " Let's go back to an example where we can start to look at the English dictionary.", "tokens": [50364, 961, 311, 352, 646, 281, 364, 1365, 689, 321, 393, 722, 281, 574, 412, 264, 3669, 25890, 13, 50586], "temperature": 0.0, "avg_logprob": -0.11982003211975098, "compression_ratio": 1.7314049586776858, "no_speech_prob": 0.05917265638709068}, {"id": 209, "seek": 92090, "start": 925.34, "end": 931.78, "text": " So let's look at some other options for this feature, this ability to use character classes.", "tokens": [50586, 407, 718, 311, 574, 412, 512, 661, 3956, 337, 341, 4111, 11, 341, 3485, 281, 764, 2517, 5359, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11982003211975098, "compression_ratio": 1.7314049586776858, "no_speech_prob": 0.05917265638709068}, {"id": 210, "seek": 92090, "start": 931.78, "end": 937.98, "text": " Built into Unix is this idea of these really what I think are pretty ugly character classes.", "tokens": [50908, 49822, 666, 1156, 970, 307, 341, 1558, 295, 613, 534, 437, 286, 519, 366, 1238, 12246, 2517, 5359, 13, 51218], "temperature": 0.0, "avg_logprob": -0.11982003211975098, "compression_ratio": 1.7314049586776858, "no_speech_prob": 0.05917265638709068}, {"id": 211, "seek": 92090, "start": 937.98, "end": 940.66, "text": " You'll see these sometimes built into the system.", "tokens": [51218, 509, 603, 536, 613, 2171, 3094, 666, 264, 1185, 13, 51352], "temperature": 0.0, "avg_logprob": -0.11982003211975098, "compression_ratio": 1.7314049586776858, "no_speech_prob": 0.05917265638709068}, {"id": 212, "seek": 92090, "start": 940.66, "end": 946.6999999999999, "text": " So bracket, bracket, colon, alpha, colon, bracket, bracket is the same as building your own character", "tokens": [51352, 407, 16904, 11, 16904, 11, 8255, 11, 8961, 11, 8255, 11, 16904, 11, 16904, 307, 264, 912, 382, 2390, 428, 1065, 2517, 51654], "temperature": 0.0, "avg_logprob": -0.11982003211975098, "compression_ratio": 1.7314049586776858, "no_speech_prob": 0.05917265638709068}, {"id": 213, "seek": 94670, "start": 946.7, "end": 951.26, "text": " class that uses bracket A through Z, capital A through Z.", "tokens": [50364, 1508, 300, 4960, 16904, 316, 807, 1176, 11, 4238, 316, 807, 1176, 13, 50592], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 214, "seek": 94670, "start": 951.26, "end": 957.1, "text": " And basically what these things on the left are are just ways of writing these out using", "tokens": [50592, 400, 1936, 437, 613, 721, 322, 264, 1411, 366, 366, 445, 2098, 295, 3579, 613, 484, 1228, 50884], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 215, "seek": 94670, "start": 957.1, "end": 961.6600000000001, "text": " words as opposed to using the categories A through Z.", "tokens": [50884, 2283, 382, 8851, 281, 1228, 264, 10479, 316, 807, 1176, 13, 51112], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 216, "seek": 94670, "start": 961.6600000000001, "end": 964.82, "text": " Pretty much if you haven't figured this out at this point in Unix, there's 50 ways to", "tokens": [51112, 10693, 709, 498, 291, 2378, 380, 8932, 341, 484, 412, 341, 935, 294, 1156, 970, 11, 456, 311, 2625, 2098, 281, 51270], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 217, "seek": 94670, "start": 964.82, "end": 965.82, "text": " do everything.", "tokens": [51270, 360, 1203, 13, 51320], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 218, "seek": 94670, "start": 965.82, "end": 969.6600000000001, "text": " So these character class abbreviations can be helpful.", "tokens": [51320, 407, 613, 2517, 1508, 35839, 763, 393, 312, 4961, 13, 51512], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 219, "seek": 94670, "start": 969.6600000000001, "end": 971.86, "text": " They make things a little more readable in scripts.", "tokens": [51512, 814, 652, 721, 257, 707, 544, 49857, 294, 23294, 13, 51622], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 220, "seek": 94670, "start": 971.86, "end": 975.5400000000001, "text": " But the other thing if you haven't noticed is that regular expressions can get pretty", "tokens": [51622, 583, 264, 661, 551, 498, 291, 2378, 380, 5694, 307, 300, 3890, 15277, 393, 483, 1238, 51806], "temperature": 0.0, "avg_logprob": -0.15136907882049305, "compression_ratio": 1.697594501718213, "no_speech_prob": 0.11273880302906036}, {"id": 221, "seek": 97554, "start": 975.54, "end": 978.9, "text": " big and pretty scary pretty fast.", "tokens": [50364, 955, 293, 1238, 6958, 1238, 2370, 13, 50532], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 222, "seek": 97554, "start": 978.9, "end": 982.66, "text": " I would say the most useful character class that we're looking at right here is the one", "tokens": [50532, 286, 576, 584, 264, 881, 4420, 2517, 1508, 300, 321, 434, 1237, 412, 558, 510, 307, 264, 472, 50720], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 223, "seek": 97554, "start": 982.66, "end": 985.86, "text": " called space because that will match any white space characters.", "tokens": [50720, 1219, 1901, 570, 300, 486, 2995, 604, 2418, 1901, 4342, 13, 50880], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 224, "seek": 97554, "start": 985.86, "end": 987.98, "text": " That includes tabs and spaces.", "tokens": [50880, 663, 5974, 20743, 293, 7673, 13, 50986], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 225, "seek": 97554, "start": 987.98, "end": 993.3, "text": " That can be really helpful when trying to match words or sentences that have spaces.", "tokens": [50986, 663, 393, 312, 534, 4961, 562, 1382, 281, 2995, 2283, 420, 16579, 300, 362, 7673, 13, 51252], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 226, "seek": 97554, "start": 993.3, "end": 998.3, "text": " Or in my first example, we talked about last names and that would really be helpful for", "tokens": [51252, 1610, 294, 452, 700, 1365, 11, 321, 2825, 466, 1036, 5288, 293, 300, 576, 534, 312, 4961, 337, 51502], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 227, "seek": 97554, "start": 998.3, "end": 1002.26, "text": " processing, finding somebody's last name.", "tokens": [51502, 9007, 11, 5006, 2618, 311, 1036, 1315, 13, 51700], "temperature": 0.0, "avg_logprob": -0.17549224853515624, "compression_ratio": 1.7211155378486056, "no_speech_prob": 0.08263842016458511}, {"id": 228, "seek": 100226, "start": 1002.26, "end": 1006.98, "text": " Because I don't like these, I'm mostly familiar with these character class abbreviations.", "tokens": [50364, 1436, 286, 500, 380, 411, 613, 11, 286, 478, 5240, 4963, 365, 613, 2517, 1508, 35839, 763, 13, 50600], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 229, "seek": 100226, "start": 1006.98, "end": 1009.46, "text": " So notice there's two character class abbreviations.", "tokens": [50600, 407, 3449, 456, 311, 732, 2517, 1508, 35839, 763, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 230, "seek": 100226, "start": 1009.46, "end": 1013.02, "text": " The previous ones with the words like alpha, upper, and lower.", "tokens": [50724, 440, 3894, 2306, 365, 264, 2283, 411, 8961, 11, 6597, 11, 293, 3126, 13, 50902], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 231, "seek": 100226, "start": 1013.02, "end": 1018.58, "text": " But then there's also these character class abbreviations, slash D, slash W, slash S,", "tokens": [50902, 583, 550, 456, 311, 611, 613, 2517, 1508, 35839, 763, 11, 17330, 413, 11, 17330, 343, 11, 17330, 318, 11, 51180], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 232, "seek": 100226, "start": 1018.58, "end": 1025.42, "text": " slash S that make things a little bit more readable when we start to use these in larger", "tokens": [51180, 17330, 318, 300, 652, 721, 257, 707, 857, 544, 49857, 562, 321, 722, 281, 764, 613, 294, 4833, 51522], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 233, "seek": 100226, "start": 1025.42, "end": 1026.98, "text": " regular expressions.", "tokens": [51522, 3890, 15277, 13, 51600], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 234, "seek": 100226, "start": 1026.98, "end": 1030.82, "text": " I'll also mention that there are actually ways to negate character classes.", "tokens": [51600, 286, 603, 611, 2152, 300, 456, 366, 767, 2098, 281, 2485, 473, 2517, 5359, 13, 51792], "temperature": 0.0, "avg_logprob": -0.16187917140492222, "compression_ratio": 1.908, "no_speech_prob": 0.4070180356502533}, {"id": 235, "seek": 103082, "start": 1030.82, "end": 1037.06, "text": " If you notice the carrot inside of the bracket-based classes at the bottom here, you'll notice", "tokens": [50364, 759, 291, 3449, 264, 22767, 1854, 295, 264, 16904, 12, 6032, 5359, 412, 264, 2767, 510, 11, 291, 603, 3449, 50676], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 236, "seek": 103082, "start": 1037.06, "end": 1040.02, "text": " that that means does not match this character.", "tokens": [50676, 300, 300, 1355, 775, 406, 2995, 341, 2517, 13, 50824], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 237, "seek": 103082, "start": 1040.02, "end": 1045.58, "text": " And if you do slash capital D, slash capital W, slash capital S, that means that you do", "tokens": [50824, 400, 498, 291, 360, 17330, 4238, 413, 11, 17330, 4238, 343, 11, 17330, 4238, 318, 11, 300, 1355, 300, 291, 360, 51102], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 238, "seek": 103082, "start": 1045.58, "end": 1048.4199999999998, "text": " not want to match that character in that given position.", "tokens": [51102, 406, 528, 281, 2995, 300, 2517, 294, 300, 2212, 2535, 13, 51244], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 239, "seek": 103082, "start": 1048.4199999999998, "end": 1051.62, "text": " I don't usually teach those when I introduce regular expressions because it's hard enough", "tokens": [51244, 286, 500, 380, 2673, 2924, 729, 562, 286, 5366, 3890, 15277, 570, 309, 311, 1152, 1547, 51404], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 240, "seek": 103082, "start": 1051.62, "end": 1056.1799999999998, "text": " getting your head around regular expressions, let alone getting your head around negative", "tokens": [51404, 1242, 428, 1378, 926, 3890, 15277, 11, 718, 3312, 1242, 428, 1378, 926, 3671, 51632], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 241, "seek": 103082, "start": 1056.1799999999998, "end": 1057.54, "text": " logic at the same time.", "tokens": [51632, 9952, 412, 264, 912, 565, 13, 51700], "temperature": 0.0, "avg_logprob": -0.11027327681009748, "compression_ratio": 1.8702290076335877, "no_speech_prob": 0.06185606122016907}, {"id": 242, "seek": 105754, "start": 1057.54, "end": 1060.98, "text": " So we'll just kind of mention that they're there and just ignore that.", "tokens": [50364, 407, 321, 603, 445, 733, 295, 2152, 300, 436, 434, 456, 293, 445, 11200, 300, 13, 50536], "temperature": 0.0, "avg_logprob": -0.1638350486755371, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.010012534447014332}, {"id": 243, "seek": 105754, "start": 1060.98, "end": 1068.54, "text": " So let's look at these two character class sets in operation.", "tokens": [50536, 407, 718, 311, 574, 412, 613, 732, 2517, 1508, 6352, 294, 6916, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1638350486755371, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.010012534447014332}, {"id": 244, "seek": 105754, "start": 1068.54, "end": 1076.7, "text": " So let's say I want to find a word that has any letter in it.", "tokens": [50914, 407, 718, 311, 584, 286, 528, 281, 915, 257, 1349, 300, 575, 604, 5063, 294, 309, 13, 51322], "temperature": 0.0, "avg_logprob": -0.1638350486755371, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.010012534447014332}, {"id": 245, "seek": 105754, "start": 1076.7, "end": 1082.26, "text": " Well notice I just did that and I said that I wanted to find a slash D, but notice that", "tokens": [51322, 1042, 3449, 286, 445, 630, 300, 293, 286, 848, 300, 286, 1415, 281, 915, 257, 17330, 413, 11, 457, 3449, 300, 51600], "temperature": 0.0, "avg_logprob": -0.1638350486755371, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.010012534447014332}, {"id": 246, "seek": 105754, "start": 1082.26, "end": 1086.78, "text": " it actually went ahead and found all of these words with the character D.", "tokens": [51600, 309, 767, 1437, 2286, 293, 1352, 439, 295, 613, 2283, 365, 264, 2517, 413, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1638350486755371, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.010012534447014332}, {"id": 247, "seek": 108678, "start": 1086.78, "end": 1087.78, "text": " Well that's a problem.", "tokens": [50364, 1042, 300, 311, 257, 1154, 13, 50414], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 248, "seek": 108678, "start": 1087.78, "end": 1093.26, "text": " You're thinking, well Jason, wait, you just showed us that slash D is a character class.", "tokens": [50414, 509, 434, 1953, 11, 731, 11181, 11, 1699, 11, 291, 445, 4712, 505, 300, 17330, 413, 307, 257, 2517, 1508, 13, 50688], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 249, "seek": 108678, "start": 1093.26, "end": 1094.26, "text": " But here's the thing.", "tokens": [50688, 583, 510, 311, 264, 551, 13, 50738], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 250, "seek": 108678, "start": 1094.26, "end": 1097.46, "text": " Remember earlier on I said there's grep and there's extended grep?", "tokens": [50738, 5459, 3071, 322, 286, 848, 456, 311, 6066, 79, 293, 456, 311, 10913, 6066, 79, 30, 50898], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 251, "seek": 108678, "start": 1097.46, "end": 1100.1399999999999, "text": " Well these slash D character sets are actually part of extended grep.", "tokens": [50898, 1042, 613, 17330, 413, 2517, 6352, 366, 767, 644, 295, 10913, 6066, 79, 13, 51032], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 252, "seek": 108678, "start": 1100.1399999999999, "end": 1107.18, "text": " So if I switch to eGrep, it still doesn't work, which is a problem because I'm trying", "tokens": [51032, 407, 498, 286, 3679, 281, 308, 38, 19919, 11, 309, 920, 1177, 380, 589, 11, 597, 307, 257, 1154, 570, 286, 478, 1382, 51384], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 253, "seek": 108678, "start": 1107.18, "end": 1108.1, "text": " to explain this.", "tokens": [51384, 281, 2903, 341, 13, 51430], "temperature": 0.0, "avg_logprob": -0.17936375935872395, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.186989888548851}, {"id": 254, "seek": 111678, "start": 1117.78, "end": 1123.3799999999999, "text": " Once you'll notice is that the slash D doesn't work, it actually goes ahead and finds the", "tokens": [50414, 3443, 291, 603, 3449, 307, 300, 264, 17330, 413, 1177, 380, 589, 11, 309, 767, 1709, 2286, 293, 10704, 264, 50694], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 255, "seek": 111678, "start": 1123.3799999999999, "end": 1125.34, "text": " letter D within here.", "tokens": [50694, 5063, 413, 1951, 510, 13, 50792], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 256, "seek": 111678, "start": 1125.34, "end": 1129.8999999999999, "text": " Slash D is actually a part of Perl regular expression.", "tokens": [50792, 6187, 1299, 413, 307, 767, 257, 644, 295, 3026, 75, 3890, 6114, 13, 51020], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 257, "seek": 111678, "start": 1129.8999999999999, "end": 1133.42, "text": " So if I actually add dash P, you'll note that it returns nothing.", "tokens": [51020, 407, 498, 286, 767, 909, 8240, 430, 11, 291, 603, 3637, 300, 309, 11247, 1825, 13, 51196], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 258, "seek": 111678, "start": 1133.42, "end": 1137.3799999999999, "text": " So that actually says u slash D, like it's a Perl regular expression and that'll pass", "tokens": [51196, 407, 300, 767, 1619, 344, 17330, 413, 11, 411, 309, 311, 257, 3026, 75, 3890, 6114, 293, 300, 603, 1320, 51394], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 259, "seek": 111678, "start": 1137.3799999999999, "end": 1138.3799999999999, "text": " numbers.", "tokens": [51394, 3547, 13, 51444], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 260, "seek": 111678, "start": 1138.3799999999999, "end": 1141.06, "text": " So that doesn't actually work in grep or eGrep.", "tokens": [51444, 407, 300, 1177, 380, 767, 589, 294, 6066, 79, 420, 308, 38, 19919, 13, 51578], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 261, "seek": 111678, "start": 1141.06, "end": 1145.54, "text": " But if I get rid of the dash P and just go with regular extended grep, what you'll find", "tokens": [51578, 583, 498, 286, 483, 3973, 295, 264, 8240, 430, 293, 445, 352, 365, 3890, 10913, 6066, 79, 11, 437, 291, 603, 915, 51802], "temperature": 0.0, "avg_logprob": -0.17581822321965143, "compression_ratio": 1.844621513944223, "no_speech_prob": 0.05338996276259422}, {"id": 262, "seek": 114554, "start": 1145.54, "end": 1152.94, "text": " is I can actually utilize slash W. And that matches any character in any word that we", "tokens": [50364, 307, 286, 393, 767, 16117, 17330, 343, 13, 400, 300, 10676, 604, 2517, 294, 604, 1349, 300, 321, 50734], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 263, "seek": 114554, "start": 1152.94, "end": 1153.94, "text": " might be looking at.", "tokens": [50734, 1062, 312, 1237, 412, 13, 50784], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 264, "seek": 114554, "start": 1153.94, "end": 1156.3799999999999, "text": " So that one does work.", "tokens": [50784, 407, 300, 472, 775, 589, 13, 50906], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 265, "seek": 114554, "start": 1156.3799999999999, "end": 1162.58, "text": " And if we go ahead and we look at slash S, what you'll notice is that that also returns", "tokens": [50906, 400, 498, 321, 352, 2286, 293, 321, 574, 412, 17330, 318, 11, 437, 291, 603, 3449, 307, 300, 300, 611, 11247, 51216], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 266, "seek": 114554, "start": 1162.58, "end": 1164.22, "text": " nothing.", "tokens": [51216, 1825, 13, 51298], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 267, "seek": 114554, "start": 1164.22, "end": 1169.42, "text": " So it looks like slash D is one of those cases where that's more of a Perl thing than it", "tokens": [51298, 407, 309, 1542, 411, 17330, 413, 307, 472, 295, 729, 3331, 689, 300, 311, 544, 295, 257, 3026, 75, 551, 813, 309, 51558], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 268, "seek": 114554, "start": 1169.42, "end": 1171.8999999999999, "text": " is an extended grep thing.", "tokens": [51558, 307, 364, 10913, 6066, 79, 551, 13, 51682], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 269, "seek": 114554, "start": 1171.8999999999999, "end": 1173.98, "text": " So I'll apologize for that and keep going.", "tokens": [51682, 407, 286, 603, 12328, 337, 300, 293, 1066, 516, 13, 51786], "temperature": 0.0, "avg_logprob": -0.13352713453660317, "compression_ratio": 1.6452991452991452, "no_speech_prob": 0.021610461175441742}, {"id": 270, "seek": 117398, "start": 1173.98, "end": 1179.54, "text": " Or slash D is the same as typing 0 through 9.", "tokens": [50364, 1610, 17330, 413, 307, 264, 912, 382, 18444, 1958, 807, 1722, 13, 50642], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 271, "seek": 117398, "start": 1179.54, "end": 1186.34, "text": " So you don't need to worry about the fact that slash D is not supported in eGrep.", "tokens": [50642, 407, 291, 500, 380, 643, 281, 3292, 466, 264, 1186, 300, 17330, 413, 307, 406, 8104, 294, 308, 38, 19919, 13, 50982], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 272, "seek": 117398, "start": 1186.34, "end": 1189.54, "text": " Let's take a look at some other ways that we can build patterns.", "tokens": [50982, 961, 311, 747, 257, 574, 412, 512, 661, 2098, 300, 321, 393, 1322, 8294, 13, 51142], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 273, "seek": 117398, "start": 1189.54, "end": 1194.26, "text": " We'll start out by taking a look at what are called anchors or positional anchors.", "tokens": [51142, 492, 603, 722, 484, 538, 1940, 257, 574, 412, 437, 366, 1219, 12723, 830, 420, 2535, 304, 12723, 830, 13, 51378], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 274, "seek": 117398, "start": 1194.26, "end": 1198.98, "text": " In other words, we can say that a certain character has to match at a certain point on", "tokens": [51378, 682, 661, 2283, 11, 321, 393, 584, 300, 257, 1629, 2517, 575, 281, 2995, 412, 257, 1629, 935, 322, 51614], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 275, "seek": 117398, "start": 1198.98, "end": 1203.42, "text": " a line, either at the beginning of the line or at the end of the line.", "tokens": [51614, 257, 1622, 11, 2139, 412, 264, 2863, 295, 264, 1622, 420, 412, 264, 917, 295, 264, 1622, 13, 51836], "temperature": 0.0, "avg_logprob": -0.10609081813267299, "compression_ratio": 1.704724409448819, "no_speech_prob": 0.03307562693953514}, {"id": 276, "seek": 120342, "start": 1203.42, "end": 1207.74, "text": " And you'll see that the characters that we're going to use for these beginning and end of", "tokens": [50364, 400, 291, 603, 536, 300, 264, 4342, 300, 321, 434, 516, 281, 764, 337, 613, 2863, 293, 917, 295, 50580], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 277, "seek": 120342, "start": 1207.74, "end": 1212.74, "text": " line matches are a carrot and a dollar sign.", "tokens": [50580, 1622, 10676, 366, 257, 22767, 293, 257, 7241, 1465, 13, 50830], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 278, "seek": 120342, "start": 1212.74, "end": 1217.1000000000001, "text": " And you'll notice that in the example in front of you, putting a carrot means to match only", "tokens": [50830, 400, 291, 603, 3449, 300, 294, 264, 1365, 294, 1868, 295, 291, 11, 3372, 257, 22767, 1355, 281, 2995, 787, 51048], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 279, "seek": 120342, "start": 1217.1000000000001, "end": 1221.94, "text": " the word car, where car is the first line character on the line.", "tokens": [51048, 264, 1349, 1032, 11, 689, 1032, 307, 264, 700, 1622, 2517, 322, 264, 1622, 13, 51290], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 280, "seek": 120342, "start": 1221.94, "end": 1228.02, "text": " So it would match car, cattle, and canine, where those are the first words on a line.", "tokens": [51290, 407, 309, 576, 2995, 1032, 11, 19992, 11, 293, 393, 533, 11, 689, 729, 366, 264, 700, 2283, 322, 257, 1622, 13, 51594], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 281, "seek": 120342, "start": 1228.02, "end": 1231.66, "text": " And the second example, floating and sailing, would match because the G is at the end of", "tokens": [51594, 400, 264, 1150, 1365, 11, 12607, 293, 27452, 11, 576, 2995, 570, 264, 460, 307, 412, 264, 917, 295, 51776], "temperature": 0.0, "avg_logprob": -0.10239257331655807, "compression_ratio": 1.8492063492063493, "no_speech_prob": 0.006096197757869959}, {"id": 282, "seek": 123166, "start": 1231.66, "end": 1234.5800000000002, "text": " the line if those are the only words on the line.", "tokens": [50364, 264, 1622, 498, 729, 366, 264, 787, 2283, 322, 264, 1622, 13, 50510], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 283, "seek": 123166, "start": 1234.5800000000002, "end": 1239.38, "text": " And if you'd like to match situations where a word is the only word on a given line, such", "tokens": [50510, 400, 498, 291, 1116, 411, 281, 2995, 6851, 689, 257, 1349, 307, 264, 787, 1349, 322, 257, 2212, 1622, 11, 1270, 50750], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 284, "seek": 123166, "start": 1239.38, "end": 1243.42, "text": " as cat, you would want a line that starts with the letter C, a line that ends with the", "tokens": [50750, 382, 3857, 11, 291, 576, 528, 257, 1622, 300, 3719, 365, 264, 5063, 383, 11, 257, 1622, 300, 5314, 365, 264, 50952], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 285, "seek": 123166, "start": 1243.42, "end": 1246.0600000000002, "text": " letter T. We'll take a look at these in one second.", "tokens": [50952, 5063, 314, 13, 492, 603, 747, 257, 574, 412, 613, 294, 472, 1150, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 286, "seek": 123166, "start": 1246.0600000000002, "end": 1250.46, "text": " Let's look at another possible anchor that we can use, which is a word boundary.", "tokens": [51084, 961, 311, 574, 412, 1071, 1944, 18487, 300, 321, 393, 764, 11, 597, 307, 257, 1349, 12866, 13, 51304], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 287, "seek": 123166, "start": 1250.46, "end": 1254.5, "text": " So remember, the carrot character and the dollar sign character have to do with the beginning", "tokens": [51304, 407, 1604, 11, 264, 22767, 2517, 293, 264, 7241, 1465, 2517, 362, 281, 360, 365, 264, 2863, 51506], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 288, "seek": 123166, "start": 1254.5, "end": 1256.3400000000001, "text": " and the end of line.", "tokens": [51506, 293, 264, 917, 295, 1622, 13, 51598], "temperature": 0.0, "avg_logprob": -0.10788932037353516, "compression_ratio": 1.7752808988764044, "no_speech_prob": 0.0028006418142467737}, {"id": 289, "seek": 125634, "start": 1256.34, "end": 1264.86, "text": " And word boundaries give us the ability to match on spaces, tabs, and basically these", "tokens": [50364, 400, 1349, 13180, 976, 505, 264, 3485, 281, 2995, 322, 7673, 11, 20743, 11, 293, 1936, 613, 50790], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 290, "seek": 125634, "start": 1264.86, "end": 1268.78, "text": " positions where there might be breaks between words.", "tokens": [50790, 8432, 689, 456, 1062, 312, 9857, 1296, 2283, 13, 50986], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 291, "seek": 125634, "start": 1268.78, "end": 1275.22, "text": " So in the example that I've got at the bottom, if I wanted to match Jason the prof, S-O-N", "tokens": [50986, 407, 294, 264, 1365, 300, 286, 600, 658, 412, 264, 2767, 11, 498, 286, 1415, 281, 2995, 11181, 264, 1740, 11, 318, 12, 46, 12, 45, 51308], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 292, "seek": 125634, "start": 1275.22, "end": 1277.3, "text": " is anchored on a word boundary.", "tokens": [51308, 307, 12723, 2769, 322, 257, 1349, 12866, 13, 51412], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 293, "seek": 125634, "start": 1277.3, "end": 1280.22, "text": " So there's a space between my name and the word the.", "tokens": [51412, 407, 456, 311, 257, 1901, 1296, 452, 1315, 293, 264, 1349, 264, 13, 51558], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 294, "seek": 125634, "start": 1280.22, "end": 1283.3, "text": " So S-O-N slash B would match that line.", "tokens": [51558, 407, 318, 12, 46, 12, 45, 17330, 363, 576, 2995, 300, 1622, 13, 51712], "temperature": 0.0, "avg_logprob": -0.15742672920227052, "compression_ratio": 1.6045454545454545, "no_speech_prob": 0.2225203812122345}, {"id": 295, "seek": 128330, "start": 1283.3, "end": 1287.34, "text": " Actually, there's no other instance of S-O-N on that line, but let's say I was in a file", "tokens": [50364, 5135, 11, 456, 311, 572, 661, 5197, 295, 318, 12, 46, 12, 45, 322, 300, 1622, 11, 457, 718, 311, 584, 286, 390, 294, 257, 3991, 50566], "temperature": 0.0, "avg_logprob": -0.10251007274705537, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.0006070492090657353}, {"id": 296, "seek": 128330, "start": 1287.34, "end": 1294.1, "text": " that had a lot of people named Jason, but I was the only one with the surname the prof.", "tokens": [50566, 300, 632, 257, 688, 295, 561, 4926, 11181, 11, 457, 286, 390, 264, 787, 472, 365, 264, 50152, 264, 1740, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10251007274705537, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.0006070492090657353}, {"id": 297, "seek": 128330, "start": 1294.1, "end": 1295.1, "text": " It would match.", "tokens": [50904, 467, 576, 2995, 13, 50954], "temperature": 0.0, "avg_logprob": -0.10251007274705537, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.0006070492090657353}, {"id": 298, "seek": 128330, "start": 1295.1, "end": 1302.46, "text": " So let's take a look at how word boundaries and anchors work in real life.", "tokens": [50954, 407, 718, 311, 747, 257, 574, 412, 577, 1349, 13180, 293, 12723, 830, 589, 294, 957, 993, 13, 51322], "temperature": 0.0, "avg_logprob": -0.10251007274705537, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.0006070492090657353}, {"id": 299, "seek": 128330, "start": 1302.46, "end": 1308.6599999999999, "text": " So to match items in word boundaries, we really need a file to do this.", "tokens": [51322, 407, 281, 2995, 4754, 294, 1349, 13180, 11, 321, 534, 643, 257, 3991, 281, 360, 341, 13, 51632], "temperature": 0.0, "avg_logprob": -0.10251007274705537, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.0006070492090657353}, {"id": 300, "seek": 130866, "start": 1308.66, "end": 1312.8600000000001, "text": " So what I'm going to do is take another look at the Etsy password file.", "tokens": [50364, 407, 437, 286, 478, 516, 281, 360, 307, 747, 1071, 574, 412, 264, 47170, 88, 11524, 3991, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 301, "seek": 130866, "start": 1312.8600000000001, "end": 1317.1000000000001, "text": " I've also bumped up the font, so hopefully things are a little bit easier to see.", "tokens": [50574, 286, 600, 611, 42696, 493, 264, 10703, 11, 370, 4696, 721, 366, 257, 707, 857, 3571, 281, 536, 13, 50786], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 302, "seek": 130866, "start": 1317.1000000000001, "end": 1320.94, "text": " So what you'll notice is in the Etsy password file, there's a number of lines here.", "tokens": [50786, 407, 437, 291, 603, 3449, 307, 294, 264, 47170, 88, 11524, 3991, 11, 456, 311, 257, 1230, 295, 3876, 510, 13, 50978], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 303, "seek": 130866, "start": 1320.94, "end": 1324.5, "text": " So I've got root, demon, bin, sys.", "tokens": [50978, 407, 286, 600, 658, 5593, 11, 14283, 11, 5171, 11, 262, 749, 13, 51156], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 304, "seek": 130866, "start": 1324.5, "end": 1328.5800000000002, "text": " So let's say I only want to see lines that start with the letter S. There's a couple", "tokens": [51156, 407, 718, 311, 584, 286, 787, 528, 281, 536, 3876, 300, 722, 365, 264, 5063, 318, 13, 821, 311, 257, 1916, 51360], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 305, "seek": 130866, "start": 1328.5800000000002, "end": 1329.5800000000002, "text": " of them.", "tokens": [51360, 295, 552, 13, 51410], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 306, "seek": 130866, "start": 1329.5800000000002, "end": 1335.02, "text": " So I'm going to use eGREP, and I'm going to say that I want to look at lines that start", "tokens": [51410, 407, 286, 478, 516, 281, 764, 308, 38, 3850, 47, 11, 293, 286, 478, 516, 281, 584, 300, 286, 528, 281, 574, 412, 3876, 300, 722, 51682], "temperature": 0.0, "avg_logprob": -0.14018536310126312, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.3005119264125824}, {"id": 307, "seek": 133502, "start": 1335.02, "end": 1339.66, "text": " with the letter S, and I want to look at the Etsy password file.", "tokens": [50364, 365, 264, 5063, 318, 11, 293, 286, 528, 281, 574, 412, 264, 47170, 88, 11524, 3991, 13, 50596], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 308, "seek": 133502, "start": 1339.66, "end": 1344.22, "text": " And what you'll notice is that shows me lines that just start with the letter S. Notice", "tokens": [50596, 400, 437, 291, 603, 3449, 307, 300, 3110, 385, 3876, 300, 445, 722, 365, 264, 5063, 318, 13, 13428, 50824], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 309, "seek": 133502, "start": 1344.22, "end": 1346.58, "text": " that the letter S might be in other parts of the line.", "tokens": [50824, 300, 264, 5063, 318, 1062, 312, 294, 661, 3166, 295, 264, 1622, 13, 50942], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 310, "seek": 133502, "start": 1346.58, "end": 1352.78, "text": " If I take out that position qualifier, what you'll notice is that the letter S appears", "tokens": [50942, 759, 286, 747, 484, 300, 2535, 4101, 9902, 11, 437, 291, 603, 3449, 307, 300, 264, 5063, 318, 7038, 51252], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 311, "seek": 133502, "start": 1352.78, "end": 1358.1, "text": " on many more lines, but I only want to be interested in those lines where the letter", "tokens": [51252, 322, 867, 544, 3876, 11, 457, 286, 787, 528, 281, 312, 3102, 294, 729, 3876, 689, 264, 5063, 51518], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 312, "seek": 133502, "start": 1358.1, "end": 1360.3, "text": " S is at the beginning.", "tokens": [51518, 318, 307, 412, 264, 2863, 13, 51628], "temperature": 0.0, "avg_logprob": -0.12207380330787515, "compression_ratio": 1.8440366972477065, "no_speech_prob": 0.10657363384962082}, {"id": 313, "seek": 136030, "start": 1361.06, "end": 1366.8999999999999, "text": " If I want to see which lines have at the end, the letter N, I can put a dollar sign, and", "tokens": [50402, 759, 286, 528, 281, 536, 597, 3876, 362, 412, 264, 917, 11, 264, 5063, 426, 11, 286, 393, 829, 257, 7241, 1465, 11, 293, 50694], "temperature": 0.0, "avg_logprob": -0.19492953431372548, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.00370656861923635}, {"id": 314, "seek": 136030, "start": 1366.8999999999999, "end": 1371.86, "text": " it'll show me only the lines that end in the letter N. Let's try H. I was thinking bin,", "tokens": [50694, 309, 603, 855, 385, 787, 264, 3876, 300, 917, 294, 264, 5063, 426, 13, 961, 311, 853, 389, 13, 286, 390, 1953, 5171, 11, 50942], "temperature": 0.0, "avg_logprob": -0.19492953431372548, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.00370656861923635}, {"id": 315, "seek": 136030, "start": 1371.86, "end": 1374.22, "text": " but I think I meant bash.", "tokens": [50942, 457, 286, 519, 286, 4140, 46183, 13, 51060], "temperature": 0.0, "avg_logprob": -0.19492953431372548, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.00370656861923635}, {"id": 316, "seek": 136030, "start": 1374.22, "end": 1382.06, "text": " And so what it'll show you is that many of these user accounts use the bash shell.", "tokens": [51060, 400, 370, 437, 309, 603, 855, 291, 307, 300, 867, 295, 613, 4195, 9402, 764, 264, 46183, 8720, 13, 51452], "temperature": 0.0, "avg_logprob": -0.19492953431372548, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.00370656861923635}, {"id": 317, "seek": 136030, "start": 1382.06, "end": 1386.82, "text": " And let's say, notice in this case I'm seeing bash and SH.", "tokens": [51452, 400, 718, 311, 584, 11, 3449, 294, 341, 1389, 286, 478, 2577, 46183, 293, 7405, 13, 51690], "temperature": 0.0, "avg_logprob": -0.19492953431372548, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.00370656861923635}, {"id": 318, "seek": 138682, "start": 1386.86, "end": 1392.46, "text": " Let's say I'm only concerned about those users in this file that have the login shell bash.", "tokens": [50366, 961, 311, 584, 286, 478, 787, 5922, 466, 729, 5022, 294, 341, 3991, 300, 362, 264, 24276, 8720, 46183, 13, 50646], "temperature": 0.0, "avg_logprob": -0.15485098838806152, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.016398567706346512}, {"id": 319, "seek": 138682, "start": 1392.46, "end": 1395.22, "text": " So let's see how we can fix that.", "tokens": [50646, 407, 718, 311, 536, 577, 321, 393, 3191, 300, 13, 50784], "temperature": 0.0, "avg_logprob": -0.15485098838806152, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.016398567706346512}, {"id": 320, "seek": 138682, "start": 1395.22, "end": 1401.1399999999999, "text": " Instead of just saying H, I could say bash, and what this will say is the last letter", "tokens": [50784, 7156, 295, 445, 1566, 389, 11, 286, 727, 584, 46183, 11, 293, 437, 341, 486, 584, 307, 264, 1036, 5063, 51080], "temperature": 0.0, "avg_logprob": -0.15485098838806152, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.016398567706346512}, {"id": 321, "seek": 138682, "start": 1401.1399999999999, "end": 1407.9399999999998, "text": " on the line must be an H, but before it must come a B, A, and an S. And now I get exactly", "tokens": [51080, 322, 264, 1622, 1633, 312, 364, 389, 11, 457, 949, 309, 1633, 808, 257, 363, 11, 316, 11, 293, 364, 318, 13, 400, 586, 286, 483, 2293, 51420], "temperature": 0.0, "avg_logprob": -0.15485098838806152, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.016398567706346512}, {"id": 322, "seek": 138682, "start": 1407.9399999999998, "end": 1411.4199999999998, "text": " the information I want, lines that end with bash.", "tokens": [51420, 264, 1589, 286, 528, 11, 3876, 300, 917, 365, 46183, 13, 51594], "temperature": 0.0, "avg_logprob": -0.15485098838806152, "compression_ratio": 1.5954545454545455, "no_speech_prob": 0.016398567706346512}, {"id": 323, "seek": 141142, "start": 1411.42, "end": 1420.98, "text": " So that's the concept behind positional values within regular expressions.", "tokens": [50364, 407, 300, 311, 264, 3410, 2261, 2535, 304, 4190, 1951, 3890, 15277, 13, 50842], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 324, "seek": 141142, "start": 1420.98, "end": 1425.42, "text": " What I have here is a file called demo that contains two lines.", "tokens": [50842, 708, 286, 362, 510, 307, 257, 3991, 1219, 10723, 300, 8306, 732, 3876, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 325, "seek": 141142, "start": 1425.42, "end": 1429.18, "text": " One is JSON space, the space prof, and the other line is JSONium.", "tokens": [51064, 1485, 307, 31828, 1901, 11, 264, 1901, 1740, 11, 293, 264, 661, 1622, 307, 31828, 2197, 13, 51252], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 326, "seek": 141142, "start": 1429.18, "end": 1431.42, "text": " So maybe I have my own element.", "tokens": [51252, 407, 1310, 286, 362, 452, 1065, 4478, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 327, "seek": 141142, "start": 1431.42, "end": 1436.3000000000002, "text": " And if I grab this file for my name, what you're going to notice is that it shows me", "tokens": [51364, 400, 498, 286, 4444, 341, 3991, 337, 452, 1315, 11, 437, 291, 434, 516, 281, 3449, 307, 300, 309, 3110, 385, 51608], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 328, "seek": 141142, "start": 1436.3000000000002, "end": 1437.5, "text": " both lines.", "tokens": [51608, 1293, 3876, 13, 51668], "temperature": 0.0, "avg_logprob": -0.12682827843560113, "compression_ratio": 1.5345622119815667, "no_speech_prob": 0.009411279112100601}, {"id": 329, "seek": 143750, "start": 1437.58, "end": 1441.98, "text": " Maybe I only want to show those lines where it's actually my name and not my name embedded", "tokens": [50368, 2704, 286, 787, 528, 281, 855, 729, 3876, 689, 309, 311, 767, 452, 1315, 293, 406, 452, 1315, 16741, 50588], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 330, "seek": 143750, "start": 1441.98, "end": 1443.22, "text": " in another word.", "tokens": [50588, 294, 1071, 1349, 13, 50650], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 331, "seek": 143750, "start": 1443.22, "end": 1448.34, "text": " And the way I can do this is to add a word boundary to my regular expression.", "tokens": [50650, 400, 264, 636, 286, 393, 360, 341, 307, 281, 909, 257, 1349, 12866, 281, 452, 3890, 6114, 13, 50906], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 332, "seek": 143750, "start": 1448.34, "end": 1451.34, "text": " And remember slash B means word boundary.", "tokens": [50906, 400, 1604, 17330, 363, 1355, 1349, 12866, 13, 51056], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 333, "seek": 143750, "start": 1451.34, "end": 1459.34, "text": " And so now it will only show me the JSON that is up against a word boundary.", "tokens": [51056, 400, 370, 586, 309, 486, 787, 855, 385, 264, 31828, 300, 307, 493, 1970, 257, 1349, 12866, 13, 51456], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 334, "seek": 143750, "start": 1459.34, "end": 1463.66, "text": " So it's pretty helpful when you know you have spaces in a file and you want to find words", "tokens": [51456, 407, 309, 311, 1238, 4961, 562, 291, 458, 291, 362, 7673, 294, 257, 3991, 293, 291, 528, 281, 915, 2283, 51672], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 335, "seek": 143750, "start": 1463.66, "end": 1465.82, "text": " that you know are on a space.", "tokens": [51672, 300, 291, 458, 366, 322, 257, 1901, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09500107464489636, "compression_ratio": 1.7096774193548387, "no_speech_prob": 0.009124445728957653}, {"id": 336, "seek": 146582, "start": 1465.82, "end": 1470.06, "text": " The last thing for us to review is the concept of quantifiers in a regular expression.", "tokens": [50364, 440, 1036, 551, 337, 505, 281, 3131, 307, 264, 3410, 295, 4426, 23463, 294, 257, 3890, 6114, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 337, "seek": 146582, "start": 1470.06, "end": 1475.4199999999998, "text": " Sometimes you want to look for zero or one or two or three of a certain character to", "tokens": [50576, 4803, 291, 528, 281, 574, 337, 4018, 420, 472, 420, 732, 420, 1045, 295, 257, 1629, 2517, 281, 50844], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 338, "seek": 146582, "start": 1475.4199999999998, "end": 1477.1399999999999, "text": " appear in a certain spot.", "tokens": [50844, 4204, 294, 257, 1629, 4008, 13, 50930], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 339, "seek": 146582, "start": 1477.1399999999999, "end": 1481.4199999999998, "text": " Other times you want to know exactly a specific number of items that you want to appear in", "tokens": [50930, 5358, 1413, 291, 528, 281, 458, 2293, 257, 2685, 1230, 295, 4754, 300, 291, 528, 281, 4204, 294, 51144], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 340, "seek": 146582, "start": 1481.4199999999998, "end": 1484.34, "text": " a given spot in your regular expression.", "tokens": [51144, 257, 2212, 4008, 294, 428, 3890, 6114, 13, 51290], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 341, "seek": 146582, "start": 1484.34, "end": 1490.9399999999998, "text": " And so regular expressions come with the plus question mark and star operator.", "tokens": [51290, 400, 370, 3890, 15277, 808, 365, 264, 1804, 1168, 1491, 293, 3543, 12973, 13, 51620], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 342, "seek": 146582, "start": 1490.9399999999998, "end": 1495.58, "text": " And while the star operator seems to work like it does on the shell, it's important to", "tokens": [51620, 400, 1339, 264, 3543, 12973, 2544, 281, 589, 411, 309, 775, 322, 264, 8720, 11, 309, 311, 1021, 281, 51852], "temperature": 0.0, "avg_logprob": -0.08947537903092866, "compression_ratio": 1.8609022556390977, "no_speech_prob": 0.014500149525702}, {"id": 343, "seek": 149558, "start": 1495.58, "end": 1500.82, "text": " note that there is a slight difference between the file glob operator on the bash command", "tokens": [50364, 3637, 300, 456, 307, 257, 4036, 2649, 1296, 264, 3991, 16125, 12973, 322, 264, 46183, 5622, 50626], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 344, "seek": 149558, "start": 1500.82, "end": 1505.1799999999998, "text": " line as opposed to the star operator within a regular expression.", "tokens": [50626, 1622, 382, 8851, 281, 264, 3543, 12973, 1951, 257, 3890, 6114, 13, 50844], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 345, "seek": 149558, "start": 1505.1799999999998, "end": 1510.06, "text": " So how would we actually utilize these within some regular expressions?", "tokens": [50844, 407, 577, 576, 321, 767, 16117, 613, 1951, 512, 3890, 15277, 30, 51088], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 346, "seek": 149558, "start": 1510.06, "end": 1514.1, "text": " Well let's take a look at some sample regular expressions and then we'll go to the command", "tokens": [51088, 1042, 718, 311, 747, 257, 574, 412, 512, 6889, 3890, 15277, 293, 550, 321, 603, 352, 281, 264, 5622, 51290], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 347, "seek": 149558, "start": 1514.1, "end": 1515.9399999999998, "text": " line.", "tokens": [51290, 1622, 13, 51382], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 348, "seek": 149558, "start": 1515.9399999999998, "end": 1523.46, "text": " So here's a sample regular expression that matches a phone number in the order of 555-555-5555.", "tokens": [51382, 407, 510, 311, 257, 6889, 3890, 6114, 300, 10676, 257, 2593, 1230, 294, 264, 1668, 295, 12330, 20, 12, 13622, 20, 12, 13622, 13622, 13, 51758], "temperature": 0.0, "avg_logprob": -0.11839262008666993, "compression_ratio": 1.834061135371179, "no_speech_prob": 0.0023961947299540043}, {"id": 349, "seek": 152346, "start": 1523.46, "end": 1529.1000000000001, "text": " So typical US phone number, three digits dash, three digits dash, four digits.", "tokens": [50364, 407, 7476, 2546, 2593, 1230, 11, 1045, 27011, 8240, 11, 1045, 27011, 8240, 11, 1451, 27011, 13, 50646], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 350, "seek": 152346, "start": 1529.1000000000001, "end": 1533.58, "text": " And you'll notice that I've got some positional notations here.", "tokens": [50646, 400, 291, 603, 3449, 300, 286, 600, 658, 512, 2535, 304, 406, 763, 510, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 351, "seek": 152346, "start": 1533.58, "end": 1537.58, "text": " So the line starts with a carrot and ends with a dollar sign.", "tokens": [50870, 407, 264, 1622, 3719, 365, 257, 22767, 293, 5314, 365, 257, 7241, 1465, 13, 51070], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 352, "seek": 152346, "start": 1537.58, "end": 1541.1000000000001, "text": " And notice that I've got a character class zero through nine.", "tokens": [51070, 400, 3449, 300, 286, 600, 658, 257, 2517, 1508, 4018, 807, 4949, 13, 51246], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 353, "seek": 152346, "start": 1541.1000000000001, "end": 1546.58, "text": " And since I know I want to match three characters, I could actually type bracket zero dash nine", "tokens": [51246, 400, 1670, 286, 458, 286, 528, 281, 2995, 1045, 4342, 11, 286, 727, 767, 2010, 16904, 4018, 8240, 4949, 51520], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 354, "seek": 152346, "start": 1546.58, "end": 1551.5, "text": " bracket, bracket zero dash nine bracket, bracket zero dash nine bracket dash.", "tokens": [51520, 16904, 11, 16904, 4018, 8240, 4949, 16904, 11, 16904, 4018, 8240, 4949, 16904, 8240, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14935341950888945, "compression_ratio": 1.990950226244344, "no_speech_prob": 0.09007363766431808}, {"id": 355, "seek": 155150, "start": 1551.54, "end": 1557.38, "text": " Or notice in this case I can use curly bracket three curly bracket, which says match exactly", "tokens": [50366, 1610, 3449, 294, 341, 1389, 286, 393, 764, 32066, 16904, 1045, 32066, 16904, 11, 597, 1619, 2995, 2293, 50658], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 356, "seek": 155150, "start": 1557.38, "end": 1561.14, "text": " three of whatever you find before you.", "tokens": [50658, 1045, 295, 2035, 291, 915, 949, 291, 13, 50846], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 357, "seek": 155150, "start": 1561.14, "end": 1566.58, "text": " So we can actually use these curly bracket number, curly bracket notations to kind of", "tokens": [50846, 407, 321, 393, 767, 764, 613, 32066, 16904, 1230, 11, 32066, 16904, 406, 763, 281, 733, 295, 51118], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 358, "seek": 155150, "start": 1566.58, "end": 1569.58, "text": " make our regular expressions a little more condensed.", "tokens": [51118, 652, 527, 3890, 15277, 257, 707, 544, 36398, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 359, "seek": 155150, "start": 1569.58, "end": 1573.3, "text": " So we'll look at this on the command line in a second and discuss.", "tokens": [51268, 407, 321, 603, 574, 412, 341, 322, 264, 5622, 1622, 294, 257, 1150, 293, 2248, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 360, "seek": 155150, "start": 1573.3, "end": 1580.18, "text": " Also talked a little bit about the slash w slash b kind of type word boundary commands", "tokens": [51454, 2743, 2825, 257, 707, 857, 466, 264, 17330, 261, 17330, 272, 733, 295, 2010, 1349, 12866, 16901, 51798], "temperature": 0.0, "avg_logprob": -0.1227074440079506, "compression_ratio": 1.7634854771784232, "no_speech_prob": 0.0302079189568758}, {"id": 361, "seek": 158018, "start": 1580.18, "end": 1583.5, "text": " and the slash w character class commands.", "tokens": [50364, 293, 264, 17330, 261, 2517, 1508, 16901, 13, 50530], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 362, "seek": 158018, "start": 1583.5, "end": 1587.3, "text": " And you'll notice that I've written a really poor regular expression here that matches", "tokens": [50530, 400, 291, 603, 3449, 300, 286, 600, 3720, 257, 534, 4716, 3890, 6114, 510, 300, 10676, 50720], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 363, "seek": 158018, "start": 1587.3, "end": 1589.14, "text": " an email address.", "tokens": [50720, 364, 3796, 2985, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 364, "seek": 158018, "start": 1589.14, "end": 1592.8200000000002, "text": " Regular expression email address matching can be helpful, but it's not something I would", "tokens": [50812, 45659, 6114, 3796, 2985, 14324, 393, 312, 4961, 11, 457, 309, 311, 406, 746, 286, 576, 50996], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 365, "seek": 158018, "start": 1592.8200000000002, "end": 1597.38, "text": " rely on because there's a number of email address formats and it's too complicated to", "tokens": [50996, 10687, 322, 570, 456, 311, 257, 1230, 295, 3796, 2985, 25879, 293, 309, 311, 886, 6179, 281, 51224], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 366, "seek": 158018, "start": 1597.38, "end": 1599.5800000000002, "text": " really catch all of them.", "tokens": [51224, 534, 3745, 439, 295, 552, 13, 51334], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 367, "seek": 158018, "start": 1599.5800000000002, "end": 1605.1000000000001, "text": " If you do a Google search for email address regular expression, take a look at some of", "tokens": [51334, 759, 291, 360, 257, 3329, 3164, 337, 3796, 2985, 3890, 6114, 11, 747, 257, 574, 412, 512, 295, 51610], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 368, "seek": 158018, "start": 1605.1000000000001, "end": 1606.1000000000001, "text": " the answers that you get.", "tokens": [51610, 264, 6338, 300, 291, 483, 13, 51660], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 369, "seek": 158018, "start": 1606.1000000000001, "end": 1608.1000000000001, "text": " They're pretty long and detailed.", "tokens": [51660, 814, 434, 1238, 938, 293, 9942, 13, 51760], "temperature": 0.0, "avg_logprob": -0.13672966068073855, "compression_ratio": 1.8095238095238095, "no_speech_prob": 0.004754234571009874}, {"id": 370, "seek": 160810, "start": 1608.6599999999999, "end": 1613.1399999999999, "text": " But what you'll notice is this email address says it's also positionally situated.", "tokens": [50392, 583, 437, 291, 603, 3449, 307, 341, 3796, 2985, 1619, 309, 311, 611, 2535, 379, 30143, 13, 50616], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 371, "seek": 160810, "start": 1613.1399999999999, "end": 1615.1399999999999, "text": " So it's got a carat and a dollar sign.", "tokens": [50616, 407, 309, 311, 658, 257, 1032, 267, 293, 257, 7241, 1465, 13, 50716], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 372, "seek": 160810, "start": 1615.1399999999999, "end": 1617.34, "text": " So the only thing on this line should be an email address.", "tokens": [50716, 407, 264, 787, 551, 322, 341, 1622, 820, 312, 364, 3796, 2985, 13, 50826], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 373, "seek": 160810, "start": 1617.34, "end": 1621.02, "text": " It says find any number of word characters.", "tokens": [50826, 467, 1619, 915, 604, 1230, 295, 1349, 4342, 13, 51010], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 374, "seek": 160810, "start": 1621.02, "end": 1624.3, "text": " So you'll notice that the star in this case says, and you'll notice the same thing with", "tokens": [51010, 407, 291, 603, 3449, 300, 264, 3543, 294, 341, 1389, 1619, 11, 293, 291, 603, 3449, 264, 912, 551, 365, 51174], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 375, "seek": 160810, "start": 1624.3, "end": 1629.06, "text": " the number, the curly bracket three notation is it affects whatever is to the left of it.", "tokens": [51174, 264, 1230, 11, 264, 32066, 16904, 1045, 24657, 307, 309, 11807, 2035, 307, 281, 264, 1411, 295, 309, 13, 51412], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 376, "seek": 160810, "start": 1629.06, "end": 1634.98, "text": " So this is look for any letter or number or valid email address character and then star", "tokens": [51412, 407, 341, 307, 574, 337, 604, 5063, 420, 1230, 420, 7363, 3796, 2985, 2517, 293, 550, 3543, 51708], "temperature": 0.0, "avg_logprob": -0.16722908640295508, "compression_ratio": 1.8846153846153846, "no_speech_prob": 0.005384114105254412}, {"id": 377, "seek": 163498, "start": 1634.98, "end": 1641.3, "text": " after it means any number of valid email address characters, followed by an at, followed any", "tokens": [50364, 934, 309, 1355, 604, 1230, 295, 7363, 3796, 2985, 4342, 11, 6263, 538, 364, 412, 11, 6263, 604, 50680], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 378, "seek": 163498, "start": 1641.3, "end": 1647.06, "text": " number of valid characters, letters or numbers, followed by a dot, followed by any other number", "tokens": [50680, 1230, 295, 7363, 4342, 11, 7825, 420, 3547, 11, 6263, 538, 257, 5893, 11, 6263, 538, 604, 661, 1230, 50968], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 379, "seek": 163498, "start": 1647.06, "end": 1649.9, "text": " of valid letters or characters.", "tokens": [50968, 295, 7363, 7825, 420, 4342, 13, 51110], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 380, "seek": 163498, "start": 1649.9, "end": 1653.1, "text": " Notice also that because I actually wanted a dot here, I didn't want to look for, remember", "tokens": [51110, 13428, 611, 300, 570, 286, 767, 1415, 257, 5893, 510, 11, 286, 994, 380, 528, 281, 574, 337, 11, 1604, 51270], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 381, "seek": 163498, "start": 1653.1, "end": 1656.14, "text": " that a dot has a special meaning with regular expressions.", "tokens": [51270, 300, 257, 5893, 575, 257, 2121, 3620, 365, 3890, 15277, 13, 51422], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 382, "seek": 163498, "start": 1656.14, "end": 1661.3, "text": " Anytime you want to tell a regular expression to use the actual character and not interpret", "tokens": [51422, 39401, 291, 528, 281, 980, 257, 3890, 6114, 281, 764, 264, 3539, 2517, 293, 406, 7302, 51680], "temperature": 0.0, "avg_logprob": -0.13244383151714617, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.0023965281434357166}, {"id": 383, "seek": 166130, "start": 1661.3, "end": 1666.06, "text": " it as a special regular expression character, you just put a slash in front of it.", "tokens": [50364, 309, 382, 257, 2121, 3890, 6114, 2517, 11, 291, 445, 829, 257, 17330, 294, 1868, 295, 309, 13, 50602], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 384, "seek": 166130, "start": 1666.06, "end": 1670.58, "text": " So let's take a look at how some of these components can work on the command line.", "tokens": [50602, 407, 718, 311, 747, 257, 574, 412, 577, 512, 295, 613, 6677, 393, 589, 322, 264, 5622, 1622, 13, 50828], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 385, "seek": 166130, "start": 1670.58, "end": 1675.86, "text": " I have a file that might be the file you're using for your lab.", "tokens": [50828, 286, 362, 257, 3991, 300, 1062, 312, 264, 3991, 291, 434, 1228, 337, 428, 2715, 13, 51092], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 386, "seek": 166130, "start": 1675.86, "end": 1679.54, "text": " And if we look at the file lab3test.txt, what you're going to notice is it's a file full", "tokens": [51092, 400, 498, 321, 574, 412, 264, 3991, 2715, 18, 31636, 13, 83, 734, 11, 437, 291, 434, 516, 281, 3449, 307, 309, 311, 257, 3991, 1577, 51276], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 387, "seek": 166130, "start": 1679.54, "end": 1681.8999999999999, "text": " of names and phone numbers.", "tokens": [51276, 295, 5288, 293, 2593, 3547, 13, 51394], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 388, "seek": 166130, "start": 1681.8999999999999, "end": 1684.3799999999999, "text": " So we're just going to look at matching the phone numbers since I've already given you", "tokens": [51394, 407, 321, 434, 445, 516, 281, 574, 412, 14324, 264, 2593, 3547, 1670, 286, 600, 1217, 2212, 291, 51518], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 389, "seek": 166130, "start": 1684.3799999999999, "end": 1686.18, "text": " a regular expression that does that.", "tokens": [51518, 257, 3890, 6114, 300, 775, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 390, "seek": 166130, "start": 1686.18, "end": 1688.02, "text": " Let's see how that works.", "tokens": [51608, 961, 311, 536, 577, 300, 1985, 13, 51700], "temperature": 0.0, "avg_logprob": -0.11145217330367477, "compression_ratio": 1.7526501766784452, "no_speech_prob": 0.11591949313879013}, {"id": 391, "seek": 168802, "start": 1688.02, "end": 1694.82, "text": " I want to build a regular expression to match just valid phone numbers in this file.", "tokens": [50364, 286, 528, 281, 1322, 257, 3890, 6114, 281, 2995, 445, 7363, 2593, 3547, 294, 341, 3991, 13, 50704], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 392, "seek": 168802, "start": 1694.82, "end": 1698.42, "text": " So I'm going to use egrep and I'm going to put my pattern in here and I'm going to use", "tokens": [50704, 407, 286, 478, 516, 281, 764, 24263, 19919, 293, 286, 478, 516, 281, 829, 452, 5102, 294, 510, 293, 286, 478, 516, 281, 764, 50884], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 393, "seek": 168802, "start": 1698.42, "end": 1701.1, "text": " lab3test.txt as my file.", "tokens": [50884, 2715, 18, 31636, 13, 83, 734, 382, 452, 3991, 13, 51018], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 394, "seek": 168802, "start": 1701.1, "end": 1708.02, "text": " So before I said if I really wanted to match a phone number, I could write a regular expression", "tokens": [51018, 407, 949, 286, 848, 498, 286, 534, 1415, 281, 2995, 257, 2593, 1230, 11, 286, 727, 2464, 257, 3890, 6114, 51364], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 395, "seek": 168802, "start": 1708.02, "end": 1712.1399999999999, "text": " that looks like this.", "tokens": [51364, 300, 1542, 411, 341, 13, 51570], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 396, "seek": 168802, "start": 1712.1399999999999, "end": 1715.78, "text": " And if I just run that, I'm not going to type, notice it matches any line that has three", "tokens": [51570, 400, 498, 286, 445, 1190, 300, 11, 286, 478, 406, 516, 281, 2010, 11, 3449, 309, 10676, 604, 1622, 300, 575, 1045, 51752], "temperature": 0.0, "avg_logprob": -0.13114118576049805, "compression_ratio": 1.791111111111111, "no_speech_prob": 0.0017545424634590745}, {"id": 397, "seek": 171578, "start": 1715.78, "end": 1719.46, "text": " numbers in a row, or any instance of three numbers in a row.", "tokens": [50364, 3547, 294, 257, 5386, 11, 420, 604, 5197, 295, 1045, 3547, 294, 257, 5386, 13, 50548], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 398, "seek": 171578, "start": 1719.46, "end": 1723.1, "text": " But notice it also gets these kind of user IDs over here.", "tokens": [50548, 583, 3449, 309, 611, 2170, 613, 733, 295, 4195, 48212, 670, 510, 13, 50730], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 399, "seek": 171578, "start": 1723.1, "end": 1731.3799999999999, "text": " So then I could say as well, what if I match a dash, let me clear this, and then what if", "tokens": [50730, 407, 550, 286, 727, 584, 382, 731, 11, 437, 498, 286, 2995, 257, 8240, 11, 718, 385, 1850, 341, 11, 293, 550, 437, 498, 51144], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 400, "seek": 171578, "start": 1731.3799999999999, "end": 1732.3799999999999, "text": " I add a dash?", "tokens": [51144, 286, 909, 257, 8240, 30, 51194], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 401, "seek": 171578, "start": 1732.3799999999999, "end": 1736.8999999999999, "text": " Well now we're getting better, we're getting closer, because it's matching, it's no longer", "tokens": [51194, 1042, 586, 321, 434, 1242, 1101, 11, 321, 434, 1242, 4966, 11, 570, 309, 311, 14324, 11, 309, 311, 572, 2854, 51420], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 402, "seek": 171578, "start": 1736.8999999999999, "end": 1738.7, "text": " matching these IDs on the left.", "tokens": [51420, 14324, 613, 48212, 322, 264, 1411, 13, 51510], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 403, "seek": 171578, "start": 1738.7, "end": 1743.18, "text": " But it is matching these two multi-formatted phone numbers and by the way, if anybody's", "tokens": [51510, 583, 309, 307, 14324, 613, 732, 4825, 12, 837, 32509, 2593, 3547, 293, 538, 264, 636, 11, 498, 4472, 311, 51734], "temperature": 0.0, "avg_logprob": -0.19163237327386526, "compression_ratio": 1.7419354838709677, "no_speech_prob": 0.0043311831541359425}, {"id": 404, "seek": 174318, "start": 1743.18, "end": 1747.5, "text": " dealt with large amounts of data that was input by humans over a period of time, you've", "tokens": [50364, 15991, 365, 2416, 11663, 295, 1412, 300, 390, 4846, 538, 6255, 670, 257, 2896, 295, 565, 11, 291, 600, 50580], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 405, "seek": 174318, "start": 1747.5, "end": 1751.94, "text": " probably run into situations like this where you have inconsistent data entry.", "tokens": [50580, 1391, 1190, 666, 6851, 411, 341, 689, 291, 362, 36891, 1412, 8729, 13, 50802], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 406, "seek": 174318, "start": 1751.94, "end": 1754.66, "text": " So again you say, well I kind of just want these dash formats, because these are the", "tokens": [50802, 407, 797, 291, 584, 11, 731, 286, 733, 295, 445, 528, 613, 8240, 25879, 11, 570, 613, 366, 264, 50938], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 407, "seek": 174318, "start": 1754.66, "end": 1757.0600000000002, "text": " only ones I really want to identify.", "tokens": [50938, 787, 2306, 286, 534, 528, 281, 5876, 13, 51058], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 408, "seek": 174318, "start": 1757.0600000000002, "end": 1763.78, "text": " So let's clear this and go back and look at how to fix a regular expression.", "tokens": [51058, 407, 718, 311, 1850, 341, 293, 352, 646, 293, 574, 412, 577, 281, 3191, 257, 3890, 6114, 13, 51394], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 409, "seek": 174318, "start": 1763.78, "end": 1767.98, "text": " So what I could do is just add one more number and I'm getting better.", "tokens": [51394, 407, 437, 286, 727, 360, 307, 445, 909, 472, 544, 1230, 293, 286, 478, 1242, 1101, 13, 51604], "temperature": 0.0, "avg_logprob": -0.11624647952892163, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.001987642841413617}, {"id": 410, "seek": 176798, "start": 1767.98, "end": 1772.78, "text": " But still notice it's still matching out on these other numbers.", "tokens": [50364, 583, 920, 3449, 309, 311, 920, 14324, 484, 322, 613, 661, 3547, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 411, "seek": 176798, "start": 1772.78, "end": 1778.14, "text": " So you notice that these are greedy matches, in other words, it keeps trying to match things.", "tokens": [50604, 407, 291, 3449, 300, 613, 366, 28228, 10676, 11, 294, 661, 2283, 11, 309, 5965, 1382, 281, 2995, 721, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 412, "seek": 176798, "start": 1778.14, "end": 1783.3, "text": " So I can keep adding these until I get the total number of characters that I want, but", "tokens": [50872, 407, 286, 393, 1066, 5127, 613, 1826, 286, 483, 264, 3217, 1230, 295, 4342, 300, 286, 528, 11, 457, 51130], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 413, "seek": 176798, "start": 1783.3, "end": 1785.8600000000001, "text": " this is getting hard to read and getting long.", "tokens": [51130, 341, 307, 1242, 1152, 281, 1401, 293, 1242, 938, 13, 51258], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 414, "seek": 176798, "start": 1785.8600000000001, "end": 1786.94, "text": " So we just talked about characters.", "tokens": [51258, 407, 321, 445, 2825, 466, 4342, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 415, "seek": 176798, "start": 1786.94, "end": 1790.8600000000001, "text": " So I know my phone number is going to have three characters.", "tokens": [51312, 407, 286, 458, 452, 2593, 1230, 307, 516, 281, 362, 1045, 4342, 13, 51508], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 416, "seek": 176798, "start": 1790.8600000000001, "end": 1797.02, "text": " So what this says is, look for a value that could be any value between zero and nine,", "tokens": [51508, 407, 437, 341, 1619, 307, 11, 574, 337, 257, 2158, 300, 727, 312, 604, 2158, 1296, 4018, 293, 4949, 11, 51816], "temperature": 0.0, "avg_logprob": -0.1424081285121077, "compression_ratio": 1.833976833976834, "no_speech_prob": 0.11591999977827072}, {"id": 417, "seek": 179702, "start": 1797.58, "end": 1799.06, "text": " look for three of them in a row.", "tokens": [50392, 574, 337, 1045, 295, 552, 294, 257, 5386, 13, 50466], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 418, "seek": 179702, "start": 1799.06, "end": 1802.9, "text": " And they don't need to be the same number, like 222, it could be 215 or whatever.", "tokens": [50466, 400, 436, 500, 380, 643, 281, 312, 264, 912, 1230, 11, 411, 5853, 17, 11, 309, 727, 312, 5080, 20, 420, 2035, 13, 50658], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 419, "seek": 179702, "start": 1802.9, "end": 1805.46, "text": " Then follow that with a dash.", "tokens": [50658, 1396, 1524, 300, 365, 257, 8240, 13, 50786], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 420, "seek": 179702, "start": 1805.46, "end": 1810.66, "text": " Then look for another series of numbers, zero through nine, and look for three of those.", "tokens": [50786, 1396, 574, 337, 1071, 2638, 295, 3547, 11, 4018, 807, 4949, 11, 293, 574, 337, 1045, 295, 729, 13, 51046], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 421, "seek": 179702, "start": 1810.66, "end": 1812.58, "text": " Let's run it.", "tokens": [51046, 961, 311, 1190, 309, 13, 51142], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 422, "seek": 179702, "start": 1812.58, "end": 1815.62, "text": " And it's still being a little too greedy, so we're going to have to extend it out here", "tokens": [51142, 400, 309, 311, 920, 885, 257, 707, 886, 28228, 11, 370, 321, 434, 516, 281, 362, 281, 10101, 309, 484, 510, 51294], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 423, "seek": 179702, "start": 1815.62, "end": 1817.18, "text": " to get these four numbers.", "tokens": [51294, 281, 483, 613, 1451, 3547, 13, 51372], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 424, "seek": 179702, "start": 1817.18, "end": 1821.26, "text": " Also notice this file is a broken phone number in it.", "tokens": [51372, 2743, 3449, 341, 3991, 307, 257, 5463, 2593, 1230, 294, 309, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 425, "seek": 179702, "start": 1821.26, "end": 1823.5, "text": " That's there on purpose, but we'll skip that for this exercise.", "tokens": [51576, 663, 311, 456, 322, 4334, 11, 457, 321, 603, 10023, 300, 337, 341, 5380, 13, 51688], "temperature": 0.0, "avg_logprob": -0.16130682673767535, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.03209427744150162}, {"id": 426, "seek": 182350, "start": 1824.46, "end": 1830.9, "text": " Now I'm going to put another dash in, zero through nine, and I'm going to put the number four.", "tokens": [50412, 823, 286, 478, 516, 281, 829, 1071, 8240, 294, 11, 4018, 807, 4949, 11, 293, 286, 478, 516, 281, 829, 264, 1230, 1451, 13, 50734], "temperature": 0.0, "avg_logprob": -0.176924950253647, "compression_ratio": 1.8597285067873304, "no_speech_prob": 0.0032727394718676805}, {"id": 427, "seek": 182350, "start": 1830.9, "end": 1837.3, "text": " And so what this says is, find three numbers, followed by a dash, followed by three numbers,", "tokens": [50734, 400, 370, 437, 341, 1619, 307, 11, 915, 1045, 3547, 11, 6263, 538, 257, 8240, 11, 6263, 538, 1045, 3547, 11, 51054], "temperature": 0.0, "avg_logprob": -0.176924950253647, "compression_ratio": 1.8597285067873304, "no_speech_prob": 0.0032727394718676805}, {"id": 428, "seek": 182350, "start": 1837.3, "end": 1841.5, "text": " followed by a dash, followed by four numbers.", "tokens": [51054, 6263, 538, 257, 8240, 11, 6263, 538, 1451, 3547, 13, 51264], "temperature": 0.0, "avg_logprob": -0.176924950253647, "compression_ratio": 1.8597285067873304, "no_speech_prob": 0.0032727394718676805}, {"id": 429, "seek": 182350, "start": 1841.5, "end": 1847.94, "text": " By the way, notice I'm using eGrep for this, and notice that it gave me the valid answer.", "tokens": [51264, 3146, 264, 636, 11, 3449, 286, 478, 1228, 308, 38, 19919, 337, 341, 11, 293, 3449, 300, 309, 2729, 385, 264, 7363, 1867, 13, 51586], "temperature": 0.0, "avg_logprob": -0.176924950253647, "compression_ratio": 1.8597285067873304, "no_speech_prob": 0.0032727394718676805}, {"id": 430, "seek": 182350, "start": 1847.94, "end": 1853.46, "text": " If I use plain grep, I just want to point out that you'll get an error because in these", "tokens": [51586, 759, 286, 764, 11121, 6066, 79, 11, 286, 445, 528, 281, 935, 484, 300, 291, 603, 483, 364, 6713, 570, 294, 613, 51862], "temperature": 0.0, "avg_logprob": -0.176924950253647, "compression_ratio": 1.8597285067873304, "no_speech_prob": 0.0032727394718676805}, {"id": 431, "seek": 185346, "start": 1853.46, "end": 1857.42, "text": " curly brackets, there is a way to make them work in grep, but it's just easier to use", "tokens": [50364, 32066, 26179, 11, 456, 307, 257, 636, 281, 652, 552, 589, 294, 6066, 79, 11, 457, 309, 311, 445, 3571, 281, 764, 50562], "temperature": 0.0, "avg_logprob": -0.12412372165256076, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0007553080795332789}, {"id": 432, "seek": 185346, "start": 1857.42, "end": 1863.58, "text": " eGrep because these curly brackets are part of the extended grep syntax.", "tokens": [50562, 308, 38, 19919, 570, 613, 32066, 26179, 366, 644, 295, 264, 10913, 6066, 79, 28431, 13, 50870], "temperature": 0.0, "avg_logprob": -0.12412372165256076, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0007553080795332789}, {"id": 433, "seek": 185346, "start": 1863.58, "end": 1874.66, "text": " So just be sure that when you utilize numeric quantifiers that you utilize eGrep to do that.", "tokens": [50870, 407, 445, 312, 988, 300, 562, 291, 16117, 7866, 299, 4426, 23463, 300, 291, 16117, 308, 38, 19919, 281, 360, 300, 13, 51424], "temperature": 0.0, "avg_logprob": -0.12412372165256076, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0007553080795332789}, {"id": 434, "seek": 185346, "start": 1874.66, "end": 1881.06, "text": " Now let's look at the plus question mark and star operators, and we'll go back to searching", "tokens": [51424, 823, 718, 311, 574, 412, 264, 1804, 1168, 1491, 293, 3543, 19077, 11, 293, 321, 603, 352, 646, 281, 10808, 51744], "temperature": 0.0, "avg_logprob": -0.12412372165256076, "compression_ratio": 1.5806451612903225, "no_speech_prob": 0.0007553080795332789}, {"id": 435, "seek": 188106, "start": 1881.06, "end": 1883.78, "text": " through files in the dictionary file.", "tokens": [50364, 807, 7098, 294, 264, 25890, 3991, 13, 50500], "temperature": 0.0, "avg_logprob": -0.16603095110724955, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.027578072622418404}, {"id": 436, "seek": 188106, "start": 1883.78, "end": 1895.22, "text": " So let's say I wanted to find some words that have, start with a B, have an O, and then", "tokens": [50500, 407, 718, 311, 584, 286, 1415, 281, 915, 512, 2283, 300, 362, 11, 722, 365, 257, 363, 11, 362, 364, 422, 11, 293, 550, 51072], "temperature": 0.0, "avg_logprob": -0.16603095110724955, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.027578072622418404}, {"id": 437, "seek": 188106, "start": 1895.22, "end": 1899.58, "text": " have the letter T, like bot.", "tokens": [51072, 362, 264, 5063, 314, 11, 411, 10592, 13, 51290], "temperature": 0.0, "avg_logprob": -0.16603095110724955, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.027578072622418404}, {"id": 438, "seek": 188106, "start": 1899.58, "end": 1906.58, "text": " So I get a lot of things, saboteurs, robots, lobotomy, but what's interesting about that", "tokens": [51290, 407, 286, 483, 257, 688, 295, 721, 11, 5560, 1370, 2156, 11, 14733, 11, 14366, 310, 8488, 11, 457, 437, 311, 1880, 466, 300, 51640], "temperature": 0.0, "avg_logprob": -0.16603095110724955, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.027578072622418404}, {"id": 439, "seek": 188106, "start": 1906.58, "end": 1909.8999999999999, "text": " is it's definitely more information than I want.", "tokens": [51640, 307, 309, 311, 2138, 544, 1589, 813, 286, 528, 13, 51806], "temperature": 0.0, "avg_logprob": -0.16603095110724955, "compression_ratio": 1.5287958115183247, "no_speech_prob": 0.027578072622418404}, {"id": 440, "seek": 190990, "start": 1909.9, "end": 1919.9, "text": " So let's say I'm curious about if there are words that have zero or one O's in them.", "tokens": [50364, 407, 718, 311, 584, 286, 478, 6369, 466, 498, 456, 366, 2283, 300, 362, 4018, 420, 472, 422, 311, 294, 552, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 441, "seek": 190990, "start": 1919.9, "end": 1927.8600000000001, "text": " And if I put a question mark, a question mark will say in this specific case that the O,", "tokens": [50864, 400, 498, 286, 829, 257, 1168, 1491, 11, 257, 1168, 1491, 486, 584, 294, 341, 2685, 1389, 300, 264, 422, 11, 51262], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 442, "seek": 190990, "start": 1927.8600000000001, "end": 1930.94, "text": " there could be zero O's or there could be one O's.", "tokens": [51262, 456, 727, 312, 4018, 422, 311, 420, 456, 727, 312, 472, 422, 311, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 443, "seek": 190990, "start": 1930.94, "end": 1932.8600000000001, "text": " Let's see if we can make any changes.", "tokens": [51416, 961, 311, 536, 498, 321, 393, 652, 604, 2962, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 444, "seek": 190990, "start": 1932.8600000000001, "end": 1933.8600000000001, "text": " And I do.", "tokens": [51512, 400, 286, 360, 13, 51562], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 445, "seek": 190990, "start": 1933.8600000000001, "end": 1938.8600000000001, "text": " Notice that it matches subtropical and it matches turbot.", "tokens": [51562, 13428, 300, 309, 10676, 7257, 1513, 804, 293, 309, 10676, 18252, 310, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1885442539137237, "compression_ratio": 1.6417910447761195, "no_speech_prob": 0.0008558477275073528}, {"id": 446, "seek": 193886, "start": 1938.86, "end": 1944.74, "text": " It says, well, match any characters where there is an O or not an O.", "tokens": [50364, 467, 1619, 11, 731, 11, 2995, 604, 4342, 689, 456, 307, 364, 422, 420, 406, 364, 422, 13, 50658], "temperature": 0.0, "avg_logprob": -0.1765482924705328, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013215038925409317}, {"id": 447, "seek": 193886, "start": 1944.74, "end": 1948.54, "text": " There could be zero O's or there could be one O.", "tokens": [50658, 821, 727, 312, 4018, 422, 311, 420, 456, 727, 312, 472, 422, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1765482924705328, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013215038925409317}, {"id": 448, "seek": 193886, "start": 1948.54, "end": 1953.9799999999998, "text": " The plus sign will match one or more of a character.", "tokens": [50848, 440, 1804, 1465, 486, 2995, 472, 420, 544, 295, 257, 2517, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1765482924705328, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013215038925409317}, {"id": 449, "seek": 193886, "start": 1953.9799999999998, "end": 1957.86, "text": " So in this case, it brings in toll booth as well as sabotage and robot.", "tokens": [51120, 407, 294, 341, 1389, 11, 309, 5607, 294, 16629, 20912, 382, 731, 382, 37167, 609, 293, 7881, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1765482924705328, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013215038925409317}, {"id": 450, "seek": 193886, "start": 1957.86, "end": 1963.6999999999998, "text": " So notice there's two O's there, one O there.", "tokens": [51314, 407, 3449, 456, 311, 732, 422, 311, 456, 11, 472, 422, 456, 13, 51606], "temperature": 0.0, "avg_logprob": -0.1765482924705328, "compression_ratio": 1.6457142857142857, "no_speech_prob": 0.013215038925409317}, {"id": 451, "seek": 196370, "start": 1963.7, "end": 1972.78, "text": " And then finally, we could use the star, which means zero to infinity.", "tokens": [50364, 400, 550, 2721, 11, 321, 727, 764, 264, 3543, 11, 597, 1355, 4018, 281, 13202, 13, 50818], "temperature": 0.0, "avg_logprob": -0.10823097620924858, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005554075352847576}, {"id": 452, "seek": 196370, "start": 1972.78, "end": 1984.26, "text": " And so notice that star actually gives us the two O's, zero O's, and one O.", "tokens": [50818, 400, 370, 3449, 300, 3543, 767, 2709, 505, 264, 732, 422, 311, 11, 4018, 422, 311, 11, 293, 472, 422, 13, 51392], "temperature": 0.0, "avg_logprob": -0.10823097620924858, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005554075352847576}, {"id": 453, "seek": 196370, "start": 1984.26, "end": 1989.1000000000001, "text": " Up to this point, we've been using grep to filter information so we can target stuff", "tokens": [51392, 5858, 281, 341, 935, 11, 321, 600, 668, 1228, 6066, 79, 281, 6608, 1589, 370, 321, 393, 3779, 1507, 51634], "temperature": 0.0, "avg_logprob": -0.10823097620924858, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005554075352847576}, {"id": 454, "seek": 196370, "start": 1989.1000000000001, "end": 1991.06, "text": " that we want in more detail.", "tokens": [51634, 300, 321, 528, 294, 544, 2607, 13, 51732], "temperature": 0.0, "avg_logprob": -0.10823097620924858, "compression_ratio": 1.4942528735632183, "no_speech_prob": 0.005554075352847576}, {"id": 455, "seek": 199106, "start": 1991.06, "end": 1994.86, "text": " But we can also use regular expressions to find things and modify them.", "tokens": [50364, 583, 321, 393, 611, 764, 3890, 15277, 281, 915, 721, 293, 16927, 552, 13, 50554], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 456, "seek": 199106, "start": 1994.86, "end": 1996.78, "text": " The set command is the stream editor.", "tokens": [50554, 440, 992, 5622, 307, 264, 4309, 9839, 13, 50650], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 457, "seek": 199106, "start": 1996.78, "end": 2000.62, "text": " And while it can be a very powerful tool, what I want to do here is just kind of introduce", "tokens": [50650, 400, 1339, 309, 393, 312, 257, 588, 4005, 2290, 11, 437, 286, 528, 281, 360, 510, 307, 445, 733, 295, 5366, 50842], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 458, "seek": 199106, "start": 2000.62, "end": 2005.22, "text": " how it works very basically so we have an idea how we can use regular expressions in", "tokens": [50842, 577, 309, 1985, 588, 1936, 370, 321, 362, 364, 1558, 577, 321, 393, 764, 3890, 15277, 294, 51072], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 459, "seek": 199106, "start": 2005.22, "end": 2006.8999999999999, "text": " a new and different way.", "tokens": [51072, 257, 777, 293, 819, 636, 13, 51156], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 460, "seek": 199106, "start": 2006.8999999999999, "end": 2013.1799999999998, "text": " So what this allows us to do is we can basically use the set editor to take stream of data", "tokens": [51156, 407, 437, 341, 4045, 505, 281, 360, 307, 321, 393, 1936, 764, 264, 992, 9839, 281, 747, 4309, 295, 1412, 51470], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 461, "seek": 199106, "start": 2013.1799999999998, "end": 2019.3, "text": " and match against a pattern and then modify that information that matches that pattern", "tokens": [51470, 293, 2995, 1970, 257, 5102, 293, 550, 16927, 300, 1589, 300, 10676, 300, 5102, 51776], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 462, "seek": 199106, "start": 2019.3, "end": 2020.3, "text": " in some way.", "tokens": [51776, 294, 512, 636, 13, 51826], "temperature": 0.0, "avg_logprob": -0.09061968912843799, "compression_ratio": 1.8218181818181818, "no_speech_prob": 0.23897212743759155}, {"id": 463, "seek": 202030, "start": 2020.74, "end": 2024.86, "text": " One obvious way you could use this is to go through a bunch of files and find the old", "tokens": [50386, 1485, 6322, 636, 291, 727, 764, 341, 307, 281, 352, 807, 257, 3840, 295, 7098, 293, 915, 264, 1331, 50592], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 464, "seek": 202030, "start": 2024.86, "end": 2028.06, "text": " boss's name and replace it with the new boss's name.", "tokens": [50592, 5741, 311, 1315, 293, 7406, 309, 365, 264, 777, 5741, 311, 1315, 13, 50752], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 465, "seek": 202030, "start": 2028.06, "end": 2030.26, "text": " It's one way that I've actually used this.", "tokens": [50752, 467, 311, 472, 636, 300, 286, 600, 767, 1143, 341, 13, 50862], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 466, "seek": 202030, "start": 2030.26, "end": 2033.02, "text": " And there's a couple other things that are useful, so it's a great way to find something", "tokens": [50862, 400, 456, 311, 257, 1916, 661, 721, 300, 366, 4420, 11, 370, 309, 311, 257, 869, 636, 281, 915, 746, 51000], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 467, "seek": 202030, "start": 2033.02, "end": 2034.02, "text": " and replace it.", "tokens": [51000, 293, 7406, 309, 13, 51050], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 468, "seek": 202030, "start": 2034.02, "end": 2035.94, "text": " So you can use set to find and replace stuff.", "tokens": [51050, 407, 291, 393, 764, 992, 281, 915, 293, 7406, 1507, 13, 51146], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 469, "seek": 202030, "start": 2035.94, "end": 2039.48, "text": " It's a really powerful tool and we're going to only look at one aspect of it.", "tokens": [51146, 467, 311, 257, 534, 4005, 2290, 293, 321, 434, 516, 281, 787, 574, 412, 472, 4171, 295, 309, 13, 51323], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 470, "seek": 202030, "start": 2039.48, "end": 2044.54, "text": " So we're going to look at a command very similar to this one that's in this example.", "tokens": [51323, 407, 321, 434, 516, 281, 574, 412, 257, 5622, 588, 2531, 281, 341, 472, 300, 311, 294, 341, 1365, 13, 51576], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 471, "seek": 202030, "start": 2044.54, "end": 2046.6599999999999, "text": " Let's jump over to the command line.", "tokens": [51576, 961, 311, 3012, 670, 281, 264, 5622, 1622, 13, 51682], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 472, "seek": 202030, "start": 2046.6599999999999, "end": 2049.02, "text": " Let's take a look at how set works.", "tokens": [51682, 961, 311, 747, 257, 574, 412, 577, 992, 1985, 13, 51800], "temperature": 0.0, "avg_logprob": -0.12091563019571425, "compression_ratio": 1.880794701986755, "no_speech_prob": 0.015899360179901123}, {"id": 473, "seek": 204902, "start": 2049.02, "end": 2051.06, "text": " For this to work, we're going to actually need a file to edit.", "tokens": [50364, 1171, 341, 281, 589, 11, 321, 434, 516, 281, 767, 643, 257, 3991, 281, 8129, 13, 50466], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 474, "seek": 204902, "start": 2051.06, "end": 2053.38, "text": " And so we're not going to actually edit the file in place today.", "tokens": [50466, 400, 370, 321, 434, 406, 516, 281, 767, 8129, 264, 3991, 294, 1081, 965, 13, 50582], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 475, "seek": 204902, "start": 2053.38, "end": 2057.94, "text": " We're just going to dump data out to standard output and what we're going to do is just", "tokens": [50582, 492, 434, 445, 516, 281, 11430, 1412, 484, 281, 3832, 5598, 293, 437, 321, 434, 516, 281, 360, 307, 445, 50810], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 476, "seek": 204902, "start": 2057.94, "end": 2060.62, "text": " modify what's printed to standard output.", "tokens": [50810, 16927, 437, 311, 13567, 281, 3832, 5598, 13, 50944], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 477, "seek": 204902, "start": 2060.62, "end": 2064.02, "text": " We could obviously redirect that back to another file.", "tokens": [50944, 492, 727, 2745, 29066, 300, 646, 281, 1071, 3991, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 478, "seek": 204902, "start": 2064.02, "end": 2068.02, "text": " We could actually edit the file in place, but these are all some more advanced things.", "tokens": [51114, 492, 727, 767, 8129, 264, 3991, 294, 1081, 11, 457, 613, 366, 439, 512, 544, 7339, 721, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 479, "seek": 204902, "start": 2068.02, "end": 2070.58, "text": " We just want to look at using regular expressions in a new way.", "tokens": [51314, 492, 445, 528, 281, 574, 412, 1228, 3890, 15277, 294, 257, 777, 636, 13, 51442], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 480, "seek": 204902, "start": 2070.58, "end": 2075.9, "text": " So I am going to actually utilize the Etsy password file for this.", "tokens": [51442, 407, 286, 669, 516, 281, 767, 16117, 264, 47170, 88, 11524, 3991, 337, 341, 13, 51708], "temperature": 0.0, "avg_logprob": -0.11745380999437019, "compression_ratio": 1.9272727272727272, "no_speech_prob": 0.004904803819954395}, {"id": 481, "seek": 207590, "start": 2075.9, "end": 2079.82, "text": " And actually, if we just look at that file real quick, which you'll notice is that everything", "tokens": [50364, 400, 767, 11, 498, 321, 445, 574, 412, 300, 3991, 957, 1702, 11, 597, 291, 603, 3449, 307, 300, 1203, 50560], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 482, "seek": 207590, "start": 2079.82, "end": 2082.54, "text": " in here is separated by colons.", "tokens": [50560, 294, 510, 307, 12005, 538, 1173, 892, 13, 50696], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 483, "seek": 207590, "start": 2082.54, "end": 2085.2200000000003, "text": " So let's say I want to get rid of those colons.", "tokens": [50696, 407, 718, 311, 584, 286, 528, 281, 483, 3973, 295, 729, 1173, 892, 13, 50830], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 484, "seek": 207590, "start": 2085.2200000000003, "end": 2086.2200000000003, "text": " So let's use set.", "tokens": [50830, 407, 718, 311, 764, 992, 13, 50880], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 485, "seek": 207590, "start": 2086.2200000000003, "end": 2087.86, "text": " We'll get rid of this.", "tokens": [50880, 492, 603, 483, 3973, 295, 341, 13, 50962], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 486, "seek": 207590, "start": 2087.86, "end": 2092.62, "text": " So set dash E and dash E means I'm going to give set an expression.", "tokens": [50962, 407, 992, 8240, 462, 293, 8240, 462, 1355, 286, 478, 516, 281, 976, 992, 364, 6114, 13, 51200], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 487, "seek": 207590, "start": 2092.62, "end": 2095.6600000000003, "text": " In this case, I'm going to use single quotes and a couple of things.", "tokens": [51200, 682, 341, 1389, 11, 286, 478, 516, 281, 764, 2167, 19963, 293, 257, 1916, 295, 721, 13, 51352], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 488, "seek": 207590, "start": 2095.6600000000003, "end": 2099.1, "text": " I'm going to set this up and talk a little bit about what it means.", "tokens": [51352, 286, 478, 516, 281, 992, 341, 493, 293, 751, 257, 707, 857, 466, 437, 309, 1355, 13, 51524], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 489, "seek": 207590, "start": 2099.1, "end": 2103.86, "text": " Notice I've got single quote S slash slash slash G single quote.", "tokens": [51524, 13428, 286, 600, 658, 2167, 6513, 318, 17330, 17330, 17330, 460, 2167, 6513, 13, 51762], "temperature": 0.0, "avg_logprob": -0.16589772360665458, "compression_ratio": 1.8195488721804511, "no_speech_prob": 0.04207231104373932}, {"id": 490, "seek": 210386, "start": 2103.86, "end": 2107.58, "text": " So whatever I want to find goes between the first two slashes.", "tokens": [50364, 407, 2035, 286, 528, 281, 915, 1709, 1296, 264, 700, 732, 1061, 12808, 13, 50550], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 491, "seek": 210386, "start": 2107.58, "end": 2110.6200000000003, "text": " So in this case, I'm saying find the colon.", "tokens": [50550, 407, 294, 341, 1389, 11, 286, 478, 1566, 915, 264, 8255, 13, 50702], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 492, "seek": 210386, "start": 2110.6200000000003, "end": 2114.2200000000003, "text": " And now I'm going to say replace it with three dashes.", "tokens": [50702, 400, 586, 286, 478, 516, 281, 584, 7406, 309, 365, 1045, 8240, 279, 13, 50882], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 493, "seek": 210386, "start": 2114.2200000000003, "end": 2118.3, "text": " And what this will do is, said we'll go through the file, find every instance, and by the", "tokens": [50882, 400, 437, 341, 486, 360, 307, 11, 848, 321, 603, 352, 807, 264, 3991, 11, 915, 633, 5197, 11, 293, 538, 264, 51086], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 494, "seek": 210386, "start": 2118.3, "end": 2120.6600000000003, "text": " way, G stands for global.", "tokens": [51086, 636, 11, 460, 7382, 337, 4338, 13, 51204], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 495, "seek": 210386, "start": 2120.6600000000003, "end": 2124.02, "text": " And that means it will find every instance of a colon in the file and replace it with", "tokens": [51204, 400, 300, 1355, 309, 486, 915, 633, 5197, 295, 257, 8255, 294, 264, 3991, 293, 7406, 309, 365, 51372], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 496, "seek": 210386, "start": 2124.02, "end": 2125.02, "text": " three dashes.", "tokens": [51372, 1045, 8240, 279, 13, 51422], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 497, "seek": 210386, "start": 2125.02, "end": 2129.98, "text": " And if you look at my output, now instead of colons, you see a bunch of three dashes.", "tokens": [51422, 400, 498, 291, 574, 412, 452, 5598, 11, 586, 2602, 295, 1173, 892, 11, 291, 536, 257, 3840, 295, 1045, 8240, 279, 13, 51670], "temperature": 0.0, "avg_logprob": -0.11611718119996967, "compression_ratio": 1.8821138211382114, "no_speech_prob": 0.00023781666823197156}, {"id": 498, "seek": 212998, "start": 2129.98, "end": 2135.42, "text": " So this can be really helpful for a bunch of reasons.", "tokens": [50364, 407, 341, 393, 312, 534, 4961, 337, 257, 3840, 295, 4112, 13, 50636], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 499, "seek": 212998, "start": 2135.42, "end": 2139.26, "text": " One of the things that we can do too is we can use all of our new found regular expression", "tokens": [50636, 1485, 295, 264, 721, 300, 321, 393, 360, 886, 307, 321, 393, 764, 439, 295, 527, 777, 1352, 3890, 6114, 50828], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 500, "seek": 212998, "start": 2139.26, "end": 2140.42, "text": " powers in this file.", "tokens": [50828, 8674, 294, 341, 3991, 13, 50886], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 501, "seek": 212998, "start": 2140.42, "end": 2145.82, "text": " So if we take another look at that password file, one of the things you'll notice at the", "tokens": [50886, 407, 498, 321, 747, 1071, 574, 412, 300, 11524, 3991, 11, 472, 295, 264, 721, 291, 603, 3449, 412, 264, 51156], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 502, "seek": 212998, "start": 2145.82, "end": 2148.94, "text": " top is, let's say there's a user called bin.", "tokens": [51156, 1192, 307, 11, 718, 311, 584, 456, 311, 257, 4195, 1219, 5171, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 503, "seek": 212998, "start": 2148.94, "end": 2152.54, "text": " But then you'll also notice that the word bin shows up a lot of times in this file.", "tokens": [51312, 583, 550, 291, 603, 611, 3449, 300, 264, 1349, 5171, 3110, 493, 257, 688, 295, 1413, 294, 341, 3991, 13, 51492], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 504, "seek": 212998, "start": 2152.54, "end": 2155.7400000000002, "text": " Let's say I just want to change that user bin.", "tokens": [51492, 961, 311, 584, 286, 445, 528, 281, 1319, 300, 4195, 5171, 13, 51652], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 505, "seek": 212998, "start": 2155.7400000000002, "end": 2159.02, "text": " But I don't want to change any other instance of bin.", "tokens": [51652, 583, 286, 500, 380, 528, 281, 1319, 604, 661, 5197, 295, 5171, 13, 51816], "temperature": 0.0, "avg_logprob": -0.10805213182492364, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.07156497240066528}, {"id": 506, "seek": 215902, "start": 2159.02, "end": 2164.62, "text": " So what I could do is, go back up to my previous command, and what I'm going to do is, instead", "tokens": [50364, 407, 437, 286, 727, 360, 307, 11, 352, 646, 493, 281, 452, 3894, 5622, 11, 293, 437, 286, 478, 516, 281, 360, 307, 11, 2602, 50644], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 507, "seek": 215902, "start": 2164.62, "end": 2168.5, "text": " of just match bin, which would match everything, I'm going to actually go ahead and what am", "tokens": [50644, 295, 445, 2995, 5171, 11, 597, 576, 2995, 1203, 11, 286, 478, 516, 281, 767, 352, 2286, 293, 437, 669, 50838], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 508, "seek": 215902, "start": 2168.5, "end": 2169.5, "text": " I going to replace bin with?", "tokens": [50838, 286, 516, 281, 7406, 5171, 365, 30, 50888], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 509, "seek": 215902, "start": 2169.5, "end": 2172.46, "text": " I'm going to replace it with JSON, really big.", "tokens": [50888, 286, 478, 516, 281, 7406, 309, 365, 31828, 11, 534, 955, 13, 51036], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 510, "seek": 215902, "start": 2172.46, "end": 2179.3, "text": " So if I just do that, notice it replaces every instance of bin in this file with the word", "tokens": [51036, 407, 498, 286, 445, 360, 300, 11, 3449, 309, 46734, 633, 5197, 295, 5171, 294, 341, 3991, 365, 264, 1349, 51378], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 511, "seek": 215902, "start": 2179.3, "end": 2180.3, "text": " JSON.", "tokens": [51378, 31828, 13, 51428], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 512, "seek": 215902, "start": 2180.3, "end": 2183.94, "text": " It replaced the one I want, but then it replaced all these other instances.", "tokens": [51428, 467, 10772, 264, 472, 286, 528, 11, 457, 550, 309, 10772, 439, 613, 661, 14519, 13, 51610], "temperature": 0.0, "avg_logprob": -0.1343498553259898, "compression_ratio": 1.8158995815899581, "no_speech_prob": 0.00781493540853262}, {"id": 513, "seek": 218394, "start": 2183.94, "end": 2191.94, "text": " So what I want to do is, use an anchor.", "tokens": [50364, 407, 437, 286, 528, 281, 360, 307, 11, 764, 364, 18487, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 514, "seek": 218394, "start": 2191.94, "end": 2196.2200000000003, "text": " So I'm going to say, only match the bin that's at the start of the line.", "tokens": [50764, 407, 286, 478, 516, 281, 584, 11, 787, 2995, 264, 5171, 300, 311, 412, 264, 722, 295, 264, 1622, 13, 50978], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 515, "seek": 218394, "start": 2196.2200000000003, "end": 2200.46, "text": " And now you'll notice that all the other bins are left in place, but if I scroll back up", "tokens": [50978, 400, 586, 291, 603, 3449, 300, 439, 264, 661, 41275, 366, 1411, 294, 1081, 11, 457, 498, 286, 11369, 646, 493, 51190], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 516, "seek": 218394, "start": 2200.46, "end": 2205.1, "text": " and look, it only replaced the one that was at the start of the line.", "tokens": [51190, 293, 574, 11, 309, 787, 10772, 264, 472, 300, 390, 412, 264, 722, 295, 264, 1622, 13, 51422], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 517, "seek": 218394, "start": 2205.1, "end": 2209.14, "text": " So said it's a really powerful tool and we'll take a look at it more as the semester progresses.", "tokens": [51422, 407, 848, 309, 311, 257, 534, 4005, 2290, 293, 321, 603, 747, 257, 574, 412, 309, 544, 382, 264, 11894, 41929, 13, 51624], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 518, "seek": 218394, "start": 2209.14, "end": 2213.58, "text": " But in this case, I just want to show you that you can use your new found regular expression", "tokens": [51624, 583, 294, 341, 1389, 11, 286, 445, 528, 281, 855, 291, 300, 291, 393, 764, 428, 777, 1352, 3890, 6114, 51846], "temperature": 0.0, "avg_logprob": -0.14670954810248482, "compression_ratio": 1.7011070110701108, "no_speech_prob": 0.0057299016043543816}, {"id": 519, "seek": 221358, "start": 2213.58, "end": 2217.7799999999997, "text": " powers, not just a filter data for viewing, but filter data for editing.", "tokens": [50364, 8674, 11, 406, 445, 257, 6608, 1412, 337, 17480, 11, 457, 6608, 1412, 337, 10000, 13, 50574], "temperature": 0.0, "avg_logprob": -0.26864463404605265, "compression_ratio": 1.1612903225806452, "no_speech_prob": 0.26564735174179077}], "language": "en"}