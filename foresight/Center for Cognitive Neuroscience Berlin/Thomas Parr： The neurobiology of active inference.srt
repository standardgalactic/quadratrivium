1
00:00:00,000 --> 00:00:14,320
It's a great pleasure to introduce Thomas Paar today, we'll be about his background.

2
00:00:14,320 --> 00:00:25,160
So he started medical school in 2012, I guess, at the UCL and did also like a bachelor there

3
00:00:25,160 --> 00:00:35,080
with a focus in new science. In 2016, he started his PhD, I hope everything is correct, he started

4
00:00:35,080 --> 00:00:45,400
his PhD working together with Carl Friston and Garen Rees. So I think Thomas is really like the

5
00:00:45,400 --> 00:00:53,520
person who most frustrated me, when I was at the at the Philly, so a long time ago. I was fascinated

6
00:00:53,520 --> 00:01:01,400
by like the idea of the free entry principle and Carl Friston and really started going, learning

7
00:01:01,400 --> 00:01:09,480
about Boltzmann equations and some kind of like message passing and so on and over years and years.

8
00:01:09,480 --> 00:01:20,600
And then in 2016-17, I realized that suddenly a number of papers were coming out, it became much

9
00:01:20,680 --> 00:01:28,120
more accessible and on these papers there were always like one name, Thomas Paar. And so I looked

10
00:01:28,120 --> 00:01:35,240
into that and I realized that there is one person out there who is so young and who plus

11
00:01:35,240 --> 00:01:46,040
started his PhD, who was able to go all through this extremely complicated math and ideas so easily

12
00:01:46,520 --> 00:01:53,000
that I thought I should really give up thinking about the free energy principle at all. And more

13
00:01:53,000 --> 00:01:59,240
impressively, I think a couple of months ago, even like a book came out like about like Active

14
00:01:59,240 --> 00:02:07,320
Influence, where he is like the first author on and I really can recommend this book. It's a

15
00:02:07,320 --> 00:02:13,240
fantastic introduction to Active Influence, to the free energy principle. So it's a really,

16
00:02:13,240 --> 00:02:20,680
really great pleasure to have you here tonight, Thomas, and I'm really looking forward to your

17
00:02:20,680 --> 00:02:29,960
talk. Thanks for joining. Well, thank you very much for the very nice invitation and the very

18
00:02:29,960 --> 00:02:39,320
nice introduction as well. First of all, can you hear me okay? Yes. And next thing is to see

19
00:02:39,320 --> 00:02:48,120
whether I can successfully share my screen. Let's try this. So I'm assuming you can see the

20
00:02:48,120 --> 00:02:58,120
PowerPoint and now hopefully in full screen. Perfect. Great. So today, I realized that you've

21
00:02:58,120 --> 00:03:05,640
already had a talk from Ryan Smith previously, so I'm assuming he's given an excellent overview of

22
00:03:05,640 --> 00:03:11,880
Active Influences as he always does. And I wanted to take the opportunity to focus in on something

23
00:03:11,880 --> 00:03:19,800
a little bit more specific in this talk, which is thinking about how dealing with Active Influence

24
00:03:19,800 --> 00:03:25,640
lets us think about neurobiology and how we can start to make connections and form neurobiological

25
00:03:25,640 --> 00:03:34,360
theories from the principles. So I'll start just by giving an introduction that I think

26
00:03:34,440 --> 00:03:39,160
will hopefully give everybody a little bit of a refresher on Active Influence and the basic

27
00:03:39,160 --> 00:03:44,440
principles. And although there are some technical elements to it, I'll try to make it as intuitive

28
00:03:44,440 --> 00:03:48,760
as possible. And it really doesn't matter if you get all the technical detail through this,

29
00:03:48,760 --> 00:03:56,680
it's just to build some intuitions. So I tend to like to start talks with this slide, which just

30
00:03:56,680 --> 00:04:03,320
shows two different sorts of system. The one on the left is one that just gradually diffuses over

31
00:04:03,320 --> 00:04:08,920
time, sort of loses form becomes progressively less interesting. Whereas the one on the right,

32
00:04:08,920 --> 00:04:13,800
despite having the same amount of randomness built into it, manages to maintain its form over time.

33
00:04:14,520 --> 00:04:19,080
And a useful starting point in thinking about Active Influence is thinking about

34
00:04:19,720 --> 00:04:25,800
what the difference between these two sorts of systems are and how the one on the right is able

35
00:04:25,800 --> 00:04:34,120
to maintain its form and resist the effect of these random fluctuations that are forcing it

36
00:04:34,120 --> 00:04:40,760
in all sorts of different directions. And it's often useful to reformulate this in terms of the

37
00:04:40,760 --> 00:04:46,520
probability density or the change in the probability distribution of all of these little particles

38
00:04:46,520 --> 00:04:50,120
over time. So again, you can see on the left, we've got something that's just diffusing out into

39
00:04:50,120 --> 00:04:55,080
nothing. And on the right, we get something that behaves from a probabilistic level pretty much

40
00:04:55,080 --> 00:05:01,080
statically. And it's really those on the right that we're interested in, those biological creatures

41
00:05:01,080 --> 00:05:05,560
are able to maintain their form, are able to resist the effects of what the environment does to them.

42
00:05:06,600 --> 00:05:11,080
Now, if we sort of write down the kinds of dynamics that we'd need for a single particle

43
00:05:11,080 --> 00:05:17,080
or a single part of the system to try and maintain this form, what we get to is a system that I think

44
00:05:17,080 --> 00:05:23,720
quite intuitively always moves when it can from regions of low probability to regions of high

45
00:05:23,720 --> 00:05:28,440
probability. So if we interpret this final distribution it gets to as some steady state,

46
00:05:29,240 --> 00:05:34,280
all we need to do to maintain that in a random environment is to always try and climb uphill

47
00:05:34,280 --> 00:05:38,520
at the probability gradients. And then almost by definition, we end up spending more time in

48
00:05:38,520 --> 00:05:45,480
highly probable states and less time in more improbable states. And this is sort of the starting

49
00:05:45,480 --> 00:05:50,840
point really for active inference, because we now have this notion that we're climbing probability

50
00:05:50,840 --> 00:05:55,240
gradients and we have some distribution that we're effectively trying to maximize.

51
00:05:56,200 --> 00:06:02,600
And we can link that back to ideas like the Bayesian brain, the idea that the brain is using some

52
00:06:02,600 --> 00:06:10,200
model of the world around it to generate predictions. And then is drawing inferences about

53
00:06:10,200 --> 00:06:15,480
the data that it actually obtains, so sensory information coming in through our eyes, ears,

54
00:06:15,480 --> 00:06:20,840
through our skin, that we can then use to draw inferences about the causes of those data.

55
00:06:22,440 --> 00:06:26,920
So here we've got the idea that there's some states of the world X that are causing some

56
00:06:26,920 --> 00:06:34,600
sensory data Y. We're then forming some posterior beliefs, so beliefs about the causes given with

57
00:06:34,600 --> 00:06:41,480
the data. And we're doing this using a generative model that comprises a likelihood in a prior,

58
00:06:41,480 --> 00:06:45,240
so prior being how plausible are the things out there in the world before we've

59
00:06:45,240 --> 00:06:51,160
made any observations. The likelihood being how likely the observations we've made are given

60
00:06:51,160 --> 00:06:56,040
the states of the world or given our hypotheses about what caused them. And together we refer

61
00:06:56,040 --> 00:07:01,800
to these two things as a generative model. Now the posterior distribution is what happens when

62
00:07:01,800 --> 00:07:08,120
we invert that model where we find the probability of some causes given the data.

63
00:07:09,080 --> 00:07:14,120
And the final term we've got at the end here is referred to either as an evidence or a marginal

64
00:07:14,120 --> 00:07:19,080
likelihood. And that's because in the context of Bayesian statistics we often use a marginal

65
00:07:19,080 --> 00:07:25,400
likelihood as a measure of the fit of a model to the data that it's trying to explain. There's

66
00:07:25,400 --> 00:07:31,080
how much evidence do those data afford some model of the world. And together we can think of these

67
00:07:31,080 --> 00:07:35,960
as being an inversion of the generative model. Now the reason I've put this here is that we

68
00:07:35,960 --> 00:07:43,880
can see that the dynamics that I'm showing in the upper left can be interpreted as the process of

69
00:07:43,880 --> 00:07:48,840
maximising the evidence for some model, maximising the fit between the brain's model of how the world

70
00:07:48,840 --> 00:07:55,400
works and how the world then engages with the model or engages with the brain by presenting it data.

71
00:07:56,760 --> 00:08:01,320
And broadly there are two ways of doing that. The first is, as we've already spoken about,

72
00:08:01,320 --> 00:08:05,880
it's changing your beliefs based upon new data such that you get a better fit to the world.

73
00:08:06,680 --> 00:08:11,480
The other way, which I think is one of the key ideas in Active Inference, is that you

74
00:08:11,480 --> 00:08:16,360
generate actions based upon your beliefs that change the world to make it more like your model.

75
00:08:17,880 --> 00:08:23,160
And this is sort of the key idea that underwrites Active Inference, that all we're trying to do

76
00:08:23,160 --> 00:08:28,680
is maximise the evidence for some model of our world. And we can either do that through

77
00:08:28,680 --> 00:08:34,360
perception by changing our model or through action by changing the world. But together,

78
00:08:34,360 --> 00:08:40,200
they come under one single objective, sometimes referred to, particularly by people like Kevin

79
00:08:40,200 --> 00:08:48,120
the Acapoway, as self-evidencing. Now I want to go a little bit into the structure of generative

80
00:08:48,120 --> 00:08:53,560
models and specifically the ways we can think about and the ways we can notate generative models,

81
00:08:54,440 --> 00:08:59,240
because it often helps to move from the slightly more mathematical abstract description to a more

82
00:08:59,240 --> 00:09:04,440
graphical notation that I think often gives a much better intuitive sense of what's going on.

83
00:09:05,240 --> 00:09:10,280
So to do that, I'm going to start with this idea that model evidence is effectively what we get out

84
00:09:10,280 --> 00:09:16,920
once we've integrated out all of the causes from our model, by which I mean if you take

85
00:09:16,920 --> 00:09:22,440
account of all of the things our model predicts and then we take into account the prior probabilities

86
00:09:22,440 --> 00:09:29,160
of all of the things that are causing those data, we can then work out what the probability of the

87
00:09:29,160 --> 00:09:35,640
data are under that model. But the model itself takes account of the things that are being generated,

88
00:09:35,640 --> 00:09:39,960
so our sensory data, and also the things that are causing them, simply a joint probability

89
00:09:39,960 --> 00:09:44,840
distribution involving all of these things. So I'm now interpreting this graphic up on

90
00:09:44,840 --> 00:09:50,520
the left as depending upon some generative model where we've effectively integrated out everything

91
00:09:50,520 --> 00:09:58,760
that we don't need to determine explicitly. Now generative models will often have some

92
00:09:58,760 --> 00:10:02,920
interesting structure, and in fact if they don't have interesting structures then they're not

93
00:10:02,920 --> 00:10:09,000
interesting generative models. And normally that structure manifests as a factorization

94
00:10:09,000 --> 00:10:14,440
of this joint distribution, but not everything depends upon everything else. And so we can

95
00:10:14,440 --> 00:10:19,080
often factorize it, and here I'm just showing a completely arbitrary factorization of some

96
00:10:19,080 --> 00:10:25,960
generative model where we have certain dependencies, so y depends directly upon x1 and x2, but not

97
00:10:25,960 --> 00:10:31,640
directly upon x3 and so on. And the reason it's useful to think about this factorization is because

98
00:10:31,640 --> 00:10:37,320
we can then express a graphical version of this model that provides a bit more intuition as to

99
00:10:37,320 --> 00:10:42,840
what's going on. So the way we do that, or one way that we can do that, is using something known as a

100
00:10:42,840 --> 00:10:47,720
factor graph. And the idea is that we take each of these factors in turn, we draw a square,

101
00:10:49,160 --> 00:10:55,160
and then we draw an arrow to whatever's on the left of the probability factor we're interested in.

102
00:10:55,160 --> 00:11:00,360
So here an arrow towards the y, because we're saying the probability of y conditioned upon or

103
00:11:00,360 --> 00:11:06,360
depending upon the other two things, and then we attach those to the same square. We then move

104
00:11:06,360 --> 00:11:11,800
on to the next factor, draw another square, and we then attach the relevant variables together

105
00:11:13,000 --> 00:11:18,040
in exactly the same way. And we carry on doing that until we have a picture of what our model is

106
00:11:18,040 --> 00:11:22,120
like. So now instead of having to look at this equation, you can look at this and say, okay,

107
00:11:22,120 --> 00:11:30,440
well x3 causes x1, x4 causes x2, and together x1 and x2 relate to or together generate our data y.

108
00:11:33,240 --> 00:11:38,360
This is just to demonstrate several models you may be familiar with generally, and the fact that we

109
00:11:38,360 --> 00:11:41,960
we're often implicitly using these kinds of generative models without necessarily thinking

110
00:11:41,960 --> 00:11:46,920
about it. So when we're performing a principal components analysis, we're often taking some

111
00:11:48,840 --> 00:11:53,720
distribution of some variable x, we're then mapping that to a higher dimensional space y,

112
00:11:55,000 --> 00:12:00,280
and that's our sort of model of how the data that we're working with are generated.

113
00:12:02,120 --> 00:12:05,960
So what we can then do when we perform a principal components analysis is take our

114
00:12:05,960 --> 00:12:11,720
high dimensional data y, work out how you go back from the y to the x, and that gives you all of your

115
00:12:11,720 --> 00:12:15,880
specific principal components and the directions that have been stretched in various different ways.

116
00:12:17,400 --> 00:12:21,160
Same principle applies for things like canonical variance analysis, where you've got some set of

117
00:12:21,720 --> 00:12:25,320
variables then mapped to two different sorts of data by stretching them and

118
00:12:26,120 --> 00:12:30,200
distorting them in various ways. And the challenge is how do you get back to the common

119
00:12:31,560 --> 00:12:37,240
hidden variable common cause for both of those datasets. And we do the same sort of thing with

120
00:12:37,240 --> 00:12:42,280
something like clustering analysis, where here the model says we're effectively sampling from

121
00:12:42,280 --> 00:12:46,920
one of several different clusters with different probabilities. We're then generating some point

122
00:12:46,920 --> 00:12:53,960
in space depending upon which cluster it falls in. And the process of inference is again undoing

123
00:12:53,960 --> 00:12:59,000
this. It's finding the posterior by taking a point in a cluster and saying, well, which cluster is

124
00:12:59,000 --> 00:13:06,120
that from? So going from the y to the s. So that's a sort of brief introduction to the idea of

125
00:13:06,120 --> 00:13:11,560
generative models, the the role they play in active inference, some key examples and some of

126
00:13:11,560 --> 00:13:16,600
the notation that I'm going to use as we go forward. What I've said so far has been relatively

127
00:13:16,600 --> 00:13:23,080
abstract. And in the next section, I want to try and work from those sort of general principles

128
00:13:23,080 --> 00:13:27,880
and these slightly abstract notions through to something more specific and more neurobiological.

129
00:13:29,000 --> 00:13:33,160
Now, the first thing we're going to do to get there is to introduce one more abstract concept,

130
00:13:33,160 --> 00:13:38,360
which is that of a Markov blanket. A Markov blanket comes up in all sorts of places in active

131
00:13:38,360 --> 00:13:43,160
inference, but I'm going to use it in a very specific way here. And this is to talk about

132
00:13:43,160 --> 00:13:48,840
conditional independence. And what I mean by conditional independence is that if you have

133
00:13:51,400 --> 00:13:55,160
two different sets of variables, so here I've noted them as the mu and the eta,

134
00:13:56,120 --> 00:14:02,760
the Markov blanket B is the set of set of variables that render the other two completely

135
00:14:02,760 --> 00:14:06,680
independent to one another, that mean that if you know everything about the blanket,

136
00:14:06,680 --> 00:14:09,880
knowing something about mu tells you nothing new about eta.

137
00:14:12,040 --> 00:14:16,520
To give you an example of this, one of the examples that most people are familiar with

138
00:14:16,520 --> 00:14:22,600
is the idea of a Markov chain, where you have a sequence of events in time. So something that

139
00:14:22,600 --> 00:14:27,400
happens in the past then influences the present, which then influences the future with no direct

140
00:14:27,400 --> 00:14:32,760
influence from the past to the future without going via the present. So if I know everything

141
00:14:32,760 --> 00:14:36,680
there is to know about the present, knowing something about the past tells me nothing new

142
00:14:36,680 --> 00:14:41,640
about the future and vice versa. So in that sense, the present is the Markov blanket that

143
00:14:41,640 --> 00:14:47,880
separates the past from the future. Now, this is a very useful concept when we're dealing with

144
00:14:47,880 --> 00:14:52,600
graphical models or generative models that have some interesting factorization structure,

145
00:14:52,600 --> 00:14:59,560
and I'll try and explain why. The Markov blanket is often referred to as or can often be identified

146
00:14:59,560 --> 00:15:04,440
by identifying variable we're interested in, so let's say we're interested in the variable x2,

147
00:15:05,080 --> 00:15:11,480
then you find the parents of x2, so the things that caused it, the children of x2, so the things

148
00:15:11,480 --> 00:15:17,240
it causes, and the parents of its children, and that gives you the Markov blanket of x2.

149
00:15:18,600 --> 00:15:25,240
Now the significance of this is that knowing this means that if we know about the blanket,

150
00:15:25,240 --> 00:15:28,600
it doesn't matter what else is going on in the generative model, you know, this might be some

151
00:15:28,600 --> 00:15:34,200
tiny section of a huge generative model with many, many variables, but we don't need to know about

152
00:15:34,200 --> 00:15:38,760
all those variables, all we need to know about are the blanket variables, so we can effectively

153
00:15:38,760 --> 00:15:46,200
ignore everything else if what we're interested in is x2. Now, how does this then matter for

154
00:15:46,200 --> 00:15:52,440
neurobiology? Well, I think the answer to that is that the brain is an extremely sparse structure,

155
00:15:52,440 --> 00:15:56,840
that synaptic message passing does not involve connections between every different neuron in

156
00:15:56,840 --> 00:16:01,240
the brain to every other neuron in the brain. There are a small number of, relatively speaking,

157
00:16:01,240 --> 00:16:08,360
of synapses between any neuron and its neighbors or neurons elsewhere in the brain,

158
00:16:09,160 --> 00:16:13,240
and so we can make an argument that if the brain is performing inference about lots of different

159
00:16:13,240 --> 00:16:19,160
variables, all it needs is to know the structure of the generative model and the variables that

160
00:16:19,160 --> 00:16:23,560
sit in that Markov blanket, or at least the neurons that are representing those other variables.

161
00:16:24,600 --> 00:16:28,440
And that tells us that we can then, when we're inverting that model, when we're performing

162
00:16:28,440 --> 00:16:35,080
inference, what I'm showing here is just one example of an inference scheme, we can, we can

163
00:16:35,080 --> 00:16:43,080
express the dynamics of that neuronal message passing in terms of connections from the other

164
00:16:43,080 --> 00:16:49,960
neural populations that are representing the variables in that Markov blanket. The details

165
00:16:49,960 --> 00:16:55,560
of this aren't that important, but this is one example of an inference scheme known as variational

166
00:16:55,560 --> 00:17:01,640
message passing, here formulated as a gradient scheme in terms of some dynamics, which has

167
00:17:01,640 --> 00:17:07,000
relevance for modeling neural populations, we're interested in how the dynamics of those

168
00:17:07,000 --> 00:17:13,000
populations evolve over time. And we can sort of do this recursively, we can couple together

169
00:17:13,000 --> 00:17:19,560
lots of different parts of this system, where the things that are being connected up are just the

170
00:17:19,560 --> 00:17:25,560
Markov blankets of different populations. Here I've expressed it in terms of an error term,

171
00:17:25,640 --> 00:17:33,000
this epsilon, which effectively represents the gradient of a free energy objective,

172
00:17:33,000 --> 00:17:37,720
which is used as an approximation to that marginal likelihood or evidence that I spoke

173
00:17:37,720 --> 00:17:43,800
about earlier. And then some beliefs about or expectations about that variable, so we can now

174
00:17:43,800 --> 00:17:49,400
use our, the errors in our predictions to update those variables. And once we've written down the

175
00:17:49,400 --> 00:17:54,440
dynamics of this sort of system, we can simulate things like firing rates, we can simulate the

176
00:17:54,440 --> 00:18:00,920
dynamics and the behavior of these networks of neurons passing messages between one another

177
00:18:01,720 --> 00:18:07,400
as a means of performing a form of inference and thus self evidencing.

178
00:18:10,280 --> 00:18:14,520
I know that this is probably still seemingly relatively abstract, but I think what will

179
00:18:14,520 --> 00:18:18,200
hopefully help is if we then go through some examples thinking about specific kinds of

180
00:18:18,200 --> 00:18:23,720
generative model and how that might then affect the kind of message passing we would see in a

181
00:18:23,720 --> 00:18:28,920
brain. And the first step to thinking about that I think is very important when we think about

182
00:18:28,920 --> 00:18:33,720
really useful generative models is they will all typically have a temporal aspect to them that

183
00:18:33,720 --> 00:18:39,320
as biological creatures we deal with things that evolve in time. And so it's worth thinking about

184
00:18:39,320 --> 00:18:43,240
how do you formulate a generative model that has that kind of dynamic aspect to it.

185
00:18:44,760 --> 00:18:50,280
And there are several different answers to that and one of them is we use something known as

186
00:18:50,360 --> 00:18:57,320
as a Taylor series approximation. So we start by saying okay at the current time what is the

187
00:18:58,520 --> 00:19:03,080
value of some variable x in the world and there's maybe some continuous variable it may be

188
00:19:03,080 --> 00:19:08,760
where my arm is in space. If we then want to know how it's going to evolve in time we could then say

189
00:19:08,760 --> 00:19:12,600
well let's take the next element of our Taylor series approximation that's taking out of its

190
00:19:12,600 --> 00:19:16,440
velocity and that tells us a little bit more about the trajectory that my arm might be on.

191
00:19:17,400 --> 00:19:22,920
We can then take another value which is the current acceleration of that position and we get

192
00:19:22,920 --> 00:19:29,160
a slightly better approximation to the trajectory. And the more terms we add in the greater approximation

193
00:19:29,160 --> 00:19:33,240
we have of the trajectory or the greater representation we have of the trajectory just as

194
00:19:33,240 --> 00:19:38,360
a series of numbers where those numbers are my current position my current velocity acceleration

195
00:19:38,360 --> 00:19:43,640
etc. So we could formulate a generative model by trying to predict each of these coefficients

196
00:19:43,720 --> 00:19:48,920
of this Taylor series or each of these generalized coordinates of motion as they're sometimes referred

197
00:19:48,920 --> 00:19:55,000
to. An alternative is we just say at time one where will I be at time two where will I be and

198
00:19:55,000 --> 00:20:02,920
we simply represent it in terms of the sequence of points over time. Often when we're discretizing

199
00:20:02,920 --> 00:20:07,720
time in this way we often we also discretize space so we might say okay where am I in some

200
00:20:07,800 --> 00:20:13,320
discretized scheme am I in location one two three four or five at each different time point

201
00:20:14,120 --> 00:20:20,680
and so we have two different ways of accounting for how things might evolve over time and each

202
00:20:20,680 --> 00:20:25,400
of these can be useful in slightly different circumstances. So if I were dealing with a

203
00:20:25,400 --> 00:20:31,160
sequential decision-making task it may be much more efficient for me to say okay at the first

204
00:20:31,160 --> 00:20:34,760
move I'm going to make is this then the second one is this the third one is this and dealing

205
00:20:34,760 --> 00:20:40,040
with something very sequential that might also be important in in sort of language processing

206
00:20:40,040 --> 00:20:43,880
where you have to think which words you're going to put together in a sequence and we're dealing

207
00:20:43,880 --> 00:20:50,040
with discrete variables over time because words are categorical instead of opposed to

208
00:20:50,040 --> 00:20:56,760
continuing as opposed to lying on a continuum. However if I were dealing with movement or

209
00:20:56,760 --> 00:21:02,120
the responses I'm getting from some photoreceptor in my retina it may be much more sensible for me

210
00:21:02,120 --> 00:21:07,400
to be working in continuous time thinking about how things evolve at a continuous scale.

211
00:21:08,840 --> 00:21:13,640
So what I'm showing here is an example of a generative model and the associated message

212
00:21:13,640 --> 00:21:19,720
passing scheme when formulated in terms of this continuous time scheme. So what we've got here

213
00:21:19,720 --> 00:21:27,400
is a factor graph at the top this blue element that uses exactly the same sort of formulation that

214
00:21:27,400 --> 00:21:35,640
we've been discussing so far so we have some variable here which might represent our position

215
00:21:35,640 --> 00:21:41,160
which then predicts our velocity at that time which then predicts the acceleration at that time.

216
00:21:41,160 --> 00:21:46,920
So we have a series of predictions here about the coupling between different orders of motion

217
00:21:47,720 --> 00:21:52,680
at each point those are predicting some sort of data here are wise which represents the

218
00:21:53,640 --> 00:21:58,040
the current position of our sensory data the current velocity the current acceleration.

219
00:21:58,840 --> 00:22:04,200
What I'm showing lower down here is the message passing scheme we get when we take account of

220
00:22:04,200 --> 00:22:10,040
the Markov blankets of each of these variables so the Markov blanket here for the the velocity

221
00:22:10,040 --> 00:22:14,440
would include the position and the acceleration but also the data I've got about those things

222
00:22:15,240 --> 00:22:21,400
and some prior beliefs about about other variables in our model and so we'd end up connecting those

223
00:22:21,480 --> 00:22:26,600
things up and although it looks like a bit of a mess of connections it's still fewer connections

224
00:22:26,600 --> 00:22:30,840
than the total number connections you would get if you match everything to everything else.

225
00:22:32,360 --> 00:22:36,760
What we've got here takes the form of effectively a predictive coding style scheme

226
00:22:36,760 --> 00:22:42,040
which people may be familiar with and the idea that that you can you can hierarchically predict

227
00:22:43,640 --> 00:22:48,200
predict some data and you can predict the thing that's predicting that data

228
00:22:48,200 --> 00:22:54,680
those data and by observing the errors in your prediction of the data you update your prediction

229
00:22:54,680 --> 00:23:00,680
which then cascades up a hierarchy allowing you to update subsequent predictions.

230
00:23:02,440 --> 00:23:07,960
What I'm showing you here is the equivalent model formulated in terms of discrete time and in this

231
00:23:07,960 --> 00:23:14,200
case discrete space as well so here we have a state at time t minus one which generates a

232
00:23:14,200 --> 00:23:19,320
state at time t which generates a state at time t plus one at each of those time points we're

233
00:23:19,320 --> 00:23:25,320
generating some observable outcomes which are represented as the o's with a variable pi here

234
00:23:25,320 --> 00:23:31,000
that represents alternative trajectories I could I could pursue so alternative policies or plans

235
00:23:31,000 --> 00:23:39,160
alternative ways I could change the sequence of events and again we have underneath the message

236
00:23:39,160 --> 00:23:45,800
passing scheme that could perform inferences about this sort of model and you can see that the

237
00:23:45,800 --> 00:23:52,600
message passing scheme to some extent looks like an inversion or or as loosely a sort of mirror

238
00:23:52,600 --> 00:23:57,560
image of the generative model because all it's doing is taking each step that was performed

239
00:23:57,560 --> 00:24:02,200
to generate some data and then inverting those steps one at a time so it's just going backwards

240
00:24:02,280 --> 00:24:10,280
from the data to arrive at the causes of those data. Now what I've got shown here on the left

241
00:24:11,240 --> 00:24:19,240
is just a sort of cartoon image of some of the key connections in in cortical micro circuits

242
00:24:19,240 --> 00:24:26,280
so here we've got superficial pyramidal cells as spiny stellate cells as SS deep pyramidal

243
00:24:26,280 --> 00:24:34,840
cells as DP and inhibitory engineer arms as the double eyes and this is clearly an oversimplification

244
00:24:34,840 --> 00:24:41,640
of what is a very complex connectivity structure but the reason I'm showing this here is that

245
00:24:42,440 --> 00:24:47,720
if we know something about this connectivity structure if we know about some of the key patterns

246
00:24:47,720 --> 00:24:52,360
we can start to ask how can the the sorts of models that we've spoken about over the last

247
00:24:52,360 --> 00:24:58,120
couple of slides then be interpreted in terms of the micro circuits that could help

248
00:24:59,800 --> 00:25:05,640
could help implement these in a biological system. So what I'm going to show in the middle

249
00:25:05,640 --> 00:25:13,240
is an example of a continuous states-based predictive coding style model that has all

250
00:25:13,240 --> 00:25:17,400
the same elements as in the slide I showed you before but are now mapped so that some of the

251
00:25:17,400 --> 00:25:24,280
connections coming in and out of it loosely map to those associated with a known cortical anatomy

252
00:25:25,880 --> 00:25:30,600
and we can do exactly the same thing here with the discrete states-based model and think about how

253
00:25:30,600 --> 00:25:36,440
we can assign those roles associated with the roles of different cortical layers

254
00:25:37,480 --> 00:25:43,560
and then use that as a way of forming hypotheses. Now what I'm showing you on this slide is not

255
00:25:43,880 --> 00:25:48,280
necessarily the last word on this and you could formulate several different hypotheses about how

256
00:25:48,280 --> 00:25:52,840
these two things map together and that's where a lot of interesting science happens. This is just

257
00:25:52,840 --> 00:25:58,680
one example of how we can associate what we know about cortical microcircuitry and anatomy

258
00:25:58,680 --> 00:26:03,320
with what we know about the anatomy of message passing for some simple generic forms of generative

259
00:26:03,320 --> 00:26:08,520
model. What I'm going to try and do over the rest of this presentation is to take the connections

260
00:26:08,600 --> 00:26:15,480
coming out of this out of each of these points and to try and think about how those might or the

261
00:26:15,480 --> 00:26:20,600
roles they might those might play when we take an active entrance approach and the sorts of structures

262
00:26:20,600 --> 00:26:27,720
in the brain that might be involved in dealing with those dealing with those things. I'm going to

263
00:26:27,720 --> 00:26:33,000
start by talking about movement and reflexes so here the kinds of things we're interested in

264
00:26:33,080 --> 00:26:39,560
are the connections that leave the cortex and go to areas like the spinal cord in the motor

265
00:26:39,560 --> 00:26:48,360
neurons, the predictions that we can then make about what we're seeing here in the central

266
00:26:48,360 --> 00:26:54,280
plot and the G that's been circled in red is a prediction about the sort of data I might expect

267
00:26:54,280 --> 00:26:59,240
to observe which may here be proprioceptive data consequent on particular sorts of action.

268
00:26:59,800 --> 00:27:05,160
And we can think of this quite simply if we think about the structure of a reflex arc.

269
00:27:06,200 --> 00:27:11,880
So what I've got here on the lower part is just the same marginal likelihood or model evidence

270
00:27:11,880 --> 00:27:17,000
that we've dealt with before this idea of it being a measure of the fit of the model both in

271
00:27:17,000 --> 00:27:24,280
terms of its accuracy, the accuracy with which it predicts some data and the simplest least complex

272
00:27:24,280 --> 00:27:29,640
version of the model that will work for that. What do we do when we change the data? Well we try and

273
00:27:29,640 --> 00:27:36,680
make the data more accurate in relation to our model by trying to align the data with our predictions.

274
00:27:37,400 --> 00:27:43,320
So when we think of the structure of a reflex arc where we can interpret it as some descending

275
00:27:43,320 --> 00:27:47,960
prediction coming from the motor cortex saying what is the proprioceptive data I expect to

276
00:27:47,960 --> 00:27:53,800
observe. By comparing that with the proprioceptive data that's actually coming into the spinal cord

277
00:27:53,800 --> 00:27:59,560
through the dorsal horn with that prediction we then get a potentially a mismatch between the two

278
00:27:59,560 --> 00:28:06,120
that can be used to generate action via the ventral horn generating movements and contractions and

279
00:28:06,120 --> 00:28:13,160
muscles that then try to fulfill these predictions. I'll show you just an example of a simulation of

280
00:28:15,000 --> 00:28:20,280
an arm using active inference that has these sort of reflex arcs built in and all we're doing here

281
00:28:20,280 --> 00:28:26,360
is we're just tapping on the biceps tendon where the red arrow is pointing and we're just seeing

282
00:28:26,360 --> 00:28:32,360
the reflexive response that we're getting as a consequence of effectively inducing this additional

283
00:28:32,360 --> 00:28:38,200
piece of sensory proprioceptive information by stretching that tendon. We induce the mismatch

284
00:28:38,200 --> 00:28:44,600
between the prediction that's coming down and the data that's coming out and that

285
00:28:45,400 --> 00:28:51,320
is corrected then by generating this additional movement. Interestingly we can do things by

286
00:28:51,320 --> 00:28:56,360
manipulating the confidence of those predictions and sometimes get things like bigger reflexes

287
00:28:56,360 --> 00:29:00,840
and hyperreflexia as the sort you might see in clinical populations with spinal cord injuries

288
00:29:02,360 --> 00:29:06,440
and so here you can see the reflex is slightly larger than it was before and slightly brisker.

289
00:29:08,360 --> 00:29:14,440
I'll show you another example of this same idea where now we've developed a mapping between

290
00:29:14,840 --> 00:29:21,160
the microcircuit and the anatomy of the ocular motor brainstem so here starting from the superior

291
00:29:21,160 --> 00:29:28,840
colliculus the the rostral interstitial nucleus of the medial longitudinal vesiculus and there is

292
00:29:28,840 --> 00:29:36,280
other centers and the cranial nerves that are responsible for generating eye movements so cranial

293
00:29:36,280 --> 00:29:45,560
nerve three and cranial nerve six here are seen here as resulting from these error terms so we're

294
00:29:45,560 --> 00:29:52,280
predicting how I would expect my eyes to move and any error in the current position of the eyes is

295
00:29:52,280 --> 00:29:58,280
corrected by generating eye movements so by inducing different priors on top of this model

296
00:29:58,280 --> 00:30:02,360
of different predictions about where I'd expect the eyes to be we can actually then generate

297
00:30:02,360 --> 00:30:09,480
different sorts of eye movements using predictive coding style active inference scheme where the key

298
00:30:09,480 --> 00:30:14,360
thing is that the predictions can be fulfilled by actually changing the positions of our muscles

299
00:30:14,360 --> 00:30:22,680
and positions of our eyes so that sort of provides a very brief overview and a couple of examples

300
00:30:23,640 --> 00:30:31,400
dealing with the role of reflexes and predictions in generating movements but it doesn't really

301
00:30:31,400 --> 00:30:36,280
tell us anything about how movements are chosen how movements are selected and you don't necessarily

302
00:30:36,280 --> 00:30:40,840
get any intelligent behavior out of that and for that we need to start looking at different sorts

303
00:30:40,840 --> 00:30:47,160
of structures and different parts of this generative model and here the key thing I want to focus on

304
00:30:47,160 --> 00:30:53,000
is the role of the basal ganglia and which here we're going to associate with the computation

305
00:30:53,000 --> 00:30:57,960
of something known as the expected free energy and I'll try and describe what that is a little bit

306
00:30:57,960 --> 00:31:05,880
more in some of the subsequent slides so what I'm showing here is a mapping of some of the

307
00:31:06,680 --> 00:31:11,880
subcortical but subcortical anatomy or more actually I should say that this is a mapping

308
00:31:11,880 --> 00:31:18,440
between parts of our Bayesian message passing scheme parts of the model inversion when we're

309
00:31:18,440 --> 00:31:26,680
dealing with the ability to plan and make decisions to some of the known subcortical anatomy of the

310
00:31:26,680 --> 00:31:34,200
brain and the key thing I want to focus on is this G that is depicted as part of the as part of

311
00:31:34,200 --> 00:31:40,760
the stride and so the things that are feeding into this are what we're getting from the cortex

312
00:31:40,760 --> 00:31:45,080
here which is some prediction of the outcomes we'd expect if I were to pursue a particular

313
00:31:45,080 --> 00:31:52,440
cause of action which is this subscript pi and the another sort of error term here which is how

314
00:31:52,440 --> 00:32:00,920
far away are those predictions from my preferences about how the world should be my prior beliefs

315
00:32:00,920 --> 00:32:09,960
about how the kind of data that I would actually actively seek out and act to get together those

316
00:32:09,960 --> 00:32:15,640
are used to calculate our expected free energy which we can then use to formulate beliefs about

317
00:32:15,640 --> 00:32:21,480
policies by saying that the policies we would select the plans we would engage in are those that

318
00:32:21,480 --> 00:32:28,440
we would expect to be associated with the lowest expected free energy and to put that more formally

319
00:32:29,560 --> 00:32:35,960
we're saying that we're going to give a prior belief that the policies the series of actions

320
00:32:35,960 --> 00:32:40,440
we're going to choose are going to be those associated with the maximum information gain

321
00:32:40,440 --> 00:32:47,800
tell us the most about the world around us that fulfill our preferences and then we're going to

322
00:32:47,800 --> 00:32:52,440
add in an additional term here which deals with habitual type policies and things that we tend to

323
00:32:52,440 --> 00:32:57,320
do because we've learned we behave in a particular way in those situations now the combination of

324
00:32:57,320 --> 00:33:02,760
this information gain and preferences are often referred to as an expected free energy and I won't

325
00:33:02,760 --> 00:33:08,360
go into the details here but that's simply because you can rearrange them mathematically to make them

326
00:33:08,360 --> 00:33:13,160
look very much like the equation for a free energy or or marginal likelihood approximation

327
00:33:14,120 --> 00:33:19,880
with an expectation around them to say that these are the what we would expect given

328
00:33:20,840 --> 00:33:25,080
our predictions about the data we would obtain because here we're dealing with beliefs about

329
00:33:25,080 --> 00:33:30,200
the future plans into the future where we haven't yet had those data and we need to deal with the

330
00:33:30,200 --> 00:33:34,360
expectations of what those data would be under different plans that we could choose

331
00:33:35,960 --> 00:33:42,040
and so here we're showing just an example of the direct pathway through the basal ganglia

332
00:33:42,040 --> 00:33:45,560
which is normally thought to facilitate movement and depends upon this expected free energy

333
00:33:46,280 --> 00:33:51,560
and an indirect pathway which here we've associated with these kinds of habitual drives and I'll

334
00:33:51,560 --> 00:33:55,080
come back to those a little bit later on when we deal with hierarchical models

335
00:33:56,600 --> 00:34:00,200
just to give you a little bit of intuition as to the information gain aspect of the expected

336
00:34:00,200 --> 00:34:05,640
free energy because I think most people most people probably understand this idea of seeking

337
00:34:06,360 --> 00:34:11,880
preferences and behaving to maximize some degree of reward but many people are less

338
00:34:11,880 --> 00:34:15,880
familiar with the idea of seeking information into the way that that might manifest in these

339
00:34:15,880 --> 00:34:23,880
kinds of models so as an example I'm going to show you just a simple simulation where we're

340
00:34:23,880 --> 00:34:28,520
going to manipulate some of the different aspects of the uncertainty in the model so here

341
00:34:28,760 --> 00:34:35,560
in the upper left I'm just showing a way of parameterizing the the uncertainty associated

342
00:34:35,560 --> 00:34:39,720
with the data generating process so this is our likelihood precision on sensory precision

343
00:34:40,520 --> 00:34:46,440
and it effectively says that when this precision is very high we can be very certain about the

344
00:34:46,440 --> 00:34:51,560
outcome we'll observe given a particular state of the world whereas when it's very low we could

345
00:34:51,560 --> 00:34:57,160
predict everything with very similar probability and we can do something very similar by manipulating

346
00:34:57,160 --> 00:35:01,320
the uncertainty in the dynamics of the world so again when this is very high it means that

347
00:35:01,320 --> 00:35:05,640
where I am now is very highly predictive of where I'll be at the next step in time

348
00:35:07,160 --> 00:35:11,960
whereas when it's very low it means that pretty much anything at the next time point is is equally

349
00:35:11,960 --> 00:35:17,960
probable and I'll show you a simple simulation where these two things are manipulated just to

350
00:35:17,960 --> 00:35:22,600
try and give you an intuition for what it means to act to to maximize one's information about the

351
00:35:22,600 --> 00:35:31,720
world so the upper simulation here top left shows four panels which can each change at some point

352
00:35:31,720 --> 00:35:40,760
in time to a different color with some random probabilities and the blue line here is designed

353
00:35:40,760 --> 00:35:46,840
to show effectively an eye tracking trace so it's a simulated agent who is allowed to choose which

354
00:35:46,840 --> 00:35:50,920
of these panels it wants to look at at any one time and you can see it samples them with a

355
00:35:50,920 --> 00:35:58,040
relatively even frequency in the middle panel what we've done is we've reduced the precision

356
00:35:58,040 --> 00:36:03,480
or increased the uncertainty associated with the likelihood in the lower left that effectively is

357
00:36:03,480 --> 00:36:08,760
like turning off the lights in that location it's effectively making that lower left location much more

358
00:36:08,760 --> 00:36:15,480
much less informative much more noisy and so effectively what what we've got here is a system

359
00:36:15,480 --> 00:36:20,680
that then ignores that it says that I can't get a good quality high quality information from there

360
00:36:20,680 --> 00:36:24,600
so I'm going to look at all of the other locations rather than rather than this in the lower left

361
00:36:29,400 --> 00:36:33,640
an analogy for this is if you're thinking about how you perform a scientific experiment you would

362
00:36:33,640 --> 00:36:38,760
probably aim to use if you had a choice between two different measuring instruments you would

363
00:36:38,760 --> 00:36:42,760
choose the one that gives you more precise measurements rather than the one that gives you

364
00:36:42,760 --> 00:36:50,840
very noisy measurements in the lower left we've manipulated the uncertainty or the volatility

365
00:36:50,840 --> 00:36:56,440
associated with the dynamics so here what's happened is the upper left location is associated

366
00:36:56,440 --> 00:37:01,640
with much more uncertain dynamics so here what happens is that I end up sampling that upper left

367
00:37:01,640 --> 00:37:08,200
location much more frequently and intuitively this makes a lot of sense because if you've looked

368
00:37:08,200 --> 00:37:12,760
somewhere very recently normally you will you'll know a lot about what was there you don't need to

369
00:37:12,760 --> 00:37:19,960
look back there anytime soon however if it's very volatile if it changes quite with some degree of

370
00:37:19,960 --> 00:37:25,160
randomness and you have very little certainty about the state of it after you've looked away

371
00:37:25,160 --> 00:37:29,240
you'll look back with a much greater frequency so we've effectively decreased the inhibition

372
00:37:29,240 --> 00:37:34,920
of return in this location by increasing its volatility or decreasing the precision associated

373
00:37:34,920 --> 00:37:40,840
with its with the dynamics in that location so the end of this was just to show you this sort

374
00:37:40,840 --> 00:37:46,280
of emergent behavior just by having an information seeking objective by by having a prior belief

375
00:37:46,280 --> 00:37:51,560
that we're going to act to minimize some expected free energy one component of which is to maximize

376
00:37:51,560 --> 00:37:59,480
our information about the world now the next thing I want to come on to is the role of hierarchical

377
00:37:59,480 --> 00:38:05,800
generative models and one of the key benefits of having a hierarchical model is that we can now deal

378
00:38:05,800 --> 00:38:10,920
with things that evolve over a range of different timescales so you might have some things that

379
00:38:10,920 --> 00:38:16,840
evolve very slowly and some things that evolve very quickly and to some extent we can separate

380
00:38:16,840 --> 00:38:26,280
out two and use slowly evolving things to to to help us predict what's happening at the faster level

381
00:38:26,280 --> 00:38:33,560
and here the key things to to note are all of these connections between higher cortical regions

382
00:38:33,560 --> 00:38:42,120
and lower cortical regions which manifests both in the in the discrete and continuous state space

383
00:38:42,120 --> 00:38:47,160
models and so it's worth them thinking about what the role of these are and how that manifests in

384
00:38:47,160 --> 00:38:51,720
terms of the generative models we've been dealing with before and it's really this that links back

385
00:38:51,720 --> 00:38:58,840
into the the idea of predictive coding as many people know it and you've probably seen graphics

386
00:38:58,840 --> 00:39:04,280
of this sort in the past where we have a range of cortical regions that are each making predictions

387
00:39:04,280 --> 00:39:08,920
about the others and so here going from going from the right to the left we've got a series of

388
00:39:08,920 --> 00:39:16,520
predictions in as these dark black lines with prediction errors passed back up using these

389
00:39:16,520 --> 00:39:22,520
these dashed lines that then allow us to correct at each level and all this this graphic shows

390
00:39:22,520 --> 00:39:28,920
is a simplification of the graphic on on the previous slide so i want to give you an intuition

391
00:39:28,920 --> 00:39:33,720
for why this is useful using a couple of examples so the first example i'm going to use is one from

392
00:39:33,720 --> 00:39:39,960
the domain of active vision so imagine imagine you're doing a task now where you have to fixate

393
00:39:39,960 --> 00:39:47,560
on this cross in the center and maintain that fixation and show you a stimulus but maintain

394
00:39:47,560 --> 00:39:52,840
fixation on the cross. Stimulus is going to disappear and then your next task is to perform

395
00:39:52,840 --> 00:39:58,120
an eye movement to the location where the stimulus appeared. It's a very simple task if the sort was

396
00:39:58,120 --> 00:40:04,600
used frequently in monkey electrophysiology and all throughout neuroscience but it's an interesting

397
00:40:04,600 --> 00:40:09,800
one because it has several different components to it that i think help in terms of thinking about

398
00:40:09,880 --> 00:40:16,920
the utility of hierarchical models so one aspect of this is making decisions about

399
00:40:16,920 --> 00:40:20,760
where you're looking at each of those time points and making inferences about which

400
00:40:20,760 --> 00:40:26,360
of several alternative locations you're going to perform an eye movement to so simple form of

401
00:40:26,360 --> 00:40:35,960
planning here so that calls into into action the sort of discrete state space model that we were

402
00:40:35,960 --> 00:40:41,720
dealing with before this this partial observable decision process this set of discrete sequential

403
00:40:41,720 --> 00:40:49,480
timing points and that's often very highly relevant structure of most simple tasks in cognitive

404
00:40:49,480 --> 00:40:56,600
neuroscience. However we also need to actually move our eyes we need to we need to move our eyes

405
00:40:56,600 --> 00:41:00,440
from one location to the next we need to know how to generate forces we need to know how to make

406
00:41:00,440 --> 00:41:05,800
sure the eyes come to rest in the right location given those decisions and that calls into play

407
00:41:06,040 --> 00:41:11,720
this sort of continuous state space predictive coding style model so how do we then how do we

408
00:41:11,720 --> 00:41:15,800
then combine the two how do we deal with the situation where we want to predict a particular

409
00:41:15,800 --> 00:41:22,840
location at a discrete level one of several alternatives but then also map that to a continuous

410
00:41:22,840 --> 00:41:29,320
motor trajectory and this is where generative models get a little bit more complicated so

411
00:41:29,320 --> 00:41:35,240
taking this a bit at a time what we've got at the top here is our mark-off decision process model

412
00:41:35,240 --> 00:41:40,920
this is our discrete state space model in discrete time now at each of those time points we're no

413
00:41:40,920 --> 00:41:47,560
longer predicting data specifically we're now predicting a short trajectory formulated in terms

414
00:41:47,560 --> 00:41:52,680
of our continuous state space models so at time one we're now going to predict a short element of

415
00:41:52,680 --> 00:41:57,320
our trajectory and this is very much like the sort of clustering model I showed you much earlier

416
00:41:57,320 --> 00:42:02,200
that you take a discrete point and you map it to a point in continuous space with some probability

417
00:42:02,440 --> 00:42:08,760
at the next time we then predict the next bit of that continuous trajectory so our priors for our

418
00:42:08,760 --> 00:42:14,360
continuous model are now inheriting from the predictions from the discrete state space model

419
00:42:14,360 --> 00:42:19,720
and then we can do the same thing again at the next time point and and so we can create a discrete

420
00:42:19,720 --> 00:42:25,880
sequence from our our sequential model and at each point in that sequence associate that with a

421
00:42:26,840 --> 00:42:34,120
short trajectory in continuous space that then predicts our continuous data allowing us to then

422
00:42:34,120 --> 00:42:37,960
predict the proprioceptive information we would expect to get from the eyes when we're performing

423
00:42:37,960 --> 00:42:45,080
a task at that sort and you can see that we now develop a hierarchical structure also in thinking

424
00:42:45,080 --> 00:42:49,880
about the relationship between the bits of the message passing scheme so here we've got the

425
00:42:49,880 --> 00:42:55,080
discrete bit of our message passing scheme our discrete microcircuit and a bit of the continuous

426
00:42:55,080 --> 00:42:59,800
microcircuit here and the interactions between the two of those were making predictions from the

427
00:42:59,800 --> 00:43:05,240
discrete one to the continuous one and then passing errors backwards to allow us to to update

428
00:43:05,880 --> 00:43:12,280
the discrete element so interpreting this again neurobiologically you can imagine

429
00:43:13,160 --> 00:43:17,400
that say we have some element of the cortex perhaps the frontal eye field that's making predictions

430
00:43:17,400 --> 00:43:23,720
about one of several alternative locations I could direct my gaze we can then map that

431
00:43:23,720 --> 00:43:29,720
via sort of output of the basal ganglia and the predictions about what policy I'm going to pursue

432
00:43:31,160 --> 00:43:36,680
through to areas like the superior colliculus which then might make use of predictive coding

433
00:43:36,680 --> 00:43:42,840
style networks of the sort depicted here to then generate eye movements as we've seen before

434
00:43:43,720 --> 00:43:48,040
and just to show you that in action we imagine that these are population of neurons that at

435
00:43:48,040 --> 00:43:53,080
each time point are trying to predict one of several in this case three different locations

436
00:43:53,080 --> 00:43:59,480
at four different time points we can then map that through structures like the superior colliculus

437
00:43:59,480 --> 00:44:05,480
and this is supposed to sort of cartoon the idea of a population code in the superior colliculus

438
00:44:05,480 --> 00:44:09,880
but then results in the eye movement to each of those discrete locations

439
00:44:11,560 --> 00:44:16,200
putting it all together we can then formulate exactly the task that I asked you to perform

440
00:44:16,200 --> 00:44:22,120
earlier and see the result of an active infant scheme performing that task

441
00:44:22,920 --> 00:44:28,760
and so here you can see it performs it very successfully by predicting sequences of eye

442
00:44:28,760 --> 00:44:36,440
movements that are conditioned upon the discrete part of the model and the idea of maintaining

443
00:44:36,440 --> 00:44:42,520
this belief about where the stimulus appeared and using that then to direct my policy selection

444
00:44:42,520 --> 00:44:50,440
and eventually the movement I select I want to give you one more example of this sort of hierarchical

445
00:44:50,440 --> 00:44:56,280
scheme in the domain of motor control and appeal to the same sort of simulations we were looking

446
00:44:56,280 --> 00:45:01,800
at earlier in terms of the reflexes exhibited by that arm the generative model we're going to use

447
00:45:01,800 --> 00:45:07,800
here is a slightly complicated one but here effectively involves transitions between different

448
00:45:07,800 --> 00:45:13,880
locations that an arm could be in or a hand could be in which then predicts different locations in

449
00:45:13,880 --> 00:45:21,720
some continuous space and different locations that a target might be in and that will then allow us

450
00:45:21,720 --> 00:45:29,000
to make predictions about about given where some target is in some continuous space which decision

451
00:45:29,000 --> 00:45:34,120
I'm going to make about the alternative points I could move my arm to and so here we have the

452
00:45:34,120 --> 00:45:39,560
simulation where we have the target appearing as the black ball out of the out of the three balls

453
00:45:39,560 --> 00:45:45,960
which is going to change at various points in time and our active infant scheme is now

454
00:45:45,960 --> 00:45:51,080
selecting these locations translating those into predictions of continuous trajectories

455
00:45:51,080 --> 00:45:56,440
and making the appropriate movements such that it reaches each of these target locations

456
00:45:56,440 --> 00:46:03,480
and what's shown in terms of the the graphic here is also an interpretation of the message

457
00:46:03,480 --> 00:46:08,760
passing in terms of the known anatomy of the motor system or part of it

458
00:46:11,000 --> 00:46:15,240
we can perform various lesions to this and see whether they behave in the same way that we might

459
00:46:15,240 --> 00:46:21,960
expect patient populations to so here we've induced a cerebellar lesion which effectively

460
00:46:22,760 --> 00:46:30,280
involves a misestimation of certain precision terms certain confidence or variance parameters

461
00:46:30,840 --> 00:46:36,280
and you see this sort of overshoot and this kind of oscillatory behavior in the way the arm behaves

462
00:46:37,160 --> 00:46:40,360
that mimics what we might expect him as cerebellar attacks here

463
00:46:42,360 --> 00:46:45,640
we can look at the higher levels of the model and actually have multiple

464
00:46:46,440 --> 00:46:50,600
hierarchical levels of the discrete scheme and here what we've done is we've induced a

465
00:46:51,560 --> 00:46:58,280
lesion in what might represent a frontal lobe lesion where now the model is able to perform

466
00:46:58,280 --> 00:47:03,240
all the movements with perfect fluency but every time there's a change of context it takes quite

467
00:47:03,240 --> 00:47:08,360
a while to adjust to that new context to deal with a new situation you'll see that instead of

468
00:47:09,160 --> 00:47:13,720
moving straight there as it was before there's a lot more hesitancy and a lot more difficulty

469
00:47:13,800 --> 00:47:20,120
constructing that long-term narrative because we've broken that slower hierarchical level

470
00:47:22,280 --> 00:47:27,480
and here a final lesion that we can introduce is one that reduces my confidence in policy

471
00:47:27,480 --> 00:47:32,520
selection so here this effectively makes me very uncertain about what I'm going to do next and

472
00:47:32,520 --> 00:47:38,040
here we get a sort of a kinetic type picture and sort we might expect in a Parkinsonian type syndrome

473
00:47:38,120 --> 00:47:47,960
so I realized that was quite a lot and I hope I'm not too far over time but I'll just try and

474
00:47:47,960 --> 00:47:54,600
summarize briefly the key ideas that we've discussed so the first was thinking about this

475
00:47:54,600 --> 00:47:59,080
idea of climbing probability gradients the idea that to maintain some form over time

476
00:48:00,040 --> 00:48:06,520
and to persist we need to effectively be climbing these probability gradients all the time

477
00:48:07,480 --> 00:48:12,440
we interpreted those probability gradients in terms of in terms of Bayes theorem and connected

478
00:48:12,440 --> 00:48:17,240
that to the Bayes in brain and the sort of message passing that might be involved in

479
00:48:18,120 --> 00:48:23,240
forming inferences about elements of some generative model and the role that might have

480
00:48:23,240 --> 00:48:31,880
in cortical micro-circuitry we also discussed the role of action and the idea that that one way to

481
00:48:31,960 --> 00:48:37,320
make our model better fit the world is to act upon it and one way to do that is to simply reduce

482
00:48:37,320 --> 00:48:43,960
any discrepancy between the predictions from that model and the data that's coming in and the

483
00:48:43,960 --> 00:48:51,160
structure of that is very similar to the idea of a reflex arc as is well known throughout

484
00:48:51,960 --> 00:48:59,800
motor neuroscience and the final thing we thought about was how we can then construct hierarchical

485
00:48:59,800 --> 00:49:07,640
models that allow us to produce simulations and theories that deal with much larger scale

486
00:49:07,640 --> 00:49:13,800
networks in the brain and take a full sort of systems level view of how particular movements

487
00:49:13,800 --> 00:49:20,920
and particular behaviors are generated through the inversion of particular forms of generative model

488
00:49:22,520 --> 00:49:28,920
with that I'd like to thank many people who've been involved either directly or indirectly in

489
00:49:28,920 --> 00:49:34,280
this work and I'll mention the book again which was mentioned at the beginning and a lot of this

490
00:49:34,280 --> 00:49:39,320
talk was loosely following some of the structure in a couple of the chapters in the book so if

491
00:49:39,320 --> 00:49:46,520
anybody's interested please do take a look I'm more than happy to answer any questions thank you for

492
00:49:46,520 --> 00:49:52,120
your attention

