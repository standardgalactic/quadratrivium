1
00:00:00,000 --> 00:00:05,600
A few days ago, Mamba, linear time sequence modelling with selective state spaces, appeared on

2
00:00:05,600 --> 00:00:10,880
archive. It is causing some excitement, partly because it shows strong empirical results,

3
00:00:10,880 --> 00:00:16,080
partly because it has an interesting design, and partly because Mamba is a great name.

4
00:00:16,080 --> 00:00:20,800
In this video, I'll give a high level summary of the paper. To understand the motivation for this

5
00:00:20,800 --> 00:00:25,920
work, it's helpful to go back in time to the distant memories of 2020, and in particular,

6
00:00:25,920 --> 00:00:30,960
the Long Range Arena, a benchmark for efficient transformers. The starting premise was that

7
00:00:30,960 --> 00:00:36,000
transformers do not scale very well to long sequence lengths, largely because of quadratic

8
00:00:36,000 --> 00:00:41,440
self-attention complexity. Given the proliferation of efficient alternatives to transformers that

9
00:00:41,440 --> 00:00:46,480
were emerging at the time, this work proposed a benchmark specifically focused on evaluating

10
00:00:46,480 --> 00:00:52,240
model quality under long context scenarios. There were tasks like long list ops, where the model

11
00:00:52,240 --> 00:00:58,320
is given an input consisting of a long list of nested operators, and needs to calculate an answer,

12
00:00:58,320 --> 00:01:03,680
and fun tasks like pathfinder, where the model gets a flattened image that looks like this,

13
00:01:03,680 --> 00:01:08,640
and needs to determine whether the two circles can be joined. This is a positive example because

14
00:01:08,640 --> 00:01:13,840
they can. This is a negative example because they cannot. Broadly, they found that if you wanted to

15
00:01:13,840 --> 00:01:19,600
maintain performance encapsulated here in this LRA score, it's quite hard to do much better than

16
00:01:19,600 --> 00:01:24,880
the vanilla transformer. Independently, and a little earlier, a creative approach to tackling

17
00:01:24,880 --> 00:01:31,520
the Long Range Dependency problem was introduced in this work on Legend Memory Units, in the context

18
00:01:31,520 --> 00:01:38,000
of recurrent neural networks. As a refresher, Legend polynomials are these beautiful creatures.

19
00:01:38,000 --> 00:01:42,960
You can construct them by making a sequence of polynomials that are orthogonal to each other.

20
00:01:42,960 --> 00:01:48,720
By projecting values onto a basis of these polynomials, you can efficiently store a history

21
00:01:48,720 --> 00:01:53,040
of the inputs to your system. That's the key idea in this funky looking equation.

22
00:01:53,040 --> 00:01:59,440
You can reconstruct the input U by reassembling it from the components of the memory vector,

23
00:01:59,440 --> 00:02:05,120
denoted here by M. This idea can then be baked into a layer where the memory block is implemented

24
00:02:05,120 --> 00:02:11,200
with simple linear operations. Building on that idea, and others, Hippo, recurrent memory with

25
00:02:11,200 --> 00:02:17,520
optimal polynomial projections, had the insight that we can phrase memory as a technical problem

26
00:02:17,520 --> 00:02:22,480
of online function approximation, where a function is summarized by storing its optimal

27
00:02:22,480 --> 00:02:28,080
coefficients in terms of some basis functions. Using this insight, the authors constructed a

28
00:02:28,080 --> 00:02:33,600
simple model based on Legendre polynomials that worked well on tasks like permuted MNIST,

29
00:02:33,600 --> 00:02:39,520
outperforming other sequence model-based baselines like LSTM and dilated RNN. This was

30
00:02:39,520 --> 00:02:44,480
followed by the work, combining recurrent convolutional and continuous time models

31
00:02:44,480 --> 00:02:49,920
with linear state space layers, which proposed a unifying framework for sequence modelling

32
00:02:49,920 --> 00:02:55,680
based on a standard state space representation. This work showed how, starting from an implicit,

33
00:02:55,680 --> 00:03:01,120
continuous time state space model, you could discretize to produce a system that can be

34
00:03:01,120 --> 00:03:07,760
interpreted as a recurrent network with the benefit of efficient inference, or as a convolutional

35
00:03:07,760 --> 00:03:13,120
model which benefits from parallelizable training. Unfortunately, a naive implementation of these

36
00:03:13,120 --> 00:03:18,800
models requires a massive amount of memory. In fact, for reasonably sized models, such as when

37
00:03:18,800 --> 00:03:25,440
the state dimension is 256, the linear state space layer uses orders of magnitude more memory than

38
00:03:25,440 --> 00:03:32,000
comparably sized RNNs or CNNs. That brings us to the S4 model introduced in efficiently modelling

39
00:03:32,000 --> 00:03:37,200
long sequences with structured state spaces. What we'd really like to be able to do is use the

40
00:03:37,200 --> 00:03:42,400
convolutional interpretation we've just seen to efficiently train our state space model.

41
00:03:42,400 --> 00:03:48,320
Now, if we want to train state space models using the convolutional representation, we can do a

42
00:03:48,320 --> 00:03:53,600
little bit of unrolling and a little bit of vectorization to get this L-dimensional kernel,

43
00:03:53,600 --> 00:03:59,520
where L is the sequence length. The scary part here is this A raised to the power of I term.

44
00:03:59,520 --> 00:04:04,720
Harking back to your linear algebra classes, as soon as you see this, you may feel a strong

45
00:04:04,720 --> 00:04:10,400
and appropriate urge to try to diagonalize A. The authors discuss diagonalization.

46
00:04:10,400 --> 00:04:15,680
Unfortunately, it cannot be used directly for the hippo matrix due to numerical issues.

47
00:04:15,680 --> 00:04:21,280
Specifically, it has entries exponentially large in the state size. However, we can address the

48
00:04:21,280 --> 00:04:27,120
hippo matrix with an adjustment. The authors note that although the hippo matrix is not normal,

49
00:04:27,120 --> 00:04:33,360
it can be decomposed as the sum of a normal and low rank matrix. Even this is still not useful

50
00:04:33,360 --> 00:04:38,720
by itself. An additional three new techniques are needed. First, instead of computing the kernel

51
00:04:38,720 --> 00:04:45,360
k-bar directly, they instead compute its spectrum, then determine k-bar by applying an inverse FFT.

52
00:04:45,360 --> 00:04:50,720
Next, we use everyone's favorite low rank trick, the Woodbury identity. That lets us perform an

53
00:04:50,720 --> 00:04:56,160
efficient matrix update. Third, it is shown that the diagonal matrix case is equivalent to the

54
00:04:56,160 --> 00:05:01,200
computation of a Cauchy kernel, a well-studied problem with stable, near-linear algorithms.

55
00:05:01,200 --> 00:05:06,560
All of this comes together in theorem three. It says that given any step size delta, computing

56
00:05:06,560 --> 00:05:12,160
the state space model convolution filter k-bar can be reduced to four Cauchy multiplies,

57
00:05:12,160 --> 00:05:18,800
requiring only big O of n plus l operations, ignoring logarithmic factors, and big O of

58
00:05:18,800 --> 00:05:25,120
n plus l space, where again, l is the sequence length and n is the state size. Empirically,

59
00:05:25,120 --> 00:05:31,120
the proposed S4 architecture does very well on the long-range arena, even doing well on the path

60
00:05:31,120 --> 00:05:36,240
x task, where the model must determine if two points are connected on a fairly large image.

61
00:05:36,240 --> 00:05:41,120
This task is not too hard when the input is arranged as an image, but it's pretty difficult

62
00:05:41,120 --> 00:05:46,640
when it's a flattened sequence, as evidenced by the fact that many other models fail on this task.

63
00:05:46,640 --> 00:05:52,800
If you'd like to gain some intuition for how S4 works, I highly recommend the annotated S4

64
00:05:52,800 --> 00:05:59,520
by Sasha Rush and Sid Karamchetti, just as with the legendary annotated transformer blog post

65
00:05:59,520 --> 00:06:05,920
back in 2018. The annotated S4 takes you step-by-step through the paper and provides snippets of

66
00:06:05,920 --> 00:06:11,200
code that implement the mathematics. It also includes nice visualizations to help you build

67
00:06:11,200 --> 00:06:16,800
intuition for how state-space models work. All of this brings us back to Mamba. This work builds

68
00:06:16,800 --> 00:06:22,720
on the state-space model approach with a few key contributions. First, a selection mechanism. Since

69
00:06:22,720 --> 00:06:28,320
previous state-space models lack the ability to efficiently select data in an input-dependent

70
00:06:28,320 --> 00:06:33,760
manner, they design a simple selection mechanism by parameterizing the state-space model parameters

71
00:06:33,760 --> 00:06:39,280
based on the input. Unfortunately, this change makes it hard for us to use the efficient convolutional

72
00:06:39,280 --> 00:06:45,280
trick we saw in S4, so the second contribution is a hardware-aware algorithm that computes the

73
00:06:45,280 --> 00:06:50,640
model recurrently with a scan instead of convolution. This leads to an implementation that is faster

74
00:06:50,640 --> 00:06:56,400
than previous methods, both in theory, scaling linearly in sequence length compared to pseudo-linear

75
00:06:56,400 --> 00:07:03,280
for all convolution-based SSMs and on modern hardware, up to three times faster on A100 GPUs.

76
00:07:03,280 --> 00:07:08,640
Spicy. The third contribution is to simplify prior deep-sequence model architectures by

77
00:07:08,640 --> 00:07:14,400
combining the design of prior state-space model architectures with the MLP block of transformers

78
00:07:14,400 --> 00:07:19,440
into a single block, leading to a simple and homogenous architecture design called Mamba

79
00:07:19,440 --> 00:07:24,640
that incorporates selective state-spaces. By building on a state-space model foundation,

80
00:07:24,640 --> 00:07:29,760
Mamba is related to many previous state-space model architectures. In addition to the models we

81
00:07:29,760 --> 00:07:36,400
met earlier, there is Linear Attention, which can be viewed as a degenerate linear SSM, the gloriously

82
00:07:36,400 --> 00:07:45,040
named Hungry Hungry Hippos or H3, Hyena, RhettNet, and RWKV. The core motivation behind Mamba is to

83
00:07:45,040 --> 00:07:49,760
use selection as a means of compression. The authors point out that you can view various

84
00:07:49,760 --> 00:07:55,120
sequence models as making different trade-offs when tackling a fundamental problem of sequence

85
00:07:55,120 --> 00:08:00,800
modeling, i.e. compressing context into a smaller state. At one extreme, we have Attention, which

86
00:08:00,800 --> 00:08:05,920
is both effective and inefficient because it explicitly does not compress context at all.

87
00:08:05,920 --> 00:08:10,560
At the other extreme, recurrent models are efficient because they have a finite state,

88
00:08:10,560 --> 00:08:15,680
but their effectiveness is limited by how well this state has compressed the context. To explore

89
00:08:15,680 --> 00:08:22,320
this trade-off, the authors focus on two synthetic tasks. One previously well-studied task is copying.

90
00:08:22,320 --> 00:08:28,080
The job of the model is to copy color sequences in the input into the output, delayed by some

91
00:08:28,080 --> 00:08:33,040
constant offset. Standard convolutional models with fixed kernels can solve this without any

92
00:08:33,040 --> 00:08:38,240
problem. The first of the harder tasks they consider is selective copying. This time, we have

93
00:08:38,240 --> 00:08:44,560
random spaces symbolized by the white blocks in between the input sequence elements. To successfully

94
00:08:44,560 --> 00:08:50,320
copy the color outputs to the output while ignoring the white blocks, we need a mechanism that behaves

95
00:08:50,320 --> 00:08:55,760
differently at different inputs. The second task they look at is induction heads. Here,

96
00:08:55,760 --> 00:09:00,240
in order to predict the appropriate color at the question mark block, the model needs to be able

97
00:09:00,240 --> 00:09:05,920
to perform retrieval back over the input sequence based on the context provided by the black square,

98
00:09:05,920 --> 00:09:11,200
in order to predict that blue is the next color in the sequence. To build a model capable of solving

99
00:09:11,200 --> 00:09:18,640
these tasks, the models start from the S4 state space model, which uses parameters a, b, c, and

100
00:09:18,640 --> 00:09:24,480
delta, which don't depend on the input sequence. Now, for the proposed algorithm, to allow the model

101
00:09:24,480 --> 00:09:31,680
to behave differently on different inputs, b, c, and delta are altered to be time-varying. They now

102
00:09:31,680 --> 00:09:36,800
depend on the input sequence. This gives the model more power, but means we can no longer use the

103
00:09:36,800 --> 00:09:43,200
efficient S4 convolution trick. To get around this, the authors develop a selective scan based on

104
00:09:43,200 --> 00:09:48,720
hardware-aware state expansion. In effect, the two big challenges to be tackled once we give up on

105
00:09:48,720 --> 00:09:54,720
convolution are the sequential nature of recurrence and the large memory usage. There are three

106
00:09:54,720 --> 00:10:00,480
techniques that come into play, kernel fusion, parallel scan, and recomputation. A key idea for

107
00:10:00,480 --> 00:10:05,520
getting around the sequential processing problem comes from simplified state space layers for

108
00:10:05,520 --> 00:10:11,760
sequence modeling, or S5, which highlighted the benefits of using parallel scans to maintain

109
00:10:11,760 --> 00:10:18,160
efficiency while avoiding the use of convolutional tricks used in S4. Mamba uses this parallel scan

110
00:10:18,160 --> 00:10:23,920
idea in combination with efficient use of memory. In particular, instead of preparing the scan input

111
00:10:23,920 --> 00:10:30,240
in GPU high bandwidth memory, they load the state space model parameters directly from the slow high

112
00:10:30,240 --> 00:10:36,320
bandwidth memory to fast SRAM, where they perform the discretization and recurrence. Then they write

113
00:10:36,320 --> 00:10:42,080
the final results back to high bandwidth memory. Last, recomputation is used to reduce the memory

114
00:10:42,080 --> 00:10:47,680
requirements. This allows them to avoid saving the intermediate states, which are necessary for back

115
00:10:47,680 --> 00:10:52,960
propagation. Putting this all together, here is the selective state space model with hardware-aware

116
00:10:53,040 --> 00:10:59,680
state expansion. The first thing we see is that BT, delta T, and CT all depend on the input

117
00:10:59,680 --> 00:11:06,080
XT via the selection mechanism. The second thing we see is that the parameters are loaded into fast

118
00:11:06,080 --> 00:11:12,240
GPU SRAM, indicated in this diagram by the color orange, where the update is performed. Then the

119
00:11:12,240 --> 00:11:18,800
output is ultimately stored back into GPU high bandwidth memory, shown in green. Next, we come to

120
00:11:18,800 --> 00:11:24,240
the simplified state space model architecture. Here, the authors combine the Hungry Hungry Hippos

121
00:11:24,240 --> 00:11:29,680
block with a gated MLP block to produce their Mamba block. The shapes here indicate that the

122
00:11:29,680 --> 00:11:35,760
dimensionality is expanded inside the block. This block is repeated and interleaved with standard

123
00:11:35,760 --> 00:11:41,360
normalization and residual connections to form the Mamba architecture. I'll mention a few other

124
00:11:41,360 --> 00:11:46,720
model details. The authors note that most prior state space models use complex numbers in their

125
00:11:46,720 --> 00:11:51,360
state, but it has been empirically observed that completely real valued state space models seem

126
00:11:51,360 --> 00:11:57,520
to work fine, and possibly even better, in some settings. So, they use real values as the default,

127
00:11:57,520 --> 00:12:03,120
which work well for all but one of their tasks. Next, we come to the empirical evaluation. First,

128
00:12:03,120 --> 00:12:08,800
we have the synthetic tasks described earlier. First, as a bit of notation, the authors abbreviate

129
00:12:08,800 --> 00:12:15,440
selective state space models as S6 models, because they are S4 models with a selection mechanism,

130
00:12:15,440 --> 00:12:22,080
and computed with a scan. That's a lot of S's. On the selective copying task, S6 works well with

131
00:12:22,080 --> 00:12:28,560
every architecture, but S4 and Hyena work comparatively poorly. On the induction heads task,

132
00:12:28,560 --> 00:12:34,400
Mamba, shown in brown, appears up here at the top, where we see it succeeds on test sequence lengths

133
00:12:34,400 --> 00:12:39,600
of up to a million tokens, which is 4,000 times longer than it saw during training,

134
00:12:39,600 --> 00:12:44,240
while none of the other methods compared to generalize to beyond twice their training length.

135
00:12:44,320 --> 00:12:49,120
Next, we have experiments on language modelling, which follow the training recipe described in

136
00:12:49,120 --> 00:12:55,920
the GBT3 work, and train on the pile dataset. Here, the metric on the y-axis is perplexity,

137
00:12:55,920 --> 00:13:01,680
so lower is better. We can see that across a sequence length of 2,048, and a sequence length

138
00:13:01,680 --> 00:13:10,480
of 8,192, that Mamba, shown in purple, outperforms the other baselines, and matches Transformer++,

139
00:13:10,480 --> 00:13:15,840
which is highlighted in orange, and represents the strongest Transformer recipe that the authors

140
00:13:15,840 --> 00:13:22,240
know of, based on the palm and llama architectures. That means rotary embeddings, swigloo MLPs,

141
00:13:22,240 --> 00:13:28,640
rmsnorm instead of leonorm, no linear bias, and higher learning rates. On zero shot downstream

142
00:13:28,640 --> 00:13:34,400
evaluations, there are lots of comparisons, generally we see Mamba achieving an average

143
00:13:35,200 --> 00:13:41,680
of baselines at twice the model size. Next, there are studies on DNA modelling. On human genome

144
00:13:41,680 --> 00:13:48,320
data, Mamba, shown in orange, scales better than Hyena DNA, shown in blue, and Transformer++,

145
00:13:48,320 --> 00:13:53,680
shown in red, when you scale up the number of parameters. It also scales better than Hyena DNA,

146
00:13:53,680 --> 00:13:59,520
when you scale up the sequence length. Mamba models also do well relative to a Hyena DNA baseline,

147
00:13:59,520 --> 00:14:06,160
when fine tuning for a species DNA classification task. Here, the metric on the y-axis is accuracy,

148
00:14:06,160 --> 00:14:10,720
so higher is better, with longer sequences as we move to the right along the x-axis.

149
00:14:10,720 --> 00:14:15,360
We see the orange and green Mamba curves rising. I won't go through them in detail,

150
00:14:15,360 --> 00:14:20,560
but there are other experiments showing that Mamba mostly works well on audio modelling and

151
00:14:20,560 --> 00:14:25,840
generation. However, there is one ablation in the appendix, identifying a case when a naive

152
00:14:25,920 --> 00:14:30,320
application of Mamba with the selection mechanism seems to hurt performance. Here,

153
00:14:30,320 --> 00:14:35,760
lower is better, and the orange line denotes the default Mamba configuration. The authors suggest

154
00:14:35,760 --> 00:14:40,880
that this may be a case where audio waveforms actually benefit from linear time-invariant

155
00:14:40,880 --> 00:14:46,560
models which have a matching inductive bias. Next, we have speed and memory benchmarks.

156
00:14:46,560 --> 00:14:51,360
The authors provide an implementation of the scan operation that is pretty fast.

157
00:14:51,360 --> 00:14:58,720
They compare the scan versus convolution and attention on an A100 GPU. Here, the y-axis measures

158
00:14:58,720 --> 00:15:05,440
time, so lower is better. We see the red line representing their implementation is considerably

159
00:15:05,440 --> 00:15:10,560
below the other mechanisms at longer sequence lengths. In terms of inference throughput,

160
00:15:10,560 --> 00:15:16,400
where higher is better, we find that Mamba, shown in blue and green, can achieve five times higher

161
00:15:16,400 --> 00:15:22,240
throughput than Transformers, benefiting from its recurrent nature. In terms of limitations,

162
00:15:22,240 --> 00:15:26,960
the authors highlight that there is no free lunch on the continuous discrete spectrum.

163
00:15:26,960 --> 00:15:32,720
The selection mechanism overcomes the weaknesses of prior state space models on discrete modalities

164
00:15:32,720 --> 00:15:38,400
such as text and DNA, but this can impede their performance on data that linear time-invariant

165
00:15:38,400 --> 00:15:44,160
state space models excel on, which we saw with the audio ablation. They also note that the empirical

166
00:15:44,160 --> 00:15:50,000
evaluation is limited to small model sizes, below the threshold of most strong open-source

167
00:15:50,000 --> 00:15:55,920
LLMs. Therefore, it remains to assess whether Mamba still compares favourably at larger sizes.

168
00:15:55,920 --> 00:16:00,720
That's it, we've reached the end. If you'd like to try it out, the authors have released code on GitHub.

