{"text": " A few days ago, Mamba, linear time sequence modelling with selective state spaces, appeared on archive. It is causing some excitement, partly because it shows strong empirical results, partly because it has an interesting design, and partly because Mamba is a great name. In this video, I'll give a high level summary of the paper. To understand the motivation for this work, it's helpful to go back in time to the distant memories of 2020, and in particular, the Long Range Arena, a benchmark for efficient transformers. The starting premise was that transformers do not scale very well to long sequence lengths, largely because of quadratic self-attention complexity. Given the proliferation of efficient alternatives to transformers that were emerging at the time, this work proposed a benchmark specifically focused on evaluating model quality under long context scenarios. There were tasks like long list ops, where the model is given an input consisting of a long list of nested operators, and needs to calculate an answer, and fun tasks like pathfinder, where the model gets a flattened image that looks like this, and needs to determine whether the two circles can be joined. This is a positive example because they can. This is a negative example because they cannot. Broadly, they found that if you wanted to maintain performance encapsulated here in this LRA score, it's quite hard to do much better than the vanilla transformer. Independently, and a little earlier, a creative approach to tackling the Long Range Dependency problem was introduced in this work on Legend Memory Units, in the context of recurrent neural networks. As a refresher, Legend polynomials are these beautiful creatures. You can construct them by making a sequence of polynomials that are orthogonal to each other. By projecting values onto a basis of these polynomials, you can efficiently store a history of the inputs to your system. That's the key idea in this funky looking equation. You can reconstruct the input U by reassembling it from the components of the memory vector, denoted here by M. This idea can then be baked into a layer where the memory block is implemented with simple linear operations. Building on that idea, and others, Hippo, recurrent memory with optimal polynomial projections, had the insight that we can phrase memory as a technical problem of online function approximation, where a function is summarized by storing its optimal coefficients in terms of some basis functions. Using this insight, the authors constructed a simple model based on Legendre polynomials that worked well on tasks like permuted MNIST, outperforming other sequence model-based baselines like LSTM and dilated RNN. This was followed by the work, combining recurrent convolutional and continuous time models with linear state space layers, which proposed a unifying framework for sequence modelling based on a standard state space representation. This work showed how, starting from an implicit, continuous time state space model, you could discretize to produce a system that can be interpreted as a recurrent network with the benefit of efficient inference, or as a convolutional model which benefits from parallelizable training. Unfortunately, a naive implementation of these models requires a massive amount of memory. In fact, for reasonably sized models, such as when the state dimension is 256, the linear state space layer uses orders of magnitude more memory than comparably sized RNNs or CNNs. That brings us to the S4 model introduced in efficiently modelling long sequences with structured state spaces. What we'd really like to be able to do is use the convolutional interpretation we've just seen to efficiently train our state space model. Now, if we want to train state space models using the convolutional representation, we can do a little bit of unrolling and a little bit of vectorization to get this L-dimensional kernel, where L is the sequence length. The scary part here is this A raised to the power of I term. Harking back to your linear algebra classes, as soon as you see this, you may feel a strong and appropriate urge to try to diagonalize A. The authors discuss diagonalization. Unfortunately, it cannot be used directly for the hippo matrix due to numerical issues. Specifically, it has entries exponentially large in the state size. However, we can address the hippo matrix with an adjustment. The authors note that although the hippo matrix is not normal, it can be decomposed as the sum of a normal and low rank matrix. Even this is still not useful by itself. An additional three new techniques are needed. First, instead of computing the kernel k-bar directly, they instead compute its spectrum, then determine k-bar by applying an inverse FFT. Next, we use everyone's favorite low rank trick, the Woodbury identity. That lets us perform an efficient matrix update. Third, it is shown that the diagonal matrix case is equivalent to the computation of a Cauchy kernel, a well-studied problem with stable, near-linear algorithms. All of this comes together in theorem three. It says that given any step size delta, computing the state space model convolution filter k-bar can be reduced to four Cauchy multiplies, requiring only big O of n plus l operations, ignoring logarithmic factors, and big O of n plus l space, where again, l is the sequence length and n is the state size. Empirically, the proposed S4 architecture does very well on the long-range arena, even doing well on the path x task, where the model must determine if two points are connected on a fairly large image. This task is not too hard when the input is arranged as an image, but it's pretty difficult when it's a flattened sequence, as evidenced by the fact that many other models fail on this task. If you'd like to gain some intuition for how S4 works, I highly recommend the annotated S4 by Sasha Rush and Sid Karamchetti, just as with the legendary annotated transformer blog post back in 2018. The annotated S4 takes you step-by-step through the paper and provides snippets of code that implement the mathematics. It also includes nice visualizations to help you build intuition for how state-space models work. All of this brings us back to Mamba. This work builds on the state-space model approach with a few key contributions. First, a selection mechanism. Since previous state-space models lack the ability to efficiently select data in an input-dependent manner, they design a simple selection mechanism by parameterizing the state-space model parameters based on the input. Unfortunately, this change makes it hard for us to use the efficient convolutional trick we saw in S4, so the second contribution is a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution. This leads to an implementation that is faster than previous methods, both in theory, scaling linearly in sequence length compared to pseudo-linear for all convolution-based SSMs and on modern hardware, up to three times faster on A100 GPUs. Spicy. The third contribution is to simplify prior deep-sequence model architectures by combining the design of prior state-space model architectures with the MLP block of transformers into a single block, leading to a simple and homogenous architecture design called Mamba that incorporates selective state-spaces. By building on a state-space model foundation, Mamba is related to many previous state-space model architectures. In addition to the models we met earlier, there is Linear Attention, which can be viewed as a degenerate linear SSM, the gloriously named Hungry Hungry Hippos or H3, Hyena, RhettNet, and RWKV. The core motivation behind Mamba is to use selection as a means of compression. The authors point out that you can view various sequence models as making different trade-offs when tackling a fundamental problem of sequence modeling, i.e. compressing context into a smaller state. At one extreme, we have Attention, which is both effective and inefficient because it explicitly does not compress context at all. At the other extreme, recurrent models are efficient because they have a finite state, but their effectiveness is limited by how well this state has compressed the context. To explore this trade-off, the authors focus on two synthetic tasks. One previously well-studied task is copying. The job of the model is to copy color sequences in the input into the output, delayed by some constant offset. Standard convolutional models with fixed kernels can solve this without any problem. The first of the harder tasks they consider is selective copying. This time, we have random spaces symbolized by the white blocks in between the input sequence elements. To successfully copy the color outputs to the output while ignoring the white blocks, we need a mechanism that behaves differently at different inputs. The second task they look at is induction heads. Here, in order to predict the appropriate color at the question mark block, the model needs to be able to perform retrieval back over the input sequence based on the context provided by the black square, in order to predict that blue is the next color in the sequence. To build a model capable of solving these tasks, the models start from the S4 state space model, which uses parameters a, b, c, and delta, which don't depend on the input sequence. Now, for the proposed algorithm, to allow the model to behave differently on different inputs, b, c, and delta are altered to be time-varying. They now depend on the input sequence. This gives the model more power, but means we can no longer use the efficient S4 convolution trick. To get around this, the authors develop a selective scan based on hardware-aware state expansion. In effect, the two big challenges to be tackled once we give up on convolution are the sequential nature of recurrence and the large memory usage. There are three techniques that come into play, kernel fusion, parallel scan, and recomputation. A key idea for getting around the sequential processing problem comes from simplified state space layers for sequence modeling, or S5, which highlighted the benefits of using parallel scans to maintain efficiency while avoiding the use of convolutional tricks used in S4. Mamba uses this parallel scan idea in combination with efficient use of memory. In particular, instead of preparing the scan input in GPU high bandwidth memory, they load the state space model parameters directly from the slow high bandwidth memory to fast SRAM, where they perform the discretization and recurrence. Then they write the final results back to high bandwidth memory. Last, recomputation is used to reduce the memory requirements. This allows them to avoid saving the intermediate states, which are necessary for back propagation. Putting this all together, here is the selective state space model with hardware-aware state expansion. The first thing we see is that BT, delta T, and CT all depend on the input XT via the selection mechanism. The second thing we see is that the parameters are loaded into fast GPU SRAM, indicated in this diagram by the color orange, where the update is performed. Then the output is ultimately stored back into GPU high bandwidth memory, shown in green. Next, we come to the simplified state space model architecture. Here, the authors combine the Hungry Hungry Hippos block with a gated MLP block to produce their Mamba block. The shapes here indicate that the dimensionality is expanded inside the block. This block is repeated and interleaved with standard normalization and residual connections to form the Mamba architecture. I'll mention a few other model details. The authors note that most prior state space models use complex numbers in their state, but it has been empirically observed that completely real valued state space models seem to work fine, and possibly even better, in some settings. So, they use real values as the default, which work well for all but one of their tasks. Next, we come to the empirical evaluation. First, we have the synthetic tasks described earlier. First, as a bit of notation, the authors abbreviate selective state space models as S6 models, because they are S4 models with a selection mechanism, and computed with a scan. That's a lot of S's. On the selective copying task, S6 works well with every architecture, but S4 and Hyena work comparatively poorly. On the induction heads task, Mamba, shown in brown, appears up here at the top, where we see it succeeds on test sequence lengths of up to a million tokens, which is 4,000 times longer than it saw during training, while none of the other methods compared to generalize to beyond twice their training length. Next, we have experiments on language modelling, which follow the training recipe described in the GBT3 work, and train on the pile dataset. Here, the metric on the y-axis is perplexity, so lower is better. We can see that across a sequence length of 2,048, and a sequence length of 8,192, that Mamba, shown in purple, outperforms the other baselines, and matches Transformer++, which is highlighted in orange, and represents the strongest Transformer recipe that the authors know of, based on the palm and llama architectures. That means rotary embeddings, swigloo MLPs, rmsnorm instead of leonorm, no linear bias, and higher learning rates. On zero shot downstream evaluations, there are lots of comparisons, generally we see Mamba achieving an average of baselines at twice the model size. Next, there are studies on DNA modelling. On human genome data, Mamba, shown in orange, scales better than Hyena DNA, shown in blue, and Transformer++, shown in red, when you scale up the number of parameters. It also scales better than Hyena DNA, when you scale up the sequence length. Mamba models also do well relative to a Hyena DNA baseline, when fine tuning for a species DNA classification task. Here, the metric on the y-axis is accuracy, so higher is better, with longer sequences as we move to the right along the x-axis. We see the orange and green Mamba curves rising. I won't go through them in detail, but there are other experiments showing that Mamba mostly works well on audio modelling and generation. However, there is one ablation in the appendix, identifying a case when a naive application of Mamba with the selection mechanism seems to hurt performance. Here, lower is better, and the orange line denotes the default Mamba configuration. The authors suggest that this may be a case where audio waveforms actually benefit from linear time-invariant models which have a matching inductive bias. Next, we have speed and memory benchmarks. The authors provide an implementation of the scan operation that is pretty fast. They compare the scan versus convolution and attention on an A100 GPU. Here, the y-axis measures time, so lower is better. We see the red line representing their implementation is considerably below the other mechanisms at longer sequence lengths. In terms of inference throughput, where higher is better, we find that Mamba, shown in blue and green, can achieve five times higher throughput than Transformers, benefiting from its recurrent nature. In terms of limitations, the authors highlight that there is no free lunch on the continuous discrete spectrum. The selection mechanism overcomes the weaknesses of prior state space models on discrete modalities such as text and DNA, but this can impede their performance on data that linear time-invariant state space models excel on, which we saw with the audio ablation. They also note that the empirical evaluation is limited to small model sizes, below the threshold of most strong open-source LLMs. Therefore, it remains to assess whether Mamba still compares favourably at larger sizes. That's it, we've reached the end. If you'd like to try it out, the authors have released code on GitHub.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.6000000000000005, "text": " A few days ago, Mamba, linear time sequence modelling with selective state spaces, appeared on", "tokens": [50364, 316, 1326, 1708, 2057, 11, 376, 23337, 11, 8213, 565, 8310, 42253, 365, 33930, 1785, 7673, 11, 8516, 322, 50644], "temperature": 0.0, "avg_logprob": -0.09649509358628888, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.0018670270219445229}, {"id": 1, "seek": 0, "start": 5.6000000000000005, "end": 10.88, "text": " archive. It is causing some excitement, partly because it shows strong empirical results,", "tokens": [50644, 23507, 13, 467, 307, 9853, 512, 14755, 11, 17031, 570, 309, 3110, 2068, 31886, 3542, 11, 50908], "temperature": 0.0, "avg_logprob": -0.09649509358628888, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.0018670270219445229}, {"id": 2, "seek": 0, "start": 10.88, "end": 16.080000000000002, "text": " partly because it has an interesting design, and partly because Mamba is a great name.", "tokens": [50908, 17031, 570, 309, 575, 364, 1880, 1715, 11, 293, 17031, 570, 376, 23337, 307, 257, 869, 1315, 13, 51168], "temperature": 0.0, "avg_logprob": -0.09649509358628888, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.0018670270219445229}, {"id": 3, "seek": 0, "start": 16.080000000000002, "end": 20.8, "text": " In this video, I'll give a high level summary of the paper. To understand the motivation for this", "tokens": [51168, 682, 341, 960, 11, 286, 603, 976, 257, 1090, 1496, 12691, 295, 264, 3035, 13, 1407, 1223, 264, 12335, 337, 341, 51404], "temperature": 0.0, "avg_logprob": -0.09649509358628888, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.0018670270219445229}, {"id": 4, "seek": 0, "start": 20.8, "end": 25.92, "text": " work, it's helpful to go back in time to the distant memories of 2020, and in particular,", "tokens": [51404, 589, 11, 309, 311, 4961, 281, 352, 646, 294, 565, 281, 264, 17275, 8495, 295, 4808, 11, 293, 294, 1729, 11, 51660], "temperature": 0.0, "avg_logprob": -0.09649509358628888, "compression_ratio": 1.6334519572953736, "no_speech_prob": 0.0018670270219445229}, {"id": 5, "seek": 2592, "start": 25.92, "end": 30.96, "text": " the Long Range Arena, a benchmark for efficient transformers. The starting premise was that", "tokens": [50364, 264, 8282, 33778, 34290, 11, 257, 18927, 337, 7148, 4088, 433, 13, 440, 2891, 22045, 390, 300, 50616], "temperature": 0.0, "avg_logprob": -0.07403871636641653, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.01969720795750618}, {"id": 6, "seek": 2592, "start": 30.96, "end": 36.0, "text": " transformers do not scale very well to long sequence lengths, largely because of quadratic", "tokens": [50616, 4088, 433, 360, 406, 4373, 588, 731, 281, 938, 8310, 26329, 11, 11611, 570, 295, 37262, 50868], "temperature": 0.0, "avg_logprob": -0.07403871636641653, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.01969720795750618}, {"id": 7, "seek": 2592, "start": 36.0, "end": 41.44, "text": " self-attention complexity. Given the proliferation of efficient alternatives to transformers that", "tokens": [50868, 2698, 12, 1591, 1251, 14024, 13, 18600, 264, 24398, 44987, 295, 7148, 20478, 281, 4088, 433, 300, 51140], "temperature": 0.0, "avg_logprob": -0.07403871636641653, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.01969720795750618}, {"id": 8, "seek": 2592, "start": 41.44, "end": 46.480000000000004, "text": " were emerging at the time, this work proposed a benchmark specifically focused on evaluating", "tokens": [51140, 645, 14989, 412, 264, 565, 11, 341, 589, 10348, 257, 18927, 4682, 5178, 322, 27479, 51392], "temperature": 0.0, "avg_logprob": -0.07403871636641653, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.01969720795750618}, {"id": 9, "seek": 2592, "start": 46.480000000000004, "end": 52.24, "text": " model quality under long context scenarios. There were tasks like long list ops, where the model", "tokens": [51392, 2316, 3125, 833, 938, 4319, 15077, 13, 821, 645, 9608, 411, 938, 1329, 44663, 11, 689, 264, 2316, 51680], "temperature": 0.0, "avg_logprob": -0.07403871636641653, "compression_ratio": 1.6845878136200716, "no_speech_prob": 0.01969720795750618}, {"id": 10, "seek": 5224, "start": 52.24, "end": 58.32, "text": " is given an input consisting of a long list of nested operators, and needs to calculate an answer,", "tokens": [50364, 307, 2212, 364, 4846, 33921, 295, 257, 938, 1329, 295, 15646, 292, 19077, 11, 293, 2203, 281, 8873, 364, 1867, 11, 50668], "temperature": 0.0, "avg_logprob": -0.06241038837264069, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.010649200528860092}, {"id": 11, "seek": 5224, "start": 58.32, "end": 63.68, "text": " and fun tasks like pathfinder, where the model gets a flattened image that looks like this,", "tokens": [50668, 293, 1019, 9608, 411, 3100, 38977, 11, 689, 264, 2316, 2170, 257, 24183, 292, 3256, 300, 1542, 411, 341, 11, 50936], "temperature": 0.0, "avg_logprob": -0.06241038837264069, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.010649200528860092}, {"id": 12, "seek": 5224, "start": 63.68, "end": 68.64, "text": " and needs to determine whether the two circles can be joined. This is a positive example because", "tokens": [50936, 293, 2203, 281, 6997, 1968, 264, 732, 13040, 393, 312, 6869, 13, 639, 307, 257, 3353, 1365, 570, 51184], "temperature": 0.0, "avg_logprob": -0.06241038837264069, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.010649200528860092}, {"id": 13, "seek": 5224, "start": 68.64, "end": 73.84, "text": " they can. This is a negative example because they cannot. Broadly, they found that if you wanted to", "tokens": [51184, 436, 393, 13, 639, 307, 257, 3671, 1365, 570, 436, 2644, 13, 14074, 356, 11, 436, 1352, 300, 498, 291, 1415, 281, 51444], "temperature": 0.0, "avg_logprob": -0.06241038837264069, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.010649200528860092}, {"id": 14, "seek": 5224, "start": 73.84, "end": 79.6, "text": " maintain performance encapsulated here in this LRA score, it's quite hard to do much better than", "tokens": [51444, 6909, 3389, 38745, 6987, 510, 294, 341, 441, 3750, 6175, 11, 309, 311, 1596, 1152, 281, 360, 709, 1101, 813, 51732], "temperature": 0.0, "avg_logprob": -0.06241038837264069, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.010649200528860092}, {"id": 15, "seek": 7960, "start": 79.6, "end": 84.88, "text": " the vanilla transformer. Independently, and a little earlier, a creative approach to tackling", "tokens": [50364, 264, 17528, 31782, 13, 21809, 2276, 11, 293, 257, 707, 3071, 11, 257, 5880, 3109, 281, 34415, 50628], "temperature": 0.0, "avg_logprob": -0.09296880318568303, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.005729140713810921}, {"id": 16, "seek": 7960, "start": 84.88, "end": 91.52, "text": " the Long Range Dependency problem was introduced in this work on Legend Memory Units, in the context", "tokens": [50628, 264, 8282, 33778, 4056, 521, 3020, 1154, 390, 7268, 294, 341, 589, 322, 21480, 38203, 1156, 1208, 11, 294, 264, 4319, 50960], "temperature": 0.0, "avg_logprob": -0.09296880318568303, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.005729140713810921}, {"id": 17, "seek": 7960, "start": 91.52, "end": 98.0, "text": " of recurrent neural networks. As a refresher, Legend polynomials are these beautiful creatures.", "tokens": [50960, 295, 18680, 1753, 18161, 9590, 13, 1018, 257, 17368, 511, 11, 21480, 22560, 12356, 366, 613, 2238, 12281, 13, 51284], "temperature": 0.0, "avg_logprob": -0.09296880318568303, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.005729140713810921}, {"id": 18, "seek": 7960, "start": 98.0, "end": 102.96, "text": " You can construct them by making a sequence of polynomials that are orthogonal to each other.", "tokens": [51284, 509, 393, 7690, 552, 538, 1455, 257, 8310, 295, 22560, 12356, 300, 366, 41488, 281, 1184, 661, 13, 51532], "temperature": 0.0, "avg_logprob": -0.09296880318568303, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.005729140713810921}, {"id": 19, "seek": 7960, "start": 102.96, "end": 108.72, "text": " By projecting values onto a basis of these polynomials, you can efficiently store a history", "tokens": [51532, 3146, 43001, 4190, 3911, 257, 5143, 295, 613, 22560, 12356, 11, 291, 393, 19621, 3531, 257, 2503, 51820], "temperature": 0.0, "avg_logprob": -0.09296880318568303, "compression_ratio": 1.6245733788395904, "no_speech_prob": 0.005729140713810921}, {"id": 20, "seek": 10872, "start": 108.72, "end": 113.03999999999999, "text": " of the inputs to your system. That's the key idea in this funky looking equation.", "tokens": [50364, 295, 264, 15743, 281, 428, 1185, 13, 663, 311, 264, 2141, 1558, 294, 341, 33499, 1237, 5367, 13, 50580], "temperature": 0.0, "avg_logprob": -0.07516256818231547, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001284065656363964}, {"id": 21, "seek": 10872, "start": 113.03999999999999, "end": 119.44, "text": " You can reconstruct the input U by reassembling it from the components of the memory vector,", "tokens": [50580, 509, 393, 31499, 264, 4846, 624, 538, 319, 29386, 1688, 309, 490, 264, 6677, 295, 264, 4675, 8062, 11, 50900], "temperature": 0.0, "avg_logprob": -0.07516256818231547, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001284065656363964}, {"id": 22, "seek": 10872, "start": 119.44, "end": 125.12, "text": " denoted here by M. This idea can then be baked into a layer where the memory block is implemented", "tokens": [50900, 1441, 23325, 510, 538, 376, 13, 639, 1558, 393, 550, 312, 19453, 666, 257, 4583, 689, 264, 4675, 3461, 307, 12270, 51184], "temperature": 0.0, "avg_logprob": -0.07516256818231547, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001284065656363964}, {"id": 23, "seek": 10872, "start": 125.12, "end": 131.2, "text": " with simple linear operations. Building on that idea, and others, Hippo, recurrent memory with", "tokens": [51184, 365, 2199, 8213, 7705, 13, 18974, 322, 300, 1558, 11, 293, 2357, 11, 2421, 27000, 11, 18680, 1753, 4675, 365, 51488], "temperature": 0.0, "avg_logprob": -0.07516256818231547, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001284065656363964}, {"id": 24, "seek": 10872, "start": 131.2, "end": 137.52, "text": " optimal polynomial projections, had the insight that we can phrase memory as a technical problem", "tokens": [51488, 16252, 26110, 32371, 11, 632, 264, 11269, 300, 321, 393, 9535, 4675, 382, 257, 6191, 1154, 51804], "temperature": 0.0, "avg_logprob": -0.07516256818231547, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001284065656363964}, {"id": 25, "seek": 13752, "start": 137.52, "end": 142.48000000000002, "text": " of online function approximation, where a function is summarized by storing its optimal", "tokens": [50364, 295, 2950, 2445, 28023, 11, 689, 257, 2445, 307, 14611, 1602, 538, 26085, 1080, 16252, 50612], "temperature": 0.0, "avg_logprob": -0.10429731041494042, "compression_ratio": 1.6, "no_speech_prob": 0.0019264908041805029}, {"id": 26, "seek": 13752, "start": 142.48000000000002, "end": 148.08, "text": " coefficients in terms of some basis functions. Using this insight, the authors constructed a", "tokens": [50612, 31994, 294, 2115, 295, 512, 5143, 6828, 13, 11142, 341, 11269, 11, 264, 16552, 17083, 257, 50892], "temperature": 0.0, "avg_logprob": -0.10429731041494042, "compression_ratio": 1.6, "no_speech_prob": 0.0019264908041805029}, {"id": 27, "seek": 13752, "start": 148.08, "end": 153.60000000000002, "text": " simple model based on Legendre polynomials that worked well on tasks like permuted MNIST,", "tokens": [50892, 2199, 2316, 2361, 322, 21480, 265, 22560, 12356, 300, 2732, 731, 322, 9608, 411, 4784, 4866, 376, 45, 19756, 11, 51168], "temperature": 0.0, "avg_logprob": -0.10429731041494042, "compression_ratio": 1.6, "no_speech_prob": 0.0019264908041805029}, {"id": 28, "seek": 13752, "start": 153.60000000000002, "end": 159.52, "text": " outperforming other sequence model-based baselines like LSTM and dilated RNN. This was", "tokens": [51168, 484, 26765, 278, 661, 8310, 2316, 12, 6032, 987, 9173, 411, 441, 6840, 44, 293, 11504, 770, 45702, 45, 13, 639, 390, 51464], "temperature": 0.0, "avg_logprob": -0.10429731041494042, "compression_ratio": 1.6, "no_speech_prob": 0.0019264908041805029}, {"id": 29, "seek": 13752, "start": 159.52, "end": 164.48000000000002, "text": " followed by the work, combining recurrent convolutional and continuous time models", "tokens": [51464, 6263, 538, 264, 589, 11, 21928, 18680, 1753, 45216, 304, 293, 10957, 565, 5245, 51712], "temperature": 0.0, "avg_logprob": -0.10429731041494042, "compression_ratio": 1.6, "no_speech_prob": 0.0019264908041805029}, {"id": 30, "seek": 16448, "start": 164.48, "end": 169.92, "text": " with linear state space layers, which proposed a unifying framework for sequence modelling", "tokens": [50364, 365, 8213, 1785, 1901, 7914, 11, 597, 10348, 257, 517, 5489, 8388, 337, 8310, 42253, 50636], "temperature": 0.0, "avg_logprob": -0.08628624363949425, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.003272600704804063}, {"id": 31, "seek": 16448, "start": 169.92, "end": 175.67999999999998, "text": " based on a standard state space representation. This work showed how, starting from an implicit,", "tokens": [50636, 2361, 322, 257, 3832, 1785, 1901, 10290, 13, 639, 589, 4712, 577, 11, 2891, 490, 364, 26947, 11, 50924], "temperature": 0.0, "avg_logprob": -0.08628624363949425, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.003272600704804063}, {"id": 32, "seek": 16448, "start": 175.67999999999998, "end": 181.12, "text": " continuous time state space model, you could discretize to produce a system that can be", "tokens": [50924, 10957, 565, 1785, 1901, 2316, 11, 291, 727, 25656, 1125, 281, 5258, 257, 1185, 300, 393, 312, 51196], "temperature": 0.0, "avg_logprob": -0.08628624363949425, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.003272600704804063}, {"id": 33, "seek": 16448, "start": 181.12, "end": 187.76, "text": " interpreted as a recurrent network with the benefit of efficient inference, or as a convolutional", "tokens": [51196, 26749, 382, 257, 18680, 1753, 3209, 365, 264, 5121, 295, 7148, 38253, 11, 420, 382, 257, 45216, 304, 51528], "temperature": 0.0, "avg_logprob": -0.08628624363949425, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.003272600704804063}, {"id": 34, "seek": 16448, "start": 187.76, "end": 193.12, "text": " model which benefits from parallelizable training. Unfortunately, a naive implementation of these", "tokens": [51528, 2316, 597, 5311, 490, 8952, 22395, 3097, 13, 8590, 11, 257, 29052, 11420, 295, 613, 51796], "temperature": 0.0, "avg_logprob": -0.08628624363949425, "compression_ratio": 1.7065217391304348, "no_speech_prob": 0.003272600704804063}, {"id": 35, "seek": 19312, "start": 193.12, "end": 198.8, "text": " models requires a massive amount of memory. In fact, for reasonably sized models, such as when", "tokens": [50364, 5245, 7029, 257, 5994, 2372, 295, 4675, 13, 682, 1186, 11, 337, 23551, 20004, 5245, 11, 1270, 382, 562, 50648], "temperature": 0.0, "avg_logprob": -0.04788840491816683, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0005702711059711874}, {"id": 36, "seek": 19312, "start": 198.8, "end": 205.44, "text": " the state dimension is 256, the linear state space layer uses orders of magnitude more memory than", "tokens": [50648, 264, 1785, 10139, 307, 38882, 11, 264, 8213, 1785, 1901, 4583, 4960, 9470, 295, 15668, 544, 4675, 813, 50980], "temperature": 0.0, "avg_logprob": -0.04788840491816683, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0005702711059711874}, {"id": 37, "seek": 19312, "start": 205.44, "end": 212.0, "text": " comparably sized RNNs or CNNs. That brings us to the S4 model introduced in efficiently modelling", "tokens": [50980, 6311, 1188, 20004, 45702, 45, 82, 420, 24859, 82, 13, 663, 5607, 505, 281, 264, 318, 19, 2316, 7268, 294, 19621, 42253, 51308], "temperature": 0.0, "avg_logprob": -0.04788840491816683, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0005702711059711874}, {"id": 38, "seek": 19312, "start": 212.0, "end": 217.20000000000002, "text": " long sequences with structured state spaces. What we'd really like to be able to do is use the", "tokens": [51308, 938, 22978, 365, 18519, 1785, 7673, 13, 708, 321, 1116, 534, 411, 281, 312, 1075, 281, 360, 307, 764, 264, 51568], "temperature": 0.0, "avg_logprob": -0.04788840491816683, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0005702711059711874}, {"id": 39, "seek": 19312, "start": 217.20000000000002, "end": 222.4, "text": " convolutional interpretation we've just seen to efficiently train our state space model.", "tokens": [51568, 45216, 304, 14174, 321, 600, 445, 1612, 281, 19621, 3847, 527, 1785, 1901, 2316, 13, 51828], "temperature": 0.0, "avg_logprob": -0.04788840491816683, "compression_ratio": 1.6550522648083623, "no_speech_prob": 0.0005702711059711874}, {"id": 40, "seek": 22240, "start": 222.4, "end": 228.32, "text": " Now, if we want to train state space models using the convolutional representation, we can do a", "tokens": [50364, 823, 11, 498, 321, 528, 281, 3847, 1785, 1901, 5245, 1228, 264, 45216, 304, 10290, 11, 321, 393, 360, 257, 50660], "temperature": 0.0, "avg_logprob": -0.08067991903850011, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00020986561139579862}, {"id": 41, "seek": 22240, "start": 228.32, "end": 233.6, "text": " little bit of unrolling and a little bit of vectorization to get this L-dimensional kernel,", "tokens": [50660, 707, 857, 295, 517, 18688, 293, 257, 707, 857, 295, 8062, 2144, 281, 483, 341, 441, 12, 18759, 28256, 11, 50924], "temperature": 0.0, "avg_logprob": -0.08067991903850011, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00020986561139579862}, {"id": 42, "seek": 22240, "start": 233.6, "end": 239.52, "text": " where L is the sequence length. The scary part here is this A raised to the power of I term.", "tokens": [50924, 689, 441, 307, 264, 8310, 4641, 13, 440, 6958, 644, 510, 307, 341, 316, 6005, 281, 264, 1347, 295, 286, 1433, 13, 51220], "temperature": 0.0, "avg_logprob": -0.08067991903850011, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00020986561139579862}, {"id": 43, "seek": 22240, "start": 239.52, "end": 244.72, "text": " Harking back to your linear algebra classes, as soon as you see this, you may feel a strong", "tokens": [51220, 389, 809, 278, 646, 281, 428, 8213, 21989, 5359, 11, 382, 2321, 382, 291, 536, 341, 11, 291, 815, 841, 257, 2068, 51480], "temperature": 0.0, "avg_logprob": -0.08067991903850011, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00020986561139579862}, {"id": 44, "seek": 22240, "start": 244.72, "end": 250.4, "text": " and appropriate urge to try to diagonalize A. The authors discuss diagonalization.", "tokens": [51480, 293, 6854, 19029, 281, 853, 281, 21539, 1125, 316, 13, 440, 16552, 2248, 21539, 2144, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08067991903850011, "compression_ratio": 1.6485507246376812, "no_speech_prob": 0.00020986561139579862}, {"id": 45, "seek": 25040, "start": 250.4, "end": 255.68, "text": " Unfortunately, it cannot be used directly for the hippo matrix due to numerical issues.", "tokens": [50364, 8590, 11, 309, 2644, 312, 1143, 3838, 337, 264, 27745, 78, 8141, 3462, 281, 29054, 2663, 13, 50628], "temperature": 0.0, "avg_logprob": -0.05896956898341669, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.0003459615109022707}, {"id": 46, "seek": 25040, "start": 255.68, "end": 261.28000000000003, "text": " Specifically, it has entries exponentially large in the state size. However, we can address the", "tokens": [50628, 26058, 11, 309, 575, 23041, 37330, 2416, 294, 264, 1785, 2744, 13, 2908, 11, 321, 393, 2985, 264, 50908], "temperature": 0.0, "avg_logprob": -0.05896956898341669, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.0003459615109022707}, {"id": 47, "seek": 25040, "start": 261.28000000000003, "end": 267.12, "text": " hippo matrix with an adjustment. The authors note that although the hippo matrix is not normal,", "tokens": [50908, 27745, 78, 8141, 365, 364, 17132, 13, 440, 16552, 3637, 300, 4878, 264, 27745, 78, 8141, 307, 406, 2710, 11, 51200], "temperature": 0.0, "avg_logprob": -0.05896956898341669, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.0003459615109022707}, {"id": 48, "seek": 25040, "start": 267.12, "end": 273.36, "text": " it can be decomposed as the sum of a normal and low rank matrix. Even this is still not useful", "tokens": [51200, 309, 393, 312, 22867, 1744, 382, 264, 2408, 295, 257, 2710, 293, 2295, 6181, 8141, 13, 2754, 341, 307, 920, 406, 4420, 51512], "temperature": 0.0, "avg_logprob": -0.05896956898341669, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.0003459615109022707}, {"id": 49, "seek": 25040, "start": 273.36, "end": 278.72, "text": " by itself. An additional three new techniques are needed. First, instead of computing the kernel", "tokens": [51512, 538, 2564, 13, 1107, 4497, 1045, 777, 7512, 366, 2978, 13, 2386, 11, 2602, 295, 15866, 264, 28256, 51780], "temperature": 0.0, "avg_logprob": -0.05896956898341669, "compression_ratio": 1.6881720430107527, "no_speech_prob": 0.0003459615109022707}, {"id": 50, "seek": 27872, "start": 278.72, "end": 285.36, "text": " k-bar directly, they instead compute its spectrum, then determine k-bar by applying an inverse FFT.", "tokens": [50364, 350, 12, 5356, 3838, 11, 436, 2602, 14722, 1080, 11143, 11, 550, 6997, 350, 12, 5356, 538, 9275, 364, 17340, 479, 25469, 13, 50696], "temperature": 0.0, "avg_logprob": -0.1028248994246773, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.00033534009708091617}, {"id": 51, "seek": 27872, "start": 285.36, "end": 290.72, "text": " Next, we use everyone's favorite low rank trick, the Woodbury identity. That lets us perform an", "tokens": [50696, 3087, 11, 321, 764, 1518, 311, 2954, 2295, 6181, 4282, 11, 264, 11558, 22536, 6575, 13, 663, 6653, 505, 2042, 364, 50964], "temperature": 0.0, "avg_logprob": -0.1028248994246773, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.00033534009708091617}, {"id": 52, "seek": 27872, "start": 290.72, "end": 296.16, "text": " efficient matrix update. Third, it is shown that the diagonal matrix case is equivalent to the", "tokens": [50964, 7148, 8141, 5623, 13, 12548, 11, 309, 307, 4898, 300, 264, 21539, 8141, 1389, 307, 10344, 281, 264, 51236], "temperature": 0.0, "avg_logprob": -0.1028248994246773, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.00033534009708091617}, {"id": 53, "seek": 27872, "start": 296.16, "end": 301.20000000000005, "text": " computation of a Cauchy kernel, a well-studied problem with stable, near-linear algorithms.", "tokens": [51236, 24903, 295, 257, 7544, 625, 88, 28256, 11, 257, 731, 12, 28349, 1091, 1154, 365, 8351, 11, 2651, 12, 28263, 14642, 13, 51488], "temperature": 0.0, "avg_logprob": -0.1028248994246773, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.00033534009708091617}, {"id": 54, "seek": 27872, "start": 301.20000000000005, "end": 306.56, "text": " All of this comes together in theorem three. It says that given any step size delta, computing", "tokens": [51488, 1057, 295, 341, 1487, 1214, 294, 20904, 1045, 13, 467, 1619, 300, 2212, 604, 1823, 2744, 8289, 11, 15866, 51756], "temperature": 0.0, "avg_logprob": -0.1028248994246773, "compression_ratio": 1.5436893203883495, "no_speech_prob": 0.00033534009708091617}, {"id": 55, "seek": 30656, "start": 306.56, "end": 312.16, "text": " the state space model convolution filter k-bar can be reduced to four Cauchy multiplies,", "tokens": [50364, 264, 1785, 1901, 2316, 45216, 6608, 350, 12, 5356, 393, 312, 9212, 281, 1451, 7544, 625, 88, 12788, 530, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08189406066105284, "compression_ratio": 1.6321428571428571, "no_speech_prob": 0.0007553648320026696}, {"id": 56, "seek": 30656, "start": 312.16, "end": 318.8, "text": " requiring only big O of n plus l operations, ignoring logarithmic factors, and big O of", "tokens": [50644, 24165, 787, 955, 422, 295, 297, 1804, 287, 7705, 11, 26258, 41473, 355, 13195, 6771, 11, 293, 955, 422, 295, 50976], "temperature": 0.0, "avg_logprob": -0.08189406066105284, "compression_ratio": 1.6321428571428571, "no_speech_prob": 0.0007553648320026696}, {"id": 57, "seek": 30656, "start": 318.8, "end": 325.12, "text": " n plus l space, where again, l is the sequence length and n is the state size. Empirically,", "tokens": [50976, 297, 1804, 287, 1901, 11, 689, 797, 11, 287, 307, 264, 8310, 4641, 293, 297, 307, 264, 1785, 2744, 13, 8599, 347, 984, 11, 51292], "temperature": 0.0, "avg_logprob": -0.08189406066105284, "compression_ratio": 1.6321428571428571, "no_speech_prob": 0.0007553648320026696}, {"id": 58, "seek": 30656, "start": 325.12, "end": 331.12, "text": " the proposed S4 architecture does very well on the long-range arena, even doing well on the path", "tokens": [51292, 264, 10348, 318, 19, 9482, 775, 588, 731, 322, 264, 938, 12, 14521, 18451, 11, 754, 884, 731, 322, 264, 3100, 51592], "temperature": 0.0, "avg_logprob": -0.08189406066105284, "compression_ratio": 1.6321428571428571, "no_speech_prob": 0.0007553648320026696}, {"id": 59, "seek": 30656, "start": 331.12, "end": 336.24, "text": " x task, where the model must determine if two points are connected on a fairly large image.", "tokens": [51592, 2031, 5633, 11, 689, 264, 2316, 1633, 6997, 498, 732, 2793, 366, 4582, 322, 257, 6457, 2416, 3256, 13, 51848], "temperature": 0.0, "avg_logprob": -0.08189406066105284, "compression_ratio": 1.6321428571428571, "no_speech_prob": 0.0007553648320026696}, {"id": 60, "seek": 33624, "start": 336.24, "end": 341.12, "text": " This task is not too hard when the input is arranged as an image, but it's pretty difficult", "tokens": [50364, 639, 5633, 307, 406, 886, 1152, 562, 264, 4846, 307, 18721, 382, 364, 3256, 11, 457, 309, 311, 1238, 2252, 50608], "temperature": 0.0, "avg_logprob": -0.07636981365109279, "compression_ratio": 1.5838926174496644, "no_speech_prob": 0.0006262626848183572}, {"id": 61, "seek": 33624, "start": 341.12, "end": 346.64, "text": " when it's a flattened sequence, as evidenced by the fact that many other models fail on this task.", "tokens": [50608, 562, 309, 311, 257, 24183, 292, 8310, 11, 382, 43699, 1232, 538, 264, 1186, 300, 867, 661, 5245, 3061, 322, 341, 5633, 13, 50884], "temperature": 0.0, "avg_logprob": -0.07636981365109279, "compression_ratio": 1.5838926174496644, "no_speech_prob": 0.0006262626848183572}, {"id": 62, "seek": 33624, "start": 346.64, "end": 352.8, "text": " If you'd like to gain some intuition for how S4 works, I highly recommend the annotated S4", "tokens": [50884, 759, 291, 1116, 411, 281, 6052, 512, 24002, 337, 577, 318, 19, 1985, 11, 286, 5405, 2748, 264, 25339, 770, 318, 19, 51192], "temperature": 0.0, "avg_logprob": -0.07636981365109279, "compression_ratio": 1.5838926174496644, "no_speech_prob": 0.0006262626848183572}, {"id": 63, "seek": 33624, "start": 352.8, "end": 359.52, "text": " by Sasha Rush and Sid Karamchetti, just as with the legendary annotated transformer blog post", "tokens": [51192, 538, 29276, 28389, 293, 19797, 8009, 335, 339, 12495, 11, 445, 382, 365, 264, 16698, 25339, 770, 31782, 6968, 2183, 51528], "temperature": 0.0, "avg_logprob": -0.07636981365109279, "compression_ratio": 1.5838926174496644, "no_speech_prob": 0.0006262626848183572}, {"id": 64, "seek": 33624, "start": 359.52, "end": 365.92, "text": " back in 2018. The annotated S4 takes you step-by-step through the paper and provides snippets of", "tokens": [51528, 646, 294, 6096, 13, 440, 25339, 770, 318, 19, 2516, 291, 1823, 12, 2322, 12, 16792, 807, 264, 3035, 293, 6417, 35623, 1385, 295, 51848], "temperature": 0.0, "avg_logprob": -0.07636981365109279, "compression_ratio": 1.5838926174496644, "no_speech_prob": 0.0006262626848183572}, {"id": 65, "seek": 36592, "start": 365.92, "end": 371.2, "text": " code that implement the mathematics. It also includes nice visualizations to help you build", "tokens": [50364, 3089, 300, 4445, 264, 18666, 13, 467, 611, 5974, 1481, 5056, 14455, 281, 854, 291, 1322, 50628], "temperature": 0.0, "avg_logprob": -0.07645389920189267, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0005526773747988045}, {"id": 66, "seek": 36592, "start": 371.2, "end": 376.8, "text": " intuition for how state-space models work. All of this brings us back to Mamba. This work builds", "tokens": [50628, 24002, 337, 577, 1785, 12, 24824, 5245, 589, 13, 1057, 295, 341, 5607, 505, 646, 281, 376, 23337, 13, 639, 589, 15182, 50908], "temperature": 0.0, "avg_logprob": -0.07645389920189267, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0005526773747988045}, {"id": 67, "seek": 36592, "start": 376.8, "end": 382.72, "text": " on the state-space model approach with a few key contributions. First, a selection mechanism. Since", "tokens": [50908, 322, 264, 1785, 12, 24824, 2316, 3109, 365, 257, 1326, 2141, 15725, 13, 2386, 11, 257, 9450, 7513, 13, 4162, 51204], "temperature": 0.0, "avg_logprob": -0.07645389920189267, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0005526773747988045}, {"id": 68, "seek": 36592, "start": 382.72, "end": 388.32, "text": " previous state-space models lack the ability to efficiently select data in an input-dependent", "tokens": [51204, 3894, 1785, 12, 24824, 5245, 5011, 264, 3485, 281, 19621, 3048, 1412, 294, 364, 4846, 12, 36763, 317, 51484], "temperature": 0.0, "avg_logprob": -0.07645389920189267, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0005526773747988045}, {"id": 69, "seek": 36592, "start": 388.32, "end": 393.76, "text": " manner, they design a simple selection mechanism by parameterizing the state-space model parameters", "tokens": [51484, 9060, 11, 436, 1715, 257, 2199, 9450, 7513, 538, 13075, 3319, 264, 1785, 12, 24824, 2316, 9834, 51756], "temperature": 0.0, "avg_logprob": -0.07645389920189267, "compression_ratio": 1.7655677655677655, "no_speech_prob": 0.0005526773747988045}, {"id": 70, "seek": 39376, "start": 393.76, "end": 399.28, "text": " based on the input. Unfortunately, this change makes it hard for us to use the efficient convolutional", "tokens": [50364, 2361, 322, 264, 4846, 13, 8590, 11, 341, 1319, 1669, 309, 1152, 337, 505, 281, 764, 264, 7148, 45216, 304, 50640], "temperature": 0.0, "avg_logprob": -0.07350653409957886, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.0019567208364605904}, {"id": 71, "seek": 39376, "start": 399.28, "end": 405.28, "text": " trick we saw in S4, so the second contribution is a hardware-aware algorithm that computes the", "tokens": [50640, 4282, 321, 1866, 294, 318, 19, 11, 370, 264, 1150, 13150, 307, 257, 8837, 12, 17074, 9284, 300, 715, 1819, 264, 50940], "temperature": 0.0, "avg_logprob": -0.07350653409957886, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.0019567208364605904}, {"id": 72, "seek": 39376, "start": 405.28, "end": 410.64, "text": " model recurrently with a scan instead of convolution. This leads to an implementation that is faster", "tokens": [50940, 2316, 18680, 1753, 356, 365, 257, 11049, 2602, 295, 45216, 13, 639, 6689, 281, 364, 11420, 300, 307, 4663, 51208], "temperature": 0.0, "avg_logprob": -0.07350653409957886, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.0019567208364605904}, {"id": 73, "seek": 39376, "start": 410.64, "end": 416.4, "text": " than previous methods, both in theory, scaling linearly in sequence length compared to pseudo-linear", "tokens": [51208, 813, 3894, 7150, 11, 1293, 294, 5261, 11, 21589, 43586, 294, 8310, 4641, 5347, 281, 35899, 12, 28263, 51496], "temperature": 0.0, "avg_logprob": -0.07350653409957886, "compression_ratio": 1.6219512195121952, "no_speech_prob": 0.0019567208364605904}, {"id": 74, "seek": 41640, "start": 416.4, "end": 423.28, "text": " for all convolution-based SSMs and on modern hardware, up to three times faster on A100 GPUs.", "tokens": [50364, 337, 439, 45216, 12, 6032, 12238, 26386, 293, 322, 4363, 8837, 11, 493, 281, 1045, 1413, 4663, 322, 316, 6879, 18407, 82, 13, 50708], "temperature": 0.0, "avg_logprob": -0.09389099984798792, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.04884050413966179}, {"id": 75, "seek": 41640, "start": 423.28, "end": 428.64, "text": " Spicy. The third contribution is to simplify prior deep-sequence model architectures by", "tokens": [50708, 35999, 13, 440, 2636, 13150, 307, 281, 20460, 4059, 2452, 12, 11834, 655, 2316, 6331, 1303, 538, 50976], "temperature": 0.0, "avg_logprob": -0.09389099984798792, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.04884050413966179}, {"id": 76, "seek": 41640, "start": 428.64, "end": 434.4, "text": " combining the design of prior state-space model architectures with the MLP block of transformers", "tokens": [50976, 21928, 264, 1715, 295, 4059, 1785, 12, 24824, 2316, 6331, 1303, 365, 264, 21601, 47, 3461, 295, 4088, 433, 51264], "temperature": 0.0, "avg_logprob": -0.09389099984798792, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.04884050413966179}, {"id": 77, "seek": 41640, "start": 434.4, "end": 439.44, "text": " into a single block, leading to a simple and homogenous architecture design called Mamba", "tokens": [51264, 666, 257, 2167, 3461, 11, 5775, 281, 257, 2199, 293, 3655, 45519, 9482, 1715, 1219, 376, 23337, 51516], "temperature": 0.0, "avg_logprob": -0.09389099984798792, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.04884050413966179}, {"id": 78, "seek": 41640, "start": 439.44, "end": 444.64, "text": " that incorporates selective state-spaces. By building on a state-space model foundation,", "tokens": [51516, 300, 50193, 33930, 1785, 12, 4952, 2116, 13, 3146, 2390, 322, 257, 1785, 12, 24824, 2316, 7030, 11, 51776], "temperature": 0.0, "avg_logprob": -0.09389099984798792, "compression_ratio": 1.6764705882352942, "no_speech_prob": 0.04884050413966179}, {"id": 79, "seek": 44464, "start": 444.64, "end": 449.76, "text": " Mamba is related to many previous state-space model architectures. In addition to the models we", "tokens": [50364, 376, 23337, 307, 4077, 281, 867, 3894, 1785, 12, 24824, 2316, 6331, 1303, 13, 682, 4500, 281, 264, 5245, 321, 50620], "temperature": 0.0, "avg_logprob": -0.11918993829523475, "compression_ratio": 1.5296442687747036, "no_speech_prob": 0.0003352812200319022}, {"id": 80, "seek": 44464, "start": 449.76, "end": 456.4, "text": " met earlier, there is Linear Attention, which can be viewed as a degenerate linear SSM, the gloriously", "tokens": [50620, 1131, 3071, 11, 456, 307, 14670, 289, 31858, 11, 597, 393, 312, 19174, 382, 257, 40520, 473, 8213, 12238, 44, 11, 264, 26623, 8994, 50952], "temperature": 0.0, "avg_logprob": -0.11918993829523475, "compression_ratio": 1.5296442687747036, "no_speech_prob": 0.0003352812200319022}, {"id": 81, "seek": 44464, "start": 456.4, "end": 465.03999999999996, "text": " named Hungry Hungry Hippos or H3, Hyena, RhettNet, and RWKV. The core motivation behind Mamba is to", "tokens": [50952, 4926, 15063, 627, 15063, 627, 2421, 427, 329, 420, 389, 18, 11, 5701, 4118, 11, 16111, 3093, 31890, 11, 293, 42513, 42, 53, 13, 440, 4965, 12335, 2261, 376, 23337, 307, 281, 51384], "temperature": 0.0, "avg_logprob": -0.11918993829523475, "compression_ratio": 1.5296442687747036, "no_speech_prob": 0.0003352812200319022}, {"id": 82, "seek": 44464, "start": 465.03999999999996, "end": 469.76, "text": " use selection as a means of compression. The authors point out that you can view various", "tokens": [51384, 764, 9450, 382, 257, 1355, 295, 19355, 13, 440, 16552, 935, 484, 300, 291, 393, 1910, 3683, 51620], "temperature": 0.0, "avg_logprob": -0.11918993829523475, "compression_ratio": 1.5296442687747036, "no_speech_prob": 0.0003352812200319022}, {"id": 83, "seek": 46976, "start": 469.76, "end": 475.12, "text": " sequence models as making different trade-offs when tackling a fundamental problem of sequence", "tokens": [50364, 8310, 5245, 382, 1455, 819, 4923, 12, 19231, 562, 34415, 257, 8088, 1154, 295, 8310, 50632], "temperature": 0.0, "avg_logprob": -0.06033221639768042, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.011329996399581432}, {"id": 84, "seek": 46976, "start": 475.12, "end": 480.8, "text": " modeling, i.e. compressing context into a smaller state. At one extreme, we have Attention, which", "tokens": [50632, 15983, 11, 741, 13, 68, 13, 14778, 278, 4319, 666, 257, 4356, 1785, 13, 1711, 472, 8084, 11, 321, 362, 31858, 11, 597, 50916], "temperature": 0.0, "avg_logprob": -0.06033221639768042, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.011329996399581432}, {"id": 85, "seek": 46976, "start": 480.8, "end": 485.92, "text": " is both effective and inefficient because it explicitly does not compress context at all.", "tokens": [50916, 307, 1293, 4942, 293, 43495, 570, 309, 20803, 775, 406, 14778, 4319, 412, 439, 13, 51172], "temperature": 0.0, "avg_logprob": -0.06033221639768042, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.011329996399581432}, {"id": 86, "seek": 46976, "start": 485.92, "end": 490.56, "text": " At the other extreme, recurrent models are efficient because they have a finite state,", "tokens": [51172, 1711, 264, 661, 8084, 11, 18680, 1753, 5245, 366, 7148, 570, 436, 362, 257, 19362, 1785, 11, 51404], "temperature": 0.0, "avg_logprob": -0.06033221639768042, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.011329996399581432}, {"id": 87, "seek": 46976, "start": 490.56, "end": 495.68, "text": " but their effectiveness is limited by how well this state has compressed the context. To explore", "tokens": [51404, 457, 641, 21208, 307, 5567, 538, 577, 731, 341, 1785, 575, 30353, 264, 4319, 13, 1407, 6839, 51660], "temperature": 0.0, "avg_logprob": -0.06033221639768042, "compression_ratio": 1.7923076923076924, "no_speech_prob": 0.011329996399581432}, {"id": 88, "seek": 49568, "start": 495.68, "end": 502.32, "text": " this trade-off, the authors focus on two synthetic tasks. One previously well-studied task is copying.", "tokens": [50364, 341, 4923, 12, 4506, 11, 264, 16552, 1879, 322, 732, 23420, 9608, 13, 1485, 8046, 731, 12, 28349, 1091, 5633, 307, 27976, 13, 50696], "temperature": 0.0, "avg_logprob": -0.062023336642256406, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0012842521537095308}, {"id": 89, "seek": 49568, "start": 502.32, "end": 508.08, "text": " The job of the model is to copy color sequences in the input into the output, delayed by some", "tokens": [50696, 440, 1691, 295, 264, 2316, 307, 281, 5055, 2017, 22978, 294, 264, 4846, 666, 264, 5598, 11, 20268, 538, 512, 50984], "temperature": 0.0, "avg_logprob": -0.062023336642256406, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0012842521537095308}, {"id": 90, "seek": 49568, "start": 508.08, "end": 513.04, "text": " constant offset. Standard convolutional models with fixed kernels can solve this without any", "tokens": [50984, 5754, 18687, 13, 21298, 45216, 304, 5245, 365, 6806, 23434, 1625, 393, 5039, 341, 1553, 604, 51232], "temperature": 0.0, "avg_logprob": -0.062023336642256406, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0012842521537095308}, {"id": 91, "seek": 49568, "start": 513.04, "end": 518.24, "text": " problem. The first of the harder tasks they consider is selective copying. This time, we have", "tokens": [51232, 1154, 13, 440, 700, 295, 264, 6081, 9608, 436, 1949, 307, 33930, 27976, 13, 639, 565, 11, 321, 362, 51492], "temperature": 0.0, "avg_logprob": -0.062023336642256406, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0012842521537095308}, {"id": 92, "seek": 49568, "start": 518.24, "end": 524.5600000000001, "text": " random spaces symbolized by the white blocks in between the input sequence elements. To successfully", "tokens": [51492, 4974, 7673, 5986, 1602, 538, 264, 2418, 8474, 294, 1296, 264, 4846, 8310, 4959, 13, 1407, 10727, 51808], "temperature": 0.0, "avg_logprob": -0.062023336642256406, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0012842521537095308}, {"id": 93, "seek": 52456, "start": 524.56, "end": 530.3199999999999, "text": " copy the color outputs to the output while ignoring the white blocks, we need a mechanism that behaves", "tokens": [50364, 5055, 264, 2017, 23930, 281, 264, 5598, 1339, 26258, 264, 2418, 8474, 11, 321, 643, 257, 7513, 300, 36896, 50652], "temperature": 0.0, "avg_logprob": -0.05073141383233471, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.00012338841042947024}, {"id": 94, "seek": 52456, "start": 530.3199999999999, "end": 535.76, "text": " differently at different inputs. The second task they look at is induction heads. Here,", "tokens": [50652, 7614, 412, 819, 15743, 13, 440, 1150, 5633, 436, 574, 412, 307, 33371, 8050, 13, 1692, 11, 50924], "temperature": 0.0, "avg_logprob": -0.05073141383233471, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.00012338841042947024}, {"id": 95, "seek": 52456, "start": 535.76, "end": 540.2399999999999, "text": " in order to predict the appropriate color at the question mark block, the model needs to be able", "tokens": [50924, 294, 1668, 281, 6069, 264, 6854, 2017, 412, 264, 1168, 1491, 3461, 11, 264, 2316, 2203, 281, 312, 1075, 51148], "temperature": 0.0, "avg_logprob": -0.05073141383233471, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.00012338841042947024}, {"id": 96, "seek": 52456, "start": 540.2399999999999, "end": 545.92, "text": " to perform retrieval back over the input sequence based on the context provided by the black square,", "tokens": [51148, 281, 2042, 19817, 3337, 646, 670, 264, 4846, 8310, 2361, 322, 264, 4319, 5649, 538, 264, 2211, 3732, 11, 51432], "temperature": 0.0, "avg_logprob": -0.05073141383233471, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.00012338841042947024}, {"id": 97, "seek": 52456, "start": 545.92, "end": 551.1999999999999, "text": " in order to predict that blue is the next color in the sequence. To build a model capable of solving", "tokens": [51432, 294, 1668, 281, 6069, 300, 3344, 307, 264, 958, 2017, 294, 264, 8310, 13, 1407, 1322, 257, 2316, 8189, 295, 12606, 51696], "temperature": 0.0, "avg_logprob": -0.05073141383233471, "compression_ratio": 1.8111111111111111, "no_speech_prob": 0.00012338841042947024}, {"id": 98, "seek": 55120, "start": 551.2, "end": 558.6400000000001, "text": " these tasks, the models start from the S4 state space model, which uses parameters a, b, c, and", "tokens": [50364, 613, 9608, 11, 264, 5245, 722, 490, 264, 318, 19, 1785, 1901, 2316, 11, 597, 4960, 9834, 257, 11, 272, 11, 269, 11, 293, 50736], "temperature": 0.0, "avg_logprob": -0.07363007115382775, "compression_ratio": 1.751111111111111, "no_speech_prob": 0.0013669648906216025}, {"id": 99, "seek": 55120, "start": 558.6400000000001, "end": 564.48, "text": " delta, which don't depend on the input sequence. Now, for the proposed algorithm, to allow the model", "tokens": [50736, 8289, 11, 597, 500, 380, 5672, 322, 264, 4846, 8310, 13, 823, 11, 337, 264, 10348, 9284, 11, 281, 2089, 264, 2316, 51028], "temperature": 0.0, "avg_logprob": -0.07363007115382775, "compression_ratio": 1.751111111111111, "no_speech_prob": 0.0013669648906216025}, {"id": 100, "seek": 55120, "start": 564.48, "end": 571.6800000000001, "text": " to behave differently on different inputs, b, c, and delta are altered to be time-varying. They now", "tokens": [51028, 281, 15158, 7614, 322, 819, 15743, 11, 272, 11, 269, 11, 293, 8289, 366, 28783, 281, 312, 565, 12, 85, 822, 278, 13, 814, 586, 51388], "temperature": 0.0, "avg_logprob": -0.07363007115382775, "compression_ratio": 1.751111111111111, "no_speech_prob": 0.0013669648906216025}, {"id": 101, "seek": 55120, "start": 571.6800000000001, "end": 576.8000000000001, "text": " depend on the input sequence. This gives the model more power, but means we can no longer use the", "tokens": [51388, 5672, 322, 264, 4846, 8310, 13, 639, 2709, 264, 2316, 544, 1347, 11, 457, 1355, 321, 393, 572, 2854, 764, 264, 51644], "temperature": 0.0, "avg_logprob": -0.07363007115382775, "compression_ratio": 1.751111111111111, "no_speech_prob": 0.0013669648906216025}, {"id": 102, "seek": 57680, "start": 576.8, "end": 583.1999999999999, "text": " efficient S4 convolution trick. To get around this, the authors develop a selective scan based on", "tokens": [50364, 7148, 318, 19, 45216, 4282, 13, 1407, 483, 926, 341, 11, 264, 16552, 1499, 257, 33930, 11049, 2361, 322, 50684], "temperature": 0.0, "avg_logprob": -0.06605459635074322, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.0034830262884497643}, {"id": 103, "seek": 57680, "start": 583.1999999999999, "end": 588.7199999999999, "text": " hardware-aware state expansion. In effect, the two big challenges to be tackled once we give up on", "tokens": [50684, 8837, 12, 17074, 1785, 11260, 13, 682, 1802, 11, 264, 732, 955, 4759, 281, 312, 9426, 1493, 1564, 321, 976, 493, 322, 50960], "temperature": 0.0, "avg_logprob": -0.06605459635074322, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.0034830262884497643}, {"id": 104, "seek": 57680, "start": 588.7199999999999, "end": 594.7199999999999, "text": " convolution are the sequential nature of recurrence and the large memory usage. There are three", "tokens": [50960, 45216, 366, 264, 42881, 3687, 295, 18680, 10760, 293, 264, 2416, 4675, 14924, 13, 821, 366, 1045, 51260], "temperature": 0.0, "avg_logprob": -0.06605459635074322, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.0034830262884497643}, {"id": 105, "seek": 57680, "start": 594.7199999999999, "end": 600.4799999999999, "text": " techniques that come into play, kernel fusion, parallel scan, and recomputation. A key idea for", "tokens": [51260, 7512, 300, 808, 666, 862, 11, 28256, 23100, 11, 8952, 11049, 11, 293, 23334, 2582, 399, 13, 316, 2141, 1558, 337, 51548], "temperature": 0.0, "avg_logprob": -0.06605459635074322, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.0034830262884497643}, {"id": 106, "seek": 57680, "start": 600.4799999999999, "end": 605.52, "text": " getting around the sequential processing problem comes from simplified state space layers for", "tokens": [51548, 1242, 926, 264, 42881, 9007, 1154, 1487, 490, 26335, 1785, 1901, 7914, 337, 51800], "temperature": 0.0, "avg_logprob": -0.06605459635074322, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.0034830262884497643}, {"id": 107, "seek": 60552, "start": 605.52, "end": 611.76, "text": " sequence modeling, or S5, which highlighted the benefits of using parallel scans to maintain", "tokens": [50364, 8310, 15983, 11, 420, 318, 20, 11, 597, 17173, 264, 5311, 295, 1228, 8952, 35116, 281, 6909, 50676], "temperature": 0.0, "avg_logprob": -0.07241269766566265, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002959418634418398}, {"id": 108, "seek": 60552, "start": 611.76, "end": 618.16, "text": " efficiency while avoiding the use of convolutional tricks used in S4. Mamba uses this parallel scan", "tokens": [50676, 10493, 1339, 20220, 264, 764, 295, 45216, 304, 11733, 1143, 294, 318, 19, 13, 376, 23337, 4960, 341, 8952, 11049, 50996], "temperature": 0.0, "avg_logprob": -0.07241269766566265, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002959418634418398}, {"id": 109, "seek": 60552, "start": 618.16, "end": 623.92, "text": " idea in combination with efficient use of memory. In particular, instead of preparing the scan input", "tokens": [50996, 1558, 294, 6562, 365, 7148, 764, 295, 4675, 13, 682, 1729, 11, 2602, 295, 10075, 264, 11049, 4846, 51284], "temperature": 0.0, "avg_logprob": -0.07241269766566265, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002959418634418398}, {"id": 110, "seek": 60552, "start": 623.92, "end": 630.24, "text": " in GPU high bandwidth memory, they load the state space model parameters directly from the slow high", "tokens": [51284, 294, 18407, 1090, 23647, 4675, 11, 436, 3677, 264, 1785, 1901, 2316, 9834, 3838, 490, 264, 2964, 1090, 51600], "temperature": 0.0, "avg_logprob": -0.07241269766566265, "compression_ratio": 1.6348547717842323, "no_speech_prob": 0.0002959418634418398}, {"id": 111, "seek": 63024, "start": 630.24, "end": 636.32, "text": " bandwidth memory to fast SRAM, where they perform the discretization and recurrence. Then they write", "tokens": [50364, 23647, 4675, 281, 2370, 20840, 2865, 11, 689, 436, 2042, 264, 25656, 2144, 293, 18680, 10760, 13, 1396, 436, 2464, 50668], "temperature": 0.0, "avg_logprob": -0.06416975470150219, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0019265866139903665}, {"id": 112, "seek": 63024, "start": 636.32, "end": 642.08, "text": " the final results back to high bandwidth memory. Last, recomputation is used to reduce the memory", "tokens": [50668, 264, 2572, 3542, 646, 281, 1090, 23647, 4675, 13, 5264, 11, 23334, 2582, 399, 307, 1143, 281, 5407, 264, 4675, 50956], "temperature": 0.0, "avg_logprob": -0.06416975470150219, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0019265866139903665}, {"id": 113, "seek": 63024, "start": 642.08, "end": 647.6800000000001, "text": " requirements. This allows them to avoid saving the intermediate states, which are necessary for back", "tokens": [50956, 7728, 13, 639, 4045, 552, 281, 5042, 6816, 264, 19376, 4368, 11, 597, 366, 4818, 337, 646, 51236], "temperature": 0.0, "avg_logprob": -0.06416975470150219, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0019265866139903665}, {"id": 114, "seek": 63024, "start": 647.6800000000001, "end": 652.96, "text": " propagation. Putting this all together, here is the selective state space model with hardware-aware", "tokens": [51236, 38377, 13, 31367, 341, 439, 1214, 11, 510, 307, 264, 33930, 1785, 1901, 2316, 365, 8837, 12, 17074, 51500], "temperature": 0.0, "avg_logprob": -0.06416975470150219, "compression_ratio": 1.6285714285714286, "no_speech_prob": 0.0019265866139903665}, {"id": 115, "seek": 65296, "start": 653.0400000000001, "end": 659.6800000000001, "text": " state expansion. The first thing we see is that BT, delta T, and CT all depend on the input", "tokens": [50368, 1785, 11260, 13, 440, 700, 551, 321, 536, 307, 300, 31144, 11, 8289, 314, 11, 293, 19529, 439, 5672, 322, 264, 4846, 50700], "temperature": 0.0, "avg_logprob": -0.11096234475412677, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.06370377540588379}, {"id": 116, "seek": 65296, "start": 659.6800000000001, "end": 666.08, "text": " XT via the selection mechanism. The second thing we see is that the parameters are loaded into fast", "tokens": [50700, 1783, 51, 5766, 264, 9450, 7513, 13, 440, 1150, 551, 321, 536, 307, 300, 264, 9834, 366, 13210, 666, 2370, 51020], "temperature": 0.0, "avg_logprob": -0.11096234475412677, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.06370377540588379}, {"id": 117, "seek": 65296, "start": 666.08, "end": 672.24, "text": " GPU SRAM, indicated in this diagram by the color orange, where the update is performed. Then the", "tokens": [51020, 18407, 20840, 2865, 11, 16176, 294, 341, 10686, 538, 264, 2017, 7671, 11, 689, 264, 5623, 307, 10332, 13, 1396, 264, 51328], "temperature": 0.0, "avg_logprob": -0.11096234475412677, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.06370377540588379}, {"id": 118, "seek": 65296, "start": 672.24, "end": 678.8000000000001, "text": " output is ultimately stored back into GPU high bandwidth memory, shown in green. Next, we come to", "tokens": [51328, 5598, 307, 6284, 12187, 646, 666, 18407, 1090, 23647, 4675, 11, 4898, 294, 3092, 13, 3087, 11, 321, 808, 281, 51656], "temperature": 0.0, "avg_logprob": -0.11096234475412677, "compression_ratio": 1.5755102040816327, "no_speech_prob": 0.06370377540588379}, {"id": 119, "seek": 67880, "start": 678.8, "end": 684.24, "text": " the simplified state space model architecture. Here, the authors combine the Hungry Hungry Hippos", "tokens": [50364, 264, 26335, 1785, 1901, 2316, 9482, 13, 1692, 11, 264, 16552, 10432, 264, 15063, 627, 15063, 627, 2421, 427, 329, 50636], "temperature": 0.0, "avg_logprob": -0.059485826852186674, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.01590188592672348}, {"id": 120, "seek": 67880, "start": 684.24, "end": 689.68, "text": " block with a gated MLP block to produce their Mamba block. The shapes here indicate that the", "tokens": [50636, 3461, 365, 257, 290, 770, 21601, 47, 3461, 281, 5258, 641, 376, 23337, 3461, 13, 440, 10854, 510, 13330, 300, 264, 50908], "temperature": 0.0, "avg_logprob": -0.059485826852186674, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.01590188592672348}, {"id": 121, "seek": 67880, "start": 689.68, "end": 695.76, "text": " dimensionality is expanded inside the block. This block is repeated and interleaved with standard", "tokens": [50908, 10139, 1860, 307, 14342, 1854, 264, 3461, 13, 639, 3461, 307, 10477, 293, 728, 306, 12865, 365, 3832, 51212], "temperature": 0.0, "avg_logprob": -0.059485826852186674, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.01590188592672348}, {"id": 122, "seek": 67880, "start": 695.76, "end": 701.3599999999999, "text": " normalization and residual connections to form the Mamba architecture. I'll mention a few other", "tokens": [51212, 2710, 2144, 293, 27980, 9271, 281, 1254, 264, 376, 23337, 9482, 13, 286, 603, 2152, 257, 1326, 661, 51492], "temperature": 0.0, "avg_logprob": -0.059485826852186674, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.01590188592672348}, {"id": 123, "seek": 67880, "start": 701.3599999999999, "end": 706.7199999999999, "text": " model details. The authors note that most prior state space models use complex numbers in their", "tokens": [51492, 2316, 4365, 13, 440, 16552, 3637, 300, 881, 4059, 1785, 1901, 5245, 764, 3997, 3547, 294, 641, 51760], "temperature": 0.0, "avg_logprob": -0.059485826852186674, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.01590188592672348}, {"id": 124, "seek": 70672, "start": 706.72, "end": 711.36, "text": " state, but it has been empirically observed that completely real valued state space models seem", "tokens": [50364, 1785, 11, 457, 309, 575, 668, 25790, 984, 13095, 300, 2584, 957, 22608, 1785, 1901, 5245, 1643, 50596], "temperature": 0.0, "avg_logprob": -0.0681425604903907, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0009696411434561014}, {"id": 125, "seek": 70672, "start": 711.36, "end": 717.52, "text": " to work fine, and possibly even better, in some settings. So, they use real values as the default,", "tokens": [50596, 281, 589, 2489, 11, 293, 6264, 754, 1101, 11, 294, 512, 6257, 13, 407, 11, 436, 764, 957, 4190, 382, 264, 7576, 11, 50904], "temperature": 0.0, "avg_logprob": -0.0681425604903907, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0009696411434561014}, {"id": 126, "seek": 70672, "start": 717.52, "end": 723.12, "text": " which work well for all but one of their tasks. Next, we come to the empirical evaluation. First,", "tokens": [50904, 597, 589, 731, 337, 439, 457, 472, 295, 641, 9608, 13, 3087, 11, 321, 808, 281, 264, 31886, 13344, 13, 2386, 11, 51184], "temperature": 0.0, "avg_logprob": -0.0681425604903907, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0009696411434561014}, {"id": 127, "seek": 70672, "start": 723.12, "end": 728.8000000000001, "text": " we have the synthetic tasks described earlier. First, as a bit of notation, the authors abbreviate", "tokens": [51184, 321, 362, 264, 23420, 9608, 7619, 3071, 13, 2386, 11, 382, 257, 857, 295, 24657, 11, 264, 16552, 35839, 473, 51468], "temperature": 0.0, "avg_logprob": -0.0681425604903907, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0009696411434561014}, {"id": 128, "seek": 70672, "start": 728.8000000000001, "end": 735.44, "text": " selective state space models as S6 models, because they are S4 models with a selection mechanism,", "tokens": [51468, 33930, 1785, 1901, 5245, 382, 318, 21, 5245, 11, 570, 436, 366, 318, 19, 5245, 365, 257, 9450, 7513, 11, 51800], "temperature": 0.0, "avg_logprob": -0.0681425604903907, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0009696411434561014}, {"id": 129, "seek": 73544, "start": 735.44, "end": 742.08, "text": " and computed with a scan. That's a lot of S's. On the selective copying task, S6 works well with", "tokens": [50364, 293, 40610, 365, 257, 11049, 13, 663, 311, 257, 688, 295, 318, 311, 13, 1282, 264, 33930, 27976, 5633, 11, 318, 21, 1985, 731, 365, 50696], "temperature": 0.0, "avg_logprob": -0.07057628468570547, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0023963383864611387}, {"id": 130, "seek": 73544, "start": 742.08, "end": 748.5600000000001, "text": " every architecture, but S4 and Hyena work comparatively poorly. On the induction heads task,", "tokens": [50696, 633, 9482, 11, 457, 318, 19, 293, 5701, 4118, 589, 6311, 19020, 22271, 13, 1282, 264, 33371, 8050, 5633, 11, 51020], "temperature": 0.0, "avg_logprob": -0.07057628468570547, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0023963383864611387}, {"id": 131, "seek": 73544, "start": 748.5600000000001, "end": 754.4000000000001, "text": " Mamba, shown in brown, appears up here at the top, where we see it succeeds on test sequence lengths", "tokens": [51020, 376, 23337, 11, 4898, 294, 6292, 11, 7038, 493, 510, 412, 264, 1192, 11, 689, 321, 536, 309, 49263, 322, 1500, 8310, 26329, 51312], "temperature": 0.0, "avg_logprob": -0.07057628468570547, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0023963383864611387}, {"id": 132, "seek": 73544, "start": 754.4000000000001, "end": 759.6, "text": " of up to a million tokens, which is 4,000 times longer than it saw during training,", "tokens": [51312, 295, 493, 281, 257, 2459, 22667, 11, 597, 307, 1017, 11, 1360, 1413, 2854, 813, 309, 1866, 1830, 3097, 11, 51572], "temperature": 0.0, "avg_logprob": -0.07057628468570547, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0023963383864611387}, {"id": 133, "seek": 73544, "start": 759.6, "end": 764.24, "text": " while none of the other methods compared to generalize to beyond twice their training length.", "tokens": [51572, 1339, 6022, 295, 264, 661, 7150, 5347, 281, 2674, 1125, 281, 4399, 6091, 641, 3097, 4641, 13, 51804], "temperature": 0.0, "avg_logprob": -0.07057628468570547, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.0023963383864611387}, {"id": 134, "seek": 76424, "start": 764.32, "end": 769.12, "text": " Next, we have experiments on language modelling, which follow the training recipe described in", "tokens": [50368, 3087, 11, 321, 362, 12050, 322, 2856, 42253, 11, 597, 1524, 264, 3097, 6782, 7619, 294, 50608], "temperature": 0.0, "avg_logprob": -0.11906149754157433, "compression_ratio": 1.5303643724696356, "no_speech_prob": 0.0005357168847694993}, {"id": 135, "seek": 76424, "start": 769.12, "end": 775.92, "text": " the GBT3 work, and train on the pile dataset. Here, the metric on the y-axis is perplexity,", "tokens": [50608, 264, 26809, 51, 18, 589, 11, 293, 3847, 322, 264, 14375, 28872, 13, 1692, 11, 264, 20678, 322, 264, 288, 12, 24633, 307, 680, 18945, 507, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11906149754157433, "compression_ratio": 1.5303643724696356, "no_speech_prob": 0.0005357168847694993}, {"id": 136, "seek": 76424, "start": 775.92, "end": 781.6800000000001, "text": " so lower is better. We can see that across a sequence length of 2,048, and a sequence length", "tokens": [50948, 370, 3126, 307, 1101, 13, 492, 393, 536, 300, 2108, 257, 8310, 4641, 295, 568, 11, 15, 13318, 11, 293, 257, 8310, 4641, 51236], "temperature": 0.0, "avg_logprob": -0.11906149754157433, "compression_ratio": 1.5303643724696356, "no_speech_prob": 0.0005357168847694993}, {"id": 137, "seek": 76424, "start": 781.6800000000001, "end": 790.48, "text": " of 8,192, that Mamba, shown in purple, outperforms the other baselines, and matches Transformer++,", "tokens": [51236, 295, 1649, 11, 3405, 17, 11, 300, 376, 23337, 11, 4898, 294, 9656, 11, 484, 26765, 82, 264, 661, 987, 9173, 11, 293, 10676, 27938, 260, 25472, 11, 51676], "temperature": 0.0, "avg_logprob": -0.11906149754157433, "compression_ratio": 1.5303643724696356, "no_speech_prob": 0.0005357168847694993}, {"id": 138, "seek": 79048, "start": 790.48, "end": 795.84, "text": " which is highlighted in orange, and represents the strongest Transformer recipe that the authors", "tokens": [50364, 597, 307, 17173, 294, 7671, 11, 293, 8855, 264, 16595, 27938, 260, 6782, 300, 264, 16552, 50632], "temperature": 0.0, "avg_logprob": -0.13469080491499466, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.002396451309323311}, {"id": 139, "seek": 79048, "start": 795.84, "end": 802.24, "text": " know of, based on the palm and llama architectures. That means rotary embeddings, swigloo MLPs,", "tokens": [50632, 458, 295, 11, 2361, 322, 264, 17018, 293, 23272, 6331, 1303, 13, 663, 1355, 45811, 12240, 29432, 11, 1693, 328, 38511, 21601, 23043, 11, 50952], "temperature": 0.0, "avg_logprob": -0.13469080491499466, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.002396451309323311}, {"id": 140, "seek": 79048, "start": 802.24, "end": 808.64, "text": " rmsnorm instead of leonorm, no linear bias, and higher learning rates. On zero shot downstream", "tokens": [50952, 367, 2592, 13403, 2602, 295, 476, 266, 687, 11, 572, 8213, 12577, 11, 293, 2946, 2539, 6846, 13, 1282, 4018, 3347, 30621, 51272], "temperature": 0.0, "avg_logprob": -0.13469080491499466, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.002396451309323311}, {"id": 141, "seek": 79048, "start": 808.64, "end": 814.4, "text": " evaluations, there are lots of comparisons, generally we see Mamba achieving an average", "tokens": [51272, 43085, 11, 456, 366, 3195, 295, 33157, 11, 5101, 321, 536, 376, 23337, 19626, 364, 4274, 51560], "temperature": 0.0, "avg_logprob": -0.13469080491499466, "compression_ratio": 1.5495867768595042, "no_speech_prob": 0.002396451309323311}, {"id": 142, "seek": 81440, "start": 815.1999999999999, "end": 821.68, "text": " of baselines at twice the model size. Next, there are studies on DNA modelling. On human genome", "tokens": [50404, 295, 987, 9173, 412, 6091, 264, 2316, 2744, 13, 3087, 11, 456, 366, 5313, 322, 8272, 42253, 13, 1282, 1952, 21953, 50728], "temperature": 0.0, "avg_logprob": -0.07973670959472656, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.04022830352187157}, {"id": 143, "seek": 81440, "start": 821.68, "end": 828.3199999999999, "text": " data, Mamba, shown in orange, scales better than Hyena DNA, shown in blue, and Transformer++,", "tokens": [50728, 1412, 11, 376, 23337, 11, 4898, 294, 7671, 11, 17408, 1101, 813, 5701, 4118, 8272, 11, 4898, 294, 3344, 11, 293, 27938, 260, 25472, 11, 51060], "temperature": 0.0, "avg_logprob": -0.07973670959472656, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.04022830352187157}, {"id": 144, "seek": 81440, "start": 828.3199999999999, "end": 833.68, "text": " shown in red, when you scale up the number of parameters. It also scales better than Hyena DNA,", "tokens": [51060, 4898, 294, 2182, 11, 562, 291, 4373, 493, 264, 1230, 295, 9834, 13, 467, 611, 17408, 1101, 813, 5701, 4118, 8272, 11, 51328], "temperature": 0.0, "avg_logprob": -0.07973670959472656, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.04022830352187157}, {"id": 145, "seek": 81440, "start": 833.68, "end": 839.52, "text": " when you scale up the sequence length. Mamba models also do well relative to a Hyena DNA baseline,", "tokens": [51328, 562, 291, 4373, 493, 264, 8310, 4641, 13, 376, 23337, 5245, 611, 360, 731, 4972, 281, 257, 5701, 4118, 8272, 20518, 11, 51620], "temperature": 0.0, "avg_logprob": -0.07973670959472656, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.04022830352187157}, {"id": 146, "seek": 83952, "start": 839.52, "end": 846.16, "text": " when fine tuning for a species DNA classification task. Here, the metric on the y-axis is accuracy,", "tokens": [50364, 562, 2489, 15164, 337, 257, 6172, 8272, 21538, 5633, 13, 1692, 11, 264, 20678, 322, 264, 288, 12, 24633, 307, 14170, 11, 50696], "temperature": 0.0, "avg_logprob": -0.0683651013807817, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.002322625368833542}, {"id": 147, "seek": 83952, "start": 846.16, "end": 850.72, "text": " so higher is better, with longer sequences as we move to the right along the x-axis.", "tokens": [50696, 370, 2946, 307, 1101, 11, 365, 2854, 22978, 382, 321, 1286, 281, 264, 558, 2051, 264, 2031, 12, 24633, 13, 50924], "temperature": 0.0, "avg_logprob": -0.0683651013807817, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.002322625368833542}, {"id": 148, "seek": 83952, "start": 850.72, "end": 855.36, "text": " We see the orange and green Mamba curves rising. I won't go through them in detail,", "tokens": [50924, 492, 536, 264, 7671, 293, 3092, 376, 23337, 19490, 11636, 13, 286, 1582, 380, 352, 807, 552, 294, 2607, 11, 51156], "temperature": 0.0, "avg_logprob": -0.0683651013807817, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.002322625368833542}, {"id": 149, "seek": 83952, "start": 855.36, "end": 860.56, "text": " but there are other experiments showing that Mamba mostly works well on audio modelling and", "tokens": [51156, 457, 456, 366, 661, 12050, 4099, 300, 376, 23337, 5240, 1985, 731, 322, 6278, 42253, 293, 51416], "temperature": 0.0, "avg_logprob": -0.0683651013807817, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.002322625368833542}, {"id": 150, "seek": 83952, "start": 860.56, "end": 865.84, "text": " generation. However, there is one ablation in the appendix, identifying a case when a naive", "tokens": [51416, 5125, 13, 2908, 11, 456, 307, 472, 410, 24278, 294, 264, 34116, 970, 11, 16696, 257, 1389, 562, 257, 29052, 51680], "temperature": 0.0, "avg_logprob": -0.0683651013807817, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.002322625368833542}, {"id": 151, "seek": 86584, "start": 865.9200000000001, "end": 870.32, "text": " application of Mamba with the selection mechanism seems to hurt performance. Here,", "tokens": [50368, 3861, 295, 376, 23337, 365, 264, 9450, 7513, 2544, 281, 4607, 3389, 13, 1692, 11, 50588], "temperature": 0.0, "avg_logprob": -0.06260387676278341, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0030749235302209854}, {"id": 152, "seek": 86584, "start": 870.32, "end": 875.76, "text": " lower is better, and the orange line denotes the default Mamba configuration. The authors suggest", "tokens": [50588, 3126, 307, 1101, 11, 293, 264, 7671, 1622, 1441, 17251, 264, 7576, 376, 23337, 11694, 13, 440, 16552, 3402, 50860], "temperature": 0.0, "avg_logprob": -0.06260387676278341, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0030749235302209854}, {"id": 153, "seek": 86584, "start": 875.76, "end": 880.88, "text": " that this may be a case where audio waveforms actually benefit from linear time-invariant", "tokens": [50860, 300, 341, 815, 312, 257, 1389, 689, 6278, 36512, 82, 767, 5121, 490, 8213, 565, 12, 259, 34033, 394, 51116], "temperature": 0.0, "avg_logprob": -0.06260387676278341, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0030749235302209854}, {"id": 154, "seek": 86584, "start": 880.88, "end": 886.5600000000001, "text": " models which have a matching inductive bias. Next, we have speed and memory benchmarks.", "tokens": [51116, 5245, 597, 362, 257, 14324, 31612, 488, 12577, 13, 3087, 11, 321, 362, 3073, 293, 4675, 43751, 13, 51400], "temperature": 0.0, "avg_logprob": -0.06260387676278341, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0030749235302209854}, {"id": 155, "seek": 86584, "start": 886.5600000000001, "end": 891.36, "text": " The authors provide an implementation of the scan operation that is pretty fast.", "tokens": [51400, 440, 16552, 2893, 364, 11420, 295, 264, 11049, 6916, 300, 307, 1238, 2370, 13, 51640], "temperature": 0.0, "avg_logprob": -0.06260387676278341, "compression_ratio": 1.6319702602230484, "no_speech_prob": 0.0030749235302209854}, {"id": 156, "seek": 89136, "start": 891.36, "end": 898.72, "text": " They compare the scan versus convolution and attention on an A100 GPU. Here, the y-axis measures", "tokens": [50364, 814, 6794, 264, 11049, 5717, 45216, 293, 3202, 322, 364, 316, 6879, 18407, 13, 1692, 11, 264, 288, 12, 24633, 8000, 50732], "temperature": 0.0, "avg_logprob": -0.072742400449865, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00024535521515645087}, {"id": 157, "seek": 89136, "start": 898.72, "end": 905.44, "text": " time, so lower is better. We see the red line representing their implementation is considerably", "tokens": [50732, 565, 11, 370, 3126, 307, 1101, 13, 492, 536, 264, 2182, 1622, 13460, 641, 11420, 307, 31308, 51068], "temperature": 0.0, "avg_logprob": -0.072742400449865, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00024535521515645087}, {"id": 158, "seek": 89136, "start": 905.44, "end": 910.5600000000001, "text": " below the other mechanisms at longer sequence lengths. In terms of inference throughput,", "tokens": [51068, 2507, 264, 661, 15902, 412, 2854, 8310, 26329, 13, 682, 2115, 295, 38253, 44629, 11, 51324], "temperature": 0.0, "avg_logprob": -0.072742400449865, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00024535521515645087}, {"id": 159, "seek": 89136, "start": 910.5600000000001, "end": 916.4, "text": " where higher is better, we find that Mamba, shown in blue and green, can achieve five times higher", "tokens": [51324, 689, 2946, 307, 1101, 11, 321, 915, 300, 376, 23337, 11, 4898, 294, 3344, 293, 3092, 11, 393, 4584, 1732, 1413, 2946, 51616], "temperature": 0.0, "avg_logprob": -0.072742400449865, "compression_ratio": 1.5447154471544715, "no_speech_prob": 0.00024535521515645087}, {"id": 160, "seek": 91640, "start": 916.4, "end": 922.24, "text": " throughput than Transformers, benefiting from its recurrent nature. In terms of limitations,", "tokens": [50364, 44629, 813, 27938, 433, 11, 47515, 490, 1080, 18680, 1753, 3687, 13, 682, 2115, 295, 15705, 11, 50656], "temperature": 0.0, "avg_logprob": -0.05753828252403481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023964657448232174}, {"id": 161, "seek": 91640, "start": 922.24, "end": 926.9599999999999, "text": " the authors highlight that there is no free lunch on the continuous discrete spectrum.", "tokens": [50656, 264, 16552, 5078, 300, 456, 307, 572, 1737, 6349, 322, 264, 10957, 27706, 11143, 13, 50892], "temperature": 0.0, "avg_logprob": -0.05753828252403481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023964657448232174}, {"id": 162, "seek": 91640, "start": 926.9599999999999, "end": 932.72, "text": " The selection mechanism overcomes the weaknesses of prior state space models on discrete modalities", "tokens": [50892, 440, 9450, 7513, 670, 9055, 264, 24381, 295, 4059, 1785, 1901, 5245, 322, 27706, 1072, 16110, 51180], "temperature": 0.0, "avg_logprob": -0.05753828252403481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023964657448232174}, {"id": 163, "seek": 91640, "start": 932.72, "end": 938.4, "text": " such as text and DNA, but this can impede their performance on data that linear time-invariant", "tokens": [51180, 1270, 382, 2487, 293, 8272, 11, 457, 341, 393, 704, 4858, 641, 3389, 322, 1412, 300, 8213, 565, 12, 259, 34033, 394, 51464], "temperature": 0.0, "avg_logprob": -0.05753828252403481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023964657448232174}, {"id": 164, "seek": 91640, "start": 938.4, "end": 944.16, "text": " state space models excel on, which we saw with the audio ablation. They also note that the empirical", "tokens": [51464, 1785, 1901, 5245, 24015, 322, 11, 597, 321, 1866, 365, 264, 6278, 410, 24278, 13, 814, 611, 3637, 300, 264, 31886, 51752], "temperature": 0.0, "avg_logprob": -0.05753828252403481, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023964657448232174}, {"id": 165, "seek": 94416, "start": 944.16, "end": 950.0, "text": " evaluation is limited to small model sizes, below the threshold of most strong open-source", "tokens": [50364, 13344, 307, 5567, 281, 1359, 2316, 11602, 11, 2507, 264, 14678, 295, 881, 2068, 1269, 12, 41676, 50656], "temperature": 0.0, "avg_logprob": -0.1281736824247572, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.003944700118154287}, {"id": 166, "seek": 94416, "start": 950.0, "end": 955.92, "text": " LLMs. Therefore, it remains to assess whether Mamba still compares favourably at larger sizes.", "tokens": [50656, 441, 43, 26386, 13, 7504, 11, 309, 7023, 281, 5877, 1968, 376, 23337, 920, 38334, 8182, 1188, 412, 4833, 11602, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1281736824247572, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.003944700118154287}, {"id": 167, "seek": 94416, "start": 955.92, "end": 960.7199999999999, "text": " That's it, we've reached the end. If you'd like to try it out, the authors have released code on GitHub.", "tokens": [50952, 663, 311, 309, 11, 321, 600, 6488, 264, 917, 13, 759, 291, 1116, 411, 281, 853, 309, 484, 11, 264, 16552, 362, 4736, 3089, 322, 23331, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1281736824247572, "compression_ratio": 1.4215686274509804, "no_speech_prob": 0.003944700118154287}], "language": "en"}