start	end	text
0	5600	A few days ago, Mamba, linear time sequence modelling with selective state spaces, appeared on
5600	10880	archive. It is causing some excitement, partly because it shows strong empirical results,
10880	16080	partly because it has an interesting design, and partly because Mamba is a great name.
16080	20800	In this video, I'll give a high level summary of the paper. To understand the motivation for this
20800	25920	work, it's helpful to go back in time to the distant memories of 2020, and in particular,
25920	30960	the Long Range Arena, a benchmark for efficient transformers. The starting premise was that
30960	36000	transformers do not scale very well to long sequence lengths, largely because of quadratic
36000	41440	self-attention complexity. Given the proliferation of efficient alternatives to transformers that
41440	46480	were emerging at the time, this work proposed a benchmark specifically focused on evaluating
46480	52240	model quality under long context scenarios. There were tasks like long list ops, where the model
52240	58320	is given an input consisting of a long list of nested operators, and needs to calculate an answer,
58320	63680	and fun tasks like pathfinder, where the model gets a flattened image that looks like this,
63680	68640	and needs to determine whether the two circles can be joined. This is a positive example because
68640	73840	they can. This is a negative example because they cannot. Broadly, they found that if you wanted to
73840	79600	maintain performance encapsulated here in this LRA score, it's quite hard to do much better than
79600	84880	the vanilla transformer. Independently, and a little earlier, a creative approach to tackling
84880	91520	the Long Range Dependency problem was introduced in this work on Legend Memory Units, in the context
91520	98000	of recurrent neural networks. As a refresher, Legend polynomials are these beautiful creatures.
98000	102960	You can construct them by making a sequence of polynomials that are orthogonal to each other.
102960	108720	By projecting values onto a basis of these polynomials, you can efficiently store a history
108720	113040	of the inputs to your system. That's the key idea in this funky looking equation.
113040	119440	You can reconstruct the input U by reassembling it from the components of the memory vector,
119440	125120	denoted here by M. This idea can then be baked into a layer where the memory block is implemented
125120	131200	with simple linear operations. Building on that idea, and others, Hippo, recurrent memory with
131200	137520	optimal polynomial projections, had the insight that we can phrase memory as a technical problem
137520	142480	of online function approximation, where a function is summarized by storing its optimal
142480	148080	coefficients in terms of some basis functions. Using this insight, the authors constructed a
148080	153600	simple model based on Legendre polynomials that worked well on tasks like permuted MNIST,
153600	159520	outperforming other sequence model-based baselines like LSTM and dilated RNN. This was
159520	164480	followed by the work, combining recurrent convolutional and continuous time models
164480	169920	with linear state space layers, which proposed a unifying framework for sequence modelling
169920	175680	based on a standard state space representation. This work showed how, starting from an implicit,
175680	181120	continuous time state space model, you could discretize to produce a system that can be
181120	187760	interpreted as a recurrent network with the benefit of efficient inference, or as a convolutional
187760	193120	model which benefits from parallelizable training. Unfortunately, a naive implementation of these
193120	198800	models requires a massive amount of memory. In fact, for reasonably sized models, such as when
198800	205440	the state dimension is 256, the linear state space layer uses orders of magnitude more memory than
205440	212000	comparably sized RNNs or CNNs. That brings us to the S4 model introduced in efficiently modelling
212000	217200	long sequences with structured state spaces. What we'd really like to be able to do is use the
217200	222400	convolutional interpretation we've just seen to efficiently train our state space model.
222400	228320	Now, if we want to train state space models using the convolutional representation, we can do a
228320	233600	little bit of unrolling and a little bit of vectorization to get this L-dimensional kernel,
233600	239520	where L is the sequence length. The scary part here is this A raised to the power of I term.
239520	244720	Harking back to your linear algebra classes, as soon as you see this, you may feel a strong
244720	250400	and appropriate urge to try to diagonalize A. The authors discuss diagonalization.
250400	255680	Unfortunately, it cannot be used directly for the hippo matrix due to numerical issues.
255680	261280	Specifically, it has entries exponentially large in the state size. However, we can address the
261280	267120	hippo matrix with an adjustment. The authors note that although the hippo matrix is not normal,
267120	273360	it can be decomposed as the sum of a normal and low rank matrix. Even this is still not useful
273360	278720	by itself. An additional three new techniques are needed. First, instead of computing the kernel
278720	285360	k-bar directly, they instead compute its spectrum, then determine k-bar by applying an inverse FFT.
285360	290720	Next, we use everyone's favorite low rank trick, the Woodbury identity. That lets us perform an
290720	296160	efficient matrix update. Third, it is shown that the diagonal matrix case is equivalent to the
296160	301200	computation of a Cauchy kernel, a well-studied problem with stable, near-linear algorithms.
301200	306560	All of this comes together in theorem three. It says that given any step size delta, computing
306560	312160	the state space model convolution filter k-bar can be reduced to four Cauchy multiplies,
312160	318800	requiring only big O of n plus l operations, ignoring logarithmic factors, and big O of
318800	325120	n plus l space, where again, l is the sequence length and n is the state size. Empirically,
325120	331120	the proposed S4 architecture does very well on the long-range arena, even doing well on the path
331120	336240	x task, where the model must determine if two points are connected on a fairly large image.
336240	341120	This task is not too hard when the input is arranged as an image, but it's pretty difficult
341120	346640	when it's a flattened sequence, as evidenced by the fact that many other models fail on this task.
346640	352800	If you'd like to gain some intuition for how S4 works, I highly recommend the annotated S4
352800	359520	by Sasha Rush and Sid Karamchetti, just as with the legendary annotated transformer blog post
359520	365920	back in 2018. The annotated S4 takes you step-by-step through the paper and provides snippets of
365920	371200	code that implement the mathematics. It also includes nice visualizations to help you build
371200	376800	intuition for how state-space models work. All of this brings us back to Mamba. This work builds
376800	382720	on the state-space model approach with a few key contributions. First, a selection mechanism. Since
382720	388320	previous state-space models lack the ability to efficiently select data in an input-dependent
388320	393760	manner, they design a simple selection mechanism by parameterizing the state-space model parameters
393760	399280	based on the input. Unfortunately, this change makes it hard for us to use the efficient convolutional
399280	405280	trick we saw in S4, so the second contribution is a hardware-aware algorithm that computes the
405280	410640	model recurrently with a scan instead of convolution. This leads to an implementation that is faster
410640	416400	than previous methods, both in theory, scaling linearly in sequence length compared to pseudo-linear
416400	423280	for all convolution-based SSMs and on modern hardware, up to three times faster on A100 GPUs.
423280	428640	Spicy. The third contribution is to simplify prior deep-sequence model architectures by
428640	434400	combining the design of prior state-space model architectures with the MLP block of transformers
434400	439440	into a single block, leading to a simple and homogenous architecture design called Mamba
439440	444640	that incorporates selective state-spaces. By building on a state-space model foundation,
444640	449760	Mamba is related to many previous state-space model architectures. In addition to the models we
449760	456400	met earlier, there is Linear Attention, which can be viewed as a degenerate linear SSM, the gloriously
456400	465040	named Hungry Hungry Hippos or H3, Hyena, RhettNet, and RWKV. The core motivation behind Mamba is to
465040	469760	use selection as a means of compression. The authors point out that you can view various
469760	475120	sequence models as making different trade-offs when tackling a fundamental problem of sequence
475120	480800	modeling, i.e. compressing context into a smaller state. At one extreme, we have Attention, which
480800	485920	is both effective and inefficient because it explicitly does not compress context at all.
485920	490560	At the other extreme, recurrent models are efficient because they have a finite state,
490560	495680	but their effectiveness is limited by how well this state has compressed the context. To explore
495680	502320	this trade-off, the authors focus on two synthetic tasks. One previously well-studied task is copying.
502320	508080	The job of the model is to copy color sequences in the input into the output, delayed by some
508080	513040	constant offset. Standard convolutional models with fixed kernels can solve this without any
513040	518240	problem. The first of the harder tasks they consider is selective copying. This time, we have
518240	524560	random spaces symbolized by the white blocks in between the input sequence elements. To successfully
524560	530320	copy the color outputs to the output while ignoring the white blocks, we need a mechanism that behaves
530320	535760	differently at different inputs. The second task they look at is induction heads. Here,
535760	540240	in order to predict the appropriate color at the question mark block, the model needs to be able
540240	545920	to perform retrieval back over the input sequence based on the context provided by the black square,
545920	551200	in order to predict that blue is the next color in the sequence. To build a model capable of solving
551200	558640	these tasks, the models start from the S4 state space model, which uses parameters a, b, c, and
558640	564480	delta, which don't depend on the input sequence. Now, for the proposed algorithm, to allow the model
564480	571680	to behave differently on different inputs, b, c, and delta are altered to be time-varying. They now
571680	576800	depend on the input sequence. This gives the model more power, but means we can no longer use the
576800	583200	efficient S4 convolution trick. To get around this, the authors develop a selective scan based on
583200	588720	hardware-aware state expansion. In effect, the two big challenges to be tackled once we give up on
588720	594720	convolution are the sequential nature of recurrence and the large memory usage. There are three
594720	600480	techniques that come into play, kernel fusion, parallel scan, and recomputation. A key idea for
600480	605520	getting around the sequential processing problem comes from simplified state space layers for
605520	611760	sequence modeling, or S5, which highlighted the benefits of using parallel scans to maintain
611760	618160	efficiency while avoiding the use of convolutional tricks used in S4. Mamba uses this parallel scan
618160	623920	idea in combination with efficient use of memory. In particular, instead of preparing the scan input
623920	630240	in GPU high bandwidth memory, they load the state space model parameters directly from the slow high
630240	636320	bandwidth memory to fast SRAM, where they perform the discretization and recurrence. Then they write
636320	642080	the final results back to high bandwidth memory. Last, recomputation is used to reduce the memory
642080	647680	requirements. This allows them to avoid saving the intermediate states, which are necessary for back
647680	652960	propagation. Putting this all together, here is the selective state space model with hardware-aware
653040	659680	state expansion. The first thing we see is that BT, delta T, and CT all depend on the input
659680	666080	XT via the selection mechanism. The second thing we see is that the parameters are loaded into fast
666080	672240	GPU SRAM, indicated in this diagram by the color orange, where the update is performed. Then the
672240	678800	output is ultimately stored back into GPU high bandwidth memory, shown in green. Next, we come to
678800	684240	the simplified state space model architecture. Here, the authors combine the Hungry Hungry Hippos
684240	689680	block with a gated MLP block to produce their Mamba block. The shapes here indicate that the
689680	695760	dimensionality is expanded inside the block. This block is repeated and interleaved with standard
695760	701360	normalization and residual connections to form the Mamba architecture. I'll mention a few other
701360	706720	model details. The authors note that most prior state space models use complex numbers in their
706720	711360	state, but it has been empirically observed that completely real valued state space models seem
711360	717520	to work fine, and possibly even better, in some settings. So, they use real values as the default,
717520	723120	which work well for all but one of their tasks. Next, we come to the empirical evaluation. First,
723120	728800	we have the synthetic tasks described earlier. First, as a bit of notation, the authors abbreviate
728800	735440	selective state space models as S6 models, because they are S4 models with a selection mechanism,
735440	742080	and computed with a scan. That's a lot of S's. On the selective copying task, S6 works well with
742080	748560	every architecture, but S4 and Hyena work comparatively poorly. On the induction heads task,
748560	754400	Mamba, shown in brown, appears up here at the top, where we see it succeeds on test sequence lengths
754400	759600	of up to a million tokens, which is 4,000 times longer than it saw during training,
759600	764240	while none of the other methods compared to generalize to beyond twice their training length.
764320	769120	Next, we have experiments on language modelling, which follow the training recipe described in
769120	775920	the GBT3 work, and train on the pile dataset. Here, the metric on the y-axis is perplexity,
775920	781680	so lower is better. We can see that across a sequence length of 2,048, and a sequence length
781680	790480	of 8,192, that Mamba, shown in purple, outperforms the other baselines, and matches Transformer++,
790480	795840	which is highlighted in orange, and represents the strongest Transformer recipe that the authors
795840	802240	know of, based on the palm and llama architectures. That means rotary embeddings, swigloo MLPs,
802240	808640	rmsnorm instead of leonorm, no linear bias, and higher learning rates. On zero shot downstream
808640	814400	evaluations, there are lots of comparisons, generally we see Mamba achieving an average
815200	821680	of baselines at twice the model size. Next, there are studies on DNA modelling. On human genome
821680	828320	data, Mamba, shown in orange, scales better than Hyena DNA, shown in blue, and Transformer++,
828320	833680	shown in red, when you scale up the number of parameters. It also scales better than Hyena DNA,
833680	839520	when you scale up the sequence length. Mamba models also do well relative to a Hyena DNA baseline,
839520	846160	when fine tuning for a species DNA classification task. Here, the metric on the y-axis is accuracy,
846160	850720	so higher is better, with longer sequences as we move to the right along the x-axis.
850720	855360	We see the orange and green Mamba curves rising. I won't go through them in detail,
855360	860560	but there are other experiments showing that Mamba mostly works well on audio modelling and
860560	865840	generation. However, there is one ablation in the appendix, identifying a case when a naive
865920	870320	application of Mamba with the selection mechanism seems to hurt performance. Here,
870320	875760	lower is better, and the orange line denotes the default Mamba configuration. The authors suggest
875760	880880	that this may be a case where audio waveforms actually benefit from linear time-invariant
880880	886560	models which have a matching inductive bias. Next, we have speed and memory benchmarks.
886560	891360	The authors provide an implementation of the scan operation that is pretty fast.
891360	898720	They compare the scan versus convolution and attention on an A100 GPU. Here, the y-axis measures
898720	905440	time, so lower is better. We see the red line representing their implementation is considerably
905440	910560	below the other mechanisms at longer sequence lengths. In terms of inference throughput,
910560	916400	where higher is better, we find that Mamba, shown in blue and green, can achieve five times higher
916400	922240	throughput than Transformers, benefiting from its recurrent nature. In terms of limitations,
922240	926960	the authors highlight that there is no free lunch on the continuous discrete spectrum.
926960	932720	The selection mechanism overcomes the weaknesses of prior state space models on discrete modalities
932720	938400	such as text and DNA, but this can impede their performance on data that linear time-invariant
938400	944160	state space models excel on, which we saw with the audio ablation. They also note that the empirical
944160	950000	evaluation is limited to small model sizes, below the threshold of most strong open-source
950000	955920	LLMs. Therefore, it remains to assess whether Mamba still compares favourably at larger sizes.
955920	960720	That's it, we've reached the end. If you'd like to try it out, the authors have released code on GitHub.
