WEBVTT

00:00.000 --> 00:05.600
A few days ago, Mamba, linear time sequence modelling with selective state spaces, appeared on

00:05.600 --> 00:10.880
archive. It is causing some excitement, partly because it shows strong empirical results,

00:10.880 --> 00:16.080
partly because it has an interesting design, and partly because Mamba is a great name.

00:16.080 --> 00:20.800
In this video, I'll give a high level summary of the paper. To understand the motivation for this

00:20.800 --> 00:25.920
work, it's helpful to go back in time to the distant memories of 2020, and in particular,

00:25.920 --> 00:30.960
the Long Range Arena, a benchmark for efficient transformers. The starting premise was that

00:30.960 --> 00:36.000
transformers do not scale very well to long sequence lengths, largely because of quadratic

00:36.000 --> 00:41.440
self-attention complexity. Given the proliferation of efficient alternatives to transformers that

00:41.440 --> 00:46.480
were emerging at the time, this work proposed a benchmark specifically focused on evaluating

00:46.480 --> 00:52.240
model quality under long context scenarios. There were tasks like long list ops, where the model

00:52.240 --> 00:58.320
is given an input consisting of a long list of nested operators, and needs to calculate an answer,

00:58.320 --> 01:03.680
and fun tasks like pathfinder, where the model gets a flattened image that looks like this,

01:03.680 --> 01:08.640
and needs to determine whether the two circles can be joined. This is a positive example because

01:08.640 --> 01:13.840
they can. This is a negative example because they cannot. Broadly, they found that if you wanted to

01:13.840 --> 01:19.600
maintain performance encapsulated here in this LRA score, it's quite hard to do much better than

01:19.600 --> 01:24.880
the vanilla transformer. Independently, and a little earlier, a creative approach to tackling

01:24.880 --> 01:31.520
the Long Range Dependency problem was introduced in this work on Legend Memory Units, in the context

01:31.520 --> 01:38.000
of recurrent neural networks. As a refresher, Legend polynomials are these beautiful creatures.

01:38.000 --> 01:42.960
You can construct them by making a sequence of polynomials that are orthogonal to each other.

01:42.960 --> 01:48.720
By projecting values onto a basis of these polynomials, you can efficiently store a history

01:48.720 --> 01:53.040
of the inputs to your system. That's the key idea in this funky looking equation.

01:53.040 --> 01:59.440
You can reconstruct the input U by reassembling it from the components of the memory vector,

01:59.440 --> 02:05.120
denoted here by M. This idea can then be baked into a layer where the memory block is implemented

02:05.120 --> 02:11.200
with simple linear operations. Building on that idea, and others, Hippo, recurrent memory with

02:11.200 --> 02:17.520
optimal polynomial projections, had the insight that we can phrase memory as a technical problem

02:17.520 --> 02:22.480
of online function approximation, where a function is summarized by storing its optimal

02:22.480 --> 02:28.080
coefficients in terms of some basis functions. Using this insight, the authors constructed a

02:28.080 --> 02:33.600
simple model based on Legendre polynomials that worked well on tasks like permuted MNIST,

02:33.600 --> 02:39.520
outperforming other sequence model-based baselines like LSTM and dilated RNN. This was

02:39.520 --> 02:44.480
followed by the work, combining recurrent convolutional and continuous time models

02:44.480 --> 02:49.920
with linear state space layers, which proposed a unifying framework for sequence modelling

02:49.920 --> 02:55.680
based on a standard state space representation. This work showed how, starting from an implicit,

02:55.680 --> 03:01.120
continuous time state space model, you could discretize to produce a system that can be

03:01.120 --> 03:07.760
interpreted as a recurrent network with the benefit of efficient inference, or as a convolutional

03:07.760 --> 03:13.120
model which benefits from parallelizable training. Unfortunately, a naive implementation of these

03:13.120 --> 03:18.800
models requires a massive amount of memory. In fact, for reasonably sized models, such as when

03:18.800 --> 03:25.440
the state dimension is 256, the linear state space layer uses orders of magnitude more memory than

03:25.440 --> 03:32.000
comparably sized RNNs or CNNs. That brings us to the S4 model introduced in efficiently modelling

03:32.000 --> 03:37.200
long sequences with structured state spaces. What we'd really like to be able to do is use the

03:37.200 --> 03:42.400
convolutional interpretation we've just seen to efficiently train our state space model.

03:42.400 --> 03:48.320
Now, if we want to train state space models using the convolutional representation, we can do a

03:48.320 --> 03:53.600
little bit of unrolling and a little bit of vectorization to get this L-dimensional kernel,

03:53.600 --> 03:59.520
where L is the sequence length. The scary part here is this A raised to the power of I term.

03:59.520 --> 04:04.720
Harking back to your linear algebra classes, as soon as you see this, you may feel a strong

04:04.720 --> 04:10.400
and appropriate urge to try to diagonalize A. The authors discuss diagonalization.

04:10.400 --> 04:15.680
Unfortunately, it cannot be used directly for the hippo matrix due to numerical issues.

04:15.680 --> 04:21.280
Specifically, it has entries exponentially large in the state size. However, we can address the

04:21.280 --> 04:27.120
hippo matrix with an adjustment. The authors note that although the hippo matrix is not normal,

04:27.120 --> 04:33.360
it can be decomposed as the sum of a normal and low rank matrix. Even this is still not useful

04:33.360 --> 04:38.720
by itself. An additional three new techniques are needed. First, instead of computing the kernel

04:38.720 --> 04:45.360
k-bar directly, they instead compute its spectrum, then determine k-bar by applying an inverse FFT.

04:45.360 --> 04:50.720
Next, we use everyone's favorite low rank trick, the Woodbury identity. That lets us perform an

04:50.720 --> 04:56.160
efficient matrix update. Third, it is shown that the diagonal matrix case is equivalent to the

04:56.160 --> 05:01.200
computation of a Cauchy kernel, a well-studied problem with stable, near-linear algorithms.

05:01.200 --> 05:06.560
All of this comes together in theorem three. It says that given any step size delta, computing

05:06.560 --> 05:12.160
the state space model convolution filter k-bar can be reduced to four Cauchy multiplies,

05:12.160 --> 05:18.800
requiring only big O of n plus l operations, ignoring logarithmic factors, and big O of

05:18.800 --> 05:25.120
n plus l space, where again, l is the sequence length and n is the state size. Empirically,

05:25.120 --> 05:31.120
the proposed S4 architecture does very well on the long-range arena, even doing well on the path

05:31.120 --> 05:36.240
x task, where the model must determine if two points are connected on a fairly large image.

05:36.240 --> 05:41.120
This task is not too hard when the input is arranged as an image, but it's pretty difficult

05:41.120 --> 05:46.640
when it's a flattened sequence, as evidenced by the fact that many other models fail on this task.

05:46.640 --> 05:52.800
If you'd like to gain some intuition for how S4 works, I highly recommend the annotated S4

05:52.800 --> 05:59.520
by Sasha Rush and Sid Karamchetti, just as with the legendary annotated transformer blog post

05:59.520 --> 06:05.920
back in 2018. The annotated S4 takes you step-by-step through the paper and provides snippets of

06:05.920 --> 06:11.200
code that implement the mathematics. It also includes nice visualizations to help you build

06:11.200 --> 06:16.800
intuition for how state-space models work. All of this brings us back to Mamba. This work builds

06:16.800 --> 06:22.720
on the state-space model approach with a few key contributions. First, a selection mechanism. Since

06:22.720 --> 06:28.320
previous state-space models lack the ability to efficiently select data in an input-dependent

06:28.320 --> 06:33.760
manner, they design a simple selection mechanism by parameterizing the state-space model parameters

06:33.760 --> 06:39.280
based on the input. Unfortunately, this change makes it hard for us to use the efficient convolutional

06:39.280 --> 06:45.280
trick we saw in S4, so the second contribution is a hardware-aware algorithm that computes the

06:45.280 --> 06:50.640
model recurrently with a scan instead of convolution. This leads to an implementation that is faster

06:50.640 --> 06:56.400
than previous methods, both in theory, scaling linearly in sequence length compared to pseudo-linear

06:56.400 --> 07:03.280
for all convolution-based SSMs and on modern hardware, up to three times faster on A100 GPUs.

07:03.280 --> 07:08.640
Spicy. The third contribution is to simplify prior deep-sequence model architectures by

07:08.640 --> 07:14.400
combining the design of prior state-space model architectures with the MLP block of transformers

07:14.400 --> 07:19.440
into a single block, leading to a simple and homogenous architecture design called Mamba

07:19.440 --> 07:24.640
that incorporates selective state-spaces. By building on a state-space model foundation,

07:24.640 --> 07:29.760
Mamba is related to many previous state-space model architectures. In addition to the models we

07:29.760 --> 07:36.400
met earlier, there is Linear Attention, which can be viewed as a degenerate linear SSM, the gloriously

07:36.400 --> 07:45.040
named Hungry Hungry Hippos or H3, Hyena, RhettNet, and RWKV. The core motivation behind Mamba is to

07:45.040 --> 07:49.760
use selection as a means of compression. The authors point out that you can view various

07:49.760 --> 07:55.120
sequence models as making different trade-offs when tackling a fundamental problem of sequence

07:55.120 --> 08:00.800
modeling, i.e. compressing context into a smaller state. At one extreme, we have Attention, which

08:00.800 --> 08:05.920
is both effective and inefficient because it explicitly does not compress context at all.

08:05.920 --> 08:10.560
At the other extreme, recurrent models are efficient because they have a finite state,

08:10.560 --> 08:15.680
but their effectiveness is limited by how well this state has compressed the context. To explore

08:15.680 --> 08:22.320
this trade-off, the authors focus on two synthetic tasks. One previously well-studied task is copying.

08:22.320 --> 08:28.080
The job of the model is to copy color sequences in the input into the output, delayed by some

08:28.080 --> 08:33.040
constant offset. Standard convolutional models with fixed kernels can solve this without any

08:33.040 --> 08:38.240
problem. The first of the harder tasks they consider is selective copying. This time, we have

08:38.240 --> 08:44.560
random spaces symbolized by the white blocks in between the input sequence elements. To successfully

08:44.560 --> 08:50.320
copy the color outputs to the output while ignoring the white blocks, we need a mechanism that behaves

08:50.320 --> 08:55.760
differently at different inputs. The second task they look at is induction heads. Here,

08:55.760 --> 09:00.240
in order to predict the appropriate color at the question mark block, the model needs to be able

09:00.240 --> 09:05.920
to perform retrieval back over the input sequence based on the context provided by the black square,

09:05.920 --> 09:11.200
in order to predict that blue is the next color in the sequence. To build a model capable of solving

09:11.200 --> 09:18.640
these tasks, the models start from the S4 state space model, which uses parameters a, b, c, and

09:18.640 --> 09:24.480
delta, which don't depend on the input sequence. Now, for the proposed algorithm, to allow the model

09:24.480 --> 09:31.680
to behave differently on different inputs, b, c, and delta are altered to be time-varying. They now

09:31.680 --> 09:36.800
depend on the input sequence. This gives the model more power, but means we can no longer use the

09:36.800 --> 09:43.200
efficient S4 convolution trick. To get around this, the authors develop a selective scan based on

09:43.200 --> 09:48.720
hardware-aware state expansion. In effect, the two big challenges to be tackled once we give up on

09:48.720 --> 09:54.720
convolution are the sequential nature of recurrence and the large memory usage. There are three

09:54.720 --> 10:00.480
techniques that come into play, kernel fusion, parallel scan, and recomputation. A key idea for

10:00.480 --> 10:05.520
getting around the sequential processing problem comes from simplified state space layers for

10:05.520 --> 10:11.760
sequence modeling, or S5, which highlighted the benefits of using parallel scans to maintain

10:11.760 --> 10:18.160
efficiency while avoiding the use of convolutional tricks used in S4. Mamba uses this parallel scan

10:18.160 --> 10:23.920
idea in combination with efficient use of memory. In particular, instead of preparing the scan input

10:23.920 --> 10:30.240
in GPU high bandwidth memory, they load the state space model parameters directly from the slow high

10:30.240 --> 10:36.320
bandwidth memory to fast SRAM, where they perform the discretization and recurrence. Then they write

10:36.320 --> 10:42.080
the final results back to high bandwidth memory. Last, recomputation is used to reduce the memory

10:42.080 --> 10:47.680
requirements. This allows them to avoid saving the intermediate states, which are necessary for back

10:47.680 --> 10:52.960
propagation. Putting this all together, here is the selective state space model with hardware-aware

10:53.040 --> 10:59.680
state expansion. The first thing we see is that BT, delta T, and CT all depend on the input

10:59.680 --> 11:06.080
XT via the selection mechanism. The second thing we see is that the parameters are loaded into fast

11:06.080 --> 11:12.240
GPU SRAM, indicated in this diagram by the color orange, where the update is performed. Then the

11:12.240 --> 11:18.800
output is ultimately stored back into GPU high bandwidth memory, shown in green. Next, we come to

11:18.800 --> 11:24.240
the simplified state space model architecture. Here, the authors combine the Hungry Hungry Hippos

11:24.240 --> 11:29.680
block with a gated MLP block to produce their Mamba block. The shapes here indicate that the

11:29.680 --> 11:35.760
dimensionality is expanded inside the block. This block is repeated and interleaved with standard

11:35.760 --> 11:41.360
normalization and residual connections to form the Mamba architecture. I'll mention a few other

11:41.360 --> 11:46.720
model details. The authors note that most prior state space models use complex numbers in their

11:46.720 --> 11:51.360
state, but it has been empirically observed that completely real valued state space models seem

11:51.360 --> 11:57.520
to work fine, and possibly even better, in some settings. So, they use real values as the default,

11:57.520 --> 12:03.120
which work well for all but one of their tasks. Next, we come to the empirical evaluation. First,

12:03.120 --> 12:08.800
we have the synthetic tasks described earlier. First, as a bit of notation, the authors abbreviate

12:08.800 --> 12:15.440
selective state space models as S6 models, because they are S4 models with a selection mechanism,

12:15.440 --> 12:22.080
and computed with a scan. That's a lot of S's. On the selective copying task, S6 works well with

12:22.080 --> 12:28.560
every architecture, but S4 and Hyena work comparatively poorly. On the induction heads task,

12:28.560 --> 12:34.400
Mamba, shown in brown, appears up here at the top, where we see it succeeds on test sequence lengths

12:34.400 --> 12:39.600
of up to a million tokens, which is 4,000 times longer than it saw during training,

12:39.600 --> 12:44.240
while none of the other methods compared to generalize to beyond twice their training length.

12:44.320 --> 12:49.120
Next, we have experiments on language modelling, which follow the training recipe described in

12:49.120 --> 12:55.920
the GBT3 work, and train on the pile dataset. Here, the metric on the y-axis is perplexity,

12:55.920 --> 13:01.680
so lower is better. We can see that across a sequence length of 2,048, and a sequence length

13:01.680 --> 13:10.480
of 8,192, that Mamba, shown in purple, outperforms the other baselines, and matches Transformer++,

13:10.480 --> 13:15.840
which is highlighted in orange, and represents the strongest Transformer recipe that the authors

13:15.840 --> 13:22.240
know of, based on the palm and llama architectures. That means rotary embeddings, swigloo MLPs,

13:22.240 --> 13:28.640
rmsnorm instead of leonorm, no linear bias, and higher learning rates. On zero shot downstream

13:28.640 --> 13:34.400
evaluations, there are lots of comparisons, generally we see Mamba achieving an average

13:35.200 --> 13:41.680
of baselines at twice the model size. Next, there are studies on DNA modelling. On human genome

13:41.680 --> 13:48.320
data, Mamba, shown in orange, scales better than Hyena DNA, shown in blue, and Transformer++,

13:48.320 --> 13:53.680
shown in red, when you scale up the number of parameters. It also scales better than Hyena DNA,

13:53.680 --> 13:59.520
when you scale up the sequence length. Mamba models also do well relative to a Hyena DNA baseline,

13:59.520 --> 14:06.160
when fine tuning for a species DNA classification task. Here, the metric on the y-axis is accuracy,

14:06.160 --> 14:10.720
so higher is better, with longer sequences as we move to the right along the x-axis.

14:10.720 --> 14:15.360
We see the orange and green Mamba curves rising. I won't go through them in detail,

14:15.360 --> 14:20.560
but there are other experiments showing that Mamba mostly works well on audio modelling and

14:20.560 --> 14:25.840
generation. However, there is one ablation in the appendix, identifying a case when a naive

14:25.920 --> 14:30.320
application of Mamba with the selection mechanism seems to hurt performance. Here,

14:30.320 --> 14:35.760
lower is better, and the orange line denotes the default Mamba configuration. The authors suggest

14:35.760 --> 14:40.880
that this may be a case where audio waveforms actually benefit from linear time-invariant

14:40.880 --> 14:46.560
models which have a matching inductive bias. Next, we have speed and memory benchmarks.

14:46.560 --> 14:51.360
The authors provide an implementation of the scan operation that is pretty fast.

14:51.360 --> 14:58.720
They compare the scan versus convolution and attention on an A100 GPU. Here, the y-axis measures

14:58.720 --> 15:05.440
time, so lower is better. We see the red line representing their implementation is considerably

15:05.440 --> 15:10.560
below the other mechanisms at longer sequence lengths. In terms of inference throughput,

15:10.560 --> 15:16.400
where higher is better, we find that Mamba, shown in blue and green, can achieve five times higher

15:16.400 --> 15:22.240
throughput than Transformers, benefiting from its recurrent nature. In terms of limitations,

15:22.240 --> 15:26.960
the authors highlight that there is no free lunch on the continuous discrete spectrum.

15:26.960 --> 15:32.720
The selection mechanism overcomes the weaknesses of prior state space models on discrete modalities

15:32.720 --> 15:38.400
such as text and DNA, but this can impede their performance on data that linear time-invariant

15:38.400 --> 15:44.160
state space models excel on, which we saw with the audio ablation. They also note that the empirical

15:44.160 --> 15:50.000
evaluation is limited to small model sizes, below the threshold of most strong open-source

15:50.000 --> 15:55.920
LLMs. Therefore, it remains to assess whether Mamba still compares favourably at larger sizes.

15:55.920 --> 16:00.720
That's it, we've reached the end. If you'd like to try it out, the authors have released code on GitHub.

