1
00:00:00,000 --> 00:00:05,720
Hello everybody, David Shapiro here with a update.

2
00:00:05,720 --> 00:00:09,120
So we're going to cover two things today real quick.

3
00:00:09,120 --> 00:00:15,720
The big thing is, I mentioned in a previous video, we need GAIA, Global AI Agency.

4
00:00:15,720 --> 00:00:17,160
And we're working towards that.

5
00:00:17,160 --> 00:00:24,080
And so I created a public repository of news and summaries, basically where I'm collecting

6
00:00:24,080 --> 00:00:29,360
evidence and news that is very encouraging because it's all moving in the right direction.

7
00:00:29,360 --> 00:00:35,520
We've got everything from the Future of Life Institute open letter to pause giant models,

8
00:00:35,520 --> 00:00:40,880
news out of the UK, the EU, which yes, I know the UK left the EU.

9
00:00:40,880 --> 00:00:47,000
I made a mistake of forgetting about Brexit in a previous video.

10
00:00:47,000 --> 00:00:53,000
The UN Secretary General talking about yes, he's amenable to the idea of an international

11
00:00:53,000 --> 00:01:00,960
AI watchdog, the United States Senate, their hearing on AI, the EU AI Act, the US House

12
00:01:00,960 --> 00:01:08,600
of Representatives and Senate having now basically matched actions with words.

13
00:01:08,600 --> 00:01:15,440
So after the hearing, both the US House of Representatives and the Senate have introduced

14
00:01:15,440 --> 00:01:20,040
legislation or commissions or policies.

15
00:01:20,040 --> 00:01:23,480
So we'll go over all those in much greater detail in just a moment.

16
00:01:23,480 --> 00:01:27,240
But before we dive into that, I've been working on this thing.

17
00:01:27,240 --> 00:01:32,680
I had this idea for a long time, and it's basically we can use large language models

18
00:01:32,680 --> 00:01:38,840
like GPT because they are trained on so much text data, they're trained on data from around

19
00:01:38,840 --> 00:01:39,840
the globe.

20
00:01:39,840 --> 00:01:46,480
Granted, it is primarily English, so there is definitely some bias, cultural bias in there.

21
00:01:46,480 --> 00:01:52,440
However, that being said, it has read so much about the world that it knows more about religion,

22
00:01:52,440 --> 00:01:57,760
politics, psychology, culture, history than any one person.

23
00:01:57,760 --> 00:02:04,440
So as far as having the ability to take perspectives, it's actually pretty good.

24
00:02:04,440 --> 00:02:08,360
And so rather than walk you through the code, I'll just show you what I've got working so

25
00:02:08,360 --> 00:02:09,360
far.

26
00:02:09,360 --> 00:02:14,360
So basically what I have it do is actually first I need to make sure, well, no, that's

27
00:02:14,360 --> 00:02:15,640
fine.

28
00:02:15,640 --> 00:02:26,000
So basically what I have it do, what it does, anyways, is I have it synthesize a persona.

29
00:02:26,000 --> 00:02:35,880
And so this persona is a whole bunch of random stuff like variables that it can pick to basically

30
00:02:35,880 --> 00:02:38,240
create a synthetic person.

31
00:02:38,240 --> 00:02:46,480
And I don't mean that in the manipulation way, but it has geographic origins, ethnicities,

32
00:02:46,480 --> 00:02:53,800
cultures, languages, political dispositions, different factors about the person, such as

33
00:02:53,800 --> 00:02:59,440
their physical health, their mental health, other habits, preferences, that sort of thing.

34
00:02:59,440 --> 00:03:05,920
And so basically what it does is to create a persona is it'll grab one of these variables

35
00:03:05,920 --> 00:03:13,120
or one option from each set of variables and then create a profile or a dossier in

36
00:03:13,120 --> 00:03:17,440
order to take the perspective of that person.

37
00:03:17,440 --> 00:03:20,520
And then it will talk through on a particular issue.

38
00:03:20,520 --> 00:03:26,600
And I picked UBI because I've been on a economics and UBI kick lately.

39
00:03:26,600 --> 00:03:35,160
And so I wanted to basically come up with a way of automatically understanding what the

40
00:03:35,160 --> 00:03:42,120
concerns that everyone is going to have could be, but not just elucidate or articulate the

41
00:03:42,120 --> 00:03:49,120
problems that they will have or the concerns that they'll have over a policy like UBI,

42
00:03:49,120 --> 00:03:52,960
but how do we actually arrive at consensus?

43
00:03:52,960 --> 00:03:58,880
And consensus is not necessarily, so a lot of people have misconception about consensus.

44
00:03:58,880 --> 00:04:01,480
Consensus doesn't mean unanimous agreement.

45
00:04:01,480 --> 00:04:06,160
Consensus means that it gets to a point that is good enough that people will kind of stop

46
00:04:06,160 --> 00:04:07,160
fighting it.

47
00:04:07,160 --> 00:04:08,720
It's about compromise.

48
00:04:08,720 --> 00:04:15,200
And so anytime that there's been any kind of contentious policy or legislation, even

49
00:04:15,200 --> 00:04:19,400
including the United States Constitution, nobody was happy with it because they had to

50
00:04:19,400 --> 00:04:24,920
come to consensus in order to get something that everyone would tolerate, everyone would

51
00:04:24,920 --> 00:04:25,920
accept.

52
00:04:25,920 --> 00:04:30,280
So it wasn't perfect to anyone's mind, but it was something that they would all accept.

53
00:04:30,280 --> 00:04:35,800
And so the idea is, okay, let's take this random profile of a person so that we can

54
00:04:35,800 --> 00:04:42,040
get a global representation of all perspectives, regardless of how small they are, because

55
00:04:42,040 --> 00:04:47,040
the thing is, when you look at some of these things like atheist, Islam, Christianity,

56
00:04:47,040 --> 00:04:54,200
Buddhist, some of these are overrepresented in some areas and some of them are underrepresented.

57
00:04:54,200 --> 00:04:55,800
Same thing for political leanings.

58
00:04:55,800 --> 00:05:01,760
I even include libertarians and anarchists and statists and communists and reactionaries

59
00:05:01,760 --> 00:05:03,800
and populists and nationalists and fascists.

60
00:05:03,800 --> 00:05:07,440
I don't know if fascism isn't in here.

61
00:05:07,440 --> 00:05:08,920
Fascism isn't in here.

62
00:05:08,920 --> 00:05:11,960
So fascist perspectives don't get represented, but nationalists do.

63
00:05:11,960 --> 00:05:14,800
So close enough, authoritarians get represented.

64
00:05:14,800 --> 00:05:20,560
So the idea is that we can get a very well-rounded representation of all human perspectives more

65
00:05:20,560 --> 00:05:21,560
or less.

66
00:05:21,560 --> 00:05:27,040
There are going to be some flaws with this because it is the first edition, first example.

67
00:05:27,040 --> 00:05:34,080
But the idea is that you can get a very, very diverse set of perspectives on a particular

68
00:05:34,080 --> 00:05:35,080
issue.

69
00:05:35,080 --> 00:05:39,720
And then once you get those perspectives, you can then work through and figure out, okay,

70
00:05:39,720 --> 00:05:45,080
what kind of policy is this person most likely to accept?

71
00:05:45,080 --> 00:05:50,240
And so by then generating a whole bunch of potential policies, you can say, okay, well,

72
00:05:50,320 --> 00:05:54,600
let's look at the commonality between all of these and let's get really creative.

73
00:05:54,600 --> 00:05:56,720
So let me just go ahead and show you how this works.

74
00:05:56,720 --> 00:05:57,720
All right.

75
00:05:57,720 --> 00:06:02,280
So the first thing it does is it grabs a random persona, so you see where it's like heavy

76
00:06:02,280 --> 00:06:07,680
social media user, they're West Asian, cultural background is West Asian, their geographic

77
00:06:07,680 --> 00:06:09,920
origin is from North Africa.

78
00:06:09,920 --> 00:06:15,400
So imagine a West Asian family in North Africa, they're a wealthy family, they're heavy social

79
00:06:15,400 --> 00:06:17,280
media user, so on and so forth.

80
00:06:17,280 --> 00:06:22,520
Okay, so I ask, what would this person think about UBI?

81
00:06:22,520 --> 00:06:27,920
And because ChatGPT is so heavily trained to be a helpful assistant, I couldn't get

82
00:06:27,920 --> 00:06:33,320
ChatGPT to take that perspective, but what I could do is say advocate zealously on behalf

83
00:06:33,320 --> 00:06:36,640
of this person, and that got really good results.

84
00:06:36,640 --> 00:06:40,800
So let's see, as an advocate for this persona, I would say that they might have mixed feelings

85
00:06:40,800 --> 00:06:42,160
about UBI.

86
00:06:42,160 --> 00:06:49,560
On one hand, they are wealthy and underemployed, which might lead them to see UBI as, let's

87
00:06:49,560 --> 00:06:50,560
see, where did it go?

88
00:06:50,560 --> 00:06:51,560
Hang on.

89
00:06:51,560 --> 00:06:52,560
Okay, it's saved.

90
00:06:52,560 --> 00:06:53,560
Sorry.

91
00:06:53,560 --> 00:06:57,160
Let's see, unnecessary for themselves and potentially harmful to their financial status

92
00:06:57,160 --> 00:06:59,120
if it results in higher taxes.

93
00:06:59,120 --> 00:07:03,080
They also might be concerned about the potential for UBI to discourage work and productivity

94
00:07:03,080 --> 00:07:06,560
given their own success in wealth oriented aspirations.

95
00:07:06,560 --> 00:07:09,760
On the other hand, their experience with financial stress and their social concerns might make

96
00:07:09,760 --> 00:07:15,400
them sympathetic to the idea of UBI as a means of reducing poverty and inequality.

97
00:07:15,400 --> 00:07:20,160
However, their nationalist political affiliation might make them more inclined to support policies

98
00:07:20,160 --> 00:07:24,200
that prioritize the needs and interests of their own country and its citizens, which

99
00:07:24,200 --> 00:07:28,160
could include skepticism towards UBI if they perceive it as benefiting outsiders as a form

100
00:07:28,160 --> 00:07:29,800
of globalist policy.

101
00:07:29,800 --> 00:07:35,120
So you see it has a very nuanced way of saying, okay, this person has lots of different beliefs

102
00:07:35,120 --> 00:07:38,760
and it's pretty complex.

103
00:07:38,760 --> 00:07:39,760
Let's see.

104
00:07:39,760 --> 00:07:43,480
Okay, so given their wealth oriented aspirations, nationalist political affiliation and their

105
00:07:43,480 --> 00:07:48,200
experience with financial stress, they might resonate with a fiscal policy or UBI implementation

106
00:07:48,200 --> 00:07:52,640
that is designed to stimulate economic growth and productivity while also addressing social

107
00:07:52,640 --> 00:07:54,720
inequality within their own country.

108
00:07:54,720 --> 00:07:58,380
For instance, they might support a UBI program that is funded through progressive taxation

109
00:07:58,380 --> 00:08:01,320
where the wealthy pay a higher percentage of their income taxes.

110
00:08:01,320 --> 00:08:04,200
This would align with their social concerns and their belief in the importance of addressing

111
00:08:04,200 --> 00:08:05,200
inequality.

112
00:08:05,200 --> 00:08:08,420
However, they would likely want assurances that this would not unduly burden them or

113
00:08:08,420 --> 00:08:10,820
negatively impact their own financial status.

114
00:08:10,820 --> 00:08:15,420
They might also resonate with a UBI program that includes work incentives or requirements

115
00:08:15,420 --> 00:08:18,640
to address their concerns about discouraging work and productivity.

116
00:08:18,640 --> 00:08:22,540
For instance, they might support a UBI that is conditional on recipients engaging in

117
00:08:22,540 --> 00:08:26,300
some kind of work, education or community service.

118
00:08:26,300 --> 00:08:31,260
So this is a theme that keeps emerging because I've run this experiment a few times.

119
00:08:31,260 --> 00:08:35,820
I don't know if that's the underlying training of chat GPT or if it's just this is the most

120
00:08:35,820 --> 00:08:42,180
logical way forward where basically either you have a requirement or an incentive or

121
00:08:42,180 --> 00:08:43,180
reward.

122
00:08:43,180 --> 00:08:48,940
So basically like you get, everyone gets a baseline UBI, but if you volunteer, you get

123
00:08:48,940 --> 00:08:50,540
more UBI, right?

124
00:08:50,540 --> 00:08:54,220
It's like kind of adjusting your tax returns here in America.

125
00:08:54,220 --> 00:08:55,220
Okay.

126
00:08:55,220 --> 00:09:00,220
So finally, given their nationalist policy, they want to UBI that prioritizes the needs

127
00:09:00,220 --> 00:09:03,140
and interests of their own country's citizens.

128
00:09:03,140 --> 00:09:09,840
So this could be something like a tax break if you buy local or any number of ways.

129
00:09:09,840 --> 00:09:17,820
So then I ask it to say, all right, come up with a policy that this person might like.

130
00:09:17,820 --> 00:09:23,260
And so in this case, it said the national prosperity dividend, which that sounds rather

131
00:09:23,260 --> 00:09:25,820
authoritarian, but okay, we'll go with it.

132
00:09:25,820 --> 00:09:33,180
So the NPD, which that's also the acronym for narcissistic personality disorder.

133
00:09:33,180 --> 00:09:35,380
Oh boy.

134
00:09:35,380 --> 00:09:38,700
Maybe let's workshop that before we go live with it.

135
00:09:38,700 --> 00:09:43,620
Anyways, so the NPD is a form of universal basic income that is designed to stimulate

136
00:09:43,620 --> 00:09:47,380
economic growth, reduce social inequality and prioritize the needs and interest.

137
00:09:47,380 --> 00:09:48,380
Okay.

138
00:09:48,380 --> 00:09:49,380
So you get that.

139
00:09:49,380 --> 00:09:54,180
So it's predicated on their engaging in some form of work, education or community service,

140
00:09:54,180 --> 00:09:57,940
more time work, vocational training, higher ed, so on and so forth.

141
00:09:57,940 --> 00:10:02,860
You know, actually like that particular thing, because this is a recurring theme, I'm not

142
00:10:02,860 --> 00:10:06,940
certain that that's actually a bad idea where like you incentivize people to continue to

143
00:10:06,940 --> 00:10:13,980
better themselves, even if it's a small carrot, like, you know, hey, if you're in, if you're

144
00:10:13,980 --> 00:10:18,380
going to higher education, even if, you know, you're at the college degree you're getting

145
00:10:18,380 --> 00:10:22,980
might not ever do anything, you get an extra $500 a month, you will at least be a better

146
00:10:22,980 --> 00:10:26,900
informed citizen and a better civic participant.

147
00:10:26,900 --> 00:10:27,900
Who knows?

148
00:10:27,900 --> 00:10:28,900
I don't know.

149
00:10:28,900 --> 00:10:30,380
Incentivize the behavior you want to see.

150
00:10:30,380 --> 00:10:31,380
Oh, here we go.

151
00:10:31,380 --> 00:10:34,500
The NPD would also include a patriotic bonus.

152
00:10:34,500 --> 00:10:35,500
Yikes.

153
00:10:35,500 --> 00:10:40,700
An additional payment for those who contribute significantly to the country's economy, culture

154
00:10:40,700 --> 00:10:41,940
or society.

155
00:10:41,940 --> 00:10:42,940
Okay.

156
00:10:42,940 --> 00:10:45,740
I wouldn't call it a patriotic bonus.

157
00:10:45,740 --> 00:10:49,860
That's a little, that's a little totalitarian, but I get the, I get the idea.

158
00:10:49,860 --> 00:10:53,300
So this would include entrepreneurs, artists, scientists and community leaders.

159
00:10:53,300 --> 00:10:58,780
So actually I think Ireland already has something like this where basically if you are a professional

160
00:10:58,780 --> 00:11:03,620
artist or author or whatever a creative type, you get a stipend.

161
00:11:03,620 --> 00:11:07,500
I don't know what they call it in Ireland, but there is precedent for this being a thing.

162
00:11:07,500 --> 00:11:12,940
So like if you're, if you're a content creator or a cultural commentator or whatever, you

163
00:11:12,940 --> 00:11:16,940
could get a little bit, an additional stipend.

164
00:11:16,940 --> 00:11:17,940
So all right.

165
00:11:18,220 --> 00:11:20,060
There you have it.

166
00:11:20,060 --> 00:11:22,780
And then if we run it again, so let me do a clear screen.

167
00:11:22,780 --> 00:11:28,500
If we run it again, let's say we get a young adult who's wealthy from Eastern Europe, whose

168
00:11:28,500 --> 00:11:32,340
culture is indigenous South American, interesting.

169
00:11:32,340 --> 00:11:33,900
They like gaming.

170
00:11:33,900 --> 00:11:34,900
They speak Spanish.

171
00:11:34,900 --> 00:11:38,420
That makes sense if they're from South America.

172
00:11:38,420 --> 00:11:43,580
They do not care about the community and they are a very fragile person.

173
00:11:43,620 --> 00:11:51,660
They're an angry, fragile person who's, who is educated, they've been very experienced.

174
00:11:51,660 --> 00:11:55,660
They had a good childhood, sorry, they've had a good, good life experience.

175
00:11:55,660 --> 00:11:56,740
They're presently single.

176
00:11:56,740 --> 00:11:58,740
They're a progressive atheist.

177
00:11:58,740 --> 00:11:59,740
Yeah.

178
00:11:59,740 --> 00:12:03,820
This actually sounds, oh, and they're worried about their career.

179
00:12:03,820 --> 00:12:04,820
Interesting.

180
00:12:04,820 --> 00:12:05,820
Okay.

181
00:12:05,820 --> 00:12:07,580
So let's see.

182
00:12:07,580 --> 00:12:09,740
I'm not going to read the whole thing to you.

183
00:12:09,740 --> 00:12:13,060
Like I did the first one, you get the idea.

184
00:12:13,260 --> 00:12:17,460
So you see how it takes into account, like the last one, they were nationalist and blah,

185
00:12:17,460 --> 00:12:18,460
blah, blah.

186
00:12:18,460 --> 00:12:19,460
And this one is progressive.

187
00:12:19,460 --> 00:12:23,260
Given their moderate intolerance, they might prefer UBI that includes some form of means

188
00:12:23,260 --> 00:12:24,260
testing.

189
00:12:24,260 --> 00:12:27,260
Means testing keeps coming up, keeps coming up as well.

190
00:12:27,260 --> 00:12:28,260
Let's see.

191
00:12:28,260 --> 00:12:30,780
Skeptical about whether everyone deserves UBI.

192
00:12:30,780 --> 00:12:31,780
Ouch.

193
00:12:31,780 --> 00:12:36,140
I, there's some people on Reddit that this sounds like.

194
00:12:36,140 --> 00:12:37,420
And that's not to trash people on Reddit.

195
00:12:37,420 --> 00:12:41,100
I actually learned a lot from some people on Reddit.

196
00:12:41,100 --> 00:12:42,100
But yeah.

197
00:12:42,100 --> 00:12:45,940
So UBI to reduce overconsumption and promote more sustainable lifestyles.

198
00:12:45,940 --> 00:12:46,940
Yeah.

199
00:12:46,940 --> 00:12:53,420
So this actually keeps coming up as well where for some people means testing sustainability.

200
00:12:53,420 --> 00:13:00,460
So basically like you might get an additional UBI bonus if you live sustainably or whatever.

201
00:13:00,460 --> 00:13:04,460
So basically like discourage overconsumption, that's a, that's a trend that keeps coming

202
00:13:04,460 --> 00:13:05,460
up.

203
00:13:05,460 --> 00:13:06,460
So okay.

204
00:13:06,460 --> 00:13:13,700
What environmental U, U, UBI, the puibi, the PB would be funded primary through a progressive

205
00:13:13,700 --> 00:13:18,340
tax system where the wealthiest individuals and okay, yeah, blah, blah, blah.

206
00:13:18,340 --> 00:13:21,820
I haven't seen too much in terms of funding.

207
00:13:21,820 --> 00:13:27,020
One of them did say fund it through carbon taxes, which I thought was, was an interesting

208
00:13:27,020 --> 00:13:31,580
way as you partially fund it through, through that to align with their environmentalist values

209
00:13:31,580 --> 00:13:33,460
would also, oh, here it is.

210
00:13:33,460 --> 00:13:34,460
Carbon tax.

211
00:13:34,460 --> 00:13:35,460
Okay.

212
00:13:35,580 --> 00:13:37,460
So these are some ideas that keep coming up.

213
00:13:37,460 --> 00:13:42,100
Again, I think that the underlying model has a little bit of bias here.

214
00:13:42,100 --> 00:13:48,420
It would be interesting to see if there is a future version of GPT that is not already

215
00:13:48,420 --> 00:13:52,580
pre-trained to be kind of on board with some of these ideas.

216
00:13:52,580 --> 00:13:57,180
Because one thing that doesn't, that has come up is like, if you have someone who is just

217
00:13:57,180 --> 00:14:02,220
adamantly opposed to it, it doesn't say like, this person will never agree to this under

218
00:14:02,220 --> 00:14:03,340
any circumstances.

219
00:14:03,340 --> 00:14:07,420
It works really hard to try and find something that they might agree with.

220
00:14:07,420 --> 00:14:15,380
So anyways, this is all saved out to, it's saved out into the UBI folder as a YAML document.

221
00:14:15,380 --> 00:14:20,220
So it just, it basically just saves the conversation as, as a whole.

222
00:14:20,220 --> 00:14:25,780
I think, yeah, it even includes the system message because the system message includes

223
00:14:25,780 --> 00:14:29,260
the, includes the, the persona.

224
00:14:29,260 --> 00:14:30,500
And the first one, it was hilarious.

225
00:14:30,500 --> 00:14:36,740
It was a radically intolerant feminist who is a Scientologist, which is just like, wow,

226
00:14:36,740 --> 00:14:40,500
this was, this was really interesting as it tried to figure out how to appease a very

227
00:14:40,500 --> 00:14:42,820
dogmatic person.

228
00:14:42,820 --> 00:14:46,620
So anyways, this is a work in progress.

229
00:14:46,620 --> 00:14:52,460
I, I don't know where it's going exactly, but the idea is that maybe you could use it

230
00:14:52,460 --> 00:14:58,500
for policy research, maybe you could use it for, it actually came, it was, I've been thinking

231
00:14:58,500 --> 00:15:04,500
about it for a long time, but I had some inspiration after talking to the Gato community about the

232
00:15:04,500 --> 00:15:10,820
Democratic inputs to AI because the idea of using a chatbot to extract information from

233
00:15:10,820 --> 00:15:13,700
a person, from a real person is one thing.

234
00:15:13,700 --> 00:15:16,980
But then I was like, you know, the model already has a tremendous amount of information.

235
00:15:16,980 --> 00:15:21,860
So why don't we just bootstrap it and ask the model to kind of think through this.

236
00:15:21,860 --> 00:15:26,820
So this is essentially a tree of thought, but each branch of the tree is a different

237
00:15:26,820 --> 00:15:27,940
persona.

238
00:15:27,940 --> 00:15:32,780
And then each of those branches has three sub branches where I ask at those three questions

239
00:15:32,780 --> 00:15:36,820
like, you know, what do you think this person will think about UBI?

240
00:15:36,820 --> 00:15:39,220
What kind of fiscal policy do you think would resonate?

241
00:15:39,220 --> 00:15:42,620
And then finally, given the persona and their disposition, can you creatively conjure up

242
00:15:42,620 --> 00:15:46,640
a policy that has a high chance of reaching consensus with this person?

243
00:15:46,640 --> 00:15:49,980
So it's, you know, basically you can create an arbitrary number of branches.

244
00:15:49,980 --> 00:15:54,220
And then as you, as those branches span out and you get all the leaves, you gather the

245
00:15:54,220 --> 00:15:56,180
leaves together and see what fits.

246
00:15:56,180 --> 00:15:57,180
Okay.

247
00:15:57,180 --> 00:16:00,020
Anyways, so you're up to date on that project.

248
00:16:00,020 --> 00:16:05,140
So let's dive back into the GAIA initiative, the global AI agencies.

249
00:16:05,140 --> 00:16:11,140
So I've been watching the news and I realized that the number of things that are piling

250
00:16:11,140 --> 00:16:13,100
up that make me feel good.

251
00:16:13,100 --> 00:16:17,780
And I know that there's a lot of people out there that are skeptical of, of anything to

252
00:16:17,780 --> 00:16:20,980
do with government or corporations.

253
00:16:20,980 --> 00:16:23,940
And for those people, I empathize with you.

254
00:16:23,940 --> 00:16:32,420
Growing up, I was more like that where I was super skeptical of all trappings of power.

255
00:16:32,420 --> 00:16:38,820
And certainly my friends were very like disestablishment, Tarianism and anarcho libertarian, whatever.

256
00:16:38,820 --> 00:16:41,660
But none of them ever did anything with it.

257
00:16:41,660 --> 00:16:43,020
So this is the world we live in.

258
00:16:43,020 --> 00:16:46,540
We live in a world with corporations and governments and stuff.

259
00:16:46,540 --> 00:16:50,380
And yes, all power needs to be scrutinized.

260
00:16:50,380 --> 00:16:53,500
Seescepticism is absolutely fine.

261
00:16:53,500 --> 00:16:55,660
That is part of the democratic process.

262
00:16:55,660 --> 00:17:00,280
That being said, if you're dogmatically against all forms of power, well, I mean, wish in

263
00:17:00,280 --> 00:17:03,180
one hand and you know, you know what to do with the other one.

264
00:17:03,180 --> 00:17:04,500
See which one fills up first.

265
00:17:04,500 --> 00:17:10,500
So anyways, I have been keeping track of all this stuff because I see the currents and

266
00:17:10,500 --> 00:17:14,160
the trends and it is actually very, very encouraging to me.

267
00:17:14,160 --> 00:17:17,020
So I've got it all in chronological order.

268
00:17:17,020 --> 00:17:23,020
So first was the Future of Life Institute published their open letter, you know, signed

269
00:17:23,020 --> 00:17:26,980
by everyone, including Elon Musk and yada, yada, yada.

270
00:17:26,980 --> 00:17:30,820
What I didn't realize was that it actually came with a policy paper.

271
00:17:30,820 --> 00:17:38,020
And so this policy paper, it's only 14 pages and it has seven policy recommendations.

272
00:17:38,020 --> 00:17:43,900
So this came out in, let's see, April 12th.

273
00:17:43,900 --> 00:17:50,420
I think I might have that wrong, the pause, the big pause.

274
00:17:50,420 --> 00:17:52,780
So the paper was published March 22nd.

275
00:17:52,780 --> 00:17:58,100
The policy recommendations came out a few days later, a couple weeks later.

276
00:17:58,100 --> 00:18:03,700
So what I did was I took all that and then I made a very brief summary of the whole thing

277
00:18:03,700 --> 00:18:04,700
right here.

278
00:18:04,700 --> 00:18:09,860
So you can click on the link and see it, but it's a few basic things, mandate, robust

279
00:18:09,860 --> 00:18:15,980
third party auditing, regulate organizations access to computational power, establish capable

280
00:18:15,980 --> 00:18:21,980
AI agencies at the national level, establish liability for AI caused harm, introduce measures

281
00:18:21,980 --> 00:18:27,860
to prevent and track AI model leaks, expand AI technical safety research, and develop

282
00:18:27,860 --> 00:18:30,660
standards for identifying and managing AI generated content.

283
00:18:30,660 --> 00:18:31,660
Okay, sure, whatever.

284
00:18:31,660 --> 00:18:35,140
It's all pretty boilerplate in the grand scheme of things.

285
00:18:35,140 --> 00:18:39,580
But so that came out March, April, March 29th.

286
00:18:39,580 --> 00:18:45,620
The UK did this, this jobby, which I haven't had a chance to summarize yet.

287
00:18:45,620 --> 00:18:53,420
But basically the idea is a pro innovation, AI regulatory framework, et cetera, et cetera.

288
00:18:53,420 --> 00:18:55,340
Again, you see how long this thing is.

289
00:18:55,340 --> 00:19:01,340
That's why it takes a little, takes a little bit of doing to summarize with a, with a GPT

290
00:19:01,340 --> 00:19:02,860
API calls.

291
00:19:02,860 --> 00:19:11,940
So anyways, they call for a regulatory sandbox, which so does the EA, the EU, good grief,

292
00:19:11,940 --> 00:19:16,140
EU AI Act also calls for regulatory sandboxes.

293
00:19:16,140 --> 00:19:20,700
So if you, if you don't know a regulatory sandbox is basically you create a safe space

294
00:19:20,700 --> 00:19:28,980
where you can experiment with AI, you know, that is a little bit more permissive.

295
00:19:28,980 --> 00:19:31,180
So and this is this is very often the case.

296
00:19:31,180 --> 00:19:36,100
So for instance, one of the most familiar ones is if you're doing medical research,

297
00:19:36,100 --> 00:19:41,580
for instance, you're allowed to use substances that are otherwise illegal.

298
00:19:41,580 --> 00:19:44,340
You just have to go through approval processes.

299
00:19:44,340 --> 00:19:45,340
It's not quite the same.

300
00:19:45,340 --> 00:19:51,140
But the idea is that like people have been able to experiment with THC and LSD and psilocybin,

301
00:19:51,140 --> 00:19:55,940
even though it's still a schedule to drug or whatever in the United States.

302
00:19:55,940 --> 00:19:59,060
You just have to, you have to be approved to do so.

303
00:19:59,060 --> 00:20:04,980
Likewise, if you are an approved entity, the idea of a regulatory sandbox is that you can

304
00:20:04,980 --> 00:20:08,580
still do whatever science you need to do as long as you do so safely.

305
00:20:08,580 --> 00:20:12,860
But also one thing about these pro innovation things is, and this is a common theme that

306
00:20:12,860 --> 00:20:16,180
I actually noticed, which is why I was inspired to do this.

307
00:20:16,180 --> 00:20:24,060
So the UK and the United States have all focused on protecting innovation and accelerating

308
00:20:24,060 --> 00:20:25,060
innovation.

309
00:20:26,060 --> 00:20:34,980
So then in early May, the UN Secretary General Antonio Guterres said that he's amenable to

310
00:20:34,980 --> 00:20:41,980
the idea of the IAEA for AI, which also open AI, I forgot to add that, but open AI basically

311
00:20:41,980 --> 00:20:45,100
published a blog calling for the same thing.

312
00:20:45,100 --> 00:20:49,780
Then a few days later, the United States Senate hearing on AI.

313
00:20:49,780 --> 00:20:56,180
This was the one with Sam Ullman and Christina Montgomery and Gary Marcus.

314
00:20:56,180 --> 00:20:58,740
This was almost a three hour talk.

315
00:20:58,740 --> 00:21:02,220
And I took the transcript of that and I summarized it here.

316
00:21:02,220 --> 00:21:08,140
So the high level summary basically just says, here's the key points that they discussed.

317
00:21:08,140 --> 00:21:11,260
And then I break each one of those down further.

318
00:21:11,260 --> 00:21:16,260
So it takes you 20 minutes to read this instead of three hours.

319
00:21:16,260 --> 00:21:20,260
It's pretty straightforward.

320
00:21:20,260 --> 00:21:21,940
There was a lot of back and forth, a lot of questions.

321
00:21:21,940 --> 00:21:24,780
I watched pretty much the entire hearing.

322
00:21:24,780 --> 00:21:28,540
But clearly, the United States government was listening.

323
00:21:28,540 --> 00:21:35,500
And I wonder if this whole thing was just an orchestrated series of events or not.

324
00:21:35,500 --> 00:21:40,100
But anyways, a month later, the EU AI Act was proposed.

325
00:21:40,100 --> 00:21:42,860
I don't think it's been adopted or ratified or anything.

326
00:21:42,860 --> 00:21:47,620
I want to explain in the comments that there's still quite a process to go through.

327
00:21:47,620 --> 00:21:49,820
They've got to get feedback.

328
00:21:49,820 --> 00:21:56,140
But then more recently, in just the last couple of days, the United States House of Representatives

329
00:21:56,140 --> 00:22:04,460
by Representative Ted Liu and a few others introduced a bipartisan commission.

330
00:22:04,460 --> 00:22:05,460
Basically it's an AI commission.

331
00:22:05,460 --> 00:22:06,460
I summarized it here.

332
00:22:06,460 --> 00:22:08,860
I didn't summarize it here.

333
00:22:08,860 --> 00:22:13,060
I copied the text here because their PDF was garbage.

334
00:22:13,060 --> 00:22:15,740
Seriously, this is the United States.

335
00:22:15,740 --> 00:22:18,340
You can pay someone who knows how to make a PDF.

336
00:22:18,340 --> 00:22:19,340
Good Lord.

337
00:22:19,340 --> 00:22:22,980
So anyways, it's relatively straightforward.

338
00:22:22,980 --> 00:22:28,540
Mostly this is just saying let's appoint a panel to come up with policy recommendations.

339
00:22:28,540 --> 00:22:31,740
It's not really, they're not going to do anything.

340
00:22:31,740 --> 00:22:35,100
Basically it's going to produce three reports.

341
00:22:35,100 --> 00:22:41,540
And so this is going to recommend what the United States Congress does for AI.

342
00:22:41,540 --> 00:22:43,620
Okay, great.

343
00:22:43,620 --> 00:22:48,380
Congressional commission, they're interested in getting more information, which means that

344
00:22:48,380 --> 00:22:49,780
they're going to solicit experts.

345
00:22:49,780 --> 00:22:57,140
So one thing that I thought was most interesting was the panel is that they want to have members

346
00:22:57,140 --> 00:23:01,700
of the commission shall have a demonstrated background in at least one of the following.

347
00:23:01,700 --> 00:23:05,020
Computer science or AI specifically.

348
00:23:05,020 --> 00:23:11,340
Social society including constitutional rights, civil liberties, ethics and the creative community.

349
00:23:11,340 --> 00:23:14,820
Industry and workforce and then government including national security.

350
00:23:14,820 --> 00:23:19,320
So when you get these kinds of people in a room together, it's not just engineers and

351
00:23:19,320 --> 00:23:21,020
not just data scientists.

352
00:23:21,020 --> 00:23:26,940
This is going to be people that are in political science, civil rights, civil liberties, industry

353
00:23:26,940 --> 00:23:30,460
insiders and then finally national security experts.

354
00:23:30,460 --> 00:23:34,060
So this is going to be a pretty comprehensive set of recommendations and I'm actually pretty

355
00:23:34,060 --> 00:23:37,740
happy to see that Ted Lu is leading that.

356
00:23:37,740 --> 00:23:41,980
And Ted Lu is, or is he the Los Angeles County?

357
00:23:41,980 --> 00:23:47,420
So it's not surprising that California Bro is going to be figuring that out.

358
00:23:47,420 --> 00:23:53,300
And then finally most recently was just a couple days ago, Senator Chuck Schumer announced

359
00:23:53,300 --> 00:23:59,740
the safe framework at the CSIS, which is really interesting.

360
00:23:59,740 --> 00:24:02,220
And I don't know if this has been ratified yet or anything.

361
00:24:02,220 --> 00:24:08,380
I haven't been able to find the actual text of the idea, but there is a one pager out

362
00:24:08,380 --> 00:24:12,540
there somewhere that summarizes it very succinctly.

363
00:24:12,540 --> 00:24:17,540
But I took the transcript from this and I summarized the talk here.

364
00:24:17,540 --> 00:24:27,300
So basically it comes down to four major components, security, which basically talks about national

365
00:24:27,300 --> 00:24:35,700
security above all else, but also corporate security and privacy of citizens, accountability,

366
00:24:35,700 --> 00:24:41,580
which talks about how do you, how do you, it's actually pretty similar to the EU thing

367
00:24:41,580 --> 00:24:44,980
where, oh, I forgot to mention this for the EU AI Act.

368
00:24:44,980 --> 00:24:50,980
The primary thing that the EU AI Act does is it bans outright social credit systems and

369
00:24:50,980 --> 00:24:57,820
surveillance like high, basically big brother bans big brother stuff.

370
00:24:57,820 --> 00:25:01,020
And so this is what the security and accountability does.

371
00:25:01,020 --> 00:25:05,600
An interesting part of this was the foundations aspect of the framework.

372
00:25:05,600 --> 00:25:12,280
So basically one of the key things of Chuck Schumer's framework is to protect the foundations

373
00:25:12,280 --> 00:25:13,900
of democracy.

374
00:25:13,900 --> 00:25:18,740
So I was talking with my wife about this and she suspects that this is a direct reaction

375
00:25:18,740 --> 00:25:25,140
to the January 6th attempted insurrection at the US Capitol when people are breaking

376
00:25:25,140 --> 00:25:30,860
into and keep in mind that many members of Congress were directly in danger.

377
00:25:30,860 --> 00:25:37,700
And so we suspect, we being my wife and I, we suspect that this is actually basically

378
00:25:37,700 --> 00:25:45,060
the Congress, House of Representatives and senators, they didn't take social media seriously

379
00:25:45,580 --> 00:25:49,420
and then, you know, Facebook happened with Cambridge Analytica.

380
00:25:49,420 --> 00:25:55,460
I don't think it's controversial to say that, that Facebook and other social media companies

381
00:25:55,460 --> 00:26:02,140
directly contributed to the widespread abuses of misinformation, but also just coordination

382
00:26:02,140 --> 00:26:03,180
of violence.

383
00:26:03,180 --> 00:26:04,620
It's that simple.

384
00:26:04,620 --> 00:26:09,300
And so they, they learned their lesson by not taking social media seriously and now they're

385
00:26:09,300 --> 00:26:12,020
taking artificial intelligence very seriously.

386
00:26:12,020 --> 00:26:17,300
So on a cynical note, this is basically the establishment wanting to protect the establishment

387
00:26:17,300 --> 00:26:19,820
and the status quo.

388
00:26:19,820 --> 00:26:21,500
That is a pretty cynical take.

389
00:26:21,500 --> 00:26:24,700
That doesn't mean that it's the only thing because I actually listened to all of Chuck

390
00:26:24,700 --> 00:26:28,860
Schumer's talk and he had a very clear, like words matter.

391
00:26:28,860 --> 00:26:31,260
He had a very clear-eyed understanding of what's coming.

392
00:26:31,260 --> 00:26:35,940
He even talked about the possibility of jobs dislocation.

393
00:26:35,940 --> 00:26:41,020
He likened it to globalization and offshoring because he said like, yes, globalization and

394
00:26:41,020 --> 00:26:47,340
offshoring did actually help the economy because it, you know, allowed us to lower the prices

395
00:26:47,340 --> 00:26:52,260
of goods and services, but at the same time, millions of Americans lost their job.

396
00:26:52,260 --> 00:26:56,180
And so the implication there was that the United States government did not do a good

397
00:26:56,180 --> 00:27:04,540
enough job to protect Americans while we were rabidly offshoring in the 90s and 2000s.

398
00:27:04,540 --> 00:27:06,700
And then finally, explainability.

399
00:27:06,700 --> 00:27:10,900
Some people commented that Chuck Schumer doesn't really seem to understand how AI works because

400
00:27:11,780 --> 00:27:16,060
it's not a database that you can like say, oh, this is where it got the data from.

401
00:27:16,060 --> 00:27:21,500
That being said, I think that these commissions will figure out, you know, that while you

402
00:27:21,500 --> 00:27:27,660
can't look at the model and infer what was in it, you can look at the training data.

403
00:27:27,660 --> 00:27:32,580
So what I suspect is that there's going to be very soon open source standards on training

404
00:27:32,580 --> 00:27:33,580
data.

405
00:27:33,580 --> 00:27:37,860
So basically, in order to be a licensed and approved and publicly available model, the

406
00:27:37,860 --> 00:27:42,980
underpinning training data will have to be publicly available, or at least inspected.

407
00:27:42,980 --> 00:27:46,220
He did talk about innovation first as well.

408
00:27:46,220 --> 00:27:50,300
So this is a common theme that's emerging at least in Britain and America.

409
00:27:50,300 --> 00:27:53,140
The EU is less concerned about innovation.

410
00:27:53,140 --> 00:27:58,700
They're more concerned about fundamental civil rights and foundational human rights,

411
00:27:58,700 --> 00:27:59,700
which is good.

412
00:27:59,700 --> 00:28:02,260
Like I would I would actually like that.

413
00:28:02,260 --> 00:28:10,020
But as an individual nation, they are highly, highly focused on focusing on innovation first

414
00:28:10,020 --> 00:28:12,660
and then safety rights, accountability and stuff.

415
00:28:12,660 --> 00:28:18,180
So it's basically here's the roadmap of how to innovate safely and fat and quickly.

416
00:28:18,180 --> 00:28:22,380
And the idea is there are geopolitical competitions happening.

417
00:28:22,380 --> 00:28:27,540
Vladimir Putin said what I think it was 2021, the nation that solves artificial intelligence

418
00:28:27,540 --> 00:28:31,780
will own, you know, the next century, and it's probably going to be a lot longer than

419
00:28:31,780 --> 00:28:32,780
that.

420
00:28:32,780 --> 00:28:37,820
Russia, unfortunately, for them does not have the economy or the they have brain drain,

421
00:28:37,820 --> 00:28:41,460
so they're not going to be a participant in artificial intelligence.

422
00:28:41,460 --> 00:28:46,780
It's basically going to come down to America versus China versus the EU.

423
00:28:46,780 --> 00:28:52,300
But the EU is more ideologically aligned with America and vice versa.

424
00:28:52,300 --> 00:28:57,420
So it'll basically come down to East versus West, which is basically, you know, the same

425
00:28:57,420 --> 00:29:03,220
idea of World War Two and the Cold War, which was Western ideology versus Eastern ideology.

426
00:29:03,220 --> 00:29:08,060
So this is what the geopolitical conflict is shaping up to be for the next century.

427
00:29:08,060 --> 00:29:10,300
Yay, repeat of the 20th century.

428
00:29:10,300 --> 00:29:12,420
Let's hope that it's not as bloody.

429
00:29:12,420 --> 00:29:18,300
And I say that flippantly, but I am dead serious because the stakes are very, very high here,

430
00:29:18,300 --> 00:29:23,260
which is why I call this the Gaia Initiative, because Gaia is Greek for Earth or Mother Earth.

431
00:29:23,260 --> 00:29:25,500
And also Gaia was the goddess of monsters, too.

432
00:29:25,500 --> 00:29:30,300
So on the topic of Malik and Shogoth and all those other things, I think that Gaia is a

433
00:29:30,300 --> 00:29:32,020
really appropriate name.

434
00:29:32,020 --> 00:29:33,580
So anyways, this is out here.

435
00:29:33,580 --> 00:29:39,260
It's just under github.com slash daveshop slash Gaia initiative.

436
00:29:39,260 --> 00:29:42,500
I will update this as interesting news comes out.

437
00:29:42,500 --> 00:29:43,940
I might forget about it for a while.

438
00:29:43,940 --> 00:29:49,620
I tend to do that, but it's up there and I find all this news very encouraging.

439
00:29:49,620 --> 00:29:50,620
So thanks for watching.

440
00:29:50,620 --> 00:29:51,620
I hope you got a lot out of it.

441
00:29:51,620 --> 00:29:51,640
Cheers.

