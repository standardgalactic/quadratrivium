WEBVTT

00:00.000 --> 00:03.840
David Shapiro here, your personal chief AI officer.

00:03.840 --> 00:06.800
So what I wanted to do today was unpack

00:06.800 --> 00:09.920
some of the recent patterns and trends

00:09.920 --> 00:11.360
that we've been seeing.

00:11.360 --> 00:13.660
Now I made a video recently where I talked about

00:13.660 --> 00:16.840
all the reasons that I think that AI is slowing down

00:16.840 --> 00:18.840
and of course I'm not the only one.

00:18.840 --> 00:22.240
Now there are plenty of people who disagree with this story

00:22.240 --> 00:24.840
and I'll address that in a minute with respect

00:24.840 --> 00:28.600
to the potential emergence of echo chambers.

00:28.600 --> 00:31.800
But first I want to address, okay, what does it mean?

00:31.800 --> 00:33.560
Now that AI is slowing down,

00:33.560 --> 00:35.280
or at least there are initial signs

00:35.280 --> 00:37.280
that AI might be slowing down in terms of progress,

00:37.280 --> 00:39.120
and that's not to say that it's stalling,

00:39.120 --> 00:42.540
it's just the rate of acceleration is deteriorating.

00:42.540 --> 00:43.800
So when I say slowing down,

00:43.800 --> 00:46.560
that's like we're still at the very early stages

00:46.560 --> 00:49.760
if this trend is reversing.

00:49.760 --> 00:51.960
So the first thing is safety.

00:51.960 --> 00:54.880
This is really great news for people in the safety crowd

00:54.880 --> 00:57.080
because it means that the singularity

00:57.080 --> 00:59.480
is not gonna happen in 2027.

00:59.480 --> 01:02.560
We can kick the can down the road a little bit further

01:02.560 --> 01:04.880
before we get an intelligence explosion

01:04.880 --> 01:08.160
if an intelligence explosion is even possible.

01:08.160 --> 01:10.060
Personally, I've started to have doubts

01:10.060 --> 01:12.600
that we're gonna get those accelerating returns,

01:12.600 --> 01:15.740
particularly as I've seen some new news

01:15.740 --> 01:18.920
about the way that the human brain might work.

01:18.920 --> 01:21.440
There is increasing evidence that the human brain

01:21.440 --> 01:23.640
is not just a matter of computation

01:23.640 --> 01:26.080
based on neural synaptic connections,

01:26.080 --> 01:28.160
but that it could be a combination of that,

01:28.160 --> 01:30.640
the electromagnetic waves that propagate across the brain

01:30.640 --> 01:32.160
as well as quantum effects.

01:32.160 --> 01:35.800
There is increasing evidence that human consciousness

01:35.800 --> 01:38.600
and human intelligence is actually the combination

01:38.600 --> 01:41.360
of several energies and several parts of physics

01:41.360 --> 01:43.640
that are all working together to create that.

01:43.640 --> 01:46.680
So I'm just like, hmm, maybe there's a lot more

01:46.680 --> 01:47.960
to intelligence than we thought,

01:47.960 --> 01:49.560
and of course there's gonna be a lot of people out there

01:49.560 --> 01:53.000
saying, see, I told you so, but it is what it is.

01:53.000 --> 01:55.960
And these are also just possibilities.

01:55.960 --> 01:59.480
But according to this possibility,

01:59.480 --> 02:01.980
it might be that there are gonna be continuing

02:01.980 --> 02:05.880
diminishing returns with respect to neural networks

02:05.880 --> 02:08.160
or even silicon-based computing

02:08.160 --> 02:10.920
that means that it will just be increasingly difficult

02:10.920 --> 02:15.720
to either reconstruct or to capture human-level intelligence.

02:15.720 --> 02:17.320
And another thing that's emerging to me

02:17.320 --> 02:21.080
is that we are going to see a very distinct bifurcation

02:21.080 --> 02:23.860
between human intelligence and machine intelligence,

02:23.860 --> 02:27.140
meaning that it's gonna be kind of like comparing apples

02:27.140 --> 02:29.260
to oranges, and it really already is,

02:29.260 --> 02:30.820
because we look at large language models

02:30.820 --> 02:33.500
which are very clearly processing information.

02:33.500 --> 02:34.820
I remember I had a conversation

02:34.820 --> 02:37.380
with some philosophers a year ago or so,

02:37.380 --> 02:41.060
and they made the somewhat asinine claim that,

02:41.060 --> 02:44.700
oh, they don't know anything, there's no information.

02:44.700 --> 02:47.160
I'm like, that's literally all that they're doing

02:47.160 --> 02:49.380
is just processing information,

02:49.380 --> 02:50.780
but it depends on definitions.

02:50.780 --> 02:52.440
And so to these philosophers,

02:52.440 --> 02:53.760
the idea that this is a machine

02:53.760 --> 02:55.320
that only processes information

02:55.320 --> 02:57.840
because their definition of information

02:57.840 --> 02:59.440
was stuff in human brains.

02:59.440 --> 03:00.560
I'm like, okay, well,

03:00.560 --> 03:02.860
that's just a bad definition of information.

03:02.860 --> 03:04.680
Anyways, going down a rabbit hole.

03:04.680 --> 03:06.840
My point is that it really depends

03:06.840 --> 03:08.160
on how you look at intelligence

03:08.160 --> 03:09.960
and how you define intelligence.

03:09.960 --> 03:12.040
And I really don't like those gotcha questions

03:12.040 --> 03:13.680
because it's like, how do you define intelligence?

03:13.680 --> 03:16.960
And it's like, I mean, it depends on who you ask.

03:16.960 --> 03:19.120
There's a million definitions of intelligence.

03:19.120 --> 03:21.120
And the fact that we don't have a good definition

03:21.120 --> 03:24.080
of intelligence means that also, by extension,

03:24.080 --> 03:25.240
we don't have a good definition

03:25.240 --> 03:27.120
of artificial general intelligence.

03:27.120 --> 03:30.080
And when you ask a mathematician what intelligence is,

03:30.080 --> 03:31.120
they're gonna give you one answer.

03:31.120 --> 03:33.440
When you ask a neuroscientist what intelligence is,

03:33.440 --> 03:35.000
they're gonna give you a different answer.

03:35.000 --> 03:36.840
If you ask a psychologist and a philosopher

03:36.840 --> 03:38.280
what intelligence is, again,

03:38.280 --> 03:41.160
they're going to give you fundamentally different answers.

03:41.160 --> 03:45.240
So moving on, another thing that this is good for,

03:45.240 --> 03:46.800
and this is gonna be really good news,

03:46.800 --> 03:49.700
really reassuring news to many of you out there,

03:49.700 --> 03:52.220
is that if AI is indeed slowing down,

03:52.220 --> 03:53.980
that means that the threat to jobs

03:53.980 --> 03:56.940
and the rate of change for jobs is going to be slower,

03:56.940 --> 03:59.140
which means the status quo that we have

03:59.140 --> 04:01.180
is going to persist a little bit longer

04:01.180 --> 04:03.120
than perhaps some of us would like.

04:03.120 --> 04:04.620
Now, what I do wanna address

04:04.620 --> 04:08.220
is that there's gonna be mixed reactions to this.

04:08.220 --> 04:11.140
So some people are like, let's just get it done,

04:11.140 --> 04:14.020
like replace my job, I'm ready to get out of the workforce,

04:14.020 --> 04:18.160
give me UBI and get me out of the workforce for good.

04:18.160 --> 04:19.100
I don't care.

04:19.140 --> 04:20.420
And other people are gonna be like,

04:20.420 --> 04:24.180
well, this will give us time to create new jobs,

04:24.180 --> 04:26.940
I don't wanna lose my job yet, and so on and so forth.

04:26.940 --> 04:28.220
Now, if I had to guess,

04:28.220 --> 04:30.580
now keep in mind that I'm speculating here,

04:30.580 --> 04:33.140
and that's a lot of what I do on this channel,

04:33.140 --> 04:36.140
my gut check now is that it's gonna be five to 10 years.

04:36.140 --> 04:37.380
And I've talked about this before

04:37.380 --> 04:39.620
where you look at the adoption curve,

04:39.620 --> 04:41.560
and it's like seven years.

04:41.560 --> 04:45.940
So maybe 2030, and 2030 seems to be a pretty sticky date.

04:45.940 --> 04:49.020
So anywhere between 2027 to 2030

04:49.020 --> 04:50.180
is when we might start seeing

04:50.180 --> 04:52.140
some really drastic change out there.

04:52.140 --> 04:53.500
Now, I could be wrong,

04:53.500 --> 04:56.340
we could have a confluence of multiple technologies,

04:56.340 --> 04:59.020
like again, I'm really waiting to see

04:59.020 --> 05:01.700
how GPT-5 and robotics mix,

05:01.700 --> 05:04.620
because you see the number of bipedal,

05:04.620 --> 05:07.700
like humanoid robotic chassis being built around the world.

05:07.700 --> 05:11.020
And like, remember, this is only gen one.

05:11.020 --> 05:15.460
So GPT-5 and Claude Four and whatever else,

05:15.460 --> 05:18.220
you combine that level of intelligence with robots,

05:18.220 --> 05:21.580
that really could change a lot for a lot of things.

05:21.580 --> 05:22.500
And I think there's,

05:22.500 --> 05:25.340
I don't know if it's proven out or to what extent,

05:25.340 --> 05:28.300
but I've heard that Tesla is already using their robots

05:28.300 --> 05:30.060
in the Tesla factories.

05:30.060 --> 05:33.580
And the economic carrot for that is really high.

05:33.580 --> 05:38.420
So don't underestimate the power of that economic incentive

05:38.420 --> 05:41.280
to get things really going.

05:41.280 --> 05:45.100
But overall, if the advancement of AI intelligence

05:45.100 --> 05:46.500
is indeed slowing down,

05:46.540 --> 05:48.780
it just gives us all more time to adapt

05:48.780 --> 05:52.060
on a cybersecurity level, on an economic level,

05:52.060 --> 05:53.580
on a military level.

05:53.580 --> 05:54.680
So it means that, you know,

05:54.680 --> 05:59.680
your life is not gonna get upended soon, hopefully.

06:00.460 --> 06:03.340
So this leads me to want to address another thing.

06:04.740 --> 06:07.100
About what, 12 months ago, a little bit more,

06:07.100 --> 06:10.700
I predicted that we would have AGI by September 2024.

06:10.700 --> 06:12.820
So that's just a few months from now.

06:12.820 --> 06:15.060
Now, what I was looking at at the time,

06:15.060 --> 06:17.060
and, you know, if you go back and watch my videos,

06:17.060 --> 06:18.580
there's a whole bunch of charts and data

06:18.580 --> 06:20.020
that I was looking at.

06:20.020 --> 06:23.540
And this is right along the curve

06:23.540 --> 06:26.700
of what Ray Kurzweil originally proposed,

06:26.700 --> 06:29.860
is to when we would have a human level, you know,

06:29.860 --> 06:33.040
intelligence in a single computer is actually 2023.

06:33.040 --> 06:34.820
So that was one piece of data.

06:34.820 --> 06:37.380
I was also looking at parameter counts going up,

06:37.380 --> 06:41.140
logarithmically, which they have been,

06:41.140 --> 06:42.180
but they've slowed down.

06:42.180 --> 06:44.300
And the one thing that I was not looking at,

06:44.300 --> 06:46.620
so this is the piece of data that I did not include

06:46.620 --> 06:48.340
in all of those calculations,

06:48.340 --> 06:50.860
was the exponentially rising costs

06:50.860 --> 06:53.660
of training subsequent generations of models.

06:53.660 --> 06:57.100
So, you know, as, I think it was Dimitris Abbas

06:57.100 --> 06:59.460
was talking about on a podcast recently,

06:59.460 --> 07:03.180
every subsequent generation from GPT-2 to 3 to 4

07:03.180 --> 07:07.540
costs 10 times as much to train, if not more.

07:07.540 --> 07:09.540
So while all these other things

07:09.540 --> 07:12.020
are going up exponentially, so is cost.

07:12.020 --> 07:14.780
And that did not figure into my calculus.

07:14.780 --> 07:17.140
And so because of that, it's like, oh, well,

07:17.140 --> 07:20.300
if I had thing, you know, recognize that,

07:20.300 --> 07:22.180
I might have said, well, we're probably gonna get

07:22.180 --> 07:24.500
diminishing returns sooner rather than later.

07:24.500 --> 07:27.500
Now I have been talking about diminishing returns

07:27.500 --> 07:29.340
pretty much for the life of this channel.

07:29.340 --> 07:32.660
And I've been wondering, when is the jig gonna be up?

07:32.660 --> 07:34.540
When are we gonna run out of steam here?

07:34.540 --> 07:37.100
And it looks like we're starting to run out of steam.

07:37.100 --> 07:39.700
Now again, you know, the train is still running,

07:39.700 --> 07:41.900
we're still burning pretty hot,

07:42.780 --> 07:44.260
but we're not accelerating anymore.

07:44.260 --> 07:48.940
We are probably on a more geometric trajectory right now

07:48.940 --> 07:50.180
if I had to guess.

07:51.180 --> 07:53.100
And it all comes down to economics.

07:53.100 --> 07:56.820
It really is just with exponentially rising costs

07:56.820 --> 08:00.420
with a, we're entering into what's called a red ocean market,

08:00.420 --> 08:02.780
which means it's not just, you know, a blue ocean

08:02.780 --> 08:07.060
out there with its just open AI with their frontier model.

08:07.060 --> 08:10.900
Lots of other models have caught up to GPT-4O,

08:10.940 --> 08:13.700
Claude 3.5 Sonnet has clearly surpassed it

08:13.700 --> 08:14.980
as far as I can tell.

08:14.980 --> 08:17.140
Obviously people like looking at a,

08:17.140 --> 08:19.260
whatever that benchmark is called,

08:19.260 --> 08:21.140
I don't really give that much weight because it's,

08:21.140 --> 08:23.620
that seems like it's mostly a popularity contest

08:23.620 --> 08:26.860
and open AI still has a lot of fanboys,

08:26.860 --> 08:29.640
but doing a side-by-side comparison of capability

08:29.640 --> 08:32.900
between chat GPT-4O and Claude 3.5,

08:32.900 --> 08:37.100
it is hands down Claude 3.5 is in another league

08:37.100 --> 08:38.140
as far as I can tell.

08:38.140 --> 08:39.580
Now obviously a lot of you out there

08:39.580 --> 08:41.060
use it for different things.

08:41.060 --> 08:43.420
So, you know, it is gonna,

08:43.420 --> 08:45.980
it's gonna depend on your use case.

08:45.980 --> 08:47.460
Another thing that I've noticed is that

08:47.460 --> 08:49.180
there's been fewer breakthroughs.

08:49.180 --> 08:53.260
So like, yes, chat GPT-4O has the voice mode,

08:53.260 --> 08:55.380
which is really, you know, okay, cool,

08:55.380 --> 08:57.860
like it can do a sultry voice and sound effects,

08:57.860 --> 08:59.100
which is great.

08:59.100 --> 09:03.260
But that was a predictable addition with multimodality,

09:03.260 --> 09:06.460
where it's like, okay, text and audio, great.

09:06.460 --> 09:10.580
This is still leaving a huge swath of economic interests

09:10.580 --> 09:13.380
and intellectual interests completely untouched.

09:14.500 --> 09:15.580
Take math for instance,

09:15.580 --> 09:17.500
they still haven't figured out math and physics

09:17.500 --> 09:18.880
and those sorts of things.

09:18.880 --> 09:23.880
And also after playing around with the ARC AGI test,

09:24.540 --> 09:28.060
yes, I have not been a fan of the ARC AGI test,

09:28.060 --> 09:30.900
but at the same time, like it does prove a point.

09:30.900 --> 09:33.280
It does prove a point that the kind of reasoning

09:33.280 --> 09:35.440
that these things do is still very different

09:35.440 --> 09:36.660
from human reasoning,

09:36.660 --> 09:38.200
which is another reason that I'm talking about

09:38.200 --> 09:40.200
a bifurcation of intelligence.

09:40.200 --> 09:41.960
That we might be, and this is again,

09:41.960 --> 09:43.900
as pure speculation on my point,

09:43.900 --> 09:46.840
we might be getting to a point where

09:46.840 --> 09:49.240
we're starting to recognize, okay,

09:49.240 --> 09:51.720
this machine is kind of an alien intelligence.

09:51.720 --> 09:54.320
It clearly has its own consistent way

09:54.320 --> 09:55.860
of approaching the world,

09:55.860 --> 09:57.980
but it is also very different from us.

09:57.980 --> 10:00.220
Now, Bill Gates was on a podcast recently saying

10:00.220 --> 10:02.800
that metacognition is gonna be the next step.

10:02.800 --> 10:04.840
Okay, sure, I mean, I've been working on cognitive

10:04.880 --> 10:06.400
architectures for a while,

10:06.400 --> 10:08.000
and there are some really sharp people out there

10:08.000 --> 10:10.840
that figured out how to give models metacognition

10:10.840 --> 10:13.120
a while ago, it's really just down to prompting.

10:14.480 --> 10:16.240
You can give, for instance,

10:16.240 --> 10:18.520
especially with these gigantic context windows,

10:18.520 --> 10:21.320
you can give one sec, one model say,

10:21.320 --> 10:23.720
hey, you're a metacognitive agent

10:23.720 --> 10:25.360
that's viewing these other thoughts.

10:25.360 --> 10:26.760
Tell us what you think about it.

10:26.760 --> 10:28.360
Help steer it on moral course.

10:28.360 --> 10:31.040
This was entirely all of my work on the ACE framework,

10:31.040 --> 10:33.600
the autonomous cognitive entity framework,

10:33.600 --> 10:36.200
which I did abandon because Microsoft Autogen

10:36.200 --> 10:40.200
and other platforms far surpassed what I could do

10:40.200 --> 10:43.440
on my own, even with help from people on the internet,

10:43.440 --> 10:44.400
because why it's Microsoft,

10:44.400 --> 10:46.600
and they have a lot more money than I do.

10:47.560 --> 10:49.920
Now, that leads me to another point that I wanna address,

10:49.920 --> 10:51.400
and that is echo chambers.

10:51.400 --> 10:53.720
So most of you in the audience,

10:53.720 --> 10:55.720
based on the polls that I've run,

10:55.720 --> 10:57.720
most of you in the audience, statistically speaking,

10:57.720 --> 10:59.120
are kind of in the middle of the bell curve

10:59.120 --> 11:01.280
where you're reasonable and you want the truth,

11:01.280 --> 11:04.040
and you want some honest, genuine thoughts.

11:04.040 --> 11:06.200
There are, however, many people out there

11:06.200 --> 11:08.440
that are on more of the tail,

11:08.440 --> 11:10.480
like left to right tail of the bell curve,

11:10.480 --> 11:14.760
where you wanna see doom or you wanna see acceleration,

11:14.760 --> 11:18.480
and you're not really interested in other narratives.

11:18.480 --> 11:20.960
And the reason that I'm using the word echo chamber,

11:20.960 --> 11:23.920
which is often pathologized,

11:23.920 --> 11:26.760
is because there have actually been a few people

11:26.760 --> 11:29.160
that did directly express to me

11:29.160 --> 11:31.360
they did not want an alternative narrative.

11:31.360 --> 11:35.400
They only wanted to double down on their personal narrative,

11:35.400 --> 11:37.440
the one that they want to be true,

11:37.440 --> 11:38.920
which, believe me,

11:38.920 --> 11:41.880
I want to have all kinds of advancements

11:41.880 --> 11:43.320
happening next year.

11:44.200 --> 11:46.320
That was not hype when I said

11:46.320 --> 11:48.840
that I was predicting AGI this year.

11:48.840 --> 11:51.240
That was a genuine prediction on my part,

11:51.240 --> 11:53.080
and I was like, man, things are happening,

11:53.080 --> 11:54.160
they're accelerating,

11:54.160 --> 11:55.320
but I don't believe that anymore

11:55.320 --> 11:56.840
because of the data that I'm seeing,

11:56.840 --> 11:58.800
because of the trends that I'm seeing.

11:58.800 --> 11:59.960
And I know that that sucks,

11:59.960 --> 12:04.960
like if someone is banking on a certain outcome,

12:05.280 --> 12:06.920
like expectations and reality,

12:06.920 --> 12:09.480
there's always a gap between expectations and reality,

12:09.480 --> 12:12.800
and when that gap gets bigger, it sucks.

12:12.800 --> 12:15.200
Now, some people are gonna take this news

12:15.200 --> 12:18.360
and interpret it in completely unexpected ways to me,

12:18.360 --> 12:19.600
and that's fine.

12:19.600 --> 12:22.840
But what I do wanna caution is for the five

12:22.840 --> 12:25.240
or less percent of you out there in the audience

12:25.240 --> 12:27.480
that are on the tails of the bell curve

12:27.480 --> 12:30.320
in terms of expectations and your disposition,

12:30.320 --> 12:32.440
your valence towards this,

12:32.440 --> 12:37.440
is if you broaden your narratives just a little bit,

12:38.280 --> 12:39.880
then you might be surprised

12:39.880 --> 12:42.160
at some of the other advantages that are happening,

12:42.160 --> 12:44.480
and also just realizing that there is a silver lining,

12:44.480 --> 12:47.440
is that the disruption that is coming

12:47.440 --> 12:48.880
is gonna take a little bit longer,

12:48.880 --> 12:51.760
which means that society will be a little bit more stable,

12:51.760 --> 12:54.960
which means that the risk of catastrophic outcomes

12:54.960 --> 12:58.160
or unintended outcomes goes down significantly.

12:58.160 --> 13:00.600
And on the topic of those narratives

13:00.600 --> 13:02.280
and those echo chambers,

13:02.280 --> 13:05.160
a lot of people have asked me to comment on Gary Marcus,

13:05.160 --> 13:06.720
and I've resisted until now,

13:07.600 --> 13:10.560
but having gotten back on Twitter,

13:10.560 --> 13:13.240
I will say that I've watched some really interesting

13:13.240 --> 13:15.360
and highly vitriolic debates

13:15.360 --> 13:20.360
between namely Gary Marcus, Yasha Bach, and Jan Lacoon.

13:21.120 --> 13:23.200
Now, these are supposed to be the adults in the room,

13:23.200 --> 13:26.040
and having watched Yasha on some interviews,

13:26.040 --> 13:28.320
like he's a very sharp guy,

13:28.320 --> 13:31.200
but even he got into the like, let's just beat up on,

13:31.200 --> 13:33.200
let's like, what is the term that kids use these days,

13:33.200 --> 13:36.040
like let's clown on Gary Marcus train,

13:36.040 --> 13:37.800
and that was honestly really disappointing

13:37.800 --> 13:39.360
because this is someone who's supposed to be like

13:39.360 --> 13:42.040
a high brow like academic researcher,

13:42.040 --> 13:44.920
and he's sharing like caricature memes of Gary,

13:44.920 --> 13:47.040
which, I mean, I would never do that.

13:47.040 --> 13:49.080
I don't particularly agree with Gary anymore,

13:49.080 --> 13:51.240
but that was incredibly immature.

13:51.240 --> 13:53.240
And then Jan Lacoon has often had this like,

13:53.240 --> 13:55.800
old man yells at cloud energy,

13:55.800 --> 13:58.040
which is weird because it's like,

13:58.040 --> 13:59.720
half of what he says I agree with,

13:59.720 --> 14:00.880
like Ferventland, the other half,

14:00.880 --> 14:02.880
I'm like, where did that come from?

14:02.880 --> 14:05.320
So, all right, so what happens?

14:05.320 --> 14:08.480
And this is not, to be fair, taking a step back,

14:08.480 --> 14:11.600
this is not a unique phenomenon in AI.

14:11.600 --> 14:14.360
Some of, one of my good friends as a physicist,

14:14.360 --> 14:16.640
this kind of thing happens in the physics community

14:16.640 --> 14:18.200
all the time, apparently,

14:18.200 --> 14:21.040
where it's like disagreements and arguments

14:21.040 --> 14:23.480
over interpretations will actually like,

14:23.480 --> 14:26.160
come to shouting matches and sometimes fist fights.

14:26.160 --> 14:29.720
Physicists are actually pretty hardcore, it turns out.

14:29.720 --> 14:34.720
So, from my reading of, you know, human nature,

14:34.920 --> 14:37.400
what I, the way that I interpret this is that

14:37.400 --> 14:40.040
we have a tightening status game.

14:40.040 --> 14:43.360
So, Gary, Yasha, Yan, all of these people,

14:43.360 --> 14:45.720
they suddenly saw themselves having much,

14:45.720 --> 14:49.720
much higher social status because of artificial intelligence.

14:49.720 --> 14:51.720
And so, one way to compare this is,

14:51.720 --> 14:54.360
imagine you're back in high school

14:54.360 --> 14:56.520
and something changes and suddenly,

14:56.520 --> 14:59.360
the nerds are all the most popular kids in school.

14:59.360 --> 15:01.560
Well, then something changes again,

15:01.560 --> 15:03.040
and it's like, oh, well, actually,

15:03.040 --> 15:05.960
instead of the top eight nerds, now it's the top five.

15:05.960 --> 15:08.600
And so, three have to get kicked off the island.

15:08.600 --> 15:09.800
That's what's happening.

15:09.800 --> 15:13.440
And so, they're scrabbling over diminishing social status

15:13.440 --> 15:16.880
because, again, with AI slowing down,

15:16.920 --> 15:20.040
it's no longer as hot and sexy as it was a year ago.

15:20.040 --> 15:22.000
It's no longer, you know, you can't just say,

15:22.000 --> 15:23.160
hey, I was gonna kill everyone

15:23.160 --> 15:26.320
and get an invitation to the TED stage anymore.

15:26.320 --> 15:27.760
And so, because of that,

15:27.760 --> 15:29.640
because the status game is narrowing,

15:29.640 --> 15:30.680
because the number of people

15:30.680 --> 15:32.760
that can be high status is going down,

15:32.760 --> 15:34.680
the rules are becoming more arbitrary

15:34.680 --> 15:36.880
and people are becoming a little bit more snippy,

15:36.880 --> 15:40.080
a little bit more vitriolic, as I said.

15:40.080 --> 15:42.760
The stakes go up because the risk of losing status,

15:42.760 --> 15:44.760
especially, this is what we saw with Ilya.

15:44.760 --> 15:46.080
I talked about this extensively.

15:46.080 --> 15:49.480
The reason that Ilya was socially canceled with an open AI

15:49.480 --> 15:52.440
is because he made the cardinal sin

15:52.440 --> 15:53.960
of attacking Sam Altman,

15:53.960 --> 15:56.760
even though he was doing it for what he believed

15:56.760 --> 15:58.880
was the right reasons,

15:58.880 --> 16:01.120
that was a violation of the social norm,

16:01.120 --> 16:03.300
which is Sam Altman is king.

16:03.300 --> 16:06.000
And of course, Sam Altman, as a power seeking person,

16:06.000 --> 16:08.440
was not going to tolerate that.

16:08.440 --> 16:10.040
Consciously or unconsciously,

16:10.040 --> 16:11.360
that was just never going to,

16:11.360 --> 16:12.920
he was never going to tolerate it.

16:12.920 --> 16:14.880
So what happens is,

16:14.880 --> 16:16.680
other AI commentators out there,

16:16.680 --> 16:18.960
namely Gary, Yasha, and Yan,

16:18.960 --> 16:20.520
are doubling down on their narratives,

16:20.520 --> 16:23.480
because basically they're gonna be doubling down

16:23.480 --> 16:25.480
on the narratives that got them that social status

16:25.480 --> 16:27.180
in the first place.

16:27.180 --> 16:30.320
And that is my read on the situation.

16:30.320 --> 16:32.440
And also, I take that as evidence

16:32.440 --> 16:34.240
that AI is slowing down,

16:34.240 --> 16:37.360
because again, if AI is running out of steam,

16:37.360 --> 16:39.760
then the amount of space

16:39.800 --> 16:43.120
that the public square needs of AI commentators

16:43.120 --> 16:44.080
is going down.

16:44.080 --> 16:45.360
It's also been a year and a half

16:45.360 --> 16:47.440
since Gary Marcus was in front of the Senate.

16:47.440 --> 16:50.400
And have you seen him or heard him any other place?

16:50.400 --> 16:54.000
No, like his 15 minutes of fame might be over,

16:54.000 --> 16:56.880
and that sucks, like that doesn't feel good.

16:56.880 --> 17:00.320
It does not feel good to feel like you're being left behind

17:00.320 --> 17:02.480
by the conversation or by society,

17:02.480 --> 17:05.600
which is to me, an explanation as to why Gary

17:05.600 --> 17:09.120
has been so incredibly salty lately.

17:09.160 --> 17:11.440
And then of course, other people

17:11.440 --> 17:13.800
that are not as aware of these status games

17:13.800 --> 17:15.140
will jump in on bullying,

17:15.140 --> 17:18.840
because if you show weakness in a high-stakes status game,

17:18.840 --> 17:20.800
people will unconsciously,

17:20.800 --> 17:23.760
it's tall poppy syndrome and a number of other phenomenon,

17:23.760 --> 17:25.760
people will unconsciously jump in on that

17:25.760 --> 17:27.800
and say, ah, time to bully that person,

17:27.800 --> 17:31.640
because they're signaling that their status is vulnerable.

17:31.640 --> 17:33.800
So that's my read on the whole situation.

17:33.800 --> 17:35.920
And yeah, it's not ideal, it's not what I hoped,

17:35.920 --> 17:37.320
it's not what I predicted,

17:37.320 --> 17:41.680
but I ignored the numbers, I ignored the money, right?

17:41.680 --> 17:45.880
Like, and hindsight, that was pretty dumb.

17:45.880 --> 17:49.000
So am I still predicting September 2024?

17:49.000 --> 17:52.360
Again, this is where I'm gonna double down on my narrative.

17:52.360 --> 17:54.280
I think that GPT-5 plus robots

17:54.280 --> 17:57.140
will surprise a lot of people with what it's capable of.

17:57.140 --> 17:58.880
Is it gonna replace all of us?

17:58.880 --> 18:01.240
No, it's gonna be like the Nestor class four

18:01.240 --> 18:03.080
from iRobot, where it's like,

18:03.080 --> 18:06.320
it's capable of running your mail for you automatically,

18:06.320 --> 18:08.200
but not much else.

18:08.200 --> 18:09.260
That's kind of what I predict.

18:09.260 --> 18:10.960
So it's like, you know, you can get rid of like,

18:10.960 --> 18:14.240
maybe some warehouse workers, some factory workers,

18:14.240 --> 18:16.820
some mail carriers, but it's not gonna like,

18:16.820 --> 18:18.080
upend the whole economy.

18:18.080 --> 18:19.680
So, all right.

18:19.680 --> 18:22.880
This has been your first episode of David Shapiro,

18:22.880 --> 18:25.600
your personal chief AI officer.

18:25.600 --> 18:27.560
Let me know how you think this went in the comments

18:27.560 --> 18:28.560
and I'll see you next time.

18:28.560 --> 18:29.400
Cheers.

