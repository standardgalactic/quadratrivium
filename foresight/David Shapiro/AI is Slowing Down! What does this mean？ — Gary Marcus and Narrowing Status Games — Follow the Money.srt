1
00:00:00,000 --> 00:00:03,840
David Shapiro here, your personal chief AI officer.

2
00:00:03,840 --> 00:00:06,800
So what I wanted to do today was unpack

3
00:00:06,800 --> 00:00:09,920
some of the recent patterns and trends

4
00:00:09,920 --> 00:00:11,360
that we've been seeing.

5
00:00:11,360 --> 00:00:13,660
Now I made a video recently where I talked about

6
00:00:13,660 --> 00:00:16,840
all the reasons that I think that AI is slowing down

7
00:00:16,840 --> 00:00:18,840
and of course I'm not the only one.

8
00:00:18,840 --> 00:00:22,240
Now there are plenty of people who disagree with this story

9
00:00:22,240 --> 00:00:24,840
and I'll address that in a minute with respect

10
00:00:24,840 --> 00:00:28,600
to the potential emergence of echo chambers.

11
00:00:28,600 --> 00:00:31,800
But first I want to address, okay, what does it mean?

12
00:00:31,800 --> 00:00:33,560
Now that AI is slowing down,

13
00:00:33,560 --> 00:00:35,280
or at least there are initial signs

14
00:00:35,280 --> 00:00:37,280
that AI might be slowing down in terms of progress,

15
00:00:37,280 --> 00:00:39,120
and that's not to say that it's stalling,

16
00:00:39,120 --> 00:00:42,540
it's just the rate of acceleration is deteriorating.

17
00:00:42,540 --> 00:00:43,800
So when I say slowing down,

18
00:00:43,800 --> 00:00:46,560
that's like we're still at the very early stages

19
00:00:46,560 --> 00:00:49,760
if this trend is reversing.

20
00:00:49,760 --> 00:00:51,960
So the first thing is safety.

21
00:00:51,960 --> 00:00:54,880
This is really great news for people in the safety crowd

22
00:00:54,880 --> 00:00:57,080
because it means that the singularity

23
00:00:57,080 --> 00:00:59,480
is not gonna happen in 2027.

24
00:00:59,480 --> 00:01:02,560
We can kick the can down the road a little bit further

25
00:01:02,560 --> 00:01:04,880
before we get an intelligence explosion

26
00:01:04,880 --> 00:01:08,160
if an intelligence explosion is even possible.

27
00:01:08,160 --> 00:01:10,060
Personally, I've started to have doubts

28
00:01:10,060 --> 00:01:12,600
that we're gonna get those accelerating returns,

29
00:01:12,600 --> 00:01:15,740
particularly as I've seen some new news

30
00:01:15,740 --> 00:01:18,920
about the way that the human brain might work.

31
00:01:18,920 --> 00:01:21,440
There is increasing evidence that the human brain

32
00:01:21,440 --> 00:01:23,640
is not just a matter of computation

33
00:01:23,640 --> 00:01:26,080
based on neural synaptic connections,

34
00:01:26,080 --> 00:01:28,160
but that it could be a combination of that,

35
00:01:28,160 --> 00:01:30,640
the electromagnetic waves that propagate across the brain

36
00:01:30,640 --> 00:01:32,160
as well as quantum effects.

37
00:01:32,160 --> 00:01:35,800
There is increasing evidence that human consciousness

38
00:01:35,800 --> 00:01:38,600
and human intelligence is actually the combination

39
00:01:38,600 --> 00:01:41,360
of several energies and several parts of physics

40
00:01:41,360 --> 00:01:43,640
that are all working together to create that.

41
00:01:43,640 --> 00:01:46,680
So I'm just like, hmm, maybe there's a lot more

42
00:01:46,680 --> 00:01:47,960
to intelligence than we thought,

43
00:01:47,960 --> 00:01:49,560
and of course there's gonna be a lot of people out there

44
00:01:49,560 --> 00:01:53,000
saying, see, I told you so, but it is what it is.

45
00:01:53,000 --> 00:01:55,960
And these are also just possibilities.

46
00:01:55,960 --> 00:01:59,480
But according to this possibility,

47
00:01:59,480 --> 00:02:01,980
it might be that there are gonna be continuing

48
00:02:01,980 --> 00:02:05,880
diminishing returns with respect to neural networks

49
00:02:05,880 --> 00:02:08,160
or even silicon-based computing

50
00:02:08,160 --> 00:02:10,920
that means that it will just be increasingly difficult

51
00:02:10,920 --> 00:02:15,720
to either reconstruct or to capture human-level intelligence.

52
00:02:15,720 --> 00:02:17,320
And another thing that's emerging to me

53
00:02:17,320 --> 00:02:21,080
is that we are going to see a very distinct bifurcation

54
00:02:21,080 --> 00:02:23,860
between human intelligence and machine intelligence,

55
00:02:23,860 --> 00:02:27,140
meaning that it's gonna be kind of like comparing apples

56
00:02:27,140 --> 00:02:29,260
to oranges, and it really already is,

57
00:02:29,260 --> 00:02:30,820
because we look at large language models

58
00:02:30,820 --> 00:02:33,500
which are very clearly processing information.

59
00:02:33,500 --> 00:02:34,820
I remember I had a conversation

60
00:02:34,820 --> 00:02:37,380
with some philosophers a year ago or so,

61
00:02:37,380 --> 00:02:41,060
and they made the somewhat asinine claim that,

62
00:02:41,060 --> 00:02:44,700
oh, they don't know anything, there's no information.

63
00:02:44,700 --> 00:02:47,160
I'm like, that's literally all that they're doing

64
00:02:47,160 --> 00:02:49,380
is just processing information,

65
00:02:49,380 --> 00:02:50,780
but it depends on definitions.

66
00:02:50,780 --> 00:02:52,440
And so to these philosophers,

67
00:02:52,440 --> 00:02:53,760
the idea that this is a machine

68
00:02:53,760 --> 00:02:55,320
that only processes information

69
00:02:55,320 --> 00:02:57,840
because their definition of information

70
00:02:57,840 --> 00:02:59,440
was stuff in human brains.

71
00:02:59,440 --> 00:03:00,560
I'm like, okay, well,

72
00:03:00,560 --> 00:03:02,860
that's just a bad definition of information.

73
00:03:02,860 --> 00:03:04,680
Anyways, going down a rabbit hole.

74
00:03:04,680 --> 00:03:06,840
My point is that it really depends

75
00:03:06,840 --> 00:03:08,160
on how you look at intelligence

76
00:03:08,160 --> 00:03:09,960
and how you define intelligence.

77
00:03:09,960 --> 00:03:12,040
And I really don't like those gotcha questions

78
00:03:12,040 --> 00:03:13,680
because it's like, how do you define intelligence?

79
00:03:13,680 --> 00:03:16,960
And it's like, I mean, it depends on who you ask.

80
00:03:16,960 --> 00:03:19,120
There's a million definitions of intelligence.

81
00:03:19,120 --> 00:03:21,120
And the fact that we don't have a good definition

82
00:03:21,120 --> 00:03:24,080
of intelligence means that also, by extension,

83
00:03:24,080 --> 00:03:25,240
we don't have a good definition

84
00:03:25,240 --> 00:03:27,120
of artificial general intelligence.

85
00:03:27,120 --> 00:03:30,080
And when you ask a mathematician what intelligence is,

86
00:03:30,080 --> 00:03:31,120
they're gonna give you one answer.

87
00:03:31,120 --> 00:03:33,440
When you ask a neuroscientist what intelligence is,

88
00:03:33,440 --> 00:03:35,000
they're gonna give you a different answer.

89
00:03:35,000 --> 00:03:36,840
If you ask a psychologist and a philosopher

90
00:03:36,840 --> 00:03:38,280
what intelligence is, again,

91
00:03:38,280 --> 00:03:41,160
they're going to give you fundamentally different answers.

92
00:03:41,160 --> 00:03:45,240
So moving on, another thing that this is good for,

93
00:03:45,240 --> 00:03:46,800
and this is gonna be really good news,

94
00:03:46,800 --> 00:03:49,700
really reassuring news to many of you out there,

95
00:03:49,700 --> 00:03:52,220
is that if AI is indeed slowing down,

96
00:03:52,220 --> 00:03:53,980
that means that the threat to jobs

97
00:03:53,980 --> 00:03:56,940
and the rate of change for jobs is going to be slower,

98
00:03:56,940 --> 00:03:59,140
which means the status quo that we have

99
00:03:59,140 --> 00:04:01,180
is going to persist a little bit longer

100
00:04:01,180 --> 00:04:03,120
than perhaps some of us would like.

101
00:04:03,120 --> 00:04:04,620
Now, what I do wanna address

102
00:04:04,620 --> 00:04:08,220
is that there's gonna be mixed reactions to this.

103
00:04:08,220 --> 00:04:11,140
So some people are like, let's just get it done,

104
00:04:11,140 --> 00:04:14,020
like replace my job, I'm ready to get out of the workforce,

105
00:04:14,020 --> 00:04:18,160
give me UBI and get me out of the workforce for good.

106
00:04:18,160 --> 00:04:19,100
I don't care.

107
00:04:19,140 --> 00:04:20,420
And other people are gonna be like,

108
00:04:20,420 --> 00:04:24,180
well, this will give us time to create new jobs,

109
00:04:24,180 --> 00:04:26,940
I don't wanna lose my job yet, and so on and so forth.

110
00:04:26,940 --> 00:04:28,220
Now, if I had to guess,

111
00:04:28,220 --> 00:04:30,580
now keep in mind that I'm speculating here,

112
00:04:30,580 --> 00:04:33,140
and that's a lot of what I do on this channel,

113
00:04:33,140 --> 00:04:36,140
my gut check now is that it's gonna be five to 10 years.

114
00:04:36,140 --> 00:04:37,380
And I've talked about this before

115
00:04:37,380 --> 00:04:39,620
where you look at the adoption curve,

116
00:04:39,620 --> 00:04:41,560
and it's like seven years.

117
00:04:41,560 --> 00:04:45,940
So maybe 2030, and 2030 seems to be a pretty sticky date.

118
00:04:45,940 --> 00:04:49,020
So anywhere between 2027 to 2030

119
00:04:49,020 --> 00:04:50,180
is when we might start seeing

120
00:04:50,180 --> 00:04:52,140
some really drastic change out there.

121
00:04:52,140 --> 00:04:53,500
Now, I could be wrong,

122
00:04:53,500 --> 00:04:56,340
we could have a confluence of multiple technologies,

123
00:04:56,340 --> 00:04:59,020
like again, I'm really waiting to see

124
00:04:59,020 --> 00:05:01,700
how GPT-5 and robotics mix,

125
00:05:01,700 --> 00:05:04,620
because you see the number of bipedal,

126
00:05:04,620 --> 00:05:07,700
like humanoid robotic chassis being built around the world.

127
00:05:07,700 --> 00:05:11,020
And like, remember, this is only gen one.

128
00:05:11,020 --> 00:05:15,460
So GPT-5 and Claude Four and whatever else,

129
00:05:15,460 --> 00:05:18,220
you combine that level of intelligence with robots,

130
00:05:18,220 --> 00:05:21,580
that really could change a lot for a lot of things.

131
00:05:21,580 --> 00:05:22,500
And I think there's,

132
00:05:22,500 --> 00:05:25,340
I don't know if it's proven out or to what extent,

133
00:05:25,340 --> 00:05:28,300
but I've heard that Tesla is already using their robots

134
00:05:28,300 --> 00:05:30,060
in the Tesla factories.

135
00:05:30,060 --> 00:05:33,580
And the economic carrot for that is really high.

136
00:05:33,580 --> 00:05:38,420
So don't underestimate the power of that economic incentive

137
00:05:38,420 --> 00:05:41,280
to get things really going.

138
00:05:41,280 --> 00:05:45,100
But overall, if the advancement of AI intelligence

139
00:05:45,100 --> 00:05:46,500
is indeed slowing down,

140
00:05:46,540 --> 00:05:48,780
it just gives us all more time to adapt

141
00:05:48,780 --> 00:05:52,060
on a cybersecurity level, on an economic level,

142
00:05:52,060 --> 00:05:53,580
on a military level.

143
00:05:53,580 --> 00:05:54,680
So it means that, you know,

144
00:05:54,680 --> 00:05:59,680
your life is not gonna get upended soon, hopefully.

145
00:06:00,460 --> 00:06:03,340
So this leads me to want to address another thing.

146
00:06:04,740 --> 00:06:07,100
About what, 12 months ago, a little bit more,

147
00:06:07,100 --> 00:06:10,700
I predicted that we would have AGI by September 2024.

148
00:06:10,700 --> 00:06:12,820
So that's just a few months from now.

149
00:06:12,820 --> 00:06:15,060
Now, what I was looking at at the time,

150
00:06:15,060 --> 00:06:17,060
and, you know, if you go back and watch my videos,

151
00:06:17,060 --> 00:06:18,580
there's a whole bunch of charts and data

152
00:06:18,580 --> 00:06:20,020
that I was looking at.

153
00:06:20,020 --> 00:06:23,540
And this is right along the curve

154
00:06:23,540 --> 00:06:26,700
of what Ray Kurzweil originally proposed,

155
00:06:26,700 --> 00:06:29,860
is to when we would have a human level, you know,

156
00:06:29,860 --> 00:06:33,040
intelligence in a single computer is actually 2023.

157
00:06:33,040 --> 00:06:34,820
So that was one piece of data.

158
00:06:34,820 --> 00:06:37,380
I was also looking at parameter counts going up,

159
00:06:37,380 --> 00:06:41,140
logarithmically, which they have been,

160
00:06:41,140 --> 00:06:42,180
but they've slowed down.

161
00:06:42,180 --> 00:06:44,300
And the one thing that I was not looking at,

162
00:06:44,300 --> 00:06:46,620
so this is the piece of data that I did not include

163
00:06:46,620 --> 00:06:48,340
in all of those calculations,

164
00:06:48,340 --> 00:06:50,860
was the exponentially rising costs

165
00:06:50,860 --> 00:06:53,660
of training subsequent generations of models.

166
00:06:53,660 --> 00:06:57,100
So, you know, as, I think it was Dimitris Abbas

167
00:06:57,100 --> 00:06:59,460
was talking about on a podcast recently,

168
00:06:59,460 --> 00:07:03,180
every subsequent generation from GPT-2 to 3 to 4

169
00:07:03,180 --> 00:07:07,540
costs 10 times as much to train, if not more.

170
00:07:07,540 --> 00:07:09,540
So while all these other things

171
00:07:09,540 --> 00:07:12,020
are going up exponentially, so is cost.

172
00:07:12,020 --> 00:07:14,780
And that did not figure into my calculus.

173
00:07:14,780 --> 00:07:17,140
And so because of that, it's like, oh, well,

174
00:07:17,140 --> 00:07:20,300
if I had thing, you know, recognize that,

175
00:07:20,300 --> 00:07:22,180
I might have said, well, we're probably gonna get

176
00:07:22,180 --> 00:07:24,500
diminishing returns sooner rather than later.

177
00:07:24,500 --> 00:07:27,500
Now I have been talking about diminishing returns

178
00:07:27,500 --> 00:07:29,340
pretty much for the life of this channel.

179
00:07:29,340 --> 00:07:32,660
And I've been wondering, when is the jig gonna be up?

180
00:07:32,660 --> 00:07:34,540
When are we gonna run out of steam here?

181
00:07:34,540 --> 00:07:37,100
And it looks like we're starting to run out of steam.

182
00:07:37,100 --> 00:07:39,700
Now again, you know, the train is still running,

183
00:07:39,700 --> 00:07:41,900
we're still burning pretty hot,

184
00:07:42,780 --> 00:07:44,260
but we're not accelerating anymore.

185
00:07:44,260 --> 00:07:48,940
We are probably on a more geometric trajectory right now

186
00:07:48,940 --> 00:07:50,180
if I had to guess.

187
00:07:51,180 --> 00:07:53,100
And it all comes down to economics.

188
00:07:53,100 --> 00:07:56,820
It really is just with exponentially rising costs

189
00:07:56,820 --> 00:08:00,420
with a, we're entering into what's called a red ocean market,

190
00:08:00,420 --> 00:08:02,780
which means it's not just, you know, a blue ocean

191
00:08:02,780 --> 00:08:07,060
out there with its just open AI with their frontier model.

192
00:08:07,060 --> 00:08:10,900
Lots of other models have caught up to GPT-4O,

193
00:08:10,940 --> 00:08:13,700
Claude 3.5 Sonnet has clearly surpassed it

194
00:08:13,700 --> 00:08:14,980
as far as I can tell.

195
00:08:14,980 --> 00:08:17,140
Obviously people like looking at a,

196
00:08:17,140 --> 00:08:19,260
whatever that benchmark is called,

197
00:08:19,260 --> 00:08:21,140
I don't really give that much weight because it's,

198
00:08:21,140 --> 00:08:23,620
that seems like it's mostly a popularity contest

199
00:08:23,620 --> 00:08:26,860
and open AI still has a lot of fanboys,

200
00:08:26,860 --> 00:08:29,640
but doing a side-by-side comparison of capability

201
00:08:29,640 --> 00:08:32,900
between chat GPT-4O and Claude 3.5,

202
00:08:32,900 --> 00:08:37,100
it is hands down Claude 3.5 is in another league

203
00:08:37,100 --> 00:08:38,140
as far as I can tell.

204
00:08:38,140 --> 00:08:39,580
Now obviously a lot of you out there

205
00:08:39,580 --> 00:08:41,060
use it for different things.

206
00:08:41,060 --> 00:08:43,420
So, you know, it is gonna,

207
00:08:43,420 --> 00:08:45,980
it's gonna depend on your use case.

208
00:08:45,980 --> 00:08:47,460
Another thing that I've noticed is that

209
00:08:47,460 --> 00:08:49,180
there's been fewer breakthroughs.

210
00:08:49,180 --> 00:08:53,260
So like, yes, chat GPT-4O has the voice mode,

211
00:08:53,260 --> 00:08:55,380
which is really, you know, okay, cool,

212
00:08:55,380 --> 00:08:57,860
like it can do a sultry voice and sound effects,

213
00:08:57,860 --> 00:08:59,100
which is great.

214
00:08:59,100 --> 00:09:03,260
But that was a predictable addition with multimodality,

215
00:09:03,260 --> 00:09:06,460
where it's like, okay, text and audio, great.

216
00:09:06,460 --> 00:09:10,580
This is still leaving a huge swath of economic interests

217
00:09:10,580 --> 00:09:13,380
and intellectual interests completely untouched.

218
00:09:14,500 --> 00:09:15,580
Take math for instance,

219
00:09:15,580 --> 00:09:17,500
they still haven't figured out math and physics

220
00:09:17,500 --> 00:09:18,880
and those sorts of things.

221
00:09:18,880 --> 00:09:23,880
And also after playing around with the ARC AGI test,

222
00:09:24,540 --> 00:09:28,060
yes, I have not been a fan of the ARC AGI test,

223
00:09:28,060 --> 00:09:30,900
but at the same time, like it does prove a point.

224
00:09:30,900 --> 00:09:33,280
It does prove a point that the kind of reasoning

225
00:09:33,280 --> 00:09:35,440
that these things do is still very different

226
00:09:35,440 --> 00:09:36,660
from human reasoning,

227
00:09:36,660 --> 00:09:38,200
which is another reason that I'm talking about

228
00:09:38,200 --> 00:09:40,200
a bifurcation of intelligence.

229
00:09:40,200 --> 00:09:41,960
That we might be, and this is again,

230
00:09:41,960 --> 00:09:43,900
as pure speculation on my point,

231
00:09:43,900 --> 00:09:46,840
we might be getting to a point where

232
00:09:46,840 --> 00:09:49,240
we're starting to recognize, okay,

233
00:09:49,240 --> 00:09:51,720
this machine is kind of an alien intelligence.

234
00:09:51,720 --> 00:09:54,320
It clearly has its own consistent way

235
00:09:54,320 --> 00:09:55,860
of approaching the world,

236
00:09:55,860 --> 00:09:57,980
but it is also very different from us.

237
00:09:57,980 --> 00:10:00,220
Now, Bill Gates was on a podcast recently saying

238
00:10:00,220 --> 00:10:02,800
that metacognition is gonna be the next step.

239
00:10:02,800 --> 00:10:04,840
Okay, sure, I mean, I've been working on cognitive

240
00:10:04,880 --> 00:10:06,400
architectures for a while,

241
00:10:06,400 --> 00:10:08,000
and there are some really sharp people out there

242
00:10:08,000 --> 00:10:10,840
that figured out how to give models metacognition

243
00:10:10,840 --> 00:10:13,120
a while ago, it's really just down to prompting.

244
00:10:14,480 --> 00:10:16,240
You can give, for instance,

245
00:10:16,240 --> 00:10:18,520
especially with these gigantic context windows,

246
00:10:18,520 --> 00:10:21,320
you can give one sec, one model say,

247
00:10:21,320 --> 00:10:23,720
hey, you're a metacognitive agent

248
00:10:23,720 --> 00:10:25,360
that's viewing these other thoughts.

249
00:10:25,360 --> 00:10:26,760
Tell us what you think about it.

250
00:10:26,760 --> 00:10:28,360
Help steer it on moral course.

251
00:10:28,360 --> 00:10:31,040
This was entirely all of my work on the ACE framework,

252
00:10:31,040 --> 00:10:33,600
the autonomous cognitive entity framework,

253
00:10:33,600 --> 00:10:36,200
which I did abandon because Microsoft Autogen

254
00:10:36,200 --> 00:10:40,200
and other platforms far surpassed what I could do

255
00:10:40,200 --> 00:10:43,440
on my own, even with help from people on the internet,

256
00:10:43,440 --> 00:10:44,400
because why it's Microsoft,

257
00:10:44,400 --> 00:10:46,600
and they have a lot more money than I do.

258
00:10:47,560 --> 00:10:49,920
Now, that leads me to another point that I wanna address,

259
00:10:49,920 --> 00:10:51,400
and that is echo chambers.

260
00:10:51,400 --> 00:10:53,720
So most of you in the audience,

261
00:10:53,720 --> 00:10:55,720
based on the polls that I've run,

262
00:10:55,720 --> 00:10:57,720
most of you in the audience, statistically speaking,

263
00:10:57,720 --> 00:10:59,120
are kind of in the middle of the bell curve

264
00:10:59,120 --> 00:11:01,280
where you're reasonable and you want the truth,

265
00:11:01,280 --> 00:11:04,040
and you want some honest, genuine thoughts.

266
00:11:04,040 --> 00:11:06,200
There are, however, many people out there

267
00:11:06,200 --> 00:11:08,440
that are on more of the tail,

268
00:11:08,440 --> 00:11:10,480
like left to right tail of the bell curve,

269
00:11:10,480 --> 00:11:14,760
where you wanna see doom or you wanna see acceleration,

270
00:11:14,760 --> 00:11:18,480
and you're not really interested in other narratives.

271
00:11:18,480 --> 00:11:20,960
And the reason that I'm using the word echo chamber,

272
00:11:20,960 --> 00:11:23,920
which is often pathologized,

273
00:11:23,920 --> 00:11:26,760
is because there have actually been a few people

274
00:11:26,760 --> 00:11:29,160
that did directly express to me

275
00:11:29,160 --> 00:11:31,360
they did not want an alternative narrative.

276
00:11:31,360 --> 00:11:35,400
They only wanted to double down on their personal narrative,

277
00:11:35,400 --> 00:11:37,440
the one that they want to be true,

278
00:11:37,440 --> 00:11:38,920
which, believe me,

279
00:11:38,920 --> 00:11:41,880
I want to have all kinds of advancements

280
00:11:41,880 --> 00:11:43,320
happening next year.

281
00:11:44,200 --> 00:11:46,320
That was not hype when I said

282
00:11:46,320 --> 00:11:48,840
that I was predicting AGI this year.

283
00:11:48,840 --> 00:11:51,240
That was a genuine prediction on my part,

284
00:11:51,240 --> 00:11:53,080
and I was like, man, things are happening,

285
00:11:53,080 --> 00:11:54,160
they're accelerating,

286
00:11:54,160 --> 00:11:55,320
but I don't believe that anymore

287
00:11:55,320 --> 00:11:56,840
because of the data that I'm seeing,

288
00:11:56,840 --> 00:11:58,800
because of the trends that I'm seeing.

289
00:11:58,800 --> 00:11:59,960
And I know that that sucks,

290
00:11:59,960 --> 00:12:04,960
like if someone is banking on a certain outcome,

291
00:12:05,280 --> 00:12:06,920
like expectations and reality,

292
00:12:06,920 --> 00:12:09,480
there's always a gap between expectations and reality,

293
00:12:09,480 --> 00:12:12,800
and when that gap gets bigger, it sucks.

294
00:12:12,800 --> 00:12:15,200
Now, some people are gonna take this news

295
00:12:15,200 --> 00:12:18,360
and interpret it in completely unexpected ways to me,

296
00:12:18,360 --> 00:12:19,600
and that's fine.

297
00:12:19,600 --> 00:12:22,840
But what I do wanna caution is for the five

298
00:12:22,840 --> 00:12:25,240
or less percent of you out there in the audience

299
00:12:25,240 --> 00:12:27,480
that are on the tails of the bell curve

300
00:12:27,480 --> 00:12:30,320
in terms of expectations and your disposition,

301
00:12:30,320 --> 00:12:32,440
your valence towards this,

302
00:12:32,440 --> 00:12:37,440
is if you broaden your narratives just a little bit,

303
00:12:38,280 --> 00:12:39,880
then you might be surprised

304
00:12:39,880 --> 00:12:42,160
at some of the other advantages that are happening,

305
00:12:42,160 --> 00:12:44,480
and also just realizing that there is a silver lining,

306
00:12:44,480 --> 00:12:47,440
is that the disruption that is coming

307
00:12:47,440 --> 00:12:48,880
is gonna take a little bit longer,

308
00:12:48,880 --> 00:12:51,760
which means that society will be a little bit more stable,

309
00:12:51,760 --> 00:12:54,960
which means that the risk of catastrophic outcomes

310
00:12:54,960 --> 00:12:58,160
or unintended outcomes goes down significantly.

311
00:12:58,160 --> 00:13:00,600
And on the topic of those narratives

312
00:13:00,600 --> 00:13:02,280
and those echo chambers,

313
00:13:02,280 --> 00:13:05,160
a lot of people have asked me to comment on Gary Marcus,

314
00:13:05,160 --> 00:13:06,720
and I've resisted until now,

315
00:13:07,600 --> 00:13:10,560
but having gotten back on Twitter,

316
00:13:10,560 --> 00:13:13,240
I will say that I've watched some really interesting

317
00:13:13,240 --> 00:13:15,360
and highly vitriolic debates

318
00:13:15,360 --> 00:13:20,360
between namely Gary Marcus, Yasha Bach, and Jan Lacoon.

319
00:13:21,120 --> 00:13:23,200
Now, these are supposed to be the adults in the room,

320
00:13:23,200 --> 00:13:26,040
and having watched Yasha on some interviews,

321
00:13:26,040 --> 00:13:28,320
like he's a very sharp guy,

322
00:13:28,320 --> 00:13:31,200
but even he got into the like, let's just beat up on,

323
00:13:31,200 --> 00:13:33,200
let's like, what is the term that kids use these days,

324
00:13:33,200 --> 00:13:36,040
like let's clown on Gary Marcus train,

325
00:13:36,040 --> 00:13:37,800
and that was honestly really disappointing

326
00:13:37,800 --> 00:13:39,360
because this is someone who's supposed to be like

327
00:13:39,360 --> 00:13:42,040
a high brow like academic researcher,

328
00:13:42,040 --> 00:13:44,920
and he's sharing like caricature memes of Gary,

329
00:13:44,920 --> 00:13:47,040
which, I mean, I would never do that.

330
00:13:47,040 --> 00:13:49,080
I don't particularly agree with Gary anymore,

331
00:13:49,080 --> 00:13:51,240
but that was incredibly immature.

332
00:13:51,240 --> 00:13:53,240
And then Jan Lacoon has often had this like,

333
00:13:53,240 --> 00:13:55,800
old man yells at cloud energy,

334
00:13:55,800 --> 00:13:58,040
which is weird because it's like,

335
00:13:58,040 --> 00:13:59,720
half of what he says I agree with,

336
00:13:59,720 --> 00:14:00,880
like Ferventland, the other half,

337
00:14:00,880 --> 00:14:02,880
I'm like, where did that come from?

338
00:14:02,880 --> 00:14:05,320
So, all right, so what happens?

339
00:14:05,320 --> 00:14:08,480
And this is not, to be fair, taking a step back,

340
00:14:08,480 --> 00:14:11,600
this is not a unique phenomenon in AI.

341
00:14:11,600 --> 00:14:14,360
Some of, one of my good friends as a physicist,

342
00:14:14,360 --> 00:14:16,640
this kind of thing happens in the physics community

343
00:14:16,640 --> 00:14:18,200
all the time, apparently,

344
00:14:18,200 --> 00:14:21,040
where it's like disagreements and arguments

345
00:14:21,040 --> 00:14:23,480
over interpretations will actually like,

346
00:14:23,480 --> 00:14:26,160
come to shouting matches and sometimes fist fights.

347
00:14:26,160 --> 00:14:29,720
Physicists are actually pretty hardcore, it turns out.

348
00:14:29,720 --> 00:14:34,720
So, from my reading of, you know, human nature,

349
00:14:34,920 --> 00:14:37,400
what I, the way that I interpret this is that

350
00:14:37,400 --> 00:14:40,040
we have a tightening status game.

351
00:14:40,040 --> 00:14:43,360
So, Gary, Yasha, Yan, all of these people,

352
00:14:43,360 --> 00:14:45,720
they suddenly saw themselves having much,

353
00:14:45,720 --> 00:14:49,720
much higher social status because of artificial intelligence.

354
00:14:49,720 --> 00:14:51,720
And so, one way to compare this is,

355
00:14:51,720 --> 00:14:54,360
imagine you're back in high school

356
00:14:54,360 --> 00:14:56,520
and something changes and suddenly,

357
00:14:56,520 --> 00:14:59,360
the nerds are all the most popular kids in school.

358
00:14:59,360 --> 00:15:01,560
Well, then something changes again,

359
00:15:01,560 --> 00:15:03,040
and it's like, oh, well, actually,

360
00:15:03,040 --> 00:15:05,960
instead of the top eight nerds, now it's the top five.

361
00:15:05,960 --> 00:15:08,600
And so, three have to get kicked off the island.

362
00:15:08,600 --> 00:15:09,800
That's what's happening.

363
00:15:09,800 --> 00:15:13,440
And so, they're scrabbling over diminishing social status

364
00:15:13,440 --> 00:15:16,880
because, again, with AI slowing down,

365
00:15:16,920 --> 00:15:20,040
it's no longer as hot and sexy as it was a year ago.

366
00:15:20,040 --> 00:15:22,000
It's no longer, you know, you can't just say,

367
00:15:22,000 --> 00:15:23,160
hey, I was gonna kill everyone

368
00:15:23,160 --> 00:15:26,320
and get an invitation to the TED stage anymore.

369
00:15:26,320 --> 00:15:27,760
And so, because of that,

370
00:15:27,760 --> 00:15:29,640
because the status game is narrowing,

371
00:15:29,640 --> 00:15:30,680
because the number of people

372
00:15:30,680 --> 00:15:32,760
that can be high status is going down,

373
00:15:32,760 --> 00:15:34,680
the rules are becoming more arbitrary

374
00:15:34,680 --> 00:15:36,880
and people are becoming a little bit more snippy,

375
00:15:36,880 --> 00:15:40,080
a little bit more vitriolic, as I said.

376
00:15:40,080 --> 00:15:42,760
The stakes go up because the risk of losing status,

377
00:15:42,760 --> 00:15:44,760
especially, this is what we saw with Ilya.

378
00:15:44,760 --> 00:15:46,080
I talked about this extensively.

379
00:15:46,080 --> 00:15:49,480
The reason that Ilya was socially canceled with an open AI

380
00:15:49,480 --> 00:15:52,440
is because he made the cardinal sin

381
00:15:52,440 --> 00:15:53,960
of attacking Sam Altman,

382
00:15:53,960 --> 00:15:56,760
even though he was doing it for what he believed

383
00:15:56,760 --> 00:15:58,880
was the right reasons,

384
00:15:58,880 --> 00:16:01,120
that was a violation of the social norm,

385
00:16:01,120 --> 00:16:03,300
which is Sam Altman is king.

386
00:16:03,300 --> 00:16:06,000
And of course, Sam Altman, as a power seeking person,

387
00:16:06,000 --> 00:16:08,440
was not going to tolerate that.

388
00:16:08,440 --> 00:16:10,040
Consciously or unconsciously,

389
00:16:10,040 --> 00:16:11,360
that was just never going to,

390
00:16:11,360 --> 00:16:12,920
he was never going to tolerate it.

391
00:16:12,920 --> 00:16:14,880
So what happens is,

392
00:16:14,880 --> 00:16:16,680
other AI commentators out there,

393
00:16:16,680 --> 00:16:18,960
namely Gary, Yasha, and Yan,

394
00:16:18,960 --> 00:16:20,520
are doubling down on their narratives,

395
00:16:20,520 --> 00:16:23,480
because basically they're gonna be doubling down

396
00:16:23,480 --> 00:16:25,480
on the narratives that got them that social status

397
00:16:25,480 --> 00:16:27,180
in the first place.

398
00:16:27,180 --> 00:16:30,320
And that is my read on the situation.

399
00:16:30,320 --> 00:16:32,440
And also, I take that as evidence

400
00:16:32,440 --> 00:16:34,240
that AI is slowing down,

401
00:16:34,240 --> 00:16:37,360
because again, if AI is running out of steam,

402
00:16:37,360 --> 00:16:39,760
then the amount of space

403
00:16:39,800 --> 00:16:43,120
that the public square needs of AI commentators

404
00:16:43,120 --> 00:16:44,080
is going down.

405
00:16:44,080 --> 00:16:45,360
It's also been a year and a half

406
00:16:45,360 --> 00:16:47,440
since Gary Marcus was in front of the Senate.

407
00:16:47,440 --> 00:16:50,400
And have you seen him or heard him any other place?

408
00:16:50,400 --> 00:16:54,000
No, like his 15 minutes of fame might be over,

409
00:16:54,000 --> 00:16:56,880
and that sucks, like that doesn't feel good.

410
00:16:56,880 --> 00:17:00,320
It does not feel good to feel like you're being left behind

411
00:17:00,320 --> 00:17:02,480
by the conversation or by society,

412
00:17:02,480 --> 00:17:05,600
which is to me, an explanation as to why Gary

413
00:17:05,600 --> 00:17:09,120
has been so incredibly salty lately.

414
00:17:09,160 --> 00:17:11,440
And then of course, other people

415
00:17:11,440 --> 00:17:13,800
that are not as aware of these status games

416
00:17:13,800 --> 00:17:15,140
will jump in on bullying,

417
00:17:15,140 --> 00:17:18,840
because if you show weakness in a high-stakes status game,

418
00:17:18,840 --> 00:17:20,800
people will unconsciously,

419
00:17:20,800 --> 00:17:23,760
it's tall poppy syndrome and a number of other phenomenon,

420
00:17:23,760 --> 00:17:25,760
people will unconsciously jump in on that

421
00:17:25,760 --> 00:17:27,800
and say, ah, time to bully that person,

422
00:17:27,800 --> 00:17:31,640
because they're signaling that their status is vulnerable.

423
00:17:31,640 --> 00:17:33,800
So that's my read on the whole situation.

424
00:17:33,800 --> 00:17:35,920
And yeah, it's not ideal, it's not what I hoped,

425
00:17:35,920 --> 00:17:37,320
it's not what I predicted,

426
00:17:37,320 --> 00:17:41,680
but I ignored the numbers, I ignored the money, right?

427
00:17:41,680 --> 00:17:45,880
Like, and hindsight, that was pretty dumb.

428
00:17:45,880 --> 00:17:49,000
So am I still predicting September 2024?

429
00:17:49,000 --> 00:17:52,360
Again, this is where I'm gonna double down on my narrative.

430
00:17:52,360 --> 00:17:54,280
I think that GPT-5 plus robots

431
00:17:54,280 --> 00:17:57,140
will surprise a lot of people with what it's capable of.

432
00:17:57,140 --> 00:17:58,880
Is it gonna replace all of us?

433
00:17:58,880 --> 00:18:01,240
No, it's gonna be like the Nestor class four

434
00:18:01,240 --> 00:18:03,080
from iRobot, where it's like,

435
00:18:03,080 --> 00:18:06,320
it's capable of running your mail for you automatically,

436
00:18:06,320 --> 00:18:08,200
but not much else.

437
00:18:08,200 --> 00:18:09,260
That's kind of what I predict.

438
00:18:09,260 --> 00:18:10,960
So it's like, you know, you can get rid of like,

439
00:18:10,960 --> 00:18:14,240
maybe some warehouse workers, some factory workers,

440
00:18:14,240 --> 00:18:16,820
some mail carriers, but it's not gonna like,

441
00:18:16,820 --> 00:18:18,080
upend the whole economy.

442
00:18:18,080 --> 00:18:19,680
So, all right.

443
00:18:19,680 --> 00:18:22,880
This has been your first episode of David Shapiro,

444
00:18:22,880 --> 00:18:25,600
your personal chief AI officer.

445
00:18:25,600 --> 00:18:27,560
Let me know how you think this went in the comments

446
00:18:27,560 --> 00:18:28,560
and I'll see you next time.

447
00:18:28,560 --> 00:18:29,400
Cheers.

