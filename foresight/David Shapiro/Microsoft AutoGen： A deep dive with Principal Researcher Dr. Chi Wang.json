{"text": " Chi, thanks for jumping on. It's a pleasure to meet you. I was really excited. Yeah, you're quite welcome. Obviously, Autogen is all the rage right now. It's very popular. There's lots and lots of videos being made about Autogen. But before we dive into that, I was wondering if you could just tell me a little bit more about your time at Microsoft as a principal researcher, like how did you get into that position? What's it been like? Yes, just give me the story. Thank you. Yeah, my name is Chi. Principal researcher at Microsoft Research. I joined long time ago. I joined about nine years ago in Microsoft. And I've been working on many different projects. Apparently now I'm focusing on Autogen. And before that, I worked on automated machine learning, machine learning for systems, data science, data analytics, data mining. So quite a lot of different things. And some of the work is more like on a theoretical set side and some of them is more to the system side. And yeah, I've been focusing on Autogen recently. So very happy to be here. Excellent. Yeah, with all the progress that's been made on large language models and working on assistance and agents, and then of course, working on agents, working with other agents, let me ask kind of a big question. Like what was the genesis behind Autogen? How was that project proposed? Like how did it come together? Like what was the theoretical work behind it? And how did you get from zero to where you are today? Yeah, so that's a very interesting question. And to fully answer that question, I need to tell a long story. But let me first tell you a short one. Short answer is like with this big kind of opportunities with larger models, so powerful techniques. At MSR, we want to ask the question, like what is the future, right? We want to be forward-looking, we want to be futuristic, kind of take the solid leadership. And because one of the famous quotes from the founder of Macro Research, we can say that the best science will be indistinguishable from magic. So that's the level of ambition we have. So we asked the question, what is the future of AI applications? And how do we empower every developer to build them? Yeah, so that's a fundamental driving question. And now a longer version of the answer is that, I started from, I thought as a tutorial, before I worked on Autogen, I worked on automated machine learning, which is another open source project called FLAML. FLAML is a solution for automating model selection, hyperparameter tuning, essentially, black box optimization to navigate a larger space without knowing the gradient. So it's a very powerful technique, but that was started before the larger model takes the storm. And when chatGVT released, right? And that's a big, big kind of upgrade of the model capabilities. And so I started working on a similar problem as automated machine learning. I started working on inference hyperparameter tuning for these chatGVT models. For example, how do you select the best model? How do you select the right prompt temperature and all the other inference parameters so that you can maximize the utility from the models while minimizing your cost? So that's the initial work on this direction. And when GT4 is released, that's another big upgrade of the level capabilities. So then I started really to ask the question, okay, if we kind of want to bring the best power of the model and really solve really difficult problems, what should be the right way to do it? And agent is apparently a very powerful notion. It's another kind of level of the automation as opposed to the previous automation in the 20 machine learning work I did. So yeah, so that's where they started. And of course, it's a whole new area. No, the shop agent is not new. It's has been there for a long time. And I remember back in college, I worked on some like game competition, like building agents that can play games with each other and compete. At that time, we were using many of the rule-based methods where a lot of deal with a lot of corner cases and so on. So it's not viable. It's a good notion, but not viable. And these larger models, especially the chat-operated models, really make it viable and a reality that we can build new software based on. And to study this new area, we kind of need to think many things from scratch and try to take some of the first principles and what is the right approach. And basically leverage every lesson we learned from the previous projects, previous experience, but try to build this one multi-agent framework that is really generic, and can support diverse applications. Yeah, so there are many different examples of sources of inspirations I can give you, but why not show it's like using everything I learned and also take every feedback I received from everyone and iterate on that. And so that's how we come here today. Excellent. Yeah, no, I mean, there's a lot to unpack there. It's fascinating to me that it started as sort of like auto ML, like automating the optimization. And I can see how going to like optimizing prompts and optimizing hyperparameters and parameter tuning could then lead to the agents, especially, like you said, if the idea is thinking to the future, like what is the sci-fi version of enabling application development in the future? So I wanted to follow up with a kind of a two-part question. So in the most basic level, what is AutoGen? But more specifically, what is the vision? Like what is it that you're trying to solve with AutoGen right now? Yeah, so yeah, in one word, AutoGen is the multi-agent AI framework, and especially focusing on multi-agent conversations so that we can connect large-level models, tools, and human inputs together to solve complex asks. There can be multiple ways to understand this. So in one way is to understand it as a programming framework for developers to build applications easily with some simple and unified abstraction so that they don't need to worry too much about the lower details, but can focus on how to define agents, how to get them to work with each other, and eventually reach the goal. It can also be understood as a tool to kind of scale up, scale up the power of our models and makes them even more useful by connecting with other tools, non-narrative model tools, or human collaborators, and kind of scale up both the complexity of the problem they can solve, the degree of automation, to some extent. Yeah, this is kind of a relatively abstract instruction, but if we think about it, about how people use it, it's quite simple. So when developers build applications with AutoGen, it basically boils down to two steps. Step one is to define agents, and step two is to get them to talk. So as simple as that. Yeah, so we try to make it very useful and generic, but on the other hand, we want to have a very simple interface for people to use. Yeah, I mean, that's exactly the vision that I kind of settled on for my hierarchical autonomous agent swarm idea, but I don't want to make it about that. Right now, it's just fascinating that we kind of converge on a very similar principle, like let's make the deployment of software as easy as possible. And so there's two things that you mentioned, like layers of abstraction, because I think that's a really good intuitive way of thinking about it, like in the same way that a Python interpreter was a layer of abstraction from compiled code, and then maybe language models are another layer of abstraction where it's natural language interface. This could be seen as, again, another layer of abstraction, where instead of looking at interacting with the language model directly, it is now a type of interpreter, but this is the agents and the multi-agent framework on top of it. So that's my intuition. Do you agree with that or disagree? Or like, how do you think of those layers of abstraction? Yeah, that's a fantastic question. The abstraction is indeed at a higher level of the agent abstraction. It unifies a number of different things. One is larger models. So when we use a single instance of a larger model, we usually do prompt engineering and try to give some input text and get some output text out of it. This agent abstraction can encapsulate that underneath and provide a more intuitive way to think of it as an agent that can converse with you. So not just as one single text completion inference anymore. It can do tasks, can persist some states and continue to take your feedback and produce more refined result and so on. So that's a larger model-based agent. There are two other kinds of backhands that can be encapsulated. One is, you can think of it as programming language or tool-based agent, which doesn't use a larger model, but they can still perform very useful actions. They can do code execution, for example, or it can execute predefined functions or it can basically execute any programming logic you've defined there. And third one is the human kind of backed agents. So these agents can be considered as some kind of user proxy. So when they need human input, the human can take over and just participate the multi-agent workflow as one of the agents. So you can see about several agents. Some of them are larger model-based, some are tool-based, others are human-based. So then they can just cooperate together through a very natural interface, which is a conversational interface. So that's kind of layer of abstraction we provide. Yeah, I appreciate that. And I'm reminded of during, I think it was the Ignite keynote speech Satya Nadella said, think of it as a reasoning engine and a natural language interface. And that was like the two simplest ways to think of generative AI, at least the language side. So the other topic that I wanted to ask about to kind of dig into was thinking about it as a kind of automation. So there's the agent-based, there's the tools-based, but then overall kind of, and this is a messaging kind of a framing that I've adopted when talking to people is, and of course it's an oversimplification, but AI is, one, AI is not new, like machine learning has been around for a while, this is just a step function in terms of capabilities that models have. But it's also just the simplest way to think about it from a production standpoint or from a software standpoint, is it's a new suite of automation tools, right? That's kind of how I think of it. Is that a fair characterization or is there something that I'm missing from that? Cause I do feel like there might be something missing or something that that characterization doesn't fully convey, but it is still fundamentally new automation, right? So if I try to understand it from an automation point of view, I think agent is fundamentally a automation concept. The automation is more like about, instead of giving every detailed instructions using code, I say step one, do this, step two, do that, using from precisely defined program language specification. And now we can make some more vague specification using like natural language specification, say I want to accomplish such a task and could the agent do it for me? So, and then underneath the agent needs to kind of break it down a big complex task into maybe smaller, solvable tasks. And until each task can be conveniently solved by a simple inference and produce a corresponding code to solve that task. And then eventually we need to recompose them, all of these intermediate steps and get to the final output back. So that is one part of the automation story. And another part of it is, think about this automating some tasks that human had to do. AutoGen really started with some very simple kind of automation. Just think about how you use chatGBT. You as a human need to ask questions and chatGBT gave you some answer. Sometimes it gave you the code and then human need to take that code and run it by yourself and get some result. If it's not correct, you send it back and chatGBT gave you some results for the game. And here in this kind of interaction, human students do a lot of work, but many of the work can be automated if we use agent. And even some kind of human feedback, like non-code, if I don't like the results, but I know my preferences, I will tell it, such as change the chart from using a dollar to percentage that kind of requirement, or teach me this lesson, teach me about math, but using a concrete money example, right? So those kind of requirements can be somewhat automated using all different ways. Sometimes we can use larger models. Sometimes we can use some retrieval augmented approach to inform retrieval to get the knowledge from somewhere. That's another kind of interesting automation that we can make. Yeah, and within that, those automation stories, because some of the examples, what was it, how did you say like a vague specification, right, a natural language specification that is not quite as rigorous as software development might have required in the past? And then of course, with these models, they have the ability to kind of think through it, or break it down into steps. So with all that, and some of the work that I have found is, or some of the problems that I've confronted, because it can think in general principles, it does know a lot about software development, and I mean, the language models know a lot about a lot of things. So with respect to autogen, and kind of getting to where you're at today, what are some of the biggest challenges that you've overcome so far, or that you haven't overcome? Maybe that would be a more interesting story. Yeah, sure, I could talk about both. For what we have overcome, I think we have kind of figured out the abstraction to the earlier about how to unify these different types of capabilities, different ways of making them work together. We'll have found one very simple interface that kind of accommodate a variety of different communication patterns. So one example is the, so there are several examples. One is the simple like one-to-one conversation. Another is hierarchical chat, like suppose one agent is more sit on top and talk to several sub-agents, it manages, and they can be nested structure, hierarchical structure. And another example is multiple one-on-one joint chat. So there's no one that is strictly sit on top. Everyone talks to the other else, but it's multiple one-on-one which are connected. So I talk to you, you talk to Katie, you can talk to me in this kind of triangle or multiple joint chat. And there's also a group chat, meaning it's not a one-on-one trend anymore. So everybody send messages to everyone else. So we see each other's message and there's a hidden group chat manager which does this kind of work. So architectural wise, it's still like one-on-one chat, but on the surface we can create an experience that simulates the group chat. And they can nest it the chat in some way, like for example, we can start with one-on-one conversation now, but at some time I decide to consult Katie. So I will hold on my current conversation and have some conversation with her. And then after I finish the conversation with her, I get back and continue the conversation with you. So that is a kind of nested chat. I believe that essentially you have the building blocks right now, right? These are some very common building blocks and we compose them together in different ways. We can build really complex workflows in general. So any arbitrarily complex communication patterns can be essentially built up within these building blocks. But I think that is what we have achieved. And we have also many examples for different applications of using these different types of patterns. This is another second thing we figured out. And the third thing I think is the ability to take human input and human control in a very natural way. And I thought earlier it's like every agent can be configured to enable the human input or disable that, depending on what you need. And also you can decide the type of environment from human. You can take over every time or you can only selectively chime in at a certain time. So that's a very useful feature because when you develop this automation, initially you don't know which step is easy to automate and which step is necessary for human to get in. So you can start from the more human loop way. And when you figure out that one step is you can confidently automate, then you can gradually reduce your human integration. So this convenience is very useful for doing all the experiments and figure out the right way. And make sure or still make sure human has a control when they need to. This is the third thing I think. So one more thing is the modularity and the reusability of the agents. That is a very important design part of it. So make sure that if you develop one useful agent in a different application, you could either directly reuse or start to modify it or extend from different ways. And make sure the barrier to hard work is not lost. I think that's also very important, seeing when we work together to build more and more complex applications. These are a number of things I think we're kind of figure out. And there are indeed a lot of challenges we haven't. And yeah, shall we get to that part? Yeah, no, I just want to reflect on some of the processes that you outlined, like removing humans from the loop step by step. Back in my time as an automation engineer, that's exactly what I would do is like, OK, I can write a script that does one part easily. Cool. Now, what's the next part? And then where do I have to jump in? And some of the other problems that you solved, like knowing. So this I think is really important, because some of the members of my team and my projects have found the same thing, is that knowing when to be quiet, knowing when not to jump in is, in many respects, more important because you don't want to end up with too much noise or wasted tokens. So that's really fascinating. Before we talk about problems you haven't overcome, can you talk about some insights from that, like that inhibition signal or keeping the noise lower? What were the key insights there? Like how did you test that and figure it out? And do you have any general principles for anyone else building agents? Yeah, this is a very good question. This is also related to another question about when do you add agents to provide the feedback and when that is not helpful, right? Because I assume the noise you're talking about is when you add more agents to service for democratics or agents that try to refine what the other agent is doing, right? They serve as a channel to provide feedback, but sometimes not feedback can be misleading and actually prevent the original agent doing the right thing. Yeah, we do observe that. And also, it's not like the more agents, the better. It's not necessarily that. For example, if you use GPD4 as the back end for an assistant agent, for a large number of problems, you'll need a simple two-agent workflow. One assistant agent, another user proxy agent. Yeah, probably I need to explain what the user proxy agent is. Basically, it refers to what I meant earlier about automating some of the work that human does. For example, using tools to execute Python code or run some predefined functions. So if you use one GPD assistant agent to suggest a solution such as code or function and use another user proxy to execute them and just provide feedback back and forth, you can solve a large number of problems very well. And some of them are also complicated and can involve multiple steps. But if you use GPD3.5 turbo, then it's much less to work in this way. So adding more agents will be much more helpful. And even for GPD4, when the problem complexity goes above a certain level, it stops to follow the main instructions. But because of the trick for one single agent to work, it actually puts a lot of careful instructions in the system message and make it know how to deal with some complex situations. But we noted that if you put too many of them, even for GPD4, and for complex tasks, start to forget these instructions and not do things as you want. Otherwise, you can just give it a simple instruction to say, try your best to solve the hardest problem and then it will be done. We're not there yet. I mean, in the future, we may. This makes me want to bring up one interesting kind of law. We, a few of us, came up called Kabuchi's law. The law has some similarity with, it's an analogy of the Conway's law in software engineering. I'm not sure if you familiar with that notion. No. So Conway's law basically saying the complexity of the software or the architecture of the software is a reflection of the organization that makes the software, that builds the software. OK. Makes sense. Yeah. So our Kabuchi's law says the model complexity will affect the model capacity or capability. We change the topology of the ideal multi-agent solution. It's a summarization of what I mentioned earlier. If you use a more powerful model, then likely you can use simpler topology of multi-agents to solve a common task and vice versa. And also, I think we need more and more research to understand this better as it's not soft. I'm seeing people trying all different kind of topologies or communication patterns for different applications. They're very creative. And what we had to figure out is what is the best topology and for a particular model and for a particular application in a kind of a very clear way to answer that question. We were not here. And this is one of actually a big challenge or a big important problem we want to solve. Yeah, next. Yeah. No, I mean, well, first, thanks for sharing some of those critical insights. And so I guess the general principle is the smarter the underpinning model, the simpler the topology can be because the more complex the instructions can be. And the more complex the tasks that an individual agent can carry out. Saying it out loud, it seems kind of obvious, but that's a good rule to generalize. So yeah, I guess let's pivot into what are some of the remaining problems? What are your biggest challenges that you either are working on or are going to be down the road? You mentioned topologies, like figuring out what is the correct topology. And of course, I can imagine that it's a moving target because as the underlying models change, almost on a monthly basis, you get new and different capabilities that kind of maybe send you back to the drawing board sometimes. Yeah, this is why having a framework that is versatile and that is flexible to do the experiments is so crucial to kind of do the fast adaptation as model moves as prominent techniques advances and as more and more small model specialized models are available, they will probably also change a lot about what was the best way to build the applications. Yeah, so this is, I think, the big value of autogen. And for unsolved research questions, there are some concrete ones I can give you a few examples. One is about this decomposition problem. As we mentioned earlier, we want to be able to achieve a state where the human can only need to specify rapidly big ambitious goal and underneath, we want the agent to be able to decompose that into solvable problems, probably multiple layers, and eventually recompose it and solve each of them and recompose it. And during this process, there are situations where the human need to provide the correct specifications because the initial one can be ambiguous. And we want the human to only provide the necessary and make a minimal kind of necessary qualifications and instructions and that agent to figure out the rest of them. That is a big challenge because if we want to solve more complex problems, we have to have a principal way to do this. And the second question also ready to do this is as we solve bigger and bigger problems, how do we do proper validation of the intermediate results? Because we don't do that. If possible, the agent will stick to some wrong intermediate results and just keep doing, keep wasting their work. And at certain time, if we need to provide validation or use agent to do self-validation, that's hard. But I do a way to do it. So we need probably into some formal language or formal way to do this proper validation so that the automation can indeed happen in a way that human desires. Yeah, so these are some just two kind of concrete pieces like problems. Yeah, in my project, we almost started in the reverse where we started with oversight of steering and oversight and supervision. So I'm curious, what's your perception or research or findings with respect to? Because you already mentioned having an assistant agent and then also having kind of a top-down hierarchical agent where you've got subordinates. What do you think about my intuition that working towards having supervisors steering QA quality assurance agents throughout the network of agents that are capable of providing some of that feedback that you mentioned earlier, is that kind of the direction that you're going? Or have you tried that and it didn't work? Or what are your thoughts in terms of having some of those specialized roles or personas as a way to help along? Yeah, there are some examples that work pretty well. I can show some of them. One is a three-agent setup to solve a multi-agent coding scenario. The application is for a supply chain optimization. It's done by another MSR team. But that solution, in my view, is quite generic. It's not restricted to that particular application. And the setup is like, it's a hierarchical setup. There's the commander on top. There's a writer who is responsible in writing Python code. The agent can also have access to some proper tools, like organization tools. And the other subagent is actually Safeguard. Safeguard is in charge of reviewing code safety. So the way it works is that the commander receives some user's question. It will first ask the writer to write the code. And after receiving the code, it will ask the Safeguard to review the code for safety. And only if the safety criteria is met, it will round the code and send the result back. Otherwise, it will just ask the writer to rewrite the code. And this can go back and forth because there can be errors. So when you debug, the writer can do that. Until the result is correct, the writer comes back with a final natural language answer to some result. And the current return that to user. So this is almost a quite simple multi-agent setup, but very effective in our application, almost 100% correct every time. One kind of lesson is if we merge the capability of the writer and the Safeguard into one agent, it doesn't work that well, especially in the code safety part. So we have the experiments in our paper. We found that if you merge them, then the accuracy for detecting code safety issue will drop significantly, both for GP4 and GP3.5 Turbo, but more significantly for GP3.5. So this kind of hints that one agent, if you ask to both suggest a solution and check the solution suggested by itself, have a bias. But we separate them and also prevent them to talk to each other, kind of make them work in an adversarial setting. It does it better. So that is one observation. But we also have other kind of scenarios where we do involve every agent in one group chat. So everyone also sees other's message and can reply back. It also works sometimes for other tasks. For example, a critique to suggest a visualization criteria for a visualization task. You can have one agent write code and another to criticize. It sort of works. But my intuition is still that if we put every agent work together always in a group chat, it may not always work because they may have the tendency to agree with each other and try hard to challenge. I would say it's case by case for different applications. There's also some benefit of doing it in this group chat because it's relatively simple. You don't need to do very hard about handling the message separation. You can simply define your agents and put them in a group chat and get them running quickly. So that's one benefit of group chat and seeing many people are using that approach. But just to be careful that it may not always work because of the limitations of the models. So that's really fascinating to me. And my intuition was the same. But it's interesting to have that validation from another perspective. So it's almost like even though the underlying model is GPT-4 running all of the agents or 3.5 turbo, there's still a positive effect from using division of labor, which the division of labor comes from the history of human work. And so just taking a moment, obviously these models do not work like human brains. But when you have an agent with a very specific task and mission and set of success criteria, that effectiveness of the division of labor still helps, even though it's just activating the latent capabilities within the same underpinning model. And then another intuition or a principle that I want to reiterate is the idea that, in some cases, group work makes sense, but in some cases, it doesn't. It's almost like the same difference in humans where the power of introverts, doing solo work on your own versus doing collaborative group work. So again, not saying that they're operating like humans, but it's really interesting to see some of these parallels emerge between multi-agent work and the nature of human labor. So yeah, very fascinating. And it's interesting because in some of the conversations that I've had and some of the observations that I've made, it's almost like what we're doing with these agents, these groups of agents, is recreating a corporation. You might have a CEO or a boss or a supervisor, and then you have the coder and then the QA. So it's a very similar structure. So do you have any other major insights or lessons that you think are either recently or super valuable that you want to share with other researchers or that you would recommend? Sure, sure. There are so many of them. I can give you a few examples. One thing is the chat, the conversation perspective. I mentioned earlier that chat GPD is a big inspiration. Certainly for many people. But for me, there's a personal story about what specific user I felt from it. It's a reminder of something I learned back in my college from a professor who told me that conversation is a provable way of making a good progress of learning. I don't remember the exact quote of that, but it's roughly that. So basically, he's trying to say, conversation is a very powerful form of either learning or making progress, or et cetera, that many people didn't realize it's how important it is. And there are some theoretical roots there. So that's one reason I'm so kind of so sure, or so I have so much belief in using conversation as the central medium of the command multi-agent interface. Again, I know there's a science, although I didn't have time to find out which reference it was. But I know that, so it gave me the confidence or the belief that this is the right thing to do. I think one of my favorite courses, Jimmy actually found me some reference from a social scientist. He mentioned something similar to that. Yeah, so this is one lesson, one kind of unique thing that I don't think many people have really. They kind of understand chatGVT is very powerful, and also get a lot of useful experience from that. But maybe this science part of it is less known. So that's one thing I would share. Another inspiration source, as I told you, so auto-gen is really inspired by many different things, many projects I've worked on before, and all the lessons I've learned. So another one, for example, is the operating system. So this is also not so obvious. When we talk about AI, why do we talk about operating systems? I think the several things, several inspiration I take from the success of operating systems. One is the idea of maximizing the utility of the most valuable resource you have. So in old days, it's like the CPU, the GPU. But I think in the new era of AI, these powerful, not even models is so valuable resource and building an operating system around them, right? And maximizing their utility, but to give them the necessary peripherals and do the right coordination. And it's super critical from the system point of view. And also, so that operating system is really you can build a platform that can support many diverse applications on top of that. So we need to design a very generic robust kind of system to do that, right? So these are all the design principles we try to use when we design AutoGen. And similarly, the idea of object-oriented programming is very useful. So many developers have very interesting ideas they want to try and develop. And now with this framework that hides a lot of complexity inside the framework, they're able to kind of do the things they want more easily. That's a part of abstraction. I already mentioned the agent, notion, automation, inspiration. The one thing I want to mention is open source, right? The concept of open source is that it can solve the common problems that community needs and make it really easy to use. So those are probably most modern kind of things that can get good open source adoption and build something that the community loves, right? Yeah, so I think that is my valuable lesson I want to share with all researchers, right? If you want to get their research adopted and get more and more impact and influence and through this open source channel, then spend a lot of effort about usability and solving the common problem that many people want to solve is what's going to be considered. I have personally found success in giving away as much valuable information and ideas as I can. That's what my whole YouTube career and computer science career is based on now. So thank you for sharing those critical insights. So on the topic of operating systems, because I'm really glad you brought that up, because I started thinking about language models as a component, like a new component of an operating system. So I'm glad to know that there's some convergence there. Is that kind of the future of Autogen? Is that what you're looking to move towards is kind of being the operating system or a major component of a future operating system that uses language models as like kind of the new CPU and maybe retrieval augmented, some kind of storage as like the new memory? Is that kind of the direction that it's going? Yeah, exactly. So it's my ambition. When I started working with Autogen, I discussed with some systems friends working on the systems. I told them this idea. And yeah, it sounds very ambitious idea to them. But I can see that some people really liked this idea. And even some 13-year people will give me stronger, strong support of this. He kind of had that idea independently. I kind of see that some of the most visionary people also realized this. And definitely, we want to pursue for that. Excellent. So taking a big step back, just in terms of the direction of research, I think, I don't know if it's official, but the rumor is right now OpenAI is working on GPT-5. And then, of course, Google with Gemini and Meta, like everyone is working on bigger and bigger models now. And so we're going to get more capabilities at the same time. Smaller models are becoming more efficient. So Satya Nadella announced small language models coming. So that way, you can probably perform very small cognitive functions, but very quickly and efficiently. So what are some of the trends that you see intersecting with your work around auto-gen and agents and agent swarms? And what I mean, I guess, to be specific, is maybe cost changing or new capabilities coming. Are there any capabilities that you're really looking for that would make your job easier? What are your thoughts on some of these new capabilities and making these multi-agent platforms more autonomous? Or is that a good idea, a bad idea? So very kind of open-ended question, like what do you see coming this time next year? Yeah, I think the idea of having specialized models to perform certain tasks in an excellent way and in a cheap way is fascinating. It's indeed worth a lot of investigation. For example, some of the hard problems I mentioned earlier about the decomposition, recombination, validation, it's possible that some specialized model can do these kind of tasks really well. Or we haven't seen that yet. But conceptually, that sounds like a possibility. Actually, I'm pretty surprised that we haven't found a special model that can solve this. So it makes me kind of wonder why. Because it's such a natural idea that if you're finding a model that can do certain things, you should be able to do certain tasks very well and you can just replace one specific agent with that. And I don't know many people are trying that. Either there's some fundamental reason we haven't figured out why we can't do that, or we should be able to see that pretty soon. I think it's only one of these two possibilities. Because the former possibility is still there because the small model, it's possible that the small model lacks some very important capability of being functioned to perform these hard tasks. Because these tasks are not easy. The composition problem, I think, even the GPD4 model, it's known to not to be too good at planning. It can do some kind of planning, but not perfectly. So if you want to get better capability than GPD4 in some specialized tasks, it's to be seeing whether we can accomplish that. But if we lower the target, if we say, let's train some small models that can do something that GPD4 is already good at, that is much more amenable. I think I already see evidence of that. So then it's more like a cost reduction story. So that is, I'm pretty sure, that's feasible. And the other part about getting better capability than the big model, in some aspect, is to be kind of, yeah, we have to hold our scientific curiosity and see what happens. That makes sense. We are almost out of time, so I want to respect everyone's time. And so I'll just say, thank you so much for jumping on and sharing some of your thoughts. I'm super excited to be along for the ride. But yeah, before we close, I'll give the floor to you. Is there anything that you'd like to put out in the world, any personal requests or personal hopes that you want to want to share with a broader audience? Thanks for giving me the opportunity to do that. Yeah, I want to say that we are still early at the new age. Agents become mature software that can do a lot of things for us. We want to build the future together with everyone from the community. So give Autogeno a try. Try to use it for applications. Let us know what's working and what's not. We are very happy to work together to improve it and answer some of the big, important problems as we mentioned. And I really want to acknowledge that all the contributors, starting from the original paper to the recent, more open source resources, joined together, and the huge developer community that's supporting us, I really learned a lot from everyone who has used and provided feedback that people are super, super creative. I think this is the right way to solve the hard problems and hope to continue to do that and support the community, support everyone. And for example, the effort you're doing with the hierarchical agent swarm, it's a very good example that you're making certain bats on certain ways of making money and work. I'm very curious to see how that experiment goes. And if altering can be of any help in this or other consumer efforts, we'll be very happy to support you if you need any feature and useful infrastructure support that kind of thing. Yes, let us know. Yeah, absolutely. No, we'll be definitely looking forward to continuing the collaboration. I think that, as you said, there is a lot of work to do. And there are some limitations. The model's limitations today are the model's limitations. There's not a lot we can do to work around that. But it is just the beginning. And that's one thing that I'll use the closing to say is remember where we were a year ago today. Chat GPT was probably published just about a year ago. But before that, it was GPT-3, GPT-3.5. And the distance that we've covered in just the last year is it is a privilege to be part of one of the greatest shifts that humanity has ever seen. And some days, it doesn't feel real. And some days, it feels a little too real and a little too overwhelming. So thank you, Xi, for helping make this a reality and spending some time talking with me. And thank you to Katie for helping put this together. And yeah, so thanks everyone. And yeah, see you all next time. Thank you so much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.8800000000000001, "text": " Chi, thanks for jumping on.", "tokens": [50364, 17730, 11, 3231, 337, 11233, 322, 13, 50458], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 1, "seek": 0, "start": 1.8800000000000001, "end": 3.68, "text": " It's a pleasure to meet you.", "tokens": [50458, 467, 311, 257, 6834, 281, 1677, 291, 13, 50548], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 2, "seek": 0, "start": 3.68, "end": 5.4, "text": " I was really excited.", "tokens": [50548, 286, 390, 534, 2919, 13, 50634], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 3, "seek": 0, "start": 5.4, "end": 7.72, "text": " Yeah, you're quite welcome.", "tokens": [50634, 865, 11, 291, 434, 1596, 2928, 13, 50750], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 4, "seek": 0, "start": 7.72, "end": 11.28, "text": " Obviously, Autogen is all the rage right now.", "tokens": [50750, 7580, 11, 6049, 8799, 307, 439, 264, 20133, 558, 586, 13, 50928], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 5, "seek": 0, "start": 11.28, "end": 12.280000000000001, "text": " It's very popular.", "tokens": [50928, 467, 311, 588, 3743, 13, 50978], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 6, "seek": 0, "start": 12.280000000000001, "end": 14.92, "text": " There's lots and lots of videos being made about Autogen.", "tokens": [50978, 821, 311, 3195, 293, 3195, 295, 2145, 885, 1027, 466, 6049, 8799, 13, 51110], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 7, "seek": 0, "start": 14.92, "end": 16.64, "text": " But before we dive into that,", "tokens": [51110, 583, 949, 321, 9192, 666, 300, 11, 51196], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 8, "seek": 0, "start": 16.64, "end": 18.8, "text": " I was wondering if you could just tell me a little bit more", "tokens": [51196, 286, 390, 6359, 498, 291, 727, 445, 980, 385, 257, 707, 857, 544, 51304], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 9, "seek": 0, "start": 18.8, "end": 21.76, "text": " about your time at Microsoft as a principal researcher,", "tokens": [51304, 466, 428, 565, 412, 8116, 382, 257, 9716, 21751, 11, 51452], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 10, "seek": 0, "start": 21.76, "end": 23.44, "text": " like how did you get into that position?", "tokens": [51452, 411, 577, 630, 291, 483, 666, 300, 2535, 30, 51536], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 11, "seek": 0, "start": 23.44, "end": 24.68, "text": " What's it been like?", "tokens": [51536, 708, 311, 309, 668, 411, 30, 51598], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 12, "seek": 0, "start": 24.68, "end": 27.12, "text": " Yes, just give me the story.", "tokens": [51598, 1079, 11, 445, 976, 385, 264, 1657, 13, 51720], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 13, "seek": 0, "start": 27.12, "end": 29.76, "text": " Thank you. Yeah, my name is Chi.", "tokens": [51720, 1044, 291, 13, 865, 11, 452, 1315, 307, 17730, 13, 51852], "temperature": 0.0, "avg_logprob": -0.24107724388173762, "compression_ratio": 1.5942492012779552, "no_speech_prob": 0.21080468595027924}, {"id": 14, "seek": 2976, "start": 30.720000000000002, "end": 33.68, "text": " Principal researcher at Microsoft Research.", "tokens": [50412, 38575, 21751, 412, 8116, 10303, 13, 50560], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 15, "seek": 2976, "start": 33.68, "end": 35.56, "text": " I joined long time ago.", "tokens": [50560, 286, 6869, 938, 565, 2057, 13, 50654], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 16, "seek": 2976, "start": 35.56, "end": 39.120000000000005, "text": " I joined about nine years ago in Microsoft.", "tokens": [50654, 286, 6869, 466, 4949, 924, 2057, 294, 8116, 13, 50832], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 17, "seek": 2976, "start": 39.120000000000005, "end": 41.84, "text": " And I've been working on many different projects.", "tokens": [50832, 400, 286, 600, 668, 1364, 322, 867, 819, 4455, 13, 50968], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 18, "seek": 2976, "start": 41.84, "end": 44.120000000000005, "text": " Apparently now I'm focusing on Autogen.", "tokens": [50968, 16755, 586, 286, 478, 8416, 322, 6049, 8799, 13, 51082], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 19, "seek": 2976, "start": 44.120000000000005, "end": 47.64, "text": " And before that, I worked on automated machine learning,", "tokens": [51082, 400, 949, 300, 11, 286, 2732, 322, 18473, 3479, 2539, 11, 51258], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 20, "seek": 2976, "start": 47.64, "end": 50.120000000000005, "text": " machine learning for systems,", "tokens": [51258, 3479, 2539, 337, 3652, 11, 51382], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 21, "seek": 2976, "start": 50.120000000000005, "end": 53.2, "text": " data science, data analytics, data mining.", "tokens": [51382, 1412, 3497, 11, 1412, 15370, 11, 1412, 15512, 13, 51536], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 22, "seek": 2976, "start": 53.2, "end": 56.56, "text": " So quite a lot of different things.", "tokens": [51536, 407, 1596, 257, 688, 295, 819, 721, 13, 51704], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 23, "seek": 2976, "start": 56.56, "end": 59.64, "text": " And some of the work is more like on a theoretical set", "tokens": [51704, 400, 512, 295, 264, 589, 307, 544, 411, 322, 257, 20864, 992, 51858], "temperature": 0.0, "avg_logprob": -0.17015480326714916, "compression_ratio": 1.6746031746031746, "no_speech_prob": 0.0006554771680384874}, {"id": 24, "seek": 5964, "start": 60.0, "end": 63.160000000000004, "text": " side and some of them is more to the system side.", "tokens": [50382, 1252, 293, 512, 295, 552, 307, 544, 281, 264, 1185, 1252, 13, 50540], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 25, "seek": 5964, "start": 64.32, "end": 69.32, "text": " And yeah, I've been focusing on Autogen recently.", "tokens": [50598, 400, 1338, 11, 286, 600, 668, 8416, 322, 6049, 8799, 3938, 13, 50848], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 26, "seek": 5964, "start": 69.56, "end": 72.16, "text": " So very happy to be here.", "tokens": [50860, 407, 588, 2055, 281, 312, 510, 13, 50990], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 27, "seek": 5964, "start": 72.16, "end": 73.0, "text": " Excellent.", "tokens": [50990, 16723, 13, 51032], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 28, "seek": 5964, "start": 73.0, "end": 76.08, "text": " Yeah, with all the progress that's been made", "tokens": [51032, 865, 11, 365, 439, 264, 4205, 300, 311, 668, 1027, 51186], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 29, "seek": 5964, "start": 76.08, "end": 78.88, "text": " on large language models and working on assistance", "tokens": [51186, 322, 2416, 2856, 5245, 293, 1364, 322, 9683, 51326], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 30, "seek": 5964, "start": 78.88, "end": 82.44, "text": " and agents, and then of course, working on agents,", "tokens": [51326, 293, 12554, 11, 293, 550, 295, 1164, 11, 1364, 322, 12554, 11, 51504], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 31, "seek": 5964, "start": 82.44, "end": 84.08, "text": " working with other agents,", "tokens": [51504, 1364, 365, 661, 12554, 11, 51586], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 32, "seek": 5964, "start": 84.08, "end": 85.88, "text": " let me ask kind of a big question.", "tokens": [51586, 718, 385, 1029, 733, 295, 257, 955, 1168, 13, 51676], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 33, "seek": 5964, "start": 85.88, "end": 89.08, "text": " Like what was the genesis behind Autogen?", "tokens": [51676, 1743, 437, 390, 264, 1049, 9374, 2261, 6049, 8799, 30, 51836], "temperature": 0.0, "avg_logprob": -0.20916326330342425, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.0004298602871131152}, {"id": 34, "seek": 8908, "start": 90.0, "end": 92.36, "text": " How was that project proposed?", "tokens": [50410, 1012, 390, 300, 1716, 10348, 30, 50528], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 35, "seek": 8908, "start": 92.36, "end": 94.67999999999999, "text": " Like how did it come together?", "tokens": [50528, 1743, 577, 630, 309, 808, 1214, 30, 50644], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 36, "seek": 8908, "start": 94.67999999999999, "end": 96.56, "text": " Like what was the theoretical work behind it?", "tokens": [50644, 1743, 437, 390, 264, 20864, 589, 2261, 309, 30, 50738], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 37, "seek": 8908, "start": 96.56, "end": 101.12, "text": " And how did you get from zero to where you are today?", "tokens": [50738, 400, 577, 630, 291, 483, 490, 4018, 281, 689, 291, 366, 965, 30, 50966], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 38, "seek": 8908, "start": 101.12, "end": 104.64, "text": " Yeah, so that's a very interesting question.", "tokens": [50966, 865, 11, 370, 300, 311, 257, 588, 1880, 1168, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 39, "seek": 8908, "start": 104.64, "end": 106.52, "text": " And to fully answer that question,", "tokens": [51142, 400, 281, 4498, 1867, 300, 1168, 11, 51236], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 40, "seek": 8908, "start": 106.52, "end": 108.52, "text": " I need to tell a long story.", "tokens": [51236, 286, 643, 281, 980, 257, 938, 1657, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 41, "seek": 8908, "start": 109.44, "end": 110.92, "text": " But let me first tell you a short one.", "tokens": [51382, 583, 718, 385, 700, 980, 291, 257, 2099, 472, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 42, "seek": 8908, "start": 110.92, "end": 115.92, "text": " Short answer is like with this big kind of opportunities", "tokens": [51456, 16881, 1867, 307, 411, 365, 341, 955, 733, 295, 4786, 51706], "temperature": 0.0, "avg_logprob": -0.1909170913696289, "compression_ratio": 1.5913043478260869, "no_speech_prob": 0.00048759017954580486}, {"id": 43, "seek": 11592, "start": 115.96000000000001, "end": 119.36, "text": " with larger models, so powerful techniques.", "tokens": [50366, 365, 4833, 5245, 11, 370, 4005, 7512, 13, 50536], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 44, "seek": 11592, "start": 119.36, "end": 121.2, "text": " At MSR, we want to ask the question,", "tokens": [50536, 1711, 7395, 49, 11, 321, 528, 281, 1029, 264, 1168, 11, 50628], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 45, "seek": 11592, "start": 121.2, "end": 123.16, "text": " like what is the future, right?", "tokens": [50628, 411, 437, 307, 264, 2027, 11, 558, 30, 50726], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 46, "seek": 11592, "start": 123.16, "end": 127.16, "text": " We want to be forward-looking, we want to be futuristic,", "tokens": [50726, 492, 528, 281, 312, 2128, 12, 16129, 11, 321, 528, 281, 312, 44932, 11, 50926], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 47, "seek": 11592, "start": 127.16, "end": 129.24, "text": " kind of take the solid leadership.", "tokens": [50926, 733, 295, 747, 264, 5100, 5848, 13, 51030], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 48, "seek": 11592, "start": 129.24, "end": 131.76, "text": " And because one of the famous quotes", "tokens": [51030, 400, 570, 472, 295, 264, 4618, 19963, 51156], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 49, "seek": 11592, "start": 131.76, "end": 134.52, "text": " from the founder of Macro Research,", "tokens": [51156, 490, 264, 14917, 295, 5707, 340, 10303, 11, 51294], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 50, "seek": 11592, "start": 134.52, "end": 137.68, "text": " we can say that the best science", "tokens": [51294, 321, 393, 584, 300, 264, 1151, 3497, 51452], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 51, "seek": 11592, "start": 137.68, "end": 140.96, "text": " will be indistinguishable from magic.", "tokens": [51452, 486, 312, 1016, 468, 7050, 742, 712, 490, 5585, 13, 51616], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 52, "seek": 11592, "start": 140.96, "end": 144.92000000000002, "text": " So that's the level of ambition we have.", "tokens": [51616, 407, 300, 311, 264, 1496, 295, 22814, 321, 362, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3356288841792515, "compression_ratio": 1.5877551020408163, "no_speech_prob": 0.0012435646494850516}, {"id": 53, "seek": 14492, "start": 144.92, "end": 146.2, "text": " So we asked the question,", "tokens": [50364, 407, 321, 2351, 264, 1168, 11, 50428], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 54, "seek": 14492, "start": 146.2, "end": 149.64, "text": " what is the future of AI applications?", "tokens": [50428, 437, 307, 264, 2027, 295, 7318, 5821, 30, 50600], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 55, "seek": 14492, "start": 149.64, "end": 152.28, "text": " And how do we empower every developer to build them?", "tokens": [50600, 400, 577, 360, 321, 11071, 633, 10754, 281, 1322, 552, 30, 50732], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 56, "seek": 14492, "start": 153.35999999999999, "end": 156.04, "text": " Yeah, so that's a fundamental driving question.", "tokens": [50786, 865, 11, 370, 300, 311, 257, 8088, 4840, 1168, 13, 50920], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 57, "seek": 14492, "start": 156.04, "end": 160.2, "text": " And now a longer version of the answer is that,", "tokens": [50920, 400, 586, 257, 2854, 3037, 295, 264, 1867, 307, 300, 11, 51128], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 58, "seek": 14492, "start": 160.2, "end": 162.56, "text": " I started from, I thought as a tutorial,", "tokens": [51128, 286, 1409, 490, 11, 286, 1194, 382, 257, 7073, 11, 51246], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 59, "seek": 14492, "start": 162.56, "end": 163.6, "text": " before I worked on Autogen,", "tokens": [51246, 949, 286, 2732, 322, 6049, 8799, 11, 51298], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 60, "seek": 14492, "start": 163.6, "end": 166.23999999999998, "text": " I worked on automated machine learning,", "tokens": [51298, 286, 2732, 322, 18473, 3479, 2539, 11, 51430], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 61, "seek": 14492, "start": 166.23999999999998, "end": 168.72, "text": " which is another open source project called FLAML.", "tokens": [51430, 597, 307, 1071, 1269, 4009, 1716, 1219, 24720, 2865, 43, 13, 51554], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 62, "seek": 14492, "start": 169.83999999999997, "end": 174.35999999999999, "text": " FLAML is a solution for automating model selection,", "tokens": [51610, 24720, 2865, 43, 307, 257, 3827, 337, 3553, 990, 2316, 9450, 11, 51836], "temperature": 0.0, "avg_logprob": -0.22005985094153363, "compression_ratio": 1.591760299625468, "no_speech_prob": 0.00013337998825591058}, {"id": 63, "seek": 17436, "start": 174.36, "end": 175.88000000000002, "text": " hyperparameter tuning, essentially,", "tokens": [50364, 9848, 2181, 335, 2398, 15164, 11, 4476, 11, 50440], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 64, "seek": 17436, "start": 175.88000000000002, "end": 180.24, "text": " black box optimization to navigate a larger space", "tokens": [50440, 2211, 2424, 19618, 281, 12350, 257, 4833, 1901, 50658], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 65, "seek": 17436, "start": 180.24, "end": 181.72000000000003, "text": " without knowing the gradient.", "tokens": [50658, 1553, 5276, 264, 16235, 13, 50732], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 66, "seek": 17436, "start": 182.96, "end": 185.8, "text": " So it's a very powerful technique,", "tokens": [50794, 407, 309, 311, 257, 588, 4005, 6532, 11, 50936], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 67, "seek": 17436, "start": 185.8, "end": 190.8, "text": " but that was started before the larger model takes the storm.", "tokens": [50936, 457, 300, 390, 1409, 949, 264, 4833, 2316, 2516, 264, 7679, 13, 51186], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 68, "seek": 17436, "start": 191.72000000000003, "end": 194.52, "text": " And when chatGVT released, right?", "tokens": [51232, 400, 562, 5081, 38, 53, 51, 4736, 11, 558, 30, 51372], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 69, "seek": 17436, "start": 194.52, "end": 199.52, "text": " And that's a big, big kind of upgrade of the model capabilities.", "tokens": [51372, 400, 300, 311, 257, 955, 11, 955, 733, 295, 11484, 295, 264, 2316, 10862, 13, 51622], "temperature": 0.0, "avg_logprob": -0.34685316318418924, "compression_ratio": 1.460093896713615, "no_speech_prob": 0.0008689587120898068}, {"id": 70, "seek": 19952, "start": 199.52, "end": 204.52, "text": " And so I started working on a similar problem", "tokens": [50364, 400, 370, 286, 1409, 1364, 322, 257, 2531, 1154, 50614], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 71, "seek": 19952, "start": 204.68, "end": 206.0, "text": " as automated machine learning.", "tokens": [50622, 382, 18473, 3479, 2539, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 72, "seek": 19952, "start": 206.0, "end": 208.4, "text": " I started working on inference hyperparameter tuning", "tokens": [50688, 286, 1409, 1364, 322, 38253, 9848, 2181, 335, 2398, 15164, 50808], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 73, "seek": 19952, "start": 208.4, "end": 210.60000000000002, "text": " for these chatGVT models.", "tokens": [50808, 337, 613, 5081, 38, 53, 51, 5245, 13, 50918], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 74, "seek": 19952, "start": 210.60000000000002, "end": 213.0, "text": " For example, how do you select the best model?", "tokens": [50918, 1171, 1365, 11, 577, 360, 291, 3048, 264, 1151, 2316, 30, 51038], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 75, "seek": 19952, "start": 213.0, "end": 216.12, "text": " How do you select the right prompt temperature", "tokens": [51038, 1012, 360, 291, 3048, 264, 558, 12391, 4292, 51194], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 76, "seek": 19952, "start": 216.12, "end": 217.96, "text": " and all the other inference parameters", "tokens": [51194, 293, 439, 264, 661, 38253, 9834, 51286], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 77, "seek": 19952, "start": 217.96, "end": 221.44, "text": " so that you can maximize the utility from the models", "tokens": [51286, 370, 300, 291, 393, 19874, 264, 14877, 490, 264, 5245, 51460], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 78, "seek": 19952, "start": 221.44, "end": 223.04000000000002, "text": " while minimizing your cost?", "tokens": [51460, 1339, 46608, 428, 2063, 30, 51540], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 79, "seek": 19952, "start": 224.08, "end": 228.56, "text": " So that's the initial work on this direction.", "tokens": [51592, 407, 300, 311, 264, 5883, 589, 322, 341, 3513, 13, 51816], "temperature": 0.0, "avg_logprob": -0.14122884090130144, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.00045102962758392096}, {"id": 80, "seek": 22856, "start": 228.64000000000001, "end": 231.52, "text": " And when GT4 is released,", "tokens": [50368, 400, 562, 17530, 19, 307, 4736, 11, 50512], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 81, "seek": 22856, "start": 231.52, "end": 235.36, "text": " that's another big upgrade of the level capabilities.", "tokens": [50512, 300, 311, 1071, 955, 11484, 295, 264, 1496, 10862, 13, 50704], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 82, "seek": 22856, "start": 235.36, "end": 239.0, "text": " So then I started really to ask the question,", "tokens": [50704, 407, 550, 286, 1409, 534, 281, 1029, 264, 1168, 11, 50886], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 83, "seek": 22856, "start": 239.0, "end": 244.0, "text": " okay, if we kind of want to bring the best power", "tokens": [50886, 1392, 11, 498, 321, 733, 295, 528, 281, 1565, 264, 1151, 1347, 51136], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 84, "seek": 22856, "start": 244.68, "end": 247.48000000000002, "text": " of the model and really solve really difficult problems,", "tokens": [51170, 295, 264, 2316, 293, 534, 5039, 534, 2252, 2740, 11, 51310], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 85, "seek": 22856, "start": 247.48000000000002, "end": 249.72, "text": " what should be the right way to do it?", "tokens": [51310, 437, 820, 312, 264, 558, 636, 281, 360, 309, 30, 51422], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 86, "seek": 22856, "start": 249.72, "end": 254.2, "text": " And agent is apparently a very powerful notion.", "tokens": [51422, 400, 9461, 307, 7970, 257, 588, 4005, 10710, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 87, "seek": 22856, "start": 254.2, "end": 257.24, "text": " It's another kind of level of the automation", "tokens": [51646, 467, 311, 1071, 733, 295, 1496, 295, 264, 17769, 51798], "temperature": 0.0, "avg_logprob": -0.1646431471172132, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.00029120210092514753}, {"id": 88, "seek": 25724, "start": 257.24, "end": 259.44, "text": " as opposed to the previous automation", "tokens": [50364, 382, 8851, 281, 264, 3894, 17769, 50474], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 89, "seek": 25724, "start": 259.44, "end": 262.68, "text": " in the 20 machine learning work I did.", "tokens": [50474, 294, 264, 945, 3479, 2539, 589, 286, 630, 13, 50636], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 90, "seek": 25724, "start": 262.68, "end": 265.0, "text": " So yeah, so that's where they started.", "tokens": [50636, 407, 1338, 11, 370, 300, 311, 689, 436, 1409, 13, 50752], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 91, "seek": 25724, "start": 266.04, "end": 270.44, "text": " And of course, it's a whole new area.", "tokens": [50804, 400, 295, 1164, 11, 309, 311, 257, 1379, 777, 1859, 13, 51024], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 92, "seek": 25724, "start": 270.44, "end": 271.72, "text": " No, the shop agent is not new.", "tokens": [51024, 883, 11, 264, 3945, 9461, 307, 406, 777, 13, 51088], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 93, "seek": 25724, "start": 271.72, "end": 275.56, "text": " It's has been there for a long time.", "tokens": [51088, 467, 311, 575, 668, 456, 337, 257, 938, 565, 13, 51280], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 94, "seek": 25724, "start": 275.56, "end": 277.24, "text": " And I remember back in college,", "tokens": [51280, 400, 286, 1604, 646, 294, 3859, 11, 51364], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 95, "seek": 25724, "start": 277.24, "end": 279.68, "text": " I worked on some like game competition,", "tokens": [51364, 286, 2732, 322, 512, 411, 1216, 6211, 11, 51486], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 96, "seek": 25724, "start": 279.68, "end": 283.08, "text": " like building agents that can play games", "tokens": [51486, 411, 2390, 12554, 300, 393, 862, 2813, 51656], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 97, "seek": 25724, "start": 283.08, "end": 284.84000000000003, "text": " with each other and compete.", "tokens": [51656, 365, 1184, 661, 293, 11831, 13, 51744], "temperature": 0.0, "avg_logprob": -0.29095348176502045, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0011330809211358428}, {"id": 98, "seek": 28484, "start": 284.96, "end": 288.23999999999995, "text": " At that time, we were using many of the rule-based methods", "tokens": [50370, 1711, 300, 565, 11, 321, 645, 1228, 867, 295, 264, 4978, 12, 6032, 7150, 50534], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 99, "seek": 28484, "start": 288.23999999999995, "end": 291.71999999999997, "text": " where a lot of deal with a lot of corner cases and so on.", "tokens": [50534, 689, 257, 688, 295, 2028, 365, 257, 688, 295, 4538, 3331, 293, 370, 322, 13, 50708], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 100, "seek": 28484, "start": 291.71999999999997, "end": 293.03999999999996, "text": " So it's not viable.", "tokens": [50708, 407, 309, 311, 406, 22024, 13, 50774], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 101, "seek": 28484, "start": 293.03999999999996, "end": 295.64, "text": " It's a good notion, but not viable.", "tokens": [50774, 467, 311, 257, 665, 10710, 11, 457, 406, 22024, 13, 50904], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 102, "seek": 28484, "start": 295.64, "end": 298.79999999999995, "text": " And these larger models,", "tokens": [50904, 400, 613, 4833, 5245, 11, 51062], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 103, "seek": 28484, "start": 298.79999999999995, "end": 300.96, "text": " especially the chat-operated models,", "tokens": [51062, 2318, 264, 5081, 12, 7192, 770, 5245, 11, 51170], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 104, "seek": 28484, "start": 300.96, "end": 305.76, "text": " really make it viable and a reality", "tokens": [51170, 534, 652, 309, 22024, 293, 257, 4103, 51410], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 105, "seek": 28484, "start": 305.76, "end": 309.03999999999996, "text": " that we can build new software based on.", "tokens": [51410, 300, 321, 393, 1322, 777, 4722, 2361, 322, 13, 51574], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 106, "seek": 28484, "start": 309.03999999999996, "end": 311.23999999999995, "text": " And to study this new area,", "tokens": [51574, 400, 281, 2979, 341, 777, 1859, 11, 51684], "temperature": 0.0, "avg_logprob": -0.22717254940826115, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.0006164780352264643}, {"id": 107, "seek": 31124, "start": 311.24, "end": 315.96000000000004, "text": " we kind of need to think many things from scratch", "tokens": [50364, 321, 733, 295, 643, 281, 519, 867, 721, 490, 8459, 50600], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 108, "seek": 31124, "start": 315.96000000000004, "end": 320.40000000000003, "text": " and try to take some of the first principles", "tokens": [50600, 293, 853, 281, 747, 512, 295, 264, 700, 9156, 50822], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 109, "seek": 31124, "start": 320.40000000000003, "end": 321.88, "text": " and what is the right approach.", "tokens": [50822, 293, 437, 307, 264, 558, 3109, 13, 50896], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 110, "seek": 31124, "start": 321.88, "end": 325.88, "text": " And basically leverage every lesson", "tokens": [50896, 400, 1936, 13982, 633, 6898, 51096], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 111, "seek": 31124, "start": 325.88, "end": 329.24, "text": " we learned from the previous projects, previous experience,", "tokens": [51096, 321, 3264, 490, 264, 3894, 4455, 11, 3894, 1752, 11, 51264], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 112, "seek": 31124, "start": 329.24, "end": 334.24, "text": " but try to build this one multi-agent framework", "tokens": [51264, 457, 853, 281, 1322, 341, 472, 4825, 12, 559, 317, 8388, 51514], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 113, "seek": 31124, "start": 336.04, "end": 338.72, "text": " that is really generic,", "tokens": [51604, 300, 307, 534, 19577, 11, 51738], "temperature": 0.0, "avg_logprob": -0.22177160602726348, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0008159478311426938}, {"id": 114, "seek": 33872, "start": 338.72, "end": 340.72, "text": " and can support diverse applications.", "tokens": [50364, 293, 393, 1406, 9521, 5821, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 115, "seek": 33872, "start": 341.72, "end": 344.68, "text": " Yeah, so there are many different examples", "tokens": [50514, 865, 11, 370, 456, 366, 867, 819, 5110, 50662], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 116, "seek": 33872, "start": 344.68, "end": 348.96000000000004, "text": " of sources of inspirations I can give you,", "tokens": [50662, 295, 7139, 295, 17432, 763, 286, 393, 976, 291, 11, 50876], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 117, "seek": 33872, "start": 348.96000000000004, "end": 352.8, "text": " but why not show it's like using everything I learned", "tokens": [50876, 457, 983, 406, 855, 309, 311, 411, 1228, 1203, 286, 3264, 51068], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 118, "seek": 33872, "start": 352.8, "end": 357.56, "text": " and also take every feedback I received from everyone", "tokens": [51068, 293, 611, 747, 633, 5824, 286, 4613, 490, 1518, 51306], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 119, "seek": 33872, "start": 357.56, "end": 359.28000000000003, "text": " and iterate on that.", "tokens": [51306, 293, 44497, 322, 300, 13, 51392], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 120, "seek": 33872, "start": 359.28000000000003, "end": 361.96000000000004, "text": " And so that's how we come here today.", "tokens": [51392, 400, 370, 300, 311, 577, 321, 808, 510, 965, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 121, "seek": 33872, "start": 361.96000000000004, "end": 362.88000000000005, "text": " Excellent.", "tokens": [51526, 16723, 13, 51572], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 122, "seek": 33872, "start": 362.88000000000005, "end": 365.44000000000005, "text": " Yeah, no, I mean, there's a lot to unpack there.", "tokens": [51572, 865, 11, 572, 11, 286, 914, 11, 456, 311, 257, 688, 281, 26699, 456, 13, 51700], "temperature": 0.0, "avg_logprob": -0.2555968562761943, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.0015971680404618382}, {"id": 123, "seek": 36544, "start": 365.44, "end": 368.32, "text": " It's fascinating to me that it started", "tokens": [50364, 467, 311, 10343, 281, 385, 300, 309, 1409, 50508], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 124, "seek": 36544, "start": 368.32, "end": 371.12, "text": " as sort of like auto ML,", "tokens": [50508, 382, 1333, 295, 411, 8399, 21601, 11, 50648], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 125, "seek": 36544, "start": 371.12, "end": 374.2, "text": " like automating the optimization.", "tokens": [50648, 411, 3553, 990, 264, 19618, 13, 50802], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 126, "seek": 36544, "start": 374.2, "end": 378.04, "text": " And I can see how going to like optimizing prompts", "tokens": [50802, 400, 286, 393, 536, 577, 516, 281, 411, 40425, 41095, 50994], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 127, "seek": 36544, "start": 378.04, "end": 380.96, "text": " and optimizing hyperparameters and parameter tuning", "tokens": [50994, 293, 40425, 9848, 2181, 335, 6202, 293, 13075, 15164, 51140], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 128, "seek": 36544, "start": 380.96, "end": 383.64, "text": " could then lead to the agents, especially, like you said,", "tokens": [51140, 727, 550, 1477, 281, 264, 12554, 11, 2318, 11, 411, 291, 848, 11, 51274], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 129, "seek": 36544, "start": 383.64, "end": 386.0, "text": " if the idea is thinking to the future,", "tokens": [51274, 498, 264, 1558, 307, 1953, 281, 264, 2027, 11, 51392], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 130, "seek": 36544, "start": 386.0, "end": 387.96, "text": " like what is the sci-fi version", "tokens": [51392, 411, 437, 307, 264, 2180, 12, 13325, 3037, 51490], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 131, "seek": 36544, "start": 387.96, "end": 391.56, "text": " of enabling application development in the future?", "tokens": [51490, 295, 23148, 3861, 3250, 294, 264, 2027, 30, 51670], "temperature": 0.0, "avg_logprob": -0.16601871957584302, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0013667125022038817}, {"id": 132, "seek": 39156, "start": 391.56, "end": 395.8, "text": " So I wanted to follow up with a kind of a two-part question.", "tokens": [50364, 407, 286, 1415, 281, 1524, 493, 365, 257, 733, 295, 257, 732, 12, 6971, 1168, 13, 50576], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 133, "seek": 39156, "start": 395.8, "end": 399.92, "text": " So in the most basic level, what is AutoGen?", "tokens": [50576, 407, 294, 264, 881, 3875, 1496, 11, 437, 307, 13738, 26647, 30, 50782], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 134, "seek": 39156, "start": 399.92, "end": 402.08, "text": " But more specifically, what is the vision?", "tokens": [50782, 583, 544, 4682, 11, 437, 307, 264, 5201, 30, 50890], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 135, "seek": 39156, "start": 402.08, "end": 404.72, "text": " Like what is it that you're trying to solve", "tokens": [50890, 1743, 437, 307, 309, 300, 291, 434, 1382, 281, 5039, 51022], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 136, "seek": 39156, "start": 404.72, "end": 406.44, "text": " with AutoGen right now?", "tokens": [51022, 365, 13738, 26647, 558, 586, 30, 51108], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 137, "seek": 39156, "start": 406.44, "end": 410.08, "text": " Yeah, so yeah, in one word,", "tokens": [51108, 865, 11, 370, 1338, 11, 294, 472, 1349, 11, 51290], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 138, "seek": 39156, "start": 410.08, "end": 414.6, "text": " AutoGen is the multi-agent AI framework,", "tokens": [51290, 13738, 26647, 307, 264, 4825, 12, 559, 317, 7318, 8388, 11, 51516], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 139, "seek": 39156, "start": 414.6, "end": 417.84000000000003, "text": " and especially focusing on multi-agent conversations", "tokens": [51516, 293, 2318, 8416, 322, 4825, 12, 559, 317, 7315, 51678], "temperature": 0.0, "avg_logprob": -0.16716550827026366, "compression_ratio": 1.54337899543379, "no_speech_prob": 0.0002530943020246923}, {"id": 140, "seek": 41784, "start": 417.84, "end": 421.52, "text": " so that we can connect large-level models, tools,", "tokens": [50364, 370, 300, 321, 393, 1745, 2416, 12, 12418, 5245, 11, 3873, 11, 50548], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 141, "seek": 41784, "start": 421.52, "end": 426.52, "text": " and human inputs together to solve complex asks.", "tokens": [50548, 293, 1952, 15743, 1214, 281, 5039, 3997, 8962, 13, 50798], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 142, "seek": 41784, "start": 426.59999999999997, "end": 429.47999999999996, "text": " There can be multiple ways to understand this.", "tokens": [50802, 821, 393, 312, 3866, 2098, 281, 1223, 341, 13, 50946], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 143, "seek": 41784, "start": 429.47999999999996, "end": 434.12, "text": " So in one way is to understand it as a programming framework", "tokens": [50946, 407, 294, 472, 636, 307, 281, 1223, 309, 382, 257, 9410, 8388, 51178], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 144, "seek": 41784, "start": 435.0, "end": 437.76, "text": " for developers to build applications easily", "tokens": [51222, 337, 8849, 281, 1322, 5821, 3612, 51360], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 145, "seek": 41784, "start": 437.76, "end": 440.47999999999996, "text": " with some simple and unified abstraction", "tokens": [51360, 365, 512, 2199, 293, 26787, 37765, 51496], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 146, "seek": 41784, "start": 440.47999999999996, "end": 442.28, "text": " so that they don't need to worry too much", "tokens": [51496, 370, 300, 436, 500, 380, 643, 281, 3292, 886, 709, 51586], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 147, "seek": 41784, "start": 442.28, "end": 443.47999999999996, "text": " about the lower details,", "tokens": [51586, 466, 264, 3126, 4365, 11, 51646], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 148, "seek": 41784, "start": 443.47999999999996, "end": 446.59999999999997, "text": " but can focus on how to define agents,", "tokens": [51646, 457, 393, 1879, 322, 577, 281, 6964, 12554, 11, 51802], "temperature": 0.0, "avg_logprob": -0.23455236396011042, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0018380500841885805}, {"id": 149, "seek": 44660, "start": 446.6, "end": 450.16, "text": " how to get them to work with each other,", "tokens": [50364, 577, 281, 483, 552, 281, 589, 365, 1184, 661, 11, 50542], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 150, "seek": 44660, "start": 450.16, "end": 453.08000000000004, "text": " and eventually reach the goal.", "tokens": [50542, 293, 4728, 2524, 264, 3387, 13, 50688], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 151, "seek": 44660, "start": 453.08000000000004, "end": 456.0, "text": " It can also be understood as a tool", "tokens": [50688, 467, 393, 611, 312, 7320, 382, 257, 2290, 50834], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 152, "seek": 44660, "start": 456.0, "end": 459.0, "text": " to kind of scale up,", "tokens": [50834, 281, 733, 295, 4373, 493, 11, 50984], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 153, "seek": 44660, "start": 459.0, "end": 461.88, "text": " scale up the power of our models", "tokens": [50984, 4373, 493, 264, 1347, 295, 527, 5245, 51128], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 154, "seek": 44660, "start": 461.88, "end": 464.44, "text": " and makes them even more useful", "tokens": [51128, 293, 1669, 552, 754, 544, 4420, 51256], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 155, "seek": 44660, "start": 464.44, "end": 466.68, "text": " by connecting with other tools,", "tokens": [51256, 538, 11015, 365, 661, 3873, 11, 51368], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 156, "seek": 44660, "start": 466.68, "end": 471.68, "text": " non-narrative model tools, or human collaborators,", "tokens": [51368, 2107, 12, 77, 2284, 1166, 2316, 3873, 11, 420, 1952, 39789, 11, 51618], "temperature": 0.0, "avg_logprob": -0.2659887482848349, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0007317045237869024}, {"id": 157, "seek": 47168, "start": 472.64, "end": 476.68, "text": " and kind of scale up both the complexity of the problem", "tokens": [50412, 293, 733, 295, 4373, 493, 1293, 264, 14024, 295, 264, 1154, 50614], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 158, "seek": 47168, "start": 476.68, "end": 480.76, "text": " they can solve, the degree of automation, to some extent.", "tokens": [50614, 436, 393, 5039, 11, 264, 4314, 295, 17769, 11, 281, 512, 8396, 13, 50818], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 159, "seek": 47168, "start": 480.76, "end": 484.08, "text": " Yeah, this is kind of a relatively abstract instruction,", "tokens": [50818, 865, 11, 341, 307, 733, 295, 257, 7226, 12649, 10951, 11, 50984], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 160, "seek": 47168, "start": 484.08, "end": 486.92, "text": " but if we think about it,", "tokens": [50984, 457, 498, 321, 519, 466, 309, 11, 51126], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 161, "seek": 47168, "start": 486.92, "end": 490.48, "text": " about how people use it, it's quite simple.", "tokens": [51126, 466, 577, 561, 764, 309, 11, 309, 311, 1596, 2199, 13, 51304], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 162, "seek": 47168, "start": 490.48, "end": 493.04, "text": " So when developers build applications with AutoGen,", "tokens": [51304, 407, 562, 8849, 1322, 5821, 365, 13738, 26647, 11, 51432], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 163, "seek": 47168, "start": 493.04, "end": 495.24, "text": " it basically boils down to two steps.", "tokens": [51432, 309, 1936, 35049, 760, 281, 732, 4439, 13, 51542], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 164, "seek": 47168, "start": 495.24, "end": 497.08, "text": " Step one is to define agents,", "tokens": [51542, 5470, 472, 307, 281, 6964, 12554, 11, 51634], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 165, "seek": 47168, "start": 497.08, "end": 499.12, "text": " and step two is to get them to talk.", "tokens": [51634, 293, 1823, 732, 307, 281, 483, 552, 281, 751, 13, 51736], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 166, "seek": 47168, "start": 499.12, "end": 501.28000000000003, "text": " So as simple as that.", "tokens": [51736, 407, 382, 2199, 382, 300, 13, 51844], "temperature": 0.0, "avg_logprob": -0.22150477533755095, "compression_ratio": 1.63671875, "no_speech_prob": 0.0005108832265250385}, {"id": 167, "seek": 50128, "start": 501.84, "end": 506.84, "text": " Yeah, so we try to make it very useful and generic,", "tokens": [50392, 865, 11, 370, 321, 853, 281, 652, 309, 588, 4420, 293, 19577, 11, 50642], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 168, "seek": 50128, "start": 506.84, "end": 507.79999999999995, "text": " but on the other hand,", "tokens": [50642, 457, 322, 264, 661, 1011, 11, 50690], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 169, "seek": 50128, "start": 507.79999999999995, "end": 512.16, "text": " we want to have a very simple interface for people to use.", "tokens": [50690, 321, 528, 281, 362, 257, 588, 2199, 9226, 337, 561, 281, 764, 13, 50908], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 170, "seek": 50128, "start": 512.16, "end": 515.24, "text": " Yeah, I mean, that's exactly the vision", "tokens": [50908, 865, 11, 286, 914, 11, 300, 311, 2293, 264, 5201, 51062], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 171, "seek": 50128, "start": 515.24, "end": 517.1999999999999, "text": " that I kind of settled on", "tokens": [51062, 300, 286, 733, 295, 14819, 322, 51160], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 172, "seek": 50128, "start": 517.1999999999999, "end": 520.92, "text": " for my hierarchical autonomous agent swarm idea,", "tokens": [51160, 337, 452, 35250, 804, 23797, 9461, 49839, 1558, 11, 51346], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 173, "seek": 50128, "start": 520.92, "end": 524.64, "text": " but I don't want to make it about that.", "tokens": [51346, 457, 286, 500, 380, 528, 281, 652, 309, 466, 300, 13, 51532], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 174, "seek": 50128, "start": 524.64, "end": 526.12, "text": " Right now, it's just fascinating", "tokens": [51532, 1779, 586, 11, 309, 311, 445, 10343, 51606], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 175, "seek": 50128, "start": 526.12, "end": 529.6, "text": " that we kind of converge on a very similar principle,", "tokens": [51606, 300, 321, 733, 295, 41881, 322, 257, 588, 2531, 8665, 11, 51780], "temperature": 0.0, "avg_logprob": -0.19411338950103185, "compression_ratio": 1.609442060085837, "no_speech_prob": 0.0006767543964087963}, {"id": 176, "seek": 52960, "start": 529.6, "end": 533.0400000000001, "text": " like let's make the deployment of software", "tokens": [50364, 411, 718, 311, 652, 264, 19317, 295, 4722, 50536], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 177, "seek": 52960, "start": 533.0400000000001, "end": 534.84, "text": " as easy as possible.", "tokens": [50536, 382, 1858, 382, 1944, 13, 50626], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 178, "seek": 52960, "start": 534.84, "end": 536.72, "text": " And so there's two things that you mentioned,", "tokens": [50626, 400, 370, 456, 311, 732, 721, 300, 291, 2835, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 179, "seek": 52960, "start": 536.72, "end": 538.28, "text": " like layers of abstraction,", "tokens": [50720, 411, 7914, 295, 37765, 11, 50798], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 180, "seek": 52960, "start": 538.28, "end": 540.44, "text": " because I think that's a really good intuitive way", "tokens": [50798, 570, 286, 519, 300, 311, 257, 534, 665, 21769, 636, 50906], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 181, "seek": 52960, "start": 540.44, "end": 541.28, "text": " of thinking about it,", "tokens": [50906, 295, 1953, 466, 309, 11, 50948], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 182, "seek": 52960, "start": 541.28, "end": 545.0400000000001, "text": " like in the same way that a Python interpreter", "tokens": [50948, 411, 294, 264, 912, 636, 300, 257, 15329, 34132, 51136], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 183, "seek": 52960, "start": 545.0400000000001, "end": 548.16, "text": " was a layer of abstraction from compiled code,", "tokens": [51136, 390, 257, 4583, 295, 37765, 490, 36548, 3089, 11, 51292], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 184, "seek": 52960, "start": 548.16, "end": 550.32, "text": " and then maybe language models", "tokens": [51292, 293, 550, 1310, 2856, 5245, 51400], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 185, "seek": 52960, "start": 550.32, "end": 551.76, "text": " are another layer of abstraction", "tokens": [51400, 366, 1071, 4583, 295, 37765, 51472], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 186, "seek": 52960, "start": 551.76, "end": 554.0, "text": " where it's natural language interface.", "tokens": [51472, 689, 309, 311, 3303, 2856, 9226, 13, 51584], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 187, "seek": 52960, "start": 554.0, "end": 556.28, "text": " This could be seen as, again,", "tokens": [51584, 639, 727, 312, 1612, 382, 11, 797, 11, 51698], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 188, "seek": 52960, "start": 556.28, "end": 557.96, "text": " another layer of abstraction,", "tokens": [51698, 1071, 4583, 295, 37765, 11, 51782], "temperature": 0.0, "avg_logprob": -0.11758757038276736, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.002395617077127099}, {"id": 189, "seek": 55796, "start": 557.96, "end": 559.84, "text": " where instead of looking at interacting", "tokens": [50364, 689, 2602, 295, 1237, 412, 18017, 50458], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 190, "seek": 55796, "start": 559.84, "end": 561.4000000000001, "text": " with the language model directly,", "tokens": [50458, 365, 264, 2856, 2316, 3838, 11, 50536], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 191, "seek": 55796, "start": 561.4000000000001, "end": 563.72, "text": " it is now a type of interpreter,", "tokens": [50536, 309, 307, 586, 257, 2010, 295, 34132, 11, 50652], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 192, "seek": 55796, "start": 563.72, "end": 565.52, "text": " but this is the agents", "tokens": [50652, 457, 341, 307, 264, 12554, 50742], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 193, "seek": 55796, "start": 565.52, "end": 569.1600000000001, "text": " and the multi-agent framework on top of it.", "tokens": [50742, 293, 264, 4825, 12, 559, 317, 8388, 322, 1192, 295, 309, 13, 50924], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 194, "seek": 55796, "start": 569.1600000000001, "end": 570.48, "text": " So that's my intuition.", "tokens": [50924, 407, 300, 311, 452, 24002, 13, 50990], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 195, "seek": 55796, "start": 570.48, "end": 572.76, "text": " Do you agree with that or disagree?", "tokens": [50990, 1144, 291, 3986, 365, 300, 420, 14091, 30, 51104], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 196, "seek": 55796, "start": 572.76, "end": 576.1600000000001, "text": " Or like, how do you think of those layers of abstraction?", "tokens": [51104, 1610, 411, 11, 577, 360, 291, 519, 295, 729, 7914, 295, 37765, 30, 51274], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 197, "seek": 55796, "start": 576.1600000000001, "end": 578.96, "text": " Yeah, that's a fantastic question.", "tokens": [51274, 865, 11, 300, 311, 257, 5456, 1168, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 198, "seek": 55796, "start": 578.96, "end": 581.76, "text": " The abstraction is indeed at a higher level", "tokens": [51414, 440, 37765, 307, 6451, 412, 257, 2946, 1496, 51554], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 199, "seek": 55796, "start": 581.76, "end": 583.6, "text": " of the agent abstraction.", "tokens": [51554, 295, 264, 9461, 37765, 13, 51646], "temperature": 0.0, "avg_logprob": -0.12785830191516, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0004952498129568994}, {"id": 200, "seek": 58360, "start": 584.6, "end": 588.0400000000001, "text": " It unifies a number of different things.", "tokens": [50414, 467, 517, 11221, 257, 1230, 295, 819, 721, 13, 50586], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 201, "seek": 58360, "start": 588.0400000000001, "end": 590.2, "text": " One is larger models.", "tokens": [50586, 1485, 307, 4833, 5245, 13, 50694], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 202, "seek": 58360, "start": 590.2, "end": 593.0400000000001, "text": " So when we use a single instance of a larger model,", "tokens": [50694, 407, 562, 321, 764, 257, 2167, 5197, 295, 257, 4833, 2316, 11, 50836], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 203, "seek": 58360, "start": 593.0400000000001, "end": 595.2, "text": " we usually do prompt engineering", "tokens": [50836, 321, 2673, 360, 12391, 7043, 50944], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 204, "seek": 58360, "start": 595.2, "end": 597.96, "text": " and try to give some input text", "tokens": [50944, 293, 853, 281, 976, 512, 4846, 2487, 51082], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 205, "seek": 58360, "start": 597.96, "end": 599.88, "text": " and get some output text out of it.", "tokens": [51082, 293, 483, 512, 5598, 2487, 484, 295, 309, 13, 51178], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 206, "seek": 58360, "start": 600.9200000000001, "end": 605.08, "text": " This agent abstraction can encapsulate that underneath", "tokens": [51230, 639, 9461, 37765, 393, 38745, 5256, 300, 7223, 51438], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 207, "seek": 58360, "start": 605.08, "end": 607.76, "text": " and provide a more intuitive way", "tokens": [51438, 293, 2893, 257, 544, 21769, 636, 51572], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 208, "seek": 58360, "start": 607.76, "end": 612.1600000000001, "text": " to think of it as an agent that can converse with you.", "tokens": [51572, 281, 519, 295, 309, 382, 364, 9461, 300, 393, 416, 4308, 365, 291, 13, 51792], "temperature": 0.0, "avg_logprob": -0.20254392121967515, "compression_ratio": 1.6126126126126126, "no_speech_prob": 0.0008964458247646689}, {"id": 209, "seek": 61216, "start": 612.16, "end": 617.16, "text": " So not just as one single text completion inference anymore.", "tokens": [50364, 407, 406, 445, 382, 472, 2167, 2487, 19372, 38253, 3602, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 210, "seek": 61216, "start": 617.64, "end": 621.4, "text": " It can do tasks, can persist some states", "tokens": [50638, 467, 393, 360, 9608, 11, 393, 13233, 512, 4368, 50826], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 211, "seek": 61216, "start": 621.4, "end": 624.24, "text": " and continue to take your feedback", "tokens": [50826, 293, 2354, 281, 747, 428, 5824, 50968], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 212, "seek": 61216, "start": 624.24, "end": 628.48, "text": " and produce more refined result and so on.", "tokens": [50968, 293, 5258, 544, 26201, 1874, 293, 370, 322, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 213, "seek": 61216, "start": 628.48, "end": 632.3199999999999, "text": " So that's a larger model-based agent.", "tokens": [51180, 407, 300, 311, 257, 4833, 2316, 12, 6032, 9461, 13, 51372], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 214, "seek": 61216, "start": 633.4, "end": 637.04, "text": " There are two other kinds of backhands", "tokens": [51426, 821, 366, 732, 661, 3685, 295, 646, 71, 2967, 51608], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 215, "seek": 61216, "start": 637.04, "end": 638.12, "text": " that can be encapsulated.", "tokens": [51608, 300, 393, 312, 38745, 6987, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2358974908527575, "compression_ratio": 1.4842105263157894, "no_speech_prob": 0.0005700198234990239}, {"id": 216, "seek": 63812, "start": 638.12, "end": 642.2, "text": " One is, you can think of it as programming language", "tokens": [50364, 1485, 307, 11, 291, 393, 519, 295, 309, 382, 9410, 2856, 50568], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 217, "seek": 63812, "start": 642.2, "end": 646.6, "text": " or tool-based agent, which doesn't use a larger model,", "tokens": [50568, 420, 2290, 12, 6032, 9461, 11, 597, 1177, 380, 764, 257, 4833, 2316, 11, 50788], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 218, "seek": 63812, "start": 646.6, "end": 650.32, "text": " but they can still perform very useful actions.", "tokens": [50788, 457, 436, 393, 920, 2042, 588, 4420, 5909, 13, 50974], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 219, "seek": 63812, "start": 650.32, "end": 652.44, "text": " They can do code execution, for example,", "tokens": [50974, 814, 393, 360, 3089, 15058, 11, 337, 1365, 11, 51080], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 220, "seek": 63812, "start": 652.44, "end": 655.04, "text": " or it can execute predefined functions", "tokens": [51080, 420, 309, 393, 14483, 659, 37716, 6828, 51210], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 221, "seek": 63812, "start": 655.04, "end": 658.88, "text": " or it can basically execute any programming logic", "tokens": [51210, 420, 309, 393, 1936, 14483, 604, 9410, 9952, 51402], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 222, "seek": 63812, "start": 658.88, "end": 662.28, "text": " you've defined there.", "tokens": [51402, 291, 600, 7642, 456, 13, 51572], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 223, "seek": 63812, "start": 662.28, "end": 666.88, "text": " And third one is the human kind of backed agents.", "tokens": [51572, 400, 2636, 472, 307, 264, 1952, 733, 295, 20391, 12554, 13, 51802], "temperature": 0.0, "avg_logprob": -0.2065310063569442, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0003458350256551057}, {"id": 224, "seek": 66688, "start": 666.88, "end": 670.84, "text": " So these agents can be considered as some kind of user proxy.", "tokens": [50364, 407, 613, 12554, 393, 312, 4888, 382, 512, 733, 295, 4195, 29690, 13, 50562], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 225, "seek": 66688, "start": 672.12, "end": 676.72, "text": " So when they need human input, the human can take over", "tokens": [50626, 407, 562, 436, 643, 1952, 4846, 11, 264, 1952, 393, 747, 670, 50856], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 226, "seek": 66688, "start": 676.72, "end": 681.72, "text": " and just participate the multi-agent workflow", "tokens": [50856, 293, 445, 8197, 264, 4825, 12, 559, 317, 20993, 51106], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 227, "seek": 66688, "start": 681.76, "end": 683.36, "text": " as one of the agents.", "tokens": [51108, 382, 472, 295, 264, 12554, 13, 51188], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 228, "seek": 66688, "start": 683.36, "end": 685.52, "text": " So you can see about several agents.", "tokens": [51188, 407, 291, 393, 536, 466, 2940, 12554, 13, 51296], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 229, "seek": 66688, "start": 685.52, "end": 688.52, "text": " Some of them are larger model-based, some are tool-based,", "tokens": [51296, 2188, 295, 552, 366, 4833, 2316, 12, 6032, 11, 512, 366, 2290, 12, 6032, 11, 51446], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 230, "seek": 66688, "start": 688.52, "end": 691.88, "text": " others are human-based.", "tokens": [51446, 2357, 366, 1952, 12, 6032, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 231, "seek": 66688, "start": 691.88, "end": 695.4, "text": " So then they can just cooperate together", "tokens": [51614, 407, 550, 436, 393, 445, 26667, 1214, 51790], "temperature": 0.0, "avg_logprob": -0.18390801612367022, "compression_ratio": 1.7373737373737375, "no_speech_prob": 0.00012145931395934895}, {"id": 232, "seek": 69540, "start": 695.4399999999999, "end": 696.88, "text": " through a very natural interface,", "tokens": [50366, 807, 257, 588, 3303, 9226, 11, 50438], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 233, "seek": 69540, "start": 696.88, "end": 700.72, "text": " which is a conversational interface.", "tokens": [50438, 597, 307, 257, 2615, 1478, 9226, 13, 50630], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 234, "seek": 69540, "start": 700.72, "end": 704.24, "text": " So that's kind of layer of abstraction we provide.", "tokens": [50630, 407, 300, 311, 733, 295, 4583, 295, 37765, 321, 2893, 13, 50806], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 235, "seek": 69540, "start": 705.36, "end": 708.04, "text": " Yeah, I appreciate that.", "tokens": [50862, 865, 11, 286, 4449, 300, 13, 50996], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 236, "seek": 69540, "start": 708.04, "end": 709.56, "text": " And I'm reminded of during,", "tokens": [50996, 400, 286, 478, 15920, 295, 1830, 11, 51072], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 237, "seek": 69540, "start": 709.56, "end": 712.48, "text": " I think it was the Ignite keynote speech Satya Nadella said,", "tokens": [51072, 286, 519, 309, 390, 264, 24754, 642, 33896, 6218, 5344, 3016, 426, 762, 3505, 848, 11, 51218], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 238, "seek": 69540, "start": 712.48, "end": 713.92, "text": " think of it as a reasoning engine", "tokens": [51218, 519, 295, 309, 382, 257, 21577, 2848, 51290], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 239, "seek": 69540, "start": 713.92, "end": 715.68, "text": " and a natural language interface.", "tokens": [51290, 293, 257, 3303, 2856, 9226, 13, 51378], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 240, "seek": 69540, "start": 715.68, "end": 717.3199999999999, "text": " And that was like the two simplest ways", "tokens": [51378, 400, 300, 390, 411, 264, 732, 22811, 2098, 51460], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 241, "seek": 69540, "start": 717.3199999999999, "end": 721.16, "text": " to think of generative AI, at least the language side.", "tokens": [51460, 281, 519, 295, 1337, 1166, 7318, 11, 412, 1935, 264, 2856, 1252, 13, 51652], "temperature": 0.0, "avg_logprob": -0.19588809932043794, "compression_ratio": 1.6446280991735538, "no_speech_prob": 0.0010976691264659166}, {"id": 242, "seek": 72116, "start": 722.16, "end": 725.76, "text": " So the other topic that I wanted to ask about", "tokens": [50414, 407, 264, 661, 4829, 300, 286, 1415, 281, 1029, 466, 50594], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 243, "seek": 72116, "start": 725.76, "end": 728.0, "text": " to kind of dig into was thinking about it", "tokens": [50594, 281, 733, 295, 2528, 666, 390, 1953, 466, 309, 50706], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 244, "seek": 72116, "start": 728.0, "end": 730.1999999999999, "text": " as a kind of automation.", "tokens": [50706, 382, 257, 733, 295, 17769, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 245, "seek": 72116, "start": 730.1999999999999, "end": 733.92, "text": " So there's the agent-based, there's the tools-based,", "tokens": [50816, 407, 456, 311, 264, 9461, 12, 6032, 11, 456, 311, 264, 3873, 12, 6032, 11, 51002], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 246, "seek": 72116, "start": 733.92, "end": 735.9599999999999, "text": " but then overall kind of,", "tokens": [51002, 457, 550, 4787, 733, 295, 11, 51104], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 247, "seek": 72116, "start": 735.9599999999999, "end": 738.8399999999999, "text": " and this is a messaging kind of a framing", "tokens": [51104, 293, 341, 307, 257, 21812, 733, 295, 257, 28971, 51248], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 248, "seek": 72116, "start": 738.8399999999999, "end": 741.64, "text": " that I've adopted when talking to people is,", "tokens": [51248, 300, 286, 600, 12175, 562, 1417, 281, 561, 307, 11, 51388], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 249, "seek": 72116, "start": 741.64, "end": 743.56, "text": " and of course it's an oversimplification,", "tokens": [51388, 293, 295, 1164, 309, 311, 364, 15488, 332, 564, 3774, 11, 51484], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 250, "seek": 72116, "start": 743.56, "end": 748.28, "text": " but AI is, one, AI is not new,", "tokens": [51484, 457, 7318, 307, 11, 472, 11, 7318, 307, 406, 777, 11, 51720], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 251, "seek": 72116, "start": 748.28, "end": 749.9599999999999, "text": " like machine learning has been around for a while,", "tokens": [51720, 411, 3479, 2539, 575, 668, 926, 337, 257, 1339, 11, 51804], "temperature": 0.0, "avg_logprob": -0.11544150665026753, "compression_ratio": 1.7106382978723405, "no_speech_prob": 0.0012640153290703893}, {"id": 252, "seek": 74996, "start": 749.96, "end": 752.6, "text": " this is just a step function in terms of capabilities", "tokens": [50364, 341, 307, 445, 257, 1823, 2445, 294, 2115, 295, 10862, 50496], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 253, "seek": 74996, "start": 752.6, "end": 755.0400000000001, "text": " that models have.", "tokens": [50496, 300, 5245, 362, 13, 50618], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 254, "seek": 74996, "start": 755.0400000000001, "end": 758.08, "text": " But it's also just the simplest way to think about it", "tokens": [50618, 583, 309, 311, 611, 445, 264, 22811, 636, 281, 519, 466, 309, 50770], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 255, "seek": 74996, "start": 758.08, "end": 760.96, "text": " from a production standpoint or from a software standpoint,", "tokens": [50770, 490, 257, 4265, 15827, 420, 490, 257, 4722, 15827, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 256, "seek": 74996, "start": 760.96, "end": 763.88, "text": " is it's a new suite of automation tools, right?", "tokens": [50914, 307, 309, 311, 257, 777, 14205, 295, 17769, 3873, 11, 558, 30, 51060], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 257, "seek": 74996, "start": 763.88, "end": 765.12, "text": " That's kind of how I think of it.", "tokens": [51060, 663, 311, 733, 295, 577, 286, 519, 295, 309, 13, 51122], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 258, "seek": 74996, "start": 765.12, "end": 767.4000000000001, "text": " Is that a fair characterization", "tokens": [51122, 1119, 300, 257, 3143, 49246, 51236], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 259, "seek": 74996, "start": 767.4000000000001, "end": 769.36, "text": " or is there something that I'm missing from that?", "tokens": [51236, 420, 307, 456, 746, 300, 286, 478, 5361, 490, 300, 30, 51334], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 260, "seek": 74996, "start": 769.36, "end": 772.48, "text": " Cause I do feel like there might be something missing", "tokens": [51334, 10865, 286, 360, 841, 411, 456, 1062, 312, 746, 5361, 51490], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 261, "seek": 74996, "start": 772.48, "end": 774.88, "text": " or something that that characterization", "tokens": [51490, 420, 746, 300, 300, 49246, 51610], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 262, "seek": 74996, "start": 774.88, "end": 776.2, "text": " doesn't fully convey,", "tokens": [51610, 1177, 380, 4498, 16965, 11, 51676], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 263, "seek": 74996, "start": 776.2, "end": 779.12, "text": " but it is still fundamentally new automation, right?", "tokens": [51676, 457, 309, 307, 920, 17879, 777, 17769, 11, 558, 30, 51822], "temperature": 0.0, "avg_logprob": -0.13454864208514875, "compression_ratio": 1.8111888111888113, "no_speech_prob": 0.0011875373311340809}, {"id": 264, "seek": 77912, "start": 780.08, "end": 784.16, "text": " So if I try to understand it from an automation point of view,", "tokens": [50412, 407, 498, 286, 853, 281, 1223, 309, 490, 364, 17769, 935, 295, 1910, 11, 50616], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 265, "seek": 77912, "start": 784.16, "end": 789.16, "text": " I think agent is fundamentally a automation concept.", "tokens": [50616, 286, 519, 9461, 307, 17879, 257, 17769, 3410, 13, 50866], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 266, "seek": 77912, "start": 790.88, "end": 794.04, "text": " The automation is more like about,", "tokens": [50952, 440, 17769, 307, 544, 411, 466, 11, 51110], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 267, "seek": 77912, "start": 794.04, "end": 798.36, "text": " instead of giving every detailed instructions using code,", "tokens": [51110, 2602, 295, 2902, 633, 9942, 9415, 1228, 3089, 11, 51326], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 268, "seek": 77912, "start": 798.36, "end": 801.32, "text": " I say step one, do this, step two, do that,", "tokens": [51326, 286, 584, 1823, 472, 11, 360, 341, 11, 1823, 732, 11, 360, 300, 11, 51474], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 269, "seek": 77912, "start": 801.32, "end": 806.32, "text": " using from precisely defined program language specification.", "tokens": [51474, 1228, 490, 13402, 7642, 1461, 2856, 31256, 13, 51724], "temperature": 0.0, "avg_logprob": -0.2695409647623698, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.0005273118731565773}, {"id": 270, "seek": 80632, "start": 806.32, "end": 811.32, "text": " And now we can make some more vague specification", "tokens": [50364, 400, 586, 321, 393, 652, 512, 544, 24247, 31256, 50614], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 271, "seek": 80632, "start": 812.5200000000001, "end": 815.24, "text": " using like natural language specification,", "tokens": [50674, 1228, 411, 3303, 2856, 31256, 11, 50810], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 272, "seek": 80632, "start": 815.24, "end": 817.88, "text": " say I want to accomplish such a task", "tokens": [50810, 584, 286, 528, 281, 9021, 1270, 257, 5633, 50942], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 273, "seek": 80632, "start": 817.88, "end": 820.24, "text": " and could the agent do it for me?", "tokens": [50942, 293, 727, 264, 9461, 360, 309, 337, 385, 30, 51060], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 274, "seek": 80632, "start": 820.24, "end": 825.24, "text": " So, and then underneath the agent needs to kind of break it down", "tokens": [51060, 407, 11, 293, 550, 7223, 264, 9461, 2203, 281, 733, 295, 1821, 309, 760, 51310], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 275, "seek": 80632, "start": 825.24, "end": 830.24, "text": " a big complex task into maybe smaller, solvable tasks.", "tokens": [51310, 257, 955, 3997, 5633, 666, 1310, 4356, 11, 1404, 17915, 9608, 13, 51560], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 276, "seek": 80632, "start": 830.7600000000001, "end": 835.5600000000001, "text": " And until each task can be conveniently solved", "tokens": [51586, 400, 1826, 1184, 5633, 393, 312, 44375, 13041, 51826], "temperature": 0.0, "avg_logprob": -0.21924481740811977, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.00027348450385034084}, {"id": 277, "seek": 83556, "start": 835.56, "end": 840.56, "text": " by a simple inference and produce a corresponding code", "tokens": [50364, 538, 257, 2199, 38253, 293, 5258, 257, 11760, 3089, 50614], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 278, "seek": 83556, "start": 841.64, "end": 842.7199999999999, "text": " to solve that task.", "tokens": [50668, 281, 5039, 300, 5633, 13, 50722], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 279, "seek": 83556, "start": 842.7199999999999, "end": 846.4399999999999, "text": " And then eventually we need to recompose them,", "tokens": [50722, 400, 550, 4728, 321, 643, 281, 48000, 541, 552, 11, 50908], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 280, "seek": 83556, "start": 846.4399999999999, "end": 848.0, "text": " all of these intermediate steps", "tokens": [50908, 439, 295, 613, 19376, 4439, 50986], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 281, "seek": 83556, "start": 848.0, "end": 850.4799999999999, "text": " and get to the final output back.", "tokens": [50986, 293, 483, 281, 264, 2572, 5598, 646, 13, 51110], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 282, "seek": 83556, "start": 850.4799999999999, "end": 853.2399999999999, "text": " So that is one part of the automation story.", "tokens": [51110, 407, 300, 307, 472, 644, 295, 264, 17769, 1657, 13, 51248], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 283, "seek": 83556, "start": 854.2399999999999, "end": 857.0, "text": " And another part of it is,", "tokens": [51298, 400, 1071, 644, 295, 309, 307, 11, 51436], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 284, "seek": 83556, "start": 857.0, "end": 859.4, "text": " think about this automating some tasks", "tokens": [51436, 519, 466, 341, 3553, 990, 512, 9608, 51556], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 285, "seek": 83556, "start": 859.4, "end": 861.88, "text": " that human had to do.", "tokens": [51556, 300, 1952, 632, 281, 360, 13, 51680], "temperature": 0.0, "avg_logprob": -0.20683007952810703, "compression_ratio": 1.6326530612244898, "no_speech_prob": 0.00037979873013682663}, {"id": 286, "seek": 86188, "start": 862.08, "end": 867.08, "text": " AutoGen really started with some very simple kind of automation.", "tokens": [50374, 13738, 26647, 534, 1409, 365, 512, 588, 2199, 733, 295, 17769, 13, 50624], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 287, "seek": 86188, "start": 868.56, "end": 870.84, "text": " Just think about how you use chatGBT.", "tokens": [50698, 1449, 519, 466, 577, 291, 764, 5081, 8769, 51, 13, 50812], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 288, "seek": 86188, "start": 872.0, "end": 875.6, "text": " You as a human need to ask questions", "tokens": [50870, 509, 382, 257, 1952, 643, 281, 1029, 1651, 51050], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 289, "seek": 86188, "start": 875.6, "end": 877.88, "text": " and chatGBT gave you some answer.", "tokens": [51050, 293, 5081, 8769, 51, 2729, 291, 512, 1867, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 290, "seek": 86188, "start": 877.88, "end": 879.68, "text": " Sometimes it gave you the code", "tokens": [51164, 4803, 309, 2729, 291, 264, 3089, 51254], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 291, "seek": 86188, "start": 879.68, "end": 882.32, "text": " and then human need to take that code", "tokens": [51254, 293, 550, 1952, 643, 281, 747, 300, 3089, 51386], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 292, "seek": 86188, "start": 882.32, "end": 886.96, "text": " and run it by yourself and get some result.", "tokens": [51386, 293, 1190, 309, 538, 1803, 293, 483, 512, 1874, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 293, "seek": 86188, "start": 886.96, "end": 889.08, "text": " If it's not correct, you send it back", "tokens": [51618, 759, 309, 311, 406, 3006, 11, 291, 2845, 309, 646, 51724], "temperature": 0.0, "avg_logprob": -0.2722803472162603, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.0007433510618284345}, {"id": 294, "seek": 88908, "start": 889.08, "end": 892.9200000000001, "text": " and chatGBT gave you some results for the game.", "tokens": [50364, 293, 5081, 8769, 51, 2729, 291, 512, 3542, 337, 264, 1216, 13, 50556], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 295, "seek": 88908, "start": 892.9200000000001, "end": 895.1600000000001, "text": " And here in this kind of interaction,", "tokens": [50556, 400, 510, 294, 341, 733, 295, 9285, 11, 50668], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 296, "seek": 88908, "start": 895.1600000000001, "end": 897.24, "text": " human students do a lot of work,", "tokens": [50668, 1952, 1731, 360, 257, 688, 295, 589, 11, 50772], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 297, "seek": 88908, "start": 897.24, "end": 902.24, "text": " but many of the work can be automated if we use agent.", "tokens": [50772, 457, 867, 295, 264, 589, 393, 312, 18473, 498, 321, 764, 9461, 13, 51022], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 298, "seek": 88908, "start": 903.12, "end": 906.76, "text": " And even some kind of human feedback,", "tokens": [51066, 400, 754, 512, 733, 295, 1952, 5824, 11, 51248], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 299, "seek": 88908, "start": 906.76, "end": 911.76, "text": " like non-code, if I don't like the results,", "tokens": [51248, 411, 2107, 12, 22332, 11, 498, 286, 500, 380, 411, 264, 3542, 11, 51498], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 300, "seek": 88908, "start": 912.96, "end": 916.0400000000001, "text": " but I know my preferences, I will tell it,", "tokens": [51558, 457, 286, 458, 452, 21910, 11, 286, 486, 980, 309, 11, 51712], "temperature": 0.0, "avg_logprob": -0.25516717889335716, "compression_ratio": 1.5360824742268042, "no_speech_prob": 0.0018950155936181545}, {"id": 301, "seek": 91604, "start": 916.04, "end": 921.04, "text": " such as change the chart from using a dollar to percentage", "tokens": [50364, 1270, 382, 1319, 264, 6927, 490, 1228, 257, 7241, 281, 9668, 50614], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 302, "seek": 91604, "start": 921.4, "end": 923.24, "text": " that kind of requirement,", "tokens": [50632, 300, 733, 295, 11695, 11, 50724], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 303, "seek": 91604, "start": 923.24, "end": 927.4, "text": " or teach me this lesson,", "tokens": [50724, 420, 2924, 385, 341, 6898, 11, 50932], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 304, "seek": 91604, "start": 927.4, "end": 928.8, "text": " teach me about math,", "tokens": [50932, 2924, 385, 466, 5221, 11, 51002], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 305, "seek": 91604, "start": 928.8, "end": 931.7199999999999, "text": " but using a concrete money example, right?", "tokens": [51002, 457, 1228, 257, 9859, 1460, 1365, 11, 558, 30, 51148], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 306, "seek": 91604, "start": 931.7199999999999, "end": 933.64, "text": " So those kind of requirements", "tokens": [51148, 407, 729, 733, 295, 7728, 51244], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 307, "seek": 91604, "start": 933.64, "end": 937.4, "text": " can be somewhat automated using all different ways.", "tokens": [51244, 393, 312, 8344, 18473, 1228, 439, 819, 2098, 13, 51432], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 308, "seek": 91604, "start": 937.4, "end": 939.88, "text": " Sometimes we can use larger models.", "tokens": [51432, 4803, 321, 393, 764, 4833, 5245, 13, 51556], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 309, "seek": 91604, "start": 940.68, "end": 945.0, "text": " Sometimes we can use some retrieval augmented approach", "tokens": [51596, 4803, 321, 393, 764, 512, 19817, 3337, 36155, 3109, 51812], "temperature": 0.0, "avg_logprob": -0.3043747391811637, "compression_ratio": 1.7128712871287128, "no_speech_prob": 0.0013245472218841314}, {"id": 310, "seek": 94500, "start": 945.0, "end": 950.0, "text": " to inform retrieval to get the knowledge from somewhere.", "tokens": [50364, 281, 1356, 19817, 3337, 281, 483, 264, 3601, 490, 4079, 13, 50614], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 311, "seek": 94500, "start": 951.12, "end": 955.48, "text": " That's another kind of interesting automation", "tokens": [50670, 663, 311, 1071, 733, 295, 1880, 17769, 50888], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 312, "seek": 94500, "start": 955.48, "end": 957.16, "text": " that we can make.", "tokens": [50888, 300, 321, 393, 652, 13, 50972], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 313, "seek": 94500, "start": 957.16, "end": 960.4, "text": " Yeah, and within that, those automation stories,", "tokens": [50972, 865, 11, 293, 1951, 300, 11, 729, 17769, 3676, 11, 51134], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 314, "seek": 94500, "start": 960.4, "end": 964.0, "text": " because some of the examples,", "tokens": [51134, 570, 512, 295, 264, 5110, 11, 51314], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 315, "seek": 94500, "start": 964.0, "end": 967.68, "text": " what was it, how did you say like a vague specification,", "tokens": [51314, 437, 390, 309, 11, 577, 630, 291, 584, 411, 257, 24247, 31256, 11, 51498], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 316, "seek": 94500, "start": 967.68, "end": 969.2, "text": " right, a natural language specification", "tokens": [51498, 558, 11, 257, 3303, 2856, 31256, 51574], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 317, "seek": 94500, "start": 969.2, "end": 972.32, "text": " that is not quite as rigorous as software development", "tokens": [51574, 300, 307, 406, 1596, 382, 29882, 382, 4722, 3250, 51730], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 318, "seek": 94500, "start": 972.32, "end": 974.12, "text": " might have required in the past?", "tokens": [51730, 1062, 362, 4739, 294, 264, 1791, 30, 51820], "temperature": 0.0, "avg_logprob": -0.27488412248327376, "compression_ratio": 1.6092436974789917, "no_speech_prob": 0.0008818829664960504}, {"id": 319, "seek": 97412, "start": 974.12, "end": 976.04, "text": " And then of course, with these models,", "tokens": [50364, 400, 550, 295, 1164, 11, 365, 613, 5245, 11, 50460], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 320, "seek": 97412, "start": 976.04, "end": 979.36, "text": " they have the ability to kind of think through it,", "tokens": [50460, 436, 362, 264, 3485, 281, 733, 295, 519, 807, 309, 11, 50626], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 321, "seek": 97412, "start": 979.36, "end": 981.92, "text": " or break it down into steps.", "tokens": [50626, 420, 1821, 309, 760, 666, 4439, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 322, "seek": 97412, "start": 981.92, "end": 985.88, "text": " So with all that, and some of the work that I have found is,", "tokens": [50754, 407, 365, 439, 300, 11, 293, 512, 295, 264, 589, 300, 286, 362, 1352, 307, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 323, "seek": 97412, "start": 985.88, "end": 988.12, "text": " or some of the problems that I've confronted,", "tokens": [50952, 420, 512, 295, 264, 2740, 300, 286, 600, 31257, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 324, "seek": 97412, "start": 988.12, "end": 989.96, "text": " because it can think in general principles,", "tokens": [51064, 570, 309, 393, 519, 294, 2674, 9156, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 325, "seek": 97412, "start": 989.96, "end": 991.96, "text": " it does know a lot about software development,", "tokens": [51156, 309, 775, 458, 257, 688, 466, 4722, 3250, 11, 51256], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 326, "seek": 97412, "start": 991.96, "end": 994.04, "text": " and I mean, the language models know a lot", "tokens": [51256, 293, 286, 914, 11, 264, 2856, 5245, 458, 257, 688, 51360], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 327, "seek": 97412, "start": 994.04, "end": 995.2, "text": " about a lot of things.", "tokens": [51360, 466, 257, 688, 295, 721, 13, 51418], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 328, "seek": 97412, "start": 995.2, "end": 998.48, "text": " So with respect to autogen,", "tokens": [51418, 407, 365, 3104, 281, 1476, 8799, 11, 51582], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 329, "seek": 97412, "start": 998.48, "end": 1001.64, "text": " and kind of getting to where you're at today,", "tokens": [51582, 293, 733, 295, 1242, 281, 689, 291, 434, 412, 965, 11, 51740], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 330, "seek": 97412, "start": 1001.64, "end": 1003.84, "text": " what are some of the biggest challenges", "tokens": [51740, 437, 366, 512, 295, 264, 3880, 4759, 51850], "temperature": 0.0, "avg_logprob": -0.1404397226598141, "compression_ratio": 1.7906137184115523, "no_speech_prob": 0.000314926786813885}, {"id": 331, "seek": 100384, "start": 1003.84, "end": 1006.0, "text": " that you've overcome so far, or that you haven't overcome?", "tokens": [50364, 300, 291, 600, 10473, 370, 1400, 11, 420, 300, 291, 2378, 380, 10473, 30, 50472], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 332, "seek": 100384, "start": 1006.0, "end": 1008.5600000000001, "text": " Maybe that would be a more interesting story.", "tokens": [50472, 2704, 300, 576, 312, 257, 544, 1880, 1657, 13, 50600], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 333, "seek": 100384, "start": 1008.5600000000001, "end": 1011.84, "text": " Yeah, sure, I could talk about both.", "tokens": [50600, 865, 11, 988, 11, 286, 727, 751, 466, 1293, 13, 50764], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 334, "seek": 100384, "start": 1011.84, "end": 1014.5600000000001, "text": " For what we have overcome,", "tokens": [50764, 1171, 437, 321, 362, 10473, 11, 50900], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 335, "seek": 100384, "start": 1015.64, "end": 1020.64, "text": " I think we have kind of figured out the abstraction", "tokens": [50954, 286, 519, 321, 362, 733, 295, 8932, 484, 264, 37765, 51204], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 336, "seek": 100384, "start": 1020.72, "end": 1025.24, "text": " to the earlier about how to unify these different types", "tokens": [51208, 281, 264, 3071, 466, 577, 281, 517, 2505, 613, 819, 3467, 51434], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 337, "seek": 100384, "start": 1025.24, "end": 1030.24, "text": " of capabilities, different ways of making them work together.", "tokens": [51434, 295, 10862, 11, 819, 2098, 295, 1455, 552, 589, 1214, 13, 51684], "temperature": 0.0, "avg_logprob": -0.18783472872328486, "compression_ratio": 1.6018957345971565, "no_speech_prob": 0.0007318546995520592}, {"id": 338, "seek": 103024, "start": 1031.08, "end": 1034.96, "text": " We'll have found one very simple interface", "tokens": [50406, 492, 603, 362, 1352, 472, 588, 2199, 9226, 50600], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 339, "seek": 103024, "start": 1034.96, "end": 1039.1200000000001, "text": " that kind of accommodate a variety of different communication", "tokens": [50600, 300, 733, 295, 21410, 257, 5673, 295, 819, 6101, 50808], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 340, "seek": 103024, "start": 1039.1200000000001, "end": 1039.96, "text": " patterns.", "tokens": [50808, 8294, 13, 50850], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 341, "seek": 103024, "start": 1039.96, "end": 1042.8, "text": " So one example is the, so there are several examples.", "tokens": [50850, 407, 472, 1365, 307, 264, 11, 370, 456, 366, 2940, 5110, 13, 50992], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 342, "seek": 103024, "start": 1042.8, "end": 1045.88, "text": " One is the simple like one-to-one conversation.", "tokens": [50992, 1485, 307, 264, 2199, 411, 472, 12, 1353, 12, 546, 3761, 13, 51146], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 343, "seek": 103024, "start": 1047.08, "end": 1052.08, "text": " Another is hierarchical chat, like suppose one agent", "tokens": [51206, 3996, 307, 35250, 804, 5081, 11, 411, 7297, 472, 9461, 51456], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 344, "seek": 103024, "start": 1052.08, "end": 1056.28, "text": " is more sit on top and talk to several sub-agents,", "tokens": [51456, 307, 544, 1394, 322, 1192, 293, 751, 281, 2940, 1422, 12, 559, 791, 11, 51666], "temperature": 0.0, "avg_logprob": -0.4790416855409921, "compression_ratio": 1.592039800995025, "no_speech_prob": 0.0015726673882454634}, {"id": 345, "seek": 105628, "start": 1057.28, "end": 1061.24, "text": " it manages, and they can be nested structure,", "tokens": [50414, 309, 22489, 11, 293, 436, 393, 312, 15646, 292, 3877, 11, 50612], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 346, "seek": 105628, "start": 1061.24, "end": 1062.36, "text": " hierarchical structure.", "tokens": [50612, 35250, 804, 3877, 13, 50668], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 347, "seek": 105628, "start": 1063.36, "end": 1068.12, "text": " And another example is multiple one-on-one joint chat.", "tokens": [50718, 400, 1071, 1365, 307, 3866, 472, 12, 266, 12, 546, 7225, 5081, 13, 50956], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 348, "seek": 105628, "start": 1068.12, "end": 1072.8799999999999, "text": " So there's no one that is strictly sit on top.", "tokens": [50956, 407, 456, 311, 572, 472, 300, 307, 20792, 1394, 322, 1192, 13, 51194], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 349, "seek": 105628, "start": 1072.8799999999999, "end": 1075.36, "text": " Everyone talks to the other else,", "tokens": [51194, 5198, 6686, 281, 264, 661, 1646, 11, 51318], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 350, "seek": 105628, "start": 1075.36, "end": 1079.44, "text": " but it's multiple one-on-one which are connected.", "tokens": [51318, 457, 309, 311, 3866, 472, 12, 266, 12, 546, 597, 366, 4582, 13, 51522], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 351, "seek": 105628, "start": 1079.44, "end": 1081.32, "text": " So I talk to you, you talk to Katie,", "tokens": [51522, 407, 286, 751, 281, 291, 11, 291, 751, 281, 19602, 11, 51616], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 352, "seek": 105628, "start": 1081.32, "end": 1084.32, "text": " you can talk to me in this kind of triangle", "tokens": [51616, 291, 393, 751, 281, 385, 294, 341, 733, 295, 13369, 51766], "temperature": 0.0, "avg_logprob": -0.2799343340324633, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.00036823994014412165}, {"id": 353, "seek": 108432, "start": 1084.36, "end": 1086.36, "text": " or multiple joint chat.", "tokens": [50366, 420, 3866, 7225, 5081, 13, 50466], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 354, "seek": 108432, "start": 1086.36, "end": 1091.28, "text": " And there's also a group chat, meaning it's not a one-on-one", "tokens": [50466, 400, 456, 311, 611, 257, 1594, 5081, 11, 3620, 309, 311, 406, 257, 472, 12, 266, 12, 546, 50712], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 355, "seek": 108432, "start": 1091.28, "end": 1092.24, "text": " trend anymore.", "tokens": [50712, 6028, 3602, 13, 50760], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 356, "seek": 108432, "start": 1092.24, "end": 1095.1599999999999, "text": " So everybody send messages to everyone else.", "tokens": [50760, 407, 2201, 2845, 7897, 281, 1518, 1646, 13, 50906], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 357, "seek": 108432, "start": 1095.1599999999999, "end": 1099.56, "text": " So we see each other's message and there's a hidden", "tokens": [50906, 407, 321, 536, 1184, 661, 311, 3636, 293, 456, 311, 257, 7633, 51126], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 358, "seek": 108432, "start": 1099.56, "end": 1102.28, "text": " group chat manager which does this kind of work.", "tokens": [51126, 1594, 5081, 6598, 597, 775, 341, 733, 295, 589, 13, 51262], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 359, "seek": 108432, "start": 1102.28, "end": 1106.24, "text": " So architectural wise, it's still like one-on-one chat,", "tokens": [51262, 407, 26621, 10829, 11, 309, 311, 920, 411, 472, 12, 266, 12, 546, 5081, 11, 51460], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 360, "seek": 108432, "start": 1106.24, "end": 1111.24, "text": " but on the surface we can create an experience", "tokens": [51460, 457, 322, 264, 3753, 321, 393, 1884, 364, 1752, 51710], "temperature": 0.0, "avg_logprob": -0.2955857296379245, "compression_ratio": 1.6492890995260663, "no_speech_prob": 0.0033223270438611507}, {"id": 361, "seek": 111124, "start": 1112.1200000000001, "end": 1114.88, "text": " that simulates the group chat.", "tokens": [50408, 300, 1034, 26192, 264, 1594, 5081, 13, 50546], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 362, "seek": 111124, "start": 1114.88, "end": 1117.4, "text": " And they can nest it the chat in some way,", "tokens": [50546, 400, 436, 393, 15646, 309, 264, 5081, 294, 512, 636, 11, 50672], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 363, "seek": 111124, "start": 1117.4, "end": 1120.4, "text": " like for example, we can start with one-on-one conversation", "tokens": [50672, 411, 337, 1365, 11, 321, 393, 722, 365, 472, 12, 266, 12, 546, 3761, 50822], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 364, "seek": 111124, "start": 1120.4, "end": 1124.76, "text": " now, but at some time I decide to consult Katie.", "tokens": [50822, 586, 11, 457, 412, 512, 565, 286, 4536, 281, 7189, 19602, 13, 51040], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 365, "seek": 111124, "start": 1124.76, "end": 1126.84, "text": " So I will hold on my current conversation", "tokens": [51040, 407, 286, 486, 1797, 322, 452, 2190, 3761, 51144], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 366, "seek": 111124, "start": 1126.84, "end": 1128.96, "text": " and have some conversation with her.", "tokens": [51144, 293, 362, 512, 3761, 365, 720, 13, 51250], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 367, "seek": 111124, "start": 1128.96, "end": 1132.16, "text": " And then after I finish the conversation with her,", "tokens": [51250, 400, 550, 934, 286, 2413, 264, 3761, 365, 720, 11, 51410], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 368, "seek": 111124, "start": 1132.16, "end": 1134.6, "text": " I get back and continue the conversation with you.", "tokens": [51410, 286, 483, 646, 293, 2354, 264, 3761, 365, 291, 13, 51532], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 369, "seek": 111124, "start": 1134.6, "end": 1137.72, "text": " So that is a kind of nested chat.", "tokens": [51532, 407, 300, 307, 257, 733, 295, 15646, 292, 5081, 13, 51688], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 370, "seek": 111124, "start": 1137.72, "end": 1140.08, "text": " I believe that essentially you have the building blocks", "tokens": [51688, 286, 1697, 300, 4476, 291, 362, 264, 2390, 8474, 51806], "temperature": 0.0, "avg_logprob": -0.16998936335245768, "compression_ratio": 1.812, "no_speech_prob": 0.01241070032119751}, {"id": 371, "seek": 114008, "start": 1140.08, "end": 1141.24, "text": " right now, right?", "tokens": [50364, 558, 586, 11, 558, 30, 50422], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 372, "seek": 114008, "start": 1141.24, "end": 1143.4399999999998, "text": " These are some very common building blocks", "tokens": [50422, 1981, 366, 512, 588, 2689, 2390, 8474, 50532], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 373, "seek": 114008, "start": 1143.4399999999998, "end": 1146.4399999999998, "text": " and we compose them together in different ways.", "tokens": [50532, 293, 321, 35925, 552, 1214, 294, 819, 2098, 13, 50682], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 374, "seek": 114008, "start": 1146.4399999999998, "end": 1150.12, "text": " We can build really complex workflows in general.", "tokens": [50682, 492, 393, 1322, 534, 3997, 43461, 294, 2674, 13, 50866], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 375, "seek": 114008, "start": 1150.12, "end": 1154.6399999999999, "text": " So any arbitrarily complex communication patterns", "tokens": [50866, 407, 604, 19071, 3289, 3997, 6101, 8294, 51092], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 376, "seek": 114008, "start": 1154.6399999999999, "end": 1157.28, "text": " can be essentially built up within these building blocks.", "tokens": [51092, 393, 312, 4476, 3094, 493, 1951, 613, 2390, 8474, 13, 51224], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 377, "seek": 114008, "start": 1157.28, "end": 1159.72, "text": " But I think that is what we have achieved.", "tokens": [51224, 583, 286, 519, 300, 307, 437, 321, 362, 11042, 13, 51346], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 378, "seek": 114008, "start": 1159.72, "end": 1163.08, "text": " And we have also many examples for different applications", "tokens": [51346, 400, 321, 362, 611, 867, 5110, 337, 819, 5821, 51514], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 379, "seek": 114008, "start": 1163.08, "end": 1167.6, "text": " of using these different types of patterns.", "tokens": [51514, 295, 1228, 613, 819, 3467, 295, 8294, 13, 51740], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 380, "seek": 114008, "start": 1167.6, "end": 1169.6799999999998, "text": " This is another second thing we figured out.", "tokens": [51740, 639, 307, 1071, 1150, 551, 321, 8932, 484, 13, 51844], "temperature": 0.0, "avg_logprob": -0.2269684564499628, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.003481893567368388}, {"id": 381, "seek": 116968, "start": 1169.68, "end": 1172.16, "text": " And the third thing I think is the ability", "tokens": [50364, 400, 264, 2636, 551, 286, 519, 307, 264, 3485, 50488], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 382, "seek": 116968, "start": 1172.16, "end": 1178.6000000000001, "text": " to take human input and human control in a very natural way.", "tokens": [50488, 281, 747, 1952, 4846, 293, 1952, 1969, 294, 257, 588, 3303, 636, 13, 50810], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 383, "seek": 116968, "start": 1178.6000000000001, "end": 1181.64, "text": " And I thought earlier it's like every agent", "tokens": [50810, 400, 286, 1194, 3071, 309, 311, 411, 633, 9461, 50962], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 384, "seek": 116968, "start": 1181.64, "end": 1186.76, "text": " can be configured to enable the human input or disable that,", "tokens": [50962, 393, 312, 30538, 281, 9528, 264, 1952, 4846, 420, 28362, 300, 11, 51218], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 385, "seek": 116968, "start": 1186.76, "end": 1189.3600000000001, "text": " depending on what you need.", "tokens": [51218, 5413, 322, 437, 291, 643, 13, 51348], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 386, "seek": 116968, "start": 1189.3600000000001, "end": 1195.3600000000001, "text": " And also you can decide the type of environment from human.", "tokens": [51348, 400, 611, 291, 393, 4536, 264, 2010, 295, 2823, 490, 1952, 13, 51648], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 387, "seek": 116968, "start": 1195.3600000000001, "end": 1199.3200000000002, "text": " You can take over every time or you can only selectively", "tokens": [51648, 509, 393, 747, 670, 633, 565, 420, 291, 393, 787, 3048, 3413, 51846], "temperature": 0.0, "avg_logprob": -0.18692824545871006, "compression_ratio": 1.6572769953051643, "no_speech_prob": 0.0009105383069254458}, {"id": 388, "seek": 119932, "start": 1199.32, "end": 1201.8, "text": " chime in at a certain time.", "tokens": [50364, 40921, 294, 412, 257, 1629, 565, 13, 50488], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 389, "seek": 119932, "start": 1201.8, "end": 1203.84, "text": " So that's a very useful feature because when", "tokens": [50488, 407, 300, 311, 257, 588, 4420, 4111, 570, 562, 50590], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 390, "seek": 119932, "start": 1203.84, "end": 1207.48, "text": " you develop this automation, initially you", "tokens": [50590, 291, 1499, 341, 17769, 11, 9105, 291, 50772], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 391, "seek": 119932, "start": 1207.48, "end": 1210.6799999999998, "text": " don't know which step is easy to automate", "tokens": [50772, 500, 380, 458, 597, 1823, 307, 1858, 281, 31605, 50932], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 392, "seek": 119932, "start": 1210.6799999999998, "end": 1215.24, "text": " and which step is necessary for human to get in.", "tokens": [50932, 293, 597, 1823, 307, 4818, 337, 1952, 281, 483, 294, 13, 51160], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 393, "seek": 119932, "start": 1215.24, "end": 1219.76, "text": " So you can start from the more human loop way.", "tokens": [51160, 407, 291, 393, 722, 490, 264, 544, 1952, 6367, 636, 13, 51386], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 394, "seek": 119932, "start": 1219.76, "end": 1221.96, "text": " And when you figure out that one step is", "tokens": [51386, 400, 562, 291, 2573, 484, 300, 472, 1823, 307, 51496], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 395, "seek": 119932, "start": 1221.96, "end": 1225.72, "text": " you can confidently automate, then you can gradually reduce", "tokens": [51496, 291, 393, 41956, 31605, 11, 550, 291, 393, 13145, 5407, 51684], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 396, "seek": 119932, "start": 1225.72, "end": 1227.04, "text": " your human integration.", "tokens": [51684, 428, 1952, 10980, 13, 51750], "temperature": 0.0, "avg_logprob": -0.2073870393418774, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0016735007520765066}, {"id": 397, "seek": 122704, "start": 1227.08, "end": 1232.8, "text": " So this convenience is very useful for doing all the experiments", "tokens": [50366, 407, 341, 19283, 307, 588, 4420, 337, 884, 439, 264, 12050, 50652], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 398, "seek": 122704, "start": 1232.8, "end": 1235.32, "text": " and figure out the right way.", "tokens": [50652, 293, 2573, 484, 264, 558, 636, 13, 50778], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 399, "seek": 122704, "start": 1235.32, "end": 1238.48, "text": " And make sure or still make sure human has a control", "tokens": [50778, 400, 652, 988, 420, 920, 652, 988, 1952, 575, 257, 1969, 50936], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 400, "seek": 122704, "start": 1238.48, "end": 1240.12, "text": " when they need to.", "tokens": [50936, 562, 436, 643, 281, 13, 51018], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 401, "seek": 122704, "start": 1240.12, "end": 1241.6399999999999, "text": " This is the third thing I think.", "tokens": [51018, 639, 307, 264, 2636, 551, 286, 519, 13, 51094], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 402, "seek": 122704, "start": 1241.6399999999999, "end": 1245.84, "text": " So one more thing is the modularity and the reusability", "tokens": [51094, 407, 472, 544, 551, 307, 264, 31111, 507, 293, 264, 38860, 2310, 51304], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 403, "seek": 122704, "start": 1245.84, "end": 1247.04, "text": " of the agents.", "tokens": [51304, 295, 264, 12554, 13, 51364], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 404, "seek": 122704, "start": 1247.04, "end": 1251.8799999999999, "text": " That is a very important design part of it.", "tokens": [51364, 663, 307, 257, 588, 1021, 1715, 644, 295, 309, 13, 51606], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 405, "seek": 122704, "start": 1251.8799999999999, "end": 1255.6, "text": " So make sure that if you develop one useful agent", "tokens": [51606, 407, 652, 988, 300, 498, 291, 1499, 472, 4420, 9461, 51792], "temperature": 0.0, "avg_logprob": -0.286205953481246, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.00017670421220827848}, {"id": 406, "seek": 125560, "start": 1255.6399999999999, "end": 1258.6, "text": " in a different application, you could either directly reuse", "tokens": [50366, 294, 257, 819, 3861, 11, 291, 727, 2139, 3838, 26225, 50514], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 407, "seek": 125560, "start": 1258.6, "end": 1262.6799999999998, "text": " or start to modify it or extend from different ways.", "tokens": [50514, 420, 722, 281, 16927, 309, 420, 10101, 490, 819, 2098, 13, 50718], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 408, "seek": 125560, "start": 1262.6799999999998, "end": 1266.12, "text": " And make sure the barrier to hard work is not lost.", "tokens": [50718, 400, 652, 988, 264, 13357, 281, 1152, 589, 307, 406, 2731, 13, 50890], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 409, "seek": 125560, "start": 1266.12, "end": 1268.52, "text": " I think that's also very important,", "tokens": [50890, 286, 519, 300, 311, 611, 588, 1021, 11, 51010], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 410, "seek": 125560, "start": 1268.52, "end": 1272.36, "text": " seeing when we work together to build more and more", "tokens": [51010, 2577, 562, 321, 589, 1214, 281, 1322, 544, 293, 544, 51202], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 411, "seek": 125560, "start": 1272.36, "end": 1274.36, "text": " complex applications.", "tokens": [51202, 3997, 5821, 13, 51302], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 412, "seek": 125560, "start": 1274.36, "end": 1279.24, "text": " These are a number of things I think we're kind of figure out.", "tokens": [51302, 1981, 366, 257, 1230, 295, 721, 286, 519, 321, 434, 733, 295, 2573, 484, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 413, "seek": 125560, "start": 1279.24, "end": 1282.76, "text": " And there are indeed a lot of challenges we haven't.", "tokens": [51546, 400, 456, 366, 6451, 257, 688, 295, 4759, 321, 2378, 380, 13, 51722], "temperature": 0.0, "avg_logprob": -0.2644993175159801, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.003881855634972453}, {"id": 414, "seek": 128276, "start": 1282.76, "end": 1286.44, "text": " And yeah, shall we get to that part?", "tokens": [50364, 400, 1338, 11, 4393, 321, 483, 281, 300, 644, 30, 50548], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 415, "seek": 128276, "start": 1286.44, "end": 1291.16, "text": " Yeah, no, I just want to reflect on some of the processes", "tokens": [50548, 865, 11, 572, 11, 286, 445, 528, 281, 5031, 322, 512, 295, 264, 7555, 50784], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 416, "seek": 128276, "start": 1291.16, "end": 1294.32, "text": " that you outlined, like removing humans from the loop", "tokens": [50784, 300, 291, 27412, 11, 411, 12720, 6255, 490, 264, 6367, 50942], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 417, "seek": 128276, "start": 1294.32, "end": 1295.64, "text": " step by step.", "tokens": [50942, 1823, 538, 1823, 13, 51008], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 418, "seek": 128276, "start": 1295.64, "end": 1297.48, "text": " Back in my time as an automation engineer,", "tokens": [51008, 5833, 294, 452, 565, 382, 364, 17769, 11403, 11, 51100], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 419, "seek": 128276, "start": 1297.48, "end": 1301.48, "text": " that's exactly what I would do is like, OK, I can write a script", "tokens": [51100, 300, 311, 2293, 437, 286, 576, 360, 307, 411, 11, 2264, 11, 286, 393, 2464, 257, 5755, 51300], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 420, "seek": 128276, "start": 1301.48, "end": 1302.56, "text": " that does one part easily.", "tokens": [51300, 300, 775, 472, 644, 3612, 13, 51354], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 421, "seek": 128276, "start": 1302.56, "end": 1303.32, "text": " Cool.", "tokens": [51354, 8561, 13, 51392], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 422, "seek": 128276, "start": 1303.32, "end": 1305.16, "text": " Now, what's the next part?", "tokens": [51392, 823, 11, 437, 311, 264, 958, 644, 30, 51484], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 423, "seek": 128276, "start": 1305.16, "end": 1307.8, "text": " And then where do I have to jump in?", "tokens": [51484, 400, 550, 689, 360, 286, 362, 281, 3012, 294, 30, 51616], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 424, "seek": 128276, "start": 1307.8, "end": 1310.64, "text": " And some of the other problems that you solved, like knowing.", "tokens": [51616, 400, 512, 295, 264, 661, 2740, 300, 291, 13041, 11, 411, 5276, 13, 51758], "temperature": 0.0, "avg_logprob": -0.21031246035117804, "compression_ratio": 1.6067415730337078, "no_speech_prob": 0.0032680309377610683}, {"id": 425, "seek": 131064, "start": 1310.68, "end": 1312.48, "text": " So this I think is really important,", "tokens": [50366, 407, 341, 286, 519, 307, 534, 1021, 11, 50456], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 426, "seek": 131064, "start": 1312.48, "end": 1315.2800000000002, "text": " because some of the members of my team and my projects", "tokens": [50456, 570, 512, 295, 264, 2679, 295, 452, 1469, 293, 452, 4455, 50596], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 427, "seek": 131064, "start": 1315.2800000000002, "end": 1319.1200000000001, "text": " have found the same thing, is that knowing when to be quiet,", "tokens": [50596, 362, 1352, 264, 912, 551, 11, 307, 300, 5276, 562, 281, 312, 5677, 11, 50788], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 428, "seek": 131064, "start": 1319.1200000000001, "end": 1322.48, "text": " knowing when not to jump in is, in many respects,", "tokens": [50788, 5276, 562, 406, 281, 3012, 294, 307, 11, 294, 867, 24126, 11, 50956], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 429, "seek": 131064, "start": 1322.48, "end": 1324.44, "text": " more important because you don't want to end up", "tokens": [50956, 544, 1021, 570, 291, 500, 380, 528, 281, 917, 493, 51054], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 430, "seek": 131064, "start": 1324.44, "end": 1327.0800000000002, "text": " with too much noise or wasted tokens.", "tokens": [51054, 365, 886, 709, 5658, 420, 19496, 22667, 13, 51186], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 431, "seek": 131064, "start": 1327.0800000000002, "end": 1328.72, "text": " So that's really fascinating.", "tokens": [51186, 407, 300, 311, 534, 10343, 13, 51268], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 432, "seek": 131064, "start": 1328.72, "end": 1332.4, "text": " Before we talk about problems you haven't overcome,", "tokens": [51268, 4546, 321, 751, 466, 2740, 291, 2378, 380, 10473, 11, 51452], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 433, "seek": 131064, "start": 1332.4, "end": 1334.24, "text": " can you talk about some insights from that,", "tokens": [51452, 393, 291, 751, 466, 512, 14310, 490, 300, 11, 51544], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 434, "seek": 131064, "start": 1334.24, "end": 1338.8400000000001, "text": " like that inhibition signal or keeping the noise lower?", "tokens": [51544, 411, 300, 20406, 849, 6358, 420, 5145, 264, 5658, 3126, 30, 51774], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 435, "seek": 131064, "start": 1338.8400000000001, "end": 1340.3600000000001, "text": " What were the key insights there?", "tokens": [51774, 708, 645, 264, 2141, 14310, 456, 30, 51850], "temperature": 0.0, "avg_logprob": -0.16237637446476863, "compression_ratio": 1.726027397260274, "no_speech_prob": 0.0014097620733082294}, {"id": 436, "seek": 134036, "start": 1340.36, "end": 1342.4799999999998, "text": " Like how did you test that and figure it out?", "tokens": [50364, 1743, 577, 630, 291, 1500, 300, 293, 2573, 309, 484, 30, 50470], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 437, "seek": 134036, "start": 1342.4799999999998, "end": 1345.76, "text": " And do you have any general principles for anyone else", "tokens": [50470, 400, 360, 291, 362, 604, 2674, 9156, 337, 2878, 1646, 50634], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 438, "seek": 134036, "start": 1345.76, "end": 1347.84, "text": " building agents?", "tokens": [50634, 2390, 12554, 30, 50738], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 439, "seek": 134036, "start": 1347.84, "end": 1349.6799999999998, "text": " Yeah, this is a very good question.", "tokens": [50738, 865, 11, 341, 307, 257, 588, 665, 1168, 13, 50830], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 440, "seek": 134036, "start": 1349.6799999999998, "end": 1355.12, "text": " This is also related to another question about when", "tokens": [50830, 639, 307, 611, 4077, 281, 1071, 1168, 466, 562, 51102], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 441, "seek": 134036, "start": 1355.12, "end": 1359.8799999999999, "text": " do you add agents to provide the feedback and when", "tokens": [51102, 360, 291, 909, 12554, 281, 2893, 264, 5824, 293, 562, 51340], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 442, "seek": 134036, "start": 1359.8799999999999, "end": 1362.6399999999999, "text": " that is not helpful, right?", "tokens": [51340, 300, 307, 406, 4961, 11, 558, 30, 51478], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 443, "seek": 134036, "start": 1362.6399999999999, "end": 1366.7199999999998, "text": " Because I assume the noise you're talking about", "tokens": [51478, 1436, 286, 6552, 264, 5658, 291, 434, 1417, 466, 51682], "temperature": 0.0, "avg_logprob": -0.20016424046006315, "compression_ratio": 1.537037037037037, "no_speech_prob": 0.0009249953436665237}, {"id": 444, "seek": 136672, "start": 1366.72, "end": 1373.08, "text": " is when you add more agents to service for democratics", "tokens": [50364, 307, 562, 291, 909, 544, 12554, 281, 2643, 337, 6366, 30292, 50682], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 445, "seek": 136672, "start": 1373.08, "end": 1380.08, "text": " or agents that try to refine what the other agent is doing,", "tokens": [50682, 420, 12554, 300, 853, 281, 33906, 437, 264, 661, 9461, 307, 884, 11, 51032], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 446, "seek": 136672, "start": 1380.08, "end": 1380.96, "text": " right?", "tokens": [51032, 558, 30, 51076], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 447, "seek": 136672, "start": 1380.96, "end": 1383.48, "text": " They serve as a channel to provide feedback,", "tokens": [51076, 814, 4596, 382, 257, 2269, 281, 2893, 5824, 11, 51202], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 448, "seek": 136672, "start": 1383.48, "end": 1387.3600000000001, "text": " but sometimes not feedback can be misleading and actually", "tokens": [51202, 457, 2171, 406, 5824, 393, 312, 36429, 293, 767, 51396], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 449, "seek": 136672, "start": 1387.3600000000001, "end": 1392.6000000000001, "text": " prevent the original agent doing the right thing.", "tokens": [51396, 4871, 264, 3380, 9461, 884, 264, 558, 551, 13, 51658], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 450, "seek": 136672, "start": 1392.6000000000001, "end": 1393.92, "text": " Yeah, we do observe that.", "tokens": [51658, 865, 11, 321, 360, 11441, 300, 13, 51724], "temperature": 0.0, "avg_logprob": -0.2485888631720292, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.0027570005040615797}, {"id": 451, "seek": 139392, "start": 1393.92, "end": 1397.76, "text": " And also, it's not like the more agents, the better.", "tokens": [50364, 400, 611, 11, 309, 311, 406, 411, 264, 544, 12554, 11, 264, 1101, 13, 50556], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 452, "seek": 139392, "start": 1397.76, "end": 1399.72, "text": " It's not necessarily that.", "tokens": [50556, 467, 311, 406, 4725, 300, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 453, "seek": 139392, "start": 1399.72, "end": 1403.4, "text": " For example, if you use GPD4 as the back end", "tokens": [50654, 1171, 1365, 11, 498, 291, 764, 460, 17349, 19, 382, 264, 646, 917, 50838], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 454, "seek": 139392, "start": 1403.4, "end": 1408.24, "text": " for an assistant agent, for a large number of problems,", "tokens": [50838, 337, 364, 10994, 9461, 11, 337, 257, 2416, 1230, 295, 2740, 11, 51080], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 455, "seek": 139392, "start": 1408.24, "end": 1411.48, "text": " you'll need a simple two-agent workflow.", "tokens": [51080, 291, 603, 643, 257, 2199, 732, 12, 559, 317, 20993, 13, 51242], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 456, "seek": 139392, "start": 1411.48, "end": 1414.8400000000001, "text": " One assistant agent, another user proxy agent.", "tokens": [51242, 1485, 10994, 9461, 11, 1071, 4195, 29690, 9461, 13, 51410], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 457, "seek": 139392, "start": 1414.8400000000001, "end": 1416.24, "text": " Yeah, probably I need to explain what", "tokens": [51410, 865, 11, 1391, 286, 643, 281, 2903, 437, 51480], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 458, "seek": 139392, "start": 1416.24, "end": 1417.8400000000001, "text": " the user proxy agent is.", "tokens": [51480, 264, 4195, 29690, 9461, 307, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 459, "seek": 139392, "start": 1417.8400000000001, "end": 1420.44, "text": " Basically, it refers to what I meant earlier", "tokens": [51560, 8537, 11, 309, 14942, 281, 437, 286, 4140, 3071, 51690], "temperature": 0.0, "avg_logprob": -0.2389140835514775, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0017812801524996758}, {"id": 460, "seek": 142044, "start": 1420.44, "end": 1423.92, "text": " about automating some of the work that human does.", "tokens": [50364, 466, 3553, 990, 512, 295, 264, 589, 300, 1952, 775, 13, 50538], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 461, "seek": 142044, "start": 1423.92, "end": 1426.0, "text": " For example, using tools to execute", "tokens": [50538, 1171, 1365, 11, 1228, 3873, 281, 14483, 50642], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 462, "seek": 142044, "start": 1426.0, "end": 1428.72, "text": " Python code or run some predefined functions.", "tokens": [50642, 15329, 3089, 420, 1190, 512, 659, 37716, 6828, 13, 50778], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 463, "seek": 142044, "start": 1428.72, "end": 1434.0800000000002, "text": " So if you use one GPD assistant agent to suggest a solution", "tokens": [50778, 407, 498, 291, 764, 472, 460, 17349, 10994, 9461, 281, 3402, 257, 3827, 51046], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 464, "seek": 142044, "start": 1434.0800000000002, "end": 1436.72, "text": " such as code or function and use another user proxy", "tokens": [51046, 1270, 382, 3089, 420, 2445, 293, 764, 1071, 4195, 29690, 51178], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 465, "seek": 142044, "start": 1436.72, "end": 1440.52, "text": " to execute them and just provide feedback back and forth,", "tokens": [51178, 281, 14483, 552, 293, 445, 2893, 5824, 646, 293, 5220, 11, 51368], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 466, "seek": 142044, "start": 1440.52, "end": 1444.0800000000002, "text": " you can solve a large number of problems very well.", "tokens": [51368, 291, 393, 5039, 257, 2416, 1230, 295, 2740, 588, 731, 13, 51546], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 467, "seek": 142044, "start": 1444.0800000000002, "end": 1446.76, "text": " And some of them are also complicated", "tokens": [51546, 400, 512, 295, 552, 366, 611, 6179, 51680], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 468, "seek": 142044, "start": 1446.76, "end": 1448.68, "text": " and can involve multiple steps.", "tokens": [51680, 293, 393, 9494, 3866, 4439, 13, 51776], "temperature": 0.0, "avg_logprob": -0.2512056804838635, "compression_ratio": 1.6245210727969348, "no_speech_prob": 0.0021143422927707434}, {"id": 469, "seek": 144868, "start": 1448.76, "end": 1453.96, "text": " But if you use GPD3.5 turbo, then it's much less", "tokens": [50368, 583, 498, 291, 764, 460, 17349, 18, 13, 20, 20902, 11, 550, 309, 311, 709, 1570, 50628], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 470, "seek": 144868, "start": 1453.96, "end": 1455.76, "text": " to work in this way.", "tokens": [50628, 281, 589, 294, 341, 636, 13, 50718], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 471, "seek": 144868, "start": 1455.76, "end": 1458.92, "text": " So adding more agents will be much more helpful.", "tokens": [50718, 407, 5127, 544, 12554, 486, 312, 709, 544, 4961, 13, 50876], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 472, "seek": 144868, "start": 1458.92, "end": 1462.1200000000001, "text": " And even for GPD4, when the problem complexity", "tokens": [50876, 400, 754, 337, 460, 17349, 19, 11, 562, 264, 1154, 14024, 51036], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 473, "seek": 144868, "start": 1462.1200000000001, "end": 1466.28, "text": " goes above a certain level, it stops", "tokens": [51036, 1709, 3673, 257, 1629, 1496, 11, 309, 10094, 51244], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 474, "seek": 144868, "start": 1466.28, "end": 1469.1200000000001, "text": " to follow the main instructions.", "tokens": [51244, 281, 1524, 264, 2135, 9415, 13, 51386], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 475, "seek": 144868, "start": 1469.1200000000001, "end": 1471.72, "text": " But because of the trick for one single agent to work,", "tokens": [51386, 583, 570, 295, 264, 4282, 337, 472, 2167, 9461, 281, 589, 11, 51516], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 476, "seek": 144868, "start": 1471.72, "end": 1475.68, "text": " it actually puts a lot of careful instructions", "tokens": [51516, 309, 767, 8137, 257, 688, 295, 5026, 9415, 51714], "temperature": 0.0, "avg_logprob": -0.24302400287828949, "compression_ratio": 1.518018018018018, "no_speech_prob": 0.0011510339099913836}, {"id": 477, "seek": 147568, "start": 1475.68, "end": 1478.3600000000001, "text": " in the system message and make it", "tokens": [50364, 294, 264, 1185, 3636, 293, 652, 309, 50498], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 478, "seek": 147568, "start": 1478.3600000000001, "end": 1482.8, "text": " know how to deal with some complex situations.", "tokens": [50498, 458, 577, 281, 2028, 365, 512, 3997, 6851, 13, 50720], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 479, "seek": 147568, "start": 1482.8, "end": 1487.1200000000001, "text": " But we noted that if you put too many of them, even for GPD4,", "tokens": [50720, 583, 321, 12964, 300, 498, 291, 829, 886, 867, 295, 552, 11, 754, 337, 460, 17349, 19, 11, 50936], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 480, "seek": 147568, "start": 1487.1200000000001, "end": 1491.16, "text": " and for complex tasks, start to forget these instructions", "tokens": [50936, 293, 337, 3997, 9608, 11, 722, 281, 2870, 613, 9415, 51138], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 481, "seek": 147568, "start": 1491.16, "end": 1495.0, "text": " and not do things as you want.", "tokens": [51138, 293, 406, 360, 721, 382, 291, 528, 13, 51330], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 482, "seek": 147568, "start": 1495.0, "end": 1498.88, "text": " Otherwise, you can just give it a simple instruction", "tokens": [51330, 10328, 11, 291, 393, 445, 976, 309, 257, 2199, 10951, 51524], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 483, "seek": 147568, "start": 1498.88, "end": 1502.0, "text": " to say, try your best to solve the hardest problem", "tokens": [51524, 281, 584, 11, 853, 428, 1151, 281, 5039, 264, 13158, 1154, 51680], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 484, "seek": 147568, "start": 1502.0, "end": 1504.96, "text": " and then it will be done.", "tokens": [51680, 293, 550, 309, 486, 312, 1096, 13, 51828], "temperature": 0.0, "avg_logprob": -0.24429128607925105, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.0017266514478251338}, {"id": 485, "seek": 150496, "start": 1505.0, "end": 1505.88, "text": " We're not there yet.", "tokens": [50366, 492, 434, 406, 456, 1939, 13, 50410], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 486, "seek": 150496, "start": 1505.88, "end": 1507.52, "text": " I mean, in the future, we may.", "tokens": [50410, 286, 914, 11, 294, 264, 2027, 11, 321, 815, 13, 50492], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 487, "seek": 150496, "start": 1507.52, "end": 1511.28, "text": " This makes me want to bring up one interesting kind of law.", "tokens": [50492, 639, 1669, 385, 528, 281, 1565, 493, 472, 1880, 733, 295, 2101, 13, 50680], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 488, "seek": 150496, "start": 1511.28, "end": 1515.76, "text": " We, a few of us, came up called Kabuchi's law.", "tokens": [50680, 492, 11, 257, 1326, 295, 505, 11, 1361, 493, 1219, 25848, 30026, 311, 2101, 13, 50904], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 489, "seek": 150496, "start": 1515.76, "end": 1518.96, "text": " The law has some similarity with,", "tokens": [50904, 440, 2101, 575, 512, 32194, 365, 11, 51064], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 490, "seek": 150496, "start": 1518.96, "end": 1521.92, "text": " it's an analogy of the Conway's law in software engineering.", "tokens": [51064, 309, 311, 364, 21663, 295, 264, 2656, 676, 311, 2101, 294, 4722, 7043, 13, 51212], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 491, "seek": 150496, "start": 1521.92, "end": 1523.92, "text": " I'm not sure if you familiar with that notion.", "tokens": [51212, 286, 478, 406, 988, 498, 291, 4963, 365, 300, 10710, 13, 51312], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 492, "seek": 150496, "start": 1523.92, "end": 1524.92, "text": " No.", "tokens": [51312, 883, 13, 51362], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 493, "seek": 150496, "start": 1524.92, "end": 1527.76, "text": " So Conway's law basically saying the complexity", "tokens": [51362, 407, 2656, 676, 311, 2101, 1936, 1566, 264, 14024, 51504], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 494, "seek": 150496, "start": 1527.76, "end": 1531.92, "text": " of the software or the architecture of the software", "tokens": [51504, 295, 264, 4722, 420, 264, 9482, 295, 264, 4722, 51712], "temperature": 0.0, "avg_logprob": -0.25460673186738614, "compression_ratio": 1.6356275303643724, "no_speech_prob": 0.030584169551730156}, {"id": 495, "seek": 153192, "start": 1531.92, "end": 1535.0, "text": " is a reflection of the organization", "tokens": [50364, 307, 257, 12914, 295, 264, 4475, 50518], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 496, "seek": 153192, "start": 1535.0, "end": 1538.04, "text": " that makes the software, that builds the software.", "tokens": [50518, 300, 1669, 264, 4722, 11, 300, 15182, 264, 4722, 13, 50670], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 497, "seek": 153192, "start": 1538.04, "end": 1538.44, "text": " OK.", "tokens": [50670, 2264, 13, 50690], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 498, "seek": 153192, "start": 1538.44, "end": 1538.8400000000001, "text": " Makes sense.", "tokens": [50690, 25245, 2020, 13, 50710], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 499, "seek": 153192, "start": 1538.8400000000001, "end": 1539.2, "text": " Yeah.", "tokens": [50710, 865, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 500, "seek": 153192, "start": 1539.2, "end": 1543.48, "text": " So our Kabuchi's law says the model complexity", "tokens": [50728, 407, 527, 25848, 30026, 311, 2101, 1619, 264, 2316, 14024, 50942], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 501, "seek": 153192, "start": 1543.48, "end": 1547.2, "text": " will affect the model capacity or capability.", "tokens": [50942, 486, 3345, 264, 2316, 6042, 420, 13759, 13, 51128], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 502, "seek": 153192, "start": 1547.2, "end": 1550.72, "text": " We change the topology of the ideal multi-agent solution.", "tokens": [51128, 492, 1319, 264, 1192, 1793, 295, 264, 7157, 4825, 12, 559, 317, 3827, 13, 51304], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 503, "seek": 153192, "start": 1550.72, "end": 1553.4, "text": " It's a summarization of what I mentioned earlier.", "tokens": [51304, 467, 311, 257, 14611, 2144, 295, 437, 286, 2835, 3071, 13, 51438], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 504, "seek": 153192, "start": 1553.4, "end": 1556.04, "text": " If you use a more powerful model,", "tokens": [51438, 759, 291, 764, 257, 544, 4005, 2316, 11, 51570], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 505, "seek": 153192, "start": 1556.04, "end": 1561.0, "text": " then likely you can use simpler topology of multi-agents", "tokens": [51570, 550, 3700, 291, 393, 764, 18587, 1192, 1793, 295, 4825, 12, 559, 791, 51818], "temperature": 0.0, "avg_logprob": -0.2728354479815509, "compression_ratio": 1.684873949579832, "no_speech_prob": 0.0009541697800159454}, {"id": 506, "seek": 156100, "start": 1561.04, "end": 1564.12, "text": " to solve a common task and vice versa.", "tokens": [50366, 281, 5039, 257, 2689, 5633, 293, 11964, 25650, 13, 50520], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 507, "seek": 156100, "start": 1564.12, "end": 1566.84, "text": " And also, I think we need more and more research", "tokens": [50520, 400, 611, 11, 286, 519, 321, 643, 544, 293, 544, 2132, 50656], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 508, "seek": 156100, "start": 1566.84, "end": 1569.04, "text": " to understand this better as it's not soft.", "tokens": [50656, 281, 1223, 341, 1101, 382, 309, 311, 406, 2787, 13, 50766], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 509, "seek": 156100, "start": 1569.04, "end": 1573.68, "text": " I'm seeing people trying all different kind of topologies", "tokens": [50766, 286, 478, 2577, 561, 1382, 439, 819, 733, 295, 1192, 6204, 50998], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 510, "seek": 156100, "start": 1573.68, "end": 1575.6, "text": " or communication patterns for different applications.", "tokens": [50998, 420, 6101, 8294, 337, 819, 5821, 13, 51094], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 511, "seek": 156100, "start": 1575.6, "end": 1577.76, "text": " They're very creative.", "tokens": [51094, 814, 434, 588, 5880, 13, 51202], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 512, "seek": 156100, "start": 1577.76, "end": 1582.32, "text": " And what we had to figure out is what is the best topology", "tokens": [51202, 400, 437, 321, 632, 281, 2573, 484, 307, 437, 307, 264, 1151, 1192, 1793, 51430], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 513, "seek": 156100, "start": 1582.32, "end": 1585.52, "text": " and for a particular model and for a particular application", "tokens": [51430, 293, 337, 257, 1729, 2316, 293, 337, 257, 1729, 3861, 51590], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 514, "seek": 156100, "start": 1585.52, "end": 1588.44, "text": " in a kind of a very clear way to answer that question.", "tokens": [51590, 294, 257, 733, 295, 257, 588, 1850, 636, 281, 1867, 300, 1168, 13, 51736], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 515, "seek": 156100, "start": 1588.44, "end": 1589.6, "text": " We were not here.", "tokens": [51736, 492, 645, 406, 510, 13, 51794], "temperature": 0.0, "avg_logprob": -0.25566643934983474, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0017812432488426566}, {"id": 516, "seek": 158960, "start": 1589.6399999999999, "end": 1593.36, "text": " And this is one of actually a big challenge", "tokens": [50366, 400, 341, 307, 472, 295, 767, 257, 955, 3430, 50552], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 517, "seek": 158960, "start": 1593.36, "end": 1595.8, "text": " or a big important problem we want to solve.", "tokens": [50552, 420, 257, 955, 1021, 1154, 321, 528, 281, 5039, 13, 50674], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 518, "seek": 158960, "start": 1595.8, "end": 1596.8799999999999, "text": " Yeah, next.", "tokens": [50674, 865, 11, 958, 13, 50728], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 519, "seek": 158960, "start": 1596.8799999999999, "end": 1597.6799999999998, "text": " Yeah.", "tokens": [50728, 865, 13, 50768], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 520, "seek": 158960, "start": 1597.6799999999998, "end": 1600.0, "text": " No, I mean, well, first, thanks for sharing", "tokens": [50768, 883, 11, 286, 914, 11, 731, 11, 700, 11, 3231, 337, 5414, 50884], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 521, "seek": 158960, "start": 1600.0, "end": 1602.32, "text": " some of those critical insights.", "tokens": [50884, 512, 295, 729, 4924, 14310, 13, 51000], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 522, "seek": 158960, "start": 1602.32, "end": 1604.9199999999998, "text": " And so I guess the general principle", "tokens": [51000, 400, 370, 286, 2041, 264, 2674, 8665, 51130], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 523, "seek": 158960, "start": 1604.9199999999998, "end": 1609.1999999999998, "text": " is the smarter the underpinning model,", "tokens": [51130, 307, 264, 20294, 264, 833, 17836, 773, 2316, 11, 51344], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 524, "seek": 158960, "start": 1609.1999999999998, "end": 1612.1599999999999, "text": " the simpler the topology can be because the more complex", "tokens": [51344, 264, 18587, 264, 1192, 1793, 393, 312, 570, 264, 544, 3997, 51492], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 525, "seek": 158960, "start": 1612.1599999999999, "end": 1613.28, "text": " the instructions can be.", "tokens": [51492, 264, 9415, 393, 312, 13, 51548], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 526, "seek": 158960, "start": 1613.28, "end": 1615.6799999999998, "text": " And the more complex the tasks that an individual agent", "tokens": [51548, 400, 264, 544, 3997, 264, 9608, 300, 364, 2609, 9461, 51668], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 527, "seek": 158960, "start": 1615.6799999999998, "end": 1618.04, "text": " can carry out.", "tokens": [51668, 393, 3985, 484, 13, 51786], "temperature": 0.0, "avg_logprob": -0.19958983711574388, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0008689565001986921}, {"id": 528, "seek": 161804, "start": 1618.08, "end": 1619.76, "text": " Saying it out loud, it seems kind of obvious,", "tokens": [50366, 34087, 309, 484, 6588, 11, 309, 2544, 733, 295, 6322, 11, 50450], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 529, "seek": 161804, "start": 1619.76, "end": 1622.28, "text": " but that's a good rule to generalize.", "tokens": [50450, 457, 300, 311, 257, 665, 4978, 281, 2674, 1125, 13, 50576], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 530, "seek": 161804, "start": 1622.28, "end": 1626.12, "text": " So yeah, I guess let's pivot into what are some of the remaining", "tokens": [50576, 407, 1338, 11, 286, 2041, 718, 311, 14538, 666, 437, 366, 512, 295, 264, 8877, 50768], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 531, "seek": 161804, "start": 1626.12, "end": 1626.6, "text": " problems?", "tokens": [50768, 2740, 30, 50792], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 532, "seek": 161804, "start": 1626.6, "end": 1630.52, "text": " What are your biggest challenges that you either are working on", "tokens": [50792, 708, 366, 428, 3880, 4759, 300, 291, 2139, 366, 1364, 322, 50988], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 533, "seek": 161804, "start": 1630.52, "end": 1632.92, "text": " or are going to be down the road?", "tokens": [50988, 420, 366, 516, 281, 312, 760, 264, 3060, 30, 51108], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 534, "seek": 161804, "start": 1632.92, "end": 1634.92, "text": " You mentioned topologies, like figuring out", "tokens": [51108, 509, 2835, 1192, 6204, 11, 411, 15213, 484, 51208], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 535, "seek": 161804, "start": 1634.92, "end": 1636.68, "text": " what is the correct topology.", "tokens": [51208, 437, 307, 264, 3006, 1192, 1793, 13, 51296], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 536, "seek": 161804, "start": 1636.68, "end": 1639.44, "text": " And of course, I can imagine that it's a moving target", "tokens": [51296, 400, 295, 1164, 11, 286, 393, 3811, 300, 309, 311, 257, 2684, 3779, 51434], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 537, "seek": 161804, "start": 1639.44, "end": 1642.44, "text": " because as the underlying models change, almost", "tokens": [51434, 570, 382, 264, 14217, 5245, 1319, 11, 1920, 51584], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 538, "seek": 161804, "start": 1642.44, "end": 1645.76, "text": " on a monthly basis, you get new and different capabilities", "tokens": [51584, 322, 257, 12878, 5143, 11, 291, 483, 777, 293, 819, 10862, 51750], "temperature": 0.0, "avg_logprob": -0.13872689239738523, "compression_ratio": 1.645484949832776, "no_speech_prob": 0.002396276919171214}, {"id": 539, "seek": 164576, "start": 1645.76, "end": 1648.64, "text": " that kind of maybe send you back to the drawing board", "tokens": [50364, 300, 733, 295, 1310, 2845, 291, 646, 281, 264, 6316, 3150, 50508], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 540, "seek": 164576, "start": 1648.64, "end": 1650.28, "text": " sometimes.", "tokens": [50508, 2171, 13, 50590], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 541, "seek": 164576, "start": 1650.28, "end": 1653.52, "text": " Yeah, this is why having a framework that", "tokens": [50590, 865, 11, 341, 307, 983, 1419, 257, 8388, 300, 50752], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 542, "seek": 164576, "start": 1653.52, "end": 1658.24, "text": " is versatile and that is flexible to do the experiments", "tokens": [50752, 307, 25057, 293, 300, 307, 11358, 281, 360, 264, 12050, 50988], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 543, "seek": 164576, "start": 1658.24, "end": 1664.2, "text": " is so crucial to kind of do the fast adaptation", "tokens": [50988, 307, 370, 11462, 281, 733, 295, 360, 264, 2370, 21549, 51286], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 544, "seek": 164576, "start": 1664.2, "end": 1669.28, "text": " as model moves as prominent techniques advances", "tokens": [51286, 382, 2316, 6067, 382, 17034, 7512, 25297, 51540], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 545, "seek": 164576, "start": 1669.28, "end": 1673.04, "text": " and as more and more small model specialized models are", "tokens": [51540, 293, 382, 544, 293, 544, 1359, 2316, 19813, 5245, 366, 51728], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 546, "seek": 164576, "start": 1673.04, "end": 1675.16, "text": " available, they will probably also", "tokens": [51728, 2435, 11, 436, 486, 1391, 611, 51834], "temperature": 0.0, "avg_logprob": -0.24063758390495577, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001225045882165432}, {"id": 547, "seek": 167516, "start": 1675.16, "end": 1679.48, "text": " change a lot about what was the best way to build", "tokens": [50364, 1319, 257, 688, 466, 437, 390, 264, 1151, 636, 281, 1322, 50580], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 548, "seek": 167516, "start": 1679.48, "end": 1681.0400000000002, "text": " the applications.", "tokens": [50580, 264, 5821, 13, 50658], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 549, "seek": 167516, "start": 1681.0400000000002, "end": 1685.44, "text": " Yeah, so this is, I think, the big value of autogen.", "tokens": [50658, 865, 11, 370, 341, 307, 11, 286, 519, 11, 264, 955, 2158, 295, 1476, 8799, 13, 50878], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 550, "seek": 167516, "start": 1685.44, "end": 1688.3200000000002, "text": " And for unsolved research questions,", "tokens": [50878, 400, 337, 2693, 29110, 2132, 1651, 11, 51022], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 551, "seek": 167516, "start": 1688.3200000000002, "end": 1692.8000000000002, "text": " there are some concrete ones I can give you a few examples.", "tokens": [51022, 456, 366, 512, 9859, 2306, 286, 393, 976, 291, 257, 1326, 5110, 13, 51246], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 552, "seek": 167516, "start": 1692.8000000000002, "end": 1696.64, "text": " One is about this decomposition problem.", "tokens": [51246, 1485, 307, 466, 341, 48356, 1154, 13, 51438], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 553, "seek": 167516, "start": 1696.64, "end": 1700.5600000000002, "text": " As we mentioned earlier, we want to be", "tokens": [51438, 1018, 321, 2835, 3071, 11, 321, 528, 281, 312, 51634], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 554, "seek": 167516, "start": 1700.5600000000002, "end": 1704.3600000000001, "text": " able to achieve a state where the human can only", "tokens": [51634, 1075, 281, 4584, 257, 1785, 689, 264, 1952, 393, 787, 51824], "temperature": 0.0, "avg_logprob": -0.22203822846108295, "compression_ratio": 1.5446428571428572, "no_speech_prob": 0.0010318312561139464}, {"id": 555, "seek": 170436, "start": 1704.36, "end": 1710.4799999999998, "text": " need to specify rapidly big ambitious goal", "tokens": [50364, 643, 281, 16500, 12910, 955, 20239, 3387, 50670], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 556, "seek": 170436, "start": 1710.4799999999998, "end": 1712.7199999999998, "text": " and underneath, we want the agent", "tokens": [50670, 293, 7223, 11, 321, 528, 264, 9461, 50782], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 557, "seek": 170436, "start": 1712.7199999999998, "end": 1718.12, "text": " to be able to decompose that into solvable problems, probably", "tokens": [50782, 281, 312, 1075, 281, 22867, 541, 300, 666, 1404, 17915, 2740, 11, 1391, 51052], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 558, "seek": 170436, "start": 1718.12, "end": 1721.6399999999999, "text": " multiple layers, and eventually recompose it", "tokens": [51052, 3866, 7914, 11, 293, 4728, 48000, 541, 309, 51228], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 559, "seek": 170436, "start": 1721.6399999999999, "end": 1725.7199999999998, "text": " and solve each of them and recompose it.", "tokens": [51228, 293, 5039, 1184, 295, 552, 293, 48000, 541, 309, 13, 51432], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 560, "seek": 170436, "start": 1725.7199999999998, "end": 1727.8799999999999, "text": " And during this process, there are situations", "tokens": [51432, 400, 1830, 341, 1399, 11, 456, 366, 6851, 51540], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 561, "seek": 170436, "start": 1727.8799999999999, "end": 1730.84, "text": " where the human need to provide the correct specifications", "tokens": [51540, 689, 264, 1952, 643, 281, 2893, 264, 3006, 29448, 51688], "temperature": 0.0, "avg_logprob": -0.25452738541823167, "compression_ratio": 1.6616161616161615, "no_speech_prob": 0.0120357945561409}, {"id": 562, "seek": 173084, "start": 1730.84, "end": 1734.4399999999998, "text": " because the initial one can be ambiguous.", "tokens": [50364, 570, 264, 5883, 472, 393, 312, 39465, 13, 50544], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 563, "seek": 173084, "start": 1734.4399999999998, "end": 1737.1599999999999, "text": " And we want the human to only provide", "tokens": [50544, 400, 321, 528, 264, 1952, 281, 787, 2893, 50680], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 564, "seek": 173084, "start": 1737.1599999999999, "end": 1740.04, "text": " the necessary and make a minimal kind of necessary", "tokens": [50680, 264, 4818, 293, 652, 257, 13206, 733, 295, 4818, 50824], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 565, "seek": 173084, "start": 1740.04, "end": 1743.1999999999998, "text": " qualifications and instructions and that agent", "tokens": [50824, 33223, 293, 9415, 293, 300, 9461, 50982], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 566, "seek": 173084, "start": 1743.1999999999998, "end": 1746.08, "text": " to figure out the rest of them.", "tokens": [50982, 281, 2573, 484, 264, 1472, 295, 552, 13, 51126], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 567, "seek": 173084, "start": 1746.08, "end": 1749.52, "text": " That is a big challenge because if we want to solve", "tokens": [51126, 663, 307, 257, 955, 3430, 570, 498, 321, 528, 281, 5039, 51298], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 568, "seek": 173084, "start": 1749.52, "end": 1752.6799999999998, "text": " more complex problems, we have to have a principal way", "tokens": [51298, 544, 3997, 2740, 11, 321, 362, 281, 362, 257, 9716, 636, 51456], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 569, "seek": 173084, "start": 1752.6799999999998, "end": 1755.1999999999998, "text": " to do this.", "tokens": [51456, 281, 360, 341, 13, 51582], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 570, "seek": 173084, "start": 1755.1999999999998, "end": 1758.32, "text": " And the second question also ready to do this", "tokens": [51582, 400, 264, 1150, 1168, 611, 1919, 281, 360, 341, 51738], "temperature": 0.0, "avg_logprob": -0.19262127166098736, "compression_ratio": 1.6846846846846846, "no_speech_prob": 0.0012249574065208435}, {"id": 571, "seek": 175832, "start": 1758.32, "end": 1761.9199999999998, "text": " is as we solve bigger and bigger problems,", "tokens": [50364, 307, 382, 321, 5039, 3801, 293, 3801, 2740, 11, 50544], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 572, "seek": 175832, "start": 1761.9199999999998, "end": 1768.24, "text": " how do we do proper validation of the intermediate results?", "tokens": [50544, 577, 360, 321, 360, 2296, 24071, 295, 264, 19376, 3542, 30, 50860], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 573, "seek": 175832, "start": 1768.24, "end": 1770.32, "text": " Because we don't do that.", "tokens": [50860, 1436, 321, 500, 380, 360, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 574, "seek": 175832, "start": 1770.32, "end": 1774.56, "text": " If possible, the agent will stick to some wrong intermediate", "tokens": [50964, 759, 1944, 11, 264, 9461, 486, 2897, 281, 512, 2085, 19376, 51176], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 575, "seek": 175832, "start": 1774.56, "end": 1780.04, "text": " results and just keep doing, keep wasting their work.", "tokens": [51176, 3542, 293, 445, 1066, 884, 11, 1066, 20457, 641, 589, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 576, "seek": 175832, "start": 1780.04, "end": 1784.6, "text": " And at certain time, if we need to provide validation", "tokens": [51450, 400, 412, 1629, 565, 11, 498, 321, 643, 281, 2893, 24071, 51678], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 577, "seek": 175832, "start": 1784.6, "end": 1787.56, "text": " or use agent to do self-validation, that's hard.", "tokens": [51678, 420, 764, 9461, 281, 360, 2698, 12, 3337, 327, 399, 11, 300, 311, 1152, 13, 51826], "temperature": 0.0, "avg_logprob": -0.2624483632517385, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.000535566417966038}, {"id": 578, "seek": 178756, "start": 1787.6799999999998, "end": 1792.44, "text": " But I do a way to do it.", "tokens": [50370, 583, 286, 360, 257, 636, 281, 360, 309, 13, 50608], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 579, "seek": 178756, "start": 1792.44, "end": 1796.1599999999999, "text": " So we need probably into some formal language", "tokens": [50608, 407, 321, 643, 1391, 666, 512, 9860, 2856, 50794], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 580, "seek": 178756, "start": 1796.1599999999999, "end": 1802.3999999999999, "text": " or formal way to do this proper validation", "tokens": [50794, 420, 9860, 636, 281, 360, 341, 2296, 24071, 51106], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 581, "seek": 178756, "start": 1802.3999999999999, "end": 1805.9199999999998, "text": " so that the automation can indeed happen in a way", "tokens": [51106, 370, 300, 264, 17769, 393, 6451, 1051, 294, 257, 636, 51282], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 582, "seek": 178756, "start": 1805.9199999999998, "end": 1809.9199999999998, "text": " that human desires.", "tokens": [51282, 300, 1952, 18005, 13, 51482], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 583, "seek": 178756, "start": 1809.9199999999998, "end": 1814.12, "text": " Yeah, so these are some just two kind of concrete pieces", "tokens": [51482, 865, 11, 370, 613, 366, 512, 445, 732, 733, 295, 9859, 3755, 51692], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 584, "seek": 178756, "start": 1814.12, "end": 1815.72, "text": " like problems.", "tokens": [51692, 411, 2740, 13, 51772], "temperature": 0.0, "avg_logprob": -0.2761156899588449, "compression_ratio": 1.5178571428571428, "no_speech_prob": 0.0010806843638420105}, {"id": 585, "seek": 181572, "start": 1815.72, "end": 1818.88, "text": " Yeah, in my project, we almost started in the reverse", "tokens": [50364, 865, 11, 294, 452, 1716, 11, 321, 1920, 1409, 294, 264, 9943, 50522], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 586, "seek": 181572, "start": 1818.88, "end": 1823.28, "text": " where we started with oversight of steering and oversight", "tokens": [50522, 689, 321, 1409, 365, 29146, 295, 14823, 293, 29146, 50742], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 587, "seek": 181572, "start": 1823.28, "end": 1825.0, "text": " and supervision.", "tokens": [50742, 293, 32675, 13, 50828], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 588, "seek": 181572, "start": 1825.0, "end": 1830.76, "text": " So I'm curious, what's your perception or research", "tokens": [50828, 407, 286, 478, 6369, 11, 437, 311, 428, 12860, 420, 2132, 51116], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 589, "seek": 181572, "start": 1830.76, "end": 1833.2, "text": " or findings with respect to?", "tokens": [51116, 420, 16483, 365, 3104, 281, 30, 51238], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 590, "seek": 181572, "start": 1833.2, "end": 1836.96, "text": " Because you already mentioned having an assistant agent", "tokens": [51238, 1436, 291, 1217, 2835, 1419, 364, 10994, 9461, 51426], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 591, "seek": 181572, "start": 1836.96, "end": 1841.16, "text": " and then also having kind of a top-down hierarchical agent", "tokens": [51426, 293, 550, 611, 1419, 733, 295, 257, 1192, 12, 5093, 35250, 804, 9461, 51636], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 592, "seek": 181572, "start": 1841.16, "end": 1843.1200000000001, "text": " where you've got subordinates.", "tokens": [51636, 689, 291, 600, 658, 1422, 6241, 1024, 13, 51734], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 593, "seek": 181572, "start": 1843.1200000000001, "end": 1845.0, "text": " What do you think about my intuition", "tokens": [51734, 708, 360, 291, 519, 466, 452, 24002, 51828], "temperature": 0.0, "avg_logprob": -0.15536382026279094, "compression_ratio": 1.5959183673469388, "no_speech_prob": 0.00040438934229314327}, {"id": 594, "seek": 184500, "start": 1845.0, "end": 1851.32, "text": " that working towards having supervisors steering QA quality", "tokens": [50364, 300, 1364, 3030, 1419, 42218, 14823, 1249, 32, 3125, 50680], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 595, "seek": 184500, "start": 1851.32, "end": 1855.52, "text": " assurance agents throughout the network of agents", "tokens": [50680, 32189, 12554, 3710, 264, 3209, 295, 12554, 50890], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 596, "seek": 184500, "start": 1855.52, "end": 1857.88, "text": " that are capable of providing some of that feedback", "tokens": [50890, 300, 366, 8189, 295, 6530, 512, 295, 300, 5824, 51008], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 597, "seek": 184500, "start": 1857.88, "end": 1860.16, "text": " that you mentioned earlier, is that kind of the direction", "tokens": [51008, 300, 291, 2835, 3071, 11, 307, 300, 733, 295, 264, 3513, 51122], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 598, "seek": 184500, "start": 1860.16, "end": 1860.88, "text": " that you're going?", "tokens": [51122, 300, 291, 434, 516, 30, 51158], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 599, "seek": 184500, "start": 1860.88, "end": 1863.28, "text": " Or have you tried that and it didn't work?", "tokens": [51158, 1610, 362, 291, 3031, 300, 293, 309, 994, 380, 589, 30, 51278], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 600, "seek": 184500, "start": 1863.28, "end": 1865.96, "text": " Or what are your thoughts in terms", "tokens": [51278, 1610, 437, 366, 428, 4598, 294, 2115, 51412], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 601, "seek": 184500, "start": 1865.96, "end": 1870.28, "text": " of having some of those specialized roles or personas", "tokens": [51412, 295, 1419, 512, 295, 729, 19813, 9604, 420, 12019, 51628], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 602, "seek": 184500, "start": 1870.28, "end": 1873.04, "text": " as a way to help along?", "tokens": [51628, 382, 257, 636, 281, 854, 2051, 30, 51766], "temperature": 0.0, "avg_logprob": -0.1677012092188785, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00028679060051217675}, {"id": 603, "seek": 187304, "start": 1873.04, "end": 1877.76, "text": " Yeah, there are some examples that work pretty well.", "tokens": [50364, 865, 11, 456, 366, 512, 5110, 300, 589, 1238, 731, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 604, "seek": 187304, "start": 1877.76, "end": 1879.8, "text": " I can show some of them.", "tokens": [50600, 286, 393, 855, 512, 295, 552, 13, 50702], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 605, "seek": 187304, "start": 1879.8, "end": 1884.6399999999999, "text": " One is a three-agent setup to solve a multi-agent coding", "tokens": [50702, 1485, 307, 257, 1045, 12, 559, 317, 8657, 281, 5039, 257, 4825, 12, 559, 317, 17720, 50944], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 606, "seek": 187304, "start": 1884.6399999999999, "end": 1885.8, "text": " scenario.", "tokens": [50944, 9005, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 607, "seek": 187304, "start": 1885.8, "end": 1888.6, "text": " The application is for a supply chain optimization.", "tokens": [51002, 440, 3861, 307, 337, 257, 5847, 5021, 19618, 13, 51142], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 608, "seek": 187304, "start": 1888.6, "end": 1891.44, "text": " It's done by another MSR team.", "tokens": [51142, 467, 311, 1096, 538, 1071, 7395, 49, 1469, 13, 51284], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 609, "seek": 187304, "start": 1891.44, "end": 1894.56, "text": " But that solution, in my view, is quite generic.", "tokens": [51284, 583, 300, 3827, 11, 294, 452, 1910, 11, 307, 1596, 19577, 13, 51440], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 610, "seek": 187304, "start": 1894.56, "end": 1897.3999999999999, "text": " It's not restricted to that particular application.", "tokens": [51440, 467, 311, 406, 20608, 281, 300, 1729, 3861, 13, 51582], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 611, "seek": 187304, "start": 1897.3999999999999, "end": 1901.48, "text": " And the setup is like, it's a hierarchical setup.", "tokens": [51582, 400, 264, 8657, 307, 411, 11, 309, 311, 257, 35250, 804, 8657, 13, 51786], "temperature": 0.0, "avg_logprob": -0.2201398390310782, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.0009395436500199139}, {"id": 612, "seek": 190148, "start": 1901.52, "end": 1903.2, "text": " There's the commander on top.", "tokens": [50366, 821, 311, 264, 17885, 322, 1192, 13, 50450], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 613, "seek": 190148, "start": 1903.2, "end": 1907.1200000000001, "text": " There's a writer who is responsible in writing Python", "tokens": [50450, 821, 311, 257, 9936, 567, 307, 6250, 294, 3579, 15329, 50646], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 614, "seek": 190148, "start": 1907.1200000000001, "end": 1907.92, "text": " code.", "tokens": [50646, 3089, 13, 50686], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 615, "seek": 190148, "start": 1907.92, "end": 1911.3600000000001, "text": " The agent can also have access to some proper tools,", "tokens": [50686, 440, 9461, 393, 611, 362, 2105, 281, 512, 2296, 3873, 11, 50858], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 616, "seek": 190148, "start": 1911.3600000000001, "end": 1913.04, "text": " like organization tools.", "tokens": [50858, 411, 4475, 3873, 13, 50942], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 617, "seek": 190148, "start": 1913.04, "end": 1917.2, "text": " And the other subagent is actually Safeguard.", "tokens": [50942, 400, 264, 661, 1422, 559, 317, 307, 767, 14152, 1146, 16981, 13, 51150], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 618, "seek": 190148, "start": 1917.2, "end": 1920.72, "text": " Safeguard is in charge of reviewing code safety.", "tokens": [51150, 14152, 1146, 16981, 307, 294, 4602, 295, 19576, 3089, 4514, 13, 51326], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 619, "seek": 190148, "start": 1920.72, "end": 1923.08, "text": " So the way it works is that the commander receives", "tokens": [51326, 407, 264, 636, 309, 1985, 307, 300, 264, 17885, 20717, 51444], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 620, "seek": 190148, "start": 1923.08, "end": 1924.28, "text": " some user's question.", "tokens": [51444, 512, 4195, 311, 1168, 13, 51504], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 621, "seek": 190148, "start": 1924.28, "end": 1928.4, "text": " It will first ask the writer to write the code.", "tokens": [51504, 467, 486, 700, 1029, 264, 9936, 281, 2464, 264, 3089, 13, 51710], "temperature": 0.0, "avg_logprob": -0.23866453624906994, "compression_ratio": 1.6160337552742616, "no_speech_prob": 0.01448777224868536}, {"id": 622, "seek": 192840, "start": 1928.4, "end": 1930.68, "text": " And after receiving the code, it will ask the Safeguard", "tokens": [50364, 400, 934, 10040, 264, 3089, 11, 309, 486, 1029, 264, 14152, 1146, 16981, 50478], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 623, "seek": 192840, "start": 1930.68, "end": 1932.88, "text": " to review the code for safety.", "tokens": [50478, 281, 3131, 264, 3089, 337, 4514, 13, 50588], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 624, "seek": 192840, "start": 1932.88, "end": 1935.68, "text": " And only if the safety criteria is met,", "tokens": [50588, 400, 787, 498, 264, 4514, 11101, 307, 1131, 11, 50728], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 625, "seek": 192840, "start": 1935.68, "end": 1939.88, "text": " it will round the code and send the result back.", "tokens": [50728, 309, 486, 3098, 264, 3089, 293, 2845, 264, 1874, 646, 13, 50938], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 626, "seek": 192840, "start": 1939.88, "end": 1943.8400000000001, "text": " Otherwise, it will just ask the writer to rewrite the code.", "tokens": [50938, 10328, 11, 309, 486, 445, 1029, 264, 9936, 281, 28132, 264, 3089, 13, 51136], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 627, "seek": 192840, "start": 1943.8400000000001, "end": 1947.1200000000001, "text": " And this can go back and forth because there can be errors.", "tokens": [51136, 400, 341, 393, 352, 646, 293, 5220, 570, 456, 393, 312, 13603, 13, 51300], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 628, "seek": 192840, "start": 1947.1200000000001, "end": 1951.0800000000002, "text": " So when you debug, the writer can do that.", "tokens": [51300, 407, 562, 291, 24083, 11, 264, 9936, 393, 360, 300, 13, 51498], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 629, "seek": 192840, "start": 1951.0800000000002, "end": 1954.0800000000002, "text": " Until the result is correct, the writer", "tokens": [51498, 9088, 264, 1874, 307, 3006, 11, 264, 9936, 51648], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 630, "seek": 192840, "start": 1954.0800000000002, "end": 1956.52, "text": " comes back with a final natural language answer", "tokens": [51648, 1487, 646, 365, 257, 2572, 3303, 2856, 1867, 51770], "temperature": 0.0, "avg_logprob": -0.20014067455730608, "compression_ratio": 1.844155844155844, "no_speech_prob": 0.0013449384132400155}, {"id": 631, "seek": 195652, "start": 1956.52, "end": 1957.4, "text": " to some result.", "tokens": [50364, 281, 512, 1874, 13, 50408], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 632, "seek": 195652, "start": 1957.4, "end": 1959.8, "text": " And the current return that to user.", "tokens": [50408, 400, 264, 2190, 2736, 300, 281, 4195, 13, 50528], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 633, "seek": 195652, "start": 1959.8, "end": 1965.28, "text": " So this is almost a quite simple multi-agent setup,", "tokens": [50528, 407, 341, 307, 1920, 257, 1596, 2199, 4825, 12, 559, 317, 8657, 11, 50802], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 634, "seek": 195652, "start": 1965.28, "end": 1969.4, "text": " but very effective in our application, almost 100%", "tokens": [50802, 457, 588, 4942, 294, 527, 3861, 11, 1920, 2319, 4, 51008], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 635, "seek": 195652, "start": 1969.4, "end": 1971.92, "text": " correct every time.", "tokens": [51008, 3006, 633, 565, 13, 51134], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 636, "seek": 195652, "start": 1971.92, "end": 1977.04, "text": " One kind of lesson is if we merge the capability of the writer", "tokens": [51134, 1485, 733, 295, 6898, 307, 498, 321, 22183, 264, 13759, 295, 264, 9936, 51390], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 637, "seek": 195652, "start": 1977.04, "end": 1979.6, "text": " and the Safeguard into one agent,", "tokens": [51390, 293, 264, 14152, 1146, 16981, 666, 472, 9461, 11, 51518], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 638, "seek": 195652, "start": 1979.6, "end": 1982.84, "text": " it doesn't work that well, especially in the code safety", "tokens": [51518, 309, 1177, 380, 589, 300, 731, 11, 2318, 294, 264, 3089, 4514, 51680], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 639, "seek": 195652, "start": 1982.84, "end": 1984.56, "text": " part.", "tokens": [51680, 644, 13, 51766], "temperature": 0.0, "avg_logprob": -0.27653730292069284, "compression_ratio": 1.5022421524663676, "no_speech_prob": 0.0008825657423585653}, {"id": 640, "seek": 198456, "start": 1984.6, "end": 1988.6399999999999, "text": " So we have the experiments in our paper.", "tokens": [50366, 407, 321, 362, 264, 12050, 294, 527, 3035, 13, 50568], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 641, "seek": 198456, "start": 1988.6399999999999, "end": 1993.6799999999998, "text": " We found that if you merge them, then the accuracy", "tokens": [50568, 492, 1352, 300, 498, 291, 22183, 552, 11, 550, 264, 14170, 50820], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 642, "seek": 198456, "start": 1993.6799999999998, "end": 1998.24, "text": " for detecting code safety issue will drop significantly,", "tokens": [50820, 337, 40237, 3089, 4514, 2734, 486, 3270, 10591, 11, 51048], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 643, "seek": 198456, "start": 1998.24, "end": 2001.0, "text": " both for GP4 and GP3.5 Turbo, but more", "tokens": [51048, 1293, 337, 26039, 19, 293, 26039, 18, 13, 20, 35848, 11, 457, 544, 51186], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 644, "seek": 198456, "start": 2001.0, "end": 2003.84, "text": " significantly for GP3.5.", "tokens": [51186, 10591, 337, 26039, 18, 13, 20, 13, 51328], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 645, "seek": 198456, "start": 2003.84, "end": 2008.72, "text": " So this kind of hints that one agent,", "tokens": [51328, 407, 341, 733, 295, 27271, 300, 472, 9461, 11, 51572], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 646, "seek": 198456, "start": 2008.72, "end": 2011.8799999999999, "text": " if you ask to both suggest a solution", "tokens": [51572, 498, 291, 1029, 281, 1293, 3402, 257, 3827, 51730], "temperature": 0.0, "avg_logprob": -0.22823345370408965, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.0002650659589562565}, {"id": 647, "seek": 201188, "start": 2011.88, "end": 2016.24, "text": " and check the solution suggested by itself,", "tokens": [50364, 293, 1520, 264, 3827, 10945, 538, 2564, 11, 50582], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 648, "seek": 201188, "start": 2016.24, "end": 2018.8400000000001, "text": " have a bias.", "tokens": [50582, 362, 257, 12577, 13, 50712], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 649, "seek": 201188, "start": 2018.8400000000001, "end": 2022.3600000000001, "text": " But we separate them and also prevent them", "tokens": [50712, 583, 321, 4994, 552, 293, 611, 4871, 552, 50888], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 650, "seek": 201188, "start": 2022.3600000000001, "end": 2025.16, "text": " to talk to each other, kind of make", "tokens": [50888, 281, 751, 281, 1184, 661, 11, 733, 295, 652, 51028], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 651, "seek": 201188, "start": 2025.16, "end": 2027.64, "text": " them work in an adversarial setting.", "tokens": [51028, 552, 589, 294, 364, 17641, 44745, 3287, 13, 51152], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 652, "seek": 201188, "start": 2027.64, "end": 2029.4, "text": " It does it better.", "tokens": [51152, 467, 775, 309, 1101, 13, 51240], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 653, "seek": 201188, "start": 2029.4, "end": 2031.64, "text": " So that is one observation.", "tokens": [51240, 407, 300, 307, 472, 14816, 13, 51352], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 654, "seek": 201188, "start": 2031.64, "end": 2034.3600000000001, "text": " But we also have other kind of scenarios", "tokens": [51352, 583, 321, 611, 362, 661, 733, 295, 15077, 51488], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 655, "seek": 201188, "start": 2034.3600000000001, "end": 2037.8400000000001, "text": " where we do involve every agent in one group chat.", "tokens": [51488, 689, 321, 360, 9494, 633, 9461, 294, 472, 1594, 5081, 13, 51662], "temperature": 0.0, "avg_logprob": -0.23195884967672414, "compression_ratio": 1.555, "no_speech_prob": 0.0011328172404319048}, {"id": 656, "seek": 203784, "start": 2037.8799999999999, "end": 2039.9199999999998, "text": " So everyone also sees other's message", "tokens": [50366, 407, 1518, 611, 8194, 661, 311, 3636, 50468], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 657, "seek": 203784, "start": 2039.9199999999998, "end": 2042.1999999999998, "text": " and can reply back.", "tokens": [50468, 293, 393, 16972, 646, 13, 50582], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 658, "seek": 203784, "start": 2042.1999999999998, "end": 2046.04, "text": " It also works sometimes for other tasks.", "tokens": [50582, 467, 611, 1985, 2171, 337, 661, 9608, 13, 50774], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 659, "seek": 203784, "start": 2046.04, "end": 2051.12, "text": " For example, a critique to suggest a visualization", "tokens": [50774, 1171, 1365, 11, 257, 25673, 281, 3402, 257, 25801, 51028], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 660, "seek": 203784, "start": 2051.12, "end": 2053.6, "text": " criteria for a visualization task.", "tokens": [51028, 11101, 337, 257, 25801, 5633, 13, 51152], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 661, "seek": 203784, "start": 2053.6, "end": 2058.88, "text": " You can have one agent write code and another to criticize.", "tokens": [51152, 509, 393, 362, 472, 9461, 2464, 3089, 293, 1071, 281, 31010, 13, 51416], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 662, "seek": 203784, "start": 2058.88, "end": 2059.72, "text": " It sort of works.", "tokens": [51416, 467, 1333, 295, 1985, 13, 51458], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 663, "seek": 203784, "start": 2059.72, "end": 2064.52, "text": " But my intuition is still that if we put every agent work", "tokens": [51458, 583, 452, 24002, 307, 920, 300, 498, 321, 829, 633, 9461, 589, 51698], "temperature": 0.0, "avg_logprob": -0.2575742675036919, "compression_ratio": 1.6161616161616161, "no_speech_prob": 0.0049789478071033955}, {"id": 664, "seek": 206452, "start": 2064.56, "end": 2068.48, "text": " together always in a group chat, it may not always work", "tokens": [50366, 1214, 1009, 294, 257, 1594, 5081, 11, 309, 815, 406, 1009, 589, 50562], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 665, "seek": 206452, "start": 2068.48, "end": 2070.64, "text": " because they may have the tendency", "tokens": [50562, 570, 436, 815, 362, 264, 18187, 50670], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 666, "seek": 206452, "start": 2070.64, "end": 2077.16, "text": " to agree with each other and try hard to challenge.", "tokens": [50670, 281, 3986, 365, 1184, 661, 293, 853, 1152, 281, 3430, 13, 50996], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 667, "seek": 206452, "start": 2077.16, "end": 2082.2, "text": " I would say it's case by case for different applications.", "tokens": [50996, 286, 576, 584, 309, 311, 1389, 538, 1389, 337, 819, 5821, 13, 51248], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 668, "seek": 206452, "start": 2082.2, "end": 2085.7599999999998, "text": " There's also some benefit of doing it in this group chat", "tokens": [51248, 821, 311, 611, 512, 5121, 295, 884, 309, 294, 341, 1594, 5081, 51426], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 669, "seek": 206452, "start": 2085.7599999999998, "end": 2087.7599999999998, "text": " because it's relatively simple.", "tokens": [51426, 570, 309, 311, 7226, 2199, 13, 51526], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 670, "seek": 206452, "start": 2087.7599999999998, "end": 2090.6, "text": " You don't need to do very hard about handling", "tokens": [51526, 509, 500, 380, 643, 281, 360, 588, 1152, 466, 13175, 51668], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 671, "seek": 206452, "start": 2090.6, "end": 2093.44, "text": " the message separation.", "tokens": [51668, 264, 3636, 14634, 13, 51810], "temperature": 0.0, "avg_logprob": -0.20392723921891098, "compression_ratio": 1.6026785714285714, "no_speech_prob": 0.011672433465719223}, {"id": 672, "seek": 209344, "start": 2093.44, "end": 2095.48, "text": " You can simply define your agents", "tokens": [50364, 509, 393, 2935, 6964, 428, 12554, 50466], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 673, "seek": 209344, "start": 2095.48, "end": 2098.92, "text": " and put them in a group chat and get them running quickly.", "tokens": [50466, 293, 829, 552, 294, 257, 1594, 5081, 293, 483, 552, 2614, 2661, 13, 50638], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 674, "seek": 209344, "start": 2098.92, "end": 2100.2000000000003, "text": " So that's one benefit of group chat", "tokens": [50638, 407, 300, 311, 472, 5121, 295, 1594, 5081, 50702], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 675, "seek": 209344, "start": 2100.2000000000003, "end": 2102.6, "text": " and seeing many people are using that approach.", "tokens": [50702, 293, 2577, 867, 561, 366, 1228, 300, 3109, 13, 50822], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 676, "seek": 209344, "start": 2102.6, "end": 2107.08, "text": " But just to be careful that it may not always", "tokens": [50822, 583, 445, 281, 312, 5026, 300, 309, 815, 406, 1009, 51046], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 677, "seek": 209344, "start": 2107.08, "end": 2111.16, "text": " work because of the limitations of the models.", "tokens": [51046, 589, 570, 295, 264, 15705, 295, 264, 5245, 13, 51250], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 678, "seek": 209344, "start": 2111.16, "end": 2113.36, "text": " So that's really fascinating to me.", "tokens": [51250, 407, 300, 311, 534, 10343, 281, 385, 13, 51360], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 679, "seek": 209344, "start": 2113.36, "end": 2115.92, "text": " And my intuition was the same.", "tokens": [51360, 400, 452, 24002, 390, 264, 912, 13, 51488], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 680, "seek": 209344, "start": 2115.92, "end": 2118.28, "text": " But it's interesting to have that validation", "tokens": [51488, 583, 309, 311, 1880, 281, 362, 300, 24071, 51606], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 681, "seek": 209344, "start": 2118.28, "end": 2119.52, "text": " from another perspective.", "tokens": [51606, 490, 1071, 4585, 13, 51668], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 682, "seek": 209344, "start": 2119.52, "end": 2123.12, "text": " So it's almost like even though the underlying model", "tokens": [51668, 407, 309, 311, 1920, 411, 754, 1673, 264, 14217, 2316, 51848], "temperature": 0.0, "avg_logprob": -0.16568573589982658, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0007791448733769357}, {"id": 683, "seek": 212312, "start": 2123.12, "end": 2127.16, "text": " is GPT-4 running all of the agents or 3.5 turbo,", "tokens": [50364, 307, 26039, 51, 12, 19, 2614, 439, 295, 264, 12554, 420, 805, 13, 20, 20902, 11, 50566], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 684, "seek": 212312, "start": 2127.16, "end": 2133.08, "text": " there's still a positive effect from using division of labor,", "tokens": [50566, 456, 311, 920, 257, 3353, 1802, 490, 1228, 10044, 295, 5938, 11, 50862], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 685, "seek": 212312, "start": 2133.08, "end": 2134.72, "text": " which the division of labor comes", "tokens": [50862, 597, 264, 10044, 295, 5938, 1487, 50944], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 686, "seek": 212312, "start": 2134.72, "end": 2138.3199999999997, "text": " from the history of human work.", "tokens": [50944, 490, 264, 2503, 295, 1952, 589, 13, 51124], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 687, "seek": 212312, "start": 2138.3199999999997, "end": 2142.0, "text": " And so just taking a moment, obviously these models", "tokens": [51124, 400, 370, 445, 1940, 257, 1623, 11, 2745, 613, 5245, 51308], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 688, "seek": 212312, "start": 2142.0, "end": 2143.3199999999997, "text": " do not work like human brains.", "tokens": [51308, 360, 406, 589, 411, 1952, 15442, 13, 51374], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 689, "seek": 212312, "start": 2143.3199999999997, "end": 2147.72, "text": " But when you have an agent with a very specific task and mission", "tokens": [51374, 583, 562, 291, 362, 364, 9461, 365, 257, 588, 2685, 5633, 293, 4447, 51594], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 690, "seek": 212312, "start": 2147.72, "end": 2151.3599999999997, "text": " and set of success criteria, that effectiveness", "tokens": [51594, 293, 992, 295, 2245, 11101, 11, 300, 21208, 51776], "temperature": 0.0, "avg_logprob": -0.14992793564943924, "compression_ratio": 1.576271186440678, "no_speech_prob": 0.0046044010668993}, {"id": 691, "seek": 215136, "start": 2151.4, "end": 2154.1200000000003, "text": " of the division of labor still helps,", "tokens": [50366, 295, 264, 10044, 295, 5938, 920, 3665, 11, 50502], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 692, "seek": 215136, "start": 2154.1200000000003, "end": 2157.1600000000003, "text": " even though it's just activating the latent capabilities", "tokens": [50502, 754, 1673, 309, 311, 445, 42481, 264, 48994, 10862, 50654], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 693, "seek": 215136, "start": 2157.1600000000003, "end": 2159.48, "text": " within the same underpinning model.", "tokens": [50654, 1951, 264, 912, 833, 17836, 773, 2316, 13, 50770], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 694, "seek": 215136, "start": 2159.48, "end": 2162.32, "text": " And then another intuition or a principle", "tokens": [50770, 400, 550, 1071, 24002, 420, 257, 8665, 50912], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 695, "seek": 215136, "start": 2162.32, "end": 2167.44, "text": " that I want to reiterate is the idea that, in some cases,", "tokens": [50912, 300, 286, 528, 281, 33528, 307, 264, 1558, 300, 11, 294, 512, 3331, 11, 51168], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 696, "seek": 215136, "start": 2167.44, "end": 2170.96, "text": " group work makes sense, but in some cases, it doesn't.", "tokens": [51168, 1594, 589, 1669, 2020, 11, 457, 294, 512, 3331, 11, 309, 1177, 380, 13, 51344], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 697, "seek": 215136, "start": 2170.96, "end": 2173.6800000000003, "text": " It's almost like the same difference in humans", "tokens": [51344, 467, 311, 1920, 411, 264, 912, 2649, 294, 6255, 51480], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 698, "seek": 215136, "start": 2173.6800000000003, "end": 2178.8, "text": " where the power of introverts, doing solo work on your own", "tokens": [51480, 689, 264, 1347, 295, 12897, 36999, 11, 884, 6944, 589, 322, 428, 1065, 51736], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 699, "seek": 215136, "start": 2178.8, "end": 2180.92, "text": " versus doing collaborative group work.", "tokens": [51736, 5717, 884, 16555, 1594, 589, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14416095062538428, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0012063563335686922}, {"id": 700, "seek": 218092, "start": 2180.96, "end": 2183.84, "text": " So again, not saying that they're operating like humans,", "tokens": [50366, 407, 797, 11, 406, 1566, 300, 436, 434, 7447, 411, 6255, 11, 50510], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 701, "seek": 218092, "start": 2183.84, "end": 2186.6800000000003, "text": " but it's really interesting to see some of these parallels", "tokens": [50510, 457, 309, 311, 534, 1880, 281, 536, 512, 295, 613, 44223, 50652], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 702, "seek": 218092, "start": 2186.6800000000003, "end": 2192.8, "text": " emerge between multi-agent work and the nature of human labor.", "tokens": [50652, 21511, 1296, 4825, 12, 559, 317, 589, 293, 264, 3687, 295, 1952, 5938, 13, 50958], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 703, "seek": 218092, "start": 2192.8, "end": 2195.6800000000003, "text": " So yeah, very fascinating.", "tokens": [50958, 407, 1338, 11, 588, 10343, 13, 51102], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 704, "seek": 218092, "start": 2195.6800000000003, "end": 2199.56, "text": " And it's interesting because in some of the conversations", "tokens": [51102, 400, 309, 311, 1880, 570, 294, 512, 295, 264, 7315, 51296], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 705, "seek": 218092, "start": 2199.56, "end": 2202.4, "text": " that I've had and some of the observations that I've made,", "tokens": [51296, 300, 286, 600, 632, 293, 512, 295, 264, 18163, 300, 286, 600, 1027, 11, 51438], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 706, "seek": 218092, "start": 2202.4, "end": 2204.64, "text": " it's almost like what we're doing with these agents,", "tokens": [51438, 309, 311, 1920, 411, 437, 321, 434, 884, 365, 613, 12554, 11, 51550], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 707, "seek": 218092, "start": 2204.64, "end": 2209.04, "text": " these groups of agents, is recreating a corporation.", "tokens": [51550, 613, 3935, 295, 12554, 11, 307, 850, 44613, 257, 22197, 13, 51770], "temperature": 0.0, "avg_logprob": -0.15474889896534108, "compression_ratio": 1.7469387755102042, "no_speech_prob": 0.00020338421745691448}, {"id": 708, "seek": 220904, "start": 2209.04, "end": 2211.68, "text": " You might have a CEO or a boss or a supervisor,", "tokens": [50364, 509, 1062, 362, 257, 9282, 420, 257, 5741, 420, 257, 24610, 11, 50496], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 709, "seek": 220904, "start": 2211.68, "end": 2214.16, "text": " and then you have the coder and then the QA.", "tokens": [50496, 293, 550, 291, 362, 264, 17656, 260, 293, 550, 264, 1249, 32, 13, 50620], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 710, "seek": 220904, "start": 2214.16, "end": 2216.48, "text": " So it's a very similar structure.", "tokens": [50620, 407, 309, 311, 257, 588, 2531, 3877, 13, 50736], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 711, "seek": 220904, "start": 2216.48, "end": 2220.2799999999997, "text": " So do you have any other major insights or lessons", "tokens": [50736, 407, 360, 291, 362, 604, 661, 2563, 14310, 420, 8820, 50926], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 712, "seek": 220904, "start": 2220.2799999999997, "end": 2224.2, "text": " that you think are either recently or super valuable", "tokens": [50926, 300, 291, 519, 366, 2139, 3938, 420, 1687, 8263, 51122], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 713, "seek": 220904, "start": 2224.2, "end": 2225.96, "text": " that you want to share with other researchers", "tokens": [51122, 300, 291, 528, 281, 2073, 365, 661, 10309, 51210], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 714, "seek": 220904, "start": 2225.96, "end": 2227.96, "text": " or that you would recommend?", "tokens": [51210, 420, 300, 291, 576, 2748, 30, 51310], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 715, "seek": 220904, "start": 2227.96, "end": 2228.72, "text": " Sure, sure.", "tokens": [51310, 4894, 11, 988, 13, 51348], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 716, "seek": 220904, "start": 2228.72, "end": 2230.52, "text": " There are so many of them.", "tokens": [51348, 821, 366, 370, 867, 295, 552, 13, 51438], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 717, "seek": 220904, "start": 2230.52, "end": 2235.12, "text": " I can give you a few examples.", "tokens": [51438, 286, 393, 976, 291, 257, 1326, 5110, 13, 51668], "temperature": 0.0, "avg_logprob": -0.14918654432920653, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.0018081099260598421}, {"id": 718, "seek": 223512, "start": 2235.12, "end": 2240.4, "text": " One thing is the chat, the conversation perspective.", "tokens": [50364, 1485, 551, 307, 264, 5081, 11, 264, 3761, 4585, 13, 50628], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 719, "seek": 223512, "start": 2240.4, "end": 2245.8399999999997, "text": " I mentioned earlier that chat GPD is a big inspiration.", "tokens": [50628, 286, 2835, 3071, 300, 5081, 460, 17349, 307, 257, 955, 10249, 13, 50900], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 720, "seek": 223512, "start": 2245.8399999999997, "end": 2247.56, "text": " Certainly for many people.", "tokens": [50900, 16628, 337, 867, 561, 13, 50986], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 721, "seek": 223512, "start": 2247.56, "end": 2250.68, "text": " But for me, there's a personal story", "tokens": [50986, 583, 337, 385, 11, 456, 311, 257, 2973, 1657, 51142], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 722, "seek": 223512, "start": 2250.68, "end": 2254.12, "text": " about what specific user I felt from it.", "tokens": [51142, 466, 437, 2685, 4195, 286, 2762, 490, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 723, "seek": 223512, "start": 2254.12, "end": 2258.24, "text": " It's a reminder of something I learned back in my college", "tokens": [51314, 467, 311, 257, 13548, 295, 746, 286, 3264, 646, 294, 452, 3859, 51520], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 724, "seek": 223512, "start": 2258.24, "end": 2264.0, "text": " from a professor who told me that conversation is a provable", "tokens": [51520, 490, 257, 8304, 567, 1907, 385, 300, 3761, 307, 257, 1439, 712, 51808], "temperature": 0.0, "avg_logprob": -0.2492379805620979, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.00198638834990561}, {"id": 725, "seek": 226400, "start": 2264.0, "end": 2269.84, "text": " way of making a good progress of learning.", "tokens": [50364, 636, 295, 1455, 257, 665, 4205, 295, 2539, 13, 50656], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 726, "seek": 226400, "start": 2269.84, "end": 2275.24, "text": " I don't remember the exact quote of that, but it's roughly that.", "tokens": [50656, 286, 500, 380, 1604, 264, 1900, 6513, 295, 300, 11, 457, 309, 311, 9810, 300, 13, 50926], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 727, "seek": 226400, "start": 2275.24, "end": 2279.12, "text": " So basically, he's trying to say, conversation", "tokens": [50926, 407, 1936, 11, 415, 311, 1382, 281, 584, 11, 3761, 51120], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 728, "seek": 226400, "start": 2279.12, "end": 2283.64, "text": " is a very powerful form of either learning or making", "tokens": [51120, 307, 257, 588, 4005, 1254, 295, 2139, 2539, 420, 1455, 51346], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 729, "seek": 226400, "start": 2283.64, "end": 2289.12, "text": " progress, or et cetera, that many people didn't realize", "tokens": [51346, 4205, 11, 420, 1030, 11458, 11, 300, 867, 561, 994, 380, 4325, 51620], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 730, "seek": 226400, "start": 2289.12, "end": 2291.68, "text": " it's how important it is.", "tokens": [51620, 309, 311, 577, 1021, 309, 307, 13, 51748], "temperature": 0.0, "avg_logprob": -0.21231392102363783, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0025459658354520798}, {"id": 731, "seek": 229168, "start": 2291.68, "end": 2295.64, "text": " And there are some theoretical roots there.", "tokens": [50364, 400, 456, 366, 512, 20864, 10669, 456, 13, 50562], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 732, "seek": 229168, "start": 2295.64, "end": 2301.08, "text": " So that's one reason I'm so kind of so sure,", "tokens": [50562, 407, 300, 311, 472, 1778, 286, 478, 370, 733, 295, 370, 988, 11, 50834], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 733, "seek": 229168, "start": 2301.08, "end": 2305.9199999999996, "text": " or so I have so much belief in using conversation", "tokens": [50834, 420, 370, 286, 362, 370, 709, 7107, 294, 1228, 3761, 51076], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 734, "seek": 229168, "start": 2305.9199999999996, "end": 2312.3599999999997, "text": " as the central medium of the command multi-agent interface.", "tokens": [51076, 382, 264, 5777, 6399, 295, 264, 5622, 4825, 12, 559, 317, 9226, 13, 51398], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 735, "seek": 229168, "start": 2312.3599999999997, "end": 2317.04, "text": " Again, I know there's a science, although I didn't have time", "tokens": [51398, 3764, 11, 286, 458, 456, 311, 257, 3497, 11, 4878, 286, 994, 380, 362, 565, 51632], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 736, "seek": 229168, "start": 2317.04, "end": 2319.68, "text": " to find out which reference it was.", "tokens": [51632, 281, 915, 484, 597, 6408, 309, 390, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24521665219907407, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.001303656492382288}, {"id": 737, "seek": 231968, "start": 2319.68, "end": 2325.6, "text": " But I know that, so it gave me the confidence or the belief", "tokens": [50364, 583, 286, 458, 300, 11, 370, 309, 2729, 385, 264, 6687, 420, 264, 7107, 50660], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 738, "seek": 231968, "start": 2325.6, "end": 2327.56, "text": " that this is the right thing to do.", "tokens": [50660, 300, 341, 307, 264, 558, 551, 281, 360, 13, 50758], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 739, "seek": 231968, "start": 2327.56, "end": 2330.24, "text": " I think one of my favorite courses,", "tokens": [50758, 286, 519, 472, 295, 452, 2954, 7712, 11, 50892], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 740, "seek": 231968, "start": 2330.24, "end": 2332.3199999999997, "text": " Jimmy actually found me some reference", "tokens": [50892, 15709, 767, 1352, 385, 512, 6408, 50996], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 741, "seek": 231968, "start": 2332.3199999999997, "end": 2335.0, "text": " from a social scientist.", "tokens": [50996, 490, 257, 2093, 12662, 13, 51130], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 742, "seek": 231968, "start": 2335.0, "end": 2337.44, "text": " He mentioned something similar to that.", "tokens": [51130, 634, 2835, 746, 2531, 281, 300, 13, 51252], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 743, "seek": 231968, "start": 2337.44, "end": 2342.8799999999997, "text": " Yeah, so this is one lesson, one kind of unique thing", "tokens": [51252, 865, 11, 370, 341, 307, 472, 6898, 11, 472, 733, 295, 3845, 551, 51524], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 744, "seek": 231968, "start": 2342.8799999999997, "end": 2346.3199999999997, "text": " that I don't think many people have really.", "tokens": [51524, 300, 286, 500, 380, 519, 867, 561, 362, 534, 13, 51696], "temperature": 0.0, "avg_logprob": -0.3160871505737305, "compression_ratio": 1.5781990521327014, "no_speech_prob": 0.0015209526754915714}, {"id": 745, "seek": 234632, "start": 2346.32, "end": 2350.6400000000003, "text": " They kind of understand chatGVT is very powerful,", "tokens": [50364, 814, 733, 295, 1223, 5081, 38, 53, 51, 307, 588, 4005, 11, 50580], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 746, "seek": 234632, "start": 2350.6400000000003, "end": 2353.84, "text": " and also get a lot of useful experience from that.", "tokens": [50580, 293, 611, 483, 257, 688, 295, 4420, 1752, 490, 300, 13, 50740], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 747, "seek": 234632, "start": 2353.84, "end": 2357.96, "text": " But maybe this science part of it is less known.", "tokens": [50740, 583, 1310, 341, 3497, 644, 295, 309, 307, 1570, 2570, 13, 50946], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 748, "seek": 234632, "start": 2357.96, "end": 2360.04, "text": " So that's one thing I would share.", "tokens": [50946, 407, 300, 311, 472, 551, 286, 576, 2073, 13, 51050], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 749, "seek": 234632, "start": 2360.04, "end": 2363.2000000000003, "text": " Another inspiration source, as I told you,", "tokens": [51050, 3996, 10249, 4009, 11, 382, 286, 1907, 291, 11, 51208], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 750, "seek": 234632, "start": 2363.2000000000003, "end": 2366.28, "text": " so auto-gen is really inspired by many different things,", "tokens": [51208, 370, 8399, 12, 1766, 307, 534, 7547, 538, 867, 819, 721, 11, 51362], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 751, "seek": 234632, "start": 2366.28, "end": 2369.2000000000003, "text": " many projects I've worked on before,", "tokens": [51362, 867, 4455, 286, 600, 2732, 322, 949, 11, 51508], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 752, "seek": 234632, "start": 2369.2000000000003, "end": 2370.6800000000003, "text": " and all the lessons I've learned.", "tokens": [51508, 293, 439, 264, 8820, 286, 600, 3264, 13, 51582], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 753, "seek": 234632, "start": 2370.6800000000003, "end": 2374.4, "text": " So another one, for example, is the operating system.", "tokens": [51582, 407, 1071, 472, 11, 337, 1365, 11, 307, 264, 7447, 1185, 13, 51768], "temperature": 0.0, "avg_logprob": -0.30683728626796175, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0007790848612785339}, {"id": 754, "seek": 237440, "start": 2374.4, "end": 2376.44, "text": " So this is also not so obvious.", "tokens": [50364, 407, 341, 307, 611, 406, 370, 6322, 13, 50466], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 755, "seek": 237440, "start": 2376.44, "end": 2380.12, "text": " When we talk about AI, why do we talk about operating systems?", "tokens": [50466, 1133, 321, 751, 466, 7318, 11, 983, 360, 321, 751, 466, 7447, 3652, 30, 50650], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 756, "seek": 237440, "start": 2380.12, "end": 2383.84, "text": " I think the several things, several inspiration", "tokens": [50650, 286, 519, 264, 2940, 721, 11, 2940, 10249, 50836], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 757, "seek": 237440, "start": 2383.84, "end": 2386.52, "text": " I take from the success of operating systems.", "tokens": [50836, 286, 747, 490, 264, 2245, 295, 7447, 3652, 13, 50970], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 758, "seek": 237440, "start": 2386.52, "end": 2392.56, "text": " One is the idea of maximizing the utility of the most valuable", "tokens": [50970, 1485, 307, 264, 1558, 295, 5138, 3319, 264, 14877, 295, 264, 881, 8263, 51272], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 759, "seek": 237440, "start": 2392.56, "end": 2395.12, "text": " resource you have.", "tokens": [51272, 7684, 291, 362, 13, 51400], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 760, "seek": 237440, "start": 2395.12, "end": 2399.52, "text": " So in old days, it's like the CPU, the GPU.", "tokens": [51400, 407, 294, 1331, 1708, 11, 309, 311, 411, 264, 13199, 11, 264, 18407, 13, 51620], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 761, "seek": 237440, "start": 2399.52, "end": 2402.36, "text": " But I think in the new era of AI,", "tokens": [51620, 583, 286, 519, 294, 264, 777, 4249, 295, 7318, 11, 51762], "temperature": 0.0, "avg_logprob": -0.2127594655873824, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.00034583074739202857}, {"id": 762, "seek": 240236, "start": 2402.44, "end": 2406.84, "text": " these powerful, not even models is so valuable resource", "tokens": [50368, 613, 4005, 11, 406, 754, 5245, 307, 370, 8263, 7684, 50588], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 763, "seek": 240236, "start": 2406.84, "end": 2413.1200000000003, "text": " and building an operating system around them, right?", "tokens": [50588, 293, 2390, 364, 7447, 1185, 926, 552, 11, 558, 30, 50902], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 764, "seek": 240236, "start": 2413.1200000000003, "end": 2415.96, "text": " And maximizing their utility, but to give them", "tokens": [50902, 400, 5138, 3319, 641, 14877, 11, 457, 281, 976, 552, 51044], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 765, "seek": 240236, "start": 2415.96, "end": 2421.2000000000003, "text": " the necessary peripherals and do the right coordination.", "tokens": [51044, 264, 4818, 26807, 1124, 293, 360, 264, 558, 21252, 13, 51306], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 766, "seek": 240236, "start": 2421.2000000000003, "end": 2426.2400000000002, "text": " And it's super critical from the system point of view.", "tokens": [51306, 400, 309, 311, 1687, 4924, 490, 264, 1185, 935, 295, 1910, 13, 51558], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 767, "seek": 240236, "start": 2426.2400000000002, "end": 2428.84, "text": " And also, so that operating system is really", "tokens": [51558, 400, 611, 11, 370, 300, 7447, 1185, 307, 534, 51688], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 768, "seek": 240236, "start": 2428.84, "end": 2431.2400000000002, "text": " you can build a platform that can support", "tokens": [51688, 291, 393, 1322, 257, 3663, 300, 393, 1406, 51808], "temperature": 0.0, "avg_logprob": -0.2968829211066751, "compression_ratio": 1.631336405529954, "no_speech_prob": 0.0003149017575196922}, {"id": 769, "seek": 243124, "start": 2431.24, "end": 2434.08, "text": " many diverse applications on top of that.", "tokens": [50364, 867, 9521, 5821, 322, 1192, 295, 300, 13, 50506], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 770, "seek": 243124, "start": 2434.08, "end": 2438.2, "text": " So we need to design a very generic robust kind of system", "tokens": [50506, 407, 321, 643, 281, 1715, 257, 588, 19577, 13956, 733, 295, 1185, 50712], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 771, "seek": 243124, "start": 2438.2, "end": 2439.3199999999997, "text": " to do that, right?", "tokens": [50712, 281, 360, 300, 11, 558, 30, 50768], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 772, "seek": 243124, "start": 2439.3199999999997, "end": 2442.4799999999996, "text": " So these are all the design principles", "tokens": [50768, 407, 613, 366, 439, 264, 1715, 9156, 50926], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 773, "seek": 243124, "start": 2442.4799999999996, "end": 2445.3599999999997, "text": " we try to use when we design AutoGen.", "tokens": [50926, 321, 853, 281, 764, 562, 321, 1715, 13738, 26647, 13, 51070], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 774, "seek": 243124, "start": 2445.3599999999997, "end": 2449.8799999999997, "text": " And similarly, the idea of object-oriented programming", "tokens": [51070, 400, 14138, 11, 264, 1558, 295, 2657, 12, 27414, 9410, 51296], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 775, "seek": 243124, "start": 2449.8799999999997, "end": 2452.4399999999996, "text": " is very useful.", "tokens": [51296, 307, 588, 4420, 13, 51424], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 776, "seek": 243124, "start": 2452.4399999999996, "end": 2454.8399999999997, "text": " So many developers have very interesting ideas", "tokens": [51424, 407, 867, 8849, 362, 588, 1880, 3487, 51544], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 777, "seek": 243124, "start": 2454.8399999999997, "end": 2457.8399999999997, "text": " they want to try and develop.", "tokens": [51544, 436, 528, 281, 853, 293, 1499, 13, 51694], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 778, "seek": 243124, "start": 2457.8399999999997, "end": 2460.4399999999996, "text": " And now with this framework that hides a lot of complexity", "tokens": [51694, 400, 586, 365, 341, 8388, 300, 35953, 257, 688, 295, 14024, 51824], "temperature": 0.0, "avg_logprob": -0.19202406589801496, "compression_ratio": 1.6144578313253013, "no_speech_prob": 0.0005974518717266619}, {"id": 779, "seek": 246044, "start": 2460.44, "end": 2464.56, "text": " inside the framework, they're able to kind of do the things", "tokens": [50364, 1854, 264, 8388, 11, 436, 434, 1075, 281, 733, 295, 360, 264, 721, 50570], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 780, "seek": 246044, "start": 2464.56, "end": 2466.4, "text": " they want more easily.", "tokens": [50570, 436, 528, 544, 3612, 13, 50662], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 781, "seek": 246044, "start": 2466.4, "end": 2468.92, "text": " That's a part of abstraction.", "tokens": [50662, 663, 311, 257, 644, 295, 37765, 13, 50788], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 782, "seek": 246044, "start": 2468.92, "end": 2475.16, "text": " I already mentioned the agent, notion, automation,", "tokens": [50788, 286, 1217, 2835, 264, 9461, 11, 10710, 11, 17769, 11, 51100], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 783, "seek": 246044, "start": 2475.16, "end": 2476.28, "text": " inspiration.", "tokens": [51100, 10249, 13, 51156], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 784, "seek": 246044, "start": 2476.28, "end": 2479.16, "text": " The one thing I want to mention is open source, right?", "tokens": [51156, 440, 472, 551, 286, 528, 281, 2152, 307, 1269, 4009, 11, 558, 30, 51300], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 785, "seek": 246044, "start": 2479.16, "end": 2482.44, "text": " The concept of open source is that it", "tokens": [51300, 440, 3410, 295, 1269, 4009, 307, 300, 309, 51464], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 786, "seek": 246044, "start": 2482.44, "end": 2486.04, "text": " can solve the common problems that community needs", "tokens": [51464, 393, 5039, 264, 2689, 2740, 300, 1768, 2203, 51644], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 787, "seek": 246044, "start": 2486.04, "end": 2488.16, "text": " and make it really easy to use.", "tokens": [51644, 293, 652, 309, 534, 1858, 281, 764, 13, 51750], "temperature": 0.0, "avg_logprob": -0.263285941265999, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.00030043715378269553}, {"id": 788, "seek": 248816, "start": 2488.16, "end": 2493.16, "text": " So those are probably most modern kind of things", "tokens": [50364, 407, 729, 366, 1391, 881, 4363, 733, 295, 721, 50614], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 789, "seek": 248816, "start": 2493.16, "end": 2497.2, "text": " that can get good open source adoption", "tokens": [50614, 300, 393, 483, 665, 1269, 4009, 19215, 50816], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 790, "seek": 248816, "start": 2497.2, "end": 2500.96, "text": " and build something that the community loves, right?", "tokens": [50816, 293, 1322, 746, 300, 264, 1768, 6752, 11, 558, 30, 51004], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 791, "seek": 248816, "start": 2500.96, "end": 2503.44, "text": " Yeah, so I think that is my valuable lesson", "tokens": [51004, 865, 11, 370, 286, 519, 300, 307, 452, 8263, 6898, 51128], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 792, "seek": 248816, "start": 2503.44, "end": 2505.3999999999996, "text": " I want to share with all researchers, right?", "tokens": [51128, 286, 528, 281, 2073, 365, 439, 10309, 11, 558, 30, 51226], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 793, "seek": 248816, "start": 2505.3999999999996, "end": 2508.96, "text": " If you want to get their research adopted", "tokens": [51226, 759, 291, 528, 281, 483, 641, 2132, 12175, 51404], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 794, "seek": 248816, "start": 2508.96, "end": 2512.64, "text": " and get more and more impact and influence", "tokens": [51404, 293, 483, 544, 293, 544, 2712, 293, 6503, 51588], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 795, "seek": 248816, "start": 2512.64, "end": 2514.8399999999997, "text": " and through this open source channel,", "tokens": [51588, 293, 807, 341, 1269, 4009, 2269, 11, 51698], "temperature": 0.0, "avg_logprob": -0.2936989575967021, "compression_ratio": 1.644859813084112, "no_speech_prob": 0.000616452016402036}, {"id": 796, "seek": 251484, "start": 2514.84, "end": 2520.48, "text": " then spend a lot of effort about usability", "tokens": [50364, 550, 3496, 257, 688, 295, 4630, 466, 46878, 50646], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 797, "seek": 251484, "start": 2520.48, "end": 2524.2000000000003, "text": " and solving the common problem that many people want", "tokens": [50646, 293, 12606, 264, 2689, 1154, 300, 867, 561, 528, 50832], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 798, "seek": 251484, "start": 2524.2000000000003, "end": 2528.6800000000003, "text": " to solve is what's going to be considered.", "tokens": [50832, 281, 5039, 307, 437, 311, 516, 281, 312, 4888, 13, 51056], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 799, "seek": 251484, "start": 2528.6800000000003, "end": 2531.1200000000003, "text": " I have personally found success in giving away", "tokens": [51056, 286, 362, 5665, 1352, 2245, 294, 2902, 1314, 51178], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 800, "seek": 251484, "start": 2531.1200000000003, "end": 2534.4, "text": " as much valuable information and ideas as I can.", "tokens": [51178, 382, 709, 8263, 1589, 293, 3487, 382, 286, 393, 13, 51342], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 801, "seek": 251484, "start": 2534.4, "end": 2538.08, "text": " That's what my whole YouTube career and computer science", "tokens": [51342, 663, 311, 437, 452, 1379, 3088, 3988, 293, 3820, 3497, 51526], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 802, "seek": 251484, "start": 2538.08, "end": 2539.7200000000003, "text": " career is based on now.", "tokens": [51526, 3988, 307, 2361, 322, 586, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 803, "seek": 251484, "start": 2539.7200000000003, "end": 2542.76, "text": " So thank you for sharing those critical insights.", "tokens": [51608, 407, 1309, 291, 337, 5414, 729, 4924, 14310, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2062058232047341, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0027138646692037582}, {"id": 804, "seek": 254276, "start": 2542.76, "end": 2545.1200000000003, "text": " So on the topic of operating systems,", "tokens": [50364, 407, 322, 264, 4829, 295, 7447, 3652, 11, 50482], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 805, "seek": 254276, "start": 2545.1200000000003, "end": 2547.0400000000004, "text": " because I'm really glad you brought that up,", "tokens": [50482, 570, 286, 478, 534, 5404, 291, 3038, 300, 493, 11, 50578], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 806, "seek": 254276, "start": 2547.0400000000004, "end": 2550.32, "text": " because I started thinking about language models", "tokens": [50578, 570, 286, 1409, 1953, 466, 2856, 5245, 50742], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 807, "seek": 254276, "start": 2550.32, "end": 2553.36, "text": " as a component, like a new component of an operating", "tokens": [50742, 382, 257, 6542, 11, 411, 257, 777, 6542, 295, 364, 7447, 50894], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 808, "seek": 254276, "start": 2553.36, "end": 2554.0400000000004, "text": " system.", "tokens": [50894, 1185, 13, 50928], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 809, "seek": 254276, "start": 2554.0400000000004, "end": 2556.6000000000004, "text": " So I'm glad to know that there's some convergence there.", "tokens": [50928, 407, 286, 478, 5404, 281, 458, 300, 456, 311, 512, 32181, 456, 13, 51056], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 810, "seek": 254276, "start": 2556.6000000000004, "end": 2558.96, "text": " Is that kind of the future of Autogen?", "tokens": [51056, 1119, 300, 733, 295, 264, 2027, 295, 6049, 8799, 30, 51174], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 811, "seek": 254276, "start": 2558.96, "end": 2560.92, "text": " Is that what you're looking to move towards", "tokens": [51174, 1119, 300, 437, 291, 434, 1237, 281, 1286, 3030, 51272], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 812, "seek": 254276, "start": 2560.92, "end": 2566.0400000000004, "text": " is kind of being the operating system or a major component", "tokens": [51272, 307, 733, 295, 885, 264, 7447, 1185, 420, 257, 2563, 6542, 51528], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 813, "seek": 254276, "start": 2566.0400000000004, "end": 2570.6800000000003, "text": " of a future operating system that uses language models", "tokens": [51528, 295, 257, 2027, 7447, 1185, 300, 4960, 2856, 5245, 51760], "temperature": 0.0, "avg_logprob": -0.15313022477286203, "compression_ratio": 1.8898305084745763, "no_speech_prob": 0.016893262043595314}, {"id": 814, "seek": 257068, "start": 2570.68, "end": 2575.16, "text": " as like kind of the new CPU and maybe retrieval augmented,", "tokens": [50364, 382, 411, 733, 295, 264, 777, 13199, 293, 1310, 19817, 3337, 36155, 11, 50588], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 815, "seek": 257068, "start": 2575.16, "end": 2578.68, "text": " some kind of storage as like the new memory?", "tokens": [50588, 512, 733, 295, 6725, 382, 411, 264, 777, 4675, 30, 50764], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 816, "seek": 257068, "start": 2578.68, "end": 2581.68, "text": " Is that kind of the direction that it's going?", "tokens": [50764, 1119, 300, 733, 295, 264, 3513, 300, 309, 311, 516, 30, 50914], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 817, "seek": 257068, "start": 2581.68, "end": 2582.3999999999996, "text": " Yeah, exactly.", "tokens": [50914, 865, 11, 2293, 13, 50950], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 818, "seek": 257068, "start": 2582.3999999999996, "end": 2585.2, "text": " So it's my ambition.", "tokens": [50950, 407, 309, 311, 452, 22814, 13, 51090], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 819, "seek": 257068, "start": 2585.2, "end": 2587.44, "text": " When I started working with Autogen,", "tokens": [51090, 1133, 286, 1409, 1364, 365, 6049, 8799, 11, 51202], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 820, "seek": 257068, "start": 2587.44, "end": 2592.64, "text": " I discussed with some systems friends working on the systems.", "tokens": [51202, 286, 7152, 365, 512, 3652, 1855, 1364, 322, 264, 3652, 13, 51462], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 821, "seek": 257068, "start": 2592.64, "end": 2594.48, "text": " I told them this idea.", "tokens": [51462, 286, 1907, 552, 341, 1558, 13, 51554], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 822, "seek": 257068, "start": 2594.48, "end": 2598.08, "text": " And yeah, it sounds very ambitious idea to them.", "tokens": [51554, 400, 1338, 11, 309, 3263, 588, 20239, 1558, 281, 552, 13, 51734], "temperature": 0.0, "avg_logprob": -0.27544948577880857, "compression_ratio": 1.63013698630137, "no_speech_prob": 0.0029777775052934885}, {"id": 823, "seek": 259808, "start": 2598.08, "end": 2603.24, "text": " But I can see that some people really liked this idea.", "tokens": [50364, 583, 286, 393, 536, 300, 512, 561, 534, 4501, 341, 1558, 13, 50622], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 824, "seek": 259808, "start": 2603.24, "end": 2606.04, "text": " And even some 13-year people will give me", "tokens": [50622, 400, 754, 512, 3705, 12, 5294, 561, 486, 976, 385, 50762], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 825, "seek": 259808, "start": 2606.04, "end": 2608.64, "text": " stronger, strong support of this.", "tokens": [50762, 7249, 11, 2068, 1406, 295, 341, 13, 50892], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 826, "seek": 259808, "start": 2608.64, "end": 2612.52, "text": " He kind of had that idea independently.", "tokens": [50892, 634, 733, 295, 632, 300, 1558, 21761, 13, 51086], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 827, "seek": 259808, "start": 2612.52, "end": 2616.36, "text": " I kind of see that some of the most visionary people also", "tokens": [51086, 286, 733, 295, 536, 300, 512, 295, 264, 881, 49442, 561, 611, 51278], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 828, "seek": 259808, "start": 2616.36, "end": 2617.16, "text": " realized this.", "tokens": [51278, 5334, 341, 13, 51318], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 829, "seek": 259808, "start": 2617.16, "end": 2622.4, "text": " And definitely, we want to pursue for that.", "tokens": [51318, 400, 2138, 11, 321, 528, 281, 12392, 337, 300, 13, 51580], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 830, "seek": 259808, "start": 2622.4, "end": 2623.2799999999997, "text": " Excellent.", "tokens": [51580, 16723, 13, 51624], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 831, "seek": 259808, "start": 2623.2799999999997, "end": 2627.64, "text": " So taking a big step back, just in terms of the direction", "tokens": [51624, 407, 1940, 257, 955, 1823, 646, 11, 445, 294, 2115, 295, 264, 3513, 51842], "temperature": 0.0, "avg_logprob": -0.26484776526382287, "compression_ratio": 1.6036036036036037, "no_speech_prob": 0.00045106702600605786}, {"id": 832, "seek": 262764, "start": 2627.64, "end": 2630.2799999999997, "text": " of research, I think, I don't know if it's official,", "tokens": [50364, 295, 2132, 11, 286, 519, 11, 286, 500, 380, 458, 498, 309, 311, 4783, 11, 50496], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 833, "seek": 262764, "start": 2630.2799999999997, "end": 2634.68, "text": " but the rumor is right now OpenAI is working on GPT-5.", "tokens": [50496, 457, 264, 29639, 307, 558, 586, 7238, 48698, 307, 1364, 322, 26039, 51, 12, 20, 13, 50716], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 834, "seek": 262764, "start": 2634.68, "end": 2637.16, "text": " And then, of course, Google with Gemini and Meta,", "tokens": [50716, 400, 550, 11, 295, 1164, 11, 3329, 365, 22894, 3812, 293, 6377, 64, 11, 50840], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 835, "seek": 262764, "start": 2637.16, "end": 2640.24, "text": " like everyone is working on bigger and bigger models now.", "tokens": [50840, 411, 1518, 307, 1364, 322, 3801, 293, 3801, 5245, 586, 13, 50994], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 836, "seek": 262764, "start": 2640.24, "end": 2644.48, "text": " And so we're going to get more capabilities at the same time.", "tokens": [50994, 400, 370, 321, 434, 516, 281, 483, 544, 10862, 412, 264, 912, 565, 13, 51206], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 837, "seek": 262764, "start": 2644.48, "end": 2646.48, "text": " Smaller models are becoming more efficient.", "tokens": [51206, 15287, 260, 5245, 366, 5617, 544, 7148, 13, 51306], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 838, "seek": 262764, "start": 2646.48, "end": 2650.2, "text": " So Satya Nadella announced small language models coming.", "tokens": [51306, 407, 5344, 3016, 426, 762, 3505, 7548, 1359, 2856, 5245, 1348, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 839, "seek": 262764, "start": 2650.2, "end": 2653.8399999999997, "text": " So that way, you can probably perform very small cognitive", "tokens": [51492, 407, 300, 636, 11, 291, 393, 1391, 2042, 588, 1359, 15605, 51674], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 840, "seek": 262764, "start": 2653.8399999999997, "end": 2656.48, "text": " functions, but very quickly and efficiently.", "tokens": [51674, 6828, 11, 457, 588, 2661, 293, 19621, 13, 51806], "temperature": 0.0, "avg_logprob": -0.1702063656592554, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.02927352301776409}, {"id": 841, "seek": 265648, "start": 2656.48, "end": 2659.2400000000002, "text": " So what are some of the trends that you", "tokens": [50364, 407, 437, 366, 512, 295, 264, 13892, 300, 291, 50502], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 842, "seek": 265648, "start": 2659.2400000000002, "end": 2663.68, "text": " see intersecting with your work around auto-gen and agents", "tokens": [50502, 536, 27815, 278, 365, 428, 589, 926, 8399, 12, 1766, 293, 12554, 50724], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 843, "seek": 265648, "start": 2663.68, "end": 2665.2, "text": " and agent swarms?", "tokens": [50724, 293, 9461, 1693, 19537, 30, 50800], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 844, "seek": 265648, "start": 2665.2, "end": 2667.04, "text": " And what I mean, I guess, to be specific,", "tokens": [50800, 400, 437, 286, 914, 11, 286, 2041, 11, 281, 312, 2685, 11, 50892], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 845, "seek": 265648, "start": 2667.04, "end": 2672.72, "text": " is maybe cost changing or new capabilities coming.", "tokens": [50892, 307, 1310, 2063, 4473, 420, 777, 10862, 1348, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 846, "seek": 265648, "start": 2672.72, "end": 2674.88, "text": " Are there any capabilities that you're really looking for", "tokens": [51176, 2014, 456, 604, 10862, 300, 291, 434, 534, 1237, 337, 51284], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 847, "seek": 265648, "start": 2674.88, "end": 2678.16, "text": " that would make your job easier?", "tokens": [51284, 300, 576, 652, 428, 1691, 3571, 30, 51448], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 848, "seek": 265648, "start": 2678.16, "end": 2680.8, "text": " What are your thoughts on some of these new capabilities", "tokens": [51448, 708, 366, 428, 4598, 322, 512, 295, 613, 777, 10862, 51580], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 849, "seek": 265648, "start": 2680.8, "end": 2685.64, "text": " and making these multi-agent platforms more autonomous?", "tokens": [51580, 293, 1455, 613, 4825, 12, 559, 317, 9473, 544, 23797, 30, 51822], "temperature": 0.0, "avg_logprob": -0.15014316922142393, "compression_ratio": 1.699588477366255, "no_speech_prob": 0.0010320547735318542}, {"id": 850, "seek": 268564, "start": 2685.68, "end": 2687.4, "text": " Or is that a good idea, a bad idea?", "tokens": [50366, 1610, 307, 300, 257, 665, 1558, 11, 257, 1578, 1558, 30, 50452], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 851, "seek": 268564, "start": 2687.4, "end": 2689.3599999999997, "text": " So very kind of open-ended question,", "tokens": [50452, 407, 588, 733, 295, 1269, 12, 3502, 1168, 11, 50550], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 852, "seek": 268564, "start": 2689.3599999999997, "end": 2693.08, "text": " like what do you see coming this time next year?", "tokens": [50550, 411, 437, 360, 291, 536, 1348, 341, 565, 958, 1064, 30, 50736], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 853, "seek": 268564, "start": 2693.08, "end": 2697.72, "text": " Yeah, I think the idea of having specialized models", "tokens": [50736, 865, 11, 286, 519, 264, 1558, 295, 1419, 19813, 5245, 50968], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 854, "seek": 268564, "start": 2697.72, "end": 2701.56, "text": " to perform certain tasks in an excellent way", "tokens": [50968, 281, 2042, 1629, 9608, 294, 364, 7103, 636, 51160], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 855, "seek": 268564, "start": 2701.56, "end": 2705.56, "text": " and in a cheap way is fascinating.", "tokens": [51160, 293, 294, 257, 7084, 636, 307, 10343, 13, 51360], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 856, "seek": 268564, "start": 2705.56, "end": 2709.44, "text": " It's indeed worth a lot of investigation.", "tokens": [51360, 467, 311, 6451, 3163, 257, 688, 295, 9627, 13, 51554], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 857, "seek": 268564, "start": 2709.44, "end": 2713.4, "text": " For example, some of the hard problems I mentioned earlier", "tokens": [51554, 1171, 1365, 11, 512, 295, 264, 1152, 2740, 286, 2835, 3071, 51752], "temperature": 0.0, "avg_logprob": -0.18796585484554892, "compression_ratio": 1.5, "no_speech_prob": 0.001547418418340385}, {"id": 858, "seek": 271340, "start": 2713.4, "end": 2717.28, "text": " about the decomposition, recombination, validation,", "tokens": [50364, 466, 264, 48356, 11, 850, 3548, 2486, 11, 24071, 11, 50558], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 859, "seek": 271340, "start": 2717.28, "end": 2720.1600000000003, "text": " it's possible that some specialized model", "tokens": [50558, 309, 311, 1944, 300, 512, 19813, 2316, 50702], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 860, "seek": 271340, "start": 2720.1600000000003, "end": 2722.48, "text": " can do these kind of tasks really well.", "tokens": [50702, 393, 360, 613, 733, 295, 9608, 534, 731, 13, 50818], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 861, "seek": 271340, "start": 2722.48, "end": 2723.76, "text": " Or we haven't seen that yet.", "tokens": [50818, 1610, 321, 2378, 380, 1612, 300, 1939, 13, 50882], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 862, "seek": 271340, "start": 2723.76, "end": 2728.1600000000003, "text": " But conceptually, that sounds like a possibility.", "tokens": [50882, 583, 3410, 671, 11, 300, 3263, 411, 257, 7959, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 863, "seek": 271340, "start": 2728.1600000000003, "end": 2733.44, "text": " Actually, I'm pretty surprised that we haven't found", "tokens": [51102, 5135, 11, 286, 478, 1238, 6100, 300, 321, 2378, 380, 1352, 51366], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 864, "seek": 271340, "start": 2733.44, "end": 2735.12, "text": " a special model that can solve this.", "tokens": [51366, 257, 2121, 2316, 300, 393, 5039, 341, 13, 51450], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 865, "seek": 271340, "start": 2735.12, "end": 2739.84, "text": " So it makes me kind of wonder why.", "tokens": [51450, 407, 309, 1669, 385, 733, 295, 2441, 983, 13, 51686], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 866, "seek": 271340, "start": 2739.84, "end": 2742.88, "text": " Because it's such a natural idea that if you're", "tokens": [51686, 1436, 309, 311, 1270, 257, 3303, 1558, 300, 498, 291, 434, 51838], "temperature": 0.0, "avg_logprob": -0.2255289577743382, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.004979745019227266}, {"id": 867, "seek": 274288, "start": 2742.88, "end": 2745.92, "text": " finding a model that can do certain things,", "tokens": [50364, 5006, 257, 2316, 300, 393, 360, 1629, 721, 11, 50516], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 868, "seek": 274288, "start": 2745.92, "end": 2748.2400000000002, "text": " you should be able to do certain tasks very well", "tokens": [50516, 291, 820, 312, 1075, 281, 360, 1629, 9608, 588, 731, 50632], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 869, "seek": 274288, "start": 2748.2400000000002, "end": 2752.32, "text": " and you can just replace one specific agent with that.", "tokens": [50632, 293, 291, 393, 445, 7406, 472, 2685, 9461, 365, 300, 13, 50836], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 870, "seek": 274288, "start": 2752.32, "end": 2754.92, "text": " And I don't know many people are trying that.", "tokens": [50836, 400, 286, 500, 380, 458, 867, 561, 366, 1382, 300, 13, 50966], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 871, "seek": 274288, "start": 2754.92, "end": 2758.2400000000002, "text": " Either there's some fundamental reason", "tokens": [50966, 13746, 456, 311, 512, 8088, 1778, 51132], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 872, "seek": 274288, "start": 2758.2400000000002, "end": 2763.96, "text": " we haven't figured out why we can't do that,", "tokens": [51132, 321, 2378, 380, 8932, 484, 983, 321, 393, 380, 360, 300, 11, 51418], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 873, "seek": 274288, "start": 2763.96, "end": 2767.92, "text": " or we should be able to see that pretty soon.", "tokens": [51418, 420, 321, 820, 312, 1075, 281, 536, 300, 1238, 2321, 13, 51616], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 874, "seek": 274288, "start": 2767.92, "end": 2770.44, "text": " I think it's only one of these two possibilities.", "tokens": [51616, 286, 519, 309, 311, 787, 472, 295, 613, 732, 12178, 13, 51742], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 875, "seek": 274288, "start": 2770.44, "end": 2772.36, "text": " Because the former possibility is still there", "tokens": [51742, 1436, 264, 5819, 7959, 307, 920, 456, 51838], "temperature": 0.0, "avg_logprob": -0.18898070300066913, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.0014536563539877534}, {"id": 876, "seek": 277236, "start": 2772.4, "end": 2776.0, "text": " because the small model, it's possible that the small model", "tokens": [50366, 570, 264, 1359, 2316, 11, 309, 311, 1944, 300, 264, 1359, 2316, 50546], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 877, "seek": 277236, "start": 2776.0, "end": 2778.28, "text": " lacks some very important capability", "tokens": [50546, 31132, 512, 588, 1021, 13759, 50660], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 878, "seek": 277236, "start": 2778.28, "end": 2781.92, "text": " of being functioned to perform these hard tasks.", "tokens": [50660, 295, 885, 2445, 292, 281, 2042, 613, 1152, 9608, 13, 50842], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 879, "seek": 277236, "start": 2781.92, "end": 2783.48, "text": " Because these tasks are not easy.", "tokens": [50842, 1436, 613, 9608, 366, 406, 1858, 13, 50920], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 880, "seek": 277236, "start": 2783.48, "end": 2788.4, "text": " The composition problem, I think, even the GPD4 model,", "tokens": [50920, 440, 12686, 1154, 11, 286, 519, 11, 754, 264, 460, 17349, 19, 2316, 11, 51166], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 881, "seek": 277236, "start": 2788.4, "end": 2791.44, "text": " it's known to not to be too good at planning.", "tokens": [51166, 309, 311, 2570, 281, 406, 281, 312, 886, 665, 412, 5038, 13, 51318], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 882, "seek": 277236, "start": 2791.44, "end": 2795.2000000000003, "text": " It can do some kind of planning, but not perfectly.", "tokens": [51318, 467, 393, 360, 512, 733, 295, 5038, 11, 457, 406, 6239, 13, 51506], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 883, "seek": 277236, "start": 2795.2000000000003, "end": 2797.8, "text": " So if you want to get better capability than GPD4", "tokens": [51506, 407, 498, 291, 528, 281, 483, 1101, 13759, 813, 460, 17349, 19, 51636], "temperature": 0.0, "avg_logprob": -0.26490428400974647, "compression_ratio": 1.61864406779661, "no_speech_prob": 0.00030053872615098953}, {"id": 884, "seek": 279780, "start": 2797.8, "end": 2803.2400000000002, "text": " in some specialized tasks, it's to be seeing", "tokens": [50364, 294, 512, 19813, 9608, 11, 309, 311, 281, 312, 2577, 50636], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 885, "seek": 279780, "start": 2803.2400000000002, "end": 2804.76, "text": " whether we can accomplish that.", "tokens": [50636, 1968, 321, 393, 9021, 300, 13, 50712], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 886, "seek": 279780, "start": 2804.76, "end": 2808.36, "text": " But if we lower the target, if we say,", "tokens": [50712, 583, 498, 321, 3126, 264, 3779, 11, 498, 321, 584, 11, 50892], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 887, "seek": 279780, "start": 2808.36, "end": 2810.2400000000002, "text": " let's train some small models that", "tokens": [50892, 718, 311, 3847, 512, 1359, 5245, 300, 50986], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 888, "seek": 279780, "start": 2810.2400000000002, "end": 2812.84, "text": " can do something that GPD4 is already good at,", "tokens": [50986, 393, 360, 746, 300, 460, 17349, 19, 307, 1217, 665, 412, 11, 51116], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 889, "seek": 279780, "start": 2812.84, "end": 2816.1200000000003, "text": " that is much more amenable.", "tokens": [51116, 300, 307, 709, 544, 18497, 712, 13, 51280], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 890, "seek": 279780, "start": 2816.1200000000003, "end": 2819.4, "text": " I think I already see evidence of that.", "tokens": [51280, 286, 519, 286, 1217, 536, 4467, 295, 300, 13, 51444], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 891, "seek": 279780, "start": 2819.4, "end": 2823.6400000000003, "text": " So then it's more like a cost reduction story.", "tokens": [51444, 407, 550, 309, 311, 544, 411, 257, 2063, 11004, 1657, 13, 51656], "temperature": 0.0, "avg_logprob": -0.26183367299509575, "compression_ratio": 1.5369458128078817, "no_speech_prob": 0.00406597601249814}, {"id": 892, "seek": 282364, "start": 2823.64, "end": 2827.96, "text": " So that is, I'm pretty sure, that's feasible.", "tokens": [50364, 407, 300, 307, 11, 286, 478, 1238, 988, 11, 300, 311, 26648, 13, 50580], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 893, "seek": 282364, "start": 2827.96, "end": 2831.7599999999998, "text": " And the other part about getting better capability", "tokens": [50580, 400, 264, 661, 644, 466, 1242, 1101, 13759, 50770], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 894, "seek": 282364, "start": 2831.7599999999998, "end": 2835.52, "text": " than the big model, in some aspect,", "tokens": [50770, 813, 264, 955, 2316, 11, 294, 512, 4171, 11, 50958], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 895, "seek": 282364, "start": 2835.52, "end": 2842.24, "text": " is to be kind of, yeah, we have to hold our scientific curiosity", "tokens": [50958, 307, 281, 312, 733, 295, 11, 1338, 11, 321, 362, 281, 1797, 527, 8134, 18769, 51294], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 896, "seek": 282364, "start": 2842.24, "end": 2845.44, "text": " and see what happens.", "tokens": [51294, 293, 536, 437, 2314, 13, 51454], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 897, "seek": 282364, "start": 2845.44, "end": 2846.3599999999997, "text": " That makes sense.", "tokens": [51454, 663, 1669, 2020, 13, 51500], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 898, "seek": 282364, "start": 2846.3599999999997, "end": 2850.8399999999997, "text": " We are almost out of time, so I want to respect everyone's time.", "tokens": [51500, 492, 366, 1920, 484, 295, 565, 11, 370, 286, 528, 281, 3104, 1518, 311, 565, 13, 51724], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 899, "seek": 282364, "start": 2850.8399999999997, "end": 2853.12, "text": " And so I'll just say, thank you so much", "tokens": [51724, 400, 370, 286, 603, 445, 584, 11, 1309, 291, 370, 709, 51838], "temperature": 0.0, "avg_logprob": -0.19245716056438408, "compression_ratio": 1.5132743362831858, "no_speech_prob": 0.0006459669675678015}, {"id": 900, "seek": 285312, "start": 2853.12, "end": 2856.2, "text": " for jumping on and sharing some of your thoughts.", "tokens": [50364, 337, 11233, 322, 293, 5414, 512, 295, 428, 4598, 13, 50518], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 901, "seek": 285312, "start": 2856.2, "end": 2860.48, "text": " I'm super excited to be along for the ride.", "tokens": [50518, 286, 478, 1687, 2919, 281, 312, 2051, 337, 264, 5077, 13, 50732], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 902, "seek": 285312, "start": 2860.48, "end": 2863.48, "text": " But yeah, before we close, I'll give the floor to you.", "tokens": [50732, 583, 1338, 11, 949, 321, 1998, 11, 286, 603, 976, 264, 4123, 281, 291, 13, 50882], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 903, "seek": 285312, "start": 2863.48, "end": 2867.68, "text": " Is there anything that you'd like to put out in the world,", "tokens": [50882, 1119, 456, 1340, 300, 291, 1116, 411, 281, 829, 484, 294, 264, 1002, 11, 51092], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 904, "seek": 285312, "start": 2867.68, "end": 2871.72, "text": " any personal requests or personal hopes", "tokens": [51092, 604, 2973, 12475, 420, 2973, 13681, 51294], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 905, "seek": 285312, "start": 2871.72, "end": 2875.2, "text": " that you want to want to share with a broader audience?", "tokens": [51294, 300, 291, 528, 281, 528, 281, 2073, 365, 257, 13227, 4034, 30, 51468], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 906, "seek": 285312, "start": 2875.2, "end": 2878.0, "text": " Thanks for giving me the opportunity to do that.", "tokens": [51468, 2561, 337, 2902, 385, 264, 2650, 281, 360, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 907, "seek": 285312, "start": 2878.0, "end": 2881.2799999999997, "text": " Yeah, I want to say that we are still", "tokens": [51608, 865, 11, 286, 528, 281, 584, 300, 321, 366, 920, 51772], "temperature": 0.0, "avg_logprob": -0.163882777375995, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.005708904936909676}, {"id": 908, "seek": 288128, "start": 2881.28, "end": 2884.52, "text": " early at the new age.", "tokens": [50364, 2440, 412, 264, 777, 3205, 13, 50526], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 909, "seek": 288128, "start": 2884.52, "end": 2887.5600000000004, "text": " Agents become mature software that", "tokens": [50526, 2725, 791, 1813, 14442, 4722, 300, 50678], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 910, "seek": 288128, "start": 2887.5600000000004, "end": 2890.4, "text": " can do a lot of things for us.", "tokens": [50678, 393, 360, 257, 688, 295, 721, 337, 505, 13, 50820], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 911, "seek": 288128, "start": 2890.4, "end": 2892.84, "text": " We want to build the future together", "tokens": [50820, 492, 528, 281, 1322, 264, 2027, 1214, 50942], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 912, "seek": 288128, "start": 2892.84, "end": 2895.96, "text": " with everyone from the community.", "tokens": [50942, 365, 1518, 490, 264, 1768, 13, 51098], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 913, "seek": 288128, "start": 2895.96, "end": 2898.5600000000004, "text": " So give Autogeno a try.", "tokens": [51098, 407, 976, 6049, 664, 5808, 257, 853, 13, 51228], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 914, "seek": 288128, "start": 2898.5600000000004, "end": 2900.6800000000003, "text": " Try to use it for applications.", "tokens": [51228, 6526, 281, 764, 309, 337, 5821, 13, 51334], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 915, "seek": 288128, "start": 2900.6800000000003, "end": 2903.52, "text": " Let us know what's working and what's not.", "tokens": [51334, 961, 505, 458, 437, 311, 1364, 293, 437, 311, 406, 13, 51476], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 916, "seek": 288128, "start": 2903.52, "end": 2908.2000000000003, "text": " We are very happy to work together to improve it", "tokens": [51476, 492, 366, 588, 2055, 281, 589, 1214, 281, 3470, 309, 51710], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 917, "seek": 288128, "start": 2908.2000000000003, "end": 2910.84, "text": " and answer some of the big, important problems", "tokens": [51710, 293, 1867, 512, 295, 264, 955, 11, 1021, 2740, 51842], "temperature": 0.0, "avg_logprob": -0.2739986570754854, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.018810858950018883}, {"id": 918, "seek": 291084, "start": 2910.92, "end": 2912.08, "text": " as we mentioned.", "tokens": [50368, 382, 321, 2835, 13, 50426], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 919, "seek": 291084, "start": 2912.08, "end": 2916.8, "text": " And I really want to acknowledge that all the contributors,", "tokens": [50426, 400, 286, 534, 528, 281, 10692, 300, 439, 264, 45627, 11, 50662], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 920, "seek": 291084, "start": 2916.8, "end": 2921.6000000000004, "text": " starting from the original paper to the recent,", "tokens": [50662, 2891, 490, 264, 3380, 3035, 281, 264, 5162, 11, 50902], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 921, "seek": 291084, "start": 2921.6000000000004, "end": 2925.08, "text": " more open source resources, joined together,", "tokens": [50902, 544, 1269, 4009, 3593, 11, 6869, 1214, 11, 51076], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 922, "seek": 291084, "start": 2925.08, "end": 2928.96, "text": " and the huge developer community that's supporting us,", "tokens": [51076, 293, 264, 2603, 10754, 1768, 300, 311, 7231, 505, 11, 51270], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 923, "seek": 291084, "start": 2928.96, "end": 2933.88, "text": " I really learned a lot from everyone who has used", "tokens": [51270, 286, 534, 3264, 257, 688, 490, 1518, 567, 575, 1143, 51516], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 924, "seek": 291084, "start": 2933.88, "end": 2938.48, "text": " and provided feedback that people are super, super creative.", "tokens": [51516, 293, 5649, 5824, 300, 561, 366, 1687, 11, 1687, 5880, 13, 51746], "temperature": 0.0, "avg_logprob": -0.35787162297888647, "compression_ratio": 1.580188679245283, "no_speech_prob": 0.0033198054879903793}, {"id": 925, "seek": 293848, "start": 2938.48, "end": 2942.72, "text": " I think this is the right way to solve the hard problems", "tokens": [50364, 286, 519, 341, 307, 264, 558, 636, 281, 5039, 264, 1152, 2740, 50576], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 926, "seek": 293848, "start": 2942.72, "end": 2946.76, "text": " and hope to continue to do that and support the community,", "tokens": [50576, 293, 1454, 281, 2354, 281, 360, 300, 293, 1406, 264, 1768, 11, 50778], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 927, "seek": 293848, "start": 2946.76, "end": 2947.84, "text": " support everyone.", "tokens": [50778, 1406, 1518, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 928, "seek": 293848, "start": 2947.84, "end": 2951.12, "text": " And for example, the effort you're doing", "tokens": [50832, 400, 337, 1365, 11, 264, 4630, 291, 434, 884, 50996], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 929, "seek": 293848, "start": 2951.12, "end": 2954.68, "text": " with the hierarchical agent swarm,", "tokens": [50996, 365, 264, 35250, 804, 9461, 49839, 11, 51174], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 930, "seek": 293848, "start": 2954.68, "end": 2958.2, "text": " it's a very good example that you're", "tokens": [51174, 309, 311, 257, 588, 665, 1365, 300, 291, 434, 51350], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 931, "seek": 293848, "start": 2958.2, "end": 2963.56, "text": " making certain bats on certain ways of making money and work.", "tokens": [51350, 1455, 1629, 26943, 322, 1629, 2098, 295, 1455, 1460, 293, 589, 13, 51618], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 932, "seek": 293848, "start": 2963.56, "end": 2966.2, "text": " I'm very curious to see how that experiment goes.", "tokens": [51618, 286, 478, 588, 6369, 281, 536, 577, 300, 5120, 1709, 13, 51750], "temperature": 0.0, "avg_logprob": -0.2476019500404276, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0017807881813496351}, {"id": 933, "seek": 296620, "start": 2966.2, "end": 2970.4399999999996, "text": " And if altering can be of any help in this or other consumer", "tokens": [50364, 400, 498, 11337, 278, 393, 312, 295, 604, 854, 294, 341, 420, 661, 9711, 50576], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 934, "seek": 296620, "start": 2970.4399999999996, "end": 2975.6, "text": " efforts, we'll be very happy to support you", "tokens": [50576, 6484, 11, 321, 603, 312, 588, 2055, 281, 1406, 291, 50834], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 935, "seek": 296620, "start": 2975.6, "end": 2981.56, "text": " if you need any feature and useful infrastructure support", "tokens": [50834, 498, 291, 643, 604, 4111, 293, 4420, 6896, 1406, 51132], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 936, "seek": 296620, "start": 2981.56, "end": 2983.3199999999997, "text": " that kind of thing.", "tokens": [51132, 300, 733, 295, 551, 13, 51220], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 937, "seek": 296620, "start": 2983.3199999999997, "end": 2985.8799999999997, "text": " Yes, let us know.", "tokens": [51220, 1079, 11, 718, 505, 458, 13, 51348], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 938, "seek": 296620, "start": 2985.8799999999997, "end": 2987.7599999999998, "text": " Yeah, absolutely.", "tokens": [51348, 865, 11, 3122, 13, 51442], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 939, "seek": 296620, "start": 2987.7599999999998, "end": 2990.4399999999996, "text": " No, we'll be definitely looking forward", "tokens": [51442, 883, 11, 321, 603, 312, 2138, 1237, 2128, 51576], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 940, "seek": 296620, "start": 2990.4399999999996, "end": 2993.56, "text": " to continuing the collaboration.", "tokens": [51576, 281, 9289, 264, 9363, 13, 51732], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 941, "seek": 296620, "start": 2993.56, "end": 2996.12, "text": " I think that, as you said, there is a lot of work to do.", "tokens": [51732, 286, 519, 300, 11, 382, 291, 848, 11, 456, 307, 257, 688, 295, 589, 281, 360, 13, 51860], "temperature": 0.0, "avg_logprob": -0.23080461852404535, "compression_ratio": 1.5605381165919283, "no_speech_prob": 0.00021644575463142246}, {"id": 942, "seek": 299612, "start": 2996.16, "end": 2997.68, "text": " And there are some limitations.", "tokens": [50366, 400, 456, 366, 512, 15705, 13, 50442], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 943, "seek": 299612, "start": 2997.68, "end": 3000.6, "text": " The model's limitations today are the model's limitations.", "tokens": [50442, 440, 2316, 311, 15705, 965, 366, 264, 2316, 311, 15705, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 944, "seek": 299612, "start": 3000.6, "end": 3003.2, "text": " There's not a lot we can do to work around that.", "tokens": [50588, 821, 311, 406, 257, 688, 321, 393, 360, 281, 589, 926, 300, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 945, "seek": 299612, "start": 3003.2, "end": 3005.24, "text": " But it is just the beginning.", "tokens": [50718, 583, 309, 307, 445, 264, 2863, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 946, "seek": 299612, "start": 3005.24, "end": 3008.64, "text": " And that's one thing that I'll use the closing to say", "tokens": [50820, 400, 300, 311, 472, 551, 300, 286, 603, 764, 264, 10377, 281, 584, 50990], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 947, "seek": 299612, "start": 3008.64, "end": 3013.04, "text": " is remember where we were a year ago today.", "tokens": [50990, 307, 1604, 689, 321, 645, 257, 1064, 2057, 965, 13, 51210], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 948, "seek": 299612, "start": 3013.04, "end": 3017.6, "text": " Chat GPT was probably published just about a year ago.", "tokens": [51210, 27503, 26039, 51, 390, 1391, 6572, 445, 466, 257, 1064, 2057, 13, 51438], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 949, "seek": 299612, "start": 3017.6, "end": 3021.7599999999998, "text": " But before that, it was GPT-3, GPT-3.5.", "tokens": [51438, 583, 949, 300, 11, 309, 390, 26039, 51, 12, 18, 11, 26039, 51, 12, 18, 13, 20, 13, 51646], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 950, "seek": 299612, "start": 3021.7599999999998, "end": 3026.1, "text": " And the distance that we've covered in just the last year", "tokens": [51646, 400, 264, 4560, 300, 321, 600, 5343, 294, 445, 264, 1036, 1064, 51863], "temperature": 0.0, "avg_logprob": -0.1344754578637295, "compression_ratio": 1.721311475409836, "no_speech_prob": 0.00030527127091772854}, {"id": 951, "seek": 302610, "start": 3026.1, "end": 3031.46, "text": " is it is a privilege to be part of one of the greatest shifts", "tokens": [50364, 307, 309, 307, 257, 12122, 281, 312, 644, 295, 472, 295, 264, 6636, 19201, 50632], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 952, "seek": 302610, "start": 3031.46, "end": 3033.2599999999998, "text": " that humanity has ever seen.", "tokens": [50632, 300, 10243, 575, 1562, 1612, 13, 50722], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 953, "seek": 302610, "start": 3033.2599999999998, "end": 3034.7799999999997, "text": " And some days, it doesn't feel real.", "tokens": [50722, 400, 512, 1708, 11, 309, 1177, 380, 841, 957, 13, 50798], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 954, "seek": 302610, "start": 3034.7799999999997, "end": 3036.7799999999997, "text": " And some days, it feels a little too real", "tokens": [50798, 400, 512, 1708, 11, 309, 3417, 257, 707, 886, 957, 50898], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 955, "seek": 302610, "start": 3036.7799999999997, "end": 3038.1, "text": " and a little too overwhelming.", "tokens": [50898, 293, 257, 707, 886, 13373, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 956, "seek": 302610, "start": 3038.1, "end": 3042.2999999999997, "text": " So thank you, Xi, for helping make this a reality", "tokens": [50964, 407, 1309, 291, 11, 15712, 11, 337, 4315, 652, 341, 257, 4103, 51174], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 957, "seek": 302610, "start": 3042.2999999999997, "end": 3044.54, "text": " and spending some time talking with me.", "tokens": [51174, 293, 6434, 512, 565, 1417, 365, 385, 13, 51286], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 958, "seek": 302610, "start": 3044.54, "end": 3047.62, "text": " And thank you to Katie for helping put this together.", "tokens": [51286, 400, 1309, 291, 281, 19602, 337, 4315, 829, 341, 1214, 13, 51440], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 959, "seek": 302610, "start": 3047.62, "end": 3049.94, "text": " And yeah, so thanks everyone.", "tokens": [51440, 400, 1338, 11, 370, 3231, 1518, 13, 51556], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 960, "seek": 302610, "start": 3049.94, "end": 3053.74, "text": " And yeah, see you all next time.", "tokens": [51556, 400, 1338, 11, 536, 291, 439, 958, 565, 13, 51746], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}, {"id": 961, "seek": 302610, "start": 3053.74, "end": 3054.54, "text": " Thank you so much.", "tokens": [51746, 1044, 291, 370, 709, 13, 51786], "temperature": 0.0, "avg_logprob": -0.19205956380875383, "compression_ratio": 1.805084745762712, "no_speech_prob": 0.0010641044937074184}], "language": "en"}