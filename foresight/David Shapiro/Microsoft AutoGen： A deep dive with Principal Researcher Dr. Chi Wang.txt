Chi, thanks for jumping on.
It's a pleasure to meet you.
I was really excited.
Yeah, you're quite welcome.
Obviously, Autogen is all the rage right now.
It's very popular.
There's lots and lots of videos being made about Autogen.
But before we dive into that,
I was wondering if you could just tell me a little bit more
about your time at Microsoft as a principal researcher,
like how did you get into that position?
What's it been like?
Yes, just give me the story.
Thank you. Yeah, my name is Chi.
Principal researcher at Microsoft Research.
I joined long time ago.
I joined about nine years ago in Microsoft.
And I've been working on many different projects.
Apparently now I'm focusing on Autogen.
And before that, I worked on automated machine learning,
machine learning for systems,
data science, data analytics, data mining.
So quite a lot of different things.
And some of the work is more like on a theoretical set
side and some of them is more to the system side.
And yeah, I've been focusing on Autogen recently.
So very happy to be here.
Excellent.
Yeah, with all the progress that's been made
on large language models and working on assistance
and agents, and then of course, working on agents,
working with other agents,
let me ask kind of a big question.
Like what was the genesis behind Autogen?
How was that project proposed?
Like how did it come together?
Like what was the theoretical work behind it?
And how did you get from zero to where you are today?
Yeah, so that's a very interesting question.
And to fully answer that question,
I need to tell a long story.
But let me first tell you a short one.
Short answer is like with this big kind of opportunities
with larger models, so powerful techniques.
At MSR, we want to ask the question,
like what is the future, right?
We want to be forward-looking, we want to be futuristic,
kind of take the solid leadership.
And because one of the famous quotes
from the founder of Macro Research,
we can say that the best science
will be indistinguishable from magic.
So that's the level of ambition we have.
So we asked the question,
what is the future of AI applications?
And how do we empower every developer to build them?
Yeah, so that's a fundamental driving question.
And now a longer version of the answer is that,
I started from, I thought as a tutorial,
before I worked on Autogen,
I worked on automated machine learning,
which is another open source project called FLAML.
FLAML is a solution for automating model selection,
hyperparameter tuning, essentially,
black box optimization to navigate a larger space
without knowing the gradient.
So it's a very powerful technique,
but that was started before the larger model takes the storm.
And when chatGVT released, right?
And that's a big, big kind of upgrade of the model capabilities.
And so I started working on a similar problem
as automated machine learning.
I started working on inference hyperparameter tuning
for these chatGVT models.
For example, how do you select the best model?
How do you select the right prompt temperature
and all the other inference parameters
so that you can maximize the utility from the models
while minimizing your cost?
So that's the initial work on this direction.
And when GT4 is released,
that's another big upgrade of the level capabilities.
So then I started really to ask the question,
okay, if we kind of want to bring the best power
of the model and really solve really difficult problems,
what should be the right way to do it?
And agent is apparently a very powerful notion.
It's another kind of level of the automation
as opposed to the previous automation
in the 20 machine learning work I did.
So yeah, so that's where they started.
And of course, it's a whole new area.
No, the shop agent is not new.
It's has been there for a long time.
And I remember back in college,
I worked on some like game competition,
like building agents that can play games
with each other and compete.
At that time, we were using many of the rule-based methods
where a lot of deal with a lot of corner cases and so on.
So it's not viable.
It's a good notion, but not viable.
And these larger models,
especially the chat-operated models,
really make it viable and a reality
that we can build new software based on.
And to study this new area,
we kind of need to think many things from scratch
and try to take some of the first principles
and what is the right approach.
And basically leverage every lesson
we learned from the previous projects, previous experience,
but try to build this one multi-agent framework
that is really generic,
and can support diverse applications.
Yeah, so there are many different examples
of sources of inspirations I can give you,
but why not show it's like using everything I learned
and also take every feedback I received from everyone
and iterate on that.
And so that's how we come here today.
Excellent.
Yeah, no, I mean, there's a lot to unpack there.
It's fascinating to me that it started
as sort of like auto ML,
like automating the optimization.
And I can see how going to like optimizing prompts
and optimizing hyperparameters and parameter tuning
could then lead to the agents, especially, like you said,
if the idea is thinking to the future,
like what is the sci-fi version
of enabling application development in the future?
So I wanted to follow up with a kind of a two-part question.
So in the most basic level, what is AutoGen?
But more specifically, what is the vision?
Like what is it that you're trying to solve
with AutoGen right now?
Yeah, so yeah, in one word,
AutoGen is the multi-agent AI framework,
and especially focusing on multi-agent conversations
so that we can connect large-level models, tools,
and human inputs together to solve complex asks.
There can be multiple ways to understand this.
So in one way is to understand it as a programming framework
for developers to build applications easily
with some simple and unified abstraction
so that they don't need to worry too much
about the lower details,
but can focus on how to define agents,
how to get them to work with each other,
and eventually reach the goal.
It can also be understood as a tool
to kind of scale up,
scale up the power of our models
and makes them even more useful
by connecting with other tools,
non-narrative model tools, or human collaborators,
and kind of scale up both the complexity of the problem
they can solve, the degree of automation, to some extent.
Yeah, this is kind of a relatively abstract instruction,
but if we think about it,
about how people use it, it's quite simple.
So when developers build applications with AutoGen,
it basically boils down to two steps.
Step one is to define agents,
and step two is to get them to talk.
So as simple as that.
Yeah, so we try to make it very useful and generic,
but on the other hand,
we want to have a very simple interface for people to use.
Yeah, I mean, that's exactly the vision
that I kind of settled on
for my hierarchical autonomous agent swarm idea,
but I don't want to make it about that.
Right now, it's just fascinating
that we kind of converge on a very similar principle,
like let's make the deployment of software
as easy as possible.
And so there's two things that you mentioned,
like layers of abstraction,
because I think that's a really good intuitive way
of thinking about it,
like in the same way that a Python interpreter
was a layer of abstraction from compiled code,
and then maybe language models
are another layer of abstraction
where it's natural language interface.
This could be seen as, again,
another layer of abstraction,
where instead of looking at interacting
with the language model directly,
it is now a type of interpreter,
but this is the agents
and the multi-agent framework on top of it.
So that's my intuition.
Do you agree with that or disagree?
Or like, how do you think of those layers of abstraction?
Yeah, that's a fantastic question.
The abstraction is indeed at a higher level
of the agent abstraction.
It unifies a number of different things.
One is larger models.
So when we use a single instance of a larger model,
we usually do prompt engineering
and try to give some input text
and get some output text out of it.
This agent abstraction can encapsulate that underneath
and provide a more intuitive way
to think of it as an agent that can converse with you.
So not just as one single text completion inference anymore.
It can do tasks, can persist some states
and continue to take your feedback
and produce more refined result and so on.
So that's a larger model-based agent.
There are two other kinds of backhands
that can be encapsulated.
One is, you can think of it as programming language
or tool-based agent, which doesn't use a larger model,
but they can still perform very useful actions.
They can do code execution, for example,
or it can execute predefined functions
or it can basically execute any programming logic
you've defined there.
And third one is the human kind of backed agents.
So these agents can be considered as some kind of user proxy.
So when they need human input, the human can take over
and just participate the multi-agent workflow
as one of the agents.
So you can see about several agents.
Some of them are larger model-based, some are tool-based,
others are human-based.
So then they can just cooperate together
through a very natural interface,
which is a conversational interface.
So that's kind of layer of abstraction we provide.
Yeah, I appreciate that.
And I'm reminded of during,
I think it was the Ignite keynote speech Satya Nadella said,
think of it as a reasoning engine
and a natural language interface.
And that was like the two simplest ways
to think of generative AI, at least the language side.
So the other topic that I wanted to ask about
to kind of dig into was thinking about it
as a kind of automation.
So there's the agent-based, there's the tools-based,
but then overall kind of,
and this is a messaging kind of a framing
that I've adopted when talking to people is,
and of course it's an oversimplification,
but AI is, one, AI is not new,
like machine learning has been around for a while,
this is just a step function in terms of capabilities
that models have.
But it's also just the simplest way to think about it
from a production standpoint or from a software standpoint,
is it's a new suite of automation tools, right?
That's kind of how I think of it.
Is that a fair characterization
or is there something that I'm missing from that?
Cause I do feel like there might be something missing
or something that that characterization
doesn't fully convey,
but it is still fundamentally new automation, right?
So if I try to understand it from an automation point of view,
I think agent is fundamentally a automation concept.
The automation is more like about,
instead of giving every detailed instructions using code,
I say step one, do this, step two, do that,
using from precisely defined program language specification.
And now we can make some more vague specification
using like natural language specification,
say I want to accomplish such a task
and could the agent do it for me?
So, and then underneath the agent needs to kind of break it down
a big complex task into maybe smaller, solvable tasks.
And until each task can be conveniently solved
by a simple inference and produce a corresponding code
to solve that task.
And then eventually we need to recompose them,
all of these intermediate steps
and get to the final output back.
So that is one part of the automation story.
And another part of it is,
think about this automating some tasks
that human had to do.
AutoGen really started with some very simple kind of automation.
Just think about how you use chatGBT.
You as a human need to ask questions
and chatGBT gave you some answer.
Sometimes it gave you the code
and then human need to take that code
and run it by yourself and get some result.
If it's not correct, you send it back
and chatGBT gave you some results for the game.
And here in this kind of interaction,
human students do a lot of work,
but many of the work can be automated if we use agent.
And even some kind of human feedback,
like non-code, if I don't like the results,
but I know my preferences, I will tell it,
such as change the chart from using a dollar to percentage
that kind of requirement,
or teach me this lesson,
teach me about math,
but using a concrete money example, right?
So those kind of requirements
can be somewhat automated using all different ways.
Sometimes we can use larger models.
Sometimes we can use some retrieval augmented approach
to inform retrieval to get the knowledge from somewhere.
That's another kind of interesting automation
that we can make.
Yeah, and within that, those automation stories,
because some of the examples,
what was it, how did you say like a vague specification,
right, a natural language specification
that is not quite as rigorous as software development
might have required in the past?
And then of course, with these models,
they have the ability to kind of think through it,
or break it down into steps.
So with all that, and some of the work that I have found is,
or some of the problems that I've confronted,
because it can think in general principles,
it does know a lot about software development,
and I mean, the language models know a lot
about a lot of things.
So with respect to autogen,
and kind of getting to where you're at today,
what are some of the biggest challenges
that you've overcome so far, or that you haven't overcome?
Maybe that would be a more interesting story.
Yeah, sure, I could talk about both.
For what we have overcome,
I think we have kind of figured out the abstraction
to the earlier about how to unify these different types
of capabilities, different ways of making them work together.
We'll have found one very simple interface
that kind of accommodate a variety of different communication
patterns.
So one example is the, so there are several examples.
One is the simple like one-to-one conversation.
Another is hierarchical chat, like suppose one agent
is more sit on top and talk to several sub-agents,
it manages, and they can be nested structure,
hierarchical structure.
And another example is multiple one-on-one joint chat.
So there's no one that is strictly sit on top.
Everyone talks to the other else,
but it's multiple one-on-one which are connected.
So I talk to you, you talk to Katie,
you can talk to me in this kind of triangle
or multiple joint chat.
And there's also a group chat, meaning it's not a one-on-one
trend anymore.
So everybody send messages to everyone else.
So we see each other's message and there's a hidden
group chat manager which does this kind of work.
So architectural wise, it's still like one-on-one chat,
but on the surface we can create an experience
that simulates the group chat.
And they can nest it the chat in some way,
like for example, we can start with one-on-one conversation
now, but at some time I decide to consult Katie.
So I will hold on my current conversation
and have some conversation with her.
And then after I finish the conversation with her,
I get back and continue the conversation with you.
So that is a kind of nested chat.
I believe that essentially you have the building blocks
right now, right?
These are some very common building blocks
and we compose them together in different ways.
We can build really complex workflows in general.
So any arbitrarily complex communication patterns
can be essentially built up within these building blocks.
But I think that is what we have achieved.
And we have also many examples for different applications
of using these different types of patterns.
This is another second thing we figured out.
And the third thing I think is the ability
to take human input and human control in a very natural way.
And I thought earlier it's like every agent
can be configured to enable the human input or disable that,
depending on what you need.
And also you can decide the type of environment from human.
You can take over every time or you can only selectively
chime in at a certain time.
So that's a very useful feature because when
you develop this automation, initially you
don't know which step is easy to automate
and which step is necessary for human to get in.
So you can start from the more human loop way.
And when you figure out that one step is
you can confidently automate, then you can gradually reduce
your human integration.
So this convenience is very useful for doing all the experiments
and figure out the right way.
And make sure or still make sure human has a control
when they need to.
This is the third thing I think.
So one more thing is the modularity and the reusability
of the agents.
That is a very important design part of it.
So make sure that if you develop one useful agent
in a different application, you could either directly reuse
or start to modify it or extend from different ways.
And make sure the barrier to hard work is not lost.
I think that's also very important,
seeing when we work together to build more and more
complex applications.
These are a number of things I think we're kind of figure out.
And there are indeed a lot of challenges we haven't.
And yeah, shall we get to that part?
Yeah, no, I just want to reflect on some of the processes
that you outlined, like removing humans from the loop
step by step.
Back in my time as an automation engineer,
that's exactly what I would do is like, OK, I can write a script
that does one part easily.
Cool.
Now, what's the next part?
And then where do I have to jump in?
And some of the other problems that you solved, like knowing.
So this I think is really important,
because some of the members of my team and my projects
have found the same thing, is that knowing when to be quiet,
knowing when not to jump in is, in many respects,
more important because you don't want to end up
with too much noise or wasted tokens.
So that's really fascinating.
Before we talk about problems you haven't overcome,
can you talk about some insights from that,
like that inhibition signal or keeping the noise lower?
What were the key insights there?
Like how did you test that and figure it out?
And do you have any general principles for anyone else
building agents?
Yeah, this is a very good question.
This is also related to another question about when
do you add agents to provide the feedback and when
that is not helpful, right?
Because I assume the noise you're talking about
is when you add more agents to service for democratics
or agents that try to refine what the other agent is doing,
right?
They serve as a channel to provide feedback,
but sometimes not feedback can be misleading and actually
prevent the original agent doing the right thing.
Yeah, we do observe that.
And also, it's not like the more agents, the better.
It's not necessarily that.
For example, if you use GPD4 as the back end
for an assistant agent, for a large number of problems,
you'll need a simple two-agent workflow.
One assistant agent, another user proxy agent.
Yeah, probably I need to explain what
the user proxy agent is.
Basically, it refers to what I meant earlier
about automating some of the work that human does.
For example, using tools to execute
Python code or run some predefined functions.
So if you use one GPD assistant agent to suggest a solution
such as code or function and use another user proxy
to execute them and just provide feedback back and forth,
you can solve a large number of problems very well.
And some of them are also complicated
and can involve multiple steps.
But if you use GPD3.5 turbo, then it's much less
to work in this way.
So adding more agents will be much more helpful.
And even for GPD4, when the problem complexity
goes above a certain level, it stops
to follow the main instructions.
But because of the trick for one single agent to work,
it actually puts a lot of careful instructions
in the system message and make it
know how to deal with some complex situations.
But we noted that if you put too many of them, even for GPD4,
and for complex tasks, start to forget these instructions
and not do things as you want.
Otherwise, you can just give it a simple instruction
to say, try your best to solve the hardest problem
and then it will be done.
We're not there yet.
I mean, in the future, we may.
This makes me want to bring up one interesting kind of law.
We, a few of us, came up called Kabuchi's law.
The law has some similarity with,
it's an analogy of the Conway's law in software engineering.
I'm not sure if you familiar with that notion.
No.
So Conway's law basically saying the complexity
of the software or the architecture of the software
is a reflection of the organization
that makes the software, that builds the software.
OK.
Makes sense.
Yeah.
So our Kabuchi's law says the model complexity
will affect the model capacity or capability.
We change the topology of the ideal multi-agent solution.
It's a summarization of what I mentioned earlier.
If you use a more powerful model,
then likely you can use simpler topology of multi-agents
to solve a common task and vice versa.
And also, I think we need more and more research
to understand this better as it's not soft.
I'm seeing people trying all different kind of topologies
or communication patterns for different applications.
They're very creative.
And what we had to figure out is what is the best topology
and for a particular model and for a particular application
in a kind of a very clear way to answer that question.
We were not here.
And this is one of actually a big challenge
or a big important problem we want to solve.
Yeah, next.
Yeah.
No, I mean, well, first, thanks for sharing
some of those critical insights.
And so I guess the general principle
is the smarter the underpinning model,
the simpler the topology can be because the more complex
the instructions can be.
And the more complex the tasks that an individual agent
can carry out.
Saying it out loud, it seems kind of obvious,
but that's a good rule to generalize.
So yeah, I guess let's pivot into what are some of the remaining
problems?
What are your biggest challenges that you either are working on
or are going to be down the road?
You mentioned topologies, like figuring out
what is the correct topology.
And of course, I can imagine that it's a moving target
because as the underlying models change, almost
on a monthly basis, you get new and different capabilities
that kind of maybe send you back to the drawing board
sometimes.
Yeah, this is why having a framework that
is versatile and that is flexible to do the experiments
is so crucial to kind of do the fast adaptation
as model moves as prominent techniques advances
and as more and more small model specialized models are
available, they will probably also
change a lot about what was the best way to build
the applications.
Yeah, so this is, I think, the big value of autogen.
And for unsolved research questions,
there are some concrete ones I can give you a few examples.
One is about this decomposition problem.
As we mentioned earlier, we want to be
able to achieve a state where the human can only
need to specify rapidly big ambitious goal
and underneath, we want the agent
to be able to decompose that into solvable problems, probably
multiple layers, and eventually recompose it
and solve each of them and recompose it.
And during this process, there are situations
where the human need to provide the correct specifications
because the initial one can be ambiguous.
And we want the human to only provide
the necessary and make a minimal kind of necessary
qualifications and instructions and that agent
to figure out the rest of them.
That is a big challenge because if we want to solve
more complex problems, we have to have a principal way
to do this.
And the second question also ready to do this
is as we solve bigger and bigger problems,
how do we do proper validation of the intermediate results?
Because we don't do that.
If possible, the agent will stick to some wrong intermediate
results and just keep doing, keep wasting their work.
And at certain time, if we need to provide validation
or use agent to do self-validation, that's hard.
But I do a way to do it.
So we need probably into some formal language
or formal way to do this proper validation
so that the automation can indeed happen in a way
that human desires.
Yeah, so these are some just two kind of concrete pieces
like problems.
Yeah, in my project, we almost started in the reverse
where we started with oversight of steering and oversight
and supervision.
So I'm curious, what's your perception or research
or findings with respect to?
Because you already mentioned having an assistant agent
and then also having kind of a top-down hierarchical agent
where you've got subordinates.
What do you think about my intuition
that working towards having supervisors steering QA quality
assurance agents throughout the network of agents
that are capable of providing some of that feedback
that you mentioned earlier, is that kind of the direction
that you're going?
Or have you tried that and it didn't work?
Or what are your thoughts in terms
of having some of those specialized roles or personas
as a way to help along?
Yeah, there are some examples that work pretty well.
I can show some of them.
One is a three-agent setup to solve a multi-agent coding
scenario.
The application is for a supply chain optimization.
It's done by another MSR team.
But that solution, in my view, is quite generic.
It's not restricted to that particular application.
And the setup is like, it's a hierarchical setup.
There's the commander on top.
There's a writer who is responsible in writing Python
code.
The agent can also have access to some proper tools,
like organization tools.
And the other subagent is actually Safeguard.
Safeguard is in charge of reviewing code safety.
So the way it works is that the commander receives
some user's question.
It will first ask the writer to write the code.
And after receiving the code, it will ask the Safeguard
to review the code for safety.
And only if the safety criteria is met,
it will round the code and send the result back.
Otherwise, it will just ask the writer to rewrite the code.
And this can go back and forth because there can be errors.
So when you debug, the writer can do that.
Until the result is correct, the writer
comes back with a final natural language answer
to some result.
And the current return that to user.
So this is almost a quite simple multi-agent setup,
but very effective in our application, almost 100%
correct every time.
One kind of lesson is if we merge the capability of the writer
and the Safeguard into one agent,
it doesn't work that well, especially in the code safety
part.
So we have the experiments in our paper.
We found that if you merge them, then the accuracy
for detecting code safety issue will drop significantly,
both for GP4 and GP3.5 Turbo, but more
significantly for GP3.5.
So this kind of hints that one agent,
if you ask to both suggest a solution
and check the solution suggested by itself,
have a bias.
But we separate them and also prevent them
to talk to each other, kind of make
them work in an adversarial setting.
It does it better.
So that is one observation.
But we also have other kind of scenarios
where we do involve every agent in one group chat.
So everyone also sees other's message
and can reply back.
It also works sometimes for other tasks.
For example, a critique to suggest a visualization
criteria for a visualization task.
You can have one agent write code and another to criticize.
It sort of works.
But my intuition is still that if we put every agent work
together always in a group chat, it may not always work
because they may have the tendency
to agree with each other and try hard to challenge.
I would say it's case by case for different applications.
There's also some benefit of doing it in this group chat
because it's relatively simple.
You don't need to do very hard about handling
the message separation.
You can simply define your agents
and put them in a group chat and get them running quickly.
So that's one benefit of group chat
and seeing many people are using that approach.
But just to be careful that it may not always
work because of the limitations of the models.
So that's really fascinating to me.
And my intuition was the same.
But it's interesting to have that validation
from another perspective.
So it's almost like even though the underlying model
is GPT-4 running all of the agents or 3.5 turbo,
there's still a positive effect from using division of labor,
which the division of labor comes
from the history of human work.
And so just taking a moment, obviously these models
do not work like human brains.
But when you have an agent with a very specific task and mission
and set of success criteria, that effectiveness
of the division of labor still helps,
even though it's just activating the latent capabilities
within the same underpinning model.
And then another intuition or a principle
that I want to reiterate is the idea that, in some cases,
group work makes sense, but in some cases, it doesn't.
It's almost like the same difference in humans
where the power of introverts, doing solo work on your own
versus doing collaborative group work.
So again, not saying that they're operating like humans,
but it's really interesting to see some of these parallels
emerge between multi-agent work and the nature of human labor.
So yeah, very fascinating.
And it's interesting because in some of the conversations
that I've had and some of the observations that I've made,
it's almost like what we're doing with these agents,
these groups of agents, is recreating a corporation.
You might have a CEO or a boss or a supervisor,
and then you have the coder and then the QA.
So it's a very similar structure.
So do you have any other major insights or lessons
that you think are either recently or super valuable
that you want to share with other researchers
or that you would recommend?
Sure, sure.
There are so many of them.
I can give you a few examples.
One thing is the chat, the conversation perspective.
I mentioned earlier that chat GPD is a big inspiration.
Certainly for many people.
But for me, there's a personal story
about what specific user I felt from it.
It's a reminder of something I learned back in my college
from a professor who told me that conversation is a provable
way of making a good progress of learning.
I don't remember the exact quote of that, but it's roughly that.
So basically, he's trying to say, conversation
is a very powerful form of either learning or making
progress, or et cetera, that many people didn't realize
it's how important it is.
And there are some theoretical roots there.
So that's one reason I'm so kind of so sure,
or so I have so much belief in using conversation
as the central medium of the command multi-agent interface.
Again, I know there's a science, although I didn't have time
to find out which reference it was.
But I know that, so it gave me the confidence or the belief
that this is the right thing to do.
I think one of my favorite courses,
Jimmy actually found me some reference
from a social scientist.
He mentioned something similar to that.
Yeah, so this is one lesson, one kind of unique thing
that I don't think many people have really.
They kind of understand chatGVT is very powerful,
and also get a lot of useful experience from that.
But maybe this science part of it is less known.
So that's one thing I would share.
Another inspiration source, as I told you,
so auto-gen is really inspired by many different things,
many projects I've worked on before,
and all the lessons I've learned.
So another one, for example, is the operating system.
So this is also not so obvious.
When we talk about AI, why do we talk about operating systems?
I think the several things, several inspiration
I take from the success of operating systems.
One is the idea of maximizing the utility of the most valuable
resource you have.
So in old days, it's like the CPU, the GPU.
But I think in the new era of AI,
these powerful, not even models is so valuable resource
and building an operating system around them, right?
And maximizing their utility, but to give them
the necessary peripherals and do the right coordination.
And it's super critical from the system point of view.
And also, so that operating system is really
you can build a platform that can support
many diverse applications on top of that.
So we need to design a very generic robust kind of system
to do that, right?
So these are all the design principles
we try to use when we design AutoGen.
And similarly, the idea of object-oriented programming
is very useful.
So many developers have very interesting ideas
they want to try and develop.
And now with this framework that hides a lot of complexity
inside the framework, they're able to kind of do the things
they want more easily.
That's a part of abstraction.
I already mentioned the agent, notion, automation,
inspiration.
The one thing I want to mention is open source, right?
The concept of open source is that it
can solve the common problems that community needs
and make it really easy to use.
So those are probably most modern kind of things
that can get good open source adoption
and build something that the community loves, right?
Yeah, so I think that is my valuable lesson
I want to share with all researchers, right?
If you want to get their research adopted
and get more and more impact and influence
and through this open source channel,
then spend a lot of effort about usability
and solving the common problem that many people want
to solve is what's going to be considered.
I have personally found success in giving away
as much valuable information and ideas as I can.
That's what my whole YouTube career and computer science
career is based on now.
So thank you for sharing those critical insights.
So on the topic of operating systems,
because I'm really glad you brought that up,
because I started thinking about language models
as a component, like a new component of an operating
system.
So I'm glad to know that there's some convergence there.
Is that kind of the future of Autogen?
Is that what you're looking to move towards
is kind of being the operating system or a major component
of a future operating system that uses language models
as like kind of the new CPU and maybe retrieval augmented,
some kind of storage as like the new memory?
Is that kind of the direction that it's going?
Yeah, exactly.
So it's my ambition.
When I started working with Autogen,
I discussed with some systems friends working on the systems.
I told them this idea.
And yeah, it sounds very ambitious idea to them.
But I can see that some people really liked this idea.
And even some 13-year people will give me
stronger, strong support of this.
He kind of had that idea independently.
I kind of see that some of the most visionary people also
realized this.
And definitely, we want to pursue for that.
Excellent.
So taking a big step back, just in terms of the direction
of research, I think, I don't know if it's official,
but the rumor is right now OpenAI is working on GPT-5.
And then, of course, Google with Gemini and Meta,
like everyone is working on bigger and bigger models now.
And so we're going to get more capabilities at the same time.
Smaller models are becoming more efficient.
So Satya Nadella announced small language models coming.
So that way, you can probably perform very small cognitive
functions, but very quickly and efficiently.
So what are some of the trends that you
see intersecting with your work around auto-gen and agents
and agent swarms?
And what I mean, I guess, to be specific,
is maybe cost changing or new capabilities coming.
Are there any capabilities that you're really looking for
that would make your job easier?
What are your thoughts on some of these new capabilities
and making these multi-agent platforms more autonomous?
Or is that a good idea, a bad idea?
So very kind of open-ended question,
like what do you see coming this time next year?
Yeah, I think the idea of having specialized models
to perform certain tasks in an excellent way
and in a cheap way is fascinating.
It's indeed worth a lot of investigation.
For example, some of the hard problems I mentioned earlier
about the decomposition, recombination, validation,
it's possible that some specialized model
can do these kind of tasks really well.
Or we haven't seen that yet.
But conceptually, that sounds like a possibility.
Actually, I'm pretty surprised that we haven't found
a special model that can solve this.
So it makes me kind of wonder why.
Because it's such a natural idea that if you're
finding a model that can do certain things,
you should be able to do certain tasks very well
and you can just replace one specific agent with that.
And I don't know many people are trying that.
Either there's some fundamental reason
we haven't figured out why we can't do that,
or we should be able to see that pretty soon.
I think it's only one of these two possibilities.
Because the former possibility is still there
because the small model, it's possible that the small model
lacks some very important capability
of being functioned to perform these hard tasks.
Because these tasks are not easy.
The composition problem, I think, even the GPD4 model,
it's known to not to be too good at planning.
It can do some kind of planning, but not perfectly.
So if you want to get better capability than GPD4
in some specialized tasks, it's to be seeing
whether we can accomplish that.
But if we lower the target, if we say,
let's train some small models that
can do something that GPD4 is already good at,
that is much more amenable.
I think I already see evidence of that.
So then it's more like a cost reduction story.
So that is, I'm pretty sure, that's feasible.
And the other part about getting better capability
than the big model, in some aspect,
is to be kind of, yeah, we have to hold our scientific curiosity
and see what happens.
That makes sense.
We are almost out of time, so I want to respect everyone's time.
And so I'll just say, thank you so much
for jumping on and sharing some of your thoughts.
I'm super excited to be along for the ride.
But yeah, before we close, I'll give the floor to you.
Is there anything that you'd like to put out in the world,
any personal requests or personal hopes
that you want to want to share with a broader audience?
Thanks for giving me the opportunity to do that.
Yeah, I want to say that we are still
early at the new age.
Agents become mature software that
can do a lot of things for us.
We want to build the future together
with everyone from the community.
So give Autogeno a try.
Try to use it for applications.
Let us know what's working and what's not.
We are very happy to work together to improve it
and answer some of the big, important problems
as we mentioned.
And I really want to acknowledge that all the contributors,
starting from the original paper to the recent,
more open source resources, joined together,
and the huge developer community that's supporting us,
I really learned a lot from everyone who has used
and provided feedback that people are super, super creative.
I think this is the right way to solve the hard problems
and hope to continue to do that and support the community,
support everyone.
And for example, the effort you're doing
with the hierarchical agent swarm,
it's a very good example that you're
making certain bats on certain ways of making money and work.
I'm very curious to see how that experiment goes.
And if altering can be of any help in this or other consumer
efforts, we'll be very happy to support you
if you need any feature and useful infrastructure support
that kind of thing.
Yes, let us know.
Yeah, absolutely.
No, we'll be definitely looking forward
to continuing the collaboration.
I think that, as you said, there is a lot of work to do.
And there are some limitations.
The model's limitations today are the model's limitations.
There's not a lot we can do to work around that.
But it is just the beginning.
And that's one thing that I'll use the closing to say
is remember where we were a year ago today.
Chat GPT was probably published just about a year ago.
But before that, it was GPT-3, GPT-3.5.
And the distance that we've covered in just the last year
is it is a privilege to be part of one of the greatest shifts
that humanity has ever seen.
And some days, it doesn't feel real.
And some days, it feels a little too real
and a little too overwhelming.
So thank you, Xi, for helping make this a reality
and spending some time talking with me.
And thank you to Katie for helping put this together.
And yeah, so thanks everyone.
And yeah, see you all next time.
Thank you so much.
