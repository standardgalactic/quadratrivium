{"text": " I think we're ready to begin um you saw me look at the camera used to be over there now you're over here okay so I ran a poll 17 hours ago I asked if y'all wanted me to do a video on the EU AI Act man god names gotta get better with names I'm good with acronyms come to me for acronyms Gato Gaia ages okay anyways so y'all were confused about what I meant by Haas which is something from here in the south basically Haas means just something big right it's short it's a southern dialect for horse or in particular a big horse so this document is a Haas because look how freaking long it is it just keeps going so I plugged it in over here and it is 70,000 tokens long so even if I had access to GPT-432 K which I did briefly and then they revoked it I think they got overwhelmed because I used it I tried to use GPT-432 K on a couple things and it was just like server is overloaded servers over anyways so hopefully I get that back but the idea what there would be I could read it and you know chop it up into maybe four sections three or four sections and read the whole thing but we don't have that and I do have GPT-3.5 turbo 16k 0613 so this model is actually super steerable it is actually better than original GPT-4 add a lot of things it's still kind of dumb especially if the instructions are too complex but as long as you give it just one task GPT-3.3.5 turbo 0613 the June 13th model is more than adequate for most things now that being said because I'm interpreting and I'm doing summarization I'm gonna use 04613 so the instructions that I gave it just now and I know that I'm ready to start the video primary purpose zoom in a little bit the primary purpose or the following is a chunk of legislation your job is to summarize this legislation with two primary goals one drastically reduce the word count and two retain enough context that it will still make sense the reason for this is the current AI technology can only read chunks that are so big method the user will give you a chunk of legislation via chat your response must only be the summarized version do not chat with the user engage with them or offer any other commentary summarize only so I copied the first hundred lines of the legislation in you here and this is what it spit out so it took all that and just said okay this is what it means so the first hundred lines which you can see here so we've got 344,000 characters 3665 lines of stuff to go through I'm not going to read the whole thing I'm not AI explained that dude just like you all are impressed with how much work I can do I'm impressed with how much that dude can read just honestly I'm maybe he uses chat GPT I don't know anyways if you're watching it's just a joke okay so anyways so ultimately basically that that first hundred lines was summarized and condensed down to the European Commission proposes a regulation for harmonized rules on artificial intelligence aiming to balance the socioeconomic benefits of AI with potential risks the proposal seeks to ensure AI systems are safe respect fundamental rights provide legal certainty for investment and innovation enhance governance and prevent market fragmentation so it has says nothing about existential risks or prevent AI from killing everyone it's sort of implicit because like if AI kills everyone you can't have investment in innovation but I would rather see legislation that is explicit it proposes a risk-based approach prohibiting certain harmful AI practices and setting requirements for high-risk AI systems okay cool we'll see how they define that later the proposal also establishes a governance system at member state level and a European artificial intelligence board at union level okay cool so this is this is going to be an international regulator great it is consistent with existing union legislation and policies including data protection consumer protection non-discrimination and gender equality so these are all very low-level risks none of this is actually talking like when they say high risk they're probably meaning like the social credit system that is used in China which that is a high risk but it is not the highest risk the proposal is based on article 114 of the Treaty of the Functioning of the European Union Union the TFEU which ensures the establishment and functioning of the internal market okay so this is mostly a market-oriented piece of legislation great so we know that this works so let's go ahead and save this out I'm gonna walk you through the whole process and then I'll yeah so this is this is how this is how we're gonna proceed for those of you that are newer to my channel this is actually the original format that I used where I would where I would walk you through the entire process so just watch that on 2x and you'll see the whole process of how I approach these things so we'll call this system summarize yes dot text and so basically I give it a system a system message up with very very clear instructions and and what I expect and and what it should expect and then it gives you very clear consistent results so then we come over to do to do to do to do step one simplify so this is I'm just re I'm borrowing code which I always do so we've got this so this is what we're gonna use to do that this is this is this is a KB service that I'm working on so I'll show you that in an upcoming video but you don't get to see that yet we don't need KB functions so delete that flask routes we're not gonna do a flask app so that's also fine actually I guess I just delete most of it okay cool so we get rid of flask cuz we're not gonna do that we're not gonna do threading we're not gonna we don't need flask or logging or JSON or YAML okay clean it up I probably should have done that before I started the video but it's good to it's good for you to see how messy my process is I am the first person to tell you I am not a professional developer I never was my professional background was I was in virtualization and automation so I'm an automation engineer which is why I approach things the way that I do which is creating discrete steps so you notice the thought the title of this is step one simplify okay cool so basically what we're gonna do is we're going to ask this handy dandy coding chatbot that I built which you can use this it is available over here I had a bunch of people submitting pull requests and they kept trying to like change the way that this thing works so I just said it to public archive so here's the thing is I welcome pull requests that are like simple fixes or small improvements don't refactor my code just fork it right like there's 19 forks if you want to if you want to fundamentally change how it works fine but I got tired of people messing up my code so I just said it to public archive and guess what like people you can still do whatever you want with it anyways so we're gonna use this as a co-pilot tool because I got frustrated with GitHub because they're co-pilot customer service whatever documentation is garbage okay let's see I have here let's just write a function that accepts a gigantic string and returns now let's see and let's see splits it by lines and then chunks 100 lines together and returns those chunks as strings so the final result will be a list of strings each string represents 100 lines of the original okay so this is this is my coding chatbot assistant that it uses it also uses GPT for you I tried it you can actually switch it to oh it's using the old one hold on now I need to update that hold on hold on that's the wrong model coding chatbot assistant see this is what I mean is someone someone updated it oh no my local one uses the right one so someone updated it with the wrong model and made a bunch of other assumptions and I was just like no this is not this is not what I wanted okay so here we go here's the function Python chunk strings line lines equals split oh wow I did it in two lines cool this functions first splits the string into lines using the delimiter then it uses a list comprehension to create chunks of 100 lines each the range yep the result is a list of strings cool excellent excellent excellent so I'm gonna go ahead and copy this out oh and another good thing is that uh like well yeah you see how it works all right so we'll go ahead and do that so chunk strings so first we want to just act equals open file and I've got it named as act text just right here yep oh and also this excuse me this this uses a scratch pad so what I can do is I can actually populate the code that I'm working on over here so now it's aware of what I'm working on so this scratch pad is a holdout so rather than giving it the code in sequence the the coding chatbot actually has a it has here let me show you the system message so here's the instructions your Python coding assistant you know me turn on word wrap your Python coding assistant etc etc scratch pad so basically tells you the scratch pad is usable you know so that you know what the user is working on so that you can talk about this and this is passed to the chatbot every time so whatever the conversation is like that that is in the past I don't know if I'm explaining that well anyways point being is that it see it can see the code that you're working on as long as you keep this updated okay so that's the first hundred lines let's go ahead and see where did it go step one simplify cool so act equals that and then we'll say chunks equals see chunk string chunk string act all right so that this basically creates the chunks that we're gonna be working on and that is that let's see I'm gonna need to watch this because mmm well here let me just show you something real quick so the first hundred lines if we take that over to playground and paste it in here you see that a hundred lines is about 4,000 tokens so that would obviously be too big for text of inchio 3 but with with GPT-4 we have 8,000 tokens and then with 3.5 16k we have 16,000 tokens so we could actually probably do more but the thing is is that again 3.5 turbo is actually you know it let's try it real quick heck with it heck with it while we're here let's go back over to chat let's give it this chunk because if it can if I can do it faster and cheaper why not why not and then we need this is our system message we need to make sure that we're gonna use 3.5 turbo 16k so that it's got enough space okay turn the temperature down we do not want you to be creative thank you very much all right and then let's see how well it summarizes it yeah so it's not this summarization is not nearly as elegant or as concise but just glancing at it it's okay I think I'm gonna stick with four but I'm glad I did this side-by-side so the the the GPT-4 it's a much more elegant summary and it's also less worthy even though so it's not quite as fast and it's not quite as cheap because we need to capture the essence of something that's really important let's use the smarter model now that being said again 3.5 turbo the the June 13th one plenty plenty good for a lot of things okay cool so I'm glad I ran that test where are we what am I doing I don't know I'm lost I'm always lost there's this meme it's like a pug in a field with a wizard hat and it says that the meme is like not all who wander are lost except Dave Dave is lost as fuck and that's me I'm the pug with a wizard hat does that make any sense I need more coffee okay all right I hope you find this entertaining because I just woke up and I was like my brain's like hey we should work on this all right now we're back here all right we've got our chunks so now what we need to do so this is this is where it gets fun so four chunk in chunks this is super straightforward let's see messages equals this is gonna be a list and then the role will be user and the content wow I know how to type I promise it's just getting it from my head to the to the hands on the keyboard to the screen that's too many steps right now content equals chunk okay cool so basically what we're doing right here is we're setting it up to look exactly like this so I'm putting the chunk in here and then we also need to load the system message so what we're gonna do is we're gonna do system equals open file system underscore summarize so this is this is the instructions dot text so system message is the new prompt engineering all right cool so then we'll do role system and content is system all right so now we're ready to pass it up to our model right here so then we'll just do response equals chatbot messages ta-da is that simple so it's ready and so this response is going to be the summary and so now I need to figure out how I want to save this save these summaries out in a way that makes sense let's see let me ask my chatbot assistant okay cool great now I want to pass a list of strings to a function and have that function save them to a specified folder as a parameter and I want it to serialize those files like so so it'll be like file 001 dot text file 0002 dot text and so on let's see you should see the script I'm working on all right cool so then we're gonna we're gonna have summaries equals list and then summaries dot append response so now basically we're gonna have the summaries all in order and what I'm gonna do is I'm gonna copy and paste this into my scratch pad so that the chatbot knows what I'm working on and I'll show you how that it says oh okay cool I probably should have told it that I'm gonna do that with the summaries it's usually pretty good at making the inference okay cool let's see def save strings if os path exists or if it doesn't exist make it otherwise enumerate strings so as long as I've been using Python I'm not entirely sure how enumerate works maybe I should ask all right but let's go over here let's go back to my script save strings cool file I05 interesting yes see I'm not even sure how that works fantastic let's see if it let's see if it works though the function first checks that the specified folder exists excellent then it uses it to make it then then iterates over the list of strings using enumerate which provides both the index and the value okay so that's what enumerate does the index is used to generate the file name with I colon O5 in the f string ensuring that the index is zero padded to five digits well that was easy cool right and then it uses the right method yeah so instead I want I was hoping it would use my save file but that's fine so let's go to here and we're gonna do summaries and so instead we're going to assume that that is already there and then we are going to do file path equals I was path join folder yeah that works and then we are going to do save file because this so my save file function encodes it in UTF-8 so just reuse something rather than do it here because you see this doesn't do it in UTF-8 you want to make sure that you have a nice standardized thing that's what I'm doing here file path and s so that should be good fine all right so save strings so then we do save strings we'll do summaries no I need to do yes summaries and then summaries that should work now that being said I want to print the response as I get them there we go and then response all right let's give this a whirl and see if it works CD what is this EU AI act zoom in a little bit Python step 01 this will probably take a while because we've got 3,000 line 3300 lines there we go cool hmm hold on oh right right right I forgot about this okay so this is an important thing what I started doing and I forgot to integrate this so what I started doing is I when I whenever I call the chatbot I return the text and the token use so because this is actually really helpful if you just go ahead and return the tokens so that you know when to summarize it so instead what I need to do is response tokens because I don't need to save I don't need to save a tuple out here but this otherwise looks good and then what I'm gonna do is I'm gonna go grab something to make the the printout look a little bit better to do to do to do to do reflective journaling tool this one has this one has good pretty output so we're gonna do formatted lines yep so take the formatted lines here and then we'll do print new line new line new line and formatted lines all right so that will that'll make it that'll make it prettier and then we'll also use so this this is this is why I appreciate when people add feedback is we added someone added the halo which is a little like thing that shows that it's thinking so I'm gonna add that real quick as well and the way that you use that is down here and so once once this is running I'll pause the video so you don't have to watch the whole thing so you're actually probably closer to the end if I had to guess oops spinner start so we'll say summarizing next chunk and then spinner stop okay so let's try this make sure it works summarizing next chunk hey look now it gives us some output better user experience it tells me what it's doing text wrap is not defined whoops what do you mean I need to import everything that you use import text wrap try that again third time's a charm all right and if this works then I'll go ahead and pause it and then we'll come back once the whole video is or once the whole thing is done why is it in hold on hold on that's not right why are you giving me this garbage that's not correct oh right I need to print out formatted text my bad don't give me this nonsense because basically what you want to do is you want to have it formatted in such a way there we go see so now we have this nice little block it's easier to read all right cool so this is working we'll watch it just a couple times you see how fast it is the the June 13th update is much much faster let's see excellent excellent excellent so we will I'm gonna pause the video and we'll come back and and take a look at the final result cheers okay and we're back so after running it all here let me just show you what the results were we've got 37 files each with a summary and I wrote a really quick I hacked together a really quick script that merged them all and so the merged result is 34,000 characters long so we reduced it by a factor of 10 to 1 just just shy of 10 to 1 which when you plug that in over here that's 6,000 tokens so we went from from 60,000 or 70,000 tokens I don't remember exactly to 6,000 tokens one of the reasons that we had a more drastic reduction reduction of token count is because we removed all the serial numbers and stuff because that's kind of superfluous if you if you remove symbols and stuff like this these are high token things if it's ordinary words like AI and each system these are easy for the model to tokenize so that means what we can do now is we can just take this whole thing and come over to chat and say let's see main purpose here turn it believe that you are a chatbot tasked with discussing the following legislation with the user make sure to well hell but I mean that that's all it is use the legislation summary below to conduct the conversation and that's it and so then we say a legislation summary and we'll copy paste it here and so then we move over to we could do 16k because that'll be that'll be faster actually here let's try that real quick let's try that temperature there okay give me a high level overview of this legislation blah blah blah okay cool we're new after five years how is it funded let's see the funding and implementation of this legislation comes from the multi-annual financial framework blah blah blah the financial impact includes allocation of resources and having watched it go through how many employees will it cost there we go ten full-time equivalent employees all right so we can clear that out just because it ultimately is superfluous so some of the questions let's see will this legislation hamper research and innovation let's see the legislation aims to balance the benefits of AI research and innovation with the need for safety and respect for fundamental rights it includes measures to support innovation such as AI regulatory sandboxes and reducing the regulatory burden on SMEs and startups so the SMEs are going to be subject matter experts like UNI as well as startups the goal is to provide legal certainty and prevent market fragmentation which can actually foster innovation by creating a level playing field for AI developers however it imposes certain requirements and obligations on high-risk AI how does this legislation define high-risk AI let's see this legislation categorizes AI systems into three risk levels unacceptable high and low or minimal high-risk AI is based on their intended purpose and usage while the specific criteria may vary high-risk AI generally include those use the safety components of products covered by union harmonization legislation as well as AI used in critical infrastructure management education vocational training employment and workers management these high-risk AI systems are subject to requirements what are unacceptable types the legislation prohibits certain AI practices that are considered unacceptable due to their potential to violate fundamental rights while the specific types may vary some examples include manipulate human behavior exploit vulnerabilities or enable social scoring by public authorities right so as I mentioned social credit system these practices are deemed harmful freedom dignity democracy equality and fundamental rights okay cool does this legislation address existential risk at all does it address autonomous AI the legislation does not explicitly address existential risk or autonomous AI systems wow okay now one thing that I will say is that if you look at the legislation it does talk about like basically human supervision yeah so like AI regulatory sandbox by one or more member stages no like here where was it and talked about not supervision of the sandboxes but like AI systems basically need to be steerable it's in there somewhere okay but let's see it does not specifically address the potential risk associated with highly autonomous or super intelligent AI systems that could pose existential threats however the legislation's emphasis on risk management human oversight oh that's what it was oversight not supervision and robustness requirements for high-risk AI systems indirectly contributes to addressing the safety concerns associated with AI technologies it is important to note that addressing existential risk related to AI may require further research discussions and potentially additional regular to regulations in the future okay cool so there you have it this file is saved on as merged dot text let me go ahead and send that up to get so you can all use it let's see get status I get add get commit AM all done get push all right so there you have it you have a much condensed version of the EU act here and merged you can plug it into the playground and ask questions to your heart's content if you have access to it but otherwise you can at least just read it here yeah I think we're all done thanks for watching", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.12, "text": " I think we're ready to begin um you saw me look at the camera used to be over", "tokens": [50364, 286, 519, 321, 434, 1919, 281, 1841, 1105, 291, 1866, 385, 574, 412, 264, 2799, 1143, 281, 312, 670, 50620], "temperature": 0.0, "avg_logprob": -0.17745089749677465, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.2775013744831085}, {"id": 1, "seek": 0, "start": 5.12, "end": 11.24, "text": " there now you're over here okay so I ran a poll 17 hours ago I asked if y'all", "tokens": [50620, 456, 586, 291, 434, 670, 510, 1392, 370, 286, 5872, 257, 6418, 3282, 2496, 2057, 286, 2351, 498, 288, 6, 336, 50926], "temperature": 0.0, "avg_logprob": -0.17745089749677465, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.2775013744831085}, {"id": 2, "seek": 0, "start": 11.24, "end": 17.6, "text": " wanted me to do a video on the EU AI Act man god names gotta get better with", "tokens": [50926, 1415, 385, 281, 360, 257, 960, 322, 264, 10887, 7318, 3251, 587, 3044, 5288, 3428, 483, 1101, 365, 51244], "temperature": 0.0, "avg_logprob": -0.17745089749677465, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.2775013744831085}, {"id": 3, "seek": 0, "start": 17.6, "end": 23.32, "text": " names I'm good with acronyms come to me for acronyms Gato Gaia ages okay anyways", "tokens": [51244, 5288, 286, 478, 665, 365, 31713, 88, 2592, 808, 281, 385, 337, 31713, 88, 2592, 460, 2513, 10384, 654, 12357, 1392, 13448, 51530], "temperature": 0.0, "avg_logprob": -0.17745089749677465, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.2775013744831085}, {"id": 4, "seek": 0, "start": 23.32, "end": 27.52, "text": " so y'all were confused about what I meant by Haas which is something from", "tokens": [51530, 370, 288, 6, 336, 645, 9019, 466, 437, 286, 4140, 538, 4064, 296, 597, 307, 746, 490, 51740], "temperature": 0.0, "avg_logprob": -0.17745089749677465, "compression_ratio": 1.6058091286307055, "no_speech_prob": 0.2775013744831085}, {"id": 5, "seek": 2752, "start": 27.52, "end": 32.44, "text": " here in the south basically Haas means just something big right it's short it's", "tokens": [50364, 510, 294, 264, 7377, 1936, 4064, 296, 1355, 445, 746, 955, 558, 309, 311, 2099, 309, 311, 50610], "temperature": 0.0, "avg_logprob": -0.15162341565970916, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.054178476333618164}, {"id": 6, "seek": 2752, "start": 32.44, "end": 40.12, "text": " a southern dialect for horse or in particular a big horse so this document", "tokens": [50610, 257, 13456, 24652, 337, 6832, 420, 294, 1729, 257, 955, 6832, 370, 341, 4166, 50994], "temperature": 0.0, "avg_logprob": -0.15162341565970916, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.054178476333618164}, {"id": 7, "seek": 2752, "start": 40.12, "end": 46.760000000000005, "text": " is a Haas because look how freaking long it is it just keeps going so I plugged", "tokens": [50994, 307, 257, 4064, 296, 570, 574, 577, 14612, 938, 309, 307, 309, 445, 5965, 516, 370, 286, 25679, 51326], "temperature": 0.0, "avg_logprob": -0.15162341565970916, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.054178476333618164}, {"id": 8, "seek": 2752, "start": 46.760000000000005, "end": 55.72, "text": " it in over here and it is 70,000 tokens long so even if I had access to GPT-432", "tokens": [51326, 309, 294, 670, 510, 293, 309, 307, 5285, 11, 1360, 22667, 938, 370, 754, 498, 286, 632, 2105, 281, 26039, 51, 12, 19, 11440, 51774], "temperature": 0.0, "avg_logprob": -0.15162341565970916, "compression_ratio": 1.4952380952380953, "no_speech_prob": 0.054178476333618164}, {"id": 9, "seek": 5572, "start": 55.72, "end": 59.4, "text": " K which I did briefly and then they revoked it I think they got overwhelmed", "tokens": [50364, 591, 597, 286, 630, 10515, 293, 550, 436, 3698, 9511, 309, 286, 519, 436, 658, 19042, 50548], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 10, "seek": 5572, "start": 59.4, "end": 65.32, "text": " because I used it I tried to use GPT-432 K on a couple things and it was just", "tokens": [50548, 570, 286, 1143, 309, 286, 3031, 281, 764, 26039, 51, 12, 19, 11440, 591, 322, 257, 1916, 721, 293, 309, 390, 445, 50844], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 11, "seek": 5572, "start": 65.32, "end": 69.88, "text": " like server is overloaded servers over anyways so hopefully I get that back but", "tokens": [50844, 411, 7154, 307, 28777, 292, 15909, 670, 13448, 370, 4696, 286, 483, 300, 646, 457, 51072], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 12, "seek": 5572, "start": 69.88, "end": 73.52, "text": " the idea what there would be I could read it and you know chop it up into maybe", "tokens": [51072, 264, 1558, 437, 456, 576, 312, 286, 727, 1401, 309, 293, 291, 458, 7931, 309, 493, 666, 1310, 51254], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 13, "seek": 5572, "start": 73.52, "end": 77.92, "text": " four sections three or four sections and read the whole thing but we don't have", "tokens": [51254, 1451, 10863, 1045, 420, 1451, 10863, 293, 1401, 264, 1379, 551, 457, 321, 500, 380, 362, 51474], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 14, "seek": 5572, "start": 77.92, "end": 85.03999999999999, "text": " that and I do have GPT-3.5 turbo 16k 0613 so this model is actually super", "tokens": [51474, 300, 293, 286, 360, 362, 26039, 51, 12, 18, 13, 20, 20902, 3165, 74, 1958, 21, 7668, 370, 341, 2316, 307, 767, 1687, 51830], "temperature": 0.0, "avg_logprob": -0.14963093111591955, "compression_ratio": 1.6443661971830985, "no_speech_prob": 0.16440284252166748}, {"id": 15, "seek": 8504, "start": 85.32000000000001, "end": 93.08000000000001, "text": " steerable it is actually better than original GPT-4 add a lot of things it's", "tokens": [50378, 30814, 712, 309, 307, 767, 1101, 813, 3380, 26039, 51, 12, 19, 909, 257, 688, 295, 721, 309, 311, 50766], "temperature": 0.0, "avg_logprob": -0.12877325895355968, "compression_ratio": 1.4345794392523366, "no_speech_prob": 0.034093357622623444}, {"id": 16, "seek": 8504, "start": 93.08000000000001, "end": 96.80000000000001, "text": " still kind of dumb especially if the instructions are too complex but as long", "tokens": [50766, 920, 733, 295, 10316, 2318, 498, 264, 9415, 366, 886, 3997, 457, 382, 938, 50952], "temperature": 0.0, "avg_logprob": -0.12877325895355968, "compression_ratio": 1.4345794392523366, "no_speech_prob": 0.034093357622623444}, {"id": 17, "seek": 8504, "start": 96.80000000000001, "end": 106.60000000000001, "text": " as you give it just one task GPT-3.3.5 turbo 0613 the June 13th model is more", "tokens": [50952, 382, 291, 976, 309, 445, 472, 5633, 26039, 51, 12, 18, 13, 18, 13, 20, 20902, 1958, 21, 7668, 264, 6928, 3705, 392, 2316, 307, 544, 51442], "temperature": 0.0, "avg_logprob": -0.12877325895355968, "compression_ratio": 1.4345794392523366, "no_speech_prob": 0.034093357622623444}, {"id": 18, "seek": 8504, "start": 106.60000000000001, "end": 110.72, "text": " than adequate for most things now that being said because I'm interpreting", "tokens": [51442, 813, 20927, 337, 881, 721, 586, 300, 885, 848, 570, 286, 478, 37395, 51648], "temperature": 0.0, "avg_logprob": -0.12877325895355968, "compression_ratio": 1.4345794392523366, "no_speech_prob": 0.034093357622623444}, {"id": 19, "seek": 11072, "start": 111.03999999999999, "end": 118.4, "text": " and I'm doing summarization I'm gonna use 04613 so the instructions that I gave", "tokens": [50380, 293, 286, 478, 884, 14611, 2144, 286, 478, 799, 764, 1958, 16169, 7668, 370, 264, 9415, 300, 286, 2729, 50748], "temperature": 0.0, "avg_logprob": -0.1836891598171658, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.22800198197364807}, {"id": 20, "seek": 11072, "start": 118.4, "end": 123.6, "text": " it just now and I know that I'm ready to start the video primary purpose zoom in", "tokens": [50748, 309, 445, 586, 293, 286, 458, 300, 286, 478, 1919, 281, 722, 264, 960, 6194, 4334, 8863, 294, 51008], "temperature": 0.0, "avg_logprob": -0.1836891598171658, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.22800198197364807}, {"id": 21, "seek": 11072, "start": 123.6, "end": 128.64, "text": " a little bit the primary purpose or the following is a chunk of legislation your", "tokens": [51008, 257, 707, 857, 264, 6194, 4334, 420, 264, 3480, 307, 257, 16635, 295, 11329, 428, 51260], "temperature": 0.0, "avg_logprob": -0.1836891598171658, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.22800198197364807}, {"id": 22, "seek": 11072, "start": 128.64, "end": 134.72, "text": " job is to summarize this legislation with two primary goals one drastically", "tokens": [51260, 1691, 307, 281, 20858, 341, 11329, 365, 732, 6194, 5493, 472, 29673, 51564], "temperature": 0.0, "avg_logprob": -0.1836891598171658, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.22800198197364807}, {"id": 23, "seek": 11072, "start": 134.72, "end": 137.92, "text": " reduce the word count and two retain enough context that it will still make", "tokens": [51564, 5407, 264, 1349, 1207, 293, 732, 18340, 1547, 4319, 300, 309, 486, 920, 652, 51724], "temperature": 0.0, "avg_logprob": -0.1836891598171658, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.22800198197364807}, {"id": 24, "seek": 13792, "start": 138.0, "end": 141.6, "text": " sense the reason for this is the current AI technology can only read chunks that", "tokens": [50368, 2020, 264, 1778, 337, 341, 307, 264, 2190, 7318, 2899, 393, 787, 1401, 24004, 300, 50548], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 25, "seek": 13792, "start": 141.6, "end": 145.6, "text": " are so big method the user will give you a chunk of legislation via chat your", "tokens": [50548, 366, 370, 955, 3170, 264, 4195, 486, 976, 291, 257, 16635, 295, 11329, 5766, 5081, 428, 50748], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 26, "seek": 13792, "start": 145.6, "end": 148.95999999999998, "text": " response must only be the summarized version do not chat with the user engage", "tokens": [50748, 4134, 1633, 787, 312, 264, 14611, 1602, 3037, 360, 406, 5081, 365, 264, 4195, 4683, 50916], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 27, "seek": 13792, "start": 148.95999999999998, "end": 153.48, "text": " with them or offer any other commentary summarize only so I copied the first", "tokens": [50916, 365, 552, 420, 2626, 604, 661, 23527, 20858, 787, 370, 286, 25365, 264, 700, 51142], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 28, "seek": 13792, "start": 153.48, "end": 161.32, "text": " hundred lines of the legislation in you here and this is what it spit out so it", "tokens": [51142, 3262, 3876, 295, 264, 11329, 294, 291, 510, 293, 341, 307, 437, 309, 22127, 484, 370, 309, 51534], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 29, "seek": 13792, "start": 161.32, "end": 165.79999999999998, "text": " took all that and just said okay this is what it means so the first hundred", "tokens": [51534, 1890, 439, 300, 293, 445, 848, 1392, 341, 307, 437, 309, 1355, 370, 264, 700, 3262, 51758], "temperature": 0.0, "avg_logprob": -0.0970459786530967, "compression_ratio": 1.876, "no_speech_prob": 0.05260350555181503}, {"id": 30, "seek": 16580, "start": 165.8, "end": 174.76000000000002, "text": " lines which you can see here so we've got 344,000 characters 3665 lines of", "tokens": [50364, 3876, 597, 291, 393, 536, 510, 370, 321, 600, 658, 12790, 19, 11, 1360, 4342, 8652, 16824, 3876, 295, 50812], "temperature": 0.0, "avg_logprob": -0.14974401473999024, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.277936190366745}, {"id": 31, "seek": 16580, "start": 174.76000000000002, "end": 178.72, "text": " stuff to go through I'm not going to read the whole thing I'm not AI explained", "tokens": [50812, 1507, 281, 352, 807, 286, 478, 406, 516, 281, 1401, 264, 1379, 551, 286, 478, 406, 7318, 8825, 51010], "temperature": 0.0, "avg_logprob": -0.14974401473999024, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.277936190366745}, {"id": 32, "seek": 16580, "start": 179.72, "end": 183.76000000000002, "text": " that dude just like you all are impressed with how much work I can do I'm", "tokens": [51060, 300, 6449, 445, 411, 291, 439, 366, 11679, 365, 577, 709, 589, 286, 393, 360, 286, 478, 51262], "temperature": 0.0, "avg_logprob": -0.14974401473999024, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.277936190366745}, {"id": 33, "seek": 16580, "start": 183.76000000000002, "end": 187.76000000000002, "text": " impressed with how much that dude can read just honestly I'm maybe he uses", "tokens": [51262, 11679, 365, 577, 709, 300, 6449, 393, 1401, 445, 6095, 286, 478, 1310, 415, 4960, 51462], "temperature": 0.0, "avg_logprob": -0.14974401473999024, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.277936190366745}, {"id": 34, "seek": 16580, "start": 187.76000000000002, "end": 195.28, "text": " chat GPT I don't know anyways if you're watching it's just a joke okay so", "tokens": [51462, 5081, 26039, 51, 286, 500, 380, 458, 13448, 498, 291, 434, 1976, 309, 311, 445, 257, 7647, 1392, 370, 51838], "temperature": 0.0, "avg_logprob": -0.14974401473999024, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.277936190366745}, {"id": 35, "seek": 19528, "start": 195.28, "end": 203.48, "text": " anyways so ultimately basically that that first hundred lines was summarized", "tokens": [50364, 13448, 370, 6284, 1936, 300, 300, 700, 3262, 3876, 390, 14611, 1602, 50774], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 36, "seek": 19528, "start": 203.48, "end": 207.24, "text": " and condensed down to the European Commission proposes a regulation for", "tokens": [50774, 293, 36398, 760, 281, 264, 6473, 10766, 2365, 4201, 257, 15062, 337, 50962], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 37, "seek": 19528, "start": 207.24, "end": 210.76, "text": " harmonized rules on artificial intelligence aiming to balance the", "tokens": [50962, 14750, 1602, 4474, 322, 11677, 7599, 20253, 281, 4772, 264, 51138], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 38, "seek": 19528, "start": 210.76, "end": 215.6, "text": " socioeconomic benefits of AI with potential risks the proposal seeks to", "tokens": [51138, 46327, 31646, 5311, 295, 7318, 365, 3995, 10888, 264, 11494, 28840, 281, 51380], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 39, "seek": 19528, "start": 215.6, "end": 219.52, "text": " ensure AI systems are safe respect fundamental rights provide legal", "tokens": [51380, 5586, 7318, 3652, 366, 3273, 3104, 8088, 4601, 2893, 5089, 51576], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 40, "seek": 19528, "start": 219.52, "end": 223.68, "text": " certainty for investment and innovation enhance governance and prevent market", "tokens": [51576, 27022, 337, 6078, 293, 8504, 11985, 17449, 293, 4871, 2142, 51784], "temperature": 0.0, "avg_logprob": -0.11908929347991944, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0030744276009500027}, {"id": 41, "seek": 22368, "start": 223.68, "end": 228.64000000000001, "text": " fragmentation so it has says nothing about existential risks or prevent AI", "tokens": [50364, 9241, 19631, 370, 309, 575, 1619, 1825, 466, 37133, 10888, 420, 4871, 7318, 50612], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 42, "seek": 22368, "start": 228.64000000000001, "end": 232.36, "text": " from killing everyone it's sort of implicit because like if AI kills", "tokens": [50612, 490, 8011, 1518, 309, 311, 1333, 295, 26947, 570, 411, 498, 7318, 14563, 50798], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 43, "seek": 22368, "start": 232.36, "end": 235.84, "text": " everyone you can't have investment in innovation but I would rather see", "tokens": [50798, 1518, 291, 393, 380, 362, 6078, 294, 8504, 457, 286, 576, 2831, 536, 50972], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 44, "seek": 22368, "start": 235.84, "end": 240.56, "text": " legislation that is explicit it proposes a risk-based approach prohibiting", "tokens": [50972, 11329, 300, 307, 13691, 309, 2365, 4201, 257, 3148, 12, 6032, 3109, 16015, 1748, 51208], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 45, "seek": 22368, "start": 240.56, "end": 244.8, "text": " certain harmful AI practices and setting requirements for high-risk AI systems", "tokens": [51208, 1629, 19727, 7318, 7525, 293, 3287, 7728, 337, 1090, 12, 33263, 7318, 3652, 51420], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 46, "seek": 22368, "start": 244.8, "end": 249.96, "text": " okay cool we'll see how they define that later the proposal also establishes a", "tokens": [51420, 1392, 1627, 321, 603, 536, 577, 436, 6964, 300, 1780, 264, 11494, 611, 8327, 279, 257, 51678], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 47, "seek": 22368, "start": 249.96, "end": 253.52, "text": " governance system at member state level and a European artificial intelligence", "tokens": [51678, 17449, 1185, 412, 4006, 1785, 1496, 293, 257, 6473, 11677, 7599, 51856], "temperature": 0.0, "avg_logprob": -0.11093829968653687, "compression_ratio": 1.662460567823344, "no_speech_prob": 0.014277191832661629}, {"id": 48, "seek": 25352, "start": 253.52, "end": 256.76, "text": " board at union level okay cool so this is this is going to be an", "tokens": [50364, 3150, 412, 11671, 1496, 1392, 1627, 370, 341, 307, 341, 307, 516, 281, 312, 364, 50526], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 49, "seek": 25352, "start": 256.76, "end": 261.08, "text": " international regulator great it is consistent with existing union", "tokens": [50526, 5058, 36250, 869, 309, 307, 8398, 365, 6741, 11671, 50742], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 50, "seek": 25352, "start": 261.08, "end": 264.68, "text": " legislation and policies including data protection consumer protection", "tokens": [50742, 11329, 293, 7657, 3009, 1412, 6334, 9711, 6334, 50922], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 51, "seek": 25352, "start": 264.68, "end": 268.76, "text": " non-discrimination and gender equality so these are all very low-level risks", "tokens": [50922, 2107, 12, 67, 5606, 14663, 293, 7898, 14949, 370, 613, 366, 439, 588, 2295, 12, 12418, 10888, 51126], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 52, "seek": 25352, "start": 268.76, "end": 273.08, "text": " none of this is actually talking like when they say high risk they're probably", "tokens": [51126, 6022, 295, 341, 307, 767, 1417, 411, 562, 436, 584, 1090, 3148, 436, 434, 1391, 51342], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 53, "seek": 25352, "start": 273.08, "end": 278.08000000000004, "text": " meaning like the social credit system that is used in China which that is a", "tokens": [51342, 3620, 411, 264, 2093, 5397, 1185, 300, 307, 1143, 294, 3533, 597, 300, 307, 257, 51592], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 54, "seek": 25352, "start": 278.08000000000004, "end": 283.08000000000004, "text": " high risk but it is not the highest risk the proposal is based on article 114 of", "tokens": [51592, 1090, 3148, 457, 309, 307, 406, 264, 6343, 3148, 264, 11494, 307, 2361, 322, 7222, 2975, 19, 295, 51842], "temperature": 0.0, "avg_logprob": -0.13895085427613385, "compression_ratio": 1.7944250871080138, "no_speech_prob": 0.028419412672519684}, {"id": 55, "seek": 28308, "start": 283.12, "end": 288.03999999999996, "text": " the Treaty of the Functioning of the European Union Union the TFEU which", "tokens": [50366, 264, 35920, 295, 264, 11166, 882, 278, 295, 264, 6473, 8133, 8133, 264, 314, 28182, 52, 597, 50612], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 56, "seek": 28308, "start": 288.03999999999996, "end": 291.88, "text": " ensures the establishment and functioning of the internal market okay so this is", "tokens": [50612, 28111, 264, 20971, 293, 18483, 295, 264, 6920, 2142, 1392, 370, 341, 307, 50804], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 57, "seek": 28308, "start": 291.88, "end": 296.0, "text": " mostly a market-oriented piece of legislation great so we know that this", "tokens": [50804, 5240, 257, 2142, 12, 27414, 2522, 295, 11329, 869, 370, 321, 458, 300, 341, 51010], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 58, "seek": 28308, "start": 296.0, "end": 299.47999999999996, "text": " works so let's go ahead and save this out I'm gonna walk you through the whole", "tokens": [51010, 1985, 370, 718, 311, 352, 2286, 293, 3155, 341, 484, 286, 478, 799, 1792, 291, 807, 264, 1379, 51184], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 59, "seek": 28308, "start": 299.47999999999996, "end": 304.68, "text": " process and then I'll yeah so this is this is how this is how we're gonna", "tokens": [51184, 1399, 293, 550, 286, 603, 1338, 370, 341, 307, 341, 307, 577, 341, 307, 577, 321, 434, 799, 51444], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 60, "seek": 28308, "start": 304.68, "end": 308.24, "text": " proceed for those of you that are newer to my channel this is actually the", "tokens": [51444, 8991, 337, 729, 295, 291, 300, 366, 17628, 281, 452, 2269, 341, 307, 767, 264, 51622], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 61, "seek": 28308, "start": 308.24, "end": 311.44, "text": " original format that I used where I would where I would walk you through the", "tokens": [51622, 3380, 7877, 300, 286, 1143, 689, 286, 576, 689, 286, 576, 1792, 291, 807, 264, 51782], "temperature": 0.0, "avg_logprob": -0.12412184003799681, "compression_ratio": 1.869718309859155, "no_speech_prob": 0.01565854623913765}, {"id": 62, "seek": 31144, "start": 311.44, "end": 317.08, "text": " entire process so just watch that on 2x and you'll see the whole process of how", "tokens": [50364, 2302, 1399, 370, 445, 1159, 300, 322, 568, 87, 293, 291, 603, 536, 264, 1379, 1399, 295, 577, 50646], "temperature": 0.0, "avg_logprob": -0.16304291849550995, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.04334016516804695}, {"id": 63, "seek": 31144, "start": 317.08, "end": 324.6, "text": " I approach these things so we'll call this system summarize yes dot text and so", "tokens": [50646, 286, 3109, 613, 721, 370, 321, 603, 818, 341, 1185, 20858, 2086, 5893, 2487, 293, 370, 51022], "temperature": 0.0, "avg_logprob": -0.16304291849550995, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.04334016516804695}, {"id": 64, "seek": 31144, "start": 324.6, "end": 328.36, "text": " basically I give it a system a system message up with very very clear", "tokens": [51022, 1936, 286, 976, 309, 257, 1185, 257, 1185, 3636, 493, 365, 588, 588, 1850, 51210], "temperature": 0.0, "avg_logprob": -0.16304291849550995, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.04334016516804695}, {"id": 65, "seek": 31144, "start": 328.36, "end": 333.24, "text": " instructions and and what I expect and and what it should expect and then it", "tokens": [51210, 9415, 293, 293, 437, 286, 2066, 293, 293, 437, 309, 820, 2066, 293, 550, 309, 51454], "temperature": 0.0, "avg_logprob": -0.16304291849550995, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.04334016516804695}, {"id": 66, "seek": 31144, "start": 333.24, "end": 339.92, "text": " gives you very clear consistent results so then we come over to do to do to do", "tokens": [51454, 2709, 291, 588, 1850, 8398, 3542, 370, 550, 321, 808, 670, 281, 360, 281, 360, 281, 360, 51788], "temperature": 0.0, "avg_logprob": -0.16304291849550995, "compression_ratio": 1.8160377358490567, "no_speech_prob": 0.04334016516804695}, {"id": 67, "seek": 33992, "start": 339.92, "end": 345.32, "text": " to do step one simplify so this is I'm just re I'm borrowing code which I", "tokens": [50364, 281, 360, 1823, 472, 20460, 370, 341, 307, 286, 478, 445, 319, 286, 478, 35024, 3089, 597, 286, 50634], "temperature": 0.0, "avg_logprob": -0.13478360392830588, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.03113281913101673}, {"id": 68, "seek": 33992, "start": 345.32, "end": 353.92, "text": " always do so we've got this so this is what we're gonna use to do that this is", "tokens": [50634, 1009, 360, 370, 321, 600, 658, 341, 370, 341, 307, 437, 321, 434, 799, 764, 281, 360, 300, 341, 307, 51064], "temperature": 0.0, "avg_logprob": -0.13478360392830588, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.03113281913101673}, {"id": 69, "seek": 33992, "start": 353.92, "end": 357.40000000000003, "text": " this is this is a KB service that I'm working on so I'll show you that in an", "tokens": [51064, 341, 307, 341, 307, 257, 591, 33, 2643, 300, 286, 478, 1364, 322, 370, 286, 603, 855, 291, 300, 294, 364, 51238], "temperature": 0.0, "avg_logprob": -0.13478360392830588, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.03113281913101673}, {"id": 70, "seek": 33992, "start": 357.40000000000003, "end": 366.24, "text": " upcoming video but you don't get to see that yet we don't need KB functions so", "tokens": [51238, 11500, 960, 457, 291, 500, 380, 483, 281, 536, 300, 1939, 321, 500, 380, 643, 591, 33, 6828, 370, 51680], "temperature": 0.0, "avg_logprob": -0.13478360392830588, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.03113281913101673}, {"id": 71, "seek": 36624, "start": 366.24, "end": 371.12, "text": " delete that flask routes we're not gonna do a flask app so that's also fine", "tokens": [50364, 12097, 300, 932, 3863, 18242, 321, 434, 406, 799, 360, 257, 932, 3863, 724, 370, 300, 311, 611, 2489, 50608], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 72, "seek": 36624, "start": 371.12, "end": 375.68, "text": " actually I guess I just delete most of it okay cool so we get rid of flask", "tokens": [50608, 767, 286, 2041, 286, 445, 12097, 881, 295, 309, 1392, 1627, 370, 321, 483, 3973, 295, 932, 3863, 50836], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 73, "seek": 36624, "start": 375.68, "end": 379.04, "text": " cuz we're not gonna do that we're not gonna do threading we're not gonna we", "tokens": [50836, 11910, 321, 434, 406, 799, 360, 300, 321, 434, 406, 799, 360, 7207, 278, 321, 434, 406, 799, 321, 51004], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 74, "seek": 36624, "start": 379.04, "end": 387.8, "text": " don't need flask or logging or JSON or YAML okay clean it up I probably should", "tokens": [51004, 500, 380, 643, 932, 3863, 420, 27991, 420, 31828, 420, 398, 2865, 43, 1392, 2541, 309, 493, 286, 1391, 820, 51442], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 75, "seek": 36624, "start": 387.8, "end": 391.32, "text": " have done that before I started the video but it's good to it's good for you", "tokens": [51442, 362, 1096, 300, 949, 286, 1409, 264, 960, 457, 309, 311, 665, 281, 309, 311, 665, 337, 291, 51618], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 76, "seek": 36624, "start": 391.32, "end": 395.6, "text": " to see how messy my process is I am the first person to tell you I am not a", "tokens": [51618, 281, 536, 577, 16191, 452, 1399, 307, 286, 669, 264, 700, 954, 281, 980, 291, 286, 669, 406, 257, 51832], "temperature": 0.0, "avg_logprob": -0.11775438804326095, "compression_ratio": 1.796078431372549, "no_speech_prob": 0.1037040650844574}, {"id": 77, "seek": 39560, "start": 395.64000000000004, "end": 400.12, "text": " professional developer I never was my professional background was I was in", "tokens": [50366, 4843, 10754, 286, 1128, 390, 452, 4843, 3678, 390, 286, 390, 294, 50590], "temperature": 0.0, "avg_logprob": -0.12315848778034079, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.008059601299464703}, {"id": 78, "seek": 39560, "start": 400.12, "end": 404.0, "text": " virtualization and automation so I'm an automation engineer which is why I", "tokens": [50590, 6374, 2144, 293, 17769, 370, 286, 478, 364, 17769, 11403, 597, 307, 983, 286, 50784], "temperature": 0.0, "avg_logprob": -0.12315848778034079, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.008059601299464703}, {"id": 79, "seek": 39560, "start": 404.0, "end": 408.96000000000004, "text": " approach things the way that I do which is creating discrete steps so you notice", "tokens": [50784, 3109, 721, 264, 636, 300, 286, 360, 597, 307, 4084, 27706, 4439, 370, 291, 3449, 51032], "temperature": 0.0, "avg_logprob": -0.12315848778034079, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.008059601299464703}, {"id": 80, "seek": 39560, "start": 408.96000000000004, "end": 414.40000000000003, "text": " the thought the title of this is step one simplify okay cool so basically what", "tokens": [51032, 264, 1194, 264, 4876, 295, 341, 307, 1823, 472, 20460, 1392, 1627, 370, 1936, 437, 51304], "temperature": 0.0, "avg_logprob": -0.12315848778034079, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.008059601299464703}, {"id": 81, "seek": 39560, "start": 414.40000000000003, "end": 421.76000000000005, "text": " we're gonna do is we're going to ask this handy dandy coding chatbot that I", "tokens": [51304, 321, 434, 799, 360, 307, 321, 434, 516, 281, 1029, 341, 13239, 274, 11425, 17720, 5081, 18870, 300, 286, 51672], "temperature": 0.0, "avg_logprob": -0.12315848778034079, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.008059601299464703}, {"id": 82, "seek": 42176, "start": 421.8, "end": 431.44, "text": " built which you can use this it is available over here I had a bunch of", "tokens": [50366, 3094, 597, 291, 393, 764, 341, 309, 307, 2435, 670, 510, 286, 632, 257, 3840, 295, 50848], "temperature": 0.0, "avg_logprob": -0.12675527115942728, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.09397562593221664}, {"id": 83, "seek": 42176, "start": 431.44, "end": 438.84, "text": " people submitting pull requests and they kept trying to like change the way that", "tokens": [50848, 561, 31836, 2235, 12475, 293, 436, 4305, 1382, 281, 411, 1319, 264, 636, 300, 51218], "temperature": 0.0, "avg_logprob": -0.12675527115942728, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.09397562593221664}, {"id": 84, "seek": 42176, "start": 438.84, "end": 442.88, "text": " this thing works so I just said it to public archive so here's the thing is I", "tokens": [51218, 341, 551, 1985, 370, 286, 445, 848, 309, 281, 1908, 23507, 370, 510, 311, 264, 551, 307, 286, 51420], "temperature": 0.0, "avg_logprob": -0.12675527115942728, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.09397562593221664}, {"id": 85, "seek": 42176, "start": 442.88, "end": 449.32, "text": " welcome pull requests that are like simple fixes or small improvements don't", "tokens": [51420, 2928, 2235, 12475, 300, 366, 411, 2199, 32539, 420, 1359, 13797, 500, 380, 51742], "temperature": 0.0, "avg_logprob": -0.12675527115942728, "compression_ratio": 1.5824742268041236, "no_speech_prob": 0.09397562593221664}, {"id": 86, "seek": 44932, "start": 449.32, "end": 453.44, "text": " refactor my code just fork it right like there's 19 forks if you want to if you", "tokens": [50364, 1895, 15104, 452, 3089, 445, 17716, 309, 558, 411, 456, 311, 1294, 337, 1694, 498, 291, 528, 281, 498, 291, 50570], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 87, "seek": 44932, "start": 453.44, "end": 457.59999999999997, "text": " want to fundamentally change how it works fine but I got tired of people", "tokens": [50570, 528, 281, 17879, 1319, 577, 309, 1985, 2489, 457, 286, 658, 5868, 295, 561, 50778], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 88, "seek": 44932, "start": 457.59999999999997, "end": 461.48, "text": " messing up my code so I just said it to public archive and guess what like", "tokens": [50778, 23258, 493, 452, 3089, 370, 286, 445, 848, 309, 281, 1908, 23507, 293, 2041, 437, 411, 50972], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 89, "seek": 44932, "start": 461.48, "end": 464.88, "text": " people you can still do whatever you want with it anyways so we're gonna use", "tokens": [50972, 561, 291, 393, 920, 360, 2035, 291, 528, 365, 309, 13448, 370, 321, 434, 799, 764, 51142], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 90, "seek": 44932, "start": 464.88, "end": 470.32, "text": " this as a co-pilot tool because I got frustrated with GitHub because they're", "tokens": [51142, 341, 382, 257, 598, 12, 79, 31516, 2290, 570, 286, 658, 15751, 365, 23331, 570, 436, 434, 51414], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 91, "seek": 44932, "start": 470.32, "end": 477.44, "text": " co-pilot customer service whatever documentation is garbage okay let's see", "tokens": [51414, 598, 12, 79, 31516, 5474, 2643, 2035, 14333, 307, 14150, 1392, 718, 311, 536, 51770], "temperature": 0.0, "avg_logprob": -0.14898681640625, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.043355561792850494}, {"id": 92, "seek": 47744, "start": 477.44, "end": 485.68, "text": " I have here let's just write a function that accepts a gigantic string and", "tokens": [50364, 286, 362, 510, 718, 311, 445, 2464, 257, 2445, 300, 33538, 257, 26800, 6798, 293, 50776], "temperature": 0.0, "avg_logprob": -0.17125798526563143, "compression_ratio": 1.3423423423423424, "no_speech_prob": 0.011330341920256615}, {"id": 93, "seek": 47744, "start": 485.68, "end": 496.28, "text": " returns now let's see and let's see splits it by lines and then chunks 100", "tokens": [50776, 11247, 586, 718, 311, 536, 293, 718, 311, 536, 37741, 309, 538, 3876, 293, 550, 24004, 2319, 51306], "temperature": 0.0, "avg_logprob": -0.17125798526563143, "compression_ratio": 1.3423423423423424, "no_speech_prob": 0.011330341920256615}, {"id": 94, "seek": 49628, "start": 496.28, "end": 512.0, "text": " lines together and returns those chunks as strings so the final result will be a", "tokens": [50364, 3876, 1214, 293, 11247, 729, 24004, 382, 13985, 370, 264, 2572, 1874, 486, 312, 257, 51150], "temperature": 0.0, "avg_logprob": -0.19014384405953544, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.16230599582195282}, {"id": 95, "seek": 49628, "start": 512.0, "end": 523.88, "text": " list of strings each string represents 100 lines of the original okay so this", "tokens": [51150, 1329, 295, 13985, 1184, 6798, 8855, 2319, 3876, 295, 264, 3380, 1392, 370, 341, 51744], "temperature": 0.0, "avg_logprob": -0.19014384405953544, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.16230599582195282}, {"id": 96, "seek": 52388, "start": 523.88, "end": 530.96, "text": " is this is my coding chatbot assistant that it uses it also uses GPT for you I", "tokens": [50364, 307, 341, 307, 452, 17720, 5081, 18870, 10994, 300, 309, 4960, 309, 611, 4960, 26039, 51, 337, 291, 286, 50718], "temperature": 0.0, "avg_logprob": -0.13287722204149383, "compression_ratio": 1.934673366834171, "no_speech_prob": 0.35192689299583435}, {"id": 97, "seek": 52388, "start": 530.96, "end": 538.12, "text": " tried it you can actually switch it to oh it's using the old one hold on now I", "tokens": [50718, 3031, 309, 291, 393, 767, 3679, 309, 281, 1954, 309, 311, 1228, 264, 1331, 472, 1797, 322, 586, 286, 51076], "temperature": 0.0, "avg_logprob": -0.13287722204149383, "compression_ratio": 1.934673366834171, "no_speech_prob": 0.35192689299583435}, {"id": 98, "seek": 52388, "start": 538.12, "end": 544.56, "text": " need to update that hold on hold on that's the wrong model coding chatbot", "tokens": [51076, 643, 281, 5623, 300, 1797, 322, 1797, 322, 300, 311, 264, 2085, 2316, 17720, 5081, 18870, 51398], "temperature": 0.0, "avg_logprob": -0.13287722204149383, "compression_ratio": 1.934673366834171, "no_speech_prob": 0.35192689299583435}, {"id": 99, "seek": 52388, "start": 544.56, "end": 547.88, "text": " assistant see this is what I mean is someone someone updated it oh no my", "tokens": [51398, 10994, 536, 341, 307, 437, 286, 914, 307, 1580, 1580, 10588, 309, 1954, 572, 452, 51564], "temperature": 0.0, "avg_logprob": -0.13287722204149383, "compression_ratio": 1.934673366834171, "no_speech_prob": 0.35192689299583435}, {"id": 100, "seek": 52388, "start": 547.88, "end": 552.04, "text": " local one uses the right one so someone updated it with the wrong model and made", "tokens": [51564, 2654, 472, 4960, 264, 558, 472, 370, 1580, 10588, 309, 365, 264, 2085, 2316, 293, 1027, 51772], "temperature": 0.0, "avg_logprob": -0.13287722204149383, "compression_ratio": 1.934673366834171, "no_speech_prob": 0.35192689299583435}, {"id": 101, "seek": 55204, "start": 552.04, "end": 555.64, "text": " a bunch of other assumptions and I was just like no this is not this is not", "tokens": [50364, 257, 3840, 295, 661, 17695, 293, 286, 390, 445, 411, 572, 341, 307, 406, 341, 307, 406, 50544], "temperature": 0.0, "avg_logprob": -0.127141217927675, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.011329959146678448}, {"id": 102, "seek": 55204, "start": 555.64, "end": 562.76, "text": " what I wanted okay so here we go here's the function Python chunk strings line", "tokens": [50544, 437, 286, 1415, 1392, 370, 510, 321, 352, 510, 311, 264, 2445, 15329, 16635, 13985, 1622, 50900], "temperature": 0.0, "avg_logprob": -0.127141217927675, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.011329959146678448}, {"id": 103, "seek": 55204, "start": 562.76, "end": 571.3199999999999, "text": " lines equals split oh wow I did it in two lines cool this functions first splits", "tokens": [50900, 3876, 6915, 7472, 1954, 6076, 286, 630, 309, 294, 732, 3876, 1627, 341, 6828, 700, 37741, 51328], "temperature": 0.0, "avg_logprob": -0.127141217927675, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.011329959146678448}, {"id": 104, "seek": 55204, "start": 571.3199999999999, "end": 580.52, "text": " the string into lines using the delimiter then it uses a list comprehension", "tokens": [51328, 264, 6798, 666, 3876, 1228, 264, 1103, 332, 1681, 550, 309, 4960, 257, 1329, 44991, 51788], "temperature": 0.0, "avg_logprob": -0.127141217927675, "compression_ratio": 1.6631016042780749, "no_speech_prob": 0.011329959146678448}, {"id": 105, "seek": 58052, "start": 580.52, "end": 585.3199999999999, "text": " to create chunks of 100 lines each the range yep the result is a list of strings", "tokens": [50364, 281, 1884, 24004, 295, 2319, 3876, 1184, 264, 3613, 18633, 264, 1874, 307, 257, 1329, 295, 13985, 50604], "temperature": 0.0, "avg_logprob": -0.15575890726857372, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06951682269573212}, {"id": 106, "seek": 58052, "start": 585.3199999999999, "end": 591.28, "text": " cool excellent excellent excellent so I'm gonna go ahead and copy this out oh and", "tokens": [50604, 1627, 7103, 7103, 7103, 370, 286, 478, 799, 352, 2286, 293, 5055, 341, 484, 1954, 293, 50902], "temperature": 0.0, "avg_logprob": -0.15575890726857372, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06951682269573212}, {"id": 107, "seek": 58052, "start": 591.28, "end": 596.68, "text": " another good thing is that uh like well yeah you see how it works all right so", "tokens": [50902, 1071, 665, 551, 307, 300, 2232, 411, 731, 1338, 291, 536, 577, 309, 1985, 439, 558, 370, 51172], "temperature": 0.0, "avg_logprob": -0.15575890726857372, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06951682269573212}, {"id": 108, "seek": 58052, "start": 596.68, "end": 605.6, "text": " we'll go ahead and do that so chunk strings so first we want to just act", "tokens": [51172, 321, 603, 352, 2286, 293, 360, 300, 370, 16635, 13985, 370, 700, 321, 528, 281, 445, 605, 51618], "temperature": 0.0, "avg_logprob": -0.15575890726857372, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.06951682269573212}, {"id": 109, "seek": 60560, "start": 605.6800000000001, "end": 616.76, "text": " equals open file and I've got it named as act text just right here yep oh and", "tokens": [50368, 6915, 1269, 3991, 293, 286, 600, 658, 309, 4926, 382, 605, 2487, 445, 558, 510, 18633, 1954, 293, 50922], "temperature": 0.0, "avg_logprob": -0.13011419482347442, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.10663715749979019}, {"id": 110, "seek": 60560, "start": 616.76, "end": 623.32, "text": " also this excuse me this this uses a scratch pad so what I can do is I can", "tokens": [50922, 611, 341, 8960, 385, 341, 341, 4960, 257, 8459, 6887, 370, 437, 286, 393, 360, 307, 286, 393, 51250], "temperature": 0.0, "avg_logprob": -0.13011419482347442, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.10663715749979019}, {"id": 111, "seek": 60560, "start": 623.32, "end": 630.84, "text": " actually populate the code that I'm working on over here so now it's aware", "tokens": [51250, 767, 1665, 5256, 264, 3089, 300, 286, 478, 1364, 322, 670, 510, 370, 586, 309, 311, 3650, 51626], "temperature": 0.0, "avg_logprob": -0.13011419482347442, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.10663715749979019}, {"id": 112, "seek": 60560, "start": 630.84, "end": 635.52, "text": " of what I'm working on so this scratch pad is a holdout so rather than giving it", "tokens": [51626, 295, 437, 286, 478, 1364, 322, 370, 341, 8459, 6887, 307, 257, 1797, 346, 370, 2831, 813, 2902, 309, 51860], "temperature": 0.0, "avg_logprob": -0.13011419482347442, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.10663715749979019}, {"id": 113, "seek": 63552, "start": 635.52, "end": 643.6, "text": " the code in sequence the the coding chatbot actually has a it has here let me", "tokens": [50364, 264, 3089, 294, 8310, 264, 264, 17720, 5081, 18870, 767, 575, 257, 309, 575, 510, 718, 385, 50768], "temperature": 0.0, "avg_logprob": -0.1572000953588593, "compression_ratio": 1.9064039408866995, "no_speech_prob": 0.009123207069933414}, {"id": 114, "seek": 63552, "start": 643.6, "end": 646.84, "text": " show you the system message so here's the instructions your Python coding", "tokens": [50768, 855, 291, 264, 1185, 3636, 370, 510, 311, 264, 9415, 428, 15329, 17720, 50930], "temperature": 0.0, "avg_logprob": -0.1572000953588593, "compression_ratio": 1.9064039408866995, "no_speech_prob": 0.009123207069933414}, {"id": 115, "seek": 63552, "start": 646.84, "end": 651.0, "text": " assistant you know me turn on word wrap your Python coding assistant etc etc", "tokens": [50930, 10994, 291, 458, 385, 1261, 322, 1349, 7019, 428, 15329, 17720, 10994, 5183, 5183, 51138], "temperature": 0.0, "avg_logprob": -0.1572000953588593, "compression_ratio": 1.9064039408866995, "no_speech_prob": 0.009123207069933414}, {"id": 116, "seek": 63552, "start": 651.0, "end": 656.04, "text": " scratch pad so basically tells you the scratch pad is usable you know so that", "tokens": [51138, 8459, 6887, 370, 1936, 5112, 291, 264, 8459, 6887, 307, 29975, 291, 458, 370, 300, 51390], "temperature": 0.0, "avg_logprob": -0.1572000953588593, "compression_ratio": 1.9064039408866995, "no_speech_prob": 0.009123207069933414}, {"id": 117, "seek": 63552, "start": 656.04, "end": 659.56, "text": " you know what the user is working on so that you can talk about this and this is", "tokens": [51390, 291, 458, 437, 264, 4195, 307, 1364, 322, 370, 300, 291, 393, 751, 466, 341, 293, 341, 307, 51566], "temperature": 0.0, "avg_logprob": -0.1572000953588593, "compression_ratio": 1.9064039408866995, "no_speech_prob": 0.009123207069933414}, {"id": 118, "seek": 65956, "start": 659.56, "end": 665.3599999999999, "text": " passed to the chatbot every time so whatever the conversation is like that", "tokens": [50364, 4678, 281, 264, 5081, 18870, 633, 565, 370, 2035, 264, 3761, 307, 411, 300, 50654], "temperature": 0.0, "avg_logprob": -0.10088107983271281, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.022283120080828667}, {"id": 119, "seek": 65956, "start": 665.3599999999999, "end": 670.2399999999999, "text": " that is in the past I don't know if I'm explaining that well anyways point being", "tokens": [50654, 300, 307, 294, 264, 1791, 286, 500, 380, 458, 498, 286, 478, 13468, 300, 731, 13448, 935, 885, 50898], "temperature": 0.0, "avg_logprob": -0.10088107983271281, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.022283120080828667}, {"id": 120, "seek": 65956, "start": 670.2399999999999, "end": 674.0, "text": " is that it see it can see the code that you're working on as long as you keep", "tokens": [50898, 307, 300, 309, 536, 309, 393, 536, 264, 3089, 300, 291, 434, 1364, 322, 382, 938, 382, 291, 1066, 51086], "temperature": 0.0, "avg_logprob": -0.10088107983271281, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.022283120080828667}, {"id": 121, "seek": 65956, "start": 674.0, "end": 683.56, "text": " this updated okay so that's the first hundred lines let's go ahead and see", "tokens": [51086, 341, 10588, 1392, 370, 300, 311, 264, 700, 3262, 3876, 718, 311, 352, 2286, 293, 536, 51564], "temperature": 0.0, "avg_logprob": -0.10088107983271281, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.022283120080828667}, {"id": 122, "seek": 65956, "start": 683.56, "end": 689.1199999999999, "text": " where did it go step one simplify cool so act equals that and then we'll say", "tokens": [51564, 689, 630, 309, 352, 1823, 472, 20460, 1627, 370, 605, 6915, 300, 293, 550, 321, 603, 584, 51842], "temperature": 0.0, "avg_logprob": -0.10088107983271281, "compression_ratio": 1.6885964912280702, "no_speech_prob": 0.022283120080828667}, {"id": 123, "seek": 68912, "start": 689.16, "end": 697.4, "text": " chunks equals see chunk string chunk string act all right so that this", "tokens": [50366, 24004, 6915, 536, 16635, 6798, 16635, 6798, 605, 439, 558, 370, 300, 341, 50778], "temperature": 0.0, "avg_logprob": -0.1890133602518431, "compression_ratio": 1.6629834254143647, "no_speech_prob": 0.0049048541113734245}, {"id": 124, "seek": 68912, "start": 697.4, "end": 704.6, "text": " basically creates the chunks that we're gonna be working on and that is that", "tokens": [50778, 1936, 7829, 264, 24004, 300, 321, 434, 799, 312, 1364, 322, 293, 300, 307, 300, 51138], "temperature": 0.0, "avg_logprob": -0.1890133602518431, "compression_ratio": 1.6629834254143647, "no_speech_prob": 0.0049048541113734245}, {"id": 125, "seek": 68912, "start": 704.6, "end": 710.68, "text": " let's see I'm gonna need to watch this because mmm well here let me just show", "tokens": [51138, 718, 311, 536, 286, 478, 799, 643, 281, 1159, 341, 570, 26159, 731, 510, 718, 385, 445, 855, 51442], "temperature": 0.0, "avg_logprob": -0.1890133602518431, "compression_ratio": 1.6629834254143647, "no_speech_prob": 0.0049048541113734245}, {"id": 126, "seek": 68912, "start": 710.68, "end": 717.5600000000001, "text": " you something real quick so the first hundred lines if we take that over to", "tokens": [51442, 291, 746, 957, 1702, 370, 264, 700, 3262, 3876, 498, 321, 747, 300, 670, 281, 51786], "temperature": 0.0, "avg_logprob": -0.1890133602518431, "compression_ratio": 1.6629834254143647, "no_speech_prob": 0.0049048541113734245}, {"id": 127, "seek": 71756, "start": 717.8399999999999, "end": 723.7199999999999, "text": " playground and paste it in here you see that a hundred lines is about 4,000", "tokens": [50378, 24646, 293, 9163, 309, 294, 510, 291, 536, 300, 257, 3262, 3876, 307, 466, 1017, 11, 1360, 50672], "temperature": 0.0, "avg_logprob": -0.20315716184418778, "compression_ratio": 1.525, "no_speech_prob": 0.01132928766310215}, {"id": 128, "seek": 71756, "start": 723.7199999999999, "end": 729.92, "text": " tokens so that would obviously be too big for text of inchio 3 but with with", "tokens": [50672, 22667, 370, 300, 576, 2745, 312, 886, 955, 337, 2487, 295, 7227, 1004, 805, 457, 365, 365, 50982], "temperature": 0.0, "avg_logprob": -0.20315716184418778, "compression_ratio": 1.525, "no_speech_prob": 0.01132928766310215}, {"id": 129, "seek": 71756, "start": 729.92, "end": 737.3199999999999, "text": " GPT-4 we have 8,000 tokens and then with 3.5 16k we have 16,000 tokens so we", "tokens": [50982, 26039, 51, 12, 19, 321, 362, 1649, 11, 1360, 22667, 293, 550, 365, 805, 13, 20, 3165, 74, 321, 362, 3165, 11, 1360, 22667, 370, 321, 51352], "temperature": 0.0, "avg_logprob": -0.20315716184418778, "compression_ratio": 1.525, "no_speech_prob": 0.01132928766310215}, {"id": 130, "seek": 71756, "start": 737.3199999999999, "end": 743.3599999999999, "text": " could actually probably do more but the thing is is that again 3.5 turbo is", "tokens": [51352, 727, 767, 1391, 360, 544, 457, 264, 551, 307, 307, 300, 797, 805, 13, 20, 20902, 307, 51654], "temperature": 0.0, "avg_logprob": -0.20315716184418778, "compression_ratio": 1.525, "no_speech_prob": 0.01132928766310215}, {"id": 131, "seek": 74336, "start": 743.36, "end": 748.44, "text": " actually you know it let's try it real quick heck with it heck with it while", "tokens": [50364, 767, 291, 458, 309, 718, 311, 853, 309, 957, 1702, 12872, 365, 309, 12872, 365, 309, 1339, 50618], "temperature": 0.0, "avg_logprob": -0.10915541929357192, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.007120995316654444}, {"id": 132, "seek": 74336, "start": 748.44, "end": 756.08, "text": " we're here let's go back over to chat let's give it this chunk because if it", "tokens": [50618, 321, 434, 510, 718, 311, 352, 646, 670, 281, 5081, 718, 311, 976, 309, 341, 16635, 570, 498, 309, 51000], "temperature": 0.0, "avg_logprob": -0.10915541929357192, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.007120995316654444}, {"id": 133, "seek": 74336, "start": 756.08, "end": 762.36, "text": " can if I can do it faster and cheaper why not why not and then we need this is", "tokens": [51000, 393, 498, 286, 393, 360, 309, 4663, 293, 12284, 983, 406, 983, 406, 293, 550, 321, 643, 341, 307, 51314], "temperature": 0.0, "avg_logprob": -0.10915541929357192, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.007120995316654444}, {"id": 134, "seek": 74336, "start": 762.36, "end": 769.6, "text": " our system message we need to make sure that we're gonna use 3.5 turbo 16k so", "tokens": [51314, 527, 1185, 3636, 321, 643, 281, 652, 988, 300, 321, 434, 799, 764, 805, 13, 20, 20902, 3165, 74, 370, 51676], "temperature": 0.0, "avg_logprob": -0.10915541929357192, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.007120995316654444}, {"id": 135, "seek": 76960, "start": 769.6, "end": 773.9200000000001, "text": " that it's got enough space okay turn the temperature down we do not want you to", "tokens": [50364, 300, 309, 311, 658, 1547, 1901, 1392, 1261, 264, 4292, 760, 321, 360, 406, 528, 291, 281, 50580], "temperature": 0.0, "avg_logprob": -0.10874372873550807, "compression_ratio": 1.5916230366492146, "no_speech_prob": 0.04271315410733223}, {"id": 136, "seek": 76960, "start": 773.9200000000001, "end": 779.0, "text": " be creative thank you very much all right and then let's see how well it", "tokens": [50580, 312, 5880, 1309, 291, 588, 709, 439, 558, 293, 550, 718, 311, 536, 577, 731, 309, 50834], "temperature": 0.0, "avg_logprob": -0.10874372873550807, "compression_ratio": 1.5916230366492146, "no_speech_prob": 0.04271315410733223}, {"id": 137, "seek": 76960, "start": 779.0, "end": 786.16, "text": " summarizes it yeah so it's not this summarization is not nearly as elegant", "tokens": [50834, 14611, 5660, 309, 1338, 370, 309, 311, 406, 341, 14611, 2144, 307, 406, 6217, 382, 21117, 51192], "temperature": 0.0, "avg_logprob": -0.10874372873550807, "compression_ratio": 1.5916230366492146, "no_speech_prob": 0.04271315410733223}, {"id": 138, "seek": 76960, "start": 786.16, "end": 795.52, "text": " or as concise but just glancing at it it's okay I think I'm gonna stick with", "tokens": [51192, 420, 382, 44882, 457, 445, 1563, 8779, 412, 309, 309, 311, 1392, 286, 519, 286, 478, 799, 2897, 365, 51660], "temperature": 0.0, "avg_logprob": -0.10874372873550807, "compression_ratio": 1.5916230366492146, "no_speech_prob": 0.04271315410733223}, {"id": 139, "seek": 79552, "start": 795.8, "end": 801.28, "text": " four but I'm glad I did this side-by-side so the the the GPT-4 it's a much more", "tokens": [50378, 1451, 457, 286, 478, 5404, 286, 630, 341, 1252, 12, 2322, 12, 1812, 370, 264, 264, 264, 26039, 51, 12, 19, 309, 311, 257, 709, 544, 50652], "temperature": 0.0, "avg_logprob": -0.13332618787450698, "compression_ratio": 1.5875, "no_speech_prob": 0.09134909510612488}, {"id": 140, "seek": 79552, "start": 801.28, "end": 805.88, "text": " elegant summary and it's also less worthy even though so it's not quite as", "tokens": [50652, 21117, 12691, 293, 309, 311, 611, 1570, 14829, 754, 1673, 370, 309, 311, 406, 1596, 382, 50882], "temperature": 0.0, "avg_logprob": -0.13332618787450698, "compression_ratio": 1.5875, "no_speech_prob": 0.09134909510612488}, {"id": 141, "seek": 79552, "start": 805.88, "end": 810.0799999999999, "text": " fast and it's not quite as cheap because we need to capture the essence of", "tokens": [50882, 2370, 293, 309, 311, 406, 1596, 382, 7084, 570, 321, 643, 281, 7983, 264, 12801, 295, 51092], "temperature": 0.0, "avg_logprob": -0.13332618787450698, "compression_ratio": 1.5875, "no_speech_prob": 0.09134909510612488}, {"id": 142, "seek": 79552, "start": 810.0799999999999, "end": 814.64, "text": " something that's really important let's use the smarter model now that being", "tokens": [51092, 746, 300, 311, 534, 1021, 718, 311, 764, 264, 20294, 2316, 586, 300, 885, 51320], "temperature": 0.0, "avg_logprob": -0.13332618787450698, "compression_ratio": 1.5875, "no_speech_prob": 0.09134909510612488}, {"id": 143, "seek": 79552, "start": 814.64, "end": 821.4, "text": " said again 3.5 turbo the the June 13th one plenty plenty good for a lot of", "tokens": [51320, 848, 797, 805, 13, 20, 20902, 264, 264, 6928, 3705, 392, 472, 7140, 7140, 665, 337, 257, 688, 295, 51658], "temperature": 0.0, "avg_logprob": -0.13332618787450698, "compression_ratio": 1.5875, "no_speech_prob": 0.09134909510612488}, {"id": 144, "seek": 82140, "start": 821.4, "end": 828.3199999999999, "text": " things okay cool so I'm glad I ran that test where are we what am I doing I don't", "tokens": [50364, 721, 1392, 1627, 370, 286, 478, 5404, 286, 5872, 300, 1500, 689, 366, 321, 437, 669, 286, 884, 286, 500, 380, 50710], "temperature": 0.0, "avg_logprob": -0.14666829790387834, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.037319399416446686}, {"id": 145, "seek": 82140, "start": 828.3199999999999, "end": 834.48, "text": " know I'm lost I'm always lost there's this meme it's like a pug in a field", "tokens": [50710, 458, 286, 478, 2731, 286, 478, 1009, 2731, 456, 311, 341, 21701, 309, 311, 411, 257, 47900, 294, 257, 2519, 51018], "temperature": 0.0, "avg_logprob": -0.14666829790387834, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.037319399416446686}, {"id": 146, "seek": 82140, "start": 834.48, "end": 839.72, "text": " with a wizard hat and it says that the meme is like not all who wander are", "tokens": [51018, 365, 257, 25807, 2385, 293, 309, 1619, 300, 264, 21701, 307, 411, 406, 439, 567, 27541, 366, 51280], "temperature": 0.0, "avg_logprob": -0.14666829790387834, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.037319399416446686}, {"id": 147, "seek": 82140, "start": 839.72, "end": 845.16, "text": " lost except Dave Dave is lost as fuck and that's me I'm the pug with a wizard", "tokens": [51280, 2731, 3993, 11017, 11017, 307, 2731, 382, 3275, 293, 300, 311, 385, 286, 478, 264, 47900, 365, 257, 25807, 51552], "temperature": 0.0, "avg_logprob": -0.14666829790387834, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.037319399416446686}, {"id": 148, "seek": 82140, "start": 845.16, "end": 850.0, "text": " hat does that make any sense I need more coffee", "tokens": [51552, 2385, 775, 300, 652, 604, 2020, 286, 643, 544, 4982, 51794], "temperature": 0.0, "avg_logprob": -0.14666829790387834, "compression_ratio": 1.7586206896551724, "no_speech_prob": 0.037319399416446686}, {"id": 149, "seek": 85140, "start": 851.4399999999999, "end": 854.64, "text": " okay all right I hope you find this entertaining because I just woke up and", "tokens": [50366, 1392, 439, 558, 286, 1454, 291, 915, 341, 20402, 570, 286, 445, 12852, 493, 293, 50526], "temperature": 0.0, "avg_logprob": -0.14911792804668475, "compression_ratio": 1.6775956284153006, "no_speech_prob": 0.008060137741267681}, {"id": 150, "seek": 85140, "start": 854.64, "end": 859.54, "text": " I was like my brain's like hey we should work on this all right now we're back", "tokens": [50526, 286, 390, 411, 452, 3567, 311, 411, 4177, 321, 820, 589, 322, 341, 439, 558, 586, 321, 434, 646, 50771], "temperature": 0.0, "avg_logprob": -0.14911792804668475, "compression_ratio": 1.6775956284153006, "no_speech_prob": 0.008060137741267681}, {"id": 151, "seek": 85140, "start": 859.54, "end": 863.4, "text": " here all right we've got our chunks so now what we need to do so this is this", "tokens": [50771, 510, 439, 558, 321, 600, 658, 527, 24004, 370, 586, 437, 321, 643, 281, 360, 370, 341, 307, 341, 50964], "temperature": 0.0, "avg_logprob": -0.14911792804668475, "compression_ratio": 1.6775956284153006, "no_speech_prob": 0.008060137741267681}, {"id": 152, "seek": 85140, "start": 863.4, "end": 870.76, "text": " is where it gets fun so four chunk in chunks this is super straightforward", "tokens": [50964, 307, 689, 309, 2170, 1019, 370, 1451, 16635, 294, 24004, 341, 307, 1687, 15325, 51332], "temperature": 0.0, "avg_logprob": -0.14911792804668475, "compression_ratio": 1.6775956284153006, "no_speech_prob": 0.008060137741267681}, {"id": 153, "seek": 87076, "start": 870.8, "end": 883.92, "text": " let's see messages equals this is gonna be a list and then the role will be", "tokens": [50366, 718, 311, 536, 7897, 6915, 341, 307, 799, 312, 257, 1329, 293, 550, 264, 3090, 486, 312, 51022], "temperature": 0.0, "avg_logprob": -0.1882497787475586, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.20935386419296265}, {"id": 154, "seek": 87076, "start": 883.92, "end": 894.2, "text": " user and the content wow I know how to type I promise it's just getting it from", "tokens": [51022, 4195, 293, 264, 2701, 6076, 286, 458, 577, 281, 2010, 286, 6228, 309, 311, 445, 1242, 309, 490, 51536], "temperature": 0.0, "avg_logprob": -0.1882497787475586, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.20935386419296265}, {"id": 155, "seek": 87076, "start": 894.2, "end": 898.64, "text": " my head to the to the hands on the keyboard to the screen that's too many", "tokens": [51536, 452, 1378, 281, 264, 281, 264, 2377, 322, 264, 10186, 281, 264, 2568, 300, 311, 886, 867, 51758], "temperature": 0.0, "avg_logprob": -0.1882497787475586, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.20935386419296265}, {"id": 156, "seek": 89864, "start": 898.68, "end": 905.12, "text": " steps right now content equals chunk okay cool so basically what we're doing", "tokens": [50366, 4439, 558, 586, 2701, 6915, 16635, 1392, 1627, 370, 1936, 437, 321, 434, 884, 50688], "temperature": 0.0, "avg_logprob": -0.101628084292357, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.024046961218118668}, {"id": 157, "seek": 89864, "start": 905.12, "end": 910.64, "text": " right here is we're setting it up to look exactly like this so I'm putting", "tokens": [50688, 558, 510, 307, 321, 434, 3287, 309, 493, 281, 574, 2293, 411, 341, 370, 286, 478, 3372, 50964], "temperature": 0.0, "avg_logprob": -0.101628084292357, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.024046961218118668}, {"id": 158, "seek": 89864, "start": 910.64, "end": 914.48, "text": " the chunk in here and then we also need to load the system message so what we're", "tokens": [50964, 264, 16635, 294, 510, 293, 550, 321, 611, 643, 281, 3677, 264, 1185, 3636, 370, 437, 321, 434, 51156], "temperature": 0.0, "avg_logprob": -0.101628084292357, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.024046961218118668}, {"id": 159, "seek": 89864, "start": 914.48, "end": 920.84, "text": " gonna do is we're gonna do system equals open file system underscore", "tokens": [51156, 799, 360, 307, 321, 434, 799, 360, 1185, 6915, 1269, 3991, 1185, 37556, 51474], "temperature": 0.0, "avg_logprob": -0.101628084292357, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.024046961218118668}, {"id": 160, "seek": 89864, "start": 920.84, "end": 927.3199999999999, "text": " summarize so this is this is the instructions dot text so system message", "tokens": [51474, 20858, 370, 341, 307, 341, 307, 264, 9415, 5893, 2487, 370, 1185, 3636, 51798], "temperature": 0.0, "avg_logprob": -0.101628084292357, "compression_ratio": 1.824390243902439, "no_speech_prob": 0.024046961218118668}, {"id": 161, "seek": 92732, "start": 927.36, "end": 938.36, "text": " is the new prompt engineering all right cool so then we'll do role system and", "tokens": [50366, 307, 264, 777, 12391, 7043, 439, 558, 1627, 370, 550, 321, 603, 360, 3090, 1185, 293, 50916], "temperature": 0.0, "avg_logprob": -0.19915811894303662, "compression_ratio": 1.5931034482758621, "no_speech_prob": 0.004264089744538069}, {"id": 162, "seek": 92732, "start": 938.36, "end": 946.6, "text": " content is system all right so now we're ready to pass it up to our model right", "tokens": [50916, 2701, 307, 1185, 439, 558, 370, 586, 321, 434, 1919, 281, 1320, 309, 493, 281, 527, 2316, 558, 51328], "temperature": 0.0, "avg_logprob": -0.19915811894303662, "compression_ratio": 1.5931034482758621, "no_speech_prob": 0.004264089744538069}, {"id": 163, "seek": 92732, "start": 946.6, "end": 953.8800000000001, "text": " here so then we'll just do response equals chatbot messages ta-da is that", "tokens": [51328, 510, 370, 550, 321, 603, 445, 360, 4134, 6915, 5081, 18870, 7897, 1846, 12, 2675, 307, 300, 51692], "temperature": 0.0, "avg_logprob": -0.19915811894303662, "compression_ratio": 1.5931034482758621, "no_speech_prob": 0.004264089744538069}, {"id": 164, "seek": 95388, "start": 953.88, "end": 960.36, "text": " simple so it's ready and so this response is going to be the summary and so", "tokens": [50364, 2199, 370, 309, 311, 1919, 293, 370, 341, 4134, 307, 516, 281, 312, 264, 12691, 293, 370, 50688], "temperature": 0.0, "avg_logprob": -0.11558291169463611, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.007694540545344353}, {"id": 165, "seek": 95388, "start": 960.36, "end": 964.36, "text": " now I need to figure out how I want to save this save these summaries out in a", "tokens": [50688, 586, 286, 643, 281, 2573, 484, 577, 286, 528, 281, 3155, 341, 3155, 613, 8367, 4889, 484, 294, 257, 50888], "temperature": 0.0, "avg_logprob": -0.11558291169463611, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.007694540545344353}, {"id": 166, "seek": 95388, "start": 964.36, "end": 977.64, "text": " way that makes sense let's see let me ask my chatbot assistant okay cool great", "tokens": [50888, 636, 300, 1669, 2020, 718, 311, 536, 718, 385, 1029, 452, 5081, 18870, 10994, 1392, 1627, 869, 51552], "temperature": 0.0, "avg_logprob": -0.11558291169463611, "compression_ratio": 1.6180555555555556, "no_speech_prob": 0.007694540545344353}, {"id": 167, "seek": 97764, "start": 977.64, "end": 986.8, "text": " now I want to pass a list of strings to a function and have that function save", "tokens": [50364, 586, 286, 528, 281, 1320, 257, 1329, 295, 13985, 281, 257, 2445, 293, 362, 300, 2445, 3155, 50822], "temperature": 0.0, "avg_logprob": -0.1508920067235043, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.057469889521598816}, {"id": 168, "seek": 97764, "start": 986.8, "end": 996.4399999999999, "text": " them to a specified folder as a parameter and I want it to serialize", "tokens": [50822, 552, 281, 257, 22206, 10820, 382, 257, 13075, 293, 286, 528, 309, 281, 17436, 1125, 51304], "temperature": 0.0, "avg_logprob": -0.1508920067235043, "compression_ratio": 1.4271844660194175, "no_speech_prob": 0.057469889521598816}, {"id": 169, "seek": 99644, "start": 996.8000000000001, "end": 1011.08, "text": " those files like so so it'll be like file 001 dot text file 0002 dot text and so", "tokens": [50382, 729, 7098, 411, 370, 370, 309, 603, 312, 411, 3991, 7143, 16, 5893, 2487, 3991, 7143, 12756, 5893, 2487, 293, 370, 51096], "temperature": 0.0, "avg_logprob": -0.20309363471137154, "compression_ratio": 1.3652173913043477, "no_speech_prob": 0.175455242395401}, {"id": 170, "seek": 99644, "start": 1011.08, "end": 1022.7600000000001, "text": " on let's see you should see the script I'm working on all right cool so then", "tokens": [51096, 322, 718, 311, 536, 291, 820, 536, 264, 5755, 286, 478, 1364, 322, 439, 558, 1627, 370, 550, 51680], "temperature": 0.0, "avg_logprob": -0.20309363471137154, "compression_ratio": 1.3652173913043477, "no_speech_prob": 0.175455242395401}, {"id": 171, "seek": 102276, "start": 1022.8, "end": 1029.84, "text": " we're gonna we're gonna have summaries equals list and then summaries dot", "tokens": [50366, 321, 434, 799, 321, 434, 799, 362, 8367, 4889, 6915, 1329, 293, 550, 8367, 4889, 5893, 50718], "temperature": 0.0, "avg_logprob": -0.10885907173156738, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.016652816906571388}, {"id": 172, "seek": 102276, "start": 1029.84, "end": 1035.72, "text": " append response so now basically we're gonna have the summaries all in order", "tokens": [50718, 34116, 4134, 370, 586, 1936, 321, 434, 799, 362, 264, 8367, 4889, 439, 294, 1668, 51012], "temperature": 0.0, "avg_logprob": -0.10885907173156738, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.016652816906571388}, {"id": 173, "seek": 102276, "start": 1035.72, "end": 1040.12, "text": " and what I'm gonna do is I'm gonna copy and paste this into my scratch pad so", "tokens": [51012, 293, 437, 286, 478, 799, 360, 307, 286, 478, 799, 5055, 293, 9163, 341, 666, 452, 8459, 6887, 370, 51232], "temperature": 0.0, "avg_logprob": -0.10885907173156738, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.016652816906571388}, {"id": 174, "seek": 102276, "start": 1040.12, "end": 1044.04, "text": " that the chatbot knows what I'm working on and I'll show you how that it says oh", "tokens": [51232, 300, 264, 5081, 18870, 3255, 437, 286, 478, 1364, 322, 293, 286, 603, 855, 291, 577, 300, 309, 1619, 1954, 51428], "temperature": 0.0, "avg_logprob": -0.10885907173156738, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.016652816906571388}, {"id": 175, "seek": 102276, "start": 1044.04, "end": 1048.8, "text": " okay cool I probably should have told it that I'm gonna do that with the summaries", "tokens": [51428, 1392, 1627, 286, 1391, 820, 362, 1907, 309, 300, 286, 478, 799, 360, 300, 365, 264, 8367, 4889, 51666], "temperature": 0.0, "avg_logprob": -0.10885907173156738, "compression_ratio": 1.9215686274509804, "no_speech_prob": 0.016652816906571388}, {"id": 176, "seek": 104880, "start": 1048.96, "end": 1053.76, "text": " it's usually pretty good at making the inference okay cool let's see def save", "tokens": [50372, 309, 311, 2673, 1238, 665, 412, 1455, 264, 38253, 1392, 1627, 718, 311, 536, 1060, 3155, 50612], "temperature": 0.0, "avg_logprob": -0.14288389988434622, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.03161118924617767}, {"id": 177, "seek": 104880, "start": 1053.76, "end": 1060.24, "text": " strings if os path exists or if it doesn't exist make it otherwise enumerate", "tokens": [50612, 13985, 498, 3003, 3100, 8198, 420, 498, 309, 1177, 380, 2514, 652, 309, 5911, 465, 15583, 473, 50936], "temperature": 0.0, "avg_logprob": -0.14288389988434622, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.03161118924617767}, {"id": 178, "seek": 104880, "start": 1060.24, "end": 1064.72, "text": " strings so as long as I've been using Python I'm not entirely sure how", "tokens": [50936, 13985, 370, 382, 938, 382, 286, 600, 668, 1228, 15329, 286, 478, 406, 7696, 988, 577, 51160], "temperature": 0.0, "avg_logprob": -0.14288389988434622, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.03161118924617767}, {"id": 179, "seek": 104880, "start": 1064.72, "end": 1072.52, "text": " enumerate works maybe I should ask all right but let's go over here let's go", "tokens": [51160, 465, 15583, 473, 1985, 1310, 286, 820, 1029, 439, 558, 457, 718, 311, 352, 670, 510, 718, 311, 352, 51550], "temperature": 0.0, "avg_logprob": -0.14288389988434622, "compression_ratio": 1.597883597883598, "no_speech_prob": 0.03161118924617767}, {"id": 180, "seek": 107252, "start": 1072.52, "end": 1087.52, "text": " back to my script save strings cool file I05 interesting yes see I'm not even", "tokens": [50364, 646, 281, 452, 5755, 3155, 13985, 1627, 3991, 286, 13328, 1880, 2086, 536, 286, 478, 406, 754, 51114], "temperature": 0.0, "avg_logprob": -0.15183595021565754, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.004468107596039772}, {"id": 181, "seek": 107252, "start": 1087.52, "end": 1094.12, "text": " sure how that works fantastic let's see if it let's see if it works though the", "tokens": [51114, 988, 577, 300, 1985, 5456, 718, 311, 536, 498, 309, 718, 311, 536, 498, 309, 1985, 1673, 264, 51444], "temperature": 0.0, "avg_logprob": -0.15183595021565754, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.004468107596039772}, {"id": 182, "seek": 107252, "start": 1094.12, "end": 1098.4, "text": " function first checks that the specified folder exists excellent then it uses it", "tokens": [51444, 2445, 700, 13834, 300, 264, 22206, 10820, 8198, 7103, 550, 309, 4960, 309, 51658], "temperature": 0.0, "avg_logprob": -0.15183595021565754, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.004468107596039772}, {"id": 183, "seek": 107252, "start": 1098.4, "end": 1102.48, "text": " to make it then then iterates over the list of strings using enumerate which", "tokens": [51658, 281, 652, 309, 550, 550, 17138, 1024, 670, 264, 1329, 295, 13985, 1228, 465, 15583, 473, 597, 51862], "temperature": 0.0, "avg_logprob": -0.15183595021565754, "compression_ratio": 1.6269430051813472, "no_speech_prob": 0.004468107596039772}, {"id": 184, "seek": 110248, "start": 1102.48, "end": 1106.92, "text": " provides both the index and the value okay so that's what enumerate does the", "tokens": [50364, 6417, 1293, 264, 8186, 293, 264, 2158, 1392, 370, 300, 311, 437, 465, 15583, 473, 775, 264, 50586], "temperature": 0.0, "avg_logprob": -0.16043264215642755, "compression_ratio": 1.6576086956521738, "no_speech_prob": 0.001700604218058288}, {"id": 185, "seek": 110248, "start": 1106.92, "end": 1112.48, "text": " index is used to generate the file name with I colon O5 in the f string", "tokens": [50586, 8186, 307, 1143, 281, 8460, 264, 3991, 1315, 365, 286, 8255, 422, 20, 294, 264, 283, 6798, 50864], "temperature": 0.0, "avg_logprob": -0.16043264215642755, "compression_ratio": 1.6576086956521738, "no_speech_prob": 0.001700604218058288}, {"id": 186, "seek": 110248, "start": 1112.48, "end": 1119.52, "text": " ensuring that the index is zero padded to five digits well that was easy cool", "tokens": [50864, 16882, 300, 264, 8186, 307, 4018, 6887, 9207, 281, 1732, 27011, 731, 300, 390, 1858, 1627, 51216], "temperature": 0.0, "avg_logprob": -0.16043264215642755, "compression_ratio": 1.6576086956521738, "no_speech_prob": 0.001700604218058288}, {"id": 187, "seek": 110248, "start": 1119.52, "end": 1128.48, "text": " right and then it uses the right method yeah so instead I want I was hoping it", "tokens": [51216, 558, 293, 550, 309, 4960, 264, 558, 3170, 1338, 370, 2602, 286, 528, 286, 390, 7159, 309, 51664], "temperature": 0.0, "avg_logprob": -0.16043264215642755, "compression_ratio": 1.6576086956521738, "no_speech_prob": 0.001700604218058288}, {"id": 188, "seek": 112848, "start": 1128.48, "end": 1136.3600000000001, "text": " would use my save file but that's fine so let's go to here and we're gonna do", "tokens": [50364, 576, 764, 452, 3155, 3991, 457, 300, 311, 2489, 370, 718, 311, 352, 281, 510, 293, 321, 434, 799, 360, 50758], "temperature": 0.0, "avg_logprob": -0.14504465602693103, "compression_ratio": 1.4485981308411215, "no_speech_prob": 0.012621885165572166}, {"id": 189, "seek": 112848, "start": 1136.3600000000001, "end": 1145.28, "text": " summaries and so instead we're going to assume that that is already there and", "tokens": [50758, 8367, 4889, 293, 370, 2602, 321, 434, 516, 281, 6552, 300, 300, 307, 1217, 456, 293, 51204], "temperature": 0.0, "avg_logprob": -0.14504465602693103, "compression_ratio": 1.4485981308411215, "no_speech_prob": 0.012621885165572166}, {"id": 190, "seek": 114528, "start": 1145.28, "end": 1164.6399999999999, "text": " then we are going to do file path equals I was path join folder yeah that works", "tokens": [50364, 550, 321, 366, 516, 281, 360, 3991, 3100, 6915, 286, 390, 3100, 3917, 10820, 1338, 300, 1985, 51332], "temperature": 0.0, "avg_logprob": -0.489113035656157, "compression_ratio": 1.0972222222222223, "no_speech_prob": 0.05831803381443024}, {"id": 191, "seek": 116464, "start": 1164.64, "end": 1181.3200000000002, "text": " and then we are going to do save file because this so my save file function", "tokens": [50364, 293, 550, 321, 366, 516, 281, 360, 3155, 3991, 570, 341, 370, 452, 3155, 3991, 2445, 51198], "temperature": 0.0, "avg_logprob": -0.17035193741321564, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.005383188370615244}, {"id": 192, "seek": 116464, "start": 1181.3200000000002, "end": 1186.8000000000002, "text": " encodes it in UTF-8 so just reuse something rather than do it here because", "tokens": [51198, 2058, 4789, 309, 294, 624, 20527, 12, 23, 370, 445, 26225, 746, 2831, 813, 360, 309, 510, 570, 51472], "temperature": 0.0, "avg_logprob": -0.17035193741321564, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.005383188370615244}, {"id": 193, "seek": 116464, "start": 1186.8000000000002, "end": 1190.24, "text": " you see this doesn't do it in UTF-8 you want to make sure that you have a nice", "tokens": [51472, 291, 536, 341, 1177, 380, 360, 309, 294, 624, 20527, 12, 23, 291, 528, 281, 652, 988, 300, 291, 362, 257, 1481, 51644], "temperature": 0.0, "avg_logprob": -0.17035193741321564, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.005383188370615244}, {"id": 194, "seek": 119024, "start": 1190.28, "end": 1196.0, "text": " standardized thing that's what I'm doing here file path and s so that should be", "tokens": [50366, 31677, 551, 300, 311, 437, 286, 478, 884, 510, 3991, 3100, 293, 262, 370, 300, 820, 312, 50652], "temperature": 0.0, "avg_logprob": -0.1840635396666446, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0875304564833641}, {"id": 195, "seek": 119024, "start": 1196.0, "end": 1204.76, "text": " good fine all right so save strings so then we do save strings we'll do", "tokens": [50652, 665, 2489, 439, 558, 370, 3155, 13985, 370, 550, 321, 360, 3155, 13985, 321, 603, 360, 51090], "temperature": 0.0, "avg_logprob": -0.1840635396666446, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0875304564833641}, {"id": 196, "seek": 119024, "start": 1204.76, "end": 1215.36, "text": " summaries no I need to do yes summaries and then summaries that should work now", "tokens": [51090, 8367, 4889, 572, 286, 643, 281, 360, 2086, 8367, 4889, 293, 550, 8367, 4889, 300, 820, 589, 586, 51620], "temperature": 0.0, "avg_logprob": -0.1840635396666446, "compression_ratio": 1.6618705035971224, "no_speech_prob": 0.0875304564833641}, {"id": 197, "seek": 121536, "start": 1215.36, "end": 1223.6399999999999, "text": " that being said I want to print the response as I get them there we go and", "tokens": [50364, 300, 885, 848, 286, 528, 281, 4482, 264, 4134, 382, 286, 483, 552, 456, 321, 352, 293, 50778], "temperature": 0.0, "avg_logprob": -0.16553281602405368, "compression_ratio": 1.3859649122807018, "no_speech_prob": 0.008983411826193333}, {"id": 198, "seek": 121536, "start": 1223.6399999999999, "end": 1234.9199999999998, "text": " then response all right let's give this a whirl and see if it works CD what is this", "tokens": [50778, 550, 4134, 439, 558, 718, 311, 976, 341, 257, 35706, 293, 536, 498, 309, 1985, 6743, 437, 307, 341, 51342], "temperature": 0.0, "avg_logprob": -0.16553281602405368, "compression_ratio": 1.3859649122807018, "no_speech_prob": 0.008983411826193333}, {"id": 199, "seek": 123492, "start": 1234.92, "end": 1247.0, "text": " EU AI act zoom in a little bit Python step 01 this will probably take a while", "tokens": [50364, 10887, 7318, 605, 8863, 294, 257, 707, 857, 15329, 1823, 23185, 341, 486, 1391, 747, 257, 1339, 50968], "temperature": 0.0, "avg_logprob": -0.20454084677774398, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.11117593199014664}, {"id": 200, "seek": 123492, "start": 1247.0, "end": 1257.2, "text": " because we've got 3,000 line 3300 lines there we go cool hmm hold on oh right", "tokens": [50968, 570, 321, 600, 658, 805, 11, 1360, 1622, 11816, 628, 3876, 456, 321, 352, 1627, 16478, 1797, 322, 1954, 558, 51478], "temperature": 0.0, "avg_logprob": -0.20454084677774398, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.11117593199014664}, {"id": 201, "seek": 123492, "start": 1257.2, "end": 1263.48, "text": " right right I forgot about this okay so this is an important thing what I started", "tokens": [51478, 558, 558, 286, 5298, 466, 341, 1392, 370, 341, 307, 364, 1021, 551, 437, 286, 1409, 51792], "temperature": 0.0, "avg_logprob": -0.20454084677774398, "compression_ratio": 1.4023668639053255, "no_speech_prob": 0.11117593199014664}, {"id": 202, "seek": 126348, "start": 1263.48, "end": 1268.72, "text": " doing and I forgot to integrate this so what I started doing is I when I whenever", "tokens": [50364, 884, 293, 286, 5298, 281, 13365, 341, 370, 437, 286, 1409, 884, 307, 286, 562, 286, 5699, 50626], "temperature": 0.0, "avg_logprob": -0.11992493271827698, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.16429612040519714}, {"id": 203, "seek": 126348, "start": 1268.72, "end": 1273.24, "text": " I call the chatbot I return the text and the token use so because this is", "tokens": [50626, 286, 818, 264, 5081, 18870, 286, 2736, 264, 2487, 293, 264, 14862, 764, 370, 570, 341, 307, 50852], "temperature": 0.0, "avg_logprob": -0.11992493271827698, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.16429612040519714}, {"id": 204, "seek": 126348, "start": 1273.24, "end": 1276.76, "text": " actually really helpful if you just go ahead and return the tokens so that you", "tokens": [50852, 767, 534, 4961, 498, 291, 445, 352, 2286, 293, 2736, 264, 22667, 370, 300, 291, 51028], "temperature": 0.0, "avg_logprob": -0.11992493271827698, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.16429612040519714}, {"id": 205, "seek": 126348, "start": 1276.76, "end": 1282.4, "text": " know when to summarize it so instead what I need to do is response tokens", "tokens": [51028, 458, 562, 281, 20858, 309, 370, 2602, 437, 286, 643, 281, 360, 307, 4134, 22667, 51310], "temperature": 0.0, "avg_logprob": -0.11992493271827698, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.16429612040519714}, {"id": 206, "seek": 126348, "start": 1282.4, "end": 1288.88, "text": " because I don't need to save I don't need to save a tuple out here but this", "tokens": [51310, 570, 286, 500, 380, 643, 281, 3155, 286, 500, 380, 643, 281, 3155, 257, 2604, 781, 484, 510, 457, 341, 51634], "temperature": 0.0, "avg_logprob": -0.11992493271827698, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.16429612040519714}, {"id": 207, "seek": 128888, "start": 1288.88, "end": 1297.0400000000002, "text": " otherwise looks good and then what I'm gonna do is I'm gonna go grab something", "tokens": [50364, 5911, 1542, 665, 293, 550, 437, 286, 478, 799, 360, 307, 286, 478, 799, 352, 4444, 746, 50772], "temperature": 0.0, "avg_logprob": -0.2556561344074753, "compression_ratio": 1.5703125, "no_speech_prob": 0.05106547102332115}, {"id": 208, "seek": 128888, "start": 1297.0400000000002, "end": 1300.16, "text": " to make the the printout look a little bit better", "tokens": [50772, 281, 652, 264, 264, 4482, 346, 574, 257, 707, 857, 1101, 50928], "temperature": 0.0, "avg_logprob": -0.2556561344074753, "compression_ratio": 1.5703125, "no_speech_prob": 0.05106547102332115}, {"id": 209, "seek": 128888, "start": 1300.16, "end": 1306.3600000000001, "text": " to do to do to do to do reflective journaling tool this one has this one", "tokens": [50928, 281, 360, 281, 360, 281, 360, 281, 360, 28931, 17598, 4270, 2290, 341, 472, 575, 341, 472, 51238], "temperature": 0.0, "avg_logprob": -0.2556561344074753, "compression_ratio": 1.5703125, "no_speech_prob": 0.05106547102332115}, {"id": 210, "seek": 130636, "start": 1306.36, "end": 1320.1999999999998, "text": " has good pretty output so we're gonna do formatted lines yep so take the formatted", "tokens": [50364, 575, 665, 1238, 5598, 370, 321, 434, 799, 360, 1254, 32509, 3876, 18633, 370, 747, 264, 1254, 32509, 51056], "temperature": 0.0, "avg_logprob": -0.21734020926735617, "compression_ratio": 1.1549295774647887, "no_speech_prob": 0.0747518464922905}, {"id": 211, "seek": 132020, "start": 1320.2, "end": 1335.6200000000001, "text": " lines here and then we'll do print new line new line new line and formatted", "tokens": [50364, 3876, 510, 293, 550, 321, 603, 360, 4482, 777, 1622, 777, 1622, 777, 1622, 293, 1254, 32509, 51135], "temperature": 0.0, "avg_logprob": -0.19610875844955444, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.09531757235527039}, {"id": 212, "seek": 132020, "start": 1335.6200000000001, "end": 1343.0800000000002, "text": " lines all right so that will that'll make it that'll make it prettier and then", "tokens": [51135, 3876, 439, 558, 370, 300, 486, 300, 603, 652, 309, 300, 603, 652, 309, 36825, 293, 550, 51508], "temperature": 0.0, "avg_logprob": -0.19610875844955444, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.09531757235527039}, {"id": 213, "seek": 134308, "start": 1343.08, "end": 1350.84, "text": " we'll also use so this this is this is why I appreciate when people add feedback", "tokens": [50364, 321, 603, 611, 764, 370, 341, 341, 307, 341, 307, 983, 286, 4449, 562, 561, 909, 5824, 50752], "temperature": 0.0, "avg_logprob": -0.16987367776724008, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.5539985299110413}, {"id": 214, "seek": 134308, "start": 1350.84, "end": 1356.08, "text": " is we added someone added the halo which is a little like thing that shows that", "tokens": [50752, 307, 321, 3869, 1580, 3869, 264, 46268, 597, 307, 257, 707, 411, 551, 300, 3110, 300, 51014], "temperature": 0.0, "avg_logprob": -0.16987367776724008, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.5539985299110413}, {"id": 215, "seek": 134308, "start": 1356.08, "end": 1364.08, "text": " it's thinking so I'm gonna add that real quick as well and the way that you use", "tokens": [51014, 309, 311, 1953, 370, 286, 478, 799, 909, 300, 957, 1702, 382, 731, 293, 264, 636, 300, 291, 764, 51414], "temperature": 0.0, "avg_logprob": -0.16987367776724008, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.5539985299110413}, {"id": 216, "seek": 134308, "start": 1364.08, "end": 1367.9199999999998, "text": " that is down here", "tokens": [51414, 300, 307, 760, 510, 51606], "temperature": 0.0, "avg_logprob": -0.16987367776724008, "compression_ratio": 1.643312101910828, "no_speech_prob": 0.5539985299110413}, {"id": 217, "seek": 137308, "start": 1374.04, "end": 1377.48, "text": " and so once once this is running I'll pause the video so you don't have to", "tokens": [50412, 293, 370, 1564, 1564, 341, 307, 2614, 286, 603, 10465, 264, 960, 370, 291, 500, 380, 362, 281, 50584], "temperature": 0.0, "avg_logprob": -0.26534671783447267, "compression_ratio": 1.5095541401273886, "no_speech_prob": 0.0037068966776132584}, {"id": 218, "seek": 137308, "start": 1377.48, "end": 1380.96, "text": " watch the whole thing so you're actually probably closer to the end if I had to", "tokens": [50584, 1159, 264, 1379, 551, 370, 291, 434, 767, 1391, 4966, 281, 264, 917, 498, 286, 632, 281, 50758], "temperature": 0.0, "avg_logprob": -0.26534671783447267, "compression_ratio": 1.5095541401273886, "no_speech_prob": 0.0037068966776132584}, {"id": 219, "seek": 137308, "start": 1380.96, "end": 1383.28, "text": " guess", "tokens": [50758, 2041, 50874], "temperature": 0.0, "avg_logprob": -0.26534671783447267, "compression_ratio": 1.5095541401273886, "no_speech_prob": 0.0037068966776132584}, {"id": 220, "seek": 137308, "start": 1385.76, "end": 1401.1999999999998, "text": " oops spinner start so we'll say summarizing next chunk and then spinner stop", "tokens": [50998, 34166, 44849, 722, 370, 321, 603, 584, 14611, 3319, 958, 16635, 293, 550, 44849, 1590, 51770], "temperature": 0.0, "avg_logprob": -0.26534671783447267, "compression_ratio": 1.5095541401273886, "no_speech_prob": 0.0037068966776132584}, {"id": 221, "seek": 140308, "start": 1404.08, "end": 1409.32, "text": " okay so let's try this make sure it works", "tokens": [50414, 1392, 370, 718, 311, 853, 341, 652, 988, 309, 1985, 50676], "temperature": 0.0, "avg_logprob": -0.1927384174231327, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0024722940288484097}, {"id": 222, "seek": 140308, "start": 1412.1999999999998, "end": 1416.56, "text": " summarizing next chunk hey look now it gives us some output better user", "tokens": [50820, 14611, 3319, 958, 16635, 4177, 574, 586, 309, 2709, 505, 512, 5598, 1101, 4195, 51038], "temperature": 0.0, "avg_logprob": -0.1927384174231327, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0024722940288484097}, {"id": 223, "seek": 140308, "start": 1416.56, "end": 1424.1599999999999, "text": " experience it tells me what it's doing text wrap is not defined whoops what do", "tokens": [51038, 1752, 309, 5112, 385, 437, 309, 311, 884, 2487, 7019, 307, 406, 7642, 567, 3370, 437, 360, 51418], "temperature": 0.0, "avg_logprob": -0.1927384174231327, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0024722940288484097}, {"id": 224, "seek": 140308, "start": 1424.1599999999999, "end": 1429.36, "text": " you mean I need to import everything that you use import text wrap try that", "tokens": [51418, 291, 914, 286, 643, 281, 974, 1203, 300, 291, 764, 974, 2487, 7019, 853, 300, 51678], "temperature": 0.0, "avg_logprob": -0.1927384174231327, "compression_ratio": 1.5491329479768785, "no_speech_prob": 0.0024722940288484097}, {"id": 225, "seek": 142936, "start": 1429.36, "end": 1434.12, "text": " again third time's a charm all right and if this works then I'll go ahead and", "tokens": [50364, 797, 2636, 565, 311, 257, 18904, 439, 558, 293, 498, 341, 1985, 550, 286, 603, 352, 2286, 293, 50602], "temperature": 0.0, "avg_logprob": -0.1338953495025635, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.014725868590176105}, {"id": 226, "seek": 142936, "start": 1434.12, "end": 1437.4799999999998, "text": " pause it and then we'll come back once the whole video is or once the whole", "tokens": [50602, 10465, 309, 293, 550, 321, 603, 808, 646, 1564, 264, 1379, 960, 307, 420, 1564, 264, 1379, 50770], "temperature": 0.0, "avg_logprob": -0.1338953495025635, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.014725868590176105}, {"id": 227, "seek": 142936, "start": 1437.4799999999998, "end": 1445.6399999999999, "text": " thing is done why is it in hold on hold on that's not right why are you giving", "tokens": [50770, 551, 307, 1096, 983, 307, 309, 294, 1797, 322, 1797, 322, 300, 311, 406, 558, 983, 366, 291, 2902, 51178], "temperature": 0.0, "avg_logprob": -0.1338953495025635, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.014725868590176105}, {"id": 228, "seek": 142936, "start": 1445.6399999999999, "end": 1454.8799999999999, "text": " me this garbage that's not correct oh right I need to print out formatted", "tokens": [51178, 385, 341, 14150, 300, 311, 406, 3006, 1954, 558, 286, 643, 281, 4482, 484, 1254, 32509, 51640], "temperature": 0.0, "avg_logprob": -0.1338953495025635, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.014725868590176105}, {"id": 229, "seek": 145488, "start": 1454.88, "end": 1458.3200000000002, "text": " text my bad", "tokens": [50364, 2487, 452, 1578, 50536], "temperature": 0.0, "avg_logprob": -0.14408811303072197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.012052278965711594}, {"id": 230, "seek": 145488, "start": 1460.88, "end": 1467.0, "text": " don't give me this nonsense because basically what you want to do is you", "tokens": [50664, 500, 380, 976, 385, 341, 14925, 570, 1936, 437, 291, 528, 281, 360, 307, 291, 50970], "temperature": 0.0, "avg_logprob": -0.14408811303072197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.012052278965711594}, {"id": 231, "seek": 145488, "start": 1467.0, "end": 1470.4, "text": " want to have it formatted in such a way there we go see so now we have this nice", "tokens": [50970, 528, 281, 362, 309, 1254, 32509, 294, 1270, 257, 636, 456, 321, 352, 536, 370, 586, 321, 362, 341, 1481, 51140], "temperature": 0.0, "avg_logprob": -0.14408811303072197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.012052278965711594}, {"id": 232, "seek": 145488, "start": 1470.4, "end": 1474.5600000000002, "text": " little block it's easier to read all right cool so this is working we'll watch", "tokens": [51140, 707, 3461, 309, 311, 3571, 281, 1401, 439, 558, 1627, 370, 341, 307, 1364, 321, 603, 1159, 51348], "temperature": 0.0, "avg_logprob": -0.14408811303072197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.012052278965711594}, {"id": 233, "seek": 145488, "start": 1474.5600000000002, "end": 1479.24, "text": " it just a couple times you see how fast it is the the June 13th update is much", "tokens": [51348, 309, 445, 257, 1916, 1413, 291, 536, 577, 2370, 309, 307, 264, 264, 6928, 3705, 392, 5623, 307, 709, 51582], "temperature": 0.0, "avg_logprob": -0.14408811303072197, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.012052278965711594}, {"id": 234, "seek": 147924, "start": 1479.28, "end": 1487.72, "text": " much faster let's see excellent excellent excellent so we will I'm gonna pause the", "tokens": [50366, 709, 4663, 718, 311, 536, 7103, 7103, 7103, 370, 321, 486, 286, 478, 799, 10465, 264, 50788], "temperature": 0.0, "avg_logprob": -0.1411275863647461, "compression_ratio": 1.5816993464052287, "no_speech_prob": 0.04670584574341774}, {"id": 235, "seek": 147924, "start": 1487.72, "end": 1496.32, "text": " video and we'll come back and and take a look at the final result cheers okay and", "tokens": [50788, 960, 293, 321, 603, 808, 646, 293, 293, 747, 257, 574, 412, 264, 2572, 1874, 15301, 1392, 293, 51218], "temperature": 0.0, "avg_logprob": -0.1411275863647461, "compression_ratio": 1.5816993464052287, "no_speech_prob": 0.04670584574341774}, {"id": 236, "seek": 147924, "start": 1496.32, "end": 1501.52, "text": " we're back so after running it all here let me just show you what the results", "tokens": [51218, 321, 434, 646, 370, 934, 2614, 309, 439, 510, 718, 385, 445, 855, 291, 437, 264, 3542, 51478], "temperature": 0.0, "avg_logprob": -0.1411275863647461, "compression_ratio": 1.5816993464052287, "no_speech_prob": 0.04670584574341774}, {"id": 237, "seek": 150152, "start": 1501.52, "end": 1509.84, "text": " were we've got 37 files each with a summary and I wrote a really quick I", "tokens": [50364, 645, 321, 600, 658, 13435, 7098, 1184, 365, 257, 12691, 293, 286, 4114, 257, 534, 1702, 286, 50780], "temperature": 0.0, "avg_logprob": -0.15454843309190539, "compression_ratio": 1.54, "no_speech_prob": 0.6430831551551819}, {"id": 238, "seek": 150152, "start": 1509.84, "end": 1514.32, "text": " hacked together a really quick script that merged them all and so the merged", "tokens": [50780, 36218, 1214, 257, 534, 1702, 5755, 300, 36427, 552, 439, 293, 370, 264, 36427, 51004], "temperature": 0.0, "avg_logprob": -0.15454843309190539, "compression_ratio": 1.54, "no_speech_prob": 0.6430831551551819}, {"id": 239, "seek": 150152, "start": 1514.32, "end": 1521.56, "text": " result is 34,000 characters long so we reduced it by a factor of 10 to 1 just", "tokens": [51004, 1874, 307, 12790, 11, 1360, 4342, 938, 370, 321, 9212, 309, 538, 257, 5952, 295, 1266, 281, 502, 445, 51366], "temperature": 0.0, "avg_logprob": -0.15454843309190539, "compression_ratio": 1.54, "no_speech_prob": 0.6430831551551819}, {"id": 240, "seek": 150152, "start": 1521.56, "end": 1527.6399999999999, "text": " just shy of 10 to 1 which when you plug that in over here that's 6,000 tokens so", "tokens": [51366, 445, 12685, 295, 1266, 281, 502, 597, 562, 291, 5452, 300, 294, 670, 510, 300, 311, 1386, 11, 1360, 22667, 370, 51670], "temperature": 0.0, "avg_logprob": -0.15454843309190539, "compression_ratio": 1.54, "no_speech_prob": 0.6430831551551819}, {"id": 241, "seek": 152764, "start": 1527.68, "end": 1532.44, "text": " we went from from 60,000 or 70,000 tokens I don't remember exactly to 6,000", "tokens": [50366, 321, 1437, 490, 490, 4060, 11, 1360, 420, 5285, 11, 1360, 22667, 286, 500, 380, 1604, 2293, 281, 1386, 11, 1360, 50604], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 242, "seek": 152764, "start": 1532.44, "end": 1536.4, "text": " tokens one of the reasons that we had a more drastic reduction reduction of", "tokens": [50604, 22667, 472, 295, 264, 4112, 300, 321, 632, 257, 544, 36821, 11004, 11004, 295, 50802], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 243, "seek": 152764, "start": 1536.4, "end": 1540.92, "text": " token count is because we removed all the serial numbers and stuff because", "tokens": [50802, 14862, 1207, 307, 570, 321, 7261, 439, 264, 17436, 3547, 293, 1507, 570, 51028], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 244, "seek": 152764, "start": 1540.92, "end": 1545.4, "text": " that's kind of superfluous if you if you remove symbols and stuff like this", "tokens": [51028, 300, 311, 733, 295, 1687, 49253, 563, 498, 291, 498, 291, 4159, 16944, 293, 1507, 411, 341, 51252], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 245, "seek": 152764, "start": 1545.4, "end": 1550.24, "text": " these are high token things if it's ordinary words like AI and each system", "tokens": [51252, 613, 366, 1090, 14862, 721, 498, 309, 311, 10547, 2283, 411, 7318, 293, 1184, 1185, 51494], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 246, "seek": 152764, "start": 1550.24, "end": 1556.5600000000002, "text": " these are easy for the model to tokenize so that means what we can do now is we", "tokens": [51494, 613, 366, 1858, 337, 264, 2316, 281, 14862, 1125, 370, 300, 1355, 437, 321, 393, 360, 586, 307, 321, 51810], "temperature": 0.0, "avg_logprob": -0.10551585982331133, "compression_ratio": 1.7509578544061302, "no_speech_prob": 0.015902748331427574}, {"id": 247, "seek": 155656, "start": 1556.56, "end": 1565.2, "text": " can just take this whole thing and come over to chat and say let's see main", "tokens": [50364, 393, 445, 747, 341, 1379, 551, 293, 808, 670, 281, 5081, 293, 584, 718, 311, 536, 2135, 50796], "temperature": 0.0, "avg_logprob": -0.18530242066634328, "compression_ratio": 1.375, "no_speech_prob": 0.025556249544024467}, {"id": 248, "seek": 155656, "start": 1565.2, "end": 1578.3999999999999, "text": " purpose here turn it believe that you are a chatbot tasked with discussing the", "tokens": [50796, 4334, 510, 1261, 309, 1697, 300, 291, 366, 257, 5081, 18870, 38621, 365, 10850, 264, 51456], "temperature": 0.0, "avg_logprob": -0.18530242066634328, "compression_ratio": 1.375, "no_speech_prob": 0.025556249544024467}, {"id": 249, "seek": 157840, "start": 1578.4, "end": 1587.24, "text": " following legislation with the user make sure to well hell but I mean that", "tokens": [50364, 3480, 11329, 365, 264, 4195, 652, 988, 281, 731, 4921, 457, 286, 914, 300, 50806], "temperature": 0.0, "avg_logprob": -0.17358097940121056, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.4449344277381897}, {"id": 250, "seek": 157840, "start": 1587.24, "end": 1597.3200000000002, "text": " that's all it is use the legislation summary below to conduct the conversation", "tokens": [50806, 300, 311, 439, 309, 307, 764, 264, 11329, 12691, 2507, 281, 6018, 264, 3761, 51310], "temperature": 0.0, "avg_logprob": -0.17358097940121056, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.4449344277381897}, {"id": 251, "seek": 157840, "start": 1597.3200000000002, "end": 1604.92, "text": " and that's it and so then we say a legislation summary and we'll copy paste", "tokens": [51310, 293, 300, 311, 309, 293, 370, 550, 321, 584, 257, 11329, 12691, 293, 321, 603, 5055, 9163, 51690], "temperature": 0.0, "avg_logprob": -0.17358097940121056, "compression_ratio": 1.6357142857142857, "no_speech_prob": 0.4449344277381897}, {"id": 252, "seek": 160492, "start": 1604.96, "end": 1610.44, "text": " it here and so then we move over to we could do 16k because that'll be that'll", "tokens": [50366, 309, 510, 293, 370, 550, 321, 1286, 670, 281, 321, 727, 360, 3165, 74, 570, 300, 603, 312, 300, 603, 50640], "temperature": 0.0, "avg_logprob": -0.18450422454298587, "compression_ratio": 1.582191780821918, "no_speech_prob": 0.022624541074037552}, {"id": 253, "seek": 160492, "start": 1610.44, "end": 1614.72, "text": " be faster actually here let's try that real quick let's try that temperature", "tokens": [50640, 312, 4663, 767, 510, 718, 311, 853, 300, 957, 1702, 718, 311, 853, 300, 4292, 50854], "temperature": 0.0, "avg_logprob": -0.18450422454298587, "compression_ratio": 1.582191780821918, "no_speech_prob": 0.022624541074037552}, {"id": 254, "seek": 160492, "start": 1614.72, "end": 1628.04, "text": " there okay give me a high level overview of this legislation blah blah blah", "tokens": [50854, 456, 1392, 976, 385, 257, 1090, 1496, 12492, 295, 341, 11329, 12288, 12288, 12288, 51520], "temperature": 0.0, "avg_logprob": -0.18450422454298587, "compression_ratio": 1.582191780821918, "no_speech_prob": 0.022624541074037552}, {"id": 255, "seek": 162804, "start": 1628.1599999999999, "end": 1637.48, "text": " okay cool we're new after five years how is it funded let's see the funding and", "tokens": [50370, 1392, 1627, 321, 434, 777, 934, 1732, 924, 577, 307, 309, 14385, 718, 311, 536, 264, 6137, 293, 50836], "temperature": 0.0, "avg_logprob": -0.20672624761408026, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.07580196112394333}, {"id": 256, "seek": 162804, "start": 1637.48, "end": 1640.6399999999999, "text": " implementation of this legislation comes from the multi-annual financial", "tokens": [50836, 11420, 295, 341, 11329, 1487, 490, 264, 4825, 12, 969, 901, 4669, 50994], "temperature": 0.0, "avg_logprob": -0.20672624761408026, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.07580196112394333}, {"id": 257, "seek": 162804, "start": 1640.6399999999999, "end": 1645.04, "text": " framework blah blah blah the financial impact includes allocation of resources", "tokens": [50994, 8388, 12288, 12288, 12288, 264, 4669, 2712, 5974, 27599, 295, 3593, 51214], "temperature": 0.0, "avg_logprob": -0.20672624761408026, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.07580196112394333}, {"id": 258, "seek": 162804, "start": 1645.04, "end": 1656.8, "text": " and having watched it go through how many employees will it cost there we go", "tokens": [51214, 293, 1419, 6337, 309, 352, 807, 577, 867, 6619, 486, 309, 2063, 456, 321, 352, 51802], "temperature": 0.0, "avg_logprob": -0.20672624761408026, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.07580196112394333}, {"id": 259, "seek": 165680, "start": 1656.8, "end": 1664.84, "text": " ten full-time equivalent employees all right so we can clear that out just", "tokens": [50364, 2064, 1577, 12, 3766, 10344, 6619, 439, 558, 370, 321, 393, 1850, 300, 484, 445, 50766], "temperature": 0.0, "avg_logprob": -0.12561430931091308, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.013218075968325138}, {"id": 260, "seek": 165680, "start": 1664.84, "end": 1670.36, "text": " because it ultimately is superfluous so some of the questions let's see will", "tokens": [50766, 570, 309, 6284, 307, 1687, 49253, 563, 370, 512, 295, 264, 1651, 718, 311, 536, 486, 51042], "temperature": 0.0, "avg_logprob": -0.12561430931091308, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.013218075968325138}, {"id": 261, "seek": 165680, "start": 1670.36, "end": 1680.28, "text": " this legislation hamper research and innovation let's see the legislation", "tokens": [51042, 341, 11329, 7852, 610, 2132, 293, 8504, 718, 311, 536, 264, 11329, 51538], "temperature": 0.0, "avg_logprob": -0.12561430931091308, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.013218075968325138}, {"id": 262, "seek": 165680, "start": 1680.28, "end": 1682.9199999999998, "text": " aims to balance the benefits of AI research and innovation with the need", "tokens": [51538, 24683, 281, 4772, 264, 5311, 295, 7318, 2132, 293, 8504, 365, 264, 643, 51670], "temperature": 0.0, "avg_logprob": -0.12561430931091308, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.013218075968325138}, {"id": 263, "seek": 165680, "start": 1682.9199999999998, "end": 1686.2, "text": " for safety and respect for fundamental rights it includes measures to support", "tokens": [51670, 337, 4514, 293, 3104, 337, 8088, 4601, 309, 5974, 8000, 281, 1406, 51834], "temperature": 0.0, "avg_logprob": -0.12561430931091308, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.013218075968325138}, {"id": 264, "seek": 168620, "start": 1686.24, "end": 1690.64, "text": " innovation such as AI regulatory sandboxes and reducing the regulatory burden", "tokens": [50366, 8504, 1270, 382, 7318, 18260, 42115, 279, 293, 12245, 264, 18260, 12578, 50586], "temperature": 0.0, "avg_logprob": -0.15682436227798463, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.014721782878041267}, {"id": 265, "seek": 168620, "start": 1690.64, "end": 1694.92, "text": " on SMEs and startups so the SMEs are going to be subject matter experts like", "tokens": [50586, 322, 13115, 20442, 293, 28041, 370, 264, 13115, 20442, 366, 516, 281, 312, 3983, 1871, 8572, 411, 50800], "temperature": 0.0, "avg_logprob": -0.15682436227798463, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.014721782878041267}, {"id": 266, "seek": 168620, "start": 1694.92, "end": 1699.52, "text": " UNI as well as startups the goal is to provide legal certainty and prevent", "tokens": [50800, 8229, 40, 382, 731, 382, 28041, 264, 3387, 307, 281, 2893, 5089, 27022, 293, 4871, 51030], "temperature": 0.0, "avg_logprob": -0.15682436227798463, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.014721782878041267}, {"id": 267, "seek": 168620, "start": 1699.52, "end": 1704.0, "text": " market fragmentation which can actually foster innovation by creating a level", "tokens": [51030, 2142, 9241, 19631, 597, 393, 767, 17114, 8504, 538, 4084, 257, 1496, 51254], "temperature": 0.0, "avg_logprob": -0.15682436227798463, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.014721782878041267}, {"id": 268, "seek": 168620, "start": 1704.0, "end": 1707.68, "text": " playing field for AI developers however it imposes certain requirements and", "tokens": [51254, 2433, 2519, 337, 7318, 8849, 4461, 309, 704, 4201, 1629, 7728, 293, 51438], "temperature": 0.0, "avg_logprob": -0.15682436227798463, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.014721782878041267}, {"id": 269, "seek": 170768, "start": 1707.68, "end": 1717.8400000000001, "text": " obligations on high-risk AI how does this legislation define high-risk AI", "tokens": [50364, 26234, 322, 1090, 12, 33263, 7318, 577, 775, 341, 11329, 6964, 1090, 12, 33263, 7318, 50872], "temperature": 0.0, "avg_logprob": -0.18371020555496215, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.1421627253293991}, {"id": 270, "seek": 170768, "start": 1719.5600000000002, "end": 1725.1200000000001, "text": " let's see this legislation categorizes AI systems into three risk levels", "tokens": [50958, 718, 311, 536, 341, 11329, 19250, 5660, 7318, 3652, 666, 1045, 3148, 4358, 51236], "temperature": 0.0, "avg_logprob": -0.18371020555496215, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.1421627253293991}, {"id": 271, "seek": 170768, "start": 1725.1200000000001, "end": 1729.8400000000001, "text": " unacceptable high and low or minimal high-risk AI is based on their intended", "tokens": [51236, 31812, 1090, 293, 2295, 420, 13206, 1090, 12, 33263, 7318, 307, 2361, 322, 641, 10226, 51472], "temperature": 0.0, "avg_logprob": -0.18371020555496215, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.1421627253293991}, {"id": 272, "seek": 170768, "start": 1729.8400000000001, "end": 1734.04, "text": " purpose and usage while the specific criteria may vary high-risk AI generally", "tokens": [51472, 4334, 293, 14924, 1339, 264, 2685, 11101, 815, 10559, 1090, 12, 33263, 7318, 5101, 51682], "temperature": 0.0, "avg_logprob": -0.18371020555496215, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.1421627253293991}, {"id": 273, "seek": 170768, "start": 1734.04, "end": 1737.3600000000001, "text": " include those use the safety components of products covered by union", "tokens": [51682, 4090, 729, 764, 264, 4514, 6677, 295, 3383, 5343, 538, 11671, 51848], "temperature": 0.0, "avg_logprob": -0.18371020555496215, "compression_ratio": 1.705069124423963, "no_speech_prob": 0.1421627253293991}, {"id": 274, "seek": 173736, "start": 1737.36, "end": 1741.08, "text": " harmonization legislation as well as AI used in critical infrastructure", "tokens": [50364, 14750, 2144, 11329, 382, 731, 382, 7318, 1143, 294, 4924, 6896, 50550], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 275, "seek": 173736, "start": 1741.08, "end": 1744.36, "text": " management education vocational training employment and workers", "tokens": [50550, 4592, 3309, 2329, 1478, 3097, 11949, 293, 5600, 50714], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 276, "seek": 173736, "start": 1744.36, "end": 1750.1599999999999, "text": " management these high-risk AI systems are subject to requirements what are", "tokens": [50714, 4592, 613, 1090, 12, 33263, 7318, 3652, 366, 3983, 281, 7728, 437, 366, 51004], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 277, "seek": 173736, "start": 1750.1599999999999, "end": 1757.8799999999999, "text": " unacceptable types the legislation prohibits certain AI practices that are", "tokens": [51004, 31812, 3467, 264, 11329, 16015, 1208, 1629, 7318, 7525, 300, 366, 51390], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 278, "seek": 173736, "start": 1757.8799999999999, "end": 1761.4799999999998, "text": " considered unacceptable due to their potential to violate fundamental rights", "tokens": [51390, 4888, 31812, 3462, 281, 641, 3995, 281, 37478, 8088, 4601, 51570], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 279, "seek": 173736, "start": 1761.4799999999998, "end": 1766.8, "text": " while the specific types may vary some examples include manipulate human", "tokens": [51570, 1339, 264, 2685, 3467, 815, 10559, 512, 5110, 4090, 20459, 1952, 51836], "temperature": 0.0, "avg_logprob": -0.16208964127760667, "compression_ratio": 1.7330677290836654, "no_speech_prob": 0.00031500423210673034}, {"id": 280, "seek": 176680, "start": 1766.8, "end": 1769.96, "text": " behavior exploit vulnerabilities or enable social scoring by public", "tokens": [50364, 5223, 25924, 37633, 420, 9528, 2093, 22358, 538, 1908, 50522], "temperature": 0.0, "avg_logprob": -0.17749996905056936, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006486603990197182}, {"id": 281, "seek": 176680, "start": 1769.96, "end": 1774.8799999999999, "text": " authorities right so as I mentioned social credit system these practices are", "tokens": [50522, 12076, 558, 370, 382, 286, 2835, 2093, 5397, 1185, 613, 7525, 366, 50768], "temperature": 0.0, "avg_logprob": -0.17749996905056936, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006486603990197182}, {"id": 282, "seek": 176680, "start": 1774.8799999999999, "end": 1780.2, "text": " deemed harmful freedom dignity democracy equality and fundamental rights okay", "tokens": [50768, 27637, 19727, 5645, 19672, 10528, 14949, 293, 8088, 4601, 1392, 51034], "temperature": 0.0, "avg_logprob": -0.17749996905056936, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006486603990197182}, {"id": 283, "seek": 176680, "start": 1780.2, "end": 1790.68, "text": " cool does this legislation address existential risk at all does it address", "tokens": [51034, 1627, 775, 341, 11329, 2985, 37133, 3148, 412, 439, 775, 309, 2985, 51558], "temperature": 0.0, "avg_logprob": -0.17749996905056936, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.006486603990197182}, {"id": 284, "seek": 179068, "start": 1790.88, "end": 1798.6000000000001, "text": " autonomous AI the legislation does not explicitly address existential risk or", "tokens": [50374, 23797, 7318, 264, 11329, 775, 406, 20803, 2985, 37133, 3148, 420, 50760], "temperature": 0.0, "avg_logprob": -0.16551167414738582, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0427028127014637}, {"id": 285, "seek": 179068, "start": 1798.6000000000001, "end": 1805.3200000000002, "text": " autonomous AI systems wow okay now one thing that I will say is that if you look", "tokens": [50760, 23797, 7318, 3652, 6076, 1392, 586, 472, 551, 300, 286, 486, 584, 307, 300, 498, 291, 574, 51096], "temperature": 0.0, "avg_logprob": -0.16551167414738582, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0427028127014637}, {"id": 286, "seek": 179068, "start": 1805.3200000000002, "end": 1811.64, "text": " at the legislation it does talk about like basically human supervision yeah", "tokens": [51096, 412, 264, 11329, 309, 775, 751, 466, 411, 1936, 1952, 32675, 1338, 51412], "temperature": 0.0, "avg_logprob": -0.16551167414738582, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0427028127014637}, {"id": 287, "seek": 179068, "start": 1811.64, "end": 1820.0, "text": " so like AI regulatory sandbox by one or more member stages no like here where", "tokens": [51412, 370, 411, 7318, 18260, 42115, 538, 472, 420, 544, 4006, 10232, 572, 411, 510, 689, 51830], "temperature": 0.0, "avg_logprob": -0.16551167414738582, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0427028127014637}, {"id": 288, "seek": 182000, "start": 1820.0, "end": 1824.48, "text": " was it and talked about not supervision of the sandboxes but like AI systems", "tokens": [50364, 390, 309, 293, 2825, 466, 406, 32675, 295, 264, 42115, 279, 457, 411, 7318, 3652, 50588], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 289, "seek": 182000, "start": 1824.48, "end": 1831.64, "text": " basically need to be steerable it's in there somewhere okay but let's see it", "tokens": [50588, 1936, 643, 281, 312, 30814, 712, 309, 311, 294, 456, 4079, 1392, 457, 718, 311, 536, 309, 50946], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 290, "seek": 182000, "start": 1831.64, "end": 1834.28, "text": " does not specifically address the potential risk associated with highly", "tokens": [50946, 775, 406, 4682, 2985, 264, 3995, 3148, 6615, 365, 5405, 51078], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 291, "seek": 182000, "start": 1834.28, "end": 1837.44, "text": " autonomous or super intelligent AI systems that could pose existential", "tokens": [51078, 23797, 420, 1687, 13232, 7318, 3652, 300, 727, 10774, 37133, 51236], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 292, "seek": 182000, "start": 1837.44, "end": 1840.8, "text": " threats however the legislation's emphasis on risk management human", "tokens": [51236, 14909, 4461, 264, 11329, 311, 16271, 322, 3148, 4592, 1952, 51404], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 293, "seek": 182000, "start": 1840.8, "end": 1843.88, "text": " oversight oh that's what it was oversight not supervision and robustness", "tokens": [51404, 29146, 1954, 300, 311, 437, 309, 390, 29146, 406, 32675, 293, 13956, 1287, 51558], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 294, "seek": 182000, "start": 1843.88, "end": 1848.0, "text": " requirements for high-risk AI systems indirectly contributes to addressing the", "tokens": [51558, 7728, 337, 1090, 12, 33263, 7318, 3652, 37779, 32035, 281, 14329, 264, 51764], "temperature": 0.0, "avg_logprob": -0.10605374123286275, "compression_ratio": 1.7793103448275862, "no_speech_prob": 0.006691789720207453}, {"id": 295, "seek": 184800, "start": 1848.0, "end": 1851.96, "text": " safety concerns associated with AI technologies it is important to note", "tokens": [50364, 4514, 7389, 6615, 365, 7318, 7943, 309, 307, 1021, 281, 3637, 50562], "temperature": 0.0, "avg_logprob": -0.18030471112354693, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.1328762322664261}, {"id": 296, "seek": 184800, "start": 1851.96, "end": 1855.2, "text": " that addressing existential risk related to AI may require further research", "tokens": [50562, 300, 14329, 37133, 3148, 4077, 281, 7318, 815, 3651, 3052, 2132, 50724], "temperature": 0.0, "avg_logprob": -0.18030471112354693, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.1328762322664261}, {"id": 297, "seek": 184800, "start": 1855.2, "end": 1859.8, "text": " discussions and potentially additional regular to regulations in the future okay", "tokens": [50724, 11088, 293, 7263, 4497, 3890, 281, 12563, 294, 264, 2027, 1392, 50954], "temperature": 0.0, "avg_logprob": -0.18030471112354693, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.1328762322664261}, {"id": 298, "seek": 184800, "start": 1859.8, "end": 1870.44, "text": " cool so there you have it this file is saved on as merged dot text let me go", "tokens": [50954, 1627, 370, 456, 291, 362, 309, 341, 3991, 307, 6624, 322, 382, 36427, 5893, 2487, 718, 385, 352, 51486], "temperature": 0.0, "avg_logprob": -0.18030471112354693, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.1328762322664261}, {"id": 299, "seek": 184800, "start": 1870.44, "end": 1877.0, "text": " ahead and send that up to get so you can all use it let's see get status I get", "tokens": [51486, 2286, 293, 2845, 300, 493, 281, 483, 370, 291, 393, 439, 764, 309, 718, 311, 536, 483, 6558, 286, 483, 51814], "temperature": 0.0, "avg_logprob": -0.18030471112354693, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.1328762322664261}, {"id": 300, "seek": 187700, "start": 1877.24, "end": 1884.4, "text": " add get commit AM all done get push all right so there you have it you have a", "tokens": [50376, 909, 483, 5599, 6475, 439, 1096, 483, 2944, 439, 558, 370, 456, 291, 362, 309, 291, 362, 257, 50734], "temperature": 0.0, "avg_logprob": -0.2065216302871704, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.11270803213119507}, {"id": 301, "seek": 187700, "start": 1884.4, "end": 1891.52, "text": " much condensed version of the EU act here and merged you can plug it", "tokens": [50734, 709, 36398, 3037, 295, 264, 10887, 605, 510, 293, 36427, 291, 393, 5452, 309, 51090], "temperature": 0.0, "avg_logprob": -0.2065216302871704, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.11270803213119507}, {"id": 302, "seek": 187700, "start": 1891.52, "end": 1897.08, "text": " into the playground and ask questions to your heart's content if you have access", "tokens": [51090, 666, 264, 24646, 293, 1029, 1651, 281, 428, 1917, 311, 2701, 498, 291, 362, 2105, 51368], "temperature": 0.0, "avg_logprob": -0.2065216302871704, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.11270803213119507}, {"id": 303, "seek": 187700, "start": 1897.08, "end": 1901.24, "text": " to it but otherwise you can at least just read it here yeah I think we're all", "tokens": [51368, 281, 309, 457, 5911, 291, 393, 412, 1935, 445, 1401, 309, 510, 1338, 286, 519, 321, 434, 439, 51576], "temperature": 0.0, "avg_logprob": -0.2065216302871704, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.11270803213119507}, {"id": 304, "seek": 187700, "start": 1901.24, "end": 1904.0, "text": " done thanks for watching", "tokens": [51576, 1096, 3231, 337, 1976, 51714], "temperature": 0.0, "avg_logprob": -0.2065216302871704, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.11270803213119507}], "language": "en"}