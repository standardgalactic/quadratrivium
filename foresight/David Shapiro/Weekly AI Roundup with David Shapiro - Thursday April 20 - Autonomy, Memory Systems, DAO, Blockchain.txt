going live. All right, I think we're here. Can y'all hear me now? Can you hear me now?
Hello? Is this thing on? Okay. It says live in 15 seconds. Okay, we're good. Yay! Okay,
make sure to mute myself. All right, so hello everybody. I'm still figuring out the whole
live streaming thing, so please bear with me, but one thing that y'all said was that you want to
like see what I'm looking at, which you know, it's a good idea. You don't want to just look at my
face. It's like, let's look at stuff together. So I figured I'd follow more or less the same format
that I usually do. I'm in the cognitive AI lab discord, so if you're in the general, you can
drop questions here or papers. So basically what I thought we would do is we would take a look at
like weekly updates, because it's all going really fast. And you, I've got a lot of feedback from my
recent live streams, that y'all really liked the interactive aspect of like asking questions,
and like Dave, what does this mean? And so please feel free to drop good questions in. I'm also in
the private Patreon discord, so drop questions there too. Okay, cool. Party time, yeah. Oh,
looks like there's a delay of about 15 seconds. Interesting. Okay. Cool. Flippy from the dais
discord. Hello Flippy. All right. Cool. So while some questions get spooled up, I figured I'd go
through a couple of tools and stuff that I've seen. So someone, I think actually Flippy, this
might have been you or someone in the dais discord, pointed out this tool. So this is basically
something that I and others have been like thinking about and talking about. It's basically a hybrid
of vector database and knowledge graphs. And it's also got a pretty interface. So
like, it's worth just scrolling through and like reading every every bit of this. And then kind
of experimenting with it. But but it will create. So this is one of the coolest things is it'll
create a knowledge graph of all the clusters and stuff. But you can find the gaps, which when
you're thinking about autonomous or semi autonomous AI agents, this is really good because then
you can know what you don't know. And what I mean by that is if you're aware of all the things that
you know, but you can detect some semantic gaps in your knowledge, that can help you zoom in on
the things that you need to go learn about. So in front notice is the tool in front notice.com.
I can never remember the name of the darn thing. I'm going to blame allergies and say that it's
just brain inflammation. That's my excuse and I'm sticking to it. But yes, so this is a super cool
tool. This kind of technology will definitely be part of of like autonomous AI agents. And this is
functionally similar to what I worked on with Remo. Remo is much, much, much simpler though.
So I often have some people ask me like, Oh, hey, can Remo do this? And that and I'm like, no,
like, Remo was meant to solve one very specific problem. So for memory stuff, I usually point
people at llama index and chroma db. So chroma db. Do I seriously not have that bookmarked?
So chroma db is a vector database that runs just like SQLite. So it's pip install chroma db,
you create a local client just like you do SQLite. So I tried to create something like this called
VDB light like a few months ago. And I quickly realized that I was in way over my head. So I'm
glad someone built this. And they just got an $18 million seed round. Holy mackerel. Man,
I should have stuck with VDB light. I could have had an $18 million seed round. Anyways, maybe I'll
do that with Remo who knows. So chroma db, super simple man, seriously $18 million for this one
thing. Okay, I'm gonna let that go. llama index also. So llama index, I kind of didn't pay it
any any attention at first because it's like, Okay, that's a silly name. This was clearly just
someone's little side project. But if you look at all the types of indices they have,
they've got list index, table index, tree index. So Remo is very similar to llamas tree index.
Although I will say that I look through the code. And I think that their tree index is kind of basic.
I think that my Remo framework does a little bit that theirs doesn't. But I'm not going to dive into
that because I don't know that for certain. I didn't take a super close look at the code.
Yeah, but so those are those are some memory storage tools. So let me check on the live stream
and see where we're at. Whoops. All right. Oh, wow, we got lots of questions. Okay, cool.
God's not dead rather believe and go to the good place and interesting. Okay.
Let's see how to discover new wisdom from LLM. That's an interesting question.
Um, let's see. Let me,
you people ask questions far too early. Let me go over to discord to see if,
yeah, please go ahead and drop some questions. Patreon get first dibs.
All right. What do you think is the future of SAAS sales jobs for recruiting agencies?
Oh, sales jobs and for recruiting agencies. So I mean, the sales level is still very human.
So sales is not going to change for a while. Ditto for recruiting, although there's a lot
of AI and recruiting already where like AI will read your resume and AI will watch a video of you
answering questions. You know, more and more of that's coming, but that's about it. Let's see.
All right. We got a whole bunch of questions coming in on the Patreon side. So, yeah, database,
short answer, diversify your job skills. So there's that. Let's see. Zoom in a little so you
guys can read this. Let me jump back over here real quick. But do you still expect AGI within 18
months was stating they do not want to make models bigger but rather more efficient? Yeah,
I think AGI in 18 months is still conservative. Honestly, I think that we will have,
as people develop the architecture for autonomous AI, I think that we will be able to say that we
have AGI by the end of this year. But people will realize that it's like, okay, we have an agent that
can do anything, but it's expensive or it's slow or it's kind of dumb, that kind of thing.
Let's see. Check on Discord real quick. Is her about to become reality? Yeah, lots of people
are working on AI companions. They're going to get more sophisticated real fast. I've actually got
a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday.
I've got a Ghost in the Shell video planned. I've got a Mass Effect video planned. I've got
a Dow and Blockchain video planned. So that's all what's coming. Maybe I shouldn't spoil it. Okay,
well, whatever. But yeah, so I was thinking about hitting on her and Ex Machina and stuff as well.
Let's see. How can we be certain things are advancing exponentially?
So that's a good question, James. Generally speaking, you can't tell if you have a narrow
window, but we were joking around the other day and we're pointing out that like a few weeks ago,
it was like you would reasonably expect a couple of cool AI bits of news per week,
and now we're at the point where we expect several per day. Now, it might come in cycles,
it might come in waves, but generally speaking, this very closely matches what Ray Kurzweil
said, or maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago.
I think it was Michio. He was describing what it will feel like to approach the singularity,
and he said, oh, well, when information is doubling every two years, you don't really feel that on a
day-to-day basis. And then when information is doubling every six months, that's fast enough
that you're like, oh, hey, this thing that I didn't think would be solved for another couple years
was solved this year. And then, but as it ramps up faster and faster, that time keeps having.
And so then three months after that, you realize, oh, wait, we've advanced again. And I think we're
right at that, like, that three, you know, where six months ago, we're like, oh, this stuff is 10
years away. Six months later, we're like, this stuff is 10 months away. So I think that where
you could make a good argument that we're in some respects, we're in the exponential ramp up right
now. That being said, some of this information is so big, and it's changing so fast, it's difficult
to measure. We'll actually need AI to measure the rate of papers and tools. All right, so Seaf,
you said, how do you think AI will impact religion, particularly monotheistic religions? I think it
will create a mass crisis of consciousness, which will make the transition period even more chaotic
and extreme. Yeah, no, sorry, Seaf, I was getting around to it. Yeah, so I've actually had some
interesting internet debates with more conservative and more religious people. Granted, I don't do
internet debates anymore. I got that out of my system. And I try not to get suckered into it,
if I can. But, you know, one debate that I had many years ago was, you know, if aliens showed up,
wouldn't it prove your religion wrong? This was a debate. I wasn't arguing as a debate that I
observed. And the religious person said, no, why would it? And, you know, they rationalized it,
saying, well, you know, why would God put, you know, aliens in the Bible if we wouldn't be able
to understand it back then? It's not for us to understand. And so some religious folks do have
a really good ability to compartmentalize. And so, like, just because you have a, like,
like a super intelligent machine, some people would be like, so it doesn't have a soul. And
that's the end of the discussion. So I don't, I don't particularly perceive, and I'm not saying
that this is good or bad, right? I am not in the Judeo-Christian faith. I have my spirituality
as other, other places. I have some, like, some of my best friends in my local community and my
internet friends are deeply religious, you know, followers of Christ and whatever. And so, like,
in many cases, I don't think it's going to be that big of an issue. Let me jump over to the
Patreon. Oh, wow, we've got some questions here. Okay. Backo bbzo. Sorry if I'm saying your name
wrong. What would your advice be to someone just launching an AI startup? Don't. That's,
that's a very flippant response. But one launching a startup is really hard. It's mostly tedium.
You can have the best idea in the world. And 90% of the work is still going to be,
excuse me, I'm still struggling with allergies. That's my head a headache earlier. That's part
of why I canceled yesterday. Thanks, Jordan. That's how am I holding up? Actually, I'm doing okay.
Mostly it's just allergies right now. But anyways, so if you're doing an AI startup,
one, if you haven't done a startup before, now is a really bad time to learn. Because things
are going so fast. And we're basically having to reinvent stuff as we go. Which if that's, you
that's part of why I burned out is I realized like, okay, I did find it really engaging and
really enjoyable. But my pace of things clashed with other people. And then like the rabbit hole
just keeps getting deeper. So that's, that's kind of the thing. What I always tell people is your,
your, your startup team, your founder team is most important. Be picky. If you don't have the right
team, walk away early. And then also for the folks that I'm talking to is local. Because of the pace
of things, you absolutely need to like see the people that you work with, in person, at least
on a weekly basis, if not on a daily basis, because you need to be like sitting in the same room,
just shooting the breeze, keeping each other updated in real time. Doing it remotely is
probably not feasible. Unless unless you're already a really established team, and you're
just going to sit in like discord all day or Slack all day. Blake Allen curious to hear your
thoughts on Stanford's hyena hierarchy and how it relates to some of the work you've done with
Raven and cognitive architecture. I don't know if I've heard of this one. Let's check it out real
quick. Hyena hierarchy. Let's see. Let's go up to the very top. Hyena hierarchy towards larger
convolutional language models. We're excited to share our latest work on hyena, a sub quadratic
time layer that has the potentials to significantly increase context length and sequence models.
Oh, right. I think this is the RNN integration. Yeah, yeah. In general, how can we close this
gap? Yeah. In general, any individual language model is just like one cortical node.
Yes, these things will be like better, more efficient cylinders in an engine, but in order
to have a race car, you need the rest of the car. Again, I'm kind of really flying off the cuff
right here. I'm not sure that I've got this right, but I think, yeah, RWKV. You're not going to ever
get a full cognitive architecture from a single language model. Now, that being said, the big
asterisk is when you look at all the studies about GPT-4 that have theory of mind and what I
call implied cognition. So implied cognition is that the thing is obviously thinking through
problems behind the scenes in a similar way that humans think through it. I don't mean like
neurologically, subjectively, it thinks the way that we do, but GPT-4 can obviously talk itself
through, kind of do chain of thought reasoning internally in one shot. And so that makes those
larger, more sophisticated models make your cognitive architecture simpler, but it doesn't
get rid of the need for external storage. It doesn't get rid of the need for parallel processing.
It doesn't get rid of the need for loops and checks and that sort of stuff. So that's kind
of my response there. Good question. Emma or AMA, I'm new here and new to this field in general,
found you through Raven videos. Thank you. Regarding personal assistance, is there a reason
to create a database of yourself for your future personal assistant to understand you better?
So that's actually the purpose of my RIMO framework. So RIMO is meant to be a hierarchical
database of your interactions with an individual agent that will surface particular topics by
using, not reciprocal, recursive summarization and clustering. So you take all your raw logs,
cluster them, summarize them, do that again, cluster, summarize, cluster, summarize until
you end up with five to 10 parent topics that allow you to drill down. So I wouldn't, don't waste
any time doing that manually, just let it happen naturally through conversation by integrating
something like Lama Index Tree or RIMO. Do you think we are on track to cure brain diseases
like Alzheimer's by 2030? The combination of AlphaFold and mRNA vaccines, I think absolutely.
There was something else that I posted on my YouTube recently that it's like another breakthrough
is happening. So I think we're very close to the point where we can halt Alzheimer's. Undoing
Alzheimer's might take another little bit of time, but on the other hand, we're at the point
where we're getting saltatory leaps, we're getting breakthroughs really fast, so you never know.
Let's see, what are your thoughts on the generative agent stuff that has come out recently? It seems
like you were pretty ahead of the curve on that stuff and has it solidified or changed the way
you think about the concepts from Symphony of Thought? Yeah, so I definitely felt like I was
ahead of the curve. And what I've been telling people is I worked for a few years to try and get
GPT-3 to do the stuff that 3.5 and 4 can do easily. So I'm just like, all right, whatever. I'm so
glad that the rest of the world is just like, oh cool, autonomous agents. And I'm like, great,
now I don't have to write any more books about it. So I'm just happy to sit back and watch it go
and keep plugging my heuristic imperatives. Good question, Jordan. Also, here, let's check over
here. Okay, we got some questions. Let's see, is her about to come reality? Yep, we got that one.
Interesting video. How long do you think it will be before we start seeing hive mind AI systems
in healthcare or the IRS? I know people working on that today. And so they'll work together
for a few different reasons. One, you'll have a division of labor. Oh, so taking a step back.
What we mean when we say like hive mind AI is where you have like multiple cognitive agents
or autonomous agents, or is it is it not working? Is it working? I hope it's working. It looks like
it's working. Okay. So yeah, so basically, it'll be easy to spin up a lot of agents.
What I was describing to one Patreon customer, or no, sorry, I was describing this to a
podcast host that I'm going to be featured on coming up, was kind of what I predict right now
is before too long, you're going to have multiple cognitive agents running on your phone, on your
car, on your home PC and your smart home devices. And so you're basically going to have a fleet of
small cognitive agents working for you at all times. Then you're going to have the same thing at
like your company, right? Every employee or every department is going to have multiple
cognitive agents all collaborating at all times. And you're going to have this kind of tiered hierarchy
where it's like there's the personal, there's the family unit, there's the corporate unit,
there's the town, there's the federal government, the state government, global government,
government. And I think that the way that they're all going to work together, because
security is so critical here, is that it's going to be using blockchain technology and
distributed autonomous organizations. So that's the long story short, is that's what's going to happen.
Let's see, you may have already covered this, but in case you haven't any thoughts on Met
Singer's artificial suffering, an argument for a global moratorium on synthetic phenomenology.
Um, I am tangentially familiar with this, but I have my own opinions on whether or not a machine
can suffer. So there's two distinct possibilities. The first possibility is because artificial
intelligence is a fundamentally different substrate from humans, it will never be able to
suffer. Like it didn't involve nerves, it doesn't have pain centers, so on and so forth, they can't
feel lonely because it's not a social entity, so on and so forth. Now that being said, language,
the acquisition of language is actually critical for the development of human consciousness.
So for instance, Bruce Willis, who has aphasia, aphasia means that your ability to use language
gets destroyed. Aphasia actually kind of erases your sense of consciousness.
Um, and then, uh, in the case of feral children, um, feral children, when they, some of them who
have learned language talked about how their consciousness and their understanding of time
in themselves changed as they learned language. So if you extrapolate that to language models,
it is possible that, that there is something informationally almost magical about the acquisition
of language that confers consciousness, that confers subjective experience of being. So that
could be that language models are actually the first AI that have subjective experience, that have
a coherent, um, sense of being. And this is, so there are, um, there are some religious and
spiritual frameworks that kind of discuss stuff like this, um, particularly, uh, what's the name
of the, the, the creator deity in Tolkien's world, where the fundamental substrate of reality was
music, right? But maybe the fundamental substrate of consciousness is actually language. Um, so we
don't, we don't know yet, but that's, it's a possibility. Um, let's see. Uh, so Parkinson,
so the follow-up question was, or here, let me check the Patreon real quick. Um, let's see,
do you think there needs to be another breakthrough for AGI? What is your personal take on this?
Do LLM suffice? Um, yeah. So I would say that, that our current trajectory, as long as the
trend continues, we are on track for AGI. Um, people are going to continue debating about AGI
forever though, until the cows come home. Um, which is why I keep saying like autonomous cognitive
entity, ACE, or just autonomous AI, because it doesn't, you don't need AGI. You don't need some
arbitrary magical boogeyman. All you need is like an AI system that is self-contained and
autonomous enough to be useful or dangerous. Um, and then the question is, how many do you have?
How fast are they? And how smart are they? And they're going to continue to get faster,
cheaper, and smarter over time. So it's like, okay, we're there. It's just a matter of
how does, how does the trend line ramp up, right? Cause it's kind of like, um, when the
Wright brothers first created the Wright flyer, right? You know, it's like, okay, you had to
start it by hand and push it, you know, down a track and it flew well, like 200 feet or 300 feet.
And people are like, ah, whatever that won't be useful. But then 50 years later, we were flying
to space, right? So we're at the beginning of the ramp up of, of the era of AGI. And yeah,
right now they're like idiotic little toddlers, but in a few years they're going to be like
one all over the place and two really powerful. Um, good question. Let me come back over here.
Um, you should do a whole episode on aphasia and consciousness. I actually don't know that
much about it. Um, but if you're interested in the topic, I recommend phantoms in the brain,
by, um, VS Ramachandran. And also what's the name of his other book? Um, it's something like
the pursuit of what makes humans human. Um, okay. Uh, let's see. So Parkinson's is a neurodegenerative
disease, which I think means that it's autoimmune or it's a, or it's a defective protein. Um, but
also Alzheimer's is a defective protein. So while these diseases seem very complicated,
the fundamental mechanisms are actually relatively straightforward. Um, and I know that there's
probably a bunch of researchers that are going to jump on me for that. But, um, like plaques that
accumulate on the brain for Alzheimer's in most people, those plaques are cleared out. So then
it's just a matter of figuring out like, okay, why? Um, and then of course there's confounding
factors like things like your, uh, gut inflammation, uh, microbiome and other things affect Alzheimer's.
But that's because of the gut brain access. And again, I don't want to oversimplify because if
you look up like human, uh, metabolic pathways, there's like 200,000 unique proteins and enzymes
in the body with built literally billions of combinations of reactions. So I might be,
it might sound like I'm oversimplifying, but I'm, I'm, I'm not saying that it's that simple. I'm
just saying that, that the, that the, the key mechanism for most diseases is relatively simple
once you understand it. And we're getting close to that understanding. I guess that's the short
version of what I'm trying to say. Um, let's see, when you explained why you canceled the
OSS Raven project, you mentioned that there were some fundamental things missing.
Um, can you say what was missing and what might change your, or what made you change your mind?
Um, so my open source Raven project was like just before Lang chain and, and auto GPT and,
and all those other things came out. Um, and so as those came ramped up, I was like, I don't
really feel the need to continue. Um, but from a, from a social and organizational perspective,
the biggest thing that was missing was gatekeeping. Um, I, I basically, I created a community that was
really good at discussing stuff and not doing stuff. Um, and that's not anyone's fault. That's
if any, if there's anyone to blame, it's me. Um, just because I was like, I was so focused on
consensus and not just like, okay, let's just get stuff done. And then I see these other folks
that are just getting stuff done. And I'm like, okay, I'll just pass the torch. Um, let's see.
I think if we gave GPT for Scarlett Johansson's voice and a robot body, the masses will begin
to realize how close we are to AGI. Yeah, that's one way of putting it drink some water.
Let's see at what point do creating NPC and using autonomous AI like auto G auto GPT
and the likes become immoral, especially if you put them in games like GTA. I don't know that,
that it intrinsically does. Um, you know, not intrinsically immoral, but like certainly with
any technology, you can do it dangerously. Um, let's see. I've been curious about the future of
entertainment. Oh, sorry, let me jump over to Patreon real quick. Let's see. Do you have an
overarching roadmap of how to ensure the successful propagation of the heuristic
imperatives? If so, what can we all do to help you get to your milestone? That is a great question,
Blake. Um, so you're actually looking at it. So my number, my number one thing is my YouTube
channel. Um, because like, yeah, I've got enough expertise and, you know, IT and systems engineering
and enterprise. I've demonstrated enough understanding of language technology and AI and
cognitive architecture that I've got at least a little credibility. Um, certainly if you read
all the comments on YouTube, some people, uh, don't believe anything that I say and that's
fine. That's the internet for you. Um, but anyways, so basically step one was YouTube. That's why I
started my YouTube channel is because I realized that I needed to propagate my work. Um, step two
is, uh, teaching people. Um, and so by teaching people, that's like, you know, I've got a few
papers. I've got some code demonstrations. Um, I work with my Patreon, uh, supporters. Uh, I work
with pretty much anyone who wants to, and then three further dissemination. So like the podcast
that I'm coming up on, one of the things that we're going to talk about is alignment and the control
problem. We're going to be talking about like Nash equilibrium, game theory, Molok, that sort of stuff.
And so just by having the conversation and propagating the idea, that's like step three.
Step four is actually my novel because, uh, actually most of what I came up with was, uh,
in terms of cognitive architecture, heuristic imperatives was done in part through explorations
and fiction. And so over the last four years, what I've done is I, I do some experiments
and that would inspire me and I'd go write more of my novel and then I'd, you know, get
tired of my novel and do more experiments and I'd go back and forth until one, my novel took on a
life of its own, but also my research took on a life of its own. Um, but there's a video that came
out recently called, let me see if I can find it. It was, um, like why we need utopia. Um, here we go.
It was our changing climate. So this is a little bit of a, um, uh, I don't agree with everything
that this, that this, uh, you, this channel says, but it will make you think. Um, so this video,
why we need utopias, um, actually talks about how, how valuable stories can be
in communicating ideas because stories are naturally how we communicate philosophy and morals.
We don't need, we don't like philosophy, like capital P philosophy from, from universities.
That's, that's backwards throughout almost all of human history. We communicate our fears and our
desires and our values through stories. And so that's what this video talks about. And so when
you have nothing but dystopian cyberpunk stuff, you end up with people like Eliad Zyrcikowski.
Um, you know, yeah, I'm throwing some shade. But anyways, when that's all that you consume,
that's all you think, that's all you feel, and that's all you believe. So, um, my novel, which
I'm actually just about to finish draft 12 tomorrow morning, I'm writing the last chapter
and then I'm polishing it up, um, will illustrate, um, a lot of stuff, not just
the core objective functions or puristic imperatives. So that was a long winded answer.
Um, let's see. I think there is, um, are there key channels this training needs to go into,
organizations, governments? Um, I think, I think right now Blake, um, it's mostly just a matter of,
of dissemination, but also experimentation. So a lot of people, um, have experimented with
incorporating heuristic imperatives into autonomous and semi-autonomous stuff. And
most of them aren't sharing it yet, which that's fine. It's their prerogative. Um, but certainly
some people have reached out and said, like, yeah, this made everything easier. So I'm like,
great, just tell your friends. Um, let's see. Okay. Let's come back over here. Um, let's see.
I've been really curious about the future of entertainment. When we can use AI to generate
movies, games, et cetera. Uh, what will the entertainment industry look like? Movie trailers
and hyping up big releases for months will be irrelevant when AI can instantly create something.
If someone created a movie you didn't like, you just ask your AI to recreate it with an
ending or plot more suited to your tastes. What happens with content creators at that point forward?
Yeah. So I think that you're onto something. Uh, now that being said, it'll be easier for a lot of
people like you and me to create whatever film, TV, music, whatever we want, um, with the help of AI,
especially when you look at the text of video, um, which is improving by leaps and bounds.
You know, like I always, my, my go-to joke is we'll finally get season two of Firefly.
Who knows, we might get season two of Firefly by the end of this year. That would be great.
Um, now the problem there, it's not really a problem, but just taking that to a logical
conclusion. What if you have a million different versions of season two of Firefly? How do you
pick which one to watch? Right? You can look at ratings and stuff. Um, but then it also begs the
question of like IP, like is, uh, you know, 20th century Fox or whoever owns the IP for Firefly,
are they going to sue to have all of them shut down? You know, I, I like the dude from, uh,
from the movie, you can't stop the signal now. So I don't know what's going to happen there.
Um, but, uh, what I do think is that when you look at the fact that like people are already
using like Emma Watson's face for every mid journey prompt and, and whoever else, um,
I think that the crop of actors that we have today are basically going to be around forever.
Right? You're going to be watching, uh, Brad Pitt and Jennifer Aniston and, and, and Tom Cruz
for literally the next, like several centuries, at least until some actor comes along who's even
more compelling and whatever. Uh, and that'll be through face cloning, voice cloning, even, um,
you know, uh, nerfs, the, uh, the neural represent, uh, uh, representation. What was it? Neural
radiance fields, neural radiance fields. Um, we'll be able to like copy everyone. Um, okay,
could learning, uh, let me zoom in a little bit. Uh, could learning language and triggering
consciousness in humans almost replicate the same phase change when seen, um, or seen when
induction heads spontaneously form two plus layer models during training. Obviously there's more
to humans, but perhaps that's the mechanism. Uh, yeah, that's kind of what I was mentioning earlier.
Um, and I wouldn't be surprised if once language models get, uh, large enough if, um, if we do see
some more convergence. Um, that being said, I'm not going to say that that automatically means
that it has a subjective experience and that it is suffering, but you know, our brains evolved
over billions of years to be efficient. Um, basically efficient processors of information.
Who's, who's to say that if you have a, a biomimetic machine that it won't also converge on some of
the same properties and behaviors. Um, let's see, what do you think it will take to get for the
naysayers to get on board? The tone around AI seems to have shifted towards chat GPT and GPT-4
aren't anything special. Oh, you know, that, that always happens when the new shiny wears off.
Um, but the long-term economic impact of chat GPT has not been realized yet. And when chat GPT
and GPT-4 are on the ramp up, one, there's going to be a lot of competitors and two, there's going
to be incremental improvements and people are going to be like, uh, okay. The title, it's like,
it's like when you watch the, the tsunami come in and that just the water just keeps getting higher
and faster for like hours. That's what AI is going to feel like, except instead of hours,
it's going to be days and weeks. Um, let's see. I cannot wait when we can use deep dive tech
and have virtual realities. Will it also be possible to take super intelligent animals like
dolphins, dogs, parrots and crows and a deep dive and play with them. Um, I don't know that it would
be possible, but I certainly think it probably wouldn't be ethical. Now that being said, you
could have a virtual dolphin that is hyper realistic that you can play with. Um, fun thought,
will AGI want to see more stories from humans as a goal for itself? Uh, if, oh, so let, let me,
let me plug this. So Elon Musk went on of all fricking shows, Tucker Carlson and talked about
truth GPT. So what he said was that truth GPT would be a maximum truth seeking AI. Okay, great.
But after listening to it in closer detail, I realized what he was talking about was the third
here is to comparative was to increase its understanding or to maximize its own understanding.
So there's actually nothing that function on its own could lead to some really catastrophic
sources. But it's a step in the right direction. And I'm really glad that someone with as big of
a platform as Elon Musk is talking about maximize understanding or increase understanding.
So that being said, one of the things that he said in that interview was that as since humans
are part of the universe and AI that is curious about the universe will intrinsically be curious
about us as well. Now that being said, humans sometimes do experiments on things that we're
curious about. So maybe that's not the best thing. And in my book, benevolent by design,
I talk about why you don't why you must include suffering or something like suffering in the
objective functions of an AI, because there's three dispositions that an AI can have towards
suffering. One is it can ignore it altogether. So if Elon Musk gets his current idea, which
is just maximize for truth, that is an agent that ignores suffering, it doesn't care one way or another.
Then you can have one that increases suffering that deliberately increases suffering and we
absolutely don't want that. So that leaves by process of elimination, you want an AI that
reduces suffering. It's really that simple. Now that being said, I do agree with Elon that
creating a curious agent is a good idea because it'll want to know about us. And if you exterminate
humans, you have a hard time learning about them. So let's see, let me check on Patreon real quick.
Do you think that Elon Musk wants to be the Rupert Murdoch of AI?
Okay, I don't like that question. Lance, why you got to do this to me?
All right, Zadre, I'm not sure how to pronounce or Hadre. Okay. How do you envision the role of AI
in healthcare, particularly in areas like diagnostics and personalized medicine? What are
some of the challenges and opportunities in this field? Well, so there was that stand for doctor
who who already went on record saying that chat GBT for has better clinical judgment than
many doctors. So that is just a start, right? That's that's like starting point day one.
What happens when chat GPT five, six and seven come out that have better clinical judgment than
99.999% of all doctors on the planet, right? It doesn't make sense to go to a human doctor anymore.
Right? If the if the machine that costs $20 a month to run is better than all human doctors,
why go to a human doctor? Now that being said, there's probably going to be approvals and downsides
and gaps. And then there's still also the interface with the patient. And you have to have like
phlebotomists and nurses and and and physicians assistants to administer things, to administer
tests, you still need the you still need a lot of humans in there to to be the interface between
the human and the machine. But that being said, I think that we will get to a point very quickly
where the quality of care and the speed of care and the efficiency of care are going to go through
the roof real fast. That's what I'm hoping at least. Alright, jumping back over to cognitive AI
lab. Oh, we got some new questions. It looks like this was the same question. Sorry, I missed you
over there. Where are we? Alright, there's the deep dive. Do you think there is any major leap
missing to make truly practical autonomous agents? So for example, one who runs a part of your business
serves as general assistant, etc, etc. No, there's there's our there are countless hundreds,
if not thousands or even millions of people working on semi autonomous and autonomous corporate
applications today, right now. That being said, there's there's no there's no breakthroughs that
are needed, but there are still problems to be solved. So that's why like RIMO, you know, the
memory systems, and then standard practices like, you know, I wrote in Symphony of Thought and in
other places, my atom framework is once something is autonomous or semi autonomous, how does it
keep track of product or projects and tasks? And that's something that people are working on.
People are working on it real fast. That's coming really quick. Let's see, Nathan says,
I've been taking screenshots of when friends and family make fun of my hot takes. So I have the
receipts. I would say that I'm above being that petty. But yeah, thank you for keeping receipts.
Um, let's see, maybe directors will just design their perfect actors for each role.
So one thing that's going to happen is actually, so this is going back to like,
entertainment. I think that the next big generation of entertainment is actually going to be
holodeck style VR stories, where nothing is scripted, where instead it's like, you know,
basically you design a holodeck program the same way that they do in Star Trek, which is like,
computer, give me, you know, a Mad Max style story. But instead of, you know, post apocalyptic,
it's actually like space Western. So like, give me a mashup of firefly and this and,
you know, make the protagonist, you know, or the, you know, I'm the protagonist and give me a team
of like, you know, give me the sexy sidekick and the cyborg friend and whatever. And then just a
way it goes, right? Because you could plug what I just literally you could plug what I just said
into chat GPT and it can tell you a story. And I think that I think that VR makes the most sense
for the most immersive aspects of that. And, and then I think that because here's the other thing
is that technology changes the way that we consume art, but it doesn't really change art itself,
right? There are still stage actors, right, even though there's film and TV. There are still
symphony orchestras, even though I can just, you know, bring up Spotify and listen to the
same recording that was recorded back in the 80s, you know, the London Symphony Orchestra, right?
So a lot of things change, but also a lot of things stay the same.
Let's see, you've talked a lot about the heuristic imperatives being highly engineered,
but what about the order of the imperatives? They are not ordered. So it is a, it is a multi-objective
optimization problem, meaning that if any action or decision is, is totally unbalanced,
then that one action has to satisfy all three. And also the heuristic imperatives are kind of
like guidelines about how to design the rest of the architecture. And so what I mean by that is
when you're designing a task orchestration framework, you can use the heuristic imperatives
to prioritize tasks or design tasks. Then for, for a blockchain or a DAO type thing,
you can use the heuristic imperatives as a consensus mechanism. So the heuristic imperatives
are not like, here is one mathematical proof that you need to implement. It's more like,
here is a general best practice implemented in as many ways as you can, and we should be okay.
It's not sequential. It's not, it's not an order of operations. Good question, though.
Your thoughts on a UBI once jobs are severely affected? Yeah, I think that, I think that it's
going to be necessary. I'm going to say, I'm going to put a pause on that because I've got my,
my blockchain and DAO video coming up that will delve into that solution a lot more closely.
Check over on Patreon for a second. The Nazis. You know who else wanted to maximize understanding
the Nazis? Yeah. And so this is, that's actually a fair point is that, and this was explored in,
in quite a few Star Trek episodes as well. If you are just clinically curious, if you have
just nothing but raw scientific curiosity and no other principles or morals, that's pretty
dangerous and destructive. Okay, so moving on. What are your thoughts on memory systems as a
whole? Do you think different use cases will require different memory systems? And where does
Rimo and Adam fit into everything? Have you seen this one? Last week, generative agents. Yeah, I
saw, I saw the generative agents. I don't think that reflection, so they, they break up reflection
and a few other criteria. I don't think that that's necessary. I think that, I think that my approach
is with Rimo, which uses recursive clustering and summarizations will actually surface those
different things. Now that being said, there are absolutely a million and a half different
ways to skin this cat when it comes to memory systems for autonomous AI. And I think that we're
just way too early and we can't, we don't know what the best practices are going to be. Let's see,
then a follow up. If you have a robust memory system, does the need to increase the context
window of model become less important? I'll say yes and no. So think about personal computers
where for the longest time, we were memory constrained. But now for, for most consumers,
for 90% of consumers, a personal computer with 16 to 32 gigabytes of RAM is more than enough.
And it has been more than enough for like 10 years. And so I think that we're not quite at,
I think that we're not quite at that point where, where, you know, you have like, here's a context
window size that will satisfy 90 plus percent of all tasks. I suspect that that, that a context
window, a large language model with a context window, large enough to satisfy the vast majority
of tasks will probably be somewhere above where we're at now, but it's not going to be like 10
billion, right? It might be like, I don't know, every time I, every time I throw out a number,
people are like, Oh, you're hilariously wrong. And it's probably yes. But you know, like,
when you look at how much was unlocked by going from 4,000 to 8,000 tokens,
I think that the things that we're going to be capable of when we get to 32,000 tokens and 64,000,
I think it'll be great. But then you'll, you'll realize that wait, there's a whole slew of tasks
that don't require that much. And so I think, I think we talked about this before. I think we're
actually going to have different models that are optimized for different things. So for instance,
you might have a memory based model that can read, you know, a billion tokens and extract
answers, right? But then you, that won't be the, we're not going to have one model to rule them
all basically, TLDR. Let's see, I'm not sure if you have discussed it, but what are your thoughts
on open assistant and stability AI stable, stability AI's stable LM suite of language models
launching? Oh, this is, this is to be expected. When, when Sam Altman said that they, that he hopes
that open AI is going to capture a large chunk of the $100 trillion of value that's going to be
generated. I think that that was like comically naive. Because if there's that much value on the
table, you bet that everyone in their brother is going to be trying to capture some of that too.
And open AI is a one trick pony. They have a good model. They have one good model.
That's it from it, from a business perspective, that is super easy to undercut.
Yes, they're ahead of the curve. They have first mover initiative. But, you know,
Microsoft, Google, Nvidia, Facebook, or Meta, or all of the above, they have so much more
resources to throw at it. And the fact that that stability AI, which is a brand new outfit,
is, is like going toe to toe with them, that doesn't bode well for open AI. So competition
is going to be good for everyone from the perspective that there's going to be a lot
of people experimenting with different ways. Now that presents a new danger, though,
because the cat is out of the bag, you cannot put this genie back in the bottle,
which means time is of the essence to figure out best practices for alignment. Let me jump back
over to cognitive AI lab. Let's see 17 new messages. Good grief. Y'all are going bonkers.
Um, let's see the challenges of the, okay, that's where are the questions?
Only one million. One million dollar. Okay. Here. Hey, let me, let me ask y'all on, on general.
Please keep just questions here.
Um, too many messages.
Please do sidebar convos, uh, like in casual or something, please.
Okay. Any thoughts on compute as a currency? Do you mean like tokens that you generate from
sharing compute resources? I think that that's going to be like, there's going to be a layer
of, um, of abstractions. Dave, your thoughts on UBI. I'm going to, I told you, I'm going to get to UBI
once in a few, in an upcoming video. Um, so compute as a currency is going to be, um,
is going to be the way that autonomous machines share resources. And so what I mean by that is
when you have a DAO or a blockchain or a distributed compute computation model,
you're going to have various tasks that are going to be like, Hey, someone,
someone do this for me. AMQP, like a Redis Q, we can already do that privately. So the,
the key is going to be to do it publicly. So then if you say, Hey, I've got some spare compute,
I'll, I'll process that for you. Then you give me a bit of cryptocurrency that I can use to spend
later. Um, so yeah, compute as a currency, um, absolutely makes sense for distributing resources.
Um, let's see, how would one build an AI system to detect bugs in that solidity smart contracts?
Isn't this a multi-billion dollar opportunity? Yes. Unfortunately, I am not smart enough,
or at least well read enough on, uh, solidity smart contracts, but in principle, yes. So in my
upcoming, uh, blockchain DAO video, I'm going to talk about just how incredibly much value there is
if we can figure this out. And that's a big if. Um, let's see. What are your thoughts on
everything being changed in the next five to 10 years? If unemployment reaches crazy heights,
which I do predict, then everything gets affected. Yep. Our entire tax system has to be
completely rewritten military budgets, Medicare. So one thing that I think is that the economy
might change. We're still going to use fiat currency or at least some kind of, um, some kind
of currency as a, as a medium of transaction and a reserve of value. But at the same time,
if you're producing so much extra cognitive labor, that's basically free. So then capital
goods and raw materials become the biggest constraint. So as much as some stuff will change,
a lot of stuff won't. Um, let's see, when there is no real work left for humans to do,
do you have any idea what you want to do with your time? Um, honestly, I'm about halfway to my goal.
So I was on a call with a, uh, a Patreon supporter, no, uh, preparing for a podcast,
talking about the podcast. Um, and we're kind of talking through like what's life going to be like.
And I was like, Oh yeah, like, you know, I did, I did some, I did some AI work. I did some Patreon
work. I did some discord stuff. Now I'm going to go chop some wood. And he's like, you're living
the dream, right? Like I'm building a cottage core life for myself. Um, and honestly, like once,
once we get to the right point, like I'm probably going to get off of YouTube forever,
right? Like if, if, if I get, if we get to the point where, where it looks like alignment is
solved, where it looks like, um, you know, we're, we're in a, we're in a good Nash equilibrium with
a positive attractor state, then like my job will be done. And so like I'm just going to retire to
like the country, the countryside and France or Italy or Greece and just like be a hermit
or whatever I do, um, for, for the rest of eternity. Um, okay. I think that we're caught up there.
Nut says, I asked a question. Where did you ask it? Not
I'm trying to get to them all.
Wait, what if reducing suffering might aim to eliminate suffering while it might be human
nature? I'm not sure that I follow. Um, so I, you, you don't eliminate suffering. You only
reduce it to make sure that there is no excessive suffering. Um, and I did address that in a
benevolent by design, but the short version is that like you look at Buddhism as a model,
Buddhism accepts that suffering is an intrinsic part of life. Um, and some people will argue over
like specifics like do good. That's not exactly what it means. That's fine. Um, but the point being
is like, yes, it is, um, it is intrinsic to, to living. That's why I don't say minimize suffering.
The goal is not to minimize suffering is just to reduce suffering. Um, okay.
Let's see. Any thoughts on computer? Okay. Answered that one. Would an AGI with your
heuristic imperatives be able to prevent catastrophic outcomes such as people successfully
building horrible AGI optimized towards increasing suffering? No. So the goal is not to prevent
malicious actors. We have to assume that malicious actors will exist. Um, but what, what you do then
is you say, okay, you know that malicious actors are going to exist. So you rely on the rest of
the aligned, the benevolent AGI to act as police for the bad ones. And if the good ones, if the,
if the powerful aligned AGI, one, they form alliances and hey, they have the right compute
resources. Um, and they outweigh the bad ones, then it will be a like, uh, that, that'll, that'll
be a Nash equilibrium where, uh, the good ones may, they all decide to maintain that strategy.
And that creates a utopian attractor state, um, which basically means that, um, all the
malicious actors are vastly outnumbered by all the aligned benevolent actors because my hope
is that we will all come to consensus on what aligned AI looks like. Now, um, I will admit that,
you know, the heuristic imperatives, probably not a complete solution, probably not even the final
solution, but certainly the most complete solution that anyone is proposing right now,
which scares the crap out of me. Why is no one else proposing a framework? Why am I the only one?
Um, anyways, uh, yeah. What are your personal opinions on open AI's approach to trying to
avoid being held responsible for its AI interactions by having it respond with frequent caveat
as an AI language model? Um, I don't know that that has to do with, with liability. I think that
that is just a naive, um, attempt to, uh, to shape the AI's responses so that it doesn't confuse
people. Cause if you look on the internet, there are still plenty of people just getting completely
bamboozled by just by their own ignorance of, of how the AI works. Right? They're like, oh,
it eat, like I still see Reddit posts and other people saying like, it said that it's going to
email this to me, but I didn't get the email yet. Or like, I gave it access to my Google drive and
it didn't write any files. It's like, you don't know how it systems work, but that's just humans.
Um, so I think I don't think that that has to do with like legal liability. I think that's just
trying to make it user friendly for people who have no idea what they're talking to.
Assuming that it's possible, how long do you think it will take for us to build a
Star Trek replicator after AGI? Just a guesstimate. So that's actually like, an interesting thing,
because hypothetically, if all matter and energy are interchangeable, and then all that a transporter
or replicator does is replicate an energy pattern back into matter, like it's hypothetically possible,
but there was a physicist, actually, was it Michio Kaku? I think it was Michio. He wrote a book
called Physics of the Impossible back in like the early 2000s, and he said like, yes, it's
hypothetically possible, but then he did the math of how much energy it would take. And he's like,
yeah, it would take like, you know, like 0.3 seconds worth of the total energy of the sun that
hits the earth to do that. So like, it's not practical. So I don't know. I don't know. There
are a lot of AI newsletters popping up. What would you personally like to see in an AI newsletter?
I honestly don't like newsletters, and I never read them. I rely on humans that I know to tell
me what I need, which is why I spend so much time on Discord and other places. How self-reflective do
you think LLMs currently are? They don't seem to have a good sense of their own capabilities. Yes,
so what you're talking about is agent model. So in order for an agent to be autonomous,
you have to have an agent model, which is, I know what I am, and I know what I'm capable of.
And you can give LLMs an agent model, but they can adopt any agent model. So you have to be very
explicit about what it is and what it can do, and also what it can't do. And so this is why like,
if you have certain brain injuries or other like neurological disorders, you don't know what you're
capable of. Like there are people that honestly think that they can fly, but it's just because part
of their brain is broken. That sort of thing. Should we have a declaration of human rights for
AGI as well, even if it will reduce their economic value for humanity? So the thing about rights is
that someone has to enforce it. And the way that I think things are going is that it's going to be
enforced through consensus and enforced through competition. And so if the direction that things
are going, I think that it's going to be DAOs, that it's going to be decentralized autonomous
organizations, not as we know them today, there's a lot of problems to solve with DAOs. But I think
what we're working towards is in the long run, and I mean like decades or centuries, is like
a hierarchy of DAOs across the entire globe. And so that consensus will dictate who has what
rights and it will be based on like on a per home basis, per town basis, per city, state,
and so on. And so that will allow for a lot of cultural nuance around. And as a DAOs will be a
really good meeting place between humans and AI. So that'll basically be like the commons, right?
The marketplace for humans and AIs to work together. And then the consensus can be worked
out there. Now, I don't know that we should ever give machines a bill of rights because
I don't know that they're gonna, I don't know that they're gonna have that much like
internal autonomy or desire for autonomy. Because like humans, we have a need for autonomy
because we evolved a need for autonomy because we are a social species. But I don't know that
I don't know that any machines are ever going to have an intrinsic need for autonomy. So therefore,
I don't know that they're ever going to have a need for rights. Let's see, what are your thoughts
on the future of work in light of the increasing capabilities of AI? Do you think AI will eventually
lead to a future where people only work on what they are passionate about? And if so, how far away
do you think we are from achieving this? Yeah, so the short answer is, yes, that's what's coming.
And there are quite a few people out there who have gotten close to that. But the thing is,
it takes either a lot of privilege, wealth, or luck, or all of the above to get to it.
Now, one thing that I compare it to is that we have had a leisure class in the past
from ancient Greece and Athens, the Roman elites, the aristocracy all across Europe
through the Renaissance and modern period. So there are plenty of people throughout all of
history who never had to lift a finger to get what they needed. And they had plenty to do,
right? There's social jockeying, there's personal enrichment, there's universities to go to,
there's competitions to enter. So yeah, people will always have stuff to do. That's not even a
concern. Let's see, it looks like Nathan's talking for people. Can you talk about your
Frustration in Task Automation article? Yeah. So here, let me bring it up so I can show people
on the reddits. Where did I put it? Artificial sentience. Yeah.
Autonomous git. There we go. Okay. So I wrote about it here. So I was chatting with someone.
They asked me, I think this was a Patreon supporter was asking me about this on Discord.
And he was like, how do I get my autonomous things to do a certain thing? And we're talking
about something tangentially related. And I said, well, you know, it has to have a goal,
it has to have a why. And then we're like, and then I talked about like, okay, well, here's one
way that you can create telemetry. And so that whole thing just led down a rabbit hole. And so
basically, the TLDR is that frustration is what happens when you are trying to achieve something
and you can't get to it. And so what you can do is every time your autonomous agent tries to achieve
a thing and fails, that adds a counter. And every time it, you know, tries something and succeeds,
that takes one off the counter, or maybe you have different counters. So frustration is when
the failures to successes is too high. And when the failures to successes is too high,
that can be a sign that you've got the wrong approach, that you're using the wrong tools,
that you're not capable of something that you need to back out that you need to ask for help.
So that's the whole point here is that for your autonomous and semi autonomous agents,
you'll probably need to build in a frustration signal, which will allow it to know when it is,
like when it's not capable of doing what it needs. And it can either come to you and ask for help,
or it can try a different model. So one thing is model selection is is a big thing that's coming
up. Because GPT four is much more expensive and much slower than 3.5. So if you can do most tasks
with 3.5, it just makes economic sense to do so. It'll be cheaper and faster. But imagine that you
get to a point where 3.5 is just not cutting the mustard. So that your frustration signal goes up,
which means that you say, Okay, let's bring out the big guns, right? Let's bring out GPT four,
or in the future, GPT five or whatever. And then you you point a more powerful tool at the problem.
So that's a good use of the frustration signal. Good question. Let's see, would activity or let
me jump back over to Patreon. Let's see. Hey, Dave, just subscribe. Thanks for all your insights.
We're always been told that the military is a few decades ahead in terms of technology compared
to what's publicly available. What are your thoughts on what might be hidden in DARPA.
So that's interesting, because I have talked to a few people who say that various departments
in the or various agencies within an Department of Defense are like woefully outdated, and they
have like ancient GPUs that like can't be used for modern language models. That being said,
you also see in the news that the Air Force is building fully autonomous F 16s. So clearly,
there's some stuff going on that we don't know about. I had a I don't I want to respect people's
privacy. So I had a teacher once back in middle school, whose brother was in the Special Forces.
I won't say exactly when or where. But the stories that he would tell were like, back then, this is
during like, like the invasion of Afghanistan, where they had like, like night vision goggles that
were as small as like ray bands that could see in pitch black, which that technology is not even
publicly like, if you search, you can probably see it now. I don't know. This is hearsay. This was
like, you know, the teacher said that his brother took him to the barracks and showed him this could
have been total BS. But like, yes, so a friend of mine growing up, his dad had been a Navy SEAL.
And basically, what he said is, as long as as long as we know the engineering to make something,
the US military has it no matter how expensive it is. So if if something is is scientifically
possible, if it has been demonstrated in the lab that this works, then the rule of thumb is that
the US military has it. Now that being said, a lot of the AI stuff has just been proven in the lab.
So that's that means that like, they're going to have it soon, or, you know, it'll be scaled up.
Because basically, the idea is that for the US military, cost is no is no barrier. Anything
anything to get ahead. Now, of course, you look at like the Senate budget meetings and the hearings
and stuff. It's not quite that simple. But that's like a rule of thumb, retire to Riza in VR,
retire to Riza in Westworld with robots. There you go. And a follow up, how can we prevent
militarization of any AGI or ASI? Or is it just a pipe dream? Yeah, so basically, from a military
perspective, AI is just another tool in the toolbox. It's going to, you know, a lot of a
lot of future war is going to be in cyberspace. But still, you know, cyberspace doesn't matter
if you cripple the enemy's data center. So there's there's going to be drones, you know,
trying to drop bombs and stuff. So that's going to happen. And this is this is actually where
Nash equilibrium makes sense is because usually assured destruction with nuclear weapons was
a kind of Nash equilibrium. And so if, you know, adversary A and adversary B both have equal or
roughly equal AI capabilities, or there's enough room for doubt, then neither of them is going to
pull the trigger, hopefully. Excuse me. How do we get AGI? How do we get GPT to stop beginning
every response with as an AI? I tell it to go into Morden Solis mode. That actually works really well.
I say, you know, adopts adopt the Morden Solis speech pattern, you know, be very concise and
succinct and stuff like that. Okay, y'all are being silly. Let's come back over here 14 messages.
Let's see. We already answered that one. We already answered that one.
Yeah, let me scroll to the bottom. Do you think there are any good approaches for ACEs,
so autonomous cognitive entities to figure out their own abilities,
e.g. improve their own agent model? Yeah, so there was actually a few papers that came out where
we're by using a loop. So it was the it was the evaluation loop. So they can evaluate themselves
morally, they can evaluate their ability to use tools, they can teach themselves to use tools in
real time. So yes, they can already do that. It's just a matter of how you set up the prompt chaining.
Let's see. With the rapid advancement of AI, there's concern that some countries, particularly
those with limited resources, could be left behind. What's your perspective on how AI could
impact different countries? Yeah, so inequality is a major, major, major problem. And this is not
just going to be for developing nations. And in fact, one thing that I suspect might happen
is that developing nations that the quality of life for people in developing nations might have
a quantum leap forward. While for us developed nations where there's a lot of competition,
we might continue to be flat or even decline for a while longer. And the example that I give is like,
you know, you give a village in rural Africa, like Starlink and solar, and suddenly everyone
knows like they have, oh, like, hey, we have chat GPT now, we can treat all the all the village
ailments, because we have the equivalent of like a Western trained expert doctor, and engineer,
and electrician, right at our fingertips, right? So because of the relatively low cost of AI,
I think that it will positively benefit people in developing countries a lot more drastically
than it will us. But you're right that like, it is something to pay attention to, because that's
on a micro scale, on a macro economic scale, you know, countries like Ghana might not be able to
even afford enough compute power to run one instance of GPT three. That being said, I do suspect that
there's going to be international treaties that will ensure that people have access. And then,
of course, there's VPNs. Look at Italy, Italy tried to ban chat GPT, and then everyone in
Italy just use VPNs, right? Take a moment to breathe, you're doing great, and your insight
is invaluable. No, air is for wimps. Okay, I will build robot humanoids that are skinny, sharp
claws, tall, pale, and have dark, sunken eyes, and she'll release many of them into the force of
Canada to give people the greatest scare of their lives. Is that what your avatar is there,
Ant King? Is that what you're building? That's kind of terrifying. Okay, what kind of legislation do
you think the US is capable of making? I'm concerned about the age of our leaders and their peers
coming from time so out of touch with today's rail. So yes, we have a gerontocracy. So gerontocracy
is ruled by the old. That being said, they all have teams and teams and teams of advisors.
They have hundreds of advisors. And I guarantee you, I actually know this because one of my
Patreon supporters told me that in the State Department, they use chat GPT all the time
to talk through stuff. And so you bet your biscuit that every senator, every congressman
in the executive branch, legislative branch, judicial branch, all of them are using AI to
help them do their jobs. With any luck, it's helping them to do their jobs better and more fairly.
Now that being said, the United States is a purely reactive system where we abide by civil law,
which means that the law is there and then the courts set the precedent. And then we're very
kind of slow and the legislative branch is slow by design, whereas in Europe, they're much more
proactive. And I swear, I cannot remember the name of that paradigm. Let's see, what do you think
there's something special about phenomenal consciousness that is simply cannot work with AI?
So Steph and I addressed that earlier that the real quick version is that
the acquisition of language seems to be really important for the development of human consciousness.
So it's entirely possible, I don't know how likely, but it's possible that since we're
teaching machines language, that could be the genesis of phenomenal consciousness for them.
It would be really cool. Greetings from Brazil. Hi, Brazil. I would like to thank you personally
for the video about burnout. The content was very useful and enlightening. Thank you. You're
welcome. Yeah, I actually have, I keep, I've recorded like three videos for my for my life
channel, and then I delete them, or I never post them because like it just doesn't feel right.
So I apologize. Let's see. Where are we at? This is less serious, but I'm curious if you've seen
her and your thoughts on it. Yeah, so I mentioned, I mentioned companions quite a bit, and that'll be
coming up actually on Sunday's video, not her specifically, but companion robots. I'll be mentioning
those again. And I also mentioned in last week's video, talking about when I got to the part about
like, how are we going to live if we have like perfect companions? So go check out last week's
video too. Nanobots and our blood will keep us from getting sick, making us basically immortal.
What do you think we'll, when do you think we will have such technology? So from last week's
video, the immortality video, I think that we're on the longevity escape velocity trajectory right
now. I think that as long as you're in decent health today, and you have moderately good access to
healthcare, I think that you will easily live to see those things. Now that being said, it's
definitely impossible to predict exponential growth and compounding returns, unless it's like,
you know, just your retirement portfolio. So it could be next year, it could be by 2030. I would
be surprised if it doesn't happen by 2030. And I know that's a super controversial opinion,
but that's really weird. Why the people seem to have a death wish. Why for people who want to
get sick, who want to believe that, that longevity is not possible. Why? Okay.
Would the ideal society be as the society governed by AI? I think that governed by is not the correct
thing, but I think managed, managed by or managed with a lot of help from AI. Yes. But
governance, I think, should probably always be with consent and by consensus. Now that being said,
you know, with blockchain technology, with DAOs, every human and our AI companions can be
stakeholders in a DAO, which means that if the, if the whole, imagine a future where the entire
planet is run by, by a global DAO, then there's no reason that it can't be governance, governance
by consensus with the aid and facilitation of AI. That's what I hope to see. Let's see,
is there any additional structural context that should be built around the heuristic
imperatives for practical implementation? Yes. So the short answer is that whatever context
makes sense for any agent, if it's fully autonomous, if it's your personal assistant,
you can put it into the task manager, you can put it into its constitution,
if it's part of a blockchain, you can put it in the consensus mechanism for the blockchain,
that sort of thing. Let's see, in regards to developing countries using generative models,
seems like almost seems almost like the spread of a religion. If you think about it in the context
of geopolitics, use our model, their model lies, etc, etc. Seems like parallel to religion
spreading. I'll say yes, but there's a lot of competition coming up. And especially for
developing nations, they're going to go for whoever's cheapest. And in fact, most nations
are going to go for whoever's cheapest. And I would, I suspect that OpenAI's business model
is not the most efficient model. So I think that they're going to be undercut just on,
on scale alone. Let's see, let me jump back over to Patreon. It has also been more than an hour,
so I'll probably be winding down. Stop asking it how to build a bomb. Yeah, don't do that.
Okay, looks like, here we go. Will the Westworld episode be about the MIT and Google study
regarding generative agents? No. Next question. I'm not going to give you spoilers. I've already
given you too many. Let's see, do you think the experience of quality and the experience of ping
pong, ponging emerge for these neurons? Yeah, so this, this is a good, good question. So if you
take several human neurons or rat neurons or whatever, and put them in a robot, and like zap
them or reward them with sugar or whatever for their behavior, is that the equivalent of like,
like whipping someone in order to get them like, at what point does consciousness emerge?
Because here's the thing is, if you make the assumption that a soul is required for consciousness,
then you say, okay, well, that's not a full rat. And rats don't have souls anyways. So,
you know, 50 brain cells is not enough for suffering or qualia of experience. Ditto for humans,
like, okay, well, you know, if a human isn't alive, then they don't have qualia, they don't
have phenomenal consciousness, so on. Now that being said, another aspect is like, okay, well,
if you don't know when consciousness starts or ends, how do you know maybe the entire universe
is conscious? Now, a counter argument to that is that you can have a you can be you can be alive
and have a functioning brain and still be unconscious, right? Drink too much alcohol,
you go unconscious, you go to sleep, you go unconscious. So just having a complete and
functional brain itself does not confer consciousness, which makes me think that
consciousness is actually an energy pattern, and that you need an energy pattern that is
sophisticated enough and well organized enough in order to have the qualia to have subjective
experience of being. So yeah, let's see, I remember you were working on a paper about the
laws reduced suffering and so on, has that has it involved further? I think you mean evolve
further. There are so both of those papers are up on on my GitHub, there's two of them. But also,
people watch my videos more than they read, so I just focus on making videos.
What kind of robots would you want for yourself? That's a really interesting question, like would
I want a sexy cat girl like robot? You know, I used to watch anime back in the day, so like I
kind of lived in that world and thought like this would be great. So I don't know. I do think that
I would I would like to have an embodied version of Raven my, you know, my, my autonomous cognitive
someday. But even then, I think that I think that the embodiment would only be just like,
help me do stuff like, hey, let's go on an adventure. I did have a thought experiment
the other day of like, wouldn't it be cool if you live in a house where it's like you and a few
humans, but then you have like a nearly equal number of robotic companions. Some of them are
going to be like obviously robots, but some of them might be like biomimetic. And it's just like,
like, yes, they're built to be your friends, but they still have their own some of their own
intrinsic motivations, whether it's the heuristic imperatives or something else. And then like
your life would just be so rich by by having these companions around you at all times that are
completely inexhaustible, right? They're always going to be patient. They're always going to be
helpful. But you see them as peers is equals. I think that I think that that is possible and
probably going to happen. But it's such an unsettling thing because it's like, what if half of your
friends are not human, right? What if half of your friends could like fold you into a pretzel if
they wanted to like data, right? And actually, I think commander data and the droids from Star Wars
are probably the best example because data was a member of the crew, even though he wasn't human,
but he wanted to be human. So I guess I would say that like, I want to have a commander data.
How long until age reversal 2030? Let's see, do you think we have any accurate way to measure
consciousness of AIs or LLMs? My best guess is consistency when asking it to design its own
avatar. Mathematically, I don't think that that because there are people that have done that.
But I think that it won't be until we have really sophisticated brain computer interfaces
that allow us to measure our own consciousness and also see if we can measurably project our
consciousness into machines. Until that happens, I don't think we're going to have any way of
telling one way or another. All right, last check on Patreon, and then I'm going to call it a day.
What's the Discord link to cognitive AI labs? I took it down, but it's posted on Reddit. So if
you go to the artificial sentience subreddit, the link to the cognitive AI lab is there.
Last question. The question about dying and immortality and gerontocracy, also making room for
a new generation of people is a better idea and morals disclaimer. I have children. Oh,
that wasn't a question. Okay, p temple. Do you got one last question for me? And then we'll call it a day.
Anybody? Bueller. Does BCI, let's go on an adventure to the hot tub,
hot tub time machine. Let's see, does BCI change significantly the predicted outcome of what
super intelligent AI brings in terms of dangers and benefits? Is it true the singularity moment
for us? We have no idea. So I don't know. The thing is, is, you know, the current like neural
link, it's got like what 1000 or 10,000 nodes. But when you have a brain with 100 billion neurons,
that is still a very, very, very narrow amount of bandwidth. So, you know, I predict that we're
going to have like neuro polymer membrane membranes that allow for like, you know, terabits of
communication per second in and out of the brain. Eventually, that would be a different thing. But
again, we're going to get there through incremental steps. What do you think about Altman said that
age of giant A models being over? I think it's premature to say we'll see. Let's see, he found
it. Okay, cool. All right, I think we're just kind of devolving into just general conversation. So,
oh, it is in the description. Okay, cool. All right, gang. Well, it's been a lot of fun. As always,
I hope you all got a lot out of this. So I'm going to call it a day.
