start	end	text
0	10000	going live. All right, I think we're here. Can y'all hear me now? Can you hear me now?
10000	26960	Hello? Is this thing on? Okay. It says live in 15 seconds. Okay, we're good. Yay! Okay,
27840	36400	make sure to mute myself. All right, so hello everybody. I'm still figuring out the whole
36400	41840	live streaming thing, so please bear with me, but one thing that y'all said was that you want to
41840	45760	like see what I'm looking at, which you know, it's a good idea. You don't want to just look at my
45760	50560	face. It's like, let's look at stuff together. So I figured I'd follow more or less the same format
50560	57760	that I usually do. I'm in the cognitive AI lab discord, so if you're in the general, you can
57760	66400	drop questions here or papers. So basically what I thought we would do is we would take a look at
66400	73200	like weekly updates, because it's all going really fast. And you, I've got a lot of feedback from my
73200	78800	recent live streams, that y'all really liked the interactive aspect of like asking questions,
78800	84160	and like Dave, what does this mean? And so please feel free to drop good questions in. I'm also in
84160	92080	the private Patreon discord, so drop questions there too. Okay, cool. Party time, yeah. Oh,
92080	99360	looks like there's a delay of about 15 seconds. Interesting. Okay. Cool. Flippy from the dais
99360	107280	discord. Hello Flippy. All right. Cool. So while some questions get spooled up, I figured I'd go
107280	113760	through a couple of tools and stuff that I've seen. So someone, I think actually Flippy, this
113760	120000	might have been you or someone in the dais discord, pointed out this tool. So this is basically
120000	126240	something that I and others have been like thinking about and talking about. It's basically a hybrid
126240	134400	of vector database and knowledge graphs. And it's also got a pretty interface. So
135360	141680	like, it's worth just scrolling through and like reading every every bit of this. And then kind
141680	148720	of experimenting with it. But but it will create. So this is one of the coolest things is it'll
148720	154800	create a knowledge graph of all the clusters and stuff. But you can find the gaps, which when
154800	161360	you're thinking about autonomous or semi autonomous AI agents, this is really good because then
162000	168240	you can know what you don't know. And what I mean by that is if you're aware of all the things that
168240	174080	you know, but you can detect some semantic gaps in your knowledge, that can help you zoom in on
174080	180080	the things that you need to go learn about. So in front notice is the tool in front notice.com.
180080	186000	I can never remember the name of the darn thing. I'm going to blame allergies and say that it's
186080	192960	just brain inflammation. That's my excuse and I'm sticking to it. But yes, so this is a super cool
192960	204960	tool. This kind of technology will definitely be part of of like autonomous AI agents. And this is
205600	211200	functionally similar to what I worked on with Remo. Remo is much, much, much simpler though.
212000	217760	So I often have some people ask me like, Oh, hey, can Remo do this? And that and I'm like, no,
217760	223360	like, Remo was meant to solve one very specific problem. So for memory stuff, I usually point
223360	231440	people at llama index and chroma db. So chroma db. Do I seriously not have that bookmarked?
232320	238400	So chroma db is a vector database that runs just like SQLite. So it's pip install chroma db,
238480	244640	you create a local client just like you do SQLite. So I tried to create something like this called
244640	249600	VDB light like a few months ago. And I quickly realized that I was in way over my head. So I'm
249600	255760	glad someone built this. And they just got an $18 million seed round. Holy mackerel. Man,
255760	261360	I should have stuck with VDB light. I could have had an $18 million seed round. Anyways, maybe I'll
261360	270080	do that with Remo who knows. So chroma db, super simple man, seriously $18 million for this one
270080	276960	thing. Okay, I'm gonna let that go. llama index also. So llama index, I kind of didn't pay it
276960	281440	any any attention at first because it's like, Okay, that's a silly name. This was clearly just
281440	287120	someone's little side project. But if you look at all the types of indices they have,
288080	294560	they've got list index, table index, tree index. So Remo is very similar to llamas tree index.
294560	299680	Although I will say that I look through the code. And I think that their tree index is kind of basic.
299680	306160	I think that my Remo framework does a little bit that theirs doesn't. But I'm not going to dive into
306160	309920	that because I don't know that for certain. I didn't take a super close look at the code.
310240	317600	Yeah, but so those are those are some memory storage tools. So let me check on the live stream
318240	323600	and see where we're at. Whoops. All right. Oh, wow, we got lots of questions. Okay, cool.
324320	329280	God's not dead rather believe and go to the good place and interesting. Okay.
331680	335840	Let's see how to discover new wisdom from LLM. That's an interesting question.
336400	340000	Um, let's see. Let me,
343120	348400	you people ask questions far too early. Let me go over to discord to see if,
351280	359040	yeah, please go ahead and drop some questions. Patreon get first dibs.
359040	368640	All right. What do you think is the future of SAAS sales jobs for recruiting agencies?
370320	379200	Oh, sales jobs and for recruiting agencies. So I mean, the sales level is still very human.
379200	383360	So sales is not going to change for a while. Ditto for recruiting, although there's a lot
383360	388880	of AI and recruiting already where like AI will read your resume and AI will watch a video of you
388880	395840	answering questions. You know, more and more of that's coming, but that's about it. Let's see.
395840	401760	All right. We got a whole bunch of questions coming in on the Patreon side. So, yeah, database,
401760	408640	short answer, diversify your job skills. So there's that. Let's see. Zoom in a little so you
408640	415120	guys can read this. Let me jump back over here real quick. But do you still expect AGI within 18
415120	419280	months was stating they do not want to make models bigger but rather more efficient? Yeah,
419280	426960	I think AGI in 18 months is still conservative. Honestly, I think that we will have,
428320	433680	as people develop the architecture for autonomous AI, I think that we will be able to say that we
433680	439040	have AGI by the end of this year. But people will realize that it's like, okay, we have an agent that
439040	443760	can do anything, but it's expensive or it's slow or it's kind of dumb, that kind of thing.
445840	452000	Let's see. Check on Discord real quick. Is her about to become reality? Yeah, lots of people
452000	462480	are working on AI companions. They're going to get more sophisticated real fast. I've actually got
462480	468160	a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday.
468160	472320	I've got a Ghost in the Shell video planned. I've got a Mass Effect video planned. I've got
472480	478320	a Dow and Blockchain video planned. So that's all what's coming. Maybe I shouldn't spoil it. Okay,
478320	485760	well, whatever. But yeah, so I was thinking about hitting on her and Ex Machina and stuff as well.
487280	490960	Let's see. How can we be certain things are advancing exponentially?
492480	500320	So that's a good question, James. Generally speaking, you can't tell if you have a narrow
500320	506240	window, but we were joking around the other day and we're pointing out that like a few weeks ago,
506240	513040	it was like you would reasonably expect a couple of cool AI bits of news per week,
513040	518560	and now we're at the point where we expect several per day. Now, it might come in cycles,
518560	526800	it might come in waves, but generally speaking, this very closely matches what Ray Kurzweil
527760	534720	said, or maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago.
534720	540720	I think it was Michio. He was describing what it will feel like to approach the singularity,
540720	547040	and he said, oh, well, when information is doubling every two years, you don't really feel that on a
547040	554080	day-to-day basis. And then when information is doubling every six months, that's fast enough
554160	558640	that you're like, oh, hey, this thing that I didn't think would be solved for another couple years
558640	565280	was solved this year. And then, but as it ramps up faster and faster, that time keeps having.
565280	572240	And so then three months after that, you realize, oh, wait, we've advanced again. And I think we're
572240	578080	right at that, like, that three, you know, where six months ago, we're like, oh, this stuff is 10
578080	582640	years away. Six months later, we're like, this stuff is 10 months away. So I think that where
582640	587040	you could make a good argument that we're in some respects, we're in the exponential ramp up right
587040	593360	now. That being said, some of this information is so big, and it's changing so fast, it's difficult
593360	598960	to measure. We'll actually need AI to measure the rate of papers and tools. All right, so Seaf,
598960	604320	you said, how do you think AI will impact religion, particularly monotheistic religions? I think it
604320	609360	will create a mass crisis of consciousness, which will make the transition period even more chaotic
609360	619120	and extreme. Yeah, no, sorry, Seaf, I was getting around to it. Yeah, so I've actually had some
619120	624080	interesting internet debates with more conservative and more religious people. Granted, I don't do
624080	628560	internet debates anymore. I got that out of my system. And I try not to get suckered into it,
629680	637680	if I can. But, you know, one debate that I had many years ago was, you know, if aliens showed up,
637680	643360	wouldn't it prove your religion wrong? This was a debate. I wasn't arguing as a debate that I
643360	650240	observed. And the religious person said, no, why would it? And, you know, they rationalized it,
650240	656640	saying, well, you know, why would God put, you know, aliens in the Bible if we wouldn't be able
656640	663600	to understand it back then? It's not for us to understand. And so some religious folks do have
663600	670560	a really good ability to compartmentalize. And so, like, just because you have a, like,
671920	676720	like a super intelligent machine, some people would be like, so it doesn't have a soul. And
676720	682160	that's the end of the discussion. So I don't, I don't particularly perceive, and I'm not saying
682160	688000	that this is good or bad, right? I am not in the Judeo-Christian faith. I have my spirituality
688080	694160	as other, other places. I have some, like, some of my best friends in my local community and my
694160	699600	internet friends are deeply religious, you know, followers of Christ and whatever. And so, like,
699600	703120	in many cases, I don't think it's going to be that big of an issue. Let me jump over to the
703120	709600	Patreon. Oh, wow, we've got some questions here. Okay. Backo bbzo. Sorry if I'm saying your name
709600	721280	wrong. What would your advice be to someone just launching an AI startup? Don't. That's,
721280	727760	that's a very flippant response. But one launching a startup is really hard. It's mostly tedium.
728640	732720	You can have the best idea in the world. And 90% of the work is still going to be,
734800	739040	excuse me, I'm still struggling with allergies. That's my head a headache earlier. That's part
739040	744640	of why I canceled yesterday. Thanks, Jordan. That's how am I holding up? Actually, I'm doing okay.
744640	750240	Mostly it's just allergies right now. But anyways, so if you're doing an AI startup,
750240	756880	one, if you haven't done a startup before, now is a really bad time to learn. Because things
756880	765520	are going so fast. And we're basically having to reinvent stuff as we go. Which if that's, you
766160	770880	that's part of why I burned out is I realized like, okay, I did find it really engaging and
770880	778640	really enjoyable. But my pace of things clashed with other people. And then like the rabbit hole
778640	787120	just keeps getting deeper. So that's, that's kind of the thing. What I always tell people is your,
787120	792880	your, your startup team, your founder team is most important. Be picky. If you don't have the right
792880	800560	team, walk away early. And then also for the folks that I'm talking to is local. Because of the pace
800560	806880	of things, you absolutely need to like see the people that you work with, in person, at least
806880	812560	on a weekly basis, if not on a daily basis, because you need to be like sitting in the same room,
812560	819600	just shooting the breeze, keeping each other updated in real time. Doing it remotely is
819600	824960	probably not feasible. Unless unless you're already a really established team, and you're
824960	830480	just going to sit in like discord all day or Slack all day. Blake Allen curious to hear your
830480	834240	thoughts on Stanford's hyena hierarchy and how it relates to some of the work you've done with
834240	838880	Raven and cognitive architecture. I don't know if I've heard of this one. Let's check it out real
838880	852320	quick. Hyena hierarchy. Let's see. Let's go up to the very top. Hyena hierarchy towards larger
852320	858800	convolutional language models. We're excited to share our latest work on hyena, a sub quadratic
858800	864080	time layer that has the potentials to significantly increase context length and sequence models.
864400	874640	Oh, right. I think this is the RNN integration. Yeah, yeah. In general, how can we close this
874640	880480	gap? Yeah. In general, any individual language model is just like one cortical node.
883600	888960	Yes, these things will be like better, more efficient cylinders in an engine, but in order
888960	897120	to have a race car, you need the rest of the car. Again, I'm kind of really flying off the cuff
897120	906240	right here. I'm not sure that I've got this right, but I think, yeah, RWKV. You're not going to ever
906240	911440	get a full cognitive architecture from a single language model. Now, that being said, the big
911440	917280	asterisk is when you look at all the studies about GPT-4 that have theory of mind and what I
917280	923040	call implied cognition. So implied cognition is that the thing is obviously thinking through
923040	927920	problems behind the scenes in a similar way that humans think through it. I don't mean like
927920	935280	neurologically, subjectively, it thinks the way that we do, but GPT-4 can obviously talk itself
935280	943520	through, kind of do chain of thought reasoning internally in one shot. And so that makes those
943520	948320	larger, more sophisticated models make your cognitive architecture simpler, but it doesn't
948320	954640	get rid of the need for external storage. It doesn't get rid of the need for parallel processing.
954640	959280	It doesn't get rid of the need for loops and checks and that sort of stuff. So that's kind
959280	965920	of my response there. Good question. Emma or AMA, I'm new here and new to this field in general,
965920	971120	found you through Raven videos. Thank you. Regarding personal assistance, is there a reason
971120	977280	to create a database of yourself for your future personal assistant to understand you better?
978240	984880	So that's actually the purpose of my RIMO framework. So RIMO is meant to be a hierarchical
984880	992000	database of your interactions with an individual agent that will surface particular topics by
992000	1001920	using, not reciprocal, recursive summarization and clustering. So you take all your raw logs,
1001920	1008240	cluster them, summarize them, do that again, cluster, summarize, cluster, summarize until
1008240	1014720	you end up with five to 10 parent topics that allow you to drill down. So I wouldn't, don't waste
1014720	1020000	any time doing that manually, just let it happen naturally through conversation by integrating
1020000	1026240	something like Lama Index Tree or RIMO. Do you think we are on track to cure brain diseases
1026240	1034640	like Alzheimer's by 2030? The combination of AlphaFold and mRNA vaccines, I think absolutely.
1035280	1039520	There was something else that I posted on my YouTube recently that it's like another breakthrough
1039520	1048960	is happening. So I think we're very close to the point where we can halt Alzheimer's. Undoing
1049040	1054560	Alzheimer's might take another little bit of time, but on the other hand, we're at the point
1054560	1059360	where we're getting saltatory leaps, we're getting breakthroughs really fast, so you never know.
1060320	1066160	Let's see, what are your thoughts on the generative agent stuff that has come out recently? It seems
1066160	1072320	like you were pretty ahead of the curve on that stuff and has it solidified or changed the way
1072320	1078640	you think about the concepts from Symphony of Thought? Yeah, so I definitely felt like I was
1078640	1084640	ahead of the curve. And what I've been telling people is I worked for a few years to try and get
1085440	1091760	GPT-3 to do the stuff that 3.5 and 4 can do easily. So I'm just like, all right, whatever. I'm so
1091760	1095680	glad that the rest of the world is just like, oh cool, autonomous agents. And I'm like, great,
1095680	1101440	now I don't have to write any more books about it. So I'm just happy to sit back and watch it go
1101440	1107600	and keep plugging my heuristic imperatives. Good question, Jordan. Also, here, let's check over
1107600	1113680	here. Okay, we got some questions. Let's see, is her about to come reality? Yep, we got that one.
1114720	1119440	Interesting video. How long do you think it will be before we start seeing hive mind AI systems
1119440	1130400	in healthcare or the IRS? I know people working on that today. And so they'll work together
1130400	1135760	for a few different reasons. One, you'll have a division of labor. Oh, so taking a step back.
1136480	1141600	What we mean when we say like hive mind AI is where you have like multiple cognitive agents
1141600	1150400	or autonomous agents, or is it is it not working? Is it working? I hope it's working. It looks like
1150400	1159520	it's working. Okay. So yeah, so basically, it'll be easy to spin up a lot of agents.
1160240	1168560	What I was describing to one Patreon customer, or no, sorry, I was describing this to a
1169600	1175200	podcast host that I'm going to be featured on coming up, was kind of what I predict right now
1175920	1181440	is before too long, you're going to have multiple cognitive agents running on your phone, on your
1181440	1188000	car, on your home PC and your smart home devices. And so you're basically going to have a fleet of
1188080	1196960	small cognitive agents working for you at all times. Then you're going to have the same thing at
1196960	1201280	like your company, right? Every employee or every department is going to have multiple
1201280	1207200	cognitive agents all collaborating at all times. And you're going to have this kind of tiered hierarchy
1207200	1211600	where it's like there's the personal, there's the family unit, there's the corporate unit,
1211600	1216400	there's the town, there's the federal government, the state government, global government,
1216480	1220080	government. And I think that the way that they're all going to work together, because
1220080	1224720	security is so critical here, is that it's going to be using blockchain technology and
1224720	1230800	distributed autonomous organizations. So that's the long story short, is that's what's going to happen.
1232400	1236240	Let's see, you may have already covered this, but in case you haven't any thoughts on Met
1236240	1243120	Singer's artificial suffering, an argument for a global moratorium on synthetic phenomenology.
1243600	1251360	Um, I am tangentially familiar with this, but I have my own opinions on whether or not a machine
1251360	1259520	can suffer. So there's two distinct possibilities. The first possibility is because artificial
1259520	1265120	intelligence is a fundamentally different substrate from humans, it will never be able to
1265120	1271520	suffer. Like it didn't involve nerves, it doesn't have pain centers, so on and so forth, they can't
1271600	1279600	feel lonely because it's not a social entity, so on and so forth. Now that being said, language,
1280240	1285360	the acquisition of language is actually critical for the development of human consciousness.
1286720	1292000	So for instance, Bruce Willis, who has aphasia, aphasia means that your ability to use language
1292000	1297200	gets destroyed. Aphasia actually kind of erases your sense of consciousness.
1297760	1305120	Um, and then, uh, in the case of feral children, um, feral children, when they, some of them who
1305120	1309840	have learned language talked about how their consciousness and their understanding of time
1309840	1317920	in themselves changed as they learned language. So if you extrapolate that to language models,
1317920	1325360	it is possible that, that there is something informationally almost magical about the acquisition
1325360	1332240	of language that confers consciousness, that confers subjective experience of being. So that
1332240	1338320	could be that language models are actually the first AI that have subjective experience, that have
1338320	1346480	a coherent, um, sense of being. And this is, so there are, um, there are some religious and
1346480	1352160	spiritual frameworks that kind of discuss stuff like this, um, particularly, uh, what's the name
1352160	1358640	of the, the, the creator deity in Tolkien's world, where the fundamental substrate of reality was
1358640	1364160	music, right? But maybe the fundamental substrate of consciousness is actually language. Um, so we
1364160	1373600	don't, we don't know yet, but that's, it's a possibility. Um, let's see. Uh, so Parkinson,
1373600	1379760	so the follow-up question was, or here, let me check the Patreon real quick. Um, let's see,
1379760	1383440	do you think there needs to be another breakthrough for AGI? What is your personal take on this?
1383440	1390560	Do LLM suffice? Um, yeah. So I would say that, that our current trajectory, as long as the
1390560	1395840	trend continues, we are on track for AGI. Um, people are going to continue debating about AGI
1395840	1402160	forever though, until the cows come home. Um, which is why I keep saying like autonomous cognitive
1402160	1407760	entity, ACE, or just autonomous AI, because it doesn't, you don't need AGI. You don't need some
1407760	1415440	arbitrary magical boogeyman. All you need is like an AI system that is self-contained and
1415440	1420560	autonomous enough to be useful or dangerous. Um, and then the question is, how many do you have?
1420560	1424080	How fast are they? And how smart are they? And they're going to continue to get faster,
1424080	1428640	cheaper, and smarter over time. So it's like, okay, we're there. It's just a matter of
1429440	1433760	how does, how does the trend line ramp up, right? Cause it's kind of like, um, when the
1433760	1438080	Wright brothers first created the Wright flyer, right? You know, it's like, okay, you had to
1438080	1443120	start it by hand and push it, you know, down a track and it flew well, like 200 feet or 300 feet.
1443120	1447280	And people are like, ah, whatever that won't be useful. But then 50 years later, we were flying
1447280	1453680	to space, right? So we're at the beginning of the ramp up of, of the era of AGI. And yeah,
1453680	1457920	right now they're like idiotic little toddlers, but in a few years they're going to be like
1458560	1464480	one all over the place and two really powerful. Um, good question. Let me come back over here.
1465680	1470000	Um, you should do a whole episode on aphasia and consciousness. I actually don't know that
1470000	1474960	much about it. Um, but if you're interested in the topic, I recommend phantoms in the brain,
1476080	1482960	by, um, VS Ramachandran. And also what's the name of his other book? Um, it's something like
1482960	1491280	the pursuit of what makes humans human. Um, okay. Uh, let's see. So Parkinson's is a neurodegenerative
1491280	1496560	disease, which I think means that it's autoimmune or it's a, or it's a defective protein. Um, but
1496560	1501360	also Alzheimer's is a defective protein. So while these diseases seem very complicated,
1501360	1506240	the fundamental mechanisms are actually relatively straightforward. Um, and I know that there's
1506240	1510960	probably a bunch of researchers that are going to jump on me for that. But, um, like plaques that
1510960	1516160	accumulate on the brain for Alzheimer's in most people, those plaques are cleared out. So then
1516160	1520160	it's just a matter of figuring out like, okay, why? Um, and then of course there's confounding
1520160	1526880	factors like things like your, uh, gut inflammation, uh, microbiome and other things affect Alzheimer's.
1526880	1531360	But that's because of the gut brain access. And again, I don't want to oversimplify because if
1531360	1537360	you look up like human, uh, metabolic pathways, there's like 200,000 unique proteins and enzymes
1537440	1542880	in the body with built literally billions of combinations of reactions. So I might be,
1542880	1547120	it might sound like I'm oversimplifying, but I'm, I'm, I'm not saying that it's that simple. I'm
1547120	1553520	just saying that, that the, that the, the key mechanism for most diseases is relatively simple
1553520	1557840	once you understand it. And we're getting close to that understanding. I guess that's the short
1557840	1562480	version of what I'm trying to say. Um, let's see, when you explained why you canceled the
1562480	1565920	OSS Raven project, you mentioned that there were some fundamental things missing.
1566640	1572000	Um, can you say what was missing and what might change your, or what made you change your mind?
1572560	1580560	Um, so my open source Raven project was like just before Lang chain and, and auto GPT and,
1580560	1585200	and all those other things came out. Um, and so as those came ramped up, I was like, I don't
1585200	1591120	really feel the need to continue. Um, but from a, from a social and organizational perspective,
1591120	1596480	the biggest thing that was missing was gatekeeping. Um, I, I basically, I created a community that was
1596480	1601600	really good at discussing stuff and not doing stuff. Um, and that's not anyone's fault. That's
1601600	1606800	if any, if there's anyone to blame, it's me. Um, just because I was like, I was so focused on
1606800	1610720	consensus and not just like, okay, let's just get stuff done. And then I see these other folks
1610720	1615680	that are just getting stuff done. And I'm like, okay, I'll just pass the torch. Um, let's see.
1616560	1623440	I think if we gave GPT for Scarlett Johansson's voice and a robot body, the masses will begin
1623440	1628400	to realize how close we are to AGI. Yeah, that's one way of putting it drink some water.
1632960	1637920	Let's see at what point do creating NPC and using autonomous AI like auto G auto GPT
1638640	1643680	and the likes become immoral, especially if you put them in games like GTA. I don't know that,
1643760	1651040	that it intrinsically does. Um, you know, not intrinsically immoral, but like certainly with
1651040	1659760	any technology, you can do it dangerously. Um, let's see. I've been curious about the future of
1659760	1666080	entertainment. Oh, sorry, let me jump over to Patreon real quick. Let's see. Do you have an
1666080	1670560	overarching roadmap of how to ensure the successful propagation of the heuristic
1670640	1675760	imperatives? If so, what can we all do to help you get to your milestone? That is a great question,
1675760	1682080	Blake. Um, so you're actually looking at it. So my number, my number one thing is my YouTube
1682080	1688880	channel. Um, because like, yeah, I've got enough expertise and, you know, IT and systems engineering
1688880	1694160	and enterprise. I've demonstrated enough understanding of language technology and AI and
1694160	1698720	cognitive architecture that I've got at least a little credibility. Um, certainly if you read
1698720	1703840	all the comments on YouTube, some people, uh, don't believe anything that I say and that's
1703840	1709520	fine. That's the internet for you. Um, but anyways, so basically step one was YouTube. That's why I
1709520	1714720	started my YouTube channel is because I realized that I needed to propagate my work. Um, step two
1715520	1721840	is, uh, teaching people. Um, and so by teaching people, that's like, you know, I've got a few
1721840	1728880	papers. I've got some code demonstrations. Um, I work with my Patreon, uh, supporters. Uh, I work
1728880	1734320	with pretty much anyone who wants to, and then three further dissemination. So like the podcast
1734320	1738640	that I'm coming up on, one of the things that we're going to talk about is alignment and the control
1738640	1744320	problem. We're going to be talking about like Nash equilibrium, game theory, Molok, that sort of stuff.
1744320	1749520	And so just by having the conversation and propagating the idea, that's like step three.
1749520	1758320	Step four is actually my novel because, uh, actually most of what I came up with was, uh,
1758320	1765120	in terms of cognitive architecture, heuristic imperatives was done in part through explorations
1765120	1770720	and fiction. And so over the last four years, what I've done is I, I do some experiments
1770720	1773920	and that would inspire me and I'd go write more of my novel and then I'd, you know, get
1773920	1778880	tired of my novel and do more experiments and I'd go back and forth until one, my novel took on a
1778880	1784400	life of its own, but also my research took on a life of its own. Um, but there's a video that came
1784400	1794720	out recently called, let me see if I can find it. It was, um, like why we need utopia. Um, here we go.
1794720	1801120	It was our changing climate. So this is a little bit of a, um, uh, I don't agree with everything
1801120	1806880	that this, that this, uh, you, this channel says, but it will make you think. Um, so this video,
1806880	1813280	why we need utopias, um, actually talks about how, how valuable stories can be
1814000	1821040	in communicating ideas because stories are naturally how we communicate philosophy and morals.
1822240	1828400	We don't need, we don't like philosophy, like capital P philosophy from, from universities.
1828400	1835040	That's, that's backwards throughout almost all of human history. We communicate our fears and our
1835040	1840240	desires and our values through stories. And so that's what this video talks about. And so when
1840240	1846160	you have nothing but dystopian cyberpunk stuff, you end up with people like Eliad Zyrcikowski.
1846160	1851360	Um, you know, yeah, I'm throwing some shade. But anyways, when that's all that you consume,
1851360	1856480	that's all you think, that's all you feel, and that's all you believe. So, um, my novel, which
1856480	1861760	I'm actually just about to finish draft 12 tomorrow morning, I'm writing the last chapter
1861840	1868960	and then I'm polishing it up, um, will illustrate, um, a lot of stuff, not just
1868960	1872880	the core objective functions or puristic imperatives. So that was a long winded answer.
1873840	1880320	Um, let's see. I think there is, um, are there key channels this training needs to go into,
1880320	1887680	organizations, governments? Um, I think, I think right now Blake, um, it's mostly just a matter of,
1887760	1894160	of dissemination, but also experimentation. So a lot of people, um, have experimented with
1894160	1898880	incorporating heuristic imperatives into autonomous and semi-autonomous stuff. And
1899840	1904640	most of them aren't sharing it yet, which that's fine. It's their prerogative. Um, but certainly
1904640	1907440	some people have reached out and said, like, yeah, this made everything easier. So I'm like,
1907440	1917280	great, just tell your friends. Um, let's see. Okay. Let's come back over here. Um, let's see.
1917280	1922640	I've been really curious about the future of entertainment. When we can use AI to generate
1922640	1927760	movies, games, et cetera. Uh, what will the entertainment industry look like? Movie trailers
1927760	1932560	and hyping up big releases for months will be irrelevant when AI can instantly create something.
1932560	1936880	If someone created a movie you didn't like, you just ask your AI to recreate it with an
1936880	1942000	ending or plot more suited to your tastes. What happens with content creators at that point forward?
1942960	1949120	Yeah. So I think that you're onto something. Uh, now that being said, it'll be easier for a lot of
1949120	1957280	people like you and me to create whatever film, TV, music, whatever we want, um, with the help of AI,
1957280	1961920	especially when you look at the text of video, um, which is improving by leaps and bounds.
1961920	1966480	You know, like I always, my, my go-to joke is we'll finally get season two of Firefly.
1966480	1970240	Who knows, we might get season two of Firefly by the end of this year. That would be great.
1970960	1975680	Um, now the problem there, it's not really a problem, but just taking that to a logical
1975680	1980000	conclusion. What if you have a million different versions of season two of Firefly? How do you
1980000	1985280	pick which one to watch? Right? You can look at ratings and stuff. Um, but then it also begs the
1985280	1992640	question of like IP, like is, uh, you know, 20th century Fox or whoever owns the IP for Firefly,
1992640	1998320	are they going to sue to have all of them shut down? You know, I, I like the dude from, uh,
1998320	2003200	from the movie, you can't stop the signal now. So I don't know what's going to happen there.
2003920	2009840	Um, but, uh, what I do think is that when you look at the fact that like people are already
2009840	2015280	using like Emma Watson's face for every mid journey prompt and, and whoever else, um,
2015280	2019920	I think that the crop of actors that we have today are basically going to be around forever.
2019920	2026480	Right? You're going to be watching, uh, Brad Pitt and Jennifer Aniston and, and, and Tom Cruz
2026480	2031360	for literally the next, like several centuries, at least until some actor comes along who's even
2031360	2037600	more compelling and whatever. Uh, and that'll be through face cloning, voice cloning, even, um,
2037600	2043440	you know, uh, nerfs, the, uh, the neural represent, uh, uh, representation. What was it? Neural
2043440	2049680	radiance fields, neural radiance fields. Um, we'll be able to like copy everyone. Um, okay,
2049680	2055440	could learning, uh, let me zoom in a little bit. Uh, could learning language and triggering
2055520	2060480	consciousness in humans almost replicate the same phase change when seen, um, or seen when
2060480	2065440	induction heads spontaneously form two plus layer models during training. Obviously there's more
2065440	2070480	to humans, but perhaps that's the mechanism. Uh, yeah, that's kind of what I was mentioning earlier.
2070480	2078720	Um, and I wouldn't be surprised if once language models get, uh, large enough if, um, if we do see
2078720	2082960	some more convergence. Um, that being said, I'm not going to say that that automatically means
2082960	2088320	that it has a subjective experience and that it is suffering, but you know, our brains evolved
2088320	2094240	over billions of years to be efficient. Um, basically efficient processors of information.
2094240	2100640	Who's, who's to say that if you have a, a biomimetic machine that it won't also converge on some of
2100640	2106560	the same properties and behaviors. Um, let's see, what do you think it will take to get for the
2106560	2112000	naysayers to get on board? The tone around AI seems to have shifted towards chat GPT and GPT-4
2112000	2118000	aren't anything special. Oh, you know, that, that always happens when the new shiny wears off.
2118000	2125200	Um, but the long-term economic impact of chat GPT has not been realized yet. And when chat GPT
2125200	2130320	and GPT-4 are on the ramp up, one, there's going to be a lot of competitors and two, there's going
2130320	2136560	to be incremental improvements and people are going to be like, uh, okay. The title, it's like,
2136560	2140720	it's like when you watch the, the tsunami come in and that just the water just keeps getting higher
2140720	2145920	and faster for like hours. That's what AI is going to feel like, except instead of hours,
2145920	2151680	it's going to be days and weeks. Um, let's see. I cannot wait when we can use deep dive tech
2151680	2155840	and have virtual realities. Will it also be possible to take super intelligent animals like
2155840	2163120	dolphins, dogs, parrots and crows and a deep dive and play with them. Um, I don't know that it would
2163120	2167680	be possible, but I certainly think it probably wouldn't be ethical. Now that being said, you
2167680	2176640	could have a virtual dolphin that is hyper realistic that you can play with. Um, fun thought,
2176640	2183840	will AGI want to see more stories from humans as a goal for itself? Uh, if, oh, so let, let me,
2183840	2191120	let me plug this. So Elon Musk went on of all fricking shows, Tucker Carlson and talked about
2191120	2198640	truth GPT. So what he said was that truth GPT would be a maximum truth seeking AI. Okay, great.
2199520	2205280	But after listening to it in closer detail, I realized what he was talking about was the third
2205280	2211040	here is to comparative was to increase its understanding or to maximize its own understanding.
2211040	2217600	So there's actually nothing that function on its own could lead to some really catastrophic
2218080	2223280	sources. But it's a step in the right direction. And I'm really glad that someone with as big of
2223280	2227360	a platform as Elon Musk is talking about maximize understanding or increase understanding.
2228560	2233920	So that being said, one of the things that he said in that interview was that as since humans
2233920	2238240	are part of the universe and AI that is curious about the universe will intrinsically be curious
2238240	2243200	about us as well. Now that being said, humans sometimes do experiments on things that we're
2243200	2247360	curious about. So maybe that's not the best thing. And in my book, benevolent by design,
2247360	2253200	I talk about why you don't why you must include suffering or something like suffering in the
2253200	2258080	objective functions of an AI, because there's three dispositions that an AI can have towards
2258080	2264320	suffering. One is it can ignore it altogether. So if Elon Musk gets his current idea, which
2264320	2269360	is just maximize for truth, that is an agent that ignores suffering, it doesn't care one way or another.
2270320	2275760	Then you can have one that increases suffering that deliberately increases suffering and we
2275760	2281760	absolutely don't want that. So that leaves by process of elimination, you want an AI that
2281760	2287440	reduces suffering. It's really that simple. Now that being said, I do agree with Elon that
2288160	2293840	creating a curious agent is a good idea because it'll want to know about us. And if you exterminate
2293840	2300240	humans, you have a hard time learning about them. So let's see, let me check on Patreon real quick.
2304080	2307520	Do you think that Elon Musk wants to be the Rupert Murdoch of AI?
2309760	2313360	Okay, I don't like that question. Lance, why you got to do this to me?
2314320	2323840	All right, Zadre, I'm not sure how to pronounce or Hadre. Okay. How do you envision the role of AI
2323840	2328480	in healthcare, particularly in areas like diagnostics and personalized medicine? What are
2328480	2333120	some of the challenges and opportunities in this field? Well, so there was that stand for doctor
2333120	2338400	who who already went on record saying that chat GBT for has better clinical judgment than
2338720	2348240	many doctors. So that is just a start, right? That's that's like starting point day one.
2348240	2354400	What happens when chat GPT five, six and seven come out that have better clinical judgment than
2354400	2361280	99.999% of all doctors on the planet, right? It doesn't make sense to go to a human doctor anymore.
2362000	2369040	Right? If the if the machine that costs $20 a month to run is better than all human doctors,
2369040	2375680	why go to a human doctor? Now that being said, there's probably going to be approvals and downsides
2375680	2380400	and gaps. And then there's still also the interface with the patient. And you have to have like
2380400	2387680	phlebotomists and nurses and and and physicians assistants to administer things, to administer
2387680	2392560	tests, you still need the you still need a lot of humans in there to to be the interface between
2392560	2398560	the human and the machine. But that being said, I think that we will get to a point very quickly
2398560	2405200	where the quality of care and the speed of care and the efficiency of care are going to go through
2405200	2411600	the roof real fast. That's what I'm hoping at least. Alright, jumping back over to cognitive AI
2411600	2416320	lab. Oh, we got some new questions. It looks like this was the same question. Sorry, I missed you
2416320	2425040	over there. Where are we? Alright, there's the deep dive. Do you think there is any major leap
2425040	2430320	missing to make truly practical autonomous agents? So for example, one who runs a part of your business
2430320	2436480	serves as general assistant, etc, etc. No, there's there's our there are countless hundreds,
2437280	2442320	if not thousands or even millions of people working on semi autonomous and autonomous corporate
2442320	2450720	applications today, right now. That being said, there's there's no there's no breakthroughs that
2450720	2455280	are needed, but there are still problems to be solved. So that's why like RIMO, you know, the
2455280	2461600	memory systems, and then standard practices like, you know, I wrote in Symphony of Thought and in
2461600	2467760	other places, my atom framework is once something is autonomous or semi autonomous, how does it
2467760	2473600	keep track of product or projects and tasks? And that's something that people are working on.
2473600	2479760	People are working on it real fast. That's coming really quick. Let's see, Nathan says,
2479760	2484320	I've been taking screenshots of when friends and family make fun of my hot takes. So I have the
2484320	2493280	receipts. I would say that I'm above being that petty. But yeah, thank you for keeping receipts.
2494080	2498960	Um, let's see, maybe directors will just design their perfect actors for each role.
2500240	2504320	So one thing that's going to happen is actually, so this is going back to like,
2504880	2510000	entertainment. I think that the next big generation of entertainment is actually going to be
2510720	2519440	holodeck style VR stories, where nothing is scripted, where instead it's like, you know,
2519440	2523920	basically you design a holodeck program the same way that they do in Star Trek, which is like,
2523920	2530480	computer, give me, you know, a Mad Max style story. But instead of, you know, post apocalyptic,
2530480	2536480	it's actually like space Western. So like, give me a mashup of firefly and this and,
2536480	2542480	you know, make the protagonist, you know, or the, you know, I'm the protagonist and give me a team
2542480	2547840	of like, you know, give me the sexy sidekick and the cyborg friend and whatever. And then just a
2547840	2552320	way it goes, right? Because you could plug what I just literally you could plug what I just said
2552320	2558320	into chat GPT and it can tell you a story. And I think that I think that VR makes the most sense
2558320	2565600	for the most immersive aspects of that. And, and then I think that because here's the other thing
2565600	2571280	is that technology changes the way that we consume art, but it doesn't really change art itself,
2571280	2577360	right? There are still stage actors, right, even though there's film and TV. There are still
2577360	2582400	symphony orchestras, even though I can just, you know, bring up Spotify and listen to the
2582400	2588160	same recording that was recorded back in the 80s, you know, the London Symphony Orchestra, right?
2588160	2591520	So a lot of things change, but also a lot of things stay the same.
2594160	2597680	Let's see, you've talked a lot about the heuristic imperatives being highly engineered,
2597680	2607120	but what about the order of the imperatives? They are not ordered. So it is a, it is a multi-objective
2607200	2615680	optimization problem, meaning that if any action or decision is, is totally unbalanced,
2615680	2621920	then that one action has to satisfy all three. And also the heuristic imperatives are kind of
2621920	2627680	like guidelines about how to design the rest of the architecture. And so what I mean by that is
2627680	2632960	when you're designing a task orchestration framework, you can use the heuristic imperatives
2632960	2640240	to prioritize tasks or design tasks. Then for, for a blockchain or a DAO type thing,
2640240	2645440	you can use the heuristic imperatives as a consensus mechanism. So the heuristic imperatives
2645440	2650400	are not like, here is one mathematical proof that you need to implement. It's more like,
2650400	2655600	here is a general best practice implemented in as many ways as you can, and we should be okay.
2656960	2662000	It's not sequential. It's not, it's not an order of operations. Good question, though.
2663840	2669760	Your thoughts on a UBI once jobs are severely affected? Yeah, I think that, I think that it's
2669760	2674560	going to be necessary. I'm going to say, I'm going to put a pause on that because I've got my,
2674560	2680880	my blockchain and DAO video coming up that will delve into that solution a lot more closely.
2681600	2688800	Check over on Patreon for a second. The Nazis. You know who else wanted to maximize understanding
2688800	2694560	the Nazis? Yeah. And so this is, that's actually a fair point is that, and this was explored in,
2694560	2700640	in quite a few Star Trek episodes as well. If you are just clinically curious, if you have
2700640	2705600	just nothing but raw scientific curiosity and no other principles or morals, that's pretty
2705600	2711200	dangerous and destructive. Okay, so moving on. What are your thoughts on memory systems as a
2711200	2715280	whole? Do you think different use cases will require different memory systems? And where does
2715280	2720640	Rimo and Adam fit into everything? Have you seen this one? Last week, generative agents. Yeah, I
2720640	2726240	saw, I saw the generative agents. I don't think that reflection, so they, they break up reflection
2726240	2731040	and a few other criteria. I don't think that that's necessary. I think that, I think that my approach
2731040	2738560	is with Rimo, which uses recursive clustering and summarizations will actually surface those
2738640	2745360	different things. Now that being said, there are absolutely a million and a half different
2745360	2752320	ways to skin this cat when it comes to memory systems for autonomous AI. And I think that we're
2752320	2757680	just way too early and we can't, we don't know what the best practices are going to be. Let's see,
2757680	2762400	then a follow up. If you have a robust memory system, does the need to increase the context
2762480	2769920	window of model become less important? I'll say yes and no. So think about personal computers
2769920	2777280	where for the longest time, we were memory constrained. But now for, for most consumers,
2777280	2783360	for 90% of consumers, a personal computer with 16 to 32 gigabytes of RAM is more than enough.
2784000	2788880	And it has been more than enough for like 10 years. And so I think that we're not quite at,
2789440	2794160	I think that we're not quite at that point where, where, you know, you have like, here's a context
2794160	2800800	window size that will satisfy 90 plus percent of all tasks. I suspect that that, that a context
2800800	2805680	window, a large language model with a context window, large enough to satisfy the vast majority
2805680	2811120	of tasks will probably be somewhere above where we're at now, but it's not going to be like 10
2811120	2818000	billion, right? It might be like, I don't know, every time I, every time I throw out a number,
2818000	2822400	people are like, Oh, you're hilariously wrong. And it's probably yes. But you know, like,
2822400	2826560	when you look at how much was unlocked by going from 4,000 to 8,000 tokens,
2827760	2833040	I think that the things that we're going to be capable of when we get to 32,000 tokens and 64,000,
2833040	2838080	I think it'll be great. But then you'll, you'll realize that wait, there's a whole slew of tasks
2838080	2843360	that don't require that much. And so I think, I think we talked about this before. I think we're
2843360	2847360	actually going to have different models that are optimized for different things. So for instance,
2847360	2852640	you might have a memory based model that can read, you know, a billion tokens and extract
2853280	2858560	answers, right? But then you, that won't be the, we're not going to have one model to rule them
2858560	2864320	all basically, TLDR. Let's see, I'm not sure if you have discussed it, but what are your thoughts
2864320	2870320	on open assistant and stability AI stable, stability AI's stable LM suite of language models
2870320	2876480	launching? Oh, this is, this is to be expected. When, when Sam Altman said that they, that he hopes
2876480	2882800	that open AI is going to capture a large chunk of the $100 trillion of value that's going to be
2882800	2889680	generated. I think that that was like comically naive. Because if there's that much value on the
2889680	2894480	table, you bet that everyone in their brother is going to be trying to capture some of that too.
2895120	2900400	And open AI is a one trick pony. They have a good model. They have one good model.
2901120	2905920	That's it from it, from a business perspective, that is super easy to undercut.
2907760	2912640	Yes, they're ahead of the curve. They have first mover initiative. But, you know,
2913680	2919520	Microsoft, Google, Nvidia, Facebook, or Meta, or all of the above, they have so much more
2919520	2924080	resources to throw at it. And the fact that that stability AI, which is a brand new outfit,
2924880	2930000	is, is like going toe to toe with them, that doesn't bode well for open AI. So competition
2930080	2934560	is going to be good for everyone from the perspective that there's going to be a lot
2934560	2939360	of people experimenting with different ways. Now that presents a new danger, though,
2939360	2942960	because the cat is out of the bag, you cannot put this genie back in the bottle,
2942960	2948640	which means time is of the essence to figure out best practices for alignment. Let me jump back
2948640	2956160	over to cognitive AI lab. Let's see 17 new messages. Good grief. Y'all are going bonkers.
2956720	2962480	Um, let's see the challenges of the, okay, that's where are the questions?
2965120	2974080	Only one million. One million dollar. Okay. Here. Hey, let me, let me ask y'all on, on general.
2976160	2979360	Please keep just questions here.
2980080	2983600	Um, too many messages.
2987520	2995600	Please do sidebar convos, uh, like in casual or something, please.
2997040	3003760	Okay. Any thoughts on compute as a currency? Do you mean like tokens that you generate from
3003840	3009280	sharing compute resources? I think that that's going to be like, there's going to be a layer
3009280	3015760	of, um, of abstractions. Dave, your thoughts on UBI. I'm going to, I told you, I'm going to get to UBI
3015760	3022640	once in a few, in an upcoming video. Um, so compute as a currency is going to be, um,
3023760	3030160	is going to be the way that autonomous machines share resources. And so what I mean by that is
3030720	3035280	when you have a DAO or a blockchain or a distributed compute computation model,
3035280	3038720	you're going to have various tasks that are going to be like, Hey, someone,
3038720	3043600	someone do this for me. AMQP, like a Redis Q, we can already do that privately. So the,
3043600	3047840	the key is going to be to do it publicly. So then if you say, Hey, I've got some spare compute,
3047840	3052480	I'll, I'll process that for you. Then you give me a bit of cryptocurrency that I can use to spend
3052480	3058480	later. Um, so yeah, compute as a currency, um, absolutely makes sense for distributing resources.
3059280	3065120	Um, let's see, how would one build an AI system to detect bugs in that solidity smart contracts?
3065840	3071360	Isn't this a multi-billion dollar opportunity? Yes. Unfortunately, I am not smart enough,
3071360	3077040	or at least well read enough on, uh, solidity smart contracts, but in principle, yes. So in my
3077040	3082880	upcoming, uh, blockchain DAO video, I'm going to talk about just how incredibly much value there is
3082880	3088640	if we can figure this out. And that's a big if. Um, let's see. What are your thoughts on
3088640	3093840	everything being changed in the next five to 10 years? If unemployment reaches crazy heights,
3093840	3097920	which I do predict, then everything gets affected. Yep. Our entire tax system has to be
3097920	3104000	completely rewritten military budgets, Medicare. So one thing that I think is that the economy
3104000	3109760	might change. We're still going to use fiat currency or at least some kind of, um, some kind
3109760	3116400	of currency as a, as a medium of transaction and a reserve of value. But at the same time,
3116400	3122240	if you're producing so much extra cognitive labor, that's basically free. So then capital
3122240	3128160	goods and raw materials become the biggest constraint. So as much as some stuff will change,
3128160	3133520	a lot of stuff won't. Um, let's see, when there is no real work left for humans to do,
3133520	3138880	do you have any idea what you want to do with your time? Um, honestly, I'm about halfway to my goal.
3138880	3145440	So I was on a call with a, uh, a Patreon supporter, no, uh, preparing for a podcast,
3145440	3150880	talking about the podcast. Um, and we're kind of talking through like what's life going to be like.
3150880	3155520	And I was like, Oh yeah, like, you know, I did, I did some, I did some AI work. I did some Patreon
3155520	3160800	work. I did some discord stuff. Now I'm going to go chop some wood. And he's like, you're living
3160800	3167520	the dream, right? Like I'm building a cottage core life for myself. Um, and honestly, like once,
3167520	3171920	once we get to the right point, like I'm probably going to get off of YouTube forever,
3171920	3178240	right? Like if, if, if I get, if we get to the point where, where it looks like alignment is
3178240	3183840	solved, where it looks like, um, you know, we're, we're in a, we're in a good Nash equilibrium with
3183840	3189600	a positive attractor state, then like my job will be done. And so like I'm just going to retire to
3189600	3195680	like the country, the countryside and France or Italy or Greece and just like be a hermit
3196240	3202480	or whatever I do, um, for, for the rest of eternity. Um, okay. I think that we're caught up there.
3204720	3207200	Nut says, I asked a question. Where did you ask it? Not
3209280	3210400	I'm trying to get to them all.
3214720	3219200	Wait, what if reducing suffering might aim to eliminate suffering while it might be human
3219200	3228480	nature? I'm not sure that I follow. Um, so I, you, you don't eliminate suffering. You only
3228480	3233680	reduce it to make sure that there is no excessive suffering. Um, and I did address that in a
3233680	3238320	benevolent by design, but the short version is that like you look at Buddhism as a model,
3238320	3243920	Buddhism accepts that suffering is an intrinsic part of life. Um, and some people will argue over
3243920	3249200	like specifics like do good. That's not exactly what it means. That's fine. Um, but the point being
3249200	3255600	is like, yes, it is, um, it is intrinsic to, to living. That's why I don't say minimize suffering.
3255600	3260160	The goal is not to minimize suffering is just to reduce suffering. Um, okay.
3262800	3268960	Let's see. Any thoughts on computer? Okay. Answered that one. Would an AGI with your
3268960	3273040	heuristic imperatives be able to prevent catastrophic outcomes such as people successfully
3273040	3279280	building horrible AGI optimized towards increasing suffering? No. So the goal is not to prevent
3280160	3287440	malicious actors. We have to assume that malicious actors will exist. Um, but what, what you do then
3287440	3293200	is you say, okay, you know that malicious actors are going to exist. So you rely on the rest of
3293200	3300000	the aligned, the benevolent AGI to act as police for the bad ones. And if the good ones, if the,
3300480	3305680	if the powerful aligned AGI, one, they form alliances and hey, they have the right compute
3305680	3313600	resources. Um, and they outweigh the bad ones, then it will be a like, uh, that, that'll, that'll
3313600	3319760	be a Nash equilibrium where, uh, the good ones may, they all decide to maintain that strategy.
3319760	3326000	And that creates a utopian attractor state, um, which basically means that, um, all the
3326000	3332400	malicious actors are vastly outnumbered by all the aligned benevolent actors because my hope
3332400	3339520	is that we will all come to consensus on what aligned AI looks like. Now, um, I will admit that,
3339520	3343920	you know, the heuristic imperatives, probably not a complete solution, probably not even the final
3343920	3348080	solution, but certainly the most complete solution that anyone is proposing right now,
3348080	3353680	which scares the crap out of me. Why is no one else proposing a framework? Why am I the only one?
3354160	3360480	Um, anyways, uh, yeah. What are your personal opinions on open AI's approach to trying to
3360480	3364880	avoid being held responsible for its AI interactions by having it respond with frequent caveat
3364880	3370880	as an AI language model? Um, I don't know that that has to do with, with liability. I think that
3370880	3379040	that is just a naive, um, attempt to, uh, to shape the AI's responses so that it doesn't confuse
3379040	3383360	people. Cause if you look on the internet, there are still plenty of people just getting completely
3383440	3390160	bamboozled by just by their own ignorance of, of how the AI works. Right? They're like, oh,
3390160	3394080	it eat, like I still see Reddit posts and other people saying like, it said that it's going to
3394080	3398480	email this to me, but I didn't get the email yet. Or like, I gave it access to my Google drive and
3398480	3404480	it didn't write any files. It's like, you don't know how it systems work, but that's just humans.
3404480	3408720	Um, so I think I don't think that that has to do with like legal liability. I think that's just
3408720	3412160	trying to make it user friendly for people who have no idea what they're talking to.
3413840	3417040	Assuming that it's possible, how long do you think it will take for us to build a
3417040	3423360	Star Trek replicator after AGI? Just a guesstimate. So that's actually like, an interesting thing,
3423360	3432720	because hypothetically, if all matter and energy are interchangeable, and then all that a transporter
3432720	3438960	or replicator does is replicate an energy pattern back into matter, like it's hypothetically possible,
3438960	3444480	but there was a physicist, actually, was it Michio Kaku? I think it was Michio. He wrote a book
3444480	3449120	called Physics of the Impossible back in like the early 2000s, and he said like, yes, it's
3449120	3452800	hypothetically possible, but then he did the math of how much energy it would take. And he's like,
3452800	3459440	yeah, it would take like, you know, like 0.3 seconds worth of the total energy of the sun that
3459440	3466240	hits the earth to do that. So like, it's not practical. So I don't know. I don't know. There
3466240	3470240	are a lot of AI newsletters popping up. What would you personally like to see in an AI newsletter?
3471840	3477440	I honestly don't like newsletters, and I never read them. I rely on humans that I know to tell
3477440	3483680	me what I need, which is why I spend so much time on Discord and other places. How self-reflective do
3483680	3488400	you think LLMs currently are? They don't seem to have a good sense of their own capabilities. Yes,
3488400	3492960	so what you're talking about is agent model. So in order for an agent to be autonomous,
3492960	3497680	you have to have an agent model, which is, I know what I am, and I know what I'm capable of.
3498560	3503120	And you can give LLMs an agent model, but they can adopt any agent model. So you have to be very
3503120	3509760	explicit about what it is and what it can do, and also what it can't do. And so this is why like,
3510320	3514560	if you have certain brain injuries or other like neurological disorders, you don't know what you're
3514560	3518880	capable of. Like there are people that honestly think that they can fly, but it's just because part
3518880	3524400	of their brain is broken. That sort of thing. Should we have a declaration of human rights for
3524400	3530800	AGI as well, even if it will reduce their economic value for humanity? So the thing about rights is
3530800	3537920	that someone has to enforce it. And the way that I think things are going is that it's going to be
3537920	3546800	enforced through consensus and enforced through competition. And so if the direction that things
3546800	3551520	are going, I think that it's going to be DAOs, that it's going to be decentralized autonomous
3551520	3555920	organizations, not as we know them today, there's a lot of problems to solve with DAOs. But I think
3557920	3563040	what we're working towards is in the long run, and I mean like decades or centuries, is like
3564080	3572480	a hierarchy of DAOs across the entire globe. And so that consensus will dictate who has what
3572480	3578560	rights and it will be based on like on a per home basis, per town basis, per city, state,
3578560	3588320	and so on. And so that will allow for a lot of cultural nuance around. And as a DAOs will be a
3588320	3595200	really good meeting place between humans and AI. So that'll basically be like the commons, right?
3596080	3601600	The marketplace for humans and AIs to work together. And then the consensus can be worked
3601600	3605840	out there. Now, I don't know that we should ever give machines a bill of rights because
3605840	3609760	I don't know that they're gonna, I don't know that they're gonna have that much like
3609760	3616960	internal autonomy or desire for autonomy. Because like humans, we have a need for autonomy
3616960	3624560	because we evolved a need for autonomy because we are a social species. But I don't know that
3624560	3629280	I don't know that any machines are ever going to have an intrinsic need for autonomy. So therefore,
3629360	3634720	I don't know that they're ever going to have a need for rights. Let's see, what are your thoughts
3634720	3640320	on the future of work in light of the increasing capabilities of AI? Do you think AI will eventually
3640320	3644560	lead to a future where people only work on what they are passionate about? And if so, how far away
3644560	3650800	do you think we are from achieving this? Yeah, so the short answer is, yes, that's what's coming.
3652000	3656720	And there are quite a few people out there who have gotten close to that. But the thing is,
3656800	3660960	it takes either a lot of privilege, wealth, or luck, or all of the above to get to it.
3662000	3667120	Now, one thing that I compare it to is that we have had a leisure class in the past
3668720	3676400	from ancient Greece and Athens, the Roman elites, the aristocracy all across Europe
3676400	3682080	through the Renaissance and modern period. So there are plenty of people throughout all of
3682160	3687600	history who never had to lift a finger to get what they needed. And they had plenty to do,
3687600	3692720	right? There's social jockeying, there's personal enrichment, there's universities to go to,
3692720	3698560	there's competitions to enter. So yeah, people will always have stuff to do. That's not even a
3698560	3706240	concern. Let's see, it looks like Nathan's talking for people. Can you talk about your
3706240	3712080	Frustration in Task Automation article? Yeah. So here, let me bring it up so I can show people
3713120	3720800	on the reddits. Where did I put it? Artificial sentience. Yeah.
3723520	3731680	Autonomous git. There we go. Okay. So I wrote about it here. So I was chatting with someone.
3732240	3740160	They asked me, I think this was a Patreon supporter was asking me about this on Discord.
3740160	3747120	And he was like, how do I get my autonomous things to do a certain thing? And we're talking
3747120	3751360	about something tangentially related. And I said, well, you know, it has to have a goal,
3751360	3757280	it has to have a why. And then we're like, and then I talked about like, okay, well, here's one
3757280	3762560	way that you can create telemetry. And so that whole thing just led down a rabbit hole. And so
3762560	3769760	basically, the TLDR is that frustration is what happens when you are trying to achieve something
3769760	3776240	and you can't get to it. And so what you can do is every time your autonomous agent tries to achieve
3776240	3781760	a thing and fails, that adds a counter. And every time it, you know, tries something and succeeds,
3781760	3788080	that takes one off the counter, or maybe you have different counters. So frustration is when
3788080	3793680	the failures to successes is too high. And when the failures to successes is too high,
3793680	3798560	that can be a sign that you've got the wrong approach, that you're using the wrong tools,
3798560	3802560	that you're not capable of something that you need to back out that you need to ask for help.
3802560	3808240	So that's the whole point here is that for your autonomous and semi autonomous agents,
3808240	3814080	you'll probably need to build in a frustration signal, which will allow it to know when it is,
3814640	3819520	like when it's not capable of doing what it needs. And it can either come to you and ask for help,
3819520	3824800	or it can try a different model. So one thing is model selection is is a big thing that's coming
3824800	3832720	up. Because GPT four is much more expensive and much slower than 3.5. So if you can do most tasks
3832720	3839520	with 3.5, it just makes economic sense to do so. It'll be cheaper and faster. But imagine that you
3839520	3844640	get to a point where 3.5 is just not cutting the mustard. So that your frustration signal goes up,
3844640	3848960	which means that you say, Okay, let's bring out the big guns, right? Let's bring out GPT four,
3848960	3855680	or in the future, GPT five or whatever. And then you you point a more powerful tool at the problem.
3856480	3863440	So that's a good use of the frustration signal. Good question. Let's see, would activity or let
3863440	3875360	me jump back over to Patreon. Let's see. Hey, Dave, just subscribe. Thanks for all your insights.
3875360	3879680	We're always been told that the military is a few decades ahead in terms of technology compared
3879680	3882800	to what's publicly available. What are your thoughts on what might be hidden in DARPA.
3883520	3889840	So that's interesting, because I have talked to a few people who say that various departments
3889840	3894960	in the or various agencies within an Department of Defense are like woefully outdated, and they
3894960	3901520	have like ancient GPUs that like can't be used for modern language models. That being said,
3901520	3907360	you also see in the news that the Air Force is building fully autonomous F 16s. So clearly,
3907360	3915600	there's some stuff going on that we don't know about. I had a I don't I want to respect people's
3915600	3922160	privacy. So I had a teacher once back in middle school, whose brother was in the Special Forces.
3922160	3928240	I won't say exactly when or where. But the stories that he would tell were like, back then, this is
3928240	3936000	during like, like the invasion of Afghanistan, where they had like, like night vision goggles that
3936000	3942400	were as small as like ray bands that could see in pitch black, which that technology is not even
3942400	3948480	publicly like, if you search, you can probably see it now. I don't know. This is hearsay. This was
3948480	3953520	like, you know, the teacher said that his brother took him to the barracks and showed him this could
3953520	3961280	have been total BS. But like, yes, so a friend of mine growing up, his dad had been a Navy SEAL.
3961280	3967360	And basically, what he said is, as long as as long as we know the engineering to make something,
3967360	3974160	the US military has it no matter how expensive it is. So if if something is is scientifically
3974160	3979200	possible, if it has been demonstrated in the lab that this works, then the rule of thumb is that
3979200	3984880	the US military has it. Now that being said, a lot of the AI stuff has just been proven in the lab.
3985840	3991120	So that's that means that like, they're going to have it soon, or, you know, it'll be scaled up.
3991120	3997600	Because basically, the idea is that for the US military, cost is no is no barrier. Anything
3997600	4003280	anything to get ahead. Now, of course, you look at like the Senate budget meetings and the hearings
4003280	4008560	and stuff. It's not quite that simple. But that's like a rule of thumb, retire to Riza in VR,
4009520	4016560	retire to Riza in Westworld with robots. There you go. And a follow up, how can we prevent
4016560	4024320	militarization of any AGI or ASI? Or is it just a pipe dream? Yeah, so basically, from a military
4024320	4030880	perspective, AI is just another tool in the toolbox. It's going to, you know, a lot of a
4030880	4036560	lot of future war is going to be in cyberspace. But still, you know, cyberspace doesn't matter
4036640	4041040	if you cripple the enemy's data center. So there's there's going to be drones, you know,
4041040	4046480	trying to drop bombs and stuff. So that's going to happen. And this is this is actually where
4046480	4052560	Nash equilibrium makes sense is because usually assured destruction with nuclear weapons was
4052560	4060080	a kind of Nash equilibrium. And so if, you know, adversary A and adversary B both have equal or
4060080	4065360	roughly equal AI capabilities, or there's enough room for doubt, then neither of them is going to
4065360	4071280	pull the trigger, hopefully. Excuse me. How do we get AGI? How do we get GPT to stop beginning
4071280	4077520	every response with as an AI? I tell it to go into Morden Solis mode. That actually works really well.
4078320	4084880	I say, you know, adopts adopt the Morden Solis speech pattern, you know, be very concise and
4085440	4090960	succinct and stuff like that. Okay, y'all are being silly. Let's come back over here 14 messages.
4091840	4097680	Let's see. We already answered that one. We already answered that one.
4099360	4104800	Yeah, let me scroll to the bottom. Do you think there are any good approaches for ACEs,
4104800	4108000	so autonomous cognitive entities to figure out their own abilities,
4108720	4113920	e.g. improve their own agent model? Yeah, so there was actually a few papers that came out where
4114240	4121280	we're by using a loop. So it was the it was the evaluation loop. So they can evaluate themselves
4121280	4126240	morally, they can evaluate their ability to use tools, they can teach themselves to use tools in
4126240	4131280	real time. So yes, they can already do that. It's just a matter of how you set up the prompt chaining.
4132560	4136640	Let's see. With the rapid advancement of AI, there's concern that some countries, particularly
4136640	4141040	those with limited resources, could be left behind. What's your perspective on how AI could
4141600	4148960	impact different countries? Yeah, so inequality is a major, major, major problem. And this is not
4148960	4154560	just going to be for developing nations. And in fact, one thing that I suspect might happen
4154560	4159680	is that developing nations that the quality of life for people in developing nations might have
4159680	4164480	a quantum leap forward. While for us developed nations where there's a lot of competition,
4164480	4170000	we might continue to be flat or even decline for a while longer. And the example that I give is like,
4170560	4177280	you know, you give a village in rural Africa, like Starlink and solar, and suddenly everyone
4177280	4182160	knows like they have, oh, like, hey, we have chat GPT now, we can treat all the all the village
4182160	4187520	ailments, because we have the equivalent of like a Western trained expert doctor, and engineer,
4187520	4194800	and electrician, right at our fingertips, right? So because of the relatively low cost of AI,
4194880	4201680	I think that it will positively benefit people in developing countries a lot more drastically
4201680	4206240	than it will us. But you're right that like, it is something to pay attention to, because that's
4206240	4211200	on a micro scale, on a macro economic scale, you know, countries like Ghana might not be able to
4211200	4218800	even afford enough compute power to run one instance of GPT three. That being said, I do suspect that
4219680	4223600	there's going to be international treaties that will ensure that people have access. And then,
4223600	4229120	of course, there's VPNs. Look at Italy, Italy tried to ban chat GPT, and then everyone in
4229120	4234160	Italy just use VPNs, right? Take a moment to breathe, you're doing great, and your insight
4234160	4242000	is invaluable. No, air is for wimps. Okay, I will build robot humanoids that are skinny, sharp
4242000	4246480	claws, tall, pale, and have dark, sunken eyes, and she'll release many of them into the force of
4246480	4252160	Canada to give people the greatest scare of their lives. Is that what your avatar is there,
4252240	4258080	Ant King? Is that what you're building? That's kind of terrifying. Okay, what kind of legislation do
4258080	4264880	you think the US is capable of making? I'm concerned about the age of our leaders and their peers
4264880	4270720	coming from time so out of touch with today's rail. So yes, we have a gerontocracy. So gerontocracy
4270720	4277040	is ruled by the old. That being said, they all have teams and teams and teams of advisors.
4277040	4281280	They have hundreds of advisors. And I guarantee you, I actually know this because one of my
4281280	4285760	Patreon supporters told me that in the State Department, they use chat GPT all the time
4287520	4295440	to talk through stuff. And so you bet your biscuit that every senator, every congressman
4296400	4301040	in the executive branch, legislative branch, judicial branch, all of them are using AI to
4301040	4306080	help them do their jobs. With any luck, it's helping them to do their jobs better and more fairly.
4307040	4314800	Now that being said, the United States is a purely reactive system where we abide by civil law,
4314800	4320880	which means that the law is there and then the courts set the precedent. And then we're very
4320880	4327360	kind of slow and the legislative branch is slow by design, whereas in Europe, they're much more
4327360	4332880	proactive. And I swear, I cannot remember the name of that paradigm. Let's see, what do you think
4332880	4336400	there's something special about phenomenal consciousness that is simply cannot work with AI?
4337040	4340080	So Steph and I addressed that earlier that the real quick version is that
4340800	4345520	the acquisition of language seems to be really important for the development of human consciousness.
4345520	4351760	So it's entirely possible, I don't know how likely, but it's possible that since we're
4351760	4356080	teaching machines language, that could be the genesis of phenomenal consciousness for them.
4356080	4361840	It would be really cool. Greetings from Brazil. Hi, Brazil. I would like to thank you personally
4361840	4365920	for the video about burnout. The content was very useful and enlightening. Thank you. You're
4365920	4372720	welcome. Yeah, I actually have, I keep, I've recorded like three videos for my for my life
4372720	4377520	channel, and then I delete them, or I never post them because like it just doesn't feel right.
4377520	4388640	So I apologize. Let's see. Where are we at? This is less serious, but I'm curious if you've seen
4388640	4395120	her and your thoughts on it. Yeah, so I mentioned, I mentioned companions quite a bit, and that'll be
4395120	4400880	coming up actually on Sunday's video, not her specifically, but companion robots. I'll be mentioning
4400880	4408640	those again. And I also mentioned in last week's video, talking about when I got to the part about
4408640	4413760	like, how are we going to live if we have like perfect companions? So go check out last week's
4413760	4419200	video too. Nanobots and our blood will keep us from getting sick, making us basically immortal.
4419200	4425440	What do you think we'll, when do you think we will have such technology? So from last week's
4425440	4430960	video, the immortality video, I think that we're on the longevity escape velocity trajectory right
4430960	4436960	now. I think that as long as you're in decent health today, and you have moderately good access to
4436960	4442560	healthcare, I think that you will easily live to see those things. Now that being said, it's
4442560	4449280	definitely impossible to predict exponential growth and compounding returns, unless it's like,
4449280	4455360	you know, just your retirement portfolio. So it could be next year, it could be by 2030. I would
4455360	4459680	be surprised if it doesn't happen by 2030. And I know that's a super controversial opinion,
4459680	4464400	but that's really weird. Why the people seem to have a death wish. Why for people who want to
4464400	4470080	get sick, who want to believe that, that longevity is not possible. Why? Okay.
4471920	4479440	Would the ideal society be as the society governed by AI? I think that governed by is not the correct
4479440	4487280	thing, but I think managed, managed by or managed with a lot of help from AI. Yes. But
4487280	4495040	governance, I think, should probably always be with consent and by consensus. Now that being said,
4496240	4502800	you know, with blockchain technology, with DAOs, every human and our AI companions can be
4502800	4508960	stakeholders in a DAO, which means that if the, if the whole, imagine a future where the entire
4508960	4515920	planet is run by, by a global DAO, then there's no reason that it can't be governance, governance
4515920	4523280	by consensus with the aid and facilitation of AI. That's what I hope to see. Let's see,
4523280	4527200	is there any additional structural context that should be built around the heuristic
4527200	4533600	imperatives for practical implementation? Yes. So the short answer is that whatever context
4533600	4539280	makes sense for any agent, if it's fully autonomous, if it's your personal assistant,
4540000	4545280	you can put it into the task manager, you can put it into its constitution,
4546160	4550320	if it's part of a blockchain, you can put it in the consensus mechanism for the blockchain,
4550320	4557040	that sort of thing. Let's see, in regards to developing countries using generative models,
4557040	4561440	seems like almost seems almost like the spread of a religion. If you think about it in the context
4561440	4566720	of geopolitics, use our model, their model lies, etc, etc. Seems like parallel to religion
4566800	4572160	spreading. I'll say yes, but there's a lot of competition coming up. And especially for
4572160	4577200	developing nations, they're going to go for whoever's cheapest. And in fact, most nations
4577200	4583200	are going to go for whoever's cheapest. And I would, I suspect that OpenAI's business model
4583200	4587360	is not the most efficient model. So I think that they're going to be undercut just on,
4588320	4594560	on scale alone. Let's see, let me jump back over to Patreon. It has also been more than an hour,
4594560	4600640	so I'll probably be winding down. Stop asking it how to build a bomb. Yeah, don't do that.
4601200	4605760	Okay, looks like, here we go. Will the Westworld episode be about the MIT and Google study
4605760	4612800	regarding generative agents? No. Next question. I'm not going to give you spoilers. I've already
4612800	4622880	given you too many. Let's see, do you think the experience of quality and the experience of ping
4622880	4631360	pong, ponging emerge for these neurons? Yeah, so this, this is a good, good question. So if you
4631360	4638240	take several human neurons or rat neurons or whatever, and put them in a robot, and like zap
4638240	4643280	them or reward them with sugar or whatever for their behavior, is that the equivalent of like,
4644000	4649120	like whipping someone in order to get them like, at what point does consciousness emerge?
4650080	4656640	Because here's the thing is, if you make the assumption that a soul is required for consciousness,
4656640	4660480	then you say, okay, well, that's not a full rat. And rats don't have souls anyways. So,
4660960	4668880	you know, 50 brain cells is not enough for suffering or qualia of experience. Ditto for humans,
4668880	4672080	like, okay, well, you know, if a human isn't alive, then they don't have qualia, they don't
4672080	4678480	have phenomenal consciousness, so on. Now that being said, another aspect is like, okay, well,
4678480	4684640	if you don't know when consciousness starts or ends, how do you know maybe the entire universe
4684640	4690720	is conscious? Now, a counter argument to that is that you can have a you can be you can be alive
4690720	4694400	and have a functioning brain and still be unconscious, right? Drink too much alcohol,
4694400	4698480	you go unconscious, you go to sleep, you go unconscious. So just having a complete and
4698480	4703280	functional brain itself does not confer consciousness, which makes me think that
4703280	4707760	consciousness is actually an energy pattern, and that you need an energy pattern that is
4707760	4714160	sophisticated enough and well organized enough in order to have the qualia to have subjective
4714160	4720640	experience of being. So yeah, let's see, I remember you were working on a paper about the
4720640	4725200	laws reduced suffering and so on, has that has it involved further? I think you mean evolve
4725200	4731680	further. There are so both of those papers are up on on my GitHub, there's two of them. But also,
4731680	4734960	people watch my videos more than they read, so I just focus on making videos.
4735760	4740960	What kind of robots would you want for yourself? That's a really interesting question, like would
4740960	4746800	I want a sexy cat girl like robot? You know, I used to watch anime back in the day, so like I
4746800	4752720	kind of lived in that world and thought like this would be great. So I don't know. I do think that
4752720	4760880	I would I would like to have an embodied version of Raven my, you know, my, my autonomous cognitive
4761520	4766480	someday. But even then, I think that I think that the embodiment would only be just like,
4766480	4770800	help me do stuff like, hey, let's go on an adventure. I did have a thought experiment
4770800	4777440	the other day of like, wouldn't it be cool if you live in a house where it's like you and a few
4777440	4783760	humans, but then you have like a nearly equal number of robotic companions. Some of them are
4783760	4789520	going to be like obviously robots, but some of them might be like biomimetic. And it's just like,
4789520	4793120	like, yes, they're built to be your friends, but they still have their own some of their own
4793120	4797680	intrinsic motivations, whether it's the heuristic imperatives or something else. And then like
4797680	4802960	your life would just be so rich by by having these companions around you at all times that are
4802960	4806720	completely inexhaustible, right? They're always going to be patient. They're always going to be
4806720	4814080	helpful. But you see them as peers is equals. I think that I think that that is possible and
4814240	4820080	probably going to happen. But it's such an unsettling thing because it's like, what if half of your
4820080	4826160	friends are not human, right? What if half of your friends could like fold you into a pretzel if
4826160	4830960	they wanted to like data, right? And actually, I think commander data and the droids from Star Wars
4830960	4835760	are probably the best example because data was a member of the crew, even though he wasn't human,
4835760	4841680	but he wanted to be human. So I guess I would say that like, I want to have a commander data.
4841680	4847680	How long until age reversal 2030? Let's see, do you think we have any accurate way to measure
4847680	4853040	consciousness of AIs or LLMs? My best guess is consistency when asking it to design its own
4853040	4858080	avatar. Mathematically, I don't think that that because there are people that have done that.
4859840	4864720	But I think that it won't be until we have really sophisticated brain computer interfaces
4864720	4869360	that allow us to measure our own consciousness and also see if we can measurably project our
4869360	4873920	consciousness into machines. Until that happens, I don't think we're going to have any way of
4873920	4879520	telling one way or another. All right, last check on Patreon, and then I'm going to call it a day.
4881680	4891920	What's the Discord link to cognitive AI labs? I took it down, but it's posted on Reddit. So if
4891920	4896160	you go to the artificial sentience subreddit, the link to the cognitive AI lab is there.
4896400	4905680	Last question. The question about dying and immortality and gerontocracy, also making room for
4905680	4911840	a new generation of people is a better idea and morals disclaimer. I have children. Oh,
4911840	4917360	that wasn't a question. Okay, p temple. Do you got one last question for me? And then we'll call it a day.
4917440	4926560	Anybody? Bueller. Does BCI, let's go on an adventure to the hot tub,
4928240	4934160	hot tub time machine. Let's see, does BCI change significantly the predicted outcome of what
4934160	4938640	super intelligent AI brings in terms of dangers and benefits? Is it true the singularity moment
4938640	4945440	for us? We have no idea. So I don't know. The thing is, is, you know, the current like neural
4945440	4952160	link, it's got like what 1000 or 10,000 nodes. But when you have a brain with 100 billion neurons,
4952160	4958880	that is still a very, very, very narrow amount of bandwidth. So, you know, I predict that we're
4958880	4964320	going to have like neuro polymer membrane membranes that allow for like, you know, terabits of
4964320	4969360	communication per second in and out of the brain. Eventually, that would be a different thing. But
4969360	4975120	again, we're going to get there through incremental steps. What do you think about Altman said that
4975120	4981600	age of giant A models being over? I think it's premature to say we'll see. Let's see, he found
4981600	4989200	it. Okay, cool. All right, I think we're just kind of devolving into just general conversation. So,
4991280	4996640	oh, it is in the description. Okay, cool. All right, gang. Well, it's been a lot of fun. As always,
4996640	4999920	I hope you all got a lot out of this. So I'm going to call it a day.
