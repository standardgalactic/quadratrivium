{"text": " Check it, check it, check it, check it. Okay, level check. I think we're here. All right, so background. It's been a little while since I've done, you're over here now, since I've done one of these tutorial coding videos, but I woke up at like 2.30 in the morning and just had to work on this. So I hammered it out. Basically, the purpose of today's video is to demonstrate using ChromaDB, which is a local vector database. It's basically like SQLite. If you're familiar with that, which is a self-contained SQL database, relational database, this is functionally very similar to SQLite, except it is a vector database, meaning it does semantic search. One thing that's really great about it is that it has its own built-in embeddings tools. I think it's based on BERT. Anyways, you can check out all the documentation here on trichroma.com. The Getting Started and Usage Guide is pretty good. It's not complete every now and then. I find that I have to go to the actual repo to look at how some of the internals work, but it is pretty brain-dead simple. So let me just go ahead and show you. This is my private instance. Oh, so before we get too lost, I do have a public instance, daveshap slash chroma db underscore under chatbot underscore public. I've got a little integration guide usage. It's, I mean, you probably don't need this. You can futz around with it, but this will get you started. I also use chatGBT to just get a really basic explanation of the code. You probably won't need it once you take a look at it. So off the top, let me just show you how this thing works. So it's a basic chatbot. You can see I didn't specify it. So it's getting all mini-LML6v2 from Huggingface. Great, so it's like, hey, let's see. What were we talking about last? This probably won't work because it's just gonna, in the future, it wouldn't work because it's gonna have multiple KB articles in the background. Oh, I need to explain like all that. So I know that I just said like KB articles, don't worry, we'll get to it. But anyways, I wanna show you that I just started it up and what it's gonna do is it's gonna use the last few messages to search its internal KB article for the last information, but it also has a user profile for me. In our previous conversations, we discussed AI alignment, morality, ethics, and epistemology within AI development. You also shared your plans to communicate your ideas on YouTube, unplug your computer, and spend more time outdoors, and use digital wellness settings to improve your work-life balance. Working on that. Additionally, we talked about your recent experience with severe insomnia and the importance of maintaining a healthy balance between work and personal life. Yes, that's actually why I created this chatbot. Let me show you. So there was a, there was, there was, God, my brain. I was using chat GPT as a reflective journaling tool. So what I mean by that is if you plug in this message, and I know I'm scattered, I'm all over the place, this is what happens when I have severe insomnia. Anyways, so basically I use chat GPT as a reflective journaling tool to figure out like how I'm feeling about things, because as an autistic person, I often need help with this. And I don't like journaling because just talking to a page is kind of dumb, but it's like, hey, I need to talk something out. And so anyways, by workshopping this system message with chat GPT, I came up with a pretty good reflective journaling tool. So you could say that this is a therapeutic tool, but by couching it in the language of reflective journaling, it's not like medical therapy or psychotherapy or anything. But it's just like, you know, I can say like, I have been working so hard and I don't know why. I actually do know why now, but this is kind of a shorthand of the conversation I had. Let's try and figure out, it's driving you so hard. Can you think of any specific goals that might be pushing you to this extra effort? So you see how the tone of this is much more straightforward and it's very focused by asking those like kind of probing follow-up questions. This is why, you know, it's in the investigation phase. Anyways, so I had this idea and I was like, okay, this is great, but I need, if I'm gonna use this as a long-term journaling tool, I'm gonna need this locally and I'm gonna need persistent storage because as this is just the playground, if I do a refresh, it's gone and that's no good. So actually here, let me go ahead and just save this to the, we're gonna call this the system message for reflective journaling. So you can use this if you want. All right, so anyways, so you see it has this and then you see it says updating user profile and updating KB, okay, cool. So you see that it fundamentally basic chat bot. So now let's start to unpack it. So first we will go look at the, just the chat file. So this is a super brain dead simple chat bot with infinite memory, infinite memory. I know some people got grumpy when I said that Pinecone had infinite memory. From a human standpoint, it functionally has infinite memory because, you know, this thing can hold probably a million KB articles, which is more than enough to document your entire life. So from a human standpoint, it is functionally infinite. All right, so from the top, we've got a few basic utility functions, save yaml save file, open file, and then a chat bot, which calls the GPT-4 model. You could switch this out to 3.5 turbo. If you don't have access to GPT-4 yet, it does not work as well. There's a reason that I use GPT-4 because it is smarter. I also set the temperature to zero because I don't like it to be too creative, especially with a lot of the functions that I have it doing. You actually want it to be more deterministic or mechanistic and that you wanna get the same results every time, especially when you're updating the user profile and the KB articles. You can see right here that every time you call the chat bot, I dump the whole thing to apilogs slash convo and it's a yaml file. So here's my private one. So apilogs, here's an example. So each item is gonna be here. Actually, that's not a good one because I changed the way that it saves it. Let me show you a more recent one. So the first element is always gonna be the system message that was in the last convo. So then here's the KB article and you can see that it was updating the KB article. And so each one of these items is like, you'll see, but anyways, I just wanted to show that it logs everything because well, sometimes it does things that you don't understand. All right, so that's an example of the apilog and then if the conversation, if the overall conversation is too long, it'll go ahead and trim the oldest chat message. So the chat GPT web interface does this automatically where it'll just kind of groom the backlog of messages. So we have to do this manually. So I just have it cut off at 7,000 tokens. You could probably do like 7,500 if you want to because a lot of these are gonna be limited but you have a user profile and a KB article that gets wedged in which are both up to 1,000 words which could be around 1,000 tokens. So having it trim at 7,000 is probably where you want it. So that's the primary, those are the helper functions and then you have a super straightforward, you instantiate ChromaDB right here. So you set the persistent directory which is, I have it right here, ChromaDB. So this is my instance, my personal instance of ChromaDB. It's not gonna be the one that you find up here. This is the public version. So if you go into ChromaDB, you'll see just a placeholder file so that the folder's already there. You don't need to instantiate it. Let's see, going back to here. So ChromaClient, so we instantiate the ChromaDB client. This is again, almost identical to SQLite or other similar things. So about a year ago, I tried to do basically the same thing. I called it VDB Lite for Vector Database Lite instead of SQL Lite, Structured Query Language Lite. But this company went and did the same thing and I think they've already got like a $30 million valuation or something. I was like, damn, I should have stuck with that. Anyways, they figured it out. I think it's based on the same underpinning technology. They're using an open source embedding transformer. I think they're also using the Facebook AI semantic search and the device engine and the background. Anyways, so you instantiate the client. You need to use the settings to have a persistent directory because by default, this entire thing is fully ephemeral. I think it does cache it somewhere, but I wanted to be very explicit saying save it here for reusability. And so then collection is ChromaClient, get or create collection named Knowledge Base. So this is my personal Knowledge Base. Then we instantiate the conversation with open AI, the chatbot. And in this case, because we're saving everything necessary into a personal user profile and the KB articles, like why even load the conversation? All right, so let me show you the system default message. So the system default message is where it starts. Your chatbot is whose mission is to assist the following user, your ultimate objectives are to minimize suffering and hence prosperity and promote understanding. The provided information about the user and the Knowledge Base article should be integrated into your interactions. This is private information not visible to the user. The user profile compiled from past conversations encapsulates critical details about the user which can aid in shaping your responses effectively, which you saw here. So you see like it actually knows quite a bit about me from our past conversations. This was populated here in the user profile and the KB article. So basically it says, then it also explains that the KB article is a topic compiled similarly from past dialogue serving as your long-term memory. While numerous KB articles exist in your backend system, the one provided is deemed most relevant to the current conversation topic. Note that the recall system operates autonomously and it may not always retrieve the most suitable KB. If the user is asking about a topic that doesn't seem to align with the provided KB, inform them of the memory pulled and request them to specify their query or share more details. This can assist the autonomous system in retrieving the correct memory in the subsequent interaction. So basically that's instructing it to do the same thing that a human will do if I say like, hey, Bill, do you remember that time that like I accidentally shot you in the face with a Roman candle because that's something that would happen in the South? And Bob would be like, you know, I don't actually remember that. And I'm like, oh, well, you woke up in the hospital. Oh yeah, I remember that, right? So we prime each other's memory and human prompting is not that different from AI prompting. Remember that the clarity of your responses and the relevance of your information recall are crucial to delivering an optimal user experience. Please ask any clarifying questions or provide any input further for refinement if necessary. So this system message, I actually got help from chat GPT to create a really compelling system message. And one thing that I recommend that people do is actually use chat GPT to work on prompting. So this is, you could call this meta prompting where you use the thing to prompt the thing. And the reason that this works really well as one, chat GPT is more articulate than most humans, including myself when used correctly. But another thing is one thing that I noticed is that chat GPT tends to write in a way that it will understand. And so if you say, if you give it some context, like this is what I'm trying to do, here's my current prompt, here's what's weak about it. Can you make it better? And then you tell it like, ask me some questions if you have any. But no, I see what you're trying to do. Let me write better instructions for you. So instruction writing for anyone who's like a teacher or technical writer or whatever, instruction writing is a very, very particular skill set and chat GPT is really good at it. So this is the default system message, which is then populated with the user profile and the most relevant KB article. So now that we're up to there, we enter into the infinite loop, which is just get the user text, save it to the user log or the chat logs. So the chat logs are all saved out here. It's just plain text and the file name has the timestamp in it as well as the speaker. So user chat bot, user chat bot, so on and so forth. So you got the raw logs there just in case anything goes wrong. And then I've also got DB logs, which we'll get to in just a second. So then what we do is we take the quote main scratch pad, which is just the last five messages, both for the user and for the chat bot. And this is what we use as the context of like working memory. And so then we use this main scratch pad, which is the last five messages. We use it to search for the top most relevant KB article. And in my case, I still only have one KB article. So we'll see how it gets to, and I'll go through the logic of how it builds KB articles in just a minute. So basically it just says, okay, here's the most recent thing. Find the KB article that is most relevant to the most recent bits of conversation. And then it'll pull that, and it's again, super straightforward. All you have to do is pass the text to it, and it will automatically embed it for you. And then I said, just give me the one most recent. Once we have larger context windows, or maybe if we decide that recent chat history doesn't need to be as big, like let's say we wanna trim this down to like 3000 tokens, and we decide that actually having more KB articles is more important, we can absolutely do that. And what you would do then is just change the end results to let, let's say, give me the four most relevant KB articles instead of the one most relevant. That will allow it to have a more sophisticated working memory. Yeah, so, but right now we're just doing one. And so then what we do is we repopulate that system default message with the profile and the KB article. And so that's right here. So that gets populated there. And then, let's see, it looks like I accidentally changed something. So let me go ahead and show you my user profile. I don't mind sharing this because I've already told you everything. I'm pretty much an open book. So the format for this is what I call a labeled list. And so I realized back in GPT-3 that GPT handles labeled lists very, very well. So that's where you use a hyphenated list, bullet list. It understands that intrinsically. And then you label the information, right? So it's just a hash table. If you're into computer science, this is called a hash table or a dictionary where it's you label the kind of, you have a parameter and then you label the parameter, right? So the data metadata. So name, David Shapiro, y'all know that. Profession, AI and cognitive architectures, y'all know that. Interests, it's got a whole bunch of interests. And oh, by the way, this was all distilled from other conversations. Beliefs, plans, and this is of course gonna get updated over time. So for instance, during some of the conversations that I just showed you with this brand new chatbot, it added this. When I told it, this is what I'm gonna do. It said, okay, I'm gonna, I think that that's relevant to what you're gonna be doing in the future. So let me just jot that down on my scratch pad for you. Preferences, so I manually added avoid superfluous words overly for both responses. And then you know how it says, as an AI model, I don't have personal opinions. I'm like, I know, I don't care. So I said, please interpret personal input as critical evaluation and valuable feedback. I said it a little bit more explicitly than that, but the point is, is that I told it that in natural language, I was down here and I said, I know you're an AI and have no personal opinions, but when I ask for them, this is what I mean. And so when I did that, it actually recorded that automatically because after every conversation, it checks the user profile. We need to find a way to speed this up because as you saw from the user interface, it's not the best. If I had more time, mental energy and patience, I would separate this out as a thread, as a separate threading thing that can be done, or even separate it out as a separate API. One of y'all can do that, submit a pull request on the public repo. And then health, so it added this entirely on its own because I said, hey, I woke up at like 2.30 in the morning because I had to work on this. And then I said, let's talk about that. And so it decided that that was a critical piece of information to add to my user profile. So that all gets populated here. And then the logs are all stored here. So you got the API logs, which will track all of that. Everything, so I use chat GPT API for everything just because that's the only way to get to GPT-4, which is the most powerful. Let's see. So then we update the system message every time. So it says, okay, whatever you said, update the system message, then we go ahead and generate a response first because the user profile is not gonna change all that much or all that often, so we can basically assume that it'll be usable. And then the KB articles also, I figured it would actually be better to update the KB articles after you have the user input and then the machine output because if you ask chat GPT for important information or it solves a problem for you, you actually wanna capture that. So we go ahead and generate the response and append that to everything. We go ahead and log it out. Then we update the user scratch pad again. Actually, why did I do this? Oh, no, this is the first time we did it. Okay, sorry, I apologize. So then we update the user scratch pad, which the user scratch pad is only the last few user messages. And the reason for that is because we want to exclude chat GPT's response because we don't want it to get confused about things that it has said about you or inferred or whatever. We only want to record your user profile from explicitly what you say. So I just captured the last three messages that you've sent and then it does a stare and compare basically where it says, okay, based on this most recent chat message, is there any, one, is there any relevant user information? And if so, go ahead and update it. So let me show you how it updates that. So system update user profile. So this is a user profile document updater chat bot. This is the system message. Your role is to manage and update a UPD and chat bot, the chat GPT came up with this idea on its own. It created the UPD definition. Your primary responsibility is to parse updates supplied by the user, meticulously analyze them. It could also extend elements such as user preferences, significant life events and deeply held beliefs. Please refrain from incorporating non-essential data or unrelated topics. The result of your efforts should exclusively be an updated UPD. If the user's update doesn't contribute any new or significant information, your output should mirror the current UPD as indicated below. However, if you discover any relevant new information, your output should feature an updated UPD that assimilates these modifications. So basically it's an absurd, right? Or if there's no differences, just keep it the same, otherwise update it. You must prioritize brevity and clarity in your output, combining condensed information when appropriate to ensure succinctness and improve comprehension. Totally rewrite or restructure UPD as necessary, adhering to the list format. Your response should not include explanatory text or context, because you know how sometimes chat GPT will say, this is your new, you know, blah, blah, blah. So in this case, I have it very reliably just spit out the user profile. Oh, and then another thing is that because we're working with a limited window, I say the UPD should not exceed approximately 1,000 words. When revising the UPD, give precedence to the most significant and relevant information, extraneous or less impactful information should be omitted, et cetera, et cetera. So I give it the current word count and then the current UPD. So that way it kind of knows, because chat GPT, especially GPT-4 is better at counting words, but just giving it the explicit number makes it easier, right? Yeah, so that's my current user profile. So now let's dive back in here. The hard part was updating the knowledge base. So if this is your first run, the collection count is gonna be zero. And so then basically you just instantiate the whole thing. So we take the most recent chat logs, the main scratch pad and start a new KB article. Now, if the collection count is not zero, which is gonna be most of the time once you get started, what you do is you basically do the same thing where you say, okay, based on the most recent conversation, give me the most relevant document, which I probably could compress this and just use the same information here. Because this is the same, this is, well, generally find the same thing. Actually, no, that's not necessarily true because we've updated the main scratch pad. So scratch that. So if the new user input and chat GPT output connects to a different KB article, let's go ahead and get that document and that document ID. And what we'll do is we'll go ahead and use system update existing KB article. So this is a system instruction where it basically says all the same stuff, here's the current KB article, and then the user will now provide you with the new information to evaluate. And so that is gonna be here where you supply it the current KB article that it found as well as the scratch pad. And so it's like, okay, cool, now let's do the same thing that we did with the user profile, which is merge that information. If there's nothing new that's relevant, leave it alone, but if there is, go ahead and update it. And so then it saves all this out to the DB logs. And so if you go to DB logs out here, you'll see a whole bunch of update statements. So it says update documented, it gives you the UUID, and this is the final output. Actually, probably what I should do is modify this so it gives you the original, the original, the new information, and then the final output. So I'll add that as a to-do item, actually. Let's see, to-do, save more info in DB logs, probably as YAML file, original article, new info, and then final article. So yeah, that's something that I'll do. Now, that being said, one of the biggest problems that we have always had, so this is the cream of the crop. This is the triple crown right here. The biggest problem that everyone has always had with long-term chatbot memory is how the heck do you keep track of memories? How the heck do you keep track of different types of memories? Like some people have internal thoughts versus external thoughts and episodic memories and this, that, and the other. And you can certainly try and tag and categorize memories with different context, right? With metadata, and I certainly recommend that, especially once your cognitive architectures get more sophisticated, right? If you do have an out-of-band like thought, like internal private thoughts, definitely keep that separate. If you have external sensory information, definitely keep that separate. But what I'm working on here, rather than just being a way to focus on episodic memory, which that's what REMO was my previous attempt, this is a way to accumulate declarative information. And so declarative information is like a statement of fact, right? That's why it's called a KB article. So rather than just a timeline, rather than just a log, keeping track of everything in chronological order, the idea here is to connect new information to a KB article. So there's no reason that you couldn't do both as well, right, because this is how human memory works. Human memory is associative, but it's also temporal. Now, if the KB article gets too large, if you added information, and now it's more than a thousand words, then I have another system prompt, which you can check them all out here. So there's system instantiate new KB, system reflective journaling, I just showed you what that was, system split KB. So that's this one. But update user profile, update KB article, new KB article, reflective journaling and split KB. So these are the operations. These are the cognitive operations, the cognitive memory operations that it's gonna be doing. And so then basically what it does is say, hey, we're gonna give you a long KB article, split it into two, into two equal parts. And so the idea here is that over time, as your KB article gets bigger, it'll branch and metastasize naturally. And so you could then add a lot of additional metadata to this, such as like access rate or related articles or parent articles or previous articles, which means that you can naturally evolve a knowledge graph of your knowledge base over time. You can also do this out of band, just by doing semantic similarity and entity links and stuff. But it would be really cool to have a more sophisticated version of this that allows it to kind of follow that branching tree over time. So there you have it. That's kind of the whole thing. So that's the chat. And all this is just real basic, just housekeeping stuff. And then at the end of every instance, it does ChromaClient persist. So now let me show you, I included a second Python script. So it's just ChromaDB peak, which uses the ChromaDB peak function here. Let me just show you that script real quick. ChromaDB peak. So same stuff, you instantiate the client, you connect to it. It tells you how many entries, and then it will show you the top 10 entries. And so in my case, I should only have one entry. Let's see, so let's go up to the top. Yep, KB presently has one entries, here below the top 10 entries. And so here you can see that it's actually got several topics, because the way that it works is that it searches for the top one most relevant KB articles. And so that's always gonna return the first one. And the first one is not yet long enough to justify splitting up. But whatever I end up talking about, I'll keep talking with the thing, and eventually it'll split it up. So in this case, it looks like it'll probably talk about AI alignment. And then it's gonna also talk about my obsession with artificial intelligence and work-life balance. Because those are kinda like two centroid in this. So let me just go ahead and actually show you how this will ultimately work. So if we go to API logs, it should be the last one. Yes, here we go. So if I plug this in, let's go here. So that's the message that I'm gonna want it. And then let's grab the split, the split message. So you'll see what I mean by how it will ultimately kinda metastasize. Zoom in a little bit. All right, we're using GPT-4, temperature zero, maximum length, a thousand. All right, so basically what it's gonna do is, the end says the user will now provide you with the KB article to split. So I submit it, and now it's gonna look at this, and it's gonna say article one, and then article two. So let's see what it ultimately does. And you can see how slow it is. So this is why ultimately you're gonna wanna do this out of band as a threaded process or do it periodically, maybe break it up and do it when the user's offline or whatever, but you see how each article now is much more specific. And so then once you go into each of these articles in the future, identifying factors and seeking professional help if necessary, yeah. And so basically it'll allow the articles to metastasize over time. Now that being said, if no new information is added to an article, it won't update it. It's that simple. Now that being said, there will probably be a need to do some KB article grooming over time, but the idea is that the KB will only grow as much as it needs to and no more, no less, and it will only grow based on the things that you have talked about, and it will record it in these very succinct, concise articles. So then what happens is that it splits these two up, and then the final thing that the chatbot does is it will do an update for the first one and then add the second one. So it's that simple. And then when you do an update, as long as you don't, if you don't specify the embedding, it'll automatically recalculate the embedding, and then you're good to go. So I haven't quite got here yet, so it might break, but I think this kinda, yeah, I think that's about it. So like I said, it's over here. ChromaDB, public, chatbot should be all set. Yeah, all right, cool.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.4000000000000001, "text": " Check it, check it, check it, check it.", "tokens": [50364, 6881, 309, 11, 1520, 309, 11, 1520, 309, 11, 1520, 309, 13, 50434], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 1, "seek": 0, "start": 1.4000000000000001, "end": 2.36, "text": " Okay, level check.", "tokens": [50434, 1033, 11, 1496, 1520, 13, 50482], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 2, "seek": 0, "start": 2.36, "end": 4.04, "text": " I think we're here.", "tokens": [50482, 286, 519, 321, 434, 510, 13, 50566], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 3, "seek": 0, "start": 4.04, "end": 7.74, "text": " All right, so background.", "tokens": [50566, 1057, 558, 11, 370, 3678, 13, 50751], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 4, "seek": 0, "start": 7.74, "end": 9.36, "text": " It's been a little while since I've done,", "tokens": [50751, 467, 311, 668, 257, 707, 1339, 1670, 286, 600, 1096, 11, 50832], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 5, "seek": 0, "start": 9.36, "end": 10.8, "text": " you're over here now,", "tokens": [50832, 291, 434, 670, 510, 586, 11, 50904], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 6, "seek": 0, "start": 10.8, "end": 13.200000000000001, "text": " since I've done one of these tutorial coding videos,", "tokens": [50904, 1670, 286, 600, 1096, 472, 295, 613, 7073, 17720, 2145, 11, 51024], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 7, "seek": 0, "start": 13.200000000000001, "end": 15.200000000000001, "text": " but I woke up at like 2.30 in the morning", "tokens": [51024, 457, 286, 12852, 493, 412, 411, 568, 13, 3446, 294, 264, 2446, 51124], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 8, "seek": 0, "start": 15.200000000000001, "end": 16.96, "text": " and just had to work on this.", "tokens": [51124, 293, 445, 632, 281, 589, 322, 341, 13, 51212], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 9, "seek": 0, "start": 16.96, "end": 17.88, "text": " So I hammered it out.", "tokens": [51212, 407, 286, 13017, 292, 309, 484, 13, 51258], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 10, "seek": 0, "start": 17.88, "end": 21.16, "text": " Basically, the purpose of today's video", "tokens": [51258, 8537, 11, 264, 4334, 295, 965, 311, 960, 51422], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 11, "seek": 0, "start": 21.16, "end": 23.72, "text": " is to demonstrate using ChromaDB,", "tokens": [51422, 307, 281, 11698, 1228, 1721, 6440, 27735, 11, 51550], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 12, "seek": 0, "start": 23.72, "end": 27.04, "text": " which is a local vector database.", "tokens": [51550, 597, 307, 257, 2654, 8062, 8149, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 13, "seek": 0, "start": 27.04, "end": 29.36, "text": " It's basically like SQLite.", "tokens": [51716, 467, 311, 1936, 411, 19200, 642, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12774043613009983, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.09524523466825485}, {"id": 14, "seek": 2936, "start": 29.36, "end": 30.88, "text": " If you're familiar with that,", "tokens": [50364, 759, 291, 434, 4963, 365, 300, 11, 50440], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 15, "seek": 2936, "start": 30.88, "end": 34.519999999999996, "text": " which is a self-contained SQL database, relational database,", "tokens": [50440, 597, 307, 257, 2698, 12, 9000, 3563, 19200, 8149, 11, 38444, 8149, 11, 50622], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 16, "seek": 2936, "start": 34.519999999999996, "end": 37.56, "text": " this is functionally very similar to SQLite,", "tokens": [50622, 341, 307, 2445, 379, 588, 2531, 281, 19200, 642, 11, 50774], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 17, "seek": 2936, "start": 37.56, "end": 39.04, "text": " except it is a vector database,", "tokens": [50774, 3993, 309, 307, 257, 8062, 8149, 11, 50848], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 18, "seek": 2936, "start": 39.04, "end": 41.2, "text": " meaning it does semantic search.", "tokens": [50848, 3620, 309, 775, 47982, 3164, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 19, "seek": 2936, "start": 41.2, "end": 42.72, "text": " One thing that's really great about it", "tokens": [50956, 1485, 551, 300, 311, 534, 869, 466, 309, 51032], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 20, "seek": 2936, "start": 42.72, "end": 45.8, "text": " is that it has its own built-in embeddings tools.", "tokens": [51032, 307, 300, 309, 575, 1080, 1065, 3094, 12, 259, 12240, 29432, 3873, 13, 51186], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 21, "seek": 2936, "start": 45.8, "end": 47.6, "text": " I think it's based on BERT.", "tokens": [51186, 286, 519, 309, 311, 2361, 322, 363, 31479, 13, 51276], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 22, "seek": 2936, "start": 47.6, "end": 49.96, "text": " Anyways, you can check out all the documentation here", "tokens": [51276, 15585, 11, 291, 393, 1520, 484, 439, 264, 14333, 510, 51394], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 23, "seek": 2936, "start": 49.96, "end": 52.4, "text": " on trichroma.com.", "tokens": [51394, 322, 504, 480, 4397, 64, 13, 1112, 13, 51516], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 24, "seek": 2936, "start": 52.4, "end": 54.519999999999996, "text": " The Getting Started and Usage Guide is pretty good.", "tokens": [51516, 440, 13674, 39715, 293, 4958, 609, 18727, 307, 1238, 665, 13, 51622], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 25, "seek": 2936, "start": 54.519999999999996, "end": 56.760000000000005, "text": " It's not complete every now and then.", "tokens": [51622, 467, 311, 406, 3566, 633, 586, 293, 550, 13, 51734], "temperature": 0.0, "avg_logprob": -0.11691538956913634, "compression_ratio": 1.5913621262458473, "no_speech_prob": 0.0009696513297967613}, {"id": 26, "seek": 5676, "start": 56.76, "end": 60.879999999999995, "text": " I find that I have to go to the actual repo", "tokens": [50364, 286, 915, 300, 286, 362, 281, 352, 281, 264, 3539, 49040, 50570], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 27, "seek": 5676, "start": 60.879999999999995, "end": 63.0, "text": " to look at how some of the internals work,", "tokens": [50570, 281, 574, 412, 577, 512, 295, 264, 2154, 1124, 589, 11, 50676], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 28, "seek": 5676, "start": 63.0, "end": 65.12, "text": " but it is pretty brain-dead simple.", "tokens": [50676, 457, 309, 307, 1238, 3567, 12, 1479, 345, 2199, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 29, "seek": 5676, "start": 65.12, "end": 67.4, "text": " So let me just go ahead and show you.", "tokens": [50782, 407, 718, 385, 445, 352, 2286, 293, 855, 291, 13, 50896], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 30, "seek": 5676, "start": 67.4, "end": 68.72, "text": " This is my private instance.", "tokens": [50896, 639, 307, 452, 4551, 5197, 13, 50962], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 31, "seek": 5676, "start": 68.72, "end": 70.64, "text": " Oh, so before we get too lost,", "tokens": [50962, 876, 11, 370, 949, 321, 483, 886, 2731, 11, 51058], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 32, "seek": 5676, "start": 70.64, "end": 72.88, "text": " I do have a public instance,", "tokens": [51058, 286, 360, 362, 257, 1908, 5197, 11, 51170], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 33, "seek": 5676, "start": 72.88, "end": 76.16, "text": " daveshap slash chroma db underscore under chatbot", "tokens": [51170, 274, 5423, 71, 569, 17330, 16209, 64, 274, 65, 37556, 833, 5081, 18870, 51334], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 34, "seek": 5676, "start": 76.16, "end": 77.75999999999999, "text": " underscore public.", "tokens": [51334, 37556, 1908, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 35, "seek": 5676, "start": 77.75999999999999, "end": 80.96, "text": " I've got a little integration guide usage.", "tokens": [51414, 286, 600, 658, 257, 707, 10980, 5934, 14924, 13, 51574], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 36, "seek": 5676, "start": 80.96, "end": 83.0, "text": " It's, I mean, you probably don't need this.", "tokens": [51574, 467, 311, 11, 286, 914, 11, 291, 1391, 500, 380, 643, 341, 13, 51676], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 37, "seek": 5676, "start": 83.0, "end": 83.84, "text": " You can futz around with it,", "tokens": [51676, 509, 393, 1877, 89, 926, 365, 309, 11, 51718], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 38, "seek": 5676, "start": 83.84, "end": 86.32, "text": " but this will get you started.", "tokens": [51718, 457, 341, 486, 483, 291, 1409, 13, 51842], "temperature": 0.0, "avg_logprob": -0.1612316263133082, "compression_ratio": 1.6145833333333333, "no_speech_prob": 0.035137537866830826}, {"id": 39, "seek": 8632, "start": 86.32, "end": 90.96, "text": " I also use chatGBT to just get a really basic explanation", "tokens": [50364, 286, 611, 764, 5081, 8769, 51, 281, 445, 483, 257, 534, 3875, 10835, 50596], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 40, "seek": 8632, "start": 90.96, "end": 91.8, "text": " of the code.", "tokens": [50596, 295, 264, 3089, 13, 50638], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 41, "seek": 8632, "start": 91.8, "end": 94.27999999999999, "text": " You probably won't need it once you take a look at it.", "tokens": [50638, 509, 1391, 1582, 380, 643, 309, 1564, 291, 747, 257, 574, 412, 309, 13, 50762], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 42, "seek": 8632, "start": 94.27999999999999, "end": 97.72, "text": " So off the top, let me just show you how this thing works.", "tokens": [50762, 407, 766, 264, 1192, 11, 718, 385, 445, 855, 291, 577, 341, 551, 1985, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 43, "seek": 8632, "start": 97.72, "end": 100.28, "text": " So it's a basic chatbot.", "tokens": [50934, 407, 309, 311, 257, 3875, 5081, 18870, 13, 51062], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 44, "seek": 8632, "start": 100.28, "end": 101.75999999999999, "text": " You can see I didn't specify it.", "tokens": [51062, 509, 393, 536, 286, 994, 380, 16500, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 45, "seek": 8632, "start": 101.75999999999999, "end": 106.75999999999999, "text": " So it's getting all mini-LML6v2 from Huggingface.", "tokens": [51136, 407, 309, 311, 1242, 439, 8382, 12, 43, 12683, 21, 85, 17, 490, 46892, 3249, 2868, 13, 51386], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 46, "seek": 8632, "start": 107.28, "end": 111.0, "text": " Great, so it's like, hey, let's see.", "tokens": [51412, 3769, 11, 370, 309, 311, 411, 11, 4177, 11, 718, 311, 536, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 47, "seek": 8632, "start": 111.0, "end": 114.61999999999999, "text": " What were we talking about last?", "tokens": [51598, 708, 645, 321, 1417, 466, 1036, 30, 51779], "temperature": 0.0, "avg_logprob": -0.1388665517171224, "compression_ratio": 1.4897119341563787, "no_speech_prob": 0.0010004276409745216}, {"id": 48, "seek": 11462, "start": 114.62, "end": 117.22, "text": " This probably won't work because it's just gonna,", "tokens": [50364, 639, 1391, 1582, 380, 589, 570, 309, 311, 445, 799, 11, 50494], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 49, "seek": 11462, "start": 117.22, "end": 118.38000000000001, "text": " in the future, it wouldn't work", "tokens": [50494, 294, 264, 2027, 11, 309, 2759, 380, 589, 50552], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 50, "seek": 11462, "start": 118.38000000000001, "end": 120.22, "text": " because it's gonna have multiple KB articles", "tokens": [50552, 570, 309, 311, 799, 362, 3866, 591, 33, 11290, 50644], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 51, "seek": 11462, "start": 120.22, "end": 121.94, "text": " in the background.", "tokens": [50644, 294, 264, 3678, 13, 50730], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 52, "seek": 11462, "start": 121.94, "end": 124.30000000000001, "text": " Oh, I need to explain like all that.", "tokens": [50730, 876, 11, 286, 643, 281, 2903, 411, 439, 300, 13, 50848], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 53, "seek": 11462, "start": 124.30000000000001, "end": 126.74000000000001, "text": " So I know that I just said like KB articles,", "tokens": [50848, 407, 286, 458, 300, 286, 445, 848, 411, 591, 33, 11290, 11, 50970], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 54, "seek": 11462, "start": 126.74000000000001, "end": 128.1, "text": " don't worry, we'll get to it.", "tokens": [50970, 500, 380, 3292, 11, 321, 603, 483, 281, 309, 13, 51038], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 55, "seek": 11462, "start": 129.46, "end": 131.98000000000002, "text": " But anyways, I wanna show you that I just started it up", "tokens": [51106, 583, 13448, 11, 286, 1948, 855, 291, 300, 286, 445, 1409, 309, 493, 51232], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 56, "seek": 11462, "start": 131.98000000000002, "end": 134.26, "text": " and what it's gonna do is it's gonna use", "tokens": [51232, 293, 437, 309, 311, 799, 360, 307, 309, 311, 799, 764, 51346], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 57, "seek": 11462, "start": 134.26, "end": 138.34, "text": " the last few messages to search its internal KB article", "tokens": [51346, 264, 1036, 1326, 7897, 281, 3164, 1080, 6920, 591, 33, 7222, 51550], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 58, "seek": 11462, "start": 138.34, "end": 139.8, "text": " for the last information,", "tokens": [51550, 337, 264, 1036, 1589, 11, 51623], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 59, "seek": 11462, "start": 139.8, "end": 142.66, "text": " but it also has a user profile for me.", "tokens": [51623, 457, 309, 611, 575, 257, 4195, 7964, 337, 385, 13, 51766], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 60, "seek": 11462, "start": 142.66, "end": 144.12, "text": " In our previous conversations,", "tokens": [51766, 682, 527, 3894, 7315, 11, 51839], "temperature": 0.0, "avg_logprob": -0.12185989431783456, "compression_ratio": 1.7816901408450705, "no_speech_prob": 0.0017544487491250038}, {"id": 61, "seek": 14412, "start": 144.12, "end": 146.04, "text": " we discussed AI alignment, morality, ethics,", "tokens": [50364, 321, 7152, 7318, 18515, 11, 29106, 11, 19769, 11, 50460], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 62, "seek": 14412, "start": 146.04, "end": 147.92000000000002, "text": " and epistemology within AI development.", "tokens": [50460, 293, 2388, 43958, 1793, 1951, 7318, 3250, 13, 50554], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 63, "seek": 14412, "start": 147.92000000000002, "end": 149.28, "text": " You also shared your plans to communicate", "tokens": [50554, 509, 611, 5507, 428, 5482, 281, 7890, 50622], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 64, "seek": 14412, "start": 149.28, "end": 151.48000000000002, "text": " your ideas on YouTube, unplug your computer,", "tokens": [50622, 428, 3487, 322, 3088, 11, 39456, 428, 3820, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 65, "seek": 14412, "start": 151.48000000000002, "end": 152.68, "text": " and spend more time outdoors,", "tokens": [50732, 293, 3496, 544, 565, 20980, 11, 50792], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 66, "seek": 14412, "start": 152.68, "end": 153.84, "text": " and use digital wellness settings", "tokens": [50792, 293, 764, 4562, 23913, 6257, 50850], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 67, "seek": 14412, "start": 153.84, "end": 155.36, "text": " to improve your work-life balance.", "tokens": [50850, 281, 3470, 428, 589, 12, 9073, 4772, 13, 50926], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 68, "seek": 14412, "start": 155.36, "end": 156.4, "text": " Working on that.", "tokens": [50926, 18337, 322, 300, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 69, "seek": 14412, "start": 157.6, "end": 159.28, "text": " Additionally, we talked about your recent experience", "tokens": [51038, 19927, 11, 321, 2825, 466, 428, 5162, 1752, 51122], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 70, "seek": 14412, "start": 159.28, "end": 160.84, "text": " with severe insomnia and the importance", "tokens": [51122, 365, 8922, 1028, 45438, 293, 264, 7379, 51200], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 71, "seek": 14412, "start": 160.84, "end": 161.98000000000002, "text": " of maintaining a healthy balance", "tokens": [51200, 295, 14916, 257, 4627, 4772, 51257], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 72, "seek": 14412, "start": 161.98000000000002, "end": 163.52, "text": " between work and personal life.", "tokens": [51257, 1296, 589, 293, 2973, 993, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 73, "seek": 14412, "start": 163.52, "end": 166.68, "text": " Yes, that's actually why I created this chatbot.", "tokens": [51334, 1079, 11, 300, 311, 767, 983, 286, 2942, 341, 5081, 18870, 13, 51492], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 74, "seek": 14412, "start": 166.68, "end": 167.52, "text": " Let me show you.", "tokens": [51492, 961, 385, 855, 291, 13, 51534], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 75, "seek": 14412, "start": 167.52, "end": 168.36, "text": " So there was a,", "tokens": [51534, 407, 456, 390, 257, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1687311705420999, "compression_ratio": 1.6215384615384616, "no_speech_prob": 0.003823844948783517}, {"id": 76, "seek": 16836, "start": 169.36, "end": 170.64000000000001, "text": " there was,", "tokens": [50414, 456, 390, 11, 50478], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 77, "seek": 16836, "start": 173.20000000000002, "end": 174.04000000000002, "text": " there was,", "tokens": [50606, 456, 390, 11, 50648], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 78, "seek": 16836, "start": 175.76000000000002, "end": 176.86, "text": " God, my brain.", "tokens": [50734, 1265, 11, 452, 3567, 13, 50789], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 79, "seek": 16836, "start": 177.96, "end": 181.0, "text": " I was using chat GPT as a reflective journaling tool.", "tokens": [50844, 286, 390, 1228, 5081, 26039, 51, 382, 257, 28931, 17598, 4270, 2290, 13, 50996], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 80, "seek": 16836, "start": 181.0, "end": 183.84, "text": " So what I mean by that is", "tokens": [50996, 407, 437, 286, 914, 538, 300, 307, 51138], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 81, "seek": 16836, "start": 185.52, "end": 188.0, "text": " if you plug in this message,", "tokens": [51222, 498, 291, 5452, 294, 341, 3636, 11, 51346], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 82, "seek": 16836, "start": 188.0, "end": 190.52, "text": " and I know I'm scattered, I'm all over the place,", "tokens": [51346, 293, 286, 458, 286, 478, 21986, 11, 286, 478, 439, 670, 264, 1081, 11, 51472], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 83, "seek": 16836, "start": 190.52, "end": 192.48000000000002, "text": " this is what happens when I have severe insomnia.", "tokens": [51472, 341, 307, 437, 2314, 562, 286, 362, 8922, 1028, 45438, 13, 51570], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 84, "seek": 16836, "start": 192.48000000000002, "end": 195.60000000000002, "text": " Anyways, so basically I use chat GPT", "tokens": [51570, 15585, 11, 370, 1936, 286, 764, 5081, 26039, 51, 51726], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 85, "seek": 16836, "start": 195.60000000000002, "end": 197.76000000000002, "text": " as a reflective journaling tool to figure out", "tokens": [51726, 382, 257, 28931, 17598, 4270, 2290, 281, 2573, 484, 51834], "temperature": 0.0, "avg_logprob": -0.18139487331353346, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.003376204054802656}, {"id": 86, "seek": 19776, "start": 197.79999999999998, "end": 199.42, "text": " like how I'm feeling about things,", "tokens": [50366, 411, 577, 286, 478, 2633, 466, 721, 11, 50447], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 87, "seek": 19776, "start": 199.42, "end": 201.12, "text": " because as an autistic person,", "tokens": [50447, 570, 382, 364, 33272, 954, 11, 50532], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 88, "seek": 19776, "start": 201.12, "end": 202.72, "text": " I often need help with this.", "tokens": [50532, 286, 2049, 643, 854, 365, 341, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 89, "seek": 19776, "start": 202.72, "end": 205.32, "text": " And I don't like journaling because just talking to a page", "tokens": [50612, 400, 286, 500, 380, 411, 17598, 4270, 570, 445, 1417, 281, 257, 3028, 50742], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 90, "seek": 19776, "start": 205.32, "end": 206.88, "text": " is kind of dumb,", "tokens": [50742, 307, 733, 295, 10316, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 91, "seek": 19776, "start": 206.88, "end": 210.95999999999998, "text": " but it's like, hey, I need to talk something out.", "tokens": [50820, 457, 309, 311, 411, 11, 4177, 11, 286, 643, 281, 751, 746, 484, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 92, "seek": 19776, "start": 210.95999999999998, "end": 212.44, "text": " And so anyways,", "tokens": [51024, 400, 370, 13448, 11, 51098], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 93, "seek": 19776, "start": 214.84, "end": 218.84, "text": " by workshopping this system message with chat GPT,", "tokens": [51218, 538, 13541, 3381, 341, 1185, 3636, 365, 5081, 26039, 51, 11, 51418], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 94, "seek": 19776, "start": 218.84, "end": 222.76, "text": " I came up with a pretty good reflective journaling tool.", "tokens": [51418, 286, 1361, 493, 365, 257, 1238, 665, 28931, 17598, 4270, 2290, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 95, "seek": 19776, "start": 222.76, "end": 225.35999999999999, "text": " So you could say that this is a therapeutic tool,", "tokens": [51614, 407, 291, 727, 584, 300, 341, 307, 257, 30395, 2290, 11, 51744], "temperature": 0.0, "avg_logprob": -0.1397706681648187, "compression_ratio": 1.5951417004048583, "no_speech_prob": 0.0013247835449874401}, {"id": 96, "seek": 22536, "start": 225.36, "end": 228.44000000000003, "text": " but by couching it in the language of reflective journaling,", "tokens": [50364, 457, 538, 16511, 278, 309, 294, 264, 2856, 295, 28931, 17598, 4270, 11, 50518], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 97, "seek": 22536, "start": 228.44000000000003, "end": 232.20000000000002, "text": " it's not like medical therapy or psychotherapy or anything.", "tokens": [50518, 309, 311, 406, 411, 4625, 9492, 420, 4681, 23208, 420, 1340, 13, 50706], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 98, "seek": 22536, "start": 233.0, "end": 235.44000000000003, "text": " But it's just like, you know, I can say like,", "tokens": [50746, 583, 309, 311, 445, 411, 11, 291, 458, 11, 286, 393, 584, 411, 11, 50868], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 99, "seek": 22536, "start": 235.44000000000003, "end": 240.36, "text": " I have been working so hard and I don't know why.", "tokens": [50868, 286, 362, 668, 1364, 370, 1152, 293, 286, 500, 380, 458, 983, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 100, "seek": 22536, "start": 240.36, "end": 241.68, "text": " I actually do know why now,", "tokens": [51114, 286, 767, 360, 458, 983, 586, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 101, "seek": 22536, "start": 241.68, "end": 244.64000000000001, "text": " but this is kind of a shorthand of the conversation I had.", "tokens": [51180, 457, 341, 307, 733, 295, 257, 402, 2652, 474, 295, 264, 3761, 286, 632, 13, 51328], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 102, "seek": 22536, "start": 245.52, "end": 247.04000000000002, "text": " Let's try and figure out, it's driving you so hard.", "tokens": [51372, 961, 311, 853, 293, 2573, 484, 11, 309, 311, 4840, 291, 370, 1152, 13, 51448], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 103, "seek": 22536, "start": 247.04000000000002, "end": 248.28000000000003, "text": " Can you think of any specific goals", "tokens": [51448, 1664, 291, 519, 295, 604, 2685, 5493, 51510], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 104, "seek": 22536, "start": 248.28000000000003, "end": 250.28000000000003, "text": " that might be pushing you to this extra effort?", "tokens": [51510, 300, 1062, 312, 7380, 291, 281, 341, 2857, 4630, 30, 51610], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 105, "seek": 22536, "start": 250.28000000000003, "end": 253.04000000000002, "text": " So you see how the tone of this is much more straightforward", "tokens": [51610, 407, 291, 536, 577, 264, 8027, 295, 341, 307, 709, 544, 15325, 51748], "temperature": 0.0, "avg_logprob": -0.13465840181858418, "compression_ratio": 1.6722408026755853, "no_speech_prob": 0.00035695265978574753}, {"id": 106, "seek": 25304, "start": 253.04, "end": 255.04, "text": " and it's very focused by asking", "tokens": [50364, 293, 309, 311, 588, 5178, 538, 3365, 50464], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 107, "seek": 25304, "start": 255.04, "end": 257.56, "text": " those like kind of probing follow-up questions.", "tokens": [50464, 729, 411, 733, 295, 1239, 278, 1524, 12, 1010, 1651, 13, 50590], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 108, "seek": 25304, "start": 257.56, "end": 261.71999999999997, "text": " This is why, you know, it's in the investigation phase.", "tokens": [50590, 639, 307, 983, 11, 291, 458, 11, 309, 311, 294, 264, 9627, 5574, 13, 50798], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 109, "seek": 25304, "start": 261.71999999999997, "end": 264.32, "text": " Anyways, so I had this idea and I was like, okay,", "tokens": [50798, 15585, 11, 370, 286, 632, 341, 1558, 293, 286, 390, 411, 11, 1392, 11, 50928], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 110, "seek": 25304, "start": 264.32, "end": 265.88, "text": " this is great, but I need,", "tokens": [50928, 341, 307, 869, 11, 457, 286, 643, 11, 51006], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 111, "seek": 25304, "start": 265.88, "end": 268.76, "text": " if I'm gonna use this as a long-term journaling tool,", "tokens": [51006, 498, 286, 478, 799, 764, 341, 382, 257, 938, 12, 7039, 17598, 4270, 2290, 11, 51150], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 112, "seek": 25304, "start": 268.76, "end": 270.32, "text": " I'm gonna need this locally", "tokens": [51150, 286, 478, 799, 643, 341, 16143, 51228], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 113, "seek": 25304, "start": 270.32, "end": 272.2, "text": " and I'm gonna need persistent storage", "tokens": [51228, 293, 286, 478, 799, 643, 24315, 6725, 51322], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 114, "seek": 25304, "start": 272.2, "end": 276.2, "text": " because as this is just the playground,", "tokens": [51322, 570, 382, 341, 307, 445, 264, 24646, 11, 51522], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 115, "seek": 25304, "start": 276.2, "end": 279.15999999999997, "text": " if I do a refresh, it's gone and that's no good.", "tokens": [51522, 498, 286, 360, 257, 15134, 11, 309, 311, 2780, 293, 300, 311, 572, 665, 13, 51670], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 116, "seek": 25304, "start": 280.6, "end": 282.96, "text": " So actually here, let me go ahead and just save this", "tokens": [51742, 407, 767, 510, 11, 718, 385, 352, 2286, 293, 445, 3155, 341, 51860], "temperature": 0.0, "avg_logprob": -0.10027462032669825, "compression_ratio": 1.6989247311827957, "no_speech_prob": 0.0009398808469995856}, {"id": 117, "seek": 28296, "start": 283.79999999999995, "end": 286.64, "text": " to the, we're gonna call this the system message", "tokens": [50406, 281, 264, 11, 321, 434, 799, 818, 341, 264, 1185, 3636, 50548], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 118, "seek": 28296, "start": 286.64, "end": 288.91999999999996, "text": " for reflective journaling.", "tokens": [50548, 337, 28931, 17598, 4270, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 119, "seek": 28296, "start": 288.91999999999996, "end": 290.47999999999996, "text": " So you can use this if you want.", "tokens": [50662, 407, 291, 393, 764, 341, 498, 291, 528, 13, 50740], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 120, "seek": 28296, "start": 291.4, "end": 294.52, "text": " All right, so anyways, so you see it has this", "tokens": [50786, 1057, 558, 11, 370, 13448, 11, 370, 291, 536, 309, 575, 341, 50942], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 121, "seek": 28296, "start": 294.52, "end": 296.84, "text": " and then you see it says updating user profile", "tokens": [50942, 293, 550, 291, 536, 309, 1619, 25113, 4195, 7964, 51058], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 122, "seek": 28296, "start": 296.84, "end": 300.67999999999995, "text": " and updating KB, okay, cool.", "tokens": [51058, 293, 25113, 591, 33, 11, 1392, 11, 1627, 13, 51250], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 123, "seek": 28296, "start": 300.67999999999995, "end": 304.0, "text": " So you see that it fundamentally basic chat bot.", "tokens": [51250, 407, 291, 536, 300, 309, 17879, 3875, 5081, 10592, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 124, "seek": 28296, "start": 304.0, "end": 306.0, "text": " So now let's start to unpack it.", "tokens": [51416, 407, 586, 718, 311, 722, 281, 26699, 309, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 125, "seek": 28296, "start": 306.0, "end": 311.0, "text": " So first we will go look at the, just the chat file.", "tokens": [51516, 407, 700, 321, 486, 352, 574, 412, 264, 11, 445, 264, 5081, 3991, 13, 51766], "temperature": 0.0, "avg_logprob": -0.1524619879545989, "compression_ratio": 1.6294642857142858, "no_speech_prob": 0.00018521861056797206}, {"id": 126, "seek": 31100, "start": 311.0, "end": 314.68, "text": " So this is a super brain dead simple chat bot", "tokens": [50364, 407, 341, 307, 257, 1687, 3567, 3116, 2199, 5081, 10592, 50548], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 127, "seek": 31100, "start": 314.68, "end": 316.72, "text": " with infinite memory, infinite memory.", "tokens": [50548, 365, 13785, 4675, 11, 13785, 4675, 13, 50650], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 128, "seek": 31100, "start": 316.72, "end": 318.68, "text": " I know some people got grumpy when I said", "tokens": [50650, 286, 458, 512, 561, 658, 677, 36142, 562, 286, 848, 50748], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 129, "seek": 31100, "start": 318.68, "end": 320.9, "text": " that Pinecone had infinite memory.", "tokens": [50748, 300, 33531, 66, 546, 632, 13785, 4675, 13, 50859], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 130, "seek": 31100, "start": 320.9, "end": 322.04, "text": " From a human standpoint,", "tokens": [50859, 3358, 257, 1952, 15827, 11, 50916], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 131, "seek": 31100, "start": 322.04, "end": 323.72, "text": " it functionally has infinite memory", "tokens": [50916, 309, 2445, 379, 575, 13785, 4675, 51000], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 132, "seek": 31100, "start": 323.72, "end": 326.44, "text": " because, you know, this thing can hold", "tokens": [51000, 570, 11, 291, 458, 11, 341, 551, 393, 1797, 51136], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 133, "seek": 31100, "start": 326.44, "end": 328.16, "text": " probably a million KB articles,", "tokens": [51136, 1391, 257, 2459, 591, 33, 11290, 11, 51222], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 134, "seek": 31100, "start": 328.16, "end": 331.96, "text": " which is more than enough to document your entire life.", "tokens": [51222, 597, 307, 544, 813, 1547, 281, 4166, 428, 2302, 993, 13, 51412], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 135, "seek": 31100, "start": 331.96, "end": 336.04, "text": " So from a human standpoint, it is functionally infinite.", "tokens": [51412, 407, 490, 257, 1952, 15827, 11, 309, 307, 2445, 379, 13785, 13, 51616], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 136, "seek": 31100, "start": 336.04, "end": 337.72, "text": " All right, so from the top,", "tokens": [51616, 1057, 558, 11, 370, 490, 264, 1192, 11, 51700], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 137, "seek": 31100, "start": 337.72, "end": 340.16, "text": " we've got a few basic utility functions,", "tokens": [51700, 321, 600, 658, 257, 1326, 3875, 14877, 6828, 11, 51822], "temperature": 0.0, "avg_logprob": -0.10857877135276794, "compression_ratio": 1.7790262172284643, "no_speech_prob": 0.00023780993069522083}, {"id": 138, "seek": 34016, "start": 340.16, "end": 342.54, "text": " save yaml save file, open file,", "tokens": [50364, 3155, 288, 335, 75, 3155, 3991, 11, 1269, 3991, 11, 50483], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 139, "seek": 34016, "start": 342.54, "end": 346.20000000000005, "text": " and then a chat bot, which calls the GPT-4 model.", "tokens": [50483, 293, 550, 257, 5081, 10592, 11, 597, 5498, 264, 26039, 51, 12, 19, 2316, 13, 50666], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 140, "seek": 34016, "start": 346.20000000000005, "end": 348.46000000000004, "text": " You could switch this out to 3.5 turbo.", "tokens": [50666, 509, 727, 3679, 341, 484, 281, 805, 13, 20, 20902, 13, 50779], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 141, "seek": 34016, "start": 348.46000000000004, "end": 350.64000000000004, "text": " If you don't have access to GPT-4 yet,", "tokens": [50779, 759, 291, 500, 380, 362, 2105, 281, 26039, 51, 12, 19, 1939, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 142, "seek": 34016, "start": 350.64000000000004, "end": 352.56, "text": " it does not work as well.", "tokens": [50888, 309, 775, 406, 589, 382, 731, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 143, "seek": 34016, "start": 352.56, "end": 355.28000000000003, "text": " There's a reason that I use GPT-4 because it is smarter.", "tokens": [50984, 821, 311, 257, 1778, 300, 286, 764, 26039, 51, 12, 19, 570, 309, 307, 20294, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 144, "seek": 34016, "start": 355.28000000000003, "end": 357.04, "text": " I also set the temperature to zero", "tokens": [51120, 286, 611, 992, 264, 4292, 281, 4018, 51208], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 145, "seek": 34016, "start": 357.04, "end": 358.70000000000005, "text": " because I don't like it to be too creative,", "tokens": [51208, 570, 286, 500, 380, 411, 309, 281, 312, 886, 5880, 11, 51291], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 146, "seek": 34016, "start": 358.70000000000005, "end": 361.76000000000005, "text": " especially with a lot of the functions that I have it doing.", "tokens": [51291, 2318, 365, 257, 688, 295, 264, 6828, 300, 286, 362, 309, 884, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 147, "seek": 34016, "start": 361.76000000000005, "end": 364.20000000000005, "text": " You actually want it to be more deterministic", "tokens": [51444, 509, 767, 528, 309, 281, 312, 544, 15957, 3142, 51566], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 148, "seek": 34016, "start": 364.20000000000005, "end": 366.24, "text": " or mechanistic and that you wanna get", "tokens": [51566, 420, 4236, 3142, 293, 300, 291, 1948, 483, 51668], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 149, "seek": 34016, "start": 366.24, "end": 367.40000000000003, "text": " the same results every time,", "tokens": [51668, 264, 912, 3542, 633, 565, 11, 51726], "temperature": 0.0, "avg_logprob": -0.1408401037517347, "compression_ratio": 1.6644295302013423, "no_speech_prob": 0.006486319936811924}, {"id": 150, "seek": 36740, "start": 367.4, "end": 370.64, "text": " especially when you're updating the user profile", "tokens": [50364, 2318, 562, 291, 434, 25113, 264, 4195, 7964, 50526], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 151, "seek": 36740, "start": 370.64, "end": 372.52, "text": " and the KB articles.", "tokens": [50526, 293, 264, 591, 33, 11290, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 152, "seek": 36740, "start": 372.52, "end": 375.79999999999995, "text": " You can see right here that every time you call the chat bot,", "tokens": [50620, 509, 393, 536, 558, 510, 300, 633, 565, 291, 818, 264, 5081, 10592, 11, 50784], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 153, "seek": 36740, "start": 375.79999999999995, "end": 380.32, "text": " I dump the whole thing to apilogs slash convo", "tokens": [50784, 286, 11430, 264, 1379, 551, 281, 1882, 388, 664, 82, 17330, 416, 3080, 51010], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 154, "seek": 36740, "start": 381.28, "end": 382.47999999999996, "text": " and it's a yaml file.", "tokens": [51058, 293, 309, 311, 257, 288, 335, 75, 3991, 13, 51118], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 155, "seek": 36740, "start": 382.47999999999996, "end": 385.38, "text": " So here's my private one.", "tokens": [51118, 407, 510, 311, 452, 4551, 472, 13, 51263], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 156, "seek": 36740, "start": 385.38, "end": 387.64, "text": " So apilogs, here's an example.", "tokens": [51263, 407, 1882, 388, 664, 82, 11, 510, 311, 364, 1365, 13, 51376], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 157, "seek": 36740, "start": 387.64, "end": 391.44, "text": " So each item is gonna be here.", "tokens": [51376, 407, 1184, 3174, 307, 799, 312, 510, 13, 51566], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 158, "seek": 36740, "start": 391.44, "end": 392.47999999999996, "text": " Actually, that's not a good one", "tokens": [51566, 5135, 11, 300, 311, 406, 257, 665, 472, 51618], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 159, "seek": 36740, "start": 392.47999999999996, "end": 394.7, "text": " because I changed the way that it saves it.", "tokens": [51618, 570, 286, 3105, 264, 636, 300, 309, 19155, 309, 13, 51729], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 160, "seek": 36740, "start": 394.7, "end": 397.03999999999996, "text": " Let me show you a more recent one.", "tokens": [51729, 961, 385, 855, 291, 257, 544, 5162, 472, 13, 51846], "temperature": 0.0, "avg_logprob": -0.12848935164804534, "compression_ratio": 1.6178861788617886, "no_speech_prob": 0.0026312852278351784}, {"id": 161, "seek": 39704, "start": 397.04, "end": 400.6, "text": " So the first element is always gonna be the system message", "tokens": [50364, 407, 264, 700, 4478, 307, 1009, 799, 312, 264, 1185, 3636, 50542], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 162, "seek": 39704, "start": 402.28000000000003, "end": 404.76000000000005, "text": " that was in the last convo.", "tokens": [50626, 300, 390, 294, 264, 1036, 416, 3080, 13, 50750], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 163, "seek": 39704, "start": 404.76000000000005, "end": 406.6, "text": " So then here's the KB article", "tokens": [50750, 407, 550, 510, 311, 264, 591, 33, 7222, 50842], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 164, "seek": 39704, "start": 406.6, "end": 409.08000000000004, "text": " and you can see that it was updating the KB article.", "tokens": [50842, 293, 291, 393, 536, 300, 309, 390, 25113, 264, 591, 33, 7222, 13, 50966], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 165, "seek": 39704, "start": 409.08000000000004, "end": 413.84000000000003, "text": " And so each one of these items is like,", "tokens": [50966, 400, 370, 1184, 472, 295, 613, 4754, 307, 411, 11, 51204], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 166, "seek": 39704, "start": 415.32000000000005, "end": 416.76, "text": " you'll see, but anyways,", "tokens": [51278, 291, 603, 536, 11, 457, 13448, 11, 51350], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 167, "seek": 39704, "start": 416.76, "end": 419.56, "text": " I just wanted to show that it logs everything", "tokens": [51350, 286, 445, 1415, 281, 855, 300, 309, 20820, 1203, 51490], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 168, "seek": 39704, "start": 419.56, "end": 421.64000000000004, "text": " because well, sometimes it does things", "tokens": [51490, 570, 731, 11, 2171, 309, 775, 721, 51594], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 169, "seek": 39704, "start": 421.64000000000004, "end": 423.20000000000005, "text": " that you don't understand.", "tokens": [51594, 300, 291, 500, 380, 1223, 13, 51672], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 170, "seek": 39704, "start": 423.20000000000005, "end": 426.36, "text": " All right, so that's an example of the apilog", "tokens": [51672, 1057, 558, 11, 370, 300, 311, 364, 1365, 295, 264, 1882, 388, 664, 51830], "temperature": 0.0, "avg_logprob": -0.15156016433448122, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0002694555732887238}, {"id": 171, "seek": 42636, "start": 426.36, "end": 428.92, "text": " and then if the conversation,", "tokens": [50364, 293, 550, 498, 264, 3761, 11, 50492], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 172, "seek": 42636, "start": 428.92, "end": 431.2, "text": " if the overall conversation is too long,", "tokens": [50492, 498, 264, 4787, 3761, 307, 886, 938, 11, 50606], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 173, "seek": 42636, "start": 431.2, "end": 435.40000000000003, "text": " it'll go ahead and trim the oldest chat message.", "tokens": [50606, 309, 603, 352, 2286, 293, 10445, 264, 14026, 5081, 3636, 13, 50816], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 174, "seek": 42636, "start": 435.40000000000003, "end": 440.40000000000003, "text": " So the chat GPT web interface does this automatically", "tokens": [50816, 407, 264, 5081, 26039, 51, 3670, 9226, 775, 341, 6772, 51066], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 175, "seek": 42636, "start": 440.78000000000003, "end": 444.3, "text": " where it'll just kind of groom the backlog of messages.", "tokens": [51085, 689, 309, 603, 445, 733, 295, 22198, 264, 47364, 295, 7897, 13, 51261], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 176, "seek": 42636, "start": 444.3, "end": 445.64, "text": " So we have to do this manually.", "tokens": [51261, 407, 321, 362, 281, 360, 341, 16945, 13, 51328], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 177, "seek": 42636, "start": 445.64, "end": 448.26, "text": " So I just have it cut off at 7,000 tokens.", "tokens": [51328, 407, 286, 445, 362, 309, 1723, 766, 412, 1614, 11, 1360, 22667, 13, 51459], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 178, "seek": 42636, "start": 448.26, "end": 452.48, "text": " You could probably do like 7,500 if you want to", "tokens": [51459, 509, 727, 1391, 360, 411, 1614, 11, 7526, 498, 291, 528, 281, 51670], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 179, "seek": 42636, "start": 452.48, "end": 454.62, "text": " because a lot of these are gonna be limited", "tokens": [51670, 570, 257, 688, 295, 613, 366, 799, 312, 5567, 51777], "temperature": 0.0, "avg_logprob": -0.10266152653125447, "compression_ratio": 1.616326530612245, "no_speech_prob": 0.0004728091007564217}, {"id": 180, "seek": 45462, "start": 454.62, "end": 457.94, "text": " but you have a user profile and a KB article", "tokens": [50364, 457, 291, 362, 257, 4195, 7964, 293, 257, 591, 33, 7222, 50530], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 181, "seek": 45462, "start": 457.94, "end": 461.7, "text": " that gets wedged in which are both up to 1,000 words", "tokens": [50530, 300, 2170, 6393, 3004, 294, 597, 366, 1293, 493, 281, 502, 11, 1360, 2283, 50718], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 182, "seek": 45462, "start": 461.7, "end": 463.98, "text": " which could be around 1,000 tokens.", "tokens": [50718, 597, 727, 312, 926, 502, 11, 1360, 22667, 13, 50832], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 183, "seek": 45462, "start": 463.98, "end": 466.98, "text": " So having it trim at 7,000 is probably where you want it.", "tokens": [50832, 407, 1419, 309, 10445, 412, 1614, 11, 1360, 307, 1391, 689, 291, 528, 309, 13, 50982], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 184, "seek": 45462, "start": 467.94, "end": 471.18, "text": " So that's the primary, those are the helper functions", "tokens": [51030, 407, 300, 311, 264, 6194, 11, 729, 366, 264, 36133, 6828, 51192], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 185, "seek": 45462, "start": 471.18, "end": 474.3, "text": " and then you have a super straightforward,", "tokens": [51192, 293, 550, 291, 362, 257, 1687, 15325, 11, 51348], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 186, "seek": 45462, "start": 474.3, "end": 477.4, "text": " you instantiate ChromaDB right here.", "tokens": [51348, 291, 9836, 13024, 1721, 6440, 27735, 558, 510, 13, 51503], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 187, "seek": 45462, "start": 477.4, "end": 479.58, "text": " So you set the persistent directory", "tokens": [51503, 407, 291, 992, 264, 24315, 21120, 51612], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 188, "seek": 45462, "start": 479.58, "end": 482.42, "text": " which is, I have it right here, ChromaDB.", "tokens": [51612, 597, 307, 11, 286, 362, 309, 558, 510, 11, 1721, 6440, 27735, 13, 51754], "temperature": 0.0, "avg_logprob": -0.12801139930198932, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.00031501808553002775}, {"id": 189, "seek": 48242, "start": 482.42, "end": 486.1, "text": " So this is my instance, my personal instance of ChromaDB.", "tokens": [50364, 407, 341, 307, 452, 5197, 11, 452, 2973, 5197, 295, 1721, 6440, 27735, 13, 50548], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 190, "seek": 48242, "start": 486.1, "end": 488.7, "text": " It's not gonna be the one that you find up here.", "tokens": [50548, 467, 311, 406, 799, 312, 264, 472, 300, 291, 915, 493, 510, 13, 50678], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 191, "seek": 48242, "start": 488.7, "end": 489.86, "text": " This is the public version.", "tokens": [50678, 639, 307, 264, 1908, 3037, 13, 50736], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 192, "seek": 48242, "start": 489.86, "end": 491.14000000000004, "text": " So if you go into ChromaDB,", "tokens": [50736, 407, 498, 291, 352, 666, 1721, 6440, 27735, 11, 50800], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 193, "seek": 48242, "start": 491.14000000000004, "end": 493.66, "text": " you'll see just a placeholder file", "tokens": [50800, 291, 603, 536, 445, 257, 1081, 20480, 3991, 50926], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 194, "seek": 48242, "start": 493.66, "end": 496.1, "text": " so that the folder's already there.", "tokens": [50926, 370, 300, 264, 10820, 311, 1217, 456, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 195, "seek": 48242, "start": 496.1, "end": 498.6, "text": " You don't need to instantiate it.", "tokens": [51048, 509, 500, 380, 643, 281, 9836, 13024, 309, 13, 51173], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 196, "seek": 48242, "start": 498.6, "end": 501.02000000000004, "text": " Let's see, going back to here.", "tokens": [51173, 961, 311, 536, 11, 516, 646, 281, 510, 13, 51294], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 197, "seek": 48242, "start": 501.02000000000004, "end": 504.78000000000003, "text": " So ChromaClient, so we instantiate the ChromaDB client.", "tokens": [51294, 407, 1721, 6440, 9966, 1196, 11, 370, 321, 9836, 13024, 264, 1721, 6440, 27735, 6423, 13, 51482], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 198, "seek": 48242, "start": 504.78000000000003, "end": 507.34000000000003, "text": " This is again, almost identical to SQLite", "tokens": [51482, 639, 307, 797, 11, 1920, 14800, 281, 19200, 642, 51610], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 199, "seek": 48242, "start": 508.58000000000004, "end": 510.02000000000004, "text": " or other similar things.", "tokens": [51672, 420, 661, 2531, 721, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 200, "seek": 48242, "start": 510.02000000000004, "end": 511.18, "text": " So about a year ago,", "tokens": [51744, 407, 466, 257, 1064, 2057, 11, 51802], "temperature": 0.0, "avg_logprob": -0.12727825883506, "compression_ratio": 1.6742424242424243, "no_speech_prob": 0.0007553600589744747}, {"id": 201, "seek": 51118, "start": 511.18, "end": 512.9, "text": " I tried to do basically the same thing.", "tokens": [50364, 286, 3031, 281, 360, 1936, 264, 912, 551, 13, 50450], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 202, "seek": 51118, "start": 512.9, "end": 515.22, "text": " I called it VDB Lite for Vector Database Lite", "tokens": [50450, 286, 1219, 309, 691, 27735, 32986, 337, 691, 20814, 40461, 651, 32986, 50566], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 203, "seek": 51118, "start": 515.22, "end": 518.58, "text": " instead of SQL Lite, Structured Query Language Lite.", "tokens": [50566, 2602, 295, 19200, 32986, 11, 745, 46847, 2326, 2109, 24445, 32986, 13, 50734], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 204, "seek": 51118, "start": 518.58, "end": 520.66, "text": " But this company went and did the same thing", "tokens": [50734, 583, 341, 2237, 1437, 293, 630, 264, 912, 551, 50838], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 205, "seek": 51118, "start": 520.66, "end": 521.5, "text": " and I think they've already got", "tokens": [50838, 293, 286, 519, 436, 600, 1217, 658, 50880], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 206, "seek": 51118, "start": 521.5, "end": 523.38, "text": " like a $30 million valuation or something.", "tokens": [50880, 411, 257, 1848, 3446, 2459, 38546, 420, 746, 13, 50974], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 207, "seek": 51118, "start": 523.38, "end": 525.96, "text": " I was like, damn, I should have stuck with that.", "tokens": [50974, 286, 390, 411, 11, 8151, 11, 286, 820, 362, 5541, 365, 300, 13, 51103], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 208, "seek": 51118, "start": 525.96, "end": 527.48, "text": " Anyways, they figured it out.", "tokens": [51103, 15585, 11, 436, 8932, 309, 484, 13, 51179], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 209, "seek": 51118, "start": 527.48, "end": 530.0600000000001, "text": " I think it's based on the same underpinning technology.", "tokens": [51179, 286, 519, 309, 311, 2361, 322, 264, 912, 833, 17836, 773, 2899, 13, 51308], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 210, "seek": 51118, "start": 530.0600000000001, "end": 533.58, "text": " They're using an open source embedding transformer.", "tokens": [51308, 814, 434, 1228, 364, 1269, 4009, 12240, 3584, 31782, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 211, "seek": 51118, "start": 533.58, "end": 538.0600000000001, "text": " I think they're also using the Facebook AI semantic search", "tokens": [51484, 286, 519, 436, 434, 611, 1228, 264, 4384, 7318, 47982, 3164, 51708], "temperature": 0.0, "avg_logprob": -0.12538261974559112, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.0015010597417131066}, {"id": 212, "seek": 53806, "start": 538.06, "end": 541.9399999999999, "text": " and the device engine and the background.", "tokens": [50364, 293, 264, 4302, 2848, 293, 264, 3678, 13, 50558], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 213, "seek": 53806, "start": 541.9399999999999, "end": 543.9, "text": " Anyways, so you instantiate the client.", "tokens": [50558, 15585, 11, 370, 291, 9836, 13024, 264, 6423, 13, 50656], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 214, "seek": 53806, "start": 543.9, "end": 546.66, "text": " You need to use the settings to have a persistent directory", "tokens": [50656, 509, 643, 281, 764, 264, 6257, 281, 362, 257, 24315, 21120, 50794], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 215, "seek": 53806, "start": 546.66, "end": 551.0, "text": " because by default, this entire thing is fully ephemeral.", "tokens": [50794, 570, 538, 7576, 11, 341, 2302, 551, 307, 4498, 308, 41245, 2790, 13, 51011], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 216, "seek": 53806, "start": 551.0, "end": 552.5, "text": " I think it does cache it somewhere,", "tokens": [51011, 286, 519, 309, 775, 19459, 309, 4079, 11, 51086], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 217, "seek": 53806, "start": 552.5, "end": 555.18, "text": " but I wanted to be very explicit saying save it here", "tokens": [51086, 457, 286, 1415, 281, 312, 588, 13691, 1566, 3155, 309, 510, 51220], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 218, "seek": 53806, "start": 556.6199999999999, "end": 558.0999999999999, "text": " for reusability.", "tokens": [51292, 337, 38860, 2310, 13, 51366], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 219, "seek": 53806, "start": 558.0999999999999, "end": 560.06, "text": " And so then collection is ChromaClient,", "tokens": [51366, 400, 370, 550, 5765, 307, 1721, 6440, 9966, 1196, 11, 51464], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 220, "seek": 53806, "start": 560.06, "end": 563.1199999999999, "text": " get or create collection named Knowledge Base.", "tokens": [51464, 483, 420, 1884, 5765, 4926, 32906, 21054, 13, 51617], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 221, "seek": 53806, "start": 563.1199999999999, "end": 566.2199999999999, "text": " So this is my personal Knowledge Base.", "tokens": [51617, 407, 341, 307, 452, 2973, 32906, 21054, 13, 51772], "temperature": 0.0, "avg_logprob": -0.23552915028163365, "compression_ratio": 1.6264150943396227, "no_speech_prob": 0.011685178615152836}, {"id": 222, "seek": 56622, "start": 566.22, "end": 569.62, "text": " Then we instantiate the conversation", "tokens": [50364, 1396, 321, 9836, 13024, 264, 3761, 50534], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 223, "seek": 56622, "start": 569.62, "end": 572.0600000000001, "text": " with open AI, the chatbot.", "tokens": [50534, 365, 1269, 7318, 11, 264, 5081, 18870, 13, 50656], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 224, "seek": 56622, "start": 572.0600000000001, "end": 576.46, "text": " And in this case, because we're saving everything necessary", "tokens": [50656, 400, 294, 341, 1389, 11, 570, 321, 434, 6816, 1203, 4818, 50876], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 225, "seek": 56622, "start": 576.46, "end": 581.46, "text": " into a personal user profile and the KB articles,", "tokens": [50876, 666, 257, 2973, 4195, 7964, 293, 264, 591, 33, 11290, 11, 51126], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 226, "seek": 56622, "start": 583.1, "end": 585.6600000000001, "text": " like why even load the conversation?", "tokens": [51208, 411, 983, 754, 3677, 264, 3761, 30, 51336], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 227, "seek": 56622, "start": 586.86, "end": 589.58, "text": " All right, so let me show you the system default message.", "tokens": [51396, 1057, 558, 11, 370, 718, 385, 855, 291, 264, 1185, 7576, 3636, 13, 51532], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 228, "seek": 56622, "start": 589.58, "end": 591.82, "text": " So the system default message is where it starts.", "tokens": [51532, 407, 264, 1185, 7576, 3636, 307, 689, 309, 3719, 13, 51644], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 229, "seek": 56622, "start": 591.82, "end": 594.34, "text": " Your chatbot is whose mission is to assist", "tokens": [51644, 2260, 5081, 18870, 307, 6104, 4447, 307, 281, 4255, 51770], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 230, "seek": 56622, "start": 594.34, "end": 595.9, "text": " the following user, your ultimate objectives", "tokens": [51770, 264, 3480, 4195, 11, 428, 9705, 15961, 51848], "temperature": 0.0, "avg_logprob": -0.16990201613482306, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.00018521686433814466}, {"id": 231, "seek": 59590, "start": 595.9, "end": 597.42, "text": " are to minimize suffering and hence prosperity", "tokens": [50364, 366, 281, 17522, 7755, 293, 16678, 22434, 50440], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 232, "seek": 59590, "start": 597.42, "end": 599.26, "text": " and promote understanding.", "tokens": [50440, 293, 9773, 3701, 13, 50532], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 233, "seek": 59590, "start": 599.26, "end": 601.12, "text": " The provided information about the user", "tokens": [50532, 440, 5649, 1589, 466, 264, 4195, 50625], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 234, "seek": 59590, "start": 601.12, "end": 602.22, "text": " and the Knowledge Base article", "tokens": [50625, 293, 264, 32906, 21054, 7222, 50680], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 235, "seek": 59590, "start": 602.22, "end": 604.3199999999999, "text": " should be integrated into your interactions.", "tokens": [50680, 820, 312, 10919, 666, 428, 13280, 13, 50785], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 236, "seek": 59590, "start": 604.3199999999999, "end": 606.54, "text": " This is private information not visible to the user.", "tokens": [50785, 639, 307, 4551, 1589, 406, 8974, 281, 264, 4195, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 237, "seek": 59590, "start": 606.54, "end": 609.18, "text": " The user profile compiled from past conversations", "tokens": [50896, 440, 4195, 7964, 36548, 490, 1791, 7315, 51028], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 238, "seek": 59590, "start": 609.18, "end": 611.66, "text": " encapsulates critical details about the user", "tokens": [51028, 38745, 26192, 4924, 4365, 466, 264, 4195, 51152], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 239, "seek": 59590, "start": 611.66, "end": 613.8199999999999, "text": " which can aid in shaping your responses effectively,", "tokens": [51152, 597, 393, 9418, 294, 25945, 428, 13019, 8659, 11, 51260], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 240, "seek": 59590, "start": 613.8199999999999, "end": 615.42, "text": " which you saw here.", "tokens": [51260, 597, 291, 1866, 510, 13, 51340], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 241, "seek": 59590, "start": 615.42, "end": 618.16, "text": " So you see like it actually knows quite a bit about me", "tokens": [51340, 407, 291, 536, 411, 309, 767, 3255, 1596, 257, 857, 466, 385, 51477], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 242, "seek": 59590, "start": 618.16, "end": 619.8199999999999, "text": " from our past conversations.", "tokens": [51477, 490, 527, 1791, 7315, 13, 51560], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 243, "seek": 59590, "start": 619.8199999999999, "end": 622.14, "text": " This was populated here in the user profile", "tokens": [51560, 639, 390, 32998, 510, 294, 264, 4195, 7964, 51676], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 244, "seek": 59590, "start": 622.14, "end": 623.46, "text": " and the KB article.", "tokens": [51676, 293, 264, 591, 33, 7222, 13, 51742], "temperature": 0.0, "avg_logprob": -0.13171343505382538, "compression_ratio": 1.8355263157894737, "no_speech_prob": 0.00048781229997985065}, {"id": 245, "seek": 62346, "start": 623.46, "end": 626.48, "text": " So basically it says, then it also explains", "tokens": [50364, 407, 1936, 309, 1619, 11, 550, 309, 611, 13948, 50515], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 246, "seek": 62346, "start": 626.48, "end": 629.4200000000001, "text": " that the KB article is a topic compiled similarly", "tokens": [50515, 300, 264, 591, 33, 7222, 307, 257, 4829, 36548, 14138, 50662], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 247, "seek": 62346, "start": 629.4200000000001, "end": 632.0600000000001, "text": " from past dialogue serving as your long-term memory.", "tokens": [50662, 490, 1791, 10221, 8148, 382, 428, 938, 12, 7039, 4675, 13, 50794], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 248, "seek": 62346, "start": 632.0600000000001, "end": 634.52, "text": " While numerous KB articles exist in your backend system,", "tokens": [50794, 3987, 12546, 591, 33, 11290, 2514, 294, 428, 38087, 1185, 11, 50917], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 249, "seek": 62346, "start": 634.52, "end": 636.32, "text": " the one provided is deemed most relevant", "tokens": [50917, 264, 472, 5649, 307, 27637, 881, 7340, 51007], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 250, "seek": 62346, "start": 636.32, "end": 638.4200000000001, "text": " to the current conversation topic.", "tokens": [51007, 281, 264, 2190, 3761, 4829, 13, 51112], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 251, "seek": 62346, "start": 638.4200000000001, "end": 640.9000000000001, "text": " Note that the recall system operates autonomously", "tokens": [51112, 11633, 300, 264, 9901, 1185, 22577, 18203, 5098, 51236], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 252, "seek": 62346, "start": 640.9000000000001, "end": 643.0600000000001, "text": " and it may not always retrieve the most suitable KB.", "tokens": [51236, 293, 309, 815, 406, 1009, 30254, 264, 881, 12873, 591, 33, 13, 51344], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 253, "seek": 62346, "start": 643.0600000000001, "end": 644.58, "text": " If the user is asking about a topic", "tokens": [51344, 759, 264, 4195, 307, 3365, 466, 257, 4829, 51420], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 254, "seek": 62346, "start": 644.58, "end": 647.0600000000001, "text": " that doesn't seem to align with the provided KB,", "tokens": [51420, 300, 1177, 380, 1643, 281, 7975, 365, 264, 5649, 591, 33, 11, 51544], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 255, "seek": 62346, "start": 647.0600000000001, "end": 648.58, "text": " inform them of the memory pulled", "tokens": [51544, 1356, 552, 295, 264, 4675, 7373, 51620], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 256, "seek": 62346, "start": 648.58, "end": 650.52, "text": " and request them to specify their query", "tokens": [51620, 293, 5308, 552, 281, 16500, 641, 14581, 51717], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 257, "seek": 62346, "start": 650.52, "end": 651.46, "text": " or share more details.", "tokens": [51717, 420, 2073, 544, 4365, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 258, "seek": 62346, "start": 651.46, "end": 653.34, "text": " This can assist the autonomous system", "tokens": [51764, 639, 393, 4255, 264, 23797, 1185, 51858], "temperature": 0.0, "avg_logprob": -0.10743066426869985, "compression_ratio": 1.7369942196531791, "no_speech_prob": 0.0001634599466342479}, {"id": 259, "seek": 65334, "start": 653.34, "end": 654.74, "text": " in retrieving the correct memory", "tokens": [50364, 294, 19817, 798, 264, 3006, 4675, 50434], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 260, "seek": 65334, "start": 654.74, "end": 656.58, "text": " in the subsequent interaction.", "tokens": [50434, 294, 264, 19962, 9285, 13, 50526], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 261, "seek": 65334, "start": 656.58, "end": 659.14, "text": " So basically that's instructing it", "tokens": [50526, 407, 1936, 300, 311, 7232, 278, 309, 50654], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 262, "seek": 65334, "start": 659.14, "end": 661.0600000000001, "text": " to do the same thing that a human will do", "tokens": [50654, 281, 360, 264, 912, 551, 300, 257, 1952, 486, 360, 50750], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 263, "seek": 65334, "start": 661.0600000000001, "end": 663.7, "text": " if I say like, hey, Bill, do you remember that time", "tokens": [50750, 498, 286, 584, 411, 11, 4177, 11, 5477, 11, 360, 291, 1604, 300, 565, 50882], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 264, "seek": 65334, "start": 663.7, "end": 665.88, "text": " that like I accidentally shot you in the face", "tokens": [50882, 300, 411, 286, 15715, 3347, 291, 294, 264, 1851, 50991], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 265, "seek": 65334, "start": 665.88, "end": 666.72, "text": " with a Roman candle", "tokens": [50991, 365, 257, 8566, 17968, 51033], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 266, "seek": 65334, "start": 666.72, "end": 668.62, "text": " because that's something that would happen in the South?", "tokens": [51033, 570, 300, 311, 746, 300, 576, 1051, 294, 264, 4242, 30, 51128], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 267, "seek": 65334, "start": 668.62, "end": 669.6600000000001, "text": " And Bob would be like, you know,", "tokens": [51128, 400, 6085, 576, 312, 411, 11, 291, 458, 11, 51180], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 268, "seek": 65334, "start": 669.6600000000001, "end": 670.74, "text": " I don't actually remember that.", "tokens": [51180, 286, 500, 380, 767, 1604, 300, 13, 51234], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 269, "seek": 65334, "start": 670.74, "end": 672.74, "text": " And I'm like, oh, well, you woke up in the hospital.", "tokens": [51234, 400, 286, 478, 411, 11, 1954, 11, 731, 11, 291, 12852, 493, 294, 264, 4530, 13, 51334], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 270, "seek": 65334, "start": 672.74, "end": 674.6600000000001, "text": " Oh yeah, I remember that, right?", "tokens": [51334, 876, 1338, 11, 286, 1604, 300, 11, 558, 30, 51430], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 271, "seek": 65334, "start": 674.6600000000001, "end": 676.46, "text": " So we prime each other's memory", "tokens": [51430, 407, 321, 5835, 1184, 661, 311, 4675, 51520], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 272, "seek": 65334, "start": 676.46, "end": 680.02, "text": " and human prompting is not that different from AI prompting.", "tokens": [51520, 293, 1952, 12391, 278, 307, 406, 300, 819, 490, 7318, 12391, 278, 13, 51698], "temperature": 0.0, "avg_logprob": -0.13730156042013958, "compression_ratio": 1.7802547770700636, "no_speech_prob": 0.0005527402390725911}, {"id": 273, "seek": 68002, "start": 680.46, "end": 683.14, "text": " Remember that the clarity of your responses", "tokens": [50386, 5459, 300, 264, 16992, 295, 428, 13019, 50520], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 274, "seek": 68002, "start": 683.14, "end": 685.14, "text": " and the relevance of your information recall", "tokens": [50520, 293, 264, 32684, 295, 428, 1589, 9901, 50620], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 275, "seek": 68002, "start": 685.14, "end": 687.66, "text": " are crucial to delivering an optimal user experience.", "tokens": [50620, 366, 11462, 281, 14666, 364, 16252, 4195, 1752, 13, 50746], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 276, "seek": 68002, "start": 687.66, "end": 689.06, "text": " Please ask any clarifying questions", "tokens": [50746, 2555, 1029, 604, 6093, 5489, 1651, 50816], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 277, "seek": 68002, "start": 689.06, "end": 693.14, "text": " or provide any input further for refinement if necessary.", "tokens": [50816, 420, 2893, 604, 4846, 3052, 337, 1895, 30229, 498, 4818, 13, 51020], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 278, "seek": 68002, "start": 693.14, "end": 694.6, "text": " So this system message,", "tokens": [51020, 407, 341, 1185, 3636, 11, 51093], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 279, "seek": 68002, "start": 694.6, "end": 697.78, "text": " I actually got help from chat GPT", "tokens": [51093, 286, 767, 658, 854, 490, 5081, 26039, 51, 51252], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 280, "seek": 68002, "start": 697.78, "end": 701.18, "text": " to create a really compelling system message.", "tokens": [51252, 281, 1884, 257, 534, 20050, 1185, 3636, 13, 51422], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 281, "seek": 68002, "start": 701.18, "end": 703.38, "text": " And one thing that I recommend that people do", "tokens": [51422, 400, 472, 551, 300, 286, 2748, 300, 561, 360, 51532], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 282, "seek": 68002, "start": 703.38, "end": 706.6999999999999, "text": " is actually use chat GPT to work on prompting.", "tokens": [51532, 307, 767, 764, 5081, 26039, 51, 281, 589, 322, 12391, 278, 13, 51698], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 283, "seek": 68002, "start": 706.6999999999999, "end": 708.54, "text": " So this is, you could call this meta prompting", "tokens": [51698, 407, 341, 307, 11, 291, 727, 818, 341, 19616, 12391, 278, 51790], "temperature": 0.0, "avg_logprob": -0.11837704976399739, "compression_ratio": 1.708185053380783, "no_speech_prob": 0.0010004387004300952}, {"id": 284, "seek": 70854, "start": 708.5799999999999, "end": 711.5, "text": " where you use the thing to prompt the thing.", "tokens": [50366, 689, 291, 764, 264, 551, 281, 12391, 264, 551, 13, 50512], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 285, "seek": 70854, "start": 711.5, "end": 713.78, "text": " And the reason that this works really well as one,", "tokens": [50512, 400, 264, 1778, 300, 341, 1985, 534, 731, 382, 472, 11, 50626], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 286, "seek": 70854, "start": 713.78, "end": 716.62, "text": " chat GPT is more articulate than most humans,", "tokens": [50626, 5081, 26039, 51, 307, 544, 30305, 813, 881, 6255, 11, 50768], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 287, "seek": 70854, "start": 716.62, "end": 719.66, "text": " including myself when used correctly.", "tokens": [50768, 3009, 2059, 562, 1143, 8944, 13, 50920], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 288, "seek": 70854, "start": 719.66, "end": 721.66, "text": " But another thing is one thing that I noticed", "tokens": [50920, 583, 1071, 551, 307, 472, 551, 300, 286, 5694, 51020], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 289, "seek": 70854, "start": 721.66, "end": 723.62, "text": " is that chat GPT tends to write", "tokens": [51020, 307, 300, 5081, 26039, 51, 12258, 281, 2464, 51118], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 290, "seek": 70854, "start": 723.62, "end": 725.78, "text": " in a way that it will understand.", "tokens": [51118, 294, 257, 636, 300, 309, 486, 1223, 13, 51226], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 291, "seek": 70854, "start": 725.78, "end": 728.14, "text": " And so if you say, if you give it some context,", "tokens": [51226, 400, 370, 498, 291, 584, 11, 498, 291, 976, 309, 512, 4319, 11, 51344], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 292, "seek": 70854, "start": 728.14, "end": 729.5799999999999, "text": " like this is what I'm trying to do,", "tokens": [51344, 411, 341, 307, 437, 286, 478, 1382, 281, 360, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 293, "seek": 70854, "start": 729.5799999999999, "end": 733.6999999999999, "text": " here's my current prompt, here's what's weak about it.", "tokens": [51416, 510, 311, 452, 2190, 12391, 11, 510, 311, 437, 311, 5336, 466, 309, 13, 51622], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 294, "seek": 70854, "start": 733.6999999999999, "end": 735.02, "text": " Can you make it better?", "tokens": [51622, 1664, 291, 652, 309, 1101, 30, 51688], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 295, "seek": 70854, "start": 735.02, "end": 737.24, "text": " And then you tell it like, ask me some questions", "tokens": [51688, 400, 550, 291, 980, 309, 411, 11, 1029, 385, 512, 1651, 51799], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 296, "seek": 70854, "start": 737.24, "end": 738.0799999999999, "text": " if you have any.", "tokens": [51799, 498, 291, 362, 604, 13, 51841], "temperature": 0.0, "avg_logprob": -0.12355111909392696, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.011329214088618755}, {"id": 297, "seek": 73808, "start": 738.1, "end": 739.5400000000001, "text": " But no, I see what you're trying to do.", "tokens": [50365, 583, 572, 11, 286, 536, 437, 291, 434, 1382, 281, 360, 13, 50437], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 298, "seek": 73808, "start": 739.5400000000001, "end": 742.0200000000001, "text": " Let me write better instructions for you.", "tokens": [50437, 961, 385, 2464, 1101, 9415, 337, 291, 13, 50561], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 299, "seek": 73808, "start": 742.0200000000001, "end": 744.5200000000001, "text": " So instruction writing for anyone who's like a teacher", "tokens": [50561, 407, 10951, 3579, 337, 2878, 567, 311, 411, 257, 5027, 50686], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 300, "seek": 73808, "start": 744.5200000000001, "end": 746.3000000000001, "text": " or technical writer or whatever,", "tokens": [50686, 420, 6191, 9936, 420, 2035, 11, 50775], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 301, "seek": 73808, "start": 746.3000000000001, "end": 749.44, "text": " instruction writing is a very, very particular skill set", "tokens": [50775, 10951, 3579, 307, 257, 588, 11, 588, 1729, 5389, 992, 50932], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 302, "seek": 73808, "start": 749.44, "end": 751.0, "text": " and chat GPT is really good at it.", "tokens": [50932, 293, 5081, 26039, 51, 307, 534, 665, 412, 309, 13, 51010], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 303, "seek": 73808, "start": 751.0, "end": 753.36, "text": " So this is the default system message,", "tokens": [51010, 407, 341, 307, 264, 7576, 1185, 3636, 11, 51128], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 304, "seek": 73808, "start": 753.36, "end": 755.6800000000001, "text": " which is then populated with the user profile", "tokens": [51128, 597, 307, 550, 32998, 365, 264, 4195, 7964, 51244], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 305, "seek": 73808, "start": 755.6800000000001, "end": 757.64, "text": " and the most relevant KB article.", "tokens": [51244, 293, 264, 881, 7340, 591, 33, 7222, 13, 51342], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 306, "seek": 73808, "start": 757.64, "end": 760.72, "text": " So now that we're up to there,", "tokens": [51342, 407, 586, 300, 321, 434, 493, 281, 456, 11, 51496], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 307, "seek": 73808, "start": 760.72, "end": 762.3000000000001, "text": " we enter into the infinite loop,", "tokens": [51496, 321, 3242, 666, 264, 13785, 6367, 11, 51575], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 308, "seek": 73808, "start": 762.3000000000001, "end": 765.5200000000001, "text": " which is just get the user text,", "tokens": [51575, 597, 307, 445, 483, 264, 4195, 2487, 11, 51736], "temperature": 0.0, "avg_logprob": -0.12587762979360728, "compression_ratio": 1.7035714285714285, "no_speech_prob": 0.00047277662088163197}, {"id": 309, "seek": 76552, "start": 765.52, "end": 768.52, "text": " save it to the user log or the chat logs.", "tokens": [50364, 3155, 309, 281, 264, 4195, 3565, 420, 264, 5081, 20820, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 310, "seek": 76552, "start": 768.52, "end": 770.46, "text": " So the chat logs are all saved out here.", "tokens": [50514, 407, 264, 5081, 20820, 366, 439, 6624, 484, 510, 13, 50611], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 311, "seek": 76552, "start": 770.46, "end": 775.22, "text": " It's just plain text and the file name has the timestamp", "tokens": [50611, 467, 311, 445, 11121, 2487, 293, 264, 3991, 1315, 575, 264, 49108, 1215, 50849], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 312, "seek": 76552, "start": 775.22, "end": 776.66, "text": " in it as well as the speaker.", "tokens": [50849, 294, 309, 382, 731, 382, 264, 8145, 13, 50921], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 313, "seek": 76552, "start": 776.66, "end": 779.4399999999999, "text": " So user chat bot, user chat bot, so on and so forth.", "tokens": [50921, 407, 4195, 5081, 10592, 11, 4195, 5081, 10592, 11, 370, 322, 293, 370, 5220, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 314, "seek": 76552, "start": 779.4399999999999, "end": 780.66, "text": " So you got the raw logs there", "tokens": [51060, 407, 291, 658, 264, 8936, 20820, 456, 51121], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 315, "seek": 76552, "start": 780.66, "end": 783.14, "text": " just in case anything goes wrong.", "tokens": [51121, 445, 294, 1389, 1340, 1709, 2085, 13, 51245], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 316, "seek": 76552, "start": 783.14, "end": 784.52, "text": " And then I've also got DB logs,", "tokens": [51245, 400, 550, 286, 600, 611, 658, 26754, 20820, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 317, "seek": 76552, "start": 784.52, "end": 786.8199999999999, "text": " which we'll get to in just a second.", "tokens": [51314, 597, 321, 603, 483, 281, 294, 445, 257, 1150, 13, 51429], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 318, "seek": 76552, "start": 786.8199999999999, "end": 791.1999999999999, "text": " So then what we do is we take the quote main scratch pad,", "tokens": [51429, 407, 550, 437, 321, 360, 307, 321, 747, 264, 6513, 2135, 8459, 6887, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 319, "seek": 76552, "start": 791.1999999999999, "end": 794.18, "text": " which is just the last five messages,", "tokens": [51648, 597, 307, 445, 264, 1036, 1732, 7897, 11, 51797], "temperature": 0.0, "avg_logprob": -0.1248946330126594, "compression_ratio": 1.76171875, "no_speech_prob": 0.01690947450697422}, {"id": 320, "seek": 79418, "start": 794.18, "end": 799.04, "text": " both for the user and for the chat bot.", "tokens": [50364, 1293, 337, 264, 4195, 293, 337, 264, 5081, 10592, 13, 50607], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 321, "seek": 79418, "start": 799.04, "end": 803.28, "text": " And this is what we use as the context of like working memory.", "tokens": [50607, 400, 341, 307, 437, 321, 764, 382, 264, 4319, 295, 411, 1364, 4675, 13, 50819], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 322, "seek": 79418, "start": 803.28, "end": 805.38, "text": " And so then we use this main scratch pad,", "tokens": [50819, 400, 370, 550, 321, 764, 341, 2135, 8459, 6887, 11, 50924], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 323, "seek": 79418, "start": 805.38, "end": 807.3399999999999, "text": " which is the last five messages.", "tokens": [50924, 597, 307, 264, 1036, 1732, 7897, 13, 51022], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 324, "seek": 79418, "start": 807.3399999999999, "end": 812.3399999999999, "text": " We use it to search for the top most relevant KB article.", "tokens": [51022, 492, 764, 309, 281, 3164, 337, 264, 1192, 881, 7340, 591, 33, 7222, 13, 51272], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 325, "seek": 79418, "start": 815.62, "end": 817.9599999999999, "text": " And in my case, I still only have one KB article.", "tokens": [51436, 400, 294, 452, 1389, 11, 286, 920, 787, 362, 472, 591, 33, 7222, 13, 51553], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 326, "seek": 79418, "start": 817.9599999999999, "end": 819.3, "text": " So we'll see how it gets to,", "tokens": [51553, 407, 321, 603, 536, 577, 309, 2170, 281, 11, 51620], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 327, "seek": 79418, "start": 819.3, "end": 820.64, "text": " and I'll go through the logic", "tokens": [51620, 293, 286, 603, 352, 807, 264, 9952, 51687], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 328, "seek": 79418, "start": 820.64, "end": 823.42, "text": " of how it builds KB articles in just a minute.", "tokens": [51687, 295, 577, 309, 15182, 591, 33, 11290, 294, 445, 257, 3456, 13, 51826], "temperature": 0.0, "avg_logprob": -0.11033062242035173, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.0008294849540106952}, {"id": 329, "seek": 82342, "start": 823.42, "end": 824.86, "text": " So basically it just says,", "tokens": [50364, 407, 1936, 309, 445, 1619, 11, 50436], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 330, "seek": 82342, "start": 824.86, "end": 826.38, "text": " okay, here's the most recent thing.", "tokens": [50436, 1392, 11, 510, 311, 264, 881, 5162, 551, 13, 50512], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 331, "seek": 82342, "start": 826.38, "end": 829.0999999999999, "text": " Find the KB article that is most relevant", "tokens": [50512, 11809, 264, 591, 33, 7222, 300, 307, 881, 7340, 50648], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 332, "seek": 82342, "start": 829.0999999999999, "end": 831.74, "text": " to the most recent bits of conversation.", "tokens": [50648, 281, 264, 881, 5162, 9239, 295, 3761, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 333, "seek": 82342, "start": 831.74, "end": 834.42, "text": " And then it'll pull that,", "tokens": [50780, 400, 550, 309, 603, 2235, 300, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 334, "seek": 82342, "start": 834.42, "end": 836.0999999999999, "text": " and it's again, super straightforward.", "tokens": [50914, 293, 309, 311, 797, 11, 1687, 15325, 13, 50998], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 335, "seek": 82342, "start": 836.0999999999999, "end": 838.8199999999999, "text": " All you have to do is pass the text to it,", "tokens": [50998, 1057, 291, 362, 281, 360, 307, 1320, 264, 2487, 281, 309, 11, 51134], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 336, "seek": 82342, "start": 838.8199999999999, "end": 841.0999999999999, "text": " and it will automatically embed it for you.", "tokens": [51134, 293, 309, 486, 6772, 12240, 309, 337, 291, 13, 51248], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 337, "seek": 82342, "start": 841.0999999999999, "end": 843.62, "text": " And then I said, just give me the one most recent.", "tokens": [51248, 400, 550, 286, 848, 11, 445, 976, 385, 264, 472, 881, 5162, 13, 51374], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 338, "seek": 82342, "start": 843.62, "end": 845.8399999999999, "text": " Once we have larger context windows,", "tokens": [51374, 3443, 321, 362, 4833, 4319, 9309, 11, 51485], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 339, "seek": 82342, "start": 845.8399999999999, "end": 850.1999999999999, "text": " or maybe if we decide that recent chat history", "tokens": [51485, 420, 1310, 498, 321, 4536, 300, 5162, 5081, 2503, 51703], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 340, "seek": 82342, "start": 850.1999999999999, "end": 851.2199999999999, "text": " doesn't need to be as big,", "tokens": [51703, 1177, 380, 643, 281, 312, 382, 955, 11, 51754], "temperature": 0.0, "avg_logprob": -0.12722823456043506, "compression_ratio": 1.6937269372693726, "no_speech_prob": 0.00031501700868830085}, {"id": 341, "seek": 85122, "start": 851.22, "end": 854.9, "text": " like let's say we wanna trim this down to like 3000 tokens,", "tokens": [50364, 411, 718, 311, 584, 321, 1948, 10445, 341, 760, 281, 411, 20984, 22667, 11, 50548], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 342, "seek": 85122, "start": 854.9, "end": 857.34, "text": " and we decide that actually having more KB articles", "tokens": [50548, 293, 321, 4536, 300, 767, 1419, 544, 591, 33, 11290, 50670], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 343, "seek": 85122, "start": 857.34, "end": 860.22, "text": " is more important, we can absolutely do that.", "tokens": [50670, 307, 544, 1021, 11, 321, 393, 3122, 360, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 344, "seek": 85122, "start": 860.22, "end": 862.82, "text": " And what you would do then is just change the end results", "tokens": [50814, 400, 437, 291, 576, 360, 550, 307, 445, 1319, 264, 917, 3542, 50944], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 345, "seek": 85122, "start": 862.82, "end": 867.26, "text": " to let, let's say, give me the four most relevant KB articles", "tokens": [50944, 281, 718, 11, 718, 311, 584, 11, 976, 385, 264, 1451, 881, 7340, 591, 33, 11290, 51166], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 346, "seek": 85122, "start": 867.26, "end": 869.74, "text": " instead of the one most relevant.", "tokens": [51166, 2602, 295, 264, 472, 881, 7340, 13, 51290], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 347, "seek": 85122, "start": 869.74, "end": 871.86, "text": " That will allow it to have a more sophisticated", "tokens": [51290, 663, 486, 2089, 309, 281, 362, 257, 544, 16950, 51396], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 348, "seek": 85122, "start": 871.86, "end": 872.82, "text": " working memory.", "tokens": [51396, 1364, 4675, 13, 51444], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 349, "seek": 85122, "start": 874.26, "end": 877.7, "text": " Yeah, so, but right now we're just doing one.", "tokens": [51516, 865, 11, 370, 11, 457, 558, 586, 321, 434, 445, 884, 472, 13, 51688], "temperature": 0.0, "avg_logprob": -0.15291771135832133, "compression_ratio": 1.64453125, "no_speech_prob": 0.0012842079158872366}, {"id": 350, "seek": 87770, "start": 877.7, "end": 881.6600000000001, "text": " And so then what we do is we repopulate", "tokens": [50364, 400, 370, 550, 437, 321, 360, 307, 321, 1085, 404, 5256, 50562], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 351, "seek": 87770, "start": 881.6600000000001, "end": 883.98, "text": " that system default message with the profile", "tokens": [50562, 300, 1185, 7576, 3636, 365, 264, 7964, 50678], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 352, "seek": 87770, "start": 883.98, "end": 885.7, "text": " and the KB article.", "tokens": [50678, 293, 264, 591, 33, 7222, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 353, "seek": 87770, "start": 885.7, "end": 886.98, "text": " And so that's right here.", "tokens": [50764, 400, 370, 300, 311, 558, 510, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 354, "seek": 87770, "start": 886.98, "end": 888.5200000000001, "text": " So that gets populated there.", "tokens": [50828, 407, 300, 2170, 32998, 456, 13, 50905], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 355, "seek": 87770, "start": 890.3000000000001, "end": 892.7, "text": " And then, let's see,", "tokens": [50994, 400, 550, 11, 718, 311, 536, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 356, "seek": 87770, "start": 892.7, "end": 895.38, "text": " it looks like I accidentally changed something.", "tokens": [51114, 309, 1542, 411, 286, 15715, 3105, 746, 13, 51248], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 357, "seek": 87770, "start": 895.38, "end": 897.48, "text": " So let me go ahead and show you my user profile.", "tokens": [51248, 407, 718, 385, 352, 2286, 293, 855, 291, 452, 4195, 7964, 13, 51353], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 358, "seek": 87770, "start": 897.48, "end": 898.86, "text": " I don't mind sharing this", "tokens": [51353, 286, 500, 380, 1575, 5414, 341, 51422], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 359, "seek": 87770, "start": 898.86, "end": 900.1800000000001, "text": " because I've already told you everything.", "tokens": [51422, 570, 286, 600, 1217, 1907, 291, 1203, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 360, "seek": 87770, "start": 900.1800000000001, "end": 902.26, "text": " I'm pretty much an open book.", "tokens": [51488, 286, 478, 1238, 709, 364, 1269, 1446, 13, 51592], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 361, "seek": 87770, "start": 902.26, "end": 906.38, "text": " So the format for this is what I call", "tokens": [51592, 407, 264, 7877, 337, 341, 307, 437, 286, 818, 51798], "temperature": 0.0, "avg_logprob": -0.10889913227932513, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.0003799714904744178}, {"id": 362, "seek": 90638, "start": 907.38, "end": 908.42, "text": " a labeled list.", "tokens": [50414, 257, 21335, 1329, 13, 50466], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 363, "seek": 90638, "start": 908.42, "end": 912.48, "text": " And so I realized back in GPT-3", "tokens": [50466, 400, 370, 286, 5334, 646, 294, 26039, 51, 12, 18, 50669], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 364, "seek": 90638, "start": 912.48, "end": 916.26, "text": " that GPT handles labeled lists very, very well.", "tokens": [50669, 300, 26039, 51, 18722, 21335, 14511, 588, 11, 588, 731, 13, 50858], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 365, "seek": 90638, "start": 916.26, "end": 919.66, "text": " So that's where you use a hyphenated list, bullet list.", "tokens": [50858, 407, 300, 311, 689, 291, 764, 257, 2477, 47059, 770, 1329, 11, 11632, 1329, 13, 51028], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 366, "seek": 90638, "start": 919.66, "end": 921.62, "text": " It understands that intrinsically.", "tokens": [51028, 467, 15146, 300, 28621, 984, 13, 51126], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 367, "seek": 90638, "start": 921.62, "end": 924.42, "text": " And then you label the information, right?", "tokens": [51126, 400, 550, 291, 7645, 264, 1589, 11, 558, 30, 51266], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 368, "seek": 90638, "start": 924.42, "end": 925.84, "text": " So it's just a hash table.", "tokens": [51266, 407, 309, 311, 445, 257, 22019, 3199, 13, 51337], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 369, "seek": 90638, "start": 925.84, "end": 927.42, "text": " If you're into computer science,", "tokens": [51337, 759, 291, 434, 666, 3820, 3497, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 370, "seek": 90638, "start": 927.42, "end": 929.7, "text": " this is called a hash table or a dictionary", "tokens": [51416, 341, 307, 1219, 257, 22019, 3199, 420, 257, 25890, 51530], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 371, "seek": 90638, "start": 929.7, "end": 931.38, "text": " where it's you label the kind of,", "tokens": [51530, 689, 309, 311, 291, 7645, 264, 733, 295, 11, 51614], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 372, "seek": 90638, "start": 931.38, "end": 934.38, "text": " you have a parameter and then you label the parameter, right?", "tokens": [51614, 291, 362, 257, 13075, 293, 550, 291, 7645, 264, 13075, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 373, "seek": 90638, "start": 934.38, "end": 936.18, "text": " So the data metadata.", "tokens": [51764, 407, 264, 1412, 26603, 13, 51854], "temperature": 0.0, "avg_logprob": -0.12065268285346753, "compression_ratio": 1.768627450980392, "no_speech_prob": 0.0018100084271281958}, {"id": 374, "seek": 93618, "start": 936.18, "end": 938.3399999999999, "text": " So name, David Shapiro, y'all know that.", "tokens": [50364, 407, 1315, 11, 4389, 44160, 5182, 11, 288, 6, 336, 458, 300, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 375, "seek": 93618, "start": 938.3399999999999, "end": 940.2199999999999, "text": " Profession, AI and cognitive architectures,", "tokens": [50472, 6039, 4311, 11, 7318, 293, 15605, 6331, 1303, 11, 50566], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 376, "seek": 93618, "start": 940.2199999999999, "end": 941.3399999999999, "text": " y'all know that.", "tokens": [50566, 288, 6, 336, 458, 300, 13, 50622], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 377, "seek": 93618, "start": 941.3399999999999, "end": 944.9, "text": " Interests, it's got a whole bunch of interests.", "tokens": [50622, 5751, 4409, 11, 309, 311, 658, 257, 1379, 3840, 295, 8847, 13, 50800], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 378, "seek": 93618, "start": 944.9, "end": 946.0999999999999, "text": " And oh, by the way,", "tokens": [50800, 400, 1954, 11, 538, 264, 636, 11, 50860], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 379, "seek": 93618, "start": 946.0999999999999, "end": 948.76, "text": " this was all distilled from other conversations.", "tokens": [50860, 341, 390, 439, 1483, 6261, 490, 661, 7315, 13, 50993], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 380, "seek": 93618, "start": 950.18, "end": 951.7399999999999, "text": " Beliefs, plans,", "tokens": [51064, 6248, 2521, 82, 11, 5482, 11, 51142], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 381, "seek": 93618, "start": 951.7399999999999, "end": 954.2399999999999, "text": " and this is of course gonna get updated over time.", "tokens": [51142, 293, 341, 307, 295, 1164, 799, 483, 10588, 670, 565, 13, 51267], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 382, "seek": 93618, "start": 955.2399999999999, "end": 957.2199999999999, "text": " So for instance,", "tokens": [51317, 407, 337, 5197, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 383, "seek": 93618, "start": 957.2199999999999, "end": 960.4599999999999, "text": " during some of the conversations that I just showed you", "tokens": [51416, 1830, 512, 295, 264, 7315, 300, 286, 445, 4712, 291, 51578], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 384, "seek": 93618, "start": 960.4599999999999, "end": 962.8199999999999, "text": " with this brand new chatbot,", "tokens": [51578, 365, 341, 3360, 777, 5081, 18870, 11, 51696], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 385, "seek": 93618, "start": 962.8199999999999, "end": 965.14, "text": " it added this.", "tokens": [51696, 309, 3869, 341, 13, 51812], "temperature": 0.0, "avg_logprob": -0.1754301901786558, "compression_ratio": 1.5826771653543308, "no_speech_prob": 0.00018520942830946296}, {"id": 386, "seek": 96514, "start": 965.18, "end": 966.58, "text": " When I told it, this is what I'm gonna do.", "tokens": [50366, 1133, 286, 1907, 309, 11, 341, 307, 437, 286, 478, 799, 360, 13, 50436], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 387, "seek": 96514, "start": 966.58, "end": 968.26, "text": " It said, okay, I'm gonna,", "tokens": [50436, 467, 848, 11, 1392, 11, 286, 478, 799, 11, 50520], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 388, "seek": 96514, "start": 968.26, "end": 969.6999999999999, "text": " I think that that's relevant", "tokens": [50520, 286, 519, 300, 300, 311, 7340, 50592], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 389, "seek": 96514, "start": 969.6999999999999, "end": 971.48, "text": " to what you're gonna be doing in the future.", "tokens": [50592, 281, 437, 291, 434, 799, 312, 884, 294, 264, 2027, 13, 50681], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 390, "seek": 96514, "start": 971.48, "end": 974.9399999999999, "text": " So let me just jot that down on my scratch pad for you.", "tokens": [50681, 407, 718, 385, 445, 27873, 300, 760, 322, 452, 8459, 6887, 337, 291, 13, 50854], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 391, "seek": 96514, "start": 974.9399999999999, "end": 978.58, "text": " Preferences, so I manually added avoid superfluous words", "tokens": [50854, 48401, 2667, 11, 370, 286, 16945, 3869, 5042, 1687, 49253, 563, 2283, 51036], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 392, "seek": 96514, "start": 978.58, "end": 980.22, "text": " overly for both responses.", "tokens": [51036, 24324, 337, 1293, 13019, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 393, "seek": 96514, "start": 980.22, "end": 981.26, "text": " And then you know how it says,", "tokens": [51118, 400, 550, 291, 458, 577, 309, 1619, 11, 51170], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 394, "seek": 96514, "start": 981.26, "end": 983.46, "text": " as an AI model, I don't have personal opinions.", "tokens": [51170, 382, 364, 7318, 2316, 11, 286, 500, 380, 362, 2973, 11819, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 395, "seek": 96514, "start": 983.46, "end": 984.98, "text": " I'm like, I know, I don't care.", "tokens": [51280, 286, 478, 411, 11, 286, 458, 11, 286, 500, 380, 1127, 13, 51356], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 396, "seek": 96514, "start": 984.98, "end": 986.9399999999999, "text": " So I said, please interpret personal input", "tokens": [51356, 407, 286, 848, 11, 1767, 7302, 2973, 4846, 51454], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 397, "seek": 96514, "start": 986.9399999999999, "end": 989.78, "text": " as critical evaluation and valuable feedback.", "tokens": [51454, 382, 4924, 13344, 293, 8263, 5824, 13, 51596], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 398, "seek": 96514, "start": 989.78, "end": 991.9399999999999, "text": " I said it a little bit more explicitly than that,", "tokens": [51596, 286, 848, 309, 257, 707, 857, 544, 20803, 813, 300, 11, 51704], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 399, "seek": 96514, "start": 991.9399999999999, "end": 994.54, "text": " but the point is, is that I told it that", "tokens": [51704, 457, 264, 935, 307, 11, 307, 300, 286, 1907, 309, 300, 51834], "temperature": 0.0, "avg_logprob": -0.1570488404945509, "compression_ratio": 1.6803519061583578, "no_speech_prob": 0.001133452053181827}, {"id": 400, "seek": 99454, "start": 994.5799999999999, "end": 997.8199999999999, "text": " in natural language, I was down here and I said,", "tokens": [50366, 294, 3303, 2856, 11, 286, 390, 760, 510, 293, 286, 848, 11, 50528], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 401, "seek": 99454, "start": 997.8199999999999, "end": 1002.8199999999999, "text": " I know you're an AI and have no personal opinions,", "tokens": [50528, 286, 458, 291, 434, 364, 7318, 293, 362, 572, 2973, 11819, 11, 50778], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 402, "seek": 99454, "start": 1003.18, "end": 1007.5799999999999, "text": " but when I ask for them, this is what I mean.", "tokens": [50796, 457, 562, 286, 1029, 337, 552, 11, 341, 307, 437, 286, 914, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 403, "seek": 99454, "start": 1007.5799999999999, "end": 1010.02, "text": " And so when I did that, it actually recorded that", "tokens": [51016, 400, 370, 562, 286, 630, 300, 11, 309, 767, 8287, 300, 51138], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 404, "seek": 99454, "start": 1010.02, "end": 1013.3, "text": " automatically because after every conversation,", "tokens": [51138, 6772, 570, 934, 633, 3761, 11, 51302], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 405, "seek": 99454, "start": 1013.3, "end": 1015.3, "text": " it checks the user profile.", "tokens": [51302, 309, 13834, 264, 4195, 7964, 13, 51402], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 406, "seek": 99454, "start": 1015.3, "end": 1016.86, "text": " We need to find a way to speed this up", "tokens": [51402, 492, 643, 281, 915, 257, 636, 281, 3073, 341, 493, 51480], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 407, "seek": 99454, "start": 1016.86, "end": 1019.5799999999999, "text": " because as you saw from the user interface,", "tokens": [51480, 570, 382, 291, 1866, 490, 264, 4195, 9226, 11, 51616], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 408, "seek": 99454, "start": 1019.5799999999999, "end": 1020.68, "text": " it's not the best.", "tokens": [51616, 309, 311, 406, 264, 1151, 13, 51671], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 409, "seek": 99454, "start": 1022.0999999999999, "end": 1024.1, "text": " If I had more time, mental energy and patience,", "tokens": [51742, 759, 286, 632, 544, 565, 11, 4973, 2281, 293, 14826, 11, 51842], "temperature": 0.0, "avg_logprob": -0.11741722236245365, "compression_ratio": 1.6192307692307693, "no_speech_prob": 0.0019873357377946377}, {"id": 410, "seek": 102410, "start": 1024.6599999999999, "end": 1027.2199999999998, "text": " I would separate this out as a thread,", "tokens": [50392, 286, 576, 4994, 341, 484, 382, 257, 7207, 11, 50520], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 411, "seek": 102410, "start": 1027.2199999999998, "end": 1029.8999999999999, "text": " as a separate threading thing that can be done,", "tokens": [50520, 382, 257, 4994, 7207, 278, 551, 300, 393, 312, 1096, 11, 50654], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 412, "seek": 102410, "start": 1029.8999999999999, "end": 1033.1399999999999, "text": " or even separate it out as a separate API.", "tokens": [50654, 420, 754, 4994, 309, 484, 382, 257, 4994, 9362, 13, 50816], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 413, "seek": 102410, "start": 1034.2199999999998, "end": 1037.4599999999998, "text": " One of y'all can do that, submit a pull request", "tokens": [50870, 1485, 295, 288, 6, 336, 393, 360, 300, 11, 10315, 257, 2235, 5308, 51032], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 414, "seek": 102410, "start": 1037.4599999999998, "end": 1038.98, "text": " on the public repo.", "tokens": [51032, 322, 264, 1908, 49040, 13, 51108], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 415, "seek": 102410, "start": 1038.98, "end": 1043.98, "text": " And then health, so it added this entirely on its own", "tokens": [51108, 400, 550, 1585, 11, 370, 309, 3869, 341, 7696, 322, 1080, 1065, 51358], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 416, "seek": 102410, "start": 1044.1399999999999, "end": 1049.1399999999999, "text": " because I said, hey, I woke up at like 2.30 in the morning", "tokens": [51366, 570, 286, 848, 11, 4177, 11, 286, 12852, 493, 412, 411, 568, 13, 3446, 294, 264, 2446, 51616], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 417, "seek": 102410, "start": 1050.1, "end": 1052.1399999999999, "text": " because I had to work on this.", "tokens": [51664, 570, 286, 632, 281, 589, 322, 341, 13, 51766], "temperature": 0.0, "avg_logprob": -0.15675903986958625, "compression_ratio": 1.6473429951690821, "no_speech_prob": 0.0015973950503394008}, {"id": 418, "seek": 105214, "start": 1052.14, "end": 1054.42, "text": " And then I said, let's talk about that.", "tokens": [50364, 400, 550, 286, 848, 11, 718, 311, 751, 466, 300, 13, 50478], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 419, "seek": 105214, "start": 1054.42, "end": 1056.9, "text": " And so it decided that that was a critical piece", "tokens": [50478, 400, 370, 309, 3047, 300, 300, 390, 257, 4924, 2522, 50602], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 420, "seek": 105214, "start": 1056.9, "end": 1059.14, "text": " of information to add to my user profile.", "tokens": [50602, 295, 1589, 281, 909, 281, 452, 4195, 7964, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 421, "seek": 105214, "start": 1059.14, "end": 1061.8600000000001, "text": " So that all gets populated here.", "tokens": [50714, 407, 300, 439, 2170, 32998, 510, 13, 50850], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 422, "seek": 105214, "start": 1061.8600000000001, "end": 1065.5400000000002, "text": " And then the logs are all stored here.", "tokens": [50850, 400, 550, 264, 20820, 366, 439, 12187, 510, 13, 51034], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 423, "seek": 105214, "start": 1065.5400000000002, "end": 1070.5400000000002, "text": " So you got the API logs, which will track all of that.", "tokens": [51034, 407, 291, 658, 264, 9362, 20820, 11, 597, 486, 2837, 439, 295, 300, 13, 51284], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 424, "seek": 105214, "start": 1070.74, "end": 1074.18, "text": " Everything, so I use chat GPT API for everything", "tokens": [51294, 5471, 11, 370, 286, 764, 5081, 26039, 51, 9362, 337, 1203, 51466], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 425, "seek": 105214, "start": 1074.18, "end": 1076.7800000000002, "text": " just because that's the only way to get to GPT-4,", "tokens": [51466, 445, 570, 300, 311, 264, 787, 636, 281, 483, 281, 26039, 51, 12, 19, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 426, "seek": 105214, "start": 1076.7800000000002, "end": 1078.14, "text": " which is the most powerful.", "tokens": [51596, 597, 307, 264, 881, 4005, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12043681315013341, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.00029593316139653325}, {"id": 427, "seek": 107814, "start": 1079.14, "end": 1080.3000000000002, "text": " Let's see.", "tokens": [50414, 961, 311, 536, 13, 50472], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 428, "seek": 107814, "start": 1082.46, "end": 1087.3000000000002, "text": " So then we update the system message every time.", "tokens": [50580, 407, 550, 321, 5623, 264, 1185, 3636, 633, 565, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 429, "seek": 107814, "start": 1087.3000000000002, "end": 1091.5, "text": " So it says, okay, whatever you said,", "tokens": [50822, 407, 309, 1619, 11, 1392, 11, 2035, 291, 848, 11, 51032], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 430, "seek": 107814, "start": 1091.5, "end": 1092.8600000000001, "text": " update the system message,", "tokens": [51032, 5623, 264, 1185, 3636, 11, 51100], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 431, "seek": 107814, "start": 1092.8600000000001, "end": 1095.66, "text": " then we go ahead and generate a response first", "tokens": [51100, 550, 321, 352, 2286, 293, 8460, 257, 4134, 700, 51240], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 432, "seek": 107814, "start": 1096.74, "end": 1100.1000000000001, "text": " because the user profile is not gonna change", "tokens": [51294, 570, 264, 4195, 7964, 307, 406, 799, 1319, 51462], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 433, "seek": 107814, "start": 1100.1000000000001, "end": 1101.5, "text": " all that much or all that often,", "tokens": [51462, 439, 300, 709, 420, 439, 300, 2049, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 434, "seek": 107814, "start": 1101.5, "end": 1104.18, "text": " so we can basically assume that it'll be usable.", "tokens": [51532, 370, 321, 393, 1936, 6552, 300, 309, 603, 312, 29975, 13, 51666], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 435, "seek": 107814, "start": 1104.18, "end": 1106.8600000000001, "text": " And then the KB articles also,", "tokens": [51666, 400, 550, 264, 591, 33, 11290, 611, 11, 51800], "temperature": 0.0, "avg_logprob": -0.1704430426320722, "compression_ratio": 1.64, "no_speech_prob": 0.0016482449136674404}, {"id": 436, "seek": 110686, "start": 1107.5, "end": 1108.82, "text": " I figured it would actually be better", "tokens": [50396, 286, 8932, 309, 576, 767, 312, 1101, 50462], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 437, "seek": 110686, "start": 1108.82, "end": 1112.8999999999999, "text": " to update the KB articles after you have the user input", "tokens": [50462, 281, 5623, 264, 591, 33, 11290, 934, 291, 362, 264, 4195, 4846, 50666], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 438, "seek": 110686, "start": 1112.8999999999999, "end": 1114.74, "text": " and then the machine output", "tokens": [50666, 293, 550, 264, 3479, 5598, 50758], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 439, "seek": 110686, "start": 1114.74, "end": 1118.54, "text": " because if you ask chat GPT for important information", "tokens": [50758, 570, 498, 291, 1029, 5081, 26039, 51, 337, 1021, 1589, 50948], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 440, "seek": 110686, "start": 1118.54, "end": 1120.1399999999999, "text": " or it solves a problem for you,", "tokens": [50948, 420, 309, 39890, 257, 1154, 337, 291, 11, 51028], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 441, "seek": 110686, "start": 1120.1399999999999, "end": 1122.4599999999998, "text": " you actually wanna capture that.", "tokens": [51028, 291, 767, 1948, 7983, 300, 13, 51144], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 442, "seek": 110686, "start": 1123.4599999999998, "end": 1126.1399999999999, "text": " So we go ahead and generate the response", "tokens": [51194, 407, 321, 352, 2286, 293, 8460, 264, 4134, 51328], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 443, "seek": 110686, "start": 1126.1399999999999, "end": 1128.1399999999999, "text": " and append that to everything.", "tokens": [51328, 293, 34116, 300, 281, 1203, 13, 51428], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 444, "seek": 110686, "start": 1128.1399999999999, "end": 1129.9399999999998, "text": " We go ahead and log it out.", "tokens": [51428, 492, 352, 2286, 293, 3565, 309, 484, 13, 51518], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 445, "seek": 110686, "start": 1129.9399999999998, "end": 1132.6599999999999, "text": " Then we update the user scratch pad again.", "tokens": [51518, 1396, 321, 5623, 264, 4195, 8459, 6887, 797, 13, 51654], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 446, "seek": 110686, "start": 1132.6599999999999, "end": 1134.1399999999999, "text": " Actually, why did I do this?", "tokens": [51654, 5135, 11, 983, 630, 286, 360, 341, 30, 51728], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 447, "seek": 110686, "start": 1135.1799999999998, "end": 1136.78, "text": " Oh, no, this is the first time we did it.", "tokens": [51780, 876, 11, 572, 11, 341, 307, 264, 700, 565, 321, 630, 309, 13, 51860], "temperature": 0.0, "avg_logprob": -0.10725778246682788, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.0006070459494367242}, {"id": 448, "seek": 113678, "start": 1136.82, "end": 1138.98, "text": " Okay, sorry, I apologize.", "tokens": [50366, 1033, 11, 2597, 11, 286, 12328, 13, 50474], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 449, "seek": 113678, "start": 1138.98, "end": 1141.34, "text": " So then we update the user scratch pad,", "tokens": [50474, 407, 550, 321, 5623, 264, 4195, 8459, 6887, 11, 50592], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 450, "seek": 113678, "start": 1141.34, "end": 1143.06, "text": " which the user scratch pad", "tokens": [50592, 597, 264, 4195, 8459, 6887, 50678], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 451, "seek": 113678, "start": 1143.06, "end": 1145.1399999999999, "text": " is only the last few user messages.", "tokens": [50678, 307, 787, 264, 1036, 1326, 4195, 7897, 13, 50782], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 452, "seek": 113678, "start": 1145.1399999999999, "end": 1149.1399999999999, "text": " And the reason for that is because we want to exclude", "tokens": [50782, 400, 264, 1778, 337, 300, 307, 570, 321, 528, 281, 33536, 50982], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 453, "seek": 113678, "start": 1150.22, "end": 1152.54, "text": " chat GPT's response", "tokens": [51036, 5081, 26039, 51, 311, 4134, 51152], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 454, "seek": 113678, "start": 1152.54, "end": 1154.1, "text": " because we don't want it to get confused", "tokens": [51152, 570, 321, 500, 380, 528, 309, 281, 483, 9019, 51230], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 455, "seek": 113678, "start": 1154.1, "end": 1155.7, "text": " about things that it has said about you", "tokens": [51230, 466, 721, 300, 309, 575, 848, 466, 291, 51310], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 456, "seek": 113678, "start": 1155.7, "end": 1157.66, "text": " or inferred or whatever.", "tokens": [51310, 420, 13596, 986, 420, 2035, 13, 51408], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 457, "seek": 113678, "start": 1157.66, "end": 1160.98, "text": " We only want to record your user profile", "tokens": [51408, 492, 787, 528, 281, 2136, 428, 4195, 7964, 51574], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 458, "seek": 113678, "start": 1160.98, "end": 1162.54, "text": " from explicitly what you say.", "tokens": [51574, 490, 20803, 437, 291, 584, 13, 51652], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 459, "seek": 113678, "start": 1162.54, "end": 1165.86, "text": " So I just captured the last three messages that you've sent", "tokens": [51652, 407, 286, 445, 11828, 264, 1036, 1045, 7897, 300, 291, 600, 2279, 51818], "temperature": 0.0, "avg_logprob": -0.07903212706247965, "compression_ratio": 1.7015503875968991, "no_speech_prob": 0.00036825507413595915}, {"id": 460, "seek": 116586, "start": 1165.8999999999999, "end": 1167.9799999999998, "text": " and then it does a stare and compare", "tokens": [50366, 293, 550, 309, 775, 257, 22432, 293, 6794, 50470], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 461, "seek": 116586, "start": 1167.9799999999998, "end": 1169.6999999999998, "text": " basically where it says, okay,", "tokens": [50470, 1936, 689, 309, 1619, 11, 1392, 11, 50556], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 462, "seek": 116586, "start": 1169.6999999999998, "end": 1172.02, "text": " based on this most recent chat message,", "tokens": [50556, 2361, 322, 341, 881, 5162, 5081, 3636, 11, 50672], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 463, "seek": 116586, "start": 1172.02, "end": 1173.1399999999999, "text": " is there any,", "tokens": [50672, 307, 456, 604, 11, 50728], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 464, "seek": 116586, "start": 1173.1399999999999, "end": 1176.1, "text": " one, is there any relevant user information?", "tokens": [50728, 472, 11, 307, 456, 604, 7340, 4195, 1589, 30, 50876], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 465, "seek": 116586, "start": 1176.1, "end": 1177.54, "text": " And if so, go ahead and update it.", "tokens": [50876, 400, 498, 370, 11, 352, 2286, 293, 5623, 309, 13, 50948], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 466, "seek": 116586, "start": 1177.54, "end": 1179.58, "text": " So let me show you how it updates that.", "tokens": [50948, 407, 718, 385, 855, 291, 577, 309, 9205, 300, 13, 51050], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 467, "seek": 116586, "start": 1179.58, "end": 1182.1, "text": " So system update user profile.", "tokens": [51050, 407, 1185, 5623, 4195, 7964, 13, 51176], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 468, "seek": 116586, "start": 1182.9399999999998, "end": 1187.6599999999999, "text": " So this is a user profile document updater chat bot.", "tokens": [51218, 407, 341, 307, 257, 4195, 7964, 4166, 3460, 771, 5081, 10592, 13, 51454], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 469, "seek": 116586, "start": 1187.6599999999999, "end": 1189.78, "text": " This is the system message.", "tokens": [51454, 639, 307, 264, 1185, 3636, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 470, "seek": 116586, "start": 1189.78, "end": 1192.9799999999998, "text": " Your role is to manage and update a UPD and chat bot,", "tokens": [51560, 2260, 3090, 307, 281, 3067, 293, 5623, 257, 624, 17349, 293, 5081, 10592, 11, 51720], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 471, "seek": 116586, "start": 1192.9799999999998, "end": 1195.3, "text": " the chat GPT came up with this idea on its own.", "tokens": [51720, 264, 5081, 26039, 51, 1361, 493, 365, 341, 1558, 322, 1080, 1065, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14021051736702597, "compression_ratio": 1.7704280155642023, "no_speech_prob": 0.0005356974434107542}, {"id": 472, "seek": 119530, "start": 1195.74, "end": 1199.62, "text": " It created the UPD definition.", "tokens": [50386, 467, 2942, 264, 624, 17349, 7123, 13, 50580], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 473, "seek": 119530, "start": 1199.62, "end": 1201.5, "text": " Your primary responsibility is to parse updates", "tokens": [50580, 2260, 6194, 6357, 307, 281, 48377, 9205, 50674], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 474, "seek": 119530, "start": 1201.5, "end": 1205.02, "text": " supplied by the user, meticulously analyze them.", "tokens": [50674, 27625, 538, 264, 4195, 11, 41566, 25038, 12477, 552, 13, 50850], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 475, "seek": 119530, "start": 1206.02, "end": 1208.5, "text": " It could also extend elements such as user preferences,", "tokens": [50900, 467, 727, 611, 10101, 4959, 1270, 382, 4195, 21910, 11, 51024], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 476, "seek": 119530, "start": 1208.5, "end": 1210.82, "text": " significant life events and deeply held beliefs.", "tokens": [51024, 4776, 993, 3931, 293, 8760, 5167, 13585, 13, 51140], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 477, "seek": 119530, "start": 1210.82, "end": 1212.82, "text": " Please refrain from incorporating non-essential data", "tokens": [51140, 2555, 46177, 490, 33613, 2107, 12, 48143, 1412, 51240], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 478, "seek": 119530, "start": 1212.82, "end": 1214.8999999999999, "text": " or unrelated topics.", "tokens": [51240, 420, 38967, 8378, 13, 51344], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 479, "seek": 119530, "start": 1214.8999999999999, "end": 1216.86, "text": " The result of your efforts should exclusively be", "tokens": [51344, 440, 1874, 295, 428, 6484, 820, 20638, 312, 51442], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 480, "seek": 119530, "start": 1216.86, "end": 1217.82, "text": " an updated UPD.", "tokens": [51442, 364, 10588, 624, 17349, 13, 51490], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 481, "seek": 119530, "start": 1217.82, "end": 1219.82, "text": " If the user's update doesn't contribute", "tokens": [51490, 759, 264, 4195, 311, 5623, 1177, 380, 10586, 51590], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 482, "seek": 119530, "start": 1219.82, "end": 1221.26, "text": " any new or significant information,", "tokens": [51590, 604, 777, 420, 4776, 1589, 11, 51662], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 483, "seek": 119530, "start": 1221.26, "end": 1222.98, "text": " your output should mirror the current UPD", "tokens": [51662, 428, 5598, 820, 8013, 264, 2190, 624, 17349, 51748], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 484, "seek": 119530, "start": 1222.98, "end": 1224.58, "text": " as indicated below.", "tokens": [51748, 382, 16176, 2507, 13, 51828], "temperature": 0.0, "avg_logprob": -0.13138953910386267, "compression_ratio": 1.6419354838709677, "no_speech_prob": 0.0010320384753867984}, {"id": 485, "seek": 122458, "start": 1224.62, "end": 1226.78, "text": " However, if you discover any relevant new information,", "tokens": [50366, 2908, 11, 498, 291, 4411, 604, 7340, 777, 1589, 11, 50474], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 486, "seek": 122458, "start": 1226.78, "end": 1229.06, "text": " your output should feature an updated UPD", "tokens": [50474, 428, 5598, 820, 4111, 364, 10588, 624, 17349, 50588], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 487, "seek": 122458, "start": 1229.06, "end": 1231.06, "text": " that assimilates these modifications.", "tokens": [50588, 300, 8249, 388, 1024, 613, 26881, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 488, "seek": 122458, "start": 1231.06, "end": 1234.6999999999998, "text": " So basically it's an absurd, right?", "tokens": [50688, 407, 1936, 309, 311, 364, 19774, 11, 558, 30, 50870], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 489, "seek": 122458, "start": 1234.6999999999998, "end": 1239.54, "text": " Or if there's no differences, just keep it the same,", "tokens": [50870, 1610, 498, 456, 311, 572, 7300, 11, 445, 1066, 309, 264, 912, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 490, "seek": 122458, "start": 1239.54, "end": 1241.54, "text": " otherwise update it.", "tokens": [51112, 5911, 5623, 309, 13, 51212], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 491, "seek": 122458, "start": 1241.54, "end": 1243.6599999999999, "text": " You must prioritize brevity and clarity in your output,", "tokens": [51212, 509, 1633, 25164, 1403, 23110, 293, 16992, 294, 428, 5598, 11, 51318], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 492, "seek": 122458, "start": 1243.6599999999999, "end": 1245.78, "text": " combining condensed information when appropriate", "tokens": [51318, 21928, 36398, 1589, 562, 6854, 51424], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 493, "seek": 122458, "start": 1245.78, "end": 1248.62, "text": " to ensure succinctness and improve comprehension.", "tokens": [51424, 281, 5586, 21578, 5460, 1287, 293, 3470, 44991, 13, 51566], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 494, "seek": 122458, "start": 1248.62, "end": 1251.54, "text": " Totally rewrite or restructure UPD as necessary,", "tokens": [51566, 22837, 28132, 420, 1472, 2885, 624, 17349, 382, 4818, 11, 51712], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 495, "seek": 122458, "start": 1251.54, "end": 1253.3799999999999, "text": " adhering to the list format.", "tokens": [51712, 30106, 278, 281, 264, 1329, 7877, 13, 51804], "temperature": 0.0, "avg_logprob": -0.11568873152773604, "compression_ratio": 1.616949152542373, "no_speech_prob": 0.00040443032048642635}, {"id": 496, "seek": 125338, "start": 1253.38, "end": 1255.5800000000002, "text": " Your response should not include explanatory text", "tokens": [50364, 2260, 4134, 820, 406, 4090, 9045, 4745, 2487, 50474], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 497, "seek": 125338, "start": 1255.5800000000002, "end": 1258.2600000000002, "text": " or context, because you know how sometimes chat GPT", "tokens": [50474, 420, 4319, 11, 570, 291, 458, 577, 2171, 5081, 26039, 51, 50608], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 498, "seek": 125338, "start": 1258.2600000000002, "end": 1261.94, "text": " will say, this is your new, you know, blah, blah, blah.", "tokens": [50608, 486, 584, 11, 341, 307, 428, 777, 11, 291, 458, 11, 12288, 11, 12288, 11, 12288, 13, 50792], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 499, "seek": 125338, "start": 1261.94, "end": 1265.38, "text": " So in this case, I have it very reliably", "tokens": [50792, 407, 294, 341, 1389, 11, 286, 362, 309, 588, 49927, 50964], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 500, "seek": 125338, "start": 1265.38, "end": 1268.2600000000002, "text": " just spit out the user profile.", "tokens": [50964, 445, 22127, 484, 264, 4195, 7964, 13, 51108], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 501, "seek": 125338, "start": 1269.22, "end": 1271.1000000000001, "text": " Oh, and then another thing is that", "tokens": [51156, 876, 11, 293, 550, 1071, 551, 307, 300, 51250], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 502, "seek": 125338, "start": 1271.1000000000001, "end": 1273.9, "text": " because we're working with a limited window,", "tokens": [51250, 570, 321, 434, 1364, 365, 257, 5567, 4910, 11, 51390], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 503, "seek": 125338, "start": 1273.9, "end": 1277.74, "text": " I say the UPD should not exceed approximately 1,000 words.", "tokens": [51390, 286, 584, 264, 624, 17349, 820, 406, 14048, 10447, 502, 11, 1360, 2283, 13, 51582], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 504, "seek": 125338, "start": 1277.74, "end": 1279.94, "text": " When revising the UPD, give precedence", "tokens": [51582, 1133, 3698, 3436, 264, 624, 17349, 11, 976, 16969, 655, 51692], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 505, "seek": 125338, "start": 1279.94, "end": 1282.14, "text": " to the most significant and relevant information,", "tokens": [51692, 281, 264, 881, 4776, 293, 7340, 1589, 11, 51802], "temperature": 0.0, "avg_logprob": -0.1091398500627087, "compression_ratio": 1.5958188153310104, "no_speech_prob": 0.001454802230000496}, {"id": 506, "seek": 128214, "start": 1282.14, "end": 1284.1000000000001, "text": " extraneous or less impactful information", "tokens": [50364, 16455, 15447, 420, 1570, 30842, 1589, 50462], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 507, "seek": 128214, "start": 1284.1000000000001, "end": 1285.9, "text": " should be omitted, et cetera, et cetera.", "tokens": [50462, 820, 312, 3406, 3944, 11, 1030, 11458, 11, 1030, 11458, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 508, "seek": 128214, "start": 1285.9, "end": 1288.26, "text": " So I give it the current word count", "tokens": [50552, 407, 286, 976, 309, 264, 2190, 1349, 1207, 50670], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 509, "seek": 128214, "start": 1288.26, "end": 1289.6200000000001, "text": " and then the current UPD.", "tokens": [50670, 293, 550, 264, 2190, 624, 17349, 13, 50738], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 510, "seek": 128214, "start": 1289.6200000000001, "end": 1292.26, "text": " So that way it kind of knows, because chat GPT,", "tokens": [50738, 407, 300, 636, 309, 733, 295, 3255, 11, 570, 5081, 26039, 51, 11, 50870], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 511, "seek": 128214, "start": 1292.26, "end": 1295.5400000000002, "text": " especially GPT-4 is better at counting words,", "tokens": [50870, 2318, 26039, 51, 12, 19, 307, 1101, 412, 13251, 2283, 11, 51034], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 512, "seek": 128214, "start": 1295.5400000000002, "end": 1299.0200000000002, "text": " but just giving it the explicit number makes it easier,", "tokens": [51034, 457, 445, 2902, 309, 264, 13691, 1230, 1669, 309, 3571, 11, 51208], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 513, "seek": 128214, "start": 1299.0200000000002, "end": 1300.5, "text": " right?", "tokens": [51208, 558, 30, 51282], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 514, "seek": 128214, "start": 1300.5, "end": 1303.74, "text": " Yeah, so that's my current user profile.", "tokens": [51282, 865, 11, 370, 300, 311, 452, 2190, 4195, 7964, 13, 51444], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 515, "seek": 128214, "start": 1303.74, "end": 1305.66, "text": " So now let's dive back in here.", "tokens": [51444, 407, 586, 718, 311, 9192, 646, 294, 510, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 516, "seek": 128214, "start": 1305.66, "end": 1307.98, "text": " The hard part was updating the knowledge base.", "tokens": [51540, 440, 1152, 644, 390, 25113, 264, 3601, 3096, 13, 51656], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 517, "seek": 128214, "start": 1307.98, "end": 1310.18, "text": " So if this is your first run,", "tokens": [51656, 407, 498, 341, 307, 428, 700, 1190, 11, 51766], "temperature": 0.0, "avg_logprob": -0.11615509295281563, "compression_ratio": 1.6129032258064515, "no_speech_prob": 0.0018100114539265633}, {"id": 518, "seek": 131018, "start": 1310.18, "end": 1312.98, "text": " the collection count is gonna be zero.", "tokens": [50364, 264, 5765, 1207, 307, 799, 312, 4018, 13, 50504], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 519, "seek": 131018, "start": 1312.98, "end": 1317.14, "text": " And so then basically you just instantiate the whole thing.", "tokens": [50504, 400, 370, 550, 1936, 291, 445, 9836, 13024, 264, 1379, 551, 13, 50712], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 520, "seek": 131018, "start": 1317.14, "end": 1319.3, "text": " So we take the most recent chat logs,", "tokens": [50712, 407, 321, 747, 264, 881, 5162, 5081, 20820, 11, 50820], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 521, "seek": 131018, "start": 1319.3, "end": 1323.9, "text": " the main scratch pad and start a new KB article.", "tokens": [50820, 264, 2135, 8459, 6887, 293, 722, 257, 777, 591, 33, 7222, 13, 51050], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 522, "seek": 131018, "start": 1323.9, "end": 1326.0600000000002, "text": " Now, if the collection count is not zero,", "tokens": [51050, 823, 11, 498, 264, 5765, 1207, 307, 406, 4018, 11, 51158], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 523, "seek": 131018, "start": 1326.0600000000002, "end": 1329.5, "text": " which is gonna be most of the time once you get started,", "tokens": [51158, 597, 307, 799, 312, 881, 295, 264, 565, 1564, 291, 483, 1409, 11, 51330], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 524, "seek": 131018, "start": 1329.5, "end": 1331.3400000000001, "text": " what you do is you basically do the same thing", "tokens": [51330, 437, 291, 360, 307, 291, 1936, 360, 264, 912, 551, 51422], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 525, "seek": 131018, "start": 1331.3400000000001, "end": 1335.02, "text": " where you say, okay, based on the most recent conversation,", "tokens": [51422, 689, 291, 584, 11, 1392, 11, 2361, 322, 264, 881, 5162, 3761, 11, 51606], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 526, "seek": 131018, "start": 1335.02, "end": 1337.54, "text": " give me the most relevant document,", "tokens": [51606, 976, 385, 264, 881, 7340, 4166, 11, 51732], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 527, "seek": 131018, "start": 1337.54, "end": 1339.54, "text": " which I probably could compress this", "tokens": [51732, 597, 286, 1391, 727, 14778, 341, 51832], "temperature": 0.0, "avg_logprob": -0.0905199363583424, "compression_ratio": 1.7984496124031009, "no_speech_prob": 0.0006877944688312709}, {"id": 528, "seek": 133954, "start": 1339.58, "end": 1342.06, "text": " and just use the same information here.", "tokens": [50366, 293, 445, 764, 264, 912, 1589, 510, 13, 50490], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 529, "seek": 133954, "start": 1342.06, "end": 1345.42, "text": " Because this is the same,", "tokens": [50490, 1436, 341, 307, 264, 912, 11, 50658], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 530, "seek": 133954, "start": 1345.42, "end": 1348.22, "text": " this is, well, generally find the same thing.", "tokens": [50658, 341, 307, 11, 731, 11, 5101, 915, 264, 912, 551, 13, 50798], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 531, "seek": 133954, "start": 1348.22, "end": 1350.3, "text": " Actually, no, that's not necessarily true", "tokens": [50798, 5135, 11, 572, 11, 300, 311, 406, 4725, 2074, 50902], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 532, "seek": 133954, "start": 1350.3, "end": 1352.34, "text": " because we've updated the main scratch pad.", "tokens": [50902, 570, 321, 600, 10588, 264, 2135, 8459, 6887, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 533, "seek": 133954, "start": 1352.34, "end": 1353.34, "text": " So scratch that.", "tokens": [51004, 407, 8459, 300, 13, 51054], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 534, "seek": 133954, "start": 1354.46, "end": 1359.46, "text": " So if the new user input and chat GPT output", "tokens": [51110, 407, 498, 264, 777, 4195, 4846, 293, 5081, 26039, 51, 5598, 51360], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 535, "seek": 133954, "start": 1360.46, "end": 1363.06, "text": " connects to a different KB article,", "tokens": [51410, 16967, 281, 257, 819, 591, 33, 7222, 11, 51540], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 536, "seek": 133954, "start": 1363.06, "end": 1366.1399999999999, "text": " let's go ahead and get that document and that document ID.", "tokens": [51540, 718, 311, 352, 2286, 293, 483, 300, 4166, 293, 300, 4166, 7348, 13, 51694], "temperature": 0.0, "avg_logprob": -0.1828765483817669, "compression_ratio": 1.6090909090909091, "no_speech_prob": 0.0011693539563566446}, {"id": 537, "seek": 136614, "start": 1366.98, "end": 1368.3400000000001, "text": " And what we'll do is we'll go ahead", "tokens": [50406, 400, 437, 321, 603, 360, 307, 321, 603, 352, 2286, 50474], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 538, "seek": 136614, "start": 1368.3400000000001, "end": 1372.22, "text": " and use system update existing KB article.", "tokens": [50474, 293, 764, 1185, 5623, 6741, 591, 33, 7222, 13, 50668], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 539, "seek": 136614, "start": 1372.22, "end": 1375.42, "text": " So this is a system instruction", "tokens": [50668, 407, 341, 307, 257, 1185, 10951, 50828], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 540, "seek": 136614, "start": 1375.42, "end": 1377.42, "text": " where it basically says all the same stuff,", "tokens": [50828, 689, 309, 1936, 1619, 439, 264, 912, 1507, 11, 50928], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 541, "seek": 136614, "start": 1377.42, "end": 1379.22, "text": " here's the current KB article,", "tokens": [50928, 510, 311, 264, 2190, 591, 33, 7222, 11, 51018], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 542, "seek": 136614, "start": 1379.22, "end": 1381.18, "text": " and then the user will now provide you", "tokens": [51018, 293, 550, 264, 4195, 486, 586, 2893, 291, 51116], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 543, "seek": 136614, "start": 1381.18, "end": 1383.3000000000002, "text": " with the new information to evaluate.", "tokens": [51116, 365, 264, 777, 1589, 281, 13059, 13, 51222], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 544, "seek": 136614, "start": 1383.3000000000002, "end": 1384.94, "text": " And so that is gonna be here", "tokens": [51222, 400, 370, 300, 307, 799, 312, 510, 51304], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 545, "seek": 136614, "start": 1384.94, "end": 1387.46, "text": " where you supply it the current KB article", "tokens": [51304, 689, 291, 5847, 309, 264, 2190, 591, 33, 7222, 51430], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 546, "seek": 136614, "start": 1387.46, "end": 1389.8200000000002, "text": " that it found as well as the scratch pad.", "tokens": [51430, 300, 309, 1352, 382, 731, 382, 264, 8459, 6887, 13, 51548], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 547, "seek": 136614, "start": 1390.7800000000002, "end": 1391.98, "text": " And so it's like, okay, cool,", "tokens": [51596, 400, 370, 309, 311, 411, 11, 1392, 11, 1627, 11, 51656], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 548, "seek": 136614, "start": 1391.98, "end": 1394.9, "text": " now let's do the same thing that we did with the user profile,", "tokens": [51656, 586, 718, 311, 360, 264, 912, 551, 300, 321, 630, 365, 264, 4195, 7964, 11, 51802], "temperature": 0.0, "avg_logprob": -0.13497635855603574, "compression_ratio": 1.75, "no_speech_prob": 0.0005527327302843332}, {"id": 549, "seek": 139490, "start": 1394.9, "end": 1396.7, "text": " which is merge that information.", "tokens": [50364, 597, 307, 22183, 300, 1589, 13, 50454], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 550, "seek": 139490, "start": 1396.7, "end": 1398.5800000000002, "text": " If there's nothing new that's relevant,", "tokens": [50454, 759, 456, 311, 1825, 777, 300, 311, 7340, 11, 50548], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 551, "seek": 139490, "start": 1398.5800000000002, "end": 1402.1000000000001, "text": " leave it alone, but if there is, go ahead and update it.", "tokens": [50548, 1856, 309, 3312, 11, 457, 498, 456, 307, 11, 352, 2286, 293, 5623, 309, 13, 50724], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 552, "seek": 139490, "start": 1402.1000000000001, "end": 1405.6200000000001, "text": " And so then it saves all this out to the DB logs.", "tokens": [50724, 400, 370, 550, 309, 19155, 439, 341, 484, 281, 264, 26754, 20820, 13, 50900], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 553, "seek": 139490, "start": 1405.6200000000001, "end": 1407.9, "text": " And so if you go to DB logs out here,", "tokens": [50900, 400, 370, 498, 291, 352, 281, 26754, 20820, 484, 510, 11, 51014], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 554, "seek": 139490, "start": 1407.9, "end": 1410.22, "text": " you'll see a whole bunch of update statements.", "tokens": [51014, 291, 603, 536, 257, 1379, 3840, 295, 5623, 12363, 13, 51130], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 555, "seek": 139490, "start": 1410.22, "end": 1413.26, "text": " So it says update documented, it gives you the UUID,", "tokens": [51130, 407, 309, 1619, 5623, 23007, 11, 309, 2709, 291, 264, 624, 52, 2777, 11, 51282], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 556, "seek": 139490, "start": 1413.26, "end": 1415.18, "text": " and this is the final output.", "tokens": [51282, 293, 341, 307, 264, 2572, 5598, 13, 51378], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 557, "seek": 139490, "start": 1415.18, "end": 1417.3400000000001, "text": " Actually, probably what I should do is modify this", "tokens": [51378, 5135, 11, 1391, 437, 286, 820, 360, 307, 16927, 341, 51486], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 558, "seek": 139490, "start": 1417.3400000000001, "end": 1419.5, "text": " so it gives you the original,", "tokens": [51486, 370, 309, 2709, 291, 264, 3380, 11, 51594], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 559, "seek": 139490, "start": 1420.5, "end": 1422.3400000000001, "text": " the original, the new information,", "tokens": [51644, 264, 3380, 11, 264, 777, 1589, 11, 51736], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 560, "seek": 139490, "start": 1422.3400000000001, "end": 1424.14, "text": " and then the final output.", "tokens": [51736, 293, 550, 264, 2572, 5598, 13, 51826], "temperature": 0.0, "avg_logprob": -0.10881783776249446, "compression_ratio": 1.8352059925093633, "no_speech_prob": 0.00039200804894790053}, {"id": 561, "seek": 142414, "start": 1424.14, "end": 1427.5800000000002, "text": " So I'll add that as a to-do item, actually.", "tokens": [50364, 407, 286, 603, 909, 300, 382, 257, 281, 12, 2595, 3174, 11, 767, 13, 50536], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 562, "seek": 142414, "start": 1430.7, "end": 1435.7, "text": " Let's see, to-do, save more info in DB logs,", "tokens": [50692, 961, 311, 536, 11, 281, 12, 2595, 11, 3155, 544, 13614, 294, 26754, 20820, 11, 50942], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 563, "seek": 142414, "start": 1437.7800000000002, "end": 1440.38, "text": " probably as YAML file,", "tokens": [51046, 1391, 382, 398, 2865, 43, 3991, 11, 51176], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 564, "seek": 142414, "start": 1441.42, "end": 1446.42, "text": " original article, new info, and then final article.", "tokens": [51228, 3380, 7222, 11, 777, 13614, 11, 293, 550, 2572, 7222, 13, 51478], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 565, "seek": 142414, "start": 1448.22, "end": 1449.7800000000002, "text": " So yeah, that's something that I'll do.", "tokens": [51568, 407, 1338, 11, 300, 311, 746, 300, 286, 603, 360, 13, 51646], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 566, "seek": 142414, "start": 1449.7800000000002, "end": 1452.9, "text": " Now, that being said, one of the biggest problems", "tokens": [51646, 823, 11, 300, 885, 848, 11, 472, 295, 264, 3880, 2740, 51802], "temperature": 0.0, "avg_logprob": -0.19251769422048545, "compression_ratio": 1.4540229885057472, "no_speech_prob": 0.0002098698605550453}, {"id": 567, "seek": 145290, "start": 1452.94, "end": 1454.74, "text": " that we have always had,", "tokens": [50366, 300, 321, 362, 1009, 632, 11, 50456], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 568, "seek": 145290, "start": 1454.74, "end": 1456.46, "text": " so this is the cream of the crop.", "tokens": [50456, 370, 341, 307, 264, 4689, 295, 264, 9086, 13, 50542], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 569, "seek": 145290, "start": 1456.46, "end": 1458.7, "text": " This is the triple crown right here.", "tokens": [50542, 639, 307, 264, 15508, 11841, 558, 510, 13, 50654], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 570, "seek": 145290, "start": 1458.7, "end": 1461.02, "text": " The biggest problem that everyone has always had", "tokens": [50654, 440, 3880, 1154, 300, 1518, 575, 1009, 632, 50770], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 571, "seek": 145290, "start": 1461.02, "end": 1462.5, "text": " with long-term chatbot memory", "tokens": [50770, 365, 938, 12, 7039, 5081, 18870, 4675, 50844], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 572, "seek": 145290, "start": 1462.5, "end": 1465.18, "text": " is how the heck do you keep track of memories?", "tokens": [50844, 307, 577, 264, 12872, 360, 291, 1066, 2837, 295, 8495, 30, 50978], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 573, "seek": 145290, "start": 1465.18, "end": 1468.26, "text": " How the heck do you keep track of different types of memories?", "tokens": [50978, 1012, 264, 12872, 360, 291, 1066, 2837, 295, 819, 3467, 295, 8495, 30, 51132], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 574, "seek": 145290, "start": 1468.26, "end": 1470.26, "text": " Like some people have internal thoughts", "tokens": [51132, 1743, 512, 561, 362, 6920, 4598, 51232], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 575, "seek": 145290, "start": 1470.26, "end": 1473.14, "text": " versus external thoughts and episodic memories", "tokens": [51232, 5717, 8320, 4598, 293, 39200, 299, 8495, 51376], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 576, "seek": 145290, "start": 1473.14, "end": 1476.3000000000002, "text": " and this, that, and the other.", "tokens": [51376, 293, 341, 11, 300, 11, 293, 264, 661, 13, 51534], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 577, "seek": 145290, "start": 1476.3000000000002, "end": 1481.3000000000002, "text": " And you can certainly try and tag and categorize memories", "tokens": [51534, 400, 291, 393, 3297, 853, 293, 6162, 293, 19250, 1125, 8495, 51784], "temperature": 0.0, "avg_logprob": -0.10668955730790851, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.002322599058970809}, {"id": 578, "seek": 148130, "start": 1482.3, "end": 1486.86, "text": " with different context, right?", "tokens": [50414, 365, 819, 4319, 11, 558, 30, 50642], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 579, "seek": 148130, "start": 1486.86, "end": 1488.8999999999999, "text": " With metadata, and I certainly recommend that,", "tokens": [50642, 2022, 26603, 11, 293, 286, 3297, 2748, 300, 11, 50744], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 580, "seek": 148130, "start": 1488.8999999999999, "end": 1490.58, "text": " especially once your cognitive architectures", "tokens": [50744, 2318, 1564, 428, 15605, 6331, 1303, 50828], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 581, "seek": 148130, "start": 1490.58, "end": 1492.3799999999999, "text": " get more sophisticated, right?", "tokens": [50828, 483, 544, 16950, 11, 558, 30, 50918], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 582, "seek": 148130, "start": 1492.3799999999999, "end": 1495.26, "text": " If you do have an out-of-band like thought,", "tokens": [50918, 759, 291, 360, 362, 364, 484, 12, 2670, 12, 4235, 411, 1194, 11, 51062], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 583, "seek": 148130, "start": 1495.26, "end": 1496.8999999999999, "text": " like internal private thoughts,", "tokens": [51062, 411, 6920, 4551, 4598, 11, 51144], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 584, "seek": 148130, "start": 1496.8999999999999, "end": 1498.5, "text": " definitely keep that separate.", "tokens": [51144, 2138, 1066, 300, 4994, 13, 51224], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 585, "seek": 148130, "start": 1498.5, "end": 1501.26, "text": " If you have external sensory information,", "tokens": [51224, 759, 291, 362, 8320, 27233, 1589, 11, 51362], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 586, "seek": 148130, "start": 1501.26, "end": 1502.94, "text": " definitely keep that separate.", "tokens": [51362, 2138, 1066, 300, 4994, 13, 51446], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 587, "seek": 148130, "start": 1502.94, "end": 1504.46, "text": " But what I'm working on here,", "tokens": [51446, 583, 437, 286, 478, 1364, 322, 510, 11, 51522], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 588, "seek": 148130, "start": 1504.46, "end": 1509.22, "text": " rather than just being a way to focus on episodic memory,", "tokens": [51522, 2831, 813, 445, 885, 257, 636, 281, 1879, 322, 39200, 299, 4675, 11, 51760], "temperature": 0.0, "avg_logprob": -0.10950634858318578, "compression_ratio": 1.6706349206349207, "no_speech_prob": 0.0008040209067985415}, {"id": 589, "seek": 150922, "start": 1509.22, "end": 1512.42, "text": " which that's what REMO was my previous attempt,", "tokens": [50364, 597, 300, 311, 437, 45991, 46, 390, 452, 3894, 5217, 11, 50524], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 590, "seek": 150922, "start": 1512.42, "end": 1515.66, "text": " this is a way to accumulate declarative information.", "tokens": [50524, 341, 307, 257, 636, 281, 33384, 16694, 1166, 1589, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 591, "seek": 150922, "start": 1516.66, "end": 1521.3, "text": " And so declarative information is like a statement of fact,", "tokens": [50736, 400, 370, 16694, 1166, 1589, 307, 411, 257, 5629, 295, 1186, 11, 50968], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 592, "seek": 150922, "start": 1521.3, "end": 1522.14, "text": " right?", "tokens": [50968, 558, 30, 51010], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 593, "seek": 150922, "start": 1522.14, "end": 1523.18, "text": " That's why it's called a KB article.", "tokens": [51010, 663, 311, 983, 309, 311, 1219, 257, 591, 33, 7222, 13, 51062], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 594, "seek": 150922, "start": 1523.18, "end": 1524.94, "text": " So rather than just a timeline,", "tokens": [51062, 407, 2831, 813, 445, 257, 12933, 11, 51150], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 595, "seek": 150922, "start": 1524.94, "end": 1526.58, "text": " rather than just a log,", "tokens": [51150, 2831, 813, 445, 257, 3565, 11, 51232], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 596, "seek": 150922, "start": 1526.58, "end": 1529.34, "text": " keeping track of everything in chronological order,", "tokens": [51232, 5145, 2837, 295, 1203, 294, 19393, 4383, 1668, 11, 51370], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 597, "seek": 150922, "start": 1529.34, "end": 1533.3, "text": " the idea here is to connect new information to a KB article.", "tokens": [51370, 264, 1558, 510, 307, 281, 1745, 777, 1589, 281, 257, 591, 33, 7222, 13, 51568], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 598, "seek": 150922, "start": 1533.3, "end": 1535.46, "text": " So there's no reason that you couldn't do both as well,", "tokens": [51568, 407, 456, 311, 572, 1778, 300, 291, 2809, 380, 360, 1293, 382, 731, 11, 51676], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 599, "seek": 150922, "start": 1535.46, "end": 1538.18, "text": " right, because this is how human memory works.", "tokens": [51676, 558, 11, 570, 341, 307, 577, 1952, 4675, 1985, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11817915256206805, "compression_ratio": 1.7060931899641576, "no_speech_prob": 0.004608482122421265}, {"id": 600, "seek": 153818, "start": 1538.66, "end": 1541.14, "text": " Human memory is associative, but it's also temporal.", "tokens": [50388, 10294, 4675, 307, 4180, 1166, 11, 457, 309, 311, 611, 30881, 13, 50512], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 601, "seek": 153818, "start": 1542.1000000000001, "end": 1544.74, "text": " Now, if the KB article gets too large,", "tokens": [50560, 823, 11, 498, 264, 591, 33, 7222, 2170, 886, 2416, 11, 50692], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 602, "seek": 153818, "start": 1544.74, "end": 1546.9, "text": " if you added information,", "tokens": [50692, 498, 291, 3869, 1589, 11, 50800], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 603, "seek": 153818, "start": 1546.9, "end": 1549.14, "text": " and now it's more than a thousand words,", "tokens": [50800, 293, 586, 309, 311, 544, 813, 257, 4714, 2283, 11, 50912], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 604, "seek": 153818, "start": 1549.14, "end": 1552.1000000000001, "text": " then I have another system prompt,", "tokens": [50912, 550, 286, 362, 1071, 1185, 12391, 11, 51060], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 605, "seek": 153818, "start": 1552.1000000000001, "end": 1554.66, "text": " which you can check them all out here.", "tokens": [51060, 597, 291, 393, 1520, 552, 439, 484, 510, 13, 51188], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 606, "seek": 153818, "start": 1554.66, "end": 1557.5800000000002, "text": " So there's system instantiate new KB,", "tokens": [51188, 407, 456, 311, 1185, 9836, 13024, 777, 591, 33, 11, 51334], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 607, "seek": 153818, "start": 1557.5800000000002, "end": 1558.74, "text": " system reflective journaling,", "tokens": [51334, 1185, 28931, 17598, 4270, 11, 51392], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 608, "seek": 153818, "start": 1558.74, "end": 1561.5800000000002, "text": " I just showed you what that was, system split KB.", "tokens": [51392, 286, 445, 4712, 291, 437, 300, 390, 11, 1185, 7472, 591, 33, 13, 51534], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 609, "seek": 153818, "start": 1561.5800000000002, "end": 1562.8200000000002, "text": " So that's this one.", "tokens": [51534, 407, 300, 311, 341, 472, 13, 51596], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 610, "seek": 153818, "start": 1562.8200000000002, "end": 1566.42, "text": " But update user profile, update KB article,", "tokens": [51596, 583, 5623, 4195, 7964, 11, 5623, 591, 33, 7222, 11, 51776], "temperature": 0.0, "avg_logprob": -0.12320603698980613, "compression_ratio": 1.6171875, "no_speech_prob": 9.027810301631689e-05}, {"id": 611, "seek": 156642, "start": 1566.42, "end": 1569.0600000000002, "text": " new KB article, reflective journaling and split KB.", "tokens": [50364, 777, 591, 33, 7222, 11, 28931, 17598, 4270, 293, 7472, 591, 33, 13, 50496], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 612, "seek": 156642, "start": 1569.0600000000002, "end": 1570.3000000000002, "text": " So these are the operations.", "tokens": [50496, 407, 613, 366, 264, 7705, 13, 50558], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 613, "seek": 156642, "start": 1570.3000000000002, "end": 1572.5, "text": " These are the cognitive operations,", "tokens": [50558, 1981, 366, 264, 15605, 7705, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 614, "seek": 156642, "start": 1572.5, "end": 1575.18, "text": " the cognitive memory operations that it's gonna be doing.", "tokens": [50668, 264, 15605, 4675, 7705, 300, 309, 311, 799, 312, 884, 13, 50802], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 615, "seek": 156642, "start": 1575.18, "end": 1577.6200000000001, "text": " And so then basically what it does is say,", "tokens": [50802, 400, 370, 550, 1936, 437, 309, 775, 307, 584, 11, 50924], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 616, "seek": 156642, "start": 1577.6200000000001, "end": 1580.02, "text": " hey, we're gonna give you a long KB article,", "tokens": [50924, 4177, 11, 321, 434, 799, 976, 291, 257, 938, 591, 33, 7222, 11, 51044], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 617, "seek": 156642, "start": 1580.02, "end": 1582.74, "text": " split it into two, into two equal parts.", "tokens": [51044, 7472, 309, 666, 732, 11, 666, 732, 2681, 3166, 13, 51180], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 618, "seek": 156642, "start": 1583.5800000000002, "end": 1587.1000000000001, "text": " And so the idea here is that over time,", "tokens": [51222, 400, 370, 264, 1558, 510, 307, 300, 670, 565, 11, 51398], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 619, "seek": 156642, "start": 1587.1000000000001, "end": 1589.22, "text": " as your KB article gets bigger,", "tokens": [51398, 382, 428, 591, 33, 7222, 2170, 3801, 11, 51504], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 620, "seek": 156642, "start": 1589.22, "end": 1592.3400000000001, "text": " it'll branch and metastasize naturally.", "tokens": [51504, 309, 603, 9819, 293, 1131, 525, 296, 1125, 8195, 13, 51660], "temperature": 0.0, "avg_logprob": -0.11653640490620076, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.0004044596862513572}, {"id": 621, "seek": 159234, "start": 1592.6599999999999, "end": 1596.74, "text": " And so you could then add a lot of additional metadata", "tokens": [50380, 400, 370, 291, 727, 550, 909, 257, 688, 295, 4497, 26603, 50584], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 622, "seek": 159234, "start": 1596.74, "end": 1600.54, "text": " to this, such as like access rate or related articles", "tokens": [50584, 281, 341, 11, 1270, 382, 411, 2105, 3314, 420, 4077, 11290, 50774], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 623, "seek": 159234, "start": 1600.54, "end": 1602.78, "text": " or parent articles or previous articles,", "tokens": [50774, 420, 2596, 11290, 420, 3894, 11290, 11, 50886], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 624, "seek": 159234, "start": 1602.78, "end": 1605.82, "text": " which means that you can naturally evolve", "tokens": [50886, 597, 1355, 300, 291, 393, 8195, 16693, 51038], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 625, "seek": 159234, "start": 1605.82, "end": 1610.26, "text": " a knowledge graph of your knowledge base over time.", "tokens": [51038, 257, 3601, 4295, 295, 428, 3601, 3096, 670, 565, 13, 51260], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 626, "seek": 159234, "start": 1610.26, "end": 1612.86, "text": " You can also do this out of band,", "tokens": [51260, 509, 393, 611, 360, 341, 484, 295, 4116, 11, 51390], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 627, "seek": 159234, "start": 1612.86, "end": 1617.86, "text": " just by doing semantic similarity and entity links and stuff.", "tokens": [51390, 445, 538, 884, 47982, 32194, 293, 13977, 6123, 293, 1507, 13, 51640], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 628, "seek": 159234, "start": 1618.1, "end": 1619.34, "text": " But it would be really cool", "tokens": [51652, 583, 309, 576, 312, 534, 1627, 51714], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 629, "seek": 159234, "start": 1619.34, "end": 1621.8799999999999, "text": " to have a more sophisticated version of this", "tokens": [51714, 281, 362, 257, 544, 16950, 3037, 295, 341, 51841], "temperature": 0.0, "avg_logprob": -0.11532902717590332, "compression_ratio": 1.6680161943319838, "no_speech_prob": 0.0029807700775563717}, {"id": 630, "seek": 162188, "start": 1621.88, "end": 1625.1200000000001, "text": " that allows it to kind of follow that branching tree", "tokens": [50364, 300, 4045, 309, 281, 733, 295, 1524, 300, 9819, 278, 4230, 50526], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 631, "seek": 162188, "start": 1625.1200000000001, "end": 1626.2800000000002, "text": " over time.", "tokens": [50526, 670, 565, 13, 50584], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 632, "seek": 162188, "start": 1626.2800000000002, "end": 1627.44, "text": " So there you have it.", "tokens": [50584, 407, 456, 291, 362, 309, 13, 50642], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 633, "seek": 162188, "start": 1627.44, "end": 1629.68, "text": " That's kind of the whole thing.", "tokens": [50642, 663, 311, 733, 295, 264, 1379, 551, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 634, "seek": 162188, "start": 1629.68, "end": 1631.0, "text": " So that's the chat.", "tokens": [50754, 407, 300, 311, 264, 5081, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 635, "seek": 162188, "start": 1631.0, "end": 1634.6000000000001, "text": " And all this is just real basic, just housekeeping stuff.", "tokens": [50820, 400, 439, 341, 307, 445, 957, 3875, 11, 445, 48033, 1507, 13, 51000], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 636, "seek": 162188, "start": 1634.6000000000001, "end": 1636.8600000000001, "text": " And then at the end of every instance,", "tokens": [51000, 400, 550, 412, 264, 917, 295, 633, 5197, 11, 51113], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 637, "seek": 162188, "start": 1636.8600000000001, "end": 1639.2800000000002, "text": " it does ChromaClient persist.", "tokens": [51113, 309, 775, 1721, 6440, 9966, 1196, 13233, 13, 51234], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 638, "seek": 162188, "start": 1639.2800000000002, "end": 1640.6000000000001, "text": " So now let me show you,", "tokens": [51234, 407, 586, 718, 385, 855, 291, 11, 51300], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 639, "seek": 162188, "start": 1642.72, "end": 1645.8000000000002, "text": " I included a second Python script.", "tokens": [51406, 286, 5556, 257, 1150, 15329, 5755, 13, 51560], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 640, "seek": 162188, "start": 1645.8000000000002, "end": 1647.5600000000002, "text": " So it's just ChromaDB peak,", "tokens": [51560, 407, 309, 311, 445, 1721, 6440, 27735, 10651, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 641, "seek": 162188, "start": 1647.5600000000002, "end": 1649.7600000000002, "text": " which uses the ChromaDB peak function here.", "tokens": [51648, 597, 4960, 264, 1721, 6440, 27735, 10651, 2445, 510, 13, 51758], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 642, "seek": 162188, "start": 1649.7600000000002, "end": 1651.7600000000002, "text": " Let me just show you that script real quick.", "tokens": [51758, 961, 385, 445, 855, 291, 300, 5755, 957, 1702, 13, 51858], "temperature": 0.0, "avg_logprob": -0.1570644952300796, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005527340108528733}, {"id": 643, "seek": 165188, "start": 1652.3600000000001, "end": 1653.72, "text": " ChromaDB peak.", "tokens": [50388, 1721, 6440, 27735, 10651, 13, 50456], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 644, "seek": 165188, "start": 1653.72, "end": 1657.68, "text": " So same stuff, you instantiate the client,", "tokens": [50456, 407, 912, 1507, 11, 291, 9836, 13024, 264, 6423, 11, 50654], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 645, "seek": 165188, "start": 1657.68, "end": 1658.7600000000002, "text": " you connect to it.", "tokens": [50654, 291, 1745, 281, 309, 13, 50708], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 646, "seek": 165188, "start": 1659.64, "end": 1661.68, "text": " It tells you how many entries,", "tokens": [50752, 467, 5112, 291, 577, 867, 23041, 11, 50854], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 647, "seek": 165188, "start": 1661.68, "end": 1664.0800000000002, "text": " and then it will show you the top 10 entries.", "tokens": [50854, 293, 550, 309, 486, 855, 291, 264, 1192, 1266, 23041, 13, 50974], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 648, "seek": 165188, "start": 1664.0800000000002, "end": 1666.74, "text": " And so in my case, I should only have one entry.", "tokens": [50974, 400, 370, 294, 452, 1389, 11, 286, 820, 787, 362, 472, 8729, 13, 51107], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 649, "seek": 165188, "start": 1669.0, "end": 1671.0800000000002, "text": " Let's see, so let's go up to the top.", "tokens": [51220, 961, 311, 536, 11, 370, 718, 311, 352, 493, 281, 264, 1192, 13, 51324], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 650, "seek": 165188, "start": 1671.0800000000002, "end": 1673.88, "text": " Yep, KB presently has one entries,", "tokens": [51324, 7010, 11, 591, 33, 1974, 356, 575, 472, 23041, 11, 51464], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 651, "seek": 165188, "start": 1673.88, "end": 1675.48, "text": " here below the top 10 entries.", "tokens": [51464, 510, 2507, 264, 1192, 1266, 23041, 13, 51544], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 652, "seek": 165188, "start": 1675.48, "end": 1676.6000000000001, "text": " And so here you can see", "tokens": [51544, 400, 370, 510, 291, 393, 536, 51600], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 653, "seek": 165188, "start": 1676.6000000000001, "end": 1679.0600000000002, "text": " that it's actually got several topics,", "tokens": [51600, 300, 309, 311, 767, 658, 2940, 8378, 11, 51723], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 654, "seek": 165188, "start": 1679.0600000000002, "end": 1680.7600000000002, "text": " because the way that it works", "tokens": [51723, 570, 264, 636, 300, 309, 1985, 51808], "temperature": 0.0, "avg_logprob": -0.15554100697434795, "compression_ratio": 1.6835443037974684, "no_speech_prob": 0.00040444626938551664}, {"id": 655, "seek": 168076, "start": 1680.76, "end": 1685.76, "text": " is that it searches for the top one most relevant KB articles.", "tokens": [50364, 307, 300, 309, 26701, 337, 264, 1192, 472, 881, 7340, 591, 33, 11290, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 656, "seek": 168076, "start": 1687.04, "end": 1690.04, "text": " And so that's always gonna return the first one.", "tokens": [50678, 400, 370, 300, 311, 1009, 799, 2736, 264, 700, 472, 13, 50828], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 657, "seek": 168076, "start": 1690.04, "end": 1691.92, "text": " And the first one is not yet long enough", "tokens": [50828, 400, 264, 700, 472, 307, 406, 1939, 938, 1547, 50922], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 658, "seek": 168076, "start": 1691.92, "end": 1693.72, "text": " to justify splitting up.", "tokens": [50922, 281, 20833, 30348, 493, 13, 51012], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 659, "seek": 168076, "start": 1693.72, "end": 1695.56, "text": " But whatever I end up talking about,", "tokens": [51012, 583, 2035, 286, 917, 493, 1417, 466, 11, 51104], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 660, "seek": 168076, "start": 1695.56, "end": 1697.2, "text": " I'll keep talking with the thing,", "tokens": [51104, 286, 603, 1066, 1417, 365, 264, 551, 11, 51186], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 661, "seek": 168076, "start": 1697.2, "end": 1698.5, "text": " and eventually it'll split it up.", "tokens": [51186, 293, 4728, 309, 603, 7472, 309, 493, 13, 51251], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 662, "seek": 168076, "start": 1698.5, "end": 1700.12, "text": " So in this case, it looks like", "tokens": [51251, 407, 294, 341, 1389, 11, 309, 1542, 411, 51332], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 663, "seek": 168076, "start": 1700.12, "end": 1702.24, "text": " it'll probably talk about AI alignment.", "tokens": [51332, 309, 603, 1391, 751, 466, 7318, 18515, 13, 51438], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 664, "seek": 168076, "start": 1702.24, "end": 1706.24, "text": " And then it's gonna also talk about my obsession", "tokens": [51438, 400, 550, 309, 311, 799, 611, 751, 466, 452, 30521, 51638], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 665, "seek": 168076, "start": 1706.24, "end": 1709.24, "text": " with artificial intelligence and work-life balance.", "tokens": [51638, 365, 11677, 7599, 293, 589, 12, 9073, 4772, 13, 51788], "temperature": 0.0, "avg_logprob": -0.1407425208169906, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.0017004332039505243}, {"id": 666, "seek": 170924, "start": 1709.24, "end": 1714.04, "text": " Because those are kinda like two centroid in this.", "tokens": [50364, 1436, 729, 366, 4144, 411, 732, 1489, 6490, 294, 341, 13, 50604], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 667, "seek": 170924, "start": 1714.04, "end": 1718.48, "text": " So let me just go ahead and actually show you", "tokens": [50604, 407, 718, 385, 445, 352, 2286, 293, 767, 855, 291, 50826], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 668, "seek": 170924, "start": 1718.48, "end": 1719.92, "text": " how this will ultimately work.", "tokens": [50826, 577, 341, 486, 6284, 589, 13, 50898], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 669, "seek": 170924, "start": 1719.92, "end": 1724.72, "text": " So if we go to API logs, it should be the last one.", "tokens": [50898, 407, 498, 321, 352, 281, 9362, 20820, 11, 309, 820, 312, 264, 1036, 472, 13, 51138], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 670, "seek": 170924, "start": 1725.76, "end": 1726.82, "text": " Yes, here we go.", "tokens": [51190, 1079, 11, 510, 321, 352, 13, 51243], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 671, "seek": 170924, "start": 1726.82, "end": 1730.68, "text": " So if I plug this in, let's go here.", "tokens": [51243, 407, 498, 286, 5452, 341, 294, 11, 718, 311, 352, 510, 13, 51436], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 672, "seek": 170924, "start": 1730.68, "end": 1732.8, "text": " So that's the message that I'm gonna want it.", "tokens": [51436, 407, 300, 311, 264, 3636, 300, 286, 478, 799, 528, 309, 13, 51542], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 673, "seek": 170924, "start": 1732.8, "end": 1737.8, "text": " And then let's grab the split, the split message.", "tokens": [51542, 400, 550, 718, 311, 4444, 264, 7472, 11, 264, 7472, 3636, 13, 51792], "temperature": 0.0, "avg_logprob": -0.12078750834745519, "compression_ratio": 1.5817307692307692, "no_speech_prob": 0.0007321445154957473}, {"id": 674, "seek": 173780, "start": 1738.32, "end": 1743.04, "text": " So you'll see what I mean by how it will ultimately", "tokens": [50390, 407, 291, 603, 536, 437, 286, 914, 538, 577, 309, 486, 6284, 50626], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 675, "seek": 173780, "start": 1743.04, "end": 1744.44, "text": " kinda metastasize.", "tokens": [50626, 4144, 1131, 525, 296, 1125, 13, 50696], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 676, "seek": 173780, "start": 1744.44, "end": 1745.52, "text": " Zoom in a little bit.", "tokens": [50696, 13453, 294, 257, 707, 857, 13, 50750], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 677, "seek": 173780, "start": 1745.52, "end": 1747.76, "text": " All right, we're using GPT-4, temperature zero,", "tokens": [50750, 1057, 558, 11, 321, 434, 1228, 26039, 51, 12, 19, 11, 4292, 4018, 11, 50862], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 678, "seek": 173780, "start": 1747.76, "end": 1750.44, "text": " maximum length, a thousand.", "tokens": [50862, 6674, 4641, 11, 257, 4714, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 679, "seek": 173780, "start": 1750.44, "end": 1754.36, "text": " All right, so basically what it's gonna do is,", "tokens": [50996, 1057, 558, 11, 370, 1936, 437, 309, 311, 799, 360, 307, 11, 51192], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 680, "seek": 173780, "start": 1754.36, "end": 1756.96, "text": " the end says the user will now provide you", "tokens": [51192, 264, 917, 1619, 264, 4195, 486, 586, 2893, 291, 51322], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 681, "seek": 173780, "start": 1756.96, "end": 1758.5, "text": " with the KB article to split.", "tokens": [51322, 365, 264, 591, 33, 7222, 281, 7472, 13, 51399], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 682, "seek": 173780, "start": 1758.5, "end": 1761.4199999999998, "text": " So I submit it, and now it's gonna look at this,", "tokens": [51399, 407, 286, 10315, 309, 11, 293, 586, 309, 311, 799, 574, 412, 341, 11, 51545], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 683, "seek": 173780, "start": 1761.4199999999998, "end": 1764.72, "text": " and it's gonna say article one, and then article two.", "tokens": [51545, 293, 309, 311, 799, 584, 7222, 472, 11, 293, 550, 7222, 732, 13, 51710], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 684, "seek": 173780, "start": 1765.76, "end": 1767.2, "text": " So let's see what it ultimately does.", "tokens": [51762, 407, 718, 311, 536, 437, 309, 6284, 775, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1664956601938807, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.006902639754116535}, {"id": 685, "seek": 176720, "start": 1767.2, "end": 1768.76, "text": " And you can see how slow it is.", "tokens": [50364, 400, 291, 393, 536, 577, 2964, 309, 307, 13, 50442], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 686, "seek": 176720, "start": 1768.76, "end": 1772.1200000000001, "text": " So this is why ultimately you're gonna wanna do this", "tokens": [50442, 407, 341, 307, 983, 6284, 291, 434, 799, 1948, 360, 341, 50610], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 687, "seek": 176720, "start": 1772.1200000000001, "end": 1776.4, "text": " out of band as a threaded process or do it periodically,", "tokens": [50610, 484, 295, 4116, 382, 257, 47493, 1399, 420, 360, 309, 38916, 11, 50824], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 688, "seek": 176720, "start": 1776.4, "end": 1780.4, "text": " maybe break it up and do it when the user's offline", "tokens": [50824, 1310, 1821, 309, 493, 293, 360, 309, 562, 264, 4195, 311, 21857, 51024], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 689, "seek": 176720, "start": 1780.4, "end": 1784.6000000000001, "text": " or whatever, but you see how each article now", "tokens": [51024, 420, 2035, 11, 457, 291, 536, 577, 1184, 7222, 586, 51234], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 690, "seek": 176720, "start": 1784.6000000000001, "end": 1787.76, "text": " is much more specific.", "tokens": [51234, 307, 709, 544, 2685, 13, 51392], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 691, "seek": 176720, "start": 1787.76, "end": 1790.04, "text": " And so then once you go into each of these articles", "tokens": [51392, 400, 370, 550, 1564, 291, 352, 666, 1184, 295, 613, 11290, 51506], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 692, "seek": 176720, "start": 1790.04, "end": 1794.52, "text": " in the future, identifying factors", "tokens": [51506, 294, 264, 2027, 11, 16696, 6771, 51730], "temperature": 0.0, "avg_logprob": -0.1379668560433895, "compression_ratio": 1.579185520361991, "no_speech_prob": 0.0002959390403702855}, {"id": 693, "seek": 179452, "start": 1794.52, "end": 1797.52, "text": " and seeking professional help if necessary, yeah.", "tokens": [50364, 293, 11670, 4843, 854, 498, 4818, 11, 1338, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 694, "seek": 179452, "start": 1797.52, "end": 1800.84, "text": " And so basically it'll allow the articles", "tokens": [50514, 400, 370, 1936, 309, 603, 2089, 264, 11290, 50680], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 695, "seek": 179452, "start": 1800.84, "end": 1802.6399999999999, "text": " to metastasize over time.", "tokens": [50680, 281, 1131, 525, 296, 1125, 670, 565, 13, 50770], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 696, "seek": 179452, "start": 1802.6399999999999, "end": 1807.6399999999999, "text": " Now that being said, if no new information", "tokens": [50770, 823, 300, 885, 848, 11, 498, 572, 777, 1589, 51020], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 697, "seek": 179452, "start": 1807.8799999999999, "end": 1810.04, "text": " is added to an article, it won't update it.", "tokens": [51032, 307, 3869, 281, 364, 7222, 11, 309, 1582, 380, 5623, 309, 13, 51140], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 698, "seek": 179452, "start": 1810.04, "end": 1811.6399999999999, "text": " It's that simple.", "tokens": [51140, 467, 311, 300, 2199, 13, 51220], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 699, "seek": 179452, "start": 1811.6399999999999, "end": 1815.2, "text": " Now that being said, there will probably be a need", "tokens": [51220, 823, 300, 885, 848, 11, 456, 486, 1391, 312, 257, 643, 51398], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 700, "seek": 179452, "start": 1815.2, "end": 1819.72, "text": " to do some KB article grooming over time,", "tokens": [51398, 281, 360, 512, 591, 33, 7222, 49700, 670, 565, 11, 51624], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 701, "seek": 179452, "start": 1819.72, "end": 1822.6, "text": " but the idea is that the KB will only grow", "tokens": [51624, 457, 264, 1558, 307, 300, 264, 591, 33, 486, 787, 1852, 51768], "temperature": 0.0, "avg_logprob": -0.09495761093584079, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.0010986669221892953}, {"id": 702, "seek": 182260, "start": 1822.6399999999999, "end": 1826.12, "text": " as much as it needs to and no more, no less,", "tokens": [50366, 382, 709, 382, 309, 2203, 281, 293, 572, 544, 11, 572, 1570, 11, 50540], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 703, "seek": 182260, "start": 1826.12, "end": 1827.84, "text": " and it will only grow based on the things", "tokens": [50540, 293, 309, 486, 787, 1852, 2361, 322, 264, 721, 50626], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 704, "seek": 182260, "start": 1827.84, "end": 1830.1999999999998, "text": " that you have talked about, and it will record it", "tokens": [50626, 300, 291, 362, 2825, 466, 11, 293, 309, 486, 2136, 309, 50744], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 705, "seek": 182260, "start": 1830.1999999999998, "end": 1834.1999999999998, "text": " in these very succinct, concise articles.", "tokens": [50744, 294, 613, 588, 21578, 5460, 11, 44882, 11290, 13, 50944], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 706, "seek": 182260, "start": 1836.28, "end": 1839.6399999999999, "text": " So then what happens is that it splits these two up,", "tokens": [51048, 407, 550, 437, 2314, 307, 300, 309, 37741, 613, 732, 493, 11, 51216], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 707, "seek": 182260, "start": 1839.6399999999999, "end": 1842.04, "text": " and then the final thing that the chatbot does", "tokens": [51216, 293, 550, 264, 2572, 551, 300, 264, 5081, 18870, 775, 51336], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 708, "seek": 182260, "start": 1842.04, "end": 1844.9199999999998, "text": " is it will do an update for the first one", "tokens": [51336, 307, 309, 486, 360, 364, 5623, 337, 264, 700, 472, 51480], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 709, "seek": 182260, "start": 1844.9199999999998, "end": 1847.12, "text": " and then add the second one.", "tokens": [51480, 293, 550, 909, 264, 1150, 472, 13, 51590], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 710, "seek": 182260, "start": 1847.12, "end": 1848.24, "text": " So it's that simple.", "tokens": [51590, 407, 309, 311, 300, 2199, 13, 51646], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 711, "seek": 182260, "start": 1848.24, "end": 1850.12, "text": " And then when you do an update, as long as you don't,", "tokens": [51646, 400, 550, 562, 291, 360, 364, 5623, 11, 382, 938, 382, 291, 500, 380, 11, 51740], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 712, "seek": 182260, "start": 1850.12, "end": 1852.32, "text": " if you don't specify the embedding,", "tokens": [51740, 498, 291, 500, 380, 16500, 264, 12240, 3584, 11, 51850], "temperature": 0.0, "avg_logprob": -0.10076804690890842, "compression_ratio": 1.8326693227091633, "no_speech_prob": 0.0019265287555754185}, {"id": 713, "seek": 185232, "start": 1852.32, "end": 1854.9199999999998, "text": " it'll automatically recalculate the embedding,", "tokens": [50364, 309, 603, 6772, 850, 304, 2444, 473, 264, 12240, 3584, 11, 50494], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 714, "seek": 185232, "start": 1854.9199999999998, "end": 1855.76, "text": " and then you're good to go.", "tokens": [50494, 293, 550, 291, 434, 665, 281, 352, 13, 50536], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 715, "seek": 185232, "start": 1855.76, "end": 1858.72, "text": " So I haven't quite got here yet, so it might break,", "tokens": [50536, 407, 286, 2378, 380, 1596, 658, 510, 1939, 11, 370, 309, 1062, 1821, 11, 50684], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 716, "seek": 185232, "start": 1858.72, "end": 1863.1599999999999, "text": " but I think this kinda, yeah, I think that's about it.", "tokens": [50684, 457, 286, 519, 341, 4144, 11, 1338, 11, 286, 519, 300, 311, 466, 309, 13, 50906], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 717, "seek": 185232, "start": 1863.1599999999999, "end": 1864.84, "text": " So like I said, it's over here.", "tokens": [50906, 407, 411, 286, 848, 11, 309, 311, 670, 510, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 718, "seek": 185232, "start": 1865.72, "end": 1869.36, "text": " ChromaDB, public, chatbot should be all set.", "tokens": [51034, 1721, 6440, 27735, 11, 1908, 11, 5081, 18870, 820, 312, 439, 992, 13, 51216], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}, {"id": 719, "seek": 185232, "start": 1870.8799999999999, "end": 1874.72, "text": " Yeah, all right, cool.", "tokens": [51292, 865, 11, 439, 558, 11, 1627, 13, 51484], "temperature": 0.0, "avg_logprob": -0.1605627921319777, "compression_ratio": 1.4635416666666667, "no_speech_prob": 0.0006562925991602242}], "language": "en"}