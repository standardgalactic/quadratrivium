WEBVTT

00:00.000 --> 00:05.720
Hello everybody, David Shapiro here with a update.

00:05.720 --> 00:09.120
So we're going to cover two things today real quick.

00:09.120 --> 00:15.720
The big thing is, I mentioned in a previous video, we need GAIA, Global AI Agency.

00:15.720 --> 00:17.160
And we're working towards that.

00:17.160 --> 00:24.080
And so I created a public repository of news and summaries, basically where I'm collecting

00:24.080 --> 00:29.360
evidence and news that is very encouraging because it's all moving in the right direction.

00:29.360 --> 00:35.520
We've got everything from the Future of Life Institute open letter to pause giant models,

00:35.520 --> 00:40.880
news out of the UK, the EU, which yes, I know the UK left the EU.

00:40.880 --> 00:47.000
I made a mistake of forgetting about Brexit in a previous video.

00:47.000 --> 00:53.000
The UN Secretary General talking about yes, he's amenable to the idea of an international

00:53.000 --> 01:00.960
AI watchdog, the United States Senate, their hearing on AI, the EU AI Act, the US House

01:00.960 --> 01:08.600
of Representatives and Senate having now basically matched actions with words.

01:08.600 --> 01:15.440
So after the hearing, both the US House of Representatives and the Senate have introduced

01:15.440 --> 01:20.040
legislation or commissions or policies.

01:20.040 --> 01:23.480
So we'll go over all those in much greater detail in just a moment.

01:23.480 --> 01:27.240
But before we dive into that, I've been working on this thing.

01:27.240 --> 01:32.680
I had this idea for a long time, and it's basically we can use large language models

01:32.680 --> 01:38.840
like GPT because they are trained on so much text data, they're trained on data from around

01:38.840 --> 01:39.840
the globe.

01:39.840 --> 01:46.480
Granted, it is primarily English, so there is definitely some bias, cultural bias in there.

01:46.480 --> 01:52.440
However, that being said, it has read so much about the world that it knows more about religion,

01:52.440 --> 01:57.760
politics, psychology, culture, history than any one person.

01:57.760 --> 02:04.440
So as far as having the ability to take perspectives, it's actually pretty good.

02:04.440 --> 02:08.360
And so rather than walk you through the code, I'll just show you what I've got working so

02:08.360 --> 02:09.360
far.

02:09.360 --> 02:14.360
So basically what I have it do is actually first I need to make sure, well, no, that's

02:14.360 --> 02:15.640
fine.

02:15.640 --> 02:26.000
So basically what I have it do, what it does, anyways, is I have it synthesize a persona.

02:26.000 --> 02:35.880
And so this persona is a whole bunch of random stuff like variables that it can pick to basically

02:35.880 --> 02:38.240
create a synthetic person.

02:38.240 --> 02:46.480
And I don't mean that in the manipulation way, but it has geographic origins, ethnicities,

02:46.480 --> 02:53.800
cultures, languages, political dispositions, different factors about the person, such as

02:53.800 --> 02:59.440
their physical health, their mental health, other habits, preferences, that sort of thing.

02:59.440 --> 03:05.920
And so basically what it does is to create a persona is it'll grab one of these variables

03:05.920 --> 03:13.120
or one option from each set of variables and then create a profile or a dossier in

03:13.120 --> 03:17.440
order to take the perspective of that person.

03:17.440 --> 03:20.520
And then it will talk through on a particular issue.

03:20.520 --> 03:26.600
And I picked UBI because I've been on a economics and UBI kick lately.

03:26.600 --> 03:35.160
And so I wanted to basically come up with a way of automatically understanding what the

03:35.160 --> 03:42.120
concerns that everyone is going to have could be, but not just elucidate or articulate the

03:42.120 --> 03:49.120
problems that they will have or the concerns that they'll have over a policy like UBI,

03:49.120 --> 03:52.960
but how do we actually arrive at consensus?

03:52.960 --> 03:58.880
And consensus is not necessarily, so a lot of people have misconception about consensus.

03:58.880 --> 04:01.480
Consensus doesn't mean unanimous agreement.

04:01.480 --> 04:06.160
Consensus means that it gets to a point that is good enough that people will kind of stop

04:06.160 --> 04:07.160
fighting it.

04:07.160 --> 04:08.720
It's about compromise.

04:08.720 --> 04:15.200
And so anytime that there's been any kind of contentious policy or legislation, even

04:15.200 --> 04:19.400
including the United States Constitution, nobody was happy with it because they had to

04:19.400 --> 04:24.920
come to consensus in order to get something that everyone would tolerate, everyone would

04:24.920 --> 04:25.920
accept.

04:25.920 --> 04:30.280
So it wasn't perfect to anyone's mind, but it was something that they would all accept.

04:30.280 --> 04:35.800
And so the idea is, okay, let's take this random profile of a person so that we can

04:35.800 --> 04:42.040
get a global representation of all perspectives, regardless of how small they are, because

04:42.040 --> 04:47.040
the thing is, when you look at some of these things like atheist, Islam, Christianity,

04:47.040 --> 04:54.200
Buddhist, some of these are overrepresented in some areas and some of them are underrepresented.

04:54.200 --> 04:55.800
Same thing for political leanings.

04:55.800 --> 05:01.760
I even include libertarians and anarchists and statists and communists and reactionaries

05:01.760 --> 05:03.800
and populists and nationalists and fascists.

05:03.800 --> 05:07.440
I don't know if fascism isn't in here.

05:07.440 --> 05:08.920
Fascism isn't in here.

05:08.920 --> 05:11.960
So fascist perspectives don't get represented, but nationalists do.

05:11.960 --> 05:14.800
So close enough, authoritarians get represented.

05:14.800 --> 05:20.560
So the idea is that we can get a very well-rounded representation of all human perspectives more

05:20.560 --> 05:21.560
or less.

05:21.560 --> 05:27.040
There are going to be some flaws with this because it is the first edition, first example.

05:27.040 --> 05:34.080
But the idea is that you can get a very, very diverse set of perspectives on a particular

05:34.080 --> 05:35.080
issue.

05:35.080 --> 05:39.720
And then once you get those perspectives, you can then work through and figure out, okay,

05:39.720 --> 05:45.080
what kind of policy is this person most likely to accept?

05:45.080 --> 05:50.240
And so by then generating a whole bunch of potential policies, you can say, okay, well,

05:50.320 --> 05:54.600
let's look at the commonality between all of these and let's get really creative.

05:54.600 --> 05:56.720
So let me just go ahead and show you how this works.

05:56.720 --> 05:57.720
All right.

05:57.720 --> 06:02.280
So the first thing it does is it grabs a random persona, so you see where it's like heavy

06:02.280 --> 06:07.680
social media user, they're West Asian, cultural background is West Asian, their geographic

06:07.680 --> 06:09.920
origin is from North Africa.

06:09.920 --> 06:15.400
So imagine a West Asian family in North Africa, they're a wealthy family, they're heavy social

06:15.400 --> 06:17.280
media user, so on and so forth.

06:17.280 --> 06:22.520
Okay, so I ask, what would this person think about UBI?

06:22.520 --> 06:27.920
And because ChatGPT is so heavily trained to be a helpful assistant, I couldn't get

06:27.920 --> 06:33.320
ChatGPT to take that perspective, but what I could do is say advocate zealously on behalf

06:33.320 --> 06:36.640
of this person, and that got really good results.

06:36.640 --> 06:40.800
So let's see, as an advocate for this persona, I would say that they might have mixed feelings

06:40.800 --> 06:42.160
about UBI.

06:42.160 --> 06:49.560
On one hand, they are wealthy and underemployed, which might lead them to see UBI as, let's

06:49.560 --> 06:50.560
see, where did it go?

06:50.560 --> 06:51.560
Hang on.

06:51.560 --> 06:52.560
Okay, it's saved.

06:52.560 --> 06:53.560
Sorry.

06:53.560 --> 06:57.160
Let's see, unnecessary for themselves and potentially harmful to their financial status

06:57.160 --> 06:59.120
if it results in higher taxes.

06:59.120 --> 07:03.080
They also might be concerned about the potential for UBI to discourage work and productivity

07:03.080 --> 07:06.560
given their own success in wealth oriented aspirations.

07:06.560 --> 07:09.760
On the other hand, their experience with financial stress and their social concerns might make

07:09.760 --> 07:15.400
them sympathetic to the idea of UBI as a means of reducing poverty and inequality.

07:15.400 --> 07:20.160
However, their nationalist political affiliation might make them more inclined to support policies

07:20.160 --> 07:24.200
that prioritize the needs and interests of their own country and its citizens, which

07:24.200 --> 07:28.160
could include skepticism towards UBI if they perceive it as benefiting outsiders as a form

07:28.160 --> 07:29.800
of globalist policy.

07:29.800 --> 07:35.120
So you see it has a very nuanced way of saying, okay, this person has lots of different beliefs

07:35.120 --> 07:38.760
and it's pretty complex.

07:38.760 --> 07:39.760
Let's see.

07:39.760 --> 07:43.480
Okay, so given their wealth oriented aspirations, nationalist political affiliation and their

07:43.480 --> 07:48.200
experience with financial stress, they might resonate with a fiscal policy or UBI implementation

07:48.200 --> 07:52.640
that is designed to stimulate economic growth and productivity while also addressing social

07:52.640 --> 07:54.720
inequality within their own country.

07:54.720 --> 07:58.380
For instance, they might support a UBI program that is funded through progressive taxation

07:58.380 --> 08:01.320
where the wealthy pay a higher percentage of their income taxes.

08:01.320 --> 08:04.200
This would align with their social concerns and their belief in the importance of addressing

08:04.200 --> 08:05.200
inequality.

08:05.200 --> 08:08.420
However, they would likely want assurances that this would not unduly burden them or

08:08.420 --> 08:10.820
negatively impact their own financial status.

08:10.820 --> 08:15.420
They might also resonate with a UBI program that includes work incentives or requirements

08:15.420 --> 08:18.640
to address their concerns about discouraging work and productivity.

08:18.640 --> 08:22.540
For instance, they might support a UBI that is conditional on recipients engaging in

08:22.540 --> 08:26.300
some kind of work, education or community service.

08:26.300 --> 08:31.260
So this is a theme that keeps emerging because I've run this experiment a few times.

08:31.260 --> 08:35.820
I don't know if that's the underlying training of chat GPT or if it's just this is the most

08:35.820 --> 08:42.180
logical way forward where basically either you have a requirement or an incentive or

08:42.180 --> 08:43.180
reward.

08:43.180 --> 08:48.940
So basically like you get, everyone gets a baseline UBI, but if you volunteer, you get

08:48.940 --> 08:50.540
more UBI, right?

08:50.540 --> 08:54.220
It's like kind of adjusting your tax returns here in America.

08:54.220 --> 08:55.220
Okay.

08:55.220 --> 09:00.220
So finally, given their nationalist policy, they want to UBI that prioritizes the needs

09:00.220 --> 09:03.140
and interests of their own country's citizens.

09:03.140 --> 09:09.840
So this could be something like a tax break if you buy local or any number of ways.

09:09.840 --> 09:17.820
So then I ask it to say, all right, come up with a policy that this person might like.

09:17.820 --> 09:23.260
And so in this case, it said the national prosperity dividend, which that sounds rather

09:23.260 --> 09:25.820
authoritarian, but okay, we'll go with it.

09:25.820 --> 09:33.180
So the NPD, which that's also the acronym for narcissistic personality disorder.

09:33.180 --> 09:35.380
Oh boy.

09:35.380 --> 09:38.700
Maybe let's workshop that before we go live with it.

09:38.700 --> 09:43.620
Anyways, so the NPD is a form of universal basic income that is designed to stimulate

09:43.620 --> 09:47.380
economic growth, reduce social inequality and prioritize the needs and interest.

09:47.380 --> 09:48.380
Okay.

09:48.380 --> 09:49.380
So you get that.

09:49.380 --> 09:54.180
So it's predicated on their engaging in some form of work, education or community service,

09:54.180 --> 09:57.940
more time work, vocational training, higher ed, so on and so forth.

09:57.940 --> 10:02.860
You know, actually like that particular thing, because this is a recurring theme, I'm not

10:02.860 --> 10:06.940
certain that that's actually a bad idea where like you incentivize people to continue to

10:06.940 --> 10:13.980
better themselves, even if it's a small carrot, like, you know, hey, if you're in, if you're

10:13.980 --> 10:18.380
going to higher education, even if, you know, you're at the college degree you're getting

10:18.380 --> 10:22.980
might not ever do anything, you get an extra $500 a month, you will at least be a better

10:22.980 --> 10:26.900
informed citizen and a better civic participant.

10:26.900 --> 10:27.900
Who knows?

10:27.900 --> 10:28.900
I don't know.

10:28.900 --> 10:30.380
Incentivize the behavior you want to see.

10:30.380 --> 10:31.380
Oh, here we go.

10:31.380 --> 10:34.500
The NPD would also include a patriotic bonus.

10:34.500 --> 10:35.500
Yikes.

10:35.500 --> 10:40.700
An additional payment for those who contribute significantly to the country's economy, culture

10:40.700 --> 10:41.940
or society.

10:41.940 --> 10:42.940
Okay.

10:42.940 --> 10:45.740
I wouldn't call it a patriotic bonus.

10:45.740 --> 10:49.860
That's a little, that's a little totalitarian, but I get the, I get the idea.

10:49.860 --> 10:53.300
So this would include entrepreneurs, artists, scientists and community leaders.

10:53.300 --> 10:58.780
So actually I think Ireland already has something like this where basically if you are a professional

10:58.780 --> 11:03.620
artist or author or whatever a creative type, you get a stipend.

11:03.620 --> 11:07.500
I don't know what they call it in Ireland, but there is precedent for this being a thing.

11:07.500 --> 11:12.940
So like if you're, if you're a content creator or a cultural commentator or whatever, you

11:12.940 --> 11:16.940
could get a little bit, an additional stipend.

11:16.940 --> 11:17.940
So all right.

11:18.220 --> 11:20.060
There you have it.

11:20.060 --> 11:22.780
And then if we run it again, so let me do a clear screen.

11:22.780 --> 11:28.500
If we run it again, let's say we get a young adult who's wealthy from Eastern Europe, whose

11:28.500 --> 11:32.340
culture is indigenous South American, interesting.

11:32.340 --> 11:33.900
They like gaming.

11:33.900 --> 11:34.900
They speak Spanish.

11:34.900 --> 11:38.420
That makes sense if they're from South America.

11:38.420 --> 11:43.580
They do not care about the community and they are a very fragile person.

11:43.620 --> 11:51.660
They're an angry, fragile person who's, who is educated, they've been very experienced.

11:51.660 --> 11:55.660
They had a good childhood, sorry, they've had a good, good life experience.

11:55.660 --> 11:56.740
They're presently single.

11:56.740 --> 11:58.740
They're a progressive atheist.

11:58.740 --> 11:59.740
Yeah.

11:59.740 --> 12:03.820
This actually sounds, oh, and they're worried about their career.

12:03.820 --> 12:04.820
Interesting.

12:04.820 --> 12:05.820
Okay.

12:05.820 --> 12:07.580
So let's see.

12:07.580 --> 12:09.740
I'm not going to read the whole thing to you.

12:09.740 --> 12:13.060
Like I did the first one, you get the idea.

12:13.260 --> 12:17.460
So you see how it takes into account, like the last one, they were nationalist and blah,

12:17.460 --> 12:18.460
blah, blah.

12:18.460 --> 12:19.460
And this one is progressive.

12:19.460 --> 12:23.260
Given their moderate intolerance, they might prefer UBI that includes some form of means

12:23.260 --> 12:24.260
testing.

12:24.260 --> 12:27.260
Means testing keeps coming up, keeps coming up as well.

12:27.260 --> 12:28.260
Let's see.

12:28.260 --> 12:30.780
Skeptical about whether everyone deserves UBI.

12:30.780 --> 12:31.780
Ouch.

12:31.780 --> 12:36.140
I, there's some people on Reddit that this sounds like.

12:36.140 --> 12:37.420
And that's not to trash people on Reddit.

12:37.420 --> 12:41.100
I actually learned a lot from some people on Reddit.

12:41.100 --> 12:42.100
But yeah.

12:42.100 --> 12:45.940
So UBI to reduce overconsumption and promote more sustainable lifestyles.

12:45.940 --> 12:46.940
Yeah.

12:46.940 --> 12:53.420
So this actually keeps coming up as well where for some people means testing sustainability.

12:53.420 --> 13:00.460
So basically like you might get an additional UBI bonus if you live sustainably or whatever.

13:00.460 --> 13:04.460
So basically like discourage overconsumption, that's a, that's a trend that keeps coming

13:04.460 --> 13:05.460
up.

13:05.460 --> 13:06.460
So okay.

13:06.460 --> 13:13.700
What environmental U, U, UBI, the puibi, the PB would be funded primary through a progressive

13:13.700 --> 13:18.340
tax system where the wealthiest individuals and okay, yeah, blah, blah, blah.

13:18.340 --> 13:21.820
I haven't seen too much in terms of funding.

13:21.820 --> 13:27.020
One of them did say fund it through carbon taxes, which I thought was, was an interesting

13:27.020 --> 13:31.580
way as you partially fund it through, through that to align with their environmentalist values

13:31.580 --> 13:33.460
would also, oh, here it is.

13:33.460 --> 13:34.460
Carbon tax.

13:34.460 --> 13:35.460
Okay.

13:35.580 --> 13:37.460
So these are some ideas that keep coming up.

13:37.460 --> 13:42.100
Again, I think that the underlying model has a little bit of bias here.

13:42.100 --> 13:48.420
It would be interesting to see if there is a future version of GPT that is not already

13:48.420 --> 13:52.580
pre-trained to be kind of on board with some of these ideas.

13:52.580 --> 13:57.180
Because one thing that doesn't, that has come up is like, if you have someone who is just

13:57.180 --> 14:02.220
adamantly opposed to it, it doesn't say like, this person will never agree to this under

14:02.220 --> 14:03.340
any circumstances.

14:03.340 --> 14:07.420
It works really hard to try and find something that they might agree with.

14:07.420 --> 14:15.380
So anyways, this is all saved out to, it's saved out into the UBI folder as a YAML document.

14:15.380 --> 14:20.220
So it just, it basically just saves the conversation as, as a whole.

14:20.220 --> 14:25.780
I think, yeah, it even includes the system message because the system message includes

14:25.780 --> 14:29.260
the, includes the, the persona.

14:29.260 --> 14:30.500
And the first one, it was hilarious.

14:30.500 --> 14:36.740
It was a radically intolerant feminist who is a Scientologist, which is just like, wow,

14:36.740 --> 14:40.500
this was, this was really interesting as it tried to figure out how to appease a very

14:40.500 --> 14:42.820
dogmatic person.

14:42.820 --> 14:46.620
So anyways, this is a work in progress.

14:46.620 --> 14:52.460
I, I don't know where it's going exactly, but the idea is that maybe you could use it

14:52.460 --> 14:58.500
for policy research, maybe you could use it for, it actually came, it was, I've been thinking

14:58.500 --> 15:04.500
about it for a long time, but I had some inspiration after talking to the Gato community about the

15:04.500 --> 15:10.820
Democratic inputs to AI because the idea of using a chatbot to extract information from

15:10.820 --> 15:13.700
a person, from a real person is one thing.

15:13.700 --> 15:16.980
But then I was like, you know, the model already has a tremendous amount of information.

15:16.980 --> 15:21.860
So why don't we just bootstrap it and ask the model to kind of think through this.

15:21.860 --> 15:26.820
So this is essentially a tree of thought, but each branch of the tree is a different

15:26.820 --> 15:27.940
persona.

15:27.940 --> 15:32.780
And then each of those branches has three sub branches where I ask at those three questions

15:32.780 --> 15:36.820
like, you know, what do you think this person will think about UBI?

15:36.820 --> 15:39.220
What kind of fiscal policy do you think would resonate?

15:39.220 --> 15:42.620
And then finally, given the persona and their disposition, can you creatively conjure up

15:42.620 --> 15:46.640
a policy that has a high chance of reaching consensus with this person?

15:46.640 --> 15:49.980
So it's, you know, basically you can create an arbitrary number of branches.

15:49.980 --> 15:54.220
And then as you, as those branches span out and you get all the leaves, you gather the

15:54.220 --> 15:56.180
leaves together and see what fits.

15:56.180 --> 15:57.180
Okay.

15:57.180 --> 16:00.020
Anyways, so you're up to date on that project.

16:00.020 --> 16:05.140
So let's dive back into the GAIA initiative, the global AI agencies.

16:05.140 --> 16:11.140
So I've been watching the news and I realized that the number of things that are piling

16:11.140 --> 16:13.100
up that make me feel good.

16:13.100 --> 16:17.780
And I know that there's a lot of people out there that are skeptical of, of anything to

16:17.780 --> 16:20.980
do with government or corporations.

16:20.980 --> 16:23.940
And for those people, I empathize with you.

16:23.940 --> 16:32.420
Growing up, I was more like that where I was super skeptical of all trappings of power.

16:32.420 --> 16:38.820
And certainly my friends were very like disestablishment, Tarianism and anarcho libertarian, whatever.

16:38.820 --> 16:41.660
But none of them ever did anything with it.

16:41.660 --> 16:43.020
So this is the world we live in.

16:43.020 --> 16:46.540
We live in a world with corporations and governments and stuff.

16:46.540 --> 16:50.380
And yes, all power needs to be scrutinized.

16:50.380 --> 16:53.500
Seescepticism is absolutely fine.

16:53.500 --> 16:55.660
That is part of the democratic process.

16:55.660 --> 17:00.280
That being said, if you're dogmatically against all forms of power, well, I mean, wish in

17:00.280 --> 17:03.180
one hand and you know, you know what to do with the other one.

17:03.180 --> 17:04.500
See which one fills up first.

17:04.500 --> 17:10.500
So anyways, I have been keeping track of all this stuff because I see the currents and

17:10.500 --> 17:14.160
the trends and it is actually very, very encouraging to me.

17:14.160 --> 17:17.020
So I've got it all in chronological order.

17:17.020 --> 17:23.020
So first was the Future of Life Institute published their open letter, you know, signed

17:23.020 --> 17:26.980
by everyone, including Elon Musk and yada, yada, yada.

17:26.980 --> 17:30.820
What I didn't realize was that it actually came with a policy paper.

17:30.820 --> 17:38.020
And so this policy paper, it's only 14 pages and it has seven policy recommendations.

17:38.020 --> 17:43.900
So this came out in, let's see, April 12th.

17:43.900 --> 17:50.420
I think I might have that wrong, the pause, the big pause.

17:50.420 --> 17:52.780
So the paper was published March 22nd.

17:52.780 --> 17:58.100
The policy recommendations came out a few days later, a couple weeks later.

17:58.100 --> 18:03.700
So what I did was I took all that and then I made a very brief summary of the whole thing

18:03.700 --> 18:04.700
right here.

18:04.700 --> 18:09.860
So you can click on the link and see it, but it's a few basic things, mandate, robust

18:09.860 --> 18:15.980
third party auditing, regulate organizations access to computational power, establish capable

18:15.980 --> 18:21.980
AI agencies at the national level, establish liability for AI caused harm, introduce measures

18:21.980 --> 18:27.860
to prevent and track AI model leaks, expand AI technical safety research, and develop

18:27.860 --> 18:30.660
standards for identifying and managing AI generated content.

18:30.660 --> 18:31.660
Okay, sure, whatever.

18:31.660 --> 18:35.140
It's all pretty boilerplate in the grand scheme of things.

18:35.140 --> 18:39.580
But so that came out March, April, March 29th.

18:39.580 --> 18:45.620
The UK did this, this jobby, which I haven't had a chance to summarize yet.

18:45.620 --> 18:53.420
But basically the idea is a pro innovation, AI regulatory framework, et cetera, et cetera.

18:53.420 --> 18:55.340
Again, you see how long this thing is.

18:55.340 --> 19:01.340
That's why it takes a little, takes a little bit of doing to summarize with a, with a GPT

19:01.340 --> 19:02.860
API calls.

19:02.860 --> 19:11.940
So anyways, they call for a regulatory sandbox, which so does the EA, the EU, good grief,

19:11.940 --> 19:16.140
EU AI Act also calls for regulatory sandboxes.

19:16.140 --> 19:20.700
So if you, if you don't know a regulatory sandbox is basically you create a safe space

19:20.700 --> 19:28.980
where you can experiment with AI, you know, that is a little bit more permissive.

19:28.980 --> 19:31.180
So and this is this is very often the case.

19:31.180 --> 19:36.100
So for instance, one of the most familiar ones is if you're doing medical research,

19:36.100 --> 19:41.580
for instance, you're allowed to use substances that are otherwise illegal.

19:41.580 --> 19:44.340
You just have to go through approval processes.

19:44.340 --> 19:45.340
It's not quite the same.

19:45.340 --> 19:51.140
But the idea is that like people have been able to experiment with THC and LSD and psilocybin,

19:51.140 --> 19:55.940
even though it's still a schedule to drug or whatever in the United States.

19:55.940 --> 19:59.060
You just have to, you have to be approved to do so.

19:59.060 --> 20:04.980
Likewise, if you are an approved entity, the idea of a regulatory sandbox is that you can

20:04.980 --> 20:08.580
still do whatever science you need to do as long as you do so safely.

20:08.580 --> 20:12.860
But also one thing about these pro innovation things is, and this is a common theme that

20:12.860 --> 20:16.180
I actually noticed, which is why I was inspired to do this.

20:16.180 --> 20:24.060
So the UK and the United States have all focused on protecting innovation and accelerating

20:24.060 --> 20:25.060
innovation.

20:26.060 --> 20:34.980
So then in early May, the UN Secretary General Antonio Guterres said that he's amenable to

20:34.980 --> 20:41.980
the idea of the IAEA for AI, which also open AI, I forgot to add that, but open AI basically

20:41.980 --> 20:45.100
published a blog calling for the same thing.

20:45.100 --> 20:49.780
Then a few days later, the United States Senate hearing on AI.

20:49.780 --> 20:56.180
This was the one with Sam Ullman and Christina Montgomery and Gary Marcus.

20:56.180 --> 20:58.740
This was almost a three hour talk.

20:58.740 --> 21:02.220
And I took the transcript of that and I summarized it here.

21:02.220 --> 21:08.140
So the high level summary basically just says, here's the key points that they discussed.

21:08.140 --> 21:11.260
And then I break each one of those down further.

21:11.260 --> 21:16.260
So it takes you 20 minutes to read this instead of three hours.

21:16.260 --> 21:20.260
It's pretty straightforward.

21:20.260 --> 21:21.940
There was a lot of back and forth, a lot of questions.

21:21.940 --> 21:24.780
I watched pretty much the entire hearing.

21:24.780 --> 21:28.540
But clearly, the United States government was listening.

21:28.540 --> 21:35.500
And I wonder if this whole thing was just an orchestrated series of events or not.

21:35.500 --> 21:40.100
But anyways, a month later, the EU AI Act was proposed.

21:40.100 --> 21:42.860
I don't think it's been adopted or ratified or anything.

21:42.860 --> 21:47.620
I want to explain in the comments that there's still quite a process to go through.

21:47.620 --> 21:49.820
They've got to get feedback.

21:49.820 --> 21:56.140
But then more recently, in just the last couple of days, the United States House of Representatives

21:56.140 --> 22:04.460
by Representative Ted Liu and a few others introduced a bipartisan commission.

22:04.460 --> 22:05.460
Basically it's an AI commission.

22:05.460 --> 22:06.460
I summarized it here.

22:06.460 --> 22:08.860
I didn't summarize it here.

22:08.860 --> 22:13.060
I copied the text here because their PDF was garbage.

22:13.060 --> 22:15.740
Seriously, this is the United States.

22:15.740 --> 22:18.340
You can pay someone who knows how to make a PDF.

22:18.340 --> 22:19.340
Good Lord.

22:19.340 --> 22:22.980
So anyways, it's relatively straightforward.

22:22.980 --> 22:28.540
Mostly this is just saying let's appoint a panel to come up with policy recommendations.

22:28.540 --> 22:31.740
It's not really, they're not going to do anything.

22:31.740 --> 22:35.100
Basically it's going to produce three reports.

22:35.100 --> 22:41.540
And so this is going to recommend what the United States Congress does for AI.

22:41.540 --> 22:43.620
Okay, great.

22:43.620 --> 22:48.380
Congressional commission, they're interested in getting more information, which means that

22:48.380 --> 22:49.780
they're going to solicit experts.

22:49.780 --> 22:57.140
So one thing that I thought was most interesting was the panel is that they want to have members

22:57.140 --> 23:01.700
of the commission shall have a demonstrated background in at least one of the following.

23:01.700 --> 23:05.020
Computer science or AI specifically.

23:05.020 --> 23:11.340
Social society including constitutional rights, civil liberties, ethics and the creative community.

23:11.340 --> 23:14.820
Industry and workforce and then government including national security.

23:14.820 --> 23:19.320
So when you get these kinds of people in a room together, it's not just engineers and

23:19.320 --> 23:21.020
not just data scientists.

23:21.020 --> 23:26.940
This is going to be people that are in political science, civil rights, civil liberties, industry

23:26.940 --> 23:30.460
insiders and then finally national security experts.

23:30.460 --> 23:34.060
So this is going to be a pretty comprehensive set of recommendations and I'm actually pretty

23:34.060 --> 23:37.740
happy to see that Ted Lu is leading that.

23:37.740 --> 23:41.980
And Ted Lu is, or is he the Los Angeles County?

23:41.980 --> 23:47.420
So it's not surprising that California Bro is going to be figuring that out.

23:47.420 --> 23:53.300
And then finally most recently was just a couple days ago, Senator Chuck Schumer announced

23:53.300 --> 23:59.740
the safe framework at the CSIS, which is really interesting.

23:59.740 --> 24:02.220
And I don't know if this has been ratified yet or anything.

24:02.220 --> 24:08.380
I haven't been able to find the actual text of the idea, but there is a one pager out

24:08.380 --> 24:12.540
there somewhere that summarizes it very succinctly.

24:12.540 --> 24:17.540
But I took the transcript from this and I summarized the talk here.

24:17.540 --> 24:27.300
So basically it comes down to four major components, security, which basically talks about national

24:27.300 --> 24:35.700
security above all else, but also corporate security and privacy of citizens, accountability,

24:35.700 --> 24:41.580
which talks about how do you, how do you, it's actually pretty similar to the EU thing

24:41.580 --> 24:44.980
where, oh, I forgot to mention this for the EU AI Act.

24:44.980 --> 24:50.980
The primary thing that the EU AI Act does is it bans outright social credit systems and

24:50.980 --> 24:57.820
surveillance like high, basically big brother bans big brother stuff.

24:57.820 --> 25:01.020
And so this is what the security and accountability does.

25:01.020 --> 25:05.600
An interesting part of this was the foundations aspect of the framework.

25:05.600 --> 25:12.280
So basically one of the key things of Chuck Schumer's framework is to protect the foundations

25:12.280 --> 25:13.900
of democracy.

25:13.900 --> 25:18.740
So I was talking with my wife about this and she suspects that this is a direct reaction

25:18.740 --> 25:25.140
to the January 6th attempted insurrection at the US Capitol when people are breaking

25:25.140 --> 25:30.860
into and keep in mind that many members of Congress were directly in danger.

25:30.860 --> 25:37.700
And so we suspect, we being my wife and I, we suspect that this is actually basically

25:37.700 --> 25:45.060
the Congress, House of Representatives and senators, they didn't take social media seriously

25:45.580 --> 25:49.420
and then, you know, Facebook happened with Cambridge Analytica.

25:49.420 --> 25:55.460
I don't think it's controversial to say that, that Facebook and other social media companies

25:55.460 --> 26:02.140
directly contributed to the widespread abuses of misinformation, but also just coordination

26:02.140 --> 26:03.180
of violence.

26:03.180 --> 26:04.620
It's that simple.

26:04.620 --> 26:09.300
And so they, they learned their lesson by not taking social media seriously and now they're

26:09.300 --> 26:12.020
taking artificial intelligence very seriously.

26:12.020 --> 26:17.300
So on a cynical note, this is basically the establishment wanting to protect the establishment

26:17.300 --> 26:19.820
and the status quo.

26:19.820 --> 26:21.500
That is a pretty cynical take.

26:21.500 --> 26:24.700
That doesn't mean that it's the only thing because I actually listened to all of Chuck

26:24.700 --> 26:28.860
Schumer's talk and he had a very clear, like words matter.

26:28.860 --> 26:31.260
He had a very clear-eyed understanding of what's coming.

26:31.260 --> 26:35.940
He even talked about the possibility of jobs dislocation.

26:35.940 --> 26:41.020
He likened it to globalization and offshoring because he said like, yes, globalization and

26:41.020 --> 26:47.340
offshoring did actually help the economy because it, you know, allowed us to lower the prices

26:47.340 --> 26:52.260
of goods and services, but at the same time, millions of Americans lost their job.

26:52.260 --> 26:56.180
And so the implication there was that the United States government did not do a good

26:56.180 --> 27:04.540
enough job to protect Americans while we were rabidly offshoring in the 90s and 2000s.

27:04.540 --> 27:06.700
And then finally, explainability.

27:06.700 --> 27:10.900
Some people commented that Chuck Schumer doesn't really seem to understand how AI works because

27:11.780 --> 27:16.060
it's not a database that you can like say, oh, this is where it got the data from.

27:16.060 --> 27:21.500
That being said, I think that these commissions will figure out, you know, that while you

27:21.500 --> 27:27.660
can't look at the model and infer what was in it, you can look at the training data.

27:27.660 --> 27:32.580
So what I suspect is that there's going to be very soon open source standards on training

27:32.580 --> 27:33.580
data.

27:33.580 --> 27:37.860
So basically, in order to be a licensed and approved and publicly available model, the

27:37.860 --> 27:42.980
underpinning training data will have to be publicly available, or at least inspected.

27:42.980 --> 27:46.220
He did talk about innovation first as well.

27:46.220 --> 27:50.300
So this is a common theme that's emerging at least in Britain and America.

27:50.300 --> 27:53.140
The EU is less concerned about innovation.

27:53.140 --> 27:58.700
They're more concerned about fundamental civil rights and foundational human rights,

27:58.700 --> 27:59.700
which is good.

27:59.700 --> 28:02.260
Like I would I would actually like that.

28:02.260 --> 28:10.020
But as an individual nation, they are highly, highly focused on focusing on innovation first

28:10.020 --> 28:12.660
and then safety rights, accountability and stuff.

28:12.660 --> 28:18.180
So it's basically here's the roadmap of how to innovate safely and fat and quickly.

28:18.180 --> 28:22.380
And the idea is there are geopolitical competitions happening.

28:22.380 --> 28:27.540
Vladimir Putin said what I think it was 2021, the nation that solves artificial intelligence

28:27.540 --> 28:31.780
will own, you know, the next century, and it's probably going to be a lot longer than

28:31.780 --> 28:32.780
that.

28:32.780 --> 28:37.820
Russia, unfortunately, for them does not have the economy or the they have brain drain,

28:37.820 --> 28:41.460
so they're not going to be a participant in artificial intelligence.

28:41.460 --> 28:46.780
It's basically going to come down to America versus China versus the EU.

28:46.780 --> 28:52.300
But the EU is more ideologically aligned with America and vice versa.

28:52.300 --> 28:57.420
So it'll basically come down to East versus West, which is basically, you know, the same

28:57.420 --> 29:03.220
idea of World War Two and the Cold War, which was Western ideology versus Eastern ideology.

29:03.220 --> 29:08.060
So this is what the geopolitical conflict is shaping up to be for the next century.

29:08.060 --> 29:10.300
Yay, repeat of the 20th century.

29:10.300 --> 29:12.420
Let's hope that it's not as bloody.

29:12.420 --> 29:18.300
And I say that flippantly, but I am dead serious because the stakes are very, very high here,

29:18.300 --> 29:23.260
which is why I call this the Gaia Initiative, because Gaia is Greek for Earth or Mother Earth.

29:23.260 --> 29:25.500
And also Gaia was the goddess of monsters, too.

29:25.500 --> 29:30.300
So on the topic of Malik and Shogoth and all those other things, I think that Gaia is a

29:30.300 --> 29:32.020
really appropriate name.

29:32.020 --> 29:33.580
So anyways, this is out here.

29:33.580 --> 29:39.260
It's just under github.com slash daveshop slash Gaia initiative.

29:39.260 --> 29:42.500
I will update this as interesting news comes out.

29:42.500 --> 29:43.940
I might forget about it for a while.

29:43.940 --> 29:49.620
I tend to do that, but it's up there and I find all this news very encouraging.

29:49.620 --> 29:50.620
So thanks for watching.

29:50.620 --> 29:51.620
I hope you got a lot out of it.

29:51.620 --> 29:51.640
Cheers.

