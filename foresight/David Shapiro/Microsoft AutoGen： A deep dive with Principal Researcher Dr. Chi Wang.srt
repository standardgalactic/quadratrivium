1
00:00:00,000 --> 00:00:01,880
Chi, thanks for jumping on.

2
00:00:01,880 --> 00:00:03,680
It's a pleasure to meet you.

3
00:00:03,680 --> 00:00:05,400
I was really excited.

4
00:00:05,400 --> 00:00:07,720
Yeah, you're quite welcome.

5
00:00:07,720 --> 00:00:11,280
Obviously, Autogen is all the rage right now.

6
00:00:11,280 --> 00:00:12,280
It's very popular.

7
00:00:12,280 --> 00:00:14,920
There's lots and lots of videos being made about Autogen.

8
00:00:14,920 --> 00:00:16,640
But before we dive into that,

9
00:00:16,640 --> 00:00:18,800
I was wondering if you could just tell me a little bit more

10
00:00:18,800 --> 00:00:21,760
about your time at Microsoft as a principal researcher,

11
00:00:21,760 --> 00:00:23,440
like how did you get into that position?

12
00:00:23,440 --> 00:00:24,680
What's it been like?

13
00:00:24,680 --> 00:00:27,120
Yes, just give me the story.

14
00:00:27,120 --> 00:00:29,760
Thank you. Yeah, my name is Chi.

15
00:00:30,720 --> 00:00:33,680
Principal researcher at Microsoft Research.

16
00:00:33,680 --> 00:00:35,560
I joined long time ago.

17
00:00:35,560 --> 00:00:39,120
I joined about nine years ago in Microsoft.

18
00:00:39,120 --> 00:00:41,840
And I've been working on many different projects.

19
00:00:41,840 --> 00:00:44,120
Apparently now I'm focusing on Autogen.

20
00:00:44,120 --> 00:00:47,640
And before that, I worked on automated machine learning,

21
00:00:47,640 --> 00:00:50,120
machine learning for systems,

22
00:00:50,120 --> 00:00:53,200
data science, data analytics, data mining.

23
00:00:53,200 --> 00:00:56,560
So quite a lot of different things.

24
00:00:56,560 --> 00:00:59,640
And some of the work is more like on a theoretical set

25
00:01:00,000 --> 00:01:03,160
side and some of them is more to the system side.

26
00:01:04,320 --> 00:01:09,320
And yeah, I've been focusing on Autogen recently.

27
00:01:09,560 --> 00:01:12,160
So very happy to be here.

28
00:01:12,160 --> 00:01:13,000
Excellent.

29
00:01:13,000 --> 00:01:16,080
Yeah, with all the progress that's been made

30
00:01:16,080 --> 00:01:18,880
on large language models and working on assistance

31
00:01:18,880 --> 00:01:22,440
and agents, and then of course, working on agents,

32
00:01:22,440 --> 00:01:24,080
working with other agents,

33
00:01:24,080 --> 00:01:25,880
let me ask kind of a big question.

34
00:01:25,880 --> 00:01:29,080
Like what was the genesis behind Autogen?

35
00:01:30,000 --> 00:01:32,360
How was that project proposed?

36
00:01:32,360 --> 00:01:34,680
Like how did it come together?

37
00:01:34,680 --> 00:01:36,560
Like what was the theoretical work behind it?

38
00:01:36,560 --> 00:01:41,120
And how did you get from zero to where you are today?

39
00:01:41,120 --> 00:01:44,640
Yeah, so that's a very interesting question.

40
00:01:44,640 --> 00:01:46,520
And to fully answer that question,

41
00:01:46,520 --> 00:01:48,520
I need to tell a long story.

42
00:01:49,440 --> 00:01:50,920
But let me first tell you a short one.

43
00:01:50,920 --> 00:01:55,920
Short answer is like with this big kind of opportunities

44
00:01:55,960 --> 00:01:59,360
with larger models, so powerful techniques.

45
00:01:59,360 --> 00:02:01,200
At MSR, we want to ask the question,

46
00:02:01,200 --> 00:02:03,160
like what is the future, right?

47
00:02:03,160 --> 00:02:07,160
We want to be forward-looking, we want to be futuristic,

48
00:02:07,160 --> 00:02:09,240
kind of take the solid leadership.

49
00:02:09,240 --> 00:02:11,760
And because one of the famous quotes

50
00:02:11,760 --> 00:02:14,520
from the founder of Macro Research,

51
00:02:14,520 --> 00:02:17,680
we can say that the best science

52
00:02:17,680 --> 00:02:20,960
will be indistinguishable from magic.

53
00:02:20,960 --> 00:02:24,920
So that's the level of ambition we have.

54
00:02:24,920 --> 00:02:26,200
So we asked the question,

55
00:02:26,200 --> 00:02:29,640
what is the future of AI applications?

56
00:02:29,640 --> 00:02:32,280
And how do we empower every developer to build them?

57
00:02:33,360 --> 00:02:36,040
Yeah, so that's a fundamental driving question.

58
00:02:36,040 --> 00:02:40,200
And now a longer version of the answer is that,

59
00:02:40,200 --> 00:02:42,560
I started from, I thought as a tutorial,

60
00:02:42,560 --> 00:02:43,600
before I worked on Autogen,

61
00:02:43,600 --> 00:02:46,240
I worked on automated machine learning,

62
00:02:46,240 --> 00:02:48,720
which is another open source project called FLAML.

63
00:02:49,840 --> 00:02:54,360
FLAML is a solution for automating model selection,

64
00:02:54,360 --> 00:02:55,880
hyperparameter tuning, essentially,

65
00:02:55,880 --> 00:03:00,240
black box optimization to navigate a larger space

66
00:03:00,240 --> 00:03:01,720
without knowing the gradient.

67
00:03:02,960 --> 00:03:05,800
So it's a very powerful technique,

68
00:03:05,800 --> 00:03:10,800
but that was started before the larger model takes the storm.

69
00:03:11,720 --> 00:03:14,520
And when chatGVT released, right?

70
00:03:14,520 --> 00:03:19,520
And that's a big, big kind of upgrade of the model capabilities.

71
00:03:19,520 --> 00:03:24,520
And so I started working on a similar problem

72
00:03:24,680 --> 00:03:26,000
as automated machine learning.

73
00:03:26,000 --> 00:03:28,400
I started working on inference hyperparameter tuning

74
00:03:28,400 --> 00:03:30,600
for these chatGVT models.

75
00:03:30,600 --> 00:03:33,000
For example, how do you select the best model?

76
00:03:33,000 --> 00:03:36,120
How do you select the right prompt temperature

77
00:03:36,120 --> 00:03:37,960
and all the other inference parameters

78
00:03:37,960 --> 00:03:41,440
so that you can maximize the utility from the models

79
00:03:41,440 --> 00:03:43,040
while minimizing your cost?

80
00:03:44,080 --> 00:03:48,560
So that's the initial work on this direction.

81
00:03:48,640 --> 00:03:51,520
And when GT4 is released,

82
00:03:51,520 --> 00:03:55,360
that's another big upgrade of the level capabilities.

83
00:03:55,360 --> 00:03:59,000
So then I started really to ask the question,

84
00:03:59,000 --> 00:04:04,000
okay, if we kind of want to bring the best power

85
00:04:04,680 --> 00:04:07,480
of the model and really solve really difficult problems,

86
00:04:07,480 --> 00:04:09,720
what should be the right way to do it?

87
00:04:09,720 --> 00:04:14,200
And agent is apparently a very powerful notion.

88
00:04:14,200 --> 00:04:17,240
It's another kind of level of the automation

89
00:04:17,240 --> 00:04:19,440
as opposed to the previous automation

90
00:04:19,440 --> 00:04:22,680
in the 20 machine learning work I did.

91
00:04:22,680 --> 00:04:25,000
So yeah, so that's where they started.

92
00:04:26,040 --> 00:04:30,440
And of course, it's a whole new area.

93
00:04:30,440 --> 00:04:31,720
No, the shop agent is not new.

94
00:04:31,720 --> 00:04:35,560
It's has been there for a long time.

95
00:04:35,560 --> 00:04:37,240
And I remember back in college,

96
00:04:37,240 --> 00:04:39,680
I worked on some like game competition,

97
00:04:39,680 --> 00:04:43,080
like building agents that can play games

98
00:04:43,080 --> 00:04:44,840
with each other and compete.

99
00:04:44,960 --> 00:04:48,240
At that time, we were using many of the rule-based methods

100
00:04:48,240 --> 00:04:51,720
where a lot of deal with a lot of corner cases and so on.

101
00:04:51,720 --> 00:04:53,040
So it's not viable.

102
00:04:53,040 --> 00:04:55,640
It's a good notion, but not viable.

103
00:04:55,640 --> 00:04:58,800
And these larger models,

104
00:04:58,800 --> 00:05:00,960
especially the chat-operated models,

105
00:05:00,960 --> 00:05:05,760
really make it viable and a reality

106
00:05:05,760 --> 00:05:09,040
that we can build new software based on.

107
00:05:09,040 --> 00:05:11,240
And to study this new area,

108
00:05:11,240 --> 00:05:15,960
we kind of need to think many things from scratch

109
00:05:15,960 --> 00:05:20,400
and try to take some of the first principles

110
00:05:20,400 --> 00:05:21,880
and what is the right approach.

111
00:05:21,880 --> 00:05:25,880
And basically leverage every lesson

112
00:05:25,880 --> 00:05:29,240
we learned from the previous projects, previous experience,

113
00:05:29,240 --> 00:05:34,240
but try to build this one multi-agent framework

114
00:05:36,040 --> 00:05:38,720
that is really generic,

115
00:05:38,720 --> 00:05:40,720
and can support diverse applications.

116
00:05:41,720 --> 00:05:44,680
Yeah, so there are many different examples

117
00:05:44,680 --> 00:05:48,960
of sources of inspirations I can give you,

118
00:05:48,960 --> 00:05:52,800
but why not show it's like using everything I learned

119
00:05:52,800 --> 00:05:57,560
and also take every feedback I received from everyone

120
00:05:57,560 --> 00:05:59,280
and iterate on that.

121
00:05:59,280 --> 00:06:01,960
And so that's how we come here today.

122
00:06:01,960 --> 00:06:02,880
Excellent.

123
00:06:02,880 --> 00:06:05,440
Yeah, no, I mean, there's a lot to unpack there.

124
00:06:05,440 --> 00:06:08,320
It's fascinating to me that it started

125
00:06:08,320 --> 00:06:11,120
as sort of like auto ML,

126
00:06:11,120 --> 00:06:14,200
like automating the optimization.

127
00:06:14,200 --> 00:06:18,040
And I can see how going to like optimizing prompts

128
00:06:18,040 --> 00:06:20,960
and optimizing hyperparameters and parameter tuning

129
00:06:20,960 --> 00:06:23,640
could then lead to the agents, especially, like you said,

130
00:06:23,640 --> 00:06:26,000
if the idea is thinking to the future,

131
00:06:26,000 --> 00:06:27,960
like what is the sci-fi version

132
00:06:27,960 --> 00:06:31,560
of enabling application development in the future?

133
00:06:31,560 --> 00:06:35,800
So I wanted to follow up with a kind of a two-part question.

134
00:06:35,800 --> 00:06:39,920
So in the most basic level, what is AutoGen?

135
00:06:39,920 --> 00:06:42,080
But more specifically, what is the vision?

136
00:06:42,080 --> 00:06:44,720
Like what is it that you're trying to solve

137
00:06:44,720 --> 00:06:46,440
with AutoGen right now?

138
00:06:46,440 --> 00:06:50,080
Yeah, so yeah, in one word,

139
00:06:50,080 --> 00:06:54,600
AutoGen is the multi-agent AI framework,

140
00:06:54,600 --> 00:06:57,840
and especially focusing on multi-agent conversations

141
00:06:57,840 --> 00:07:01,520
so that we can connect large-level models, tools,

142
00:07:01,520 --> 00:07:06,520
and human inputs together to solve complex asks.

143
00:07:06,600 --> 00:07:09,480
There can be multiple ways to understand this.

144
00:07:09,480 --> 00:07:14,120
So in one way is to understand it as a programming framework

145
00:07:15,000 --> 00:07:17,760
for developers to build applications easily

146
00:07:17,760 --> 00:07:20,480
with some simple and unified abstraction

147
00:07:20,480 --> 00:07:22,280
so that they don't need to worry too much

148
00:07:22,280 --> 00:07:23,480
about the lower details,

149
00:07:23,480 --> 00:07:26,600
but can focus on how to define agents,

150
00:07:26,600 --> 00:07:30,160
how to get them to work with each other,

151
00:07:30,160 --> 00:07:33,080
and eventually reach the goal.

152
00:07:33,080 --> 00:07:36,000
It can also be understood as a tool

153
00:07:36,000 --> 00:07:39,000
to kind of scale up,

154
00:07:39,000 --> 00:07:41,880
scale up the power of our models

155
00:07:41,880 --> 00:07:44,440
and makes them even more useful

156
00:07:44,440 --> 00:07:46,680
by connecting with other tools,

157
00:07:46,680 --> 00:07:51,680
non-narrative model tools, or human collaborators,

158
00:07:52,640 --> 00:07:56,680
and kind of scale up both the complexity of the problem

159
00:07:56,680 --> 00:08:00,760
they can solve, the degree of automation, to some extent.

160
00:08:00,760 --> 00:08:04,080
Yeah, this is kind of a relatively abstract instruction,

161
00:08:04,080 --> 00:08:06,920
but if we think about it,

162
00:08:06,920 --> 00:08:10,480
about how people use it, it's quite simple.

163
00:08:10,480 --> 00:08:13,040
So when developers build applications with AutoGen,

164
00:08:13,040 --> 00:08:15,240
it basically boils down to two steps.

165
00:08:15,240 --> 00:08:17,080
Step one is to define agents,

166
00:08:17,080 --> 00:08:19,120
and step two is to get them to talk.

167
00:08:19,120 --> 00:08:21,280
So as simple as that.

168
00:08:21,840 --> 00:08:26,840
Yeah, so we try to make it very useful and generic,

169
00:08:26,840 --> 00:08:27,800
but on the other hand,

170
00:08:27,800 --> 00:08:32,160
we want to have a very simple interface for people to use.

171
00:08:32,160 --> 00:08:35,240
Yeah, I mean, that's exactly the vision

172
00:08:35,240 --> 00:08:37,200
that I kind of settled on

173
00:08:37,200 --> 00:08:40,920
for my hierarchical autonomous agent swarm idea,

174
00:08:40,920 --> 00:08:44,640
but I don't want to make it about that.

175
00:08:44,640 --> 00:08:46,120
Right now, it's just fascinating

176
00:08:46,120 --> 00:08:49,600
that we kind of converge on a very similar principle,

177
00:08:49,600 --> 00:08:53,040
like let's make the deployment of software

178
00:08:53,040 --> 00:08:54,840
as easy as possible.

179
00:08:54,840 --> 00:08:56,720
And so there's two things that you mentioned,

180
00:08:56,720 --> 00:08:58,280
like layers of abstraction,

181
00:08:58,280 --> 00:09:00,440
because I think that's a really good intuitive way

182
00:09:00,440 --> 00:09:01,280
of thinking about it,

183
00:09:01,280 --> 00:09:05,040
like in the same way that a Python interpreter

184
00:09:05,040 --> 00:09:08,160
was a layer of abstraction from compiled code,

185
00:09:08,160 --> 00:09:10,320
and then maybe language models

186
00:09:10,320 --> 00:09:11,760
are another layer of abstraction

187
00:09:11,760 --> 00:09:14,000
where it's natural language interface.

188
00:09:14,000 --> 00:09:16,280
This could be seen as, again,

189
00:09:16,280 --> 00:09:17,960
another layer of abstraction,

190
00:09:17,960 --> 00:09:19,840
where instead of looking at interacting

191
00:09:19,840 --> 00:09:21,400
with the language model directly,

192
00:09:21,400 --> 00:09:23,720
it is now a type of interpreter,

193
00:09:23,720 --> 00:09:25,520
but this is the agents

194
00:09:25,520 --> 00:09:29,160
and the multi-agent framework on top of it.

195
00:09:29,160 --> 00:09:30,480
So that's my intuition.

196
00:09:30,480 --> 00:09:32,760
Do you agree with that or disagree?

197
00:09:32,760 --> 00:09:36,160
Or like, how do you think of those layers of abstraction?

198
00:09:36,160 --> 00:09:38,960
Yeah, that's a fantastic question.

199
00:09:38,960 --> 00:09:41,760
The abstraction is indeed at a higher level

200
00:09:41,760 --> 00:09:43,600
of the agent abstraction.

201
00:09:44,600 --> 00:09:48,040
It unifies a number of different things.

202
00:09:48,040 --> 00:09:50,200
One is larger models.

203
00:09:50,200 --> 00:09:53,040
So when we use a single instance of a larger model,

204
00:09:53,040 --> 00:09:55,200
we usually do prompt engineering

205
00:09:55,200 --> 00:09:57,960
and try to give some input text

206
00:09:57,960 --> 00:09:59,880
and get some output text out of it.

207
00:10:00,920 --> 00:10:05,080
This agent abstraction can encapsulate that underneath

208
00:10:05,080 --> 00:10:07,760
and provide a more intuitive way

209
00:10:07,760 --> 00:10:12,160
to think of it as an agent that can converse with you.

210
00:10:12,160 --> 00:10:17,160
So not just as one single text completion inference anymore.

211
00:10:17,640 --> 00:10:21,400
It can do tasks, can persist some states

212
00:10:21,400 --> 00:10:24,240
and continue to take your feedback

213
00:10:24,240 --> 00:10:28,480
and produce more refined result and so on.

214
00:10:28,480 --> 00:10:32,320
So that's a larger model-based agent.

215
00:10:33,400 --> 00:10:37,040
There are two other kinds of backhands

216
00:10:37,040 --> 00:10:38,120
that can be encapsulated.

217
00:10:38,120 --> 00:10:42,200
One is, you can think of it as programming language

218
00:10:42,200 --> 00:10:46,600
or tool-based agent, which doesn't use a larger model,

219
00:10:46,600 --> 00:10:50,320
but they can still perform very useful actions.

220
00:10:50,320 --> 00:10:52,440
They can do code execution, for example,

221
00:10:52,440 --> 00:10:55,040
or it can execute predefined functions

222
00:10:55,040 --> 00:10:58,880
or it can basically execute any programming logic

223
00:10:58,880 --> 00:11:02,280
you've defined there.

224
00:11:02,280 --> 00:11:06,880
And third one is the human kind of backed agents.

225
00:11:06,880 --> 00:11:10,840
So these agents can be considered as some kind of user proxy.

226
00:11:12,120 --> 00:11:16,720
So when they need human input, the human can take over

227
00:11:16,720 --> 00:11:21,720
and just participate the multi-agent workflow

228
00:11:21,760 --> 00:11:23,360
as one of the agents.

229
00:11:23,360 --> 00:11:25,520
So you can see about several agents.

230
00:11:25,520 --> 00:11:28,520
Some of them are larger model-based, some are tool-based,

231
00:11:28,520 --> 00:11:31,880
others are human-based.

232
00:11:31,880 --> 00:11:35,400
So then they can just cooperate together

233
00:11:35,440 --> 00:11:36,880
through a very natural interface,

234
00:11:36,880 --> 00:11:40,720
which is a conversational interface.

235
00:11:40,720 --> 00:11:44,240
So that's kind of layer of abstraction we provide.

236
00:11:45,360 --> 00:11:48,040
Yeah, I appreciate that.

237
00:11:48,040 --> 00:11:49,560
And I'm reminded of during,

238
00:11:49,560 --> 00:11:52,480
I think it was the Ignite keynote speech Satya Nadella said,

239
00:11:52,480 --> 00:11:53,920
think of it as a reasoning engine

240
00:11:53,920 --> 00:11:55,680
and a natural language interface.

241
00:11:55,680 --> 00:11:57,320
And that was like the two simplest ways

242
00:11:57,320 --> 00:12:01,160
to think of generative AI, at least the language side.

243
00:12:02,160 --> 00:12:05,760
So the other topic that I wanted to ask about

244
00:12:05,760 --> 00:12:08,000
to kind of dig into was thinking about it

245
00:12:08,000 --> 00:12:10,200
as a kind of automation.

246
00:12:10,200 --> 00:12:13,920
So there's the agent-based, there's the tools-based,

247
00:12:13,920 --> 00:12:15,960
but then overall kind of,

248
00:12:15,960 --> 00:12:18,840
and this is a messaging kind of a framing

249
00:12:18,840 --> 00:12:21,640
that I've adopted when talking to people is,

250
00:12:21,640 --> 00:12:23,560
and of course it's an oversimplification,

251
00:12:23,560 --> 00:12:28,280
but AI is, one, AI is not new,

252
00:12:28,280 --> 00:12:29,960
like machine learning has been around for a while,

253
00:12:29,960 --> 00:12:32,600
this is just a step function in terms of capabilities

254
00:12:32,600 --> 00:12:35,040
that models have.

255
00:12:35,040 --> 00:12:38,080
But it's also just the simplest way to think about it

256
00:12:38,080 --> 00:12:40,960
from a production standpoint or from a software standpoint,

257
00:12:40,960 --> 00:12:43,880
is it's a new suite of automation tools, right?

258
00:12:43,880 --> 00:12:45,120
That's kind of how I think of it.

259
00:12:45,120 --> 00:12:47,400
Is that a fair characterization

260
00:12:47,400 --> 00:12:49,360
or is there something that I'm missing from that?

261
00:12:49,360 --> 00:12:52,480
Cause I do feel like there might be something missing

262
00:12:52,480 --> 00:12:54,880
or something that that characterization

263
00:12:54,880 --> 00:12:56,200
doesn't fully convey,

264
00:12:56,200 --> 00:12:59,120
but it is still fundamentally new automation, right?

265
00:13:00,080 --> 00:13:04,160
So if I try to understand it from an automation point of view,

266
00:13:04,160 --> 00:13:09,160
I think agent is fundamentally a automation concept.

267
00:13:10,880 --> 00:13:14,040
The automation is more like about,

268
00:13:14,040 --> 00:13:18,360
instead of giving every detailed instructions using code,

269
00:13:18,360 --> 00:13:21,320
I say step one, do this, step two, do that,

270
00:13:21,320 --> 00:13:26,320
using from precisely defined program language specification.

271
00:13:26,320 --> 00:13:31,320
And now we can make some more vague specification

272
00:13:32,520 --> 00:13:35,240
using like natural language specification,

273
00:13:35,240 --> 00:13:37,880
say I want to accomplish such a task

274
00:13:37,880 --> 00:13:40,240
and could the agent do it for me?

275
00:13:40,240 --> 00:13:45,240
So, and then underneath the agent needs to kind of break it down

276
00:13:45,240 --> 00:13:50,240
a big complex task into maybe smaller, solvable tasks.

277
00:13:50,760 --> 00:13:55,560
And until each task can be conveniently solved

278
00:13:55,560 --> 00:14:00,560
by a simple inference and produce a corresponding code

279
00:14:01,640 --> 00:14:02,720
to solve that task.

280
00:14:02,720 --> 00:14:06,440
And then eventually we need to recompose them,

281
00:14:06,440 --> 00:14:08,000
all of these intermediate steps

282
00:14:08,000 --> 00:14:10,480
and get to the final output back.

283
00:14:10,480 --> 00:14:13,240
So that is one part of the automation story.

284
00:14:14,240 --> 00:14:17,000
And another part of it is,

285
00:14:17,000 --> 00:14:19,400
think about this automating some tasks

286
00:14:19,400 --> 00:14:21,880
that human had to do.

287
00:14:22,080 --> 00:14:27,080
AutoGen really started with some very simple kind of automation.

288
00:14:28,560 --> 00:14:30,840
Just think about how you use chatGBT.

289
00:14:32,000 --> 00:14:35,600
You as a human need to ask questions

290
00:14:35,600 --> 00:14:37,880
and chatGBT gave you some answer.

291
00:14:37,880 --> 00:14:39,680
Sometimes it gave you the code

292
00:14:39,680 --> 00:14:42,320
and then human need to take that code

293
00:14:42,320 --> 00:14:46,960
and run it by yourself and get some result.

294
00:14:46,960 --> 00:14:49,080
If it's not correct, you send it back

295
00:14:49,080 --> 00:14:52,920
and chatGBT gave you some results for the game.

296
00:14:52,920 --> 00:14:55,160
And here in this kind of interaction,

297
00:14:55,160 --> 00:14:57,240
human students do a lot of work,

298
00:14:57,240 --> 00:15:02,240
but many of the work can be automated if we use agent.

299
00:15:03,120 --> 00:15:06,760
And even some kind of human feedback,

300
00:15:06,760 --> 00:15:11,760
like non-code, if I don't like the results,

301
00:15:12,960 --> 00:15:16,040
but I know my preferences, I will tell it,

302
00:15:16,040 --> 00:15:21,040
such as change the chart from using a dollar to percentage

303
00:15:21,400 --> 00:15:23,240
that kind of requirement,

304
00:15:23,240 --> 00:15:27,400
or teach me this lesson,

305
00:15:27,400 --> 00:15:28,800
teach me about math,

306
00:15:28,800 --> 00:15:31,720
but using a concrete money example, right?

307
00:15:31,720 --> 00:15:33,640
So those kind of requirements

308
00:15:33,640 --> 00:15:37,400
can be somewhat automated using all different ways.

309
00:15:37,400 --> 00:15:39,880
Sometimes we can use larger models.

310
00:15:40,680 --> 00:15:45,000
Sometimes we can use some retrieval augmented approach

311
00:15:45,000 --> 00:15:50,000
to inform retrieval to get the knowledge from somewhere.

312
00:15:51,120 --> 00:15:55,480
That's another kind of interesting automation

313
00:15:55,480 --> 00:15:57,160
that we can make.

314
00:15:57,160 --> 00:16:00,400
Yeah, and within that, those automation stories,

315
00:16:00,400 --> 00:16:04,000
because some of the examples,

316
00:16:04,000 --> 00:16:07,680
what was it, how did you say like a vague specification,

317
00:16:07,680 --> 00:16:09,200
right, a natural language specification

318
00:16:09,200 --> 00:16:12,320
that is not quite as rigorous as software development

319
00:16:12,320 --> 00:16:14,120
might have required in the past?

320
00:16:14,120 --> 00:16:16,040
And then of course, with these models,

321
00:16:16,040 --> 00:16:19,360
they have the ability to kind of think through it,

322
00:16:19,360 --> 00:16:21,920
or break it down into steps.

323
00:16:21,920 --> 00:16:25,880
So with all that, and some of the work that I have found is,

324
00:16:25,880 --> 00:16:28,120
or some of the problems that I've confronted,

325
00:16:28,120 --> 00:16:29,960
because it can think in general principles,

326
00:16:29,960 --> 00:16:31,960
it does know a lot about software development,

327
00:16:31,960 --> 00:16:34,040
and I mean, the language models know a lot

328
00:16:34,040 --> 00:16:35,200
about a lot of things.

329
00:16:35,200 --> 00:16:38,480
So with respect to autogen,

330
00:16:38,480 --> 00:16:41,640
and kind of getting to where you're at today,

331
00:16:41,640 --> 00:16:43,840
what are some of the biggest challenges

332
00:16:43,840 --> 00:16:46,000
that you've overcome so far, or that you haven't overcome?

333
00:16:46,000 --> 00:16:48,560
Maybe that would be a more interesting story.

334
00:16:48,560 --> 00:16:51,840
Yeah, sure, I could talk about both.

335
00:16:51,840 --> 00:16:54,560
For what we have overcome,

336
00:16:55,640 --> 00:17:00,640
I think we have kind of figured out the abstraction

337
00:17:00,720 --> 00:17:05,240
to the earlier about how to unify these different types

338
00:17:05,240 --> 00:17:10,240
of capabilities, different ways of making them work together.

339
00:17:11,080 --> 00:17:14,960
We'll have found one very simple interface

340
00:17:14,960 --> 00:17:19,120
that kind of accommodate a variety of different communication

341
00:17:19,120 --> 00:17:19,960
patterns.

342
00:17:19,960 --> 00:17:22,800
So one example is the, so there are several examples.

343
00:17:22,800 --> 00:17:25,880
One is the simple like one-to-one conversation.

344
00:17:27,080 --> 00:17:32,080
Another is hierarchical chat, like suppose one agent

345
00:17:32,080 --> 00:17:36,280
is more sit on top and talk to several sub-agents,

346
00:17:37,280 --> 00:17:41,240
it manages, and they can be nested structure,

347
00:17:41,240 --> 00:17:42,360
hierarchical structure.

348
00:17:43,360 --> 00:17:48,120
And another example is multiple one-on-one joint chat.

349
00:17:48,120 --> 00:17:52,880
So there's no one that is strictly sit on top.

350
00:17:52,880 --> 00:17:55,360
Everyone talks to the other else,

351
00:17:55,360 --> 00:17:59,440
but it's multiple one-on-one which are connected.

352
00:17:59,440 --> 00:18:01,320
So I talk to you, you talk to Katie,

353
00:18:01,320 --> 00:18:04,320
you can talk to me in this kind of triangle

354
00:18:04,360 --> 00:18:06,360
or multiple joint chat.

355
00:18:06,360 --> 00:18:11,280
And there's also a group chat, meaning it's not a one-on-one

356
00:18:11,280 --> 00:18:12,240
trend anymore.

357
00:18:12,240 --> 00:18:15,160
So everybody send messages to everyone else.

358
00:18:15,160 --> 00:18:19,560
So we see each other's message and there's a hidden

359
00:18:19,560 --> 00:18:22,280
group chat manager which does this kind of work.

360
00:18:22,280 --> 00:18:26,240
So architectural wise, it's still like one-on-one chat,

361
00:18:26,240 --> 00:18:31,240
but on the surface we can create an experience

362
00:18:32,120 --> 00:18:34,880
that simulates the group chat.

363
00:18:34,880 --> 00:18:37,400
And they can nest it the chat in some way,

364
00:18:37,400 --> 00:18:40,400
like for example, we can start with one-on-one conversation

365
00:18:40,400 --> 00:18:44,760
now, but at some time I decide to consult Katie.

366
00:18:44,760 --> 00:18:46,840
So I will hold on my current conversation

367
00:18:46,840 --> 00:18:48,960
and have some conversation with her.

368
00:18:48,960 --> 00:18:52,160
And then after I finish the conversation with her,

369
00:18:52,160 --> 00:18:54,600
I get back and continue the conversation with you.

370
00:18:54,600 --> 00:18:57,720
So that is a kind of nested chat.

371
00:18:57,720 --> 00:19:00,080
I believe that essentially you have the building blocks

372
00:19:00,080 --> 00:19:01,240
right now, right?

373
00:19:01,240 --> 00:19:03,440
These are some very common building blocks

374
00:19:03,440 --> 00:19:06,440
and we compose them together in different ways.

375
00:19:06,440 --> 00:19:10,120
We can build really complex workflows in general.

376
00:19:10,120 --> 00:19:14,640
So any arbitrarily complex communication patterns

377
00:19:14,640 --> 00:19:17,280
can be essentially built up within these building blocks.

378
00:19:17,280 --> 00:19:19,720
But I think that is what we have achieved.

379
00:19:19,720 --> 00:19:23,080
And we have also many examples for different applications

380
00:19:23,080 --> 00:19:27,600
of using these different types of patterns.

381
00:19:27,600 --> 00:19:29,680
This is another second thing we figured out.

382
00:19:29,680 --> 00:19:32,160
And the third thing I think is the ability

383
00:19:32,160 --> 00:19:38,600
to take human input and human control in a very natural way.

384
00:19:38,600 --> 00:19:41,640
And I thought earlier it's like every agent

385
00:19:41,640 --> 00:19:46,760
can be configured to enable the human input or disable that,

386
00:19:46,760 --> 00:19:49,360
depending on what you need.

387
00:19:49,360 --> 00:19:55,360
And also you can decide the type of environment from human.

388
00:19:55,360 --> 00:19:59,320
You can take over every time or you can only selectively

389
00:19:59,320 --> 00:20:01,800
chime in at a certain time.

390
00:20:01,800 --> 00:20:03,840
So that's a very useful feature because when

391
00:20:03,840 --> 00:20:07,480
you develop this automation, initially you

392
00:20:07,480 --> 00:20:10,680
don't know which step is easy to automate

393
00:20:10,680 --> 00:20:15,240
and which step is necessary for human to get in.

394
00:20:15,240 --> 00:20:19,760
So you can start from the more human loop way.

395
00:20:19,760 --> 00:20:21,960
And when you figure out that one step is

396
00:20:21,960 --> 00:20:25,720
you can confidently automate, then you can gradually reduce

397
00:20:25,720 --> 00:20:27,040
your human integration.

398
00:20:27,080 --> 00:20:32,800
So this convenience is very useful for doing all the experiments

399
00:20:32,800 --> 00:20:35,320
and figure out the right way.

400
00:20:35,320 --> 00:20:38,480
And make sure or still make sure human has a control

401
00:20:38,480 --> 00:20:40,120
when they need to.

402
00:20:40,120 --> 00:20:41,640
This is the third thing I think.

403
00:20:41,640 --> 00:20:45,840
So one more thing is the modularity and the reusability

404
00:20:45,840 --> 00:20:47,040
of the agents.

405
00:20:47,040 --> 00:20:51,880
That is a very important design part of it.

406
00:20:51,880 --> 00:20:55,600
So make sure that if you develop one useful agent

407
00:20:55,640 --> 00:20:58,600
in a different application, you could either directly reuse

408
00:20:58,600 --> 00:21:02,680
or start to modify it or extend from different ways.

409
00:21:02,680 --> 00:21:06,120
And make sure the barrier to hard work is not lost.

410
00:21:06,120 --> 00:21:08,520
I think that's also very important,

411
00:21:08,520 --> 00:21:12,360
seeing when we work together to build more and more

412
00:21:12,360 --> 00:21:14,360
complex applications.

413
00:21:14,360 --> 00:21:19,240
These are a number of things I think we're kind of figure out.

414
00:21:19,240 --> 00:21:22,760
And there are indeed a lot of challenges we haven't.

415
00:21:22,760 --> 00:21:26,440
And yeah, shall we get to that part?

416
00:21:26,440 --> 00:21:31,160
Yeah, no, I just want to reflect on some of the processes

417
00:21:31,160 --> 00:21:34,320
that you outlined, like removing humans from the loop

418
00:21:34,320 --> 00:21:35,640
step by step.

419
00:21:35,640 --> 00:21:37,480
Back in my time as an automation engineer,

420
00:21:37,480 --> 00:21:41,480
that's exactly what I would do is like, OK, I can write a script

421
00:21:41,480 --> 00:21:42,560
that does one part easily.

422
00:21:42,560 --> 00:21:43,320
Cool.

423
00:21:43,320 --> 00:21:45,160
Now, what's the next part?

424
00:21:45,160 --> 00:21:47,800
And then where do I have to jump in?

425
00:21:47,800 --> 00:21:50,640
And some of the other problems that you solved, like knowing.

426
00:21:50,680 --> 00:21:52,480
So this I think is really important,

427
00:21:52,480 --> 00:21:55,280
because some of the members of my team and my projects

428
00:21:55,280 --> 00:21:59,120
have found the same thing, is that knowing when to be quiet,

429
00:21:59,120 --> 00:22:02,480
knowing when not to jump in is, in many respects,

430
00:22:02,480 --> 00:22:04,440
more important because you don't want to end up

431
00:22:04,440 --> 00:22:07,080
with too much noise or wasted tokens.

432
00:22:07,080 --> 00:22:08,720
So that's really fascinating.

433
00:22:08,720 --> 00:22:12,400
Before we talk about problems you haven't overcome,

434
00:22:12,400 --> 00:22:14,240
can you talk about some insights from that,

435
00:22:14,240 --> 00:22:18,840
like that inhibition signal or keeping the noise lower?

436
00:22:18,840 --> 00:22:20,360
What were the key insights there?

437
00:22:20,360 --> 00:22:22,480
Like how did you test that and figure it out?

438
00:22:22,480 --> 00:22:25,760
And do you have any general principles for anyone else

439
00:22:25,760 --> 00:22:27,840
building agents?

440
00:22:27,840 --> 00:22:29,680
Yeah, this is a very good question.

441
00:22:29,680 --> 00:22:35,120
This is also related to another question about when

442
00:22:35,120 --> 00:22:39,880
do you add agents to provide the feedback and when

443
00:22:39,880 --> 00:22:42,640
that is not helpful, right?

444
00:22:42,640 --> 00:22:46,720
Because I assume the noise you're talking about

445
00:22:46,720 --> 00:22:53,080
is when you add more agents to service for democratics

446
00:22:53,080 --> 00:23:00,080
or agents that try to refine what the other agent is doing,

447
00:23:00,080 --> 00:23:00,960
right?

448
00:23:00,960 --> 00:23:03,480
They serve as a channel to provide feedback,

449
00:23:03,480 --> 00:23:07,360
but sometimes not feedback can be misleading and actually

450
00:23:07,360 --> 00:23:12,600
prevent the original agent doing the right thing.

451
00:23:12,600 --> 00:23:13,920
Yeah, we do observe that.

452
00:23:13,920 --> 00:23:17,760
And also, it's not like the more agents, the better.

453
00:23:17,760 --> 00:23:19,720
It's not necessarily that.

454
00:23:19,720 --> 00:23:23,400
For example, if you use GPD4 as the back end

455
00:23:23,400 --> 00:23:28,240
for an assistant agent, for a large number of problems,

456
00:23:28,240 --> 00:23:31,480
you'll need a simple two-agent workflow.

457
00:23:31,480 --> 00:23:34,840
One assistant agent, another user proxy agent.

458
00:23:34,840 --> 00:23:36,240
Yeah, probably I need to explain what

459
00:23:36,240 --> 00:23:37,840
the user proxy agent is.

460
00:23:37,840 --> 00:23:40,440
Basically, it refers to what I meant earlier

461
00:23:40,440 --> 00:23:43,920
about automating some of the work that human does.

462
00:23:43,920 --> 00:23:46,000
For example, using tools to execute

463
00:23:46,000 --> 00:23:48,720
Python code or run some predefined functions.

464
00:23:48,720 --> 00:23:54,080
So if you use one GPD assistant agent to suggest a solution

465
00:23:54,080 --> 00:23:56,720
such as code or function and use another user proxy

466
00:23:56,720 --> 00:24:00,520
to execute them and just provide feedback back and forth,

467
00:24:00,520 --> 00:24:04,080
you can solve a large number of problems very well.

468
00:24:04,080 --> 00:24:06,760
And some of them are also complicated

469
00:24:06,760 --> 00:24:08,680
and can involve multiple steps.

470
00:24:08,760 --> 00:24:13,960
But if you use GPD3.5 turbo, then it's much less

471
00:24:13,960 --> 00:24:15,760
to work in this way.

472
00:24:15,760 --> 00:24:18,920
So adding more agents will be much more helpful.

473
00:24:18,920 --> 00:24:22,120
And even for GPD4, when the problem complexity

474
00:24:22,120 --> 00:24:26,280
goes above a certain level, it stops

475
00:24:26,280 --> 00:24:29,120
to follow the main instructions.

476
00:24:29,120 --> 00:24:31,720
But because of the trick for one single agent to work,

477
00:24:31,720 --> 00:24:35,680
it actually puts a lot of careful instructions

478
00:24:35,680 --> 00:24:38,360
in the system message and make it

479
00:24:38,360 --> 00:24:42,800
know how to deal with some complex situations.

480
00:24:42,800 --> 00:24:47,120
But we noted that if you put too many of them, even for GPD4,

481
00:24:47,120 --> 00:24:51,160
and for complex tasks, start to forget these instructions

482
00:24:51,160 --> 00:24:55,000
and not do things as you want.

483
00:24:55,000 --> 00:24:58,880
Otherwise, you can just give it a simple instruction

484
00:24:58,880 --> 00:25:02,000
to say, try your best to solve the hardest problem

485
00:25:02,000 --> 00:25:04,960
and then it will be done.

486
00:25:05,000 --> 00:25:05,880
We're not there yet.

487
00:25:05,880 --> 00:25:07,520
I mean, in the future, we may.

488
00:25:07,520 --> 00:25:11,280
This makes me want to bring up one interesting kind of law.

489
00:25:11,280 --> 00:25:15,760
We, a few of us, came up called Kabuchi's law.

490
00:25:15,760 --> 00:25:18,960
The law has some similarity with,

491
00:25:18,960 --> 00:25:21,920
it's an analogy of the Conway's law in software engineering.

492
00:25:21,920 --> 00:25:23,920
I'm not sure if you familiar with that notion.

493
00:25:23,920 --> 00:25:24,920
No.

494
00:25:24,920 --> 00:25:27,760
So Conway's law basically saying the complexity

495
00:25:27,760 --> 00:25:31,920
of the software or the architecture of the software

496
00:25:31,920 --> 00:25:35,000
is a reflection of the organization

497
00:25:35,000 --> 00:25:38,040
that makes the software, that builds the software.

498
00:25:38,040 --> 00:25:38,440
OK.

499
00:25:38,440 --> 00:25:38,840
Makes sense.

500
00:25:38,840 --> 00:25:39,200
Yeah.

501
00:25:39,200 --> 00:25:43,480
So our Kabuchi's law says the model complexity

502
00:25:43,480 --> 00:25:47,200
will affect the model capacity or capability.

503
00:25:47,200 --> 00:25:50,720
We change the topology of the ideal multi-agent solution.

504
00:25:50,720 --> 00:25:53,400
It's a summarization of what I mentioned earlier.

505
00:25:53,400 --> 00:25:56,040
If you use a more powerful model,

506
00:25:56,040 --> 00:26:01,000
then likely you can use simpler topology of multi-agents

507
00:26:01,040 --> 00:26:04,120
to solve a common task and vice versa.

508
00:26:04,120 --> 00:26:06,840
And also, I think we need more and more research

509
00:26:06,840 --> 00:26:09,040
to understand this better as it's not soft.

510
00:26:09,040 --> 00:26:13,680
I'm seeing people trying all different kind of topologies

511
00:26:13,680 --> 00:26:15,600
or communication patterns for different applications.

512
00:26:15,600 --> 00:26:17,760
They're very creative.

513
00:26:17,760 --> 00:26:22,320
And what we had to figure out is what is the best topology

514
00:26:22,320 --> 00:26:25,520
and for a particular model and for a particular application

515
00:26:25,520 --> 00:26:28,440
in a kind of a very clear way to answer that question.

516
00:26:28,440 --> 00:26:29,600
We were not here.

517
00:26:29,640 --> 00:26:33,360
And this is one of actually a big challenge

518
00:26:33,360 --> 00:26:35,800
or a big important problem we want to solve.

519
00:26:35,800 --> 00:26:36,880
Yeah, next.

520
00:26:36,880 --> 00:26:37,680
Yeah.

521
00:26:37,680 --> 00:26:40,000
No, I mean, well, first, thanks for sharing

522
00:26:40,000 --> 00:26:42,320
some of those critical insights.

523
00:26:42,320 --> 00:26:44,920
And so I guess the general principle

524
00:26:44,920 --> 00:26:49,200
is the smarter the underpinning model,

525
00:26:49,200 --> 00:26:52,160
the simpler the topology can be because the more complex

526
00:26:52,160 --> 00:26:53,280
the instructions can be.

527
00:26:53,280 --> 00:26:55,680
And the more complex the tasks that an individual agent

528
00:26:55,680 --> 00:26:58,040
can carry out.

529
00:26:58,080 --> 00:26:59,760
Saying it out loud, it seems kind of obvious,

530
00:26:59,760 --> 00:27:02,280
but that's a good rule to generalize.

531
00:27:02,280 --> 00:27:06,120
So yeah, I guess let's pivot into what are some of the remaining

532
00:27:06,120 --> 00:27:06,600
problems?

533
00:27:06,600 --> 00:27:10,520
What are your biggest challenges that you either are working on

534
00:27:10,520 --> 00:27:12,920
or are going to be down the road?

535
00:27:12,920 --> 00:27:14,920
You mentioned topologies, like figuring out

536
00:27:14,920 --> 00:27:16,680
what is the correct topology.

537
00:27:16,680 --> 00:27:19,440
And of course, I can imagine that it's a moving target

538
00:27:19,440 --> 00:27:22,440
because as the underlying models change, almost

539
00:27:22,440 --> 00:27:25,760
on a monthly basis, you get new and different capabilities

540
00:27:25,760 --> 00:27:28,640
that kind of maybe send you back to the drawing board

541
00:27:28,640 --> 00:27:30,280
sometimes.

542
00:27:30,280 --> 00:27:33,520
Yeah, this is why having a framework that

543
00:27:33,520 --> 00:27:38,240
is versatile and that is flexible to do the experiments

544
00:27:38,240 --> 00:27:44,200
is so crucial to kind of do the fast adaptation

545
00:27:44,200 --> 00:27:49,280
as model moves as prominent techniques advances

546
00:27:49,280 --> 00:27:53,040
and as more and more small model specialized models are

547
00:27:53,040 --> 00:27:55,160
available, they will probably also

548
00:27:55,160 --> 00:27:59,480
change a lot about what was the best way to build

549
00:27:59,480 --> 00:28:01,040
the applications.

550
00:28:01,040 --> 00:28:05,440
Yeah, so this is, I think, the big value of autogen.

551
00:28:05,440 --> 00:28:08,320
And for unsolved research questions,

552
00:28:08,320 --> 00:28:12,800
there are some concrete ones I can give you a few examples.

553
00:28:12,800 --> 00:28:16,640
One is about this decomposition problem.

554
00:28:16,640 --> 00:28:20,560
As we mentioned earlier, we want to be

555
00:28:20,560 --> 00:28:24,360
able to achieve a state where the human can only

556
00:28:24,360 --> 00:28:30,480
need to specify rapidly big ambitious goal

557
00:28:30,480 --> 00:28:32,720
and underneath, we want the agent

558
00:28:32,720 --> 00:28:38,120
to be able to decompose that into solvable problems, probably

559
00:28:38,120 --> 00:28:41,640
multiple layers, and eventually recompose it

560
00:28:41,640 --> 00:28:45,720
and solve each of them and recompose it.

561
00:28:45,720 --> 00:28:47,880
And during this process, there are situations

562
00:28:47,880 --> 00:28:50,840
where the human need to provide the correct specifications

563
00:28:50,840 --> 00:28:54,440
because the initial one can be ambiguous.

564
00:28:54,440 --> 00:28:57,160
And we want the human to only provide

565
00:28:57,160 --> 00:29:00,040
the necessary and make a minimal kind of necessary

566
00:29:00,040 --> 00:29:03,200
qualifications and instructions and that agent

567
00:29:03,200 --> 00:29:06,080
to figure out the rest of them.

568
00:29:06,080 --> 00:29:09,520
That is a big challenge because if we want to solve

569
00:29:09,520 --> 00:29:12,680
more complex problems, we have to have a principal way

570
00:29:12,680 --> 00:29:15,200
to do this.

571
00:29:15,200 --> 00:29:18,320
And the second question also ready to do this

572
00:29:18,320 --> 00:29:21,920
is as we solve bigger and bigger problems,

573
00:29:21,920 --> 00:29:28,240
how do we do proper validation of the intermediate results?

574
00:29:28,240 --> 00:29:30,320
Because we don't do that.

575
00:29:30,320 --> 00:29:34,560
If possible, the agent will stick to some wrong intermediate

576
00:29:34,560 --> 00:29:40,040
results and just keep doing, keep wasting their work.

577
00:29:40,040 --> 00:29:44,600
And at certain time, if we need to provide validation

578
00:29:44,600 --> 00:29:47,560
or use agent to do self-validation, that's hard.

579
00:29:47,680 --> 00:29:52,440
But I do a way to do it.

580
00:29:52,440 --> 00:29:56,160
So we need probably into some formal language

581
00:29:56,160 --> 00:30:02,400
or formal way to do this proper validation

582
00:30:02,400 --> 00:30:05,920
so that the automation can indeed happen in a way

583
00:30:05,920 --> 00:30:09,920
that human desires.

584
00:30:09,920 --> 00:30:14,120
Yeah, so these are some just two kind of concrete pieces

585
00:30:14,120 --> 00:30:15,720
like problems.

586
00:30:15,720 --> 00:30:18,880
Yeah, in my project, we almost started in the reverse

587
00:30:18,880 --> 00:30:23,280
where we started with oversight of steering and oversight

588
00:30:23,280 --> 00:30:25,000
and supervision.

589
00:30:25,000 --> 00:30:30,760
So I'm curious, what's your perception or research

590
00:30:30,760 --> 00:30:33,200
or findings with respect to?

591
00:30:33,200 --> 00:30:36,960
Because you already mentioned having an assistant agent

592
00:30:36,960 --> 00:30:41,160
and then also having kind of a top-down hierarchical agent

593
00:30:41,160 --> 00:30:43,120
where you've got subordinates.

594
00:30:43,120 --> 00:30:45,000
What do you think about my intuition

595
00:30:45,000 --> 00:30:51,320
that working towards having supervisors steering QA quality

596
00:30:51,320 --> 00:30:55,520
assurance agents throughout the network of agents

597
00:30:55,520 --> 00:30:57,880
that are capable of providing some of that feedback

598
00:30:57,880 --> 00:31:00,160
that you mentioned earlier, is that kind of the direction

599
00:31:00,160 --> 00:31:00,880
that you're going?

600
00:31:00,880 --> 00:31:03,280
Or have you tried that and it didn't work?

601
00:31:03,280 --> 00:31:05,960
Or what are your thoughts in terms

602
00:31:05,960 --> 00:31:10,280
of having some of those specialized roles or personas

603
00:31:10,280 --> 00:31:13,040
as a way to help along?

604
00:31:13,040 --> 00:31:17,760
Yeah, there are some examples that work pretty well.

605
00:31:17,760 --> 00:31:19,800
I can show some of them.

606
00:31:19,800 --> 00:31:24,640
One is a three-agent setup to solve a multi-agent coding

607
00:31:24,640 --> 00:31:25,800
scenario.

608
00:31:25,800 --> 00:31:28,600
The application is for a supply chain optimization.

609
00:31:28,600 --> 00:31:31,440
It's done by another MSR team.

610
00:31:31,440 --> 00:31:34,560
But that solution, in my view, is quite generic.

611
00:31:34,560 --> 00:31:37,400
It's not restricted to that particular application.

612
00:31:37,400 --> 00:31:41,480
And the setup is like, it's a hierarchical setup.

613
00:31:41,520 --> 00:31:43,200
There's the commander on top.

614
00:31:43,200 --> 00:31:47,120
There's a writer who is responsible in writing Python

615
00:31:47,120 --> 00:31:47,920
code.

616
00:31:47,920 --> 00:31:51,360
The agent can also have access to some proper tools,

617
00:31:51,360 --> 00:31:53,040
like organization tools.

618
00:31:53,040 --> 00:31:57,200
And the other subagent is actually Safeguard.

619
00:31:57,200 --> 00:32:00,720
Safeguard is in charge of reviewing code safety.

620
00:32:00,720 --> 00:32:03,080
So the way it works is that the commander receives

621
00:32:03,080 --> 00:32:04,280
some user's question.

622
00:32:04,280 --> 00:32:08,400
It will first ask the writer to write the code.

623
00:32:08,400 --> 00:32:10,680
And after receiving the code, it will ask the Safeguard

624
00:32:10,680 --> 00:32:12,880
to review the code for safety.

625
00:32:12,880 --> 00:32:15,680
And only if the safety criteria is met,

626
00:32:15,680 --> 00:32:19,880
it will round the code and send the result back.

627
00:32:19,880 --> 00:32:23,840
Otherwise, it will just ask the writer to rewrite the code.

628
00:32:23,840 --> 00:32:27,120
And this can go back and forth because there can be errors.

629
00:32:27,120 --> 00:32:31,080
So when you debug, the writer can do that.

630
00:32:31,080 --> 00:32:34,080
Until the result is correct, the writer

631
00:32:34,080 --> 00:32:36,520
comes back with a final natural language answer

632
00:32:36,520 --> 00:32:37,400
to some result.

633
00:32:37,400 --> 00:32:39,800
And the current return that to user.

634
00:32:39,800 --> 00:32:45,280
So this is almost a quite simple multi-agent setup,

635
00:32:45,280 --> 00:32:49,400
but very effective in our application, almost 100%

636
00:32:49,400 --> 00:32:51,920
correct every time.

637
00:32:51,920 --> 00:32:57,040
One kind of lesson is if we merge the capability of the writer

638
00:32:57,040 --> 00:32:59,600
and the Safeguard into one agent,

639
00:32:59,600 --> 00:33:02,840
it doesn't work that well, especially in the code safety

640
00:33:02,840 --> 00:33:04,560
part.

641
00:33:04,600 --> 00:33:08,640
So we have the experiments in our paper.

642
00:33:08,640 --> 00:33:13,680
We found that if you merge them, then the accuracy

643
00:33:13,680 --> 00:33:18,240
for detecting code safety issue will drop significantly,

644
00:33:18,240 --> 00:33:21,000
both for GP4 and GP3.5 Turbo, but more

645
00:33:21,000 --> 00:33:23,840
significantly for GP3.5.

646
00:33:23,840 --> 00:33:28,720
So this kind of hints that one agent,

647
00:33:28,720 --> 00:33:31,880
if you ask to both suggest a solution

648
00:33:31,880 --> 00:33:36,240
and check the solution suggested by itself,

649
00:33:36,240 --> 00:33:38,840
have a bias.

650
00:33:38,840 --> 00:33:42,360
But we separate them and also prevent them

651
00:33:42,360 --> 00:33:45,160
to talk to each other, kind of make

652
00:33:45,160 --> 00:33:47,640
them work in an adversarial setting.

653
00:33:47,640 --> 00:33:49,400
It does it better.

654
00:33:49,400 --> 00:33:51,640
So that is one observation.

655
00:33:51,640 --> 00:33:54,360
But we also have other kind of scenarios

656
00:33:54,360 --> 00:33:57,840
where we do involve every agent in one group chat.

657
00:33:57,880 --> 00:33:59,920
So everyone also sees other's message

658
00:33:59,920 --> 00:34:02,200
and can reply back.

659
00:34:02,200 --> 00:34:06,040
It also works sometimes for other tasks.

660
00:34:06,040 --> 00:34:11,120
For example, a critique to suggest a visualization

661
00:34:11,120 --> 00:34:13,600
criteria for a visualization task.

662
00:34:13,600 --> 00:34:18,880
You can have one agent write code and another to criticize.

663
00:34:18,880 --> 00:34:19,720
It sort of works.

664
00:34:19,720 --> 00:34:24,520
But my intuition is still that if we put every agent work

665
00:34:24,560 --> 00:34:28,480
together always in a group chat, it may not always work

666
00:34:28,480 --> 00:34:30,640
because they may have the tendency

667
00:34:30,640 --> 00:34:37,160
to agree with each other and try hard to challenge.

668
00:34:37,160 --> 00:34:42,200
I would say it's case by case for different applications.

669
00:34:42,200 --> 00:34:45,760
There's also some benefit of doing it in this group chat

670
00:34:45,760 --> 00:34:47,760
because it's relatively simple.

671
00:34:47,760 --> 00:34:50,600
You don't need to do very hard about handling

672
00:34:50,600 --> 00:34:53,440
the message separation.

673
00:34:53,440 --> 00:34:55,480
You can simply define your agents

674
00:34:55,480 --> 00:34:58,920
and put them in a group chat and get them running quickly.

675
00:34:58,920 --> 00:35:00,200
So that's one benefit of group chat

676
00:35:00,200 --> 00:35:02,600
and seeing many people are using that approach.

677
00:35:02,600 --> 00:35:07,080
But just to be careful that it may not always

678
00:35:07,080 --> 00:35:11,160
work because of the limitations of the models.

679
00:35:11,160 --> 00:35:13,360
So that's really fascinating to me.

680
00:35:13,360 --> 00:35:15,920
And my intuition was the same.

681
00:35:15,920 --> 00:35:18,280
But it's interesting to have that validation

682
00:35:18,280 --> 00:35:19,520
from another perspective.

683
00:35:19,520 --> 00:35:23,120
So it's almost like even though the underlying model

684
00:35:23,120 --> 00:35:27,160
is GPT-4 running all of the agents or 3.5 turbo,

685
00:35:27,160 --> 00:35:33,080
there's still a positive effect from using division of labor,

686
00:35:33,080 --> 00:35:34,720
which the division of labor comes

687
00:35:34,720 --> 00:35:38,320
from the history of human work.

688
00:35:38,320 --> 00:35:42,000
And so just taking a moment, obviously these models

689
00:35:42,000 --> 00:35:43,320
do not work like human brains.

690
00:35:43,320 --> 00:35:47,720
But when you have an agent with a very specific task and mission

691
00:35:47,720 --> 00:35:51,360
and set of success criteria, that effectiveness

692
00:35:51,400 --> 00:35:54,120
of the division of labor still helps,

693
00:35:54,120 --> 00:35:57,160
even though it's just activating the latent capabilities

694
00:35:57,160 --> 00:35:59,480
within the same underpinning model.

695
00:35:59,480 --> 00:36:02,320
And then another intuition or a principle

696
00:36:02,320 --> 00:36:07,440
that I want to reiterate is the idea that, in some cases,

697
00:36:07,440 --> 00:36:10,960
group work makes sense, but in some cases, it doesn't.

698
00:36:10,960 --> 00:36:13,680
It's almost like the same difference in humans

699
00:36:13,680 --> 00:36:18,800
where the power of introverts, doing solo work on your own

700
00:36:18,800 --> 00:36:20,920
versus doing collaborative group work.

701
00:36:20,960 --> 00:36:23,840
So again, not saying that they're operating like humans,

702
00:36:23,840 --> 00:36:26,680
but it's really interesting to see some of these parallels

703
00:36:26,680 --> 00:36:32,800
emerge between multi-agent work and the nature of human labor.

704
00:36:32,800 --> 00:36:35,680
So yeah, very fascinating.

705
00:36:35,680 --> 00:36:39,560
And it's interesting because in some of the conversations

706
00:36:39,560 --> 00:36:42,400
that I've had and some of the observations that I've made,

707
00:36:42,400 --> 00:36:44,640
it's almost like what we're doing with these agents,

708
00:36:44,640 --> 00:36:49,040
these groups of agents, is recreating a corporation.

709
00:36:49,040 --> 00:36:51,680
You might have a CEO or a boss or a supervisor,

710
00:36:51,680 --> 00:36:54,160
and then you have the coder and then the QA.

711
00:36:54,160 --> 00:36:56,480
So it's a very similar structure.

712
00:36:56,480 --> 00:37:00,280
So do you have any other major insights or lessons

713
00:37:00,280 --> 00:37:04,200
that you think are either recently or super valuable

714
00:37:04,200 --> 00:37:05,960
that you want to share with other researchers

715
00:37:05,960 --> 00:37:07,960
or that you would recommend?

716
00:37:07,960 --> 00:37:08,720
Sure, sure.

717
00:37:08,720 --> 00:37:10,520
There are so many of them.

718
00:37:10,520 --> 00:37:15,120
I can give you a few examples.

719
00:37:15,120 --> 00:37:20,400
One thing is the chat, the conversation perspective.

720
00:37:20,400 --> 00:37:25,840
I mentioned earlier that chat GPD is a big inspiration.

721
00:37:25,840 --> 00:37:27,560
Certainly for many people.

722
00:37:27,560 --> 00:37:30,680
But for me, there's a personal story

723
00:37:30,680 --> 00:37:34,120
about what specific user I felt from it.

724
00:37:34,120 --> 00:37:38,240
It's a reminder of something I learned back in my college

725
00:37:38,240 --> 00:37:44,000
from a professor who told me that conversation is a provable

726
00:37:44,000 --> 00:37:49,840
way of making a good progress of learning.

727
00:37:49,840 --> 00:37:55,240
I don't remember the exact quote of that, but it's roughly that.

728
00:37:55,240 --> 00:37:59,120
So basically, he's trying to say, conversation

729
00:37:59,120 --> 00:38:03,640
is a very powerful form of either learning or making

730
00:38:03,640 --> 00:38:09,120
progress, or et cetera, that many people didn't realize

731
00:38:09,120 --> 00:38:11,680
it's how important it is.

732
00:38:11,680 --> 00:38:15,640
And there are some theoretical roots there.

733
00:38:15,640 --> 00:38:21,080
So that's one reason I'm so kind of so sure,

734
00:38:21,080 --> 00:38:25,920
or so I have so much belief in using conversation

735
00:38:25,920 --> 00:38:32,360
as the central medium of the command multi-agent interface.

736
00:38:32,360 --> 00:38:37,040
Again, I know there's a science, although I didn't have time

737
00:38:37,040 --> 00:38:39,680
to find out which reference it was.

738
00:38:39,680 --> 00:38:45,600
But I know that, so it gave me the confidence or the belief

739
00:38:45,600 --> 00:38:47,560
that this is the right thing to do.

740
00:38:47,560 --> 00:38:50,240
I think one of my favorite courses,

741
00:38:50,240 --> 00:38:52,320
Jimmy actually found me some reference

742
00:38:52,320 --> 00:38:55,000
from a social scientist.

743
00:38:55,000 --> 00:38:57,440
He mentioned something similar to that.

744
00:38:57,440 --> 00:39:02,880
Yeah, so this is one lesson, one kind of unique thing

745
00:39:02,880 --> 00:39:06,320
that I don't think many people have really.

746
00:39:06,320 --> 00:39:10,640
They kind of understand chatGVT is very powerful,

747
00:39:10,640 --> 00:39:13,840
and also get a lot of useful experience from that.

748
00:39:13,840 --> 00:39:17,960
But maybe this science part of it is less known.

749
00:39:17,960 --> 00:39:20,040
So that's one thing I would share.

750
00:39:20,040 --> 00:39:23,200
Another inspiration source, as I told you,

751
00:39:23,200 --> 00:39:26,280
so auto-gen is really inspired by many different things,

752
00:39:26,280 --> 00:39:29,200
many projects I've worked on before,

753
00:39:29,200 --> 00:39:30,680
and all the lessons I've learned.

754
00:39:30,680 --> 00:39:34,400
So another one, for example, is the operating system.

755
00:39:34,400 --> 00:39:36,440
So this is also not so obvious.

756
00:39:36,440 --> 00:39:40,120
When we talk about AI, why do we talk about operating systems?

757
00:39:40,120 --> 00:39:43,840
I think the several things, several inspiration

758
00:39:43,840 --> 00:39:46,520
I take from the success of operating systems.

759
00:39:46,520 --> 00:39:52,560
One is the idea of maximizing the utility of the most valuable

760
00:39:52,560 --> 00:39:55,120
resource you have.

761
00:39:55,120 --> 00:39:59,520
So in old days, it's like the CPU, the GPU.

762
00:39:59,520 --> 00:40:02,360
But I think in the new era of AI,

763
00:40:02,440 --> 00:40:06,840
these powerful, not even models is so valuable resource

764
00:40:06,840 --> 00:40:13,120
and building an operating system around them, right?

765
00:40:13,120 --> 00:40:15,960
And maximizing their utility, but to give them

766
00:40:15,960 --> 00:40:21,200
the necessary peripherals and do the right coordination.

767
00:40:21,200 --> 00:40:26,240
And it's super critical from the system point of view.

768
00:40:26,240 --> 00:40:28,840
And also, so that operating system is really

769
00:40:28,840 --> 00:40:31,240
you can build a platform that can support

770
00:40:31,240 --> 00:40:34,080
many diverse applications on top of that.

771
00:40:34,080 --> 00:40:38,200
So we need to design a very generic robust kind of system

772
00:40:38,200 --> 00:40:39,320
to do that, right?

773
00:40:39,320 --> 00:40:42,480
So these are all the design principles

774
00:40:42,480 --> 00:40:45,360
we try to use when we design AutoGen.

775
00:40:45,360 --> 00:40:49,880
And similarly, the idea of object-oriented programming

776
00:40:49,880 --> 00:40:52,440
is very useful.

777
00:40:52,440 --> 00:40:54,840
So many developers have very interesting ideas

778
00:40:54,840 --> 00:40:57,840
they want to try and develop.

779
00:40:57,840 --> 00:41:00,440
And now with this framework that hides a lot of complexity

780
00:41:00,440 --> 00:41:04,560
inside the framework, they're able to kind of do the things

781
00:41:04,560 --> 00:41:06,400
they want more easily.

782
00:41:06,400 --> 00:41:08,920
That's a part of abstraction.

783
00:41:08,920 --> 00:41:15,160
I already mentioned the agent, notion, automation,

784
00:41:15,160 --> 00:41:16,280
inspiration.

785
00:41:16,280 --> 00:41:19,160
The one thing I want to mention is open source, right?

786
00:41:19,160 --> 00:41:22,440
The concept of open source is that it

787
00:41:22,440 --> 00:41:26,040
can solve the common problems that community needs

788
00:41:26,040 --> 00:41:28,160
and make it really easy to use.

789
00:41:28,160 --> 00:41:33,160
So those are probably most modern kind of things

790
00:41:33,160 --> 00:41:37,200
that can get good open source adoption

791
00:41:37,200 --> 00:41:40,960
and build something that the community loves, right?

792
00:41:40,960 --> 00:41:43,440
Yeah, so I think that is my valuable lesson

793
00:41:43,440 --> 00:41:45,400
I want to share with all researchers, right?

794
00:41:45,400 --> 00:41:48,960
If you want to get their research adopted

795
00:41:48,960 --> 00:41:52,640
and get more and more impact and influence

796
00:41:52,640 --> 00:41:54,840
and through this open source channel,

797
00:41:54,840 --> 00:42:00,480
then spend a lot of effort about usability

798
00:42:00,480 --> 00:42:04,200
and solving the common problem that many people want

799
00:42:04,200 --> 00:42:08,680
to solve is what's going to be considered.

800
00:42:08,680 --> 00:42:11,120
I have personally found success in giving away

801
00:42:11,120 --> 00:42:14,400
as much valuable information and ideas as I can.

802
00:42:14,400 --> 00:42:18,080
That's what my whole YouTube career and computer science

803
00:42:18,080 --> 00:42:19,720
career is based on now.

804
00:42:19,720 --> 00:42:22,760
So thank you for sharing those critical insights.

805
00:42:22,760 --> 00:42:25,120
So on the topic of operating systems,

806
00:42:25,120 --> 00:42:27,040
because I'm really glad you brought that up,

807
00:42:27,040 --> 00:42:30,320
because I started thinking about language models

808
00:42:30,320 --> 00:42:33,360
as a component, like a new component of an operating

809
00:42:33,360 --> 00:42:34,040
system.

810
00:42:34,040 --> 00:42:36,600
So I'm glad to know that there's some convergence there.

811
00:42:36,600 --> 00:42:38,960
Is that kind of the future of Autogen?

812
00:42:38,960 --> 00:42:40,920
Is that what you're looking to move towards

813
00:42:40,920 --> 00:42:46,040
is kind of being the operating system or a major component

814
00:42:46,040 --> 00:42:50,680
of a future operating system that uses language models

815
00:42:50,680 --> 00:42:55,160
as like kind of the new CPU and maybe retrieval augmented,

816
00:42:55,160 --> 00:42:58,680
some kind of storage as like the new memory?

817
00:42:58,680 --> 00:43:01,680
Is that kind of the direction that it's going?

818
00:43:01,680 --> 00:43:02,400
Yeah, exactly.

819
00:43:02,400 --> 00:43:05,200
So it's my ambition.

820
00:43:05,200 --> 00:43:07,440
When I started working with Autogen,

821
00:43:07,440 --> 00:43:12,640
I discussed with some systems friends working on the systems.

822
00:43:12,640 --> 00:43:14,480
I told them this idea.

823
00:43:14,480 --> 00:43:18,080
And yeah, it sounds very ambitious idea to them.

824
00:43:18,080 --> 00:43:23,240
But I can see that some people really liked this idea.

825
00:43:23,240 --> 00:43:26,040
And even some 13-year people will give me

826
00:43:26,040 --> 00:43:28,640
stronger, strong support of this.

827
00:43:28,640 --> 00:43:32,520
He kind of had that idea independently.

828
00:43:32,520 --> 00:43:36,360
I kind of see that some of the most visionary people also

829
00:43:36,360 --> 00:43:37,160
realized this.

830
00:43:37,160 --> 00:43:42,400
And definitely, we want to pursue for that.

831
00:43:42,400 --> 00:43:43,280
Excellent.

832
00:43:43,280 --> 00:43:47,640
So taking a big step back, just in terms of the direction

833
00:43:47,640 --> 00:43:50,280
of research, I think, I don't know if it's official,

834
00:43:50,280 --> 00:43:54,680
but the rumor is right now OpenAI is working on GPT-5.

835
00:43:54,680 --> 00:43:57,160
And then, of course, Google with Gemini and Meta,

836
00:43:57,160 --> 00:44:00,240
like everyone is working on bigger and bigger models now.

837
00:44:00,240 --> 00:44:04,480
And so we're going to get more capabilities at the same time.

838
00:44:04,480 --> 00:44:06,480
Smaller models are becoming more efficient.

839
00:44:06,480 --> 00:44:10,200
So Satya Nadella announced small language models coming.

840
00:44:10,200 --> 00:44:13,840
So that way, you can probably perform very small cognitive

841
00:44:13,840 --> 00:44:16,480
functions, but very quickly and efficiently.

842
00:44:16,480 --> 00:44:19,240
So what are some of the trends that you

843
00:44:19,240 --> 00:44:23,680
see intersecting with your work around auto-gen and agents

844
00:44:23,680 --> 00:44:25,200
and agent swarms?

845
00:44:25,200 --> 00:44:27,040
And what I mean, I guess, to be specific,

846
00:44:27,040 --> 00:44:32,720
is maybe cost changing or new capabilities coming.

847
00:44:32,720 --> 00:44:34,880
Are there any capabilities that you're really looking for

848
00:44:34,880 --> 00:44:38,160
that would make your job easier?

849
00:44:38,160 --> 00:44:40,800
What are your thoughts on some of these new capabilities

850
00:44:40,800 --> 00:44:45,640
and making these multi-agent platforms more autonomous?

851
00:44:45,680 --> 00:44:47,400
Or is that a good idea, a bad idea?

852
00:44:47,400 --> 00:44:49,360
So very kind of open-ended question,

853
00:44:49,360 --> 00:44:53,080
like what do you see coming this time next year?

854
00:44:53,080 --> 00:44:57,720
Yeah, I think the idea of having specialized models

855
00:44:57,720 --> 00:45:01,560
to perform certain tasks in an excellent way

856
00:45:01,560 --> 00:45:05,560
and in a cheap way is fascinating.

857
00:45:05,560 --> 00:45:09,440
It's indeed worth a lot of investigation.

858
00:45:09,440 --> 00:45:13,400
For example, some of the hard problems I mentioned earlier

859
00:45:13,400 --> 00:45:17,280
about the decomposition, recombination, validation,

860
00:45:17,280 --> 00:45:20,160
it's possible that some specialized model

861
00:45:20,160 --> 00:45:22,480
can do these kind of tasks really well.

862
00:45:22,480 --> 00:45:23,760
Or we haven't seen that yet.

863
00:45:23,760 --> 00:45:28,160
But conceptually, that sounds like a possibility.

864
00:45:28,160 --> 00:45:33,440
Actually, I'm pretty surprised that we haven't found

865
00:45:33,440 --> 00:45:35,120
a special model that can solve this.

866
00:45:35,120 --> 00:45:39,840
So it makes me kind of wonder why.

867
00:45:39,840 --> 00:45:42,880
Because it's such a natural idea that if you're

868
00:45:42,880 --> 00:45:45,920
finding a model that can do certain things,

869
00:45:45,920 --> 00:45:48,240
you should be able to do certain tasks very well

870
00:45:48,240 --> 00:45:52,320
and you can just replace one specific agent with that.

871
00:45:52,320 --> 00:45:54,920
And I don't know many people are trying that.

872
00:45:54,920 --> 00:45:58,240
Either there's some fundamental reason

873
00:45:58,240 --> 00:46:03,960
we haven't figured out why we can't do that,

874
00:46:03,960 --> 00:46:07,920
or we should be able to see that pretty soon.

875
00:46:07,920 --> 00:46:10,440
I think it's only one of these two possibilities.

876
00:46:10,440 --> 00:46:12,360
Because the former possibility is still there

877
00:46:12,400 --> 00:46:16,000
because the small model, it's possible that the small model

878
00:46:16,000 --> 00:46:18,280
lacks some very important capability

879
00:46:18,280 --> 00:46:21,920
of being functioned to perform these hard tasks.

880
00:46:21,920 --> 00:46:23,480
Because these tasks are not easy.

881
00:46:23,480 --> 00:46:28,400
The composition problem, I think, even the GPD4 model,

882
00:46:28,400 --> 00:46:31,440
it's known to not to be too good at planning.

883
00:46:31,440 --> 00:46:35,200
It can do some kind of planning, but not perfectly.

884
00:46:35,200 --> 00:46:37,800
So if you want to get better capability than GPD4

885
00:46:37,800 --> 00:46:43,240
in some specialized tasks, it's to be seeing

886
00:46:43,240 --> 00:46:44,760
whether we can accomplish that.

887
00:46:44,760 --> 00:46:48,360
But if we lower the target, if we say,

888
00:46:48,360 --> 00:46:50,240
let's train some small models that

889
00:46:50,240 --> 00:46:52,840
can do something that GPD4 is already good at,

890
00:46:52,840 --> 00:46:56,120
that is much more amenable.

891
00:46:56,120 --> 00:46:59,400
I think I already see evidence of that.

892
00:46:59,400 --> 00:47:03,640
So then it's more like a cost reduction story.

893
00:47:03,640 --> 00:47:07,960
So that is, I'm pretty sure, that's feasible.

894
00:47:07,960 --> 00:47:11,760
And the other part about getting better capability

895
00:47:11,760 --> 00:47:15,520
than the big model, in some aspect,

896
00:47:15,520 --> 00:47:22,240
is to be kind of, yeah, we have to hold our scientific curiosity

897
00:47:22,240 --> 00:47:25,440
and see what happens.

898
00:47:25,440 --> 00:47:26,360
That makes sense.

899
00:47:26,360 --> 00:47:30,840
We are almost out of time, so I want to respect everyone's time.

900
00:47:30,840 --> 00:47:33,120
And so I'll just say, thank you so much

901
00:47:33,120 --> 00:47:36,200
for jumping on and sharing some of your thoughts.

902
00:47:36,200 --> 00:47:40,480
I'm super excited to be along for the ride.

903
00:47:40,480 --> 00:47:43,480
But yeah, before we close, I'll give the floor to you.

904
00:47:43,480 --> 00:47:47,680
Is there anything that you'd like to put out in the world,

905
00:47:47,680 --> 00:47:51,720
any personal requests or personal hopes

906
00:47:51,720 --> 00:47:55,200
that you want to want to share with a broader audience?

907
00:47:55,200 --> 00:47:58,000
Thanks for giving me the opportunity to do that.

908
00:47:58,000 --> 00:48:01,280
Yeah, I want to say that we are still

909
00:48:01,280 --> 00:48:04,520
early at the new age.

910
00:48:04,520 --> 00:48:07,560
Agents become mature software that

911
00:48:07,560 --> 00:48:10,400
can do a lot of things for us.

912
00:48:10,400 --> 00:48:12,840
We want to build the future together

913
00:48:12,840 --> 00:48:15,960
with everyone from the community.

914
00:48:15,960 --> 00:48:18,560
So give Autogeno a try.

915
00:48:18,560 --> 00:48:20,680
Try to use it for applications.

916
00:48:20,680 --> 00:48:23,520
Let us know what's working and what's not.

917
00:48:23,520 --> 00:48:28,200
We are very happy to work together to improve it

918
00:48:28,200 --> 00:48:30,840
and answer some of the big, important problems

919
00:48:30,920 --> 00:48:32,080
as we mentioned.

920
00:48:32,080 --> 00:48:36,800
And I really want to acknowledge that all the contributors,

921
00:48:36,800 --> 00:48:41,600
starting from the original paper to the recent,

922
00:48:41,600 --> 00:48:45,080
more open source resources, joined together,

923
00:48:45,080 --> 00:48:48,960
and the huge developer community that's supporting us,

924
00:48:48,960 --> 00:48:53,880
I really learned a lot from everyone who has used

925
00:48:53,880 --> 00:48:58,480
and provided feedback that people are super, super creative.

926
00:48:58,480 --> 00:49:02,720
I think this is the right way to solve the hard problems

927
00:49:02,720 --> 00:49:06,760
and hope to continue to do that and support the community,

928
00:49:06,760 --> 00:49:07,840
support everyone.

929
00:49:07,840 --> 00:49:11,120
And for example, the effort you're doing

930
00:49:11,120 --> 00:49:14,680
with the hierarchical agent swarm,

931
00:49:14,680 --> 00:49:18,200
it's a very good example that you're

932
00:49:18,200 --> 00:49:23,560
making certain bats on certain ways of making money and work.

933
00:49:23,560 --> 00:49:26,200
I'm very curious to see how that experiment goes.

934
00:49:26,200 --> 00:49:30,440
And if altering can be of any help in this or other consumer

935
00:49:30,440 --> 00:49:35,600
efforts, we'll be very happy to support you

936
00:49:35,600 --> 00:49:41,560
if you need any feature and useful infrastructure support

937
00:49:41,560 --> 00:49:43,320
that kind of thing.

938
00:49:43,320 --> 00:49:45,880
Yes, let us know.

939
00:49:45,880 --> 00:49:47,760
Yeah, absolutely.

940
00:49:47,760 --> 00:49:50,440
No, we'll be definitely looking forward

941
00:49:50,440 --> 00:49:53,560
to continuing the collaboration.

942
00:49:53,560 --> 00:49:56,120
I think that, as you said, there is a lot of work to do.

943
00:49:56,160 --> 00:49:57,680
And there are some limitations.

944
00:49:57,680 --> 00:50:00,600
The model's limitations today are the model's limitations.

945
00:50:00,600 --> 00:50:03,200
There's not a lot we can do to work around that.

946
00:50:03,200 --> 00:50:05,240
But it is just the beginning.

947
00:50:05,240 --> 00:50:08,640
And that's one thing that I'll use the closing to say

948
00:50:08,640 --> 00:50:13,040
is remember where we were a year ago today.

949
00:50:13,040 --> 00:50:17,600
Chat GPT was probably published just about a year ago.

950
00:50:17,600 --> 00:50:21,760
But before that, it was GPT-3, GPT-3.5.

951
00:50:21,760 --> 00:50:26,100
And the distance that we've covered in just the last year

952
00:50:26,100 --> 00:50:31,460
is it is a privilege to be part of one of the greatest shifts

953
00:50:31,460 --> 00:50:33,260
that humanity has ever seen.

954
00:50:33,260 --> 00:50:34,780
And some days, it doesn't feel real.

955
00:50:34,780 --> 00:50:36,780
And some days, it feels a little too real

956
00:50:36,780 --> 00:50:38,100
and a little too overwhelming.

957
00:50:38,100 --> 00:50:42,300
So thank you, Xi, for helping make this a reality

958
00:50:42,300 --> 00:50:44,540
and spending some time talking with me.

959
00:50:44,540 --> 00:50:47,620
And thank you to Katie for helping put this together.

960
00:50:47,620 --> 00:50:49,940
And yeah, so thanks everyone.

961
00:50:49,940 --> 00:50:53,740
And yeah, see you all next time.

962
00:50:53,740 --> 00:50:54,540
Thank you so much.

