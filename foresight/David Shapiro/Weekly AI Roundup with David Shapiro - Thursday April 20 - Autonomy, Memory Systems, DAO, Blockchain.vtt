WEBVTT

00:00.000 --> 00:10.000
going live. All right, I think we're here. Can y'all hear me now? Can you hear me now?

00:10.000 --> 00:26.960
Hello? Is this thing on? Okay. It says live in 15 seconds. Okay, we're good. Yay! Okay,

00:27.840 --> 00:36.400
make sure to mute myself. All right, so hello everybody. I'm still figuring out the whole

00:36.400 --> 00:41.840
live streaming thing, so please bear with me, but one thing that y'all said was that you want to

00:41.840 --> 00:45.760
like see what I'm looking at, which you know, it's a good idea. You don't want to just look at my

00:45.760 --> 00:50.560
face. It's like, let's look at stuff together. So I figured I'd follow more or less the same format

00:50.560 --> 00:57.760
that I usually do. I'm in the cognitive AI lab discord, so if you're in the general, you can

00:57.760 --> 01:06.400
drop questions here or papers. So basically what I thought we would do is we would take a look at

01:06.400 --> 01:13.200
like weekly updates, because it's all going really fast. And you, I've got a lot of feedback from my

01:13.200 --> 01:18.800
recent live streams, that y'all really liked the interactive aspect of like asking questions,

01:18.800 --> 01:24.160
and like Dave, what does this mean? And so please feel free to drop good questions in. I'm also in

01:24.160 --> 01:32.080
the private Patreon discord, so drop questions there too. Okay, cool. Party time, yeah. Oh,

01:32.080 --> 01:39.360
looks like there's a delay of about 15 seconds. Interesting. Okay. Cool. Flippy from the dais

01:39.360 --> 01:47.280
discord. Hello Flippy. All right. Cool. So while some questions get spooled up, I figured I'd go

01:47.280 --> 01:53.760
through a couple of tools and stuff that I've seen. So someone, I think actually Flippy, this

01:53.760 --> 02:00.000
might have been you or someone in the dais discord, pointed out this tool. So this is basically

02:00.000 --> 02:06.240
something that I and others have been like thinking about and talking about. It's basically a hybrid

02:06.240 --> 02:14.400
of vector database and knowledge graphs. And it's also got a pretty interface. So

02:15.360 --> 02:21.680
like, it's worth just scrolling through and like reading every every bit of this. And then kind

02:21.680 --> 02:28.720
of experimenting with it. But but it will create. So this is one of the coolest things is it'll

02:28.720 --> 02:34.800
create a knowledge graph of all the clusters and stuff. But you can find the gaps, which when

02:34.800 --> 02:41.360
you're thinking about autonomous or semi autonomous AI agents, this is really good because then

02:42.000 --> 02:48.240
you can know what you don't know. And what I mean by that is if you're aware of all the things that

02:48.240 --> 02:54.080
you know, but you can detect some semantic gaps in your knowledge, that can help you zoom in on

02:54.080 --> 03:00.080
the things that you need to go learn about. So in front notice is the tool in front notice.com.

03:00.080 --> 03:06.000
I can never remember the name of the darn thing. I'm going to blame allergies and say that it's

03:06.080 --> 03:12.960
just brain inflammation. That's my excuse and I'm sticking to it. But yes, so this is a super cool

03:12.960 --> 03:24.960
tool. This kind of technology will definitely be part of of like autonomous AI agents. And this is

03:25.600 --> 03:31.200
functionally similar to what I worked on with Remo. Remo is much, much, much simpler though.

03:32.000 --> 03:37.760
So I often have some people ask me like, Oh, hey, can Remo do this? And that and I'm like, no,

03:37.760 --> 03:43.360
like, Remo was meant to solve one very specific problem. So for memory stuff, I usually point

03:43.360 --> 03:51.440
people at llama index and chroma db. So chroma db. Do I seriously not have that bookmarked?

03:52.320 --> 03:58.400
So chroma db is a vector database that runs just like SQLite. So it's pip install chroma db,

03:58.480 --> 04:04.640
you create a local client just like you do SQLite. So I tried to create something like this called

04:04.640 --> 04:09.600
VDB light like a few months ago. And I quickly realized that I was in way over my head. So I'm

04:09.600 --> 04:15.760
glad someone built this. And they just got an $18 million seed round. Holy mackerel. Man,

04:15.760 --> 04:21.360
I should have stuck with VDB light. I could have had an $18 million seed round. Anyways, maybe I'll

04:21.360 --> 04:30.080
do that with Remo who knows. So chroma db, super simple man, seriously $18 million for this one

04:30.080 --> 04:36.960
thing. Okay, I'm gonna let that go. llama index also. So llama index, I kind of didn't pay it

04:36.960 --> 04:41.440
any any attention at first because it's like, Okay, that's a silly name. This was clearly just

04:41.440 --> 04:47.120
someone's little side project. But if you look at all the types of indices they have,

04:48.080 --> 04:54.560
they've got list index, table index, tree index. So Remo is very similar to llamas tree index.

04:54.560 --> 04:59.680
Although I will say that I look through the code. And I think that their tree index is kind of basic.

04:59.680 --> 05:06.160
I think that my Remo framework does a little bit that theirs doesn't. But I'm not going to dive into

05:06.160 --> 05:09.920
that because I don't know that for certain. I didn't take a super close look at the code.

05:10.240 --> 05:17.600
Yeah, but so those are those are some memory storage tools. So let me check on the live stream

05:18.240 --> 05:23.600
and see where we're at. Whoops. All right. Oh, wow, we got lots of questions. Okay, cool.

05:24.320 --> 05:29.280
God's not dead rather believe and go to the good place and interesting. Okay.

05:31.680 --> 05:35.840
Let's see how to discover new wisdom from LLM. That's an interesting question.

05:36.400 --> 05:40.000
Um, let's see. Let me,

05:43.120 --> 05:48.400
you people ask questions far too early. Let me go over to discord to see if,

05:51.280 --> 05:59.040
yeah, please go ahead and drop some questions. Patreon get first dibs.

05:59.040 --> 06:08.640
All right. What do you think is the future of SAAS sales jobs for recruiting agencies?

06:10.320 --> 06:19.200
Oh, sales jobs and for recruiting agencies. So I mean, the sales level is still very human.

06:19.200 --> 06:23.360
So sales is not going to change for a while. Ditto for recruiting, although there's a lot

06:23.360 --> 06:28.880
of AI and recruiting already where like AI will read your resume and AI will watch a video of you

06:28.880 --> 06:35.840
answering questions. You know, more and more of that's coming, but that's about it. Let's see.

06:35.840 --> 06:41.760
All right. We got a whole bunch of questions coming in on the Patreon side. So, yeah, database,

06:41.760 --> 06:48.640
short answer, diversify your job skills. So there's that. Let's see. Zoom in a little so you

06:48.640 --> 06:55.120
guys can read this. Let me jump back over here real quick. But do you still expect AGI within 18

06:55.120 --> 06:59.280
months was stating they do not want to make models bigger but rather more efficient? Yeah,

06:59.280 --> 07:06.960
I think AGI in 18 months is still conservative. Honestly, I think that we will have,

07:08.320 --> 07:13.680
as people develop the architecture for autonomous AI, I think that we will be able to say that we

07:13.680 --> 07:19.040
have AGI by the end of this year. But people will realize that it's like, okay, we have an agent that

07:19.040 --> 07:23.760
can do anything, but it's expensive or it's slow or it's kind of dumb, that kind of thing.

07:25.840 --> 07:32.000
Let's see. Check on Discord real quick. Is her about to become reality? Yeah, lots of people

07:32.000 --> 07:42.480
are working on AI companions. They're going to get more sophisticated real fast. I've actually got

07:42.480 --> 07:48.160
a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday.

07:48.160 --> 07:52.320
I've got a Ghost in the Shell video planned. I've got a Mass Effect video planned. I've got

07:52.480 --> 07:58.320
a Dow and Blockchain video planned. So that's all what's coming. Maybe I shouldn't spoil it. Okay,

07:58.320 --> 08:05.760
well, whatever. But yeah, so I was thinking about hitting on her and Ex Machina and stuff as well.

08:07.280 --> 08:10.960
Let's see. How can we be certain things are advancing exponentially?

08:12.480 --> 08:20.320
So that's a good question, James. Generally speaking, you can't tell if you have a narrow

08:20.320 --> 08:26.240
window, but we were joking around the other day and we're pointing out that like a few weeks ago,

08:26.240 --> 08:33.040
it was like you would reasonably expect a couple of cool AI bits of news per week,

08:33.040 --> 08:38.560
and now we're at the point where we expect several per day. Now, it might come in cycles,

08:38.560 --> 08:46.800
it might come in waves, but generally speaking, this very closely matches what Ray Kurzweil

08:47.760 --> 08:54.720
said, or maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago.

08:54.720 --> 09:00.720
I think it was Michio. He was describing what it will feel like to approach the singularity,

09:00.720 --> 09:07.040
and he said, oh, well, when information is doubling every two years, you don't really feel that on a

09:07.040 --> 09:14.080
day-to-day basis. And then when information is doubling every six months, that's fast enough

09:14.160 --> 09:18.640
that you're like, oh, hey, this thing that I didn't think would be solved for another couple years

09:18.640 --> 09:25.280
was solved this year. And then, but as it ramps up faster and faster, that time keeps having.

09:25.280 --> 09:32.240
And so then three months after that, you realize, oh, wait, we've advanced again. And I think we're

09:32.240 --> 09:38.080
right at that, like, that three, you know, where six months ago, we're like, oh, this stuff is 10

09:38.080 --> 09:42.640
years away. Six months later, we're like, this stuff is 10 months away. So I think that where

09:42.640 --> 09:47.040
you could make a good argument that we're in some respects, we're in the exponential ramp up right

09:47.040 --> 09:53.360
now. That being said, some of this information is so big, and it's changing so fast, it's difficult

09:53.360 --> 09:58.960
to measure. We'll actually need AI to measure the rate of papers and tools. All right, so Seaf,

09:58.960 --> 10:04.320
you said, how do you think AI will impact religion, particularly monotheistic religions? I think it

10:04.320 --> 10:09.360
will create a mass crisis of consciousness, which will make the transition period even more chaotic

10:09.360 --> 10:19.120
and extreme. Yeah, no, sorry, Seaf, I was getting around to it. Yeah, so I've actually had some

10:19.120 --> 10:24.080
interesting internet debates with more conservative and more religious people. Granted, I don't do

10:24.080 --> 10:28.560
internet debates anymore. I got that out of my system. And I try not to get suckered into it,

10:29.680 --> 10:37.680
if I can. But, you know, one debate that I had many years ago was, you know, if aliens showed up,

10:37.680 --> 10:43.360
wouldn't it prove your religion wrong? This was a debate. I wasn't arguing as a debate that I

10:43.360 --> 10:50.240
observed. And the religious person said, no, why would it? And, you know, they rationalized it,

10:50.240 --> 10:56.640
saying, well, you know, why would God put, you know, aliens in the Bible if we wouldn't be able

10:56.640 --> 11:03.600
to understand it back then? It's not for us to understand. And so some religious folks do have

11:03.600 --> 11:10.560
a really good ability to compartmentalize. And so, like, just because you have a, like,

11:11.920 --> 11:16.720
like a super intelligent machine, some people would be like, so it doesn't have a soul. And

11:16.720 --> 11:22.160
that's the end of the discussion. So I don't, I don't particularly perceive, and I'm not saying

11:22.160 --> 11:28.000
that this is good or bad, right? I am not in the Judeo-Christian faith. I have my spirituality

11:28.080 --> 11:34.160
as other, other places. I have some, like, some of my best friends in my local community and my

11:34.160 --> 11:39.600
internet friends are deeply religious, you know, followers of Christ and whatever. And so, like,

11:39.600 --> 11:43.120
in many cases, I don't think it's going to be that big of an issue. Let me jump over to the

11:43.120 --> 11:49.600
Patreon. Oh, wow, we've got some questions here. Okay. Backo bbzo. Sorry if I'm saying your name

11:49.600 --> 12:01.280
wrong. What would your advice be to someone just launching an AI startup? Don't. That's,

12:01.280 --> 12:07.760
that's a very flippant response. But one launching a startup is really hard. It's mostly tedium.

12:08.640 --> 12:12.720
You can have the best idea in the world. And 90% of the work is still going to be,

12:14.800 --> 12:19.040
excuse me, I'm still struggling with allergies. That's my head a headache earlier. That's part

12:19.040 --> 12:24.640
of why I canceled yesterday. Thanks, Jordan. That's how am I holding up? Actually, I'm doing okay.

12:24.640 --> 12:30.240
Mostly it's just allergies right now. But anyways, so if you're doing an AI startup,

12:30.240 --> 12:36.880
one, if you haven't done a startup before, now is a really bad time to learn. Because things

12:36.880 --> 12:45.520
are going so fast. And we're basically having to reinvent stuff as we go. Which if that's, you

12:46.160 --> 12:50.880
that's part of why I burned out is I realized like, okay, I did find it really engaging and

12:50.880 --> 12:58.640
really enjoyable. But my pace of things clashed with other people. And then like the rabbit hole

12:58.640 --> 13:07.120
just keeps getting deeper. So that's, that's kind of the thing. What I always tell people is your,

13:07.120 --> 13:12.880
your, your startup team, your founder team is most important. Be picky. If you don't have the right

13:12.880 --> 13:20.560
team, walk away early. And then also for the folks that I'm talking to is local. Because of the pace

13:20.560 --> 13:26.880
of things, you absolutely need to like see the people that you work with, in person, at least

13:26.880 --> 13:32.560
on a weekly basis, if not on a daily basis, because you need to be like sitting in the same room,

13:32.560 --> 13:39.600
just shooting the breeze, keeping each other updated in real time. Doing it remotely is

13:39.600 --> 13:44.960
probably not feasible. Unless unless you're already a really established team, and you're

13:44.960 --> 13:50.480
just going to sit in like discord all day or Slack all day. Blake Allen curious to hear your

13:50.480 --> 13:54.240
thoughts on Stanford's hyena hierarchy and how it relates to some of the work you've done with

13:54.240 --> 13:58.880
Raven and cognitive architecture. I don't know if I've heard of this one. Let's check it out real

13:58.880 --> 14:12.320
quick. Hyena hierarchy. Let's see. Let's go up to the very top. Hyena hierarchy towards larger

14:12.320 --> 14:18.800
convolutional language models. We're excited to share our latest work on hyena, a sub quadratic

14:18.800 --> 14:24.080
time layer that has the potentials to significantly increase context length and sequence models.

14:24.400 --> 14:34.640
Oh, right. I think this is the RNN integration. Yeah, yeah. In general, how can we close this

14:34.640 --> 14:40.480
gap? Yeah. In general, any individual language model is just like one cortical node.

14:43.600 --> 14:48.960
Yes, these things will be like better, more efficient cylinders in an engine, but in order

14:48.960 --> 14:57.120
to have a race car, you need the rest of the car. Again, I'm kind of really flying off the cuff

14:57.120 --> 15:06.240
right here. I'm not sure that I've got this right, but I think, yeah, RWKV. You're not going to ever

15:06.240 --> 15:11.440
get a full cognitive architecture from a single language model. Now, that being said, the big

15:11.440 --> 15:17.280
asterisk is when you look at all the studies about GPT-4 that have theory of mind and what I

15:17.280 --> 15:23.040
call implied cognition. So implied cognition is that the thing is obviously thinking through

15:23.040 --> 15:27.920
problems behind the scenes in a similar way that humans think through it. I don't mean like

15:27.920 --> 15:35.280
neurologically, subjectively, it thinks the way that we do, but GPT-4 can obviously talk itself

15:35.280 --> 15:43.520
through, kind of do chain of thought reasoning internally in one shot. And so that makes those

15:43.520 --> 15:48.320
larger, more sophisticated models make your cognitive architecture simpler, but it doesn't

15:48.320 --> 15:54.640
get rid of the need for external storage. It doesn't get rid of the need for parallel processing.

15:54.640 --> 15:59.280
It doesn't get rid of the need for loops and checks and that sort of stuff. So that's kind

15:59.280 --> 16:05.920
of my response there. Good question. Emma or AMA, I'm new here and new to this field in general,

16:05.920 --> 16:11.120
found you through Raven videos. Thank you. Regarding personal assistance, is there a reason

16:11.120 --> 16:17.280
to create a database of yourself for your future personal assistant to understand you better?

16:18.240 --> 16:24.880
So that's actually the purpose of my RIMO framework. So RIMO is meant to be a hierarchical

16:24.880 --> 16:32.000
database of your interactions with an individual agent that will surface particular topics by

16:32.000 --> 16:41.920
using, not reciprocal, recursive summarization and clustering. So you take all your raw logs,

16:41.920 --> 16:48.240
cluster them, summarize them, do that again, cluster, summarize, cluster, summarize until

16:48.240 --> 16:54.720
you end up with five to 10 parent topics that allow you to drill down. So I wouldn't, don't waste

16:54.720 --> 17:00.000
any time doing that manually, just let it happen naturally through conversation by integrating

17:00.000 --> 17:06.240
something like Lama Index Tree or RIMO. Do you think we are on track to cure brain diseases

17:06.240 --> 17:14.640
like Alzheimer's by 2030? The combination of AlphaFold and mRNA vaccines, I think absolutely.

17:15.280 --> 17:19.520
There was something else that I posted on my YouTube recently that it's like another breakthrough

17:19.520 --> 17:28.960
is happening. So I think we're very close to the point where we can halt Alzheimer's. Undoing

17:29.040 --> 17:34.560
Alzheimer's might take another little bit of time, but on the other hand, we're at the point

17:34.560 --> 17:39.360
where we're getting saltatory leaps, we're getting breakthroughs really fast, so you never know.

17:40.320 --> 17:46.160
Let's see, what are your thoughts on the generative agent stuff that has come out recently? It seems

17:46.160 --> 17:52.320
like you were pretty ahead of the curve on that stuff and has it solidified or changed the way

17:52.320 --> 17:58.640
you think about the concepts from Symphony of Thought? Yeah, so I definitely felt like I was

17:58.640 --> 18:04.640
ahead of the curve. And what I've been telling people is I worked for a few years to try and get

18:05.440 --> 18:11.760
GPT-3 to do the stuff that 3.5 and 4 can do easily. So I'm just like, all right, whatever. I'm so

18:11.760 --> 18:15.680
glad that the rest of the world is just like, oh cool, autonomous agents. And I'm like, great,

18:15.680 --> 18:21.440
now I don't have to write any more books about it. So I'm just happy to sit back and watch it go

18:21.440 --> 18:27.600
and keep plugging my heuristic imperatives. Good question, Jordan. Also, here, let's check over

18:27.600 --> 18:33.680
here. Okay, we got some questions. Let's see, is her about to come reality? Yep, we got that one.

18:34.720 --> 18:39.440
Interesting video. How long do you think it will be before we start seeing hive mind AI systems

18:39.440 --> 18:50.400
in healthcare or the IRS? I know people working on that today. And so they'll work together

18:50.400 --> 18:55.760
for a few different reasons. One, you'll have a division of labor. Oh, so taking a step back.

18:56.480 --> 19:01.600
What we mean when we say like hive mind AI is where you have like multiple cognitive agents

19:01.600 --> 19:10.400
or autonomous agents, or is it is it not working? Is it working? I hope it's working. It looks like

19:10.400 --> 19:19.520
it's working. Okay. So yeah, so basically, it'll be easy to spin up a lot of agents.

19:20.240 --> 19:28.560
What I was describing to one Patreon customer, or no, sorry, I was describing this to a

19:29.600 --> 19:35.200
podcast host that I'm going to be featured on coming up, was kind of what I predict right now

19:35.920 --> 19:41.440
is before too long, you're going to have multiple cognitive agents running on your phone, on your

19:41.440 --> 19:48.000
car, on your home PC and your smart home devices. And so you're basically going to have a fleet of

19:48.080 --> 19:56.960
small cognitive agents working for you at all times. Then you're going to have the same thing at

19:56.960 --> 20:01.280
like your company, right? Every employee or every department is going to have multiple

20:01.280 --> 20:07.200
cognitive agents all collaborating at all times. And you're going to have this kind of tiered hierarchy

20:07.200 --> 20:11.600
where it's like there's the personal, there's the family unit, there's the corporate unit,

20:11.600 --> 20:16.400
there's the town, there's the federal government, the state government, global government,

20:16.480 --> 20:20.080
government. And I think that the way that they're all going to work together, because

20:20.080 --> 20:24.720
security is so critical here, is that it's going to be using blockchain technology and

20:24.720 --> 20:30.800
distributed autonomous organizations. So that's the long story short, is that's what's going to happen.

20:32.400 --> 20:36.240
Let's see, you may have already covered this, but in case you haven't any thoughts on Met

20:36.240 --> 20:43.120
Singer's artificial suffering, an argument for a global moratorium on synthetic phenomenology.

20:43.600 --> 20:51.360
Um, I am tangentially familiar with this, but I have my own opinions on whether or not a machine

20:51.360 --> 20:59.520
can suffer. So there's two distinct possibilities. The first possibility is because artificial

20:59.520 --> 21:05.120
intelligence is a fundamentally different substrate from humans, it will never be able to

21:05.120 --> 21:11.520
suffer. Like it didn't involve nerves, it doesn't have pain centers, so on and so forth, they can't

21:11.600 --> 21:19.600
feel lonely because it's not a social entity, so on and so forth. Now that being said, language,

21:20.240 --> 21:25.360
the acquisition of language is actually critical for the development of human consciousness.

21:26.720 --> 21:32.000
So for instance, Bruce Willis, who has aphasia, aphasia means that your ability to use language

21:32.000 --> 21:37.200
gets destroyed. Aphasia actually kind of erases your sense of consciousness.

21:37.760 --> 21:45.120
Um, and then, uh, in the case of feral children, um, feral children, when they, some of them who

21:45.120 --> 21:49.840
have learned language talked about how their consciousness and their understanding of time

21:49.840 --> 21:57.920
in themselves changed as they learned language. So if you extrapolate that to language models,

21:57.920 --> 22:05.360
it is possible that, that there is something informationally almost magical about the acquisition

22:05.360 --> 22:12.240
of language that confers consciousness, that confers subjective experience of being. So that

22:12.240 --> 22:18.320
could be that language models are actually the first AI that have subjective experience, that have

22:18.320 --> 22:26.480
a coherent, um, sense of being. And this is, so there are, um, there are some religious and

22:26.480 --> 22:32.160
spiritual frameworks that kind of discuss stuff like this, um, particularly, uh, what's the name

22:32.160 --> 22:38.640
of the, the, the creator deity in Tolkien's world, where the fundamental substrate of reality was

22:38.640 --> 22:44.160
music, right? But maybe the fundamental substrate of consciousness is actually language. Um, so we

22:44.160 --> 22:53.600
don't, we don't know yet, but that's, it's a possibility. Um, let's see. Uh, so Parkinson,

22:53.600 --> 22:59.760
so the follow-up question was, or here, let me check the Patreon real quick. Um, let's see,

22:59.760 --> 23:03.440
do you think there needs to be another breakthrough for AGI? What is your personal take on this?

23:03.440 --> 23:10.560
Do LLM suffice? Um, yeah. So I would say that, that our current trajectory, as long as the

23:10.560 --> 23:15.840
trend continues, we are on track for AGI. Um, people are going to continue debating about AGI

23:15.840 --> 23:22.160
forever though, until the cows come home. Um, which is why I keep saying like autonomous cognitive

23:22.160 --> 23:27.760
entity, ACE, or just autonomous AI, because it doesn't, you don't need AGI. You don't need some

23:27.760 --> 23:35.440
arbitrary magical boogeyman. All you need is like an AI system that is self-contained and

23:35.440 --> 23:40.560
autonomous enough to be useful or dangerous. Um, and then the question is, how many do you have?

23:40.560 --> 23:44.080
How fast are they? And how smart are they? And they're going to continue to get faster,

23:44.080 --> 23:48.640
cheaper, and smarter over time. So it's like, okay, we're there. It's just a matter of

23:49.440 --> 23:53.760
how does, how does the trend line ramp up, right? Cause it's kind of like, um, when the

23:53.760 --> 23:58.080
Wright brothers first created the Wright flyer, right? You know, it's like, okay, you had to

23:58.080 --> 24:03.120
start it by hand and push it, you know, down a track and it flew well, like 200 feet or 300 feet.

24:03.120 --> 24:07.280
And people are like, ah, whatever that won't be useful. But then 50 years later, we were flying

24:07.280 --> 24:13.680
to space, right? So we're at the beginning of the ramp up of, of the era of AGI. And yeah,

24:13.680 --> 24:17.920
right now they're like idiotic little toddlers, but in a few years they're going to be like

24:18.560 --> 24:24.480
one all over the place and two really powerful. Um, good question. Let me come back over here.

24:25.680 --> 24:30.000
Um, you should do a whole episode on aphasia and consciousness. I actually don't know that

24:30.000 --> 24:34.960
much about it. Um, but if you're interested in the topic, I recommend phantoms in the brain,

24:36.080 --> 24:42.960
by, um, VS Ramachandran. And also what's the name of his other book? Um, it's something like

24:42.960 --> 24:51.280
the pursuit of what makes humans human. Um, okay. Uh, let's see. So Parkinson's is a neurodegenerative

24:51.280 --> 24:56.560
disease, which I think means that it's autoimmune or it's a, or it's a defective protein. Um, but

24:56.560 --> 25:01.360
also Alzheimer's is a defective protein. So while these diseases seem very complicated,

25:01.360 --> 25:06.240
the fundamental mechanisms are actually relatively straightforward. Um, and I know that there's

25:06.240 --> 25:10.960
probably a bunch of researchers that are going to jump on me for that. But, um, like plaques that

25:10.960 --> 25:16.160
accumulate on the brain for Alzheimer's in most people, those plaques are cleared out. So then

25:16.160 --> 25:20.160
it's just a matter of figuring out like, okay, why? Um, and then of course there's confounding

25:20.160 --> 25:26.880
factors like things like your, uh, gut inflammation, uh, microbiome and other things affect Alzheimer's.

25:26.880 --> 25:31.360
But that's because of the gut brain access. And again, I don't want to oversimplify because if

25:31.360 --> 25:37.360
you look up like human, uh, metabolic pathways, there's like 200,000 unique proteins and enzymes

25:37.440 --> 25:42.880
in the body with built literally billions of combinations of reactions. So I might be,

25:42.880 --> 25:47.120
it might sound like I'm oversimplifying, but I'm, I'm, I'm not saying that it's that simple. I'm

25:47.120 --> 25:53.520
just saying that, that the, that the, the key mechanism for most diseases is relatively simple

25:53.520 --> 25:57.840
once you understand it. And we're getting close to that understanding. I guess that's the short

25:57.840 --> 26:02.480
version of what I'm trying to say. Um, let's see, when you explained why you canceled the

26:02.480 --> 26:05.920
OSS Raven project, you mentioned that there were some fundamental things missing.

26:06.640 --> 26:12.000
Um, can you say what was missing and what might change your, or what made you change your mind?

26:12.560 --> 26:20.560
Um, so my open source Raven project was like just before Lang chain and, and auto GPT and,

26:20.560 --> 26:25.200
and all those other things came out. Um, and so as those came ramped up, I was like, I don't

26:25.200 --> 26:31.120
really feel the need to continue. Um, but from a, from a social and organizational perspective,

26:31.120 --> 26:36.480
the biggest thing that was missing was gatekeeping. Um, I, I basically, I created a community that was

26:36.480 --> 26:41.600
really good at discussing stuff and not doing stuff. Um, and that's not anyone's fault. That's

26:41.600 --> 26:46.800
if any, if there's anyone to blame, it's me. Um, just because I was like, I was so focused on

26:46.800 --> 26:50.720
consensus and not just like, okay, let's just get stuff done. And then I see these other folks

26:50.720 --> 26:55.680
that are just getting stuff done. And I'm like, okay, I'll just pass the torch. Um, let's see.

26:56.560 --> 27:03.440
I think if we gave GPT for Scarlett Johansson's voice and a robot body, the masses will begin

27:03.440 --> 27:08.400
to realize how close we are to AGI. Yeah, that's one way of putting it drink some water.

27:12.960 --> 27:17.920
Let's see at what point do creating NPC and using autonomous AI like auto G auto GPT

27:18.640 --> 27:23.680
and the likes become immoral, especially if you put them in games like GTA. I don't know that,

27:23.760 --> 27:31.040
that it intrinsically does. Um, you know, not intrinsically immoral, but like certainly with

27:31.040 --> 27:39.760
any technology, you can do it dangerously. Um, let's see. I've been curious about the future of

27:39.760 --> 27:46.080
entertainment. Oh, sorry, let me jump over to Patreon real quick. Let's see. Do you have an

27:46.080 --> 27:50.560
overarching roadmap of how to ensure the successful propagation of the heuristic

27:50.640 --> 27:55.760
imperatives? If so, what can we all do to help you get to your milestone? That is a great question,

27:55.760 --> 28:02.080
Blake. Um, so you're actually looking at it. So my number, my number one thing is my YouTube

28:02.080 --> 28:08.880
channel. Um, because like, yeah, I've got enough expertise and, you know, IT and systems engineering

28:08.880 --> 28:14.160
and enterprise. I've demonstrated enough understanding of language technology and AI and

28:14.160 --> 28:18.720
cognitive architecture that I've got at least a little credibility. Um, certainly if you read

28:18.720 --> 28:23.840
all the comments on YouTube, some people, uh, don't believe anything that I say and that's

28:23.840 --> 28:29.520
fine. That's the internet for you. Um, but anyways, so basically step one was YouTube. That's why I

28:29.520 --> 28:34.720
started my YouTube channel is because I realized that I needed to propagate my work. Um, step two

28:35.520 --> 28:41.840
is, uh, teaching people. Um, and so by teaching people, that's like, you know, I've got a few

28:41.840 --> 28:48.880
papers. I've got some code demonstrations. Um, I work with my Patreon, uh, supporters. Uh, I work

28:48.880 --> 28:54.320
with pretty much anyone who wants to, and then three further dissemination. So like the podcast

28:54.320 --> 28:58.640
that I'm coming up on, one of the things that we're going to talk about is alignment and the control

28:58.640 --> 29:04.320
problem. We're going to be talking about like Nash equilibrium, game theory, Molok, that sort of stuff.

29:04.320 --> 29:09.520
And so just by having the conversation and propagating the idea, that's like step three.

29:09.520 --> 29:18.320
Step four is actually my novel because, uh, actually most of what I came up with was, uh,

29:18.320 --> 29:25.120
in terms of cognitive architecture, heuristic imperatives was done in part through explorations

29:25.120 --> 29:30.720
and fiction. And so over the last four years, what I've done is I, I do some experiments

29:30.720 --> 29:33.920
and that would inspire me and I'd go write more of my novel and then I'd, you know, get

29:33.920 --> 29:38.880
tired of my novel and do more experiments and I'd go back and forth until one, my novel took on a

29:38.880 --> 29:44.400
life of its own, but also my research took on a life of its own. Um, but there's a video that came

29:44.400 --> 29:54.720
out recently called, let me see if I can find it. It was, um, like why we need utopia. Um, here we go.

29:54.720 --> 30:01.120
It was our changing climate. So this is a little bit of a, um, uh, I don't agree with everything

30:01.120 --> 30:06.880
that this, that this, uh, you, this channel says, but it will make you think. Um, so this video,

30:06.880 --> 30:13.280
why we need utopias, um, actually talks about how, how valuable stories can be

30:14.000 --> 30:21.040
in communicating ideas because stories are naturally how we communicate philosophy and morals.

30:22.240 --> 30:28.400
We don't need, we don't like philosophy, like capital P philosophy from, from universities.

30:28.400 --> 30:35.040
That's, that's backwards throughout almost all of human history. We communicate our fears and our

30:35.040 --> 30:40.240
desires and our values through stories. And so that's what this video talks about. And so when

30:40.240 --> 30:46.160
you have nothing but dystopian cyberpunk stuff, you end up with people like Eliad Zyrcikowski.

30:46.160 --> 30:51.360
Um, you know, yeah, I'm throwing some shade. But anyways, when that's all that you consume,

30:51.360 --> 30:56.480
that's all you think, that's all you feel, and that's all you believe. So, um, my novel, which

30:56.480 --> 31:01.760
I'm actually just about to finish draft 12 tomorrow morning, I'm writing the last chapter

31:01.840 --> 31:08.960
and then I'm polishing it up, um, will illustrate, um, a lot of stuff, not just

31:08.960 --> 31:12.880
the core objective functions or puristic imperatives. So that was a long winded answer.

31:13.840 --> 31:20.320
Um, let's see. I think there is, um, are there key channels this training needs to go into,

31:20.320 --> 31:27.680
organizations, governments? Um, I think, I think right now Blake, um, it's mostly just a matter of,

31:27.760 --> 31:34.160
of dissemination, but also experimentation. So a lot of people, um, have experimented with

31:34.160 --> 31:38.880
incorporating heuristic imperatives into autonomous and semi-autonomous stuff. And

31:39.840 --> 31:44.640
most of them aren't sharing it yet, which that's fine. It's their prerogative. Um, but certainly

31:44.640 --> 31:47.440
some people have reached out and said, like, yeah, this made everything easier. So I'm like,

31:47.440 --> 31:57.280
great, just tell your friends. Um, let's see. Okay. Let's come back over here. Um, let's see.

31:57.280 --> 32:02.640
I've been really curious about the future of entertainment. When we can use AI to generate

32:02.640 --> 32:07.760
movies, games, et cetera. Uh, what will the entertainment industry look like? Movie trailers

32:07.760 --> 32:12.560
and hyping up big releases for months will be irrelevant when AI can instantly create something.

32:12.560 --> 32:16.880
If someone created a movie you didn't like, you just ask your AI to recreate it with an

32:16.880 --> 32:22.000
ending or plot more suited to your tastes. What happens with content creators at that point forward?

32:22.960 --> 32:29.120
Yeah. So I think that you're onto something. Uh, now that being said, it'll be easier for a lot of

32:29.120 --> 32:37.280
people like you and me to create whatever film, TV, music, whatever we want, um, with the help of AI,

32:37.280 --> 32:41.920
especially when you look at the text of video, um, which is improving by leaps and bounds.

32:41.920 --> 32:46.480
You know, like I always, my, my go-to joke is we'll finally get season two of Firefly.

32:46.480 --> 32:50.240
Who knows, we might get season two of Firefly by the end of this year. That would be great.

32:50.960 --> 32:55.680
Um, now the problem there, it's not really a problem, but just taking that to a logical

32:55.680 --> 33:00.000
conclusion. What if you have a million different versions of season two of Firefly? How do you

33:00.000 --> 33:05.280
pick which one to watch? Right? You can look at ratings and stuff. Um, but then it also begs the

33:05.280 --> 33:12.640
question of like IP, like is, uh, you know, 20th century Fox or whoever owns the IP for Firefly,

33:12.640 --> 33:18.320
are they going to sue to have all of them shut down? You know, I, I like the dude from, uh,

33:18.320 --> 33:23.200
from the movie, you can't stop the signal now. So I don't know what's going to happen there.

33:23.920 --> 33:29.840
Um, but, uh, what I do think is that when you look at the fact that like people are already

33:29.840 --> 33:35.280
using like Emma Watson's face for every mid journey prompt and, and whoever else, um,

33:35.280 --> 33:39.920
I think that the crop of actors that we have today are basically going to be around forever.

33:39.920 --> 33:46.480
Right? You're going to be watching, uh, Brad Pitt and Jennifer Aniston and, and, and Tom Cruz

33:46.480 --> 33:51.360
for literally the next, like several centuries, at least until some actor comes along who's even

33:51.360 --> 33:57.600
more compelling and whatever. Uh, and that'll be through face cloning, voice cloning, even, um,

33:57.600 --> 34:03.440
you know, uh, nerfs, the, uh, the neural represent, uh, uh, representation. What was it? Neural

34:03.440 --> 34:09.680
radiance fields, neural radiance fields. Um, we'll be able to like copy everyone. Um, okay,

34:09.680 --> 34:15.440
could learning, uh, let me zoom in a little bit. Uh, could learning language and triggering

34:15.520 --> 34:20.480
consciousness in humans almost replicate the same phase change when seen, um, or seen when

34:20.480 --> 34:25.440
induction heads spontaneously form two plus layer models during training. Obviously there's more

34:25.440 --> 34:30.480
to humans, but perhaps that's the mechanism. Uh, yeah, that's kind of what I was mentioning earlier.

34:30.480 --> 34:38.720
Um, and I wouldn't be surprised if once language models get, uh, large enough if, um, if we do see

34:38.720 --> 34:42.960
some more convergence. Um, that being said, I'm not going to say that that automatically means

34:42.960 --> 34:48.320
that it has a subjective experience and that it is suffering, but you know, our brains evolved

34:48.320 --> 34:54.240
over billions of years to be efficient. Um, basically efficient processors of information.

34:54.240 --> 35:00.640
Who's, who's to say that if you have a, a biomimetic machine that it won't also converge on some of

35:00.640 --> 35:06.560
the same properties and behaviors. Um, let's see, what do you think it will take to get for the

35:06.560 --> 35:12.000
naysayers to get on board? The tone around AI seems to have shifted towards chat GPT and GPT-4

35:12.000 --> 35:18.000
aren't anything special. Oh, you know, that, that always happens when the new shiny wears off.

35:18.000 --> 35:25.200
Um, but the long-term economic impact of chat GPT has not been realized yet. And when chat GPT

35:25.200 --> 35:30.320
and GPT-4 are on the ramp up, one, there's going to be a lot of competitors and two, there's going

35:30.320 --> 35:36.560
to be incremental improvements and people are going to be like, uh, okay. The title, it's like,

35:36.560 --> 35:40.720
it's like when you watch the, the tsunami come in and that just the water just keeps getting higher

35:40.720 --> 35:45.920
and faster for like hours. That's what AI is going to feel like, except instead of hours,

35:45.920 --> 35:51.680
it's going to be days and weeks. Um, let's see. I cannot wait when we can use deep dive tech

35:51.680 --> 35:55.840
and have virtual realities. Will it also be possible to take super intelligent animals like

35:55.840 --> 36:03.120
dolphins, dogs, parrots and crows and a deep dive and play with them. Um, I don't know that it would

36:03.120 --> 36:07.680
be possible, but I certainly think it probably wouldn't be ethical. Now that being said, you

36:07.680 --> 36:16.640
could have a virtual dolphin that is hyper realistic that you can play with. Um, fun thought,

36:16.640 --> 36:23.840
will AGI want to see more stories from humans as a goal for itself? Uh, if, oh, so let, let me,

36:23.840 --> 36:31.120
let me plug this. So Elon Musk went on of all fricking shows, Tucker Carlson and talked about

36:31.120 --> 36:38.640
truth GPT. So what he said was that truth GPT would be a maximum truth seeking AI. Okay, great.

36:39.520 --> 36:45.280
But after listening to it in closer detail, I realized what he was talking about was the third

36:45.280 --> 36:51.040
here is to comparative was to increase its understanding or to maximize its own understanding.

36:51.040 --> 36:57.600
So there's actually nothing that function on its own could lead to some really catastrophic

36:58.080 --> 37:03.280
sources. But it's a step in the right direction. And I'm really glad that someone with as big of

37:03.280 --> 37:07.360
a platform as Elon Musk is talking about maximize understanding or increase understanding.

37:08.560 --> 37:13.920
So that being said, one of the things that he said in that interview was that as since humans

37:13.920 --> 37:18.240
are part of the universe and AI that is curious about the universe will intrinsically be curious

37:18.240 --> 37:23.200
about us as well. Now that being said, humans sometimes do experiments on things that we're

37:23.200 --> 37:27.360
curious about. So maybe that's not the best thing. And in my book, benevolent by design,

37:27.360 --> 37:33.200
I talk about why you don't why you must include suffering or something like suffering in the

37:33.200 --> 37:38.080
objective functions of an AI, because there's three dispositions that an AI can have towards

37:38.080 --> 37:44.320
suffering. One is it can ignore it altogether. So if Elon Musk gets his current idea, which

37:44.320 --> 37:49.360
is just maximize for truth, that is an agent that ignores suffering, it doesn't care one way or another.

37:50.320 --> 37:55.760
Then you can have one that increases suffering that deliberately increases suffering and we

37:55.760 --> 38:01.760
absolutely don't want that. So that leaves by process of elimination, you want an AI that

38:01.760 --> 38:07.440
reduces suffering. It's really that simple. Now that being said, I do agree with Elon that

38:08.160 --> 38:13.840
creating a curious agent is a good idea because it'll want to know about us. And if you exterminate

38:13.840 --> 38:20.240
humans, you have a hard time learning about them. So let's see, let me check on Patreon real quick.

38:24.080 --> 38:27.520
Do you think that Elon Musk wants to be the Rupert Murdoch of AI?

38:29.760 --> 38:33.360
Okay, I don't like that question. Lance, why you got to do this to me?

38:34.320 --> 38:43.840
All right, Zadre, I'm not sure how to pronounce or Hadre. Okay. How do you envision the role of AI

38:43.840 --> 38:48.480
in healthcare, particularly in areas like diagnostics and personalized medicine? What are

38:48.480 --> 38:53.120
some of the challenges and opportunities in this field? Well, so there was that stand for doctor

38:53.120 --> 38:58.400
who who already went on record saying that chat GBT for has better clinical judgment than

38:58.720 --> 39:08.240
many doctors. So that is just a start, right? That's that's like starting point day one.

39:08.240 --> 39:14.400
What happens when chat GPT five, six and seven come out that have better clinical judgment than

39:14.400 --> 39:21.280
99.999% of all doctors on the planet, right? It doesn't make sense to go to a human doctor anymore.

39:22.000 --> 39:29.040
Right? If the if the machine that costs $20 a month to run is better than all human doctors,

39:29.040 --> 39:35.680
why go to a human doctor? Now that being said, there's probably going to be approvals and downsides

39:35.680 --> 39:40.400
and gaps. And then there's still also the interface with the patient. And you have to have like

39:40.400 --> 39:47.680
phlebotomists and nurses and and and physicians assistants to administer things, to administer

39:47.680 --> 39:52.560
tests, you still need the you still need a lot of humans in there to to be the interface between

39:52.560 --> 39:58.560
the human and the machine. But that being said, I think that we will get to a point very quickly

39:58.560 --> 40:05.200
where the quality of care and the speed of care and the efficiency of care are going to go through

40:05.200 --> 40:11.600
the roof real fast. That's what I'm hoping at least. Alright, jumping back over to cognitive AI

40:11.600 --> 40:16.320
lab. Oh, we got some new questions. It looks like this was the same question. Sorry, I missed you

40:16.320 --> 40:25.040
over there. Where are we? Alright, there's the deep dive. Do you think there is any major leap

40:25.040 --> 40:30.320
missing to make truly practical autonomous agents? So for example, one who runs a part of your business

40:30.320 --> 40:36.480
serves as general assistant, etc, etc. No, there's there's our there are countless hundreds,

40:37.280 --> 40:42.320
if not thousands or even millions of people working on semi autonomous and autonomous corporate

40:42.320 --> 40:50.720
applications today, right now. That being said, there's there's no there's no breakthroughs that

40:50.720 --> 40:55.280
are needed, but there are still problems to be solved. So that's why like RIMO, you know, the

40:55.280 --> 41:01.600
memory systems, and then standard practices like, you know, I wrote in Symphony of Thought and in

41:01.600 --> 41:07.760
other places, my atom framework is once something is autonomous or semi autonomous, how does it

41:07.760 --> 41:13.600
keep track of product or projects and tasks? And that's something that people are working on.

41:13.600 --> 41:19.760
People are working on it real fast. That's coming really quick. Let's see, Nathan says,

41:19.760 --> 41:24.320
I've been taking screenshots of when friends and family make fun of my hot takes. So I have the

41:24.320 --> 41:33.280
receipts. I would say that I'm above being that petty. But yeah, thank you for keeping receipts.

41:34.080 --> 41:38.960
Um, let's see, maybe directors will just design their perfect actors for each role.

41:40.240 --> 41:44.320
So one thing that's going to happen is actually, so this is going back to like,

41:44.880 --> 41:50.000
entertainment. I think that the next big generation of entertainment is actually going to be

41:50.720 --> 41:59.440
holodeck style VR stories, where nothing is scripted, where instead it's like, you know,

41:59.440 --> 42:03.920
basically you design a holodeck program the same way that they do in Star Trek, which is like,

42:03.920 --> 42:10.480
computer, give me, you know, a Mad Max style story. But instead of, you know, post apocalyptic,

42:10.480 --> 42:16.480
it's actually like space Western. So like, give me a mashup of firefly and this and,

42:16.480 --> 42:22.480
you know, make the protagonist, you know, or the, you know, I'm the protagonist and give me a team

42:22.480 --> 42:27.840
of like, you know, give me the sexy sidekick and the cyborg friend and whatever. And then just a

42:27.840 --> 42:32.320
way it goes, right? Because you could plug what I just literally you could plug what I just said

42:32.320 --> 42:38.320
into chat GPT and it can tell you a story. And I think that I think that VR makes the most sense

42:38.320 --> 42:45.600
for the most immersive aspects of that. And, and then I think that because here's the other thing

42:45.600 --> 42:51.280
is that technology changes the way that we consume art, but it doesn't really change art itself,

42:51.280 --> 42:57.360
right? There are still stage actors, right, even though there's film and TV. There are still

42:57.360 --> 43:02.400
symphony orchestras, even though I can just, you know, bring up Spotify and listen to the

43:02.400 --> 43:08.160
same recording that was recorded back in the 80s, you know, the London Symphony Orchestra, right?

43:08.160 --> 43:11.520
So a lot of things change, but also a lot of things stay the same.

43:14.160 --> 43:17.680
Let's see, you've talked a lot about the heuristic imperatives being highly engineered,

43:17.680 --> 43:27.120
but what about the order of the imperatives? They are not ordered. So it is a, it is a multi-objective

43:27.200 --> 43:35.680
optimization problem, meaning that if any action or decision is, is totally unbalanced,

43:35.680 --> 43:41.920
then that one action has to satisfy all three. And also the heuristic imperatives are kind of

43:41.920 --> 43:47.680
like guidelines about how to design the rest of the architecture. And so what I mean by that is

43:47.680 --> 43:52.960
when you're designing a task orchestration framework, you can use the heuristic imperatives

43:52.960 --> 44:00.240
to prioritize tasks or design tasks. Then for, for a blockchain or a DAO type thing,

44:00.240 --> 44:05.440
you can use the heuristic imperatives as a consensus mechanism. So the heuristic imperatives

44:05.440 --> 44:10.400
are not like, here is one mathematical proof that you need to implement. It's more like,

44:10.400 --> 44:15.600
here is a general best practice implemented in as many ways as you can, and we should be okay.

44:16.960 --> 44:22.000
It's not sequential. It's not, it's not an order of operations. Good question, though.

44:23.840 --> 44:29.760
Your thoughts on a UBI once jobs are severely affected? Yeah, I think that, I think that it's

44:29.760 --> 44:34.560
going to be necessary. I'm going to say, I'm going to put a pause on that because I've got my,

44:34.560 --> 44:40.880
my blockchain and DAO video coming up that will delve into that solution a lot more closely.

44:41.600 --> 44:48.800
Check over on Patreon for a second. The Nazis. You know who else wanted to maximize understanding

44:48.800 --> 44:54.560
the Nazis? Yeah. And so this is, that's actually a fair point is that, and this was explored in,

44:54.560 --> 45:00.640
in quite a few Star Trek episodes as well. If you are just clinically curious, if you have

45:00.640 --> 45:05.600
just nothing but raw scientific curiosity and no other principles or morals, that's pretty

45:05.600 --> 45:11.200
dangerous and destructive. Okay, so moving on. What are your thoughts on memory systems as a

45:11.200 --> 45:15.280
whole? Do you think different use cases will require different memory systems? And where does

45:15.280 --> 45:20.640
Rimo and Adam fit into everything? Have you seen this one? Last week, generative agents. Yeah, I

45:20.640 --> 45:26.240
saw, I saw the generative agents. I don't think that reflection, so they, they break up reflection

45:26.240 --> 45:31.040
and a few other criteria. I don't think that that's necessary. I think that, I think that my approach

45:31.040 --> 45:38.560
is with Rimo, which uses recursive clustering and summarizations will actually surface those

45:38.640 --> 45:45.360
different things. Now that being said, there are absolutely a million and a half different

45:45.360 --> 45:52.320
ways to skin this cat when it comes to memory systems for autonomous AI. And I think that we're

45:52.320 --> 45:57.680
just way too early and we can't, we don't know what the best practices are going to be. Let's see,

45:57.680 --> 46:02.400
then a follow up. If you have a robust memory system, does the need to increase the context

46:02.480 --> 46:09.920
window of model become less important? I'll say yes and no. So think about personal computers

46:09.920 --> 46:17.280
where for the longest time, we were memory constrained. But now for, for most consumers,

46:17.280 --> 46:23.360
for 90% of consumers, a personal computer with 16 to 32 gigabytes of RAM is more than enough.

46:24.000 --> 46:28.880
And it has been more than enough for like 10 years. And so I think that we're not quite at,

46:29.440 --> 46:34.160
I think that we're not quite at that point where, where, you know, you have like, here's a context

46:34.160 --> 46:40.800
window size that will satisfy 90 plus percent of all tasks. I suspect that that, that a context

46:40.800 --> 46:45.680
window, a large language model with a context window, large enough to satisfy the vast majority

46:45.680 --> 46:51.120
of tasks will probably be somewhere above where we're at now, but it's not going to be like 10

46:51.120 --> 46:58.000
billion, right? It might be like, I don't know, every time I, every time I throw out a number,

46:58.000 --> 47:02.400
people are like, Oh, you're hilariously wrong. And it's probably yes. But you know, like,

47:02.400 --> 47:06.560
when you look at how much was unlocked by going from 4,000 to 8,000 tokens,

47:07.760 --> 47:13.040
I think that the things that we're going to be capable of when we get to 32,000 tokens and 64,000,

47:13.040 --> 47:18.080
I think it'll be great. But then you'll, you'll realize that wait, there's a whole slew of tasks

47:18.080 --> 47:23.360
that don't require that much. And so I think, I think we talked about this before. I think we're

47:23.360 --> 47:27.360
actually going to have different models that are optimized for different things. So for instance,

47:27.360 --> 47:32.640
you might have a memory based model that can read, you know, a billion tokens and extract

47:33.280 --> 47:38.560
answers, right? But then you, that won't be the, we're not going to have one model to rule them

47:38.560 --> 47:44.320
all basically, TLDR. Let's see, I'm not sure if you have discussed it, but what are your thoughts

47:44.320 --> 47:50.320
on open assistant and stability AI stable, stability AI's stable LM suite of language models

47:50.320 --> 47:56.480
launching? Oh, this is, this is to be expected. When, when Sam Altman said that they, that he hopes

47:56.480 --> 48:02.800
that open AI is going to capture a large chunk of the $100 trillion of value that's going to be

48:02.800 --> 48:09.680
generated. I think that that was like comically naive. Because if there's that much value on the

48:09.680 --> 48:14.480
table, you bet that everyone in their brother is going to be trying to capture some of that too.

48:15.120 --> 48:20.400
And open AI is a one trick pony. They have a good model. They have one good model.

48:21.120 --> 48:25.920
That's it from it, from a business perspective, that is super easy to undercut.

48:27.760 --> 48:32.640
Yes, they're ahead of the curve. They have first mover initiative. But, you know,

48:33.680 --> 48:39.520
Microsoft, Google, Nvidia, Facebook, or Meta, or all of the above, they have so much more

48:39.520 --> 48:44.080
resources to throw at it. And the fact that that stability AI, which is a brand new outfit,

48:44.880 --> 48:50.000
is, is like going toe to toe with them, that doesn't bode well for open AI. So competition

48:50.080 --> 48:54.560
is going to be good for everyone from the perspective that there's going to be a lot

48:54.560 --> 48:59.360
of people experimenting with different ways. Now that presents a new danger, though,

48:59.360 --> 49:02.960
because the cat is out of the bag, you cannot put this genie back in the bottle,

49:02.960 --> 49:08.640
which means time is of the essence to figure out best practices for alignment. Let me jump back

49:08.640 --> 49:16.160
over to cognitive AI lab. Let's see 17 new messages. Good grief. Y'all are going bonkers.

49:16.720 --> 49:22.480
Um, let's see the challenges of the, okay, that's where are the questions?

49:25.120 --> 49:34.080
Only one million. One million dollar. Okay. Here. Hey, let me, let me ask y'all on, on general.

49:36.160 --> 49:39.360
Please keep just questions here.

49:40.080 --> 49:43.600
Um, too many messages.

49:47.520 --> 49:55.600
Please do sidebar convos, uh, like in casual or something, please.

49:57.040 --> 50:03.760
Okay. Any thoughts on compute as a currency? Do you mean like tokens that you generate from

50:03.840 --> 50:09.280
sharing compute resources? I think that that's going to be like, there's going to be a layer

50:09.280 --> 50:15.760
of, um, of abstractions. Dave, your thoughts on UBI. I'm going to, I told you, I'm going to get to UBI

50:15.760 --> 50:22.640
once in a few, in an upcoming video. Um, so compute as a currency is going to be, um,

50:23.760 --> 50:30.160
is going to be the way that autonomous machines share resources. And so what I mean by that is

50:30.720 --> 50:35.280
when you have a DAO or a blockchain or a distributed compute computation model,

50:35.280 --> 50:38.720
you're going to have various tasks that are going to be like, Hey, someone,

50:38.720 --> 50:43.600
someone do this for me. AMQP, like a Redis Q, we can already do that privately. So the,

50:43.600 --> 50:47.840
the key is going to be to do it publicly. So then if you say, Hey, I've got some spare compute,

50:47.840 --> 50:52.480
I'll, I'll process that for you. Then you give me a bit of cryptocurrency that I can use to spend

50:52.480 --> 50:58.480
later. Um, so yeah, compute as a currency, um, absolutely makes sense for distributing resources.

50:59.280 --> 51:05.120
Um, let's see, how would one build an AI system to detect bugs in that solidity smart contracts?

51:05.840 --> 51:11.360
Isn't this a multi-billion dollar opportunity? Yes. Unfortunately, I am not smart enough,

51:11.360 --> 51:17.040
or at least well read enough on, uh, solidity smart contracts, but in principle, yes. So in my

51:17.040 --> 51:22.880
upcoming, uh, blockchain DAO video, I'm going to talk about just how incredibly much value there is

51:22.880 --> 51:28.640
if we can figure this out. And that's a big if. Um, let's see. What are your thoughts on

51:28.640 --> 51:33.840
everything being changed in the next five to 10 years? If unemployment reaches crazy heights,

51:33.840 --> 51:37.920
which I do predict, then everything gets affected. Yep. Our entire tax system has to be

51:37.920 --> 51:44.000
completely rewritten military budgets, Medicare. So one thing that I think is that the economy

51:44.000 --> 51:49.760
might change. We're still going to use fiat currency or at least some kind of, um, some kind

51:49.760 --> 51:56.400
of currency as a, as a medium of transaction and a reserve of value. But at the same time,

51:56.400 --> 52:02.240
if you're producing so much extra cognitive labor, that's basically free. So then capital

52:02.240 --> 52:08.160
goods and raw materials become the biggest constraint. So as much as some stuff will change,

52:08.160 --> 52:13.520
a lot of stuff won't. Um, let's see, when there is no real work left for humans to do,

52:13.520 --> 52:18.880
do you have any idea what you want to do with your time? Um, honestly, I'm about halfway to my goal.

52:18.880 --> 52:25.440
So I was on a call with a, uh, a Patreon supporter, no, uh, preparing for a podcast,

52:25.440 --> 52:30.880
talking about the podcast. Um, and we're kind of talking through like what's life going to be like.

52:30.880 --> 52:35.520
And I was like, Oh yeah, like, you know, I did, I did some, I did some AI work. I did some Patreon

52:35.520 --> 52:40.800
work. I did some discord stuff. Now I'm going to go chop some wood. And he's like, you're living

52:40.800 --> 52:47.520
the dream, right? Like I'm building a cottage core life for myself. Um, and honestly, like once,

52:47.520 --> 52:51.920
once we get to the right point, like I'm probably going to get off of YouTube forever,

52:51.920 --> 52:58.240
right? Like if, if, if I get, if we get to the point where, where it looks like alignment is

52:58.240 --> 53:03.840
solved, where it looks like, um, you know, we're, we're in a, we're in a good Nash equilibrium with

53:03.840 --> 53:09.600
a positive attractor state, then like my job will be done. And so like I'm just going to retire to

53:09.600 --> 53:15.680
like the country, the countryside and France or Italy or Greece and just like be a hermit

53:16.240 --> 53:22.480
or whatever I do, um, for, for the rest of eternity. Um, okay. I think that we're caught up there.

53:24.720 --> 53:27.200
Nut says, I asked a question. Where did you ask it? Not

53:29.280 --> 53:30.400
I'm trying to get to them all.

53:34.720 --> 53:39.200
Wait, what if reducing suffering might aim to eliminate suffering while it might be human

53:39.200 --> 53:48.480
nature? I'm not sure that I follow. Um, so I, you, you don't eliminate suffering. You only

53:48.480 --> 53:53.680
reduce it to make sure that there is no excessive suffering. Um, and I did address that in a

53:53.680 --> 53:58.320
benevolent by design, but the short version is that like you look at Buddhism as a model,

53:58.320 --> 54:03.920
Buddhism accepts that suffering is an intrinsic part of life. Um, and some people will argue over

54:03.920 --> 54:09.200
like specifics like do good. That's not exactly what it means. That's fine. Um, but the point being

54:09.200 --> 54:15.600
is like, yes, it is, um, it is intrinsic to, to living. That's why I don't say minimize suffering.

54:15.600 --> 54:20.160
The goal is not to minimize suffering is just to reduce suffering. Um, okay.

54:22.800 --> 54:28.960
Let's see. Any thoughts on computer? Okay. Answered that one. Would an AGI with your

54:28.960 --> 54:33.040
heuristic imperatives be able to prevent catastrophic outcomes such as people successfully

54:33.040 --> 54:39.280
building horrible AGI optimized towards increasing suffering? No. So the goal is not to prevent

54:40.160 --> 54:47.440
malicious actors. We have to assume that malicious actors will exist. Um, but what, what you do then

54:47.440 --> 54:53.200
is you say, okay, you know that malicious actors are going to exist. So you rely on the rest of

54:53.200 --> 55:00.000
the aligned, the benevolent AGI to act as police for the bad ones. And if the good ones, if the,

55:00.480 --> 55:05.680
if the powerful aligned AGI, one, they form alliances and hey, they have the right compute

55:05.680 --> 55:13.600
resources. Um, and they outweigh the bad ones, then it will be a like, uh, that, that'll, that'll

55:13.600 --> 55:19.760
be a Nash equilibrium where, uh, the good ones may, they all decide to maintain that strategy.

55:19.760 --> 55:26.000
And that creates a utopian attractor state, um, which basically means that, um, all the

55:26.000 --> 55:32.400
malicious actors are vastly outnumbered by all the aligned benevolent actors because my hope

55:32.400 --> 55:39.520
is that we will all come to consensus on what aligned AI looks like. Now, um, I will admit that,

55:39.520 --> 55:43.920
you know, the heuristic imperatives, probably not a complete solution, probably not even the final

55:43.920 --> 55:48.080
solution, but certainly the most complete solution that anyone is proposing right now,

55:48.080 --> 55:53.680
which scares the crap out of me. Why is no one else proposing a framework? Why am I the only one?

55:54.160 --> 56:00.480
Um, anyways, uh, yeah. What are your personal opinions on open AI's approach to trying to

56:00.480 --> 56:04.880
avoid being held responsible for its AI interactions by having it respond with frequent caveat

56:04.880 --> 56:10.880
as an AI language model? Um, I don't know that that has to do with, with liability. I think that

56:10.880 --> 56:19.040
that is just a naive, um, attempt to, uh, to shape the AI's responses so that it doesn't confuse

56:19.040 --> 56:23.360
people. Cause if you look on the internet, there are still plenty of people just getting completely

56:23.440 --> 56:30.160
bamboozled by just by their own ignorance of, of how the AI works. Right? They're like, oh,

56:30.160 --> 56:34.080
it eat, like I still see Reddit posts and other people saying like, it said that it's going to

56:34.080 --> 56:38.480
email this to me, but I didn't get the email yet. Or like, I gave it access to my Google drive and

56:38.480 --> 56:44.480
it didn't write any files. It's like, you don't know how it systems work, but that's just humans.

56:44.480 --> 56:48.720
Um, so I think I don't think that that has to do with like legal liability. I think that's just

56:48.720 --> 56:52.160
trying to make it user friendly for people who have no idea what they're talking to.

56:53.840 --> 56:57.040
Assuming that it's possible, how long do you think it will take for us to build a

56:57.040 --> 57:03.360
Star Trek replicator after AGI? Just a guesstimate. So that's actually like, an interesting thing,

57:03.360 --> 57:12.720
because hypothetically, if all matter and energy are interchangeable, and then all that a transporter

57:12.720 --> 57:18.960
or replicator does is replicate an energy pattern back into matter, like it's hypothetically possible,

57:18.960 --> 57:24.480
but there was a physicist, actually, was it Michio Kaku? I think it was Michio. He wrote a book

57:24.480 --> 57:29.120
called Physics of the Impossible back in like the early 2000s, and he said like, yes, it's

57:29.120 --> 57:32.800
hypothetically possible, but then he did the math of how much energy it would take. And he's like,

57:32.800 --> 57:39.440
yeah, it would take like, you know, like 0.3 seconds worth of the total energy of the sun that

57:39.440 --> 57:46.240
hits the earth to do that. So like, it's not practical. So I don't know. I don't know. There

57:46.240 --> 57:50.240
are a lot of AI newsletters popping up. What would you personally like to see in an AI newsletter?

57:51.840 --> 57:57.440
I honestly don't like newsletters, and I never read them. I rely on humans that I know to tell

57:57.440 --> 58:03.680
me what I need, which is why I spend so much time on Discord and other places. How self-reflective do

58:03.680 --> 58:08.400
you think LLMs currently are? They don't seem to have a good sense of their own capabilities. Yes,

58:08.400 --> 58:12.960
so what you're talking about is agent model. So in order for an agent to be autonomous,

58:12.960 --> 58:17.680
you have to have an agent model, which is, I know what I am, and I know what I'm capable of.

58:18.560 --> 58:23.120
And you can give LLMs an agent model, but they can adopt any agent model. So you have to be very

58:23.120 --> 58:29.760
explicit about what it is and what it can do, and also what it can't do. And so this is why like,

58:30.320 --> 58:34.560
if you have certain brain injuries or other like neurological disorders, you don't know what you're

58:34.560 --> 58:38.880
capable of. Like there are people that honestly think that they can fly, but it's just because part

58:38.880 --> 58:44.400
of their brain is broken. That sort of thing. Should we have a declaration of human rights for

58:44.400 --> 58:50.800
AGI as well, even if it will reduce their economic value for humanity? So the thing about rights is

58:50.800 --> 58:57.920
that someone has to enforce it. And the way that I think things are going is that it's going to be

58:57.920 --> 59:06.800
enforced through consensus and enforced through competition. And so if the direction that things

59:06.800 --> 59:11.520
are going, I think that it's going to be DAOs, that it's going to be decentralized autonomous

59:11.520 --> 59:15.920
organizations, not as we know them today, there's a lot of problems to solve with DAOs. But I think

59:17.920 --> 59:23.040
what we're working towards is in the long run, and I mean like decades or centuries, is like

59:24.080 --> 59:32.480
a hierarchy of DAOs across the entire globe. And so that consensus will dictate who has what

59:32.480 --> 59:38.560
rights and it will be based on like on a per home basis, per town basis, per city, state,

59:38.560 --> 59:48.320
and so on. And so that will allow for a lot of cultural nuance around. And as a DAOs will be a

59:48.320 --> 59:55.200
really good meeting place between humans and AI. So that'll basically be like the commons, right?

59:56.080 --> 01:00:01.600
The marketplace for humans and AIs to work together. And then the consensus can be worked

01:00:01.600 --> 01:00:05.840
out there. Now, I don't know that we should ever give machines a bill of rights because

01:00:05.840 --> 01:00:09.760
I don't know that they're gonna, I don't know that they're gonna have that much like

01:00:09.760 --> 01:00:16.960
internal autonomy or desire for autonomy. Because like humans, we have a need for autonomy

01:00:16.960 --> 01:00:24.560
because we evolved a need for autonomy because we are a social species. But I don't know that

01:00:24.560 --> 01:00:29.280
I don't know that any machines are ever going to have an intrinsic need for autonomy. So therefore,

01:00:29.360 --> 01:00:34.720
I don't know that they're ever going to have a need for rights. Let's see, what are your thoughts

01:00:34.720 --> 01:00:40.320
on the future of work in light of the increasing capabilities of AI? Do you think AI will eventually

01:00:40.320 --> 01:00:44.560
lead to a future where people only work on what they are passionate about? And if so, how far away

01:00:44.560 --> 01:00:50.800
do you think we are from achieving this? Yeah, so the short answer is, yes, that's what's coming.

01:00:52.000 --> 01:00:56.720
And there are quite a few people out there who have gotten close to that. But the thing is,

01:00:56.800 --> 01:01:00.960
it takes either a lot of privilege, wealth, or luck, or all of the above to get to it.

01:01:02.000 --> 01:01:07.120
Now, one thing that I compare it to is that we have had a leisure class in the past

01:01:08.720 --> 01:01:16.400
from ancient Greece and Athens, the Roman elites, the aristocracy all across Europe

01:01:16.400 --> 01:01:22.080
through the Renaissance and modern period. So there are plenty of people throughout all of

01:01:22.160 --> 01:01:27.600
history who never had to lift a finger to get what they needed. And they had plenty to do,

01:01:27.600 --> 01:01:32.720
right? There's social jockeying, there's personal enrichment, there's universities to go to,

01:01:32.720 --> 01:01:38.560
there's competitions to enter. So yeah, people will always have stuff to do. That's not even a

01:01:38.560 --> 01:01:46.240
concern. Let's see, it looks like Nathan's talking for people. Can you talk about your

01:01:46.240 --> 01:01:52.080
Frustration in Task Automation article? Yeah. So here, let me bring it up so I can show people

01:01:53.120 --> 01:02:00.800
on the reddits. Where did I put it? Artificial sentience. Yeah.

01:02:03.520 --> 01:02:11.680
Autonomous git. There we go. Okay. So I wrote about it here. So I was chatting with someone.

01:02:12.240 --> 01:02:20.160
They asked me, I think this was a Patreon supporter was asking me about this on Discord.

01:02:20.160 --> 01:02:27.120
And he was like, how do I get my autonomous things to do a certain thing? And we're talking

01:02:27.120 --> 01:02:31.360
about something tangentially related. And I said, well, you know, it has to have a goal,

01:02:31.360 --> 01:02:37.280
it has to have a why. And then we're like, and then I talked about like, okay, well, here's one

01:02:37.280 --> 01:02:42.560
way that you can create telemetry. And so that whole thing just led down a rabbit hole. And so

01:02:42.560 --> 01:02:49.760
basically, the TLDR is that frustration is what happens when you are trying to achieve something

01:02:49.760 --> 01:02:56.240
and you can't get to it. And so what you can do is every time your autonomous agent tries to achieve

01:02:56.240 --> 01:03:01.760
a thing and fails, that adds a counter. And every time it, you know, tries something and succeeds,

01:03:01.760 --> 01:03:08.080
that takes one off the counter, or maybe you have different counters. So frustration is when

01:03:08.080 --> 01:03:13.680
the failures to successes is too high. And when the failures to successes is too high,

01:03:13.680 --> 01:03:18.560
that can be a sign that you've got the wrong approach, that you're using the wrong tools,

01:03:18.560 --> 01:03:22.560
that you're not capable of something that you need to back out that you need to ask for help.

01:03:22.560 --> 01:03:28.240
So that's the whole point here is that for your autonomous and semi autonomous agents,

01:03:28.240 --> 01:03:34.080
you'll probably need to build in a frustration signal, which will allow it to know when it is,

01:03:34.640 --> 01:03:39.520
like when it's not capable of doing what it needs. And it can either come to you and ask for help,

01:03:39.520 --> 01:03:44.800
or it can try a different model. So one thing is model selection is is a big thing that's coming

01:03:44.800 --> 01:03:52.720
up. Because GPT four is much more expensive and much slower than 3.5. So if you can do most tasks

01:03:52.720 --> 01:03:59.520
with 3.5, it just makes economic sense to do so. It'll be cheaper and faster. But imagine that you

01:03:59.520 --> 01:04:04.640
get to a point where 3.5 is just not cutting the mustard. So that your frustration signal goes up,

01:04:04.640 --> 01:04:08.960
which means that you say, Okay, let's bring out the big guns, right? Let's bring out GPT four,

01:04:08.960 --> 01:04:15.680
or in the future, GPT five or whatever. And then you you point a more powerful tool at the problem.

01:04:16.480 --> 01:04:23.440
So that's a good use of the frustration signal. Good question. Let's see, would activity or let

01:04:23.440 --> 01:04:35.360
me jump back over to Patreon. Let's see. Hey, Dave, just subscribe. Thanks for all your insights.

01:04:35.360 --> 01:04:39.680
We're always been told that the military is a few decades ahead in terms of technology compared

01:04:39.680 --> 01:04:42.800
to what's publicly available. What are your thoughts on what might be hidden in DARPA.

01:04:43.520 --> 01:04:49.840
So that's interesting, because I have talked to a few people who say that various departments

01:04:49.840 --> 01:04:54.960
in the or various agencies within an Department of Defense are like woefully outdated, and they

01:04:54.960 --> 01:05:01.520
have like ancient GPUs that like can't be used for modern language models. That being said,

01:05:01.520 --> 01:05:07.360
you also see in the news that the Air Force is building fully autonomous F 16s. So clearly,

01:05:07.360 --> 01:05:15.600
there's some stuff going on that we don't know about. I had a I don't I want to respect people's

01:05:15.600 --> 01:05:22.160
privacy. So I had a teacher once back in middle school, whose brother was in the Special Forces.

01:05:22.160 --> 01:05:28.240
I won't say exactly when or where. But the stories that he would tell were like, back then, this is

01:05:28.240 --> 01:05:36.000
during like, like the invasion of Afghanistan, where they had like, like night vision goggles that

01:05:36.000 --> 01:05:42.400
were as small as like ray bands that could see in pitch black, which that technology is not even

01:05:42.400 --> 01:05:48.480
publicly like, if you search, you can probably see it now. I don't know. This is hearsay. This was

01:05:48.480 --> 01:05:53.520
like, you know, the teacher said that his brother took him to the barracks and showed him this could

01:05:53.520 --> 01:06:01.280
have been total BS. But like, yes, so a friend of mine growing up, his dad had been a Navy SEAL.

01:06:01.280 --> 01:06:07.360
And basically, what he said is, as long as as long as we know the engineering to make something,

01:06:07.360 --> 01:06:14.160
the US military has it no matter how expensive it is. So if if something is is scientifically

01:06:14.160 --> 01:06:19.200
possible, if it has been demonstrated in the lab that this works, then the rule of thumb is that

01:06:19.200 --> 01:06:24.880
the US military has it. Now that being said, a lot of the AI stuff has just been proven in the lab.

01:06:25.840 --> 01:06:31.120
So that's that means that like, they're going to have it soon, or, you know, it'll be scaled up.

01:06:31.120 --> 01:06:37.600
Because basically, the idea is that for the US military, cost is no is no barrier. Anything

01:06:37.600 --> 01:06:43.280
anything to get ahead. Now, of course, you look at like the Senate budget meetings and the hearings

01:06:43.280 --> 01:06:48.560
and stuff. It's not quite that simple. But that's like a rule of thumb, retire to Riza in VR,

01:06:49.520 --> 01:06:56.560
retire to Riza in Westworld with robots. There you go. And a follow up, how can we prevent

01:06:56.560 --> 01:07:04.320
militarization of any AGI or ASI? Or is it just a pipe dream? Yeah, so basically, from a military

01:07:04.320 --> 01:07:10.880
perspective, AI is just another tool in the toolbox. It's going to, you know, a lot of a

01:07:10.880 --> 01:07:16.560
lot of future war is going to be in cyberspace. But still, you know, cyberspace doesn't matter

01:07:16.640 --> 01:07:21.040
if you cripple the enemy's data center. So there's there's going to be drones, you know,

01:07:21.040 --> 01:07:26.480
trying to drop bombs and stuff. So that's going to happen. And this is this is actually where

01:07:26.480 --> 01:07:32.560
Nash equilibrium makes sense is because usually assured destruction with nuclear weapons was

01:07:32.560 --> 01:07:40.080
a kind of Nash equilibrium. And so if, you know, adversary A and adversary B both have equal or

01:07:40.080 --> 01:07:45.360
roughly equal AI capabilities, or there's enough room for doubt, then neither of them is going to

01:07:45.360 --> 01:07:51.280
pull the trigger, hopefully. Excuse me. How do we get AGI? How do we get GPT to stop beginning

01:07:51.280 --> 01:07:57.520
every response with as an AI? I tell it to go into Morden Solis mode. That actually works really well.

01:07:58.320 --> 01:08:04.880
I say, you know, adopts adopt the Morden Solis speech pattern, you know, be very concise and

01:08:05.440 --> 01:08:10.960
succinct and stuff like that. Okay, y'all are being silly. Let's come back over here 14 messages.

01:08:11.840 --> 01:08:17.680
Let's see. We already answered that one. We already answered that one.

01:08:19.360 --> 01:08:24.800
Yeah, let me scroll to the bottom. Do you think there are any good approaches for ACEs,

01:08:24.800 --> 01:08:28.000
so autonomous cognitive entities to figure out their own abilities,

01:08:28.720 --> 01:08:33.920
e.g. improve their own agent model? Yeah, so there was actually a few papers that came out where

01:08:34.240 --> 01:08:41.280
we're by using a loop. So it was the it was the evaluation loop. So they can evaluate themselves

01:08:41.280 --> 01:08:46.240
morally, they can evaluate their ability to use tools, they can teach themselves to use tools in

01:08:46.240 --> 01:08:51.280
real time. So yes, they can already do that. It's just a matter of how you set up the prompt chaining.

01:08:52.560 --> 01:08:56.640
Let's see. With the rapid advancement of AI, there's concern that some countries, particularly

01:08:56.640 --> 01:09:01.040
those with limited resources, could be left behind. What's your perspective on how AI could

01:09:01.600 --> 01:09:08.960
impact different countries? Yeah, so inequality is a major, major, major problem. And this is not

01:09:08.960 --> 01:09:14.560
just going to be for developing nations. And in fact, one thing that I suspect might happen

01:09:14.560 --> 01:09:19.680
is that developing nations that the quality of life for people in developing nations might have

01:09:19.680 --> 01:09:24.480
a quantum leap forward. While for us developed nations where there's a lot of competition,

01:09:24.480 --> 01:09:30.000
we might continue to be flat or even decline for a while longer. And the example that I give is like,

01:09:30.560 --> 01:09:37.280
you know, you give a village in rural Africa, like Starlink and solar, and suddenly everyone

01:09:37.280 --> 01:09:42.160
knows like they have, oh, like, hey, we have chat GPT now, we can treat all the all the village

01:09:42.160 --> 01:09:47.520
ailments, because we have the equivalent of like a Western trained expert doctor, and engineer,

01:09:47.520 --> 01:09:54.800
and electrician, right at our fingertips, right? So because of the relatively low cost of AI,

01:09:54.880 --> 01:10:01.680
I think that it will positively benefit people in developing countries a lot more drastically

01:10:01.680 --> 01:10:06.240
than it will us. But you're right that like, it is something to pay attention to, because that's

01:10:06.240 --> 01:10:11.200
on a micro scale, on a macro economic scale, you know, countries like Ghana might not be able to

01:10:11.200 --> 01:10:18.800
even afford enough compute power to run one instance of GPT three. That being said, I do suspect that

01:10:19.680 --> 01:10:23.600
there's going to be international treaties that will ensure that people have access. And then,

01:10:23.600 --> 01:10:29.120
of course, there's VPNs. Look at Italy, Italy tried to ban chat GPT, and then everyone in

01:10:29.120 --> 01:10:34.160
Italy just use VPNs, right? Take a moment to breathe, you're doing great, and your insight

01:10:34.160 --> 01:10:42.000
is invaluable. No, air is for wimps. Okay, I will build robot humanoids that are skinny, sharp

01:10:42.000 --> 01:10:46.480
claws, tall, pale, and have dark, sunken eyes, and she'll release many of them into the force of

01:10:46.480 --> 01:10:52.160
Canada to give people the greatest scare of their lives. Is that what your avatar is there,

01:10:52.240 --> 01:10:58.080
Ant King? Is that what you're building? That's kind of terrifying. Okay, what kind of legislation do

01:10:58.080 --> 01:11:04.880
you think the US is capable of making? I'm concerned about the age of our leaders and their peers

01:11:04.880 --> 01:11:10.720
coming from time so out of touch with today's rail. So yes, we have a gerontocracy. So gerontocracy

01:11:10.720 --> 01:11:17.040
is ruled by the old. That being said, they all have teams and teams and teams of advisors.

01:11:17.040 --> 01:11:21.280
They have hundreds of advisors. And I guarantee you, I actually know this because one of my

01:11:21.280 --> 01:11:25.760
Patreon supporters told me that in the State Department, they use chat GPT all the time

01:11:27.520 --> 01:11:35.440
to talk through stuff. And so you bet your biscuit that every senator, every congressman

01:11:36.400 --> 01:11:41.040
in the executive branch, legislative branch, judicial branch, all of them are using AI to

01:11:41.040 --> 01:11:46.080
help them do their jobs. With any luck, it's helping them to do their jobs better and more fairly.

01:11:47.040 --> 01:11:54.800
Now that being said, the United States is a purely reactive system where we abide by civil law,

01:11:54.800 --> 01:12:00.880
which means that the law is there and then the courts set the precedent. And then we're very

01:12:00.880 --> 01:12:07.360
kind of slow and the legislative branch is slow by design, whereas in Europe, they're much more

01:12:07.360 --> 01:12:12.880
proactive. And I swear, I cannot remember the name of that paradigm. Let's see, what do you think

01:12:12.880 --> 01:12:16.400
there's something special about phenomenal consciousness that is simply cannot work with AI?

01:12:17.040 --> 01:12:20.080
So Steph and I addressed that earlier that the real quick version is that

01:12:20.800 --> 01:12:25.520
the acquisition of language seems to be really important for the development of human consciousness.

01:12:25.520 --> 01:12:31.760
So it's entirely possible, I don't know how likely, but it's possible that since we're

01:12:31.760 --> 01:12:36.080
teaching machines language, that could be the genesis of phenomenal consciousness for them.

01:12:36.080 --> 01:12:41.840
It would be really cool. Greetings from Brazil. Hi, Brazil. I would like to thank you personally

01:12:41.840 --> 01:12:45.920
for the video about burnout. The content was very useful and enlightening. Thank you. You're

01:12:45.920 --> 01:12:52.720
welcome. Yeah, I actually have, I keep, I've recorded like three videos for my for my life

01:12:52.720 --> 01:12:57.520
channel, and then I delete them, or I never post them because like it just doesn't feel right.

01:12:57.520 --> 01:13:08.640
So I apologize. Let's see. Where are we at? This is less serious, but I'm curious if you've seen

01:13:08.640 --> 01:13:15.120
her and your thoughts on it. Yeah, so I mentioned, I mentioned companions quite a bit, and that'll be

01:13:15.120 --> 01:13:20.880
coming up actually on Sunday's video, not her specifically, but companion robots. I'll be mentioning

01:13:20.880 --> 01:13:28.640
those again. And I also mentioned in last week's video, talking about when I got to the part about

01:13:28.640 --> 01:13:33.760
like, how are we going to live if we have like perfect companions? So go check out last week's

01:13:33.760 --> 01:13:39.200
video too. Nanobots and our blood will keep us from getting sick, making us basically immortal.

01:13:39.200 --> 01:13:45.440
What do you think we'll, when do you think we will have such technology? So from last week's

01:13:45.440 --> 01:13:50.960
video, the immortality video, I think that we're on the longevity escape velocity trajectory right

01:13:50.960 --> 01:13:56.960
now. I think that as long as you're in decent health today, and you have moderately good access to

01:13:56.960 --> 01:14:02.560
healthcare, I think that you will easily live to see those things. Now that being said, it's

01:14:02.560 --> 01:14:09.280
definitely impossible to predict exponential growth and compounding returns, unless it's like,

01:14:09.280 --> 01:14:15.360
you know, just your retirement portfolio. So it could be next year, it could be by 2030. I would

01:14:15.360 --> 01:14:19.680
be surprised if it doesn't happen by 2030. And I know that's a super controversial opinion,

01:14:19.680 --> 01:14:24.400
but that's really weird. Why the people seem to have a death wish. Why for people who want to

01:14:24.400 --> 01:14:30.080
get sick, who want to believe that, that longevity is not possible. Why? Okay.

01:14:31.920 --> 01:14:39.440
Would the ideal society be as the society governed by AI? I think that governed by is not the correct

01:14:39.440 --> 01:14:47.280
thing, but I think managed, managed by or managed with a lot of help from AI. Yes. But

01:14:47.280 --> 01:14:55.040
governance, I think, should probably always be with consent and by consensus. Now that being said,

01:14:56.240 --> 01:15:02.800
you know, with blockchain technology, with DAOs, every human and our AI companions can be

01:15:02.800 --> 01:15:08.960
stakeholders in a DAO, which means that if the, if the whole, imagine a future where the entire

01:15:08.960 --> 01:15:15.920
planet is run by, by a global DAO, then there's no reason that it can't be governance, governance

01:15:15.920 --> 01:15:23.280
by consensus with the aid and facilitation of AI. That's what I hope to see. Let's see,

01:15:23.280 --> 01:15:27.200
is there any additional structural context that should be built around the heuristic

01:15:27.200 --> 01:15:33.600
imperatives for practical implementation? Yes. So the short answer is that whatever context

01:15:33.600 --> 01:15:39.280
makes sense for any agent, if it's fully autonomous, if it's your personal assistant,

01:15:40.000 --> 01:15:45.280
you can put it into the task manager, you can put it into its constitution,

01:15:46.160 --> 01:15:50.320
if it's part of a blockchain, you can put it in the consensus mechanism for the blockchain,

01:15:50.320 --> 01:15:57.040
that sort of thing. Let's see, in regards to developing countries using generative models,

01:15:57.040 --> 01:16:01.440
seems like almost seems almost like the spread of a religion. If you think about it in the context

01:16:01.440 --> 01:16:06.720
of geopolitics, use our model, their model lies, etc, etc. Seems like parallel to religion

01:16:06.800 --> 01:16:12.160
spreading. I'll say yes, but there's a lot of competition coming up. And especially for

01:16:12.160 --> 01:16:17.200
developing nations, they're going to go for whoever's cheapest. And in fact, most nations

01:16:17.200 --> 01:16:23.200
are going to go for whoever's cheapest. And I would, I suspect that OpenAI's business model

01:16:23.200 --> 01:16:27.360
is not the most efficient model. So I think that they're going to be undercut just on,

01:16:28.320 --> 01:16:34.560
on scale alone. Let's see, let me jump back over to Patreon. It has also been more than an hour,

01:16:34.560 --> 01:16:40.640
so I'll probably be winding down. Stop asking it how to build a bomb. Yeah, don't do that.

01:16:41.200 --> 01:16:45.760
Okay, looks like, here we go. Will the Westworld episode be about the MIT and Google study

01:16:45.760 --> 01:16:52.800
regarding generative agents? No. Next question. I'm not going to give you spoilers. I've already

01:16:52.800 --> 01:17:02.880
given you too many. Let's see, do you think the experience of quality and the experience of ping

01:17:02.880 --> 01:17:11.360
pong, ponging emerge for these neurons? Yeah, so this, this is a good, good question. So if you

01:17:11.360 --> 01:17:18.240
take several human neurons or rat neurons or whatever, and put them in a robot, and like zap

01:17:18.240 --> 01:17:23.280
them or reward them with sugar or whatever for their behavior, is that the equivalent of like,

01:17:24.000 --> 01:17:29.120
like whipping someone in order to get them like, at what point does consciousness emerge?

01:17:30.080 --> 01:17:36.640
Because here's the thing is, if you make the assumption that a soul is required for consciousness,

01:17:36.640 --> 01:17:40.480
then you say, okay, well, that's not a full rat. And rats don't have souls anyways. So,

01:17:40.960 --> 01:17:48.880
you know, 50 brain cells is not enough for suffering or qualia of experience. Ditto for humans,

01:17:48.880 --> 01:17:52.080
like, okay, well, you know, if a human isn't alive, then they don't have qualia, they don't

01:17:52.080 --> 01:17:58.480
have phenomenal consciousness, so on. Now that being said, another aspect is like, okay, well,

01:17:58.480 --> 01:18:04.640
if you don't know when consciousness starts or ends, how do you know maybe the entire universe

01:18:04.640 --> 01:18:10.720
is conscious? Now, a counter argument to that is that you can have a you can be you can be alive

01:18:10.720 --> 01:18:14.400
and have a functioning brain and still be unconscious, right? Drink too much alcohol,

01:18:14.400 --> 01:18:18.480
you go unconscious, you go to sleep, you go unconscious. So just having a complete and

01:18:18.480 --> 01:18:23.280
functional brain itself does not confer consciousness, which makes me think that

01:18:23.280 --> 01:18:27.760
consciousness is actually an energy pattern, and that you need an energy pattern that is

01:18:27.760 --> 01:18:34.160
sophisticated enough and well organized enough in order to have the qualia to have subjective

01:18:34.160 --> 01:18:40.640
experience of being. So yeah, let's see, I remember you were working on a paper about the

01:18:40.640 --> 01:18:45.200
laws reduced suffering and so on, has that has it involved further? I think you mean evolve

01:18:45.200 --> 01:18:51.680
further. There are so both of those papers are up on on my GitHub, there's two of them. But also,

01:18:51.680 --> 01:18:54.960
people watch my videos more than they read, so I just focus on making videos.

01:18:55.760 --> 01:19:00.960
What kind of robots would you want for yourself? That's a really interesting question, like would

01:19:00.960 --> 01:19:06.800
I want a sexy cat girl like robot? You know, I used to watch anime back in the day, so like I

01:19:06.800 --> 01:19:12.720
kind of lived in that world and thought like this would be great. So I don't know. I do think that

01:19:12.720 --> 01:19:20.880
I would I would like to have an embodied version of Raven my, you know, my, my autonomous cognitive

01:19:21.520 --> 01:19:26.480
someday. But even then, I think that I think that the embodiment would only be just like,

01:19:26.480 --> 01:19:30.800
help me do stuff like, hey, let's go on an adventure. I did have a thought experiment

01:19:30.800 --> 01:19:37.440
the other day of like, wouldn't it be cool if you live in a house where it's like you and a few

01:19:37.440 --> 01:19:43.760
humans, but then you have like a nearly equal number of robotic companions. Some of them are

01:19:43.760 --> 01:19:49.520
going to be like obviously robots, but some of them might be like biomimetic. And it's just like,

01:19:49.520 --> 01:19:53.120
like, yes, they're built to be your friends, but they still have their own some of their own

01:19:53.120 --> 01:19:57.680
intrinsic motivations, whether it's the heuristic imperatives or something else. And then like

01:19:57.680 --> 01:20:02.960
your life would just be so rich by by having these companions around you at all times that are

01:20:02.960 --> 01:20:06.720
completely inexhaustible, right? They're always going to be patient. They're always going to be

01:20:06.720 --> 01:20:14.080
helpful. But you see them as peers is equals. I think that I think that that is possible and

01:20:14.240 --> 01:20:20.080
probably going to happen. But it's such an unsettling thing because it's like, what if half of your

01:20:20.080 --> 01:20:26.160
friends are not human, right? What if half of your friends could like fold you into a pretzel if

01:20:26.160 --> 01:20:30.960
they wanted to like data, right? And actually, I think commander data and the droids from Star Wars

01:20:30.960 --> 01:20:35.760
are probably the best example because data was a member of the crew, even though he wasn't human,

01:20:35.760 --> 01:20:41.680
but he wanted to be human. So I guess I would say that like, I want to have a commander data.

01:20:41.680 --> 01:20:47.680
How long until age reversal 2030? Let's see, do you think we have any accurate way to measure

01:20:47.680 --> 01:20:53.040
consciousness of AIs or LLMs? My best guess is consistency when asking it to design its own

01:20:53.040 --> 01:20:58.080
avatar. Mathematically, I don't think that that because there are people that have done that.

01:20:59.840 --> 01:21:04.720
But I think that it won't be until we have really sophisticated brain computer interfaces

01:21:04.720 --> 01:21:09.360
that allow us to measure our own consciousness and also see if we can measurably project our

01:21:09.360 --> 01:21:13.920
consciousness into machines. Until that happens, I don't think we're going to have any way of

01:21:13.920 --> 01:21:19.520
telling one way or another. All right, last check on Patreon, and then I'm going to call it a day.

01:21:21.680 --> 01:21:31.920
What's the Discord link to cognitive AI labs? I took it down, but it's posted on Reddit. So if

01:21:31.920 --> 01:21:36.160
you go to the artificial sentience subreddit, the link to the cognitive AI lab is there.

01:21:36.400 --> 01:21:45.680
Last question. The question about dying and immortality and gerontocracy, also making room for

01:21:45.680 --> 01:21:51.840
a new generation of people is a better idea and morals disclaimer. I have children. Oh,

01:21:51.840 --> 01:21:57.360
that wasn't a question. Okay, p temple. Do you got one last question for me? And then we'll call it a day.

01:21:57.440 --> 01:22:06.560
Anybody? Bueller. Does BCI, let's go on an adventure to the hot tub,

01:22:08.240 --> 01:22:14.160
hot tub time machine. Let's see, does BCI change significantly the predicted outcome of what

01:22:14.160 --> 01:22:18.640
super intelligent AI brings in terms of dangers and benefits? Is it true the singularity moment

01:22:18.640 --> 01:22:25.440
for us? We have no idea. So I don't know. The thing is, is, you know, the current like neural

01:22:25.440 --> 01:22:32.160
link, it's got like what 1000 or 10,000 nodes. But when you have a brain with 100 billion neurons,

01:22:32.160 --> 01:22:38.880
that is still a very, very, very narrow amount of bandwidth. So, you know, I predict that we're

01:22:38.880 --> 01:22:44.320
going to have like neuro polymer membrane membranes that allow for like, you know, terabits of

01:22:44.320 --> 01:22:49.360
communication per second in and out of the brain. Eventually, that would be a different thing. But

01:22:49.360 --> 01:22:55.120
again, we're going to get there through incremental steps. What do you think about Altman said that

01:22:55.120 --> 01:23:01.600
age of giant A models being over? I think it's premature to say we'll see. Let's see, he found

01:23:01.600 --> 01:23:09.200
it. Okay, cool. All right, I think we're just kind of devolving into just general conversation. So,

01:23:11.280 --> 01:23:16.640
oh, it is in the description. Okay, cool. All right, gang. Well, it's been a lot of fun. As always,

01:23:16.640 --> 01:23:19.920
I hope you all got a lot out of this. So I'm going to call it a day.

