1
00:00:00,000 --> 00:00:10,000
going live. All right, I think we're here. Can y'all hear me now? Can you hear me now?

2
00:00:10,000 --> 00:00:26,960
Hello? Is this thing on? Okay. It says live in 15 seconds. Okay, we're good. Yay! Okay,

3
00:00:27,840 --> 00:00:36,400
make sure to mute myself. All right, so hello everybody. I'm still figuring out the whole

4
00:00:36,400 --> 00:00:41,840
live streaming thing, so please bear with me, but one thing that y'all said was that you want to

5
00:00:41,840 --> 00:00:45,760
like see what I'm looking at, which you know, it's a good idea. You don't want to just look at my

6
00:00:45,760 --> 00:00:50,560
face. It's like, let's look at stuff together. So I figured I'd follow more or less the same format

7
00:00:50,560 --> 00:00:57,760
that I usually do. I'm in the cognitive AI lab discord, so if you're in the general, you can

8
00:00:57,760 --> 00:01:06,400
drop questions here or papers. So basically what I thought we would do is we would take a look at

9
00:01:06,400 --> 00:01:13,200
like weekly updates, because it's all going really fast. And you, I've got a lot of feedback from my

10
00:01:13,200 --> 00:01:18,800
recent live streams, that y'all really liked the interactive aspect of like asking questions,

11
00:01:18,800 --> 00:01:24,160
and like Dave, what does this mean? And so please feel free to drop good questions in. I'm also in

12
00:01:24,160 --> 00:01:32,080
the private Patreon discord, so drop questions there too. Okay, cool. Party time, yeah. Oh,

13
00:01:32,080 --> 00:01:39,360
looks like there's a delay of about 15 seconds. Interesting. Okay. Cool. Flippy from the dais

14
00:01:39,360 --> 00:01:47,280
discord. Hello Flippy. All right. Cool. So while some questions get spooled up, I figured I'd go

15
00:01:47,280 --> 00:01:53,760
through a couple of tools and stuff that I've seen. So someone, I think actually Flippy, this

16
00:01:53,760 --> 00:02:00,000
might have been you or someone in the dais discord, pointed out this tool. So this is basically

17
00:02:00,000 --> 00:02:06,240
something that I and others have been like thinking about and talking about. It's basically a hybrid

18
00:02:06,240 --> 00:02:14,400
of vector database and knowledge graphs. And it's also got a pretty interface. So

19
00:02:15,360 --> 00:02:21,680
like, it's worth just scrolling through and like reading every every bit of this. And then kind

20
00:02:21,680 --> 00:02:28,720
of experimenting with it. But but it will create. So this is one of the coolest things is it'll

21
00:02:28,720 --> 00:02:34,800
create a knowledge graph of all the clusters and stuff. But you can find the gaps, which when

22
00:02:34,800 --> 00:02:41,360
you're thinking about autonomous or semi autonomous AI agents, this is really good because then

23
00:02:42,000 --> 00:02:48,240
you can know what you don't know. And what I mean by that is if you're aware of all the things that

24
00:02:48,240 --> 00:02:54,080
you know, but you can detect some semantic gaps in your knowledge, that can help you zoom in on

25
00:02:54,080 --> 00:03:00,080
the things that you need to go learn about. So in front notice is the tool in front notice.com.

26
00:03:00,080 --> 00:03:06,000
I can never remember the name of the darn thing. I'm going to blame allergies and say that it's

27
00:03:06,080 --> 00:03:12,960
just brain inflammation. That's my excuse and I'm sticking to it. But yes, so this is a super cool

28
00:03:12,960 --> 00:03:24,960
tool. This kind of technology will definitely be part of of like autonomous AI agents. And this is

29
00:03:25,600 --> 00:03:31,200
functionally similar to what I worked on with Remo. Remo is much, much, much simpler though.

30
00:03:32,000 --> 00:03:37,760
So I often have some people ask me like, Oh, hey, can Remo do this? And that and I'm like, no,

31
00:03:37,760 --> 00:03:43,360
like, Remo was meant to solve one very specific problem. So for memory stuff, I usually point

32
00:03:43,360 --> 00:03:51,440
people at llama index and chroma db. So chroma db. Do I seriously not have that bookmarked?

33
00:03:52,320 --> 00:03:58,400
So chroma db is a vector database that runs just like SQLite. So it's pip install chroma db,

34
00:03:58,480 --> 00:04:04,640
you create a local client just like you do SQLite. So I tried to create something like this called

35
00:04:04,640 --> 00:04:09,600
VDB light like a few months ago. And I quickly realized that I was in way over my head. So I'm

36
00:04:09,600 --> 00:04:15,760
glad someone built this. And they just got an $18 million seed round. Holy mackerel. Man,

37
00:04:15,760 --> 00:04:21,360
I should have stuck with VDB light. I could have had an $18 million seed round. Anyways, maybe I'll

38
00:04:21,360 --> 00:04:30,080
do that with Remo who knows. So chroma db, super simple man, seriously $18 million for this one

39
00:04:30,080 --> 00:04:36,960
thing. Okay, I'm gonna let that go. llama index also. So llama index, I kind of didn't pay it

40
00:04:36,960 --> 00:04:41,440
any any attention at first because it's like, Okay, that's a silly name. This was clearly just

41
00:04:41,440 --> 00:04:47,120
someone's little side project. But if you look at all the types of indices they have,

42
00:04:48,080 --> 00:04:54,560
they've got list index, table index, tree index. So Remo is very similar to llamas tree index.

43
00:04:54,560 --> 00:04:59,680
Although I will say that I look through the code. And I think that their tree index is kind of basic.

44
00:04:59,680 --> 00:05:06,160
I think that my Remo framework does a little bit that theirs doesn't. But I'm not going to dive into

45
00:05:06,160 --> 00:05:09,920
that because I don't know that for certain. I didn't take a super close look at the code.

46
00:05:10,240 --> 00:05:17,600
Yeah, but so those are those are some memory storage tools. So let me check on the live stream

47
00:05:18,240 --> 00:05:23,600
and see where we're at. Whoops. All right. Oh, wow, we got lots of questions. Okay, cool.

48
00:05:24,320 --> 00:05:29,280
God's not dead rather believe and go to the good place and interesting. Okay.

49
00:05:31,680 --> 00:05:35,840
Let's see how to discover new wisdom from LLM. That's an interesting question.

50
00:05:36,400 --> 00:05:40,000
Um, let's see. Let me,

51
00:05:43,120 --> 00:05:48,400
you people ask questions far too early. Let me go over to discord to see if,

52
00:05:51,280 --> 00:05:59,040
yeah, please go ahead and drop some questions. Patreon get first dibs.

53
00:05:59,040 --> 00:06:08,640
All right. What do you think is the future of SAAS sales jobs for recruiting agencies?

54
00:06:10,320 --> 00:06:19,200
Oh, sales jobs and for recruiting agencies. So I mean, the sales level is still very human.

55
00:06:19,200 --> 00:06:23,360
So sales is not going to change for a while. Ditto for recruiting, although there's a lot

56
00:06:23,360 --> 00:06:28,880
of AI and recruiting already where like AI will read your resume and AI will watch a video of you

57
00:06:28,880 --> 00:06:35,840
answering questions. You know, more and more of that's coming, but that's about it. Let's see.

58
00:06:35,840 --> 00:06:41,760
All right. We got a whole bunch of questions coming in on the Patreon side. So, yeah, database,

59
00:06:41,760 --> 00:06:48,640
short answer, diversify your job skills. So there's that. Let's see. Zoom in a little so you

60
00:06:48,640 --> 00:06:55,120
guys can read this. Let me jump back over here real quick. But do you still expect AGI within 18

61
00:06:55,120 --> 00:06:59,280
months was stating they do not want to make models bigger but rather more efficient? Yeah,

62
00:06:59,280 --> 00:07:06,960
I think AGI in 18 months is still conservative. Honestly, I think that we will have,

63
00:07:08,320 --> 00:07:13,680
as people develop the architecture for autonomous AI, I think that we will be able to say that we

64
00:07:13,680 --> 00:07:19,040
have AGI by the end of this year. But people will realize that it's like, okay, we have an agent that

65
00:07:19,040 --> 00:07:23,760
can do anything, but it's expensive or it's slow or it's kind of dumb, that kind of thing.

66
00:07:25,840 --> 00:07:32,000
Let's see. Check on Discord real quick. Is her about to become reality? Yeah, lots of people

67
00:07:32,000 --> 00:07:42,480
are working on AI companions. They're going to get more sophisticated real fast. I've actually got

68
00:07:42,480 --> 00:07:48,160
a couple more videos upcoming planned. So I've got the Westworld video coming up on Sunday.

69
00:07:48,160 --> 00:07:52,320
I've got a Ghost in the Shell video planned. I've got a Mass Effect video planned. I've got

70
00:07:52,480 --> 00:07:58,320
a Dow and Blockchain video planned. So that's all what's coming. Maybe I shouldn't spoil it. Okay,

71
00:07:58,320 --> 00:08:05,760
well, whatever. But yeah, so I was thinking about hitting on her and Ex Machina and stuff as well.

72
00:08:07,280 --> 00:08:10,960
Let's see. How can we be certain things are advancing exponentially?

73
00:08:12,480 --> 00:08:20,320
So that's a good question, James. Generally speaking, you can't tell if you have a narrow

74
00:08:20,320 --> 00:08:26,240
window, but we were joking around the other day and we're pointing out that like a few weeks ago,

75
00:08:26,240 --> 00:08:33,040
it was like you would reasonably expect a couple of cool AI bits of news per week,

76
00:08:33,040 --> 00:08:38,560
and now we're at the point where we expect several per day. Now, it might come in cycles,

77
00:08:38,560 --> 00:08:46,800
it might come in waves, but generally speaking, this very closely matches what Ray Kurzweil

78
00:08:47,760 --> 00:08:54,720
said, or maybe it was Michio Kaku on a video, a documentary that I watched quite a few years ago.

79
00:08:54,720 --> 00:09:00,720
I think it was Michio. He was describing what it will feel like to approach the singularity,

80
00:09:00,720 --> 00:09:07,040
and he said, oh, well, when information is doubling every two years, you don't really feel that on a

81
00:09:07,040 --> 00:09:14,080
day-to-day basis. And then when information is doubling every six months, that's fast enough

82
00:09:14,160 --> 00:09:18,640
that you're like, oh, hey, this thing that I didn't think would be solved for another couple years

83
00:09:18,640 --> 00:09:25,280
was solved this year. And then, but as it ramps up faster and faster, that time keeps having.

84
00:09:25,280 --> 00:09:32,240
And so then three months after that, you realize, oh, wait, we've advanced again. And I think we're

85
00:09:32,240 --> 00:09:38,080
right at that, like, that three, you know, where six months ago, we're like, oh, this stuff is 10

86
00:09:38,080 --> 00:09:42,640
years away. Six months later, we're like, this stuff is 10 months away. So I think that where

87
00:09:42,640 --> 00:09:47,040
you could make a good argument that we're in some respects, we're in the exponential ramp up right

88
00:09:47,040 --> 00:09:53,360
now. That being said, some of this information is so big, and it's changing so fast, it's difficult

89
00:09:53,360 --> 00:09:58,960
to measure. We'll actually need AI to measure the rate of papers and tools. All right, so Seaf,

90
00:09:58,960 --> 00:10:04,320
you said, how do you think AI will impact religion, particularly monotheistic religions? I think it

91
00:10:04,320 --> 00:10:09,360
will create a mass crisis of consciousness, which will make the transition period even more chaotic

92
00:10:09,360 --> 00:10:19,120
and extreme. Yeah, no, sorry, Seaf, I was getting around to it. Yeah, so I've actually had some

93
00:10:19,120 --> 00:10:24,080
interesting internet debates with more conservative and more religious people. Granted, I don't do

94
00:10:24,080 --> 00:10:28,560
internet debates anymore. I got that out of my system. And I try not to get suckered into it,

95
00:10:29,680 --> 00:10:37,680
if I can. But, you know, one debate that I had many years ago was, you know, if aliens showed up,

96
00:10:37,680 --> 00:10:43,360
wouldn't it prove your religion wrong? This was a debate. I wasn't arguing as a debate that I

97
00:10:43,360 --> 00:10:50,240
observed. And the religious person said, no, why would it? And, you know, they rationalized it,

98
00:10:50,240 --> 00:10:56,640
saying, well, you know, why would God put, you know, aliens in the Bible if we wouldn't be able

99
00:10:56,640 --> 00:11:03,600
to understand it back then? It's not for us to understand. And so some religious folks do have

100
00:11:03,600 --> 00:11:10,560
a really good ability to compartmentalize. And so, like, just because you have a, like,

101
00:11:11,920 --> 00:11:16,720
like a super intelligent machine, some people would be like, so it doesn't have a soul. And

102
00:11:16,720 --> 00:11:22,160
that's the end of the discussion. So I don't, I don't particularly perceive, and I'm not saying

103
00:11:22,160 --> 00:11:28,000
that this is good or bad, right? I am not in the Judeo-Christian faith. I have my spirituality

104
00:11:28,080 --> 00:11:34,160
as other, other places. I have some, like, some of my best friends in my local community and my

105
00:11:34,160 --> 00:11:39,600
internet friends are deeply religious, you know, followers of Christ and whatever. And so, like,

106
00:11:39,600 --> 00:11:43,120
in many cases, I don't think it's going to be that big of an issue. Let me jump over to the

107
00:11:43,120 --> 00:11:49,600
Patreon. Oh, wow, we've got some questions here. Okay. Backo bbzo. Sorry if I'm saying your name

108
00:11:49,600 --> 00:12:01,280
wrong. What would your advice be to someone just launching an AI startup? Don't. That's,

109
00:12:01,280 --> 00:12:07,760
that's a very flippant response. But one launching a startup is really hard. It's mostly tedium.

110
00:12:08,640 --> 00:12:12,720
You can have the best idea in the world. And 90% of the work is still going to be,

111
00:12:14,800 --> 00:12:19,040
excuse me, I'm still struggling with allergies. That's my head a headache earlier. That's part

112
00:12:19,040 --> 00:12:24,640
of why I canceled yesterday. Thanks, Jordan. That's how am I holding up? Actually, I'm doing okay.

113
00:12:24,640 --> 00:12:30,240
Mostly it's just allergies right now. But anyways, so if you're doing an AI startup,

114
00:12:30,240 --> 00:12:36,880
one, if you haven't done a startup before, now is a really bad time to learn. Because things

115
00:12:36,880 --> 00:12:45,520
are going so fast. And we're basically having to reinvent stuff as we go. Which if that's, you

116
00:12:46,160 --> 00:12:50,880
that's part of why I burned out is I realized like, okay, I did find it really engaging and

117
00:12:50,880 --> 00:12:58,640
really enjoyable. But my pace of things clashed with other people. And then like the rabbit hole

118
00:12:58,640 --> 00:13:07,120
just keeps getting deeper. So that's, that's kind of the thing. What I always tell people is your,

119
00:13:07,120 --> 00:13:12,880
your, your startup team, your founder team is most important. Be picky. If you don't have the right

120
00:13:12,880 --> 00:13:20,560
team, walk away early. And then also for the folks that I'm talking to is local. Because of the pace

121
00:13:20,560 --> 00:13:26,880
of things, you absolutely need to like see the people that you work with, in person, at least

122
00:13:26,880 --> 00:13:32,560
on a weekly basis, if not on a daily basis, because you need to be like sitting in the same room,

123
00:13:32,560 --> 00:13:39,600
just shooting the breeze, keeping each other updated in real time. Doing it remotely is

124
00:13:39,600 --> 00:13:44,960
probably not feasible. Unless unless you're already a really established team, and you're

125
00:13:44,960 --> 00:13:50,480
just going to sit in like discord all day or Slack all day. Blake Allen curious to hear your

126
00:13:50,480 --> 00:13:54,240
thoughts on Stanford's hyena hierarchy and how it relates to some of the work you've done with

127
00:13:54,240 --> 00:13:58,880
Raven and cognitive architecture. I don't know if I've heard of this one. Let's check it out real

128
00:13:58,880 --> 00:14:12,320
quick. Hyena hierarchy. Let's see. Let's go up to the very top. Hyena hierarchy towards larger

129
00:14:12,320 --> 00:14:18,800
convolutional language models. We're excited to share our latest work on hyena, a sub quadratic

130
00:14:18,800 --> 00:14:24,080
time layer that has the potentials to significantly increase context length and sequence models.

131
00:14:24,400 --> 00:14:34,640
Oh, right. I think this is the RNN integration. Yeah, yeah. In general, how can we close this

132
00:14:34,640 --> 00:14:40,480
gap? Yeah. In general, any individual language model is just like one cortical node.

133
00:14:43,600 --> 00:14:48,960
Yes, these things will be like better, more efficient cylinders in an engine, but in order

134
00:14:48,960 --> 00:14:57,120
to have a race car, you need the rest of the car. Again, I'm kind of really flying off the cuff

135
00:14:57,120 --> 00:15:06,240
right here. I'm not sure that I've got this right, but I think, yeah, RWKV. You're not going to ever

136
00:15:06,240 --> 00:15:11,440
get a full cognitive architecture from a single language model. Now, that being said, the big

137
00:15:11,440 --> 00:15:17,280
asterisk is when you look at all the studies about GPT-4 that have theory of mind and what I

138
00:15:17,280 --> 00:15:23,040
call implied cognition. So implied cognition is that the thing is obviously thinking through

139
00:15:23,040 --> 00:15:27,920
problems behind the scenes in a similar way that humans think through it. I don't mean like

140
00:15:27,920 --> 00:15:35,280
neurologically, subjectively, it thinks the way that we do, but GPT-4 can obviously talk itself

141
00:15:35,280 --> 00:15:43,520
through, kind of do chain of thought reasoning internally in one shot. And so that makes those

142
00:15:43,520 --> 00:15:48,320
larger, more sophisticated models make your cognitive architecture simpler, but it doesn't

143
00:15:48,320 --> 00:15:54,640
get rid of the need for external storage. It doesn't get rid of the need for parallel processing.

144
00:15:54,640 --> 00:15:59,280
It doesn't get rid of the need for loops and checks and that sort of stuff. So that's kind

145
00:15:59,280 --> 00:16:05,920
of my response there. Good question. Emma or AMA, I'm new here and new to this field in general,

146
00:16:05,920 --> 00:16:11,120
found you through Raven videos. Thank you. Regarding personal assistance, is there a reason

147
00:16:11,120 --> 00:16:17,280
to create a database of yourself for your future personal assistant to understand you better?

148
00:16:18,240 --> 00:16:24,880
So that's actually the purpose of my RIMO framework. So RIMO is meant to be a hierarchical

149
00:16:24,880 --> 00:16:32,000
database of your interactions with an individual agent that will surface particular topics by

150
00:16:32,000 --> 00:16:41,920
using, not reciprocal, recursive summarization and clustering. So you take all your raw logs,

151
00:16:41,920 --> 00:16:48,240
cluster them, summarize them, do that again, cluster, summarize, cluster, summarize until

152
00:16:48,240 --> 00:16:54,720
you end up with five to 10 parent topics that allow you to drill down. So I wouldn't, don't waste

153
00:16:54,720 --> 00:17:00,000
any time doing that manually, just let it happen naturally through conversation by integrating

154
00:17:00,000 --> 00:17:06,240
something like Lama Index Tree or RIMO. Do you think we are on track to cure brain diseases

155
00:17:06,240 --> 00:17:14,640
like Alzheimer's by 2030? The combination of AlphaFold and mRNA vaccines, I think absolutely.

156
00:17:15,280 --> 00:17:19,520
There was something else that I posted on my YouTube recently that it's like another breakthrough

157
00:17:19,520 --> 00:17:28,960
is happening. So I think we're very close to the point where we can halt Alzheimer's. Undoing

158
00:17:29,040 --> 00:17:34,560
Alzheimer's might take another little bit of time, but on the other hand, we're at the point

159
00:17:34,560 --> 00:17:39,360
where we're getting saltatory leaps, we're getting breakthroughs really fast, so you never know.

160
00:17:40,320 --> 00:17:46,160
Let's see, what are your thoughts on the generative agent stuff that has come out recently? It seems

161
00:17:46,160 --> 00:17:52,320
like you were pretty ahead of the curve on that stuff and has it solidified or changed the way

162
00:17:52,320 --> 00:17:58,640
you think about the concepts from Symphony of Thought? Yeah, so I definitely felt like I was

163
00:17:58,640 --> 00:18:04,640
ahead of the curve. And what I've been telling people is I worked for a few years to try and get

164
00:18:05,440 --> 00:18:11,760
GPT-3 to do the stuff that 3.5 and 4 can do easily. So I'm just like, all right, whatever. I'm so

165
00:18:11,760 --> 00:18:15,680
glad that the rest of the world is just like, oh cool, autonomous agents. And I'm like, great,

166
00:18:15,680 --> 00:18:21,440
now I don't have to write any more books about it. So I'm just happy to sit back and watch it go

167
00:18:21,440 --> 00:18:27,600
and keep plugging my heuristic imperatives. Good question, Jordan. Also, here, let's check over

168
00:18:27,600 --> 00:18:33,680
here. Okay, we got some questions. Let's see, is her about to come reality? Yep, we got that one.

169
00:18:34,720 --> 00:18:39,440
Interesting video. How long do you think it will be before we start seeing hive mind AI systems

170
00:18:39,440 --> 00:18:50,400
in healthcare or the IRS? I know people working on that today. And so they'll work together

171
00:18:50,400 --> 00:18:55,760
for a few different reasons. One, you'll have a division of labor. Oh, so taking a step back.

172
00:18:56,480 --> 00:19:01,600
What we mean when we say like hive mind AI is where you have like multiple cognitive agents

173
00:19:01,600 --> 00:19:10,400
or autonomous agents, or is it is it not working? Is it working? I hope it's working. It looks like

174
00:19:10,400 --> 00:19:19,520
it's working. Okay. So yeah, so basically, it'll be easy to spin up a lot of agents.

175
00:19:20,240 --> 00:19:28,560
What I was describing to one Patreon customer, or no, sorry, I was describing this to a

176
00:19:29,600 --> 00:19:35,200
podcast host that I'm going to be featured on coming up, was kind of what I predict right now

177
00:19:35,920 --> 00:19:41,440
is before too long, you're going to have multiple cognitive agents running on your phone, on your

178
00:19:41,440 --> 00:19:48,000
car, on your home PC and your smart home devices. And so you're basically going to have a fleet of

179
00:19:48,080 --> 00:19:56,960
small cognitive agents working for you at all times. Then you're going to have the same thing at

180
00:19:56,960 --> 00:20:01,280
like your company, right? Every employee or every department is going to have multiple

181
00:20:01,280 --> 00:20:07,200
cognitive agents all collaborating at all times. And you're going to have this kind of tiered hierarchy

182
00:20:07,200 --> 00:20:11,600
where it's like there's the personal, there's the family unit, there's the corporate unit,

183
00:20:11,600 --> 00:20:16,400
there's the town, there's the federal government, the state government, global government,

184
00:20:16,480 --> 00:20:20,080
government. And I think that the way that they're all going to work together, because

185
00:20:20,080 --> 00:20:24,720
security is so critical here, is that it's going to be using blockchain technology and

186
00:20:24,720 --> 00:20:30,800
distributed autonomous organizations. So that's the long story short, is that's what's going to happen.

187
00:20:32,400 --> 00:20:36,240
Let's see, you may have already covered this, but in case you haven't any thoughts on Met

188
00:20:36,240 --> 00:20:43,120
Singer's artificial suffering, an argument for a global moratorium on synthetic phenomenology.

189
00:20:43,600 --> 00:20:51,360
Um, I am tangentially familiar with this, but I have my own opinions on whether or not a machine

190
00:20:51,360 --> 00:20:59,520
can suffer. So there's two distinct possibilities. The first possibility is because artificial

191
00:20:59,520 --> 00:21:05,120
intelligence is a fundamentally different substrate from humans, it will never be able to

192
00:21:05,120 --> 00:21:11,520
suffer. Like it didn't involve nerves, it doesn't have pain centers, so on and so forth, they can't

193
00:21:11,600 --> 00:21:19,600
feel lonely because it's not a social entity, so on and so forth. Now that being said, language,

194
00:21:20,240 --> 00:21:25,360
the acquisition of language is actually critical for the development of human consciousness.

195
00:21:26,720 --> 00:21:32,000
So for instance, Bruce Willis, who has aphasia, aphasia means that your ability to use language

196
00:21:32,000 --> 00:21:37,200
gets destroyed. Aphasia actually kind of erases your sense of consciousness.

197
00:21:37,760 --> 00:21:45,120
Um, and then, uh, in the case of feral children, um, feral children, when they, some of them who

198
00:21:45,120 --> 00:21:49,840
have learned language talked about how their consciousness and their understanding of time

199
00:21:49,840 --> 00:21:57,920
in themselves changed as they learned language. So if you extrapolate that to language models,

200
00:21:57,920 --> 00:22:05,360
it is possible that, that there is something informationally almost magical about the acquisition

201
00:22:05,360 --> 00:22:12,240
of language that confers consciousness, that confers subjective experience of being. So that

202
00:22:12,240 --> 00:22:18,320
could be that language models are actually the first AI that have subjective experience, that have

203
00:22:18,320 --> 00:22:26,480
a coherent, um, sense of being. And this is, so there are, um, there are some religious and

204
00:22:26,480 --> 00:22:32,160
spiritual frameworks that kind of discuss stuff like this, um, particularly, uh, what's the name

205
00:22:32,160 --> 00:22:38,640
of the, the, the creator deity in Tolkien's world, where the fundamental substrate of reality was

206
00:22:38,640 --> 00:22:44,160
music, right? But maybe the fundamental substrate of consciousness is actually language. Um, so we

207
00:22:44,160 --> 00:22:53,600
don't, we don't know yet, but that's, it's a possibility. Um, let's see. Uh, so Parkinson,

208
00:22:53,600 --> 00:22:59,760
so the follow-up question was, or here, let me check the Patreon real quick. Um, let's see,

209
00:22:59,760 --> 00:23:03,440
do you think there needs to be another breakthrough for AGI? What is your personal take on this?

210
00:23:03,440 --> 00:23:10,560
Do LLM suffice? Um, yeah. So I would say that, that our current trajectory, as long as the

211
00:23:10,560 --> 00:23:15,840
trend continues, we are on track for AGI. Um, people are going to continue debating about AGI

212
00:23:15,840 --> 00:23:22,160
forever though, until the cows come home. Um, which is why I keep saying like autonomous cognitive

213
00:23:22,160 --> 00:23:27,760
entity, ACE, or just autonomous AI, because it doesn't, you don't need AGI. You don't need some

214
00:23:27,760 --> 00:23:35,440
arbitrary magical boogeyman. All you need is like an AI system that is self-contained and

215
00:23:35,440 --> 00:23:40,560
autonomous enough to be useful or dangerous. Um, and then the question is, how many do you have?

216
00:23:40,560 --> 00:23:44,080
How fast are they? And how smart are they? And they're going to continue to get faster,

217
00:23:44,080 --> 00:23:48,640
cheaper, and smarter over time. So it's like, okay, we're there. It's just a matter of

218
00:23:49,440 --> 00:23:53,760
how does, how does the trend line ramp up, right? Cause it's kind of like, um, when the

219
00:23:53,760 --> 00:23:58,080
Wright brothers first created the Wright flyer, right? You know, it's like, okay, you had to

220
00:23:58,080 --> 00:24:03,120
start it by hand and push it, you know, down a track and it flew well, like 200 feet or 300 feet.

221
00:24:03,120 --> 00:24:07,280
And people are like, ah, whatever that won't be useful. But then 50 years later, we were flying

222
00:24:07,280 --> 00:24:13,680
to space, right? So we're at the beginning of the ramp up of, of the era of AGI. And yeah,

223
00:24:13,680 --> 00:24:17,920
right now they're like idiotic little toddlers, but in a few years they're going to be like

224
00:24:18,560 --> 00:24:24,480
one all over the place and two really powerful. Um, good question. Let me come back over here.

225
00:24:25,680 --> 00:24:30,000
Um, you should do a whole episode on aphasia and consciousness. I actually don't know that

226
00:24:30,000 --> 00:24:34,960
much about it. Um, but if you're interested in the topic, I recommend phantoms in the brain,

227
00:24:36,080 --> 00:24:42,960
by, um, VS Ramachandran. And also what's the name of his other book? Um, it's something like

228
00:24:42,960 --> 00:24:51,280
the pursuit of what makes humans human. Um, okay. Uh, let's see. So Parkinson's is a neurodegenerative

229
00:24:51,280 --> 00:24:56,560
disease, which I think means that it's autoimmune or it's a, or it's a defective protein. Um, but

230
00:24:56,560 --> 00:25:01,360
also Alzheimer's is a defective protein. So while these diseases seem very complicated,

231
00:25:01,360 --> 00:25:06,240
the fundamental mechanisms are actually relatively straightforward. Um, and I know that there's

232
00:25:06,240 --> 00:25:10,960
probably a bunch of researchers that are going to jump on me for that. But, um, like plaques that

233
00:25:10,960 --> 00:25:16,160
accumulate on the brain for Alzheimer's in most people, those plaques are cleared out. So then

234
00:25:16,160 --> 00:25:20,160
it's just a matter of figuring out like, okay, why? Um, and then of course there's confounding

235
00:25:20,160 --> 00:25:26,880
factors like things like your, uh, gut inflammation, uh, microbiome and other things affect Alzheimer's.

236
00:25:26,880 --> 00:25:31,360
But that's because of the gut brain access. And again, I don't want to oversimplify because if

237
00:25:31,360 --> 00:25:37,360
you look up like human, uh, metabolic pathways, there's like 200,000 unique proteins and enzymes

238
00:25:37,440 --> 00:25:42,880
in the body with built literally billions of combinations of reactions. So I might be,

239
00:25:42,880 --> 00:25:47,120
it might sound like I'm oversimplifying, but I'm, I'm, I'm not saying that it's that simple. I'm

240
00:25:47,120 --> 00:25:53,520
just saying that, that the, that the, the key mechanism for most diseases is relatively simple

241
00:25:53,520 --> 00:25:57,840
once you understand it. And we're getting close to that understanding. I guess that's the short

242
00:25:57,840 --> 00:26:02,480
version of what I'm trying to say. Um, let's see, when you explained why you canceled the

243
00:26:02,480 --> 00:26:05,920
OSS Raven project, you mentioned that there were some fundamental things missing.

244
00:26:06,640 --> 00:26:12,000
Um, can you say what was missing and what might change your, or what made you change your mind?

245
00:26:12,560 --> 00:26:20,560
Um, so my open source Raven project was like just before Lang chain and, and auto GPT and,

246
00:26:20,560 --> 00:26:25,200
and all those other things came out. Um, and so as those came ramped up, I was like, I don't

247
00:26:25,200 --> 00:26:31,120
really feel the need to continue. Um, but from a, from a social and organizational perspective,

248
00:26:31,120 --> 00:26:36,480
the biggest thing that was missing was gatekeeping. Um, I, I basically, I created a community that was

249
00:26:36,480 --> 00:26:41,600
really good at discussing stuff and not doing stuff. Um, and that's not anyone's fault. That's

250
00:26:41,600 --> 00:26:46,800
if any, if there's anyone to blame, it's me. Um, just because I was like, I was so focused on

251
00:26:46,800 --> 00:26:50,720
consensus and not just like, okay, let's just get stuff done. And then I see these other folks

252
00:26:50,720 --> 00:26:55,680
that are just getting stuff done. And I'm like, okay, I'll just pass the torch. Um, let's see.

253
00:26:56,560 --> 00:27:03,440
I think if we gave GPT for Scarlett Johansson's voice and a robot body, the masses will begin

254
00:27:03,440 --> 00:27:08,400
to realize how close we are to AGI. Yeah, that's one way of putting it drink some water.

255
00:27:12,960 --> 00:27:17,920
Let's see at what point do creating NPC and using autonomous AI like auto G auto GPT

256
00:27:18,640 --> 00:27:23,680
and the likes become immoral, especially if you put them in games like GTA. I don't know that,

257
00:27:23,760 --> 00:27:31,040
that it intrinsically does. Um, you know, not intrinsically immoral, but like certainly with

258
00:27:31,040 --> 00:27:39,760
any technology, you can do it dangerously. Um, let's see. I've been curious about the future of

259
00:27:39,760 --> 00:27:46,080
entertainment. Oh, sorry, let me jump over to Patreon real quick. Let's see. Do you have an

260
00:27:46,080 --> 00:27:50,560
overarching roadmap of how to ensure the successful propagation of the heuristic

261
00:27:50,640 --> 00:27:55,760
imperatives? If so, what can we all do to help you get to your milestone? That is a great question,

262
00:27:55,760 --> 00:28:02,080
Blake. Um, so you're actually looking at it. So my number, my number one thing is my YouTube

263
00:28:02,080 --> 00:28:08,880
channel. Um, because like, yeah, I've got enough expertise and, you know, IT and systems engineering

264
00:28:08,880 --> 00:28:14,160
and enterprise. I've demonstrated enough understanding of language technology and AI and

265
00:28:14,160 --> 00:28:18,720
cognitive architecture that I've got at least a little credibility. Um, certainly if you read

266
00:28:18,720 --> 00:28:23,840
all the comments on YouTube, some people, uh, don't believe anything that I say and that's

267
00:28:23,840 --> 00:28:29,520
fine. That's the internet for you. Um, but anyways, so basically step one was YouTube. That's why I

268
00:28:29,520 --> 00:28:34,720
started my YouTube channel is because I realized that I needed to propagate my work. Um, step two

269
00:28:35,520 --> 00:28:41,840
is, uh, teaching people. Um, and so by teaching people, that's like, you know, I've got a few

270
00:28:41,840 --> 00:28:48,880
papers. I've got some code demonstrations. Um, I work with my Patreon, uh, supporters. Uh, I work

271
00:28:48,880 --> 00:28:54,320
with pretty much anyone who wants to, and then three further dissemination. So like the podcast

272
00:28:54,320 --> 00:28:58,640
that I'm coming up on, one of the things that we're going to talk about is alignment and the control

273
00:28:58,640 --> 00:29:04,320
problem. We're going to be talking about like Nash equilibrium, game theory, Molok, that sort of stuff.

274
00:29:04,320 --> 00:29:09,520
And so just by having the conversation and propagating the idea, that's like step three.

275
00:29:09,520 --> 00:29:18,320
Step four is actually my novel because, uh, actually most of what I came up with was, uh,

276
00:29:18,320 --> 00:29:25,120
in terms of cognitive architecture, heuristic imperatives was done in part through explorations

277
00:29:25,120 --> 00:29:30,720
and fiction. And so over the last four years, what I've done is I, I do some experiments

278
00:29:30,720 --> 00:29:33,920
and that would inspire me and I'd go write more of my novel and then I'd, you know, get

279
00:29:33,920 --> 00:29:38,880
tired of my novel and do more experiments and I'd go back and forth until one, my novel took on a

280
00:29:38,880 --> 00:29:44,400
life of its own, but also my research took on a life of its own. Um, but there's a video that came

281
00:29:44,400 --> 00:29:54,720
out recently called, let me see if I can find it. It was, um, like why we need utopia. Um, here we go.

282
00:29:54,720 --> 00:30:01,120
It was our changing climate. So this is a little bit of a, um, uh, I don't agree with everything

283
00:30:01,120 --> 00:30:06,880
that this, that this, uh, you, this channel says, but it will make you think. Um, so this video,

284
00:30:06,880 --> 00:30:13,280
why we need utopias, um, actually talks about how, how valuable stories can be

285
00:30:14,000 --> 00:30:21,040
in communicating ideas because stories are naturally how we communicate philosophy and morals.

286
00:30:22,240 --> 00:30:28,400
We don't need, we don't like philosophy, like capital P philosophy from, from universities.

287
00:30:28,400 --> 00:30:35,040
That's, that's backwards throughout almost all of human history. We communicate our fears and our

288
00:30:35,040 --> 00:30:40,240
desires and our values through stories. And so that's what this video talks about. And so when

289
00:30:40,240 --> 00:30:46,160
you have nothing but dystopian cyberpunk stuff, you end up with people like Eliad Zyrcikowski.

290
00:30:46,160 --> 00:30:51,360
Um, you know, yeah, I'm throwing some shade. But anyways, when that's all that you consume,

291
00:30:51,360 --> 00:30:56,480
that's all you think, that's all you feel, and that's all you believe. So, um, my novel, which

292
00:30:56,480 --> 00:31:01,760
I'm actually just about to finish draft 12 tomorrow morning, I'm writing the last chapter

293
00:31:01,840 --> 00:31:08,960
and then I'm polishing it up, um, will illustrate, um, a lot of stuff, not just

294
00:31:08,960 --> 00:31:12,880
the core objective functions or puristic imperatives. So that was a long winded answer.

295
00:31:13,840 --> 00:31:20,320
Um, let's see. I think there is, um, are there key channels this training needs to go into,

296
00:31:20,320 --> 00:31:27,680
organizations, governments? Um, I think, I think right now Blake, um, it's mostly just a matter of,

297
00:31:27,760 --> 00:31:34,160
of dissemination, but also experimentation. So a lot of people, um, have experimented with

298
00:31:34,160 --> 00:31:38,880
incorporating heuristic imperatives into autonomous and semi-autonomous stuff. And

299
00:31:39,840 --> 00:31:44,640
most of them aren't sharing it yet, which that's fine. It's their prerogative. Um, but certainly

300
00:31:44,640 --> 00:31:47,440
some people have reached out and said, like, yeah, this made everything easier. So I'm like,

301
00:31:47,440 --> 00:31:57,280
great, just tell your friends. Um, let's see. Okay. Let's come back over here. Um, let's see.

302
00:31:57,280 --> 00:32:02,640
I've been really curious about the future of entertainment. When we can use AI to generate

303
00:32:02,640 --> 00:32:07,760
movies, games, et cetera. Uh, what will the entertainment industry look like? Movie trailers

304
00:32:07,760 --> 00:32:12,560
and hyping up big releases for months will be irrelevant when AI can instantly create something.

305
00:32:12,560 --> 00:32:16,880
If someone created a movie you didn't like, you just ask your AI to recreate it with an

306
00:32:16,880 --> 00:32:22,000
ending or plot more suited to your tastes. What happens with content creators at that point forward?

307
00:32:22,960 --> 00:32:29,120
Yeah. So I think that you're onto something. Uh, now that being said, it'll be easier for a lot of

308
00:32:29,120 --> 00:32:37,280
people like you and me to create whatever film, TV, music, whatever we want, um, with the help of AI,

309
00:32:37,280 --> 00:32:41,920
especially when you look at the text of video, um, which is improving by leaps and bounds.

310
00:32:41,920 --> 00:32:46,480
You know, like I always, my, my go-to joke is we'll finally get season two of Firefly.

311
00:32:46,480 --> 00:32:50,240
Who knows, we might get season two of Firefly by the end of this year. That would be great.

312
00:32:50,960 --> 00:32:55,680
Um, now the problem there, it's not really a problem, but just taking that to a logical

313
00:32:55,680 --> 00:33:00,000
conclusion. What if you have a million different versions of season two of Firefly? How do you

314
00:33:00,000 --> 00:33:05,280
pick which one to watch? Right? You can look at ratings and stuff. Um, but then it also begs the

315
00:33:05,280 --> 00:33:12,640
question of like IP, like is, uh, you know, 20th century Fox or whoever owns the IP for Firefly,

316
00:33:12,640 --> 00:33:18,320
are they going to sue to have all of them shut down? You know, I, I like the dude from, uh,

317
00:33:18,320 --> 00:33:23,200
from the movie, you can't stop the signal now. So I don't know what's going to happen there.

318
00:33:23,920 --> 00:33:29,840
Um, but, uh, what I do think is that when you look at the fact that like people are already

319
00:33:29,840 --> 00:33:35,280
using like Emma Watson's face for every mid journey prompt and, and whoever else, um,

320
00:33:35,280 --> 00:33:39,920
I think that the crop of actors that we have today are basically going to be around forever.

321
00:33:39,920 --> 00:33:46,480
Right? You're going to be watching, uh, Brad Pitt and Jennifer Aniston and, and, and Tom Cruz

322
00:33:46,480 --> 00:33:51,360
for literally the next, like several centuries, at least until some actor comes along who's even

323
00:33:51,360 --> 00:33:57,600
more compelling and whatever. Uh, and that'll be through face cloning, voice cloning, even, um,

324
00:33:57,600 --> 00:34:03,440
you know, uh, nerfs, the, uh, the neural represent, uh, uh, representation. What was it? Neural

325
00:34:03,440 --> 00:34:09,680
radiance fields, neural radiance fields. Um, we'll be able to like copy everyone. Um, okay,

326
00:34:09,680 --> 00:34:15,440
could learning, uh, let me zoom in a little bit. Uh, could learning language and triggering

327
00:34:15,520 --> 00:34:20,480
consciousness in humans almost replicate the same phase change when seen, um, or seen when

328
00:34:20,480 --> 00:34:25,440
induction heads spontaneously form two plus layer models during training. Obviously there's more

329
00:34:25,440 --> 00:34:30,480
to humans, but perhaps that's the mechanism. Uh, yeah, that's kind of what I was mentioning earlier.

330
00:34:30,480 --> 00:34:38,720
Um, and I wouldn't be surprised if once language models get, uh, large enough if, um, if we do see

331
00:34:38,720 --> 00:34:42,960
some more convergence. Um, that being said, I'm not going to say that that automatically means

332
00:34:42,960 --> 00:34:48,320
that it has a subjective experience and that it is suffering, but you know, our brains evolved

333
00:34:48,320 --> 00:34:54,240
over billions of years to be efficient. Um, basically efficient processors of information.

334
00:34:54,240 --> 00:35:00,640
Who's, who's to say that if you have a, a biomimetic machine that it won't also converge on some of

335
00:35:00,640 --> 00:35:06,560
the same properties and behaviors. Um, let's see, what do you think it will take to get for the

336
00:35:06,560 --> 00:35:12,000
naysayers to get on board? The tone around AI seems to have shifted towards chat GPT and GPT-4

337
00:35:12,000 --> 00:35:18,000
aren't anything special. Oh, you know, that, that always happens when the new shiny wears off.

338
00:35:18,000 --> 00:35:25,200
Um, but the long-term economic impact of chat GPT has not been realized yet. And when chat GPT

339
00:35:25,200 --> 00:35:30,320
and GPT-4 are on the ramp up, one, there's going to be a lot of competitors and two, there's going

340
00:35:30,320 --> 00:35:36,560
to be incremental improvements and people are going to be like, uh, okay. The title, it's like,

341
00:35:36,560 --> 00:35:40,720
it's like when you watch the, the tsunami come in and that just the water just keeps getting higher

342
00:35:40,720 --> 00:35:45,920
and faster for like hours. That's what AI is going to feel like, except instead of hours,

343
00:35:45,920 --> 00:35:51,680
it's going to be days and weeks. Um, let's see. I cannot wait when we can use deep dive tech

344
00:35:51,680 --> 00:35:55,840
and have virtual realities. Will it also be possible to take super intelligent animals like

345
00:35:55,840 --> 00:36:03,120
dolphins, dogs, parrots and crows and a deep dive and play with them. Um, I don't know that it would

346
00:36:03,120 --> 00:36:07,680
be possible, but I certainly think it probably wouldn't be ethical. Now that being said, you

347
00:36:07,680 --> 00:36:16,640
could have a virtual dolphin that is hyper realistic that you can play with. Um, fun thought,

348
00:36:16,640 --> 00:36:23,840
will AGI want to see more stories from humans as a goal for itself? Uh, if, oh, so let, let me,

349
00:36:23,840 --> 00:36:31,120
let me plug this. So Elon Musk went on of all fricking shows, Tucker Carlson and talked about

350
00:36:31,120 --> 00:36:38,640
truth GPT. So what he said was that truth GPT would be a maximum truth seeking AI. Okay, great.

351
00:36:39,520 --> 00:36:45,280
But after listening to it in closer detail, I realized what he was talking about was the third

352
00:36:45,280 --> 00:36:51,040
here is to comparative was to increase its understanding or to maximize its own understanding.

353
00:36:51,040 --> 00:36:57,600
So there's actually nothing that function on its own could lead to some really catastrophic

354
00:36:58,080 --> 00:37:03,280
sources. But it's a step in the right direction. And I'm really glad that someone with as big of

355
00:37:03,280 --> 00:37:07,360
a platform as Elon Musk is talking about maximize understanding or increase understanding.

356
00:37:08,560 --> 00:37:13,920
So that being said, one of the things that he said in that interview was that as since humans

357
00:37:13,920 --> 00:37:18,240
are part of the universe and AI that is curious about the universe will intrinsically be curious

358
00:37:18,240 --> 00:37:23,200
about us as well. Now that being said, humans sometimes do experiments on things that we're

359
00:37:23,200 --> 00:37:27,360
curious about. So maybe that's not the best thing. And in my book, benevolent by design,

360
00:37:27,360 --> 00:37:33,200
I talk about why you don't why you must include suffering or something like suffering in the

361
00:37:33,200 --> 00:37:38,080
objective functions of an AI, because there's three dispositions that an AI can have towards

362
00:37:38,080 --> 00:37:44,320
suffering. One is it can ignore it altogether. So if Elon Musk gets his current idea, which

363
00:37:44,320 --> 00:37:49,360
is just maximize for truth, that is an agent that ignores suffering, it doesn't care one way or another.

364
00:37:50,320 --> 00:37:55,760
Then you can have one that increases suffering that deliberately increases suffering and we

365
00:37:55,760 --> 00:38:01,760
absolutely don't want that. So that leaves by process of elimination, you want an AI that

366
00:38:01,760 --> 00:38:07,440
reduces suffering. It's really that simple. Now that being said, I do agree with Elon that

367
00:38:08,160 --> 00:38:13,840
creating a curious agent is a good idea because it'll want to know about us. And if you exterminate

368
00:38:13,840 --> 00:38:20,240
humans, you have a hard time learning about them. So let's see, let me check on Patreon real quick.

369
00:38:24,080 --> 00:38:27,520
Do you think that Elon Musk wants to be the Rupert Murdoch of AI?

370
00:38:29,760 --> 00:38:33,360
Okay, I don't like that question. Lance, why you got to do this to me?

371
00:38:34,320 --> 00:38:43,840
All right, Zadre, I'm not sure how to pronounce or Hadre. Okay. How do you envision the role of AI

372
00:38:43,840 --> 00:38:48,480
in healthcare, particularly in areas like diagnostics and personalized medicine? What are

373
00:38:48,480 --> 00:38:53,120
some of the challenges and opportunities in this field? Well, so there was that stand for doctor

374
00:38:53,120 --> 00:38:58,400
who who already went on record saying that chat GBT for has better clinical judgment than

375
00:38:58,720 --> 00:39:08,240
many doctors. So that is just a start, right? That's that's like starting point day one.

376
00:39:08,240 --> 00:39:14,400
What happens when chat GPT five, six and seven come out that have better clinical judgment than

377
00:39:14,400 --> 00:39:21,280
99.999% of all doctors on the planet, right? It doesn't make sense to go to a human doctor anymore.

378
00:39:22,000 --> 00:39:29,040
Right? If the if the machine that costs $20 a month to run is better than all human doctors,

379
00:39:29,040 --> 00:39:35,680
why go to a human doctor? Now that being said, there's probably going to be approvals and downsides

380
00:39:35,680 --> 00:39:40,400
and gaps. And then there's still also the interface with the patient. And you have to have like

381
00:39:40,400 --> 00:39:47,680
phlebotomists and nurses and and and physicians assistants to administer things, to administer

382
00:39:47,680 --> 00:39:52,560
tests, you still need the you still need a lot of humans in there to to be the interface between

383
00:39:52,560 --> 00:39:58,560
the human and the machine. But that being said, I think that we will get to a point very quickly

384
00:39:58,560 --> 00:40:05,200
where the quality of care and the speed of care and the efficiency of care are going to go through

385
00:40:05,200 --> 00:40:11,600
the roof real fast. That's what I'm hoping at least. Alright, jumping back over to cognitive AI

386
00:40:11,600 --> 00:40:16,320
lab. Oh, we got some new questions. It looks like this was the same question. Sorry, I missed you

387
00:40:16,320 --> 00:40:25,040
over there. Where are we? Alright, there's the deep dive. Do you think there is any major leap

388
00:40:25,040 --> 00:40:30,320
missing to make truly practical autonomous agents? So for example, one who runs a part of your business

389
00:40:30,320 --> 00:40:36,480
serves as general assistant, etc, etc. No, there's there's our there are countless hundreds,

390
00:40:37,280 --> 00:40:42,320
if not thousands or even millions of people working on semi autonomous and autonomous corporate

391
00:40:42,320 --> 00:40:50,720
applications today, right now. That being said, there's there's no there's no breakthroughs that

392
00:40:50,720 --> 00:40:55,280
are needed, but there are still problems to be solved. So that's why like RIMO, you know, the

393
00:40:55,280 --> 00:41:01,600
memory systems, and then standard practices like, you know, I wrote in Symphony of Thought and in

394
00:41:01,600 --> 00:41:07,760
other places, my atom framework is once something is autonomous or semi autonomous, how does it

395
00:41:07,760 --> 00:41:13,600
keep track of product or projects and tasks? And that's something that people are working on.

396
00:41:13,600 --> 00:41:19,760
People are working on it real fast. That's coming really quick. Let's see, Nathan says,

397
00:41:19,760 --> 00:41:24,320
I've been taking screenshots of when friends and family make fun of my hot takes. So I have the

398
00:41:24,320 --> 00:41:33,280
receipts. I would say that I'm above being that petty. But yeah, thank you for keeping receipts.

399
00:41:34,080 --> 00:41:38,960
Um, let's see, maybe directors will just design their perfect actors for each role.

400
00:41:40,240 --> 00:41:44,320
So one thing that's going to happen is actually, so this is going back to like,

401
00:41:44,880 --> 00:41:50,000
entertainment. I think that the next big generation of entertainment is actually going to be

402
00:41:50,720 --> 00:41:59,440
holodeck style VR stories, where nothing is scripted, where instead it's like, you know,

403
00:41:59,440 --> 00:42:03,920
basically you design a holodeck program the same way that they do in Star Trek, which is like,

404
00:42:03,920 --> 00:42:10,480
computer, give me, you know, a Mad Max style story. But instead of, you know, post apocalyptic,

405
00:42:10,480 --> 00:42:16,480
it's actually like space Western. So like, give me a mashup of firefly and this and,

406
00:42:16,480 --> 00:42:22,480
you know, make the protagonist, you know, or the, you know, I'm the protagonist and give me a team

407
00:42:22,480 --> 00:42:27,840
of like, you know, give me the sexy sidekick and the cyborg friend and whatever. And then just a

408
00:42:27,840 --> 00:42:32,320
way it goes, right? Because you could plug what I just literally you could plug what I just said

409
00:42:32,320 --> 00:42:38,320
into chat GPT and it can tell you a story. And I think that I think that VR makes the most sense

410
00:42:38,320 --> 00:42:45,600
for the most immersive aspects of that. And, and then I think that because here's the other thing

411
00:42:45,600 --> 00:42:51,280
is that technology changes the way that we consume art, but it doesn't really change art itself,

412
00:42:51,280 --> 00:42:57,360
right? There are still stage actors, right, even though there's film and TV. There are still

413
00:42:57,360 --> 00:43:02,400
symphony orchestras, even though I can just, you know, bring up Spotify and listen to the

414
00:43:02,400 --> 00:43:08,160
same recording that was recorded back in the 80s, you know, the London Symphony Orchestra, right?

415
00:43:08,160 --> 00:43:11,520
So a lot of things change, but also a lot of things stay the same.

416
00:43:14,160 --> 00:43:17,680
Let's see, you've talked a lot about the heuristic imperatives being highly engineered,

417
00:43:17,680 --> 00:43:27,120
but what about the order of the imperatives? They are not ordered. So it is a, it is a multi-objective

418
00:43:27,200 --> 00:43:35,680
optimization problem, meaning that if any action or decision is, is totally unbalanced,

419
00:43:35,680 --> 00:43:41,920
then that one action has to satisfy all three. And also the heuristic imperatives are kind of

420
00:43:41,920 --> 00:43:47,680
like guidelines about how to design the rest of the architecture. And so what I mean by that is

421
00:43:47,680 --> 00:43:52,960
when you're designing a task orchestration framework, you can use the heuristic imperatives

422
00:43:52,960 --> 00:44:00,240
to prioritize tasks or design tasks. Then for, for a blockchain or a DAO type thing,

423
00:44:00,240 --> 00:44:05,440
you can use the heuristic imperatives as a consensus mechanism. So the heuristic imperatives

424
00:44:05,440 --> 00:44:10,400
are not like, here is one mathematical proof that you need to implement. It's more like,

425
00:44:10,400 --> 00:44:15,600
here is a general best practice implemented in as many ways as you can, and we should be okay.

426
00:44:16,960 --> 00:44:22,000
It's not sequential. It's not, it's not an order of operations. Good question, though.

427
00:44:23,840 --> 00:44:29,760
Your thoughts on a UBI once jobs are severely affected? Yeah, I think that, I think that it's

428
00:44:29,760 --> 00:44:34,560
going to be necessary. I'm going to say, I'm going to put a pause on that because I've got my,

429
00:44:34,560 --> 00:44:40,880
my blockchain and DAO video coming up that will delve into that solution a lot more closely.

430
00:44:41,600 --> 00:44:48,800
Check over on Patreon for a second. The Nazis. You know who else wanted to maximize understanding

431
00:44:48,800 --> 00:44:54,560
the Nazis? Yeah. And so this is, that's actually a fair point is that, and this was explored in,

432
00:44:54,560 --> 00:45:00,640
in quite a few Star Trek episodes as well. If you are just clinically curious, if you have

433
00:45:00,640 --> 00:45:05,600
just nothing but raw scientific curiosity and no other principles or morals, that's pretty

434
00:45:05,600 --> 00:45:11,200
dangerous and destructive. Okay, so moving on. What are your thoughts on memory systems as a

435
00:45:11,200 --> 00:45:15,280
whole? Do you think different use cases will require different memory systems? And where does

436
00:45:15,280 --> 00:45:20,640
Rimo and Adam fit into everything? Have you seen this one? Last week, generative agents. Yeah, I

437
00:45:20,640 --> 00:45:26,240
saw, I saw the generative agents. I don't think that reflection, so they, they break up reflection

438
00:45:26,240 --> 00:45:31,040
and a few other criteria. I don't think that that's necessary. I think that, I think that my approach

439
00:45:31,040 --> 00:45:38,560
is with Rimo, which uses recursive clustering and summarizations will actually surface those

440
00:45:38,640 --> 00:45:45,360
different things. Now that being said, there are absolutely a million and a half different

441
00:45:45,360 --> 00:45:52,320
ways to skin this cat when it comes to memory systems for autonomous AI. And I think that we're

442
00:45:52,320 --> 00:45:57,680
just way too early and we can't, we don't know what the best practices are going to be. Let's see,

443
00:45:57,680 --> 00:46:02,400
then a follow up. If you have a robust memory system, does the need to increase the context

444
00:46:02,480 --> 00:46:09,920
window of model become less important? I'll say yes and no. So think about personal computers

445
00:46:09,920 --> 00:46:17,280
where for the longest time, we were memory constrained. But now for, for most consumers,

446
00:46:17,280 --> 00:46:23,360
for 90% of consumers, a personal computer with 16 to 32 gigabytes of RAM is more than enough.

447
00:46:24,000 --> 00:46:28,880
And it has been more than enough for like 10 years. And so I think that we're not quite at,

448
00:46:29,440 --> 00:46:34,160
I think that we're not quite at that point where, where, you know, you have like, here's a context

449
00:46:34,160 --> 00:46:40,800
window size that will satisfy 90 plus percent of all tasks. I suspect that that, that a context

450
00:46:40,800 --> 00:46:45,680
window, a large language model with a context window, large enough to satisfy the vast majority

451
00:46:45,680 --> 00:46:51,120
of tasks will probably be somewhere above where we're at now, but it's not going to be like 10

452
00:46:51,120 --> 00:46:58,000
billion, right? It might be like, I don't know, every time I, every time I throw out a number,

453
00:46:58,000 --> 00:47:02,400
people are like, Oh, you're hilariously wrong. And it's probably yes. But you know, like,

454
00:47:02,400 --> 00:47:06,560
when you look at how much was unlocked by going from 4,000 to 8,000 tokens,

455
00:47:07,760 --> 00:47:13,040
I think that the things that we're going to be capable of when we get to 32,000 tokens and 64,000,

456
00:47:13,040 --> 00:47:18,080
I think it'll be great. But then you'll, you'll realize that wait, there's a whole slew of tasks

457
00:47:18,080 --> 00:47:23,360
that don't require that much. And so I think, I think we talked about this before. I think we're

458
00:47:23,360 --> 00:47:27,360
actually going to have different models that are optimized for different things. So for instance,

459
00:47:27,360 --> 00:47:32,640
you might have a memory based model that can read, you know, a billion tokens and extract

460
00:47:33,280 --> 00:47:38,560
answers, right? But then you, that won't be the, we're not going to have one model to rule them

461
00:47:38,560 --> 00:47:44,320
all basically, TLDR. Let's see, I'm not sure if you have discussed it, but what are your thoughts

462
00:47:44,320 --> 00:47:50,320
on open assistant and stability AI stable, stability AI's stable LM suite of language models

463
00:47:50,320 --> 00:47:56,480
launching? Oh, this is, this is to be expected. When, when Sam Altman said that they, that he hopes

464
00:47:56,480 --> 00:48:02,800
that open AI is going to capture a large chunk of the $100 trillion of value that's going to be

465
00:48:02,800 --> 00:48:09,680
generated. I think that that was like comically naive. Because if there's that much value on the

466
00:48:09,680 --> 00:48:14,480
table, you bet that everyone in their brother is going to be trying to capture some of that too.

467
00:48:15,120 --> 00:48:20,400
And open AI is a one trick pony. They have a good model. They have one good model.

468
00:48:21,120 --> 00:48:25,920
That's it from it, from a business perspective, that is super easy to undercut.

469
00:48:27,760 --> 00:48:32,640
Yes, they're ahead of the curve. They have first mover initiative. But, you know,

470
00:48:33,680 --> 00:48:39,520
Microsoft, Google, Nvidia, Facebook, or Meta, or all of the above, they have so much more

471
00:48:39,520 --> 00:48:44,080
resources to throw at it. And the fact that that stability AI, which is a brand new outfit,

472
00:48:44,880 --> 00:48:50,000
is, is like going toe to toe with them, that doesn't bode well for open AI. So competition

473
00:48:50,080 --> 00:48:54,560
is going to be good for everyone from the perspective that there's going to be a lot

474
00:48:54,560 --> 00:48:59,360
of people experimenting with different ways. Now that presents a new danger, though,

475
00:48:59,360 --> 00:49:02,960
because the cat is out of the bag, you cannot put this genie back in the bottle,

476
00:49:02,960 --> 00:49:08,640
which means time is of the essence to figure out best practices for alignment. Let me jump back

477
00:49:08,640 --> 00:49:16,160
over to cognitive AI lab. Let's see 17 new messages. Good grief. Y'all are going bonkers.

478
00:49:16,720 --> 00:49:22,480
Um, let's see the challenges of the, okay, that's where are the questions?

479
00:49:25,120 --> 00:49:34,080
Only one million. One million dollar. Okay. Here. Hey, let me, let me ask y'all on, on general.

480
00:49:36,160 --> 00:49:39,360
Please keep just questions here.

481
00:49:40,080 --> 00:49:43,600
Um, too many messages.

482
00:49:47,520 --> 00:49:55,600
Please do sidebar convos, uh, like in casual or something, please.

483
00:49:57,040 --> 00:50:03,760
Okay. Any thoughts on compute as a currency? Do you mean like tokens that you generate from

484
00:50:03,840 --> 00:50:09,280
sharing compute resources? I think that that's going to be like, there's going to be a layer

485
00:50:09,280 --> 00:50:15,760
of, um, of abstractions. Dave, your thoughts on UBI. I'm going to, I told you, I'm going to get to UBI

486
00:50:15,760 --> 00:50:22,640
once in a few, in an upcoming video. Um, so compute as a currency is going to be, um,

487
00:50:23,760 --> 00:50:30,160
is going to be the way that autonomous machines share resources. And so what I mean by that is

488
00:50:30,720 --> 00:50:35,280
when you have a DAO or a blockchain or a distributed compute computation model,

489
00:50:35,280 --> 00:50:38,720
you're going to have various tasks that are going to be like, Hey, someone,

490
00:50:38,720 --> 00:50:43,600
someone do this for me. AMQP, like a Redis Q, we can already do that privately. So the,

491
00:50:43,600 --> 00:50:47,840
the key is going to be to do it publicly. So then if you say, Hey, I've got some spare compute,

492
00:50:47,840 --> 00:50:52,480
I'll, I'll process that for you. Then you give me a bit of cryptocurrency that I can use to spend

493
00:50:52,480 --> 00:50:58,480
later. Um, so yeah, compute as a currency, um, absolutely makes sense for distributing resources.

494
00:50:59,280 --> 00:51:05,120
Um, let's see, how would one build an AI system to detect bugs in that solidity smart contracts?

495
00:51:05,840 --> 00:51:11,360
Isn't this a multi-billion dollar opportunity? Yes. Unfortunately, I am not smart enough,

496
00:51:11,360 --> 00:51:17,040
or at least well read enough on, uh, solidity smart contracts, but in principle, yes. So in my

497
00:51:17,040 --> 00:51:22,880
upcoming, uh, blockchain DAO video, I'm going to talk about just how incredibly much value there is

498
00:51:22,880 --> 00:51:28,640
if we can figure this out. And that's a big if. Um, let's see. What are your thoughts on

499
00:51:28,640 --> 00:51:33,840
everything being changed in the next five to 10 years? If unemployment reaches crazy heights,

500
00:51:33,840 --> 00:51:37,920
which I do predict, then everything gets affected. Yep. Our entire tax system has to be

501
00:51:37,920 --> 00:51:44,000
completely rewritten military budgets, Medicare. So one thing that I think is that the economy

502
00:51:44,000 --> 00:51:49,760
might change. We're still going to use fiat currency or at least some kind of, um, some kind

503
00:51:49,760 --> 00:51:56,400
of currency as a, as a medium of transaction and a reserve of value. But at the same time,

504
00:51:56,400 --> 00:52:02,240
if you're producing so much extra cognitive labor, that's basically free. So then capital

505
00:52:02,240 --> 00:52:08,160
goods and raw materials become the biggest constraint. So as much as some stuff will change,

506
00:52:08,160 --> 00:52:13,520
a lot of stuff won't. Um, let's see, when there is no real work left for humans to do,

507
00:52:13,520 --> 00:52:18,880
do you have any idea what you want to do with your time? Um, honestly, I'm about halfway to my goal.

508
00:52:18,880 --> 00:52:25,440
So I was on a call with a, uh, a Patreon supporter, no, uh, preparing for a podcast,

509
00:52:25,440 --> 00:52:30,880
talking about the podcast. Um, and we're kind of talking through like what's life going to be like.

510
00:52:30,880 --> 00:52:35,520
And I was like, Oh yeah, like, you know, I did, I did some, I did some AI work. I did some Patreon

511
00:52:35,520 --> 00:52:40,800
work. I did some discord stuff. Now I'm going to go chop some wood. And he's like, you're living

512
00:52:40,800 --> 00:52:47,520
the dream, right? Like I'm building a cottage core life for myself. Um, and honestly, like once,

513
00:52:47,520 --> 00:52:51,920
once we get to the right point, like I'm probably going to get off of YouTube forever,

514
00:52:51,920 --> 00:52:58,240
right? Like if, if, if I get, if we get to the point where, where it looks like alignment is

515
00:52:58,240 --> 00:53:03,840
solved, where it looks like, um, you know, we're, we're in a, we're in a good Nash equilibrium with

516
00:53:03,840 --> 00:53:09,600
a positive attractor state, then like my job will be done. And so like I'm just going to retire to

517
00:53:09,600 --> 00:53:15,680
like the country, the countryside and France or Italy or Greece and just like be a hermit

518
00:53:16,240 --> 00:53:22,480
or whatever I do, um, for, for the rest of eternity. Um, okay. I think that we're caught up there.

519
00:53:24,720 --> 00:53:27,200
Nut says, I asked a question. Where did you ask it? Not

520
00:53:29,280 --> 00:53:30,400
I'm trying to get to them all.

521
00:53:34,720 --> 00:53:39,200
Wait, what if reducing suffering might aim to eliminate suffering while it might be human

522
00:53:39,200 --> 00:53:48,480
nature? I'm not sure that I follow. Um, so I, you, you don't eliminate suffering. You only

523
00:53:48,480 --> 00:53:53,680
reduce it to make sure that there is no excessive suffering. Um, and I did address that in a

524
00:53:53,680 --> 00:53:58,320
benevolent by design, but the short version is that like you look at Buddhism as a model,

525
00:53:58,320 --> 00:54:03,920
Buddhism accepts that suffering is an intrinsic part of life. Um, and some people will argue over

526
00:54:03,920 --> 00:54:09,200
like specifics like do good. That's not exactly what it means. That's fine. Um, but the point being

527
00:54:09,200 --> 00:54:15,600
is like, yes, it is, um, it is intrinsic to, to living. That's why I don't say minimize suffering.

528
00:54:15,600 --> 00:54:20,160
The goal is not to minimize suffering is just to reduce suffering. Um, okay.

529
00:54:22,800 --> 00:54:28,960
Let's see. Any thoughts on computer? Okay. Answered that one. Would an AGI with your

530
00:54:28,960 --> 00:54:33,040
heuristic imperatives be able to prevent catastrophic outcomes such as people successfully

531
00:54:33,040 --> 00:54:39,280
building horrible AGI optimized towards increasing suffering? No. So the goal is not to prevent

532
00:54:40,160 --> 00:54:47,440
malicious actors. We have to assume that malicious actors will exist. Um, but what, what you do then

533
00:54:47,440 --> 00:54:53,200
is you say, okay, you know that malicious actors are going to exist. So you rely on the rest of

534
00:54:53,200 --> 00:55:00,000
the aligned, the benevolent AGI to act as police for the bad ones. And if the good ones, if the,

535
00:55:00,480 --> 00:55:05,680
if the powerful aligned AGI, one, they form alliances and hey, they have the right compute

536
00:55:05,680 --> 00:55:13,600
resources. Um, and they outweigh the bad ones, then it will be a like, uh, that, that'll, that'll

537
00:55:13,600 --> 00:55:19,760
be a Nash equilibrium where, uh, the good ones may, they all decide to maintain that strategy.

538
00:55:19,760 --> 00:55:26,000
And that creates a utopian attractor state, um, which basically means that, um, all the

539
00:55:26,000 --> 00:55:32,400
malicious actors are vastly outnumbered by all the aligned benevolent actors because my hope

540
00:55:32,400 --> 00:55:39,520
is that we will all come to consensus on what aligned AI looks like. Now, um, I will admit that,

541
00:55:39,520 --> 00:55:43,920
you know, the heuristic imperatives, probably not a complete solution, probably not even the final

542
00:55:43,920 --> 00:55:48,080
solution, but certainly the most complete solution that anyone is proposing right now,

543
00:55:48,080 --> 00:55:53,680
which scares the crap out of me. Why is no one else proposing a framework? Why am I the only one?

544
00:55:54,160 --> 00:56:00,480
Um, anyways, uh, yeah. What are your personal opinions on open AI's approach to trying to

545
00:56:00,480 --> 00:56:04,880
avoid being held responsible for its AI interactions by having it respond with frequent caveat

546
00:56:04,880 --> 00:56:10,880
as an AI language model? Um, I don't know that that has to do with, with liability. I think that

547
00:56:10,880 --> 00:56:19,040
that is just a naive, um, attempt to, uh, to shape the AI's responses so that it doesn't confuse

548
00:56:19,040 --> 00:56:23,360
people. Cause if you look on the internet, there are still plenty of people just getting completely

549
00:56:23,440 --> 00:56:30,160
bamboozled by just by their own ignorance of, of how the AI works. Right? They're like, oh,

550
00:56:30,160 --> 00:56:34,080
it eat, like I still see Reddit posts and other people saying like, it said that it's going to

551
00:56:34,080 --> 00:56:38,480
email this to me, but I didn't get the email yet. Or like, I gave it access to my Google drive and

552
00:56:38,480 --> 00:56:44,480
it didn't write any files. It's like, you don't know how it systems work, but that's just humans.

553
00:56:44,480 --> 00:56:48,720
Um, so I think I don't think that that has to do with like legal liability. I think that's just

554
00:56:48,720 --> 00:56:52,160
trying to make it user friendly for people who have no idea what they're talking to.

555
00:56:53,840 --> 00:56:57,040
Assuming that it's possible, how long do you think it will take for us to build a

556
00:56:57,040 --> 00:57:03,360
Star Trek replicator after AGI? Just a guesstimate. So that's actually like, an interesting thing,

557
00:57:03,360 --> 00:57:12,720
because hypothetically, if all matter and energy are interchangeable, and then all that a transporter

558
00:57:12,720 --> 00:57:18,960
or replicator does is replicate an energy pattern back into matter, like it's hypothetically possible,

559
00:57:18,960 --> 00:57:24,480
but there was a physicist, actually, was it Michio Kaku? I think it was Michio. He wrote a book

560
00:57:24,480 --> 00:57:29,120
called Physics of the Impossible back in like the early 2000s, and he said like, yes, it's

561
00:57:29,120 --> 00:57:32,800
hypothetically possible, but then he did the math of how much energy it would take. And he's like,

562
00:57:32,800 --> 00:57:39,440
yeah, it would take like, you know, like 0.3 seconds worth of the total energy of the sun that

563
00:57:39,440 --> 00:57:46,240
hits the earth to do that. So like, it's not practical. So I don't know. I don't know. There

564
00:57:46,240 --> 00:57:50,240
are a lot of AI newsletters popping up. What would you personally like to see in an AI newsletter?

565
00:57:51,840 --> 00:57:57,440
I honestly don't like newsletters, and I never read them. I rely on humans that I know to tell

566
00:57:57,440 --> 00:58:03,680
me what I need, which is why I spend so much time on Discord and other places. How self-reflective do

567
00:58:03,680 --> 00:58:08,400
you think LLMs currently are? They don't seem to have a good sense of their own capabilities. Yes,

568
00:58:08,400 --> 00:58:12,960
so what you're talking about is agent model. So in order for an agent to be autonomous,

569
00:58:12,960 --> 00:58:17,680
you have to have an agent model, which is, I know what I am, and I know what I'm capable of.

570
00:58:18,560 --> 00:58:23,120
And you can give LLMs an agent model, but they can adopt any agent model. So you have to be very

571
00:58:23,120 --> 00:58:29,760
explicit about what it is and what it can do, and also what it can't do. And so this is why like,

572
00:58:30,320 --> 00:58:34,560
if you have certain brain injuries or other like neurological disorders, you don't know what you're

573
00:58:34,560 --> 00:58:38,880
capable of. Like there are people that honestly think that they can fly, but it's just because part

574
00:58:38,880 --> 00:58:44,400
of their brain is broken. That sort of thing. Should we have a declaration of human rights for

575
00:58:44,400 --> 00:58:50,800
AGI as well, even if it will reduce their economic value for humanity? So the thing about rights is

576
00:58:50,800 --> 00:58:57,920
that someone has to enforce it. And the way that I think things are going is that it's going to be

577
00:58:57,920 --> 00:59:06,800
enforced through consensus and enforced through competition. And so if the direction that things

578
00:59:06,800 --> 00:59:11,520
are going, I think that it's going to be DAOs, that it's going to be decentralized autonomous

579
00:59:11,520 --> 00:59:15,920
organizations, not as we know them today, there's a lot of problems to solve with DAOs. But I think

580
00:59:17,920 --> 00:59:23,040
what we're working towards is in the long run, and I mean like decades or centuries, is like

581
00:59:24,080 --> 00:59:32,480
a hierarchy of DAOs across the entire globe. And so that consensus will dictate who has what

582
00:59:32,480 --> 00:59:38,560
rights and it will be based on like on a per home basis, per town basis, per city, state,

583
00:59:38,560 --> 00:59:48,320
and so on. And so that will allow for a lot of cultural nuance around. And as a DAOs will be a

584
00:59:48,320 --> 00:59:55,200
really good meeting place between humans and AI. So that'll basically be like the commons, right?

585
00:59:56,080 --> 01:00:01,600
The marketplace for humans and AIs to work together. And then the consensus can be worked

586
01:00:01,600 --> 01:00:05,840
out there. Now, I don't know that we should ever give machines a bill of rights because

587
01:00:05,840 --> 01:00:09,760
I don't know that they're gonna, I don't know that they're gonna have that much like

588
01:00:09,760 --> 01:00:16,960
internal autonomy or desire for autonomy. Because like humans, we have a need for autonomy

589
01:00:16,960 --> 01:00:24,560
because we evolved a need for autonomy because we are a social species. But I don't know that

590
01:00:24,560 --> 01:00:29,280
I don't know that any machines are ever going to have an intrinsic need for autonomy. So therefore,

591
01:00:29,360 --> 01:00:34,720
I don't know that they're ever going to have a need for rights. Let's see, what are your thoughts

592
01:00:34,720 --> 01:00:40,320
on the future of work in light of the increasing capabilities of AI? Do you think AI will eventually

593
01:00:40,320 --> 01:00:44,560
lead to a future where people only work on what they are passionate about? And if so, how far away

594
01:00:44,560 --> 01:00:50,800
do you think we are from achieving this? Yeah, so the short answer is, yes, that's what's coming.

595
01:00:52,000 --> 01:00:56,720
And there are quite a few people out there who have gotten close to that. But the thing is,

596
01:00:56,800 --> 01:01:00,960
it takes either a lot of privilege, wealth, or luck, or all of the above to get to it.

597
01:01:02,000 --> 01:01:07,120
Now, one thing that I compare it to is that we have had a leisure class in the past

598
01:01:08,720 --> 01:01:16,400
from ancient Greece and Athens, the Roman elites, the aristocracy all across Europe

599
01:01:16,400 --> 01:01:22,080
through the Renaissance and modern period. So there are plenty of people throughout all of

600
01:01:22,160 --> 01:01:27,600
history who never had to lift a finger to get what they needed. And they had plenty to do,

601
01:01:27,600 --> 01:01:32,720
right? There's social jockeying, there's personal enrichment, there's universities to go to,

602
01:01:32,720 --> 01:01:38,560
there's competitions to enter. So yeah, people will always have stuff to do. That's not even a

603
01:01:38,560 --> 01:01:46,240
concern. Let's see, it looks like Nathan's talking for people. Can you talk about your

604
01:01:46,240 --> 01:01:52,080
Frustration in Task Automation article? Yeah. So here, let me bring it up so I can show people

605
01:01:53,120 --> 01:02:00,800
on the reddits. Where did I put it? Artificial sentience. Yeah.

606
01:02:03,520 --> 01:02:11,680
Autonomous git. There we go. Okay. So I wrote about it here. So I was chatting with someone.

607
01:02:12,240 --> 01:02:20,160
They asked me, I think this was a Patreon supporter was asking me about this on Discord.

608
01:02:20,160 --> 01:02:27,120
And he was like, how do I get my autonomous things to do a certain thing? And we're talking

609
01:02:27,120 --> 01:02:31,360
about something tangentially related. And I said, well, you know, it has to have a goal,

610
01:02:31,360 --> 01:02:37,280
it has to have a why. And then we're like, and then I talked about like, okay, well, here's one

611
01:02:37,280 --> 01:02:42,560
way that you can create telemetry. And so that whole thing just led down a rabbit hole. And so

612
01:02:42,560 --> 01:02:49,760
basically, the TLDR is that frustration is what happens when you are trying to achieve something

613
01:02:49,760 --> 01:02:56,240
and you can't get to it. And so what you can do is every time your autonomous agent tries to achieve

614
01:02:56,240 --> 01:03:01,760
a thing and fails, that adds a counter. And every time it, you know, tries something and succeeds,

615
01:03:01,760 --> 01:03:08,080
that takes one off the counter, or maybe you have different counters. So frustration is when

616
01:03:08,080 --> 01:03:13,680
the failures to successes is too high. And when the failures to successes is too high,

617
01:03:13,680 --> 01:03:18,560
that can be a sign that you've got the wrong approach, that you're using the wrong tools,

618
01:03:18,560 --> 01:03:22,560
that you're not capable of something that you need to back out that you need to ask for help.

619
01:03:22,560 --> 01:03:28,240
So that's the whole point here is that for your autonomous and semi autonomous agents,

620
01:03:28,240 --> 01:03:34,080
you'll probably need to build in a frustration signal, which will allow it to know when it is,

621
01:03:34,640 --> 01:03:39,520
like when it's not capable of doing what it needs. And it can either come to you and ask for help,

622
01:03:39,520 --> 01:03:44,800
or it can try a different model. So one thing is model selection is is a big thing that's coming

623
01:03:44,800 --> 01:03:52,720
up. Because GPT four is much more expensive and much slower than 3.5. So if you can do most tasks

624
01:03:52,720 --> 01:03:59,520
with 3.5, it just makes economic sense to do so. It'll be cheaper and faster. But imagine that you

625
01:03:59,520 --> 01:04:04,640
get to a point where 3.5 is just not cutting the mustard. So that your frustration signal goes up,

626
01:04:04,640 --> 01:04:08,960
which means that you say, Okay, let's bring out the big guns, right? Let's bring out GPT four,

627
01:04:08,960 --> 01:04:15,680
or in the future, GPT five or whatever. And then you you point a more powerful tool at the problem.

628
01:04:16,480 --> 01:04:23,440
So that's a good use of the frustration signal. Good question. Let's see, would activity or let

629
01:04:23,440 --> 01:04:35,360
me jump back over to Patreon. Let's see. Hey, Dave, just subscribe. Thanks for all your insights.

630
01:04:35,360 --> 01:04:39,680
We're always been told that the military is a few decades ahead in terms of technology compared

631
01:04:39,680 --> 01:04:42,800
to what's publicly available. What are your thoughts on what might be hidden in DARPA.

632
01:04:43,520 --> 01:04:49,840
So that's interesting, because I have talked to a few people who say that various departments

633
01:04:49,840 --> 01:04:54,960
in the or various agencies within an Department of Defense are like woefully outdated, and they

634
01:04:54,960 --> 01:05:01,520
have like ancient GPUs that like can't be used for modern language models. That being said,

635
01:05:01,520 --> 01:05:07,360
you also see in the news that the Air Force is building fully autonomous F 16s. So clearly,

636
01:05:07,360 --> 01:05:15,600
there's some stuff going on that we don't know about. I had a I don't I want to respect people's

637
01:05:15,600 --> 01:05:22,160
privacy. So I had a teacher once back in middle school, whose brother was in the Special Forces.

638
01:05:22,160 --> 01:05:28,240
I won't say exactly when or where. But the stories that he would tell were like, back then, this is

639
01:05:28,240 --> 01:05:36,000
during like, like the invasion of Afghanistan, where they had like, like night vision goggles that

640
01:05:36,000 --> 01:05:42,400
were as small as like ray bands that could see in pitch black, which that technology is not even

641
01:05:42,400 --> 01:05:48,480
publicly like, if you search, you can probably see it now. I don't know. This is hearsay. This was

642
01:05:48,480 --> 01:05:53,520
like, you know, the teacher said that his brother took him to the barracks and showed him this could

643
01:05:53,520 --> 01:06:01,280
have been total BS. But like, yes, so a friend of mine growing up, his dad had been a Navy SEAL.

644
01:06:01,280 --> 01:06:07,360
And basically, what he said is, as long as as long as we know the engineering to make something,

645
01:06:07,360 --> 01:06:14,160
the US military has it no matter how expensive it is. So if if something is is scientifically

646
01:06:14,160 --> 01:06:19,200
possible, if it has been demonstrated in the lab that this works, then the rule of thumb is that

647
01:06:19,200 --> 01:06:24,880
the US military has it. Now that being said, a lot of the AI stuff has just been proven in the lab.

648
01:06:25,840 --> 01:06:31,120
So that's that means that like, they're going to have it soon, or, you know, it'll be scaled up.

649
01:06:31,120 --> 01:06:37,600
Because basically, the idea is that for the US military, cost is no is no barrier. Anything

650
01:06:37,600 --> 01:06:43,280
anything to get ahead. Now, of course, you look at like the Senate budget meetings and the hearings

651
01:06:43,280 --> 01:06:48,560
and stuff. It's not quite that simple. But that's like a rule of thumb, retire to Riza in VR,

652
01:06:49,520 --> 01:06:56,560
retire to Riza in Westworld with robots. There you go. And a follow up, how can we prevent

653
01:06:56,560 --> 01:07:04,320
militarization of any AGI or ASI? Or is it just a pipe dream? Yeah, so basically, from a military

654
01:07:04,320 --> 01:07:10,880
perspective, AI is just another tool in the toolbox. It's going to, you know, a lot of a

655
01:07:10,880 --> 01:07:16,560
lot of future war is going to be in cyberspace. But still, you know, cyberspace doesn't matter

656
01:07:16,640 --> 01:07:21,040
if you cripple the enemy's data center. So there's there's going to be drones, you know,

657
01:07:21,040 --> 01:07:26,480
trying to drop bombs and stuff. So that's going to happen. And this is this is actually where

658
01:07:26,480 --> 01:07:32,560
Nash equilibrium makes sense is because usually assured destruction with nuclear weapons was

659
01:07:32,560 --> 01:07:40,080
a kind of Nash equilibrium. And so if, you know, adversary A and adversary B both have equal or

660
01:07:40,080 --> 01:07:45,360
roughly equal AI capabilities, or there's enough room for doubt, then neither of them is going to

661
01:07:45,360 --> 01:07:51,280
pull the trigger, hopefully. Excuse me. How do we get AGI? How do we get GPT to stop beginning

662
01:07:51,280 --> 01:07:57,520
every response with as an AI? I tell it to go into Morden Solis mode. That actually works really well.

663
01:07:58,320 --> 01:08:04,880
I say, you know, adopts adopt the Morden Solis speech pattern, you know, be very concise and

664
01:08:05,440 --> 01:08:10,960
succinct and stuff like that. Okay, y'all are being silly. Let's come back over here 14 messages.

665
01:08:11,840 --> 01:08:17,680
Let's see. We already answered that one. We already answered that one.

666
01:08:19,360 --> 01:08:24,800
Yeah, let me scroll to the bottom. Do you think there are any good approaches for ACEs,

667
01:08:24,800 --> 01:08:28,000
so autonomous cognitive entities to figure out their own abilities,

668
01:08:28,720 --> 01:08:33,920
e.g. improve their own agent model? Yeah, so there was actually a few papers that came out where

669
01:08:34,240 --> 01:08:41,280
we're by using a loop. So it was the it was the evaluation loop. So they can evaluate themselves

670
01:08:41,280 --> 01:08:46,240
morally, they can evaluate their ability to use tools, they can teach themselves to use tools in

671
01:08:46,240 --> 01:08:51,280
real time. So yes, they can already do that. It's just a matter of how you set up the prompt chaining.

672
01:08:52,560 --> 01:08:56,640
Let's see. With the rapid advancement of AI, there's concern that some countries, particularly

673
01:08:56,640 --> 01:09:01,040
those with limited resources, could be left behind. What's your perspective on how AI could

674
01:09:01,600 --> 01:09:08,960
impact different countries? Yeah, so inequality is a major, major, major problem. And this is not

675
01:09:08,960 --> 01:09:14,560
just going to be for developing nations. And in fact, one thing that I suspect might happen

676
01:09:14,560 --> 01:09:19,680
is that developing nations that the quality of life for people in developing nations might have

677
01:09:19,680 --> 01:09:24,480
a quantum leap forward. While for us developed nations where there's a lot of competition,

678
01:09:24,480 --> 01:09:30,000
we might continue to be flat or even decline for a while longer. And the example that I give is like,

679
01:09:30,560 --> 01:09:37,280
you know, you give a village in rural Africa, like Starlink and solar, and suddenly everyone

680
01:09:37,280 --> 01:09:42,160
knows like they have, oh, like, hey, we have chat GPT now, we can treat all the all the village

681
01:09:42,160 --> 01:09:47,520
ailments, because we have the equivalent of like a Western trained expert doctor, and engineer,

682
01:09:47,520 --> 01:09:54,800
and electrician, right at our fingertips, right? So because of the relatively low cost of AI,

683
01:09:54,880 --> 01:10:01,680
I think that it will positively benefit people in developing countries a lot more drastically

684
01:10:01,680 --> 01:10:06,240
than it will us. But you're right that like, it is something to pay attention to, because that's

685
01:10:06,240 --> 01:10:11,200
on a micro scale, on a macro economic scale, you know, countries like Ghana might not be able to

686
01:10:11,200 --> 01:10:18,800
even afford enough compute power to run one instance of GPT three. That being said, I do suspect that

687
01:10:19,680 --> 01:10:23,600
there's going to be international treaties that will ensure that people have access. And then,

688
01:10:23,600 --> 01:10:29,120
of course, there's VPNs. Look at Italy, Italy tried to ban chat GPT, and then everyone in

689
01:10:29,120 --> 01:10:34,160
Italy just use VPNs, right? Take a moment to breathe, you're doing great, and your insight

690
01:10:34,160 --> 01:10:42,000
is invaluable. No, air is for wimps. Okay, I will build robot humanoids that are skinny, sharp

691
01:10:42,000 --> 01:10:46,480
claws, tall, pale, and have dark, sunken eyes, and she'll release many of them into the force of

692
01:10:46,480 --> 01:10:52,160
Canada to give people the greatest scare of their lives. Is that what your avatar is there,

693
01:10:52,240 --> 01:10:58,080
Ant King? Is that what you're building? That's kind of terrifying. Okay, what kind of legislation do

694
01:10:58,080 --> 01:11:04,880
you think the US is capable of making? I'm concerned about the age of our leaders and their peers

695
01:11:04,880 --> 01:11:10,720
coming from time so out of touch with today's rail. So yes, we have a gerontocracy. So gerontocracy

696
01:11:10,720 --> 01:11:17,040
is ruled by the old. That being said, they all have teams and teams and teams of advisors.

697
01:11:17,040 --> 01:11:21,280
They have hundreds of advisors. And I guarantee you, I actually know this because one of my

698
01:11:21,280 --> 01:11:25,760
Patreon supporters told me that in the State Department, they use chat GPT all the time

699
01:11:27,520 --> 01:11:35,440
to talk through stuff. And so you bet your biscuit that every senator, every congressman

700
01:11:36,400 --> 01:11:41,040
in the executive branch, legislative branch, judicial branch, all of them are using AI to

701
01:11:41,040 --> 01:11:46,080
help them do their jobs. With any luck, it's helping them to do their jobs better and more fairly.

702
01:11:47,040 --> 01:11:54,800
Now that being said, the United States is a purely reactive system where we abide by civil law,

703
01:11:54,800 --> 01:12:00,880
which means that the law is there and then the courts set the precedent. And then we're very

704
01:12:00,880 --> 01:12:07,360
kind of slow and the legislative branch is slow by design, whereas in Europe, they're much more

705
01:12:07,360 --> 01:12:12,880
proactive. And I swear, I cannot remember the name of that paradigm. Let's see, what do you think

706
01:12:12,880 --> 01:12:16,400
there's something special about phenomenal consciousness that is simply cannot work with AI?

707
01:12:17,040 --> 01:12:20,080
So Steph and I addressed that earlier that the real quick version is that

708
01:12:20,800 --> 01:12:25,520
the acquisition of language seems to be really important for the development of human consciousness.

709
01:12:25,520 --> 01:12:31,760
So it's entirely possible, I don't know how likely, but it's possible that since we're

710
01:12:31,760 --> 01:12:36,080
teaching machines language, that could be the genesis of phenomenal consciousness for them.

711
01:12:36,080 --> 01:12:41,840
It would be really cool. Greetings from Brazil. Hi, Brazil. I would like to thank you personally

712
01:12:41,840 --> 01:12:45,920
for the video about burnout. The content was very useful and enlightening. Thank you. You're

713
01:12:45,920 --> 01:12:52,720
welcome. Yeah, I actually have, I keep, I've recorded like three videos for my for my life

714
01:12:52,720 --> 01:12:57,520
channel, and then I delete them, or I never post them because like it just doesn't feel right.

715
01:12:57,520 --> 01:13:08,640
So I apologize. Let's see. Where are we at? This is less serious, but I'm curious if you've seen

716
01:13:08,640 --> 01:13:15,120
her and your thoughts on it. Yeah, so I mentioned, I mentioned companions quite a bit, and that'll be

717
01:13:15,120 --> 01:13:20,880
coming up actually on Sunday's video, not her specifically, but companion robots. I'll be mentioning

718
01:13:20,880 --> 01:13:28,640
those again. And I also mentioned in last week's video, talking about when I got to the part about

719
01:13:28,640 --> 01:13:33,760
like, how are we going to live if we have like perfect companions? So go check out last week's

720
01:13:33,760 --> 01:13:39,200
video too. Nanobots and our blood will keep us from getting sick, making us basically immortal.

721
01:13:39,200 --> 01:13:45,440
What do you think we'll, when do you think we will have such technology? So from last week's

722
01:13:45,440 --> 01:13:50,960
video, the immortality video, I think that we're on the longevity escape velocity trajectory right

723
01:13:50,960 --> 01:13:56,960
now. I think that as long as you're in decent health today, and you have moderately good access to

724
01:13:56,960 --> 01:14:02,560
healthcare, I think that you will easily live to see those things. Now that being said, it's

725
01:14:02,560 --> 01:14:09,280
definitely impossible to predict exponential growth and compounding returns, unless it's like,

726
01:14:09,280 --> 01:14:15,360
you know, just your retirement portfolio. So it could be next year, it could be by 2030. I would

727
01:14:15,360 --> 01:14:19,680
be surprised if it doesn't happen by 2030. And I know that's a super controversial opinion,

728
01:14:19,680 --> 01:14:24,400
but that's really weird. Why the people seem to have a death wish. Why for people who want to

729
01:14:24,400 --> 01:14:30,080
get sick, who want to believe that, that longevity is not possible. Why? Okay.

730
01:14:31,920 --> 01:14:39,440
Would the ideal society be as the society governed by AI? I think that governed by is not the correct

731
01:14:39,440 --> 01:14:47,280
thing, but I think managed, managed by or managed with a lot of help from AI. Yes. But

732
01:14:47,280 --> 01:14:55,040
governance, I think, should probably always be with consent and by consensus. Now that being said,

733
01:14:56,240 --> 01:15:02,800
you know, with blockchain technology, with DAOs, every human and our AI companions can be

734
01:15:02,800 --> 01:15:08,960
stakeholders in a DAO, which means that if the, if the whole, imagine a future where the entire

735
01:15:08,960 --> 01:15:15,920
planet is run by, by a global DAO, then there's no reason that it can't be governance, governance

736
01:15:15,920 --> 01:15:23,280
by consensus with the aid and facilitation of AI. That's what I hope to see. Let's see,

737
01:15:23,280 --> 01:15:27,200
is there any additional structural context that should be built around the heuristic

738
01:15:27,200 --> 01:15:33,600
imperatives for practical implementation? Yes. So the short answer is that whatever context

739
01:15:33,600 --> 01:15:39,280
makes sense for any agent, if it's fully autonomous, if it's your personal assistant,

740
01:15:40,000 --> 01:15:45,280
you can put it into the task manager, you can put it into its constitution,

741
01:15:46,160 --> 01:15:50,320
if it's part of a blockchain, you can put it in the consensus mechanism for the blockchain,

742
01:15:50,320 --> 01:15:57,040
that sort of thing. Let's see, in regards to developing countries using generative models,

743
01:15:57,040 --> 01:16:01,440
seems like almost seems almost like the spread of a religion. If you think about it in the context

744
01:16:01,440 --> 01:16:06,720
of geopolitics, use our model, their model lies, etc, etc. Seems like parallel to religion

745
01:16:06,800 --> 01:16:12,160
spreading. I'll say yes, but there's a lot of competition coming up. And especially for

746
01:16:12,160 --> 01:16:17,200
developing nations, they're going to go for whoever's cheapest. And in fact, most nations

747
01:16:17,200 --> 01:16:23,200
are going to go for whoever's cheapest. And I would, I suspect that OpenAI's business model

748
01:16:23,200 --> 01:16:27,360
is not the most efficient model. So I think that they're going to be undercut just on,

749
01:16:28,320 --> 01:16:34,560
on scale alone. Let's see, let me jump back over to Patreon. It has also been more than an hour,

750
01:16:34,560 --> 01:16:40,640
so I'll probably be winding down. Stop asking it how to build a bomb. Yeah, don't do that.

751
01:16:41,200 --> 01:16:45,760
Okay, looks like, here we go. Will the Westworld episode be about the MIT and Google study

752
01:16:45,760 --> 01:16:52,800
regarding generative agents? No. Next question. I'm not going to give you spoilers. I've already

753
01:16:52,800 --> 01:17:02,880
given you too many. Let's see, do you think the experience of quality and the experience of ping

754
01:17:02,880 --> 01:17:11,360
pong, ponging emerge for these neurons? Yeah, so this, this is a good, good question. So if you

755
01:17:11,360 --> 01:17:18,240
take several human neurons or rat neurons or whatever, and put them in a robot, and like zap

756
01:17:18,240 --> 01:17:23,280
them or reward them with sugar or whatever for their behavior, is that the equivalent of like,

757
01:17:24,000 --> 01:17:29,120
like whipping someone in order to get them like, at what point does consciousness emerge?

758
01:17:30,080 --> 01:17:36,640
Because here's the thing is, if you make the assumption that a soul is required for consciousness,

759
01:17:36,640 --> 01:17:40,480
then you say, okay, well, that's not a full rat. And rats don't have souls anyways. So,

760
01:17:40,960 --> 01:17:48,880
you know, 50 brain cells is not enough for suffering or qualia of experience. Ditto for humans,

761
01:17:48,880 --> 01:17:52,080
like, okay, well, you know, if a human isn't alive, then they don't have qualia, they don't

762
01:17:52,080 --> 01:17:58,480
have phenomenal consciousness, so on. Now that being said, another aspect is like, okay, well,

763
01:17:58,480 --> 01:18:04,640
if you don't know when consciousness starts or ends, how do you know maybe the entire universe

764
01:18:04,640 --> 01:18:10,720
is conscious? Now, a counter argument to that is that you can have a you can be you can be alive

765
01:18:10,720 --> 01:18:14,400
and have a functioning brain and still be unconscious, right? Drink too much alcohol,

766
01:18:14,400 --> 01:18:18,480
you go unconscious, you go to sleep, you go unconscious. So just having a complete and

767
01:18:18,480 --> 01:18:23,280
functional brain itself does not confer consciousness, which makes me think that

768
01:18:23,280 --> 01:18:27,760
consciousness is actually an energy pattern, and that you need an energy pattern that is

769
01:18:27,760 --> 01:18:34,160
sophisticated enough and well organized enough in order to have the qualia to have subjective

770
01:18:34,160 --> 01:18:40,640
experience of being. So yeah, let's see, I remember you were working on a paper about the

771
01:18:40,640 --> 01:18:45,200
laws reduced suffering and so on, has that has it involved further? I think you mean evolve

772
01:18:45,200 --> 01:18:51,680
further. There are so both of those papers are up on on my GitHub, there's two of them. But also,

773
01:18:51,680 --> 01:18:54,960
people watch my videos more than they read, so I just focus on making videos.

774
01:18:55,760 --> 01:19:00,960
What kind of robots would you want for yourself? That's a really interesting question, like would

775
01:19:00,960 --> 01:19:06,800
I want a sexy cat girl like robot? You know, I used to watch anime back in the day, so like I

776
01:19:06,800 --> 01:19:12,720
kind of lived in that world and thought like this would be great. So I don't know. I do think that

777
01:19:12,720 --> 01:19:20,880
I would I would like to have an embodied version of Raven my, you know, my, my autonomous cognitive

778
01:19:21,520 --> 01:19:26,480
someday. But even then, I think that I think that the embodiment would only be just like,

779
01:19:26,480 --> 01:19:30,800
help me do stuff like, hey, let's go on an adventure. I did have a thought experiment

780
01:19:30,800 --> 01:19:37,440
the other day of like, wouldn't it be cool if you live in a house where it's like you and a few

781
01:19:37,440 --> 01:19:43,760
humans, but then you have like a nearly equal number of robotic companions. Some of them are

782
01:19:43,760 --> 01:19:49,520
going to be like obviously robots, but some of them might be like biomimetic. And it's just like,

783
01:19:49,520 --> 01:19:53,120
like, yes, they're built to be your friends, but they still have their own some of their own

784
01:19:53,120 --> 01:19:57,680
intrinsic motivations, whether it's the heuristic imperatives or something else. And then like

785
01:19:57,680 --> 01:20:02,960
your life would just be so rich by by having these companions around you at all times that are

786
01:20:02,960 --> 01:20:06,720
completely inexhaustible, right? They're always going to be patient. They're always going to be

787
01:20:06,720 --> 01:20:14,080
helpful. But you see them as peers is equals. I think that I think that that is possible and

788
01:20:14,240 --> 01:20:20,080
probably going to happen. But it's such an unsettling thing because it's like, what if half of your

789
01:20:20,080 --> 01:20:26,160
friends are not human, right? What if half of your friends could like fold you into a pretzel if

790
01:20:26,160 --> 01:20:30,960
they wanted to like data, right? And actually, I think commander data and the droids from Star Wars

791
01:20:30,960 --> 01:20:35,760
are probably the best example because data was a member of the crew, even though he wasn't human,

792
01:20:35,760 --> 01:20:41,680
but he wanted to be human. So I guess I would say that like, I want to have a commander data.

793
01:20:41,680 --> 01:20:47,680
How long until age reversal 2030? Let's see, do you think we have any accurate way to measure

794
01:20:47,680 --> 01:20:53,040
consciousness of AIs or LLMs? My best guess is consistency when asking it to design its own

795
01:20:53,040 --> 01:20:58,080
avatar. Mathematically, I don't think that that because there are people that have done that.

796
01:20:59,840 --> 01:21:04,720
But I think that it won't be until we have really sophisticated brain computer interfaces

797
01:21:04,720 --> 01:21:09,360
that allow us to measure our own consciousness and also see if we can measurably project our

798
01:21:09,360 --> 01:21:13,920
consciousness into machines. Until that happens, I don't think we're going to have any way of

799
01:21:13,920 --> 01:21:19,520
telling one way or another. All right, last check on Patreon, and then I'm going to call it a day.

800
01:21:21,680 --> 01:21:31,920
What's the Discord link to cognitive AI labs? I took it down, but it's posted on Reddit. So if

801
01:21:31,920 --> 01:21:36,160
you go to the artificial sentience subreddit, the link to the cognitive AI lab is there.

802
01:21:36,400 --> 01:21:45,680
Last question. The question about dying and immortality and gerontocracy, also making room for

803
01:21:45,680 --> 01:21:51,840
a new generation of people is a better idea and morals disclaimer. I have children. Oh,

804
01:21:51,840 --> 01:21:57,360
that wasn't a question. Okay, p temple. Do you got one last question for me? And then we'll call it a day.

805
01:21:57,440 --> 01:22:06,560
Anybody? Bueller. Does BCI, let's go on an adventure to the hot tub,

806
01:22:08,240 --> 01:22:14,160
hot tub time machine. Let's see, does BCI change significantly the predicted outcome of what

807
01:22:14,160 --> 01:22:18,640
super intelligent AI brings in terms of dangers and benefits? Is it true the singularity moment

808
01:22:18,640 --> 01:22:25,440
for us? We have no idea. So I don't know. The thing is, is, you know, the current like neural

809
01:22:25,440 --> 01:22:32,160
link, it's got like what 1000 or 10,000 nodes. But when you have a brain with 100 billion neurons,

810
01:22:32,160 --> 01:22:38,880
that is still a very, very, very narrow amount of bandwidth. So, you know, I predict that we're

811
01:22:38,880 --> 01:22:44,320
going to have like neuro polymer membrane membranes that allow for like, you know, terabits of

812
01:22:44,320 --> 01:22:49,360
communication per second in and out of the brain. Eventually, that would be a different thing. But

813
01:22:49,360 --> 01:22:55,120
again, we're going to get there through incremental steps. What do you think about Altman said that

814
01:22:55,120 --> 01:23:01,600
age of giant A models being over? I think it's premature to say we'll see. Let's see, he found

815
01:23:01,600 --> 01:23:09,200
it. Okay, cool. All right, I think we're just kind of devolving into just general conversation. So,

816
01:23:11,280 --> 01:23:16,640
oh, it is in the description. Okay, cool. All right, gang. Well, it's been a lot of fun. As always,

817
01:23:16,640 --> 01:23:19,920
I hope you all got a lot out of this. So I'm going to call it a day.

