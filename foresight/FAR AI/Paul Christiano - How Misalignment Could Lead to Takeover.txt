So this talk is going to be a little bit on the crazier end, it's going to be completely
non-technical.
Yeah, but these are things I believe that I think are important for how much and how
we should think about and worry about and work on alignment.
So things I'm very happy to discuss and debate over the next day of the workshop and later
today.
I'm not sure, hopefully, a reasonable amount of time for questions as I go through it,
but yeah, we have more opportunities to talk later on and feel free to object or just
may punt things to talk about later.
Yeah, so I'm going to be, we've talked so far about why models may end up being misaligned
and why that may be hard to measure.
I'm going to talk about how that actually ultimately leads to sort of total human disempowerment.
I'm calling takeover here for short.
The structure of the talk, I'm first going to talk a little bit about why I think AI systems
will eventually be in a position to disempower humanity.
That is, unless we deliberately change the way we deploy AI, then I'm going to talk about
why misaligned AI systems might be motivated to disempower humanity, basically just slightly
extending or repeating arguments from earlier in the day.
And then I'm going to talk a little bit about why AI systems may ultimately sort of effectively
coordinate to disempower humanity, that is why you may have simultaneous failures across
many systems rather than a single system behaving badly.
So start with why I think AI systems will likely be able to take over.
I think a thing worth saying is that I'm going to talk about what I see as like the single
most likely scenario resulting in catastrophe.
This is not the only way you end up with even the scenario kind of catastrophe.
So I'm going to talk about a system where AI systems are extremely broadly deployed in
the world prior to anything bad happening.
So I imagine that, for example, AI systems are designing and running warehouses and factories
and data centers, they're operating and designing robots, they're writing the large majority
of all code that is written, they handle complicated litigation, they would fight a war, they do
most trades, they run most investment firms.
When humans act in these domains, they do so with a lot of AI systems to understand what's
going on and to provide strategic advice.
So this world is pretty different from the world of today.
I think we are starting to be able to see what such a world would look like.
I think it's not clear how far away this kind of world is, right?
If you're doubling once a year, it doesn't take you that long to go from billions of
dollars to trillions of dollars of economic impact.
But this is the sort of setting in which this entire story takes place.
So one thing that can happen is you have a lot of AI systems operating is that the world
can get pretty complicated for humans, hard for humans to understand, right?
So one simple way this can happen is that you have AI systems with deep expertise in
the domains where they're operating, right?
They've seen huge amounts of data compared to what any human has seen about the specific
domain they operate in.
They think very quickly.
They're very numerous.
Another point is that right now, when a human deploys an AI system, they often have to understand
it quite well.
But that process of understanding a domain and applying AI is itself something that is
likely to be subject to automation.
Yeah.
Great.
I agree with this remark, that is, I think that you have a better chance of understanding
an AI system if there are fewer of them and there's more human labor going into each
deployment.
But it is clearly, in fact, there's a general case for everything I said.
I'm going to talk about a bunch of factors that can exacerbate risk, but I think you
could cut out actually almost every single thing I say in this talk and you'd be left
with some risk.
And it's just that each of these makes the situation, from my perspective, a little bit
worse.
The other thing is, I think both, I think for you, the world are insurance and other
reasons, but I don't really understand what this is.
I know human understands that well what this is.
Yeah.
I'm not sure.
I don't know.
You could take something from a person like that, probably, or I probably wouldn't understand
something about this.
So I would say that in the world of today, most of us are very used to living in a world
where most things would happen.
I don't understand how they happen.
I don't quite understand why they happen.
I trust there was some kind of reason why they happen.
But if you showed me an Amazon warehouse, and you're like, what's going on?
I'd be like, I don't know.
I guess it somehow delivers things.
Something's going on.
So the situation is already, I'm kind of trusting someone to make these decisions in
a way that points at goals I like.
I think that it's not inherently even a bad thing to be in this kind of world.
Like, I think we're always going to be in a world where most people don't understand
most things.
I think that it is not inherently bad that no human understands things and only as understand
something.
I think it's mostly just important for understanding the context in which we're talking about concerns
about AI takeover.
I think that if you're imagining a world where there's like a world, there's a human
world and one AI system that's like, how can I scheme and outmaneuver the humans?
I think that is a possible thing that could happen as a source of risk.
I think the most likely risk scenarios are more like a world that is increasingly, there
are very few humans who understand many important domains.
And so when we're talking about AI's taking over, I think it's, at least when I visualize
this, it really changes my picture of how this is going to go when I think about that
kind of world.
Is it mentally, especially in the case of AI, that you need to keep in hand, that very,
very smart, and you get to start into a drone, into a car manufacturing, et cetera, and right
now they're doing those for us, but in the cycle, do you think you can get out of it?
Yeah, that seems like a reasonable mental image.
There's a little bit depends exactly what your associations are with a little green man.
But yeah, there's a lot of, there's a lot of stuff happening in the world being done
by AI's of all sorts of shapes and sizes.
Some of them are doing things that we do understand well, right?
There's a lot of things like, yep, this AI system, like just does a simple function,
which we understand, and some of them are more complicated.
On this slide, I've just thrown up an example of a random biological system.
Biological systems are relevant because humans don't understand them that well.
We just see that these things that were optimized in the world.
I have a similar vibe, right?
If you ask me, like how even things I understand kind of well,
like how a data center is managed, from my perspective, it feels a little bit like
a biological system.
And like someone heard it and it's somehow going to recover.
And like, I don't totally understand how that works, but I have some sense of
like the telos, like that's the kind of relationship I often have to systems even
maintained by humans and built by humans.
I just expect to have more and more of that sense as the world becomes more
complicated, right?
The way humans relate to this world is increasingly like, you know, the way
we raise the complex systems in general, and the only difference from the world
today, I think that's already the case to a great extent.
The difference is just there is no human who knows what's going on really.
Or at least most people who are making most of those decisions, I know most
details aren't humans.
So one upshot of this that's important to me is that it pushes us increasingly to
train AI systems based on outcomes of their decisions.
I think the less you understand or the less humans have time to understand
details of what's happening in the domain or the kinds of decisions an AI is
making, the more important it is to say, let's just do what the AI said and see
what happens, right?
The more we are able, right, if you're trying to select an organism or
something, you're not going to look at how the organism makes decisions, you're
going to see how well does it do, how fit is it?
That's like one implication of this.
Another implication is that it becomes harder for humans to understand what's
going on or intervene if things are going poorly.
So an upshot of that is that AI is in practice, bad behavior by AI is mostly
detected or corrected by other AI systems, right?
So AI systems are mostly monitoring other AI's where you can imagine humans
monitoring or automated monitoring, but at some point sophisticated AI systems
are responsible for most monitoring.
Even normal law enforcement or fighting a potential war would be done by AI
systems, right?
Because decisions are made quickly and because conflict is often deliberately
made complex by a party to the conflict.
AI systems are running a lot of critical infrastructure.
Just if you want to achieve something in the world, AI plays an important part
in that process.
So none of this is necessarily bad, right?
I think positive stories about AI will also have the step where good AI keeps
bad AI in check.
I think it just is something that like now requires trust on behalf of all of
humanity, extended to all of these AI systems we've built.
You might hope that we could trust the AI systems we've built because we built
them, we have freedom to design them.
But I think that depends on some decisions we make.
And yeah, we'll discuss how it can get somewhat more ugly.
But the point I'm going to make right now or in this first section of the
talk is not that it would necessarily get ugly, just that if for some reason,
all of the AI systems in the world were like, what we really want to do is
disempower humanity, then I think it is quite likely that they would succeed.
And I think this isn't even really the spicy take are getting my optimistic
futures. This is also the case.
And the point is that the core question is just to do you somehow end up in a
situation where all of these AI systems, including the AI systems responsible for
doing monitoring or doing law enforcement or running your data centers or running
your infrastructure, where all of those AI systems, for some reason, want to
disempower humans.
No, I think the optimistic scenario still involves a point where AI systems
acting collectively could take over.
I think in the optimistic scenario.
Yeah.
In the optimistic scenario, you, uh, you wait until you are ready.
You don't end up in the situation until in fact they would not try and take over.
And you're confident of that either because you've determined that the
empirics shake out such that this talk is just crazy talk, very plausible.
Or this talk was justified and we've addressed the problems or whatever.
This is our already on the case of our grid, for example, if the power grid goes
down now, a large fraction of humanity is going to die.
I think it's plausible.
Well, a large fraction of humanity is maybe a small potatoes, but yeah, I think
it's going to depend when I say if all AI's want to take over, the arguments
I'm going to make are going to reply to like a particular class of AI systems.
Right.
So it's not going to be like all electronic devices failed.
It's going to be some class of AI is produced in a certain way.
Um, and I think that what happens over time is just, it becomes more and more
plausible for like the top end of that distribution, right?
Of the systems that are most sophisticated or like, right.
Right now, if all systems trained with deep learning simultaneously failed
and we're like, we hate the humans, we really want to disempower them, I
think it would be fine.
It's not totally obvious and it depends how smart they are about how they failed.
Um, I think we'd probably be okay.
But I mean, again, I think that the whole story we're telling this is kind of
just like, I think you could take out and there's still be some risk.
Yeah.
If what failed?
Yeah, I think that's, you can imagine cases where it's like, if all the cars
failed, that's enough to kill everyone.
This is not necessarily a striking claim.
The striking claim is definitely the one that it is plausible that misaligned
AI systems may want to take over.
I guess this is in some sense the part we've been most talking about throughout
the day.
I'm going to dwell on this a bit and then I'm going to talk about why these
failures may be correlated in a problematic way.
That is why you might have all AI systems trying to take over at the same time.
Um, this, I'm going to talk about two failure modes, both of which we've
touched on earlier.
So one is reward hacking.
That is that disempowering humanity may be an effective strategy for AI systems
collectively to get a lot of reward.
And the second is deceptive alignment scenario that Rohan talked about.
Okay.
So I mentioned briefly before this idea that you may train AI systems by
evaluating the outcomes of the actions they propose.
Um, so just to briefly review what that actually entails, right?
We have some policy, some big neural net.
It proposes some actions and some of those actions to distinguish between them.
We actually need to execute the action, measure the results and decide how much
we like the results.
Let's say the reward is to start judgment of how much we like the results.
And then we adjust policies to maximize the expected reward of the actions
they propose.
Right.
And there's a, it's plausible that if you do this, you get a policy which is
implicitly or explicitly considering many possible actions and selecting the
actions that will lead to the highest reward.
There are other ways you could end up at that same outcome, right?
We could do model based RL and explicitly have a loop in which we predict the
consequences of different actions.
You could have decision transformers, the condition on like high quality actions.
It could just have some other planning process needed still into a model,
whatever, all of these leads to like the same endpoint, which is sometimes
because we don't know how to achieve goals.
We're going to train AI systems to take actions that lead to high reward where
reward means we measure the outcome and then we decide how much we like that
result that we measured.
This can potentially lead to takeover.
If corrupting measurements in some situations is the best way to get a high
reward, right?
So you can imagine some spectrum of forms of reward hacking, right?
A really simple one is you could try and flatter and deceive humans.
Like that's a reasonable way to get high reward.
Maybe on reflection, humans are like, that's not actually what we wanted.
So if they were more careful, they would give it a lower reward.
You can move to, right?
If you're writing code and part of how a human evaluates the code you write is
by performing simple tests, you could try and anticipate or understand those
tests and write code, which breaks those tests or causes them to behave in an
unintended way.
Then when a human looks at the results of those tests, they're more likely to
have a favorable judgment to your code.
If they don't understand the mechanism for the problem.
Or you can move out to creating elaborate false logs to hide errors, right?
If I'm an AI system and part of how I'm evaluated is what happens when the
code I write gets deployed.
One thing I might want to do is say, okay, this code should change the way that
logs get collected or should modify logs to make it.
So in retrospect, a human thinks this outcome was better.
And you can imagine as you go down the spectrum, these sort of corresponds to
more extensive evaluations.
One reason you might use these more extensive evaluations is to prevent
problems or oversights from earlier stages.
You can imagine compromising a server to hide errors.
So not just changing log files, but if a human goes to try and figure out what
happened, you might say, if I just replace the SSH server on this machine, it's
going to be really hard for human to understand what's going on after the
fact remotely, it's going to be unlikely.
They'll notice that something bad has happened.
You can get to these more extreme outcomes.
So Jay mentioned this idea of like, if my system is being trained and what it
wants is to get a high reward, ultimately, that reward is just about a bunch
of measurements that could enter it into the training data set.
One sec, I guess.
And then you can just imagine the most extreme case of saying like, okay, if all
the systems collectively disempowered humanity, then it does not matter if
humans would like to go in later and change this.
I think one dynamic that's worth pointing out, and then I'll get to Jacob, is
like, if you have some of these smaller failures, right?
If you imagine the system which behaves badly on bricks a server, a human is
likely to say, okay, that was bad behavior.
I'm going to go give it a low reward.
And the results of that could either be to train a system to not do funny things
or to train a system to, if you get up to funny business, ensure that a human
never goes and flags that as low reward, right?
Both of those seem like policies that will avoid that bad behavior.
It's very unclear which one gradient to send gets you to.
Yeah.
Yeah, so I would say in some sense, like it depends a little bit on what the system
wants or how you set up training.
In some sense, a lot of these things are like errors, right?
So I'd say that like, right, if you care about as a system, if what the AI cares
about is just don't do things that will ultimately be assigned a low reward.
And we would ultimately detect and assign low reward to many of these failures.
Then all of them would be errors, except the one at the far right.
And so I think a reasonable guess for what happens is you have systems which
make some, some number of these errors along the left.
At some point they're like, okay, I understand what's going on here.
I understand humans are going to escalate and take more extensive measures
to notice bad behavior.
Bad behavior is not an effective strategy.
And then you have this fork in the road, which a J referred to of like either
you learn to just not do anything bad or he's like, okay, I understand.
I understand the class of bad behavior, which is likely to be detected.
And so I'm not going to engage in bad behavior that would be likely to be
detected and it's just unclear to me which way you go.
That's an important, I don't think you, like it's fairly likely you don't see
a march along the spectrum because at some point those are mistakes.
Like if you try and do something bad and it's a half measure.
Well, so like, right, but it seems like a place where, like you say, no,
it's kind of like to figure out what you do with the, you know,
for example, hold on, instead, but I think you can ask, like, where does the
school come from?
I mean, it seems.
Is that.
You have arrested.
Oh,
You
had that after,
There's a function of that.
There's course providing, and as asked,
you can use this online course.
There's a module there.
And then, of course, something like that.
You can get this to a ground point.
But you get two minutes by.
You want all of this to do on.
And, excuse me, there's a no-go now.
I'm trying to get this thing up.
Get that to share with my students.
I'm trying to get this thing down.
I didn't care.
I don't want both of them to put it on the slide.
Okay.
That's not what, that's not sort of the,
how, like, years ago, we all got together.
But no, this is not how we built the app.
We're not going to build the app just online,
but, you know, the board of people's software.
We're going to sort of date this and that.
If we add it once, it'll be fine.
Whatever type of person it is.
And that's the person's head.
It's going to try to set the end.
Right?
And so, it's one thing where it's a work model.
And that's a work model.
And so, this thing is, it's not being provided.
It's not going to work.
And I'm not trying to change that.
There is a serious option of that.
As it's built, exactly.
I'm not going to keep it plain, though.
Like, as you go, like, you know,
while you're on it.
Okay.
I get to see how I can make the end of the course.
That's not a work model.
I don't need to sort of figure this out.
I'm just starting to map things.
Well, maybe they want me to start mapping
and think about the answers to these questions.
But I don't have which one to report.
So, and, you know, everything that's
all in the system.
So, I've been thinking very long with it.
But the point is, how much does this go away
to actually get to it
that you're maintaining, you know,
proper interest over quite a few months
and you're taking care of the signal from the person
that makes all the evidence about when to report
what they actually care about.
Well, I guess there's several things to respond to
and then maybe a meta thought.
I'll start with the meta thought,
which is this seems like a great discussion.
I'm very excited about it and happy to argue a bunch.
I'm not going to be able to give a satisfying answer
to these questions.
And a lot of my high level take right now is
I consider this, both of the issues I'm going to discuss
here plausible.
I think if they're real issues,
we're probably going to get clear experimental evidence
in advance.
Right now, I'm just like, this seems like a thing
that could happen.
Yeah, it's, yeah, I'm not, no.
Great.
Yeah, so on the object level with respect to the,
my key questions here and I'm very interested in strategies
that don't like training strategies that don't run into this
like potential failure mode.
I think the key question becomes like one,
what is the prior?
How do you parameterize your beliefs about what the reward
function is to what is like this kind of rule by which to take
observations as evidence and then like three,
how do you, maybe those are actually just the two key
questions.
Yeah.
I think a general thing about this talk,
maybe one thing is that the conclusions are going to be
somewhat speculative where like,
I think these are fairly plausible outcomes,
but like, you know, tens of percent rather than like,
very likely a second thing is there's a lot of stuff you might do,
none of which I'm going to touch on.
And the best I can say is like,
I've spent a long time thinking about options and I would say
there are no options that seem super great to me right now,
but lots of plausible approaches that might end up addressing
this issue.
That's kind of where I'm at at a high level.
Okay.
I'd also like to talk more about it,
but I think I'm going to move on because I,
it's like six minutes is my understanding.
I don't know where Richard is, but anyway,
I'm just going to run through stuff.
Second failure mode at 112 on is that like in general,
like you may have a system which cares about the actual like numbers
coming into this training set because you did select it on the
basis of numbers in the training set.
You could also end up with a system that cares about some other
arbitrary thing that cares about a number and a reward register or
cares about survival or cares about paper clips.
Such systems might want to be deployed without being trained by
changed by gradient descent might think that a low loss,
getting a low loss during training will prevent them from being
changed or this aspect of them from being changed and therefore
get a low loss in training,
but yet prefer to take over if doing so is possible.
Yeah, my high level take is both of these seem plausible to me.
They're connected.
I think having these two makes things more plausible than if you
have just one.
I don't know how this shakes out.
I think there's a lot of things we can do to try and change this
basic dynamic.
I want to spend five minutes talking about why these failures
could be simultaneous.
I think this has already been alluded to in the past,
but it's an important part of the threat model.
If you're in this world where a systems are very broadly deployed.
Right.
So if one a system behaves badly,
I think that's not generally a problem in this world.
There's a lot of other systems running around that would oppose
it or that would flag the problem for humans or would allow humans
to correct the problem.
And the concern is that while takeover is impossible,
all models behave well.
For every model,
it would be a losing strategy to try and like take over your cluster.
If you know that a human's just going to come in and correct that
problem.
When takeover is possible,
you may have it be the case that all models behave badly.
That is,
it is simultaneously for all of them,
the case that takeover becomes available as a policy.
So if you have this correlated,
if all the systems you produce or all the smartest days you produce
end up having the same problem,
then you have a problem.
If that's the case,
if that's how it shakes out and if eventually a systems are in a
position that they could by acting jointly takeover,
then taking over as a reasonable strategy,
if and only if all the other eyes are also going to try and take over.
You have this kind of weird thing.
The dynamics are not totally unlike the dynamics,
like the normal human revolt or coup.
So you have these two different equilibria and one,
all the systems are like, great,
the humans are doing their human thing.
They're basically in control.
I shouldn't try and rock the boat because some other AI will crush me.
There's another equilibrium where the humans have no control.
AI systems are not feeling deterred by other eyes because they know
other AI systems will also participate.
I think there's a few things that could potentially move you between
these equilibria, right?
So a very salient option is there's just exogenous shocks that
decrease the amount of human control of a situation.
If you imagine there's like a hot war going on,
that's something that makes it harder for humans to detect and
respond to problems, right?
It gives more cover for something weird to happen.
It makes it hard to intervene.
You can have sort of,
you can imagine at least a cascading loss of control where some systems
think probably takeover will be successful.
If they start doing weird stuff,
the world becomes increasingly weird.
It looks increasingly plausible that humans will lose control.
As that happens, more and more systems are like,
okay, it looks like this is going off the rails now.
I am now going to participate and try and break down my little
part of the world, grab my own reward channel.
You can have models interacting with each other again,
as you would in like a human revolt,
just deliberately trying to say like,
I think probably now is a time where takeover will be successful.
They would like to share that information with the other models
that all may join in.
And more generally, like the easier it would be for air systems
acting jointly to take over,
the more unstable the situation becomes, right?
So if it's very hard for air systems to take over,
the situation might be kind of stable.
This equilibrium might be sticky.
In the world where air systems could very easily overpower humans,
it's kind of an extremely unstable equilibrium, right?
Because a smaller number of air systems defecting might quickly
make it clear that the selling points come.
Imagine that this has to happen in point five,
like couldn't you see the case that say the AIP in North Korea,
they talk to each other and say,
well, the situation in this country is kind of bad.
This is our chance and we see a takeover in some segment
of the world of the population.
Or you think it has to be the case that one day,
everything is there in the next day,
they're going over the White House.
I think it depends a lot on how like the gold misgeneralization
shakes out.
So if you have AI systems that are just like,
look, I would love to like wire head right now.
I just love to for 30 seconds control this computer.
Then you'll see like nice, great localized failures
like AI does something really weird.
I mean, the basic dynamic is if AI systems do that,
I think if we just take that as a fact of life,
then that can be a great source of evidence.
The most concerning world is one where you then just try
and train systems to say, hey, look,
these systems just randomly messed up the server
they were running on.
We're going to train them as that's what you should not do.
And if you do that, then you end up with systems
which like given will learn that given the opportunity
for like a little local bad behavior, they shouldn't do it.
And maybe if you're lucky,
once you can have North Korea get taken over by AI systems,
and then you're like, oh, that was really bad.
And hopefully you don't then include that as a training
data point for other.
If you include that as like the,
here's an example of what you should not do.
It's very tempting, right?
AI systems is something really terrible.
You'd be like, don't do that thing.
But you do have this inadvertent,
there's like this risk of overfitting to the problems
you're able to correct where something really bad happens.
And then you say, don't do that thing.
And they learn, okay, if we're going to do something bad,
it's got to be bigger.
Got to think really, truly big.
I think this is probably about the end.
I'm probably just happy to,
happy to just wrap up here.
I have one more question.
Yeah.
I mean, I think that it is unlikely.
I would be surprised personally if you like had something truly
crazy, like a country gets taken over by AI systems.
I think it's very likely that you see some kind of crazy behavior
by AI systems.
I think it won't be so like, again, the country case,
you need zero imagination to draw the analogy between that and
something terrible.
I think in cases like you have a really crazy behavior where a system
bricks your server for some reason,
you need a little bit more imagination.
But I think that's like,
it's just a question of how close do you get or how crazy do things
get before I systems learn not to try crazy half measures.
Yeah.
I think a lot depends on both what kind of evidence we're able to get
from a lab and I think if this sort of phenomenon is real,
I think there's a very good chance of getting like fairly compelling
demonstrations in a lab that requires some imagination to bridge
from examples in the lab to examples in the wild.
And you'll have some kinds of failures in the wild.
And it's a question of just how crazy or analogous to those have to
be before they're moving.
Like we already have some slightly weird stuff.
I think that's pretty underwhelming.
I think we're going to have like much better if this is real.
This is a real kind of concern.
We have much crazier stuff than we see today.
But the concern, I think the worst case of those has to get pretty
crazy or like requires a lot of will to stop doing things.
And so we need pretty crazy demonstrations.
I'm hoping that, you know,
more mild evidence will be enough to get people not to go there.
Yeah.
I think the language model that's like,
it looks like you're going to give me a bad rating.
Do you really want to do that?
I know where your family lives. I can kill them.
I think like if that happened, people would not be like,
we're done with this language model stuff.
Like I think that's just not that far anymore from where we're at.
I mean, this is maybe empirical prediction.
I would love it if the first time a language model was like,
I will murder your family.
We're just like, we're done.
No more language models.
But I think that's not the track we're currently on.
And I would love to get us on that track instead, but I'm not.
Yeah.
So just like, about this thing that climate change is happening
in a place actually,
we're not alone.
We've had decades of mourning,
but we've had a sense of community,
time, and sense of survival.
