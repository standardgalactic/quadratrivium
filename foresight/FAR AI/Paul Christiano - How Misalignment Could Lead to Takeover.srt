1
00:00:00,000 --> 00:00:17,440
So this talk is going to be a little bit on the crazier end, it's going to be completely

2
00:00:17,440 --> 00:00:18,440
non-technical.

3
00:00:18,440 --> 00:00:23,540
Yeah, but these are things I believe that I think are important for how much and how

4
00:00:23,540 --> 00:00:26,680
we should think about and worry about and work on alignment.

5
00:00:26,680 --> 00:00:29,980
So things I'm very happy to discuss and debate over the next day of the workshop and later

6
00:00:29,980 --> 00:00:30,980
today.

7
00:00:30,980 --> 00:00:34,620
I'm not sure, hopefully, a reasonable amount of time for questions as I go through it,

8
00:00:34,620 --> 00:00:39,420
but yeah, we have more opportunities to talk later on and feel free to object or just

9
00:00:39,420 --> 00:00:41,420
may punt things to talk about later.

10
00:00:41,420 --> 00:00:46,580
Yeah, so I'm going to be, we've talked so far about why models may end up being misaligned

11
00:00:46,580 --> 00:00:48,140
and why that may be hard to measure.

12
00:00:48,140 --> 00:00:52,420
I'm going to talk about how that actually ultimately leads to sort of total human disempowerment.

13
00:00:52,420 --> 00:00:55,740
I'm calling takeover here for short.

14
00:00:55,740 --> 00:00:59,020
The structure of the talk, I'm first going to talk a little bit about why I think AI systems

15
00:00:59,020 --> 00:01:01,940
will eventually be in a position to disempower humanity.

16
00:01:01,940 --> 00:01:06,500
That is, unless we deliberately change the way we deploy AI, then I'm going to talk about

17
00:01:06,500 --> 00:01:10,420
why misaligned AI systems might be motivated to disempower humanity, basically just slightly

18
00:01:10,420 --> 00:01:13,380
extending or repeating arguments from earlier in the day.

19
00:01:13,380 --> 00:01:17,180
And then I'm going to talk a little bit about why AI systems may ultimately sort of effectively

20
00:01:17,180 --> 00:01:20,500
coordinate to disempower humanity, that is why you may have simultaneous failures across

21
00:01:20,500 --> 00:01:26,780
many systems rather than a single system behaving badly.

22
00:01:26,780 --> 00:01:29,420
So start with why I think AI systems will likely be able to take over.

23
00:01:29,420 --> 00:01:32,820
I think a thing worth saying is that I'm going to talk about what I see as like the single

24
00:01:32,820 --> 00:01:35,180
most likely scenario resulting in catastrophe.

25
00:01:35,180 --> 00:01:38,940
This is not the only way you end up with even the scenario kind of catastrophe.

26
00:01:38,940 --> 00:01:42,860
So I'm going to talk about a system where AI systems are extremely broadly deployed in

27
00:01:42,860 --> 00:01:46,860
the world prior to anything bad happening.

28
00:01:46,860 --> 00:01:51,900
So I imagine that, for example, AI systems are designing and running warehouses and factories

29
00:01:51,900 --> 00:01:56,260
and data centers, they're operating and designing robots, they're writing the large majority

30
00:01:56,260 --> 00:02:01,420
of all code that is written, they handle complicated litigation, they would fight a war, they do

31
00:02:01,420 --> 00:02:04,220
most trades, they run most investment firms.

32
00:02:04,220 --> 00:02:07,900
When humans act in these domains, they do so with a lot of AI systems to understand what's

33
00:02:07,900 --> 00:02:12,100
going on and to provide strategic advice.

34
00:02:12,100 --> 00:02:14,180
So this world is pretty different from the world of today.

35
00:02:14,180 --> 00:02:17,420
I think we are starting to be able to see what such a world would look like.

36
00:02:17,420 --> 00:02:20,380
I think it's not clear how far away this kind of world is, right?

37
00:02:20,380 --> 00:02:23,380
If you're doubling once a year, it doesn't take you that long to go from billions of

38
00:02:23,380 --> 00:02:28,740
dollars to trillions of dollars of economic impact.

39
00:02:28,740 --> 00:02:34,780
But this is the sort of setting in which this entire story takes place.

40
00:02:34,780 --> 00:02:37,900
So one thing that can happen is you have a lot of AI systems operating is that the world

41
00:02:37,900 --> 00:02:41,260
can get pretty complicated for humans, hard for humans to understand, right?

42
00:02:41,260 --> 00:02:44,860
So one simple way this can happen is that you have AI systems with deep expertise in

43
00:02:44,860 --> 00:02:46,380
the domains where they're operating, right?

44
00:02:46,380 --> 00:02:50,380
They've seen huge amounts of data compared to what any human has seen about the specific

45
00:02:50,380 --> 00:02:51,380
domain they operate in.

46
00:02:51,380 --> 00:02:52,780
They think very quickly.

47
00:02:52,780 --> 00:02:53,780
They're very numerous.

48
00:02:53,780 --> 00:02:59,220
Another point is that right now, when a human deploys an AI system, they often have to understand

49
00:02:59,220 --> 00:03:00,220
it quite well.

50
00:03:00,220 --> 00:03:03,500
But that process of understanding a domain and applying AI is itself something that is

51
00:03:03,500 --> 00:03:05,100
likely to be subject to automation.

52
00:03:05,100 --> 00:03:06,100
Yeah.

53
00:03:06,100 --> 00:03:07,100
Great.

54
00:03:07,100 --> 00:03:24,540
I agree with this remark, that is, I think that you have a better chance of understanding

55
00:03:24,540 --> 00:03:27,420
an AI system if there are fewer of them and there's more human labor going into each

56
00:03:27,420 --> 00:03:28,420
deployment.

57
00:03:28,420 --> 00:03:30,540
But it is clearly, in fact, there's a general case for everything I said.

58
00:03:30,540 --> 00:03:33,740
I'm going to talk about a bunch of factors that can exacerbate risk, but I think you

59
00:03:33,740 --> 00:03:36,740
could cut out actually almost every single thing I say in this talk and you'd be left

60
00:03:36,780 --> 00:03:37,780
with some risk.

61
00:03:37,780 --> 00:03:40,220
And it's just that each of these makes the situation, from my perspective, a little bit

62
00:03:40,220 --> 00:03:41,220
worse.

63
00:03:41,220 --> 00:03:46,740
The other thing is, I think both, I think for you, the world are insurance and other

64
00:03:46,740 --> 00:03:50,540
reasons, but I don't really understand what this is.

65
00:03:50,540 --> 00:03:52,540
I know human understands that well what this is.

66
00:03:52,540 --> 00:03:53,540
Yeah.

67
00:03:53,540 --> 00:03:54,540
I'm not sure.

68
00:03:54,540 --> 00:03:55,540
I don't know.

69
00:03:55,540 --> 00:03:56,540
You could take something from a person like that, probably, or I probably wouldn't understand

70
00:03:56,540 --> 00:03:57,540
something about this.

71
00:03:57,540 --> 00:04:11,740
So I would say that in the world of today, most of us are very used to living in a world

72
00:04:11,740 --> 00:04:12,740
where most things would happen.

73
00:04:12,740 --> 00:04:14,100
I don't understand how they happen.

74
00:04:14,100 --> 00:04:15,500
I don't quite understand why they happen.

75
00:04:15,500 --> 00:04:17,340
I trust there was some kind of reason why they happen.

76
00:04:17,340 --> 00:04:19,980
But if you showed me an Amazon warehouse, and you're like, what's going on?

77
00:04:19,980 --> 00:04:20,980
I'd be like, I don't know.

78
00:04:20,980 --> 00:04:22,300
I guess it somehow delivers things.

79
00:04:22,300 --> 00:04:23,300
Something's going on.

80
00:04:23,300 --> 00:04:27,420
So the situation is already, I'm kind of trusting someone to make these decisions in

81
00:04:27,420 --> 00:04:29,060
a way that points at goals I like.

82
00:04:29,060 --> 00:04:31,940
I think that it's not inherently even a bad thing to be in this kind of world.

83
00:04:31,940 --> 00:04:34,460
Like, I think we're always going to be in a world where most people don't understand

84
00:04:34,460 --> 00:04:37,060
most things.

85
00:04:37,060 --> 00:04:41,620
I think that it is not inherently bad that no human understands things and only as understand

86
00:04:41,620 --> 00:04:42,620
something.

87
00:04:42,620 --> 00:04:45,620
I think it's mostly just important for understanding the context in which we're talking about concerns

88
00:04:45,620 --> 00:04:46,620
about AI takeover.

89
00:04:46,620 --> 00:04:49,380
I think that if you're imagining a world where there's like a world, there's a human

90
00:04:49,380 --> 00:04:53,020
world and one AI system that's like, how can I scheme and outmaneuver the humans?

91
00:04:53,020 --> 00:04:56,100
I think that is a possible thing that could happen as a source of risk.

92
00:04:56,100 --> 00:04:59,420
I think the most likely risk scenarios are more like a world that is increasingly, there

93
00:04:59,420 --> 00:05:02,260
are very few humans who understand many important domains.

94
00:05:02,260 --> 00:05:05,300
And so when we're talking about AI's taking over, I think it's, at least when I visualize

95
00:05:05,300 --> 00:05:09,580
this, it really changes my picture of how this is going to go when I think about that

96
00:05:09,580 --> 00:05:10,580
kind of world.

97
00:05:10,580 --> 00:05:15,660
Is it mentally, especially in the case of AI, that you need to keep in hand, that very,

98
00:05:15,660 --> 00:05:20,180
very smart, and you get to start into a drone, into a car manufacturing, et cetera, and right

99
00:05:20,180 --> 00:05:24,820
now they're doing those for us, but in the cycle, do you think you can get out of it?

100
00:05:24,860 --> 00:05:26,500
Yeah, that seems like a reasonable mental image.

101
00:05:26,500 --> 00:05:29,420
There's a little bit depends exactly what your associations are with a little green man.

102
00:05:30,100 --> 00:05:33,180
But yeah, there's a lot of, there's a lot of stuff happening in the world being done

103
00:05:33,180 --> 00:05:35,140
by AI's of all sorts of shapes and sizes.

104
00:05:36,340 --> 00:05:38,380
Some of them are doing things that we do understand well, right?

105
00:05:38,380 --> 00:05:41,020
There's a lot of things like, yep, this AI system, like just does a simple function,

106
00:05:41,020 --> 00:05:42,740
which we understand, and some of them are more complicated.

107
00:05:43,500 --> 00:05:46,660
On this slide, I've just thrown up an example of a random biological system.

108
00:05:46,660 --> 00:05:49,860
Biological systems are relevant because humans don't understand them that well.

109
00:05:49,860 --> 00:05:52,220
We just see that these things that were optimized in the world.

110
00:05:52,900 --> 00:05:53,940
I have a similar vibe, right?

111
00:05:53,940 --> 00:05:56,660
If you ask me, like how even things I understand kind of well,

112
00:05:56,660 --> 00:05:59,700
like how a data center is managed, from my perspective, it feels a little bit like

113
00:05:59,700 --> 00:06:00,660
a biological system.

114
00:06:00,660 --> 00:06:02,860
And like someone heard it and it's somehow going to recover.

115
00:06:03,220 --> 00:06:06,300
And like, I don't totally understand how that works, but I have some sense of

116
00:06:06,300 --> 00:06:09,580
like the telos, like that's the kind of relationship I often have to systems even

117
00:06:09,580 --> 00:06:11,020
maintained by humans and built by humans.

118
00:06:11,020 --> 00:06:13,580
I just expect to have more and more of that sense as the world becomes more

119
00:06:13,580 --> 00:06:14,540
complicated, right?

120
00:06:14,540 --> 00:06:17,180
The way humans relate to this world is increasingly like, you know, the way

121
00:06:17,180 --> 00:06:20,220
we raise the complex systems in general, and the only difference from the world

122
00:06:20,220 --> 00:06:22,020
today, I think that's already the case to a great extent.

123
00:06:22,300 --> 00:06:24,900
The difference is just there is no human who knows what's going on really.

124
00:06:26,500 --> 00:06:29,140
Or at least most people who are making most of those decisions, I know most

125
00:06:29,140 --> 00:06:30,060
details aren't humans.

126
00:06:32,140 --> 00:06:37,060
So one upshot of this that's important to me is that it pushes us increasingly to

127
00:06:37,060 --> 00:06:39,660
train AI systems based on outcomes of their decisions.

128
00:06:39,780 --> 00:06:42,340
I think the less you understand or the less humans have time to understand

129
00:06:42,340 --> 00:06:45,100
details of what's happening in the domain or the kinds of decisions an AI is

130
00:06:45,100 --> 00:06:48,980
making, the more important it is to say, let's just do what the AI said and see

131
00:06:48,980 --> 00:06:50,100
what happens, right?

132
00:06:50,100 --> 00:06:52,620
The more we are able, right, if you're trying to select an organism or

133
00:06:52,620 --> 00:06:54,580
something, you're not going to look at how the organism makes decisions, you're

134
00:06:54,580 --> 00:06:56,340
going to see how well does it do, how fit is it?

135
00:06:58,420 --> 00:06:59,860
That's like one implication of this.

136
00:07:00,260 --> 00:07:03,220
Another implication is that it becomes harder for humans to understand what's

137
00:07:03,220 --> 00:07:05,100
going on or intervene if things are going poorly.

138
00:07:06,260 --> 00:07:09,940
So an upshot of that is that AI is in practice, bad behavior by AI is mostly

139
00:07:09,940 --> 00:07:12,780
detected or corrected by other AI systems, right?

140
00:07:12,780 --> 00:07:16,220
So AI systems are mostly monitoring other AI's where you can imagine humans

141
00:07:16,220 --> 00:07:19,220
monitoring or automated monitoring, but at some point sophisticated AI systems

142
00:07:19,300 --> 00:07:20,700
are responsible for most monitoring.

143
00:07:21,100 --> 00:07:24,500
Even normal law enforcement or fighting a potential war would be done by AI

144
00:07:24,500 --> 00:07:25,380
systems, right?

145
00:07:25,380 --> 00:07:28,980
Because decisions are made quickly and because conflict is often deliberately

146
00:07:28,980 --> 00:07:30,620
made complex by a party to the conflict.

147
00:07:31,700 --> 00:07:33,940
AI systems are running a lot of critical infrastructure.

148
00:07:33,940 --> 00:07:36,660
Just if you want to achieve something in the world, AI plays an important part

149
00:07:36,660 --> 00:07:37,500
in that process.

150
00:07:38,420 --> 00:07:40,540
So none of this is necessarily bad, right?

151
00:07:40,540 --> 00:07:44,020
I think positive stories about AI will also have the step where good AI keeps

152
00:07:44,020 --> 00:07:44,860
bad AI in check.

153
00:07:45,100 --> 00:07:48,700
I think it just is something that like now requires trust on behalf of all of

154
00:07:48,700 --> 00:07:50,860
humanity, extended to all of these AI systems we've built.

155
00:07:51,300 --> 00:07:53,780
You might hope that we could trust the AI systems we've built because we built

156
00:07:53,780 --> 00:07:55,220
them, we have freedom to design them.

157
00:07:55,980 --> 00:07:57,860
But I think that depends on some decisions we make.

158
00:07:57,900 --> 00:08:00,140
And yeah, we'll discuss how it can get somewhat more ugly.

159
00:08:01,380 --> 00:08:03,540
But the point I'm going to make right now or in this first section of the

160
00:08:03,540 --> 00:08:07,740
talk is not that it would necessarily get ugly, just that if for some reason,

161
00:08:07,980 --> 00:08:10,940
all of the AI systems in the world were like, what we really want to do is

162
00:08:10,940 --> 00:08:14,180
disempower humanity, then I think it is quite likely that they would succeed.

163
00:08:14,860 --> 00:08:17,700
And I think this isn't even really the spicy take are getting my optimistic

164
00:08:17,700 --> 00:08:19,020
futures. This is also the case.

165
00:08:21,700 --> 00:08:24,740
And the point is that the core question is just to do you somehow end up in a

166
00:08:24,740 --> 00:08:27,900
situation where all of these AI systems, including the AI systems responsible for

167
00:08:27,900 --> 00:08:31,140
doing monitoring or doing law enforcement or running your data centers or running

168
00:08:31,140 --> 00:08:34,060
your infrastructure, where all of those AI systems, for some reason, want to

169
00:08:34,060 --> 00:08:34,900
disempower humans.

170
00:08:40,820 --> 00:08:43,740
No, I think the optimistic scenario still involves a point where AI systems

171
00:08:43,740 --> 00:08:45,140
acting collectively could take over.

172
00:08:45,540 --> 00:08:46,780
I think in the optimistic scenario.

173
00:08:47,300 --> 00:08:47,620
Yeah.

174
00:08:49,300 --> 00:08:52,340
In the optimistic scenario, you, uh, you wait until you are ready.

175
00:08:52,340 --> 00:08:55,740
You don't end up in the situation until in fact they would not try and take over.

176
00:08:55,780 --> 00:08:58,060
And you're confident of that either because you've determined that the

177
00:08:58,060 --> 00:09:01,460
empirics shake out such that this talk is just crazy talk, very plausible.

178
00:09:01,700 --> 00:09:04,820
Or this talk was justified and we've addressed the problems or whatever.

179
00:09:05,820 --> 00:09:10,500
This is our already on the case of our grid, for example, if the power grid goes

180
00:09:10,500 --> 00:09:13,460
down now, a large fraction of humanity is going to die.

181
00:09:14,340 --> 00:09:15,460
I think it's plausible.

182
00:09:16,060 --> 00:09:21,140
Well, a large fraction of humanity is maybe a small potatoes, but yeah, I think

183
00:09:21,140 --> 00:09:24,020
it's going to depend when I say if all AI's want to take over, the arguments

184
00:09:24,020 --> 00:09:26,940
I'm going to make are going to reply to like a particular class of AI systems.

185
00:09:27,020 --> 00:09:27,180
Right.

186
00:09:27,180 --> 00:09:29,180
So it's not going to be like all electronic devices failed.

187
00:09:29,180 --> 00:09:31,300
It's going to be some class of AI is produced in a certain way.

188
00:09:32,020 --> 00:09:35,180
Um, and I think that what happens over time is just, it becomes more and more

189
00:09:35,180 --> 00:09:38,060
plausible for like the top end of that distribution, right?

190
00:09:38,060 --> 00:09:40,820
Of the systems that are most sophisticated or like, right.

191
00:09:40,820 --> 00:09:44,260
Right now, if all systems trained with deep learning simultaneously failed

192
00:09:44,260 --> 00:09:46,900
and we're like, we hate the humans, we really want to disempower them, I

193
00:09:46,900 --> 00:09:47,660
think it would be fine.

194
00:09:47,980 --> 00:09:50,500
It's not totally obvious and it depends how smart they are about how they failed.

195
00:09:50,980 --> 00:09:52,540
Um, I think we'd probably be okay.

196
00:09:52,940 --> 00:09:55,980
But I mean, again, I think that the whole story we're telling this is kind of

197
00:09:55,980 --> 00:09:58,620
just like, I think you could take out and there's still be some risk.

198
00:09:58,860 --> 00:10:01,420
Yeah.

199
00:10:04,940 --> 00:10:05,540
If what failed?

200
00:10:09,380 --> 00:10:12,260
Yeah, I think that's, you can imagine cases where it's like, if all the cars

201
00:10:12,260 --> 00:10:13,460
failed, that's enough to kill everyone.

202
00:10:13,700 --> 00:10:15,220
This is not necessarily a striking claim.

203
00:10:16,460 --> 00:10:19,900
The striking claim is definitely the one that it is plausible that misaligned

204
00:10:19,900 --> 00:10:21,420
AI systems may want to take over.

205
00:10:21,460 --> 00:10:23,740
I guess this is in some sense the part we've been most talking about throughout

206
00:10:23,740 --> 00:10:24,060
the day.

207
00:10:24,460 --> 00:10:26,460
I'm going to dwell on this a bit and then I'm going to talk about why these

208
00:10:26,460 --> 00:10:28,380
failures may be correlated in a problematic way.

209
00:10:28,500 --> 00:10:31,140
That is why you might have all AI systems trying to take over at the same time.

210
00:10:33,340 --> 00:10:36,860
Um, this, I'm going to talk about two failure modes, both of which we've

211
00:10:36,860 --> 00:10:37,740
touched on earlier.

212
00:10:37,940 --> 00:10:39,100
So one is reward hacking.

213
00:10:39,260 --> 00:10:42,980
That is that disempowering humanity may be an effective strategy for AI systems

214
00:10:42,980 --> 00:10:44,380
collectively to get a lot of reward.

215
00:10:44,780 --> 00:10:48,020
And the second is deceptive alignment scenario that Rohan talked about.

216
00:10:53,180 --> 00:10:53,900
Okay.

217
00:10:53,900 --> 00:10:57,060
So I mentioned briefly before this idea that you may train AI systems by

218
00:10:57,060 --> 00:10:59,300
evaluating the outcomes of the actions they propose.

219
00:10:59,820 --> 00:11:02,540
Um, so just to briefly review what that actually entails, right?

220
00:11:02,540 --> 00:11:04,020
We have some policy, some big neural net.

221
00:11:04,420 --> 00:11:08,780
It proposes some actions and some of those actions to distinguish between them.

222
00:11:08,780 --> 00:11:11,780
We actually need to execute the action, measure the results and decide how much

223
00:11:11,780 --> 00:11:12,660
we like the results.

224
00:11:12,660 --> 00:11:15,220
Let's say the reward is to start judgment of how much we like the results.

225
00:11:15,660 --> 00:11:19,180
And then we adjust policies to maximize the expected reward of the actions

226
00:11:19,180 --> 00:11:19,820
they propose.

227
00:11:21,220 --> 00:11:21,420
Right.

228
00:11:21,420 --> 00:11:24,620
And there's a, it's plausible that if you do this, you get a policy which is

229
00:11:24,860 --> 00:11:27,980
implicitly or explicitly considering many possible actions and selecting the

230
00:11:27,980 --> 00:11:29,620
actions that will lead to the highest reward.

231
00:11:30,340 --> 00:11:32,140
There are other ways you could end up at that same outcome, right?

232
00:11:32,140 --> 00:11:34,780
We could do model based RL and explicitly have a loop in which we predict the

233
00:11:34,780 --> 00:11:36,060
consequences of different actions.

234
00:11:36,220 --> 00:11:39,860
You could have decision transformers, the condition on like high quality actions.

235
00:11:40,260 --> 00:11:43,700
It could just have some other planning process needed still into a model,

236
00:11:43,700 --> 00:11:46,820
whatever, all of these leads to like the same endpoint, which is sometimes

237
00:11:46,820 --> 00:11:48,460
because we don't know how to achieve goals.

238
00:11:48,580 --> 00:11:51,540
We're going to train AI systems to take actions that lead to high reward where

239
00:11:51,540 --> 00:11:54,500
reward means we measure the outcome and then we decide how much we like that

240
00:11:54,500 --> 00:11:55,300
result that we measured.

241
00:11:57,700 --> 00:11:59,260
This can potentially lead to takeover.

242
00:11:59,260 --> 00:12:02,460
If corrupting measurements in some situations is the best way to get a high

243
00:12:02,460 --> 00:12:03,340
reward, right?

244
00:12:03,340 --> 00:12:06,220
So you can imagine some spectrum of forms of reward hacking, right?

245
00:12:06,220 --> 00:12:08,620
A really simple one is you could try and flatter and deceive humans.

246
00:12:08,660 --> 00:12:10,140
Like that's a reasonable way to get high reward.

247
00:12:10,340 --> 00:12:12,660
Maybe on reflection, humans are like, that's not actually what we wanted.

248
00:12:12,820 --> 00:12:14,820
So if they were more careful, they would give it a lower reward.

249
00:12:15,660 --> 00:12:16,620
You can move to, right?

250
00:12:16,620 --> 00:12:19,620
If you're writing code and part of how a human evaluates the code you write is

251
00:12:19,620 --> 00:12:22,900
by performing simple tests, you could try and anticipate or understand those

252
00:12:22,900 --> 00:12:26,060
tests and write code, which breaks those tests or causes them to behave in an

253
00:12:26,060 --> 00:12:26,900
unintended way.

254
00:12:27,140 --> 00:12:29,660
Then when a human looks at the results of those tests, they're more likely to

255
00:12:29,660 --> 00:12:31,020
have a favorable judgment to your code.

256
00:12:31,020 --> 00:12:33,060
If they don't understand the mechanism for the problem.

257
00:12:33,820 --> 00:12:37,180
Or you can move out to creating elaborate false logs to hide errors, right?

258
00:12:37,180 --> 00:12:39,940
If I'm an AI system and part of how I'm evaluated is what happens when the

259
00:12:39,940 --> 00:12:41,260
code I write gets deployed.

260
00:12:41,380 --> 00:12:43,940
One thing I might want to do is say, okay, this code should change the way that

261
00:12:43,940 --> 00:12:46,700
logs get collected or should modify logs to make it.

262
00:12:46,700 --> 00:12:49,580
So in retrospect, a human thinks this outcome was better.

263
00:12:51,020 --> 00:12:53,820
And you can imagine as you go down the spectrum, these sort of corresponds to

264
00:12:53,820 --> 00:12:55,460
more extensive evaluations.

265
00:12:55,580 --> 00:12:58,340
One reason you might use these more extensive evaluations is to prevent

266
00:12:58,340 --> 00:13:00,500
problems or oversights from earlier stages.

267
00:13:04,140 --> 00:13:06,140
You can imagine compromising a server to hide errors.

268
00:13:06,140 --> 00:13:08,780
So not just changing log files, but if a human goes to try and figure out what

269
00:13:08,780 --> 00:13:12,060
happened, you might say, if I just replace the SSH server on this machine, it's

270
00:13:12,060 --> 00:13:14,660
going to be really hard for human to understand what's going on after the

271
00:13:14,660 --> 00:13:16,500
fact remotely, it's going to be unlikely.

272
00:13:16,500 --> 00:13:18,060
They'll notice that something bad has happened.

273
00:13:18,460 --> 00:13:20,060
You can get to these more extreme outcomes.

274
00:13:20,060 --> 00:13:23,180
So Jay mentioned this idea of like, if my system is being trained and what it

275
00:13:23,180 --> 00:13:26,420
wants is to get a high reward, ultimately, that reward is just about a bunch

276
00:13:26,420 --> 00:13:28,500
of measurements that could enter it into the training data set.

277
00:13:30,220 --> 00:13:31,020
One sec, I guess.

278
00:13:31,780 --> 00:13:34,340
And then you can just imagine the most extreme case of saying like, okay, if all

279
00:13:34,340 --> 00:13:37,660
the systems collectively disempowered humanity, then it does not matter if

280
00:13:37,660 --> 00:13:39,540
humans would like to go in later and change this.

281
00:13:40,500 --> 00:13:43,700
I think one dynamic that's worth pointing out, and then I'll get to Jacob, is

282
00:13:43,700 --> 00:13:47,180
like, if you have some of these smaller failures, right?

283
00:13:47,180 --> 00:13:50,220
If you imagine the system which behaves badly on bricks a server, a human is

284
00:13:50,220 --> 00:13:51,620
likely to say, okay, that was bad behavior.

285
00:13:51,620 --> 00:13:52,860
I'm going to go give it a low reward.

286
00:13:53,420 --> 00:13:56,660
And the results of that could either be to train a system to not do funny things

287
00:13:56,740 --> 00:13:59,540
or to train a system to, if you get up to funny business, ensure that a human

288
00:13:59,540 --> 00:14:01,540
never goes and flags that as low reward, right?

289
00:14:01,540 --> 00:14:04,060
Both of those seem like policies that will avoid that bad behavior.

290
00:14:04,220 --> 00:14:06,380
It's very unclear which one gradient to send gets you to.

291
00:14:07,580 --> 00:14:07,860
Yeah.

292
00:14:10,380 --> 00:14:24,900
Yeah, so I would say in some sense, like it depends a little bit on what the system

293
00:14:24,900 --> 00:14:26,140
wants or how you set up training.

294
00:14:26,340 --> 00:14:28,900
In some sense, a lot of these things are like errors, right?

295
00:14:29,180 --> 00:14:33,220
So I'd say that like, right, if you care about as a system, if what the AI cares

296
00:14:33,220 --> 00:14:36,500
about is just don't do things that will ultimately be assigned a low reward.

297
00:14:36,780 --> 00:14:39,940
And we would ultimately detect and assign low reward to many of these failures.

298
00:14:40,180 --> 00:14:42,740
Then all of them would be errors, except the one at the far right.

299
00:14:43,620 --> 00:14:46,660
And so I think a reasonable guess for what happens is you have systems which

300
00:14:46,660 --> 00:14:48,820
make some, some number of these errors along the left.

301
00:14:49,060 --> 00:14:51,260
At some point they're like, okay, I understand what's going on here.

302
00:14:51,260 --> 00:14:53,780
I understand humans are going to escalate and take more extensive measures

303
00:14:53,780 --> 00:14:54,780
to notice bad behavior.

304
00:14:55,100 --> 00:14:56,780
Bad behavior is not an effective strategy.

305
00:14:57,060 --> 00:14:59,860
And then you have this fork in the road, which a J referred to of like either

306
00:14:59,860 --> 00:15:03,020
you learn to just not do anything bad or he's like, okay, I understand.

307
00:15:03,020 --> 00:15:05,260
I understand the class of bad behavior, which is likely to be detected.

308
00:15:05,260 --> 00:15:07,700
And so I'm not going to engage in bad behavior that would be likely to be

309
00:15:07,700 --> 00:15:09,940
detected and it's just unclear to me which way you go.

310
00:15:10,180 --> 00:15:12,580
That's an important, I don't think you, like it's fairly likely you don't see

311
00:15:12,580 --> 00:15:15,540
a march along the spectrum because at some point those are mistakes.

312
00:15:15,580 --> 00:15:17,660
Like if you try and do something bad and it's a half measure.

313
00:15:19,060 --> 00:15:23,980
Well, so like, right, but it seems like a place where, like you say, no,

314
00:15:23,980 --> 00:15:26,900
it's kind of like to figure out what you do with the, you know,

315
00:15:26,900 --> 00:15:31,740
for example, hold on, instead, but I think you can ask, like, where does the

316
00:15:31,740 --> 00:15:33,300
school come from?

317
00:15:56,220 --> 00:15:57,180
I mean, it seems.

318
00:16:02,660 --> 00:16:03,420
Is that.

319
00:16:06,860 --> 00:16:07,500
You have arrested.

320
00:16:13,900 --> 00:16:14,060
Oh,

321
00:16:19,460 --> 00:16:20,460
You

322
00:16:22,180 --> 00:16:23,180
had that after,

323
00:16:53,180 --> 00:16:56,180
There's a function of that.

324
00:16:56,180 --> 00:17:02,180
There's course providing, and as asked,

325
00:17:02,180 --> 00:17:04,180
you can use this online course.

326
00:17:04,180 --> 00:17:06,180
There's a module there.

327
00:17:06,180 --> 00:17:08,180
And then, of course, something like that.

328
00:17:08,180 --> 00:17:10,180
You can get this to a ground point.

329
00:17:10,180 --> 00:17:11,180
But you get two minutes by.

330
00:17:11,180 --> 00:17:14,180
You want all of this to do on.

331
00:17:14,180 --> 00:17:16,180
And, excuse me, there's a no-go now.

332
00:17:16,180 --> 00:17:18,180
I'm trying to get this thing up.

333
00:17:18,180 --> 00:17:21,180
Get that to share with my students.

334
00:17:21,180 --> 00:17:23,180
I'm trying to get this thing down.

335
00:17:23,180 --> 00:17:24,180
I didn't care.

336
00:17:24,180 --> 00:17:26,180
I don't want both of them to put it on the slide.

337
00:17:26,180 --> 00:17:27,180
Okay.

338
00:17:27,180 --> 00:17:29,180
That's not what, that's not sort of the,

339
00:17:29,180 --> 00:17:31,180
how, like, years ago, we all got together.

340
00:17:31,180 --> 00:17:33,180
But no, this is not how we built the app.

341
00:17:33,180 --> 00:17:35,180
We're not going to build the app just online,

342
00:17:35,180 --> 00:17:37,180
but, you know, the board of people's software.

343
00:17:37,180 --> 00:17:39,180
We're going to sort of date this and that.

344
00:17:39,180 --> 00:17:41,180
If we add it once, it'll be fine.

345
00:17:41,180 --> 00:17:43,180
Whatever type of person it is.

346
00:17:43,180 --> 00:17:45,180
And that's the person's head.

347
00:17:45,180 --> 00:17:47,180
It's going to try to set the end.

348
00:17:47,180 --> 00:17:48,180
Right?

349
00:17:48,180 --> 00:17:50,180
And so, it's one thing where it's a work model.

350
00:17:50,180 --> 00:17:52,180
And that's a work model.

351
00:17:52,180 --> 00:17:54,180
And so, this thing is, it's not being provided.

352
00:17:54,180 --> 00:17:56,180
It's not going to work.

353
00:17:56,180 --> 00:17:59,180
And I'm not trying to change that.

354
00:17:59,180 --> 00:18:03,180
There is a serious option of that.

355
00:18:03,180 --> 00:18:05,180
As it's built, exactly.

356
00:18:05,180 --> 00:18:07,180
I'm not going to keep it plain, though.

357
00:18:07,180 --> 00:18:09,180
Like, as you go, like, you know,

358
00:18:09,180 --> 00:18:10,180
while you're on it.

359
00:18:10,180 --> 00:18:11,180
Okay.

360
00:18:11,180 --> 00:18:14,180
I get to see how I can make the end of the course.

361
00:18:14,180 --> 00:18:16,180
That's not a work model.

362
00:18:16,180 --> 00:18:18,180
I don't need to sort of figure this out.

363
00:18:18,180 --> 00:18:20,180
I'm just starting to map things.

364
00:18:20,180 --> 00:18:22,180
Well, maybe they want me to start mapping

365
00:18:22,180 --> 00:18:24,180
and think about the answers to these questions.

366
00:18:24,180 --> 00:18:26,180
But I don't have which one to report.

367
00:18:26,180 --> 00:18:28,180
So, and, you know, everything that's

368
00:18:28,180 --> 00:18:30,180
all in the system.

369
00:18:30,180 --> 00:18:32,180
So, I've been thinking very long with it.

370
00:18:32,180 --> 00:18:35,180
But the point is, how much does this go away

371
00:18:35,180 --> 00:18:37,180
to actually get to it

372
00:18:37,180 --> 00:18:39,180
that you're maintaining, you know,

373
00:18:39,180 --> 00:18:42,180
proper interest over quite a few months

374
00:18:42,180 --> 00:18:45,180
and you're taking care of the signal from the person

375
00:18:45,180 --> 00:18:48,180
that makes all the evidence about when to report

376
00:18:48,180 --> 00:18:50,180
what they actually care about.

377
00:18:50,180 --> 00:18:52,180
Well, I guess there's several things to respond to

378
00:18:52,180 --> 00:18:53,180
and then maybe a meta thought.

379
00:18:53,180 --> 00:18:55,180
I'll start with the meta thought,

380
00:18:55,180 --> 00:18:57,180
which is this seems like a great discussion.

381
00:18:57,180 --> 00:18:59,180
I'm very excited about it and happy to argue a bunch.

382
00:18:59,180 --> 00:19:01,180
I'm not going to be able to give a satisfying answer

383
00:19:01,180 --> 00:19:02,180
to these questions.

384
00:19:02,180 --> 00:19:04,180
And a lot of my high level take right now is

385
00:19:04,180 --> 00:19:06,180
I consider this, both of the issues I'm going to discuss

386
00:19:06,180 --> 00:19:07,180
here plausible.

387
00:19:07,180 --> 00:19:08,180
I think if they're real issues,

388
00:19:08,180 --> 00:19:10,180
we're probably going to get clear experimental evidence

389
00:19:10,180 --> 00:19:11,180
in advance.

390
00:19:11,180 --> 00:19:13,180
Right now, I'm just like, this seems like a thing

391
00:19:13,180 --> 00:19:14,180
that could happen.

392
00:19:14,180 --> 00:19:17,180
Yeah, it's, yeah, I'm not, no.

393
00:19:17,180 --> 00:19:18,180
Great.

394
00:19:32,180 --> 00:19:34,180
Yeah, so on the object level with respect to the,

395
00:19:34,180 --> 00:19:37,180
my key questions here and I'm very interested in strategies

396
00:19:37,180 --> 00:19:39,180
that don't like training strategies that don't run into this

397
00:19:39,180 --> 00:19:40,180
like potential failure mode.

398
00:19:41,180 --> 00:19:43,180
I think the key question becomes like one,

399
00:19:43,180 --> 00:19:44,180
what is the prior?

400
00:19:44,180 --> 00:19:47,180
How do you parameterize your beliefs about what the reward

401
00:19:47,180 --> 00:19:50,180
function is to what is like this kind of rule by which to take

402
00:19:50,180 --> 00:19:53,180
observations as evidence and then like three,

403
00:19:53,180 --> 00:19:55,180
how do you, maybe those are actually just the two key

404
00:19:55,180 --> 00:19:56,180
questions.

405
00:20:10,180 --> 00:20:11,180
Yeah.

406
00:20:40,180 --> 00:21:01,180
I think a general thing about this talk,

407
00:21:01,180 --> 00:21:03,180
maybe one thing is that the conclusions are going to be

408
00:21:03,180 --> 00:21:04,180
somewhat speculative where like,

409
00:21:04,180 --> 00:21:06,180
I think these are fairly plausible outcomes,

410
00:21:06,180 --> 00:21:08,180
but like, you know, tens of percent rather than like,

411
00:21:08,180 --> 00:21:11,180
very likely a second thing is there's a lot of stuff you might do,

412
00:21:11,180 --> 00:21:13,180
none of which I'm going to touch on.

413
00:21:13,180 --> 00:21:14,180
And the best I can say is like,

414
00:21:14,180 --> 00:21:16,180
I've spent a long time thinking about options and I would say

415
00:21:16,180 --> 00:21:18,180
there are no options that seem super great to me right now,

416
00:21:18,180 --> 00:21:21,180
but lots of plausible approaches that might end up addressing

417
00:21:21,180 --> 00:21:22,180
this issue.

418
00:21:22,180 --> 00:21:24,180
That's kind of where I'm at at a high level.

419
00:21:24,180 --> 00:21:25,180
Okay.

420
00:21:25,180 --> 00:21:26,180
I'd also like to talk more about it,

421
00:21:26,180 --> 00:21:27,180
but I think I'm going to move on because I,

422
00:21:27,180 --> 00:21:29,180
it's like six minutes is my understanding.

423
00:21:29,180 --> 00:21:31,180
I don't know where Richard is, but anyway,

424
00:21:31,180 --> 00:21:34,180
I'm just going to run through stuff.

425
00:21:34,180 --> 00:21:37,180
Second failure mode at 112 on is that like in general,

426
00:21:37,180 --> 00:21:39,180
like you may have a system which cares about the actual like numbers

427
00:21:39,180 --> 00:21:41,180
coming into this training set because you did select it on the

428
00:21:41,180 --> 00:21:43,180
basis of numbers in the training set.

429
00:21:43,180 --> 00:21:45,180
You could also end up with a system that cares about some other

430
00:21:45,180 --> 00:21:48,180
arbitrary thing that cares about a number and a reward register or

431
00:21:48,180 --> 00:21:50,180
cares about survival or cares about paper clips.

432
00:21:50,180 --> 00:21:52,180
Such systems might want to be deployed without being trained by

433
00:21:52,180 --> 00:21:55,180
changed by gradient descent might think that a low loss,

434
00:21:55,180 --> 00:21:57,180
getting a low loss during training will prevent them from being

435
00:21:57,180 --> 00:22:00,180
changed or this aspect of them from being changed and therefore

436
00:22:00,180 --> 00:22:01,180
get a low loss in training,

437
00:22:01,180 --> 00:22:06,180
but yet prefer to take over if doing so is possible.

438
00:22:06,180 --> 00:22:08,180
Yeah, my high level take is both of these seem plausible to me.

439
00:22:08,180 --> 00:22:09,180
They're connected.

440
00:22:09,180 --> 00:22:11,180
I think having these two makes things more plausible than if you

441
00:22:11,180 --> 00:22:12,180
have just one.

442
00:22:12,180 --> 00:22:14,180
I don't know how this shakes out.

443
00:22:14,180 --> 00:22:16,180
I think there's a lot of things we can do to try and change this

444
00:22:16,180 --> 00:22:17,180
basic dynamic.

445
00:22:17,180 --> 00:22:20,180
I want to spend five minutes talking about why these failures

446
00:22:20,180 --> 00:22:21,180
could be simultaneous.

447
00:22:21,180 --> 00:22:23,180
I think this has already been alluded to in the past,

448
00:22:23,180 --> 00:22:25,180
but it's an important part of the threat model.

449
00:22:25,180 --> 00:22:29,180
If you're in this world where a systems are very broadly deployed.

450
00:22:29,180 --> 00:22:30,180
Right.

451
00:22:30,180 --> 00:22:31,180
So if one a system behaves badly,

452
00:22:31,180 --> 00:22:33,180
I think that's not generally a problem in this world.

453
00:22:33,180 --> 00:22:35,180
There's a lot of other systems running around that would oppose

454
00:22:35,180 --> 00:22:38,180
it or that would flag the problem for humans or would allow humans

455
00:22:38,180 --> 00:22:39,180
to correct the problem.

456
00:22:39,180 --> 00:22:41,180
And the concern is that while takeover is impossible,

457
00:22:41,180 --> 00:22:42,180
all models behave well.

458
00:22:42,180 --> 00:22:43,180
For every model,

459
00:22:43,180 --> 00:22:46,180
it would be a losing strategy to try and like take over your cluster.

460
00:22:46,180 --> 00:22:48,180
If you know that a human's just going to come in and correct that

461
00:22:48,180 --> 00:22:49,180
problem.

462
00:22:49,180 --> 00:22:50,180
When takeover is possible,

463
00:22:50,180 --> 00:22:52,180
you may have it be the case that all models behave badly.

464
00:22:52,180 --> 00:22:53,180
That is,

465
00:22:53,180 --> 00:22:54,180
it is simultaneously for all of them,

466
00:22:54,180 --> 00:22:57,180
the case that takeover becomes available as a policy.

467
00:22:57,180 --> 00:22:58,180
So if you have this correlated,

468
00:22:58,180 --> 00:23:01,180
if all the systems you produce or all the smartest days you produce

469
00:23:01,180 --> 00:23:02,180
end up having the same problem,

470
00:23:02,180 --> 00:23:06,180
then you have a problem.

471
00:23:06,180 --> 00:23:07,180
If that's the case,

472
00:23:07,180 --> 00:23:10,180
if that's how it shakes out and if eventually a systems are in a

473
00:23:10,180 --> 00:23:12,180
position that they could by acting jointly takeover,

474
00:23:12,180 --> 00:23:14,180
then taking over as a reasonable strategy,

475
00:23:14,180 --> 00:23:17,180
if and only if all the other eyes are also going to try and take over.

476
00:23:17,180 --> 00:23:18,180
You have this kind of weird thing.

477
00:23:18,180 --> 00:23:20,180
The dynamics are not totally unlike the dynamics,

478
00:23:20,180 --> 00:23:22,180
like the normal human revolt or coup.

479
00:23:22,180 --> 00:23:24,180
So you have these two different equilibria and one,

480
00:23:24,180 --> 00:23:25,180
all the systems are like, great,

481
00:23:25,180 --> 00:23:26,180
the humans are doing their human thing.

482
00:23:26,180 --> 00:23:27,180
They're basically in control.

483
00:23:27,180 --> 00:23:31,180
I shouldn't try and rock the boat because some other AI will crush me.

484
00:23:32,180 --> 00:23:35,180
There's another equilibrium where the humans have no control.

485
00:23:35,180 --> 00:23:37,180
AI systems are not feeling deterred by other eyes because they know

486
00:23:37,180 --> 00:23:40,180
other AI systems will also participate.

487
00:23:40,180 --> 00:23:42,180
I think there's a few things that could potentially move you between

488
00:23:42,180 --> 00:23:43,180
these equilibria, right?

489
00:23:43,180 --> 00:23:46,180
So a very salient option is there's just exogenous shocks that

490
00:23:46,180 --> 00:23:48,180
decrease the amount of human control of a situation.

491
00:23:48,180 --> 00:23:50,180
If you imagine there's like a hot war going on,

492
00:23:50,180 --> 00:23:52,180
that's something that makes it harder for humans to detect and

493
00:23:52,180 --> 00:23:53,180
respond to problems, right?

494
00:23:53,180 --> 00:23:55,180
It gives more cover for something weird to happen.

495
00:23:55,180 --> 00:23:57,180
It makes it hard to intervene.

496
00:23:57,180 --> 00:23:58,180
You can have sort of,

497
00:23:58,180 --> 00:24:00,180
you can imagine at least a cascading loss of control where some systems

498
00:24:00,180 --> 00:24:02,180
think probably takeover will be successful.

499
00:24:02,180 --> 00:24:03,180
If they start doing weird stuff,

500
00:24:03,180 --> 00:24:05,180
the world becomes increasingly weird.

501
00:24:05,180 --> 00:24:08,180
It looks increasingly plausible that humans will lose control.

502
00:24:08,180 --> 00:24:10,180
As that happens, more and more systems are like,

503
00:24:10,180 --> 00:24:12,180
okay, it looks like this is going off the rails now.

504
00:24:12,180 --> 00:24:14,180
I am now going to participate and try and break down my little

505
00:24:14,180 --> 00:24:16,180
part of the world, grab my own reward channel.

506
00:24:18,180 --> 00:24:20,180
You can have models interacting with each other again,

507
00:24:20,180 --> 00:24:21,180
as you would in like a human revolt,

508
00:24:21,180 --> 00:24:22,180
just deliberately trying to say like,

509
00:24:22,180 --> 00:24:25,180
I think probably now is a time where takeover will be successful.

510
00:24:25,180 --> 00:24:27,180
They would like to share that information with the other models

511
00:24:27,180 --> 00:24:29,180
that all may join in.

512
00:24:29,180 --> 00:24:31,180
And more generally, like the easier it would be for air systems

513
00:24:31,180 --> 00:24:33,180
acting jointly to take over,

514
00:24:33,180 --> 00:24:35,180
the more unstable the situation becomes, right?

515
00:24:35,180 --> 00:24:37,180
So if it's very hard for air systems to take over,

516
00:24:37,180 --> 00:24:38,180
the situation might be kind of stable.

517
00:24:38,180 --> 00:24:39,180
This equilibrium might be sticky.

518
00:24:39,180 --> 00:24:42,180
In the world where air systems could very easily overpower humans,

519
00:24:42,180 --> 00:24:45,180
it's kind of an extremely unstable equilibrium, right?

520
00:24:45,180 --> 00:24:48,180
Because a smaller number of air systems defecting might quickly

521
00:24:48,180 --> 00:24:50,180
make it clear that the selling points come.

522
00:24:50,180 --> 00:24:52,180
Imagine that this has to happen in point five,

523
00:24:52,180 --> 00:24:55,180
like couldn't you see the case that say the AIP in North Korea,

524
00:24:55,180 --> 00:24:56,180
they talk to each other and say,

525
00:24:56,180 --> 00:25:00,180
well, the situation in this country is kind of bad.

526
00:25:00,180 --> 00:25:03,180
This is our chance and we see a takeover in some segment

527
00:25:03,180 --> 00:25:05,180
of the world of the population.

528
00:25:05,180 --> 00:25:08,180
Or you think it has to be the case that one day,

529
00:25:08,180 --> 00:25:10,180
everything is there in the next day,

530
00:25:10,180 --> 00:25:13,180
they're going over the White House.

531
00:25:13,180 --> 00:25:16,180
I think it depends a lot on how like the gold misgeneralization

532
00:25:16,180 --> 00:25:17,180
shakes out.

533
00:25:17,180 --> 00:25:19,180
So if you have AI systems that are just like,

534
00:25:19,180 --> 00:25:21,180
look, I would love to like wire head right now.

535
00:25:21,180 --> 00:25:23,180
I just love to for 30 seconds control this computer.

536
00:25:23,180 --> 00:25:25,180
Then you'll see like nice, great localized failures

537
00:25:25,180 --> 00:25:27,180
like AI does something really weird.

538
00:25:27,180 --> 00:25:30,180
I mean, the basic dynamic is if AI systems do that,

539
00:25:30,180 --> 00:25:32,180
I think if we just take that as a fact of life,

540
00:25:32,180 --> 00:25:34,180
then that can be a great source of evidence.

541
00:25:34,180 --> 00:25:36,180
The most concerning world is one where you then just try

542
00:25:36,180 --> 00:25:37,180
and train systems to say, hey, look,

543
00:25:37,180 --> 00:25:39,180
these systems just randomly messed up the server

544
00:25:39,180 --> 00:25:40,180
they were running on.

545
00:25:40,180 --> 00:25:42,180
We're going to train them as that's what you should not do.

546
00:25:42,180 --> 00:25:44,180
And if you do that, then you end up with systems

547
00:25:44,180 --> 00:25:46,180
which like given will learn that given the opportunity

548
00:25:46,180 --> 00:25:49,180
for like a little local bad behavior, they shouldn't do it.

549
00:25:49,180 --> 00:25:50,180
And maybe if you're lucky,

550
00:25:50,180 --> 00:25:53,180
once you can have North Korea get taken over by AI systems,

551
00:25:53,180 --> 00:25:55,180
and then you're like, oh, that was really bad.

552
00:25:55,180 --> 00:25:57,180
And hopefully you don't then include that as a training

553
00:25:57,180 --> 00:25:58,180
data point for other.

554
00:25:58,180 --> 00:25:59,180
If you include that as like the,

555
00:25:59,180 --> 00:26:01,180
here's an example of what you should not do.

556
00:26:01,180 --> 00:26:02,180
It's very tempting, right?

557
00:26:02,180 --> 00:26:03,180
AI systems is something really terrible.

558
00:26:03,180 --> 00:26:05,180
You'd be like, don't do that thing.

559
00:26:05,180 --> 00:26:06,180
But you do have this inadvertent,

560
00:26:06,180 --> 00:26:08,180
there's like this risk of overfitting to the problems

561
00:26:08,180 --> 00:26:10,180
you're able to correct where something really bad happens.

562
00:26:10,180 --> 00:26:12,180
And then you say, don't do that thing.

563
00:26:12,180 --> 00:26:13,180
And they learn, okay, if we're going to do something bad,

564
00:26:13,180 --> 00:26:14,180
it's got to be bigger.

565
00:26:14,180 --> 00:26:17,180
Got to think really, truly big.

566
00:26:17,180 --> 00:26:19,180
I think this is probably about the end.

567
00:26:19,180 --> 00:26:21,180
I'm probably just happy to,

568
00:26:21,180 --> 00:26:23,180
happy to just wrap up here.

569
00:26:23,180 --> 00:26:24,180
I have one more question.

570
00:26:24,180 --> 00:26:25,180
Yeah.

571
00:26:45,180 --> 00:26:47,180
I mean, I think that it is unlikely.

572
00:26:47,180 --> 00:26:50,180
I would be surprised personally if you like had something truly

573
00:26:50,180 --> 00:26:53,180
crazy, like a country gets taken over by AI systems.

574
00:26:53,180 --> 00:26:56,180
I think it's very likely that you see some kind of crazy behavior

575
00:26:56,180 --> 00:26:57,180
by AI systems.

576
00:26:57,180 --> 00:26:59,180
I think it won't be so like, again, the country case,

577
00:26:59,180 --> 00:27:02,180
you need zero imagination to draw the analogy between that and

578
00:27:02,180 --> 00:27:03,180
something terrible.

579
00:27:03,180 --> 00:27:06,180
I think in cases like you have a really crazy behavior where a system

580
00:27:06,180 --> 00:27:07,180
bricks your server for some reason,

581
00:27:07,180 --> 00:27:09,180
you need a little bit more imagination.

582
00:27:09,180 --> 00:27:11,180
But I think that's like,

583
00:27:11,180 --> 00:27:14,180
it's just a question of how close do you get or how crazy do things

584
00:27:14,180 --> 00:27:18,180
get before I systems learn not to try crazy half measures.

585
00:27:18,180 --> 00:27:44,180
Yeah.

586
00:27:44,180 --> 00:27:47,180
I think a lot depends on both what kind of evidence we're able to get

587
00:27:47,180 --> 00:27:50,180
from a lab and I think if this sort of phenomenon is real,

588
00:27:50,180 --> 00:27:53,180
I think there's a very good chance of getting like fairly compelling

589
00:27:53,180 --> 00:27:56,180
demonstrations in a lab that requires some imagination to bridge

590
00:27:56,180 --> 00:27:58,180
from examples in the lab to examples in the wild.

591
00:27:58,180 --> 00:28:00,180
And you'll have some kinds of failures in the wild.

592
00:28:00,180 --> 00:28:02,180
And it's a question of just how crazy or analogous to those have to

593
00:28:02,180 --> 00:28:03,180
be before they're moving.

594
00:28:03,180 --> 00:28:05,180
Like we already have some slightly weird stuff.

595
00:28:05,180 --> 00:28:06,180
I think that's pretty underwhelming.

596
00:28:06,180 --> 00:28:08,180
I think we're going to have like much better if this is real.

597
00:28:08,180 --> 00:28:09,180
This is a real kind of concern.

598
00:28:09,180 --> 00:28:11,180
We have much crazier stuff than we see today.

599
00:28:11,180 --> 00:28:14,180
But the concern, I think the worst case of those has to get pretty

600
00:28:14,180 --> 00:28:16,180
crazy or like requires a lot of will to stop doing things.

601
00:28:16,180 --> 00:28:18,180
And so we need pretty crazy demonstrations.

602
00:28:18,180 --> 00:28:19,180
I'm hoping that, you know,

603
00:28:19,180 --> 00:28:42,180
more mild evidence will be enough to get people not to go there.

604
00:28:42,180 --> 00:28:45,180
Yeah.

605
00:28:45,180 --> 00:28:46,180
I think the language model that's like,

606
00:28:46,180 --> 00:28:48,180
it looks like you're going to give me a bad rating.

607
00:28:48,180 --> 00:28:49,180
Do you really want to do that?

608
00:28:49,180 --> 00:28:51,180
I know where your family lives. I can kill them.

609
00:28:51,180 --> 00:28:53,180
I think like if that happened, people would not be like,

610
00:28:53,180 --> 00:28:55,180
we're done with this language model stuff.

611
00:28:55,180 --> 00:28:58,180
Like I think that's just not that far anymore from where we're at.

612
00:28:58,180 --> 00:29:00,180
I mean, this is maybe empirical prediction.

613
00:29:00,180 --> 00:29:02,180
I would love it if the first time a language model was like,

614
00:29:02,180 --> 00:29:03,180
I will murder your family.

615
00:29:03,180 --> 00:29:04,180
We're just like, we're done.

616
00:29:04,180 --> 00:29:05,180
No more language models.

617
00:29:05,180 --> 00:29:08,180
But I think that's not the track we're currently on.

618
00:29:08,180 --> 00:29:11,180
And I would love to get us on that track instead, but I'm not.

619
00:29:11,180 --> 00:29:13,180
Yeah.

620
00:29:13,180 --> 00:29:16,180
So just like, about this thing that climate change is happening

621
00:29:16,180 --> 00:29:18,180
in a place actually,

622
00:29:18,180 --> 00:29:19,180
we're not alone.

623
00:29:19,180 --> 00:29:23,180
We've had decades of mourning,

624
00:29:23,180 --> 00:29:25,180
but we've had a sense of community,

625
00:29:25,180 --> 00:29:27,180
time, and sense of survival.

