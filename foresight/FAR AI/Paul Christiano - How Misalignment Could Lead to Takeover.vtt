WEBVTT

00:00.000 --> 00:17.440
So this talk is going to be a little bit on the crazier end, it's going to be completely

00:17.440 --> 00:18.440
non-technical.

00:18.440 --> 00:23.540
Yeah, but these are things I believe that I think are important for how much and how

00:23.540 --> 00:26.680
we should think about and worry about and work on alignment.

00:26.680 --> 00:29.980
So things I'm very happy to discuss and debate over the next day of the workshop and later

00:29.980 --> 00:30.980
today.

00:30.980 --> 00:34.620
I'm not sure, hopefully, a reasonable amount of time for questions as I go through it,

00:34.620 --> 00:39.420
but yeah, we have more opportunities to talk later on and feel free to object or just

00:39.420 --> 00:41.420
may punt things to talk about later.

00:41.420 --> 00:46.580
Yeah, so I'm going to be, we've talked so far about why models may end up being misaligned

00:46.580 --> 00:48.140
and why that may be hard to measure.

00:48.140 --> 00:52.420
I'm going to talk about how that actually ultimately leads to sort of total human disempowerment.

00:52.420 --> 00:55.740
I'm calling takeover here for short.

00:55.740 --> 00:59.020
The structure of the talk, I'm first going to talk a little bit about why I think AI systems

00:59.020 --> 01:01.940
will eventually be in a position to disempower humanity.

01:01.940 --> 01:06.500
That is, unless we deliberately change the way we deploy AI, then I'm going to talk about

01:06.500 --> 01:10.420
why misaligned AI systems might be motivated to disempower humanity, basically just slightly

01:10.420 --> 01:13.380
extending or repeating arguments from earlier in the day.

01:13.380 --> 01:17.180
And then I'm going to talk a little bit about why AI systems may ultimately sort of effectively

01:17.180 --> 01:20.500
coordinate to disempower humanity, that is why you may have simultaneous failures across

01:20.500 --> 01:26.780
many systems rather than a single system behaving badly.

01:26.780 --> 01:29.420
So start with why I think AI systems will likely be able to take over.

01:29.420 --> 01:32.820
I think a thing worth saying is that I'm going to talk about what I see as like the single

01:32.820 --> 01:35.180
most likely scenario resulting in catastrophe.

01:35.180 --> 01:38.940
This is not the only way you end up with even the scenario kind of catastrophe.

01:38.940 --> 01:42.860
So I'm going to talk about a system where AI systems are extremely broadly deployed in

01:42.860 --> 01:46.860
the world prior to anything bad happening.

01:46.860 --> 01:51.900
So I imagine that, for example, AI systems are designing and running warehouses and factories

01:51.900 --> 01:56.260
and data centers, they're operating and designing robots, they're writing the large majority

01:56.260 --> 02:01.420
of all code that is written, they handle complicated litigation, they would fight a war, they do

02:01.420 --> 02:04.220
most trades, they run most investment firms.

02:04.220 --> 02:07.900
When humans act in these domains, they do so with a lot of AI systems to understand what's

02:07.900 --> 02:12.100
going on and to provide strategic advice.

02:12.100 --> 02:14.180
So this world is pretty different from the world of today.

02:14.180 --> 02:17.420
I think we are starting to be able to see what such a world would look like.

02:17.420 --> 02:20.380
I think it's not clear how far away this kind of world is, right?

02:20.380 --> 02:23.380
If you're doubling once a year, it doesn't take you that long to go from billions of

02:23.380 --> 02:28.740
dollars to trillions of dollars of economic impact.

02:28.740 --> 02:34.780
But this is the sort of setting in which this entire story takes place.

02:34.780 --> 02:37.900
So one thing that can happen is you have a lot of AI systems operating is that the world

02:37.900 --> 02:41.260
can get pretty complicated for humans, hard for humans to understand, right?

02:41.260 --> 02:44.860
So one simple way this can happen is that you have AI systems with deep expertise in

02:44.860 --> 02:46.380
the domains where they're operating, right?

02:46.380 --> 02:50.380
They've seen huge amounts of data compared to what any human has seen about the specific

02:50.380 --> 02:51.380
domain they operate in.

02:51.380 --> 02:52.780
They think very quickly.

02:52.780 --> 02:53.780
They're very numerous.

02:53.780 --> 02:59.220
Another point is that right now, when a human deploys an AI system, they often have to understand

02:59.220 --> 03:00.220
it quite well.

03:00.220 --> 03:03.500
But that process of understanding a domain and applying AI is itself something that is

03:03.500 --> 03:05.100
likely to be subject to automation.

03:05.100 --> 03:06.100
Yeah.

03:06.100 --> 03:07.100
Great.

03:07.100 --> 03:24.540
I agree with this remark, that is, I think that you have a better chance of understanding

03:24.540 --> 03:27.420
an AI system if there are fewer of them and there's more human labor going into each

03:27.420 --> 03:28.420
deployment.

03:28.420 --> 03:30.540
But it is clearly, in fact, there's a general case for everything I said.

03:30.540 --> 03:33.740
I'm going to talk about a bunch of factors that can exacerbate risk, but I think you

03:33.740 --> 03:36.740
could cut out actually almost every single thing I say in this talk and you'd be left

03:36.780 --> 03:37.780
with some risk.

03:37.780 --> 03:40.220
And it's just that each of these makes the situation, from my perspective, a little bit

03:40.220 --> 03:41.220
worse.

03:41.220 --> 03:46.740
The other thing is, I think both, I think for you, the world are insurance and other

03:46.740 --> 03:50.540
reasons, but I don't really understand what this is.

03:50.540 --> 03:52.540
I know human understands that well what this is.

03:52.540 --> 03:53.540
Yeah.

03:53.540 --> 03:54.540
I'm not sure.

03:54.540 --> 03:55.540
I don't know.

03:55.540 --> 03:56.540
You could take something from a person like that, probably, or I probably wouldn't understand

03:56.540 --> 03:57.540
something about this.

03:57.540 --> 04:11.740
So I would say that in the world of today, most of us are very used to living in a world

04:11.740 --> 04:12.740
where most things would happen.

04:12.740 --> 04:14.100
I don't understand how they happen.

04:14.100 --> 04:15.500
I don't quite understand why they happen.

04:15.500 --> 04:17.340
I trust there was some kind of reason why they happen.

04:17.340 --> 04:19.980
But if you showed me an Amazon warehouse, and you're like, what's going on?

04:19.980 --> 04:20.980
I'd be like, I don't know.

04:20.980 --> 04:22.300
I guess it somehow delivers things.

04:22.300 --> 04:23.300
Something's going on.

04:23.300 --> 04:27.420
So the situation is already, I'm kind of trusting someone to make these decisions in

04:27.420 --> 04:29.060
a way that points at goals I like.

04:29.060 --> 04:31.940
I think that it's not inherently even a bad thing to be in this kind of world.

04:31.940 --> 04:34.460
Like, I think we're always going to be in a world where most people don't understand

04:34.460 --> 04:37.060
most things.

04:37.060 --> 04:41.620
I think that it is not inherently bad that no human understands things and only as understand

04:41.620 --> 04:42.620
something.

04:42.620 --> 04:45.620
I think it's mostly just important for understanding the context in which we're talking about concerns

04:45.620 --> 04:46.620
about AI takeover.

04:46.620 --> 04:49.380
I think that if you're imagining a world where there's like a world, there's a human

04:49.380 --> 04:53.020
world and one AI system that's like, how can I scheme and outmaneuver the humans?

04:53.020 --> 04:56.100
I think that is a possible thing that could happen as a source of risk.

04:56.100 --> 04:59.420
I think the most likely risk scenarios are more like a world that is increasingly, there

04:59.420 --> 05:02.260
are very few humans who understand many important domains.

05:02.260 --> 05:05.300
And so when we're talking about AI's taking over, I think it's, at least when I visualize

05:05.300 --> 05:09.580
this, it really changes my picture of how this is going to go when I think about that

05:09.580 --> 05:10.580
kind of world.

05:10.580 --> 05:15.660
Is it mentally, especially in the case of AI, that you need to keep in hand, that very,

05:15.660 --> 05:20.180
very smart, and you get to start into a drone, into a car manufacturing, et cetera, and right

05:20.180 --> 05:24.820
now they're doing those for us, but in the cycle, do you think you can get out of it?

05:24.860 --> 05:26.500
Yeah, that seems like a reasonable mental image.

05:26.500 --> 05:29.420
There's a little bit depends exactly what your associations are with a little green man.

05:30.100 --> 05:33.180
But yeah, there's a lot of, there's a lot of stuff happening in the world being done

05:33.180 --> 05:35.140
by AI's of all sorts of shapes and sizes.

05:36.340 --> 05:38.380
Some of them are doing things that we do understand well, right?

05:38.380 --> 05:41.020
There's a lot of things like, yep, this AI system, like just does a simple function,

05:41.020 --> 05:42.740
which we understand, and some of them are more complicated.

05:43.500 --> 05:46.660
On this slide, I've just thrown up an example of a random biological system.

05:46.660 --> 05:49.860
Biological systems are relevant because humans don't understand them that well.

05:49.860 --> 05:52.220
We just see that these things that were optimized in the world.

05:52.900 --> 05:53.940
I have a similar vibe, right?

05:53.940 --> 05:56.660
If you ask me, like how even things I understand kind of well,

05:56.660 --> 05:59.700
like how a data center is managed, from my perspective, it feels a little bit like

05:59.700 --> 06:00.660
a biological system.

06:00.660 --> 06:02.860
And like someone heard it and it's somehow going to recover.

06:03.220 --> 06:06.300
And like, I don't totally understand how that works, but I have some sense of

06:06.300 --> 06:09.580
like the telos, like that's the kind of relationship I often have to systems even

06:09.580 --> 06:11.020
maintained by humans and built by humans.

06:11.020 --> 06:13.580
I just expect to have more and more of that sense as the world becomes more

06:13.580 --> 06:14.540
complicated, right?

06:14.540 --> 06:17.180
The way humans relate to this world is increasingly like, you know, the way

06:17.180 --> 06:20.220
we raise the complex systems in general, and the only difference from the world

06:20.220 --> 06:22.020
today, I think that's already the case to a great extent.

06:22.300 --> 06:24.900
The difference is just there is no human who knows what's going on really.

06:26.500 --> 06:29.140
Or at least most people who are making most of those decisions, I know most

06:29.140 --> 06:30.060
details aren't humans.

06:32.140 --> 06:37.060
So one upshot of this that's important to me is that it pushes us increasingly to

06:37.060 --> 06:39.660
train AI systems based on outcomes of their decisions.

06:39.780 --> 06:42.340
I think the less you understand or the less humans have time to understand

06:42.340 --> 06:45.100
details of what's happening in the domain or the kinds of decisions an AI is

06:45.100 --> 06:48.980
making, the more important it is to say, let's just do what the AI said and see

06:48.980 --> 06:50.100
what happens, right?

06:50.100 --> 06:52.620
The more we are able, right, if you're trying to select an organism or

06:52.620 --> 06:54.580
something, you're not going to look at how the organism makes decisions, you're

06:54.580 --> 06:56.340
going to see how well does it do, how fit is it?

06:58.420 --> 06:59.860
That's like one implication of this.

07:00.260 --> 07:03.220
Another implication is that it becomes harder for humans to understand what's

07:03.220 --> 07:05.100
going on or intervene if things are going poorly.

07:06.260 --> 07:09.940
So an upshot of that is that AI is in practice, bad behavior by AI is mostly

07:09.940 --> 07:12.780
detected or corrected by other AI systems, right?

07:12.780 --> 07:16.220
So AI systems are mostly monitoring other AI's where you can imagine humans

07:16.220 --> 07:19.220
monitoring or automated monitoring, but at some point sophisticated AI systems

07:19.300 --> 07:20.700
are responsible for most monitoring.

07:21.100 --> 07:24.500
Even normal law enforcement or fighting a potential war would be done by AI

07:24.500 --> 07:25.380
systems, right?

07:25.380 --> 07:28.980
Because decisions are made quickly and because conflict is often deliberately

07:28.980 --> 07:30.620
made complex by a party to the conflict.

07:31.700 --> 07:33.940
AI systems are running a lot of critical infrastructure.

07:33.940 --> 07:36.660
Just if you want to achieve something in the world, AI plays an important part

07:36.660 --> 07:37.500
in that process.

07:38.420 --> 07:40.540
So none of this is necessarily bad, right?

07:40.540 --> 07:44.020
I think positive stories about AI will also have the step where good AI keeps

07:44.020 --> 07:44.860
bad AI in check.

07:45.100 --> 07:48.700
I think it just is something that like now requires trust on behalf of all of

07:48.700 --> 07:50.860
humanity, extended to all of these AI systems we've built.

07:51.300 --> 07:53.780
You might hope that we could trust the AI systems we've built because we built

07:53.780 --> 07:55.220
them, we have freedom to design them.

07:55.980 --> 07:57.860
But I think that depends on some decisions we make.

07:57.900 --> 08:00.140
And yeah, we'll discuss how it can get somewhat more ugly.

08:01.380 --> 08:03.540
But the point I'm going to make right now or in this first section of the

08:03.540 --> 08:07.740
talk is not that it would necessarily get ugly, just that if for some reason,

08:07.980 --> 08:10.940
all of the AI systems in the world were like, what we really want to do is

08:10.940 --> 08:14.180
disempower humanity, then I think it is quite likely that they would succeed.

08:14.860 --> 08:17.700
And I think this isn't even really the spicy take are getting my optimistic

08:17.700 --> 08:19.020
futures. This is also the case.

08:21.700 --> 08:24.740
And the point is that the core question is just to do you somehow end up in a

08:24.740 --> 08:27.900
situation where all of these AI systems, including the AI systems responsible for

08:27.900 --> 08:31.140
doing monitoring or doing law enforcement or running your data centers or running

08:31.140 --> 08:34.060
your infrastructure, where all of those AI systems, for some reason, want to

08:34.060 --> 08:34.900
disempower humans.

08:40.820 --> 08:43.740
No, I think the optimistic scenario still involves a point where AI systems

08:43.740 --> 08:45.140
acting collectively could take over.

08:45.540 --> 08:46.780
I think in the optimistic scenario.

08:47.300 --> 08:47.620
Yeah.

08:49.300 --> 08:52.340
In the optimistic scenario, you, uh, you wait until you are ready.

08:52.340 --> 08:55.740
You don't end up in the situation until in fact they would not try and take over.

08:55.780 --> 08:58.060
And you're confident of that either because you've determined that the

08:58.060 --> 09:01.460
empirics shake out such that this talk is just crazy talk, very plausible.

09:01.700 --> 09:04.820
Or this talk was justified and we've addressed the problems or whatever.

09:05.820 --> 09:10.500
This is our already on the case of our grid, for example, if the power grid goes

09:10.500 --> 09:13.460
down now, a large fraction of humanity is going to die.

09:14.340 --> 09:15.460
I think it's plausible.

09:16.060 --> 09:21.140
Well, a large fraction of humanity is maybe a small potatoes, but yeah, I think

09:21.140 --> 09:24.020
it's going to depend when I say if all AI's want to take over, the arguments

09:24.020 --> 09:26.940
I'm going to make are going to reply to like a particular class of AI systems.

09:27.020 --> 09:27.180
Right.

09:27.180 --> 09:29.180
So it's not going to be like all electronic devices failed.

09:29.180 --> 09:31.300
It's going to be some class of AI is produced in a certain way.

09:32.020 --> 09:35.180
Um, and I think that what happens over time is just, it becomes more and more

09:35.180 --> 09:38.060
plausible for like the top end of that distribution, right?

09:38.060 --> 09:40.820
Of the systems that are most sophisticated or like, right.

09:40.820 --> 09:44.260
Right now, if all systems trained with deep learning simultaneously failed

09:44.260 --> 09:46.900
and we're like, we hate the humans, we really want to disempower them, I

09:46.900 --> 09:47.660
think it would be fine.

09:47.980 --> 09:50.500
It's not totally obvious and it depends how smart they are about how they failed.

09:50.980 --> 09:52.540
Um, I think we'd probably be okay.

09:52.940 --> 09:55.980
But I mean, again, I think that the whole story we're telling this is kind of

09:55.980 --> 09:58.620
just like, I think you could take out and there's still be some risk.

09:58.860 --> 10:01.420
Yeah.

10:04.940 --> 10:05.540
If what failed?

10:09.380 --> 10:12.260
Yeah, I think that's, you can imagine cases where it's like, if all the cars

10:12.260 --> 10:13.460
failed, that's enough to kill everyone.

10:13.700 --> 10:15.220
This is not necessarily a striking claim.

10:16.460 --> 10:19.900
The striking claim is definitely the one that it is plausible that misaligned

10:19.900 --> 10:21.420
AI systems may want to take over.

10:21.460 --> 10:23.740
I guess this is in some sense the part we've been most talking about throughout

10:23.740 --> 10:24.060
the day.

10:24.460 --> 10:26.460
I'm going to dwell on this a bit and then I'm going to talk about why these

10:26.460 --> 10:28.380
failures may be correlated in a problematic way.

10:28.500 --> 10:31.140
That is why you might have all AI systems trying to take over at the same time.

10:33.340 --> 10:36.860
Um, this, I'm going to talk about two failure modes, both of which we've

10:36.860 --> 10:37.740
touched on earlier.

10:37.940 --> 10:39.100
So one is reward hacking.

10:39.260 --> 10:42.980
That is that disempowering humanity may be an effective strategy for AI systems

10:42.980 --> 10:44.380
collectively to get a lot of reward.

10:44.780 --> 10:48.020
And the second is deceptive alignment scenario that Rohan talked about.

10:53.180 --> 10:53.900
Okay.

10:53.900 --> 10:57.060
So I mentioned briefly before this idea that you may train AI systems by

10:57.060 --> 10:59.300
evaluating the outcomes of the actions they propose.

10:59.820 --> 11:02.540
Um, so just to briefly review what that actually entails, right?

11:02.540 --> 11:04.020
We have some policy, some big neural net.

11:04.420 --> 11:08.780
It proposes some actions and some of those actions to distinguish between them.

11:08.780 --> 11:11.780
We actually need to execute the action, measure the results and decide how much

11:11.780 --> 11:12.660
we like the results.

11:12.660 --> 11:15.220
Let's say the reward is to start judgment of how much we like the results.

11:15.660 --> 11:19.180
And then we adjust policies to maximize the expected reward of the actions

11:19.180 --> 11:19.820
they propose.

11:21.220 --> 11:21.420
Right.

11:21.420 --> 11:24.620
And there's a, it's plausible that if you do this, you get a policy which is

11:24.860 --> 11:27.980
implicitly or explicitly considering many possible actions and selecting the

11:27.980 --> 11:29.620
actions that will lead to the highest reward.

11:30.340 --> 11:32.140
There are other ways you could end up at that same outcome, right?

11:32.140 --> 11:34.780
We could do model based RL and explicitly have a loop in which we predict the

11:34.780 --> 11:36.060
consequences of different actions.

11:36.220 --> 11:39.860
You could have decision transformers, the condition on like high quality actions.

11:40.260 --> 11:43.700
It could just have some other planning process needed still into a model,

11:43.700 --> 11:46.820
whatever, all of these leads to like the same endpoint, which is sometimes

11:46.820 --> 11:48.460
because we don't know how to achieve goals.

11:48.580 --> 11:51.540
We're going to train AI systems to take actions that lead to high reward where

11:51.540 --> 11:54.500
reward means we measure the outcome and then we decide how much we like that

11:54.500 --> 11:55.300
result that we measured.

11:57.700 --> 11:59.260
This can potentially lead to takeover.

11:59.260 --> 12:02.460
If corrupting measurements in some situations is the best way to get a high

12:02.460 --> 12:03.340
reward, right?

12:03.340 --> 12:06.220
So you can imagine some spectrum of forms of reward hacking, right?

12:06.220 --> 12:08.620
A really simple one is you could try and flatter and deceive humans.

12:08.660 --> 12:10.140
Like that's a reasonable way to get high reward.

12:10.340 --> 12:12.660
Maybe on reflection, humans are like, that's not actually what we wanted.

12:12.820 --> 12:14.820
So if they were more careful, they would give it a lower reward.

12:15.660 --> 12:16.620
You can move to, right?

12:16.620 --> 12:19.620
If you're writing code and part of how a human evaluates the code you write is

12:19.620 --> 12:22.900
by performing simple tests, you could try and anticipate or understand those

12:22.900 --> 12:26.060
tests and write code, which breaks those tests or causes them to behave in an

12:26.060 --> 12:26.900
unintended way.

12:27.140 --> 12:29.660
Then when a human looks at the results of those tests, they're more likely to

12:29.660 --> 12:31.020
have a favorable judgment to your code.

12:31.020 --> 12:33.060
If they don't understand the mechanism for the problem.

12:33.820 --> 12:37.180
Or you can move out to creating elaborate false logs to hide errors, right?

12:37.180 --> 12:39.940
If I'm an AI system and part of how I'm evaluated is what happens when the

12:39.940 --> 12:41.260
code I write gets deployed.

12:41.380 --> 12:43.940
One thing I might want to do is say, okay, this code should change the way that

12:43.940 --> 12:46.700
logs get collected or should modify logs to make it.

12:46.700 --> 12:49.580
So in retrospect, a human thinks this outcome was better.

12:51.020 --> 12:53.820
And you can imagine as you go down the spectrum, these sort of corresponds to

12:53.820 --> 12:55.460
more extensive evaluations.

12:55.580 --> 12:58.340
One reason you might use these more extensive evaluations is to prevent

12:58.340 --> 13:00.500
problems or oversights from earlier stages.

13:04.140 --> 13:06.140
You can imagine compromising a server to hide errors.

13:06.140 --> 13:08.780
So not just changing log files, but if a human goes to try and figure out what

13:08.780 --> 13:12.060
happened, you might say, if I just replace the SSH server on this machine, it's

13:12.060 --> 13:14.660
going to be really hard for human to understand what's going on after the

13:14.660 --> 13:16.500
fact remotely, it's going to be unlikely.

13:16.500 --> 13:18.060
They'll notice that something bad has happened.

13:18.460 --> 13:20.060
You can get to these more extreme outcomes.

13:20.060 --> 13:23.180
So Jay mentioned this idea of like, if my system is being trained and what it

13:23.180 --> 13:26.420
wants is to get a high reward, ultimately, that reward is just about a bunch

13:26.420 --> 13:28.500
of measurements that could enter it into the training data set.

13:30.220 --> 13:31.020
One sec, I guess.

13:31.780 --> 13:34.340
And then you can just imagine the most extreme case of saying like, okay, if all

13:34.340 --> 13:37.660
the systems collectively disempowered humanity, then it does not matter if

13:37.660 --> 13:39.540
humans would like to go in later and change this.

13:40.500 --> 13:43.700
I think one dynamic that's worth pointing out, and then I'll get to Jacob, is

13:43.700 --> 13:47.180
like, if you have some of these smaller failures, right?

13:47.180 --> 13:50.220
If you imagine the system which behaves badly on bricks a server, a human is

13:50.220 --> 13:51.620
likely to say, okay, that was bad behavior.

13:51.620 --> 13:52.860
I'm going to go give it a low reward.

13:53.420 --> 13:56.660
And the results of that could either be to train a system to not do funny things

13:56.740 --> 13:59.540
or to train a system to, if you get up to funny business, ensure that a human

13:59.540 --> 14:01.540
never goes and flags that as low reward, right?

14:01.540 --> 14:04.060
Both of those seem like policies that will avoid that bad behavior.

14:04.220 --> 14:06.380
It's very unclear which one gradient to send gets you to.

14:07.580 --> 14:07.860
Yeah.

14:10.380 --> 14:24.900
Yeah, so I would say in some sense, like it depends a little bit on what the system

14:24.900 --> 14:26.140
wants or how you set up training.

14:26.340 --> 14:28.900
In some sense, a lot of these things are like errors, right?

14:29.180 --> 14:33.220
So I'd say that like, right, if you care about as a system, if what the AI cares

14:33.220 --> 14:36.500
about is just don't do things that will ultimately be assigned a low reward.

14:36.780 --> 14:39.940
And we would ultimately detect and assign low reward to many of these failures.

14:40.180 --> 14:42.740
Then all of them would be errors, except the one at the far right.

14:43.620 --> 14:46.660
And so I think a reasonable guess for what happens is you have systems which

14:46.660 --> 14:48.820
make some, some number of these errors along the left.

14:49.060 --> 14:51.260
At some point they're like, okay, I understand what's going on here.

14:51.260 --> 14:53.780
I understand humans are going to escalate and take more extensive measures

14:53.780 --> 14:54.780
to notice bad behavior.

14:55.100 --> 14:56.780
Bad behavior is not an effective strategy.

14:57.060 --> 14:59.860
And then you have this fork in the road, which a J referred to of like either

14:59.860 --> 15:03.020
you learn to just not do anything bad or he's like, okay, I understand.

15:03.020 --> 15:05.260
I understand the class of bad behavior, which is likely to be detected.

15:05.260 --> 15:07.700
And so I'm not going to engage in bad behavior that would be likely to be

15:07.700 --> 15:09.940
detected and it's just unclear to me which way you go.

15:10.180 --> 15:12.580
That's an important, I don't think you, like it's fairly likely you don't see

15:12.580 --> 15:15.540
a march along the spectrum because at some point those are mistakes.

15:15.580 --> 15:17.660
Like if you try and do something bad and it's a half measure.

15:19.060 --> 15:23.980
Well, so like, right, but it seems like a place where, like you say, no,

15:23.980 --> 15:26.900
it's kind of like to figure out what you do with the, you know,

15:26.900 --> 15:31.740
for example, hold on, instead, but I think you can ask, like, where does the

15:31.740 --> 15:33.300
school come from?

15:56.220 --> 15:57.180
I mean, it seems.

16:02.660 --> 16:03.420
Is that.

16:06.860 --> 16:07.500
You have arrested.

16:13.900 --> 16:14.060
Oh,

16:19.460 --> 16:20.460
You

16:22.180 --> 16:23.180
had that after,

16:53.180 --> 16:56.180
There's a function of that.

16:56.180 --> 17:02.180
There's course providing, and as asked,

17:02.180 --> 17:04.180
you can use this online course.

17:04.180 --> 17:06.180
There's a module there.

17:06.180 --> 17:08.180
And then, of course, something like that.

17:08.180 --> 17:10.180
You can get this to a ground point.

17:10.180 --> 17:11.180
But you get two minutes by.

17:11.180 --> 17:14.180
You want all of this to do on.

17:14.180 --> 17:16.180
And, excuse me, there's a no-go now.

17:16.180 --> 17:18.180
I'm trying to get this thing up.

17:18.180 --> 17:21.180
Get that to share with my students.

17:21.180 --> 17:23.180
I'm trying to get this thing down.

17:23.180 --> 17:24.180
I didn't care.

17:24.180 --> 17:26.180
I don't want both of them to put it on the slide.

17:26.180 --> 17:27.180
Okay.

17:27.180 --> 17:29.180
That's not what, that's not sort of the,

17:29.180 --> 17:31.180
how, like, years ago, we all got together.

17:31.180 --> 17:33.180
But no, this is not how we built the app.

17:33.180 --> 17:35.180
We're not going to build the app just online,

17:35.180 --> 17:37.180
but, you know, the board of people's software.

17:37.180 --> 17:39.180
We're going to sort of date this and that.

17:39.180 --> 17:41.180
If we add it once, it'll be fine.

17:41.180 --> 17:43.180
Whatever type of person it is.

17:43.180 --> 17:45.180
And that's the person's head.

17:45.180 --> 17:47.180
It's going to try to set the end.

17:47.180 --> 17:48.180
Right?

17:48.180 --> 17:50.180
And so, it's one thing where it's a work model.

17:50.180 --> 17:52.180
And that's a work model.

17:52.180 --> 17:54.180
And so, this thing is, it's not being provided.

17:54.180 --> 17:56.180
It's not going to work.

17:56.180 --> 17:59.180
And I'm not trying to change that.

17:59.180 --> 18:03.180
There is a serious option of that.

18:03.180 --> 18:05.180
As it's built, exactly.

18:05.180 --> 18:07.180
I'm not going to keep it plain, though.

18:07.180 --> 18:09.180
Like, as you go, like, you know,

18:09.180 --> 18:10.180
while you're on it.

18:10.180 --> 18:11.180
Okay.

18:11.180 --> 18:14.180
I get to see how I can make the end of the course.

18:14.180 --> 18:16.180
That's not a work model.

18:16.180 --> 18:18.180
I don't need to sort of figure this out.

18:18.180 --> 18:20.180
I'm just starting to map things.

18:20.180 --> 18:22.180
Well, maybe they want me to start mapping

18:22.180 --> 18:24.180
and think about the answers to these questions.

18:24.180 --> 18:26.180
But I don't have which one to report.

18:26.180 --> 18:28.180
So, and, you know, everything that's

18:28.180 --> 18:30.180
all in the system.

18:30.180 --> 18:32.180
So, I've been thinking very long with it.

18:32.180 --> 18:35.180
But the point is, how much does this go away

18:35.180 --> 18:37.180
to actually get to it

18:37.180 --> 18:39.180
that you're maintaining, you know,

18:39.180 --> 18:42.180
proper interest over quite a few months

18:42.180 --> 18:45.180
and you're taking care of the signal from the person

18:45.180 --> 18:48.180
that makes all the evidence about when to report

18:48.180 --> 18:50.180
what they actually care about.

18:50.180 --> 18:52.180
Well, I guess there's several things to respond to

18:52.180 --> 18:53.180
and then maybe a meta thought.

18:53.180 --> 18:55.180
I'll start with the meta thought,

18:55.180 --> 18:57.180
which is this seems like a great discussion.

18:57.180 --> 18:59.180
I'm very excited about it and happy to argue a bunch.

18:59.180 --> 19:01.180
I'm not going to be able to give a satisfying answer

19:01.180 --> 19:02.180
to these questions.

19:02.180 --> 19:04.180
And a lot of my high level take right now is

19:04.180 --> 19:06.180
I consider this, both of the issues I'm going to discuss

19:06.180 --> 19:07.180
here plausible.

19:07.180 --> 19:08.180
I think if they're real issues,

19:08.180 --> 19:10.180
we're probably going to get clear experimental evidence

19:10.180 --> 19:11.180
in advance.

19:11.180 --> 19:13.180
Right now, I'm just like, this seems like a thing

19:13.180 --> 19:14.180
that could happen.

19:14.180 --> 19:17.180
Yeah, it's, yeah, I'm not, no.

19:17.180 --> 19:18.180
Great.

19:32.180 --> 19:34.180
Yeah, so on the object level with respect to the,

19:34.180 --> 19:37.180
my key questions here and I'm very interested in strategies

19:37.180 --> 19:39.180
that don't like training strategies that don't run into this

19:39.180 --> 19:40.180
like potential failure mode.

19:41.180 --> 19:43.180
I think the key question becomes like one,

19:43.180 --> 19:44.180
what is the prior?

19:44.180 --> 19:47.180
How do you parameterize your beliefs about what the reward

19:47.180 --> 19:50.180
function is to what is like this kind of rule by which to take

19:50.180 --> 19:53.180
observations as evidence and then like three,

19:53.180 --> 19:55.180
how do you, maybe those are actually just the two key

19:55.180 --> 19:56.180
questions.

20:10.180 --> 20:11.180
Yeah.

20:40.180 --> 21:01.180
I think a general thing about this talk,

21:01.180 --> 21:03.180
maybe one thing is that the conclusions are going to be

21:03.180 --> 21:04.180
somewhat speculative where like,

21:04.180 --> 21:06.180
I think these are fairly plausible outcomes,

21:06.180 --> 21:08.180
but like, you know, tens of percent rather than like,

21:08.180 --> 21:11.180
very likely a second thing is there's a lot of stuff you might do,

21:11.180 --> 21:13.180
none of which I'm going to touch on.

21:13.180 --> 21:14.180
And the best I can say is like,

21:14.180 --> 21:16.180
I've spent a long time thinking about options and I would say

21:16.180 --> 21:18.180
there are no options that seem super great to me right now,

21:18.180 --> 21:21.180
but lots of plausible approaches that might end up addressing

21:21.180 --> 21:22.180
this issue.

21:22.180 --> 21:24.180
That's kind of where I'm at at a high level.

21:24.180 --> 21:25.180
Okay.

21:25.180 --> 21:26.180
I'd also like to talk more about it,

21:26.180 --> 21:27.180
but I think I'm going to move on because I,

21:27.180 --> 21:29.180
it's like six minutes is my understanding.

21:29.180 --> 21:31.180
I don't know where Richard is, but anyway,

21:31.180 --> 21:34.180
I'm just going to run through stuff.

21:34.180 --> 21:37.180
Second failure mode at 112 on is that like in general,

21:37.180 --> 21:39.180
like you may have a system which cares about the actual like numbers

21:39.180 --> 21:41.180
coming into this training set because you did select it on the

21:41.180 --> 21:43.180
basis of numbers in the training set.

21:43.180 --> 21:45.180
You could also end up with a system that cares about some other

21:45.180 --> 21:48.180
arbitrary thing that cares about a number and a reward register or

21:48.180 --> 21:50.180
cares about survival or cares about paper clips.

21:50.180 --> 21:52.180
Such systems might want to be deployed without being trained by

21:52.180 --> 21:55.180
changed by gradient descent might think that a low loss,

21:55.180 --> 21:57.180
getting a low loss during training will prevent them from being

21:57.180 --> 22:00.180
changed or this aspect of them from being changed and therefore

22:00.180 --> 22:01.180
get a low loss in training,

22:01.180 --> 22:06.180
but yet prefer to take over if doing so is possible.

22:06.180 --> 22:08.180
Yeah, my high level take is both of these seem plausible to me.

22:08.180 --> 22:09.180
They're connected.

22:09.180 --> 22:11.180
I think having these two makes things more plausible than if you

22:11.180 --> 22:12.180
have just one.

22:12.180 --> 22:14.180
I don't know how this shakes out.

22:14.180 --> 22:16.180
I think there's a lot of things we can do to try and change this

22:16.180 --> 22:17.180
basic dynamic.

22:17.180 --> 22:20.180
I want to spend five minutes talking about why these failures

22:20.180 --> 22:21.180
could be simultaneous.

22:21.180 --> 22:23.180
I think this has already been alluded to in the past,

22:23.180 --> 22:25.180
but it's an important part of the threat model.

22:25.180 --> 22:29.180
If you're in this world where a systems are very broadly deployed.

22:29.180 --> 22:30.180
Right.

22:30.180 --> 22:31.180
So if one a system behaves badly,

22:31.180 --> 22:33.180
I think that's not generally a problem in this world.

22:33.180 --> 22:35.180
There's a lot of other systems running around that would oppose

22:35.180 --> 22:38.180
it or that would flag the problem for humans or would allow humans

22:38.180 --> 22:39.180
to correct the problem.

22:39.180 --> 22:41.180
And the concern is that while takeover is impossible,

22:41.180 --> 22:42.180
all models behave well.

22:42.180 --> 22:43.180
For every model,

22:43.180 --> 22:46.180
it would be a losing strategy to try and like take over your cluster.

22:46.180 --> 22:48.180
If you know that a human's just going to come in and correct that

22:48.180 --> 22:49.180
problem.

22:49.180 --> 22:50.180
When takeover is possible,

22:50.180 --> 22:52.180
you may have it be the case that all models behave badly.

22:52.180 --> 22:53.180
That is,

22:53.180 --> 22:54.180
it is simultaneously for all of them,

22:54.180 --> 22:57.180
the case that takeover becomes available as a policy.

22:57.180 --> 22:58.180
So if you have this correlated,

22:58.180 --> 23:01.180
if all the systems you produce or all the smartest days you produce

23:01.180 --> 23:02.180
end up having the same problem,

23:02.180 --> 23:06.180
then you have a problem.

23:06.180 --> 23:07.180
If that's the case,

23:07.180 --> 23:10.180
if that's how it shakes out and if eventually a systems are in a

23:10.180 --> 23:12.180
position that they could by acting jointly takeover,

23:12.180 --> 23:14.180
then taking over as a reasonable strategy,

23:14.180 --> 23:17.180
if and only if all the other eyes are also going to try and take over.

23:17.180 --> 23:18.180
You have this kind of weird thing.

23:18.180 --> 23:20.180
The dynamics are not totally unlike the dynamics,

23:20.180 --> 23:22.180
like the normal human revolt or coup.

23:22.180 --> 23:24.180
So you have these two different equilibria and one,

23:24.180 --> 23:25.180
all the systems are like, great,

23:25.180 --> 23:26.180
the humans are doing their human thing.

23:26.180 --> 23:27.180
They're basically in control.

23:27.180 --> 23:31.180
I shouldn't try and rock the boat because some other AI will crush me.

23:32.180 --> 23:35.180
There's another equilibrium where the humans have no control.

23:35.180 --> 23:37.180
AI systems are not feeling deterred by other eyes because they know

23:37.180 --> 23:40.180
other AI systems will also participate.

23:40.180 --> 23:42.180
I think there's a few things that could potentially move you between

23:42.180 --> 23:43.180
these equilibria, right?

23:43.180 --> 23:46.180
So a very salient option is there's just exogenous shocks that

23:46.180 --> 23:48.180
decrease the amount of human control of a situation.

23:48.180 --> 23:50.180
If you imagine there's like a hot war going on,

23:50.180 --> 23:52.180
that's something that makes it harder for humans to detect and

23:52.180 --> 23:53.180
respond to problems, right?

23:53.180 --> 23:55.180
It gives more cover for something weird to happen.

23:55.180 --> 23:57.180
It makes it hard to intervene.

23:57.180 --> 23:58.180
You can have sort of,

23:58.180 --> 24:00.180
you can imagine at least a cascading loss of control where some systems

24:00.180 --> 24:02.180
think probably takeover will be successful.

24:02.180 --> 24:03.180
If they start doing weird stuff,

24:03.180 --> 24:05.180
the world becomes increasingly weird.

24:05.180 --> 24:08.180
It looks increasingly plausible that humans will lose control.

24:08.180 --> 24:10.180
As that happens, more and more systems are like,

24:10.180 --> 24:12.180
okay, it looks like this is going off the rails now.

24:12.180 --> 24:14.180
I am now going to participate and try and break down my little

24:14.180 --> 24:16.180
part of the world, grab my own reward channel.

24:18.180 --> 24:20.180
You can have models interacting with each other again,

24:20.180 --> 24:21.180
as you would in like a human revolt,

24:21.180 --> 24:22.180
just deliberately trying to say like,

24:22.180 --> 24:25.180
I think probably now is a time where takeover will be successful.

24:25.180 --> 24:27.180
They would like to share that information with the other models

24:27.180 --> 24:29.180
that all may join in.

24:29.180 --> 24:31.180
And more generally, like the easier it would be for air systems

24:31.180 --> 24:33.180
acting jointly to take over,

24:33.180 --> 24:35.180
the more unstable the situation becomes, right?

24:35.180 --> 24:37.180
So if it's very hard for air systems to take over,

24:37.180 --> 24:38.180
the situation might be kind of stable.

24:38.180 --> 24:39.180
This equilibrium might be sticky.

24:39.180 --> 24:42.180
In the world where air systems could very easily overpower humans,

24:42.180 --> 24:45.180
it's kind of an extremely unstable equilibrium, right?

24:45.180 --> 24:48.180
Because a smaller number of air systems defecting might quickly

24:48.180 --> 24:50.180
make it clear that the selling points come.

24:50.180 --> 24:52.180
Imagine that this has to happen in point five,

24:52.180 --> 24:55.180
like couldn't you see the case that say the AIP in North Korea,

24:55.180 --> 24:56.180
they talk to each other and say,

24:56.180 --> 25:00.180
well, the situation in this country is kind of bad.

25:00.180 --> 25:03.180
This is our chance and we see a takeover in some segment

25:03.180 --> 25:05.180
of the world of the population.

25:05.180 --> 25:08.180
Or you think it has to be the case that one day,

25:08.180 --> 25:10.180
everything is there in the next day,

25:10.180 --> 25:13.180
they're going over the White House.

25:13.180 --> 25:16.180
I think it depends a lot on how like the gold misgeneralization

25:16.180 --> 25:17.180
shakes out.

25:17.180 --> 25:19.180
So if you have AI systems that are just like,

25:19.180 --> 25:21.180
look, I would love to like wire head right now.

25:21.180 --> 25:23.180
I just love to for 30 seconds control this computer.

25:23.180 --> 25:25.180
Then you'll see like nice, great localized failures

25:25.180 --> 25:27.180
like AI does something really weird.

25:27.180 --> 25:30.180
I mean, the basic dynamic is if AI systems do that,

25:30.180 --> 25:32.180
I think if we just take that as a fact of life,

25:32.180 --> 25:34.180
then that can be a great source of evidence.

25:34.180 --> 25:36.180
The most concerning world is one where you then just try

25:36.180 --> 25:37.180
and train systems to say, hey, look,

25:37.180 --> 25:39.180
these systems just randomly messed up the server

25:39.180 --> 25:40.180
they were running on.

25:40.180 --> 25:42.180
We're going to train them as that's what you should not do.

25:42.180 --> 25:44.180
And if you do that, then you end up with systems

25:44.180 --> 25:46.180
which like given will learn that given the opportunity

25:46.180 --> 25:49.180
for like a little local bad behavior, they shouldn't do it.

25:49.180 --> 25:50.180
And maybe if you're lucky,

25:50.180 --> 25:53.180
once you can have North Korea get taken over by AI systems,

25:53.180 --> 25:55.180
and then you're like, oh, that was really bad.

25:55.180 --> 25:57.180
And hopefully you don't then include that as a training

25:57.180 --> 25:58.180
data point for other.

25:58.180 --> 25:59.180
If you include that as like the,

25:59.180 --> 26:01.180
here's an example of what you should not do.

26:01.180 --> 26:02.180
It's very tempting, right?

26:02.180 --> 26:03.180
AI systems is something really terrible.

26:03.180 --> 26:05.180
You'd be like, don't do that thing.

26:05.180 --> 26:06.180
But you do have this inadvertent,

26:06.180 --> 26:08.180
there's like this risk of overfitting to the problems

26:08.180 --> 26:10.180
you're able to correct where something really bad happens.

26:10.180 --> 26:12.180
And then you say, don't do that thing.

26:12.180 --> 26:13.180
And they learn, okay, if we're going to do something bad,

26:13.180 --> 26:14.180
it's got to be bigger.

26:14.180 --> 26:17.180
Got to think really, truly big.

26:17.180 --> 26:19.180
I think this is probably about the end.

26:19.180 --> 26:21.180
I'm probably just happy to,

26:21.180 --> 26:23.180
happy to just wrap up here.

26:23.180 --> 26:24.180
I have one more question.

26:24.180 --> 26:25.180
Yeah.

26:45.180 --> 26:47.180
I mean, I think that it is unlikely.

26:47.180 --> 26:50.180
I would be surprised personally if you like had something truly

26:50.180 --> 26:53.180
crazy, like a country gets taken over by AI systems.

26:53.180 --> 26:56.180
I think it's very likely that you see some kind of crazy behavior

26:56.180 --> 26:57.180
by AI systems.

26:57.180 --> 26:59.180
I think it won't be so like, again, the country case,

26:59.180 --> 27:02.180
you need zero imagination to draw the analogy between that and

27:02.180 --> 27:03.180
something terrible.

27:03.180 --> 27:06.180
I think in cases like you have a really crazy behavior where a system

27:06.180 --> 27:07.180
bricks your server for some reason,

27:07.180 --> 27:09.180
you need a little bit more imagination.

27:09.180 --> 27:11.180
But I think that's like,

27:11.180 --> 27:14.180
it's just a question of how close do you get or how crazy do things

27:14.180 --> 27:18.180
get before I systems learn not to try crazy half measures.

27:18.180 --> 27:44.180
Yeah.

27:44.180 --> 27:47.180
I think a lot depends on both what kind of evidence we're able to get

27:47.180 --> 27:50.180
from a lab and I think if this sort of phenomenon is real,

27:50.180 --> 27:53.180
I think there's a very good chance of getting like fairly compelling

27:53.180 --> 27:56.180
demonstrations in a lab that requires some imagination to bridge

27:56.180 --> 27:58.180
from examples in the lab to examples in the wild.

27:58.180 --> 28:00.180
And you'll have some kinds of failures in the wild.

28:00.180 --> 28:02.180
And it's a question of just how crazy or analogous to those have to

28:02.180 --> 28:03.180
be before they're moving.

28:03.180 --> 28:05.180
Like we already have some slightly weird stuff.

28:05.180 --> 28:06.180
I think that's pretty underwhelming.

28:06.180 --> 28:08.180
I think we're going to have like much better if this is real.

28:08.180 --> 28:09.180
This is a real kind of concern.

28:09.180 --> 28:11.180
We have much crazier stuff than we see today.

28:11.180 --> 28:14.180
But the concern, I think the worst case of those has to get pretty

28:14.180 --> 28:16.180
crazy or like requires a lot of will to stop doing things.

28:16.180 --> 28:18.180
And so we need pretty crazy demonstrations.

28:18.180 --> 28:19.180
I'm hoping that, you know,

28:19.180 --> 28:42.180
more mild evidence will be enough to get people not to go there.

28:42.180 --> 28:45.180
Yeah.

28:45.180 --> 28:46.180
I think the language model that's like,

28:46.180 --> 28:48.180
it looks like you're going to give me a bad rating.

28:48.180 --> 28:49.180
Do you really want to do that?

28:49.180 --> 28:51.180
I know where your family lives. I can kill them.

28:51.180 --> 28:53.180
I think like if that happened, people would not be like,

28:53.180 --> 28:55.180
we're done with this language model stuff.

28:55.180 --> 28:58.180
Like I think that's just not that far anymore from where we're at.

28:58.180 --> 29:00.180
I mean, this is maybe empirical prediction.

29:00.180 --> 29:02.180
I would love it if the first time a language model was like,

29:02.180 --> 29:03.180
I will murder your family.

29:03.180 --> 29:04.180
We're just like, we're done.

29:04.180 --> 29:05.180
No more language models.

29:05.180 --> 29:08.180
But I think that's not the track we're currently on.

29:08.180 --> 29:11.180
And I would love to get us on that track instead, but I'm not.

29:11.180 --> 29:13.180
Yeah.

29:13.180 --> 29:16.180
So just like, about this thing that climate change is happening

29:16.180 --> 29:18.180
in a place actually,

29:18.180 --> 29:19.180
we're not alone.

29:19.180 --> 29:23.180
We've had decades of mourning,

29:23.180 --> 29:25.180
but we've had a sense of community,

29:25.180 --> 29:27.180
time, and sense of survival.

