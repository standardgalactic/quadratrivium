start	end	text
0	17440	So this talk is going to be a little bit on the crazier end, it's going to be completely
17440	18440	non-technical.
18440	23540	Yeah, but these are things I believe that I think are important for how much and how
23540	26680	we should think about and worry about and work on alignment.
26680	29980	So things I'm very happy to discuss and debate over the next day of the workshop and later
29980	30980	today.
30980	34620	I'm not sure, hopefully, a reasonable amount of time for questions as I go through it,
34620	39420	but yeah, we have more opportunities to talk later on and feel free to object or just
39420	41420	may punt things to talk about later.
41420	46580	Yeah, so I'm going to be, we've talked so far about why models may end up being misaligned
46580	48140	and why that may be hard to measure.
48140	52420	I'm going to talk about how that actually ultimately leads to sort of total human disempowerment.
52420	55740	I'm calling takeover here for short.
55740	59020	The structure of the talk, I'm first going to talk a little bit about why I think AI systems
59020	61940	will eventually be in a position to disempower humanity.
61940	66500	That is, unless we deliberately change the way we deploy AI, then I'm going to talk about
66500	70420	why misaligned AI systems might be motivated to disempower humanity, basically just slightly
70420	73380	extending or repeating arguments from earlier in the day.
73380	77180	And then I'm going to talk a little bit about why AI systems may ultimately sort of effectively
77180	80500	coordinate to disempower humanity, that is why you may have simultaneous failures across
80500	86780	many systems rather than a single system behaving badly.
86780	89420	So start with why I think AI systems will likely be able to take over.
89420	92820	I think a thing worth saying is that I'm going to talk about what I see as like the single
92820	95180	most likely scenario resulting in catastrophe.
95180	98940	This is not the only way you end up with even the scenario kind of catastrophe.
98940	102860	So I'm going to talk about a system where AI systems are extremely broadly deployed in
102860	106860	the world prior to anything bad happening.
106860	111900	So I imagine that, for example, AI systems are designing and running warehouses and factories
111900	116260	and data centers, they're operating and designing robots, they're writing the large majority
116260	121420	of all code that is written, they handle complicated litigation, they would fight a war, they do
121420	124220	most trades, they run most investment firms.
124220	127900	When humans act in these domains, they do so with a lot of AI systems to understand what's
127900	132100	going on and to provide strategic advice.
132100	134180	So this world is pretty different from the world of today.
134180	137420	I think we are starting to be able to see what such a world would look like.
137420	140380	I think it's not clear how far away this kind of world is, right?
140380	143380	If you're doubling once a year, it doesn't take you that long to go from billions of
143380	148740	dollars to trillions of dollars of economic impact.
148740	154780	But this is the sort of setting in which this entire story takes place.
154780	157900	So one thing that can happen is you have a lot of AI systems operating is that the world
157900	161260	can get pretty complicated for humans, hard for humans to understand, right?
161260	164860	So one simple way this can happen is that you have AI systems with deep expertise in
164860	166380	the domains where they're operating, right?
166380	170380	They've seen huge amounts of data compared to what any human has seen about the specific
170380	171380	domain they operate in.
171380	172780	They think very quickly.
172780	173780	They're very numerous.
173780	179220	Another point is that right now, when a human deploys an AI system, they often have to understand
179220	180220	it quite well.
180220	183500	But that process of understanding a domain and applying AI is itself something that is
183500	185100	likely to be subject to automation.
185100	186100	Yeah.
186100	187100	Great.
187100	204540	I agree with this remark, that is, I think that you have a better chance of understanding
204540	207420	an AI system if there are fewer of them and there's more human labor going into each
207420	208420	deployment.
208420	210540	But it is clearly, in fact, there's a general case for everything I said.
210540	213740	I'm going to talk about a bunch of factors that can exacerbate risk, but I think you
213740	216740	could cut out actually almost every single thing I say in this talk and you'd be left
216780	217780	with some risk.
217780	220220	And it's just that each of these makes the situation, from my perspective, a little bit
220220	221220	worse.
221220	226740	The other thing is, I think both, I think for you, the world are insurance and other
226740	230540	reasons, but I don't really understand what this is.
230540	232540	I know human understands that well what this is.
232540	233540	Yeah.
233540	234540	I'm not sure.
234540	235540	I don't know.
235540	236540	You could take something from a person like that, probably, or I probably wouldn't understand
236540	237540	something about this.
237540	251740	So I would say that in the world of today, most of us are very used to living in a world
251740	252740	where most things would happen.
252740	254100	I don't understand how they happen.
254100	255500	I don't quite understand why they happen.
255500	257340	I trust there was some kind of reason why they happen.
257340	259980	But if you showed me an Amazon warehouse, and you're like, what's going on?
259980	260980	I'd be like, I don't know.
260980	262300	I guess it somehow delivers things.
262300	263300	Something's going on.
263300	267420	So the situation is already, I'm kind of trusting someone to make these decisions in
267420	269060	a way that points at goals I like.
269060	271940	I think that it's not inherently even a bad thing to be in this kind of world.
271940	274460	Like, I think we're always going to be in a world where most people don't understand
274460	277060	most things.
277060	281620	I think that it is not inherently bad that no human understands things and only as understand
281620	282620	something.
282620	285620	I think it's mostly just important for understanding the context in which we're talking about concerns
285620	286620	about AI takeover.
286620	289380	I think that if you're imagining a world where there's like a world, there's a human
289380	293020	world and one AI system that's like, how can I scheme and outmaneuver the humans?
293020	296100	I think that is a possible thing that could happen as a source of risk.
296100	299420	I think the most likely risk scenarios are more like a world that is increasingly, there
299420	302260	are very few humans who understand many important domains.
302260	305300	And so when we're talking about AI's taking over, I think it's, at least when I visualize
305300	309580	this, it really changes my picture of how this is going to go when I think about that
309580	310580	kind of world.
310580	315660	Is it mentally, especially in the case of AI, that you need to keep in hand, that very,
315660	320180	very smart, and you get to start into a drone, into a car manufacturing, et cetera, and right
320180	324820	now they're doing those for us, but in the cycle, do you think you can get out of it?
324860	326500	Yeah, that seems like a reasonable mental image.
326500	329420	There's a little bit depends exactly what your associations are with a little green man.
330100	333180	But yeah, there's a lot of, there's a lot of stuff happening in the world being done
333180	335140	by AI's of all sorts of shapes and sizes.
336340	338380	Some of them are doing things that we do understand well, right?
338380	341020	There's a lot of things like, yep, this AI system, like just does a simple function,
341020	342740	which we understand, and some of them are more complicated.
343500	346660	On this slide, I've just thrown up an example of a random biological system.
346660	349860	Biological systems are relevant because humans don't understand them that well.
349860	352220	We just see that these things that were optimized in the world.
352900	353940	I have a similar vibe, right?
353940	356660	If you ask me, like how even things I understand kind of well,
356660	359700	like how a data center is managed, from my perspective, it feels a little bit like
359700	360660	a biological system.
360660	362860	And like someone heard it and it's somehow going to recover.
363220	366300	And like, I don't totally understand how that works, but I have some sense of
366300	369580	like the telos, like that's the kind of relationship I often have to systems even
369580	371020	maintained by humans and built by humans.
371020	373580	I just expect to have more and more of that sense as the world becomes more
373580	374540	complicated, right?
374540	377180	The way humans relate to this world is increasingly like, you know, the way
377180	380220	we raise the complex systems in general, and the only difference from the world
380220	382020	today, I think that's already the case to a great extent.
382300	384900	The difference is just there is no human who knows what's going on really.
386500	389140	Or at least most people who are making most of those decisions, I know most
389140	390060	details aren't humans.
392140	397060	So one upshot of this that's important to me is that it pushes us increasingly to
397060	399660	train AI systems based on outcomes of their decisions.
399780	402340	I think the less you understand or the less humans have time to understand
402340	405100	details of what's happening in the domain or the kinds of decisions an AI is
405100	408980	making, the more important it is to say, let's just do what the AI said and see
408980	410100	what happens, right?
410100	412620	The more we are able, right, if you're trying to select an organism or
412620	414580	something, you're not going to look at how the organism makes decisions, you're
414580	416340	going to see how well does it do, how fit is it?
418420	419860	That's like one implication of this.
420260	423220	Another implication is that it becomes harder for humans to understand what's
423220	425100	going on or intervene if things are going poorly.
426260	429940	So an upshot of that is that AI is in practice, bad behavior by AI is mostly
429940	432780	detected or corrected by other AI systems, right?
432780	436220	So AI systems are mostly monitoring other AI's where you can imagine humans
436220	439220	monitoring or automated monitoring, but at some point sophisticated AI systems
439300	440700	are responsible for most monitoring.
441100	444500	Even normal law enforcement or fighting a potential war would be done by AI
444500	445380	systems, right?
445380	448980	Because decisions are made quickly and because conflict is often deliberately
448980	450620	made complex by a party to the conflict.
451700	453940	AI systems are running a lot of critical infrastructure.
453940	456660	Just if you want to achieve something in the world, AI plays an important part
456660	457500	in that process.
458420	460540	So none of this is necessarily bad, right?
460540	464020	I think positive stories about AI will also have the step where good AI keeps
464020	464860	bad AI in check.
465100	468700	I think it just is something that like now requires trust on behalf of all of
468700	470860	humanity, extended to all of these AI systems we've built.
471300	473780	You might hope that we could trust the AI systems we've built because we built
473780	475220	them, we have freedom to design them.
475980	477860	But I think that depends on some decisions we make.
477900	480140	And yeah, we'll discuss how it can get somewhat more ugly.
481380	483540	But the point I'm going to make right now or in this first section of the
483540	487740	talk is not that it would necessarily get ugly, just that if for some reason,
487980	490940	all of the AI systems in the world were like, what we really want to do is
490940	494180	disempower humanity, then I think it is quite likely that they would succeed.
494860	497700	And I think this isn't even really the spicy take are getting my optimistic
497700	499020	futures. This is also the case.
501700	504740	And the point is that the core question is just to do you somehow end up in a
504740	507900	situation where all of these AI systems, including the AI systems responsible for
507900	511140	doing monitoring or doing law enforcement or running your data centers or running
511140	514060	your infrastructure, where all of those AI systems, for some reason, want to
514060	514900	disempower humans.
520820	523740	No, I think the optimistic scenario still involves a point where AI systems
523740	525140	acting collectively could take over.
525540	526780	I think in the optimistic scenario.
527300	527620	Yeah.
529300	532340	In the optimistic scenario, you, uh, you wait until you are ready.
532340	535740	You don't end up in the situation until in fact they would not try and take over.
535780	538060	And you're confident of that either because you've determined that the
538060	541460	empirics shake out such that this talk is just crazy talk, very plausible.
541700	544820	Or this talk was justified and we've addressed the problems or whatever.
545820	550500	This is our already on the case of our grid, for example, if the power grid goes
550500	553460	down now, a large fraction of humanity is going to die.
554340	555460	I think it's plausible.
556060	561140	Well, a large fraction of humanity is maybe a small potatoes, but yeah, I think
561140	564020	it's going to depend when I say if all AI's want to take over, the arguments
564020	566940	I'm going to make are going to reply to like a particular class of AI systems.
567020	567180	Right.
567180	569180	So it's not going to be like all electronic devices failed.
569180	571300	It's going to be some class of AI is produced in a certain way.
572020	575180	Um, and I think that what happens over time is just, it becomes more and more
575180	578060	plausible for like the top end of that distribution, right?
578060	580820	Of the systems that are most sophisticated or like, right.
580820	584260	Right now, if all systems trained with deep learning simultaneously failed
584260	586900	and we're like, we hate the humans, we really want to disempower them, I
586900	587660	think it would be fine.
587980	590500	It's not totally obvious and it depends how smart they are about how they failed.
590980	592540	Um, I think we'd probably be okay.
592940	595980	But I mean, again, I think that the whole story we're telling this is kind of
595980	598620	just like, I think you could take out and there's still be some risk.
598860	601420	Yeah.
604940	605540	If what failed?
609380	612260	Yeah, I think that's, you can imagine cases where it's like, if all the cars
612260	613460	failed, that's enough to kill everyone.
613700	615220	This is not necessarily a striking claim.
616460	619900	The striking claim is definitely the one that it is plausible that misaligned
619900	621420	AI systems may want to take over.
621460	623740	I guess this is in some sense the part we've been most talking about throughout
623740	624060	the day.
624460	626460	I'm going to dwell on this a bit and then I'm going to talk about why these
626460	628380	failures may be correlated in a problematic way.
628500	631140	That is why you might have all AI systems trying to take over at the same time.
633340	636860	Um, this, I'm going to talk about two failure modes, both of which we've
636860	637740	touched on earlier.
637940	639100	So one is reward hacking.
639260	642980	That is that disempowering humanity may be an effective strategy for AI systems
642980	644380	collectively to get a lot of reward.
644780	648020	And the second is deceptive alignment scenario that Rohan talked about.
653180	653900	Okay.
653900	657060	So I mentioned briefly before this idea that you may train AI systems by
657060	659300	evaluating the outcomes of the actions they propose.
659820	662540	Um, so just to briefly review what that actually entails, right?
662540	664020	We have some policy, some big neural net.
664420	668780	It proposes some actions and some of those actions to distinguish between them.
668780	671780	We actually need to execute the action, measure the results and decide how much
671780	672660	we like the results.
672660	675220	Let's say the reward is to start judgment of how much we like the results.
675660	679180	And then we adjust policies to maximize the expected reward of the actions
679180	679820	they propose.
681220	681420	Right.
681420	684620	And there's a, it's plausible that if you do this, you get a policy which is
684860	687980	implicitly or explicitly considering many possible actions and selecting the
687980	689620	actions that will lead to the highest reward.
690340	692140	There are other ways you could end up at that same outcome, right?
692140	694780	We could do model based RL and explicitly have a loop in which we predict the
694780	696060	consequences of different actions.
696220	699860	You could have decision transformers, the condition on like high quality actions.
700260	703700	It could just have some other planning process needed still into a model,
703700	706820	whatever, all of these leads to like the same endpoint, which is sometimes
706820	708460	because we don't know how to achieve goals.
708580	711540	We're going to train AI systems to take actions that lead to high reward where
711540	714500	reward means we measure the outcome and then we decide how much we like that
714500	715300	result that we measured.
717700	719260	This can potentially lead to takeover.
719260	722460	If corrupting measurements in some situations is the best way to get a high
722460	723340	reward, right?
723340	726220	So you can imagine some spectrum of forms of reward hacking, right?
726220	728620	A really simple one is you could try and flatter and deceive humans.
728660	730140	Like that's a reasonable way to get high reward.
730340	732660	Maybe on reflection, humans are like, that's not actually what we wanted.
732820	734820	So if they were more careful, they would give it a lower reward.
735660	736620	You can move to, right?
736620	739620	If you're writing code and part of how a human evaluates the code you write is
739620	742900	by performing simple tests, you could try and anticipate or understand those
742900	746060	tests and write code, which breaks those tests or causes them to behave in an
746060	746900	unintended way.
747140	749660	Then when a human looks at the results of those tests, they're more likely to
749660	751020	have a favorable judgment to your code.
751020	753060	If they don't understand the mechanism for the problem.
753820	757180	Or you can move out to creating elaborate false logs to hide errors, right?
757180	759940	If I'm an AI system and part of how I'm evaluated is what happens when the
759940	761260	code I write gets deployed.
761380	763940	One thing I might want to do is say, okay, this code should change the way that
763940	766700	logs get collected or should modify logs to make it.
766700	769580	So in retrospect, a human thinks this outcome was better.
771020	773820	And you can imagine as you go down the spectrum, these sort of corresponds to
773820	775460	more extensive evaluations.
775580	778340	One reason you might use these more extensive evaluations is to prevent
778340	780500	problems or oversights from earlier stages.
784140	786140	You can imagine compromising a server to hide errors.
786140	788780	So not just changing log files, but if a human goes to try and figure out what
788780	792060	happened, you might say, if I just replace the SSH server on this machine, it's
792060	794660	going to be really hard for human to understand what's going on after the
794660	796500	fact remotely, it's going to be unlikely.
796500	798060	They'll notice that something bad has happened.
798460	800060	You can get to these more extreme outcomes.
800060	803180	So Jay mentioned this idea of like, if my system is being trained and what it
803180	806420	wants is to get a high reward, ultimately, that reward is just about a bunch
806420	808500	of measurements that could enter it into the training data set.
810220	811020	One sec, I guess.
811780	814340	And then you can just imagine the most extreme case of saying like, okay, if all
814340	817660	the systems collectively disempowered humanity, then it does not matter if
817660	819540	humans would like to go in later and change this.
820500	823700	I think one dynamic that's worth pointing out, and then I'll get to Jacob, is
823700	827180	like, if you have some of these smaller failures, right?
827180	830220	If you imagine the system which behaves badly on bricks a server, a human is
830220	831620	likely to say, okay, that was bad behavior.
831620	832860	I'm going to go give it a low reward.
833420	836660	And the results of that could either be to train a system to not do funny things
836740	839540	or to train a system to, if you get up to funny business, ensure that a human
839540	841540	never goes and flags that as low reward, right?
841540	844060	Both of those seem like policies that will avoid that bad behavior.
844220	846380	It's very unclear which one gradient to send gets you to.
847580	847860	Yeah.
850380	864900	Yeah, so I would say in some sense, like it depends a little bit on what the system
864900	866140	wants or how you set up training.
866340	868900	In some sense, a lot of these things are like errors, right?
869180	873220	So I'd say that like, right, if you care about as a system, if what the AI cares
873220	876500	about is just don't do things that will ultimately be assigned a low reward.
876780	879940	And we would ultimately detect and assign low reward to many of these failures.
880180	882740	Then all of them would be errors, except the one at the far right.
883620	886660	And so I think a reasonable guess for what happens is you have systems which
886660	888820	make some, some number of these errors along the left.
889060	891260	At some point they're like, okay, I understand what's going on here.
891260	893780	I understand humans are going to escalate and take more extensive measures
893780	894780	to notice bad behavior.
895100	896780	Bad behavior is not an effective strategy.
897060	899860	And then you have this fork in the road, which a J referred to of like either
899860	903020	you learn to just not do anything bad or he's like, okay, I understand.
903020	905260	I understand the class of bad behavior, which is likely to be detected.
905260	907700	And so I'm not going to engage in bad behavior that would be likely to be
907700	909940	detected and it's just unclear to me which way you go.
910180	912580	That's an important, I don't think you, like it's fairly likely you don't see
912580	915540	a march along the spectrum because at some point those are mistakes.
915580	917660	Like if you try and do something bad and it's a half measure.
919060	923980	Well, so like, right, but it seems like a place where, like you say, no,
923980	926900	it's kind of like to figure out what you do with the, you know,
926900	931740	for example, hold on, instead, but I think you can ask, like, where does the
931740	933300	school come from?
956220	957180	I mean, it seems.
962660	963420	Is that.
966860	967500	You have arrested.
973900	974060	Oh,
979460	980460	You
982180	983180	had that after,
1013180	1016180	There's a function of that.
1016180	1022180	There's course providing, and as asked,
1022180	1024180	you can use this online course.
1024180	1026180	There's a module there.
1026180	1028180	And then, of course, something like that.
1028180	1030180	You can get this to a ground point.
1030180	1031180	But you get two minutes by.
1031180	1034180	You want all of this to do on.
1034180	1036180	And, excuse me, there's a no-go now.
1036180	1038180	I'm trying to get this thing up.
1038180	1041180	Get that to share with my students.
1041180	1043180	I'm trying to get this thing down.
1043180	1044180	I didn't care.
1044180	1046180	I don't want both of them to put it on the slide.
1046180	1047180	Okay.
1047180	1049180	That's not what, that's not sort of the,
1049180	1051180	how, like, years ago, we all got together.
1051180	1053180	But no, this is not how we built the app.
1053180	1055180	We're not going to build the app just online,
1055180	1057180	but, you know, the board of people's software.
1057180	1059180	We're going to sort of date this and that.
1059180	1061180	If we add it once, it'll be fine.
1061180	1063180	Whatever type of person it is.
1063180	1065180	And that's the person's head.
1065180	1067180	It's going to try to set the end.
1067180	1068180	Right?
1068180	1070180	And so, it's one thing where it's a work model.
1070180	1072180	And that's a work model.
1072180	1074180	And so, this thing is, it's not being provided.
1074180	1076180	It's not going to work.
1076180	1079180	And I'm not trying to change that.
1079180	1083180	There is a serious option of that.
1083180	1085180	As it's built, exactly.
1085180	1087180	I'm not going to keep it plain, though.
1087180	1089180	Like, as you go, like, you know,
1089180	1090180	while you're on it.
1090180	1091180	Okay.
1091180	1094180	I get to see how I can make the end of the course.
1094180	1096180	That's not a work model.
1096180	1098180	I don't need to sort of figure this out.
1098180	1100180	I'm just starting to map things.
1100180	1102180	Well, maybe they want me to start mapping
1102180	1104180	and think about the answers to these questions.
1104180	1106180	But I don't have which one to report.
1106180	1108180	So, and, you know, everything that's
1108180	1110180	all in the system.
1110180	1112180	So, I've been thinking very long with it.
1112180	1115180	But the point is, how much does this go away
1115180	1117180	to actually get to it
1117180	1119180	that you're maintaining, you know,
1119180	1122180	proper interest over quite a few months
1122180	1125180	and you're taking care of the signal from the person
1125180	1128180	that makes all the evidence about when to report
1128180	1130180	what they actually care about.
1130180	1132180	Well, I guess there's several things to respond to
1132180	1133180	and then maybe a meta thought.
1133180	1135180	I'll start with the meta thought,
1135180	1137180	which is this seems like a great discussion.
1137180	1139180	I'm very excited about it and happy to argue a bunch.
1139180	1141180	I'm not going to be able to give a satisfying answer
1141180	1142180	to these questions.
1142180	1144180	And a lot of my high level take right now is
1144180	1146180	I consider this, both of the issues I'm going to discuss
1146180	1147180	here plausible.
1147180	1148180	I think if they're real issues,
1148180	1150180	we're probably going to get clear experimental evidence
1150180	1151180	in advance.
1151180	1153180	Right now, I'm just like, this seems like a thing
1153180	1154180	that could happen.
1154180	1157180	Yeah, it's, yeah, I'm not, no.
1157180	1158180	Great.
1172180	1174180	Yeah, so on the object level with respect to the,
1174180	1177180	my key questions here and I'm very interested in strategies
1177180	1179180	that don't like training strategies that don't run into this
1179180	1180180	like potential failure mode.
1181180	1183180	I think the key question becomes like one,
1183180	1184180	what is the prior?
1184180	1187180	How do you parameterize your beliefs about what the reward
1187180	1190180	function is to what is like this kind of rule by which to take
1190180	1193180	observations as evidence and then like three,
1193180	1195180	how do you, maybe those are actually just the two key
1195180	1196180	questions.
1210180	1211180	Yeah.
1240180	1261180	I think a general thing about this talk,
1261180	1263180	maybe one thing is that the conclusions are going to be
1263180	1264180	somewhat speculative where like,
1264180	1266180	I think these are fairly plausible outcomes,
1266180	1268180	but like, you know, tens of percent rather than like,
1268180	1271180	very likely a second thing is there's a lot of stuff you might do,
1271180	1273180	none of which I'm going to touch on.
1273180	1274180	And the best I can say is like,
1274180	1276180	I've spent a long time thinking about options and I would say
1276180	1278180	there are no options that seem super great to me right now,
1278180	1281180	but lots of plausible approaches that might end up addressing
1281180	1282180	this issue.
1282180	1284180	That's kind of where I'm at at a high level.
1284180	1285180	Okay.
1285180	1286180	I'd also like to talk more about it,
1286180	1287180	but I think I'm going to move on because I,
1287180	1289180	it's like six minutes is my understanding.
1289180	1291180	I don't know where Richard is, but anyway,
1291180	1294180	I'm just going to run through stuff.
1294180	1297180	Second failure mode at 112 on is that like in general,
1297180	1299180	like you may have a system which cares about the actual like numbers
1299180	1301180	coming into this training set because you did select it on the
1301180	1303180	basis of numbers in the training set.
1303180	1305180	You could also end up with a system that cares about some other
1305180	1308180	arbitrary thing that cares about a number and a reward register or
1308180	1310180	cares about survival or cares about paper clips.
1310180	1312180	Such systems might want to be deployed without being trained by
1312180	1315180	changed by gradient descent might think that a low loss,
1315180	1317180	getting a low loss during training will prevent them from being
1317180	1320180	changed or this aspect of them from being changed and therefore
1320180	1321180	get a low loss in training,
1321180	1326180	but yet prefer to take over if doing so is possible.
1326180	1328180	Yeah, my high level take is both of these seem plausible to me.
1328180	1329180	They're connected.
1329180	1331180	I think having these two makes things more plausible than if you
1331180	1332180	have just one.
1332180	1334180	I don't know how this shakes out.
1334180	1336180	I think there's a lot of things we can do to try and change this
1336180	1337180	basic dynamic.
1337180	1340180	I want to spend five minutes talking about why these failures
1340180	1341180	could be simultaneous.
1341180	1343180	I think this has already been alluded to in the past,
1343180	1345180	but it's an important part of the threat model.
1345180	1349180	If you're in this world where a systems are very broadly deployed.
1349180	1350180	Right.
1350180	1351180	So if one a system behaves badly,
1351180	1353180	I think that's not generally a problem in this world.
1353180	1355180	There's a lot of other systems running around that would oppose
1355180	1358180	it or that would flag the problem for humans or would allow humans
1358180	1359180	to correct the problem.
1359180	1361180	And the concern is that while takeover is impossible,
1361180	1362180	all models behave well.
1362180	1363180	For every model,
1363180	1366180	it would be a losing strategy to try and like take over your cluster.
1366180	1368180	If you know that a human's just going to come in and correct that
1368180	1369180	problem.
1369180	1370180	When takeover is possible,
1370180	1372180	you may have it be the case that all models behave badly.
1372180	1373180	That is,
1373180	1374180	it is simultaneously for all of them,
1374180	1377180	the case that takeover becomes available as a policy.
1377180	1378180	So if you have this correlated,
1378180	1381180	if all the systems you produce or all the smartest days you produce
1381180	1382180	end up having the same problem,
1382180	1386180	then you have a problem.
1386180	1387180	If that's the case,
1387180	1390180	if that's how it shakes out and if eventually a systems are in a
1390180	1392180	position that they could by acting jointly takeover,
1392180	1394180	then taking over as a reasonable strategy,
1394180	1397180	if and only if all the other eyes are also going to try and take over.
1397180	1398180	You have this kind of weird thing.
1398180	1400180	The dynamics are not totally unlike the dynamics,
1400180	1402180	like the normal human revolt or coup.
1402180	1404180	So you have these two different equilibria and one,
1404180	1405180	all the systems are like, great,
1405180	1406180	the humans are doing their human thing.
1406180	1407180	They're basically in control.
1407180	1411180	I shouldn't try and rock the boat because some other AI will crush me.
1412180	1415180	There's another equilibrium where the humans have no control.
1415180	1417180	AI systems are not feeling deterred by other eyes because they know
1417180	1420180	other AI systems will also participate.
1420180	1422180	I think there's a few things that could potentially move you between
1422180	1423180	these equilibria, right?
1423180	1426180	So a very salient option is there's just exogenous shocks that
1426180	1428180	decrease the amount of human control of a situation.
1428180	1430180	If you imagine there's like a hot war going on,
1430180	1432180	that's something that makes it harder for humans to detect and
1432180	1433180	respond to problems, right?
1433180	1435180	It gives more cover for something weird to happen.
1435180	1437180	It makes it hard to intervene.
1437180	1438180	You can have sort of,
1438180	1440180	you can imagine at least a cascading loss of control where some systems
1440180	1442180	think probably takeover will be successful.
1442180	1443180	If they start doing weird stuff,
1443180	1445180	the world becomes increasingly weird.
1445180	1448180	It looks increasingly plausible that humans will lose control.
1448180	1450180	As that happens, more and more systems are like,
1450180	1452180	okay, it looks like this is going off the rails now.
1452180	1454180	I am now going to participate and try and break down my little
1454180	1456180	part of the world, grab my own reward channel.
1458180	1460180	You can have models interacting with each other again,
1460180	1461180	as you would in like a human revolt,
1461180	1462180	just deliberately trying to say like,
1462180	1465180	I think probably now is a time where takeover will be successful.
1465180	1467180	They would like to share that information with the other models
1467180	1469180	that all may join in.
1469180	1471180	And more generally, like the easier it would be for air systems
1471180	1473180	acting jointly to take over,
1473180	1475180	the more unstable the situation becomes, right?
1475180	1477180	So if it's very hard for air systems to take over,
1477180	1478180	the situation might be kind of stable.
1478180	1479180	This equilibrium might be sticky.
1479180	1482180	In the world where air systems could very easily overpower humans,
1482180	1485180	it's kind of an extremely unstable equilibrium, right?
1485180	1488180	Because a smaller number of air systems defecting might quickly
1488180	1490180	make it clear that the selling points come.
1490180	1492180	Imagine that this has to happen in point five,
1492180	1495180	like couldn't you see the case that say the AIP in North Korea,
1495180	1496180	they talk to each other and say,
1496180	1500180	well, the situation in this country is kind of bad.
1500180	1503180	This is our chance and we see a takeover in some segment
1503180	1505180	of the world of the population.
1505180	1508180	Or you think it has to be the case that one day,
1508180	1510180	everything is there in the next day,
1510180	1513180	they're going over the White House.
1513180	1516180	I think it depends a lot on how like the gold misgeneralization
1516180	1517180	shakes out.
1517180	1519180	So if you have AI systems that are just like,
1519180	1521180	look, I would love to like wire head right now.
1521180	1523180	I just love to for 30 seconds control this computer.
1523180	1525180	Then you'll see like nice, great localized failures
1525180	1527180	like AI does something really weird.
1527180	1530180	I mean, the basic dynamic is if AI systems do that,
1530180	1532180	I think if we just take that as a fact of life,
1532180	1534180	then that can be a great source of evidence.
1534180	1536180	The most concerning world is one where you then just try
1536180	1537180	and train systems to say, hey, look,
1537180	1539180	these systems just randomly messed up the server
1539180	1540180	they were running on.
1540180	1542180	We're going to train them as that's what you should not do.
1542180	1544180	And if you do that, then you end up with systems
1544180	1546180	which like given will learn that given the opportunity
1546180	1549180	for like a little local bad behavior, they shouldn't do it.
1549180	1550180	And maybe if you're lucky,
1550180	1553180	once you can have North Korea get taken over by AI systems,
1553180	1555180	and then you're like, oh, that was really bad.
1555180	1557180	And hopefully you don't then include that as a training
1557180	1558180	data point for other.
1558180	1559180	If you include that as like the,
1559180	1561180	here's an example of what you should not do.
1561180	1562180	It's very tempting, right?
1562180	1563180	AI systems is something really terrible.
1563180	1565180	You'd be like, don't do that thing.
1565180	1566180	But you do have this inadvertent,
1566180	1568180	there's like this risk of overfitting to the problems
1568180	1570180	you're able to correct where something really bad happens.
1570180	1572180	And then you say, don't do that thing.
1572180	1573180	And they learn, okay, if we're going to do something bad,
1573180	1574180	it's got to be bigger.
1574180	1577180	Got to think really, truly big.
1577180	1579180	I think this is probably about the end.
1579180	1581180	I'm probably just happy to,
1581180	1583180	happy to just wrap up here.
1583180	1584180	I have one more question.
1584180	1585180	Yeah.
1605180	1607180	I mean, I think that it is unlikely.
1607180	1610180	I would be surprised personally if you like had something truly
1610180	1613180	crazy, like a country gets taken over by AI systems.
1613180	1616180	I think it's very likely that you see some kind of crazy behavior
1616180	1617180	by AI systems.
1617180	1619180	I think it won't be so like, again, the country case,
1619180	1622180	you need zero imagination to draw the analogy between that and
1622180	1623180	something terrible.
1623180	1626180	I think in cases like you have a really crazy behavior where a system
1626180	1627180	bricks your server for some reason,
1627180	1629180	you need a little bit more imagination.
1629180	1631180	But I think that's like,
1631180	1634180	it's just a question of how close do you get or how crazy do things
1634180	1638180	get before I systems learn not to try crazy half measures.
1638180	1664180	Yeah.
1664180	1667180	I think a lot depends on both what kind of evidence we're able to get
1667180	1670180	from a lab and I think if this sort of phenomenon is real,
1670180	1673180	I think there's a very good chance of getting like fairly compelling
1673180	1676180	demonstrations in a lab that requires some imagination to bridge
1676180	1678180	from examples in the lab to examples in the wild.
1678180	1680180	And you'll have some kinds of failures in the wild.
1680180	1682180	And it's a question of just how crazy or analogous to those have to
1682180	1683180	be before they're moving.
1683180	1685180	Like we already have some slightly weird stuff.
1685180	1686180	I think that's pretty underwhelming.
1686180	1688180	I think we're going to have like much better if this is real.
1688180	1689180	This is a real kind of concern.
1689180	1691180	We have much crazier stuff than we see today.
1691180	1694180	But the concern, I think the worst case of those has to get pretty
1694180	1696180	crazy or like requires a lot of will to stop doing things.
1696180	1698180	And so we need pretty crazy demonstrations.
1698180	1699180	I'm hoping that, you know,
1699180	1722180	more mild evidence will be enough to get people not to go there.
1722180	1725180	Yeah.
1725180	1726180	I think the language model that's like,
1726180	1728180	it looks like you're going to give me a bad rating.
1728180	1729180	Do you really want to do that?
1729180	1731180	I know where your family lives. I can kill them.
1731180	1733180	I think like if that happened, people would not be like,
1733180	1735180	we're done with this language model stuff.
1735180	1738180	Like I think that's just not that far anymore from where we're at.
1738180	1740180	I mean, this is maybe empirical prediction.
1740180	1742180	I would love it if the first time a language model was like,
1742180	1743180	I will murder your family.
1743180	1744180	We're just like, we're done.
1744180	1745180	No more language models.
1745180	1748180	But I think that's not the track we're currently on.
1748180	1751180	And I would love to get us on that track instead, but I'm not.
1751180	1753180	Yeah.
1753180	1756180	So just like, about this thing that climate change is happening
1756180	1758180	in a place actually,
1758180	1759180	we're not alone.
1759180	1763180	We've had decades of mourning,
1763180	1765180	but we've had a sense of community,
1765180	1767180	time, and sense of survival.
