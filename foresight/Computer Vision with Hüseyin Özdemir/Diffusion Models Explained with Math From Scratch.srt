1
00:00:00,000 --> 00:00:06,000
Hi, in this video I will talk about diffusion models.

2
00:00:06,000 --> 00:00:13,000
First, the references for this video.

3
00:00:13,000 --> 00:00:21,000
Denoising diffusion probabilistic models.

4
00:00:21,000 --> 00:00:30,000
Improved denoising diffusion probabilistic models.

5
00:00:30,000 --> 00:00:39,000
Understanding diffusion models a unified perspective.

6
00:00:39,000 --> 00:00:45,000
Let's begin with standard Gaussian distribution.

7
00:00:45,000 --> 00:00:55,000
In order to generate the sample from standard Gaussian distribution, one method is to use cumulative distribution function.

8
00:00:55,000 --> 00:01:02,000
A value is sampled uniformly on y-axis between 0 and 1.

9
00:01:02,000 --> 00:01:08,000
Y value is mapped to x-axis.

10
00:01:08,000 --> 00:01:18,000
And x-value is a sample from standard Gaussian distribution.

11
00:01:18,000 --> 00:01:25,000
What if we want a new BERT image from distribution of BERT images?

12
00:01:25,000 --> 00:01:32,000
Problem is that we don't know the probability distribution of BERT images.

13
00:01:32,000 --> 00:01:41,000
In generator AI, the aim is to estimate and sample from high-dimensional complex data distribution.

14
00:01:41,000 --> 00:01:53,000
A deep neural network can be used to learn or model an approximate distribution using large amount of data.

15
00:01:53,000 --> 00:01:57,000
A diffusion model is a type of generator model.

16
00:01:57,000 --> 00:02:05,000
Training requires two types of processes.

17
00:02:05,000 --> 00:02:12,000
In forward process, training image is distracted with noise.

18
00:02:12,000 --> 00:02:19,000
In reverse process, it is learned how to recover training image.

19
00:02:19,000 --> 00:02:26,000
After training, reverse process is used to generate new samples.

20
00:02:26,000 --> 00:02:32,000
Let's inspect them in full detail.

21
00:02:32,000 --> 00:02:40,000
Forward process.

22
00:02:40,000 --> 00:02:47,000
X0 is a training image sampled from a real image distribution.

23
00:02:47,000 --> 00:03:00,000
In forward process, noise is added to training image iteratively.

24
00:03:00,000 --> 00:03:07,000
Noise addition is performed according to this equation.

25
00:03:07,000 --> 00:03:11,000
Xt is noisy image at time step t.

26
00:03:11,000 --> 00:03:16,000
Xt-1 is noisy image at time step t-1.

27
00:03:16,000 --> 00:03:22,000
Epsilon t-1 is noise sampled from standard Gaussian distribution.

28
00:03:22,000 --> 00:03:29,000
Beta t is variance parameter for time step t.

29
00:03:29,000 --> 00:03:39,000
At the beginning of forward process, X0 is mapped to minus 1, 1 interval.

30
00:03:39,000 --> 00:03:52,000
Xt-1 is scaled down, that is pixel values approach 0 to keep them at a certain range after noise addition.

31
00:03:52,000 --> 00:04:01,000
Standard Gaussian noise is scaled to adjust the variance of added noise.

32
00:04:01,000 --> 00:04:12,000
X capital t approaches standard Gaussian noise for large capital t, for example 1000.

33
00:04:12,000 --> 00:04:20,000
Set of beta t values is called variance schedule.

34
00:04:20,000 --> 00:04:26,000
Xt depends on Xt-1 and does not depend on other time steps.

35
00:04:26,000 --> 00:04:34,000
So forward process is a Markov process.

36
00:04:34,000 --> 00:04:47,000
Using reparameterization Q of Xt given Xt-1, the transition step of forward process can be expressed as a Gaussian distribution.

37
00:04:47,000 --> 00:04:59,000
So forward process can be written as a joint probability distribution conditioned on X0.

38
00:04:59,000 --> 00:05:05,000
It is possible to obtain Xt directly from X0.

39
00:05:05,000 --> 00:05:11,000
Define alpha t as 1 minus beta t.

40
00:05:11,000 --> 00:05:17,000
Write the equation for Xt.

41
00:05:17,000 --> 00:05:24,000
Then inside the equation expand Xt-1.

42
00:05:24,000 --> 00:05:32,000
Epsilons are independent and identically distributed noise samples.

43
00:05:32,000 --> 00:05:42,000
On the right hand side of the equation there are two noise components epsilon t-1 and epsilon t-2.

44
00:05:42,000 --> 00:05:53,000
Since these noise samples are Gaussian, their sum is also Gaussian with variance as the sum of their variances.

45
00:05:53,000 --> 00:05:59,000
If we continue the same way a pattern shows up.

46
00:05:59,000 --> 00:06:12,000
Xt can be written in terms of X0 and noise using alpha bar, cumulative multiplication of alpha values.

47
00:06:12,000 --> 00:06:19,000
Variance schedule.

48
00:06:19,000 --> 00:06:25,000
Variance of edit noise is controlled with beta t.

49
00:06:25,000 --> 00:06:35,000
If beta t increases linearly from beta 1 to beta capital T, then it is linear variance schedule.

50
00:06:35,000 --> 00:06:41,000
Another alternative is cosine variance schedule.

51
00:06:41,000 --> 00:06:49,000
And these are the related equations.

52
00:06:49,000 --> 00:06:56,000
These are the samples created with linear variance schedule.

53
00:06:56,000 --> 00:07:00,000
And cosine variance schedule.

54
00:07:00,000 --> 00:07:06,000
It is obvious that structures in the image are lost too early with linear schedule.

55
00:07:06,000 --> 00:07:12,000
And lots of samples resemble pure noise.

56
00:07:12,000 --> 00:07:24,000
By looking at beta t versus time step graph, with cosine schedule less noise is added until later time steps.

57
00:07:24,000 --> 00:07:37,000
And alpha bar t versus time step graph shows that with linear schedule alpha bar t value decreases faster.

58
00:07:37,000 --> 00:07:47,000
Reverse process.

59
00:07:47,000 --> 00:07:56,000
Transitions in forward process are known and controlled with hyperparameter beta t.

60
00:07:56,000 --> 00:08:01,000
In reverse process the aim is to construct X0 iteratively.

61
00:08:01,000 --> 00:08:04,000
Starting with noise image X capital T.

62
00:08:04,000 --> 00:08:10,000
Reverse process is also a Markov chain.

63
00:08:10,000 --> 00:08:16,000
Reverse process can be expressed as a joint probability distribution.

64
00:08:16,000 --> 00:08:23,000
X capital T is sampled from standard Gaussian distribution.

65
00:08:23,000 --> 00:08:35,000
P theta of XT minus 1 given XT is a transition in reverse process.

66
00:08:35,000 --> 00:08:41,000
X0 is observed or known variable.

67
00:08:41,000 --> 00:08:48,000
X1 to X capital T are hidden or latent variables.

68
00:08:48,000 --> 00:08:57,000
We need to find parameters such that the likelihood of sampling or observing X0 is maximum.

69
00:08:57,000 --> 00:09:07,000
Integrating joint distribution over latent variables to obtain marginal distribution of X0 is intractable.

70
00:09:07,000 --> 00:09:15,000
Because different Markov chains starting at different X capital T can lead to same X0.

71
00:09:15,000 --> 00:09:25,000
Another option is to weave the problem from variational Bayesian perspective.

72
00:09:25,000 --> 00:09:29,000
Maximizing log likelihood is equivalent to maximizing likelihood.

73
00:09:29,000 --> 00:09:36,000
Because logarithm is a monotonically increasing function.

74
00:09:36,000 --> 00:09:42,000
Inside the integral reverse joint distribution is multiplied and divided by forward joint distribution.

75
00:09:42,000 --> 00:09:53,000
To incorporate forward process into the equation.

76
00:09:53,000 --> 00:10:00,000
Expected value of a random variable is a weighted average.

77
00:10:00,000 --> 00:10:06,000
X is the value of random variable.

78
00:10:06,000 --> 00:10:12,000
And weight function is probability distribution.

79
00:10:12,000 --> 00:10:19,000
In our case the weight function is the joint distribution of forward process.

80
00:10:19,000 --> 00:10:29,000
So we can replace integral with an expectation operator.

81
00:10:29,000 --> 00:10:34,000
Note that log is a concave function.

82
00:10:34,000 --> 00:10:40,000
Let's take two points X1 and X2 on X axis.

83
00:10:40,000 --> 00:10:49,000
And their linear combination.

84
00:10:49,000 --> 00:10:58,000
Log of linear combination is greater than or equal to the linear combination of log values.

85
00:10:58,000 --> 00:11:05,000
This is Jensen's inequality.

86
00:11:05,000 --> 00:11:09,000
Interchanging the places of expectation and logarithm.

87
00:11:09,000 --> 00:11:15,000
The equation is converted to an inequality.

88
00:11:15,000 --> 00:11:21,000
Log likelihood is the evidence.

89
00:11:21,000 --> 00:11:27,000
And right hand side is evidence lower bound.

90
00:11:27,000 --> 00:11:35,000
It is also called variational lower bound.

91
00:11:35,000 --> 00:11:44,000
Maximizing VLB means maximizing evidence.

92
00:11:44,000 --> 00:11:53,000
Joint distributions can be written as multiplications of Gaussian transitions.

93
00:11:53,000 --> 00:12:10,000
Take t equals one terms out of product operators.

94
00:12:10,000 --> 00:12:17,000
By adding x0 it is explicitly shown that forward process is conditioned on x0.

95
00:12:17,000 --> 00:12:26,000
It is added because the beginning of forward Markov chain is a target for the reverse Markov chain.

96
00:12:26,000 --> 00:12:32,000
Apply Bayes rule in denominator.

97
00:12:32,000 --> 00:12:38,000
The direction of reverse Markov chain is fixed thanks to conditioning on x0.

98
00:12:38,000 --> 00:12:47,000
Xt minus one is in somewhere between Xt and X0.

99
00:12:47,000 --> 00:12:54,000
Let's expand the expression inside product operator in denominator.

100
00:12:54,000 --> 00:13:02,000
Some terms cancel each other out.

101
00:13:02,000 --> 00:13:11,000
Rearranging denominator we get this equation.

102
00:13:11,000 --> 00:13:20,000
One more cancellation in denominator.

103
00:13:20,000 --> 00:13:33,000
Right hand side can be written as a sum of expected values.

104
00:13:33,000 --> 00:13:49,000
As a side note the expected value of x over joint distribution of x and y is equal to expected value of x over its marginal distribution.

105
00:13:49,000 --> 00:13:59,000
If a latent variable is not present inside expectation then it has no effect on expected value.

106
00:13:59,000 --> 00:14:22,000
Unrelated latent variables are removed from waiting distribution.

107
00:14:22,000 --> 00:14:29,000
Distributions inside colored rectangles are different.

108
00:14:29,000 --> 00:14:38,000
We can find a relation between those two using Bayes rule.

109
00:14:38,000 --> 00:14:47,000
Now there is one more expectation for the right most term.

110
00:14:47,000 --> 00:15:04,000
Two of the expectations can be written as KL divergences.

111
00:15:04,000 --> 00:15:10,000
Now we have VLB expression.

112
00:15:10,000 --> 00:15:22,000
Minus VLB is minimized with gradient descent instead of maximizing VLB.

113
00:15:22,000 --> 00:15:30,000
There are three terms L0, LT minus one and L-capitality.

114
00:15:30,000 --> 00:15:41,000
L-capitality is constant with respect to data and can be ignored.

115
00:15:41,000 --> 00:15:48,000
Computing L0

116
00:15:48,000 --> 00:15:56,000
Less step of reverse process can be designed as an independent discrete decoder.

117
00:15:56,000 --> 00:16:02,000
Here is the related distribution.

118
00:16:02,000 --> 00:16:08,000
And bounds of integrals.

119
00:16:08,000 --> 00:16:20,000
Image data is comprised of integers between 0 and 255.

120
00:16:20,000 --> 00:16:30,000
Mu theta i is the predicted value for x0i.

121
00:16:30,000 --> 00:16:38,000
For the height coordinate a Gaussian with mean mu theta i and variance sigma 1 square is used.

122
00:16:38,000 --> 00:16:46,000
X0i is the ground root.

123
00:16:46,000 --> 00:16:55,000
And bounds of integral.

124
00:16:55,000 --> 00:17:01,000
Consider this example case.

125
00:17:01,000 --> 00:17:09,000
Then these are the related numbers.

126
00:17:09,000 --> 00:17:18,000
Same operation is performed for all decoordinates and integration results are multiplied.

127
00:17:18,000 --> 00:17:32,000
If predicted image is close to ground root training image x0, then p theta of x0 given x1 is high.

128
00:17:32,000 --> 00:17:41,000
Deriving simple loss by minimizing LT minus one term.

129
00:17:41,000 --> 00:17:48,000
Let's begin with reverse transition conditioned on x0.

130
00:17:48,000 --> 00:18:00,000
Apply Bayes rule to find the relation between forward and backward transitions.

131
00:18:00,000 --> 00:18:16,000
Forward Markov chain is already conditioned on x0.

132
00:18:16,000 --> 00:18:24,000
These three distributions are Gaussian.

133
00:18:24,000 --> 00:18:33,000
Let's write their functional expressions.

134
00:18:33,000 --> 00:18:47,000
And combine the exponential terms.

135
00:18:47,000 --> 00:18:55,000
Expand square terms.

136
00:18:55,000 --> 00:19:09,000
And rearrange the equation.

137
00:19:09,000 --> 00:19:25,000
Start equalizing denominators inside parentheses.

138
00:19:25,000 --> 00:19:41,000
Inside exponential factor out the common term.

139
00:19:41,000 --> 00:19:51,000
Now notice the quadratic expression starting with xt minus one square.

140
00:19:51,000 --> 00:20:01,000
That quadratic expression can be written as square of difference of two terms.

141
00:20:01,000 --> 00:20:14,000
And resulting expression is a Gaussian distribution.

142
00:20:14,000 --> 00:20:18,000
Note that x0 is available only in training.

143
00:20:18,000 --> 00:20:26,000
So approximation should be performed without x0.

144
00:20:26,000 --> 00:20:31,000
Using noise addition equation from forward process.

145
00:20:31,000 --> 00:20:43,000
Mean of reverse transition can be written in terms of xt and epsilon.

146
00:20:43,000 --> 00:20:57,000
For reverse process ground route is approximated with learn transition.

147
00:20:57,000 --> 00:21:08,000
Since reference is Gaussian, its approximation is also modeled as Gaussian.

148
00:21:08,000 --> 00:21:24,000
In the DPM paper covariance is fixed and only mean is learned.

149
00:21:24,000 --> 00:21:31,000
Noise addition in forward process.

150
00:21:31,000 --> 00:21:44,000
Reverse transition.

151
00:21:44,000 --> 00:21:51,000
KL divergence is a measure of dissimilarity.

152
00:21:51,000 --> 00:22:04,000
We need to find parameters minimizing the expression inside arc mean.

153
00:22:04,000 --> 00:22:23,000
Expression inside rectangle is KL divergence between two Gaussians.

154
00:22:23,000 --> 00:22:41,000
Let's use the equations for covariance and means.

155
00:22:41,000 --> 00:22:57,000
Simplifying we get this equation.

156
00:22:57,000 --> 00:23:03,000
This is the resulting expression.

157
00:23:03,000 --> 00:23:17,000
In practice time step dependent multiplier can be set to one.

158
00:23:17,000 --> 00:23:26,000
In VLB expression xt minus one terms are summed for t is greater than one.

159
00:23:26,000 --> 00:23:33,000
So there is one more expectation.

160
00:23:33,000 --> 00:23:37,000
Here is the loss equation.

161
00:23:37,000 --> 00:23:42,000
It is called simple loss.

162
00:23:42,000 --> 00:23:52,000
And expectation is over time step x0 and epsilon.

163
00:23:52,000 --> 00:23:57,000
Note that simple loss is obtained minimizing xt minus one term.

164
00:23:57,000 --> 00:24:02,000
And xt minus one is defined for t is greater than one.

165
00:24:02,000 --> 00:24:10,000
So what happens when t is equal to one?

166
00:24:10,000 --> 00:24:18,000
Remember discrete decoder from previous section.

167
00:24:18,000 --> 00:24:27,000
Using simple loss when t is equal to one means approximating discrete decoder.

168
00:24:27,000 --> 00:24:38,000
Integral is replaced by multiplication of Gaussian density and bandwidth.

169
00:24:38,000 --> 00:24:41,000
Here is the illustration for the height coordinate.

170
00:24:41,000 --> 00:24:49,000
Area of the rectangle is computed instead of integral.

171
00:24:49,000 --> 00:24:58,000
Let's write functional expression of Gaussian.

172
00:24:58,000 --> 00:25:02,000
X0 is the original training image.

173
00:25:02,000 --> 00:25:07,000
Mu theta is the predicted image.

174
00:25:07,000 --> 00:25:16,000
Take log of both sides.

175
00:25:16,000 --> 00:25:22,000
C is constant with respect to theta.

176
00:25:22,000 --> 00:25:29,000
Use forward and reverse transition equations.

177
00:25:29,000 --> 00:25:46,000
Note that alpha bar one is equal to alpha one.

178
00:25:46,000 --> 00:25:57,000
Ignoring variance and constant term the resultant expression is equal to simple loss.

179
00:25:57,000 --> 00:26:04,000
Training.

180
00:26:04,000 --> 00:26:08,000
Unit is used to model transitions in reverse process.

181
00:26:08,000 --> 00:26:13,000
And it has three parts.

182
00:26:13,000 --> 00:26:16,000
Encoder decreases spatial resolution.

183
00:26:16,000 --> 00:26:20,000
Increases number of channels.

184
00:26:20,000 --> 00:26:24,000
Button neck.

185
00:26:24,000 --> 00:26:27,000
Decoder increases spatial resolution.

186
00:26:27,000 --> 00:26:32,000
Decreases number of channels.

187
00:26:32,000 --> 00:26:42,000
Skip connections transfer features from encoder to decoder.

188
00:26:42,000 --> 00:26:45,000
For diffusion models unit has two inputs.

189
00:26:45,000 --> 00:26:47,000
Noise image and time step.

190
00:26:47,000 --> 00:26:54,000
Predicted noise is the output.

191
00:26:54,000 --> 00:26:59,000
Theta is model parameters.

192
00:26:59,000 --> 00:27:08,000
Self-attention blocks and group normalization layers are added to improve performance.

193
00:27:08,000 --> 00:27:17,000
Time step signal is fed to all residual blocks inside unit after sinusoidal position embedding.

194
00:27:17,000 --> 00:27:31,000
Time step signal is needed because noise added in forward process and predicted in reverse process depends on time step.

195
00:27:31,000 --> 00:27:36,000
For each training image in the minibatch.

196
00:27:36,000 --> 00:27:44,000
A time step is sampled uniformly between one and capital T.

197
00:27:44,000 --> 00:27:49,000
Noisy image is created.

198
00:27:49,000 --> 00:27:55,000
Noisy image and time step are fed to unit.

199
00:27:55,000 --> 00:27:59,000
Noise is predicted.

200
00:27:59,000 --> 00:28:06,000
Loss and gradient are computed.

201
00:28:06,000 --> 00:28:15,000
Sampling.

202
00:28:15,000 --> 00:28:28,000
After finishing training any sample X0 can be generated as a result of an iterative process starting at time step capital T with a noise image.

203
00:28:29,000 --> 00:28:41,000
Markov transitions are computed for all time steps until X0 is obtained.

204
00:28:41,000 --> 00:28:52,000
Transition equation is obtained using reparameterization.

205
00:28:52,000 --> 00:28:59,000
This expression implies a probability distribution.

206
00:28:59,000 --> 00:29:19,000
As X0, any image is needed, not a distribution. So Z is 0 when T is equal to 1.

207
00:29:19,000 --> 00:29:31,000
Also remember that during training with simple loss, discrete decoder for L0 term focuses on the distance between mu theta and X0.

208
00:29:31,000 --> 00:29:41,000
Approximated discrete decoder even ignores variance completely.

209
00:29:41,000 --> 00:29:48,000
Learning covariance.

210
00:29:48,000 --> 00:29:58,000
We have seen the method with learned mean and fixed covariance.

211
00:29:58,000 --> 00:30:11,000
In practice setting variance to beta T or beta tilde T provides similar sample quality.

212
00:30:11,000 --> 00:30:21,000
It is possible to design reverse process transitions with learned mean and learned covariance.

213
00:30:21,000 --> 00:30:27,000
These are two extreme values for variance.

214
00:30:27,000 --> 00:30:39,000
Instead of predicting variance directly, network learns to make linear interpolation between two extremes in log domain.

215
00:30:39,000 --> 00:30:46,000
Network predicts V and V controls interpolation.

216
00:30:46,000 --> 00:30:55,000
Simple loss doesn't depend on covariance so loss expression is modified.

217
00:30:55,000 --> 00:31:05,000
Creating descent is designed such that VLV loss guides covariance and has no effect on mean.

218
00:31:05,000 --> 00:31:08,000
That's all for this video.

219
00:31:08,000 --> 00:31:20,000
See you next time.

