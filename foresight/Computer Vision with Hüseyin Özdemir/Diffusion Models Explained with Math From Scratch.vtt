WEBVTT

00:00.000 --> 00:06.000
Hi, in this video I will talk about diffusion models.

00:06.000 --> 00:13.000
First, the references for this video.

00:13.000 --> 00:21.000
Denoising diffusion probabilistic models.

00:21.000 --> 00:30.000
Improved denoising diffusion probabilistic models.

00:30.000 --> 00:39.000
Understanding diffusion models a unified perspective.

00:39.000 --> 00:45.000
Let's begin with standard Gaussian distribution.

00:45.000 --> 00:55.000
In order to generate the sample from standard Gaussian distribution, one method is to use cumulative distribution function.

00:55.000 --> 01:02.000
A value is sampled uniformly on y-axis between 0 and 1.

01:02.000 --> 01:08.000
Y value is mapped to x-axis.

01:08.000 --> 01:18.000
And x-value is a sample from standard Gaussian distribution.

01:18.000 --> 01:25.000
What if we want a new BERT image from distribution of BERT images?

01:25.000 --> 01:32.000
Problem is that we don't know the probability distribution of BERT images.

01:32.000 --> 01:41.000
In generator AI, the aim is to estimate and sample from high-dimensional complex data distribution.

01:41.000 --> 01:53.000
A deep neural network can be used to learn or model an approximate distribution using large amount of data.

01:53.000 --> 01:57.000
A diffusion model is a type of generator model.

01:57.000 --> 02:05.000
Training requires two types of processes.

02:05.000 --> 02:12.000
In forward process, training image is distracted with noise.

02:12.000 --> 02:19.000
In reverse process, it is learned how to recover training image.

02:19.000 --> 02:26.000
After training, reverse process is used to generate new samples.

02:26.000 --> 02:32.000
Let's inspect them in full detail.

02:32.000 --> 02:40.000
Forward process.

02:40.000 --> 02:47.000
X0 is a training image sampled from a real image distribution.

02:47.000 --> 03:00.000
In forward process, noise is added to training image iteratively.

03:00.000 --> 03:07.000
Noise addition is performed according to this equation.

03:07.000 --> 03:11.000
Xt is noisy image at time step t.

03:11.000 --> 03:16.000
Xt-1 is noisy image at time step t-1.

03:16.000 --> 03:22.000
Epsilon t-1 is noise sampled from standard Gaussian distribution.

03:22.000 --> 03:29.000
Beta t is variance parameter for time step t.

03:29.000 --> 03:39.000
At the beginning of forward process, X0 is mapped to minus 1, 1 interval.

03:39.000 --> 03:52.000
Xt-1 is scaled down, that is pixel values approach 0 to keep them at a certain range after noise addition.

03:52.000 --> 04:01.000
Standard Gaussian noise is scaled to adjust the variance of added noise.

04:01.000 --> 04:12.000
X capital t approaches standard Gaussian noise for large capital t, for example 1000.

04:12.000 --> 04:20.000
Set of beta t values is called variance schedule.

04:20.000 --> 04:26.000
Xt depends on Xt-1 and does not depend on other time steps.

04:26.000 --> 04:34.000
So forward process is a Markov process.

04:34.000 --> 04:47.000
Using reparameterization Q of Xt given Xt-1, the transition step of forward process can be expressed as a Gaussian distribution.

04:47.000 --> 04:59.000
So forward process can be written as a joint probability distribution conditioned on X0.

04:59.000 --> 05:05.000
It is possible to obtain Xt directly from X0.

05:05.000 --> 05:11.000
Define alpha t as 1 minus beta t.

05:11.000 --> 05:17.000
Write the equation for Xt.

05:17.000 --> 05:24.000
Then inside the equation expand Xt-1.

05:24.000 --> 05:32.000
Epsilons are independent and identically distributed noise samples.

05:32.000 --> 05:42.000
On the right hand side of the equation there are two noise components epsilon t-1 and epsilon t-2.

05:42.000 --> 05:53.000
Since these noise samples are Gaussian, their sum is also Gaussian with variance as the sum of their variances.

05:53.000 --> 05:59.000
If we continue the same way a pattern shows up.

05:59.000 --> 06:12.000
Xt can be written in terms of X0 and noise using alpha bar, cumulative multiplication of alpha values.

06:12.000 --> 06:19.000
Variance schedule.

06:19.000 --> 06:25.000
Variance of edit noise is controlled with beta t.

06:25.000 --> 06:35.000
If beta t increases linearly from beta 1 to beta capital T, then it is linear variance schedule.

06:35.000 --> 06:41.000
Another alternative is cosine variance schedule.

06:41.000 --> 06:49.000
And these are the related equations.

06:49.000 --> 06:56.000
These are the samples created with linear variance schedule.

06:56.000 --> 07:00.000
And cosine variance schedule.

07:00.000 --> 07:06.000
It is obvious that structures in the image are lost too early with linear schedule.

07:06.000 --> 07:12.000
And lots of samples resemble pure noise.

07:12.000 --> 07:24.000
By looking at beta t versus time step graph, with cosine schedule less noise is added until later time steps.

07:24.000 --> 07:37.000
And alpha bar t versus time step graph shows that with linear schedule alpha bar t value decreases faster.

07:37.000 --> 07:47.000
Reverse process.

07:47.000 --> 07:56.000
Transitions in forward process are known and controlled with hyperparameter beta t.

07:56.000 --> 08:01.000
In reverse process the aim is to construct X0 iteratively.

08:01.000 --> 08:04.000
Starting with noise image X capital T.

08:04.000 --> 08:10.000
Reverse process is also a Markov chain.

08:10.000 --> 08:16.000
Reverse process can be expressed as a joint probability distribution.

08:16.000 --> 08:23.000
X capital T is sampled from standard Gaussian distribution.

08:23.000 --> 08:35.000
P theta of XT minus 1 given XT is a transition in reverse process.

08:35.000 --> 08:41.000
X0 is observed or known variable.

08:41.000 --> 08:48.000
X1 to X capital T are hidden or latent variables.

08:48.000 --> 08:57.000
We need to find parameters such that the likelihood of sampling or observing X0 is maximum.

08:57.000 --> 09:07.000
Integrating joint distribution over latent variables to obtain marginal distribution of X0 is intractable.

09:07.000 --> 09:15.000
Because different Markov chains starting at different X capital T can lead to same X0.

09:15.000 --> 09:25.000
Another option is to weave the problem from variational Bayesian perspective.

09:25.000 --> 09:29.000
Maximizing log likelihood is equivalent to maximizing likelihood.

09:29.000 --> 09:36.000
Because logarithm is a monotonically increasing function.

09:36.000 --> 09:42.000
Inside the integral reverse joint distribution is multiplied and divided by forward joint distribution.

09:42.000 --> 09:53.000
To incorporate forward process into the equation.

09:53.000 --> 10:00.000
Expected value of a random variable is a weighted average.

10:00.000 --> 10:06.000
X is the value of random variable.

10:06.000 --> 10:12.000
And weight function is probability distribution.

10:12.000 --> 10:19.000
In our case the weight function is the joint distribution of forward process.

10:19.000 --> 10:29.000
So we can replace integral with an expectation operator.

10:29.000 --> 10:34.000
Note that log is a concave function.

10:34.000 --> 10:40.000
Let's take two points X1 and X2 on X axis.

10:40.000 --> 10:49.000
And their linear combination.

10:49.000 --> 10:58.000
Log of linear combination is greater than or equal to the linear combination of log values.

10:58.000 --> 11:05.000
This is Jensen's inequality.

11:05.000 --> 11:09.000
Interchanging the places of expectation and logarithm.

11:09.000 --> 11:15.000
The equation is converted to an inequality.

11:15.000 --> 11:21.000
Log likelihood is the evidence.

11:21.000 --> 11:27.000
And right hand side is evidence lower bound.

11:27.000 --> 11:35.000
It is also called variational lower bound.

11:35.000 --> 11:44.000
Maximizing VLB means maximizing evidence.

11:44.000 --> 11:53.000
Joint distributions can be written as multiplications of Gaussian transitions.

11:53.000 --> 12:10.000
Take t equals one terms out of product operators.

12:10.000 --> 12:17.000
By adding x0 it is explicitly shown that forward process is conditioned on x0.

12:17.000 --> 12:26.000
It is added because the beginning of forward Markov chain is a target for the reverse Markov chain.

12:26.000 --> 12:32.000
Apply Bayes rule in denominator.

12:32.000 --> 12:38.000
The direction of reverse Markov chain is fixed thanks to conditioning on x0.

12:38.000 --> 12:47.000
Xt minus one is in somewhere between Xt and X0.

12:47.000 --> 12:54.000
Let's expand the expression inside product operator in denominator.

12:54.000 --> 13:02.000
Some terms cancel each other out.

13:02.000 --> 13:11.000
Rearranging denominator we get this equation.

13:11.000 --> 13:20.000
One more cancellation in denominator.

13:20.000 --> 13:33.000
Right hand side can be written as a sum of expected values.

13:33.000 --> 13:49.000
As a side note the expected value of x over joint distribution of x and y is equal to expected value of x over its marginal distribution.

13:49.000 --> 13:59.000
If a latent variable is not present inside expectation then it has no effect on expected value.

13:59.000 --> 14:22.000
Unrelated latent variables are removed from waiting distribution.

14:22.000 --> 14:29.000
Distributions inside colored rectangles are different.

14:29.000 --> 14:38.000
We can find a relation between those two using Bayes rule.

14:38.000 --> 14:47.000
Now there is one more expectation for the right most term.

14:47.000 --> 15:04.000
Two of the expectations can be written as KL divergences.

15:04.000 --> 15:10.000
Now we have VLB expression.

15:10.000 --> 15:22.000
Minus VLB is minimized with gradient descent instead of maximizing VLB.

15:22.000 --> 15:30.000
There are three terms L0, LT minus one and L-capitality.

15:30.000 --> 15:41.000
L-capitality is constant with respect to data and can be ignored.

15:41.000 --> 15:48.000
Computing L0

15:48.000 --> 15:56.000
Less step of reverse process can be designed as an independent discrete decoder.

15:56.000 --> 16:02.000
Here is the related distribution.

16:02.000 --> 16:08.000
And bounds of integrals.

16:08.000 --> 16:20.000
Image data is comprised of integers between 0 and 255.

16:20.000 --> 16:30.000
Mu theta i is the predicted value for x0i.

16:30.000 --> 16:38.000
For the height coordinate a Gaussian with mean mu theta i and variance sigma 1 square is used.

16:38.000 --> 16:46.000
X0i is the ground root.

16:46.000 --> 16:55.000
And bounds of integral.

16:55.000 --> 17:01.000
Consider this example case.

17:01.000 --> 17:09.000
Then these are the related numbers.

17:09.000 --> 17:18.000
Same operation is performed for all decoordinates and integration results are multiplied.

17:18.000 --> 17:32.000
If predicted image is close to ground root training image x0, then p theta of x0 given x1 is high.

17:32.000 --> 17:41.000
Deriving simple loss by minimizing LT minus one term.

17:41.000 --> 17:48.000
Let's begin with reverse transition conditioned on x0.

17:48.000 --> 18:00.000
Apply Bayes rule to find the relation between forward and backward transitions.

18:00.000 --> 18:16.000
Forward Markov chain is already conditioned on x0.

18:16.000 --> 18:24.000
These three distributions are Gaussian.

18:24.000 --> 18:33.000
Let's write their functional expressions.

18:33.000 --> 18:47.000
And combine the exponential terms.

18:47.000 --> 18:55.000
Expand square terms.

18:55.000 --> 19:09.000
And rearrange the equation.

19:09.000 --> 19:25.000
Start equalizing denominators inside parentheses.

19:25.000 --> 19:41.000
Inside exponential factor out the common term.

19:41.000 --> 19:51.000
Now notice the quadratic expression starting with xt minus one square.

19:51.000 --> 20:01.000
That quadratic expression can be written as square of difference of two terms.

20:01.000 --> 20:14.000
And resulting expression is a Gaussian distribution.

20:14.000 --> 20:18.000
Note that x0 is available only in training.

20:18.000 --> 20:26.000
So approximation should be performed without x0.

20:26.000 --> 20:31.000
Using noise addition equation from forward process.

20:31.000 --> 20:43.000
Mean of reverse transition can be written in terms of xt and epsilon.

20:43.000 --> 20:57.000
For reverse process ground route is approximated with learn transition.

20:57.000 --> 21:08.000
Since reference is Gaussian, its approximation is also modeled as Gaussian.

21:08.000 --> 21:24.000
In the DPM paper covariance is fixed and only mean is learned.

21:24.000 --> 21:31.000
Noise addition in forward process.

21:31.000 --> 21:44.000
Reverse transition.

21:44.000 --> 21:51.000
KL divergence is a measure of dissimilarity.

21:51.000 --> 22:04.000
We need to find parameters minimizing the expression inside arc mean.

22:04.000 --> 22:23.000
Expression inside rectangle is KL divergence between two Gaussians.

22:23.000 --> 22:41.000
Let's use the equations for covariance and means.

22:41.000 --> 22:57.000
Simplifying we get this equation.

22:57.000 --> 23:03.000
This is the resulting expression.

23:03.000 --> 23:17.000
In practice time step dependent multiplier can be set to one.

23:17.000 --> 23:26.000
In VLB expression xt minus one terms are summed for t is greater than one.

23:26.000 --> 23:33.000
So there is one more expectation.

23:33.000 --> 23:37.000
Here is the loss equation.

23:37.000 --> 23:42.000
It is called simple loss.

23:42.000 --> 23:52.000
And expectation is over time step x0 and epsilon.

23:52.000 --> 23:57.000
Note that simple loss is obtained minimizing xt minus one term.

23:57.000 --> 24:02.000
And xt minus one is defined for t is greater than one.

24:02.000 --> 24:10.000
So what happens when t is equal to one?

24:10.000 --> 24:18.000
Remember discrete decoder from previous section.

24:18.000 --> 24:27.000
Using simple loss when t is equal to one means approximating discrete decoder.

24:27.000 --> 24:38.000
Integral is replaced by multiplication of Gaussian density and bandwidth.

24:38.000 --> 24:41.000
Here is the illustration for the height coordinate.

24:41.000 --> 24:49.000
Area of the rectangle is computed instead of integral.

24:49.000 --> 24:58.000
Let's write functional expression of Gaussian.

24:58.000 --> 25:02.000
X0 is the original training image.

25:02.000 --> 25:07.000
Mu theta is the predicted image.

25:07.000 --> 25:16.000
Take log of both sides.

25:16.000 --> 25:22.000
C is constant with respect to theta.

25:22.000 --> 25:29.000
Use forward and reverse transition equations.

25:29.000 --> 25:46.000
Note that alpha bar one is equal to alpha one.

25:46.000 --> 25:57.000
Ignoring variance and constant term the resultant expression is equal to simple loss.

25:57.000 --> 26:04.000
Training.

26:04.000 --> 26:08.000
Unit is used to model transitions in reverse process.

26:08.000 --> 26:13.000
And it has three parts.

26:13.000 --> 26:16.000
Encoder decreases spatial resolution.

26:16.000 --> 26:20.000
Increases number of channels.

26:20.000 --> 26:24.000
Button neck.

26:24.000 --> 26:27.000
Decoder increases spatial resolution.

26:27.000 --> 26:32.000
Decreases number of channels.

26:32.000 --> 26:42.000
Skip connections transfer features from encoder to decoder.

26:42.000 --> 26:45.000
For diffusion models unit has two inputs.

26:45.000 --> 26:47.000
Noise image and time step.

26:47.000 --> 26:54.000
Predicted noise is the output.

26:54.000 --> 26:59.000
Theta is model parameters.

26:59.000 --> 27:08.000
Self-attention blocks and group normalization layers are added to improve performance.

27:08.000 --> 27:17.000
Time step signal is fed to all residual blocks inside unit after sinusoidal position embedding.

27:17.000 --> 27:31.000
Time step signal is needed because noise added in forward process and predicted in reverse process depends on time step.

27:31.000 --> 27:36.000
For each training image in the minibatch.

27:36.000 --> 27:44.000
A time step is sampled uniformly between one and capital T.

27:44.000 --> 27:49.000
Noisy image is created.

27:49.000 --> 27:55.000
Noisy image and time step are fed to unit.

27:55.000 --> 27:59.000
Noise is predicted.

27:59.000 --> 28:06.000
Loss and gradient are computed.

28:06.000 --> 28:15.000
Sampling.

28:15.000 --> 28:28.000
After finishing training any sample X0 can be generated as a result of an iterative process starting at time step capital T with a noise image.

28:29.000 --> 28:41.000
Markov transitions are computed for all time steps until X0 is obtained.

28:41.000 --> 28:52.000
Transition equation is obtained using reparameterization.

28:52.000 --> 28:59.000
This expression implies a probability distribution.

28:59.000 --> 29:19.000
As X0, any image is needed, not a distribution. So Z is 0 when T is equal to 1.

29:19.000 --> 29:31.000
Also remember that during training with simple loss, discrete decoder for L0 term focuses on the distance between mu theta and X0.

29:31.000 --> 29:41.000
Approximated discrete decoder even ignores variance completely.

29:41.000 --> 29:48.000
Learning covariance.

29:48.000 --> 29:58.000
We have seen the method with learned mean and fixed covariance.

29:58.000 --> 30:11.000
In practice setting variance to beta T or beta tilde T provides similar sample quality.

30:11.000 --> 30:21.000
It is possible to design reverse process transitions with learned mean and learned covariance.

30:21.000 --> 30:27.000
These are two extreme values for variance.

30:27.000 --> 30:39.000
Instead of predicting variance directly, network learns to make linear interpolation between two extremes in log domain.

30:39.000 --> 30:46.000
Network predicts V and V controls interpolation.

30:46.000 --> 30:55.000
Simple loss doesn't depend on covariance so loss expression is modified.

30:55.000 --> 31:05.000
Creating descent is designed such that VLV loss guides covariance and has no effect on mean.

31:05.000 --> 31:08.000
That's all for this video.

31:08.000 --> 31:20.000
See you next time.

