Hi, in this video I will talk about diffusion models.
First, the references for this video.
Denoising diffusion probabilistic models.
Improved denoising diffusion probabilistic models.
Understanding diffusion models a unified perspective.
Let's begin with standard Gaussian distribution.
In order to generate the sample from standard Gaussian distribution, one method is to use cumulative distribution function.
A value is sampled uniformly on y-axis between 0 and 1.
Y value is mapped to x-axis.
And x-value is a sample from standard Gaussian distribution.
What if we want a new BERT image from distribution of BERT images?
Problem is that we don't know the probability distribution of BERT images.
In generator AI, the aim is to estimate and sample from high-dimensional complex data distribution.
A deep neural network can be used to learn or model an approximate distribution using large amount of data.
A diffusion model is a type of generator model.
Training requires two types of processes.
In forward process, training image is distracted with noise.
In reverse process, it is learned how to recover training image.
After training, reverse process is used to generate new samples.
Let's inspect them in full detail.
Forward process.
X0 is a training image sampled from a real image distribution.
In forward process, noise is added to training image iteratively.
Noise addition is performed according to this equation.
Xt is noisy image at time step t.
Xt-1 is noisy image at time step t-1.
Epsilon t-1 is noise sampled from standard Gaussian distribution.
Beta t is variance parameter for time step t.
At the beginning of forward process, X0 is mapped to minus 1, 1 interval.
Xt-1 is scaled down, that is pixel values approach 0 to keep them at a certain range after noise addition.
Standard Gaussian noise is scaled to adjust the variance of added noise.
X capital t approaches standard Gaussian noise for large capital t, for example 1000.
Set of beta t values is called variance schedule.
Xt depends on Xt-1 and does not depend on other time steps.
So forward process is a Markov process.
Using reparameterization Q of Xt given Xt-1, the transition step of forward process can be expressed as a Gaussian distribution.
So forward process can be written as a joint probability distribution conditioned on X0.
It is possible to obtain Xt directly from X0.
Define alpha t as 1 minus beta t.
Write the equation for Xt.
Then inside the equation expand Xt-1.
Epsilons are independent and identically distributed noise samples.
On the right hand side of the equation there are two noise components epsilon t-1 and epsilon t-2.
Since these noise samples are Gaussian, their sum is also Gaussian with variance as the sum of their variances.
If we continue the same way a pattern shows up.
Xt can be written in terms of X0 and noise using alpha bar, cumulative multiplication of alpha values.
Variance schedule.
Variance of edit noise is controlled with beta t.
If beta t increases linearly from beta 1 to beta capital T, then it is linear variance schedule.
Another alternative is cosine variance schedule.
And these are the related equations.
These are the samples created with linear variance schedule.
And cosine variance schedule.
It is obvious that structures in the image are lost too early with linear schedule.
And lots of samples resemble pure noise.
By looking at beta t versus time step graph, with cosine schedule less noise is added until later time steps.
And alpha bar t versus time step graph shows that with linear schedule alpha bar t value decreases faster.
Reverse process.
Transitions in forward process are known and controlled with hyperparameter beta t.
In reverse process the aim is to construct X0 iteratively.
Starting with noise image X capital T.
Reverse process is also a Markov chain.
Reverse process can be expressed as a joint probability distribution.
X capital T is sampled from standard Gaussian distribution.
P theta of XT minus 1 given XT is a transition in reverse process.
X0 is observed or known variable.
X1 to X capital T are hidden or latent variables.
We need to find parameters such that the likelihood of sampling or observing X0 is maximum.
Integrating joint distribution over latent variables to obtain marginal distribution of X0 is intractable.
Because different Markov chains starting at different X capital T can lead to same X0.
Another option is to weave the problem from variational Bayesian perspective.
Maximizing log likelihood is equivalent to maximizing likelihood.
Because logarithm is a monotonically increasing function.
Inside the integral reverse joint distribution is multiplied and divided by forward joint distribution.
To incorporate forward process into the equation.
Expected value of a random variable is a weighted average.
X is the value of random variable.
And weight function is probability distribution.
In our case the weight function is the joint distribution of forward process.
So we can replace integral with an expectation operator.
Note that log is a concave function.
Let's take two points X1 and X2 on X axis.
And their linear combination.
Log of linear combination is greater than or equal to the linear combination of log values.
This is Jensen's inequality.
Interchanging the places of expectation and logarithm.
The equation is converted to an inequality.
Log likelihood is the evidence.
And right hand side is evidence lower bound.
It is also called variational lower bound.
Maximizing VLB means maximizing evidence.
Joint distributions can be written as multiplications of Gaussian transitions.
Take t equals one terms out of product operators.
By adding x0 it is explicitly shown that forward process is conditioned on x0.
It is added because the beginning of forward Markov chain is a target for the reverse Markov chain.
Apply Bayes rule in denominator.
The direction of reverse Markov chain is fixed thanks to conditioning on x0.
Xt minus one is in somewhere between Xt and X0.
Let's expand the expression inside product operator in denominator.
Some terms cancel each other out.
Rearranging denominator we get this equation.
One more cancellation in denominator.
Right hand side can be written as a sum of expected values.
As a side note the expected value of x over joint distribution of x and y is equal to expected value of x over its marginal distribution.
If a latent variable is not present inside expectation then it has no effect on expected value.
Unrelated latent variables are removed from waiting distribution.
Distributions inside colored rectangles are different.
We can find a relation between those two using Bayes rule.
Now there is one more expectation for the right most term.
Two of the expectations can be written as KL divergences.
Now we have VLB expression.
Minus VLB is minimized with gradient descent instead of maximizing VLB.
There are three terms L0, LT minus one and L-capitality.
L-capitality is constant with respect to data and can be ignored.
Computing L0
Less step of reverse process can be designed as an independent discrete decoder.
Here is the related distribution.
And bounds of integrals.
Image data is comprised of integers between 0 and 255.
Mu theta i is the predicted value for x0i.
For the height coordinate a Gaussian with mean mu theta i and variance sigma 1 square is used.
X0i is the ground root.
And bounds of integral.
Consider this example case.
Then these are the related numbers.
Same operation is performed for all decoordinates and integration results are multiplied.
If predicted image is close to ground root training image x0, then p theta of x0 given x1 is high.
Deriving simple loss by minimizing LT minus one term.
Let's begin with reverse transition conditioned on x0.
Apply Bayes rule to find the relation between forward and backward transitions.
Forward Markov chain is already conditioned on x0.
These three distributions are Gaussian.
Let's write their functional expressions.
And combine the exponential terms.
Expand square terms.
And rearrange the equation.
Start equalizing denominators inside parentheses.
Inside exponential factor out the common term.
Now notice the quadratic expression starting with xt minus one square.
That quadratic expression can be written as square of difference of two terms.
And resulting expression is a Gaussian distribution.
Note that x0 is available only in training.
So approximation should be performed without x0.
Using noise addition equation from forward process.
Mean of reverse transition can be written in terms of xt and epsilon.
For reverse process ground route is approximated with learn transition.
Since reference is Gaussian, its approximation is also modeled as Gaussian.
In the DPM paper covariance is fixed and only mean is learned.
Noise addition in forward process.
Reverse transition.
KL divergence is a measure of dissimilarity.
We need to find parameters minimizing the expression inside arc mean.
Expression inside rectangle is KL divergence between two Gaussians.
Let's use the equations for covariance and means.
Simplifying we get this equation.
This is the resulting expression.
In practice time step dependent multiplier can be set to one.
In VLB expression xt minus one terms are summed for t is greater than one.
So there is one more expectation.
Here is the loss equation.
It is called simple loss.
And expectation is over time step x0 and epsilon.
Note that simple loss is obtained minimizing xt minus one term.
And xt minus one is defined for t is greater than one.
So what happens when t is equal to one?
Remember discrete decoder from previous section.
Using simple loss when t is equal to one means approximating discrete decoder.
Integral is replaced by multiplication of Gaussian density and bandwidth.
Here is the illustration for the height coordinate.
Area of the rectangle is computed instead of integral.
Let's write functional expression of Gaussian.
X0 is the original training image.
Mu theta is the predicted image.
Take log of both sides.
C is constant with respect to theta.
Use forward and reverse transition equations.
Note that alpha bar one is equal to alpha one.
Ignoring variance and constant term the resultant expression is equal to simple loss.
Training.
Unit is used to model transitions in reverse process.
And it has three parts.
Encoder decreases spatial resolution.
Increases number of channels.
Button neck.
Decoder increases spatial resolution.
Decreases number of channels.
Skip connections transfer features from encoder to decoder.
For diffusion models unit has two inputs.
Noise image and time step.
Predicted noise is the output.
Theta is model parameters.
Self-attention blocks and group normalization layers are added to improve performance.
Time step signal is fed to all residual blocks inside unit after sinusoidal position embedding.
Time step signal is needed because noise added in forward process and predicted in reverse process depends on time step.
For each training image in the minibatch.
A time step is sampled uniformly between one and capital T.
Noisy image is created.
Noisy image and time step are fed to unit.
Noise is predicted.
Loss and gradient are computed.
Sampling.
After finishing training any sample X0 can be generated as a result of an iterative process starting at time step capital T with a noise image.
Markov transitions are computed for all time steps until X0 is obtained.
Transition equation is obtained using reparameterization.
This expression implies a probability distribution.
As X0, any image is needed, not a distribution. So Z is 0 when T is equal to 1.
Also remember that during training with simple loss, discrete decoder for L0 term focuses on the distance between mu theta and X0.
Approximated discrete decoder even ignores variance completely.
Learning covariance.
We have seen the method with learned mean and fixed covariance.
In practice setting variance to beta T or beta tilde T provides similar sample quality.
It is possible to design reverse process transitions with learned mean and learned covariance.
These are two extreme values for variance.
Instead of predicting variance directly, network learns to make linear interpolation between two extremes in log domain.
Network predicts V and V controls interpolation.
Simple loss doesn't depend on covariance so loss expression is modified.
Creating descent is designed such that VLV loss guides covariance and has no effect on mean.
That's all for this video.
See you next time.
