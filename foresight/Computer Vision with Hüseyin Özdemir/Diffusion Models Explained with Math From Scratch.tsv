start	end	text
0	6000	Hi, in this video I will talk about diffusion models.
6000	13000	First, the references for this video.
13000	21000	Denoising diffusion probabilistic models.
21000	30000	Improved denoising diffusion probabilistic models.
30000	39000	Understanding diffusion models a unified perspective.
39000	45000	Let's begin with standard Gaussian distribution.
45000	55000	In order to generate the sample from standard Gaussian distribution, one method is to use cumulative distribution function.
55000	62000	A value is sampled uniformly on y-axis between 0 and 1.
62000	68000	Y value is mapped to x-axis.
68000	78000	And x-value is a sample from standard Gaussian distribution.
78000	85000	What if we want a new BERT image from distribution of BERT images?
85000	92000	Problem is that we don't know the probability distribution of BERT images.
92000	101000	In generator AI, the aim is to estimate and sample from high-dimensional complex data distribution.
101000	113000	A deep neural network can be used to learn or model an approximate distribution using large amount of data.
113000	117000	A diffusion model is a type of generator model.
117000	125000	Training requires two types of processes.
125000	132000	In forward process, training image is distracted with noise.
132000	139000	In reverse process, it is learned how to recover training image.
139000	146000	After training, reverse process is used to generate new samples.
146000	152000	Let's inspect them in full detail.
152000	160000	Forward process.
160000	167000	X0 is a training image sampled from a real image distribution.
167000	180000	In forward process, noise is added to training image iteratively.
180000	187000	Noise addition is performed according to this equation.
187000	191000	Xt is noisy image at time step t.
191000	196000	Xt-1 is noisy image at time step t-1.
196000	202000	Epsilon t-1 is noise sampled from standard Gaussian distribution.
202000	209000	Beta t is variance parameter for time step t.
209000	219000	At the beginning of forward process, X0 is mapped to minus 1, 1 interval.
219000	232000	Xt-1 is scaled down, that is pixel values approach 0 to keep them at a certain range after noise addition.
232000	241000	Standard Gaussian noise is scaled to adjust the variance of added noise.
241000	252000	X capital t approaches standard Gaussian noise for large capital t, for example 1000.
252000	260000	Set of beta t values is called variance schedule.
260000	266000	Xt depends on Xt-1 and does not depend on other time steps.
266000	274000	So forward process is a Markov process.
274000	287000	Using reparameterization Q of Xt given Xt-1, the transition step of forward process can be expressed as a Gaussian distribution.
287000	299000	So forward process can be written as a joint probability distribution conditioned on X0.
299000	305000	It is possible to obtain Xt directly from X0.
305000	311000	Define alpha t as 1 minus beta t.
311000	317000	Write the equation for Xt.
317000	324000	Then inside the equation expand Xt-1.
324000	332000	Epsilons are independent and identically distributed noise samples.
332000	342000	On the right hand side of the equation there are two noise components epsilon t-1 and epsilon t-2.
342000	353000	Since these noise samples are Gaussian, their sum is also Gaussian with variance as the sum of their variances.
353000	359000	If we continue the same way a pattern shows up.
359000	372000	Xt can be written in terms of X0 and noise using alpha bar, cumulative multiplication of alpha values.
372000	379000	Variance schedule.
379000	385000	Variance of edit noise is controlled with beta t.
385000	395000	If beta t increases linearly from beta 1 to beta capital T, then it is linear variance schedule.
395000	401000	Another alternative is cosine variance schedule.
401000	409000	And these are the related equations.
409000	416000	These are the samples created with linear variance schedule.
416000	420000	And cosine variance schedule.
420000	426000	It is obvious that structures in the image are lost too early with linear schedule.
426000	432000	And lots of samples resemble pure noise.
432000	444000	By looking at beta t versus time step graph, with cosine schedule less noise is added until later time steps.
444000	457000	And alpha bar t versus time step graph shows that with linear schedule alpha bar t value decreases faster.
457000	467000	Reverse process.
467000	476000	Transitions in forward process are known and controlled with hyperparameter beta t.
476000	481000	In reverse process the aim is to construct X0 iteratively.
481000	484000	Starting with noise image X capital T.
484000	490000	Reverse process is also a Markov chain.
490000	496000	Reverse process can be expressed as a joint probability distribution.
496000	503000	X capital T is sampled from standard Gaussian distribution.
503000	515000	P theta of XT minus 1 given XT is a transition in reverse process.
515000	521000	X0 is observed or known variable.
521000	528000	X1 to X capital T are hidden or latent variables.
528000	537000	We need to find parameters such that the likelihood of sampling or observing X0 is maximum.
537000	547000	Integrating joint distribution over latent variables to obtain marginal distribution of X0 is intractable.
547000	555000	Because different Markov chains starting at different X capital T can lead to same X0.
555000	565000	Another option is to weave the problem from variational Bayesian perspective.
565000	569000	Maximizing log likelihood is equivalent to maximizing likelihood.
569000	576000	Because logarithm is a monotonically increasing function.
576000	582000	Inside the integral reverse joint distribution is multiplied and divided by forward joint distribution.
582000	593000	To incorporate forward process into the equation.
593000	600000	Expected value of a random variable is a weighted average.
600000	606000	X is the value of random variable.
606000	612000	And weight function is probability distribution.
612000	619000	In our case the weight function is the joint distribution of forward process.
619000	629000	So we can replace integral with an expectation operator.
629000	634000	Note that log is a concave function.
634000	640000	Let's take two points X1 and X2 on X axis.
640000	649000	And their linear combination.
649000	658000	Log of linear combination is greater than or equal to the linear combination of log values.
658000	665000	This is Jensen's inequality.
665000	669000	Interchanging the places of expectation and logarithm.
669000	675000	The equation is converted to an inequality.
675000	681000	Log likelihood is the evidence.
681000	687000	And right hand side is evidence lower bound.
687000	695000	It is also called variational lower bound.
695000	704000	Maximizing VLB means maximizing evidence.
704000	713000	Joint distributions can be written as multiplications of Gaussian transitions.
713000	730000	Take t equals one terms out of product operators.
730000	737000	By adding x0 it is explicitly shown that forward process is conditioned on x0.
737000	746000	It is added because the beginning of forward Markov chain is a target for the reverse Markov chain.
746000	752000	Apply Bayes rule in denominator.
752000	758000	The direction of reverse Markov chain is fixed thanks to conditioning on x0.
758000	767000	Xt minus one is in somewhere between Xt and X0.
767000	774000	Let's expand the expression inside product operator in denominator.
774000	782000	Some terms cancel each other out.
782000	791000	Rearranging denominator we get this equation.
791000	800000	One more cancellation in denominator.
800000	813000	Right hand side can be written as a sum of expected values.
813000	829000	As a side note the expected value of x over joint distribution of x and y is equal to expected value of x over its marginal distribution.
829000	839000	If a latent variable is not present inside expectation then it has no effect on expected value.
839000	862000	Unrelated latent variables are removed from waiting distribution.
862000	869000	Distributions inside colored rectangles are different.
869000	878000	We can find a relation between those two using Bayes rule.
878000	887000	Now there is one more expectation for the right most term.
887000	904000	Two of the expectations can be written as KL divergences.
904000	910000	Now we have VLB expression.
910000	922000	Minus VLB is minimized with gradient descent instead of maximizing VLB.
922000	930000	There are three terms L0, LT minus one and L-capitality.
930000	941000	L-capitality is constant with respect to data and can be ignored.
941000	948000	Computing L0
948000	956000	Less step of reverse process can be designed as an independent discrete decoder.
956000	962000	Here is the related distribution.
962000	968000	And bounds of integrals.
968000	980000	Image data is comprised of integers between 0 and 255.
980000	990000	Mu theta i is the predicted value for x0i.
990000	998000	For the height coordinate a Gaussian with mean mu theta i and variance sigma 1 square is used.
998000	1006000	X0i is the ground root.
1006000	1015000	And bounds of integral.
1015000	1021000	Consider this example case.
1021000	1029000	Then these are the related numbers.
1029000	1038000	Same operation is performed for all decoordinates and integration results are multiplied.
1038000	1052000	If predicted image is close to ground root training image x0, then p theta of x0 given x1 is high.
1052000	1061000	Deriving simple loss by minimizing LT minus one term.
1061000	1068000	Let's begin with reverse transition conditioned on x0.
1068000	1080000	Apply Bayes rule to find the relation between forward and backward transitions.
1080000	1096000	Forward Markov chain is already conditioned on x0.
1096000	1104000	These three distributions are Gaussian.
1104000	1113000	Let's write their functional expressions.
1113000	1127000	And combine the exponential terms.
1127000	1135000	Expand square terms.
1135000	1149000	And rearrange the equation.
1149000	1165000	Start equalizing denominators inside parentheses.
1165000	1181000	Inside exponential factor out the common term.
1181000	1191000	Now notice the quadratic expression starting with xt minus one square.
1191000	1201000	That quadratic expression can be written as square of difference of two terms.
1201000	1214000	And resulting expression is a Gaussian distribution.
1214000	1218000	Note that x0 is available only in training.
1218000	1226000	So approximation should be performed without x0.
1226000	1231000	Using noise addition equation from forward process.
1231000	1243000	Mean of reverse transition can be written in terms of xt and epsilon.
1243000	1257000	For reverse process ground route is approximated with learn transition.
1257000	1268000	Since reference is Gaussian, its approximation is also modeled as Gaussian.
1268000	1284000	In the DPM paper covariance is fixed and only mean is learned.
1284000	1291000	Noise addition in forward process.
1291000	1304000	Reverse transition.
1304000	1311000	KL divergence is a measure of dissimilarity.
1311000	1324000	We need to find parameters minimizing the expression inside arc mean.
1324000	1343000	Expression inside rectangle is KL divergence between two Gaussians.
1343000	1361000	Let's use the equations for covariance and means.
1361000	1377000	Simplifying we get this equation.
1377000	1383000	This is the resulting expression.
1383000	1397000	In practice time step dependent multiplier can be set to one.
1397000	1406000	In VLB expression xt minus one terms are summed for t is greater than one.
1406000	1413000	So there is one more expectation.
1413000	1417000	Here is the loss equation.
1417000	1422000	It is called simple loss.
1422000	1432000	And expectation is over time step x0 and epsilon.
1432000	1437000	Note that simple loss is obtained minimizing xt minus one term.
1437000	1442000	And xt minus one is defined for t is greater than one.
1442000	1450000	So what happens when t is equal to one?
1450000	1458000	Remember discrete decoder from previous section.
1458000	1467000	Using simple loss when t is equal to one means approximating discrete decoder.
1467000	1478000	Integral is replaced by multiplication of Gaussian density and bandwidth.
1478000	1481000	Here is the illustration for the height coordinate.
1481000	1489000	Area of the rectangle is computed instead of integral.
1489000	1498000	Let's write functional expression of Gaussian.
1498000	1502000	X0 is the original training image.
1502000	1507000	Mu theta is the predicted image.
1507000	1516000	Take log of both sides.
1516000	1522000	C is constant with respect to theta.
1522000	1529000	Use forward and reverse transition equations.
1529000	1546000	Note that alpha bar one is equal to alpha one.
1546000	1557000	Ignoring variance and constant term the resultant expression is equal to simple loss.
1557000	1564000	Training.
1564000	1568000	Unit is used to model transitions in reverse process.
1568000	1573000	And it has three parts.
1573000	1576000	Encoder decreases spatial resolution.
1576000	1580000	Increases number of channels.
1580000	1584000	Button neck.
1584000	1587000	Decoder increases spatial resolution.
1587000	1592000	Decreases number of channels.
1592000	1602000	Skip connections transfer features from encoder to decoder.
1602000	1605000	For diffusion models unit has two inputs.
1605000	1607000	Noise image and time step.
1607000	1614000	Predicted noise is the output.
1614000	1619000	Theta is model parameters.
1619000	1628000	Self-attention blocks and group normalization layers are added to improve performance.
1628000	1637000	Time step signal is fed to all residual blocks inside unit after sinusoidal position embedding.
1637000	1651000	Time step signal is needed because noise added in forward process and predicted in reverse process depends on time step.
1651000	1656000	For each training image in the minibatch.
1656000	1664000	A time step is sampled uniformly between one and capital T.
1664000	1669000	Noisy image is created.
1669000	1675000	Noisy image and time step are fed to unit.
1675000	1679000	Noise is predicted.
1679000	1686000	Loss and gradient are computed.
1686000	1695000	Sampling.
1695000	1708000	After finishing training any sample X0 can be generated as a result of an iterative process starting at time step capital T with a noise image.
1709000	1721000	Markov transitions are computed for all time steps until X0 is obtained.
1721000	1732000	Transition equation is obtained using reparameterization.
1732000	1739000	This expression implies a probability distribution.
1739000	1759000	As X0, any image is needed, not a distribution. So Z is 0 when T is equal to 1.
1759000	1771000	Also remember that during training with simple loss, discrete decoder for L0 term focuses on the distance between mu theta and X0.
1771000	1781000	Approximated discrete decoder even ignores variance completely.
1781000	1788000	Learning covariance.
1788000	1798000	We have seen the method with learned mean and fixed covariance.
1798000	1811000	In practice setting variance to beta T or beta tilde T provides similar sample quality.
1811000	1821000	It is possible to design reverse process transitions with learned mean and learned covariance.
1821000	1827000	These are two extreme values for variance.
1827000	1839000	Instead of predicting variance directly, network learns to make linear interpolation between two extremes in log domain.
1839000	1846000	Network predicts V and V controls interpolation.
1846000	1855000	Simple loss doesn't depend on covariance so loss expression is modified.
1855000	1865000	Creating descent is designed such that VLV loss guides covariance and has no effect on mean.
1865000	1868000	That's all for this video.
1868000	1880000	See you next time.
