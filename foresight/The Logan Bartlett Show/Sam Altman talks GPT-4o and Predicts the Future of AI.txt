We've had like the idea of voice control computers for a long time.
They've never, to me, felt natural to use.
And this one, the fluidity, the pliability, whatever you want to call it,
I just can't believe how much I love using it.
Welcome to Logan Bartlett Show.
On this episode, you're going to hear is a conversation I have with co-founder
and CEO of OpenAI, Sam Altman.
Now, if this is your first time listening to the Logan Bartlett Show,
this is a podcast where I discuss with leaders in technology,
as well as investors, some of the lessons that they've learned
in operating or investing in businesses, mostly in the technology field.
And this discussion with Sam is a little bit different
in which I pushed on a number of things related to artificial intelligence,
as well as where OpenAI is headed, given how topical it is in the news,
and Sam's perspective on such a leading frontier that is artificial intelligence.
You'll hear that discussion with Sam here now.
Thanks for doing this.
Yeah, of course.
All right, I want to start off easy.
What's the weirdest thing that's changed in your life
in the last four or five years running OpenAI?
Like, what's the most unusual shift that's happened?
I mean, quite a lot of things, but the sort of inability to just be like mostly
anonymous in public is very, very strange.
I think if I had thought about that, I would have said,
okay, this is like a weirder.
This would be a weirder thing than it sounds like.
But I didn't really think about it.
And it's like a much weirder thing.
It's like a strangely isolating way to live.
You believed in AI and the power of the business,
so did you just not think through the derivative implications
of running something that wasn't powerful?
Yeah, I think I didn't think.
There were all of these other things.
I'm like, oh, yeah, I was going to be like really important.
OpenAI is going to be a really important company.
I didn't think I would not be able to go out to dinner in my own city.
That's weird.
That's weird.
You made an announcement earlier today.
We did.
Multimodal 4.0?
Yeah.
That's the Omega sign, right?
Oh, just like Omni.
Yeah, Omni.
Okay, sorry.
It works across text, voice, vision.
Can you speak to why this is important?
Because I think it's like an incredible way to use a computer.
We've had the idea of voice-controlled computers for a long time.
You know, we had Siri and we had things before that.
They've never, to me, felt natural to use.
And this one, many different reasons.
What it can do, the speed, adding in other modalities, the inflection, the naturalness,
the fact that you can do things like say, hey, talk faster or talk in this other voice,
the fluidity, the pliability, whatever you want to call it.
I just can't believe how much I love using it.
Spike Johnson would be proud.
Are there use cases that you've gravitated to?
Well, I've only had it for a week or something.
But one surprising one is putting my phone on the table while I'm really in the zone of working.
And then without having to change windows or change what I'm doing,
using it is another channel.
So I'm working on something.
I would normally stop what I'm doing, switch to another tab, Google something,
click around or whatever.
But while I'm still doing it, to just ask and get an instant response,
without changing from what I was looking at on my computer,
that's been a surprisingly cool thing.
What actually made this possible?
Was it an architectural shift or more compute?
I mean, it was all of the things that we've learned over the last several years.
We've been working on audio models.
We've been working on visual models.
We've been working on tying them together.
We've been working on more efficient ways to train our models.
It's not like, okay, we unlocked this one crazy new thing all at once,
but it was putting a lot of pieces together.
Do you think you need to develop an on-device model
to decrease latency to the point for usability?
For video, maybe it would be hard to deal with network latency at some point.
A thing that I've always thought would be super amazing is to put on someday a pair of AR goggles
or whatever and just speak the world in real time and watch things change.
That might get harder over network latency, but for this,
two, 300 milliseconds of latency feels faster than a human responding to me in many cases.
Is video, in this case, images?
Oh, sorry.
I meant video if you wanted generated video, not input video.
Got it.
Currently, it's working with actual video as is.
Well, frame by frame.
Frame by frame, so it's okay.
Got it.
You're literally recently to chat GPT, maybe not being,
they're the next big launch, not being GPT-5.
It feels like there's been an iterative approach to model development that you guys have taken.
Is it fair to say that's how we should think about it going forward,
that it's not going to be some big launch, here's chat GPT-5, but instead?
We honestly don't know yet.
I think that definitely one thing I've learned is that AI and surprise do not go well together.
Although the traditional way of technology launches products,
we should probably do something different.
Now, we could still call it GPT-5 and launch it in a different way,
or we could call it something different.
But I don't think we figured out how to do the naming of branding for these things yet.
It made sense to me from GPT-1 to GPT-4 at the launch.
Now, obviously, GPT-4 has continued to get much better.
We also have this idea that there's going to be one underlying virtual brand,
and it can think harder in some cases than others, or maybe it's different models,
but maybe these are just in care if they're different or not.
I don't think we know the answer to how we're going to product market all of this yet.
Does that mean maybe that the needs of the compute to make incremental progress on models
might be less than what it's been historically?
I sort of think we'll always use as much compute as we get.
Now, we are finding incredible efficiency gains, and that's really important.
One of the cool things that we launched today is obviously the voice mode,
but maybe the most important thing is we were able to make this so
efficient that we're able to serve it to free users.
Best model in the world, by a good amount, if you will look at that little thing,
served to anybody who wants to download ChatGPT for free, and there's a remarkable efficiency
gain over GPT-4 and GPT-4 Turbo, and we have a lot more to gain there.
I've heard you say that ChatGPT didn't actually change the world in and of itself,
but maybe just changed people's expectations for the world.
I don't think you can find much evidence in the economic
measurement of your choice that ChatGPT really inflected productivity or whatever.
Maybe customer support.
Maybe some areas.
If you look at global GDP, can you detect when ChatGPT launched?
Probably not.
Is there a point that you think will be able to determine a GDP?
Yeah, I don't know if you'll ever be able to say this was the one model that did it,
but I think if we look at the graph a couple of decades in the future,
something changed.
Yeah.
Are there applications or areas that you think are most promising in the next 12 months?
I'm sure I'm biased just because of what we do here, but coding I think is a really big one.
Kind of related to the bitter lesson.
You spent some time recently talking about the difference between deeply specialized models
trained on specific data for specific purposes versus generalized models that are capable of
true reasoning.
I would bet that it's the generalized model that's going to matter.
And what is the most important thing there as you think about someone that's focused
singularly on a data set and all the integrations associated with something very narrow?
If the model can do generalized reasoning, if it can figure out new things,
then if it needs to figure out how to work with a new kind of data, you can feed it in
and it can do it, but it doesn't go the other way around.
Like a bunch of specialized models that I don't think,
a bunch of specialized models put together can't figure out the generalized reasoning.
So the implications for that of coding specific models probably would be?
I think a better way of saying this is I think the most important thing to figure out is the
true reasoning capability, and then we can use it for all sorts of things.
What do you think the principal means of communication between humans and AI is in two years?
Natural language seems pretty good.
I'm interested in this general idea that we should design a future that humans
and AIs can use together using the same way.
So I'm more excited about humanoid robots than I am for other forms of robots,
because I think the world is very much now designed for humans,
and I don't want that to get reconfigured for some more efficient kind of thing.
I like the idea that AIs, that we talk to AI in language that is very
well-humored, optimized, and that they even talk to each other that way.
But I think this is generally an interesting direction to push.
You said recently something to the effect of the models might ultimately get commoditized
over time, but the most important thing would likely be the personalization of the models
to each individual. First, do I have that, right?
I'm not certain on this, but it's like a thing that would seem like reasonable to me.
Beyond personalization, do you think it's just normal business UI and ease of use that ultimately
wins for end users? Those will for sure be important. They always are.
I can imagine other things where there's like a sort of marketplace or a network
effect of our sort that matters where we want our agents to communicate.
There's different companies in an app store, but I sort of think that the rules of business
kind of generally apply, and whenever you have a new technology, you're tempted to say they don't,
but that's always like fake news and not always, usually fake news.
All of the traditional ways that you create enduring value will still matter here.
When you see open source models catch up to benchmarks and all of that,
what's your reaction to it? I think it's great. I think that there are
like many other kinds of technology. There will be a place for open source,
there'll be a place for hosted models, and that's fine. It's good.
I'm not going to ask about any specifics related to this, but there have been press reports related
to looking to raise major amounts of money. Wall Street Journal I think was a credible one
to galvanize investment in fabs. Semi-industry, ATSMC, and NVIDIA have been ramping pretty
aggressively to meet expectations of the need for AI infrastructure. You recently said that you
think the world needs more AI infrastructure, and then you said a lot more AI infrastructure.
Is there something you're seeing on the demand side that would require way more AI infrastructure
than what we're currently getting at ATSMC and NVIDIA? First of all, I'm confident that we will
figure out how to bring costs to deliver current systems way, way down. I'm also confident that
as we do that demand will increase by a huge amount. Third, I'm confident that by building
bigger and better systems, there will be even more demand. We should all hope for a world where
intelligence is too cheap to meter. It's just wildly abundant. People use it for all sorts of
things, and you don't even think about whether, oh, do I want this reading all my emails and
responding to them for me, or do I want this curing cancer? Of course you pick curing cancer,
but the answer is you'd love for it to do both things, and I just want to make sure we have
enough for everybody to have that. I don't need you to comment on your own personal efforts here,
although again, if you want to, please let me know. Humane and limitless and some of these
like different physical device systems, what do you think those have gotten wrong? Where do you
think the adoption maybe hasn't met user desires just yet? I think it's just early. I have been an
early adopter of many types of computing. I had and very much loved the Compact TC-1000.
When I was a freshman in college, I thought it was just so cool. That was a long way from the iPad,
long, long way from the iPad, but it was directionally right. Then I got a trio. I was very
not cool college kid. I had an old palm trio, and that was not a thing that kids had,
and that was a long way from the iPhone, but we got there eventually. These things feel like
a very promising direction that's going to take some iteration.
You mentioned recently that a number of businesses that are building on top of
GPT-4 will be steamrolled, I think was your term, by future GPT. Can you elaborate on that
point and second, what are the characteristics of AI-first businesses that you think will survive
GPT's advancement? The only framework that I have found that works for this is you can either
build a business that bets against the next model being really good or a model that bets on that
happening and benefits from it happening. If you're doing a lot of work to make one use case
really work that was just beyond the capability of GPT-4, GPT-4, oh no, and then you get it to work,
but then GPT-5 comes out and it does that and everything else really well, you're sad about
the effort you put into that one thing to get it to barely work. If you had something that just
worked okay across the board and people were finding things to use for, but you didn't put in
tons of work to make this one thing possible, and then GPT-5 or whatever we call it comes along,
it's just way better everything. You got the rising tide lift at all your boats effect.
What I would suggest is you're not building an AI business in most cases, you're building a business
and AI is a technology that you use. In the early days of the app store, I think there were a lot
of things that filled in some very obvious crack and then eventually Apple fixed that and there
you didn't keep needing a flashlight app from the app store, it was just part of the OS and
now it's going to happen. Then there were I think things like Bloober that were enabled by having
smartphones, but really built a very defensible long-term business and then you just want to
go for that latter category. I can come up with a lot of incumbent businesses that leverage you
all that fit that framework in some ways. Are there any novel types of concepts that you sort of
think is in that example, the Uber and it could be a real company if you think of one or even
if it's a toy or just something that's interesting that you think is enabled in that way?
I would actually bet on the new companies for many of these cases. A very common example people use
is trying to build the AI diagnostician and people talk about, oh, I don't want to do a
startup here because Mayo Clinic or Take Your Pick is going to do it. I would actually bet it's
a new company that does something like that. Do you have any advice for CEOs beyond that who
want to be proactive about preparing for these types of disruptions? I would say bet that
intelligence as a service gets better and cheaper every year and it is necessary,
but not sufficient for you to win. The big companies that take years to implement this,
you can beat them, but every other startup that's paying attention is going to do this too.
You still have to figure out what's the long-term defensibility of my business now.
The playing field is way more open than it's been in a long time. There's incredible new things to
do, but you don't get a pass on the hard work of building enduring value even though you can now
do it in more ways. Is there a job title or a type of job responsibility that you could envision
existing or being mainstream in five years because of AI that is maybe niche or non-existent today?
That's a great question and I don't think I've ever gotten it before. People always ask
what job is going to go away. The new one is a more interesting question. Let me think for a
second. There's a lot of things that I could talk about that I think are sort of less interesting
or less huge. What I'm trying to do is come up with the areas of what will 100 million people do
or 50 million people do. The broad category of new kinds of art, entertainment, more human-to-human
connection. I don't know what that job title is going to be, but I don't know if we get there in
five years, but I think there's going to be a premium on human in-person fantastic experiences.
I don't know what we'll call that, but I could see that being a very huge category of something
The most recent public tender of OpenAI was $90 billion or something in and about there.
Are there one or two things that you sort of look at as milestones that will get OpenAI to be a
trillion-dollar company short of AGI? I think if we can just keep improving our technology at the
rate we've been doing it and figuring out how to continue to make good products with it and revenue
keeps growing like it's growing. I don't know about specific numbers, but I think we'll be fine.
Is the business monetization model today the one that you think creates the
$1 trillion equity value? The ChBG subscription model really works well for us. Surprisingly,
I wouldn't have bet on that. I wouldn't have been confident it's going to do as well as it has,
but it's been good. Do you think post-AGI, whatever that term actually means,
will be able to, I don't know, ask AGI what the monetization model is that might be different?
Yeah, should be able to. I think we maybe saw in November not to rehash that the existing
OpenAI structure left some things to be desired, which I don't think we need to rehash in total.
You talked about it enough, I think, but you spoke into making changes along the way. What do you
think the appropriate structure is going forward? I think we're close to being ready to talk about
that. We've been hard at work on all sorts of conversations and brainstorming there.
I think hopefully in this year, I think we'll be right to talk about this calendar year.
Can you tell me first? When Larry and Brett Taylor got Battlefield promoted to board
directors, I was waiting for what my call never came through. One of the interesting things I think
about preconceptions around AI to your point on the monetization model and all that is,
I think we've all, I've heard you speak about it, manual work obviously first, followed by white
color, followed by creative. Obviously, it's proven to be the opposite in some ways. Are there
other things that are counter-intuitive that you've looked at being like, well, I would have
presupposed it to be this way, but it's actually proven to be the exact opposite.
That's definitely the mega surprise to me, the one that you mentioned. There's other,
like, I don't think I would have expected it to be so good at legal work so early,
just because I think of that as like a very precise, complex thing. But no, definitely the
big one is the observation of physical labor, cognitive labor, creative labor.
For those that haven't heard, you make the point about AGI and why you dislike the term.
Can you elaborate on that point? Because I no longer think it's like a moment in time.
I obviously have so many naive conceptions when you start any company, and particularly in a
field that's like moving around as much as this one is. But my naive conception when we started
is that we would get to a moment where we didn't have AGI and then we did, and it would be a real
discontinuity. And I still think there's some chance of a real discontinuity. But on the whole,
I think it's going to look much more like a continuous exponential curve where what matters
is the pace of progress year over year over year. And you and I will probably not agree on the month
or even the year that we're like, okay, now that's AGI. We can come up with other tests that we will
agree with, but even that is harder than it sounds. And GPT-4 is definitely not over a threshold that
I think almost anyone would call an AGI. And I don't expect our next big model to be either. But I
can imagine that we're like only maybe one or two or some small number of ideas away and a little
bit more scale from something we're like, this is now kind of different. And I think it's important
to stay vigilant about that. Is there a more modern like Turing test, we can call it the
Bartlett test, where that you think like, hey, when it crosses this threshold, I think when it's
capable of doing better research than like all of OpenAI put together, even one OpenAI researcher,
that is like a somehow very important thing that feels like it could or maybe even should be a
discontinuity. Does that feel close? Probably not, but I wouldn't rule it out. What do you think the
biggest obstacles that you see to reaching AGI, it sounds like you think maybe the scaling laws
have runway currently and holding for the next couple of years? Yeah, I think the biggest obstacles
are new research. And you know, one of the things I've had to learn shifting from like
internet software to AI is research does not kind of work on the same schedule as engineering,
which usually means it takes much longer, it doesn't work, but sometimes means it works
tremendously faster than anyone could have predicted. What is that? Can you elaborate on
that point that it's like, not as linear in progress? I think the best way to elaborate on
that is like historical examples. I'm going to get the numbers wrong here, but I'm sure no one
will try to correct you. Someone will. I think the neutron was first theorized in the early 1900s.
It was maybe first detected in the 10s or 20s, and the work on what became the atomic bomb
started in the 30s and happened in the 40s. From not really having no idea that there was
even the idea of a thing like a neutron to being able to like make an atomic bomb and just like
break all of our intuitions about physics. That's like wildly quick. There are other examples that
are sort of less pure science. Like there's the famous quote about the Wright brothers, again,
I'm going to get the numbers wrong here, but let's say it was like 1906, they said they thought
flight was 50 years away in 1908, they did it, whatever, something like that. And then many,
many other examples throughout the history of science and engineering. There's also plenty
of things that we theorize that never happen or take decades or centuries longer than we thought,
but sometimes it does go really fast. Interpreability, where are we on this path,
and how important is that long-term for AI? There's different kinds of interpretability,
so there's the like, do I understand like every, what's happening at like every mechanical layer
through the network? And then there's, can I like look at the output and say there's a logical flaw
here or whatever? I am excited about the work going on at Open AI and elsewhere in this direction.
And I think that interpretability as a broader field seems like promising and exciting.
Yeah, I won't pin you down. I assume you'll have a nice announcement when you're ready to say something.
But do you think that that is going to be a requisite to mainstream AI adoption maybe within
enterprises or something? GBT-4 is like quite widely adopted at this point.
Yeah, yeah, that's right. There's maybe a few things that I think you could ask questions about
or maybe accuse is too strong of a term, but that people are suspicious about, one of which is
I think there's this needle-threading that exists between being excited about AGI, but also feels
like you have a personal kind of apprehension about you, Sam, Open AI, generally being the
ones to harness it and unilaterally make decisions, which has led to some, you know,
some body, some governmental structure where there's elected leaders instead of you
making these decisions. Yeah, I think for like, I think it'd be a mistake to go regulate, heavily
regulate like current capability models, but when the models, which I believe they will,
pose significant catastrophic risk to the world, I think having some sort of oversight
is probably a good thing. Now, there is some needle-threading about where you set those
thresholds and how you test for it, and it would be a real shame to sort of stop the
tremendous upsides of this technology and letting people that want to go train models in their
basement be able to do that. That'd be really, really bad, but you know, if we have international
rules for nuclear weapons, I think it's a good thing. The regulatory capture group, which I'm
sure we can think of, which PCs fall into that bucket of accusatory around this regular regulation,
what do you think they don't see about the potential risks inherent in AI?
Well, I think they just don't get, I don't think they're like on the whole seriously wrestled with
AGI. These were also people who like, some of the loudest voices about AI regulatory capture were,
you know, totally decrying it as a possibility not that long ago, not all, but I do have empathy
for where they're coming from, which is like regulation has not been really bad for technology,
like look what happened to the European technology industry. Like, I get it, I really do, and yet
I think that there is a threshold that we are heading towards, above which we may all feel
a little bit different. Do you think open source models themselves present inherent danger in
in some ways? No current one does, but I could imagine one that could. I've heard you say that
safety is kind of a false framing in some ways, because it's more of a discussion about
what we explicitly accept, like airlines. Yeah, it's more like safety is not a binary thing.
Like you are willing to get on airplanes because you think they're pretty safe,
even though you know they crash once in a while. And what it takes for to call an airline safe is
like a matter of some discussion, some people have different opinions on. And it's a topical point
right now. Topical point right now. They have gotten just unbelievably safe overall, like triumphantly
safe. But safe does not mean no one will ever die on an airplane. Similarly, medicine, we really
speak with side effects and some people have adverse consequences around it. And then there's
the implicit side of safety as well, like social media, right, or things that have negative association.
Is there something that you could imagine seeing on the safety paradigm that would
cause you to act differently than pushing forward? Yeah, we have this thing called our
preparedness framework. That's sort of exactly that saying that, you know, in these categories
at these levels, we'd act differently. I've had LA's are on the podcast. How was that?
It was wonderful. We sat for the longest podcast I've ever done. I think it was four hours of us.
He has more free time than me, so I apologize, I can't go that long.
Listen, we can do multiple sessions. We don't need to do them all now. I think that
his points, I think, stay fairly consistent. I'm grateful he exists.
He's a very interesting guy I sit down with for four hours and talk. We went a bunch of different
directions. But I'd be remiss as a friend of the pod, did not ask a fast takeoff question.
I'm curious, like, there's so many different fast takeoff scenarios. And one of the
constraints that I think we point to today is just a lack of AI infrastructure, right?
And I guess if there was some researcher who developed a modification to the current
transformer architecture, where suddenly the amount of data and hardware scale needed drastically
reduced more like human brain or something like that, is it possible we could see like a fast
takeoff scenario? Possible, of course. And it may not even need a modification. It is
still not what I believe is the most probable path, but I don't discount it. And I think it's
important that we consider it in the space of what could happen. I think things will turn to be
more continuous even if they're accelerating. I don't think we're likely to go to sleep one day
with like pretty good AI and wake up the next day with genuine superintelligence.
But even if the takeoff happens over a year or a few years, that's still fast in some sense.
There's another question about even if you got to this like really powerful AGI,
how much does that change society on the next day versus the next year versus the next decade?
And my guess is in most ways, it's not a next day or next year thing, but over the course
of a decade, then the world will look quite different. I think the inertia of society is
like a good helpful thing here. One of the things I think people also find
they have suspiciousness around. I imagine the questions you don't love
getting are Elon, equity, and November board structure. Those are probably the three...
Which one of those do you like the least? I don't hate any of them. I just don't have
any new to say on any of them. Well, I guess I'm not going to ask the equity one specifically,
because I think you've answered that in more than enough ways. Although people still don't seem
to like the answer that enough money is a thing. Yeah, if I made a trillion dollars and then gave
it away, it would fit with I think the expectation or the sort of way it's usually done. There's
another saying I thought about. Oh, that's true. Trying that in some ways. Yeah, comparatively.
No, I just mean like most people who make a ton of money. Yeah. What do you feel like your
motivations, this pursuit of AGI, outside of the equity, I think most people take solace in the
fact that like, oh, well, even if I have some higher mission, I still get paid for it in some
ways. What are your motivations now coming into work every day? What's the most fulfillment derived
from? Look, I tell people this all the time, I'm willing to make a lot of other life trade-offs
and sacrifices right now because I think this is the most exciting, most important,
like best thing I will ever touch. And it's an insane time. And I'm happy it won't be forever.
Like, you know, someday I get to go retire on the farm and I'll remember this fondly,
but be like, oh man, those were stressful, long, long stressful days. But it's also just incredibly
cool. Like I can't believe this is happening to me. It's like this is like amazing.
Was there a single moment, I guess we go back to the fame example of not being able to go out in
your city or whatever, but has there been a single moment that was most surreal that like, oh geez,
I don't know. I mean, you've done a podcast with Bill Gates. I'm sure you have your speed dial.
If I took your phone right now, it would have a lot of very interesting people on it. Was there a
single moment over the course of the last couple of years that you were like, this is a uniquely
surreal moment? And kind of every day there's something that's like, wow, if I could like,
if I had like a little bit more mental space to step back, it's like, this would be crazy.
Kind of efficient water. But yeah, it is kind of like that effect.
After all of that, like, November stuff happened that, you know, like that day or the next day
or whatever, I got like, I don't know, 10, 20 texts, I mean, like that from like,
major world like presidents, prime ministers of countries, whatever. And that was not the weird
part. The weird part was that happened. And I was like, you know, kind of responding saying like,
thanks for whatever. And it felt like very normal. And then we had these like insane,
super jammed like four and a half days and just this like, crazy state. And it was just like,
weird, like, not sleeping much, not really eating. Energy levels like very high, very clear, very
focused, but just like your body was like in some weird, like a adrenaline charge state for a long
time. And then it was like, all this happened that week before Thanksgiving, it was kind of crazy,
crazy, crazy, got resolved on Tuesday night. You canceled our podcast.
Canceled our podcast. Sorry, I don't usually cancel things. Anyway, then on that Wednesday,
like now it's the Wednesday before Thanksgiving, Ali and I drove up to Napa and stopped at this diner.
God, it's very good. And on the journey up there, I realized I hadn't like eaten in like days.
And then all of a sudden, like kind of like normal, it was just like, okay, you know,
this is like normally where we'd be doing on weekend, heading out like, whatever. And
go to God's order like four entrees, like heavy, like, you know, fried, like heavy entrees,
like two milkshakes just for me. And I was sat there and ate. And it was very satisfying.
And as I was doing that, one of them, president of this one country texted again and just said,
like, oh, I'm so happy it's all resolved like great, whatever. And then it hit me that like,
oh yeah, like all of these people had texted me and it wasn't weird. And the weird part was like
realizing that that had like happened in the middle of it. And that that should have been
this very weird experience. And it wasn't. So that was like one that sticks out.
Yeah, that is interesting. My takeaway is human adaptability to almost anything is just like
much more remarkably strongly realized. And you can get used to anything as the new normal,
go to our bad pretty fast. And I kind of like over the last couple of years have
learned that lesson many times. But I think it says something remarkable about
humanity and good for us and good as we stare down this like big transition.
I remember post 9-11, I'm sure you remember exactly where but I was in
some of New Jersey in our town, you know, whatever dozens of people passed away.
And the how close the town came together after a terrorist attack happened,
and it seemed so normal, like that it was just the normalcy of it, or I have friends in Israel
right now, and you talk to them about it. And they're like, no, it's normal. I'm like, well,
there's a war going on, like it's got to be surreal. And they're like, well, I mean,
what are you gonna do? You go about your day, you go get your food, all that.
And it's amazing these psychological impacting things. At the end of the day, we need to go get
food and we need to, you know, talk to our friends and all this stuff. So it is amazing how much
that can happen. Really, like genuinely, that's been my big surprising takeaway to like feel it.
So this really, as you think about like models becoming smarter and smarter, what you kind of
touched on this a little bit earlier with the creative element, like what do you think remains
uniquely human as models start doing more and more capabilities of what we used to consider?
I think many, many years from now, humans are still going to care about other humans. I,
you know, I was reading the internet a little bit, and I was like, oh, everyone's going to
fall in love with Chatchapiti now. And everybody's, you know, like, it's going to be the Chachapiti
girlfriend, whatever, whatever. I bet not. I bet we're, I think we're so wired to care long-term
about other humans and all sorts of like big and small ways that that's going to remain like our
obsession with other people. Sounds like you hear a lot of conspiracy theories about me. You probably
don't hear a lot of conspiracy theories about AI. You might not care if you did hear one. I think
we're like not going to watch robots play each other in soccer probably as our like main hobby.
As you run open AI, the company itself, and you, you built a lot of rules or frameworks at YC
on how to run businesses, and then you've, you've broken a lot of them. Some. Some.
Are there, are there different types of people you hire for this
business than you would have had you started a consumer internet company within the executive
ranks or a B2B software company or something? Researchers are very different than my product
engineers for the most part. But Brad or Mira or some of the executives like researchers are
unique, but does open AI bring in a different type of executive or do you hire for a different
trade? So I mostly have not like, I am sometimes you hire externally for executives, but I'm a big
believer that if like, you generally promote, it's not, it's probably a mistake to only promote
people to be executives because that could reinforce a monoculture and, you know, I think you want to
bring in some new, very senior people. But we mostly like homegrown talent here. And I think
that's a positive given how different what we do is from what you would do somewhere else.
Is there a decision that you've made over the course of open AI
that, that felt the most important at the time of making it? And how did you go about making it?
It'd be hard to point to just a single one, but the decision that we're going to do,
what we call iterative deployment, that we're not going to go build AGI in secret and then
put it out into the world at once, which was the prevailing wisdom and the LA's are playing
in others. I think that was like a quite important decision we made. And it felt like a really
important one at the time. If another company that betting on language models was an important
decision and felt like an important one at the time, I actually don't know the story of betting on
language models. How did that come to be originally? Well, we were, we had these other projects we
were doing the robot thing and video games. And there was a very small effort started with one
person looking at, looking at language modeling and Ilya really believed in it, really believed in
the general direction became language models, let's say. And we did GPT-1, we did GPT-2,
we started the study scaling laws, scaled up GPT-3, and then we made a big bet this was what
we were going to do. And it was not, it looks so, all of these things look so obvious and
really don't feel that way at the time. One other thing you brought up recently was the,
there's two approaches to AI, the replication of yourself and then the smartest employee.
Oh, it's not, not AI itself, but like how you want to use it. Like when you imagine using
your personal AI. So there's a subtle distinction when you said it, but can you, can you expound
it? Because it seemed like a fairly profound distinction of how at least Sam thinks about
the, the future of AI use cases. So can you explain that point again? Cause clearly I misunderstood
it. If you're going to text me in, you know, five years in the future, I think you want to be clear
of whether you're texting me or my AI assistant. And then if it's my AI assistant that's going to
like, you know, bundle messages together and you'll get a reply later or, or, you know, if it can
easily do something, you might ask my human assistant to do it and fine, you'll know that.
I think there will be value in keeping what those things are separate and not that it's like,
all right, the AI is truly just an extension of Sam. I don't know if I'm talking to Sam or Sam's
AI ghost, but that's okay. Cause it's the same thing. It's this merged entity. I think, I think
there will be like Sam and Sam's AI assistant. And also I want that for myself. Like I don't
want to feel like this thing is just like this weird extension of me, but that it's a separate
entity that I can communicate with across a barrier. You see it in, in music or creative
where it becomes pretty easy to replicate a Drake or a Taylor Swift audio. We probably need some
form of validation or some centralization that validates, say this is actually the creative
work of XYZ person. You're probably going to want some version of that at a personal level too.
Yeah, but it's like, you know, the way I think about like open AI is it's, I don't like,
there's different people and I'm asking them to do things and they go off or they ask me to do
things and I go off. But it's not a single like board. And I think that's like a way we're all
comfortable. And so, so what is that? Can you, can you tie that back? Like the, the decentralization
of, of letting individuals do their, not, not, well, also that, but I meant more just kind of
like, what is the abstraction of what my personal AI is going to be like? Like, do I think of that
as this is just me and it's going to like take over my computer and do what's best and because
it's me, that's going to be totally fine. It's answering messages on my behalf and it's, you
know, going to just like, I'm slowly going to like take my hands off the controls that it's
slowly going to like be me. Or do I think of this as like, this is a really great person I work with
that I can say, hey, can you do this thing you're back when you're done? But I think of it as not me.
As you think about the educational system and as we think about like the class of,
college class of 2030 or 2035 or whatever, whatever, some, some group in the future,
are there changes specifically that you think should be made within the college educational
system to prepare people for the future we have? The biggest one is I think people should not only
be allowed but required to use the tools. There will be some cases where we want people to do
something the old-fashioned way because it helps with understanding. You know, like I remember
sometimes in math class or whatever, there'd be something you can't use. No calculators on the test.
Yeah. But on the whole, like in real life, you get to use the calculator. And so you need to
understand it, but then you got to be proficient in using the calculator too. And if you did math
class and never got to use the calculator, you would be like a less, less good at the work you
need to do later. You know, if all of the open AI researchers never got to use the calculator,
open AI probably wouldn't have happened, computers at least, you know. We don't try to teach people
not to use calculators, not to use computers. And I think we shouldn't train people not to use AI
either. It's just going to be an important part of like doing valuable work in the future.
Last one. In planning for AGI and beyond, you wrote the first AGI will be just a point
along the continuum of intelligence, which we spoke about earlier. We think it's likely the
progress will continue from there, possibly sustaining the rate of progress we've seen
over the past decade for a long period of time. Do you ever personally stop and process or visualize
what the future will look like in that? Or is it just too abstract to contemplate?
All the time. I mean, I don't visualize it like, you know, we have these like flying cars in a
Star Wars future city and I like that. But like, definitely what it means when one person can do
the work of hundreds or thousands of well-coordinated people and what it means when,
I don't want to say we can discover all of science, but kind of what it feels like,
like what it would feel to us is if we could discover all of science.
Be pretty cool. Yeah. Sam, thanks for doing this. Thank you.
Thank you for listening to this episode of The Logan Barlett Show with CEO and co-founder
of OpenAI, Sam Altman. If you enjoyed this conversation, really appreciate it. If you
like and subscribe and share with anyone else that you think might find interesting,
as well as come back for next week where we'll have another exciting episode with a different
founder and CEO of an important company in technology. Thanks everyone for listening and have a good week.
