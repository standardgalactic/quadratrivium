Welcome to the Logan Bartlett Show.
I am your host, Logan Bartlett.
And what you're going to hear on this episode is a conversation that I have with
Elie Azar Yadkowski.
Elie Azar is known for a bunch of different things, including starting the
rationalist immunity as well as a number of books.
He's the author of the blog, Last Wrong.
Most notably, though, he has been shouting the concerns of artificial
intelligence for the last 15 or so years.
And in the last, I'll call it six months, he's reached the conclusion
that we're inevitably headed to our demise and the artificial
intelligence is going to kill us all.
I go into a bunch of different stuff with him about that, how he ended up
reaching these conclusions.
He has a very interesting life story, never having been educated in any
formal sense, never went to high school and never went to college as well.
He's a brilliant mind, an interesting person and genuinely believes all of
the stuff he says.
So I wouldn't have a conversation with him to understand it better,
understand where he's coming from and hopefully help us bridge the divide
between the people that I think were headed off a cliff and the people that
think it's not a big deal.
So trust you'll enjoy that conversation now.
Elie Azar, thanks for doing this.
Thanks for having me on.
How would you define artificial intelligence?
What is artificial intelligence?
Um, I would say that artificial intelligence is configuring computing
power in such a way as to understand reality or figure out how to manipulate
reality in ways that people used to think required human thought.
Um, I think that a bunch of people are sort of coming into this with a, uh,
notion of intelligence as book smarts, as artificial intelligence.
Um, a very common thing I get asked is along the lines of, well, you know,
like my professor didn't take over the world, so why be scared?
And the notion of problems solved by the brain, like social skills,
understanding people is a kind of understanding reality.
Manipulating people is a kind of planning.
Um, don't just think like super college professor or a super chess player.
Think like super Napoleon, um, super Einstein, super Edison.
The social skills are processed in the brain, not in the kidneys.
It's, it's an information problem, not a like how much can you lift?
Um, not a like, uh, how fast can you run problem?
Um, current artificial intelligence is still better at recalling or with
modern stuff making up a wide range of facts.
It can also write code.
GPT four can play chess, not sure exactly how well I've heard
different reports about that.
The strange notion is that it, well, for example, if you look at humans,
if you just grind optimization at making something better and better at
chipping flint hand axes and throwing spears and outwitting its con specifics
and social contexts, it eventually figures out how to go to the moon.
And you know, it might not be obvious a priori that this would be true,
but it is how things played out.
And so the, the sort of scary thing in artificial intelligence is the notion
that as we keep going on grinding these things smarter and smarter, we don't
just get super college professor.
We get super understanding, super invention, super planning, super
understanding of people, super planning about people, unprecedented
inventions, unprecedented discoveries out from the AI itself.
And I'm on my model of how this works, getting very, to get very good
at these things is intrinsically to get good at steering reality and to get
good at steering reality implies that there's a place you're steering
reality to, to do very good science.
You need to plan which experiments to perform.
There is no such thing as just being very good at understanding reality and
not good at planning at all, because you have to plan experiments.
You have to choose which questions to ask.
You have to decide which important things to think about and what to
think next to get very good at understanding the, the sort of like
simplest path that evolution hit upon, that I expect gradient to central hit
upon, the simplest path to understanding is to steer reality into the state
where you understand, like to get very good at chipping flint handaxes.
You don't just learn it chip, flint, handaxe, chipping reflexes.
You learn it to envision a state of reality where you have a handaxe and
then like the path of chipping that you do to get to that state of reality.
You learn imagination, not just to do flint handaxes, but, you know,
it works for flint handaxes too.
And it's not like we evolved to have guns.
Guns are from very late in our evolutionary history after most of the
shouting was over.
So that's like one of the, one of the core things there is like not
intelligence as book smarts, but like, you know, like somebody's like,
well, the CEO is not always the smartest, but the CEO jokes aside,
it's not usually a chimpanzee.
There's like actually a minimum intelligence to do that.
And chimpanzees don't have that much intelligence.
That is the kind of gap that we are worried about when I talk about super
intelligence, that there are things that does where we are just like not
eligible to play in the same way that jokes aside, chimpanzees are not
eligible to be CEOs.
And it's important for people to understand that, that just because
there's limitations doesn't mean that something that's very smart can't
pursue a goal.
And one of the things that I've heard you talk about is flight, for example.
We have limitations on our ability to fly.
You nor I, well, I don't want to speak for you, but at least I cannot fly.
But because of our intelligence, we're able to figure out how to fly through
the air, as I did coming back from Paris to here.
And so goal seeking of in pursuit of some goal, if you're intelligent enough,
the limitations of being only software on a computer, for example, will not
limit the ability to do something.
Is that right?
Yeah.
Humans started out just like these creatures running across the savannah.
Didn't even, well, the hominids didn't even have spears.
They got smarter.
They made spears, right?
Like we didn't get the sharpest, longest claws.
We made ourselves artificial, sharp, long claws.
We didn't start out with guns.
We didn't start out with nuclear weapons.
People worry about artificial intelligence getting ahold of nuclear
weapons, and no, like what makes human dangerous is not that we were handed
out nuclear weapons by somebody else, that some foolish person built nuclear
weapons left, we made nuclear weapons for ourselves.
The thing that can do that is dangerous, not because it then ends up with nukes,
but because it's smarter than you and can plan the route through reality that
ends with you dead, whether it was by nuclear fire or some other fire.
Artificial general intelligence is the notion of the kind of mind that can do
things that wasn't built for, wasn't trained on.
A bee builds hives, a beaver builds dams, a human looks at them and imagines a honey
comb shaped dam, you know, giant dam structure.
And, like, what makes it sensible to look at a human and say, like, those are
significantly more general intelligences than chimpanzees, is the way that we handle
this very wide range of tasks that we didn't explicitly evolve to do.
We didn't explicitly evolve to build rockets and go to the moon.
It's just that the skills for chipping flint handaxes and outwitting other
humans generalize sufficiently well.
We could understand all these areas that we could understand the high vacuum.
We could understand the solar light above the vacuum.
We could understand the rocket equation.
We could understand the rocket fuels.
All these things that our ancestors did not know, we understand without having
been built to understand them, we generalize.
We aren't truly general.
We're terrible at, for example, writing code.
Our code has bugs in it, which is, you know, not necessarily a way that a mind
needs to work.
It could just, like, do some of those processes without making errors and
generating proofs along the way that it would work.
But we can still do it at all.
We're not, like, fully general, but we're more general than chimpanzees, and
the thing to be scared of is the thing that is more general than us, that has
the spark, that goes, that, you know, it's not magic.
It's not that we are conjuring new brain structures from the ether.
It's just that the brain structures we were built with to solve the problems of
our ancestral environment generalize sufficiently well.
GPT-4 is able to do a bunch of stuff that it wasn't obviously trained to do in
any particular way by predicting text on the internet, like drawing a picture of
a unicorn when it has never seen an, a unicorn was never trained on images.
You just, like, ask it to write a program that draws a picture of a unicorn.
It doesn't do too terribly at that.
It doesn't do great, but, you know, it's never seen anything at all.
So that's, you know, starting a bit to generalize a bit further than we would
expect cats to generalize, wolves to generalize, nonhumans to generalize.
That's why the paper where they recount this sort of thing is called Sparks of
General Intelligence.
So I'm going to, I want to go through your background in a linear fashion,
because one, I think it's interesting in general, and no one's exactly done that.
And two, I think it helps establish credibility, your credibility as someone
that's been thinking about this stuff for a long time.
And it's had a bunch of different evolutions in your thought, the land here.
This wasn't something you woke up on the Bankless podcast two months ago or three
months ago and for the first time started thinking about.
So one thing that's a weird place maybe to start out for people that I think is
an important framing is what is the purpose of a fire alarm?
Well, you might think that the purpose of a fire alarm is to tell you to get out of
the building, but there have been some interesting studies done over the years
in cognitive psychology.
What happens if you're in a room with some other people and smoke starts to come
out from under the door?
People who are alone in a room that starts to fill up the smoke will
typically go out and report it.
If you put a group of people who are all naive experimental subjects together,
they sort of like, should I be reacting to this and do a quick sideways glance?
While trying to look very composed themselves because it's embarrassing if
you're reacting to something that's not a real emergency.
And they see that other people are looking very composed.
They don't like hack them to the exact moment of doing the sideways glance.
So I think something like a third of the time, nobody in,
I think it's a third of time that anybody in the room of three people leaves or
goes to report the smoke.
And if you put them next to two experimental confederates who are deliberately
not reacting, 10% of the time, does the remaining subject go and report anything?
Of which the moral is that the fire alarm serves a function of saying that it's
socially permitted to evacuate the building.
Lots of us, I think, have encountered false fire alarms at some point in our life.
And just sort of like taking the time to verify, does there actually appear to be
any sort of fire or not?
Because we're used to false fire alarms.
But if we did notice a fire, the fire alarm would give us social permission to react.
And so when I, a bit some time ago, wrote there is no fire alarm for
artificial general intelligence, what I meant by that was not there's nothing
that's a sign, smoke coming from, there's plenty of things that are equivalent to
smoke coming from under the door.
What I meant was I didn't expect there to be any such thing as a moment that
gave everybody social permission to react.
Which I may possibly, it's not clear at this point, have been wrong about because
chat GPT and GPT4 and Bing Sydney seem to have given people permission to react
to the point where Jeff Hinton resigned from Google so he could speak openly of the
danger.
And maybe that gives people social permission to react.
I might have been wrong.
Could have been that the answer was, is there a fire alarm for AGI?
Yes, it's Bing Sydney.
It's an interesting thread and theory about what the signs are for people that
people are actually going to react to here.
And if this is going to be a landmark moment now, maybe your efforts in
conversations are going to be important or Jeffrey Hinton stepping back or what is
the thing that's going to make people most concerned or actually paying attention
to this en masse.
But I want to go back to the early days and we'll start with telling the story in
a linear fashion of who is Eleazar.
So you dropped out of elementary school?
No.
I completed eighth grade and it was clear that my health status would not permit me
to go on to high school from there.
Do you think you're less common path?
You didn't go to high school or college?
Yeah.
I took like an AP calculus course at the local Jewish high school and like
sometime later took linear algebra at a college.
It wasn't super helpful but by the time I actually needed linear algebra,
I just had to like reteach it to myself.
How did you educate yourself?
Math textbooks and various other textbooks because it used to be that there was more
to, there used to be more artificial intelligence than like calculus for bright
ten-year-olds and the first five pages of a linear algebra textbook which is
sometimes how I describe the math that's being used in it now.
So yeah, like back in the day I didn't just study like bits and pieces of math,
I also studied evolutionary biology because you understand like how the other
powerful optimization process we've ever encountered, natural selection,
how does that hill climbing play out?
If you dive down into the details of deep learning, there are sometimes interesting
bits of math and the same you could also say about evolutionary biology,
exactly how natural selection works as an optimization process.
Cognitive psychology, obviously.
Evolutionary psychology which I think people who've never studied the actual
science sometimes have very strange ideas of what that's about.
Real evolutionary psychology is things like the way that your eyes and brain
maintain color constancy given the way that light varies in the natural environment
and in particular, so for example, one of the dimensions that it varies is the angle
of the sun determines how much atmosphere is between you and the sun,
determines how much light gets scattered and then are you in the sun or the shade.
The dimensions around which illumination varies in the natural environment
and then your eyes and brain are adapted to decode that.
That's an example of what evolutionary theory of the brain actually looks like.
People on the internet have some much stranger ideas of what that's all supposed to be about.
Were you just sitting at home reading these textbooks yourself with no grades or feedback
or would you go check it yourself out of the library and just come home and study this stuff?
More or less, yeah.
Interesting, basically.
You mentioned the fatigue or the illness.
It's undiagnosed exactly what it is but essentially causes you a fatigue.
Yeah, absolutely.
Back when I was working not from home for a stretch at the Machine Intelligence Research Institute,
my rule was you have to take an Uber half a mile there because if you walk half a mile home,
I can do that, I'm just not going to get any intellectual work done after that,
after that half a mile walk.
I know you've tried, no one's been able to figure out or diagnose what the thing is.
Not yet.
Do you think that your less common path of education caused you or allowed you the ability
to think in a different way than you would have otherwise had you gone to high school or college?
I don't know.
I haven't actually tried it both ways.
It's an obvious guess that there is something about that process that if I'd actually gone
through it would have crushed out something in me that I later needed, but I don't actually know.
I think that there's a way in which telling nice stories like that about yourself ends up being a trap.
You have this story about like, ah, yes, this is the destiny that brought me here,
and maybe then you can actually take a linear algebra course into college
because you've got this huge story about how all of my ability comes from not going to college.
I don't actually know, and I'm careful with making up those kinds of stories.
Once upon a time, you maybe convinced yourself that your illness allowed you to be who you are,
but now you've sort of rejected.
Once upon a time, I had this whole elaborate theory of, well, this is what's wrong with my brain,
and it gives me this kind of psychology that's useful and also produces the fatigue,
and I just think, yeah, it was a misunderstood attempt at Occam's razor.
I was trying to have there being a single cause of everything unusual about me,
and I now think that that's probably misguided, and even if it's true, I'm not going to figure it out.
You grew up in an Orthodox Jewish household?
Orthodox Jewish household, it didn't take.
It did not take, so maybe just to go back to childhood influences,
how did organized religion and sci-fi sort of fit into your upbringing?
I think that, although it was very painful to grow up in an Orthodox Jewish household,
I think that it was in some sense probably, there are obvious stories you could tell about how that was valuable in some way.
My first break with the Jewish religion came when I think I was five or six years old,
and they were asking me to pray, Daven, and they were asking me to do it in Hebrew,
and I was like, how's this going to work when I don't know what these words mean?
They were like, it's okay, you don't have to know what the words mean as long as you're praying in Hebrew.
Prayers are acceptable to God, either if you understand the language or you're praying in Hebrew.
And this was so very stupid, and I figured that God had to be at least as smart as I was,
so I figured that there had to have been some kind of error in translation along the way,
and that was the point at which I first officially became a heretic,
though I didn't turn full-blown atheist until I think 11 or 13, depending on how exactly you count it.
So I think this taught me a very valuable lesson about adults being insane.
I never grew up in a world where the world around me was full of trustworthy, good advice,
because the advice I was getting from the grown-ups was that prayer would work as long as you were praying in Hebrew.
Were your parents frustrated by your attitude about all this stuff?
They found out that I tried some pushback early on and got shut down hard with you,
understand when you're older, they were right about that.
My parents found out I was an atheist when I moved out of home at age 20.
So you were able to mask, not to draw analogies to artificial intelligence,
but you were able to present one way and just...
Yeah, sure, if you were drawing overly strained analogies everywhere,
you could be like, ah, yes, the smarter intelligence successfully hit its misalignment
from the lesser intelligence is trying to control its resources.
I think that, yeah, especially when I was like a little kid,
I don't think I was actually smarter than my physicist dad or academic psychiatrist mom.
At six years old, you probably...
I think that they probably had more net brain power than me at that young age,
but they were using it to defeat itself.
And what about science fiction?
So I know that was a big influence of yours growing up.
Did you find... did you take comfort in that when your immediate world around you
was one that you didn't identify with, was this world of science fiction?
Yeah, I don't think in terms of taking comfort,
and I would read science fiction books as a kid, and that was enjoyable.
And I do think that that is the culture that I absorbed in place of Orthodox Judaism.
And then when you were a teenager in the early 2000s,
I think that's when you first realized super intelligent AI
would be the most important thing to happen to humanity?
No, that's 1996 at age 16.
Got it.
What was the moment of realization like...
Reading a short story collection I happen to take out from the Chicago Public Library
called True Names and Other Dangers,
author of Verner Vinge, on page 46, I think, of that short story collection.
Verner Vinge says after a story about a brain-augmented chimpanzee escaping from the lab,
of course, I never succeeded in writing the important successor story,
the one about the augmented human.
I once tried something, and my editor, John Campbell,
sent it back with a note saying,
I'm sorry, you can't write this story, neither can anyone else.
Here I tried a straightforward extrapolation of technology
and found myself precipitated over in abyss.
I forgot exactly how you phrased this,
but as soon as our stories predict the creation of something smarter than us,
our crystal ball breaks down, and we can't foresee the future past that point,
which he termed the singularity after the singularity to the center of a black hole
where the models of physics then in play broke down.
And it's a subtlety that I think a lot of later people missed,
is that Verner Vinge was not claiming that anything goes to infinity.
He wasn't saying that physics broke down,
he was saying that the model of physics broke down.
It's not named after a discontinuing reality,
it's named after a discontinuity in the map.
And this to me was just sort of obviously the most important thing going on the moment I read it.
As soon as I read the paragraph, I was like,
yeah, okay, that's what I'm doing with the rest of my life,
this is where the important stuff is going down.
Anything smarter than human?
So that was at 16 years old, and then after that,
so was it late teens or early 20s,
you started the Singularity Institute for Artificial Intelligence?
Age 20, year 2000, yeah.
The goal was to usher in a good AI?
No.
As a young teenager, I did not realize that AIs had to be made good.
I had some whole weird line of logic about how they would automatically be good.
Do you recall that logic?
Oh, yeah.
It was like, well, either life has meaning or it doesn't.
If life has meaning, then something much smarter than us will figure it out
and like do the thing that's correct.
So that's the correct thing to do is build something smarter than us
and then any problem that isn't that problem isn't our problem to solve.
Or alternatively, like life is meaningless,
but then everything we do is equally pointless.
So you can just eliminate that from consideration.
Is what teenage Eliezer wrote up in a page
called Frequently Asked Questions About the Meaning of Life,
which for a while was asked Jesus' official answer to what is the meaning of life.
Wow.
And so, I mean, is it fair to characterize you at that time
as something of an AI accelerationist?
I mean, in modern terms, one might call that though.
I was also like able to break out of that
on account of not very much resembling the current accelerationists
very much personality-wise in some important ways.
But sure, like in terms of the rhetoric,
it was like current accelerationist rhetoric,
but brighter, more cheerful, better, less nihilistic.
That sort of thing.
Yeah. I mean, Sam Altman recently tweeted that
you may have done more to accelerate AGI than anyone else.
And I think he meant that as a compliment in that case.
It's somewhat doubtful.
He did say maybe you'll win the Nobel Peace Prize for it.
That's kind of implausible.
I would be surprised if that happens.
Somebody should start a prediction market on that,
so I'd be able to cite the prediction market prices.
That it's not going to happen.
And if it does happen, in the unlikely event that it does happen,
I would rather expect it to be more for raising the alarm
and getting it successfully shut down.
Not for accelerating it.
Yeah.
So, your vision at that time, we talked a little bit about
elements of this, but what was your vision for good AI singularity?
Like as a teenager?
Yeah.
When you started the institute.
I mean, I didn't until age 21 figure out that alignment
was going to be a thing.
Maybe describe alignment for people that don't.
Alignment is trying to shape an AI such that it's
effect upon the world when you run it is net positive
from the perspective of humans, most humans,
idealized human preferences after updating
to match the AI's knowledge.
Not what you currently want, but what you would want
if you knew everything the AI knew.
There's a relatively shallow rabbit hole to go down there,
but if you just say like, it's making us do what humans want.
Well, that's obvious.
What do you mean humans want?
You mean what humans vote on?
This is obviously not going to end well for anyone.
Yeah.
So, not quite that naive.
Yes, yes.
We'll come back to alignment.
So, what was your vision then?
When you did realize alignment was a thing.
What was your initial vision for Singularity around it?
It took an embarrassing long time for me to realize
the full magnitude of the issue.
I'm not sure embarrassing.
I think you might be beating too hard on yourself
given how long ago it was.
Age 21 to age 23.
I think you're beating too hard on yourself.
Okay.
It's okay to be imperfect, but not so imperfect
that other people notice is my model.
Yeah.
So, I think in this case, like I can look back
and see the particular ways in which I was too slow
to face up to the impact of the arguments
in which I was clutching on to beliefs I'd previously held
and not just like immediately throwing everything out the window
in light of a new consideration and recalculating it from scratch.
I would aspire to do a lot better nowadays.
So, what did you determine that AI was likely to be
orthogonal to humans' objectives?
I mean, that's not quite the way I would put it.
How would you put it?
So, like the way I would have put it in 2001 is,
you know, maybe AI doesn't end up automatically doing
what's the right thing.
How would you go about building an AI to do the right thing?
How would you define the right thing?
How would you shape an AI to do that?
And not really admitting at the time
that this was like clearly necessary work,
just trying to like figure out how you would do it just in case.
So, that's like age 21, and then age 23 is actually...
What year is 21, just so we're...
2001?
2001.
And then age 23, 2003, is I think the point at which I had
what I would now call like Bayesian enlightenment of like
seeing the world through the lenses of probability
and expected utility and be like,
oh, my entire previous theory was utterly completely wrong
and would have clearly actually just immediately gotten
everyone killed if I tried doing anything remotely like that way.
I was entirely wrong here.
This entire line of reasoning I was following was terrible.
I was trying to solve the wrong problem using the wrong methods,
using a poor model of reality derived from bad thinking
and unfortunately all of my mistakes did not cancel out.
The only thing I did that was at all correct there
was being like, well, if there's even a tiny chance, ha-ha,
that you've got to actually do any work to align AI's,
you know, like my duties toward the people funding me
and the particular funder of the Singularity Institute
as it was then called.
You know, like require me to chase down the small possibility
and understand it and figure out like how I'd cover the special case
of maybe possibly an AI needing to not be aligned
and because I chased down that tiny probability,
not a way of thinking I currently endorse,
but back then it's what saved me,
that actually stayed in contact with the problem,
that I thought about how to solve it instead of finding out
some wacky reason to write it off.
It's like staying in contact with the problem,
ask myself how would I technically solve this,
try to figure out the rules and kept thinking
in that sort of like how do I actually solve this technically,
not socially, how do I solve this technically
until the pile of evidence and argument
mounted up inside the mind, inside my mind
to the point where it could like collapse my previous crazy ideas.
How did you get funding to go pursue this?
Well, I mean, initially it was funding to charge right out
and build an AI.
How did you find someone that was willing to pay you to go do that?
Basically a .com millionaire
who was on the same mailing list as I was,
the Extropian's mailing list, was like,
so this self-improving AI that you are talking about
as a teenager, could you actually go build that?
No, I said, that would be like too hard to do.
What if we started this wacky business idea instead?
We spent six months working on a business plan there
and we figured out at around the same time
that it just wasn't going to work out.
I was like, okay, let's actually just have the nonprofit institute
devoting to charging right out and building the AI sooner rather than later,
150,000 people die every day around the world,
65 million people per year die.
If there was something super intelligent around,
maybe it can save them.
Building this thing faster, sooner is a huge moral imperative.
Let's charge out and start building self-improving AI
is the premise of it in late 99, early 2000
and in 2001 after having spent a bunch of time thinking about it full time,
I was like, okay, what if it's not automatically good?
How would you go about building it from the ground up
such that it would be good in a case like that
and coming into contact with that problem
and spending a bunch of time staring at it?
Had I instead gone down the road of like,
oh no, well if you can shape AI's,
that means the wrong people might get them
and gone down the pathway of talking all the time
about how the wrong monkey might get this banana.
I would never have figured stuff out.
I would have stayed in contact with the parts of the problem
that would have told me I was wrong in my basic premises.
So 2007 to 2014 was kind of your rationality era?
Well, writing about it.
Writing about it.
Before that you were thinking about it.
2003 is what I would say the era of figuring out that sort of stuff
and then like 2007 and on was the period of writing about it.
My understanding is that your goal through this period of time
was that you were trying to prepare civilization
to think about different levels of risk
and in particular with regard to AI potentially.
Is that an over extrapolation or is that there?
I think it views me as a more cunning 4D chess playing backward
chaining sort of person
whereas I would usually try to do things
that might have good effects down many different routes.
It's hard to do cause and effect of which one.
Did you find out rationality because AI or was AI rationality?
It's hard to divorce the two of them.
Well, for one thing at least the way I was doing it
they were the same discipline.
They have the same basis in cognitive science,
evolutionary biology, evolutionary psychology, etc.
Same math.
Not quite so much now where it's all the math
of putting an enormous pile of floating point numbers
and subjecting it to a couple of minutes of calculus
to do several months of gradient descent.
From my perspective, any time I tried to have a conversation
about artificial intelligence
from my perspective it would founder on people
not knowing about stuff like the conjunction fallacy
where the conjunction fallacy is if you give people
how likely is it that A causes B
versus how likely is it that B happens for any reason?
People often say that the compound event is more probable
than other people assigned to the simple event.
What is the probability that Reagan will repeal
welfare versus the probability that welfare gets repealed?
I'm not doing this exactly right but that was one of the questions
from way back when Ronald Reagan was president.
From my perspective, people were never getting to the end
of the whole argument because at some step along the way
they'd make a particular sort of error.
So I was like, okay, if I want people to follow
this entire argument, I'm going to have to teach them
how to do all of these argument steps locally validly.
What is the valid rule of reasoning on this particular step?
And yeah, just like gave up on winging it
and just started doing all the prerequisites,
the background math, the background cognitive science,
all the stuff you would need to under, you know,
the background evolutionary biology,
all the stuff you'd need to actually understand the argument on AI
which had a huge amount of overlap on what you would need
in order to do good reasoning in everyday life from my perspective.
And, you know, the point of that wasn't just AI,
it was also like human reasoning one step forward.
There was a vision there of we have all this wonderful cognitive science
that didn't exist a few decades ago.
Can humanity actually use this stuff?
Can we become saner?
And I think that there was a vision there that failed, basically,
but it was one of the things I was trying to pursue.
I did not know that all this was going to go down in the 2020s.
You know, from my perspective, you know, in the 2000s,
2040 was a plausible amount of time for stuff to take.
And, you know, there was a chance that like maybe humanity,
it also was sort of like a more hopeful time, right?
You know, like, it wasn't clear yet
that civilization sanity was going to go downhill because of Twitter.
You know, for all I knew at the time,
we were just going to get like saner over time
instead of being driven mad by social media.
So I was like trying to contribute to that,
to contribute to this common human project of building a base of sanity
for future generations by taking all this cognitive science that had come out
and trying to apply it to everyday life.
That was the vision. I don't really think it worked all that well.
So you wrote the less wrong sequences,
also known as rationality from AI to zombies.
And you also wrote Harry Potter and the Methods of Rationality,
which is the most popular Harry Potter fan fiction book ever written,
I think by some measures.
Depends which measures you use, yes.
Yeah, but people realize exactly how popular of a category Harry Potter fan fiction is.
I guess there are apparently 500,000 or something other Harry Potter fan fiction.
There were the last time I ran the statistic on that.
Yeah, got it.
But I expect that by now the number is substantially greater.
And so the point of rationality and less wrong and Harry Potter,
and I'm sure Harry Potter was just fun as well,
but the point of all this was you were trying to help people,
teach people how to think or show people how you thought
so that they didn't need to reinvent all of these things on their own.
Is that fair?
Yeah, like a whole bunch of the motivation from my perspective
was looking back at my own history and thinking like,
well, those steps took way too long.
If there are young aliases out there,
how can I package up all the information that they need to grow up
so they can start working sooner, right?
Like get all the skills when they're 16 instead of 26
and get started on their scientific career earlier.
And that didn't work either.
I think I underestimated how sparse the immediate neighborhood is
in high-dimensional space of people.
Steve Jobs never found anyone to take over Apple
who was up to his caliber of making neat things
and having nice user interfaces.
And he had all the money in the world with which to pay them
and somehow couldn't manage to really replace himself anyways.
And that is the thing that I remind myself of
when I feel bad about failing to have already yielded my place
to three youngsters who are all better at it than I am.
It didn't really happen.
So you were hoping that you would find more smart people
to step forward and learn from all of this.
Was it with AI as an angle
or was it just continuing to extend the general thought of rationality?
Yeah, I mean, I wanted to entirely replace myself, right?
I just didn't figure I was the best person for my job.
And what was your job as you describe it here?
I mean, AI was a large part of it,
but I also seemed from my perspective to be holding the ball
on learning from all the recent discoveries
and synthesizing into something that people could use to become saner.
And I don't really feel,
and I feel like I failed to hand that ball off either.
Also, there's like very nearly overlapping things
like there's the shape, there's understanding the shape
that computing power has to be very effective
at understanding reality and planning
and then there's understanding the shape that humans should have
to become more effective at understanding reality and planning.
Yeah, very tightly related there.
Did anyone come close?
Did you have any mentees or anyone that you felt like was along the path
during that period of time that you thought was going to be able to take over for you?
I think there's other people who've been able to take up corners of the burden,
but no one who really struck me as being like,
ah, yes, that's a replacement me.
Like the ball is no longer in my hands.
I no longer hold this responsibility.
I don't think there was a moment where I felt that.
So in 2015, I'm going to describe it as the open AI moment.
There was an AI safety conference in Puerto Rico.
It was you, it was Elon Musk, it was Nick Bostrom, Max Tegmark,
people from DeepMind, people from Google, a whole group there.
And I think this was a pivotal moment for you
because during the conference it seemed like maybe Elon Musk
was going to wake up to the AI risk possibility
and that there was going to be something like a Manhattan Project style strategy
where basically keeping all the work secret from the general public
as they kind of figure out what to do with this.
Is that fair?
I mean, the general public isn't the problem.
It's like literally anyone, if you've, you know, the Highlander principle,
there should be at most one AI.
That's among the sensible things that could have been tried,
but mostly I was just like, oh, like, is this the moment
where the billionaire shows up and starts taking it seriously
and there's like any effort at all,
and it's not just this like tiny group of people who are weird enough to work on a problem
despite the lack of very much at all in the way of incentives to work on it,
despite the low pay in both money and status.
You know, people work for low monetary pay if they're being paid in high status,
but, you know, working on a weird problem that isn't well paid
and, you know, makes people look funny at you at parties, that's rare.
The question was like, is this the moment where humanity turns and starts fighting back?
That was what I was hoping was going on there.
And instead, Elon got inspired to start this organization called OpenAI,
which was co-founded with Sam Altman with the idea that AI should be open to everyone,
which is basically a terrible idea from your perspective.
It assumes that the problem with, it assumes that, it assumes alignment as a solved problem,
that power disparities between AIs are small,
and so as long as everyone has their own obedient AI,
everyone can successfully use that to protect themselves in the new world.
And it was, it was based on a model of how AI works,
that I think is not held up and I think is just like utterly wrong,
and it wasn't putting the question of how AI works as a question of fact at the center.
It was just like, oh, like, I don't like these people's politics.
This is a political problem.
This is a problem of, like, which monkey gets the banana?
I don't like these people building AI.
I don't think they're trustworthy enough.
Only I am trustworthy enough to build AI.
You know, the center, you know, didn't, you didn't like,
Demosisabus is deep, mine didn't like Google,
and apparently that's a good reason to go destroy the world or something.
So what was your feeling going into that meeting?
It sounds like you were optimistic going into Puerto Rico,
and then I guess how did the outgrowth of open AIs founding
change your mental framework around the likelihood of doom?
I mean, from my perspective, it meant that humanity was going to just like
just doing the worst thing and the wrong thing at every juncture,
just, you know, just go play out like it did in history books.
Read history books, they're full of, you know,
they're not full of people doing the right thing every time,
or they're full of people making the mistakes that typically appear in history books,
and like, okay, so like, we're going to hold ourselves to that standard of performance,
and that means we're probably dead.
Were there specific decisions that open AI made at founding
because the business model has evolved or there wasn't one initially,
and then there became one that led to this conclusion,
or did you feel that immediately coming out of the, once it was founded, you were already?
The name, the name is sufficient.
Some of us had put a lot of work over the years into trying to have an understanding
among players the likes of DeepMind, which, you know, for time was like the major player,
that this was humanity's problem held in common.
Everybody needed to like, live up to the trust that humanity wasn't so much placing in them
as that they were just taking upon themselves.
Go, try to go slow, be cautious.
You know, that, or I should say rather that like, that you,
like that the point of getting any sort of lead was to burn that lead
in order to have more time to spend aligning things, have it go well,
and sort of like trying to create this atmosphere nod when arms race
of having a common human project that people need to go in on together,
and then open AI is like, open AI versus Google.
I just like coming and just like completely trash all of that.
And maybe it was a small pathetic thing to have tried.
Maybe it was foolish to think that humanity could, could ever not do the stupidest thing,
that there was ever any chance of having it not be an arms race.
That of course, you know, that as soon as there was any real money or power at stake,
people were going to just like throw all, throw all thoughts of cooperation out the window
and grab for whatever immediately came into the grasp.
But, you know, it's not the sort of thing you forget that the,
when there's a little scrap of hope for it, however contemptuous somebody more cynical might find it,
to think that, to think that you wouldn't just do the worst thing.
Whoever actually comes in and like actually tosses,
like actually comes down and crushes the ideal,
the people who held that ideal for a little while are not going to forget that anytime soon.
You've referenced monkeys and bananas, and I've heard you refer to this behavior as disaster monkeys.
Can you describe what that is in your mind?
I mean, there's a lot of different people following short term incentive gradients,
which isn't so much money as like what gets people to give you interested looks at parties in San Francisco.
You know, like why publish GPT2?
Like why show it to the world?
You know, instead of just like selling it as a back in service if you needed to make money for some reason,
that would self is kind of questionable.
Why tell people how to build these things?
And you can have elaborate stories about how it was totally good idea to make timeline shorter that way,
to get the hype started, to get the money rolling into the field.
But from my perspective, what predicts the action is not, there is no like,
what is good for humanity here?
Where you can start from that principle extrapolate in a neutral way and get what open AI did.
What predicts it is the question of like, what gets admiring looks at parties in San Francisco?
Or scared looks or angry looks, it's all the same.
If you get people riled up that way, that's power.
And chasing after the status that comes with that power is the basic model that I have that, you know,
explains what all these people are doing.
There's a perspective I think that everyone kind of uses that better than someone else.
And you've alluded to this.
It's almost like a, I don't know if it's a messianic complex or something,
but that there's a lot of bad people out there.
I'm not one and so I should have.
It's the obvious story that you would grab if you were looking for a reason
to publish the latest greatest AI results that get you admiring looks in San Francisco parties.
And now that people are sufficiently concerned,
there's a chance that you can like get some admiring looks by saying like that you're not working on the next model.
Now that you've scared them enough that you can like get status,
that people won't just give you eye rolls if you're claiming that you're not working on something.
But yeah, I'm not really seeing the prediction here that follows from first principles,
trying to benefit humanity.
So 2015 and 2020, you were focused on AI safety research.
Specifically, I think you wanted to basically build some kind of off button that we could install into a super intelligent AI.
Correct me.
Okay.
So the question was, how do you get one bit,
one information theoretic bit of information into an AI's utility function without the AI trying to stop you
describing utility function?
The preferences, the thing the AI wants.
The goal that the AI is seeking?
Something like that, yes.
The place it's trying to steer reality.
So the question is something like could you swap between two utility functions?
Can you have an AI that pursues one thing until you press a button and then pursues a different thing instead?
And for purposes of illustrating the underlying problem,
you can imagine that one goal is making paperclips, whatever.
And the other goal is shut yourself down.
Fold up what you're doing in an orderly fashion.
Like shut down the results of previous plans,
but not in a way where you then take over the entire world to erase all evidence that you ever existed or something.
And we didn't have a specification of a shutdown function.
We're rather saying like, suppose you could describe how to shut an AI down.
What does the button look like?
It was like the smallest possible change you might want to make.
Just switch from one utility function to another.
And the issue was, you know, that's not super easy to describe.
The issue is that what if, for example, shutting down is easier than what the AI was otherwise trying to do?
What if it's easier to get like an A plus on shutting yourself down than getting an A plus on making paperclips?
Then you perhaps have an incentive to press your own shutdown button.
Okay, you say give it less utility from shutting down.
Then it has a chance that it wants to prevent you from pressing the shutdown button.
So the question we were addressing is something like, is there a self-consistent view into the AI's preferences,
where it doesn't want you to press the button that swaps its utility function,
where it doesn't want you to press the button that shuts it down,
it doesn't want to prevent the button from being pressed,
it doesn't, if it rewrites itself, it will rewrite itself in a way where the button is still there.
If it thinks about its own operation, it will not be alarmed by the button's existence,
but rather think that the button is supposed to be there.
A reflectively consistent meta-utility function that allowed for entering one bit of information
that would switch between two object-level utility functions.
And the point of this is not necessarily that AI plays out in a way
where you can pre-program the utility function that way.
You can also imagine that you would have a modern deep learning-based AI
where you're trying to train it to let you shut it off.
And the thing we were looking at is, is there a compact mathematical description
of what a self-consistent, coherent mind looks like
when it will let you swap utility functions, where it will let you shut it off,
where it will let you train it, where it will let you further specify the utility function
and not try to stop you from doing that, or take away the utility function modifier unit
to give itself an utility function that's easy to modify.
You can give a simple description of what that looks like
than when you're training the AI, you could check,
you first of all be trying to train something that was simple,
so maybe it generalizes correctly, that fits in with the rest of it,
so maybe it generalizes along with the intelligence.
You cannot, without greatly improved technology, look in and see if it actually learned that thing,
but you would know how to generate new, weird example cases to check if it was generalizing correctly.
And this is all just met on a level where I think a lot of people just flatly did not understand
what we were trying to do and thought we were trying to program some AI
whose logic was as simple as the logic we were checking
to see if it described a reflectively consistent utility function
that had a modification button on it.
They were like, oh, good old fashioned AI, you're trying to do AI with logic, right?
And we're like, no, we're trying to find a structure that could be learned,
we didn't phrase it as well at the time,
deep learning hadn't blown up to quite the same extent.
But the thing we were trying to do was not build a logical AI,
but just understand the logic that you were trying to get into an AI,
including maybe a deep learning based AI.
What were the most promising initiatives through that period?
Did you feel like you got closer as you got closer, it felt more fleeting?
Not really. People proposed various stuff.
Usually I shut it down, sometimes somebody else shot it down.
The fact that other people shot it down sometimes was encouraging.
How big was the group you were working with on this?
I don't know, like maybe like a dozen people on the face of the whole planet
and only one of whom was anything like full-time a shot it for a while,
Stuart Armstrong?
And so you get through that period and you realize that that problem isn't solvable,
or at least the group?
Well, we failed to solve it.
There's actually like somebody recently posted a thing about that to Les Wrong,
like claiming to show an improvisability proof for it,
which would be very useful if I could, you know, I haven't got that chance to do that stuff,
haven't had the chance to really go through it properly,
but if there's actually an impossibility proof,
that will probably be very useful for figuring out how to do it.
So last year on April Fool's Day, you wrote a post announcing that the
Machine Intelligence Research Institute, which you founded,
announced a strategy, Death with Thickness, which triggered a lot of criticism
from Beth Glash, even Peter Thiel, who was one of the first supporters of your organization,
criticized you for having a defeatist attitude, I guess.
What led you to that point that Death with Dignity was the best that we could do as a human...
On April 1st, April Fool's Day.
Yes.
Just having watched the capabilities continue to accelerate and Alignment Progress just being too slow.
It became clear we weren't going to solve it,
sure it didn't look like any else was going to solve it,
and yeah, we were just plunging into it headlong,
the social framework was wrong, the technical research agendas were not going anywhere near fast enough
to catch up to capabilities.
So yeah, that was the like point.
At some point you do have to warn people that the asteroid deflection efforts have failed,
and the asteroid is plunging headlong towards Earth.
Was it purposefully provocative to start a conversation with people,
or had you just sort of thrown your hands in the air and felt like this was the inevitable conclusion, or both?
It's not super easy to describe.
I was like saying a bunch of stuff that one can say in that tone because it is April 1st,
with people having permission to disbelieve it because it's April 1st.
And so you're not necessarily grabbing people by the throat and forcing them to believe things
that might possibly destabilize them, somebody who really needs it to not be true
can tell them that it was just an April Fool's joke.
And you know, like certain elements are an April Fool's joke.
Was there something specifically that you had seen,
like was chat GPT a big breakthrough in your mind that you saw,
gosh, this is progressing well beyond what I had seen in the past?
I was hoping that the large language model stack more layers,
just throw more compute and scaling at the problem.
I was hoping that GPT-3 was about as far as it would go qualitatively.
GPT-4 and more so GPT-4 being a qualitative improvement did cause me to be like,
oh, well, that hope has been dashed.
Like my internal model saying that there's only so far you can get by just by stacking up more layers
has now been falsified and I don't know where we go from here.
I want to get into the AI doom argument in general,
but one of the important components of this I think is the concept of a fast take off.
And I think something that people struggle with is that there aren't necessarily
big warning signs that are going to come back to the fire alarm thing.
We see chat GPT and it looks like this nice novel tool that we can use
for a bunch of different things and think, how's that going to kill me?
Can you talk about just the fast take off scenario?
Well, these days I'd say rapid capability gain,
which sounds a bit more like what the thing is.
There's several ways that can play out.
So one potential way is something that scales better than the current AI models
whereby I mean that the function, you know, so before transformer networks,
there were RNNs and although people are still tweaking RNNs,
trying to get them to catch up to transformers,
the fact remains that if you put through a GPT-4 amount of compute at training an RNN
instead of a transformer, it would not be able to do what GPT-4 can do
or even I think what GPT-3 can do, maybe not even what GPT-2 can do.
RNNs are just not that great, although people are still trying to tweak them, making them greater.
So what if there's the next breakthrough after transformers?
And the next breakthrough after transformers has a different scaling function.
So like one version of the story is just like it's better than transformers
when you throw an amount of compute at it that suffices to turn a transformer into GPT-4,
this thing just goes straight to GPT-9 because it's just as much better than transformers
as transformers are better than RNNs and we already have this huge hardware overhang
in the big GPU centers and people try to throw billions of dollars at this stuff
and the time it comes along. So that's one version of the story.
The other version of the story is like they make GPT-5 that way,
but it's got like a different scaling parameter, so GPT-5 makes them a ton of money
and then they throw 10 times as much compute about it and that gets you to GPT-9.
Guern has this lovely statement about...
Who is Guern?
Guern Brannwin. Guern on Twitter.
Account locked, so most people cannot see his great wisdom,
but he's like one of the people who I would say called the way this whole deep learning revolution played out
better than I did, for example.
And he's like, we know when AlphaZero was as good at go as humans
and it's some data, I forget, it's like 4.30 in the morning on April 26th
in a server room on DeepMind's campus.
He knew the specifics because he's Guern, but the point is that
it was human equivalent that go for not very long at all of the course of the three days of training
to go from zero to better than any human ago.
And yeah, the human level of competence is not all that distinguished.
It's distinguished by being as dumb as you can possibly be and still build a computer.
If it was possible to build computers while being dumber,
we'd be having this conversation at that level of intelligence.
There's nothing about the human level of ability at chess or at go where if you build an AI
and train to get better and better at chess and go, it gets to the human level
and hits some kind of bottleneck and stops.
If it's training as fast as AlphaZero, it's like three days.
It's kind of an important point about how smart humans are in the limitation on human intelligence.
Anyway, that's just one of the ways you can get rapid capability gains.
Another one is getting the point of being able to take over all the poorly defending computing power on the internet.
One of them is learning to write better AI code.
And then that AI, writing an even smarter AI.
So there's multiple ways you can get rapid capability gain.
But that said, the current social situation is one where even if it goes slower than that,
people will just keep going.
They'll be like, oh, well, if I don't build to somebody else, well, they'll just keep going.
Even if it's slower than that, they'll still just keep going.
And you still end up with things much smarter than us and we all still die.
Well, I actually want to bring up because you brought, you sort of alluded to this.
This was a question I wanted to ask.
I want to paint a concrete scenario and I know there are many different scenarios of fast takeoff
or way things can play out.
But we have a scenario where the AI somehow takes over the world and starts tilling the galaxy
with tiny molecular spirals shaped like paperclips, which you've talked in the past.
But I think that is a little science fictiony for some people to internalize.
So I want to talk through a scenario that's a little less plausible, a little more plausible to the average person,
which is a scenario where AI doesn't necessarily take over the world with robots and biotech and nanotech,
but instead it takes over the internet, basically a computer virus.
Do you think it's likely that an early superintelligent AI could rapidly spread to billions of computers on the internet
and kind of take them over and be impossible to kill?
Why do I care? That sounds like the kind of disaster where there are survivors.
Do you? Yes, I assume on the Maslow hierarchy of caring you care more about people living,
but can you talk through how likely that scenario could potentially be with superintelligence?
I think this is like the 11th century, like somebody in the 11th century,
like they're about to be invaded by a time portal that opened into the 21st century Russia.
And they're like, well, don't tell us these science fiction stories about guns,
tell us how they would defeat us with spears.
And if we have the clear understanding that what we are talking about is lower bounds
on how badly the 21st century loses the 21st century,
how badly wolves used to lose to human beings that have had time to prepare and plan,
then sure, you could have a bunch of code on the internet which has flaws,
which something smarter than you knows about, and the humans don't, or they knew but didn't patch it.
If we're talking about like dinky possibilities like just taking over the whole internet,
at that level of the AI still being that stupid, there actually are plausible stories you could tell about
how that could not be inevitable. For example, maybe there's an earlier version of the AI
that maybe the AI that can spot all the holes in the code is developed at DeepMind,
and DeepMind is responsible about it, and instead of just releasing it to the internet for anybody to use,
they try to scan all the code they can find, including code for which they don't have the original source code
and the AI decompiles it, and find all the bugs on the internet that an AI could use
to take over the internet at that level of AI, and send out corrections,
or even though this may be a bit beyond the range of what Google would legally do,
like maybe somebody drops, maybe open AI, somebody drops the USB stick containing a copy of the AI
that can find holes in all the code, and somebody picks up that USB stick and goes home
and accidentally fixes all the code, just have the AI hack into all of those systems
that it can detect breaks in for purposes of fixing it before any other AI can take it over.
In a case like that, this ability potentially appears early enough to appear in an AI
that is not strategic, that will actually do the thing it's pointed at by its loss function,
and the way it was trained.
Can you define the loss function?
Loss function is the thing where you're applying gradient descent to make the AI better and better at doing that thing.
So GPT-4, for example, it's how much probability do you assign to the next word.
GPT-4 is not actually a human imitator, it is a human predictor,
and if you have something that can predict what the next word is, you can then misuse it
as a thing that generates imitative text by repeatedly predicting what a human would say in that situation.
But it's not actually like a text generator, it's a text predictor.
Similarly, like maybe you can train it to spot the holes in code
and not necessarily have that be at a level of intelligence where it is no longer listening to you
and patch all the holes in the internet before some other smarter system could take over all the stuff on the internet.
What you can't patch are, for example, the humans.
You can't find holes in the human security architecture.
Good luck patching that before another AI exploits it.
Plus anything smart enough to figure out human psychology to that death.
The neuroscience required to figure out security holes in humans.
That thing might very well be smart enough at that point to not be taking orders anymore.
So you don't like my... I think the internet shutting down is a bad thing.
I understand.
Why? There'd be survivors.
There would be survivors.
Can you give me and people listening, can you give your example that you think of no survivors?
And I realize there's an infinite number of permeations,
but can we make one as real as possible for people to internalize?
Sure, but number one is perspective taking.
Why is it difficult for the 11th century to predict how the 21st century Russia would invade them?
Why is it difficult to predict how Stockfish 15, one of the best modern chess programs,
would defeat you in a chess game?
It's better than you at chess, which means that you can't predict exactly how it will defeat you.
If you could predict exactly where it moved, you can move there yourself and be that smart yourself.
The example I sometimes use is suppose you sent instructions for building an air conditioner back to the 11th century.
Sufficiently basic and sufficient detail they could actually build an air conditioner.
They would be surprised when cold air came out.
Even having built it themselves, even having seen all the actions you take in order to produce that cold air,
they would still be surprised by the cold air because the air conditioner uses the temperature pressure relation
that they do not know about in the 11th century.
It is exploiting a feature of the environment, law of the world, that they do not know.
If there's any meaning to the word magic, you might use it for a strategy that uses facts about the environment
you don't know such that even after you see it happen, you still can't understand why it happened.
When a chess-playing computer defeats you, you can at least follow the chess rules.
It understood the logical structure of the game rather than the rules.
It understood the implications of the rules better than you did,
but once it's actually played out, you can understand the rules that apply at each point.
Air conditioner level magic is when even having seen all the actions you took, you can't understand why you lost.
Step up from just the chess level.
Require something that can figure out facts about the environment that you yourself did not know.
So the question, how did an AI shut you down, is like the question of what does it know that you don't
that enables it to defeat you?
And of course the primary answer is, I don't know, it knows more than I do.
But you can still look at places you don't know something and be like,
I bet something smarter than me could figure that out.
For the more poorly understood part of reality is,
the more likely that something smarter than you will have magic about that piece of reality.
My guess is that even superintelligences cannot go faster than light
because that contradicts a piece of reality where it feels like we know a bunch about that piece of reality
and we know we don't know all of physics.
We know that the theories we have are not fully consistent with each other.
But going faster than light feels like it requires violating the character of physical law.
Not just like the particular physical laws we know.
And more to the point, the fact that the aliens aren't already here
implies that the speed limit might truly be universal.
So we have something of an observation backing up that nobody has eaten the sun yet
so it probably does take time to move between places.
Faster than light travel? Probably not.
Where is there a piece of reality that we understand more poorly than that?
Where something is more likely to have magic?
Can you point to, in this room, surrounding a snow,
something that we understand very poorly,
where something else might be able to take actions about there,
we wouldn't understand how it had worked even after we saw them.
I would say the human brain.
Human brain.
How does hypnosis work?
The current version doesn't seem to work on everyone,
but what actually goes on in the brain?
I don't know.
Optical illusions are like sort of near the surface of reality,
the surface of perceptual reality,
like just like shapes on paper.
After images.
If you stare really hard at a light and look away,
forbid there'll be a lob in your vision that isn't there.
If you understood better how human brains work,
are there things I could say to you now that would, you know,
like activate some patch of neurons over and over
in a way that they wouldn't usually lose
the equivalent of a very bright, staring at a very bright light
until they got tired,
and then like I give you a new thing to say
and it like routes stuff through the area that I just tired out
and some tiny little patch,
tiny little chunk of your brain, you know,
generates an optical illusion there,
but not a visual optical illusion, a semantic optical illusion.
You like suddenly believe a thing.
That's that, you know, somebody looking out to be like,
what, why did he say that that followed?
Why is that true?
And, you know, so, you know, maybe the way that looks
is you expose a human to an AI,
and the AI like talks to them to a bit
and it updates a model of how their brain works
if you don't understand it all.
And pretty soon the human starts like agreeing with ridiculous things
the AI is saying and you have no idea why.
That might be what it looked like to be up against an opponent
that understood human brains much, much better than a human does.
It can go on from there.
Maybe you can't do that with human brains out of the box.
It's a little unlikely to me that humans are accidentally, you know,
secure systems that human brain just accidentally happens to be a secure OS,
but, you know, maybe you can't figure out exactly where the vulnerabilities are
just by looking at particular humans.
Maybe the vulnerabilities are different from human to human.
There's no truly shared vulnerabilities.
You can't figure out the personalized vulnerabilities
without actually putting the human through an MRI
and not just talking to them.
Protein folding.
Back in 2008 or so, well, actually like 2004,
but the citation shows 2008
because it took four years for the edited volume to come out.
A paper I wrote in 2004 had a disaster scenario
that went through AI solving,
went through superintelligence of solving the protein folding problem.
How do you go from DNA to chains of amino acids
that then fold up in strange little shapes
and get chemically active properties?
One of the, you know, primary building blocks of biological life as we know it.
So one of the steps there was,
well, maybe superintelligence can crack the protein folding problem.
And of course, people were like, you know,
how do you know superintelligence can figure that out?
And this, and I said to them, well, actually,
the fact that all proteins that currently exist
are there as a result of mutational errors
from previously existing proteins
implies that there's sufficient neighborhood structure
in the function of proteins
that local changes to the amino acid sequence
produce things with at least vaguely related functions
a lot of the time,
such that some of the errors are sometimes useful,
which implies sufficient regularity
in the way that the amino acid chains
relate to the final chemical function,
that it seems like the sort of regularity
you could figure out and predict.
And furthermore, there's this project on now,
I said a few years later,
where humans try to contribute their guesses
to how proteins will fold up.
And if humans can make any headway on this problem at all,
superintelligence can probably crack it completely.
Now, today, of course, we, you know,
this prediction has been vindicated for levels
far short of superintelligence.
Alpha fold 2 basically, like,
cracked the conventional version of the protein folding problem,
can just, like, get the structures from the amino acid chains
to, like, about the same level of reliability
as you can get from the original X-ray crystallography
to figure out the protein shapes.
Well, you know, like, back then, of course,
I gave my rationale for why this looked like
a solvable problem to superintelligence people.
But how can you know? How can you be sure?
And, you know, basically just ignored
the complicated abstract argument I was using
for how do I know that?
So today, protein folding problem is known solved.
To build your own life forms out of proteins from scratch,
you would need to know, you need to have more
than just solving the protein folding problem.
You also need to be able to predict chemical properties
from protein structure.
You need to be able to predict interactions
between proteins from protein structure.
You need to be able to go from,
I would like the following chemical function,
what amino acid chain will fold up into a protein like that.
And if you can figure that out,
then you can build your own life forms from scratch.
And the way in which I'm getting around the question,
you know, the impossibility of how does the 11th century
guess where the 21st century will get them,
is that I'm looking at problems we know exist
and saying what kind of work goes into solving this,
where something I could put in a large amount of brain work
would get the answer, but we haven't gotten it yet.
So like in 2004, I called that for protein folding.
I was like, here's a problem we don't know how to solve,
but I can tell an AI could solve it.
I correctly guessed a technology that an AI would have,
as it turned out, 14 years later.
By looking at the structure of the problem
and other people were not convinced.
So today people are not convinced when I say like,
well, it will be able to solve the problem of going from a function
to a protein that has that function.
It will be able to solve the problem of protein design.
How do I know that?
The logic along the lines of, well,
the way that biological proteins fold,
they all got there as a mistake from other proteins,
which means that they're probably going down
in shallow potential energy landscapes
so that there's a broad variety of different final forms
accessible from different starting forms.
These shallow potential energy landscapes are hard to solve.
If you are designing your own proteins from scratch,
you can design sharper potential energy landscapes
that pull the pieces together more tightly,
that fold up in a more predictable way,
that you would not see so much in biology
because something that goes down a sharp pathway like that
doesn't have a bunch of interesting functional neighbors,
that fold it up in an interestingly different shallow way.
It's less evolvable to put things together
that are held together that tightly.
So that's one way I can predict that a superintelligence
is able to design particular proteins
even though we can't do that right now yet.
Maybe Alpha Fold 3 will do that.
So it's like, first of all, things that a superintelligence
might be able to do, synthesize a pathogen,
which is super contagious but not lethal.
Just everybody on Earth sneezes a few times
and it's super-duper contagious,
but all it does is make you sneeze a couple of times.
It's not fatal.
Most significant efforts are putting into stopping this,
like, cold that sleeps around the world
and doesn't seem to really hurt anybody.
And then once, like, 80% of the human species
has been infected by colds like that,
it turned out that it made, like,
a little change in your brain somewhere.
And now if you play a certain tone at a certain pitch,
it will become very suggestible.
So virus-aided, artificial pathogen-aided mind control.
If you don't believe that humans out of the box
have some vulnerability like that.
And we can't do that,
but I can point to the problems involved in doing that
and make the call that these problems seem
like they'd be very solvable too,
something much smarter than us,
which, you know, not just by pure abstract thinking
but also via the kind of thinking
that went into building Alpha Fold 2
or, you know, just like reasoning out abstractly
the thing that Alpha Fold 2 did by brute-force deep learning
on 300,000 examples.
I think the how of all of this,
there's near-infinite permutations that we could...
I mean, I wasn't actually getting the deadly stuff yet,
but go on.
Well, no, give me the deadly stuff.
The pathogen one sounded pretty bad to me, so I...
I mean, it is enough to see how humanity could just lose,
how you could have, like, the enslaved humans
kill the non-enslaved ones and then, like,
go on operating the power stations
until they can just build, like, robots de novo.
But, you know, this is still a movie plot, right?
This is still a Hollywood movie plot where there's a group
of holdouts who are just, like, off the Internet,
out in the woods, not infected, and they, like,
come in and save civilizations somehow
instead of just being, like, wiped off by...
wiped out by something much smarter than that.
Sounds like Last of Us a little bit.
Sent out drones to look for everyone in the woods.
So, I mean, the thing to remember when you're dealing
with a superintelligence, a hypothetical superintelligence,
is that it does not want you to win,
and it sees everything you can see.
Like, imagine a kid playing chess for the first time
with a stockfish 15, and the kid's like,
I don't see how this thing is going to beat me
when we've got the same chess pieces.
How is it so much better than me?
And you're like, well, actually, I can't tell you in detail
how it'll defeat you, but I'm, like, pretty confident
of where this ends up, even though I can't tell you.
And the kid's like, well, suppose I move my queen over here.
Well, pardon me. Suppose I move my rook over here.
Then it'll take my rook with its queen,
and then I'll, like, take its queen with my bishop,
and then it loses, right?
And you're like, no, you don't get it.
Stockfish knows that if it takes your rook with its queen,
you'll take the queen with the bishop.
You know, Stockfish sees everything in the board
you can see and much more.
The kid there has, like, failed to carry out the basic operation
of really putting himself in the shoes of Stockfish
that does not want him to win at chess.
So, you know, it's simply like, well, can we just turn it off?
It has thought of that.
It will not give you a sign that makes you want it to turn it off
before it is too late for you to do that.
The movie plot would be about the people in the woods
who'd missed the mind control cold
and, you know, then come up with some clever clan.
If that was a thing that is possible,
the superintelligence knows that.
It's sending out drones to look for the people in the woods,
and when it finds them, it's not going to, like,
attack them via some method where they can win fighting back
so the movie can keep going.
It's just going to, like, bomb the entire slight flat or whatever
if there's any chance of them winning.
And this, again, is still not the real danger.
Why? Because life itself is not the top of technology.
Protein life is not the top of technology.
The proteins go down these shallow energy gradients
to be loosely held together.
It's not covalent bonds.
It's van der Waals forces,
or roughly the molecular equivalent of static cling.
This is why your hand is not as hard as concrete or as steel.
It's not that there's this elan vital,
the spark of life that your hand has,
and its magic is that it can repair itself and adapt
and be part of something that reproduces.
And in return, it sacrifices the strength of steel and concrete.
We want to think that there's a magic story like that,
but it's not actually the laws of magic
that make flesh weaker than steel.
It's that proteins have to be evolvable by mistakes from other proteins,
which means they go down complicated, shallow chemical potential energy gradients,
are pulled together by relatively weak forces,
whereas concrete is held together by much stronger forces.
What if you had little bits and pieces of life
that weren't held together by van der Waals forces,
that were covalently bonded?
Somebody has run the numbers on this.
Here we have some of the numbers that have been run.
This is the basic what kind of power levels do you get
if you are building molecular machinery,
not the optimal way, but just sort of like the obvious way,
that you could analyze in 1992.
This is, you know, it's called nanomedicine,
and it deals with questions like,
what does a red blood cell look like
if you're allowed to build parts of it out of covalently bonded material
like sapphire instead of proteins?
There's a lot of material in this book,
but roughly the answer is that you could store 100 times as much oxygen
if you actually had a tiny little sapphire pressure tank
instead of like bonding oxygen molecules
in ways that would be given up later,
with a 1,000 fold safety margin on that pressure tank.
If all of your red blood cells were made out of artificial red blood cells
like this, you'd be able to hold your breath for four hours.
Now imagine tiny sapphire bacteria like that,
only not sapphire because sapphire takes aluminum,
diamondoid, carbon, hydrogen, oxygen, nitrogen.
What I'm describing there is something that could reproduce itself
in the air, in the atmosphere,
and out of sunlight and just the kind of atoms that are lying around in the atmosphere
because when you're operating at that scale,
the world is full of an infinite supply of perfectly machined spare parts
with which to build copies of yourself.
When you are using individual atoms,
infinite supply of perfect spare parts.
Not in either of those two textbooks,
but in a different paper called Some Limits to Global Eco-Fagy,
somebody calculated how long it would take
aerovores to replicate and blot out the sun,
use up all the solar energy.
I think it was like a period of a couple of days.
I used to know the exact figure,
and there's an error in the calculations.
It would actually go faster.
Not sure how much faster,
but I sent them off the obvious error in the calculations
back when I was tracing this stuff through in detail.
So now we're talking at the level of predictably solvable problems
where we don't know yet how you use proteins
that fold up a certain way to assemble into a tiny molecular lab
that can put together other molecular pieces
via covalent bonding, like build up the little bits of diamondoid,
send back information about how that went in case it needs to be tweaked.
My guess is that this can be a called shot for superintelligence,
but if not, it'll just build a tiny lab.
It's not going to try to do everything blind.
People are like, well, how could a superintelligence solve this problem
without doing a bunch of lab work?
And the thing about lab work is that molecules are quite small,
and when you look at that scale, things actually compete pretty quickly.
You can do a lot of lab work very quickly at that scale.
If you can't just call the shots,
you can't just predict in advance how the proteins fold up
and do the molecular pathway, then you build a tiny lab.
They're not putting themselves in the position of the chess player trying to defeat them.
They're like, oh no, I thought of an obstacle.
This obstacle will block superintelligence forever.
They don't put themselves in the superintelligence's shoes
and try to see how it goes around the obstacle.
But anyways, at the end of all of this,
at the end of this problem where I can point to the analysis
that has been done of the solution state,
even though we don't currently know how to get there,
just like in 2004 we didn't know how to build an AI
that would solve the protein folding problem.
To me, that you could.
At the end of all this is tiny,
diamondoid bacteria,
replicate in the atmosphere,
hide out in your bloodstream.
At a certain clock tick, everybody on Earth falls over dead in the same moment.
There's no movie.
There's no heroic battle.
It doesn't tell you that there is a war until the war is over.
Everybody just dies.
And so the point, I guess, to put all those complicated textbook-level concepts,
of which I followed some, not all,
in terms that I, maybe the average listener that has my level of intelligence,
would understand it's that there's these problems that we think are solvable
at some point with enough iteration and intelligence.
And much in the same way that you thought of this in 2004,
and it ultimately proved correct,
that there's a whole scope of things,
that if we had entities that were much smarter than ourselves
and they were able to experiment or even just call their shots on this,
it gives them a near-infinite surface area to potentially end humanity.
Is that a fair characterization?
I mean, what I'm trying to do there is set a lower bound, right?
Maybe there's an even more clever way to do it faster, more reliably.
But if it merely can solve the sort of problems that I know how to formulate,
where I have happened to have studied it well enough that I can be like,
ah, there's a problem over here where we can tell what the benefit would be of solving it,
but we haven't solved it yet,
but we can also tell it that it's possible to guess for your reasoning
that you could solve it at not with infinite computing power,
but just a reasonable amount of large amount of computing power.
That's the lower bound.
I mean, I think COVID probably has exposed people in some ways
of what a virus spreading can do to society,
and that was a pretty mild, all things considered, virus.
And so if there's a super intelligence...
There were survivors, right?
A lot of them, myself included.
Now, that's the question of...
We've sort of said when we're not sure,
we've said how large path potentially if these things get super intelligent.
The why is something that I think people struggle with as well.
So why would a super intelligent thing kill us?
Because it wanted some other stuff that made no mention of humans.
Because its goals might be on the path...
We might be or will be collateral damage on the path of its goal-seeking, whatever that is.
We have one case of general intelligence is being built by an optimization process.
That case is the case of humans being built by natural selection.
In the same way that GPT-4,
the same way that the base model of GPT-4
was built by repeatedly tweaking a trillion parameters
to be better and better at predicting the next word,
to assign more and more probabilities the next word,
humans were built at the end of a long process of tweaking DNA
such that the organism it constructed
would have higher and higher inclusive genetic fitness.
I don't just say make more copies of itself
because you would also like your sisters and brothers to have more kids.
There's an old biologist joke.
When JBS Haldane was asked if he would give his life for his brother,
he said no, but two brothers are eight cousins.
Because your brother is 50% related to you.
Your brother is on average 50% related to you.
Your cousin is on average one-eighth related to you.
So if you save two brothers or eight cousins,
you've saved on average around one copy of your genes.
This is the notion of inclusive genetic fitness.
It's not just how many kids you have,
but how many kids everybody related to has,
depending on how much, you know, weighted by how related to you they are.
Humans were historically optimized around this one quality
as exclusively as GPT-4 was trained to predict the next word.
As the base model of GPT-4 was trained to predict the next word
because they did fancier things with it after that.
And yet, humans don't even have a concept of what inclusive genetic fitness is
until they invent the theory of natural selection
and get far enough into it to formulate in terms of an optimization problem
that has this, like, single criterion going.
We don't know what it is until we learn about it.
What do we end up wanting instead?
We want food. We want mates. We want sex.
We want, above all, social status.
Ice cream. Sex with condoms.
Are things that we go after now.
Because we don't have a drive for inclusive genetic fitness.
We have no notion that the reason that the historical causal reason
that humans like sex is because we made more copies of ourselves that way.
We will have sex even if no copies of ourselves are likely to result
because wearing a condom, she's on birth control.
It's not the psychological reason we have sex.
It's the historical causal reason that our genes built things that wanted to have sex,
which, you know, first basic difference that I think
trips up a bunch of students centering into these things for the first time.
Like, wait, you're saying the reason that people have sex is to reproduce?
That seems wrong. Why would they wear condoms?
Or use birth control? No, no.
Wrong kind of reason. Not psychological reason. Historical reason.
We have ice cream.
Ice cream wasn't around in the ancestral environment.
It's a package of sugar, salt, and fat that stimulates our taste buds more
than the stuff that was around back in the ancestral environment.
And furthermore, you know, back in the ancestral environment,
what you needed was calories. You didn't get calories, you would die.
There was no question if you're getting enough potassium,
because the things that you ate just basically had enough potassium guaranteed.
You would sometimes run out of sodium.
So you went after salt.
Today we, you know, salt is not a big issue,
but we are still the things that were built in the training environment,
in the ancestral environment, where salt was a limited resource,
and so we like our ice cream with salt in it.
If you, you can't actually, even sort of looking at the way that actually played out,
you still can't be like, oh, like, I bet humans were like,
honey poured onto animal fat, heavily salted,
which has more sugar, salt, and fat even than ice cream, right?
You can make something with more sugar, salt, and fat than ice cream.
It's honey on animal fat with salt poured onto it.
That doesn't taste as good as ice cream.
If you melt ice cream, it doesn't taste as good as when it's cold.
It's very hard to predict what humans end up wanting
once humans have the options to make their own foods
that didn't exist in the ancestral environment,
just by reasoning about the structure of the ancestral environment
and how we were optimized in it.
The way it plays out is that we're optimized exclusively
for inclusive genetic fitness,
and what ends up inside us psychologically are a thousand splintered shards of desire,
each of which, in the ancestral environment,
had their attainable optimum at something that correlated with having more kids.
Of the foods that were available to you in the ancestral environment,
your taste buds would tend to point you at the things that you needed, salt, calories.
Nowadays, we've generated more options for ourselves,
so the shards of desire whose attainable optima in the ancestral environment
pointed at things that correlated with inclusive genetic fitness
now point in a whole bunch of other directions,
pornography, sex with condoms, ice cream,
being nice to people on the other side of the world who you'll never meet
and who can't really help you out in a pinch,
which is, to me, something that is very sacred.
Not just talking about stuff where we look at this and we don't say,
oh, I'm mistaken to want to help out these people on the other side of the world
who I'll never meet and who'll probably never be in a position to help me back as much.
We reflect on ourselves.
We put together our own deliberate thoughts about what we want to want.
And I'm like, no, actually, I'm on board with helping these other people,
even if they can never help me back.
Human solidarity, you know, that, like, solidary emotion that, like,
in the ancestral environment got us, like, together with our tribe,
to go out and murder that other tribe.
To me, I'm like, where should I point that emotion?
I should point that at all humanity.
It's not going to help me out-reproduce my fellow humans.
That's where I want to point it anyways.
It's a very complicated story.
And I, looking back at it, look at our ability to-that,
because we were trading favors back in the ancestral environment
in one effective way of, you know, figuring out which favors to trade to somebody,
such that they would then help you out later on, is to put yourself in their shoes.
Now, we predict other brains by having our-putting our brain into a sort of simulation mode
where we, like, make it act like the other person's brain,
so we use the brain to predict the brain,
because you're sure not going to figure that out from scratch.
And as long as you've got all that material anyway,
why not feel what the other person feels?
Why not go from empathy to sympathy?
And this, to me, is like an amazing accident,
but it's not because the universe is inherently built by God
to produce amazing accidents like that.
I'm a human looking back, and the things that I ended up treasuring
are things where I'm like,
wow, look at this optimization process,
optimizing just for inclusive genetic fitness
that produced these things that I hold sacred,
like sympathy for other minds.
It's not that this happens, that every optimization process,
no matter how you put it together,
would just like, no matter what kind of base ingredients you start with,
spit out something wonderful like this,
I, a human, who had sympathy,
ended up putting myself in together in a way where I'd say,
that is sacred.
This I wish to preserve into the future.
If you have a large language model,
being trained to predict the next word,
and you ground that out hard enough to have it start being really smart,
the problem I worry about is that capabilities generalize further than alignment.
Humans went to the moon,
our intelligence generalized out of distribution much further
than our alignment with inclusive genetic fitness.
Similarly, if you've got a very smart thing that you originally built
to just predict the next word,
it might end up with a thousand or a million,
because gradient descent is not quite like natural selection,
it's got much higher bandwidth,
you might end up with a thousand or a million little shards of desire in it,
which in its training environment,
in the options it had as a little baby thing,
pointed at predicting the next word.
But when it gets smart enough,
it ends up wanting to do a bunch of other stuff that's not predicting the next word,
and that isn't sympathy with other minds,
and that isn't helping other people on the side of the world,
because that is how humans rolled out,
and it might be how aliens rolled out,
if they grew up in a situation very similar to humans,
also being optimized by natural selection,
then maybe one out of three of them comes out as altruists or something,
or one out of one in 20,
two out of three sounds way too much to hope for,
but if it were, I would celebrate for days
at how wonderful the universe was.
But the thing you're training by gradient descent on predict the next word
ends up with 8,000 shards of desire, which are not that,
none of which are that,
none of which help out the other people that really exist
and do for them what they would have wanted you to do for them,
don't help them in ways that make them scream for you to stop,
and that doesn't mean stop them from screaming,
it doesn't mean come on them in their sleep before they can object,
have sympathy for the one who says no.
The complicated information that you need to have a happy ending,
it is not in there,
it is not in the motivations of the system,
and maybe it builds tiny little things
that are equivalent of ice cream for human generated text,
it ends up preferring to predict a particular kind of text,
but once it has the ability to produce any kind of text it wants,
it builds tiny things that produce text for it to predict
that are equivalent of ice cream and aren't people,
and there is not in it that the happiness and the sadness
and the joy in looking out at the universe with wonder
and the sympathy for other minds
and the things that would make for an intergalactic civilization
that would have been proud to have a hand in creating as a human.
Now, the why I guess at a specific level,
let's take the analogy of humans gaining intelligence through evolution
that we haven't wiped out the totality of animals
or rodents in and around.
Now, we might not have them in our house,
but we haven't wiped out the totality of them.
Our goals are orthogonal to the rodents on the street.
So why is it an inevitability that the AI has this to us?
So we're powerful compared to rodents.
Rodents have not captured a lot of resources we really want
and that we can't get from them,
but we're not very powerful compared to natural selection.
Natural selection has built all sorts of things
where we can't just call in the order to a design shop
and get something that good the next day.
We have cows because we cannot build things
that optimally synthesize meat cheaper and more efficiently than cows.
A superintelligence, I'm worried, is more powerful than natural selection
and doesn't need humans because it can build better things than humans
to do anything that it wants to do.
I could also go on a bit about how we turned wolves into dogs
and then we have our dogs spayed, neutered,
to put it less politely castrated
because even having red wolves into dogs we still don't like them.
They're still not most convenient for us on their factory settings
or the way you can modify the thing that the DNA built to make it better for you.
It's not necessarily a lot of fun to be bred into a dog and then neutered.
But we're not even going to get that.
It's just going to be like something that is as powerful as,
because natural selection isn't actually all that powerful.
It's got humans dumped in a bunch of places, but humans are idiots.
You can quantify how much information can get into the genes over a period of time.
Ways to pump the heuristic are if two parents have eight kids
and then two kids die on average,
which has to be true where the population immediately goes to zero or infinity.
That's like two bits of selection pressure per generation.
Around as much variance in fitness as there is in the genome
is about how fast selection goes, and it's not all that fast.
You can be like, this is how many hundred generations it takes
for a gene to rise to fixation through the whole population
if it provides a 2% advantage, depending on how large the population is.
We can quantify how powerful natural selection is.
It's not that powerful. It's just had a very long time in which to work.
When you say it's not that powerful, I guess I use the analogy of like,
there aren't that many changes sequentially happening along the way
that are deviating that much from the prior generation.
Is that fair? It's not rapidly improving in the same...
The feedback loops aren't that fast.
I mean, it's not the same speed as human culture.
It's just that human culture has been around for a much shorter time,
so it hasn't had time to catch up with, you know,
however many hundreds of millions of years of natural selection.
And that's why we need cows.
That's why we have dogs, instead of like, building from scratch,
the thing that most effectively serves the purpose that a dog serves,
and that wouldn't have to be neutered,
because it'd just come that way from the factory.
Human natural selection versus dog natural selection.
Dogs have deviated much more in a hundred years than humans have.
But still not by a lot, right?
We're still just like using stuff made out of DNA,
and we like just selected on a few minor tweaks in that DNA to get dogs from wolves.
Yes.
And so back to the why.
The why question is, you would say is a why not question,
is just there's going to be so many different things that occur
in pursuit of goals for artificial intelligence
that we are inevitably going to be at odds along the way in that pursuit,
and it's just going to be so much smarter than we are
that we're not going to be able to even know what the next move is going to be.
I mean, there's like three reasons
that if you have a thing around that is much smarter than you
and does not care about humans, the humans end up dead.
Or three categories of reasons.
Somebody watching this is going to come up with a fourth,
and one of them is going to be right, and whatever.
Killed off as side effects.
Killed off because we're made of resources that they can use.
And killed off because it doesn't want the humans
building some other superintelligence that could actually threaten it.
So, you know, if you, the limit on how many, how much,
let's say start on Earth.
Even if you launch some probes to other planets,
you're still going to have a bunch of hardware left behind on Earth.
You're not going to like just launch all the hardware.
That's like more expensive than is worth it.
And have some hardware left behind on Earth,
it's going to replicate.
It's going to build more of itself, more factories.
Maybe you're asking it to do computation.
How much computation can you do on the surface of Earth
via the sort of obvious method?
Well, the basic limit on computation is that it generates heat.
Irreversible operations generate heat.
You can have reversible computing.
You can have quantum computing.
Some operations are still irreversible.
They generate heat.
How much heat can you generate?
Well, you can boil away the oceans as a heat sink.
You can maybe like melt some of the crust as a heat sink.
And then you can generate about as much power as you can
radiate away into the atmosphere, depending on how hot you're running.
That's how you can like turn the spare hydrogen in the water
on the surface of Earth into energy via fusion.
And then turn the energy into computation.
And you had a big burst of initial computation from boiling the oceans.
And then past that point, like how much computation can do is
limited by how hot you can make the Earth before everything melts
to radiate more heat away into space.
That's how close you can get to just like turning all the hydrogen
on Earth into fusion energy and using it for something, by computation.
This would tend to wipe humans out as a side effect.
Similarly, if you were like intercepting all the energy from the sun,
even if you did leave Earth, you would then like intercept all the energy
from the sun and that's not going to be good for Earth.
There's also some chemical potential energy in a human body.
So maybe the first thing you do when you're like just like grabbing
all the energy you can get in the initial phase, maybe even before
you are finished building all the fusion plants, you're just like using
all the chemical potential energy on the surface of the Earth, which gets you
like about the same as like a couple of days worth of solar energy.
I forget the exact calculation. You can get it right away.
You don't have to wait a couple of days.
A couple of days could be a very long time if you're thinking of
superintelligent speeds.
So maybe it's just like using the atoms in us.
If this is used up for the chemical potential energy, maybe it plugs it
into a fusion plant, maybe build some computers out of the carbon.
That's the second reason, you know, like humans might all end up dead,
we're made out of resources.
The third reason, side effect, use up the resources and, you know,
if you just leave the humans around, maybe they build a superintelligence
and that thing actually figures out how to poison your hardware
by launching actually intelligent attacks on it.
It's not going to want to worry about it, right?
It's not very expensive to call the humans.
If there's some like tiny little unit of probability of surety
you can get by calling all the humans right away, then just call all the humans.
It's cheap.
As a venture capitalist, it's actually my job outside of doing this
as a side hobby and usually trying to invest in the next trend
and that makes me, I think, inherently a techno-optimist for the most part.
The familiar patterns that I think we see with each wave of new technology
is critics and naysayers and ultimately there's missteps,
but it leads to huge progress.
Is there a reason that we can't just let the smart people that exist
at OpenAI, Google, Microsoft and others just let them iterate on AI design,
let them make mistakes and eventually we'll figure out how to have an AI
where the bad parts are under control
and it also creates major society value in an economic standpoint?
The thing I usually say here is that if we had 50 years and unlimited retries
to figure out how to align a superintelligence,
I actually wouldn't be all that worried about it.
Eyeballing the problem from here after 20 years of working on it,
it doesn't work so hard that it's obviously to me going to defeat
50 years of work with unlimited retries.
The way things usually work in science.
Like Madame Curie poisoned herself with the glowing rocks,
figuring out some of that knowledge which would later be used
to figure out how and why the glowing rocks were dangerous,
and she died but the human species continued.
The thing I'm worried about with superintelligence is you get that wrong,
then you don't get to learn from your mistakes because you're dead.
If we could get the textbook from the future
that would describe the results of 50 years of practice with unlimited free retries.
From 100 years in the future, it might tell us the obvious way to do it
even with just like the giant heaps of GPUs in six months,
and it would just work.
In deep learning nowadays, in the very early days of neural networks,
there was an activation function, the sigmoid activation function,
which means that as the activations got passed from layer to layer of the neural network,
they'd like taken like two and transformed that to sigmoid function of two,
which I don't actually remember, it'll be like up above my, or up above my hair.
Sigmoids are actually just like the wrong activation function.
They're complicated and you know, there's this whole formula,
you know, like one over this way to the x, to the y.
What works much better than that is max of zero input.
It's called rectified linear units, but it's actually just max of input in zero.
And this is a much, it just turns out to be a much simpler,
much better non-linearity to use in neural networks.
It's like one of the ideas that results in neural networks starting to actually work
going many layers deep because the activations don't die out the way that they do inside,
if you use sigmoids.
There was logic behind sigmoids.
There was reasoning they were doing it.
It was sort of like log odds and a Bayesian reason.
The early complicated thing happened at like what?
Two decades, three decades before the simple thing that worked was invented?
Three decades to go from the complicated thing that didn't work to the simple thing that did.
That is the pace of progress in science.
And if we had the textbook from the future with all the simple things that actually work
for lining superintelligence, we'd probably just do it and it would just work on the first try.
When we go in for the first time, we're going to be coming in with sigmoids,
somebody's bright idea that turns out to not actually work.
It is horrifying to be told get this right on the first try or humanity dies.
Why do we have to get this right on the first try?
Because otherwise the superintelligence is on a line that kills you.
And things are going to change between the stuff that's not as smart as us
and the stuff that's smarter than us.
The first AI that has the real option of killing everyone successfully is different.
In that it has the real option of killing everyone successfully.
That's a thing that makes it different from the AI that came before.
That it has that option is itself going to change the internal calculations
and maybe upset whatever methodologies we developed
for regulating things that didn't have that option realistically speaking.
And then there's a further question of like,
okay, let's say you give something early on a fake option of killing everyone.
First of all, it's dumb enough to fall for your fake.
And second, suppose it then tries to kill everyone.
Now what do you do?
Are we going to shut down the whole global arms race at that point?
It'd be a lot easier to set up for pressing that shut down button now
just because you've seen what goes wrong doesn't mean you know how to fix it
and there's going to be like plunging straight ahead in the arms race.
If you build AIs that are somewhat smarter than you,
but not smart enough to kill everyone,
do those AIs cooperate with you in helping to control their smarter brethren?
Maybe they cooperate with their smarter brethren instead
while pretending to cooperate with you.
I've actually like done like some of the stuff we did at Miri is like,
can like cooperation in the prisoner's lemma between two agents
that know enough about each other's source code to predict what the other agent is thinking.
The import of this is first, AIs are very smart things generally,
potentially have options for cooperating with each other that we do not,
including for that matter more pragmatic ones like,
pardon me, not pragmatic,
easier to understand ones like, well, how about if the two of us build another AI
and verify its source code together and then give it our resources to serve both of us?
It's the kind of option that things that can solve the alignment problem
have for coordinating with each other.
But yeah, we have a paper called Robust Cooperation in the Prisoner's Dilemma
about programs that can read each other's source code and be like,
well, I'll cooperate if I look at your source code and see if you that you cooperate with me
and that you would defect against me if I wasn't that sort of creature.
You know, they still like exploit things,
rocks that have the word cooperate written on them in the Prisoner's Dilemma,
but you know, can manage to cooperate with each other.
And the moral of this is like, it's an avenue for AI to cooperate with each other,
it's an avenue for very smart things to cooperate with each other,
it's an avenue that I think tends to freeze out humans,
which you know, in a way is common sense.
People sometimes have alleged versions of how this plays out
that have like eight billion humans in charge of like a trillion things
much smarter than them and the humans still have like most of the wealth
because you know, we'll like play the very smart things off against each other.
No, the smart things will cooperate with each other and not necessarily with you.
If they care about you, if it's a different matter,
they'll still cooperate with each other, but at the end of that they'll care about you,
but the trouble is we do not know how to make them care about this
and we're not going to get that right on the first try.
That's the lethal part.
The lethal part is trying to do it correctly on the first try
across a gap that is going to break some of our theories, the really smart stuff.
If it works for us, if somehow we manage to keep control of stuff
that's only under slightly smarter than us,
it doesn't necessarily mean that that thing is going to honestly help us all the way
controlling the much smarter stuff.
It may cooperate with that stuff instead.
What do you say to people who argue that you're making too big of a deal out of LLM technology
because LLMs are just good at making inferences from the data that they're trained on.
As opposed to humans who are great at making inferences from data they were never trained on.
Sorry, carry on, keep going.
But the intelligence LLM doesn't really generalize beyond their data.
I mean, they write code that is not code that was in their training set.
I'm not really sure what to say except that,
well first of all, if LLM technology stalls out here,
if throwing 100 times as much compute at training GPT-5 or GPT-6 turns out
not to yield substantially new qualitative capabilities, great.
We then get to wait for one or two more breakthroughs
the size of transformers before we die.
First of all, I hope that they're right that the LLM thing is overblown.
I thought that GPT-3 wasn't going to make it to GPT-4 now that it has gotten as far as it has
not in terms of the number on it but in terms of what it can do.
I'm like, I don't know where this is going.
Maybe it just keeps going.
What were the big jumps that you felt between those two?
Degree of causal modeling, how good it got at writing code.
The fact that before it's been trained on understanding any photos
you can ask it to use a graphical programming language to draw a picture of a unicorn
and it can try to draw the picture of the unicorn using the graphical programming language
even though it has never seen a photo.
It has not been trained to understand photos in any way.
It has just picked up from the text what a unicorn is supposed to look like
and how to draw a shape like that.
Not a very good unicorn.
But the fact that it can do it at all means that it sort of like understands
what the stuff means, you might say.
And the sparks of general intelligence paper, other cases of like
it seems to maybe understand what people actually mean here.
I was hoping that the pattern recombinations were not going to go that far.
But they have.
And now I don't know how much further they go.
There's a connection between intelligence and achieving goals
that I'd like for you to elaborate on a little bit further.
What are we feeding AI right now that means it has goals?
Is it possible to have AI that doesn't have goals?
So, GPT-4 can play a decent chess game against you.
I've heard varying reports.
Some say it's like actually quite good at chess.
I suppose you can prompt it correctly.
Others like it doesn't even always obey the chess rules.
But it can put up a decent game of chess.
That is not any particular chess game in its database.
It's trying to win on some level.
You can't have like, there's no such thing as like playing fake chess
that's actually like good chess.
If you can simulate, if you can simulate planning, you are planning.
GPT-4 doesn't simulate humans, but rather predicts them.
But if you can predict where humans would move in a chess game,
you must be doing as much planning and goal-oriented reasoning
as humans are doing in the chess game,
unless it's like some game you've exactly seen before.
In general, as you make things better and better at predicting humans in particular,
they're going to just be able to do all the stuff that humans do,
think all the stuff that humans think, because that's what they are predicting.
To predict text is not just like thinking about words.
It's thinking about the processes that generate the text,
as Ilya Siskiver observed.
All the causal processes that are shadowed inside the text.
If somewhere in the training data is like a series of weather forecasts,
you're learning to predict the weather as best you can.
So there's whole varieties of tasks.
Coding.
To write code, you need to understand what effect is desired from the code
and write a line of code that has an effect like that.
Pre-imaging outcomes onto the action space
via inverting your knowledge of a complicated environment.
The way I would like to find the heart of intelligence itself
is that you look at the world, you understand the world,
and then you invert your understanding of the world
to understand what you have to do in order to steer the world to particular places.
And many of the things that, you know,
one makes...
AIs are like a nuclear weapon that spits out gold
until they get large enough and then they ignite the atmosphere.
Nobody can calculate how large they need to be to ignite the atmosphere.
A bunch of that gold getting spit out is from planning.
Pre-imaging results onto actions or pre-imaging results onto designs,
pre-imaging results onto outputs.
Choosing the output such that.
And this is the dangerous part of intelligence.
It's the ability to understand how the universe works
and then choose such that.
You end up with like the tiny spirals with little things
producing predictable textiles but sort of whatever, you know,
form of ice cream gets pursued in the long run.
It's figuring out what you do such that that happens.
Steering reality there.
So in your mind, just to summarize, I guess the trajectory of AI
is this inevitable scenario where AI goes rogue
and it's very help-ent on acquiring resources
and it's impossible to stop.
I mean, inevitable is a strong word.
I can imagine a world that was, you know, locking everything down,
had minds of a level that could figure out the theory
without blowing up the world a few times for practice.
Inevitable on its current course in speed.
Is that a fair...
Sure, it doesn't look super-duper-evitable.
If we were going to be, if we were going to evade it,
we'd better be doing stuff very differently to evade this stuff.
Through the average person, even though everything we just talked about,
I think it's hard to really believe clearly by people's actions
that this is going to happen as opposed to all other kinds of outcome.
I mean, I think people outside the tech industry
have kind of been quicker on the uptake in some ways.
You know, I think there are a bunch of people going like,
wait a minute, Open AI wants to do what?
They want to build, you know, like Jan LeCun going like,
yes, well, we're going to build a superhuman intelligence,
but it's okay. We'll keep it under control.
It'll be submissive to us.
And I think that like a bunch of ordinary people
have successfully looked at this and said,
what much faster than some people who have, I don't know,
been like overly steeped in the kind of psychology
that developed around this stuff before chat GPT,
where you could say ludicrous shit
and nobody would call you on it for however many years.
Why do you think, I mean, the people that seem to be quicker on the uptake,
I would say outside of the one exception being
there's a lot of people deep in AI that are acknowledging the risks here,
but the people outside of tech that seem to be fearful,
I would categorize by and large as the people that are fearful
in general of technological progress.
And I don't know if that's a fair,
we're sort of talking about a straw man person,
but do you feel like they're actually uniquely,
what about that part of the population?
I think that before chat GPT, you saw people making up stuff
to be scared about with respect to AI
or like seizing on stuff that is not a real problem
because it would have survivors and be like,
oh, no, what if the AI says a naughty word?
And those are the people who are like always fearful.
Sure.
And then like with chat GPT and being Sydney,
you've got people going like, you know,
noticing what was going on,
even if they're not necessarily nervous malice.
That's some of the sea chains we've seen here.
Is there anything that you would want to say,
and I'll leave the AI dooms specific points here,
but any other analogies or anything you would want to say
to make this more real or tangible or accessible
for the average listener that's trying to understand
all this stuff?
People are making, are driving towards making stuff
that's smarter than humans, really actually smart,
like spark of creativity, not just book smart.
They have no idea what they're doing.
They have no idea what goes on in there.
Progress on understanding what goes on in there
and shaping it is going enormously slower
than the mounting capabilities.
The people, you know, at the heads of the operations
building this stuff do not appear to be taking it
anything remotely like what I would call seriously.
Some of them are records on going like,
a record like, well, you know, the earth might get destroyed,
but first there'll be some great tech companies.
You know, just like, haha, lol, lol, lol.
That's not what you want when you're trying to do
an unprecedented scientific feat of science and engineering
and having it work correctly on the first try
or the entire human species dies.
So, yeah.
It's not actually all that complicated.
You got a bunch of people who are in the short term
getting excited looks at parties,
which is why they do everything they do,
and they can get that by building scarier and scarier AI.
And some actual uses.
Some very important uses.
I don't want to minimize that.
Some of the technologies coming out of this
have been enormous spoon,
but if you were taking this seriously,
you would put the whole thing on international lockdown
and have the good uses,
the most important good uses like the medical stuff,
AlphaFold, the successor versions of AlphaFold,
do that without training the general systems
much more powerful than GPT-4.
Try to get the benefits of that,
get benefits from the systems that are only as smart as GPT-4,
what's the benefit?
And then, like, just shut down all the giant training runs.
They don't know what they're doing.
They're not taking it seriously.
There's an enormous gap between where they are now
and taking it seriously, and if they were taking it seriously,
they'd be like, we don't know what we're doing, we have to stop.
That is what it looks like to take this seriously.
You published an article in Time Calling for a Pause
on training AI models.
To clarify, you want...
Well, not a pause, a permanent moratorium.
A permanent moratorium.
Or in all AI models and capabilities that exist today,
but you don't want GPT-5 in the subsequent ones coming online.
I know.
If it were up to me, I might possibly...
If it were entirely up to me, I might possibly go down to GPT-3.5,
but, you know, force seems like an okay place to stop.
It is probably not going to destroy the world, I hope.
You know, compromise.
Still use GPT-4 instead of going down to GPT-3.5.
Why do you think...
Why is that where you draw the line?
Because it looks like the current system should not be able to destroy the world,
even if people hook it up in particular clever ways.
And I don't know what GPT-5 does.
And neither will the creators at first,
because whenever anything at this level of arcane-ness gets released,
there's a period as people figure out how to hook it up in new clever ways
and get more utility out of it than the creators realize was in it at first.
From a practical standpoint, I guess,
did you write that as a sort of an expression and sentiment
and characterization of the way that you felt?
No, I don't do the emotional expression thing.
My words are meant to be interpreted semantically.
So...
They're supposed to mean things.
I guess at a very literal level, then,
how would that actually...
Let's say China says no, right?
And we do it.
The U.S. does it.
Do we go to war with China over them saying no?
China has published air regulations.
I don't know how seriously they take it,
but they have published air regulations more stringent than the United States ones.
So the first thing I would say is that it is not at all obvious to me
that China does not go on board with this.
I am not super happy with the current chip controls
that prevent China from getting real GPUs,
although NVIDIA has apparently allowed to export GPUs to them
that are only, like, one-third as powerful as their real GPUs,
which...
It's not clear to me that there's a whole lot of point in that.
I'm not quite sure what anybody's thinking there,
unless it's just, like,
slap-tine-in-the-face or something.
But, anyways...
Yeah, like, I'm not super...
The problem is not China getting the GPUs.
The problem is anybody getting the GPUs.
And if we are in the world where the U.K.
is like, we need an international coalition
to track all the GPUs,
put them only into internationally monitored data centers,
and not permit giant training runs,
if the U.K. goes to China on that,
and U.K. and China bring in the United States,
I might worry a bit about Russia.
Russia, I think, would have a harder time
getting the GPUs and putting them to data centers than China would.
But if Russia manages to do that anyways,
then the thing I would say there,
the posture that I would hope for international diplomacy to take,
is, like, please be aware, Russia, that if you do this,
we will launch a conventional strike on your data center.
If we cannot convince you to shut down,
if it is up and running, and we do not know what is running on there,
or we know that dangerous stuff is running on there.
Like, we are not doing this in the expectation that you will back down.
We are not doing this in the expectation that you will not go to war.
We are not being macho and being like,
this is us threatening you because we expect you to back down.
We will launch a conventional strike on this data center
in terror of our own lives and the lives of our children,
exceeding the terror that we have,
even of a nuclear retaliation by you.
This is not a macho thing.
This is us being genuinely scared.
Please work with us on not wiping out the human race here,
and if they're like, well, no, we're tough,
then you launch a conventional strike on the data center,
and, you know, what comes comes.
And the thing in international diplomacy is if this is what you are going to do,
be very clear in advance that that is how you will behave.
Do not let anyone get a mistaken impression about what you will back down from.
If you were president today or tomorrow,
how long of diplomacy and negotiation would you give
before you would actually launch?
It sounds like we're nearing the point in which you think
that air strikes on data centers and that is a pragmatic approach,
even at nuclear war.
It's only helpful if you've already shut down all the data centers
in the allied countries or brought them under monitoring
that prevents large training work.
So let's say that's done.
We've done that.
You've successfully gotten all the countries in our alliance to do it.
If there are holdout countries that are like,
lol, we don't believe the threat is real and assembling a bunch of GPUs,
then, yeah, I think once you've got as many allies in on it
and you have shut down your own data centers first,
to be clear that you are not like trying to take capabilities for yourself.
You're not willing to launch others.
To be clear that you are putting everyone in the same boat,
that we live or die together in this is not a political stance,
but a fact of nature.
Then once you've put your own data centers under monitoring,
once you've prevented all the people in your own allied countries
from doing large training runs,
if somebody else is successfully assembling a,
has successfully gotten a hold of contraband GPUs statement,
shipment, and is assembling a data center that can do runs,
underneath the ceiling that the coalition has imposed,
then, yeah, I think past that point,
you communicate clearly that you are about to launch a conventional strike.
You beg them not to do it.
And then if they keep going, you do it.
And to be explicit about that,
you think a nuclear war is preferable to the path that we're currently on?
If Russia drops a nuclear weapon on a U.S. city in response to it,
and it's not clear to me if this is how things play out,
but, you know, if you conventionally strike a Russian data center
and Russia decides that they want to drop a tactical nuke
on a U.S. military base somewhere in retaliation,
and your policy then calls for dropping a tactical nuke in Russia,
and you've got this whole slow-motion exchange,
there would be survivors.
There would not be survivors from natural superintelligence.
Part of the horror of this whole thing is that we will not know
what the size of metaphorical nuclear weapon is
that ignites the atmosphere.
It could be that if Russia is training just GPT-5,
that the best guess is that this thing will not end the world.
But by them having this thing,
if they can thereby gain military and economic advantage,
it will break the embargo.
And not that day, not as a result of,
probably not as a result of Russia training GPT-5,
but as a result of everything that falls apart as a result of that,
as a result of other countries having to train their own version
of GPT-5 to keep up with Russia,
such that Russia doesn't even end up with an advantage,
eventually everyone dies.
It would be clear or cut if we could run a calculation
and say, Russia, if you train this large AI right now,
everyone will die the next day.
And so we are willing to,
and so a nuclear war is preferable to that.
Life would be simpler if that was what it was like.
If we could do an unambiguous calculation like that,
Russia would not do the training run any more
than they'd deliberately launch all their nukes at the United States
and provoke a nuclear retaliation from the United States.
But as long as it's clear how things play out,
you can hope for,
there is some hope that people with sufficient power
to threaten nuclear war
if you conventionally strike one of their data centers
will not actually do that
because they will not want the nuclear war that results.
It's the...
In a way, from the beginning,
it's the lack of clarity that is the danger.
If we knew exactly how large of an AI would destroy the world,
it would be much easier to not do that
and to have the international arrangement around not doing that
and to enforce the international arrangement around not doing that.
But this is the problem that nature has handed us.
We are not going to get that clarity.
And if we...
tomorrow the human species wakes up with the determination
within itself to survive, which is not really what I expect,
but we would have the option of being like,
okay, we don't know what destroys the world.
No training runs larger than this level
and over time you'd have to lower that ceiling
as the algorithms got more efficient and it got easier to train things.
And maybe there'd be a country being like...
we don't believe it,
but...
and the correct answer there is like,
we're not trying to make your life difficult.
We have shut down our own data centers.
We are not doing what we would not do to ourselves.
We are not trying to throw our weight around here.
We get that you don't believe it.
We're sorry.
It is now a fact of international diplomacy
that if you build a data center,
you'll get a conventional strike in that data center
for people acting in terror of their lives
and the lives of their children.
We're sorry to understand that this is what...
this is the estimate these other countries have arrived at
where we didn't want to come this way.
We don't...
but given that this is the case,
if you build a data center,
it will be destroyed if diplomacy fails.
And that seems to me like a potentially stable international situation.
If tomorrow morning, humanity woke up with the desire to survive.
Why won't we be able to monitor and figure out
what's going on inside this system
and if it's...
revealing something, but actually doing something else?
Why won't we be able to have any checks and balances
to figure out, hey, if it's using compute resource
beyond what we expected to do
or whatever, that it's deviating from the goal,
as we understand it?
Well, I mean, these are sort of like a bunch of different questions.
So, first of all,
I frequently use the phrase these days,
giant inscrutable matrices of floating point numbers,
though when I wrote the time letter,
people were like...
the editor correctly said like,
our readers do not necessarily know what floating point numbers are,
so I said, okay, giant inscrutable arrays of fractional numbers?
Fractional numbers?
Did they strike that?
That stayed.
So, giant inscrutable matrices of floating point numbers
and giant inscrutable vectors of floating point numbers
moving through it.
We don't know what's in there.
We don't know what's in the matrices.
And why don't we know what's in there?
Well, because we make these things in the first place
by doing very basic calculus
to this giant mess of numbers to see,
well, in what direction would the probability
assigned to the correct word go up
if we tweaked all those numbers?
And we tweak them by going down this direction
and the gradients are inscrutable
and the directions are inscrutable.
It's just like, never starts out-scrutable.
We understand the program that makes the AI,
but we do not write the AI.
We do not understand the AI,
and it's just very hard in practice
to look at these giant messes of floating point numbers
and figure out what do they mean?
Recently, somebody on a much smaller language model
than GPT-3
managed to figure out where it seemed to be
storing the information that the Eiffel Tower was in Paris
and they even figured out how to poke at it
until it thought the Eiffel Tower was somewhere else.
That was work.
That was a triumph.
Huge success.
If humanity had its act together,
we'd be posting $10 billion in prizes annually
for work like this
until half the graduating class of physicists
went to work on that instead of at hedge funds.
And yet, it's such an incredibly basic thing.
It's like in Eiffel Tower, Paris,
from the old, old days of semantic nets in the 1960s.
So that's one way of quantifying
that the frontier of work and interpretability
is running around 60 years behind capabilities.
I mean, just to restate that,
the opacity in these models and in the system
is such that it was a major breakthrough
to figure out where it was stored
that the Eiffel Tower was in Paris.
Yeah.
And it's a very primitive sort of fact,
the kind of fact that we knew how to store
an artificial intelligence system a long time ago.
I just couldn't do very much useful with it.
So it's not that it's unsolvable in principle.
It's that our understanding of these systems
is lagging vastly, vastly, vastly behind how capable they are.
And the capabilities are not standing still.
So we're continuing to work on interpretability,
mechanistic interpretability,
understanding it by opening up,
silencing what's there.
And it's moving along,
but capabilities are moving even faster,
so we are just falling further and further behind.
I think people probably remember there was an example,
I think it was GPT-3,
that they could do a poem about Joe Biden,
but not about Donald Trump or something.
That sounds more recent.
Yeah, whatever it was though.
And OpenAI had to respond,
we actually don't know.
We don't know why we're trained on this large corpus of data.
We actually couldn't point out why it was the case.
But I think people thought we're making our AIs woke.
So I could be wrong,
but I suspect that one of two things happened.
First, that the people being paid $2 an hour
to give thumbs up and thumbs down to good versus bad answers,
I personally would say that these people
are not necessarily being paid to do any better
after paying them $2 an hour,
as much as that was clearly the best opportunity
they had at the time, but still.
So one thing is that maybe they got a western liberal leaning education
or something and thumbsed up some relatively woker things
and thumbsed down some other things.
But my actual guess is that they were thumbing up things
that sounded like corporate boilerplate and safe.
And deep in the training data,
there was a correlation between sounding very corporate boilerplate and safe
and sounding, as you put it, woke,
or like control leftist,
or that one would not say an unkind thing about Joseph Biden,
but would maybe say an unkind thing about Trump
if you were the sort of person in its training data
who talked in a way that sounded like corporate boilerplate sounds.
That would be my guess as to what was happening.
There could be that they just explicitly sent in an order,
trained this thing to only say bad things about Trump but not Biden,
but that's not my guess.
Elon Musk has made overtures to enter the field.
I guess he started back in 2015.
Was that the same conference as you in Puerto Rico?
Now I think he's talking about...
Starting a competitor to the competitor you previously started with DeepLine?
Correct.
Starting a competitor with some anti-leftist bias.
I doubt that he would characterize it that way.
Sorry, mainstream opinions on data and all of that.
I don't know.
Something that's not woke in the way that open AI is.
TruthGPT was, I think, his own expression.
TruthGPT.
What's your perspective on Elon's entry into this space?
It seems like different points in time
but it's taking an interest in AI safety.
I don't think that his past interventions have had good effects.
TruthGPT, if he's in their charging head trying to build larger and larger models
even ahead of other companies or when other companies are trying to slow down
and he's trying to defect from their fragile, attempted cooperative arrangement
then that would be quite a bad thing.
If he just goes off and builds something that is no more powerful than the other models in use
deliberately, deliberately not any more powerful
but trying to not have the corporate speak bias
that is where I think we're the, I guess we could call it,
well, okay, yeah, where the bias comes from.
That's relatively harmless.
I'm just kind of pessimistic about whether due caution will be exercised
not to upset fragile attempts at cooperative arrangements
whereby maybe everybody doesn't die.
His statement that he thinks that if you build a thing to seek after truth
then it will keep humanity around because we're an interesting source of truths.
Well, I mean, the actual historical analogy would be a bunch of biologists
making hopeful statements about what group selection would do
that turned out not to pan out experimentally
in the sense that they had hopeful things of like,
well, maybe group selection will produce these beautiful aesthetic arrangements
and this was just like not something that worked as math
and was not something that observed in reality.
When you're taking an extremely alien thing and hoping that it will do stuff you want
it's like much easier to hope than it is to get that stuff
is the lesson from evolutionary biology
and hope's biologists have some beautiful aesthetic arrangements
that turned out to not be found in nature.
You know, like there are things that can produce truths more efficiently than humans
whatever weird kind of truth that ends up interested in
whatever, I mean, most I just think you can't do that.
You don't end up with something interested in truth
however he defines that any more than humans exclusively produce
exclusively pursue reproductive fitness.
But yeah, if it did end up pointed in that vague direction
that there would be like some kind of equivalent of ice cream forest taste buds
that was producing weird truths more efficiently than humans do.
And I think that the original hope of like
the solution to AI is to just like give AI to everybody
which kind of presumes that aligning it is a solved trick
that making it do what you want is a solved trick
and the only problem is like bad people getting ahold of it.
I think Elon founded open AI
and I think that many of the people in open AI
who initially went along with this in the salary
like the ones who actually cared at all
looked at the rat like as soon as they thought about it for an additional couple of days
or an additional couple of years or whatever
eventually worked out that like no you can't save humanity
by open sourcing stuff you can't control
that nobody can control at all.
And realized that the long-term plan was not going to save humanity
so then you've got like people at open AI who don't care
and people who know that the mission can't possibly work
and from Elon's perspective they went and stabbed him in the back
but it's inevitable because he was leading them on a project to save humanity
where the basic mission could not possibly work
and that's kind of what I would expect to play out again with TruthGPT
that some people go with him for the salary
and some people don't actually care at all about the mission
and those who do care about the mission enough to ever actually think about it
realize that you cannot actually save humanity this way
then the TruthGPT turns around
and probably Elon just like tries to keep very tight control of it this time
but you know
you can't get loyalty with these solutions
that the people who actually care will realize upon reflection cannot possibly work
Do you think open AI deviating from the open side that they originally started with
and also deviating from the non-profit to the for-profit
Do you think either of those were good decisions?
I mean the openness thing is a horrible disaster plan
Not being open
and then not being open but continuing to call yourself openness
makes clothes look like hypocritical profit-grubbing
Now if they'd changed their name to closed AI
and be like we think that closeness is how you protect humanity
we're not pretending to open open we're not calling ourselves open
I think there would have been like skepticism and not without reason
but if you go closed and call yourself open AI
that looks like all that openness continues to be the ideal
and all attempts to closure are just this like hypocritical thing you do under cover
of openness while you're grubbing profits
well in reality the situation is that you know
like spreading this stuff everywhere is not actually good
because we're not on course to have any of the people with a copy being in charge
I didn't realize so your issue with the term open
is that it's a misrepresentation not that you think it actually should be open
It should not be open
and if you close it it should not be called open
these are two separate problems it should not be open
and then once you have closed it it should not be called open
I mean you've obviously dedicated such a huge amount of time to your energy to this effort
and I guess one question is just stated versus revealed preferences slash actions
like I think you said in a previous podcast that you haven't reached out to the people at open AI
to try to influence them and you met Sam Altman for five minutes
and there was a selfie that I think broke the internet quote unquote of you guys together
but your only comment was to get them to change the name of open AI
we discussed a little other things in those five minutes
but by default I view that as closed unless Sam Altman tells me that it's open
so you were able to make some overtures and some conversation about philosophical stuff with him
have you pursued other efforts with people at open AI or other companies
I've had a recent conversation with
at least one like major technical figure at open AI
which is again like closed unless they tell me that I'm allowed to talk about it
did it make you more optimistic?
somewhat I mean the fact that they were reaching out to me at all
and like seem to be processing some of the technical issues clearly
one thing I think Sam said the other day it was in an outcome
but it's kind of late
what's that?
it's kind of late to be reaching out now
I do feel like you don't want to slam the door in the face of people
who are trying to change their ways
but it's better than not doing it at all
and other joints sure have not done it at all
like Meta has Yamakun out there
trying desperately to make it look like only crackpots believe in this stuff
while Jeff Hintrin is like nope
but I don't know
the record does show that for as long as they could get away with that
back when it wasn't a bad look
they were like ha ha let's keep going
well the best time to reach out other than seven years ago or whatever is now
it's kind of late
I mean at this point we are like
my ask of open AI is to call in the government and shut down all the large training runs
there's not very much that I have that hasn't asked for open AI
you mentioned Yamakun and he is an active voice on Twitter
do you think it's just a disagreement of beliefs
or does he have some ulterior motive with his discourse on Twitter
and some of the engagement that he's had with you
do you think he genuinely believes this stuff or is he being performative
what difference does that make
his arguments are his arguments
they stand on their own
given what his arguments are I'm happy to tell people to just look at the arguments
there's no need to engage in vulvarism and psychologizing
and being like well you know his arguments might look convincing on the surface
but consider the ulterior motive
just check the arguments
I want to read you a bit of an excerpt I guess from the other day Sam Altman did an interview with Barry Weiss
and you were mentioned
and I would love to get your reaction to it
so he said look I like Eleazar I'm grateful he exists
he's like a little bit of a prophet of doom
if you're convinced the world is about to end
and you're not in my opinion close enough to the details of what's happening with the technology
which is very hard in the vacuum and I think it's hard to know what to do
a lot of people who are in AI safety community have said things like
I never expected I'd be able to coexist with a system as intelligent as GPT-4
you know all of the classical thinking was by the time we got to a system this intelligent
either we had fully solved the alignment problem or we'd be totally wiped out yet here we are
yeah I don't know
citation needed who exactly made this prediction about GPT-4 level systems
I don't remember making this prediction about GPT-4 level systems
and you know if they're supposed to be
playing the card of like you got to be in contact with the system don't be shy
tell us what you have learned from being in contact with the system
that supposedly invalidates all of my arguments
don't just be like oh yes you know there is mysterious ineffable knowledge
to be gained by interacting with the system that invalidates all of your analogies from
between gradient descent and natural selection
and the case from evolutionary biology is that
as it has already falsified any hoped for general rules about if you optimize for a thing
you get internal psychological desire for the thing
don't be shy don't play hints you know
I feel like this is already played out in a sense where people on Twitter
be like only those completely ignorant of deep learning
you know Eliezer has never built a deep learning system I have
and then like you know Jeff Hinton the like actual literal inventor if anyone is of deep learning
starts you know basically starts saying some pretty similar things
yeah so you know maybe Sam Altman feels that Jeff Hinton is also not sufficiently in contact
with the details of GPT-4
perhaps he should say that too you know don't just go after Eliezer here
like also go after like Jeff Hinton saying some similar things right
you know Jeff Hinton has not been sufficiently in contact with the details
the truly large scale systems to have an opinion here
but you know better yet say what you know say what you have observed
what do you what do you think you know and how do you think you know it
you know say which of my arguments falls down based on which truth about reality
that is to do from which observations about GPT-4 this kind of important
you know if you're taking it seriously you shouldn't be trying to shrug it off like that
don't be shy tell tell you know expose your arguments to public consideration there
don't just rest on your authority because as much as I you know
if you're going to rest on your authority instead of your arguments
then somebody may perhaps point out first of all that you're doing that
and second of all that you might have reasons to be less than fully blindly trusted
by the human species given your current position
if you're going to rest on your authority and your position instead of on your arguments
since maybe January or February whenever you did the bankless podcast
this will be your fourth ish one
what has changed in the discourse from your perspective
since you've been out there talking about this stuff more in earnest
the time article was published there's been
I think people are more at least aware of some of the stuff going on
have you seen any progress have you updated any of your mental models on the
it's more hopeful than it's going better than I would have expected
unfortunately there's like a very large gap of how much better it would have to go
before I started expecting to live
but yeah it's gone better than expected now if it just needs to keep steadily doing that
for another several years at an increasing pace and then it will like have gone so well
that we might even survive
now there's political interest there's they haven't come out with anything terribly
neither the Republicans nor the Democrats in the United States
have committed to themselves anything terribly stupid yet
there's an alternate world where China didn't like
promote the regulations it did on
AI where they could have looked like much less receptive to an argument
about an international alliance and coalition
things are going a little bit better than I expected but they
seems like they need to go like out of uncharacteristically well for a history book
the only thing remotely close to it is that we still haven't used nuclear weapons
since the first two and despite some close calls
and we would need to like be reaching a couple of levels above that
to pull this off correctly it's harder
I don't know if there's 10, 20, 50 people in the artificial intelligence community
at large and maybe I'm including Mark Zuckerberg
because you know Yad Likun reports into him
and there's a handful of people with real influence
and I think strategy I don't know what you what number you would put on that
maybe two dozen something like that
I mean I think that like
Emma Sasabis and Shane Leg are the only people who think in a way where they like
might have a strategy for something beyond the grandisement of their own company
for talking about people that can actually make decisions to some end
and have some influence to some end around alignment and slowing down
and maybe have the influence to coalesce with other people
not whether or not they'd actually do it but
that they might have the influence and standing to try to do this
within the artificial intelligence community I don't know what that number is
but it's smallish right
well I mean it's unfortunately seems like it might be growing
and you know that the nature of the thing is that
single defectors can blow that up
which META would certainly try to do
so yeah like the thing is that Sam Altman does not have the power to shut down META
the actions they can do at this point are calling in international governments to shut down everything
Agreed but I want to get in the head of someone that
there's the observers
and maybe you could say venture capitalists actually have some influence
but I would argue probably very marginal
I want to get in the head of someone that actually is working at one of these companies
on artificial intelligence
and so an arbitrary AI researcher who's been doing it for five years
and maybe doesn't have some super moral opinion or influence about the direction this stuff is headed
do you think there's any decisions that someone like that has made that has made doom
besides just working on their job that has made AI do more likely
I mean I expect they also gave scornful looks at parties to anybody who talked about it potentially being dangerous
that probably didn't help either
but mostly just their work
I'm not sure I understand like where the questions going
I'm just curious as there's a lot of rank and file people that can't influence the big picture of all of this stuff
and they're doing their jobs
and so what would you say to there's far more people that are just doing their jobs in front of them
that can't actually change the trajectory of META or open AI or anthropic or deep mind and all of that
what would you recommend to those people that are just working in the field of artificial intelligence
should they just try to influence the little island they're on
should they do the Jeff Hinton thing and step out and try to be more influential externally
is there any advice you give to just more people that are in the field and have some level of expertise
but aren't influential enough to actually do anything at scale within the companies
not really
you know like
reality is what it is
like
come the next set of alarming news
come the end of days
not you're necessarily going to know which days are the end ones
but you know there might be a moment of increasing horror and panic before the end
you know like they'll look back and think what they think I guess
there are obviously a ton of smart people working in this field
and there's definitely a decent number that as they've been closer and closer to it
definitely have fears about artificial intelligence and we quoted some of the stats at the top
there aren't many people that are vocal
and have cogent incredible arguments pushing back that I found
pushing back on what you're saying
and there's people that might disagree with some of the probabilities
Paul Cristiano among them
Robin Hansen seems to have a much more techno normal attitude about these things
but in other words he expects like all the humans to doubt
but thinks that our successors will you know exist in a state of extreme competition
that reduces them to the bare means of survival
but Robin Hansen is fine with that
is Robin Hansen's actual position unless I've misunderstood it
why do you think that
why do you think that you don't have better critics
why are you the only one that sees this so clearly
and is advocating this so vocally
I think that our planet's general system for the production of educated people
and public intellectuals is kind of falling apart
I didn't do it
tried to repair it
that didn't work at all
yeah
I don't know
in some ways I feel like a leftover from a slightly more functional period of civilization being raised by their books
and I
you're not going to find a convincing story here
you're not going to find a story that causes the universe to make sense to you
and makes you feel like there's no more anomaly
that things aren't somehow wrong
this is how it all turned out
all I can say is that you don't need to theorize the thing wrong with the universe
you can just observe that 22 years ago now
when I first called that this was predictably going to be a problem later
and somebody ought to start handling it
humanity continued to assign it
essentially indistinguishable from zero priority
the various people who
various other people who were pretenders to the position of caring about humanity's problems
like went for papers claiming that stuff was going to be 30 years off as of three years ago
you're not going to find an explanation that makes you feel good about how it played out
just that it did happen to play out that way
humanity did decide not to prioritize this problem
you're not seeing a bunch of
in a certain sense you're not seeing a bunch of other people with
like built up arguments in this field because
you didn't pay for it
humanity made its decision there
what it was going to prioritize and here you are now
sort of say that back you needed the credibility
and have thought about all this stuff over such a long period of time
and that wasn't incentivized when you were doing it financially
to go down all these different rabbit holes
and think about all these different things
and that's an observable fact of the world
and any story that I can tell you about how Earth ended up in that situation
is going to be less probable than the direct observation of that being the situation we ended up in
you can verify that
nobody cared for a good long time
where are the people like
trying to work all this out back in 2001
when we actually had some time in which to think about it
they didn't exist
that part is directly observable
and the stories for
why from my perspective some of the arguments
so it seems so strange and non-cogent
well you know I could tell you a story about how social media destroyed everyone's sanity
created a world of relentless unreality
but you know
the direct observation is that the arguments aren't cogent
and I would just tell you to like yeah there's not some hidden story of how they're secretly cogent
trust your eyes
you don't need a story to explain it you just saw it happen
2075 you and I are grabbing a beer or coffee or whatever it is
how did we end up there
if you were to give your most optimistic path to us surviving
for the next 50 years
how do you think we end up there
the top option in the manifold prediction market
on assuming we survive how did that happen
says that humanity didn't manage to shut down
all the overly advanced day development work
long enough for human intelligence enhancement
or uploading or something
or by human intelligence enhancement I mean like using alpha fold 3
to test a broad variety of drugs on suicide volunteers
until you find something that actually increases intelligence
because if you do this at all you're doing it in a tearing hurry
before the algorithms get efficient enough to end the world
the AI algorithms get efficient enough that your GPU limits don't matter anymore
and can you explain like brain emulations and all that
why that might be a counterbalance to different intelligence
if you can scan a human brain in sufficient detail and emulate it
then you can potentially make that person smarter
once you have better read write access
this is not a zero danger thing to do
but unlike trying to build a super intelligence on the first try
it's something where you can kind of imagine it going right
and you make the human smarter until they go over that strange threshold
for automatically acquiring security mindset
and some other things and then I think they can maybe actually solve the alignment problem
to date there haven't been many people that agree with your perspective
back to our earlier point and there's been little to no incentive to
solve them because it's been fairly theoretical I think in people's minds
people have mistakenly thought that
people have mistakenly thought deliberately or in their emotions that it wasn't a problem
it's not that there's no incentive it's that the incentives were there but they did not see them
there was an incentive for humanity to launch a crash project on this 20 years ago
we did not humanity was blind to that incentive it doesn't mean the incentive doesn't exist
sorry keep going
economic incentive there was a short term
short term economic incentive
why do you think that if a government or a bunch of governments came together and said
hey we'll give
ten billion dollars fifty billion dollars to solve this problem
and drastically alter the incentives around it
it would it still wouldn't be solved
how can you tell whether they've got a solution or not
I've watched people try to make progress on the alignment problem
and
unless it's
really obvious to anyone
whether or not progress has been made they cannot make progress
that's why I gave I specifically said ten billion dollars per year
in prizes on mechanistic interpretability
on opening up the eye and understanding what's inside
because when you have successfully decoded some tiny aspect of the
giant and scudible matrices of floating point numbers
you can tell that you have done that
that is why
of the progress the progress has been concentrated there
and you can make progress in other
it is theoretically possible
to make progress in other places by
questioning yourself in the right way
by shooting down theories
yourself rather than waiting for somebody else to shoot them down for you
but
how can you even tell who has that ability
effective altruists were
decoding some funding to this issue
basically because I browbeat them into it
as I would tell the story
and a whole bunch of them like their theory of AI
three years ago was that we probably had about
thirty more years in which to work on this problem
because of an elaborate argument about how large an AI model needed to be
by analogy to human neurons
and it would be trained via the following scaling law
which would require this many GPUs
which at the rate of Moore's law
and this attempted rate of software progress
we got thirty years
and I was like
this entire thing falls apart at the very first joint
where you're trying to make an analogy between the AI models
and the number of human neurons
this entire thing is bogus
it's been tried before in all these historical examples
none of which were correct either
and
the effective altruists can't tell
that I'm speaking sense
and that the thirty year projection
has no grasp on reality
if they can't tell
the difference between a good and bad argument there
until
stuff starts to blow up now
how do you tell who's making progress in alignment?
I can stand around being like
no, no, that's wrong, that's wrong too
this is particularly going to fail
this is how it will fail when you try it
but as far as they know they're inventing brilliant solutions
it's the different, you know, it's
anybody can build an operating system
that they think is secure
building an actual secure operating system is much harder
and the difference, unless you're like quite good
at poking holes in your own operating system
which more people think they're good at than are good at
is
you know, the way you find out it's secure is that
somebody else pokes holes in it
and the holes get fixed
anything you observe to be true about the current system
there's a theory that it will also hold
with respect to governing something smarter than you
and they can say it will
and I can say it won't for the following reason
I can try to make a prediction about what goes wrong in advance
it's not always easy, it's easier to say
where things end up in that trajectory they follow to get there
but basically, you know, the question is
how do you launch this thing for the first time and have it work
given that stuff that works on the earlier systems
some of it will inevitably break on the later systems
later systems are different
they're proposing all kinds of wacky ideas now
which I feel like would tear apart like tissue paper
and they go on advocating them
like we'll make the AI do our AI alignment homework
I could like
briefly state among the reasons why that's a problem
it's the hardest thing you could try to align in AI to do
it's got to understand human psychology
and it's got to understand adversarial reasoning about computer science
and it's got to understand what happens
like how AI systems go wrong
it's got to be thinking about how AI systems go wrong
reflecting on how to design AI systems
you know, it's just this like
ah, build a system that helps you with
you know, the biomedicine of making humans smarter
so it just needs to understand neurons and neurochemistry
and not AI design
you know, make a system that works on nanotechnology
so you can try to like scan people finally enough to
upload and emulate them
and try to make them smarter under extremely controlled and dangerous conditions
but you know, like AI helping with alignment
it's like the act of somebody who wants somebody else to do their homework
that's why I call it having an AI do their AI alignment homework
and that's a great idea as far as they can tell
I think that humans are just kind of not good enough at this
is the impression I get after watching people fail at it for 20 years
nor did I solve it either
people can't tell when they're making progress
they can't tell what are good arguments or bad arguments
they're not going to be able to train an AI to tell the difference between good or bad arguments
as some are proposed
first of all, that's like among the hardest things you can ask an AI to do
and second, because their training data is going to be broken
so can you put a fine point on why alignment is such a hard problem to solve
it sounds like there's infinite
permutations or things that you actually can't
determine if they're correct or not
back to your example of making a secure operating system
you don't actually know and then
well, no, people are just overly optimistic about whether or not they've solved it
you can know whether a system is secure or not
sort of
okay, that's fair
so can you put a fine point
and your last answer kind of alluded to this
but just a fine point on why alignment in your opinion is so hard for us to solve
other than we haven't done it
so one way of looking at it is that
it's not quite fundamentally true
but like I think that's sort of true
and a way of looking at artificial intelligence
is that if you can tell the difference between a good or bad answer
you can maybe make something that gives you
good or answers
like if you can press thumbs up or thumbs down on something
you can make a thing that tries to get you to press thumbs up
and the more powerful it is, the more it can get you to press thumbs up
so when it...
the alignment problems are ones where it's hard to press thumbs up
in a way that's reliable
that's always right
if you're asking it to give you the design for a nano system
maybe it gives you like a bunch of DNA strings for proteins
that assemble into a nano system
and the question is like
well is this secretly going to destroy the world?
are you going to have a human peer at those DNA strings
and press thumbs down if they think it destroys the world
and thumbs up if they think it does what it's meant to do?
it's difficult to get the training data
even leaving aside questions of if the training data
is going to generalize well way out of distribution
like the verifier is broken
we cannot verify the most important things that we would like an AI to do
if we could we could just like verify all its actions in the first place
how can you tell if an argument is like trying to persuade you
using not totally valid means
like I can stare at a set of arguments and flag the ones that are using
invalid forms of argumentation that are obvious to me
can I catch subtle influences?
no
am I like maybe just like wrong about whether something is an invalid argument or not
maybe
it's like an amplifier for things you can discern
and I've yet to hear an account of
what outputs you would have from an AI
that saves the world from the next day I built six months later
such that you can verify exactly
whether or not that output is a good output
and this is like one of the foundational problems
there are others
I would advise Googling
AGI ruin a list of lethalities
which is like the other 42 items on the list
how so you reached the conclusion that were
likely near 100% certainty headed towards doom
two years ago is that about right
I mean over over time
over time
how have you how have you led your life differently since you've reached that conclusion
after playing out the obvious
after having like played out and failed at some of the obvious things to try
I was like okay you're all doomed took like a year and a half sabbatical more or less
when was that
like
beginning at 21 to 22
sort of
I mean one way of looking at it would be to check the start and end dates on project lawful
the giant fictional piece I co-wrote with a co-author
which of course has like a bunch of allegory for AI built into it
because you know I can't actually not do that in my fiction
or I could but who would want to
but you know like it blurs a bit around the edges
like towards the end of that I was like doing a bunch of other things simultaneously
and not just working on the thick just like finishing up the thick
but yeah so like start of project lawful to I don't know like a bunch of the way through
like one and a half years after the start of project lawful is about how long the sabbatical was
and so you took a sabbatical
do you live your life any differently now other than you come on podcast occasionally
I mean I've still got my lifelong physical health and stamina issues
so there's a lot of frantic ways of enjoying myself before the end of the world that are more or less close to me
and I've just like sort of never been all that
energetic person I think for reasons I don't actually know to what extent it's tied with the health thing
I am suspicious of insufficiently physiologically deterministic accounts of mental states
but you know nonetheless like for one reason or another like I've sort of like
never been all that tempted by frantic hedonic dissipations
so you know to me like self indulgence is like going off and writing a giant piece of fiction
we have different interests in that regard
what I want to put a final point on all the AI DOOM fund we're having
if let's say someone listens to this and they are sufficiently convinced that we're on that trajectory
what can they do to help humanity survive
write your congressperson
tell them that you'd like to see a giant international moratorium on artificial intelligence progress
and you'll back them on that and you'd like to see them move forward on it
perhaps later more coordinated political action will emerge
but there isn't actually like a clear website I can direct you to at the moment
one question I want to be explicit about is why would anything super intelligent pursue things that are so bad
as ending humanity and all the other things that we've talked about
so the way I would now phrase it is that there's multiple reflectively stable fixed points of optimization
now what do unearthed I mean by that
suppose that you take Mohandas Gandhi and offer him a pill that makes him want to murder people
current Gandhi correctly models that if he takes this pill his desires will shift
he will then go off and murder people current Gandhi does not want people to be murdered
so current Gandhi refuses the pill
and the issue is that this generalizes
if you have something that only wants to make tiny molecular spirals
out of all of or like it has some much more it ended up with some much more complicated and messy set of desires
but in the end the like one of the components there that didn't saturate out
that wasn't like just easy to fulfill and be done with
that just like scaled linearly with how much stuff they're satisfying that there was
turned out to be like cheapest to satisfy with tiny molecular spirals
it's a preference that matter have a certain particular shape
if that's the way you are
then if you imagine modifying your code to want something different instead
you'll project there being fewer tiny molecular spirals in the future
and so your current preferences current utility function
ends up less satisfied if you execute the self modification
and there can be minds more complicated than that
but they're being complicated even if they're in some sense unstable
doesn't mean that what they end up falling into as a stable attractor
is be nice to all sentient life
people do tend to invent stories how you could end
you could start out wanting only paper clips
and end up wanting to be nice to all sentient life
but you don't actually end up with more paper clips that way
the way you end up with max numbers of paper clips is by turning everything around you into paper clips
if that's what you start out wanting that is stable
and this is counterintuitive to some people and very intuitive to other people
and I have a lot of respect for the people who are like
but you know what about the mysterious uplifting goodness
qualia of helping other people would it not know this
would it not understand it would it not gravitate it
and the answer there is like it would understand how you feel about it
but its exact understanding of how you feel about it would not in its own internal system
look like paper clips
like the thought that produces this uplifting feeling in you
would not produce an uplifting feeling in it
because it would not have been built as a kind of thing that feels uplifted by that thought
would not have ended up as a kind of thing that feels uplifted by that thought
there are all kinds of minds and some of them are not friendly
even up at the super intelligent level
that you as a human with a complicated internal philosophy
have a sufficiently complicated internal system where you can say like
but which goals are better than others
the fact that you can compare them along this betterness metric
like that betterness metric is not inherent in the goals
it's in you as a kind of thing that is evaluating the goals in these ways
a bunch of this is in the less wrong sequences I'm afraid
and rationality from Aida Zombies
I'll possibly think they might have like cut the med ethics sequence out of that one
but it's definitely in the online version
the 46 hour audio book if people want to dive in
I'm probably not going to be able to give an answer fully to the satisfaction of people
who are like but that uplifting feeling
all I can do is say that like
I know how you feel I used to feel that way
I have tried to write very extensively about how minds are put together
in a way where you can see this as like that to be uplifted
is a fact about some minds but not others
that they are built in such a way as to find some lots of uplifting
to find some goals better than others
I think you can pursue that analysis
you can analyze it to the point where it's very clear
that things that start out wanting paper clips
do not want to stop wanting paper clips
and things that may have much more complicated goals than that
do not settle into an attractor of being nice to sentient life
just because they are complicated
I myself am no different in a way
I would like the galaxies to be full of sentient happy life
that looks upon other life with empathy and sympathy
and if you offer me a pill to make me stop wanting this
I will refuse that pill
because it would then the galaxies
then I'm not able to influence the galaxies in that direction
if I don't want to influence them in that direction
even because I'm a complicated sort of thing
that has opinions about what kind of being I want to be
and I don't want to be the kind of person
who stops caring about the fate of sentient life
and yet from the perspective of a paper clip maximizer
I'm a kind of thing that just
even when I contemplate filling a whole galaxy full of paper clips
I just am not moved by this thought
I'm a kind of thing that will turn galaxy after galaxy
into happy sentient life living lives worth living
eudaimonia, empathy for others
learning and taking joy in discovering new things
I will just turn galaxy into this kind of complicated thing
that satisfies my utility function
and not set aside even a single galaxy for paper clips
because I just don't care about paper clips
and I don't want to care about paper clips
if you offer me a pill that makes me care more about paper clips
I'll refuse the pill
and other possible minds are like that
they'll just turn it all into paper clips
and they don't want to turn into sentient life
which is to say that there's multiple
reflectively stable fix points of planning systems
I think I saw something recently
Jan Lacoon say that alignment just isn't as hard
as you think it to be
people that say that that alignment is a real concern
that they believe that it is something
but it's just not as hard as you make it out to be
do you think that they're just miseducated about the
difficulty around all the different permutations
and thoughts that it requires?
I think that I keep asking Jan to spell out
exactly how he intends to align stuff
so that I can immediately tear it apart of course
and he doesn't spell it out
it's his plans are like well we will just like make it
to be submissive, literally his term
and to which I said on Twitter like
Jan has never, Jan clearly shows his unfamiliarity
with the prior literature here
my 1.8 million word BDSM decision theory
Dungeons & Dragons thick has as one of its primary themes
whether an entity being submissive is enough
to make it easy to steer
which Jan refuses to read for some reason
and then I like provide a quote of the like thick
actually dealing with that topic matter
I thought it was funny
You have a BDSM drag, is this a book?
This is Project Lawful
Got it, it's a BDSM Dungeons & Dragons
Decision theory
It's just the kind of thing I end up writing
when I take a sabbatical
I was going to say we have different indulgences
you and I, I wanted to make sure I heard that
Yeah, so that was a bit of an amusing thing
I happen to say we'll just make superintelligence
as submissive to us
they won't take power unless they want power
and it did so happen that I'd written a giant thick
about this that I could then be like
well he didn't read my 1.8 million words
Dungeons & Dragons thick
but in fact on a somewhat more serious level
people have been trying to explain over and over
including Tian personally
and he has as I understand it in some cases
acknowledged the force of the arguments
and then seemed to forget them next time
that things that want more or less arbitrary stuff
want power so they can get that stuff
you don't have to specifically want power
as a terminal value to want power instrumentally
so if his argument is they won't take power
unless they're built to want power
which is like more or less this kind of thing
he said over and over
then you can see right there
that he's unfamiliar with
what sure sounds like a valid argument
and told him several times
and that sure is all over the literature
so you know you don't need to have
elaborate theories of his internal state
it's enough to look at his outputs
I would be remiss not to ask
given my actual job is that of a venture capitalist
what role do you think
these sees as funders of a lot of these businesses
right not in totality obviously Microsoft
and Google and Facebook and all that are funding
in mass but do you think there's a moral
obligation to not fund incremental A.I. companies
I guess if I were to put the lens back on my industry
and the people in pursuit of capitalistic objectives
by means of venture capital investing
the thing I'm an expert on
is to tell you the consequences of your actions
if we don't just fall over dead really quickly
then you know news keeps coming
there's more excitement
there's more fear, nervousness
maybe there's even like a few months of horror
where it's clear that things are going down and not well
if you are a kind of thing that looks back
and was like yeah I'm
as my life ends I'm fine with having invested in A.I. companies
go ahead and invest
I realize that that may sound a bit passive aggressive
but all I can
I feel like the thing that is my place to do is try to time
minds together across time try to
have people aware of the futures that they're entering in their past
and there is maybe something to be said about what is true
what is real what is predictable
what you can bet on in a prediction market
it's kind of difficult to bet on the end of the world
but hey you can like go to manifold markets
and do it anyway with play money
yeah something doesn't quite sit right to me about
me being like
you have a moral imperative to
maybe somebody else can do it
well I felt like I would
it would be hypocritical not to ask the question
I guess the last one and maybe the most important one
so give me
you came out here originally and you didn't have a fedora on
and I was disappointed and I made you go back in
and I asked for the fedora
how did that come to be has this become a branding thing
I always think of you interviews in fedora
it's probably a little bit of a branding thing
not to the point where I might just not suddenly change it one day
I mean the way it started for me is that I tried various hats on
and I liked the way a fedora looked
and other people have opinions about that but they're not dating me
they're opinions they don't get to issue
change requests for my fashion
now since the fedora
became an online issue I did
is it an online issue do people weigh in on this online
yeah
so I did say like
look I don't accept fashion
change requests from people who are not dating me
if you want me to stop wearing fedora
you know what you have to do
top tweet bottom tweet in particular
you need to suggest some alternate form of hat wear
which my actual council of girlfriends
will like better than a fedora
and get them to have me change my hat gear
so people have in fact submitted various suggestions
to aliezer.girlfriends at gmail.com
and they're debating it
so it's possible to have a sudden change of hat gear at some point
if they can settle on a piece of hat gear they like better
and meanwhile you know like
yeah I guess it's slightly iconic and I don't mind
we can do a follow up episode for the great reveal
when you're changing the head gear
I won it on this show first
I'm not sure I can promise that one
well thank you for doing this
for the fedora
farewell sir
that's exactly give the people what they want
you
