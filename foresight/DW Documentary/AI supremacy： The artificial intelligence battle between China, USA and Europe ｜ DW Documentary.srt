1
00:00:00,000 --> 00:00:09,240
After many, many years later, maybe we will look back.

2
00:00:09,240 --> 00:00:15,080
We recognize that, oh, that's the moment when everything changed.

3
00:00:15,080 --> 00:00:19,200
In five years, AI is going to literally be in everything we do.

4
00:00:19,200 --> 00:00:26,400
As humans, we have to, first of all, understand that this is going to happen.

5
00:00:26,400 --> 00:00:31,440
Many technologists try to solve human problems using technology when actually what we need

6
00:00:31,440 --> 00:00:35,200
are human solutions.

7
00:00:35,200 --> 00:00:38,360
The deeper question is, what does it mean to be human?

8
00:00:38,360 --> 00:00:41,920
What are the things I'll still be proud of?

9
00:00:41,920 --> 00:00:48,080
I find this moment extremely profound, because it really forces us as a humanity to think

10
00:00:48,080 --> 00:00:52,760
through exactly what consciousness is, what makes humans human.

11
00:00:52,760 --> 00:00:56,040
There's a whole lot at stake here.

12
00:00:56,040 --> 00:01:00,800
It's careers, unbelievable amounts of money, and who gets to shape the future.

13
00:01:26,040 --> 00:01:32,240
It's already begun.

14
00:01:32,240 --> 00:01:36,320
A global race for supremacy in the age of artificial intelligence.

15
00:01:36,320 --> 00:01:42,000
China, the United States, and the European Union are vying for economic growth, political

16
00:01:42,000 --> 00:01:44,440
influence, and power.

17
00:01:44,440 --> 00:01:50,200
So are the big tech companies, and startups are hot on their heels.

18
00:01:50,200 --> 00:01:55,840
For those who lose, there'll be no second chances.

19
00:01:55,840 --> 00:02:01,120
Jonas Andrewles founded one of the most influential European AI companies.

20
00:02:01,120 --> 00:02:06,680
With his help, the European Union could catch up with the world's AI leaders, become independent

21
00:02:06,680 --> 00:02:13,840
from the US and China, secure a prosperous future.

22
00:02:13,840 --> 00:02:18,560
This is a watershed moment for Europe, the last roll of the dice.

23
00:02:18,560 --> 00:02:23,360
If Europe wants to decide how to use technology in accordance with European values, then it

24
00:02:23,360 --> 00:02:27,880
has to be able to build that technology itself.

25
00:02:27,880 --> 00:02:33,200
Thomas Wolff co-founded Huggingface, a major open source AI platform.

26
00:02:33,200 --> 00:02:37,880
He wants to stop one of the most powerful technologies in human history from ending

27
00:02:37,880 --> 00:02:40,640
up in the hands of a few corporations.

28
00:02:40,640 --> 00:02:46,160
What we need is a multitude of players, not just one that owns AI.

29
00:02:46,160 --> 00:02:51,360
We don't want a future where such a fundamental technology is in the hands of a single company.

30
00:02:51,360 --> 00:02:55,680
There are plenty of films where that's the mark of a dystopia, one company that controls

31
00:02:55,680 --> 00:03:03,680
everything.

32
00:03:03,680 --> 00:03:06,720
Han Chao is a Chinese AI entrepreneur.

33
00:03:06,720 --> 00:03:13,280
He wants his company, Gina AI, to be successful on both the Western and Chinese markets.

34
00:03:13,280 --> 00:03:18,480
So what role do Chinese AI companies play in this global race, and how is the Chinese

35
00:03:18,480 --> 00:03:22,200
Communist Party using them to achieve its political goals?

36
00:03:22,200 --> 00:03:28,200
Chai GPT or GPT-4, it basically serves as a brain, and this brain cannot be made in

37
00:03:28,200 --> 00:03:29,200
the US.

38
00:03:29,200 --> 00:03:32,640
So this is some worry that the Chinese government has.

39
00:03:32,640 --> 00:03:40,320
The culture of each country, how each government runs this country will eventually be reflected

40
00:03:40,320 --> 00:03:44,320
into this brain.

41
00:03:44,320 --> 00:03:49,480
It is absolutely clear that at the highest levels of leadership in the United States

42
00:03:49,480 --> 00:03:55,240
and in China, artificial intelligence is viewed as foundational to the future of economic

43
00:03:55,240 --> 00:03:59,640
and military power.

44
00:03:59,640 --> 00:04:05,840
We've created an economic and financial system that's based on the assumption that everything's

45
00:04:05,840 --> 00:04:07,560
going to keep running smoothly.

46
00:04:07,560 --> 00:04:10,200
We get cheap gas from Russia.

47
00:04:10,200 --> 00:04:14,000
The Americans look after us, so we don't need to spend money on arms.

48
00:04:14,000 --> 00:04:19,280
And as always friendly, we've gotten comfortable, and now we have to break out of that comfort

49
00:04:19,280 --> 00:04:25,160
and say, we can go on the political offensive again, we can compete, we can create competitive

50
00:04:25,160 --> 00:04:30,520
conditions here, we can make sure that the cool companies we have can also grow and play

51
00:04:30,520 --> 00:04:45,480
a role in the global market.

52
00:04:45,480 --> 00:04:51,520
At the end of 2023, German Vice Chancellor Robert Harbeck made significant progress towards

53
00:04:51,520 --> 00:04:53,640
these geopolitical goals.

54
00:04:53,640 --> 00:05:02,200
Aleph Alfa, a German AI company, raised around half a billion euros from investors.

55
00:05:02,200 --> 00:05:07,640
It was one of the largest rounds of European financing for artificial intelligence technology,

56
00:05:07,640 --> 00:05:12,120
and it was a signal that the European Union can produce elite players in the field of

57
00:05:12,120 --> 00:05:15,160
AI.

58
00:05:15,160 --> 00:05:19,400
Aleph Alfa's founder and CEO is Jonas Androulis.

59
00:05:19,400 --> 00:05:25,800
His given priority to investment from German industry, SAP, Bosch and the Schwarz Group,

60
00:05:25,800 --> 00:05:43,440
which owns supermarket retailers Liedl and Kauffland.

61
00:05:43,440 --> 00:05:45,880
I was an amateur radio enthusiast.

62
00:05:45,880 --> 00:05:49,000
I soldered radios together and built my own antennas.

63
00:05:49,000 --> 00:05:53,240
Early on, my father had computers at home, so I was able to start programming and playing

64
00:05:53,240 --> 00:06:03,320
around with them at a very young age.

65
00:06:03,320 --> 00:06:07,320
When we started out, the term generative AI didn't exist.

66
00:06:07,320 --> 00:06:09,360
Hardly anyone had heard of open AI.

67
00:06:09,360 --> 00:06:10,840
We were very technical.

68
00:06:10,840 --> 00:06:13,840
We managed to create category defining innovations.

69
00:06:13,840 --> 00:06:21,720
We were just nerds, researchers.

70
00:06:21,720 --> 00:06:26,560
And there were times when I felt like I wasn't coping with the amount of work and the challenges.

71
00:06:26,560 --> 00:06:31,120
Every night, I could answer emails until I was so tired I fell off the couch.

72
00:06:31,120 --> 00:06:39,080
And there were still things I was neglecting, which I didn't want to neglect.

73
00:06:39,080 --> 00:06:44,960
In spring 2023, the European AI landscape was a lonely place.

74
00:06:44,960 --> 00:06:50,960
With his startup, Alfa, Jonas Androulis had built the only generative AI that could compete

75
00:06:50,960 --> 00:06:53,120
at an international level.

76
00:06:53,120 --> 00:06:59,320
He'd acquired the expertise from his work as a high-ranking AI researcher at Apple.

77
00:06:59,320 --> 00:07:08,000
Suddenly he'd become one of Europe's great hopes in the global AI race.

78
00:07:08,000 --> 00:07:13,120
Right now we simply can't cope with the onslaught of potential customers and partners.

79
00:07:13,120 --> 00:07:18,600
Most of the German stock index companies have been in touch, lots of medium-sized companies.

80
00:07:18,600 --> 00:07:23,720
There are briefings, press engagements, events, an unbelievable amount of stuff.

81
00:07:23,720 --> 00:07:28,600
It's like the Cambrian explosion right now, an incredible amount of new and creative things

82
00:07:28,600 --> 00:07:29,600
are emerging.

83
00:07:29,600 --> 00:07:36,760
We're the only Europeans to be involved on this scale.

84
00:07:36,760 --> 00:07:38,480
It was an exciting development.

85
00:07:38,480 --> 00:07:43,240
We also had things like open AI and maybe ours were even cooler.

86
00:07:43,240 --> 00:07:47,680
What is missing and what I think deserves more attention in the EU is why there are

87
00:07:47,680 --> 00:07:54,440
not more domestic companies that actually grow and scale.

88
00:07:54,440 --> 00:08:01,200
A lot of company leaders, startup innovators end up going to the US and the access to capital

89
00:08:01,200 --> 00:08:06,280
is really one of the main challenges.

90
00:08:06,680 --> 00:08:14,160
If you look at what we did, it's enabling technology.

91
00:08:14,160 --> 00:08:19,640
What I need for that is a carefully selected, effective team of brilliant researchers.

92
00:08:19,640 --> 00:08:22,200
Then I need money.

93
00:08:22,200 --> 00:08:26,040
More money than you normally get as a German startup.

94
00:08:26,040 --> 00:08:29,120
These days we're talking about billions.

95
00:08:29,120 --> 00:08:34,160
And then you need partners to help you, the kind of help that money can't buy.

96
00:08:34,160 --> 00:08:39,040
Open AI doesn't just get 10 billion from Microsoft, it also gets incredible support

97
00:08:39,040 --> 00:08:45,960
in integrating its technology into all Microsoft products and platforms.

98
00:08:45,960 --> 00:08:51,440
At the time, Aleph Alpha had 60 employees at various locations.

99
00:08:51,440 --> 00:08:56,040
Most were at the company's headquarters in Heidelberg in southwest Germany, unlike

100
00:08:56,040 --> 00:09:02,480
Open AI, and Drullus wasn't gearing his AI towards private users, but rather industry

101
00:09:02,480 --> 00:09:04,280
in the public sector.

102
00:09:04,280 --> 00:09:08,320
But they tend to be sluggish and not easy to win over.

103
00:09:08,320 --> 00:09:11,400
That posed a challenge for Jonas Andrullus.

104
00:09:11,400 --> 00:09:15,280
He needed pilot projects to prove his technology works.

105
00:09:15,280 --> 00:09:19,080
You stand here and you're greeted by a virtual person.

106
00:09:19,080 --> 00:09:20,680
Hello, may I help you?

107
00:09:20,680 --> 00:09:21,680
Where do you want to go?

108
00:09:21,680 --> 00:09:24,120
Then I can use the screen.

109
00:09:24,120 --> 00:09:27,120
We have to keep moving in this direction.

110
00:09:27,120 --> 00:09:28,120
That's not good.

111
00:09:28,120 --> 00:09:29,120
That's my job.

112
00:09:29,360 --> 00:09:32,400
I know it is, but we're very happy to still have you.

113
00:09:32,400 --> 00:09:35,400
She'll probably be taking her well-earned retirement soon.

114
00:09:35,400 --> 00:09:37,720
It's become a standard question.

115
00:09:37,720 --> 00:09:42,160
I ask if you'd like to stay on a little longer, because I need you, and because there aren't

116
00:09:42,160 --> 00:09:45,200
enough skilled workers coming in.

117
00:09:45,200 --> 00:09:47,200
It's a big issue.

118
00:09:48,800 --> 00:09:51,800
That's important for us.

119
00:09:52,800 --> 00:09:54,800
Solidarity.

120
00:09:55,800 --> 00:09:59,800
And, of course, the main topic is innovation.

121
00:10:00,800 --> 00:10:06,800
Heidelberg is one of the first municipalities in the world to introduce an AI citizen assistant

122
00:10:06,800 --> 00:10:10,800
using a language model provided by Aleph Alpha.

123
00:10:13,800 --> 00:10:18,800
We have a partner, a customer, with whom we can look at these new technologies.

124
00:10:18,800 --> 00:10:23,800
They also act as a testimonial for us, because anyone can go and try it out.

125
00:10:23,800 --> 00:10:29,800
That's a huge advantage, because a lot of our customers don't want to be named right now.

126
00:10:29,800 --> 00:10:32,800
They don't want people to know exactly what they're doing,

127
00:10:32,800 --> 00:10:37,800
so it's great to have a pilot customer with a bit of vision and courage.

128
00:10:38,800 --> 00:10:44,800
The hope is that in the long term, AI will improve public administration and speed up services.

129
00:10:45,800 --> 00:10:53,800
I just enter a question. Motor vehicle traffic on the B-37, which is a busy road.

130
00:10:53,800 --> 00:10:57,800
Then I get it to search. Of course, that's a very general question.

131
00:10:57,800 --> 00:11:02,800
The question now is what point in time, whether we're talking daily or annually.

132
00:11:02,800 --> 00:11:06,800
Of course, the AI has to work out what the user actually wants.

133
00:11:06,800 --> 00:11:09,800
The B-37 is closed between 7 a.m. and 6 p.m.

134
00:11:09,800 --> 00:11:13,800
Now I can ask, how do I apply for child benefit?

135
00:11:15,800 --> 00:11:19,800
That wasn't the right answer. It can't find anything now.

136
00:11:19,800 --> 00:11:24,800
So the error messages that we get back from the public and from tests we do ourselves

137
00:11:24,800 --> 00:11:30,800
get passed on to Aleph Alpha to figure out what needs to change to make the inputs more accurate.

138
00:11:30,800 --> 00:11:33,800
It's always about the accuracy of the input.

139
00:11:36,800 --> 00:11:40,800
The technology is still in the test phase and not yet reliable.

140
00:11:41,800 --> 00:11:44,800
2023 was a delicate time for Aleph Alpha.

141
00:11:44,800 --> 00:11:48,800
Jonas and Droolers needed fresh money from investors.

142
00:11:48,800 --> 00:11:52,800
Meanwhile, Microsoft and OpenAI were gaining more of an advantage.

143
00:11:53,800 --> 00:11:56,800
In fact, a race starts today and we're going to move.

144
00:11:56,800 --> 00:12:03,800
We're going to move fast and for us, every day, we want to bring out new things.

145
00:12:04,800 --> 00:12:09,800
On March 14th, 2023, OpenAI released ChatGPT4,

146
00:12:09,800 --> 00:12:13,800
the most powerful artificial intelligence to date.

147
00:12:13,800 --> 00:12:17,800
At the same time, it greatly reduced costs for users.

148
00:12:17,800 --> 00:12:22,800
For Jonas and Droolers and his team, it was a threat to their business model.

149
00:12:22,800 --> 00:12:26,800
We might have a poem about fly fishing from the perspective of a fish.

150
00:12:27,800 --> 00:12:32,800
And it goes on for hours.

151
00:12:32,800 --> 00:12:36,800
In the depths where shadows weave and play,

152
00:12:36,800 --> 00:12:39,800
in this cool clear waters where I stay,

153
00:12:39,800 --> 00:12:42,800
I am the fish beneath the silver stream,

154
00:12:42,800 --> 00:12:45,800
where life's a dream or so it seems,

155
00:12:45,800 --> 00:12:47,800
a wily being, sleek and sly,

156
00:12:47,800 --> 00:12:50,800
with ancient instincts to live and die.

157
00:12:50,800 --> 00:12:53,800
Yeah, it goes on.

158
00:12:53,800 --> 00:12:56,800
Yeah, they see the ends of my heart.

159
00:12:58,800 --> 00:13:01,800
I went to an apartment with a number of colleagues

160
00:13:01,800 --> 00:13:05,800
and we watched on a big projector screen

161
00:13:05,800 --> 00:13:07,800
the announcement of GPT4.

162
00:13:07,800 --> 00:13:10,800
Someone had a ChatGPT Pro account,

163
00:13:10,800 --> 00:13:13,800
so they could use GPT4 and we could play with it.

164
00:13:13,800 --> 00:13:20,800
And we were, like, very impressed and surprised by how good it was.

165
00:13:20,800 --> 00:13:24,800
It's not upsetting when someone comes out with a great piece of technology

166
00:13:24,800 --> 00:13:28,800
because we're researchers and building technology and that's how it is.

167
00:13:28,800 --> 00:13:31,800
You know, when you're a violinist and you go and you watch

168
00:13:31,800 --> 00:13:35,800
an amazing solo by an incredible violinist,

169
00:13:35,800 --> 00:13:39,800
you don't feel, oh, you know, I'll give up.

170
00:13:39,800 --> 00:13:41,800
You know, it's inspiring.

171
00:13:41,800 --> 00:13:43,800
I was a little stressed out.

172
00:13:43,800 --> 00:13:47,800
I was in the middle of conversations with potential investors, business partners,

173
00:13:47,800 --> 00:13:50,800
and I knew that in every conversation I was going into,

174
00:13:50,800 --> 00:13:53,800
somebody would say, but GPT4.

175
00:13:53,800 --> 00:13:56,800
I would have wanted to build GPT4 myself.

176
00:13:56,800 --> 00:14:00,800
Two years ago, get 200 million,

177
00:14:00,800 --> 00:14:05,800
be the first model at that level of capabilities

178
00:14:05,800 --> 00:14:07,800
out of Heidelberg, out of Europe.

179
00:14:07,800 --> 00:14:10,800
It caused a lot of frustration in the team and I saw that.

180
00:14:10,800 --> 00:14:12,800
That's painful to see.

181
00:14:13,800 --> 00:14:17,800
In fact, comparing open AI with ALEF Alpha was absurd.

182
00:14:17,800 --> 00:14:21,800
Open AI was almost half owned by tech giant Microsoft,

183
00:14:21,800 --> 00:14:25,800
which had pumped over 10 billion dollars into the company.

184
00:14:25,800 --> 00:14:29,800
Jonas Androulas had raised just 28 million euros.

185
00:14:29,800 --> 00:14:32,800
Still, he wanted to take them on.

186
00:14:32,800 --> 00:14:35,800
We're all under enormous pressure.

187
00:14:35,800 --> 00:14:37,800
We're fighting for security.

188
00:14:38,800 --> 00:14:41,800
We're all under enormous pressure.

189
00:14:41,800 --> 00:14:43,800
We're fighting for survival.

190
00:14:43,800 --> 00:14:46,800
We've created something world-class with a lot less money.

191
00:14:46,800 --> 00:14:50,800
We're basically at the forefront on the highest level.

192
00:14:50,800 --> 00:14:56,800
But we all know that there's now a wave of Microsoft money rolling towards us,

193
00:14:56,800 --> 00:14:59,800
and we can't do anything to stop it.

194
00:15:07,800 --> 00:15:11,800
Is it very easier to speak in French or English?

195
00:15:11,800 --> 00:15:14,800
I might find it difficult to make really French

196
00:15:14,800 --> 00:15:16,800
without English words everywhere.

197
00:15:16,800 --> 00:15:17,800
Yes.

198
00:15:17,800 --> 00:15:19,800
Yeah, because, yeah.

199
00:15:19,800 --> 00:15:22,800
While Jonas Androulas was filling the heat

200
00:15:22,800 --> 00:15:26,800
from industry top dogs Microsoft and open AI,

201
00:15:26,800 --> 00:15:28,800
Thomas Wolff was more worried

202
00:15:28,800 --> 00:15:31,800
about the fact that he could not speak English.

203
00:15:31,800 --> 00:15:34,800
He was worried about the fact that he could not speak English.

204
00:15:34,800 --> 00:15:38,800
With Microsoft and open AI, Thomas Wolff was more relaxed.

205
00:15:38,800 --> 00:15:40,800
He co-founded Huggingface,

206
00:15:40,800 --> 00:15:45,800
which has 200 employees and offices in Paris, New York and Amsterdam.

207
00:15:45,800 --> 00:15:48,800
The company has built a successful platform

208
00:15:48,800 --> 00:15:51,800
where programmers and companies can share AI models

209
00:15:51,800 --> 00:15:53,800
and further develop them.

210
00:15:53,800 --> 00:15:56,800
The philosophy, the mission and the values that we push

211
00:15:56,800 --> 00:15:59,800
are actually very European by some way.

212
00:15:59,800 --> 00:16:01,800
Being careful about the data,

213
00:16:01,800 --> 00:16:03,800
trying to build something responsible

214
00:16:03,800 --> 00:16:06,800
and not just go fast and break it.

215
00:16:08,800 --> 00:16:12,800
There are definitely Anglo-American values in chat GPT.

216
00:16:12,800 --> 00:16:15,800
We wondered whether we could set up a project

217
00:16:15,800 --> 00:16:17,800
to analyze and document that.

218
00:16:17,800 --> 00:16:20,800
For example, with benchmarks that could show

219
00:16:20,800 --> 00:16:24,800
whether a model has Anglo-American, French or German values.

220
00:16:25,800 --> 00:16:29,800
It would be interesting to do a comparative study

221
00:16:29,800 --> 00:16:33,800
between chat GPT and bloom chat, wouldn't it?

222
00:16:33,800 --> 00:16:36,800
If you ask a question in different languages,

223
00:16:36,800 --> 00:16:40,800
how different are the answers depending on the kind of question?

224
00:16:40,800 --> 00:16:43,800
Is the approach more American or European?

225
00:16:43,800 --> 00:16:46,800
That would be an interesting study.

226
00:16:46,800 --> 00:16:49,800
That would be an interesting study.

227
00:16:49,800 --> 00:16:52,800
That would be an interesting study.

228
00:16:57,800 --> 00:17:00,800
When I talk about pluralism of values,

229
00:17:00,800 --> 00:17:04,800
I mean that every population in the world has its own value system.

230
00:17:04,800 --> 00:17:07,800
We have a lot of different nationalities here

231
00:17:07,800 --> 00:17:11,800
and we have to ask ourselves, what are our values?

232
00:17:11,800 --> 00:17:14,800
What's important to us?

233
00:17:15,800 --> 00:17:21,800
Optimistic sparks in Europe, like new startups.

234
00:17:21,800 --> 00:17:25,800
Germany, Aleph Alpha is already a big player.

235
00:17:25,800 --> 00:17:29,800
In the UK, stability is obviously a very visible player.

236
00:17:29,800 --> 00:17:32,800
Here in Europe, in France, there's Mistral,

237
00:17:32,800 --> 00:17:35,800
there's a new player in Finland.

238
00:17:35,800 --> 00:17:37,800
In almost every European country,

239
00:17:37,800 --> 00:17:42,800
I see at least one or two startups with this ambition

240
00:17:43,800 --> 00:17:46,800
to become and to build something big.

241
00:17:47,800 --> 00:17:51,800
As idealistic as Thomas and his team from Hugging Face appear,

242
00:17:51,800 --> 00:17:53,800
there is also criticism.

243
00:17:53,800 --> 00:17:58,800
Open source or not, the end result is that a small elite of tech professionals

244
00:17:58,800 --> 00:18:00,800
is determining what our future looks like

245
00:18:00,800 --> 00:18:03,800
and what risks we're exposed to.

246
00:18:06,800 --> 00:18:08,800
All the business people I meet say,

247
00:18:08,800 --> 00:18:12,800
we need education, society has to educate itself.

248
00:18:12,800 --> 00:18:14,800
We only create the systems,

249
00:18:14,800 --> 00:18:17,800
but you can design those systems in different ways.

250
00:18:17,800 --> 00:18:21,800
For example, you can make it so that a person can understand what's going on,

251
00:18:21,800 --> 00:18:22,800
at least a little.

252
00:18:22,800 --> 00:18:26,800
This could be one of the obligations we impose on the industry.

253
00:18:32,800 --> 00:18:35,800
These machines have processed all cultural knowledge

254
00:18:35,800 --> 00:18:39,800
and are created by mathematicians who don't know anything about culture.

255
00:18:39,800 --> 00:18:41,800
That's a bit of an exaggeration, of course,

256
00:18:41,800 --> 00:18:45,800
but we have to find ways of explaining this to people who aren't interested in the math.

257
00:18:45,800 --> 00:18:48,800
They just use the machines as tools.

258
00:18:48,800 --> 00:18:50,800
They need to understand where the limits are,

259
00:18:50,800 --> 00:18:54,800
in which situations the machine will or won't work.

260
00:18:54,800 --> 00:18:56,800
Just like with GPS devices,

261
00:18:56,800 --> 00:18:59,800
where we recognize when they give us the wrong route.

262
00:19:00,800 --> 00:19:04,800
Isn't it great that such a huge research field is opening up?

263
00:19:09,800 --> 00:19:11,800
But there's also a huge gulf opening up.

264
00:19:11,800 --> 00:19:13,800
Who's actually responsible in the end?

265
00:19:13,800 --> 00:19:15,800
Because as a developer or researcher,

266
00:19:15,800 --> 00:19:17,800
you have a certain responsibility.

267
00:19:17,800 --> 00:19:19,800
It's not about restricting research,

268
00:19:19,800 --> 00:19:23,800
but when there are applications that are harmful to society,

269
00:19:23,800 --> 00:19:25,800
we have to be aware of that.

270
00:19:25,800 --> 00:19:28,800
There is a huge potential for manipulation.

271
00:19:28,800 --> 00:19:32,800
Just think of the influence of chat GPT on elections.

272
00:19:32,800 --> 00:19:34,800
I think there needs to be an antidote.

273
00:19:34,800 --> 00:19:36,800
More education.

274
00:19:36,800 --> 00:19:38,800
That's a good topic for the upcoming elections.

275
00:19:38,800 --> 00:19:42,800
What education do we need to stop us being manipulated?

276
00:19:48,800 --> 00:19:50,800
That's a big question.

277
00:19:50,800 --> 00:19:54,800
What happens when people start asking AI who they should vote for?

278
00:19:54,800 --> 00:19:58,800
Because the AI will give them an answer, as it always does.

279
00:19:58,800 --> 00:20:00,800
Who decides how it answers?

280
00:20:00,800 --> 00:20:02,800
Who should decide that?

281
00:20:02,800 --> 00:20:04,800
Unfortunately, I have no answer to that.

282
00:20:14,800 --> 00:20:17,800
Generative AI is developing at breathtaking speed

283
00:20:17,800 --> 00:20:20,800
and tech giants are battling it out in the ring.

284
00:20:20,800 --> 00:20:23,800
That's spurring development even more.

285
00:20:23,800 --> 00:20:26,800
There's a massive new market up for grabs.

286
00:20:29,800 --> 00:20:32,800
Leading AI experts worry that big tech,

287
00:20:32,800 --> 00:20:34,800
in its eagerness to compete,

288
00:20:34,800 --> 00:20:38,800
is creating technologies beyond our control.

289
00:20:39,800 --> 00:20:41,800
I think we've made a mistake

290
00:20:41,800 --> 00:20:45,800
when my Swedish countryman Carl von Linnaeus branded our species

291
00:20:45,800 --> 00:20:48,800
as homo sapiens, because sapiens means the thinking homo,

292
00:20:48,800 --> 00:20:50,800
the smart one.

293
00:20:50,800 --> 00:20:53,800
We're not going to be the smartest anymore.

294
00:20:53,800 --> 00:20:56,800
Maybe we should re-brand ourselves the homo sentience,

295
00:20:56,800 --> 00:20:58,800
the feeling human.

296
00:20:58,800 --> 00:21:03,800
We can feel curiosity, meaning, purpose, love.

297
00:21:03,800 --> 00:21:06,800
That is what really makes us unique.

298
00:21:10,800 --> 00:21:13,800
We should ask, how can we keep control over the machine?

299
00:21:13,800 --> 00:21:16,800
So that we can use them as tools to build a world

300
00:21:16,800 --> 00:21:21,800
where we can really have human flourishing with positive experiences.

301
00:21:27,800 --> 00:21:30,800
In 2014, when I founded the Future Life Institute,

302
00:21:30,800 --> 00:21:36,800
it was quite taboo to even talk about AI safety at all,

303
00:21:36,800 --> 00:21:39,800
because that would imply that it wasn't totally safe.

304
00:21:39,800 --> 00:21:42,800
And a lot of AI researchers thought that it would be bad for funding,

305
00:21:42,800 --> 00:21:45,800
and that only weird people worried about this.

306
00:21:47,800 --> 00:21:49,800
It was very much like coming out of the closet moment

307
00:21:49,800 --> 00:21:51,800
for people to sign this letter and say,

308
00:21:51,800 --> 00:21:54,800
oh, you too are worried?

309
00:21:54,800 --> 00:21:56,800
I think we should slow down a little bit.

310
00:21:56,800 --> 00:21:57,800
Oh, I didn't know that.

311
00:21:57,800 --> 00:22:00,800
And then it suddenly became socially acceptable.

312
00:22:03,800 --> 00:22:06,800
Max Tegmark and his Future of Life Institute

313
00:22:06,800 --> 00:22:08,800
published an open letter,

314
00:22:08,800 --> 00:22:10,800
warning that artificial intelligence

315
00:22:10,800 --> 00:22:13,800
posed an existential danger to humanity.

316
00:22:13,800 --> 00:22:17,800
Civilisation itself could be under threat.

317
00:22:27,800 --> 00:22:30,800
The letter was signed by hundreds of AI researchers

318
00:22:30,800 --> 00:22:32,800
and tech industry leaders,

319
00:22:32,800 --> 00:22:36,800
including Tesla boss and ex-owner Elon Musk,

320
00:22:36,800 --> 00:22:39,800
Apple co-founder Steve Wozniak,

321
00:22:39,800 --> 00:22:42,800
and touring award winner Joshua Benio.

322
00:22:42,800 --> 00:22:46,800
And it's been quite shocking that once we put this letter out

323
00:22:46,800 --> 00:22:49,800
and kind of a who's who of AI researchers signed it,

324
00:22:49,800 --> 00:22:51,800
the conversation really exploded.

325
00:23:02,800 --> 00:23:05,800
My worst fears are that we cause significant...

326
00:23:05,800 --> 00:23:07,800
We, the field, the technology, the industry

327
00:23:07,800 --> 00:23:10,800
cause significant harm to the world.

328
00:23:12,800 --> 00:23:14,800
I think that could happen in a lot of different ways.

329
00:23:14,800 --> 00:23:16,800
It's why we started the company.

330
00:23:19,800 --> 00:23:21,800
I think if this technology goes wrong,

331
00:23:21,800 --> 00:23:23,800
it can go quite wrong.

332
00:23:25,800 --> 00:23:27,800
And we want to be vocal about that.

333
00:23:27,800 --> 00:23:29,800
We want to work with the government.

334
00:23:29,800 --> 00:23:31,800
I think he was serious about that.

335
00:23:31,800 --> 00:23:33,800
I think that's kind of...

336
00:23:33,800 --> 00:23:35,800
So we were talking about existential risks

337
00:23:35,800 --> 00:23:37,800
and I also believe there are existential risks.

338
00:23:37,800 --> 00:23:39,800
There are also a whole spectrum of other risks.

339
00:23:39,800 --> 00:23:42,800
And I know of some, I talked to him a couple of times about this.

340
00:23:42,800 --> 00:23:45,800
He very much recognises them as well.

341
00:23:45,800 --> 00:23:47,800
On the one hand, of course, these warnings

342
00:23:47,800 --> 00:23:50,800
about the major power of this new technology

343
00:23:50,800 --> 00:23:53,800
also amplify the significance of the products

344
00:23:53,800 --> 00:23:55,800
that these people are building.

345
00:23:55,800 --> 00:23:58,800
So it could also have an indirect marketing effect.

346
00:23:58,800 --> 00:24:01,800
Right? Like look at the incredible things that we're building.

347
00:24:01,800 --> 00:24:04,800
But also, let's make sure that nothing goes wrong.

348
00:24:04,800 --> 00:24:07,800
And for that, they look to the politicians.

349
00:24:07,800 --> 00:24:09,800
The net effect of that could be that

350
00:24:09,800 --> 00:24:12,800
if heaven forbid something goes wrong,

351
00:24:12,800 --> 00:24:14,800
they could say, well, we warned you,

352
00:24:14,800 --> 00:24:16,800
but the politicians did not act

353
00:24:16,800 --> 00:24:18,800
or they did not act in time.

354
00:24:18,800 --> 00:24:20,800
So I'm looking at a paper here

355
00:24:20,800 --> 00:24:22,800
entitled Large Language Models Trained on Media Diets

356
00:24:22,800 --> 00:24:24,800
Can Predict Public Opinion.

357
00:24:24,800 --> 00:24:26,800
This is just posted about a month ago.

358
00:24:26,800 --> 00:24:29,800
This work was done at MIT and then also at Google.

359
00:24:29,800 --> 00:24:32,800
The conclusion is that large language models

360
00:24:32,800 --> 00:24:35,800
can indeed predict public opinion.

361
00:24:35,800 --> 00:24:38,800
I want to think about this in the context of elections.

362
00:24:38,800 --> 00:24:42,800
Should we be concerned about large language models

363
00:24:42,800 --> 00:24:45,800
that can predict survey opinion

364
00:24:45,800 --> 00:24:48,800
and then can help organisations into these fine-tuned strategies

365
00:24:48,800 --> 00:24:50,800
to elicit behaviours from voters?

366
00:24:50,800 --> 00:24:52,800
Should we be worried about this for our elections?

367
00:24:52,800 --> 00:24:54,800
Yeah. Thank you, Senator Holly, for the question.

368
00:24:54,800 --> 00:24:57,800
It's one of my areas of greatest concern.

369
00:24:57,800 --> 00:24:59,800
The more general ability of these models

370
00:24:59,800 --> 00:25:01,800
to manipulate, to persuade,

371
00:25:01,800 --> 00:25:05,800
to provide sort of one-on-one interactive disinformation.

372
00:25:05,800 --> 00:25:07,800
I'm nervous about it.

373
00:25:07,800 --> 00:25:10,800
I think people are able to adapt quite quickly

374
00:25:10,800 --> 00:25:13,800
when Photoshop came onto the scene a long time ago.

375
00:25:13,800 --> 00:25:16,800
For a while, people were really quite fooled

376
00:25:16,800 --> 00:25:18,800
by Photoshopped images

377
00:25:18,800 --> 00:25:20,800
and then pretty quickly developed

378
00:25:20,800 --> 00:25:23,800
an understanding that images might be Photoshopped.

379
00:25:23,800 --> 00:25:26,800
This will be like that, but on steroids.

380
00:25:26,800 --> 00:25:29,800
And the interactivity,

381
00:25:29,800 --> 00:25:32,800
the ability to really model, predict humans well

382
00:25:32,800 --> 00:25:35,800
as you talked about, I think is going to require

383
00:25:35,800 --> 00:25:38,800
a combination of companies doing the right thing,

384
00:25:38,800 --> 00:25:40,800
regulation and public education.

385
00:25:53,800 --> 00:25:56,800
2024 is a crucial election year.

386
00:25:56,800 --> 00:25:58,800
Not only in the United States, but worldwide.

387
00:25:58,800 --> 00:26:01,800
There will be European Parliament elections.

388
00:26:01,800 --> 00:26:03,800
There will be elections in India.

389
00:26:03,800 --> 00:26:05,800
I mean, it's a large amount of people in the world

390
00:26:05,800 --> 00:26:07,800
will actually go to the polls.

391
00:26:07,800 --> 00:26:10,800
And while we're living in this big experiment

392
00:26:10,800 --> 00:26:13,800
where it's very hard for independent researchers,

393
00:26:13,800 --> 00:26:15,800
journalists, civil society organisations

394
00:26:15,800 --> 00:26:17,800
to probe these models,

395
00:26:17,800 --> 00:26:20,800
that we may only find out, you know,

396
00:26:20,800 --> 00:26:24,800
what the harms and malign uses

397
00:26:24,800 --> 00:26:28,800
as a weapon against democracy were when it is too late.

398
00:26:31,800 --> 00:26:34,800
Shortly after Sam Altman appeared before the US Senate,

399
00:26:34,800 --> 00:26:36,800
he co-signed a statement

400
00:26:36,800 --> 00:26:38,800
along with a number of high-ranking executives

401
00:26:38,800 --> 00:26:41,800
from Google, Microsoft and other tech companies.

402
00:26:51,800 --> 00:26:54,800
The fact that it was the companies

403
00:26:54,800 --> 00:26:58,800
who themselves were asking for this type of regulation

404
00:26:58,800 --> 00:27:00,800
and it was the leading researchers

405
00:27:00,800 --> 00:27:03,800
who were asking for the government to get involved,

406
00:27:03,800 --> 00:27:06,800
that really was the turning point in the conversation.

407
00:27:10,800 --> 00:27:13,800
To understand the effect that generative AI

408
00:27:13,800 --> 00:27:17,800
was having behind the scenes of global politics at the time,

409
00:27:17,800 --> 00:27:21,800
you have to travel north to a small Swedish city called Luleå,

410
00:27:21,800 --> 00:27:26,800
around 150 kilometres south of the Arctic Circle.

411
00:27:26,800 --> 00:27:28,800
When people say that artificial intelligence

412
00:27:28,800 --> 00:27:31,800
is going to be like the next industrial revolution,

413
00:27:31,800 --> 00:27:34,800
I think they're underestimating its impact.

414
00:27:34,800 --> 00:27:36,800
It's not just going to be a new technology

415
00:27:36,800 --> 00:27:38,800
like the steam engine.

416
00:27:38,800 --> 00:27:41,800
It's like building a new species.

417
00:27:41,800 --> 00:27:44,800
A species that's much smarter than us.

418
00:27:44,800 --> 00:27:47,800
President Biden himself was having meetings

419
00:27:47,800 --> 00:27:50,800
on artificial intelligence in some cases

420
00:27:50,800 --> 00:27:52,800
as often as three times per week.

421
00:27:52,800 --> 00:27:55,800
And I will tell you that not very many things

422
00:27:55,800 --> 00:27:59,800
get on the president's calendar for three times a week.

423
00:28:03,800 --> 00:28:06,800
May 31st, 2023.

424
00:28:06,800 --> 00:28:10,800
The sirens and motorcades descending on this Swedish coastal city

425
00:28:10,800 --> 00:28:13,800
gave a sense of how much was at stake.

426
00:28:13,800 --> 00:28:16,800
Leaders came here to discuss nothing less

427
00:28:16,800 --> 00:28:20,800
than how humanity should react to the arrival of this new,

428
00:28:20,800 --> 00:28:23,800
albeit artificial, form of intelligence.

429
00:28:23,800 --> 00:28:26,800
What role should politicians play?

430
00:28:26,800 --> 00:28:30,800
Democracy needs to show that we are as fast as technology.

431
00:28:30,800 --> 00:28:34,800
You saw the first letter on asking for a course of six months.

432
00:28:34,800 --> 00:28:38,800
You saw yesterday a number of very, very insightful people

433
00:28:38,800 --> 00:28:41,800
signing up to say you need to do something

434
00:28:41,800 --> 00:28:43,800
for the very existential risks.

435
00:28:43,800 --> 00:28:46,800
And then you have the non-existential risks as well.

436
00:28:46,800 --> 00:28:49,800
Why is it important for the European Union

437
00:28:49,800 --> 00:28:52,800
to have a common policy with the US concerning AI

438
00:28:52,800 --> 00:28:54,800
and shouldn't other parts of the globe

439
00:28:54,800 --> 00:28:56,800
be included in the conversation?

440
00:28:56,800 --> 00:28:58,800
Europe is important, but this is bigger than Europe.

441
00:28:58,800 --> 00:29:01,800
US is important, but it is bigger than the US.

442
00:29:01,800 --> 00:29:04,800
But if the two of us take the lead with close friends,

443
00:29:04,800 --> 00:29:06,800
I think we can push something

444
00:29:06,800 --> 00:29:08,800
that will make us all much more comfortable

445
00:29:08,800 --> 00:29:11,800
with the fact that generative AI is now in the world

446
00:29:11,800 --> 00:29:14,800
and is developing at amazing speeds.

447
00:29:31,800 --> 00:29:35,800
Jonas Androulos was also invited to the top-level meeting in Sweden

448
00:29:35,800 --> 00:29:38,800
to represent the views of European AI startups

449
00:29:38,800 --> 00:29:41,800
and call for fair competition.

450
00:29:44,800 --> 00:29:47,800
Of course there are other AI companies in Europe,

451
00:29:47,800 --> 00:29:49,800
but we're the one that's keeping pace the most

452
00:29:49,800 --> 00:29:51,800
with the global leaders.

453
00:29:51,800 --> 00:29:53,800
I assume that's the reason why we're here,

454
00:29:53,800 --> 00:29:56,800
not because we're so charming.

455
00:30:06,800 --> 00:30:09,800
What kind of change is coming in some industries?

456
00:30:09,800 --> 00:30:11,800
How do you feel about that?

457
00:30:11,800 --> 00:30:14,800
We can raise more capital.

458
00:30:14,800 --> 00:30:16,800
I think we have...

459
00:30:16,800 --> 00:30:18,800
Two weeks ago I was at SFI conference,

460
00:30:18,800 --> 00:30:20,800
SAP SFI conference,

461
00:30:20,800 --> 00:30:22,800
and Christian Klein on his opening keynote

462
00:30:22,800 --> 00:30:23,800
he kind of said,

463
00:30:23,800 --> 00:30:25,800
our key partners for generative AI

464
00:30:25,800 --> 00:30:28,800
are Aleph, Alfa, Google and Microsoft.

465
00:30:28,800 --> 00:30:31,800
And then I'll have events coming up

466
00:30:31,800 --> 00:30:33,800
with HPE, with Antonio Neri.

467
00:30:33,800 --> 00:30:36,800
What do you think about

468
00:30:36,800 --> 00:30:39,800
our colleagues on the other end here,

469
00:30:39,800 --> 00:30:42,800
from Anthropics and the latest...

470
00:30:45,800 --> 00:30:47,800
Statements, etc.

471
00:30:47,800 --> 00:30:49,800
Oh, so the statements on safety?

472
00:30:49,800 --> 00:30:51,800
Yeah, like yesterday and so on.

473
00:30:51,800 --> 00:30:56,800
Long-term it is possible to conceive catastrophic events.

474
00:30:56,800 --> 00:30:58,800
I've had Brussels and Berlin

475
00:30:58,800 --> 00:31:02,800
and they basically are scared.

476
00:31:04,800 --> 00:31:07,800
We will start with Jonas Androulis,

477
00:31:07,800 --> 00:31:10,800
who's the founder and CEO of Aleph, Alfa.

478
00:31:10,800 --> 00:31:11,800
The floor is yours,

479
00:31:11,800 --> 00:31:13,800
and thank you very much for being with us today.

480
00:31:13,800 --> 00:31:15,800
All right, thanks for having me.

481
00:31:15,800 --> 00:31:17,800
I think we're all a little bit dizzy.

482
00:31:17,800 --> 00:31:19,800
The speed of change,

483
00:31:19,800 --> 00:31:21,800
like everybody I know that is in AI

484
00:31:21,800 --> 00:31:22,800
is kind of stressed out,

485
00:31:22,800 --> 00:31:24,800
and with this technology

486
00:31:24,800 --> 00:31:27,800
we're only even just stretching the surface.

487
00:31:27,800 --> 00:31:29,800
I fear that knowledge work

488
00:31:29,800 --> 00:31:31,800
is in the hands of the world

489
00:31:31,800 --> 00:31:33,800
and I fear that knowledge work

490
00:31:33,800 --> 00:31:36,800
is an important part of what is happening in Europe.

491
00:31:36,800 --> 00:31:39,800
So this is an opportunity for us

492
00:31:39,800 --> 00:31:41,800
to build new empires,

493
00:31:41,800 --> 00:31:42,800
to build new value,

494
00:31:42,800 --> 00:31:44,800
but it's also a risk

495
00:31:44,800 --> 00:31:47,800
that we're losing a substantial pillar

496
00:31:47,800 --> 00:31:48,800
that we're standing on.

497
00:31:48,800 --> 00:31:51,800
Thinking about how we can

498
00:31:51,800 --> 00:31:54,800
make this a fair playing field,

499
00:31:54,800 --> 00:31:57,800
because I think it's in everybody's interest

500
00:31:57,800 --> 00:32:00,800
that Europe will contribute to a safer future in AI.

501
00:32:00,800 --> 00:32:02,800
Thank you very much.

502
00:32:10,800 --> 00:32:12,800
While the U.S. and the EU

503
00:32:12,800 --> 00:32:14,800
were trying to come up with a common strategy

504
00:32:14,800 --> 00:32:16,800
on the other side of the world in China,

505
00:32:16,800 --> 00:32:19,800
an artificial intelligence ecosystem

506
00:32:19,800 --> 00:32:22,800
was emerging with its own set of rules.

507
00:32:22,800 --> 00:32:25,800
AI is a key part of China's efforts

508
00:32:25,800 --> 00:32:28,800
to become a global power.

509
00:32:28,800 --> 00:32:31,800
I always remember my mom and my dad

510
00:32:31,800 --> 00:32:33,800
pushing me to this Olympic school

511
00:32:33,800 --> 00:32:36,800
in order to get specialized in mathematics

512
00:32:36,800 --> 00:32:37,800
and also English school.

513
00:32:37,800 --> 00:32:39,800
It's like extra work

514
00:32:39,800 --> 00:32:41,800
besides this regular schoolwork.

515
00:32:41,800 --> 00:32:43,800
So basically you have to take the lessons

516
00:32:43,800 --> 00:32:45,800
on Saturday, on weekend.

517
00:32:45,800 --> 00:32:48,800
It makes me a quick learner.

518
00:32:48,800 --> 00:32:50,800
And my mom is correct, right?

519
00:32:50,800 --> 00:32:53,800
So in order to keep progress,

520
00:32:53,800 --> 00:32:55,800
in order to keep pushing yourself,

521
00:32:55,800 --> 00:32:56,800
you have to keep learning.

522
00:32:56,800 --> 00:32:59,800
And I always tell my employees

523
00:32:59,800 --> 00:33:00,800
also to keep learning,

524
00:33:00,800 --> 00:33:03,800
to keep up this fast pace in AI.

525
00:33:05,800 --> 00:33:07,800
My father was a professor

526
00:33:07,800 --> 00:33:09,800
in computer science,

527
00:33:09,800 --> 00:33:11,800
so I am very lucky to get

528
00:33:11,800 --> 00:33:14,800
in touch with AI in the very early days.

529
00:33:14,800 --> 00:33:16,800
Back in 2009, I was trying to

530
00:33:16,800 --> 00:33:18,800
build some AI models,

531
00:33:18,800 --> 00:33:19,800
very simple AI models.

532
00:33:19,800 --> 00:33:21,800
Nowadays, if you look

533
00:33:21,800 --> 00:33:23,800
from today's large language model perspective,

534
00:33:23,800 --> 00:33:26,800
that model is like a very simple,

535
00:33:26,800 --> 00:33:28,800
simple like a small ant.

536
00:33:32,800 --> 00:33:35,800
Han Xiaoh has worked in both the East and the West.

537
00:33:35,800 --> 00:33:39,800
He's held positions at the Chinese tech giant Tencent

538
00:33:39,800 --> 00:33:42,800
and German online retailer Zalando.

539
00:33:42,800 --> 00:33:45,800
Three years ago, he founded his own company.

540
00:33:45,800 --> 00:33:47,800
Gina is an AI startup

541
00:33:47,800 --> 00:33:50,800
with offices in Shenzhen and Beijing.

542
00:33:50,800 --> 00:33:52,800
But its headquarters are in Berlin.

543
00:33:52,800 --> 00:33:54,800
Oh, where do we have another interview here?

544
00:33:54,800 --> 00:33:55,800
Why?

545
00:33:55,800 --> 00:33:57,800
Yes, for the website.

546
00:33:57,800 --> 00:33:58,800
For the website?

547
00:33:58,800 --> 00:34:00,800
We have the internship program with interns

548
00:34:00,800 --> 00:34:02,800
and now we want to add also like employees experience

549
00:34:02,800 --> 00:34:05,800
so people can see how it is to work at Gina.

550
00:34:05,800 --> 00:34:07,800
Not only from an intern perspective.

551
00:34:07,800 --> 00:34:08,800
Yes, I see, I see.

552
00:34:11,800 --> 00:34:14,800
So, Kalimia of course from India,

553
00:34:14,800 --> 00:34:19,800
Isabella from South Africa.

554
00:34:19,800 --> 00:34:21,800
Aladdin from Tunisia.

555
00:34:22,800 --> 00:34:25,800
Jack Mian from Malaysia.

556
00:34:25,800 --> 00:34:28,800
Michelle finally from Germany.

557
00:34:31,800 --> 00:34:33,800
I have a limited amount of wires.

558
00:34:33,800 --> 00:34:35,800
So I can only power one monitor.

559
00:34:35,800 --> 00:34:38,800
Sometimes I really need to show people the architecture

560
00:34:38,800 --> 00:34:40,800
and also show people the results of the model.

561
00:34:40,800 --> 00:34:43,800
So it's nice that I have this second screen

562
00:34:43,800 --> 00:34:45,800
where I can draw on it as well.

563
00:34:45,800 --> 00:34:47,800
It basically becomes touchscreen.

564
00:34:50,800 --> 00:34:53,800
So this is basically showing the progress

565
00:34:53,800 --> 00:34:55,800
of training the model.

566
00:34:55,800 --> 00:34:57,800
It's kind of like stock market, right?

567
00:34:57,800 --> 00:34:59,800
I see this model performs relatively good

568
00:34:59,800 --> 00:35:02,800
because you can see it's increasing over time.

569
00:35:02,800 --> 00:35:05,800
But sometimes it's not very successful.

570
00:35:05,800 --> 00:35:09,800
For example, this one, this model start very high

571
00:35:09,800 --> 00:35:11,800
but then the progress kind of stopped.

572
00:35:11,800 --> 00:35:14,800
That is wasting our time, it's wasting GPU resources,

573
00:35:14,800 --> 00:35:16,800
energy and so on, right?

574
00:35:16,800 --> 00:35:19,800
Han Shao and his team are working on optimizing

575
00:35:19,800 --> 00:35:22,800
AI models for specific applications.

576
00:35:22,800 --> 00:35:26,800
For example, linking text, video and images.

577
00:35:26,800 --> 00:35:29,800
Their goal is to make communication between humans

578
00:35:29,800 --> 00:35:32,800
and machines more intuitive and natural.

579
00:35:32,800 --> 00:35:34,800
A lot of people may recognize this.

580
00:35:34,800 --> 00:35:38,800
This guy, this is a kind of grand-par meme, right?

581
00:35:38,800 --> 00:35:41,800
So it's very popular on social media, right?

582
00:35:41,800 --> 00:35:44,800
So if you upload this picture to the algorithm,

583
00:35:44,800 --> 00:35:46,800
it will generate a story.

584
00:35:46,800 --> 00:35:51,800
You can generate comedy, erotic, fantasy, horror,

585
00:35:51,800 --> 00:35:53,800
all this kind of story.

586
00:35:53,800 --> 00:35:57,800
So we just keep it default and then we just do.

587
00:35:57,800 --> 00:35:59,800
It wasn't supposed to be like this.

588
00:35:59,800 --> 00:36:01,800
I was meant for more.

589
00:36:01,800 --> 00:36:05,800
He whispered to the room, his words echoing into silence.

590
00:36:05,800 --> 00:36:08,800
I am more than the lonely man I've become,

591
00:36:08,800 --> 00:36:11,800
more than these disappointments.

592
00:36:12,800 --> 00:36:16,800
Suddenly, his eyes glinted, a revelation forming within his mind.

593
00:36:16,800 --> 00:36:20,800
Perhaps, it is time I showed the world that again.

594
00:36:20,800 --> 00:36:24,800
With strengthened resolve, Arthur placed the coffee down,

595
00:36:24,800 --> 00:36:26,800
marking an end to his solitary reflection

596
00:36:26,800 --> 00:36:28,800
and the beginning of a new chapter.

597
00:36:28,800 --> 00:36:31,800
So basically, this is what you can do

598
00:36:31,800 --> 00:36:34,800
when you push multimodal AI into an extreme, right?

599
00:36:34,800 --> 00:36:36,800
So you can see from a single image,

600
00:36:36,800 --> 00:36:40,800
you are able to generate not only a text description,

601
00:36:40,800 --> 00:36:43,800
but an emotional audio story.

602
00:36:57,800 --> 00:37:01,800
Eventually, Chinese companies will be in the leading position

603
00:37:01,800 --> 00:37:03,800
in this generative AI.

604
00:37:03,800 --> 00:37:09,800
Two months ago, I was participating in this World AI Congress in Shanghai.

605
00:37:09,800 --> 00:37:13,800
And during that conference, there were 30 large language models

606
00:37:13,800 --> 00:37:16,800
released on one day.

607
00:37:16,800 --> 00:37:20,800
Some from big companies like Tencent, Alibaba, Baidu,

608
00:37:20,800 --> 00:37:25,800
some also from middle-sized companies from different industries.

609
00:37:25,800 --> 00:37:27,800
For example, from Bank.

610
00:37:27,800 --> 00:37:41,800
This Chinese company is usually very good at learning from U.S. companies.

611
00:37:41,800 --> 00:37:44,800
So they kind of copycat what U.S. companies are doing

612
00:37:44,800 --> 00:37:46,800
and then make it even better.

613
00:37:49,800 --> 00:37:54,800
I don't doubt that one day, you will see one of the top models

614
00:37:54,800 --> 00:37:58,800
in the benchmark in the leaderboard are actually from China.

615
00:38:02,800 --> 00:38:07,800
The question of which companies will dominate the age of artificial intelligence

616
00:38:07,800 --> 00:38:10,800
has real geopolitical consequences.

617
00:38:10,800 --> 00:38:15,800
China is using the expertise of its tech companies to expand its power.

618
00:38:15,800 --> 00:38:19,800
Western nations, meanwhile, are trying to counter this.

619
00:38:24,800 --> 00:38:26,800
I'm going to go back.

620
00:38:35,800 --> 00:38:37,800
My name is Jeffrey Kane.

621
00:38:37,800 --> 00:38:40,800
I was a long-time journalist and foreign correspondent in China.

622
00:38:40,800 --> 00:38:42,800
I wrote a book called The Perfect Police State,

623
00:38:42,800 --> 00:38:44,800
and I was an advisor to the U.S. Congress,

624
00:38:44,800 --> 00:38:49,800
to the House of Representatives on sanctions and Chinese politics.

625
00:38:49,800 --> 00:38:53,800
From what I have seen around the world in China and elsewhere,

626
00:38:53,800 --> 00:38:58,800
I am deeply concerned that we do not know how to manage AI yet.

627
00:38:58,800 --> 00:39:00,800
We do not know what's coming.

628
00:39:00,800 --> 00:39:03,800
We do not know how to rein in this technology

629
00:39:03,800 --> 00:39:07,800
and put it to the good use of our democracy.

630
00:39:10,800 --> 00:39:15,800
China has been leading in bringing technology under state control

631
00:39:15,800 --> 00:39:18,800
and, in fact, using it as an instrument for state power,

632
00:39:18,800 --> 00:39:24,800
whether it is for internal control and censorship, a grip on society,

633
00:39:24,800 --> 00:39:29,800
or whether it is their global ambition to have digital infrastructure around the world

634
00:39:29,800 --> 00:39:34,800
and to work with countries, for example, I think, about the African continent.

635
00:39:34,800 --> 00:39:39,800
It is, of course, a vision that is at direct odds with that of democratic societies.

636
00:39:39,800 --> 00:39:44,800
In 2017, China's national strategy for artificial intelligence,

637
00:39:44,800 --> 00:39:46,800
and this is a public document,

638
00:39:46,800 --> 00:39:51,800
set out the explicit goal of dominating global AI technology.

639
00:39:51,800 --> 00:39:55,800
And so I think the United States has explicitly set the goal

640
00:39:55,800 --> 00:40:03,800
that we are not going to assist China in rising as an AI-enabled authoritarian superpower.

641
00:40:06,800 --> 00:40:09,800
Ironically, in the past, it's been large U.S. companies

642
00:40:09,800 --> 00:40:12,800
that have undermined their government's policies

643
00:40:12,800 --> 00:40:16,800
in order to gain access to the massive Chinese market,

644
00:40:16,800 --> 00:40:19,800
foremost among them, Microsoft.

645
00:40:19,800 --> 00:40:25,800
Microsoft is the most pivotal and important western company operating in China

646
00:40:25,800 --> 00:40:30,800
that has helped the Chinese government develop its AI dystopia.

647
00:40:30,800 --> 00:40:34,800
Microsoft set up an office in China called Microsoft Research Asia.

648
00:40:34,800 --> 00:40:38,800
This was a gesture from Bill Gates back in the 1990s

649
00:40:38,800 --> 00:40:42,800
because he wanted to guarantee stronger market access to China.

650
00:40:42,800 --> 00:40:47,800
This laboratory has gone on to train the who's who list,

651
00:40:47,800 --> 00:40:52,800
the superstars of the Chinese artificial intelligence world.

652
00:40:52,800 --> 00:40:57,800
Many of the key people in this laboratory have gone on to found companies

653
00:40:57,800 --> 00:41:01,800
such as MakeV, SenseTime, or either found them

654
00:41:01,800 --> 00:41:04,800
or they've taken on very senior roles in them.

655
00:41:05,800 --> 00:41:13,800
That was like the incubator of the modern Chinese internet or AI industry.

656
00:41:13,800 --> 00:41:17,800
A lot of great people, great researchers, startup founders

657
00:41:17,800 --> 00:41:19,800
actually come from Microsoft Research.

658
00:41:19,800 --> 00:41:25,800
And those talents are now becoming kind of the very, very big influencers,

659
00:41:25,800 --> 00:41:29,800
opinion leaders, and really like entrepreneurs in China.

660
00:41:30,800 --> 00:41:33,800
Microsoft helped build China's tech elite.

661
00:41:33,800 --> 00:41:36,800
This in turn has been used by the Chinese government

662
00:41:36,800 --> 00:41:42,800
to create a gigantic surveillance state that operates with the help of AI.

663
00:41:44,800 --> 00:41:46,800
Like in China, in Beijing and Shenzhen,

664
00:41:46,800 --> 00:41:50,800
you can find the most CCTV camera in the world.

665
00:41:50,800 --> 00:41:56,800
And to be honest, like a general public get used to it.

666
00:41:57,800 --> 00:42:02,800
So they don't see this as intrusion to their own privacy

667
00:42:02,800 --> 00:42:06,800
or having a software that analyze their behavior,

668
00:42:06,800 --> 00:42:11,800
because the kind of the narrative there was to protect them,

669
00:42:11,800 --> 00:42:14,800
to make the society more secure,

670
00:42:14,800 --> 00:42:17,800
provide, protect from terrorists and so on.

671
00:42:17,800 --> 00:42:21,800
So in general, like the public over the last 10 years

672
00:42:21,800 --> 00:42:25,800
has already accept the fact that there are surveillance everywhere.

673
00:42:32,800 --> 00:42:36,800
And now, not only you have an option of listening to all the information

674
00:42:36,800 --> 00:42:38,800
that people exchange in society,

675
00:42:38,800 --> 00:42:42,800
now you also have the cognitive capacity to process all of this.

676
00:42:42,800 --> 00:42:44,800
So that's a scary, scary future.

677
00:42:44,800 --> 00:42:47,800
Unfortunately, it's definitely not an impossible one.

678
00:42:51,800 --> 00:42:56,800
Right now we have like over 500 city brands across the country.

679
00:42:56,800 --> 00:42:58,800
That means one city is just like Shanghai.

680
00:42:58,800 --> 00:43:01,800
They have a lot of big data analysis center.

681
00:43:01,800 --> 00:43:05,800
They're collecting all this data from different areas.

682
00:43:05,800 --> 00:43:09,800
And they have the machine, they have the algorithm,

683
00:43:09,800 --> 00:43:14,800
like centralized it and do the computation analysis

684
00:43:14,800 --> 00:43:16,800
and making all these decisions.

685
00:43:22,800 --> 00:43:26,800
The Chinese government has used all forms of AI so far.

686
00:43:26,800 --> 00:43:28,800
They see AI as an extremely powerful tool

687
00:43:28,800 --> 00:43:32,800
that they can use for the military, for national security,

688
00:43:32,800 --> 00:43:34,800
for state surveillance, police work,

689
00:43:34,800 --> 00:43:37,800
also the management of cities, traffic.

690
00:43:37,800 --> 00:43:40,800
They have been selling these same technologies all over the world,

691
00:43:40,800 --> 00:43:42,800
especially to authoritarian governments

692
00:43:42,800 --> 00:43:45,800
with the promise of total surveillance

693
00:43:45,800 --> 00:43:49,800
and a nation free of crime, free of dissident,

694
00:43:49,800 --> 00:43:51,800
it's a brave new world

695
00:43:51,800 --> 00:43:54,800
because we have not yet found a solution to this in the West.

696
00:44:01,800 --> 00:44:03,800
China is forging ahead.

697
00:44:03,800 --> 00:44:06,800
The US is pursuing its own interests.

698
00:44:06,800 --> 00:44:08,800
And the EU?

699
00:44:08,800 --> 00:44:11,800
It's striving for independence.

700
00:44:12,800 --> 00:44:16,800
If Europeans don't play a part in shaping this future technology,

701
00:44:16,800 --> 00:44:18,800
then it will be American or Chinese AI

702
00:44:18,800 --> 00:44:22,800
that will penetrate our lives to an unprecedented extent.

703
00:44:24,800 --> 00:44:28,800
It will know us as well as our closest friends and relatives.

704
00:44:28,800 --> 00:44:31,800
It will communicate with us around the clock

705
00:44:31,800 --> 00:44:34,800
and influence our thoughts and actions.

706
00:44:39,800 --> 00:44:41,800
To prevent this, the EU needs companies

707
00:44:41,800 --> 00:44:43,800
that can not only program

708
00:44:43,800 --> 00:44:46,800
but also build their own hardware infrastructure

709
00:44:46,800 --> 00:44:49,800
to keep highly sensitive data safe.

710
00:44:59,800 --> 00:45:02,800
The most precious commodity, the most important resource

711
00:45:02,800 --> 00:45:05,800
for the future of generative AI is GPU power.

712
00:45:05,800 --> 00:45:07,800
In other words, computing power.

713
00:45:07,800 --> 00:45:10,800
In the future, it will be as essential as electricity

714
00:45:10,800 --> 00:45:14,800
and water have been for developments that have taken place in the past.

715
00:45:18,800 --> 00:45:22,800
There's already a saying in Silicon Valley, the GPU poor,

716
00:45:22,800 --> 00:45:25,800
the people with fewer graphics cards.

717
00:45:31,800 --> 00:45:34,800
Training high-end AI language models

718
00:45:34,800 --> 00:45:37,800
requires thousands of high-performance graphics cards,

719
00:45:37,800 --> 00:45:40,800
which is also why supplies are scarce.

720
00:45:40,800 --> 00:45:42,800
That's another reason why many smaller players

721
00:45:42,800 --> 00:45:45,800
ally themselves with large tech companies.

722
00:45:50,800 --> 00:45:54,800
A lot of the deals in the field of generative AI in recent months

723
00:45:54,800 --> 00:45:56,800
have come at the cost of independence.

724
00:45:56,800 --> 00:45:59,800
Many companies have partnered with large corporations

725
00:45:59,800 --> 00:46:02,800
by accepting restrictions on things like hardware selection,

726
00:46:02,800 --> 00:46:05,800
cloud selection, integration.

727
00:46:05,800 --> 00:46:08,800
We absolutely didn't want to do that.

728
00:46:11,800 --> 00:46:14,800
Early on, Jonas Androulis recognized the value

729
00:46:14,800 --> 00:46:17,800
of having one's own hardware.

730
00:46:17,800 --> 00:46:20,800
He built a data center for his company in Germany.

731
00:46:20,800 --> 00:46:25,800
For that reason, Alif Alpha is becoming increasingly strategically important

732
00:46:25,800 --> 00:46:27,800
for politicians.

733
00:46:28,800 --> 00:46:32,800
The media talks about you as Germany's answer to chatGPT.

734
00:46:32,800 --> 00:46:34,800
Is that right?

735
00:46:35,800 --> 00:46:37,800
That's wrong.

736
00:46:37,800 --> 00:46:40,800
You could say Germany's answer to open AI,

737
00:46:40,800 --> 00:46:43,800
but chatGPT is a product aimed at the consumer.

738
00:46:43,800 --> 00:46:46,800
It's really intended to help school kids do their homework

739
00:46:46,800 --> 00:46:49,800
and to help them understand what's going on in the world.

740
00:46:49,800 --> 00:46:51,800
That's right.

741
00:46:51,800 --> 00:46:54,800
That's not our target group at all.

742
00:46:54,800 --> 00:46:58,800
We want to go where the most complex and critical processes are.

743
00:46:58,800 --> 00:47:01,800
For example, in the financial industry,

744
00:47:01,800 --> 00:47:04,800
in administration, in security, in healthcare,

745
00:47:04,800 --> 00:47:08,800
that's where we want to build systems that assist and support people.

746
00:47:08,800 --> 00:47:11,800
We're in a government ministry here.

747
00:47:11,800 --> 00:47:14,800
We're in a government ministry here.

748
00:47:14,800 --> 00:47:17,800
We're in a government ministry here.

749
00:47:17,800 --> 00:47:21,800
And public administration could benefit enormously from AI.

750
00:47:21,800 --> 00:47:23,800
We have an incredible number of processes

751
00:47:23,800 --> 00:47:26,800
that could be systematized and carried out.

752
00:47:26,800 --> 00:47:29,800
So the focus of my work here was a bit like asking

753
00:47:29,800 --> 00:47:33,800
how the public sector could act as a mainstay consumer,

754
00:47:33,800 --> 00:47:36,800
generating work, if you can put it like that,

755
00:47:36,800 --> 00:47:40,800
which would create demand for German and European AI technology.

756
00:47:40,800 --> 00:47:43,800
I mean, it's an AI company that targets the public sector,

757
00:47:43,800 --> 00:47:45,800
and we are the public sector.

758
00:47:45,800 --> 00:47:48,800
So we only have to see that we generate opportunities

759
00:47:48,800 --> 00:47:50,800
for these technologies to be tested,

760
00:47:50,800 --> 00:47:53,800
be it through customer experience, funding decisions, or even permits.

761
00:47:53,800 --> 00:47:57,800
We talk a lot about regulation and that kind of thing,

762
00:47:57,800 --> 00:48:00,800
but if we continue to be dependent on foreign countries

763
00:48:00,800 --> 00:48:03,800
and commercial enterprises for the future,

764
00:48:03,800 --> 00:48:06,800
we're going to be able to do that.

765
00:48:06,800 --> 00:48:09,800
And if we continue to be dependent on foreign countries

766
00:48:09,800 --> 00:48:12,800
and commercial enterprises for this essential technology,

767
00:48:12,800 --> 00:48:15,800
then in the future, things could potentially end up

768
00:48:15,800 --> 00:48:18,800
like they did with energy, like with gas recently,

769
00:48:18,800 --> 00:48:21,800
where we wanted to say certain things, but we couldn't,

770
00:48:21,800 --> 00:48:25,800
because otherwise it would have gotten cold and dark around here.

771
00:48:30,800 --> 00:48:34,800
The data center that so far ensured Aleph Alpha's independence

772
00:48:34,800 --> 00:48:37,800
is the US company Hewlett Packard Enterprise.

773
00:48:43,800 --> 00:48:46,800
Hewlett Packard Enterprise is one of the biggest players

774
00:48:46,800 --> 00:48:49,800
when it comes to setting up computer infrastructure.

775
00:48:49,800 --> 00:48:51,800
They build data centers.

776
00:48:51,800 --> 00:48:54,800
They set up internal server rooms.

777
00:48:54,800 --> 00:48:57,800
A lot of the high quality infrastructure

778
00:48:57,800 --> 00:49:00,800
in which the modern world runs comes from HPE.

779
00:49:05,800 --> 00:49:07,800
MUSIC

780
00:49:16,800 --> 00:49:20,800
Andrew Lewis secured a strategic partnership with HPE,

781
00:49:20,800 --> 00:49:22,800
giving him access to hardware

782
00:49:22,800 --> 00:49:25,800
without tying him to the company exclusively.

783
00:49:25,800 --> 00:49:29,800
He also hoped it would help him gain a foothold in the US market.

784
00:49:29,800 --> 00:49:32,800
He finalized the deal in Las Vegas

785
00:49:32,800 --> 00:49:35,800
with HPE CEO Antonio Neri.

786
00:49:53,800 --> 00:49:56,800
We're announcing a major joint project with HPE today.

787
00:49:56,800 --> 00:49:59,800
There will be a press release going out simultaneously.

788
00:49:59,800 --> 00:50:02,800
It'll be a big joint market venture.

789
00:50:02,800 --> 00:50:04,800
That's the important thing.

790
00:50:04,800 --> 00:50:06,800
Not that I'm going to go on a stage,

791
00:50:06,800 --> 00:50:09,800
but that we're now taking a joint step with a major partner.

792
00:50:09,800 --> 00:50:12,800
Am I nervous? Maybe a little.

793
00:50:12,800 --> 00:50:15,800
We've been working towards this for months.

794
00:50:15,800 --> 00:50:17,800
I'm sure it'll go well.

795
00:50:21,800 --> 00:50:25,800
What is the competitive advantage of Marvel, eventually?

796
00:50:25,800 --> 00:50:28,800
We're building our bond.

797
00:50:28,800 --> 00:50:30,800
We have an independent tax tax,

798
00:50:30,800 --> 00:50:33,800
so we're not relying on any external dependencies.

799
00:50:33,800 --> 00:50:36,800
We recently solved explainability in a new way

800
00:50:36,800 --> 00:50:39,800
so you can not only see positive, confirming sources,

801
00:50:39,800 --> 00:50:41,800
but also disagreeing sources.

802
00:50:41,800 --> 00:50:43,800
You share with me an example of that.

803
00:50:43,800 --> 00:50:46,800
Yes, exactly. From your own kind of speech.

804
00:50:46,800 --> 00:50:49,800
They are analysts here, so be careful.

805
00:50:51,800 --> 00:50:53,800
Americans move fast.

806
00:50:53,800 --> 00:50:55,800
They're willing to take risks,

807
00:50:55,800 --> 00:50:59,800
but, of course, this partnership also has to benefit HPE,

808
00:50:59,800 --> 00:51:03,800
and any partnership can come to an end at any time.

809
00:51:10,800 --> 00:51:13,800
Jonas Andrewles wanted to avoid becoming dependent

810
00:51:13,800 --> 00:51:15,800
on a large corporation,

811
00:51:15,800 --> 00:51:18,800
as with open AI and Microsoft.

812
00:51:18,800 --> 00:51:21,800
But that strategy brought with it a major risk.

813
00:51:21,800 --> 00:51:24,800
If his technology doesn't keep up with the competition,

814
00:51:24,800 --> 00:51:27,800
he'll be out of the race.

815
00:51:27,800 --> 00:51:29,800
We'll have more money soon.

816
00:51:29,800 --> 00:51:34,800
I'm hoping that yesterday helped a little bit with that.

817
00:51:34,800 --> 00:51:38,800
Yeah, certainly didn't hurt,

818
00:51:38,800 --> 00:51:41,800
and I've got some kind of immediate feedback

819
00:51:41,800 --> 00:51:46,800
after the show from investors on my cell phone.

820
00:51:46,800 --> 00:51:50,800
And, of course, I want to put this money to work.

821
00:51:51,800 --> 00:51:54,800
I think if we can get your help,

822
00:51:54,800 --> 00:51:58,800
if we could get your help to really say,

823
00:51:58,800 --> 00:52:00,800
okay, these are the application use cases,

824
00:52:00,800 --> 00:52:04,800
I think we can get that list from you,

825
00:52:04,800 --> 00:52:06,800
and then we can turn around and look at,

826
00:52:06,800 --> 00:52:08,800
okay, one, how do we package it?

827
00:52:08,800 --> 00:52:11,800
Two, can we use it internally?

828
00:52:11,800 --> 00:52:13,800
I think that would be great.

829
00:52:13,800 --> 00:52:14,800
And then, obviously, three,

830
00:52:14,800 --> 00:52:17,800
how do we make sure we line up the services offer?

831
00:52:17,800 --> 00:52:19,800
Thanks again for the partnership. Great to see you.

832
00:52:19,800 --> 00:52:21,800
Thanks a lot.

833
00:52:21,800 --> 00:52:24,800
I think what always attracted us to the relationship was,

834
00:52:24,800 --> 00:52:28,800
LFLFA's mission was to enable enterprise application use cases

835
00:52:28,800 --> 00:52:31,800
for LLMs and multimodal models,

836
00:52:31,800 --> 00:52:34,800
and most of the customers in the Valley

837
00:52:34,800 --> 00:52:36,800
or most of the companies in Silicon Valley

838
00:52:36,800 --> 00:52:38,800
were much more consumer-oriented.

839
00:52:38,800 --> 00:52:41,800
So this concept of a single-tenant LLM

840
00:52:41,800 --> 00:52:45,800
that can be trained with your data for your application

841
00:52:45,800 --> 00:52:48,800
really fits our core customer base.

842
00:52:50,800 --> 00:52:52,800
For all their friendliness,

843
00:52:52,800 --> 00:52:56,800
and Drulles knew the Americans wanted to see concrete results,

844
00:52:56,800 --> 00:52:59,800
he had to deliver and fast.

845
00:53:08,800 --> 00:53:11,800
I will show you anger.

846
00:53:14,800 --> 00:53:16,800
I have a smile for you.

847
00:53:17,800 --> 00:53:20,800
Are you happy with the trade fair so far?

848
00:53:27,800 --> 00:53:29,800
What's been most interesting?

849
00:53:41,800 --> 00:53:44,800
The AI race is also a competition for attention.

850
00:53:45,800 --> 00:53:47,800
It's about catching the eye of investors

851
00:53:47,800 --> 00:53:51,800
sitting on panels, being noticed, being quoted.

852
00:54:05,800 --> 00:54:09,800
We've already heard from a few masterminds on this topic today,

853
00:54:09,800 --> 00:54:11,800
and there are still a few more to come.

854
00:54:12,800 --> 00:54:14,800
With you, we'll be asking to what extent

855
00:54:14,800 --> 00:54:17,800
AI itself will drive innovation.

856
00:54:19,800 --> 00:54:23,800
This one's niche technology is now the subject of massive hype.

857
00:54:23,800 --> 00:54:26,800
Jonas S. Drulles is suddenly in the spotlight.

858
00:54:26,800 --> 00:54:30,800
His AI is being tested and evaluated,

859
00:54:30,800 --> 00:54:32,800
not always favourably.

860
00:54:32,800 --> 00:54:36,800
News Magazine did cite accused LFLFA of allowing its AI

861
00:54:36,800 --> 00:54:40,800
to be provoked into making racist and chauvinistic statements.

862
00:54:41,800 --> 00:54:44,800
And Drulles pointed out that its basic technology

863
00:54:44,800 --> 00:54:47,800
has deliberately not been restricted.

864
00:54:50,800 --> 00:54:53,800
That's just low-hanging fruit for journalists.

865
00:54:53,800 --> 00:54:55,800
I took a screenshot.

866
00:54:55,800 --> 00:54:57,800
The model used a bad word.

867
00:54:57,800 --> 00:55:00,800
Every time I fine-tune the model or tune the instructions,

868
00:55:00,800 --> 00:55:03,800
that diminishes it in certain areas.

869
00:55:04,800 --> 00:55:09,800
It loses capabilities in exchange for me making it more pleasant or safer,

870
00:55:09,800 --> 00:55:12,800
and those might be the exact capabilities that I need

871
00:55:12,800 --> 00:55:16,800
in an industrial context for automating processes.

872
00:55:16,800 --> 00:55:21,800
We want the embedding technology to become a well-known brand,

873
00:55:21,800 --> 00:55:23,800
just like the iPhone.

874
00:55:23,800 --> 00:55:25,800
That's the most important thing.

875
00:55:25,800 --> 00:55:28,800
Don't forget to subscribe to our channel

876
00:55:28,800 --> 00:55:30,800
for more videos like this.

877
00:55:46,800 --> 00:55:49,800
We want to think about whether it makes sense or not.

878
00:55:52,800 --> 00:55:55,800
We want to become a company like OpenAI,

879
00:55:55,800 --> 00:55:58,800
the top provider in the embedding world.

880
00:56:17,800 --> 00:56:23,800
What is the best way to push the team to focus on this product?

881
00:56:23,800 --> 00:56:28,800
We have teams with different cultural backgrounds,

882
00:56:28,800 --> 00:56:33,800
and sometimes it's very hard to organize everybody to concentrate on one thing.

883
00:56:33,800 --> 00:56:36,800
This is also because the AI is developing so fast,

884
00:56:36,800 --> 00:56:39,800
and a lot of hypes are here and there,

885
00:56:39,800 --> 00:56:42,800
and people want to try this out, try that out.

886
00:56:42,800 --> 00:56:46,800
So I just talk to my CEO and make sure that all the senior engineers,

887
00:56:46,800 --> 00:56:49,800
senior leaders are kind of on the same page.

888
00:56:52,800 --> 00:56:54,800
For me as a CEO,

889
00:56:54,800 --> 00:56:58,800
my primary job is actually killing the fun,

890
00:56:58,800 --> 00:57:02,800
killing the fun part by killing all this distraction

891
00:57:02,800 --> 00:57:05,800
to make sure that people concentrate on the single mission

892
00:57:05,800 --> 00:57:07,800
to make this company successful.

893
00:57:07,800 --> 00:57:10,800
We are kind of a developer-driven company,

894
00:57:10,800 --> 00:57:13,800
and most of our customers or users are actually developers,

895
00:57:13,800 --> 00:57:14,800
software engineers.

896
00:57:14,800 --> 00:57:18,800
So for example, right now there is a talentus sitting there,

897
00:57:18,800 --> 00:57:21,800
and their engineering team is also our customer.

898
00:57:21,800 --> 00:57:28,800
The biggest challenge is the competition in AI is just too intense.

899
00:57:28,800 --> 00:57:30,800
Investors are not stupid, like most of the investors,

900
00:57:30,800 --> 00:57:32,800
especially when it comes to later round,

901
00:57:32,800 --> 00:57:35,800
investors have a very strict evaluation about this company.

902
00:57:35,800 --> 00:57:38,800
So the companies that we are competing with,

903
00:57:38,800 --> 00:57:41,800
such as Hackingface from France,

904
00:57:41,800 --> 00:57:44,800
and Coher from the US,

905
00:57:44,800 --> 00:57:48,800
so those companies are not like those guys who previously worked at Google,

906
00:57:48,800 --> 00:57:51,800
or graduated from MIT, Stanford,

907
00:57:51,800 --> 00:57:53,800
so they are very smart people.

908
00:57:53,800 --> 00:57:56,800
Most of the investors will look at us as not as a small company,

909
00:57:56,800 --> 00:57:59,800
but they will evaluate us with more,

910
00:57:59,800 --> 00:58:02,800
not based on the hype, but based on the performance of the company.

911
00:58:02,800 --> 00:58:06,800
So that means we have to show two things,

912
00:58:06,800 --> 00:58:09,800
either the hyper growth of the user,

913
00:58:09,800 --> 00:58:12,800
so we need to grow the user base super fast,

914
00:58:12,800 --> 00:58:15,800
or we show them a solid revenue.

915
00:58:25,800 --> 00:58:27,800
Summer 2023.

916
00:58:27,800 --> 00:58:33,800
Thomas Wolff has managed to make time for a family vacation in Brittany, France.

917
00:58:33,800 --> 00:58:36,800
As Chief Scientific Officer, he's primarily responsible

918
00:58:36,800 --> 00:58:39,800
for research and development at Hackingface,

919
00:58:39,800 --> 00:58:43,800
a job that allows him to take a break from time to time.

920
00:58:46,800 --> 00:58:50,800
What are you up to today? Anything special?

921
00:58:50,800 --> 00:58:53,800
We're practicing ceiling with a trapeze.

922
00:58:53,800 --> 00:58:56,800
Didn't you do that yesterday?

923
00:58:56,800 --> 00:58:59,800
Yes, today we'll do something else.

924
00:59:04,800 --> 00:59:06,800
I'm here to help.

925
00:59:06,800 --> 00:59:11,800
Meanwhile, Thomas' business partner, Clemente Long, is in the spotlight.

926
00:59:11,800 --> 00:59:15,800
He's the CEO of Hackingface, the public face of the company.

927
00:59:17,800 --> 00:59:22,800
Tech industry heavyweights like Google, Amazon, NVIDIA and AMD

928
00:59:22,800 --> 00:59:27,800
have invested $235 million into Hackingface.

929
00:59:27,800 --> 00:59:32,800
The open development platform for AI models has become a billion-dollar business.

930
00:59:34,800 --> 00:59:36,800
It's not.

931
00:59:38,800 --> 00:59:40,800
The company gained even more prestige

932
00:59:40,800 --> 00:59:43,800
when Mark Zuckerberg's Meta used Hackingface

933
00:59:43,800 --> 00:59:47,800
to publish its high-end language model, Lama 2.

934
00:59:51,800 --> 00:59:55,800
It's a model that was recently published by Facebook, or Meta.

935
00:59:55,800 --> 00:59:59,800
It's similar to ChatGBT, an open-source competitor.

936
00:59:59,800 --> 01:00:01,800
The difference is that it's free.

937
01:00:01,800 --> 01:00:04,800
You can just install it on your computer.

938
01:00:04,800 --> 01:00:08,800
You don't have to access it through the ChatGBT interface or pay for it.

939
01:00:08,800 --> 01:00:10,800
It's like a set of Lego.

940
01:00:10,800 --> 01:00:13,800
Everything is open. Everything is freely accessible.

941
01:00:13,800 --> 01:00:15,800
You can also buy it pre-built.

942
01:00:15,800 --> 01:00:18,800
If someone builds an open-source model for you,

943
01:00:18,800 --> 01:00:20,800
it's like they're building Lego for you,

944
01:00:20,800 --> 01:00:23,800
a beautiful sports car made from Lego.

945
01:00:23,800 --> 01:00:24,800
And then it's yours.

946
01:00:24,800 --> 01:00:27,800
You can open up the hood and look inside.

947
01:00:28,800 --> 01:00:32,800
The greatest advantage of open-source, its free accessibility,

948
01:00:32,800 --> 01:00:35,800
is also its greatest weakness.

949
01:00:35,800 --> 01:00:38,800
What if a model was developed further by criminals,

950
01:00:38,800 --> 01:00:43,800
terrorists or other bad actors and used to cause harm?

951
01:00:43,800 --> 01:00:46,800
Any security mechanisms built into a language model

952
01:00:46,800 --> 01:00:48,800
can easily be removed.

953
01:00:53,800 --> 01:00:56,800
That's a big question we're asking ourselves.

954
01:00:57,800 --> 01:01:00,800
In the beginning, our aim was to make this technology

955
01:01:00,800 --> 01:01:03,800
as widely accessible as possible.

956
01:01:03,800 --> 01:01:05,800
We thought it would help lots of developers,

957
01:01:05,800 --> 01:01:07,800
but there are two sides to the technology.

958
01:01:07,800 --> 01:01:09,800
There are some people who can access it

959
01:01:09,800 --> 01:01:11,800
who really shouldn't be able to.

960
01:01:21,800 --> 01:01:25,800
There was a guy I met last year who had an AI.

961
01:01:25,800 --> 01:01:29,800
Designed to develop medicines, molecules that are good for your health.

962
01:01:29,800 --> 01:01:33,800
Just as an experiment, he put in a minus sign

963
01:01:33,800 --> 01:01:37,800
and trained it to look for molecules that were bad for your health.

964
01:01:37,800 --> 01:01:42,800
Within four hours, it discovered thousands of chemical weapons,

965
01:01:42,800 --> 01:01:47,800
including VX, the most powerful nerve gas

966
01:01:47,800 --> 01:01:51,800
that we here in the US have developed.

967
01:01:51,800 --> 01:01:54,800
Of course, you shouldn't open-source things like that.

968
01:01:54,800 --> 01:01:56,800
It's just crazy.

969
01:01:56,800 --> 01:01:58,800
I'm a scientist. I love open-source, right?

970
01:01:58,800 --> 01:02:01,800
And it's undeniable if you think about the pace of progress,

971
01:02:01,800 --> 01:02:04,800
how do you make sure that there is the most progress?

972
01:02:04,800 --> 01:02:06,800
Open-source is your friend.

973
01:02:06,800 --> 01:02:11,800
Having said that, I just cannot completely ignore all the dangers.

974
01:02:11,800 --> 01:02:14,800
And of course, the only argument I have seen so far

975
01:02:14,800 --> 01:02:17,800
from supporters of open-sourcing, everything is saying,

976
01:02:17,800 --> 01:02:19,800
well, we will figure it out.

977
01:02:20,800 --> 01:02:22,800
It's easy to comment from the sidelines

978
01:02:22,800 --> 01:02:24,800
to simply warn that it's all dangerous.

979
01:02:24,800 --> 01:02:27,800
It's more difficult to actively get involved,

980
01:02:27,800 --> 01:02:30,800
to try to create something positive, something good.

981
01:02:30,800 --> 01:02:32,800
It won't necessarily be successful.

982
01:02:32,800 --> 01:02:35,800
There'll be mistakes and then fresh attempts,

983
01:02:35,800 --> 01:02:37,800
but I'm going to try.

984
01:02:37,800 --> 01:02:40,800
It's risky, but we'll follow the path we think is right.

985
01:02:52,800 --> 01:03:04,800
MIT is one of the most renowned tech universities in the world.

986
01:03:04,800 --> 01:03:07,800
It has close ties to industry.

987
01:03:07,800 --> 01:03:11,800
The research carried out here has the potential to change the world.

988
01:03:11,800 --> 01:03:16,800
Needless to say, MIT is at the forefront of artificial intelligence.

989
01:03:16,800 --> 01:03:20,800
In addition to his work with the Future of Life Institute,

990
01:03:20,800 --> 01:03:22,800
Max Tegmark is a professor here.

991
01:03:22,800 --> 01:03:26,800
The topic of AI security is part of his day-to-day.

992
01:03:28,800 --> 01:03:32,800
So we want to ramp up the effort and pace at which we do things.

993
01:03:32,800 --> 01:03:36,800
And it's also very inspiring whenever I go to Silicon Valley

994
01:03:36,800 --> 01:03:38,800
and meet with various companies there,

995
01:03:38,800 --> 01:03:39,800
how quickly they do things,

996
01:03:39,800 --> 01:03:41,800
often compared to what we do in universities.

997
01:03:41,800 --> 01:03:43,800
So I thought it'd be fun to...

998
01:03:44,800 --> 01:03:45,800
Now we have a whole...

999
01:03:45,800 --> 01:03:48,800
We're lucky to have a whole bunch of talented people here.

1000
01:03:48,800 --> 01:03:50,800
We can wrap up.

1001
01:03:52,800 --> 01:03:55,800
Tegmark and his fellow campaigners want to keep a close eye

1002
01:03:55,800 --> 01:03:58,800
on the tech startups from Silicon Valley,

1003
01:03:58,800 --> 01:04:01,800
uncover risks and use scientific methodology

1004
01:04:01,800 --> 01:04:04,800
to show people just how little time we have left

1005
01:04:04,800 --> 01:04:07,800
to counteract the pull of the tech industry.

1006
01:04:09,800 --> 01:04:11,800
So you've been working very hard on finishing our paper.

1007
01:04:11,800 --> 01:04:14,800
We had a very, very long conversation about it yesterday.

1008
01:04:14,800 --> 01:04:17,800
I thought the very last part of what we talked about

1009
01:04:17,800 --> 01:04:19,800
might be kind of fun for the whole group.

1010
01:04:19,800 --> 01:04:20,800
Yeah, I completely agree.

1011
01:04:20,800 --> 01:04:22,800
Do you want to draw that table on the board

1012
01:04:22,800 --> 01:04:26,800
and maybe they can contribute to good quotes for it?

1013
01:04:26,800 --> 01:04:30,800
So we basically, as you know,

1014
01:04:30,800 --> 01:04:33,800
modeled the conflict between the movement

1015
01:04:33,800 --> 01:04:36,800
to replace human livelihoods

1016
01:04:36,800 --> 01:04:38,800
and maybe replace humans period

1017
01:04:38,800 --> 01:04:41,800
versus the movement to resist this

1018
01:04:41,800 --> 01:04:44,800
and to preserve the status quo.

1019
01:04:44,800 --> 01:04:45,800
So this doesn't just come...

1020
01:04:45,800 --> 01:04:47,800
It's not just something Peter pulled out of a hat.

1021
01:04:47,800 --> 01:04:49,800
It just actually comes from the math.

1022
01:04:49,800 --> 01:04:51,800
So if you're naive, like,

1023
01:04:51,800 --> 01:04:54,800
oh, we have AI that can do everything a human can do but better,

1024
01:04:54,800 --> 01:04:56,800
my life will still be good.

1025
01:04:56,800 --> 01:04:58,800
So we call that naivete.

1026
01:04:58,800 --> 01:05:00,800
So if a lot of people believe this,

1027
01:05:00,800 --> 01:05:03,800
then they will not invest personal sacrifices

1028
01:05:03,800 --> 01:05:06,800
and personal costs to greater unite

1029
01:05:06,800 --> 01:05:09,800
and for the movement to be in a better position

1030
01:05:09,800 --> 01:05:10,800
to resist as a team.

1031
01:05:10,800 --> 01:05:13,800
There's companies and open-source developers

1032
01:05:13,800 --> 01:05:15,800
that are working day and night

1033
01:05:15,800 --> 01:05:18,800
with the goal of taking people's income streams

1034
01:05:18,800 --> 01:05:20,800
by creating AI models that are better

1035
01:05:20,800 --> 01:05:23,800
than them at their job and their capabilities.

1036
01:05:23,800 --> 01:05:26,800
So once you lose your income streams and your leverage,

1037
01:05:26,800 --> 01:05:27,800
like, it's too late.

1038
01:05:27,800 --> 01:05:30,800
Your options are more limited.

1039
01:05:30,800 --> 01:05:33,800
The biggest danger is that we'll look back in 20 years

1040
01:05:33,800 --> 01:05:35,800
and realize that we've automated everything

1041
01:05:35,800 --> 01:05:38,800
because it was so easy and because it worked

1042
01:05:38,800 --> 01:05:41,800
and the AI behaved correctly in 99% of cases

1043
01:05:41,800 --> 01:05:44,800
and suddenly we no longer have control

1044
01:05:44,800 --> 01:05:47,800
over something that's crucial for society.

1045
01:05:59,800 --> 01:06:02,800
The process has already begun.

1046
01:06:02,800 --> 01:06:06,800
Until now, it's been the intellectual and creative abilities

1047
01:06:06,800 --> 01:06:09,800
of humans that have set us apart from other creatures

1048
01:06:09,800 --> 01:06:11,800
and machines.

1049
01:06:11,800 --> 01:06:15,800
But what if those qualities are now being taken away?

1050
01:06:24,800 --> 01:06:27,800
My name is Dr. Inongo Lumumba-Casango,

1051
01:06:27,800 --> 01:06:28,800
a.k.a. Samus.

1052
01:06:28,800 --> 01:06:30,800
I'm a rapper, I'm a producer,

1053
01:06:30,800 --> 01:06:32,800
and I'm an assistant professor

1054
01:06:32,800 --> 01:06:34,800
at Brown University and the Music Department.

1055
01:06:34,800 --> 01:06:38,800
Initially, I wasn't sort of tapped into all of the discussions

1056
01:06:38,800 --> 01:06:40,800
that were happening around AI.

1057
01:06:40,800 --> 01:06:42,800
Of course, peripherally, I was sort of listening,

1058
01:06:42,800 --> 01:06:44,800
watching, reading,

1059
01:06:44,800 --> 01:06:47,800
but I really started to tap into these conversations

1060
01:06:47,800 --> 01:06:49,800
when I noticed what was happening

1061
01:06:49,800 --> 01:06:51,800
at the intersection of hip-hop and AI,

1062
01:06:51,800 --> 01:06:53,800
and that's when I realized, whoa, this thing is moving

1063
01:06:53,800 --> 01:06:54,800
really quickly.

1064
01:06:54,800 --> 01:06:56,800
I mean, last year, we were talking about

1065
01:06:56,800 --> 01:06:58,800
a sort of AI-generated rapper,

1066
01:06:58,800 --> 01:07:02,800
and this year, we're talking about rappers like Drake

1067
01:07:02,800 --> 01:07:05,800
and artists like The Weeknd having their voices

1068
01:07:05,800 --> 01:07:08,800
actually sort of cloned using AI technologies,

1069
01:07:08,800 --> 01:07:11,800
and so the speed at which this has become

1070
01:07:11,800 --> 01:07:14,800
sort of an immediate challenge for working artists

1071
01:07:14,800 --> 01:07:16,800
is very alarming.

1072
01:07:16,800 --> 01:07:19,800
Ultimately, it's the logic of capitalism,

1073
01:07:19,800 --> 01:07:23,800
and as a human creator, what you can do is

1074
01:07:23,800 --> 01:07:27,800
try not to be left behind.

1075
01:07:27,800 --> 01:07:32,800
As a Chinese, we always feel that, like,

1076
01:07:32,800 --> 01:07:35,800
technology, if you use it in a smarter way,

1077
01:07:35,800 --> 01:07:38,800
it can, like, push you,

1078
01:07:38,800 --> 01:07:41,800
uplift you yourself

1079
01:07:41,800 --> 01:07:46,800
to become a smarter, greater creator.

1080
01:07:46,800 --> 01:07:49,800
You know, machine and AI could do the job faster,

1081
01:07:49,800 --> 01:07:52,800
cheaper, and they don't have strike,

1082
01:07:52,800 --> 01:07:56,800
and, you know, they don't resist any, like,

1083
01:07:56,800 --> 01:08:00,800
ridiculous demand from the clients or from the bosses,

1084
01:08:00,800 --> 01:08:05,800
and I can see that Chinese companies are already,

1085
01:08:05,800 --> 01:08:08,800
like, using it to replace human labors.

1086
01:08:08,800 --> 01:08:11,800
So I think this is a very critical moment right now

1087
01:08:11,800 --> 01:08:14,800
for the creators around the world.

1088
01:08:14,800 --> 01:08:17,800
So this is something happening,

1089
01:08:17,800 --> 01:08:21,800
and it's gonna be big in the next three to five years.

1090
01:08:26,800 --> 01:08:29,800
So for folks like myself who, you know,

1091
01:08:29,800 --> 01:08:31,800
I've been able to build a life for myself,

1092
01:08:31,800 --> 01:08:34,800
but I would definitely not say that I'm in the,

1093
01:08:34,800 --> 01:08:37,800
sort of, like, top tier of the music industry.

1094
01:08:37,800 --> 01:08:40,800
There's a way that I think we're able to skirt under the radar

1095
01:08:40,800 --> 01:08:42,800
and continue doing work as we're doing it,

1096
01:08:42,800 --> 01:08:45,800
because it's so much about experimentation,

1097
01:08:45,800 --> 01:08:48,800
it's so much about trying out weird things,

1098
01:08:48,800 --> 01:08:51,800
and AI is so much about averaging.

1099
01:08:51,800 --> 01:08:55,800
It's the people who are invested in playing

1100
01:08:55,800 --> 01:08:58,800
in that space of the anomaly and playing with the unexpected,

1101
01:08:58,800 --> 01:09:01,800
who will, sort of, continue to thrive.

1102
01:09:11,800 --> 01:09:13,800
The impact of artificial intelligence

1103
01:09:13,800 --> 01:09:17,800
on our society is far-reaching and complex.

1104
01:09:17,800 --> 01:09:19,800
How can we regulate a technology

1105
01:09:19,800 --> 01:09:21,800
that's developing so quickly

1106
01:09:21,800 --> 01:09:25,800
and whose potential is almost impossible to gauge?

1107
01:09:26,800 --> 01:09:28,800
The United States is struggling.

1108
01:09:28,800 --> 01:09:32,800
Here in Washington, the tech industry's influence is huge,

1109
01:09:32,800 --> 01:09:35,800
and governing majorities are fragile.

1110
01:09:35,800 --> 01:09:39,800
The fact of the matter is that the U.S. government moves slowly.

1111
01:09:39,800 --> 01:09:41,800
It is a democracy.

1112
01:09:41,800 --> 01:09:44,800
That slowness is built into the system.

1113
01:09:44,800 --> 01:09:46,800
The U.S. government is not supposed to be efficient

1114
01:09:46,800 --> 01:09:49,800
and not supposed to be able to tackle problems quickly,

1115
01:09:49,800 --> 01:09:52,800
because a government that is too efficient,

1116
01:09:52,800 --> 01:09:55,800
you know, can use that against its own citizens, too.

1117
01:09:55,800 --> 01:09:58,800
I think that we've now reached a state in our society

1118
01:09:58,800 --> 01:10:01,800
that many philosophers and writers

1119
01:10:01,800 --> 01:10:03,800
in the 20th century warned about,

1120
01:10:03,800 --> 01:10:07,800
which is the inability to govern technology

1121
01:10:07,800 --> 01:10:10,800
due to the increasing pace of change.

1122
01:10:13,800 --> 01:10:17,800
Well, I certainly would not bet against democracies,

1123
01:10:17,800 --> 01:10:20,800
but it will be a really tough adjustment period.

1124
01:10:21,800 --> 01:10:23,800
The first major piece of legislation

1125
01:10:23,800 --> 01:10:25,800
aimed at regulating artificial intelligence

1126
01:10:25,800 --> 01:10:30,800
on a far-reaching scale came from the EU, the AI Act.

1127
01:10:31,800 --> 01:10:34,800
I definitely kind of really admire European Union

1128
01:10:34,800 --> 01:10:36,800
for being essentially the leader in this space.

1129
01:10:36,800 --> 01:10:39,800
They took on this kind of regulation very seriously

1130
01:10:39,800 --> 01:10:42,800
before anyone else really fought seriously all that.

1131
01:10:51,800 --> 01:10:54,800
Jonas Androulas has come to Brussels.

1132
01:10:56,800 --> 01:10:58,800
Together with other startup founders,

1133
01:10:58,800 --> 01:11:00,800
he wants to let politicians know

1134
01:11:00,800 --> 01:11:03,800
that strong regulation could put smaller European players

1135
01:11:03,800 --> 01:11:06,800
at a disadvantage compared to the competition

1136
01:11:06,800 --> 01:11:08,800
in the U.S. and China.

1137
01:11:14,800 --> 01:11:17,800
Meetings like this are always a bit difficult,

1138
01:11:17,800 --> 01:11:19,800
because you say your piece,

1139
01:11:19,800 --> 01:11:22,800
and you never really know what reaction you're going to get.

1140
01:11:22,800 --> 01:11:24,800
A few new people will listen,

1141
01:11:24,800 --> 01:11:26,800
and of course it's clear that cooperation

1142
01:11:26,800 --> 01:11:29,800
within Europe and with Europe is important,

1143
01:11:29,800 --> 01:11:33,800
but it's always hard to say how much we can achieve here and now.

1144
01:11:48,800 --> 01:11:49,800
No.

1145
01:12:05,800 --> 01:12:06,800
Do you have documents?

1146
01:12:06,800 --> 01:12:11,800
I could just talk. I've prepared something.

1147
01:12:12,800 --> 01:12:16,800
It's your session. You can decide where you want to sit.

1148
01:12:16,800 --> 01:12:34,800
So, good afternoon to all of you. I'm pleased to welcome you to the European Parliament to this

1149
01:12:34,800 --> 01:12:42,800
meeting, an important meeting at the right time. Something that will happen and we already see

1150
01:12:42,800 --> 01:12:48,800
basically a few steps down the road is like the cloud, like the hyperscalers have done

1151
01:12:48,800 --> 01:12:54,800
with cloud compute, there will be an infrastructure for general intelligence, that all the value

1152
01:12:54,800 --> 01:13:01,800
creation, all the apps, all the new innovations in the world will build upon. And for us, there

1153
01:13:01,800 --> 01:13:07,800
will be no second chance. If we cannot move fast, then we won't be able to try again in 12 months.

1154
01:13:07,800 --> 01:13:09,800
Thank you very much.

1155
01:13:10,800 --> 01:13:13,800
APPLAUSE

1156
01:13:28,800 --> 01:13:34,800
Androulos has repeated his message over and over again, whether on international stages,

1157
01:13:34,800 --> 01:13:40,800
to German politicians or here at the European Parliament. Over the course of a year, networking

1158
01:13:40,800 --> 01:13:43,800
and lobbying has become second nature to him.

1159
01:13:48,800 --> 01:13:54,800
So, I started my career as an investment banker and management consultant, wearing a suit and

1160
01:13:54,800 --> 01:13:56,800
38 degree weather with no air conditioning.

1161
01:14:04,800 --> 01:14:07,800
MUSIC

1162
01:14:17,800 --> 01:14:21,800
I don't think I'd make a good politician. I realize that in my days at Apple.

1163
01:14:24,800 --> 01:14:30,800
What's the probability of success? Is it worth investing this time? Is it worth fighting this battle?

1164
01:14:30,800 --> 01:14:35,800
I think so. I think it's a battle worth fighting, but I also have moments when I think that doing

1165
01:14:35,800 --> 01:14:37,800
something else would be pretty nice.

1166
01:14:46,800 --> 01:14:51,800
Shortly after his meeting with the European Parliament, it passed the AI Act.

1167
01:14:51,800 --> 01:14:54,800
MUSIC

1168
01:14:56,800 --> 01:15:00,800
Ten or even five years down the line, it is this governance structure that will give Europe the

1169
01:15:00,800 --> 01:15:05,800
ability to deal with the rapid evolution of AI and to reap the most benefits from it.

1170
01:15:05,800 --> 01:15:10,800
And we have worked first and foremost to ensure our citizens' rights and freedoms are not just

1171
01:15:10,800 --> 01:15:16,800
respected, but protected and strengthened. We don't want mass surveillance. We don't want social

1172
01:15:16,800 --> 01:15:19,800
scoring. We don't want predictive policing in the European Union. Full stop.

1173
01:15:28,800 --> 01:15:31,800
My name is Dragosh Daraque. I'm a member of the European Parliament, representing the

1174
01:15:31,800 --> 01:15:36,800
Renew Group, the Liberal Group in the European Parliament. I'm a judge by profession. I was

1175
01:15:36,800 --> 01:15:42,800
also a member of government in Romania, Minister of Digitalization, Minister of Interior, prior

1176
01:15:42,800 --> 01:15:43,800
to coming to Parliament.

1177
01:15:46,800 --> 01:15:52,800
AI will play very much into the power balance. Why? Because it drives our economies, but not

1178
01:15:52,800 --> 01:15:57,800
only. It becomes also a geopolitical factor, both in terms of how warfare is going to look

1179
01:15:57,800 --> 01:16:03,800
like, but also how this technology will play into many of the processes that will keep

1180
01:16:03,800 --> 01:16:09,800
one part of the world or the other competitive. And therefore, also the way you write the

1181
01:16:09,800 --> 01:16:15,800
standards and how those standards become globally accepted standards is very important in that

1182
01:16:15,800 --> 01:16:19,800
power balance that you mentioned earlier. So we're going to see very soon also, I think,

1183
01:16:19,800 --> 01:16:24,800
a competition or possible clash in terms of global standards. And that is why we have

1184
01:16:24,800 --> 01:16:32,800
to take measures to protect our interests and also to make sure that, again, our understanding

1185
01:16:32,800 --> 01:16:37,800
of the role of technology is one that is shared by as many on the global stage as possible.

1186
01:16:37,800 --> 01:16:42,800
In renegotiations, Germany, France and Italy lobbied again to soften the rules of the

1187
01:16:42,800 --> 01:16:49,800
AI Act to protect domestic players like Alpha Alpha from heavy regulation. But in the end,

1188
01:16:49,800 --> 01:16:52,800
the European Parliament prevailed.

1189
01:17:02,800 --> 01:17:06,800
In terms of regulation, we're the economy that's leading the way. And there's a concern that

1190
01:17:06,800 --> 01:17:14,800
that will take too much creativity out of the market. So in Europe, we're better at regulation

1191
01:17:14,800 --> 01:17:18,800
than at putting technology on the market, unfortunately.

1192
01:17:18,800 --> 01:17:24,800
The truth is, it's ultimately going to be good for the tech industry as well to be regulated,

1193
01:17:24,800 --> 01:17:31,800
level playing field. Even seatbelts in cars were viciously opposed by the auto industry at first.

1194
01:17:31,800 --> 01:17:38,800
But then when we got the law saying all cars have to have seatbelts, they started to sell much more cars.

1195
01:18:02,800 --> 01:18:09,800
Han Shao has travelled to Shenzhen. In order to keep his team on the same page, the CEO has to visit

1196
01:18:09,800 --> 01:18:12,800
the various company offices regularly.

1197
01:18:12,800 --> 01:18:20,800
So you see that there's red letters on the building that is basically our office. But we are not that big.

1198
01:18:20,800 --> 01:18:23,800
We are just one small room inside that big building.

1199
01:18:24,800 --> 01:18:34,800
He wants to take his company Gina to the next level. That will require all his employees to pull together

1200
01:18:34,800 --> 01:18:36,800
as much as possible.

1201
01:18:41,800 --> 01:18:43,800
I'll just put this down.

1202
01:18:47,800 --> 01:18:51,800
I brought some waffles. Try them. They're delicious.

1203
01:18:54,800 --> 01:18:56,800
You have to eat them.

1204
01:18:56,800 --> 01:18:58,800
You don't eat them.

1205
01:19:00,800 --> 01:19:05,800
I told them you have to eat now and make a happy face to the camera.

1206
01:19:14,800 --> 01:19:19,800
When I work at Germany, people are greeting each other like telling jokes,

1207
01:19:19,800 --> 01:19:24,800
talking random stuff, football match yesterday, all these kind of things.

1208
01:19:24,800 --> 01:19:30,800
Here is more introvert. The office is more introvert.

1209
01:19:30,800 --> 01:19:33,800
It's just like different working cultures.

1210
01:19:33,800 --> 01:19:37,800
Both of them are pretty productive under my weep.

1211
01:19:37,800 --> 01:19:51,800
I always think that a sequence of small success will make the team stronger

1212
01:19:51,800 --> 01:19:54,800
and make the team more confident on building things.

1213
01:19:54,800 --> 01:19:57,800
Because larger success means larger hope.

1214
01:19:57,800 --> 01:20:02,800
A larger hope could mean larger disappointment.

1215
01:20:03,800 --> 01:20:08,800
Here in the start-up, everything moves very quickly.

1216
01:20:08,800 --> 01:20:10,800
Then we become a little bit stressed.

1217
01:20:10,800 --> 01:20:15,800
We become a little bit nervous because we could lose the advantages

1218
01:20:15,800 --> 01:20:19,800
against the other competitors, against the market and so on.

1219
01:20:19,800 --> 01:20:26,800
It's not about what we did. It's about how people perceive us on what we did.

1220
01:20:27,800 --> 01:20:34,800
In Germany, there is also a team working on releasing a new large-language model.

1221
01:20:34,800 --> 01:20:39,800
Yesterday, the leaders told me that this model can be ready on Monday,

1222
01:20:39,800 --> 01:20:42,800
but it has been postponed for many times.

1223
01:20:42,800 --> 01:20:45,800
I have to see how it goes.

1224
01:20:49,800 --> 01:20:53,800
In the evening, Han Xiaoh has another meeting with a potential investor.

1225
01:20:54,800 --> 01:20:58,800
On the way, he calls his technical director to ask whether the launch

1226
01:20:58,800 --> 01:21:01,800
of the new language model is going as planned.

1227
01:21:07,800 --> 01:21:11,800
Hey, Wan Nan, your embedding platform has to get into the global best model list.

1228
01:21:11,800 --> 01:21:14,800
This company won't succeed unless everyone does their best.

1229
01:21:19,800 --> 01:21:22,800
If you don't get into the top 10, it'll be much more difficult.

1230
01:21:23,800 --> 01:21:26,800
Who uses a platform that's not in the top 10?

1231
01:21:31,800 --> 01:21:34,800
You need to think more about these practical things.

1232
01:21:34,800 --> 01:21:37,800
Is the LinkedIn post done? The Twitter post?

1233
01:21:37,800 --> 01:21:39,800
There needs to be a strategy here.

1234
01:21:42,800 --> 01:21:45,800
Okay, that's it. Bye.

1235
01:21:46,800 --> 01:21:53,800
We want to get into the top 10 model, a leaderboard.

1236
01:21:53,800 --> 01:21:59,800
Our models get into the top 10, but the German team just told me

1237
01:21:59,800 --> 01:22:02,800
they probably cannot get into the top 10.

1238
01:22:02,800 --> 01:22:10,800
That's why I get a little bit intense on my conversation,

1239
01:22:10,800 --> 01:22:14,800
because I said this is something that we promised to ourselves.

1240
01:22:14,800 --> 01:22:18,800
In this world, it's a very, very attention-based world.

1241
01:22:18,800 --> 01:22:27,800
If you cannot get into the top 10, even if you get into number 11, nobody cares.

1242
01:22:27,800 --> 01:22:33,800
This is why I'm telling the team that it's not about engineering only.

1243
01:22:33,800 --> 01:22:38,800
You also have to think about the whole company, the marketing sales.

1244
01:22:38,800 --> 01:22:44,800
It all depends on the top 10 models on this leaderboard.

1245
01:23:09,800 --> 01:23:14,800
Hi, Grace.

1246
01:23:24,800 --> 01:23:28,800
Grace Liu works for a Chinese investment bank.

1247
01:23:28,800 --> 01:23:34,800
The two first met a couple of years ago during the start-up phase of Han Xiaos' company.

1248
01:23:38,800 --> 01:23:43,800
You're just starting to bring AI-generated content to people, right?

1249
01:23:45,800 --> 01:23:49,800
Multimodal AI. We're working on two things right now.

1250
01:23:49,800 --> 01:23:55,800
One is prompt technology, and the other is embedding technology.

1251
01:23:55,800 --> 01:24:00,800
This year will be quite a challenge for you.

1252
01:24:00,800 --> 01:24:05,800
We've made a new software with prompt perfect aimed at developers.

1253
01:24:05,800 --> 01:24:09,800
We've already got 200,000 registered users.

1254
01:24:09,800 --> 01:24:13,800
Ah, that means there's a lot of demand.

1255
01:24:13,800 --> 01:24:17,800
I don't think I can jump to my conclusion yet,

1256
01:24:17,800 --> 01:24:23,800
but Han mentioned something very interesting to me about his new development and two new products.

1257
01:24:23,800 --> 01:24:28,800
Actually, the most important thing is the CEO, him or herself, right?

1258
01:24:28,800 --> 01:24:34,800
And whether he is a good entrepreneur, not only a scientist or a good developer.

1259
01:24:34,800 --> 01:24:39,800
Meetings like this one put opportunities on the table for Han Xiaos' company,

1260
01:24:39,800 --> 01:24:41,800
both in China and in the West.

1261
01:24:41,800 --> 01:24:44,800
And there's good news about his important project.

1262
01:24:44,800 --> 01:24:50,800
The new developer tool performs just as well as the equivalent technology from OpenAI.

1263
01:24:50,800 --> 01:24:57,800
By the end of 2023, Jonas Androulas has plenty to celebrate.

1264
01:24:57,800 --> 01:25:00,800
He's completed a major round of financing.

1265
01:25:00,800 --> 01:25:06,800
The company prevailed and convinced enough investors to raise half a billion dollars.

1266
01:25:06,800 --> 01:25:09,800
That's money Androulas is going to need,

1267
01:25:09,800 --> 01:25:14,800
because competitor OpenAI is already triggering a new technology.

1268
01:25:15,800 --> 01:25:20,800
We did a lot of things that smart people told me.

1269
01:25:20,800 --> 01:25:23,800
Four years ago, they would be impossible.

1270
01:25:23,800 --> 01:25:23,880
They signed the contract,

1271
01:25:38,880 --> 01:25:40,880
But what about technologies?

1272
01:25:40,880 --> 01:25:44,720
four years ago, they would be impossible.

1273
01:25:44,720 --> 01:25:49,120
Build deep tech, AIR&D out of Germany, impossible.

1274
01:25:49,120 --> 01:25:53,280
Fund this with mostly European capital, impossible.

1275
01:25:53,280 --> 01:25:57,080
Build our own data center, impossible.

1276
01:25:57,080 --> 01:26:01,180
Contribute category defining research, impossible.

1277
01:26:01,180 --> 01:26:03,400
And now we are entering into a new era

1278
01:26:03,400 --> 01:26:06,400
and I'm super happy to have you all with us.

1279
01:26:06,400 --> 01:26:08,160
Yeah, and thanks for being here

1280
01:26:08,160 --> 01:26:10,280
and help us make this the best party

1281
01:26:10,280 --> 01:26:11,880
that Heidelberg has ever seen.

1282
01:26:11,880 --> 01:26:12,720
Thanks.

1283
01:26:21,720 --> 01:26:24,320
And for Thomas Wolff, quiet holidays

1284
01:26:24,320 --> 01:26:26,480
may soon be a thing of the past.

1285
01:26:26,480 --> 01:26:30,920
Hugging face is now valued at $4.5 billion.

1286
01:26:30,920 --> 01:26:34,400
Thanks to successful startups and the AI Act,

1287
01:26:34,400 --> 01:26:36,840
the EU at least has a seat at the table

1288
01:26:36,840 --> 01:26:39,360
alongside the US and China.

1289
01:26:39,360 --> 01:26:43,560
For now, for humanity at large, the question remains,

1290
01:26:43,560 --> 01:26:45,920
what kind of world are we building right now

1291
01:26:45,920 --> 01:26:49,480
for ourselves and for our children?

1292
01:26:49,480 --> 01:26:52,160
I was holding my little baby Leo,

1293
01:26:52,160 --> 01:26:53,960
you know, who just turned nine months old

1294
01:26:55,400 --> 01:26:57,480
and looking into his eyes and thinking that, you know,

1295
01:26:57,480 --> 01:27:01,160
right now his language abilities are much worse

1296
01:27:01,160 --> 01:27:06,160
than chat's GPT-4 and he's never gonna catch up with AI ever.

1297
01:27:07,040 --> 01:27:09,440
I have two kids that are in middle school

1298
01:27:09,440 --> 01:27:10,720
and I'm thinking exactly about that,

1299
01:27:10,720 --> 01:27:12,720
what I should teach them about,

1300
01:27:12,720 --> 01:27:16,040
so they kind of prepared for the AI-infused future.

1301
01:27:16,040 --> 01:27:19,480
How do we teach our kids to kind of build something

1302
01:27:19,480 --> 01:27:21,960
like unique and individual?

1303
01:27:21,960 --> 01:27:24,560
The machine is our something,

1304
01:27:24,560 --> 01:27:28,000
it's our brothers or sisters, so we're working together.

1305
01:27:28,000 --> 01:27:30,720
So I think that's how I feel.

1306
01:27:31,320 --> 01:27:34,280
Even in these pivotal moments,

1307
01:27:34,280 --> 01:27:37,960
in these complex times, people always find a way.

1308
01:27:37,960 --> 01:27:40,160
They're creative and resourceful.

1309
01:27:40,160 --> 01:27:42,800
My son is already learning to code.

1310
01:27:42,800 --> 01:27:44,960
He's really interested in AI.

1311
01:27:44,960 --> 01:27:48,400
He wants to understand things and create things using AI.

1312
01:27:48,400 --> 01:27:50,440
Our children will probably create a world

1313
01:27:50,440 --> 01:27:52,840
that's completely different from ours.

1314
01:27:52,840 --> 01:27:55,600
We have a lot of people who are interested in AI,

1315
01:27:55,600 --> 01:27:57,400
who are interested in AI,

1316
01:27:57,400 --> 01:27:58,800
who will probably create a world

1317
01:27:58,800 --> 01:28:01,120
that's completely different from ours.

1318
01:28:01,120 --> 01:28:02,800
But I'm not worried.

1319
01:28:02,800 --> 01:28:05,800
At the end of the day, I'm an optimist.

