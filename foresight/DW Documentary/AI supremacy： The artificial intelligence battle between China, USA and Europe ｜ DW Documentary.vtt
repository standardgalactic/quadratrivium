WEBVTT

00:00.000 --> 00:09.240
After many, many years later, maybe we will look back.

00:09.240 --> 00:15.080
We recognize that, oh, that's the moment when everything changed.

00:15.080 --> 00:19.200
In five years, AI is going to literally be in everything we do.

00:19.200 --> 00:26.400
As humans, we have to, first of all, understand that this is going to happen.

00:26.400 --> 00:31.440
Many technologists try to solve human problems using technology when actually what we need

00:31.440 --> 00:35.200
are human solutions.

00:35.200 --> 00:38.360
The deeper question is, what does it mean to be human?

00:38.360 --> 00:41.920
What are the things I'll still be proud of?

00:41.920 --> 00:48.080
I find this moment extremely profound, because it really forces us as a humanity to think

00:48.080 --> 00:52.760
through exactly what consciousness is, what makes humans human.

00:52.760 --> 00:56.040
There's a whole lot at stake here.

00:56.040 --> 01:00.800
It's careers, unbelievable amounts of money, and who gets to shape the future.

01:26.040 --> 01:32.240
It's already begun.

01:32.240 --> 01:36.320
A global race for supremacy in the age of artificial intelligence.

01:36.320 --> 01:42.000
China, the United States, and the European Union are vying for economic growth, political

01:42.000 --> 01:44.440
influence, and power.

01:44.440 --> 01:50.200
So are the big tech companies, and startups are hot on their heels.

01:50.200 --> 01:55.840
For those who lose, there'll be no second chances.

01:55.840 --> 02:01.120
Jonas Andrewles founded one of the most influential European AI companies.

02:01.120 --> 02:06.680
With his help, the European Union could catch up with the world's AI leaders, become independent

02:06.680 --> 02:13.840
from the US and China, secure a prosperous future.

02:13.840 --> 02:18.560
This is a watershed moment for Europe, the last roll of the dice.

02:18.560 --> 02:23.360
If Europe wants to decide how to use technology in accordance with European values, then it

02:23.360 --> 02:27.880
has to be able to build that technology itself.

02:27.880 --> 02:33.200
Thomas Wolff co-founded Huggingface, a major open source AI platform.

02:33.200 --> 02:37.880
He wants to stop one of the most powerful technologies in human history from ending

02:37.880 --> 02:40.640
up in the hands of a few corporations.

02:40.640 --> 02:46.160
What we need is a multitude of players, not just one that owns AI.

02:46.160 --> 02:51.360
We don't want a future where such a fundamental technology is in the hands of a single company.

02:51.360 --> 02:55.680
There are plenty of films where that's the mark of a dystopia, one company that controls

02:55.680 --> 03:03.680
everything.

03:03.680 --> 03:06.720
Han Chao is a Chinese AI entrepreneur.

03:06.720 --> 03:13.280
He wants his company, Gina AI, to be successful on both the Western and Chinese markets.

03:13.280 --> 03:18.480
So what role do Chinese AI companies play in this global race, and how is the Chinese

03:18.480 --> 03:22.200
Communist Party using them to achieve its political goals?

03:22.200 --> 03:28.200
Chai GPT or GPT-4, it basically serves as a brain, and this brain cannot be made in

03:28.200 --> 03:29.200
the US.

03:29.200 --> 03:32.640
So this is some worry that the Chinese government has.

03:32.640 --> 03:40.320
The culture of each country, how each government runs this country will eventually be reflected

03:40.320 --> 03:44.320
into this brain.

03:44.320 --> 03:49.480
It is absolutely clear that at the highest levels of leadership in the United States

03:49.480 --> 03:55.240
and in China, artificial intelligence is viewed as foundational to the future of economic

03:55.240 --> 03:59.640
and military power.

03:59.640 --> 04:05.840
We've created an economic and financial system that's based on the assumption that everything's

04:05.840 --> 04:07.560
going to keep running smoothly.

04:07.560 --> 04:10.200
We get cheap gas from Russia.

04:10.200 --> 04:14.000
The Americans look after us, so we don't need to spend money on arms.

04:14.000 --> 04:19.280
And as always friendly, we've gotten comfortable, and now we have to break out of that comfort

04:19.280 --> 04:25.160
and say, we can go on the political offensive again, we can compete, we can create competitive

04:25.160 --> 04:30.520
conditions here, we can make sure that the cool companies we have can also grow and play

04:30.520 --> 04:45.480
a role in the global market.

04:45.480 --> 04:51.520
At the end of 2023, German Vice Chancellor Robert Harbeck made significant progress towards

04:51.520 --> 04:53.640
these geopolitical goals.

04:53.640 --> 05:02.200
Aleph Alfa, a German AI company, raised around half a billion euros from investors.

05:02.200 --> 05:07.640
It was one of the largest rounds of European financing for artificial intelligence technology,

05:07.640 --> 05:12.120
and it was a signal that the European Union can produce elite players in the field of

05:12.120 --> 05:15.160
AI.

05:15.160 --> 05:19.400
Aleph Alfa's founder and CEO is Jonas Androulis.

05:19.400 --> 05:25.800
His given priority to investment from German industry, SAP, Bosch and the Schwarz Group,

05:25.800 --> 05:43.440
which owns supermarket retailers Liedl and Kauffland.

05:43.440 --> 05:45.880
I was an amateur radio enthusiast.

05:45.880 --> 05:49.000
I soldered radios together and built my own antennas.

05:49.000 --> 05:53.240
Early on, my father had computers at home, so I was able to start programming and playing

05:53.240 --> 06:03.320
around with them at a very young age.

06:03.320 --> 06:07.320
When we started out, the term generative AI didn't exist.

06:07.320 --> 06:09.360
Hardly anyone had heard of open AI.

06:09.360 --> 06:10.840
We were very technical.

06:10.840 --> 06:13.840
We managed to create category defining innovations.

06:13.840 --> 06:21.720
We were just nerds, researchers.

06:21.720 --> 06:26.560
And there were times when I felt like I wasn't coping with the amount of work and the challenges.

06:26.560 --> 06:31.120
Every night, I could answer emails until I was so tired I fell off the couch.

06:31.120 --> 06:39.080
And there were still things I was neglecting, which I didn't want to neglect.

06:39.080 --> 06:44.960
In spring 2023, the European AI landscape was a lonely place.

06:44.960 --> 06:50.960
With his startup, Alfa, Jonas Androulis had built the only generative AI that could compete

06:50.960 --> 06:53.120
at an international level.

06:53.120 --> 06:59.320
He'd acquired the expertise from his work as a high-ranking AI researcher at Apple.

06:59.320 --> 07:08.000
Suddenly he'd become one of Europe's great hopes in the global AI race.

07:08.000 --> 07:13.120
Right now we simply can't cope with the onslaught of potential customers and partners.

07:13.120 --> 07:18.600
Most of the German stock index companies have been in touch, lots of medium-sized companies.

07:18.600 --> 07:23.720
There are briefings, press engagements, events, an unbelievable amount of stuff.

07:23.720 --> 07:28.600
It's like the Cambrian explosion right now, an incredible amount of new and creative things

07:28.600 --> 07:29.600
are emerging.

07:29.600 --> 07:36.760
We're the only Europeans to be involved on this scale.

07:36.760 --> 07:38.480
It was an exciting development.

07:38.480 --> 07:43.240
We also had things like open AI and maybe ours were even cooler.

07:43.240 --> 07:47.680
What is missing and what I think deserves more attention in the EU is why there are

07:47.680 --> 07:54.440
not more domestic companies that actually grow and scale.

07:54.440 --> 08:01.200
A lot of company leaders, startup innovators end up going to the US and the access to capital

08:01.200 --> 08:06.280
is really one of the main challenges.

08:06.680 --> 08:14.160
If you look at what we did, it's enabling technology.

08:14.160 --> 08:19.640
What I need for that is a carefully selected, effective team of brilliant researchers.

08:19.640 --> 08:22.200
Then I need money.

08:22.200 --> 08:26.040
More money than you normally get as a German startup.

08:26.040 --> 08:29.120
These days we're talking about billions.

08:29.120 --> 08:34.160
And then you need partners to help you, the kind of help that money can't buy.

08:34.160 --> 08:39.040
Open AI doesn't just get 10 billion from Microsoft, it also gets incredible support

08:39.040 --> 08:45.960
in integrating its technology into all Microsoft products and platforms.

08:45.960 --> 08:51.440
At the time, Aleph Alpha had 60 employees at various locations.

08:51.440 --> 08:56.040
Most were at the company's headquarters in Heidelberg in southwest Germany, unlike

08:56.040 --> 09:02.480
Open AI, and Drullus wasn't gearing his AI towards private users, but rather industry

09:02.480 --> 09:04.280
in the public sector.

09:04.280 --> 09:08.320
But they tend to be sluggish and not easy to win over.

09:08.320 --> 09:11.400
That posed a challenge for Jonas Andrullus.

09:11.400 --> 09:15.280
He needed pilot projects to prove his technology works.

09:15.280 --> 09:19.080
You stand here and you're greeted by a virtual person.

09:19.080 --> 09:20.680
Hello, may I help you?

09:20.680 --> 09:21.680
Where do you want to go?

09:21.680 --> 09:24.120
Then I can use the screen.

09:24.120 --> 09:27.120
We have to keep moving in this direction.

09:27.120 --> 09:28.120
That's not good.

09:28.120 --> 09:29.120
That's my job.

09:29.360 --> 09:32.400
I know it is, but we're very happy to still have you.

09:32.400 --> 09:35.400
She'll probably be taking her well-earned retirement soon.

09:35.400 --> 09:37.720
It's become a standard question.

09:37.720 --> 09:42.160
I ask if you'd like to stay on a little longer, because I need you, and because there aren't

09:42.160 --> 09:45.200
enough skilled workers coming in.

09:45.200 --> 09:47.200
It's a big issue.

09:48.800 --> 09:51.800
That's important for us.

09:52.800 --> 09:54.800
Solidarity.

09:55.800 --> 09:59.800
And, of course, the main topic is innovation.

10:00.800 --> 10:06.800
Heidelberg is one of the first municipalities in the world to introduce an AI citizen assistant

10:06.800 --> 10:10.800
using a language model provided by Aleph Alpha.

10:13.800 --> 10:18.800
We have a partner, a customer, with whom we can look at these new technologies.

10:18.800 --> 10:23.800
They also act as a testimonial for us, because anyone can go and try it out.

10:23.800 --> 10:29.800
That's a huge advantage, because a lot of our customers don't want to be named right now.

10:29.800 --> 10:32.800
They don't want people to know exactly what they're doing,

10:32.800 --> 10:37.800
so it's great to have a pilot customer with a bit of vision and courage.

10:38.800 --> 10:44.800
The hope is that in the long term, AI will improve public administration and speed up services.

10:45.800 --> 10:53.800
I just enter a question. Motor vehicle traffic on the B-37, which is a busy road.

10:53.800 --> 10:57.800
Then I get it to search. Of course, that's a very general question.

10:57.800 --> 11:02.800
The question now is what point in time, whether we're talking daily or annually.

11:02.800 --> 11:06.800
Of course, the AI has to work out what the user actually wants.

11:06.800 --> 11:09.800
The B-37 is closed between 7 a.m. and 6 p.m.

11:09.800 --> 11:13.800
Now I can ask, how do I apply for child benefit?

11:15.800 --> 11:19.800
That wasn't the right answer. It can't find anything now.

11:19.800 --> 11:24.800
So the error messages that we get back from the public and from tests we do ourselves

11:24.800 --> 11:30.800
get passed on to Aleph Alpha to figure out what needs to change to make the inputs more accurate.

11:30.800 --> 11:33.800
It's always about the accuracy of the input.

11:36.800 --> 11:40.800
The technology is still in the test phase and not yet reliable.

11:41.800 --> 11:44.800
2023 was a delicate time for Aleph Alpha.

11:44.800 --> 11:48.800
Jonas and Droolers needed fresh money from investors.

11:48.800 --> 11:52.800
Meanwhile, Microsoft and OpenAI were gaining more of an advantage.

11:53.800 --> 11:56.800
In fact, a race starts today and we're going to move.

11:56.800 --> 12:03.800
We're going to move fast and for us, every day, we want to bring out new things.

12:04.800 --> 12:09.800
On March 14th, 2023, OpenAI released ChatGPT4,

12:09.800 --> 12:13.800
the most powerful artificial intelligence to date.

12:13.800 --> 12:17.800
At the same time, it greatly reduced costs for users.

12:17.800 --> 12:22.800
For Jonas and Droolers and his team, it was a threat to their business model.

12:22.800 --> 12:26.800
We might have a poem about fly fishing from the perspective of a fish.

12:27.800 --> 12:32.800
And it goes on for hours.

12:32.800 --> 12:36.800
In the depths where shadows weave and play,

12:36.800 --> 12:39.800
in this cool clear waters where I stay,

12:39.800 --> 12:42.800
I am the fish beneath the silver stream,

12:42.800 --> 12:45.800
where life's a dream or so it seems,

12:45.800 --> 12:47.800
a wily being, sleek and sly,

12:47.800 --> 12:50.800
with ancient instincts to live and die.

12:50.800 --> 12:53.800
Yeah, it goes on.

12:53.800 --> 12:56.800
Yeah, they see the ends of my heart.

12:58.800 --> 13:01.800
I went to an apartment with a number of colleagues

13:01.800 --> 13:05.800
and we watched on a big projector screen

13:05.800 --> 13:07.800
the announcement of GPT4.

13:07.800 --> 13:10.800
Someone had a ChatGPT Pro account,

13:10.800 --> 13:13.800
so they could use GPT4 and we could play with it.

13:13.800 --> 13:20.800
And we were, like, very impressed and surprised by how good it was.

13:20.800 --> 13:24.800
It's not upsetting when someone comes out with a great piece of technology

13:24.800 --> 13:28.800
because we're researchers and building technology and that's how it is.

13:28.800 --> 13:31.800
You know, when you're a violinist and you go and you watch

13:31.800 --> 13:35.800
an amazing solo by an incredible violinist,

13:35.800 --> 13:39.800
you don't feel, oh, you know, I'll give up.

13:39.800 --> 13:41.800
You know, it's inspiring.

13:41.800 --> 13:43.800
I was a little stressed out.

13:43.800 --> 13:47.800
I was in the middle of conversations with potential investors, business partners,

13:47.800 --> 13:50.800
and I knew that in every conversation I was going into,

13:50.800 --> 13:53.800
somebody would say, but GPT4.

13:53.800 --> 13:56.800
I would have wanted to build GPT4 myself.

13:56.800 --> 14:00.800
Two years ago, get 200 million,

14:00.800 --> 14:05.800
be the first model at that level of capabilities

14:05.800 --> 14:07.800
out of Heidelberg, out of Europe.

14:07.800 --> 14:10.800
It caused a lot of frustration in the team and I saw that.

14:10.800 --> 14:12.800
That's painful to see.

14:13.800 --> 14:17.800
In fact, comparing open AI with ALEF Alpha was absurd.

14:17.800 --> 14:21.800
Open AI was almost half owned by tech giant Microsoft,

14:21.800 --> 14:25.800
which had pumped over 10 billion dollars into the company.

14:25.800 --> 14:29.800
Jonas Androulas had raised just 28 million euros.

14:29.800 --> 14:32.800
Still, he wanted to take them on.

14:32.800 --> 14:35.800
We're all under enormous pressure.

14:35.800 --> 14:37.800
We're fighting for security.

14:38.800 --> 14:41.800
We're all under enormous pressure.

14:41.800 --> 14:43.800
We're fighting for survival.

14:43.800 --> 14:46.800
We've created something world-class with a lot less money.

14:46.800 --> 14:50.800
We're basically at the forefront on the highest level.

14:50.800 --> 14:56.800
But we all know that there's now a wave of Microsoft money rolling towards us,

14:56.800 --> 14:59.800
and we can't do anything to stop it.

15:07.800 --> 15:11.800
Is it very easier to speak in French or English?

15:11.800 --> 15:14.800
I might find it difficult to make really French

15:14.800 --> 15:16.800
without English words everywhere.

15:16.800 --> 15:17.800
Yes.

15:17.800 --> 15:19.800
Yeah, because, yeah.

15:19.800 --> 15:22.800
While Jonas Androulas was filling the heat

15:22.800 --> 15:26.800
from industry top dogs Microsoft and open AI,

15:26.800 --> 15:28.800
Thomas Wolff was more worried

15:28.800 --> 15:31.800
about the fact that he could not speak English.

15:31.800 --> 15:34.800
He was worried about the fact that he could not speak English.

15:34.800 --> 15:38.800
With Microsoft and open AI, Thomas Wolff was more relaxed.

15:38.800 --> 15:40.800
He co-founded Huggingface,

15:40.800 --> 15:45.800
which has 200 employees and offices in Paris, New York and Amsterdam.

15:45.800 --> 15:48.800
The company has built a successful platform

15:48.800 --> 15:51.800
where programmers and companies can share AI models

15:51.800 --> 15:53.800
and further develop them.

15:53.800 --> 15:56.800
The philosophy, the mission and the values that we push

15:56.800 --> 15:59.800
are actually very European by some way.

15:59.800 --> 16:01.800
Being careful about the data,

16:01.800 --> 16:03.800
trying to build something responsible

16:03.800 --> 16:06.800
and not just go fast and break it.

16:08.800 --> 16:12.800
There are definitely Anglo-American values in chat GPT.

16:12.800 --> 16:15.800
We wondered whether we could set up a project

16:15.800 --> 16:17.800
to analyze and document that.

16:17.800 --> 16:20.800
For example, with benchmarks that could show

16:20.800 --> 16:24.800
whether a model has Anglo-American, French or German values.

16:25.800 --> 16:29.800
It would be interesting to do a comparative study

16:29.800 --> 16:33.800
between chat GPT and bloom chat, wouldn't it?

16:33.800 --> 16:36.800
If you ask a question in different languages,

16:36.800 --> 16:40.800
how different are the answers depending on the kind of question?

16:40.800 --> 16:43.800
Is the approach more American or European?

16:43.800 --> 16:46.800
That would be an interesting study.

16:46.800 --> 16:49.800
That would be an interesting study.

16:49.800 --> 16:52.800
That would be an interesting study.

16:57.800 --> 17:00.800
When I talk about pluralism of values,

17:00.800 --> 17:04.800
I mean that every population in the world has its own value system.

17:04.800 --> 17:07.800
We have a lot of different nationalities here

17:07.800 --> 17:11.800
and we have to ask ourselves, what are our values?

17:11.800 --> 17:14.800
What's important to us?

17:15.800 --> 17:21.800
Optimistic sparks in Europe, like new startups.

17:21.800 --> 17:25.800
Germany, Aleph Alpha is already a big player.

17:25.800 --> 17:29.800
In the UK, stability is obviously a very visible player.

17:29.800 --> 17:32.800
Here in Europe, in France, there's Mistral,

17:32.800 --> 17:35.800
there's a new player in Finland.

17:35.800 --> 17:37.800
In almost every European country,

17:37.800 --> 17:42.800
I see at least one or two startups with this ambition

17:43.800 --> 17:46.800
to become and to build something big.

17:47.800 --> 17:51.800
As idealistic as Thomas and his team from Hugging Face appear,

17:51.800 --> 17:53.800
there is also criticism.

17:53.800 --> 17:58.800
Open source or not, the end result is that a small elite of tech professionals

17:58.800 --> 18:00.800
is determining what our future looks like

18:00.800 --> 18:03.800
and what risks we're exposed to.

18:06.800 --> 18:08.800
All the business people I meet say,

18:08.800 --> 18:12.800
we need education, society has to educate itself.

18:12.800 --> 18:14.800
We only create the systems,

18:14.800 --> 18:17.800
but you can design those systems in different ways.

18:17.800 --> 18:21.800
For example, you can make it so that a person can understand what's going on,

18:21.800 --> 18:22.800
at least a little.

18:22.800 --> 18:26.800
This could be one of the obligations we impose on the industry.

18:32.800 --> 18:35.800
These machines have processed all cultural knowledge

18:35.800 --> 18:39.800
and are created by mathematicians who don't know anything about culture.

18:39.800 --> 18:41.800
That's a bit of an exaggeration, of course,

18:41.800 --> 18:45.800
but we have to find ways of explaining this to people who aren't interested in the math.

18:45.800 --> 18:48.800
They just use the machines as tools.

18:48.800 --> 18:50.800
They need to understand where the limits are,

18:50.800 --> 18:54.800
in which situations the machine will or won't work.

18:54.800 --> 18:56.800
Just like with GPS devices,

18:56.800 --> 18:59.800
where we recognize when they give us the wrong route.

19:00.800 --> 19:04.800
Isn't it great that such a huge research field is opening up?

19:09.800 --> 19:11.800
But there's also a huge gulf opening up.

19:11.800 --> 19:13.800
Who's actually responsible in the end?

19:13.800 --> 19:15.800
Because as a developer or researcher,

19:15.800 --> 19:17.800
you have a certain responsibility.

19:17.800 --> 19:19.800
It's not about restricting research,

19:19.800 --> 19:23.800
but when there are applications that are harmful to society,

19:23.800 --> 19:25.800
we have to be aware of that.

19:25.800 --> 19:28.800
There is a huge potential for manipulation.

19:28.800 --> 19:32.800
Just think of the influence of chat GPT on elections.

19:32.800 --> 19:34.800
I think there needs to be an antidote.

19:34.800 --> 19:36.800
More education.

19:36.800 --> 19:38.800
That's a good topic for the upcoming elections.

19:38.800 --> 19:42.800
What education do we need to stop us being manipulated?

19:48.800 --> 19:50.800
That's a big question.

19:50.800 --> 19:54.800
What happens when people start asking AI who they should vote for?

19:54.800 --> 19:58.800
Because the AI will give them an answer, as it always does.

19:58.800 --> 20:00.800
Who decides how it answers?

20:00.800 --> 20:02.800
Who should decide that?

20:02.800 --> 20:04.800
Unfortunately, I have no answer to that.

20:14.800 --> 20:17.800
Generative AI is developing at breathtaking speed

20:17.800 --> 20:20.800
and tech giants are battling it out in the ring.

20:20.800 --> 20:23.800
That's spurring development even more.

20:23.800 --> 20:26.800
There's a massive new market up for grabs.

20:29.800 --> 20:32.800
Leading AI experts worry that big tech,

20:32.800 --> 20:34.800
in its eagerness to compete,

20:34.800 --> 20:38.800
is creating technologies beyond our control.

20:39.800 --> 20:41.800
I think we've made a mistake

20:41.800 --> 20:45.800
when my Swedish countryman Carl von Linnaeus branded our species

20:45.800 --> 20:48.800
as homo sapiens, because sapiens means the thinking homo,

20:48.800 --> 20:50.800
the smart one.

20:50.800 --> 20:53.800
We're not going to be the smartest anymore.

20:53.800 --> 20:56.800
Maybe we should re-brand ourselves the homo sentience,

20:56.800 --> 20:58.800
the feeling human.

20:58.800 --> 21:03.800
We can feel curiosity, meaning, purpose, love.

21:03.800 --> 21:06.800
That is what really makes us unique.

21:10.800 --> 21:13.800
We should ask, how can we keep control over the machine?

21:13.800 --> 21:16.800
So that we can use them as tools to build a world

21:16.800 --> 21:21.800
where we can really have human flourishing with positive experiences.

21:27.800 --> 21:30.800
In 2014, when I founded the Future Life Institute,

21:30.800 --> 21:36.800
it was quite taboo to even talk about AI safety at all,

21:36.800 --> 21:39.800
because that would imply that it wasn't totally safe.

21:39.800 --> 21:42.800
And a lot of AI researchers thought that it would be bad for funding,

21:42.800 --> 21:45.800
and that only weird people worried about this.

21:47.800 --> 21:49.800
It was very much like coming out of the closet moment

21:49.800 --> 21:51.800
for people to sign this letter and say,

21:51.800 --> 21:54.800
oh, you too are worried?

21:54.800 --> 21:56.800
I think we should slow down a little bit.

21:56.800 --> 21:57.800
Oh, I didn't know that.

21:57.800 --> 22:00.800
And then it suddenly became socially acceptable.

22:03.800 --> 22:06.800
Max Tegmark and his Future of Life Institute

22:06.800 --> 22:08.800
published an open letter,

22:08.800 --> 22:10.800
warning that artificial intelligence

22:10.800 --> 22:13.800
posed an existential danger to humanity.

22:13.800 --> 22:17.800
Civilisation itself could be under threat.

22:27.800 --> 22:30.800
The letter was signed by hundreds of AI researchers

22:30.800 --> 22:32.800
and tech industry leaders,

22:32.800 --> 22:36.800
including Tesla boss and ex-owner Elon Musk,

22:36.800 --> 22:39.800
Apple co-founder Steve Wozniak,

22:39.800 --> 22:42.800
and touring award winner Joshua Benio.

22:42.800 --> 22:46.800
And it's been quite shocking that once we put this letter out

22:46.800 --> 22:49.800
and kind of a who's who of AI researchers signed it,

22:49.800 --> 22:51.800
the conversation really exploded.

23:02.800 --> 23:05.800
My worst fears are that we cause significant...

23:05.800 --> 23:07.800
We, the field, the technology, the industry

23:07.800 --> 23:10.800
cause significant harm to the world.

23:12.800 --> 23:14.800
I think that could happen in a lot of different ways.

23:14.800 --> 23:16.800
It's why we started the company.

23:19.800 --> 23:21.800
I think if this technology goes wrong,

23:21.800 --> 23:23.800
it can go quite wrong.

23:25.800 --> 23:27.800
And we want to be vocal about that.

23:27.800 --> 23:29.800
We want to work with the government.

23:29.800 --> 23:31.800
I think he was serious about that.

23:31.800 --> 23:33.800
I think that's kind of...

23:33.800 --> 23:35.800
So we were talking about existential risks

23:35.800 --> 23:37.800
and I also believe there are existential risks.

23:37.800 --> 23:39.800
There are also a whole spectrum of other risks.

23:39.800 --> 23:42.800
And I know of some, I talked to him a couple of times about this.

23:42.800 --> 23:45.800
He very much recognises them as well.

23:45.800 --> 23:47.800
On the one hand, of course, these warnings

23:47.800 --> 23:50.800
about the major power of this new technology

23:50.800 --> 23:53.800
also amplify the significance of the products

23:53.800 --> 23:55.800
that these people are building.

23:55.800 --> 23:58.800
So it could also have an indirect marketing effect.

23:58.800 --> 24:01.800
Right? Like look at the incredible things that we're building.

24:01.800 --> 24:04.800
But also, let's make sure that nothing goes wrong.

24:04.800 --> 24:07.800
And for that, they look to the politicians.

24:07.800 --> 24:09.800
The net effect of that could be that

24:09.800 --> 24:12.800
if heaven forbid something goes wrong,

24:12.800 --> 24:14.800
they could say, well, we warned you,

24:14.800 --> 24:16.800
but the politicians did not act

24:16.800 --> 24:18.800
or they did not act in time.

24:18.800 --> 24:20.800
So I'm looking at a paper here

24:20.800 --> 24:22.800
entitled Large Language Models Trained on Media Diets

24:22.800 --> 24:24.800
Can Predict Public Opinion.

24:24.800 --> 24:26.800
This is just posted about a month ago.

24:26.800 --> 24:29.800
This work was done at MIT and then also at Google.

24:29.800 --> 24:32.800
The conclusion is that large language models

24:32.800 --> 24:35.800
can indeed predict public opinion.

24:35.800 --> 24:38.800
I want to think about this in the context of elections.

24:38.800 --> 24:42.800
Should we be concerned about large language models

24:42.800 --> 24:45.800
that can predict survey opinion

24:45.800 --> 24:48.800
and then can help organisations into these fine-tuned strategies

24:48.800 --> 24:50.800
to elicit behaviours from voters?

24:50.800 --> 24:52.800
Should we be worried about this for our elections?

24:52.800 --> 24:54.800
Yeah. Thank you, Senator Holly, for the question.

24:54.800 --> 24:57.800
It's one of my areas of greatest concern.

24:57.800 --> 24:59.800
The more general ability of these models

24:59.800 --> 25:01.800
to manipulate, to persuade,

25:01.800 --> 25:05.800
to provide sort of one-on-one interactive disinformation.

25:05.800 --> 25:07.800
I'm nervous about it.

25:07.800 --> 25:10.800
I think people are able to adapt quite quickly

25:10.800 --> 25:13.800
when Photoshop came onto the scene a long time ago.

25:13.800 --> 25:16.800
For a while, people were really quite fooled

25:16.800 --> 25:18.800
by Photoshopped images

25:18.800 --> 25:20.800
and then pretty quickly developed

25:20.800 --> 25:23.800
an understanding that images might be Photoshopped.

25:23.800 --> 25:26.800
This will be like that, but on steroids.

25:26.800 --> 25:29.800
And the interactivity,

25:29.800 --> 25:32.800
the ability to really model, predict humans well

25:32.800 --> 25:35.800
as you talked about, I think is going to require

25:35.800 --> 25:38.800
a combination of companies doing the right thing,

25:38.800 --> 25:40.800
regulation and public education.

25:53.800 --> 25:56.800
2024 is a crucial election year.

25:56.800 --> 25:58.800
Not only in the United States, but worldwide.

25:58.800 --> 26:01.800
There will be European Parliament elections.

26:01.800 --> 26:03.800
There will be elections in India.

26:03.800 --> 26:05.800
I mean, it's a large amount of people in the world

26:05.800 --> 26:07.800
will actually go to the polls.

26:07.800 --> 26:10.800
And while we're living in this big experiment

26:10.800 --> 26:13.800
where it's very hard for independent researchers,

26:13.800 --> 26:15.800
journalists, civil society organisations

26:15.800 --> 26:17.800
to probe these models,

26:17.800 --> 26:20.800
that we may only find out, you know,

26:20.800 --> 26:24.800
what the harms and malign uses

26:24.800 --> 26:28.800
as a weapon against democracy were when it is too late.

26:31.800 --> 26:34.800
Shortly after Sam Altman appeared before the US Senate,

26:34.800 --> 26:36.800
he co-signed a statement

26:36.800 --> 26:38.800
along with a number of high-ranking executives

26:38.800 --> 26:41.800
from Google, Microsoft and other tech companies.

26:51.800 --> 26:54.800
The fact that it was the companies

26:54.800 --> 26:58.800
who themselves were asking for this type of regulation

26:58.800 --> 27:00.800
and it was the leading researchers

27:00.800 --> 27:03.800
who were asking for the government to get involved,

27:03.800 --> 27:06.800
that really was the turning point in the conversation.

27:10.800 --> 27:13.800
To understand the effect that generative AI

27:13.800 --> 27:17.800
was having behind the scenes of global politics at the time,

27:17.800 --> 27:21.800
you have to travel north to a small Swedish city called Luleå,

27:21.800 --> 27:26.800
around 150 kilometres south of the Arctic Circle.

27:26.800 --> 27:28.800
When people say that artificial intelligence

27:28.800 --> 27:31.800
is going to be like the next industrial revolution,

27:31.800 --> 27:34.800
I think they're underestimating its impact.

27:34.800 --> 27:36.800
It's not just going to be a new technology

27:36.800 --> 27:38.800
like the steam engine.

27:38.800 --> 27:41.800
It's like building a new species.

27:41.800 --> 27:44.800
A species that's much smarter than us.

27:44.800 --> 27:47.800
President Biden himself was having meetings

27:47.800 --> 27:50.800
on artificial intelligence in some cases

27:50.800 --> 27:52.800
as often as three times per week.

27:52.800 --> 27:55.800
And I will tell you that not very many things

27:55.800 --> 27:59.800
get on the president's calendar for three times a week.

28:03.800 --> 28:06.800
May 31st, 2023.

28:06.800 --> 28:10.800
The sirens and motorcades descending on this Swedish coastal city

28:10.800 --> 28:13.800
gave a sense of how much was at stake.

28:13.800 --> 28:16.800
Leaders came here to discuss nothing less

28:16.800 --> 28:20.800
than how humanity should react to the arrival of this new,

28:20.800 --> 28:23.800
albeit artificial, form of intelligence.

28:23.800 --> 28:26.800
What role should politicians play?

28:26.800 --> 28:30.800
Democracy needs to show that we are as fast as technology.

28:30.800 --> 28:34.800
You saw the first letter on asking for a course of six months.

28:34.800 --> 28:38.800
You saw yesterday a number of very, very insightful people

28:38.800 --> 28:41.800
signing up to say you need to do something

28:41.800 --> 28:43.800
for the very existential risks.

28:43.800 --> 28:46.800
And then you have the non-existential risks as well.

28:46.800 --> 28:49.800
Why is it important for the European Union

28:49.800 --> 28:52.800
to have a common policy with the US concerning AI

28:52.800 --> 28:54.800
and shouldn't other parts of the globe

28:54.800 --> 28:56.800
be included in the conversation?

28:56.800 --> 28:58.800
Europe is important, but this is bigger than Europe.

28:58.800 --> 29:01.800
US is important, but it is bigger than the US.

29:01.800 --> 29:04.800
But if the two of us take the lead with close friends,

29:04.800 --> 29:06.800
I think we can push something

29:06.800 --> 29:08.800
that will make us all much more comfortable

29:08.800 --> 29:11.800
with the fact that generative AI is now in the world

29:11.800 --> 29:14.800
and is developing at amazing speeds.

29:31.800 --> 29:35.800
Jonas Androulos was also invited to the top-level meeting in Sweden

29:35.800 --> 29:38.800
to represent the views of European AI startups

29:38.800 --> 29:41.800
and call for fair competition.

29:44.800 --> 29:47.800
Of course there are other AI companies in Europe,

29:47.800 --> 29:49.800
but we're the one that's keeping pace the most

29:49.800 --> 29:51.800
with the global leaders.

29:51.800 --> 29:53.800
I assume that's the reason why we're here,

29:53.800 --> 29:56.800
not because we're so charming.

30:06.800 --> 30:09.800
What kind of change is coming in some industries?

30:09.800 --> 30:11.800
How do you feel about that?

30:11.800 --> 30:14.800
We can raise more capital.

30:14.800 --> 30:16.800
I think we have...

30:16.800 --> 30:18.800
Two weeks ago I was at SFI conference,

30:18.800 --> 30:20.800
SAP SFI conference,

30:20.800 --> 30:22.800
and Christian Klein on his opening keynote

30:22.800 --> 30:23.800
he kind of said,

30:23.800 --> 30:25.800
our key partners for generative AI

30:25.800 --> 30:28.800
are Aleph, Alfa, Google and Microsoft.

30:28.800 --> 30:31.800
And then I'll have events coming up

30:31.800 --> 30:33.800
with HPE, with Antonio Neri.

30:33.800 --> 30:36.800
What do you think about

30:36.800 --> 30:39.800
our colleagues on the other end here,

30:39.800 --> 30:42.800
from Anthropics and the latest...

30:45.800 --> 30:47.800
Statements, etc.

30:47.800 --> 30:49.800
Oh, so the statements on safety?

30:49.800 --> 30:51.800
Yeah, like yesterday and so on.

30:51.800 --> 30:56.800
Long-term it is possible to conceive catastrophic events.

30:56.800 --> 30:58.800
I've had Brussels and Berlin

30:58.800 --> 31:02.800
and they basically are scared.

31:04.800 --> 31:07.800
We will start with Jonas Androulis,

31:07.800 --> 31:10.800
who's the founder and CEO of Aleph, Alfa.

31:10.800 --> 31:11.800
The floor is yours,

31:11.800 --> 31:13.800
and thank you very much for being with us today.

31:13.800 --> 31:15.800
All right, thanks for having me.

31:15.800 --> 31:17.800
I think we're all a little bit dizzy.

31:17.800 --> 31:19.800
The speed of change,

31:19.800 --> 31:21.800
like everybody I know that is in AI

31:21.800 --> 31:22.800
is kind of stressed out,

31:22.800 --> 31:24.800
and with this technology

31:24.800 --> 31:27.800
we're only even just stretching the surface.

31:27.800 --> 31:29.800
I fear that knowledge work

31:29.800 --> 31:31.800
is in the hands of the world

31:31.800 --> 31:33.800
and I fear that knowledge work

31:33.800 --> 31:36.800
is an important part of what is happening in Europe.

31:36.800 --> 31:39.800
So this is an opportunity for us

31:39.800 --> 31:41.800
to build new empires,

31:41.800 --> 31:42.800
to build new value,

31:42.800 --> 31:44.800
but it's also a risk

31:44.800 --> 31:47.800
that we're losing a substantial pillar

31:47.800 --> 31:48.800
that we're standing on.

31:48.800 --> 31:51.800
Thinking about how we can

31:51.800 --> 31:54.800
make this a fair playing field,

31:54.800 --> 31:57.800
because I think it's in everybody's interest

31:57.800 --> 32:00.800
that Europe will contribute to a safer future in AI.

32:00.800 --> 32:02.800
Thank you very much.

32:10.800 --> 32:12.800
While the U.S. and the EU

32:12.800 --> 32:14.800
were trying to come up with a common strategy

32:14.800 --> 32:16.800
on the other side of the world in China,

32:16.800 --> 32:19.800
an artificial intelligence ecosystem

32:19.800 --> 32:22.800
was emerging with its own set of rules.

32:22.800 --> 32:25.800
AI is a key part of China's efforts

32:25.800 --> 32:28.800
to become a global power.

32:28.800 --> 32:31.800
I always remember my mom and my dad

32:31.800 --> 32:33.800
pushing me to this Olympic school

32:33.800 --> 32:36.800
in order to get specialized in mathematics

32:36.800 --> 32:37.800
and also English school.

32:37.800 --> 32:39.800
It's like extra work

32:39.800 --> 32:41.800
besides this regular schoolwork.

32:41.800 --> 32:43.800
So basically you have to take the lessons

32:43.800 --> 32:45.800
on Saturday, on weekend.

32:45.800 --> 32:48.800
It makes me a quick learner.

32:48.800 --> 32:50.800
And my mom is correct, right?

32:50.800 --> 32:53.800
So in order to keep progress,

32:53.800 --> 32:55.800
in order to keep pushing yourself,

32:55.800 --> 32:56.800
you have to keep learning.

32:56.800 --> 32:59.800
And I always tell my employees

32:59.800 --> 33:00.800
also to keep learning,

33:00.800 --> 33:03.800
to keep up this fast pace in AI.

33:05.800 --> 33:07.800
My father was a professor

33:07.800 --> 33:09.800
in computer science,

33:09.800 --> 33:11.800
so I am very lucky to get

33:11.800 --> 33:14.800
in touch with AI in the very early days.

33:14.800 --> 33:16.800
Back in 2009, I was trying to

33:16.800 --> 33:18.800
build some AI models,

33:18.800 --> 33:19.800
very simple AI models.

33:19.800 --> 33:21.800
Nowadays, if you look

33:21.800 --> 33:23.800
from today's large language model perspective,

33:23.800 --> 33:26.800
that model is like a very simple,

33:26.800 --> 33:28.800
simple like a small ant.

33:32.800 --> 33:35.800
Han Xiaoh has worked in both the East and the West.

33:35.800 --> 33:39.800
He's held positions at the Chinese tech giant Tencent

33:39.800 --> 33:42.800
and German online retailer Zalando.

33:42.800 --> 33:45.800
Three years ago, he founded his own company.

33:45.800 --> 33:47.800
Gina is an AI startup

33:47.800 --> 33:50.800
with offices in Shenzhen and Beijing.

33:50.800 --> 33:52.800
But its headquarters are in Berlin.

33:52.800 --> 33:54.800
Oh, where do we have another interview here?

33:54.800 --> 33:55.800
Why?

33:55.800 --> 33:57.800
Yes, for the website.

33:57.800 --> 33:58.800
For the website?

33:58.800 --> 34:00.800
We have the internship program with interns

34:00.800 --> 34:02.800
and now we want to add also like employees experience

34:02.800 --> 34:05.800
so people can see how it is to work at Gina.

34:05.800 --> 34:07.800
Not only from an intern perspective.

34:07.800 --> 34:08.800
Yes, I see, I see.

34:11.800 --> 34:14.800
So, Kalimia of course from India,

34:14.800 --> 34:19.800
Isabella from South Africa.

34:19.800 --> 34:21.800
Aladdin from Tunisia.

34:22.800 --> 34:25.800
Jack Mian from Malaysia.

34:25.800 --> 34:28.800
Michelle finally from Germany.

34:31.800 --> 34:33.800
I have a limited amount of wires.

34:33.800 --> 34:35.800
So I can only power one monitor.

34:35.800 --> 34:38.800
Sometimes I really need to show people the architecture

34:38.800 --> 34:40.800
and also show people the results of the model.

34:40.800 --> 34:43.800
So it's nice that I have this second screen

34:43.800 --> 34:45.800
where I can draw on it as well.

34:45.800 --> 34:47.800
It basically becomes touchscreen.

34:50.800 --> 34:53.800
So this is basically showing the progress

34:53.800 --> 34:55.800
of training the model.

34:55.800 --> 34:57.800
It's kind of like stock market, right?

34:57.800 --> 34:59.800
I see this model performs relatively good

34:59.800 --> 35:02.800
because you can see it's increasing over time.

35:02.800 --> 35:05.800
But sometimes it's not very successful.

35:05.800 --> 35:09.800
For example, this one, this model start very high

35:09.800 --> 35:11.800
but then the progress kind of stopped.

35:11.800 --> 35:14.800
That is wasting our time, it's wasting GPU resources,

35:14.800 --> 35:16.800
energy and so on, right?

35:16.800 --> 35:19.800
Han Shao and his team are working on optimizing

35:19.800 --> 35:22.800
AI models for specific applications.

35:22.800 --> 35:26.800
For example, linking text, video and images.

35:26.800 --> 35:29.800
Their goal is to make communication between humans

35:29.800 --> 35:32.800
and machines more intuitive and natural.

35:32.800 --> 35:34.800
A lot of people may recognize this.

35:34.800 --> 35:38.800
This guy, this is a kind of grand-par meme, right?

35:38.800 --> 35:41.800
So it's very popular on social media, right?

35:41.800 --> 35:44.800
So if you upload this picture to the algorithm,

35:44.800 --> 35:46.800
it will generate a story.

35:46.800 --> 35:51.800
You can generate comedy, erotic, fantasy, horror,

35:51.800 --> 35:53.800
all this kind of story.

35:53.800 --> 35:57.800
So we just keep it default and then we just do.

35:57.800 --> 35:59.800
It wasn't supposed to be like this.

35:59.800 --> 36:01.800
I was meant for more.

36:01.800 --> 36:05.800
He whispered to the room, his words echoing into silence.

36:05.800 --> 36:08.800
I am more than the lonely man I've become,

36:08.800 --> 36:11.800
more than these disappointments.

36:12.800 --> 36:16.800
Suddenly, his eyes glinted, a revelation forming within his mind.

36:16.800 --> 36:20.800
Perhaps, it is time I showed the world that again.

36:20.800 --> 36:24.800
With strengthened resolve, Arthur placed the coffee down,

36:24.800 --> 36:26.800
marking an end to his solitary reflection

36:26.800 --> 36:28.800
and the beginning of a new chapter.

36:28.800 --> 36:31.800
So basically, this is what you can do

36:31.800 --> 36:34.800
when you push multimodal AI into an extreme, right?

36:34.800 --> 36:36.800
So you can see from a single image,

36:36.800 --> 36:40.800
you are able to generate not only a text description,

36:40.800 --> 36:43.800
but an emotional audio story.

36:57.800 --> 37:01.800
Eventually, Chinese companies will be in the leading position

37:01.800 --> 37:03.800
in this generative AI.

37:03.800 --> 37:09.800
Two months ago, I was participating in this World AI Congress in Shanghai.

37:09.800 --> 37:13.800
And during that conference, there were 30 large language models

37:13.800 --> 37:16.800
released on one day.

37:16.800 --> 37:20.800
Some from big companies like Tencent, Alibaba, Baidu,

37:20.800 --> 37:25.800
some also from middle-sized companies from different industries.

37:25.800 --> 37:27.800
For example, from Bank.

37:27.800 --> 37:41.800
This Chinese company is usually very good at learning from U.S. companies.

37:41.800 --> 37:44.800
So they kind of copycat what U.S. companies are doing

37:44.800 --> 37:46.800
and then make it even better.

37:49.800 --> 37:54.800
I don't doubt that one day, you will see one of the top models

37:54.800 --> 37:58.800
in the benchmark in the leaderboard are actually from China.

38:02.800 --> 38:07.800
The question of which companies will dominate the age of artificial intelligence

38:07.800 --> 38:10.800
has real geopolitical consequences.

38:10.800 --> 38:15.800
China is using the expertise of its tech companies to expand its power.

38:15.800 --> 38:19.800
Western nations, meanwhile, are trying to counter this.

38:24.800 --> 38:26.800
I'm going to go back.

38:35.800 --> 38:37.800
My name is Jeffrey Kane.

38:37.800 --> 38:40.800
I was a long-time journalist and foreign correspondent in China.

38:40.800 --> 38:42.800
I wrote a book called The Perfect Police State,

38:42.800 --> 38:44.800
and I was an advisor to the U.S. Congress,

38:44.800 --> 38:49.800
to the House of Representatives on sanctions and Chinese politics.

38:49.800 --> 38:53.800
From what I have seen around the world in China and elsewhere,

38:53.800 --> 38:58.800
I am deeply concerned that we do not know how to manage AI yet.

38:58.800 --> 39:00.800
We do not know what's coming.

39:00.800 --> 39:03.800
We do not know how to rein in this technology

39:03.800 --> 39:07.800
and put it to the good use of our democracy.

39:10.800 --> 39:15.800
China has been leading in bringing technology under state control

39:15.800 --> 39:18.800
and, in fact, using it as an instrument for state power,

39:18.800 --> 39:24.800
whether it is for internal control and censorship, a grip on society,

39:24.800 --> 39:29.800
or whether it is their global ambition to have digital infrastructure around the world

39:29.800 --> 39:34.800
and to work with countries, for example, I think, about the African continent.

39:34.800 --> 39:39.800
It is, of course, a vision that is at direct odds with that of democratic societies.

39:39.800 --> 39:44.800
In 2017, China's national strategy for artificial intelligence,

39:44.800 --> 39:46.800
and this is a public document,

39:46.800 --> 39:51.800
set out the explicit goal of dominating global AI technology.

39:51.800 --> 39:55.800
And so I think the United States has explicitly set the goal

39:55.800 --> 40:03.800
that we are not going to assist China in rising as an AI-enabled authoritarian superpower.

40:06.800 --> 40:09.800
Ironically, in the past, it's been large U.S. companies

40:09.800 --> 40:12.800
that have undermined their government's policies

40:12.800 --> 40:16.800
in order to gain access to the massive Chinese market,

40:16.800 --> 40:19.800
foremost among them, Microsoft.

40:19.800 --> 40:25.800
Microsoft is the most pivotal and important western company operating in China

40:25.800 --> 40:30.800
that has helped the Chinese government develop its AI dystopia.

40:30.800 --> 40:34.800
Microsoft set up an office in China called Microsoft Research Asia.

40:34.800 --> 40:38.800
This was a gesture from Bill Gates back in the 1990s

40:38.800 --> 40:42.800
because he wanted to guarantee stronger market access to China.

40:42.800 --> 40:47.800
This laboratory has gone on to train the who's who list,

40:47.800 --> 40:52.800
the superstars of the Chinese artificial intelligence world.

40:52.800 --> 40:57.800
Many of the key people in this laboratory have gone on to found companies

40:57.800 --> 41:01.800
such as MakeV, SenseTime, or either found them

41:01.800 --> 41:04.800
or they've taken on very senior roles in them.

41:05.800 --> 41:13.800
That was like the incubator of the modern Chinese internet or AI industry.

41:13.800 --> 41:17.800
A lot of great people, great researchers, startup founders

41:17.800 --> 41:19.800
actually come from Microsoft Research.

41:19.800 --> 41:25.800
And those talents are now becoming kind of the very, very big influencers,

41:25.800 --> 41:29.800
opinion leaders, and really like entrepreneurs in China.

41:30.800 --> 41:33.800
Microsoft helped build China's tech elite.

41:33.800 --> 41:36.800
This in turn has been used by the Chinese government

41:36.800 --> 41:42.800
to create a gigantic surveillance state that operates with the help of AI.

41:44.800 --> 41:46.800
Like in China, in Beijing and Shenzhen,

41:46.800 --> 41:50.800
you can find the most CCTV camera in the world.

41:50.800 --> 41:56.800
And to be honest, like a general public get used to it.

41:57.800 --> 42:02.800
So they don't see this as intrusion to their own privacy

42:02.800 --> 42:06.800
or having a software that analyze their behavior,

42:06.800 --> 42:11.800
because the kind of the narrative there was to protect them,

42:11.800 --> 42:14.800
to make the society more secure,

42:14.800 --> 42:17.800
provide, protect from terrorists and so on.

42:17.800 --> 42:21.800
So in general, like the public over the last 10 years

42:21.800 --> 42:25.800
has already accept the fact that there are surveillance everywhere.

42:32.800 --> 42:36.800
And now, not only you have an option of listening to all the information

42:36.800 --> 42:38.800
that people exchange in society,

42:38.800 --> 42:42.800
now you also have the cognitive capacity to process all of this.

42:42.800 --> 42:44.800
So that's a scary, scary future.

42:44.800 --> 42:47.800
Unfortunately, it's definitely not an impossible one.

42:51.800 --> 42:56.800
Right now we have like over 500 city brands across the country.

42:56.800 --> 42:58.800
That means one city is just like Shanghai.

42:58.800 --> 43:01.800
They have a lot of big data analysis center.

43:01.800 --> 43:05.800
They're collecting all this data from different areas.

43:05.800 --> 43:09.800
And they have the machine, they have the algorithm,

43:09.800 --> 43:14.800
like centralized it and do the computation analysis

43:14.800 --> 43:16.800
and making all these decisions.

43:22.800 --> 43:26.800
The Chinese government has used all forms of AI so far.

43:26.800 --> 43:28.800
They see AI as an extremely powerful tool

43:28.800 --> 43:32.800
that they can use for the military, for national security,

43:32.800 --> 43:34.800
for state surveillance, police work,

43:34.800 --> 43:37.800
also the management of cities, traffic.

43:37.800 --> 43:40.800
They have been selling these same technologies all over the world,

43:40.800 --> 43:42.800
especially to authoritarian governments

43:42.800 --> 43:45.800
with the promise of total surveillance

43:45.800 --> 43:49.800
and a nation free of crime, free of dissident,

43:49.800 --> 43:51.800
it's a brave new world

43:51.800 --> 43:54.800
because we have not yet found a solution to this in the West.

44:01.800 --> 44:03.800
China is forging ahead.

44:03.800 --> 44:06.800
The US is pursuing its own interests.

44:06.800 --> 44:08.800
And the EU?

44:08.800 --> 44:11.800
It's striving for independence.

44:12.800 --> 44:16.800
If Europeans don't play a part in shaping this future technology,

44:16.800 --> 44:18.800
then it will be American or Chinese AI

44:18.800 --> 44:22.800
that will penetrate our lives to an unprecedented extent.

44:24.800 --> 44:28.800
It will know us as well as our closest friends and relatives.

44:28.800 --> 44:31.800
It will communicate with us around the clock

44:31.800 --> 44:34.800
and influence our thoughts and actions.

44:39.800 --> 44:41.800
To prevent this, the EU needs companies

44:41.800 --> 44:43.800
that can not only program

44:43.800 --> 44:46.800
but also build their own hardware infrastructure

44:46.800 --> 44:49.800
to keep highly sensitive data safe.

44:59.800 --> 45:02.800
The most precious commodity, the most important resource

45:02.800 --> 45:05.800
for the future of generative AI is GPU power.

45:05.800 --> 45:07.800
In other words, computing power.

45:07.800 --> 45:10.800
In the future, it will be as essential as electricity

45:10.800 --> 45:14.800
and water have been for developments that have taken place in the past.

45:18.800 --> 45:22.800
There's already a saying in Silicon Valley, the GPU poor,

45:22.800 --> 45:25.800
the people with fewer graphics cards.

45:31.800 --> 45:34.800
Training high-end AI language models

45:34.800 --> 45:37.800
requires thousands of high-performance graphics cards,

45:37.800 --> 45:40.800
which is also why supplies are scarce.

45:40.800 --> 45:42.800
That's another reason why many smaller players

45:42.800 --> 45:45.800
ally themselves with large tech companies.

45:50.800 --> 45:54.800
A lot of the deals in the field of generative AI in recent months

45:54.800 --> 45:56.800
have come at the cost of independence.

45:56.800 --> 45:59.800
Many companies have partnered with large corporations

45:59.800 --> 46:02.800
by accepting restrictions on things like hardware selection,

46:02.800 --> 46:05.800
cloud selection, integration.

46:05.800 --> 46:08.800
We absolutely didn't want to do that.

46:11.800 --> 46:14.800
Early on, Jonas Androulis recognized the value

46:14.800 --> 46:17.800
of having one's own hardware.

46:17.800 --> 46:20.800
He built a data center for his company in Germany.

46:20.800 --> 46:25.800
For that reason, Alif Alpha is becoming increasingly strategically important

46:25.800 --> 46:27.800
for politicians.

46:28.800 --> 46:32.800
The media talks about you as Germany's answer to chatGPT.

46:32.800 --> 46:34.800
Is that right?

46:35.800 --> 46:37.800
That's wrong.

46:37.800 --> 46:40.800
You could say Germany's answer to open AI,

46:40.800 --> 46:43.800
but chatGPT is a product aimed at the consumer.

46:43.800 --> 46:46.800
It's really intended to help school kids do their homework

46:46.800 --> 46:49.800
and to help them understand what's going on in the world.

46:49.800 --> 46:51.800
That's right.

46:51.800 --> 46:54.800
That's not our target group at all.

46:54.800 --> 46:58.800
We want to go where the most complex and critical processes are.

46:58.800 --> 47:01.800
For example, in the financial industry,

47:01.800 --> 47:04.800
in administration, in security, in healthcare,

47:04.800 --> 47:08.800
that's where we want to build systems that assist and support people.

47:08.800 --> 47:11.800
We're in a government ministry here.

47:11.800 --> 47:14.800
We're in a government ministry here.

47:14.800 --> 47:17.800
We're in a government ministry here.

47:17.800 --> 47:21.800
And public administration could benefit enormously from AI.

47:21.800 --> 47:23.800
We have an incredible number of processes

47:23.800 --> 47:26.800
that could be systematized and carried out.

47:26.800 --> 47:29.800
So the focus of my work here was a bit like asking

47:29.800 --> 47:33.800
how the public sector could act as a mainstay consumer,

47:33.800 --> 47:36.800
generating work, if you can put it like that,

47:36.800 --> 47:40.800
which would create demand for German and European AI technology.

47:40.800 --> 47:43.800
I mean, it's an AI company that targets the public sector,

47:43.800 --> 47:45.800
and we are the public sector.

47:45.800 --> 47:48.800
So we only have to see that we generate opportunities

47:48.800 --> 47:50.800
for these technologies to be tested,

47:50.800 --> 47:53.800
be it through customer experience, funding decisions, or even permits.

47:53.800 --> 47:57.800
We talk a lot about regulation and that kind of thing,

47:57.800 --> 48:00.800
but if we continue to be dependent on foreign countries

48:00.800 --> 48:03.800
and commercial enterprises for the future,

48:03.800 --> 48:06.800
we're going to be able to do that.

48:06.800 --> 48:09.800
And if we continue to be dependent on foreign countries

48:09.800 --> 48:12.800
and commercial enterprises for this essential technology,

48:12.800 --> 48:15.800
then in the future, things could potentially end up

48:15.800 --> 48:18.800
like they did with energy, like with gas recently,

48:18.800 --> 48:21.800
where we wanted to say certain things, but we couldn't,

48:21.800 --> 48:25.800
because otherwise it would have gotten cold and dark around here.

48:30.800 --> 48:34.800
The data center that so far ensured Aleph Alpha's independence

48:34.800 --> 48:37.800
is the US company Hewlett Packard Enterprise.

48:43.800 --> 48:46.800
Hewlett Packard Enterprise is one of the biggest players

48:46.800 --> 48:49.800
when it comes to setting up computer infrastructure.

48:49.800 --> 48:51.800
They build data centers.

48:51.800 --> 48:54.800
They set up internal server rooms.

48:54.800 --> 48:57.800
A lot of the high quality infrastructure

48:57.800 --> 49:00.800
in which the modern world runs comes from HPE.

49:05.800 --> 49:07.800
MUSIC

49:16.800 --> 49:20.800
Andrew Lewis secured a strategic partnership with HPE,

49:20.800 --> 49:22.800
giving him access to hardware

49:22.800 --> 49:25.800
without tying him to the company exclusively.

49:25.800 --> 49:29.800
He also hoped it would help him gain a foothold in the US market.

49:29.800 --> 49:32.800
He finalized the deal in Las Vegas

49:32.800 --> 49:35.800
with HPE CEO Antonio Neri.

49:53.800 --> 49:56.800
We're announcing a major joint project with HPE today.

49:56.800 --> 49:59.800
There will be a press release going out simultaneously.

49:59.800 --> 50:02.800
It'll be a big joint market venture.

50:02.800 --> 50:04.800
That's the important thing.

50:04.800 --> 50:06.800
Not that I'm going to go on a stage,

50:06.800 --> 50:09.800
but that we're now taking a joint step with a major partner.

50:09.800 --> 50:12.800
Am I nervous? Maybe a little.

50:12.800 --> 50:15.800
We've been working towards this for months.

50:15.800 --> 50:17.800
I'm sure it'll go well.

50:21.800 --> 50:25.800
What is the competitive advantage of Marvel, eventually?

50:25.800 --> 50:28.800
We're building our bond.

50:28.800 --> 50:30.800
We have an independent tax tax,

50:30.800 --> 50:33.800
so we're not relying on any external dependencies.

50:33.800 --> 50:36.800
We recently solved explainability in a new way

50:36.800 --> 50:39.800
so you can not only see positive, confirming sources,

50:39.800 --> 50:41.800
but also disagreeing sources.

50:41.800 --> 50:43.800
You share with me an example of that.

50:43.800 --> 50:46.800
Yes, exactly. From your own kind of speech.

50:46.800 --> 50:49.800
They are analysts here, so be careful.

50:51.800 --> 50:53.800
Americans move fast.

50:53.800 --> 50:55.800
They're willing to take risks,

50:55.800 --> 50:59.800
but, of course, this partnership also has to benefit HPE,

50:59.800 --> 51:03.800
and any partnership can come to an end at any time.

51:10.800 --> 51:13.800
Jonas Andrewles wanted to avoid becoming dependent

51:13.800 --> 51:15.800
on a large corporation,

51:15.800 --> 51:18.800
as with open AI and Microsoft.

51:18.800 --> 51:21.800
But that strategy brought with it a major risk.

51:21.800 --> 51:24.800
If his technology doesn't keep up with the competition,

51:24.800 --> 51:27.800
he'll be out of the race.

51:27.800 --> 51:29.800
We'll have more money soon.

51:29.800 --> 51:34.800
I'm hoping that yesterday helped a little bit with that.

51:34.800 --> 51:38.800
Yeah, certainly didn't hurt,

51:38.800 --> 51:41.800
and I've got some kind of immediate feedback

51:41.800 --> 51:46.800
after the show from investors on my cell phone.

51:46.800 --> 51:50.800
And, of course, I want to put this money to work.

51:51.800 --> 51:54.800
I think if we can get your help,

51:54.800 --> 51:58.800
if we could get your help to really say,

51:58.800 --> 52:00.800
okay, these are the application use cases,

52:00.800 --> 52:04.800
I think we can get that list from you,

52:04.800 --> 52:06.800
and then we can turn around and look at,

52:06.800 --> 52:08.800
okay, one, how do we package it?

52:08.800 --> 52:11.800
Two, can we use it internally?

52:11.800 --> 52:13.800
I think that would be great.

52:13.800 --> 52:14.800
And then, obviously, three,

52:14.800 --> 52:17.800
how do we make sure we line up the services offer?

52:17.800 --> 52:19.800
Thanks again for the partnership. Great to see you.

52:19.800 --> 52:21.800
Thanks a lot.

52:21.800 --> 52:24.800
I think what always attracted us to the relationship was,

52:24.800 --> 52:28.800
LFLFA's mission was to enable enterprise application use cases

52:28.800 --> 52:31.800
for LLMs and multimodal models,

52:31.800 --> 52:34.800
and most of the customers in the Valley

52:34.800 --> 52:36.800
or most of the companies in Silicon Valley

52:36.800 --> 52:38.800
were much more consumer-oriented.

52:38.800 --> 52:41.800
So this concept of a single-tenant LLM

52:41.800 --> 52:45.800
that can be trained with your data for your application

52:45.800 --> 52:48.800
really fits our core customer base.

52:50.800 --> 52:52.800
For all their friendliness,

52:52.800 --> 52:56.800
and Drulles knew the Americans wanted to see concrete results,

52:56.800 --> 52:59.800
he had to deliver and fast.

53:08.800 --> 53:11.800
I will show you anger.

53:14.800 --> 53:16.800
I have a smile for you.

53:17.800 --> 53:20.800
Are you happy with the trade fair so far?

53:27.800 --> 53:29.800
What's been most interesting?

53:41.800 --> 53:44.800
The AI race is also a competition for attention.

53:45.800 --> 53:47.800
It's about catching the eye of investors

53:47.800 --> 53:51.800
sitting on panels, being noticed, being quoted.

54:05.800 --> 54:09.800
We've already heard from a few masterminds on this topic today,

54:09.800 --> 54:11.800
and there are still a few more to come.

54:12.800 --> 54:14.800
With you, we'll be asking to what extent

54:14.800 --> 54:17.800
AI itself will drive innovation.

54:19.800 --> 54:23.800
This one's niche technology is now the subject of massive hype.

54:23.800 --> 54:26.800
Jonas S. Drulles is suddenly in the spotlight.

54:26.800 --> 54:30.800
His AI is being tested and evaluated,

54:30.800 --> 54:32.800
not always favourably.

54:32.800 --> 54:36.800
News Magazine did cite accused LFLFA of allowing its AI

54:36.800 --> 54:40.800
to be provoked into making racist and chauvinistic statements.

54:41.800 --> 54:44.800
And Drulles pointed out that its basic technology

54:44.800 --> 54:47.800
has deliberately not been restricted.

54:50.800 --> 54:53.800
That's just low-hanging fruit for journalists.

54:53.800 --> 54:55.800
I took a screenshot.

54:55.800 --> 54:57.800
The model used a bad word.

54:57.800 --> 55:00.800
Every time I fine-tune the model or tune the instructions,

55:00.800 --> 55:03.800
that diminishes it in certain areas.

55:04.800 --> 55:09.800
It loses capabilities in exchange for me making it more pleasant or safer,

55:09.800 --> 55:12.800
and those might be the exact capabilities that I need

55:12.800 --> 55:16.800
in an industrial context for automating processes.

55:16.800 --> 55:21.800
We want the embedding technology to become a well-known brand,

55:21.800 --> 55:23.800
just like the iPhone.

55:23.800 --> 55:25.800
That's the most important thing.

55:25.800 --> 55:28.800
Don't forget to subscribe to our channel

55:28.800 --> 55:30.800
for more videos like this.

55:46.800 --> 55:49.800
We want to think about whether it makes sense or not.

55:52.800 --> 55:55.800
We want to become a company like OpenAI,

55:55.800 --> 55:58.800
the top provider in the embedding world.

56:17.800 --> 56:23.800
What is the best way to push the team to focus on this product?

56:23.800 --> 56:28.800
We have teams with different cultural backgrounds,

56:28.800 --> 56:33.800
and sometimes it's very hard to organize everybody to concentrate on one thing.

56:33.800 --> 56:36.800
This is also because the AI is developing so fast,

56:36.800 --> 56:39.800
and a lot of hypes are here and there,

56:39.800 --> 56:42.800
and people want to try this out, try that out.

56:42.800 --> 56:46.800
So I just talk to my CEO and make sure that all the senior engineers,

56:46.800 --> 56:49.800
senior leaders are kind of on the same page.

56:52.800 --> 56:54.800
For me as a CEO,

56:54.800 --> 56:58.800
my primary job is actually killing the fun,

56:58.800 --> 57:02.800
killing the fun part by killing all this distraction

57:02.800 --> 57:05.800
to make sure that people concentrate on the single mission

57:05.800 --> 57:07.800
to make this company successful.

57:07.800 --> 57:10.800
We are kind of a developer-driven company,

57:10.800 --> 57:13.800
and most of our customers or users are actually developers,

57:13.800 --> 57:14.800
software engineers.

57:14.800 --> 57:18.800
So for example, right now there is a talentus sitting there,

57:18.800 --> 57:21.800
and their engineering team is also our customer.

57:21.800 --> 57:28.800
The biggest challenge is the competition in AI is just too intense.

57:28.800 --> 57:30.800
Investors are not stupid, like most of the investors,

57:30.800 --> 57:32.800
especially when it comes to later round,

57:32.800 --> 57:35.800
investors have a very strict evaluation about this company.

57:35.800 --> 57:38.800
So the companies that we are competing with,

57:38.800 --> 57:41.800
such as Hackingface from France,

57:41.800 --> 57:44.800
and Coher from the US,

57:44.800 --> 57:48.800
so those companies are not like those guys who previously worked at Google,

57:48.800 --> 57:51.800
or graduated from MIT, Stanford,

57:51.800 --> 57:53.800
so they are very smart people.

57:53.800 --> 57:56.800
Most of the investors will look at us as not as a small company,

57:56.800 --> 57:59.800
but they will evaluate us with more,

57:59.800 --> 58:02.800
not based on the hype, but based on the performance of the company.

58:02.800 --> 58:06.800
So that means we have to show two things,

58:06.800 --> 58:09.800
either the hyper growth of the user,

58:09.800 --> 58:12.800
so we need to grow the user base super fast,

58:12.800 --> 58:15.800
or we show them a solid revenue.

58:25.800 --> 58:27.800
Summer 2023.

58:27.800 --> 58:33.800
Thomas Wolff has managed to make time for a family vacation in Brittany, France.

58:33.800 --> 58:36.800
As Chief Scientific Officer, he's primarily responsible

58:36.800 --> 58:39.800
for research and development at Hackingface,

58:39.800 --> 58:43.800
a job that allows him to take a break from time to time.

58:46.800 --> 58:50.800
What are you up to today? Anything special?

58:50.800 --> 58:53.800
We're practicing ceiling with a trapeze.

58:53.800 --> 58:56.800
Didn't you do that yesterday?

58:56.800 --> 58:59.800
Yes, today we'll do something else.

59:04.800 --> 59:06.800
I'm here to help.

59:06.800 --> 59:11.800
Meanwhile, Thomas' business partner, Clemente Long, is in the spotlight.

59:11.800 --> 59:15.800
He's the CEO of Hackingface, the public face of the company.

59:17.800 --> 59:22.800
Tech industry heavyweights like Google, Amazon, NVIDIA and AMD

59:22.800 --> 59:27.800
have invested $235 million into Hackingface.

59:27.800 --> 59:32.800
The open development platform for AI models has become a billion-dollar business.

59:34.800 --> 59:36.800
It's not.

59:38.800 --> 59:40.800
The company gained even more prestige

59:40.800 --> 59:43.800
when Mark Zuckerberg's Meta used Hackingface

59:43.800 --> 59:47.800
to publish its high-end language model, Lama 2.

59:51.800 --> 59:55.800
It's a model that was recently published by Facebook, or Meta.

59:55.800 --> 59:59.800
It's similar to ChatGBT, an open-source competitor.

59:59.800 --> 01:00:01.800
The difference is that it's free.

01:00:01.800 --> 01:00:04.800
You can just install it on your computer.

01:00:04.800 --> 01:00:08.800
You don't have to access it through the ChatGBT interface or pay for it.

01:00:08.800 --> 01:00:10.800
It's like a set of Lego.

01:00:10.800 --> 01:00:13.800
Everything is open. Everything is freely accessible.

01:00:13.800 --> 01:00:15.800
You can also buy it pre-built.

01:00:15.800 --> 01:00:18.800
If someone builds an open-source model for you,

01:00:18.800 --> 01:00:20.800
it's like they're building Lego for you,

01:00:20.800 --> 01:00:23.800
a beautiful sports car made from Lego.

01:00:23.800 --> 01:00:24.800
And then it's yours.

01:00:24.800 --> 01:00:27.800
You can open up the hood and look inside.

01:00:28.800 --> 01:00:32.800
The greatest advantage of open-source, its free accessibility,

01:00:32.800 --> 01:00:35.800
is also its greatest weakness.

01:00:35.800 --> 01:00:38.800
What if a model was developed further by criminals,

01:00:38.800 --> 01:00:43.800
terrorists or other bad actors and used to cause harm?

01:00:43.800 --> 01:00:46.800
Any security mechanisms built into a language model

01:00:46.800 --> 01:00:48.800
can easily be removed.

01:00:53.800 --> 01:00:56.800
That's a big question we're asking ourselves.

01:00:57.800 --> 01:01:00.800
In the beginning, our aim was to make this technology

01:01:00.800 --> 01:01:03.800
as widely accessible as possible.

01:01:03.800 --> 01:01:05.800
We thought it would help lots of developers,

01:01:05.800 --> 01:01:07.800
but there are two sides to the technology.

01:01:07.800 --> 01:01:09.800
There are some people who can access it

01:01:09.800 --> 01:01:11.800
who really shouldn't be able to.

01:01:21.800 --> 01:01:25.800
There was a guy I met last year who had an AI.

01:01:25.800 --> 01:01:29.800
Designed to develop medicines, molecules that are good for your health.

01:01:29.800 --> 01:01:33.800
Just as an experiment, he put in a minus sign

01:01:33.800 --> 01:01:37.800
and trained it to look for molecules that were bad for your health.

01:01:37.800 --> 01:01:42.800
Within four hours, it discovered thousands of chemical weapons,

01:01:42.800 --> 01:01:47.800
including VX, the most powerful nerve gas

01:01:47.800 --> 01:01:51.800
that we here in the US have developed.

01:01:51.800 --> 01:01:54.800
Of course, you shouldn't open-source things like that.

01:01:54.800 --> 01:01:56.800
It's just crazy.

01:01:56.800 --> 01:01:58.800
I'm a scientist. I love open-source, right?

01:01:58.800 --> 01:02:01.800
And it's undeniable if you think about the pace of progress,

01:02:01.800 --> 01:02:04.800
how do you make sure that there is the most progress?

01:02:04.800 --> 01:02:06.800
Open-source is your friend.

01:02:06.800 --> 01:02:11.800
Having said that, I just cannot completely ignore all the dangers.

01:02:11.800 --> 01:02:14.800
And of course, the only argument I have seen so far

01:02:14.800 --> 01:02:17.800
from supporters of open-sourcing, everything is saying,

01:02:17.800 --> 01:02:19.800
well, we will figure it out.

01:02:20.800 --> 01:02:22.800
It's easy to comment from the sidelines

01:02:22.800 --> 01:02:24.800
to simply warn that it's all dangerous.

01:02:24.800 --> 01:02:27.800
It's more difficult to actively get involved,

01:02:27.800 --> 01:02:30.800
to try to create something positive, something good.

01:02:30.800 --> 01:02:32.800
It won't necessarily be successful.

01:02:32.800 --> 01:02:35.800
There'll be mistakes and then fresh attempts,

01:02:35.800 --> 01:02:37.800
but I'm going to try.

01:02:37.800 --> 01:02:40.800
It's risky, but we'll follow the path we think is right.

01:02:52.800 --> 01:03:04.800
MIT is one of the most renowned tech universities in the world.

01:03:04.800 --> 01:03:07.800
It has close ties to industry.

01:03:07.800 --> 01:03:11.800
The research carried out here has the potential to change the world.

01:03:11.800 --> 01:03:16.800
Needless to say, MIT is at the forefront of artificial intelligence.

01:03:16.800 --> 01:03:20.800
In addition to his work with the Future of Life Institute,

01:03:20.800 --> 01:03:22.800
Max Tegmark is a professor here.

01:03:22.800 --> 01:03:26.800
The topic of AI security is part of his day-to-day.

01:03:28.800 --> 01:03:32.800
So we want to ramp up the effort and pace at which we do things.

01:03:32.800 --> 01:03:36.800
And it's also very inspiring whenever I go to Silicon Valley

01:03:36.800 --> 01:03:38.800
and meet with various companies there,

01:03:38.800 --> 01:03:39.800
how quickly they do things,

01:03:39.800 --> 01:03:41.800
often compared to what we do in universities.

01:03:41.800 --> 01:03:43.800
So I thought it'd be fun to...

01:03:44.800 --> 01:03:45.800
Now we have a whole...

01:03:45.800 --> 01:03:48.800
We're lucky to have a whole bunch of talented people here.

01:03:48.800 --> 01:03:50.800
We can wrap up.

01:03:52.800 --> 01:03:55.800
Tegmark and his fellow campaigners want to keep a close eye

01:03:55.800 --> 01:03:58.800
on the tech startups from Silicon Valley,

01:03:58.800 --> 01:04:01.800
uncover risks and use scientific methodology

01:04:01.800 --> 01:04:04.800
to show people just how little time we have left

01:04:04.800 --> 01:04:07.800
to counteract the pull of the tech industry.

01:04:09.800 --> 01:04:11.800
So you've been working very hard on finishing our paper.

01:04:11.800 --> 01:04:14.800
We had a very, very long conversation about it yesterday.

01:04:14.800 --> 01:04:17.800
I thought the very last part of what we talked about

01:04:17.800 --> 01:04:19.800
might be kind of fun for the whole group.

01:04:19.800 --> 01:04:20.800
Yeah, I completely agree.

01:04:20.800 --> 01:04:22.800
Do you want to draw that table on the board

01:04:22.800 --> 01:04:26.800
and maybe they can contribute to good quotes for it?

01:04:26.800 --> 01:04:30.800
So we basically, as you know,

01:04:30.800 --> 01:04:33.800
modeled the conflict between the movement

01:04:33.800 --> 01:04:36.800
to replace human livelihoods

01:04:36.800 --> 01:04:38.800
and maybe replace humans period

01:04:38.800 --> 01:04:41.800
versus the movement to resist this

01:04:41.800 --> 01:04:44.800
and to preserve the status quo.

01:04:44.800 --> 01:04:45.800
So this doesn't just come...

01:04:45.800 --> 01:04:47.800
It's not just something Peter pulled out of a hat.

01:04:47.800 --> 01:04:49.800
It just actually comes from the math.

01:04:49.800 --> 01:04:51.800
So if you're naive, like,

01:04:51.800 --> 01:04:54.800
oh, we have AI that can do everything a human can do but better,

01:04:54.800 --> 01:04:56.800
my life will still be good.

01:04:56.800 --> 01:04:58.800
So we call that naivete.

01:04:58.800 --> 01:05:00.800
So if a lot of people believe this,

01:05:00.800 --> 01:05:03.800
then they will not invest personal sacrifices

01:05:03.800 --> 01:05:06.800
and personal costs to greater unite

01:05:06.800 --> 01:05:09.800
and for the movement to be in a better position

01:05:09.800 --> 01:05:10.800
to resist as a team.

01:05:10.800 --> 01:05:13.800
There's companies and open-source developers

01:05:13.800 --> 01:05:15.800
that are working day and night

01:05:15.800 --> 01:05:18.800
with the goal of taking people's income streams

01:05:18.800 --> 01:05:20.800
by creating AI models that are better

01:05:20.800 --> 01:05:23.800
than them at their job and their capabilities.

01:05:23.800 --> 01:05:26.800
So once you lose your income streams and your leverage,

01:05:26.800 --> 01:05:27.800
like, it's too late.

01:05:27.800 --> 01:05:30.800
Your options are more limited.

01:05:30.800 --> 01:05:33.800
The biggest danger is that we'll look back in 20 years

01:05:33.800 --> 01:05:35.800
and realize that we've automated everything

01:05:35.800 --> 01:05:38.800
because it was so easy and because it worked

01:05:38.800 --> 01:05:41.800
and the AI behaved correctly in 99% of cases

01:05:41.800 --> 01:05:44.800
and suddenly we no longer have control

01:05:44.800 --> 01:05:47.800
over something that's crucial for society.

01:05:59.800 --> 01:06:02.800
The process has already begun.

01:06:02.800 --> 01:06:06.800
Until now, it's been the intellectual and creative abilities

01:06:06.800 --> 01:06:09.800
of humans that have set us apart from other creatures

01:06:09.800 --> 01:06:11.800
and machines.

01:06:11.800 --> 01:06:15.800
But what if those qualities are now being taken away?

01:06:24.800 --> 01:06:27.800
My name is Dr. Inongo Lumumba-Casango,

01:06:27.800 --> 01:06:28.800
a.k.a. Samus.

01:06:28.800 --> 01:06:30.800
I'm a rapper, I'm a producer,

01:06:30.800 --> 01:06:32.800
and I'm an assistant professor

01:06:32.800 --> 01:06:34.800
at Brown University and the Music Department.

01:06:34.800 --> 01:06:38.800
Initially, I wasn't sort of tapped into all of the discussions

01:06:38.800 --> 01:06:40.800
that were happening around AI.

01:06:40.800 --> 01:06:42.800
Of course, peripherally, I was sort of listening,

01:06:42.800 --> 01:06:44.800
watching, reading,

01:06:44.800 --> 01:06:47.800
but I really started to tap into these conversations

01:06:47.800 --> 01:06:49.800
when I noticed what was happening

01:06:49.800 --> 01:06:51.800
at the intersection of hip-hop and AI,

01:06:51.800 --> 01:06:53.800
and that's when I realized, whoa, this thing is moving

01:06:53.800 --> 01:06:54.800
really quickly.

01:06:54.800 --> 01:06:56.800
I mean, last year, we were talking about

01:06:56.800 --> 01:06:58.800
a sort of AI-generated rapper,

01:06:58.800 --> 01:07:02.800
and this year, we're talking about rappers like Drake

01:07:02.800 --> 01:07:05.800
and artists like The Weeknd having their voices

01:07:05.800 --> 01:07:08.800
actually sort of cloned using AI technologies,

01:07:08.800 --> 01:07:11.800
and so the speed at which this has become

01:07:11.800 --> 01:07:14.800
sort of an immediate challenge for working artists

01:07:14.800 --> 01:07:16.800
is very alarming.

01:07:16.800 --> 01:07:19.800
Ultimately, it's the logic of capitalism,

01:07:19.800 --> 01:07:23.800
and as a human creator, what you can do is

01:07:23.800 --> 01:07:27.800
try not to be left behind.

01:07:27.800 --> 01:07:32.800
As a Chinese, we always feel that, like,

01:07:32.800 --> 01:07:35.800
technology, if you use it in a smarter way,

01:07:35.800 --> 01:07:38.800
it can, like, push you,

01:07:38.800 --> 01:07:41.800
uplift you yourself

01:07:41.800 --> 01:07:46.800
to become a smarter, greater creator.

01:07:46.800 --> 01:07:49.800
You know, machine and AI could do the job faster,

01:07:49.800 --> 01:07:52.800
cheaper, and they don't have strike,

01:07:52.800 --> 01:07:56.800
and, you know, they don't resist any, like,

01:07:56.800 --> 01:08:00.800
ridiculous demand from the clients or from the bosses,

01:08:00.800 --> 01:08:05.800
and I can see that Chinese companies are already,

01:08:05.800 --> 01:08:08.800
like, using it to replace human labors.

01:08:08.800 --> 01:08:11.800
So I think this is a very critical moment right now

01:08:11.800 --> 01:08:14.800
for the creators around the world.

01:08:14.800 --> 01:08:17.800
So this is something happening,

01:08:17.800 --> 01:08:21.800
and it's gonna be big in the next three to five years.

01:08:26.800 --> 01:08:29.800
So for folks like myself who, you know,

01:08:29.800 --> 01:08:31.800
I've been able to build a life for myself,

01:08:31.800 --> 01:08:34.800
but I would definitely not say that I'm in the,

01:08:34.800 --> 01:08:37.800
sort of, like, top tier of the music industry.

01:08:37.800 --> 01:08:40.800
There's a way that I think we're able to skirt under the radar

01:08:40.800 --> 01:08:42.800
and continue doing work as we're doing it,

01:08:42.800 --> 01:08:45.800
because it's so much about experimentation,

01:08:45.800 --> 01:08:48.800
it's so much about trying out weird things,

01:08:48.800 --> 01:08:51.800
and AI is so much about averaging.

01:08:51.800 --> 01:08:55.800
It's the people who are invested in playing

01:08:55.800 --> 01:08:58.800
in that space of the anomaly and playing with the unexpected,

01:08:58.800 --> 01:09:01.800
who will, sort of, continue to thrive.

01:09:11.800 --> 01:09:13.800
The impact of artificial intelligence

01:09:13.800 --> 01:09:17.800
on our society is far-reaching and complex.

01:09:17.800 --> 01:09:19.800
How can we regulate a technology

01:09:19.800 --> 01:09:21.800
that's developing so quickly

01:09:21.800 --> 01:09:25.800
and whose potential is almost impossible to gauge?

01:09:26.800 --> 01:09:28.800
The United States is struggling.

01:09:28.800 --> 01:09:32.800
Here in Washington, the tech industry's influence is huge,

01:09:32.800 --> 01:09:35.800
and governing majorities are fragile.

01:09:35.800 --> 01:09:39.800
The fact of the matter is that the U.S. government moves slowly.

01:09:39.800 --> 01:09:41.800
It is a democracy.

01:09:41.800 --> 01:09:44.800
That slowness is built into the system.

01:09:44.800 --> 01:09:46.800
The U.S. government is not supposed to be efficient

01:09:46.800 --> 01:09:49.800
and not supposed to be able to tackle problems quickly,

01:09:49.800 --> 01:09:52.800
because a government that is too efficient,

01:09:52.800 --> 01:09:55.800
you know, can use that against its own citizens, too.

01:09:55.800 --> 01:09:58.800
I think that we've now reached a state in our society

01:09:58.800 --> 01:10:01.800
that many philosophers and writers

01:10:01.800 --> 01:10:03.800
in the 20th century warned about,

01:10:03.800 --> 01:10:07.800
which is the inability to govern technology

01:10:07.800 --> 01:10:10.800
due to the increasing pace of change.

01:10:13.800 --> 01:10:17.800
Well, I certainly would not bet against democracies,

01:10:17.800 --> 01:10:20.800
but it will be a really tough adjustment period.

01:10:21.800 --> 01:10:23.800
The first major piece of legislation

01:10:23.800 --> 01:10:25.800
aimed at regulating artificial intelligence

01:10:25.800 --> 01:10:30.800
on a far-reaching scale came from the EU, the AI Act.

01:10:31.800 --> 01:10:34.800
I definitely kind of really admire European Union

01:10:34.800 --> 01:10:36.800
for being essentially the leader in this space.

01:10:36.800 --> 01:10:39.800
They took on this kind of regulation very seriously

01:10:39.800 --> 01:10:42.800
before anyone else really fought seriously all that.

01:10:51.800 --> 01:10:54.800
Jonas Androulas has come to Brussels.

01:10:56.800 --> 01:10:58.800
Together with other startup founders,

01:10:58.800 --> 01:11:00.800
he wants to let politicians know

01:11:00.800 --> 01:11:03.800
that strong regulation could put smaller European players

01:11:03.800 --> 01:11:06.800
at a disadvantage compared to the competition

01:11:06.800 --> 01:11:08.800
in the U.S. and China.

01:11:14.800 --> 01:11:17.800
Meetings like this are always a bit difficult,

01:11:17.800 --> 01:11:19.800
because you say your piece,

01:11:19.800 --> 01:11:22.800
and you never really know what reaction you're going to get.

01:11:22.800 --> 01:11:24.800
A few new people will listen,

01:11:24.800 --> 01:11:26.800
and of course it's clear that cooperation

01:11:26.800 --> 01:11:29.800
within Europe and with Europe is important,

01:11:29.800 --> 01:11:33.800
but it's always hard to say how much we can achieve here and now.

01:11:48.800 --> 01:11:49.800
No.

01:12:05.800 --> 01:12:06.800
Do you have documents?

01:12:06.800 --> 01:12:11.800
I could just talk. I've prepared something.

01:12:12.800 --> 01:12:16.800
It's your session. You can decide where you want to sit.

01:12:16.800 --> 01:12:34.800
So, good afternoon to all of you. I'm pleased to welcome you to the European Parliament to this

01:12:34.800 --> 01:12:42.800
meeting, an important meeting at the right time. Something that will happen and we already see

01:12:42.800 --> 01:12:48.800
basically a few steps down the road is like the cloud, like the hyperscalers have done

01:12:48.800 --> 01:12:54.800
with cloud compute, there will be an infrastructure for general intelligence, that all the value

01:12:54.800 --> 01:13:01.800
creation, all the apps, all the new innovations in the world will build upon. And for us, there

01:13:01.800 --> 01:13:07.800
will be no second chance. If we cannot move fast, then we won't be able to try again in 12 months.

01:13:07.800 --> 01:13:09.800
Thank you very much.

01:13:10.800 --> 01:13:13.800
APPLAUSE

01:13:28.800 --> 01:13:34.800
Androulos has repeated his message over and over again, whether on international stages,

01:13:34.800 --> 01:13:40.800
to German politicians or here at the European Parliament. Over the course of a year, networking

01:13:40.800 --> 01:13:43.800
and lobbying has become second nature to him.

01:13:48.800 --> 01:13:54.800
So, I started my career as an investment banker and management consultant, wearing a suit and

01:13:54.800 --> 01:13:56.800
38 degree weather with no air conditioning.

01:14:04.800 --> 01:14:07.800
MUSIC

01:14:17.800 --> 01:14:21.800
I don't think I'd make a good politician. I realize that in my days at Apple.

01:14:24.800 --> 01:14:30.800
What's the probability of success? Is it worth investing this time? Is it worth fighting this battle?

01:14:30.800 --> 01:14:35.800
I think so. I think it's a battle worth fighting, but I also have moments when I think that doing

01:14:35.800 --> 01:14:37.800
something else would be pretty nice.

01:14:46.800 --> 01:14:51.800
Shortly after his meeting with the European Parliament, it passed the AI Act.

01:14:51.800 --> 01:14:54.800
MUSIC

01:14:56.800 --> 01:15:00.800
Ten or even five years down the line, it is this governance structure that will give Europe the

01:15:00.800 --> 01:15:05.800
ability to deal with the rapid evolution of AI and to reap the most benefits from it.

01:15:05.800 --> 01:15:10.800
And we have worked first and foremost to ensure our citizens' rights and freedoms are not just

01:15:10.800 --> 01:15:16.800
respected, but protected and strengthened. We don't want mass surveillance. We don't want social

01:15:16.800 --> 01:15:19.800
scoring. We don't want predictive policing in the European Union. Full stop.

01:15:28.800 --> 01:15:31.800
My name is Dragosh Daraque. I'm a member of the European Parliament, representing the

01:15:31.800 --> 01:15:36.800
Renew Group, the Liberal Group in the European Parliament. I'm a judge by profession. I was

01:15:36.800 --> 01:15:42.800
also a member of government in Romania, Minister of Digitalization, Minister of Interior, prior

01:15:42.800 --> 01:15:43.800
to coming to Parliament.

01:15:46.800 --> 01:15:52.800
AI will play very much into the power balance. Why? Because it drives our economies, but not

01:15:52.800 --> 01:15:57.800
only. It becomes also a geopolitical factor, both in terms of how warfare is going to look

01:15:57.800 --> 01:16:03.800
like, but also how this technology will play into many of the processes that will keep

01:16:03.800 --> 01:16:09.800
one part of the world or the other competitive. And therefore, also the way you write the

01:16:09.800 --> 01:16:15.800
standards and how those standards become globally accepted standards is very important in that

01:16:15.800 --> 01:16:19.800
power balance that you mentioned earlier. So we're going to see very soon also, I think,

01:16:19.800 --> 01:16:24.800
a competition or possible clash in terms of global standards. And that is why we have

01:16:24.800 --> 01:16:32.800
to take measures to protect our interests and also to make sure that, again, our understanding

01:16:32.800 --> 01:16:37.800
of the role of technology is one that is shared by as many on the global stage as possible.

01:16:37.800 --> 01:16:42.800
In renegotiations, Germany, France and Italy lobbied again to soften the rules of the

01:16:42.800 --> 01:16:49.800
AI Act to protect domestic players like Alpha Alpha from heavy regulation. But in the end,

01:16:49.800 --> 01:16:52.800
the European Parliament prevailed.

01:17:02.800 --> 01:17:06.800
In terms of regulation, we're the economy that's leading the way. And there's a concern that

01:17:06.800 --> 01:17:14.800
that will take too much creativity out of the market. So in Europe, we're better at regulation

01:17:14.800 --> 01:17:18.800
than at putting technology on the market, unfortunately.

01:17:18.800 --> 01:17:24.800
The truth is, it's ultimately going to be good for the tech industry as well to be regulated,

01:17:24.800 --> 01:17:31.800
level playing field. Even seatbelts in cars were viciously opposed by the auto industry at first.

01:17:31.800 --> 01:17:38.800
But then when we got the law saying all cars have to have seatbelts, they started to sell much more cars.

01:18:02.800 --> 01:18:09.800
Han Shao has travelled to Shenzhen. In order to keep his team on the same page, the CEO has to visit

01:18:09.800 --> 01:18:12.800
the various company offices regularly.

01:18:12.800 --> 01:18:20.800
So you see that there's red letters on the building that is basically our office. But we are not that big.

01:18:20.800 --> 01:18:23.800
We are just one small room inside that big building.

01:18:24.800 --> 01:18:34.800
He wants to take his company Gina to the next level. That will require all his employees to pull together

01:18:34.800 --> 01:18:36.800
as much as possible.

01:18:41.800 --> 01:18:43.800
I'll just put this down.

01:18:47.800 --> 01:18:51.800
I brought some waffles. Try them. They're delicious.

01:18:54.800 --> 01:18:56.800
You have to eat them.

01:18:56.800 --> 01:18:58.800
You don't eat them.

01:19:00.800 --> 01:19:05.800
I told them you have to eat now and make a happy face to the camera.

01:19:14.800 --> 01:19:19.800
When I work at Germany, people are greeting each other like telling jokes,

01:19:19.800 --> 01:19:24.800
talking random stuff, football match yesterday, all these kind of things.

01:19:24.800 --> 01:19:30.800
Here is more introvert. The office is more introvert.

01:19:30.800 --> 01:19:33.800
It's just like different working cultures.

01:19:33.800 --> 01:19:37.800
Both of them are pretty productive under my weep.

01:19:37.800 --> 01:19:51.800
I always think that a sequence of small success will make the team stronger

01:19:51.800 --> 01:19:54.800
and make the team more confident on building things.

01:19:54.800 --> 01:19:57.800
Because larger success means larger hope.

01:19:57.800 --> 01:20:02.800
A larger hope could mean larger disappointment.

01:20:03.800 --> 01:20:08.800
Here in the start-up, everything moves very quickly.

01:20:08.800 --> 01:20:10.800
Then we become a little bit stressed.

01:20:10.800 --> 01:20:15.800
We become a little bit nervous because we could lose the advantages

01:20:15.800 --> 01:20:19.800
against the other competitors, against the market and so on.

01:20:19.800 --> 01:20:26.800
It's not about what we did. It's about how people perceive us on what we did.

01:20:27.800 --> 01:20:34.800
In Germany, there is also a team working on releasing a new large-language model.

01:20:34.800 --> 01:20:39.800
Yesterday, the leaders told me that this model can be ready on Monday,

01:20:39.800 --> 01:20:42.800
but it has been postponed for many times.

01:20:42.800 --> 01:20:45.800
I have to see how it goes.

01:20:49.800 --> 01:20:53.800
In the evening, Han Xiaoh has another meeting with a potential investor.

01:20:54.800 --> 01:20:58.800
On the way, he calls his technical director to ask whether the launch

01:20:58.800 --> 01:21:01.800
of the new language model is going as planned.

01:21:07.800 --> 01:21:11.800
Hey, Wan Nan, your embedding platform has to get into the global best model list.

01:21:11.800 --> 01:21:14.800
This company won't succeed unless everyone does their best.

01:21:19.800 --> 01:21:22.800
If you don't get into the top 10, it'll be much more difficult.

01:21:23.800 --> 01:21:26.800
Who uses a platform that's not in the top 10?

01:21:31.800 --> 01:21:34.800
You need to think more about these practical things.

01:21:34.800 --> 01:21:37.800
Is the LinkedIn post done? The Twitter post?

01:21:37.800 --> 01:21:39.800
There needs to be a strategy here.

01:21:42.800 --> 01:21:45.800
Okay, that's it. Bye.

01:21:46.800 --> 01:21:53.800
We want to get into the top 10 model, a leaderboard.

01:21:53.800 --> 01:21:59.800
Our models get into the top 10, but the German team just told me

01:21:59.800 --> 01:22:02.800
they probably cannot get into the top 10.

01:22:02.800 --> 01:22:10.800
That's why I get a little bit intense on my conversation,

01:22:10.800 --> 01:22:14.800
because I said this is something that we promised to ourselves.

01:22:14.800 --> 01:22:18.800
In this world, it's a very, very attention-based world.

01:22:18.800 --> 01:22:27.800
If you cannot get into the top 10, even if you get into number 11, nobody cares.

01:22:27.800 --> 01:22:33.800
This is why I'm telling the team that it's not about engineering only.

01:22:33.800 --> 01:22:38.800
You also have to think about the whole company, the marketing sales.

01:22:38.800 --> 01:22:44.800
It all depends on the top 10 models on this leaderboard.

01:23:09.800 --> 01:23:14.800
Hi, Grace.

01:23:24.800 --> 01:23:28.800
Grace Liu works for a Chinese investment bank.

01:23:28.800 --> 01:23:34.800
The two first met a couple of years ago during the start-up phase of Han Xiaos' company.

01:23:38.800 --> 01:23:43.800
You're just starting to bring AI-generated content to people, right?

01:23:45.800 --> 01:23:49.800
Multimodal AI. We're working on two things right now.

01:23:49.800 --> 01:23:55.800
One is prompt technology, and the other is embedding technology.

01:23:55.800 --> 01:24:00.800
This year will be quite a challenge for you.

01:24:00.800 --> 01:24:05.800
We've made a new software with prompt perfect aimed at developers.

01:24:05.800 --> 01:24:09.800
We've already got 200,000 registered users.

01:24:09.800 --> 01:24:13.800
Ah, that means there's a lot of demand.

01:24:13.800 --> 01:24:17.800
I don't think I can jump to my conclusion yet,

01:24:17.800 --> 01:24:23.800
but Han mentioned something very interesting to me about his new development and two new products.

01:24:23.800 --> 01:24:28.800
Actually, the most important thing is the CEO, him or herself, right?

01:24:28.800 --> 01:24:34.800
And whether he is a good entrepreneur, not only a scientist or a good developer.

01:24:34.800 --> 01:24:39.800
Meetings like this one put opportunities on the table for Han Xiaos' company,

01:24:39.800 --> 01:24:41.800
both in China and in the West.

01:24:41.800 --> 01:24:44.800
And there's good news about his important project.

01:24:44.800 --> 01:24:50.800
The new developer tool performs just as well as the equivalent technology from OpenAI.

01:24:50.800 --> 01:24:57.800
By the end of 2023, Jonas Androulas has plenty to celebrate.

01:24:57.800 --> 01:25:00.800
He's completed a major round of financing.

01:25:00.800 --> 01:25:06.800
The company prevailed and convinced enough investors to raise half a billion dollars.

01:25:06.800 --> 01:25:09.800
That's money Androulas is going to need,

01:25:09.800 --> 01:25:14.800
because competitor OpenAI is already triggering a new technology.

01:25:15.800 --> 01:25:20.800
We did a lot of things that smart people told me.

01:25:20.800 --> 01:25:23.800
Four years ago, they would be impossible.

01:25:23.800 --> 01:25:23.880
They signed the contract,

01:25:38.880 --> 01:25:40.880
But what about technologies?

01:25:40.880 --> 01:25:44.720
four years ago, they would be impossible.

01:25:44.720 --> 01:25:49.120
Build deep tech, AIR&D out of Germany, impossible.

01:25:49.120 --> 01:25:53.280
Fund this with mostly European capital, impossible.

01:25:53.280 --> 01:25:57.080
Build our own data center, impossible.

01:25:57.080 --> 01:26:01.180
Contribute category defining research, impossible.

01:26:01.180 --> 01:26:03.400
And now we are entering into a new era

01:26:03.400 --> 01:26:06.400
and I'm super happy to have you all with us.

01:26:06.400 --> 01:26:08.160
Yeah, and thanks for being here

01:26:08.160 --> 01:26:10.280
and help us make this the best party

01:26:10.280 --> 01:26:11.880
that Heidelberg has ever seen.

01:26:11.880 --> 01:26:12.720
Thanks.

01:26:21.720 --> 01:26:24.320
And for Thomas Wolff, quiet holidays

01:26:24.320 --> 01:26:26.480
may soon be a thing of the past.

01:26:26.480 --> 01:26:30.920
Hugging face is now valued at $4.5 billion.

01:26:30.920 --> 01:26:34.400
Thanks to successful startups and the AI Act,

01:26:34.400 --> 01:26:36.840
the EU at least has a seat at the table

01:26:36.840 --> 01:26:39.360
alongside the US and China.

01:26:39.360 --> 01:26:43.560
For now, for humanity at large, the question remains,

01:26:43.560 --> 01:26:45.920
what kind of world are we building right now

01:26:45.920 --> 01:26:49.480
for ourselves and for our children?

01:26:49.480 --> 01:26:52.160
I was holding my little baby Leo,

01:26:52.160 --> 01:26:53.960
you know, who just turned nine months old

01:26:55.400 --> 01:26:57.480
and looking into his eyes and thinking that, you know,

01:26:57.480 --> 01:27:01.160
right now his language abilities are much worse

01:27:01.160 --> 01:27:06.160
than chat's GPT-4 and he's never gonna catch up with AI ever.

01:27:07.040 --> 01:27:09.440
I have two kids that are in middle school

01:27:09.440 --> 01:27:10.720
and I'm thinking exactly about that,

01:27:10.720 --> 01:27:12.720
what I should teach them about,

01:27:12.720 --> 01:27:16.040
so they kind of prepared for the AI-infused future.

01:27:16.040 --> 01:27:19.480
How do we teach our kids to kind of build something

01:27:19.480 --> 01:27:21.960
like unique and individual?

01:27:21.960 --> 01:27:24.560
The machine is our something,

01:27:24.560 --> 01:27:28.000
it's our brothers or sisters, so we're working together.

01:27:28.000 --> 01:27:30.720
So I think that's how I feel.

01:27:31.320 --> 01:27:34.280
Even in these pivotal moments,

01:27:34.280 --> 01:27:37.960
in these complex times, people always find a way.

01:27:37.960 --> 01:27:40.160
They're creative and resourceful.

01:27:40.160 --> 01:27:42.800
My son is already learning to code.

01:27:42.800 --> 01:27:44.960
He's really interested in AI.

01:27:44.960 --> 01:27:48.400
He wants to understand things and create things using AI.

01:27:48.400 --> 01:27:50.440
Our children will probably create a world

01:27:50.440 --> 01:27:52.840
that's completely different from ours.

01:27:52.840 --> 01:27:55.600
We have a lot of people who are interested in AI,

01:27:55.600 --> 01:27:57.400
who are interested in AI,

01:27:57.400 --> 01:27:58.800
who will probably create a world

01:27:58.800 --> 01:28:01.120
that's completely different from ours.

01:28:01.120 --> 01:28:02.800
But I'm not worried.

01:28:02.800 --> 01:28:05.800
At the end of the day, I'm an optimist.

