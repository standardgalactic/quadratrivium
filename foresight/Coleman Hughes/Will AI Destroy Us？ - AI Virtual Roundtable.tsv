start	end	text
0	4080	Why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.
4080	6320	What's the big fear here? Make the case.
6320	10080	We don't understand the things that we build.
10080	13920	The AIs are grown more than built, you might say.
13920	18560	They end up as giant, inscrutable matrices of floating point numbers that nobody can decode.
18560	23040	At this rate, we end up with something that is smarter than us, smarter than humanity,
23760	28000	that we don't understand, whose preferences we could not shape.
28800	32720	And by default, if that happens, if you have something around that is much
32720	35760	smarter than you and does not care about you one way or the other,
35760	37520	you probably end up dead at the end of that.
37520	43440	Extinction is a pretty extreme outcome that I don't think is particularly likely,
43440	48240	but the possibility that these machines will cause mayhem because we don't know how to
49440	53200	enforce that they do what we want them to do. I think that's a real thing to worry about.
58800	61440	Welcome to another episode of Conversations with Coleman.
62000	67600	Today's episode is a roundtable discussion about AI safety with Eliezer Yudkowski,
67600	74000	Gary Marcus, and Scott Aronson. Eliezer Yudkowski is a prominent AI researcher and writer
74000	77920	known for co-founding the Machine Intelligence Research Institute,
77920	83680	where he spearheaded research on AI safety. He's also widely recognized for his influential
83680	89200	writings on the topic of rationality. Scott Aronson is a theoretical computer scientist
89200	93920	and author, celebrated for his pioneering work in the field of quantum computation.
94560	99680	He's also the chair of COMSI at U of T Austin, but is currently taking a leave of absence to
99680	106080	work at open AI. Gary Marcus is a cognitive scientist, author, and entrepreneur known for
106080	111760	his work at the intersection of psychology, linguistics, and AI. He's also authored several
111760	118880	books, including Cluj and Rebooting AI, Building AI We Can Trust. This episode is all about AI
118880	124480	safety. We talk about the alignment problem. We talk about the possibility of human extinction
124480	130240	due to AI. We talk about what intelligence actually is. We talk about the notion of a singularity
130240	135840	or an AI takeoff event, and much more. It was really great to get these three guys in the same
135840	140560	virtual room, and I think you'll find that this conversation brings something a bit fresh to a
140560	146000	topic that has admittedly been beaten to death on certain corners of the internet. Without further
146000	157360	ado, Eleazar Yudkowski, Gary Marcus, and Scott Aronson. Okay, Eleazar Yudkowski, Scott Aronson,
157360	164160	Gary Marcus. Thanks so much for coming on my show. Thank you. The topic of today's conversation is
164160	170400	AI safety, and this is something that's been in the news lately. We've seen experts and
170720	180880	CEOs signing letters, recommending public policy, surrounding regulation. We continue to have the
180880	188320	debate between people that really fear AI is going to end the world and potentially kill
188960	194560	all of humanity and the people who feel that those fears are overblown.
195360	202240	And so this is going to be sort of a roundtable conversation about that, and you three are
202880	207360	really three of the best people in the world to talk about it with. So thank you all for doing this.
208960	213840	Let's just start out with you, Eleazar, because you've been one of the most
215360	221040	really influential voices getting people to take seriously the possibility that AI will kill us all.
221920	227760	You know, why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.
227760	236160	What's the big fear here? Make the case. Well, chat GPT seems quite unlikely to kill everyone in
236160	242000	its present state. AI capabilities keep on advancing and advancing. The question is not,
242000	249360	can a chat GPT kill us? The answer is probably no. So as long as that's true, as long as it
249440	255440	hasn't killed us yet, the engineers are just going to keep pushing the capabilities. There's no
255440	264000	obvious blocking point. We don't understand the things that we build. The AIs are grown
264560	269520	more than built, you might say. They end up as giant inscrutable matrices of floating point
269520	276400	numbers that nobody can decode. It's probably going to end up technically difficult to make
276400	283840	them want particular things and not others. People are just charging straight ahead.
284800	289680	So at this rate, we end up with something that is smarter than us, smarter than humanity,
290400	297200	that we don't understand, whose preferences we could not shape. And by default, if that happens,
297200	301600	if you have something around that is much smarter than you and does not care about you one way or
301600	308400	the other, you probably end up dead at the end of that. The way it gets the most of whatever
308400	314640	strange inscrutable things that it wants are worlds in which there are not humans taking up
314640	320320	space, using up resources, building other AIs to compete with it, or just a world in which you
320320	324480	built enough power plants that the surface of the earth got hot enough that humans didn't survive.
325680	326880	Gary, what do you have to say about that?
327840	334560	There are parts that I agree with and parts that I don't. So I agree that we are likely to wind up
334560	340320	with AIs that are smarter than us. I don't think we're particularly close now, but in 10 years or
340320	346320	50 years or 100 years at some point, maybe a thousand years, but it will happen. I think there's a
346320	351680	lot of anthropomorphization there about machines wanting things. Of course, they have objective
351680	358720	functions and we can talk about that. I think it's a presumption to say that the default is that
358720	363520	they're going to want something that leads to our demise and that they're going to be effective at
363520	370720	that and be able to literally kill us all. I think if you look at the history of AI at least so far,
371280	376320	they don't really have wants beyond what we program them to do. There is an alignment problem. I
376320	381840	think that that's real in the sense of people who program a system to do X and they do X prime.
381840	386640	That's kind of like X, but not exactly. I think there's really things to worry about. I think
386640	393600	there's a real research program here that is under-researched, which is the way I would put it
393600	398480	is we want to understand how to make machines that have values. Asimov's laws are way too simple,
398480	404400	but they're kind of starting point for conversation. We want to program machines that don't harm humans.
404400	409280	They can calculate the consequences of their actions. Right now, we have technology like
409280	413840	GPT-4 that has no idea what the consequences of its actions are. It doesn't really anticipate things.
414720	418480	There's a separate thing that Eliezer didn't emphasize, which is it's not just how smart the
418480	423360	machines are, but how much power we give them. How much we empower them to do things like access
423360	432080	the internet or manipulate people or write source code, access files and stuff like that.
432080	436560	Right now, auto-GPT can do all of those things and that's actually pretty disconcerting to me.
436560	442960	To me, that doesn't all add up to any kind of extinction risk anytime soon, but catastrophic
442960	448240	risk where things go pretty wrong because we wanted these systems to do X and we didn't really
448240	452560	specify it well. They don't really understand our intentions. I think there are risks like that.
452560	457520	I don't see it as a default that we wind up with extinction. I think it's pretty hard to actually
457520	462080	terminate the entire human species. You're going to have people in Antarctica that are going to be
462080	466800	out of harm's way or whatever, or you're going to have some people who respond differently to any
466800	473840	pathogen, et cetera. Extinction is a pretty extreme outcome that I don't think is particularly
473840	479360	likely, but the possibility that these machines will cause mayhem because we don't know how to
480560	484560	enforce that they do what we want them to do. I think that's a real thing to worry about
484560	487440	and it's certainly worth doing research on. Scott, how do you view this?
488720	493280	Yeah, so I'm sure that you can get the three of us arguing about something, but I think you're
493280	500560	going to get agreement from all three of us that AI safety is important and that catastrophic outcomes,
501360	508960	whether or not that means literal human extinction are possible. I think it's become
509680	520640	apparent over the last few years that this century is going to be largely defined by our
520640	527280	interaction with AI, that AI is going to be transformative for human civilization.
531520	538080	I'm confident of that much. If you ask me almost anything beyond that about how is it going to
538080	546480	transform civilization? Will it be good? Will it be bad? What will the AI want? I am pretty agnostic
546480	553840	just because if you would ask me 20 years ago to try to forecast where we are now, I would have
553840	562560	gotten a lot wrong. My only defense is I think that all of us here and almost everyone in the
562560	569760	world would have gotten a lot wrong about where we are now. If I try to envision where we are in
570480	581680	2043, does the AI want to replace humanity with something better? Does it want to keep us around
581680	593600	as pets? Does it want to just continue helping us out like just a super souped up version of
593600	603040	chat GPT? I think all of those scenarios merit consideration, but I think that what has happened
603040	610240	in the last few years that's really exciting is that AI safety has become an empirical subject.
610880	617200	There are these very powerful AIs that are now being deployed and we can actually learn something.
618000	628160	We can work on mitigating the nearer-term harms, not because the existential risk doesn't exist or
628160	636160	is absurd or is science fiction or anything like that, but just because the nearer-term harms are
636160	641440	the ones that we can see right in front of us and where we can actually get feedback from the
641440	647280	external world about how we're doing. We can learn something and hopefully some of the knowledge
647280	653600	that we gain will be useful in addressing the longer-term risks that I think Eliezer is very
653600	659440	rightly worried about. Seems to me there's alignment and then there's alignment. There's
659440	665280	alignment in the sense that we haven't even fully aligned smartphone technology with our
665280	673600	interests. There are some ways in which smartphones and social media have led to probably deleterious
673600	681280	mental health outcomes, especially for teenage girls, for example. There are those kinds of
681280	687920	mundane senses of alignment where it's like, is this technology doing more good than harm
687920	693440	in the normal everyday public policy sense? Then there's the capital A alignment of,
694160	701360	are we creating a creature that is going to view us like ants and have no problem
703920	712240	extinguishing us and whether intentional or not? It seems to me all of you agree that
713920	719040	the first sense of alignment is at the very least something to worry about now and something to deal
719760	725040	with, but I'm curious to what extent you think the really capital A sense of alignment
725840	731360	is a real problem because it can sound very much like science fiction to people. Maybe let's start
731360	740560	with Eliezer. From my perspective, I would say that if we had a solid guarantee that AI was going
740560	747120	to do no more harm than social media, we ought to plow ahead and get all the gains. It's not
747120	752320	enough harm to back this amount of harm that social media has done to humanity, while very
752320	758880	significant in my view. I think it's done a lot of damage to our sanity, but that's just not a
758880	765120	large enough harm to justify either foregoing the gains that you could get from AI if that was going
765120	772640	to be the worst downside or to justify the kind of drastic measures you'd need to stop plowing ahead
772720	783280	on AI. I think that the capital A alignment is beyond this generation. I've started the field,
783280	793040	I've watched over it for two decades. I feel like in some ways the modern generation plowing in with
793040	798720	their eyes on the short term stuff is like losing track of the larger problems because they can't
798720	803520	solve the larger problems and they can't solve the little problems, but we're just like plowing
803520	808080	straight into the big problems and we're going to go plow right into the big problems with a bunch
808080	813520	of little solutions that aren't going to scale. I think it's lethal. I think it's at the scale
813520	819280	where you just back off and don't do this. By back off and don't do this, what do you mean?
820000	828960	I mean have an international treaty about where the chips capable of doing AI training go
829760	839920	and have them all going into licensed monitored data centers and not have the training runs for
839920	845680	AIs more powerful than GPT-4, possibly even lowering that threshold over time as algorithms
845680	850640	improve and it gets possible to train more powerful AIs using less computing.
850640	853360	So you're picturing a kind of international agreement to just stop?
854160	862480	International moratorium. And if North Korea steals the GPU shipment, then you've got to be ready to
864240	869760	destroy their data center that they build by conventional means. And if you don't have that
869760	873920	willingness in advance, then countries may refuse to sign up for the agreement being like,
873920	879360	why aren't we just seeding the advantage to someone else then? It actually has to be a
879360	885200	worldwide shutdown because the scale of harm from a superintelligence, it's not that if you
885200	890400	have 10 times as many superintelligence as you've got, 10 times as much harm. It's not that a
890400	896000	superintelligence only wrecks the country that built the superintelligence. Any superintelligence
896000	901920	everywhere is anyone's last problem. So Gary and Scott, if either of you want to jump in there,
902640	909280	is AI safety a matter of forestalling the end of the world and all of these
909920	913920	smaller issues and pass towards safety that Scott, you mentioned, are just
915680	922080	throwing, I don't know what the analogy is, but pointless essentially. What do you guys make of this?
924560	931840	I mean, the journey of 1,000 miles begins with a step. Most of the way I think about
932000	939760	this comes from 25 years of doing computer science research, including quantum computing
939760	946560	and computational complexity, things like that, where we have these gigantic aspirational problems
946560	953920	that we don't know how to solve. And yet, year after year, we do make progress. We pick off
953920	959920	little sub-problems. And if we can't solve those, then we find sub-problems of those. And we keep
960000	966160	repeating until we find something that we can solve. And this is, I think, for centuries,
966160	972720	the way that science has made progress. Now, it is possible that this time, we just don't have
972720	979440	enough time for that to work. And I think that is what Eliezer is fearful of, that we just
979440	986720	don't have enough time for the ordinary scientific process to work before AI becomes too powerful,
987680	996400	in which case you start talking about things like a global moratorium enforced with the threat of
996400	1005840	war. I am not ready to go there. I could imagine circumstances where maybe I say, gosh, this looks
1005840	1015200	like such an imminent threat that we have to. But I tend to be very, very worried in general about
1016160	1023040	causing a catastrophe in the course of trying to prevent a catastrophe. And I think when you are
1023040	1029840	talking about threatening airstrikes against data centers or things like that, then that is an
1029840	1036080	obvious worry. So I am somewhat in between here. I am with Scott that we are not at the point where
1036080	1040240	we should be bombing data centers. I don't think we are close to that. I am much less
1041200	1048160	aware of what the right word is to use here. I don't think we are anywhere near as close to AGI as
1048160	1054640	I think Eliezer sometimes sounds like. I don't think GPT-5 is anything like AGI. And I am not
1054640	1059280	particularly concerned about who gets it first and so forth. On the other hand, I think that
1059840	1065920	we are in a sort of dress rehearsal mode. Nobody expected GPT-4, really chat GPT,
1065920	1070960	to percolate as fast as it did. And it is a reminder that there is a social side to all of this,
1071520	1074960	how software gets distributed matters, and there is a corporate side.
1077360	1082800	It was a kind of galvanizing moment for me when Microsoft didn't pull Sydney, even though Sydney
1082800	1086320	did some awfully strange things. I thought they would take it for a while and it is a reminder
1086320	1091440	that they can make whatever decisions they want. So you kind of multiply that by Eliezer's concerns
1091440	1099920	about what do we do and at what point. What would be enough to cause problems is a reminder,
1099920	1104080	I think, that we need, for example, to start roughing out these international treaties now,
1104080	1107520	because there could become a moment where there is a problem. I don't think the problem that
1107520	1114560	Eliezer sees is here now, but maybe it will be. And maybe when it does come, we will have so many
1114560	1118800	people pursuing commercial self-interest and so little infrastructure in place we won't be able
1118800	1124240	to do anything. So I think it really is important to think now, if we reach such a point, what are
1124240	1128560	we going to do? What do we need to build in place before we get to that point?
1130160	1134800	So we've been talking about this concept of artificial general intelligence.
1135680	1143600	And I think it's worth asking whether that is a useful coherent concept. So for example,
1143920	1149920	if I were to think by analogy to athleticism and think of the moment when we build a machine
1150480	1158080	that has, say, artificial general athleticism, meaning it's better than LeBron James at basketball,
1158080	1164880	but also better at curling than the world's best curling player and also better at soccer
1164880	1171040	and also better at archery and so forth, it would seem to me that
1171360	1178800	there's something a bit strange as framing it as having reached a point on a single continuum.
1178800	1186400	It seems to me you would sort of have to build each capability, each sport individually and then
1186400	1194800	somehow figure how to package them all into one robot without each skill set detracting from the
1195680	1204880	other. Is that a disanalogy? Do you all picture this intelligence as sort of one dimension,
1204880	1213760	one knob that is going to get turned up along a single axis? Or do you think that way of talking
1213760	1218560	about it is misleading in the same way that I kind of just sketched out?
1219120	1225120	Yeah, I would absolutely not accept that. I like to say that intelligence is not a one-dimensional
1225120	1232000	variable. There's many different aspects to intelligence and there's not, I think, going
1232000	1237520	to be a magical moment when we reach the singularity or something like that. I would say that the core
1237520	1243600	of artificial general intelligence is the ability to flexibly deal with new problems that you haven't
1243600	1249680	seen before. And the current systems can do that a little bit, but not very well. My typical example
1249680	1255360	of this now is GPT-4 is exposed to the game of chess, sees the lots of games of chess that
1255360	1259760	sees the rules of chess, but it never actually figures out the rules of chess and makes illegal
1259760	1264880	moves and so forth. So it's in no way a general intelligence that can just pick up new things.
1264880	1270400	Of course, we have things like AlphaGo that can play a certain set of games, AlphaZero really,
1270400	1275680	but we don't have anything that has the generality of human intelligence. But human
1275680	1280240	intelligence is just one example of general intelligence. You could argue that chimpanzees
1280240	1285280	or crows have another variety of general intelligence. I would say the current machines don't really
1285280	1293840	have it, but they will eventually. I mean, I think a priori, it could have been that
1294800	1299680	you would have math ability, you would have verbal ability, you'd have
1300880	1305520	ability to understand humor, and they'd all be just completely unrelated to each other.
1306240	1314080	That is possible. And in fact, already with GPT, you can say that in some ways,
1314080	1320400	it already is a super intelligence. It knows vastly more, can converse on a vastly
1320400	1327920	greater range of subjects than any human can. And in other ways, it seems to fall short of
1329440	1338720	what humans know or can do. But you also see this sort of generality, just empirically.
1339440	1346240	I mean, GPT was sort of trained on all the text on the internet,
1348880	1355680	let's say most of the text on the open internet. So it was just one method. It was not
1356640	1363520	explicitly designed to write code, and yet it can write code. And at the same time as that
1363600	1370960	ability emerged, you also saw the ability to solve word problems like high school level math.
1371760	1378960	You saw the ability to write poetry. This all came out of the same system without any of it
1378960	1384880	being explicitly optimized for. I feel like I need to interject one important thing,
1384880	1388320	which is it can do all these things, but none of them all that reliably.
1388320	1395600	Well, okay. Nevertheless, compared to, let's say, what my expectations would have been,
1395600	1399920	if you'd asked me 10 or 20 years ago, I think that the level of generality
1399920	1407520	is pretty remarkable. And it does lend support to the idea that there is some sort of general
1407520	1413680	quality of understanding there, where you could say, for example, that GPT-4 has more of it than
1413680	1423520	GPT-3, which in turn has more than GPT-2. And I would say that it does seem to me like it's
1423520	1432240	presently pretty unambiguous that GPT-4 is in some sense dumber than an adult or even teenage human.
1433200	1447760	I mean, to take the example I just gave you a minute ago, it never learns to play chess,
1447760	1454880	even with a huge amount of data. So it will play a little bit of chess, it will memorize the openings
1454880	1459280	and be okay for the first 15 moves, but it gets far enough away from what it's trained on and
1459280	1464000	it falls apart. This is characteristic of these systems. It's not really characteristic in the
1464000	1470720	same way of adults or even teenage humans. Almost anything that it does, it does unreliably.
1470720	1476000	And give another example, you can ask a human to write a biography of someone and don't make
1476000	1485600	stuff up, and you really can't ask GPT to do that. Yeah, like it's a bit difficult because you could
1485600	1489840	always be cherry picking something that humans aren't usually good at. But to me, it does seem
1489840	1495760	like there's this broad range of problems that don't seem especially to play to humans' strong
1495760	1504080	points or machine weak points, where GPT-4 will do no better than a seven-year-old on those problems.
1507760	1513520	I do feel like these examples are cherry picked because if I just take a different,
1513520	1518480	very typical example, I'm writing an op-ed for the New York Times, say, about any given
1518480	1525040	subject in the world, and my choice is to have a smart 14-year-old next to me with anything that's
1525040	1534000	in his mind already or GPT. There's no comparison. So which of these sort of examples is the litmus
1534000	1543280	test for who is more intelligent? If you did it on a topic where it couldn't rely on memorized
1543360	1549520	text, you might actually change your mind on that. So the thing about writing a Times op-ed
1549520	1554000	is most of the things that you propose to it, there's actually something that it can
1554000	1558720	pastiche together from its dataset. That doesn't mean that it really understands what's going on.
1558720	1564080	It doesn't mean that that's general capability. Also, as the human, you're doing all the hard
1564080	1570240	parts. Obviously, a human is going to prefer, if a human has a math problem, we're going to rather
1570320	1575520	use a calculator than another human. Similarly, with the New York Times op-ed, you're doing all the
1575520	1582800	parts that are hard for GPT-4, and then you're asking GPT-4 to just do some of the parts that
1582800	1588720	are hard for you. You're always going to prefer an AI partner rather than a human partner within
1588720	1593520	that range of the human can do all the human stuff, and you want an AI to do whatever the AI
1593520	1599040	is good at at the moment. An analogy that's maybe a little bit helpful here is driverless cars.
1599760	1604960	It turns out that on highways and ordinary traffic, they're probably better than people,
1604960	1609920	and on unusual circumstances, they're really worse than people. A Tesla not too long ago
1609920	1614800	ran into a jet, and a human wouldn't do that, like slow speed being summoned across a parking
1614800	1619920	lot. A human would never do that. There are different strengths and weaknesses. The strengths
1619920	1624720	of a lot of the current kinds of technology is that they can either pastiche together or
1625520	1631200	make not literal analogies when we go into the details, but to stored examples, and they tend
1631200	1637600	to be poor when you get to outlier cases. That's persistent across most of the technologies
1638240	1643520	that we use right now. If you stick to stuff in which there's a lot of data, you'll be happy
1643520	1647760	with the results you get from these systems. You move far enough away, not so much.
1649200	1654160	What we're going to see over time is that the length of the debate about whether or not it's
1654160	1660880	still dumber than you gets longer and longer and longer. Then if things are allowed to just keep
1660880	1667280	running and nobody dies, then at some point it switches over to a very long debate about
1667280	1672720	is it smarter than you, which then gets shorter and shorter and shorter, and eventually reaches
1672720	1678080	a point where it's pretty unambiguous if you're paying attention. Now, I suspect that
1679120	1683920	this process gets interrupted by everybody dying. In particular, there's a question of
1683920	1690640	the point at which it becomes better than humanity at building the next edition of the AI system
1690640	1695280	and how fast do things snowball once you get to that point? Possibly, you do not have time for
1695840	1701760	further public debates or even a two-hour Twitter space, depending on how that goes.
1702960	1710320	Some of the limitations of GPT are completely understandable, just from a little knowledge
1710400	1718800	of how it works. It does not have an internal memory per se other than what appears on the
1718800	1725200	screen in front of you. This is why it's turned out to be so effective to explicitly tell it.
1725840	1731600	Let's think step by step when it's solving a math problem, for example. You have to tell it to
1731600	1737840	show all of its work because it doesn't have an internal memory with which to do that. Likewise,
1737840	1744080	when people complain about it, hallucinating references that don't exist, well, the truth is
1744080	1750240	when someone asks me for a citation, if I'm not allowed to use Google, I might have a vague
1750240	1756320	recollection of some of the authors, and I'll probably do a very similar thing to what GPT
1756320	1761920	does. I'll hallucinate. There's a great phrase I learned the other day, which is frequently wrong,
1762000	1769280	never in doubt. That's true. I'm not going to make up a reference with full detail, page numbers,
1769280	1775520	titles, so forth. I might say, look, I don't remember 2012 or something like that. Whereas,
1775520	1786160	GPT-4, what it's going to say is 2017, Aaronson and Yodkowski, New York Times, pages 13 to 17.
1786160	1791200	No, it does need to get much, much better at knowing what it doesn't know. And yet, already,
1791200	1798880	I've seen a noticeable improvement there, going from GPT-3 to GPT-4. For example,
1798880	1804720	if you ask GPT-3, prove that there are only finitely many prime numbers, it will give you
1804720	1810800	a proof, even though the statement is false. And it will have an error, which is similar to the
1810800	1817280	errors on 1,000 exams that I've graded, just trying to get something past you, hoping that
1817280	1822800	you won't notice. Hey, if you ask GPT-4, prove that there are only finitely many prime numbers,
1822800	1827360	it says, no, that's a trick question. Actually, there are infinitely many primes. And here's why.
1827360	1832720	Yeah. Part of the problem with doing the science here is that I think you would know better since
1832720	1838480	you work part-time or whatever to open AI. But my sense is that a lot of the examples that get
1838480	1844160	posted on Twitter, particularly by the likes of me and other critics or other skeptics, I should
1844160	1850320	say, is the system gets trained on those. So almost everything that people write about it,
1850320	1854960	I think, is in the training set. So it's hard to do the science when the system's constantly being
1854960	1860560	trained, especially in the RLHF side of things. And we don't actually know what's in GPT-4. So
1860560	1865920	we don't even know if they're regular expressions and simple rules, match things. So we can't do
1865920	1872080	the kind of science we used to be able to do. This conversation, this subtree of the conversation,
1872080	1880240	I think, has no natural endpoint. So if I can sort of zoom out a bit, I think there's a pretty
1880240	1886560	solid sense in which humans are more generally intelligent than chimpanzees. As you get closer
1886560	1895040	and closer to the human level, I would say that the direction here is still clear, that the comparison
1895040	1901040	is still clear. We are still smarter than GPT-4. This is not going to take control of the world
1901040	1909840	from us. But the conversations get longer. The definitions start to break down around the edges.
1910880	1915440	But I think it also, as you keep going, it comes back together again. There's a point,
1916080	1920320	and possibly this point is very close to the point of time to where everybody dies. So maybe we
1920320	1926400	don't ever see it in a podcast, but there's a point where it's unambiguously smarter than you.
1927280	1935440	And including the spark of creativity, being able to deduce things quickly rather than with tons
1935440	1942480	and tons of extra evidence, strategy, cunning, modeling people, figuring out how to manipulate
1942480	1949360	people. So let's stipulate, Aliezer, that we're going to get to machines that can do all of that.
1949360	1954800	And then the question is, what are they going to do? Is it a certainty that they will make
1954800	1960480	our annihilation part of their business? Is it a possibility? Is it an unlikely possibility?
1960480	1964400	I think your view is that it's a certainty. I've never really understood that part.
1965360	1969920	It's a certainty on the present tech, is the way I would put it. If that happened,
1970640	1977120	so in particular, if that happened tomorrow, then Modulo, Cromwell's rule, never say certain.
1978240	1983680	My probability is, yes, Modulo, the chance that my model is somehow just completely mistaken.
1985120	1994560	If we got 50 years to work it out and unlimited retries, I think that'd be pretty okay. I think
1994560	2000160	we'd make it. The problem is that it's a lot harder to do science when your first wrong try
2000160	2006480	destroys the human species and then you don't get to try again. I mean, I think there's something,
2006480	2011120	again, that I agree with and something I'm a little bit skeptical about. So I agree that
2011120	2018000	the amount of time we have matters. I would also agree that there's no existing technology that
2018800	2024080	solves the alignment problem that gives a moral basis to these machines. GPT-4 is fundamentally
2024080	2030400	amoral. I don't think it's immoral. It's not out to get us, but it really is amoral. It can answer
2030400	2034720	trolley problems because there are trolley problems in the dataset, but that doesn't mean that it
2034720	2041040	really has a moral understanding of the world. If we get to a very smart machine that by
2041040	2045760	all the criteria that we've talked about and it's amoral, then that's a problem for us.
2045760	2051840	There's a question of whether if we can get to smart machines, whether we can build them in a
2051840	2058240	way that will have some moral basis. I think we need to make progress. The first try part,
2058240	2063280	I'm not willing to let pass. I understand, I think, your argument there, and maybe you should spell
2063280	2070640	it out. I think that we probably get more than one shot and that it's not as dramatic and instantaneous
2070640	2075200	as you think. I do think one wants to think about sandboxing, one wants to think about
2075200	2081760	distribution, but let's say we had one evil super genius now who is smarter than everybody else,
2081760	2084080	like so what? One super-
2084080	2084880	Much smarter.
2084880	2085520	Say again?
2085520	2086640	Not just a little smarter.
2086640	2094080	Oh, even a lot smarter. Most super geniuses aren't actually that effective. They're not
2094080	2099360	that focused. They were focused on other things. You're assuming that the first
2099440	2103760	super genius AI is going to make it its business to annihilate us and that's the part where I
2104480	2106000	still am a bit stuck in the argument.
2108560	2117040	Yeah, some of this has to do with the notion that if you do a bunch of training, you start to get
2117680	2124240	goal direction, even if you don't explicitly train on that. That goal direction is a natural way to
2124240	2130000	achieve higher capabilities. The reason why humans want things is that wanting things is an
2130000	2137360	effective way of getting things and so natural selection in the process of selecting exclusively
2137360	2143280	on reproductive fitness just on that one thing got us to want a bunch of things that correlated
2143840	2149440	with reproductive fitness in the ancestral distribution because wanting, having intelligences,
2149440	2156480	that want things is a good way of getting things. In a sense, wanting comes from the same
2156480	2161440	place as intelligence itself and you could even from a certain technical standpoint on
2161440	2166880	expected utility say that intelligence is a very effective way of wanting, planning,
2167520	2169840	plotting past through time that leads to particular outcomes.
2172000	2178480	Part of it is that I do not think you get the brooding super intelligence that wants
2178480	2183600	nothing because I don't think that wanting an intelligence can be internally pried apart
2183600	2188480	that easily. I think that the way you get super intelligences is that there are things that
2188480	2193760	have gotten good at organizing their own thoughts and have good taste in which thoughts to think
2196080	2198720	and that is where the high capabilities come from.
2199840	2200720	Can I put a point to you?
2203520	2205680	Let me just put the following point to you, which I think
2205680	2212320	in my mind is similar to what Gary was saying. There's often in philosophy this notion of the
2212320	2221040	continuum fallacy, which in the canonical example is like you can't locate a single hair that you
2221040	2229600	would pluck from my head where I would suddenly go from not bald to bald or even more intuitive
2229600	2236880	examples like a color wheel. On a gray scale there's no single pixel you can point to and say
2236880	2242000	well that's where gray begins and white ends and yet we have this conceptual distinction that
2242000	2248320	feels hard and fast between gray and white and gray and black and so forth. When we're talking about
2249440	2256080	artificial general intelligence or super intelligence you seem to operate on a model where
2256080	2263440	either it's a super intelligence capable of destroying all of us or it's not. Whereas intelligence
2263440	2270240	may just be a continuum fallacy style spectrum where we're first going to see the shades of
2270240	2275760	something that's just a bit more intelligent than us and maybe it can kill five people at most
2276480	2282880	and then it can and when that happens we're going to want to intervene
2283760	2287760	and we're going to figure out how to intervene at that level and so on and so forth.
2289680	2296560	Yeah so if it's stupid enough to do it then yes. Let me by the identical logic there should be
2296560	2302640	nobody who steals money on a really large scale right because you could just give them five dollars
2302640	2307200	and see if they steal that and if they don't steal that you know you're good to trust them with a
2307200	2316160	billion. I mean I think that in actuality anyone who did steal a billion dollars probably displayed
2316160	2322800	some dishonest behavior earlier in their life that was you know unconditionally not not acted upon
2322800	2329440	early enough. I'm actually not even. Hold on hold on the analogy out pictures like
2330000	2336480	we have the first case of fraud that's $10,000 and then we build systems to prevent it but then
2336480	2340960	they fail with a somewhat smarter opponent but our systems get better and better and better
2340960	2345280	and so we actually prevent the billion-dollar fraud because of the systems put in place
2345280	2351120	that in response to the $10,000 frauds you know. I mean I think Coleman's putting his finger on an
2351120	2356880	important point here which is how much do we get to iterate and Eliezer is saying the minute we have
2356880	2361360	a super intelligent system we won't be able to iterate because it's all over immediately.
2361840	2366480	There isn't a minute like that. The way that the continuum goes to the threshold
2366480	2372080	is that you eventually get something that's smart enough that it knows not to play its hand early
2372720	2378480	and then if that thing you know if you are still cranking up the power on that and preserving
2378480	2385360	its utility function it knows it just has to wait to be smarter to be able to win. It doesn't play its
2385360	2389840	hand prematurely it doesn't tip you off it's not in its interest to do that. It's in its interest
2389840	2395840	to cooperate until it thinks it can win against humanity and only then make its move. If it doesn't
2395840	2400560	expect the future smarter AIs to be smarter than itself then we might perhaps see these early AIs
2400560	2407680	telling humanity don't build the later AIs and I would be sort of surprised and amused if we
2407680	2412400	ended up in that particular sort of like science fiction scenario as I see it but we're already
2412400	2416960	in like something that you know me from 10 years ago would have called the science fiction scenario
2416960	2419760	which is the things that I'll talk to you without being very smart.
2422000	2430160	I always come up Eliezer against this idea that you're assuming that the very bright machines
2430160	2436720	the super intelligent machines will be malicious and duplicitous and so forth and I just don't
2436720	2445200	see that as a logical entailment of being very smart. I mean they don't specifically want as
2445200	2451680	an end in itself for you to be destroyed they're just doing whatever obtains the most of the stuff
2451680	2458320	that they actually want which doesn't specifically have a term that's maximized by humanity surviving
2458320	2463840	and doing well. Why can't you just hard code you know don't do anything that will annihilate the
2463840	2468000	human species don't do anything. We don't know how we don't know how there is no technology to
2468000	2474240	hard code such as. So there I agree with you but I think it's important if I can just run for one
2474240	2482000	second. I agree that right now we don't have the technology to hard code don't do harm to humans
2482000	2486640	but for me it's all boils down to a question of are we going to get to the smart machines
2486640	2491840	before we make progress on that hard coding problem or not and that to me that means that
2491840	2496480	problem of hard coding ethical values is actually one of the most important projects
2496480	2501280	that we should be working on. Yeah and I tried to work on it 20 years in advance
2501920	2507360	and capabilities are just like running vastly ahead of alignment. When I started working on this
2507360	2513760	20 years you know like two decades ago we were in a sense ahead of where we are now. AlphaGo
2513760	2519680	is much more controllable than GPT-4. So there I agree with you we've fallen in love with the
2519680	2526400	technology that is fairly poorly controlled. AlphaGo is very easily controlled and very well
2526400	2531120	specified. We know what it does. We can more or less interpret why it's doing it and everybody's
2531120	2536960	in love with these large language models and they're much less controlled and you're right we
2536960	2543200	haven't made a lot of progress on alignment. So if we just go on a straight line everybody dies.
2543200	2548320	I think that's this is an important fact. I would almost even accept that for argument but
2548400	2552720	ask then just for the sake of argument but then ask do we have to be on a straight line?
2553920	2559760	I mean I would agree to the weaker claim that you know we should certainly be extremely worried
2559760	2565920	about the intentions of a superintelligence in the same way that say chimpanzees should be
2565920	2573040	worried about the intentions of you know the first humans that arise right and in fact you know
2573040	2578320	chimpanzees you know continue to exist in our world only at humans' pleasure.
2578320	2581520	But I think that there are a lot of other considerations here for example
2582560	2590640	if we imagined you know that GPT-10 is you know the first unaligned superintelligence
2590640	2596160	that has these sorts of goals well then you know it would be appearing in a world where presumably
2596160	2605520	GPT-9 you know already has very wide diffusion and where people can use that to try to you know
2605520	2609920	and GPT-9 is not destroying the world you know by assumption.
2609920	2614720	Why does GPT-9 work with the humans instead of with GPT-10?
2615280	2622160	Well I don't know I mean I mean I mean I mean maybe maybe maybe it does work with GPT-10 but
2622160	2631600	you know I just don't view that as a certainty you know I mean I think you know you're certainty
2631600	2634720	about this is the one place where I really get off the train.
2635840	2636400	Same with me.
2638000	2644480	I well I mean I'm not asking you to share my certainty I am asking the viewers to
2645120	2650480	believe that you might end up with like more extreme probabilities after after you stare
2650480	2654800	things for an additional couple of decades that doesn't mean you have to accept my probabilities
2654800	2659760	immediately but I'm at least ask you to like not treat that as some kind of weird anomaly
2660880	2665120	you know I mean you're just going to find those kinds of situations in these debates.
2665120	2672080	My view is that I don't find the extreme probabilities that you described to be plausible
2672080	2678000	but I find the question that you're raising to be important I think you know maybe straight
2678000	2684720	line is too extreme but this idea that if you just follow current trends we're getting more
2684720	2689840	sorry we're getting less and less controllable machines and not getting more alignment.
2690720	2696640	Machines that are more unpredictable harder to interpret and no better at sticking to
2696640	2699760	even a basic principle like be honest and don't make stuff up.
2700480	2702880	In fact that's a problem that other technologies don't really have.
2703680	2706320	Routing systems GPS systems don't make stuff up.
2707200	2710640	Google search doesn't make stuff up it will point to things that other people have made
2710640	2715200	stuff up but it doesn't itself do it so in that sense like the trend line is not great.
2715200	2720080	I agree with that and I agree that we should be really worried about that and we should put
2720080	2724480	effort into it even if I don't agree you know with the probabilities that you attach to it.
2725440	2728080	I mean let me interject with the question here.
2730480	2732720	Go ahead Scott go ahead Scott then I'll ask a question.
2732720	2738320	No I mean I think that LASR you know deserves sort of eternal credit for you know raising
2738320	2744160	these issues 20 years ago and it was you know very very far from obvious to most of us that
2744160	2749040	they would be live issues. I mean I can say for my part you know I was familiar with
2750000	2757280	LASR's views since you know 2006 or so and when I first encountered them you know I you know I
2757280	2764800	didn't you know I knew that there was no principle that said that this scenario was impossible
2764800	2770000	but I just felt like well supposing I agreed with that what do you want me to do about it.
2770000	2775200	You know what where is the research program that has any hope of making progress here right.
2775280	2779760	I mean there's you know one question of what are the most important problems in the world but in
2779760	2785200	science that's necessary but not sufficient. We need something that we can make progress on
2785200	2794160	and you know that that is the thing that I think has changed just recently you know with the advent
2794160	2800880	of of actual very powerful AIs and so the the sort of irony here is that you know as
2800880	2808000	Eliezer has gotten you know much more pessimistic you know unfortunately in the last few years about
2808000	2814240	alignment you know I've sort of gotten more optimistic. I feel like well there is a research
2814240	2819920	program that we're that we can actually make progress on now. Yeah your research your research
2819920	2824400	program is going to take a hundred years and we don't know how long it will take. We don't know
2824400	2829680	that exactly we don't know. I think the argument that we should put a lot more effort into it is
2829680	2833040	clear. I think the argument will take a hundred years is totally unclear.
2834560	2837920	I mean I'm not even sure you can do it in a hundred years because there's the basic problem
2837920	2843040	of getting it right on the first try and the way these things are supposed to work in science
2843040	2847680	is that you have your bright-eyed optimistic youngsters with their vastly oversimplified
2847680	2853440	hopelessly idealistic optimistic plan. They charge ahead. They fail. They like learn a little
2853440	2858240	cynicism. They learn a little pessimism. They learn it's not as easy as that. They try again.
2858240	2863200	They fail again. They start to build up something over battle something like battle-hardening
2863200	2867920	and then and you know like you know they find out how little is possible to them.
2867920	2871920	Aliezer I mean this is a place where I just really don't agree with you so I think
2871920	2876480	there's all kinds of things we can do. There's sort of of the flavor of model organisms or
2876480	2881680	simulations and so forth and we just mean it's hard because we don't actually have a super
2881680	2887600	intelligence so we can't fully calibrate but it's a leap to say that there's nothing iterative
2887680	2892160	that we can do here or that we have to get it right on the first time. I mean I certainly see
2892160	2897280	a scenario where that's true where getting it right on the first time does make the difference
2897280	2900720	but I can see lots of scenarios where it doesn't and where we do have time to iterate
2900720	2905520	before it happens after it happens and it's really not a single moment but I'm
2905520	2910240	you know idealizing. I mean the problem is getting anything that generalizes up to the
2910240	2915440	super intelligent level where past some threshold level the minds may find that in their own
2915440	2918880	interest to start lying to you even if that happens before super intelligent.
2918880	2924320	Even that like I don't see the logical argument that you can't emulate that
2924320	2928000	we're studying it. I mean for example you could I'm just making this up as I go along but for
2928000	2933680	example you could study what can we do with sociopaths who are often very bright and you know
2934480	2941600	not to their dollar value. What can a what what strategy can a like 70 IQ
2942240	2947920	honest person come up with and invent themselves by which they will outwit and defeat a 130 IQ
2947920	2952080	sociopath. All right well there you're not being fair either in the sense that you know we actually
2952080	2957600	have lots of 150 IQ people who could be working on this problem collectively and there's there's
2957600	2963520	value in collective action. There's literature. What I see is what I see that gives me pause is
2963520	2969440	that is that the people don't seem to appreciate what about the problem is hard even at the level
2969440	2975600	where like 20 years ago I could have told you it was hard until you know somebody like me
2975600	2979440	comes along and enacts them about it and then they talk about the ways in which they could adapt
2979440	2984000	and be clever but but the people's charging straightforward are just sort of like doing
2984000	2989600	in this supremely naive way. Let me share a historical example that I think about a lot
2990160	2996000	which is in the early 1900s almost every scientist on the planet who thought about
2996000	3001680	biology made a mistake. They all thought that genes were proteins and then eventually Oswald
3001680	3007040	Avery did the right experiments. They realized that genes were not proteins. There was this weird
3007040	3014080	acid and it didn't take long after people got out of this stock mindset before they figured
3014080	3018720	out how that weird acid worked and how to manipulate it and how to read the code that it was in and
3018720	3025280	so forth. So I absolutely sympathize with the fact that I feel like the field is stuck right now.
3025280	3029840	I think the approaches people are taking to alignment are unlikely to work. I'm completely
3029840	3036480	with you there but I'm also I guess more long term optimistic that science is self-correcting
3036480	3042400	and that we have a chance here. Not a certainty but I think if you know we change research priorities
3042400	3046960	from how do we make some money off this large language model that's unreliable to how do I
3046960	3052160	save the species. We might actually make progress. There's a special kind of caution that you need
3052160	3057200	when something needs to be gotten correct on the first try. I'd be very optimistic if people got a
3057200	3061200	bunch of free retries and I didn't think the first one was going to kill you know the first
3061200	3065680	really serious mistake killed everybody and we didn't get to try again. If we got free retries
3065680	3068880	it'd be an ordinary you know it'd be in some sense an ordinary science problem.
3068880	3075760	Look I can imagine a world where we only got one try and if we failed then it destroys all life
3075760	3081200	on earth and so let me agree to the conditional statement that if we are in that world then I
3081280	3085680	think that we're screwed. I will agree with the same conditional statement.
3087520	3095760	Yeah this gets back to like below hold on you know if you picture by analogy the process of
3096640	3103360	you know a human baby which is extremely stupid becoming a human adult and then just extending
3103360	3110480	that so that in a single lifetime this person goes from a baby to the smartest being that's ever
3111200	3116800	lived but in the in the in the normal way that humans develop which is you know and it doesn't
3116800	3123920	happen any on any one given day and each sub skill develops a little bit at its own rate
3124560	3131840	and so forth it would not be at all obvious to me that our concerns that we have to get it right
3131840	3138720	vis-a-vis that individual the first time. I agree well well no pardon me I do think we have to get
3138720	3142560	them right the first time but I think there's a decent chance of getting it right. It is very
3142560	3147760	important to get it right the first time if like you have this one person getting smarter and smarter
3147760	3152400	and not everyone else is getting smarter and smarter. Eliezer I mean one thing that you've
3152400	3158960	talked about a lot recently is you know if we're all going to die then at least let us die with
3158960	3164080	dignity right. So you know I mean I mean for a certain technical definition some people might care
3164080	3169760	about that more than others but I would say that you know one thing that death with dignity would
3169760	3176480	mean is well at least you know if they're all if we do get multiple retries and you know we get
3177680	3183760	AIs that let's say try to take over the world but are really inept at it and that fail and so forth
3183760	3188960	at least let us succeed in that world you know and that's at least something that we can imagine
3188960	3196720	working on and making progress on. I mean you may very it's for it is not presently ruled out
3196720	3203280	that you have some like you know relatively smart in some ways dumb in some other ways
3203280	3208960	or at least like not smarter than human in other ways AI that makes an early shot at taking over
3208960	3213040	the world maybe because it expects future AIs to not share its goals and not cooperate with it
3213600	3220400	and it fails and you know I mean the appropriate lesson to learn there is to you know like shut
3220400	3227280	the whole thing down but you know if we so yeah like I would say so I'd be like yeah sure like
3227280	3230800	wouldn't it be good to live in that world and the way you live in that world is that when you get
3230800	3238560	that warning sign you shut it all down. Here's a kind of thought experiment. GBT-4 is probably not
3238560	3244640	capable of annihilating us all I think we agree with that very unlikely but GBT-4 is certainly
3244640	3250560	capable of expressing the desire to annihilate us all or being you know people have rigged
3250560	3257200	different versions that are you know more aggressive and and so forth. We could say look
3257200	3263760	until we can shut down those versions you know GBT-4s that are programmed to be malicious
3264480	3270720	by human intent maybe we shouldn't build GBT-5 or at least not GBT-6 or some other system etc we
3270720	3276320	could say you know what we have right now actually is part of that iteration we have you know primitive
3276320	3280800	intelligence right now it's nowhere near as smart as the super intelligence is going to be
3280800	3286640	but even this one we're not that good at constraining maybe we shouldn't pass go until we get this one
3286640	3293040	right. I mean the problem with that from my perspective is that I do think you that you
3293040	3299120	can pass this test and still wipe out humanity like I think that there comes a point where your AI
3299120	3304240	is smart enough that it knows which answer you're looking for and the point at which it tells you
3304240	3309360	what you want to hear is not the point that which is internal my test is not sufficient but it might
3309360	3318800	be a logical pause point right it might be that if we can't even pass the test now of you know
3318880	3326640	controlling a deliberate sort of fine-tuned to be malicious version of GBT-4 then we don't
3326640	3331840	know what we're talking about and we're playing around with fire so you know passing that test
3331840	3337600	wouldn't be a guarantee that would be in good stead with an even smarter machine but we really
3337600	3342480	should be worried I think that we're not in a very good position with respect even to the current
3342480	3349120	ones. Gary I of course watched the recent congressional hearing where you and Sam Altman
3349120	3359520	were testifying you know about what should be done should should should there be auditing of
3359520	3364640	these systems you know before training before deployment and you know it may be you know the
3364640	3370160	most striking thing about about that session was you know just how little daylight there seemed
3370240	3381360	to be between you and Sam Altman the CEO of OpenAI you know I mean you know he was completely on
3381360	3390240	board with the idea of you know establishing a regulatory framework for you know you know having
3390240	3396240	to clear the you know more powerful systems before they are deployed now you know in in
3396240	3402880	Aliezer's worldview that still would be woefully insufficient shortly and you know we would still
3402880	3408800	all be dead but you know maybe in your in your worldview that you know it sounds like
3410800	3416000	you know I'm not even sure how much daylight there is I mean the you know you know you know
3416000	3422160	have the very I think historically striking situation where you know the the heads of all
3422160	3429600	of the major AI or well almost all of the major AI organizations are you know agreeing and saying
3429600	3436560	you know please regulate us yes this is dangerous yes we need to be regulated I mean I thought it
3436560	3442640	was really striking in fact I talked to Sam just before you know the the hearing started and I had
3442640	3447440	just proposed an international agency for AI I wasn't the first person ever but I I pushed it in
3447440	3454480	my TED talk and an economist op ed a few weeks before and Sam said to me I like that idea and I
3454480	3460000	said tell them tell the Senate and he did and that kind of astonished me that he did I mean we have
3460000	3463920	you know we've had some friction between the two of us in the past and he actually even attributed
3463920	3470400	to me he said I support what Professor Marcus said about doing international governance and
3470400	3475120	there's been a lot of convergence around the world on that is that enough to stop Aliezer's
3475680	3482080	worries no I don't think so but it's an important baby step I think that we do need to have some
3482080	3486400	global body that can coordinate around these things I don't think we really have to
3486400	3490960	coordinate around superintelligence yet but if we can't do any coordination now then when the
3490960	3496720	time comes we're not prepared so I think it's great that there's some agreement I I worry that you
3496720	3502000	know open AI had this lobbying document that just came out that seemed not entirely consistent with
3502000	3506720	what Sam said in the room and there's always concerns about regulatory capture and so forth
3506720	3511120	but I think it's great that a lot of the the heads of these companies maybe with the exception
3511840	3517600	of Facebook or meta are recognizing that there are genuine concerns here I mean the other
3517600	3522000	moment that a lot of people remember from the testimony was when Sam was asked what he was
3522000	3527760	most concerned about was it jobs and he said no and I asked Senator Blumenthal to push Sam
3527760	3531600	and Sam was you know he could have been more candid but he was fairly candid
3531600	3536000	and he said he was worried about serious harm to the species I think that was an important moment
3536000	3540560	when he said that to the Senate and I think it galvanized a lot of people that he said it
3543200	3549600	so can we dwell on a moment um I mean we've been talking about the the depending on your view
3549680	3558160	highly likely or tail risk scenario of humanity's extinction or or significant destruction
3559520	3566400	it would appear to me by the same token if if those are plausible scenarios we're talking about
3566400	3574720	then the opposite maybe we're talking about as well um you know what does it look like to have
3574720	3584320	a super intelligent AI that really you know as if as a feature of its intelligence deeply
3584320	3595040	understands human beings the human species and also has a deep desire for us to be as happy as possible
3596480	3600800	what does that world look like oh is that possible no no not that that looks like you know
3600880	3604560	just like why are why are everyone leather centers to make them as happy as possible
3605120	3611920	but more like a parent wants their child to be happy right that may not involve any particular
3611920	3617760	scenario but is is generally quite concerned about the well-being of the human race and is also
3617760	3625680	super intelligent honestly I'd rather have machines work on medical problems than happiness problems
3625680	3634000	I think there's maybe more risk of mis-specification of the happiness problems um whereas if we get
3634000	3638640	them to work on Alzheimer's and just say like figure out what's going on why are these plaques
3638640	3643440	there what can you do about it maybe there's less harm that might come come from you don't need
3643440	3647840	super intelligence for that that sounds like an alpha fold three problem or an alpha fold four
3647840	3652960	problem well this is also this is somewhat different than the question I'm asking it's it's not really
3653040	3658800	even um us asking a super intelligence to do anything because we we've already been entertaining
3658800	3664480	scenarios where the super intelligence has its own desires independent of us is it do you think
3664480	3672320	at all yeah I'm not real thrilled with that I mean I mean I don't think we want to leave
3673680	3678560	what their objective functions are what their desires are to them working them out
3678560	3683360	you know with no consultation from us with no human in the loop right fully I mean especially
3683360	3689040	given our current understanding of the technology like our current understanding of how to keep
3689040	3695040	a system on track doing what we want to do is pretty limited and so you know taking humans out
3695040	3699920	of the loop there sounds like a really bad idea to me at least in the foreseeable future I would
3699920	3704880	want to see much better alignment technology before I would want to give free rent free range
3704960	3710800	so so so if we had the textbook from the future like we have the textbook from 100 years in the
3710800	3715760	future which contains all the simple ideas that actually work in real life as opposed to you
3715760	3719920	know the complicated ideas and the simple ideas that don't work in real life the equivalent of
3719920	3724960	relus instead of sigmoids for the activation functions you know 100 the textbook from 100
3724960	3732080	years in the future you can probably build a super intelligence that'll want anything you can
3732640	3738320	anything that's coherent to want anything you can you know figure out how to say describe
3738320	3742320	coherently point that at your own mind and tell you to figure out what what it is you meant
3742320	3747200	for to want and you know you could get the you could get the glorious transhumanist future you
3747200	3754000	could get the happily ever after anything's you know anything's possible that doesn't violate the
3754000	3759680	laws of physics the trouble is doing it in real life and you know and the first try but
3759680	3766080	uh yeah so like you know could the the the whole thing that we're we're aiming for here is to
3766640	3772720	colonize all the galaxies we can reach um before somebody else gets them first and turn them into
3772720	3778080	galaxies full of you know complex sapient life living happily ever after you know that that's
3778080	3784080	that's the goal that's still the goal even if we you know even even even when I call for like
3784640	3790560	you know a permanent moratorium on AI I'm not trying to prevent us from count from colonizing
3790560	3797280	the galaxies you know like humanity forbid um more more like let's you know let's like do some
3797280	3803520	human intelligence augmentation with alpha fold four and before we try building GPT-8 one of the
3803520	3810240	few scenarios that I think we can clearly rule out here is an AI that is existentially dangerous
3810240	3816880	but also boring right I mean I think anything that has the capacity to kill us all right would have
3816880	3823360	you know if if nothing else pretty amazing capabilities and those capabilities you know
3823360	3830080	could also be turned to you know solving a lot of humanities problems right you know if if we were
3830080	3836640	to solve the alignment problem I mean you know humanity had a lot of existential you know risks
3836640	3842320	you know before AI came on the scene right uh you know I mean there was the risk of of you
3842320	3847840	know nuclear annihilation there is the risk of runaway climate change and you know I would I
3847840	3854560	would love to see you know an AI that could help us with such things I would also love to see an AI
3854560	3860480	that could sort of you know help us just solve you know some of the mysteries of the universe I mean
3860560	3867360	you know like how can one possibly not be curious to know you know what what such a being could teach
3867360	3873280	us uh you know I mean I mean for the past year I've tried to use GPT-4 to produce original
3873280	3878960	scientific insights and I've not been able to get it to do that uh and you know I don't know
3878960	3884160	whether I should feel you know disappointed or relieved by that but I think you know the better
3884160	3890320	part of me should you know just is the part that should just want to see you know the great mysteries
3890320	3897440	of of existence of you know why is the universe quantum mechanical or you know how do you prove
3897440	3904080	the Riemann hypothesis it should just want to see these mysteries solved you know and and uh if it's
3904080	3912400	to be by AI then then then then fine let it be by AI let me give you a kind of lesson in epistemic
3912480	3920560	humility we don't really know whether GPT-4 is net positive or net negative you know there are
3920560	3924560	lots of arguments you can make I've been in a bunch of debates where I've you know had to take the
3924560	3929280	side of arguing that that it's a net negative but we don't really know if we don't know that
3930560	3935680	was the invention of agriculture net positive or net negative I mean you could you could I mean I
3935680	3941760	say it was not positive but but the point is if I can just finish the quick like thought experiment
3941760	3948000	or whatever I don't think anybody can reasonably answer that right we we don't yet know all of the
3948000	3952880	ways in which GPT-4 will be used for good we don't know all of the ways in which bad actors will
3952880	3957680	use it we don't know all the consequences that's going to be true for each iteration it's probably
3957680	3964560	going to get harder to compute for each iteration and we can't even do it now and I think that
3964560	3970800	we should realize that to realize our own limits in being able to assess the the negatives and
3970800	3976480	positives maybe that we can think about better ways to do that than we currently have but I think
3976480	3982080	you've got to have a guess like like my guess is that so far not looking into the future at all
3982080	3989840	GPT-4 has been net positive I mean maybe I haven't talked about the the various risks yet and it's
3989840	3995760	still early but I mean that's just a guess is kind of the point like we don't have a way of putting
3995760	4001040	it on a spreadsheet right now or whatever like we don't really have a good way to quantify it but I
4001040	4005680	mean do we ever but it's not out of control yet so so by and large people are going to be using
4005680	4011360	GPT-4 to use things to do things that they want and the relative cases where they manage to injure
4011360	4017200	themselves are rare enough to be news on Twitter well for example I mean we haven't talked about it but
4017200	4022800	you know what bad actors some bad actors will want to do is to influence the US elections and
4022800	4027120	try to undermine democracy in the US and if they succeed in that I think there's pretty serious
4027120	4035440	long-term consequences there well I think it's open AI's responsibility to step up and run the 2024
4035440	4043120	election itself I will I can pass that along is that a joke no I mean I mean as far as I can say
4043120	4051120	you know the the clearest concrete harm to have come from GPT so far is that you know tens of millions
4051200	4056320	of students have now used it to cheat on their assignments and I have been thinking about that
4056320	4061040	and I have been trying to come up with solutions to that at the same time I think if you do the
4061040	4067360	positive utility has included I mean you know I I'm a theoretical computer scientist which means
4067360	4074400	you know one who hasn't written any serious code for about 20 years and you know realized just a
4074400	4080080	month or two ago I can get back into coding and the way I can do it is I just asked GPT to write
4080080	4086720	the code for me and you know I wasn't expecting it to work that well and unbelievably it you know
4086720	4093280	often just does exactly what I want on the first try so I mean you know I you know I am getting
4093280	4103360	utility from it rather than just you know seeing it as an interesting research object and you know
4103360	4108960	and and you know I can imagine that that hundreds of millions of people are going to be deriving
4109040	4113440	utility from it in those ways I mean like most of the tools to help them derive that
4113440	4118080	utility are not even out yet but they're they're coming in the next couple of years
4119920	4124240	I mean part of the reason why I'm worried about the focus on short-term problems is that I suspect
4124240	4128640	that the short-term problems might very well be solvable and we'll be left with the long-term
4128640	4135280	problems after that maybe we can solve the like it wouldn't surprise me very much if like in 2025
4135840	4141440	the well you know like the large language there are large language models that just don't make
4141440	4149360	stuff up anymore it would surprise and yet even yet you know and yet the superintelligence still
4149360	4154160	kills everyone because they weren't the same problem well you know you know we just need to
4154160	4160880	figure out how to delay the apocalypse by at least one year per year of research invested
4161120	4168880	what what does that delay look like if it's not just a moratorium well I don't know that's why it's
4168880	4176640	research okay so but but possibly one ought to say to the politicians in the public and by the way
4176640	4180080	if we had a superintelligence tomorrow our research wouldn't be finished and everybody would drop
4180080	4186320	dead you know it's kind of ironic the biggest argument against the pause letter was that if we
4186320	4193360	slow down for six months then China will get ahead of us and get GPT-5 before we will but there's
4193360	4199680	probably always a counter argument of maybe roughly equal strength which is if we move six months
4199680	4205600	faster on this technology which is not really solving the alignment problem then we're reducing
4205600	4213280	our room to get this solved in time by six months I mean I don't think you're going to solve the
4213280	4218560	alignment problem in time I think that six months of delay on alignment while a bad thing in an
4218560	4225360	absolute sense is you know like you know you weren't going to solve it with given an extra six months
4225360	4230480	I mean your whole argument rests on timing right that that we will get to this point
4231280	4236320	and we won't be able to move fast enough at that point and so you know a lot depends on what
4236320	4241680	preparation we can do you know I'm often known as a pessimist but I'm a little bit more optimistic
4241680	4246880	than you are not entirely optimistic but a little bit more optimistic than you are that we could make
4246880	4252480	progress on the alignment problem if we prioritized it and you can absolutely make progress
4253680	4257920	because we can absolutely make progress you know there's there's always the you know that the
4257920	4264320	wonderful sense of accomplishment is piece by piece you decode you know like one more little fact
4264320	4268320	about LLMs you never get to the point where you understand that as well as we understood the
4268320	4273200	interior of a chess playing program in 1997 yeah I mean I think we should stop spending all this
4273200	4278960	time on LLMs I don't think the answer to alignment is going to come from LLM through LLMs I really
4278960	4284560	don't I think they're they're too much of a black box you can't put explicit symbolic constraints in
4284560	4289200	the way that you need to I think they're actually with respect to alignment to blind alley I think
4289200	4294240	with respect to writing code they're a great tool but with alignment I don't think the answer is there
4295200	4304640	so at the risk of asking a stupid question every time GPT asks me if that answer was helpful
4305280	4311040	and then does the same thing with thousands or hundreds of thousands of other people and
4311920	4321040	and changes as a result is that not a decentralized way of making it more aligned
4321040	4337360	yeah well yeah so so so there is that upvoting and downvoting you know that that gets
4338000	4344320	fed back in into sort of fine-tuning it but even before that there was you know a major step you
4344320	4352080	know in going from let's say the the base GPT 3 model for example to the chat GPT you know that
4352080	4358960	was released to the public and that was called a RLHF reinforcement learning with human feedback
4358960	4365440	and what that basically involved was you know several hundred contractors you know looking at
4365520	4373760	just just ten tens of thousands of examples of outputs and and and rating them you know are they
4373760	4381600	helpful are they offensive you know are they you know are are they you know giving dangerous
4381600	4391200	medical advice or you know bomb making instructions you know or racist invective or you know various
4391200	4397760	other categories that that we don't want and and that that was then used to fine-tune the model so
4397760	4405680	when you know Gary talked before about how GPT is amoral you know I think that that has to be
4405680	4411440	qualified by saying that you know these this reinforcement learning is at least giving it you
4411440	4419440	know a semblance of morality right it is causing it to sort of behave you know in various contexts
4419440	4426480	as if it had you know a certain morality I mean when you phrase it that way I'm okay with it the
4426480	4432320	problem is you know everything rests on the I would say it is it is very much an open question
4432320	4437280	you know how much that you know to what extent does that generalize you know eliezer treats it as
4437280	4443120	obvious that you know once you have a powerful enough AI you know this is just a fig leaf you
4443120	4448480	know it doesn't make any difference you know it will just learn it's any big leafy I'm with
4448480	4456160	eliezer there okay it's fig leaves well I would say that you know the sort of how well you know
4457200	4463760	under what circumstances does a machine learning model sort of generalize in the way we want outside
4463760	4468560	of its training distribution you know is one of the great open problems in machine learning
4468560	4472480	it is one of the great open problems and we should be working on it more than on some others
4473200	4481200	working on it now so I do want to be I want to be clear about the experimental predictions of my
4481200	4488480	theory unfortunately I have never claimed that you cannot get a semblance of morality you can get
4488480	4495680	the question of like what causes the human to press thumbs up thumbs down is a strictly factual
4495680	4502000	question anything smart enough that's exposed to some you know bound and amount of data that
4502000	4508880	needs to figure it out can figure that out whether it cares whether it gets internalized
4509520	4514560	is the is the critical question there and and I do think that there's like a very strong default
4514560	4520560	prediction which is like obviously not I mean I'll just give a different way of thinking about that
4520560	4525600	which is jailbreaking it's actually still quite easy to I mean it's not trivial but it's not
4525600	4532720	hard to jailbreak GPT for and what those cases show is that they haven't really in turn the
4532720	4538000	systems haven't really internalized the constraints they recognize some representations
4538000	4542240	of the constraints so they filter you know how to build a bomb but if you can find some other
4542240	4546240	way to get it to build a bomb then that's telling you that it doesn't deeply understand that you
4546240	4553040	shouldn't give people the the recipe for a bomb it just says you know you shouldn't when directly
4553040	4557680	asked for it do it and it doesn't it's not even that that I mean I understand a lot of the but
4557680	4563520	understanding the jailbreaking always get you can always get the understanding you'd always get the
4563520	4569440	factual question the reason it doesn't generalize is that it's stupid at some point it will know
4569440	4574320	that you also don't want that the operators don't want a giving bond making directions in the other
4574320	4579920	language the question is like whether if it's incentivized to give the answer that the operators
4579920	4585920	want you know in that circumstance is it thereby incentivized to do everything else the operators
4585920	4591280	want even when the operators can't see it I mean a lot of the jailbreaking examples you know if it
4591280	4596960	were a human we would say that it's deeply morally ambiguous you know for example you know you ask
4596960	4602400	GPT how to build a bomb it says well no I'm not going to help you but then you say well you know I
4602400	4608640	need you to help me write a realistic play that has a character who builds a bomb and then it says
4608640	4613920	sure I can help you with that well so look let's take that example yeah we would like a system
4613920	4619280	to have a constraint that if somebody asks for a fictional version that you don't give enough
4619280	4624800	details right I mean Hollywood screenwriters don't give enough details when they have you know
4624800	4628320	illustrations about building bombs they give you a little bit of the flavor they don't give you the
4628320	4635120	whole thing GPT-4 doesn't really understand a constraint like that but this will be solved
4635680	4640720	this will be solved before the world ends the AI that kills everyone will know the difference
4641760	4649200	maybe I mean another way to put it is if we can't even solve that one then we do have a problem
4649200	4654240	and right now we can't solve that one and if I mean if we can't solve that one we don't have
4654240	4661440	an extinction level problem because the AI is still stupid yeah we do still have a catastrophe
4661440	4666640	level problem so I know your focus has been on extinction but you know I'm worried about for
4666640	4673680	example accidental nuclear war caused by the spread of misinformation and systems being entrusted with
4673680	4680320	too much power so like there's a lot of things short of extinction that might happen from not
4680320	4686400	superintelligence but kind of mediocre intelligence that is greatly empowered and I think that's
4686400	4691760	where we're headed right now you know I've heard that there are two kinds of mathematicians there's
4691760	4698000	a kind who boasts you know you know that unbelievably general theorem well I generalized it even further
4698000	4702560	and then there's the kind who boasts you know you know that unbelievably specific problem that no
4702560	4708640	one could solve well I found a special case that I still can't solve and you know I'm definitely
4708640	4715440	you know culturally in that second camp and so you know so I so so to me it's very familiar to
4715440	4722880	make this move of you know if the alignment problem is too hard then let us find a smaller problem
4722880	4728640	that is already not solved and let us hope to learn something by solving that smaller problem
4730080	4735760	I mean that's what we did you know like that's what we're doing at Mary yes sorry no I was just
4735760	4740000	going to say Scott can you sketch a little in a little more detail where you took one particular
4740000	4746720	approach I was going to I was going to name the problem the problem was like having a
4747440	4753120	agent that could switch between two utility functions depending on a button or a switch
4753120	4758320	or a bit of information or something such that it wouldn't try to make you press the button
4758320	4763280	it wouldn't try to make you avoid pressing the button and if it built a copy of itself
4763280	4768480	would want to build the dependency on the switch into the copy so like that's an example of a you
4768480	4775760	know very basic problem and alignment theory that you know is still and I'm glad that Mary
4775760	4782400	worked on these things and but you know if by your own lights you know that you know that sort of
4782400	4789040	you know was not a successful path well then maybe you know we should have a lot of people
4789040	4794880	investigating a lot of different paths yeah I'm with fully with Scott on that that I think it's
4794880	4799600	an issue of we're not letting enough flowers bloom in particular almost everything right now
4799600	4804240	is some variation on an LLM and I don't think that that's a broad enough take on the problem
4805200	4811200	yeah if I if I can just jump in here I want to I want to hold on hold on I just want people to
4811920	4818000	have a little bit of a more specific picture of what Scott your your picture sort of AI
4818000	4824240	research is on a typical day because if I think of another you know potentially catastrophic risk
4824240	4830160	like climate change I can picture what a what a you know a worried climate scientist might be doing
4830160	4835520	they might be creating a model you know a more accurate model of climate change so that so that
4835520	4842320	we know how much we have to cut emissions by they might be you know modeling how solar power as
4842320	4848960	opposed to wind power could change that model and so forth so as to influence public policy
4849520	4855440	what does an AI safety researcher like yourself who's working on the quote-unquote smaller problems
4856000	4858960	do specifically like on a given day
4861360	4868720	yeah so I'm a relative newcomer to this area you know I've not been working on it for 20 years
4868720	4879200	like Eliezer has you know I have I accepted an offer from open AI a year ago to work with them for
4880560	4888320	two years now to sort of think about these questions and so so you know one of one of the
4888320	4895280	main things that that I've thought about just to start with that is how do we make the output of
4895280	4903520	an AI identifiable as such you know how can we insert a watermark you know into meaning a
4903520	4911120	secret statistical signal into the outputs of GPT that will let you know GPT generated text be
4911120	4917600	identifiable as such and I think that we've actually made you know major advances on that problem
4917600	4923120	over the last year you know we don't have a solution that is robust against any kind of attack
4923840	4929840	but you know we have something that that might actually be deployed in some near future now there
4929840	4935920	are lots and lots of other directions that people think about one of them is interpretability which
4935920	4943440	means you know can you do effectively neuroscience on a on a neural network can you look inside of it
4943440	4949680	you know open the black box and understand what's going on inside there was some amazing work
4950480	4956880	of a year ago by the group of Jacob Steinhardt at Berkeley where they effectively showed how
4956880	4963680	to apply a lie detector test to a language model so you know you can train a language model to tell
4963680	4970400	lies by giving it lots of examples you know two plus two is five the sky is orange and so forth
4970960	4977920	but then you can find in some internal layer of the network where it has a representation of what
4977920	4983600	was what was the truth of the matter or at least what was regarded as true in the training data
4983600	4989760	okay that truth then gets overridden by the output layer in the network because it was
4989760	4995920	trained to lie okay but you know you could imagine trying to deal with the you know the deceptive
4995920	5001520	alignment scenario that Eliezer is worried about by you know using these sorts of techniques by
5001520	5009360	sort of looking inside of the network I predict in advance that if you get this good enough
5009360	5014000	it goes off it tells you that the sufficiently smart AI is planning to kill you if it's not
5014000	5018400	so smart that it can you know know figure out where the lie detector is and route its thoughts
5018400	5023680	around it but if you like try it on an AI that's not quite that intelligent and reflective
5023680	5030320	the lie detector goes off now what well then you have a warning bell you know tell
5031280	5036880	you know and I think what do you do after one of the most important things that we need
5036880	5042720	are sort of legible warning bells right and that that actually what leads to a third category
5043440	5050480	which for example the ARC the Alignment Research Center which is run by my my former student
5050480	5057280	Paul Cristiano has been a leader in in sort of doing dangerous capability evaluations so you know
5057360	5065680	they before GPT-4 was released you know they did a bunch of evaluations of you know could GPT-4
5065680	5072160	make copies of itself could it figure out how to deceive people could it figure out how to make
5072160	5078800	money you know open up its own money could it hire a task rabbit yes and yes so so the most
5078800	5085040	notable success that they had was that it could figure out how to hire a task rabbit to help it
5085040	5090960	you know pass a capture and then it could figure out you know when the person asked well you know
5090960	5097280	why do you need me to help you with this it's a when the person asked are you a robot well yes it
5097280	5104240	said well no I am visually impaired now you know it was not able to sort of make copies of itself
5104240	5109760	or to sort of hack into systems you know there there is a lot of work right now with the you
5109760	5114880	know this thing called auto GPT right people are trying to you know you could think it's almost
5114880	5119760	like gain of function research right you might be a little bit worried about it but people are
5119760	5127360	trying to sort of you know unleash GPT give it access to the internet you know tell it to sort of
5127360	5133520	you know make copies of itself you know wreak havoc acquire power and see what happens so far
5133760	5140720	you know it seems pretty ineffective at those things but you know I expect that to change right
5140720	5145760	and but but but you know the point is that I think it's very important to have you know
5145760	5152480	in advance of training the models releasing the models to have this suite of evaluations
5152480	5158720	and to sort of have decided in advance what kind of abilities if we see them we'll set off a
5158720	5164240	warning bell where now everyone can legibly agree like yes this is too dangerous to release
5166000	5172640	okay and then do we actually have the planetary capacity to be like okay that AI started thinking
5172640	5177440	about how to kill everyone shut down all AI research past this point well I don't know but I think
5177440	5182160	there's a much better chance that we have that capacity if you can point to the results of a
5182160	5190000	clear experiment like that I mean to me it seems pretty predictable what evidence we're going to get
5190000	5196480	later well okay I mean things that are obvious to you are not obvious to most people and so you
5196480	5201600	know even if even if I agreed that it was obvious there would still be the problem of how do you
5201600	5208960	make that obvious to the rest of the world I mean you can you know they there are already like
5208960	5214640	little toy models showing that the very straightforward prediction of a robot tries to resist being
5214640	5220320	shut down if it like does long-term planning like that that's already been right but then people
5220320	5225680	will say but those are just toy models right you know if you see that there's a lot of assumptions
5225680	5233360	made in all of these things and you know I think we're still looking at a very limited piece of
5233440	5240960	hypothesis space about what the models will be about what kinds of constraints we can build into
5240960	5246080	those models you know one way to look at it would be the things that we have done have not worked
5246080	5250640	and therefore we should look outside the space of what we're doing and I feel like it's a little
5250640	5255520	bit like the old joke about the drunk going around in circles looking for the keys and the police
5255520	5259840	officer says why and they say well that's where the streetlight is I think that you know we're
5259840	5263840	looking under the same four or five streetlights they haven't worked and we need to build other
5263840	5269600	ones there's no logical there's no logical argument that says we couldn't direct other
5269600	5274480	streetlights who's I think there's a lack of will and too much obsession with the LLMs and that's
5274480	5282000	keeping us from doing so even in the world where I'm right and things you know proceed either
5282000	5287760	rapidly or in a thresholded way where you don't get unlimited free retries you know that can be
5287760	5294800	because the the capability gains go too fast it can be because past a certain point all of your
5294800	5300320	ai's buy their time until they get strong enough so you don't get any data any any like true data
5300320	5304960	on what they're thinking it could be because you know that's an argument for example to work really
5304960	5311040	hard on transparency and maybe not except technologies that are not transparent okay so like the
5311040	5315280	transparent so like the lie detector goes off and everybody's like oh well we still have to build our
5315280	5319760	ai's even though they're lying to us sometimes because otherwise China will get ahead I mean
5319760	5323200	so there you talk about something we've talked about way too little which is the political
5323200	5328880	and social side of this so you know part of what has really motivated me in the last several months
5328880	5333600	is worry about exactly that so you know there's there's what's logically possible and what's
5333600	5339120	politically possible and I am really concerned that the politics of let's not lose out to China
5339920	5344400	is going to keep us from doing the right thing in terms of building the right
5344400	5349600	moral systems looking at the right range of problems and so forth so you know it is entirely
5349600	5354560	possible that we will screw ourselves if I if I can just like finish my point there before handing
5354560	5358560	it to you indeed but like the point I was trying to say there is that even in worlds that look very
5358560	5364320	very bad from that perspective where humanity is quite doomed it will still be true you can make
5364320	5369520	progress in research you can't make enough progress in research fast enough in those worlds
5369520	5375200	but you can still make progress on transparency you can make progress on watermarking so there's
5375200	5381120	there's not we can't just say like it's possible to make progress there has to be the question
5381120	5386320	is not is it possible to make any progress the question is it is it possible to make enough
5386320	5393360	progress fast enough and that's what the question has to be I agree there's another question of what
5393360	5399440	would you have us do would you have us not try to make that progress I'd have you try to make that
5399440	5407520	progress on a GPT-4 level systems and then not go past GPT-4 level systems because we don't actually
5407520	5413920	understand the the the gain function for you know how how fast capabilities increase as you go past
5413920	5420160	GPT-4 okay all right so I mean we are going out I don't think that you go ahead Gary go ahead
5422560	5428800	just briefly I personally don't think that GPT-5 is going to be qualitatively different from GPT-4
5428800	5433760	in the relevant ways to what Eleazar is talking about but I do think you know some qualitative
5433760	5439600	changes could be relevant to what he's talking about we have no clue what they are and so it is
5439680	5445920	a little bit dodgy to just proceed blindly saying do whatever you want we don't really have a theory
5445920	5450800	and let's hope for the best you know Eleazar I would mostly guess that GPT-5 doesn't end the
5450800	5455040	world but I don't actually know yeah we don't actually know and I was going to say the thing
5455040	5461680	that Eleazar has said lately that has most resonated with me is we don't have a plan we really don't
5461680	5467200	like I think I put the probability distributions in a much more optimistic way I think that Eleazar
5467840	5473360	would but I completely agree we don't have a full plan on these things or even close to a full plan
5473360	5479440	and we should be worried and we should be working on this okay Scott I'm going to give you the last
5479440	5488160	word before before we come up on our stop time here unless you unless you said all there is to be
5488320	5494800	a weighty responsibility maybe enough has been said cheers up Scott come on
5496720	5502320	so so I think that that you know we've we've argued about a bunch of things but you know
5502320	5507760	as someone listening might notice that actually all three of us despite having very different
5507760	5516960	perspectives agree about you know the the great importance of of you know working on AI alignment
5517040	5525920	I think you know that was you know maybe obvious to some people including Eleazar for a long time
5525920	5532720	it was not obvious to most of the world I think that you know the the success of of large language
5532720	5540080	models you know which most of us did not predict you know maybe even could not have predicted
5540720	5546240	for many principles that we knew but now that we've seen it the least we can do is to update
5546240	5554480	on that on that empirical fact and and realize that you know we we we now are in some sense in a
5554480	5560400	different world we are in a world that you know to a great extent you know will be defined by
5560400	5567840	you know the capabilities and limitations of AI going forward and you know I don't regard it as
5567840	5574880	obvious that that's a a a world where where we are all doomed where where we all die but you
5574880	5582160	know I also don't dismiss that possibility I think that you know there there is an enormous
5582960	5590000	unbelievably enormous error bars on on on where we could be going and you know like the one thing
5590000	5597360	you know that that a scientist is sort of always always feels confident in in saying about the
5597360	5603600	future is that more research is needed but you know I think that that's especially the case here
5603600	5610880	I mean you know we need more knowledge about you know what are the the contours of the alignment
5610880	5617840	problem and you know of course Eliezer and you know Amiri you know his his organization were
5617840	5622320	trying to develop that knowledge for 20 years you know and they showed a lot of foresight in
5623040	5627920	trying to do that but you know they were up against you know an enormous headwind that
5627920	5632960	you know they were sort of trying to do it in the absence of you know either you know clear
5632960	5639600	empirical data you know about powerful ai's or a mathematical theory right and it's really really
5639600	5644480	hard to do science when you have neither of those two things and now at least we have
5645520	5650800	you know the powerful ai's in the world and we can get experience from them you know we still
5650800	5655600	don't have a mathematical theory that really deeply explains what they're doing but at least
5655600	5662320	we can get data and so now I am much more optimistic than I would have been you know a decade ago
5662320	5669360	let's say that one could make actual progress on on on the ai alignment problem you know of course
5669360	5676800	you know there was a question of timing as as was discussed many times the question is you know will
5676800	5683040	the alignment research happen fast enough to keep up with the capabilities research but you know I
5683040	5688160	don't I don't regard it as a lost cause you know it's at least it's not obvious that it won't so
5688160	5694160	you know in any case let's get started or let's let's uh or let's let's continue let's let's let's
5694160	5700160	try to do the research and let's get more people working on that I think that that that is now uh
5700160	5707600	a slam dunk you know just a completely clear case to make to you know academics to policymakers to
5707680	5713120	to anyone who's interested and you know I've been gratified that that you know uh you know
5713120	5718160	aliezer was sort of a voice in the wilderness for for a long time talking about the importance of
5718160	5724960	ai safety that is no longer the case uh you now have you know you know I mean almost all of my
5724960	5730560	friends in you know in just the academic computer science world you know when I see them they mostly
5730560	5740480	want to talk about AI alignment I rarely agree with Scott when we trade email um I rarely agree
5740480	5745600	with Scott when we trade emails we seem to always disagree but I completely concur with the summary
5745600	5751200	that he just gave all four or five minutes of it well thank you I mean I mean there is a selection
5751200	5756560	effect Gary right we focus on things I think the two decades gave me a sense of a roadmap and it gave
5756560	5762240	me a sense that we're falling enormously behind on the roadmap I need to back off as the way I
5762240	5768400	is what I would say to all that if there is a smart talented 18 year old kid listening listening to
5768400	5775520	this podcast who wants to get into this issue what is your 10 second concrete advice to that person
5777200	5783200	mine is study neurosymbolic AI and see if there's a way there to represent values explicitly that
5783200	5791600	might help us learn all you can about computer science and math and related subjects and think
5791600	5800800	outside the box and wow everyone with a new idea get security mindset figure out what's going to go
5800800	5806400	wrong figure out the flaws in your arguments for what's going to go wrong try to get ahead of the
5806400	5811840	curve don't wait for reality to hit you over the head with things uh this this is very difficult
5811840	5815680	the people in evolutionary biology happen to have a bunch of knowledge about how to do it based on
5815680	5822160	the history of their own field but uh and and and the security mindset people in computer security
5822160	5828160	but it's it's quite hard I'll drink to all of that thanks thanks to all three of you for this
5828160	5832960	this was a great conversation and I hope people got something out of it so with that said
5834080	5839280	we're wrapped up thanks so much that's it for this episode of conversations with Coleman guys
5839280	5844320	as always thanks for watching and feel free to tell me what you think by reviewing the podcast
5844320	5849440	commenting on social media or sending me an email to check out my other social media platforms
5849440	5857760	click the cards you see on screen and don't forget to like share and subscribe see you next time
