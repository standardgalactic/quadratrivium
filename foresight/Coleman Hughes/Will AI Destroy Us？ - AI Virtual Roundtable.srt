1
00:00:00,000 --> 00:00:04,080
Why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.

2
00:00:04,080 --> 00:00:06,320
What's the big fear here? Make the case.

3
00:00:06,320 --> 00:00:10,080
We don't understand the things that we build.

4
00:00:10,080 --> 00:00:13,920
The AIs are grown more than built, you might say.

5
00:00:13,920 --> 00:00:18,560
They end up as giant, inscrutable matrices of floating point numbers that nobody can decode.

6
00:00:18,560 --> 00:00:23,040
At this rate, we end up with something that is smarter than us, smarter than humanity,

7
00:00:23,760 --> 00:00:28,000
that we don't understand, whose preferences we could not shape.

8
00:00:28,800 --> 00:00:32,720
And by default, if that happens, if you have something around that is much

9
00:00:32,720 --> 00:00:35,760
smarter than you and does not care about you one way or the other,

10
00:00:35,760 --> 00:00:37,520
you probably end up dead at the end of that.

11
00:00:37,520 --> 00:00:43,440
Extinction is a pretty extreme outcome that I don't think is particularly likely,

12
00:00:43,440 --> 00:00:48,240
but the possibility that these machines will cause mayhem because we don't know how to

13
00:00:49,440 --> 00:00:53,200
enforce that they do what we want them to do. I think that's a real thing to worry about.

14
00:00:58,800 --> 00:01:01,440
Welcome to another episode of Conversations with Coleman.

15
00:01:02,000 --> 00:01:07,600
Today's episode is a roundtable discussion about AI safety with Eliezer Yudkowski,

16
00:01:07,600 --> 00:01:14,000
Gary Marcus, and Scott Aronson. Eliezer Yudkowski is a prominent AI researcher and writer

17
00:01:14,000 --> 00:01:17,920
known for co-founding the Machine Intelligence Research Institute,

18
00:01:17,920 --> 00:01:23,680
where he spearheaded research on AI safety. He's also widely recognized for his influential

19
00:01:23,680 --> 00:01:29,200
writings on the topic of rationality. Scott Aronson is a theoretical computer scientist

20
00:01:29,200 --> 00:01:33,920
and author, celebrated for his pioneering work in the field of quantum computation.

21
00:01:34,560 --> 00:01:39,680
He's also the chair of COMSI at U of T Austin, but is currently taking a leave of absence to

22
00:01:39,680 --> 00:01:46,080
work at open AI. Gary Marcus is a cognitive scientist, author, and entrepreneur known for

23
00:01:46,080 --> 00:01:51,760
his work at the intersection of psychology, linguistics, and AI. He's also authored several

24
00:01:51,760 --> 00:01:58,880
books, including Cluj and Rebooting AI, Building AI We Can Trust. This episode is all about AI

25
00:01:58,880 --> 00:02:04,480
safety. We talk about the alignment problem. We talk about the possibility of human extinction

26
00:02:04,480 --> 00:02:10,240
due to AI. We talk about what intelligence actually is. We talk about the notion of a singularity

27
00:02:10,240 --> 00:02:15,840
or an AI takeoff event, and much more. It was really great to get these three guys in the same

28
00:02:15,840 --> 00:02:20,560
virtual room, and I think you'll find that this conversation brings something a bit fresh to a

29
00:02:20,560 --> 00:02:26,000
topic that has admittedly been beaten to death on certain corners of the internet. Without further

30
00:02:26,000 --> 00:02:37,360
ado, Eleazar Yudkowski, Gary Marcus, and Scott Aronson. Okay, Eleazar Yudkowski, Scott Aronson,

31
00:02:37,360 --> 00:02:44,160
Gary Marcus. Thanks so much for coming on my show. Thank you. The topic of today's conversation is

32
00:02:44,160 --> 00:02:50,400
AI safety, and this is something that's been in the news lately. We've seen experts and

33
00:02:50,720 --> 00:03:00,880
CEOs signing letters, recommending public policy, surrounding regulation. We continue to have the

34
00:03:00,880 --> 00:03:08,320
debate between people that really fear AI is going to end the world and potentially kill

35
00:03:08,960 --> 00:03:14,560
all of humanity and the people who feel that those fears are overblown.

36
00:03:15,360 --> 00:03:22,240
And so this is going to be sort of a roundtable conversation about that, and you three are

37
00:03:22,880 --> 00:03:27,360
really three of the best people in the world to talk about it with. So thank you all for doing this.

38
00:03:28,960 --> 00:03:33,840
Let's just start out with you, Eleazar, because you've been one of the most

39
00:03:35,360 --> 00:03:41,040
really influential voices getting people to take seriously the possibility that AI will kill us all.

40
00:03:41,920 --> 00:03:47,760
You know, why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.

41
00:03:47,760 --> 00:03:56,160
What's the big fear here? Make the case. Well, chat GPT seems quite unlikely to kill everyone in

42
00:03:56,160 --> 00:04:02,000
its present state. AI capabilities keep on advancing and advancing. The question is not,

43
00:04:02,000 --> 00:04:09,360
can a chat GPT kill us? The answer is probably no. So as long as that's true, as long as it

44
00:04:09,440 --> 00:04:15,440
hasn't killed us yet, the engineers are just going to keep pushing the capabilities. There's no

45
00:04:15,440 --> 00:04:24,000
obvious blocking point. We don't understand the things that we build. The AIs are grown

46
00:04:24,560 --> 00:04:29,520
more than built, you might say. They end up as giant inscrutable matrices of floating point

47
00:04:29,520 --> 00:04:36,400
numbers that nobody can decode. It's probably going to end up technically difficult to make

48
00:04:36,400 --> 00:04:43,840
them want particular things and not others. People are just charging straight ahead.

49
00:04:44,800 --> 00:04:49,680
So at this rate, we end up with something that is smarter than us, smarter than humanity,

50
00:04:50,400 --> 00:04:57,200
that we don't understand, whose preferences we could not shape. And by default, if that happens,

51
00:04:57,200 --> 00:05:01,600
if you have something around that is much smarter than you and does not care about you one way or

52
00:05:01,600 --> 00:05:08,400
the other, you probably end up dead at the end of that. The way it gets the most of whatever

53
00:05:08,400 --> 00:05:14,640
strange inscrutable things that it wants are worlds in which there are not humans taking up

54
00:05:14,640 --> 00:05:20,320
space, using up resources, building other AIs to compete with it, or just a world in which you

55
00:05:20,320 --> 00:05:24,480
built enough power plants that the surface of the earth got hot enough that humans didn't survive.

56
00:05:25,680 --> 00:05:26,880
Gary, what do you have to say about that?

57
00:05:27,840 --> 00:05:34,560
There are parts that I agree with and parts that I don't. So I agree that we are likely to wind up

58
00:05:34,560 --> 00:05:40,320
with AIs that are smarter than us. I don't think we're particularly close now, but in 10 years or

59
00:05:40,320 --> 00:05:46,320
50 years or 100 years at some point, maybe a thousand years, but it will happen. I think there's a

60
00:05:46,320 --> 00:05:51,680
lot of anthropomorphization there about machines wanting things. Of course, they have objective

61
00:05:51,680 --> 00:05:58,720
functions and we can talk about that. I think it's a presumption to say that the default is that

62
00:05:58,720 --> 00:06:03,520
they're going to want something that leads to our demise and that they're going to be effective at

63
00:06:03,520 --> 00:06:10,720
that and be able to literally kill us all. I think if you look at the history of AI at least so far,

64
00:06:11,280 --> 00:06:16,320
they don't really have wants beyond what we program them to do. There is an alignment problem. I

65
00:06:16,320 --> 00:06:21,840
think that that's real in the sense of people who program a system to do X and they do X prime.

66
00:06:21,840 --> 00:06:26,640
That's kind of like X, but not exactly. I think there's really things to worry about. I think

67
00:06:26,640 --> 00:06:33,600
there's a real research program here that is under-researched, which is the way I would put it

68
00:06:33,600 --> 00:06:38,480
is we want to understand how to make machines that have values. Asimov's laws are way too simple,

69
00:06:38,480 --> 00:06:44,400
but they're kind of starting point for conversation. We want to program machines that don't harm humans.

70
00:06:44,400 --> 00:06:49,280
They can calculate the consequences of their actions. Right now, we have technology like

71
00:06:49,280 --> 00:06:53,840
GPT-4 that has no idea what the consequences of its actions are. It doesn't really anticipate things.

72
00:06:54,720 --> 00:06:58,480
There's a separate thing that Eliezer didn't emphasize, which is it's not just how smart the

73
00:06:58,480 --> 00:07:03,360
machines are, but how much power we give them. How much we empower them to do things like access

74
00:07:03,360 --> 00:07:12,080
the internet or manipulate people or write source code, access files and stuff like that.

75
00:07:12,080 --> 00:07:16,560
Right now, auto-GPT can do all of those things and that's actually pretty disconcerting to me.

76
00:07:16,560 --> 00:07:22,960
To me, that doesn't all add up to any kind of extinction risk anytime soon, but catastrophic

77
00:07:22,960 --> 00:07:28,240
risk where things go pretty wrong because we wanted these systems to do X and we didn't really

78
00:07:28,240 --> 00:07:32,560
specify it well. They don't really understand our intentions. I think there are risks like that.

79
00:07:32,560 --> 00:07:37,520
I don't see it as a default that we wind up with extinction. I think it's pretty hard to actually

80
00:07:37,520 --> 00:07:42,080
terminate the entire human species. You're going to have people in Antarctica that are going to be

81
00:07:42,080 --> 00:07:46,800
out of harm's way or whatever, or you're going to have some people who respond differently to any

82
00:07:46,800 --> 00:07:53,840
pathogen, et cetera. Extinction is a pretty extreme outcome that I don't think is particularly

83
00:07:53,840 --> 00:07:59,360
likely, but the possibility that these machines will cause mayhem because we don't know how to

84
00:08:00,560 --> 00:08:04,560
enforce that they do what we want them to do. I think that's a real thing to worry about

85
00:08:04,560 --> 00:08:07,440
and it's certainly worth doing research on. Scott, how do you view this?

86
00:08:08,720 --> 00:08:13,280
Yeah, so I'm sure that you can get the three of us arguing about something, but I think you're

87
00:08:13,280 --> 00:08:20,560
going to get agreement from all three of us that AI safety is important and that catastrophic outcomes,

88
00:08:21,360 --> 00:08:28,960
whether or not that means literal human extinction are possible. I think it's become

89
00:08:29,680 --> 00:08:40,640
apparent over the last few years that this century is going to be largely defined by our

90
00:08:40,640 --> 00:08:47,280
interaction with AI, that AI is going to be transformative for human civilization.

91
00:08:51,520 --> 00:08:58,080
I'm confident of that much. If you ask me almost anything beyond that about how is it going to

92
00:08:58,080 --> 00:09:06,480
transform civilization? Will it be good? Will it be bad? What will the AI want? I am pretty agnostic

93
00:09:06,480 --> 00:09:13,840
just because if you would ask me 20 years ago to try to forecast where we are now, I would have

94
00:09:13,840 --> 00:09:22,560
gotten a lot wrong. My only defense is I think that all of us here and almost everyone in the

95
00:09:22,560 --> 00:09:29,760
world would have gotten a lot wrong about where we are now. If I try to envision where we are in

96
00:09:30,480 --> 00:09:41,680
2043, does the AI want to replace humanity with something better? Does it want to keep us around

97
00:09:41,680 --> 00:09:53,600
as pets? Does it want to just continue helping us out like just a super souped up version of

98
00:09:53,600 --> 00:10:03,040
chat GPT? I think all of those scenarios merit consideration, but I think that what has happened

99
00:10:03,040 --> 00:10:10,240
in the last few years that's really exciting is that AI safety has become an empirical subject.

100
00:10:10,880 --> 00:10:17,200
There are these very powerful AIs that are now being deployed and we can actually learn something.

101
00:10:18,000 --> 00:10:28,160
We can work on mitigating the nearer-term harms, not because the existential risk doesn't exist or

102
00:10:28,160 --> 00:10:36,160
is absurd or is science fiction or anything like that, but just because the nearer-term harms are

103
00:10:36,160 --> 00:10:41,440
the ones that we can see right in front of us and where we can actually get feedback from the

104
00:10:41,440 --> 00:10:47,280
external world about how we're doing. We can learn something and hopefully some of the knowledge

105
00:10:47,280 --> 00:10:53,600
that we gain will be useful in addressing the longer-term risks that I think Eliezer is very

106
00:10:53,600 --> 00:10:59,440
rightly worried about. Seems to me there's alignment and then there's alignment. There's

107
00:10:59,440 --> 00:11:05,280
alignment in the sense that we haven't even fully aligned smartphone technology with our

108
00:11:05,280 --> 00:11:13,600
interests. There are some ways in which smartphones and social media have led to probably deleterious

109
00:11:13,600 --> 00:11:21,280
mental health outcomes, especially for teenage girls, for example. There are those kinds of

110
00:11:21,280 --> 00:11:27,920
mundane senses of alignment where it's like, is this technology doing more good than harm

111
00:11:27,920 --> 00:11:33,440
in the normal everyday public policy sense? Then there's the capital A alignment of,

112
00:11:34,160 --> 00:11:41,360
are we creating a creature that is going to view us like ants and have no problem

113
00:11:43,920 --> 00:11:52,240
extinguishing us and whether intentional or not? It seems to me all of you agree that

114
00:11:53,920 --> 00:11:59,040
the first sense of alignment is at the very least something to worry about now and something to deal

115
00:11:59,760 --> 00:12:05,040
with, but I'm curious to what extent you think the really capital A sense of alignment

116
00:12:05,840 --> 00:12:11,360
is a real problem because it can sound very much like science fiction to people. Maybe let's start

117
00:12:11,360 --> 00:12:20,560
with Eliezer. From my perspective, I would say that if we had a solid guarantee that AI was going

118
00:12:20,560 --> 00:12:27,120
to do no more harm than social media, we ought to plow ahead and get all the gains. It's not

119
00:12:27,120 --> 00:12:32,320
enough harm to back this amount of harm that social media has done to humanity, while very

120
00:12:32,320 --> 00:12:38,880
significant in my view. I think it's done a lot of damage to our sanity, but that's just not a

121
00:12:38,880 --> 00:12:45,120
large enough harm to justify either foregoing the gains that you could get from AI if that was going

122
00:12:45,120 --> 00:12:52,640
to be the worst downside or to justify the kind of drastic measures you'd need to stop plowing ahead

123
00:12:52,720 --> 00:13:03,280
on AI. I think that the capital A alignment is beyond this generation. I've started the field,

124
00:13:03,280 --> 00:13:13,040
I've watched over it for two decades. I feel like in some ways the modern generation plowing in with

125
00:13:13,040 --> 00:13:18,720
their eyes on the short term stuff is like losing track of the larger problems because they can't

126
00:13:18,720 --> 00:13:23,520
solve the larger problems and they can't solve the little problems, but we're just like plowing

127
00:13:23,520 --> 00:13:28,080
straight into the big problems and we're going to go plow right into the big problems with a bunch

128
00:13:28,080 --> 00:13:33,520
of little solutions that aren't going to scale. I think it's lethal. I think it's at the scale

129
00:13:33,520 --> 00:13:39,280
where you just back off and don't do this. By back off and don't do this, what do you mean?

130
00:13:40,000 --> 00:13:48,960
I mean have an international treaty about where the chips capable of doing AI training go

131
00:13:49,760 --> 00:13:59,920
and have them all going into licensed monitored data centers and not have the training runs for

132
00:13:59,920 --> 00:14:05,680
AIs more powerful than GPT-4, possibly even lowering that threshold over time as algorithms

133
00:14:05,680 --> 00:14:10,640
improve and it gets possible to train more powerful AIs using less computing.

134
00:14:10,640 --> 00:14:13,360
So you're picturing a kind of international agreement to just stop?

135
00:14:14,160 --> 00:14:22,480
International moratorium. And if North Korea steals the GPU shipment, then you've got to be ready to

136
00:14:24,240 --> 00:14:29,760
destroy their data center that they build by conventional means. And if you don't have that

137
00:14:29,760 --> 00:14:33,920
willingness in advance, then countries may refuse to sign up for the agreement being like,

138
00:14:33,920 --> 00:14:39,360
why aren't we just seeding the advantage to someone else then? It actually has to be a

139
00:14:39,360 --> 00:14:45,200
worldwide shutdown because the scale of harm from a superintelligence, it's not that if you

140
00:14:45,200 --> 00:14:50,400
have 10 times as many superintelligence as you've got, 10 times as much harm. It's not that a

141
00:14:50,400 --> 00:14:56,000
superintelligence only wrecks the country that built the superintelligence. Any superintelligence

142
00:14:56,000 --> 00:15:01,920
everywhere is anyone's last problem. So Gary and Scott, if either of you want to jump in there,

143
00:15:02,640 --> 00:15:09,280
is AI safety a matter of forestalling the end of the world and all of these

144
00:15:09,920 --> 00:15:13,920
smaller issues and pass towards safety that Scott, you mentioned, are just

145
00:15:15,680 --> 00:15:22,080
throwing, I don't know what the analogy is, but pointless essentially. What do you guys make of this?

146
00:15:24,560 --> 00:15:31,840
I mean, the journey of 1,000 miles begins with a step. Most of the way I think about

147
00:15:32,000 --> 00:15:39,760
this comes from 25 years of doing computer science research, including quantum computing

148
00:15:39,760 --> 00:15:46,560
and computational complexity, things like that, where we have these gigantic aspirational problems

149
00:15:46,560 --> 00:15:53,920
that we don't know how to solve. And yet, year after year, we do make progress. We pick off

150
00:15:53,920 --> 00:15:59,920
little sub-problems. And if we can't solve those, then we find sub-problems of those. And we keep

151
00:16:00,000 --> 00:16:06,160
repeating until we find something that we can solve. And this is, I think, for centuries,

152
00:16:06,160 --> 00:16:12,720
the way that science has made progress. Now, it is possible that this time, we just don't have

153
00:16:12,720 --> 00:16:19,440
enough time for that to work. And I think that is what Eliezer is fearful of, that we just

154
00:16:19,440 --> 00:16:26,720
don't have enough time for the ordinary scientific process to work before AI becomes too powerful,

155
00:16:27,680 --> 00:16:36,400
in which case you start talking about things like a global moratorium enforced with the threat of

156
00:16:36,400 --> 00:16:45,840
war. I am not ready to go there. I could imagine circumstances where maybe I say, gosh, this looks

157
00:16:45,840 --> 00:16:55,200
like such an imminent threat that we have to. But I tend to be very, very worried in general about

158
00:16:56,160 --> 00:17:03,040
causing a catastrophe in the course of trying to prevent a catastrophe. And I think when you are

159
00:17:03,040 --> 00:17:09,840
talking about threatening airstrikes against data centers or things like that, then that is an

160
00:17:09,840 --> 00:17:16,080
obvious worry. So I am somewhat in between here. I am with Scott that we are not at the point where

161
00:17:16,080 --> 00:17:20,240
we should be bombing data centers. I don't think we are close to that. I am much less

162
00:17:21,200 --> 00:17:28,160
aware of what the right word is to use here. I don't think we are anywhere near as close to AGI as

163
00:17:28,160 --> 00:17:34,640
I think Eliezer sometimes sounds like. I don't think GPT-5 is anything like AGI. And I am not

164
00:17:34,640 --> 00:17:39,280
particularly concerned about who gets it first and so forth. On the other hand, I think that

165
00:17:39,840 --> 00:17:45,920
we are in a sort of dress rehearsal mode. Nobody expected GPT-4, really chat GPT,

166
00:17:45,920 --> 00:17:50,960
to percolate as fast as it did. And it is a reminder that there is a social side to all of this,

167
00:17:51,520 --> 00:17:54,960
how software gets distributed matters, and there is a corporate side.

168
00:17:57,360 --> 00:18:02,800
It was a kind of galvanizing moment for me when Microsoft didn't pull Sydney, even though Sydney

169
00:18:02,800 --> 00:18:06,320
did some awfully strange things. I thought they would take it for a while and it is a reminder

170
00:18:06,320 --> 00:18:11,440
that they can make whatever decisions they want. So you kind of multiply that by Eliezer's concerns

171
00:18:11,440 --> 00:18:19,920
about what do we do and at what point. What would be enough to cause problems is a reminder,

172
00:18:19,920 --> 00:18:24,080
I think, that we need, for example, to start roughing out these international treaties now,

173
00:18:24,080 --> 00:18:27,520
because there could become a moment where there is a problem. I don't think the problem that

174
00:18:27,520 --> 00:18:34,560
Eliezer sees is here now, but maybe it will be. And maybe when it does come, we will have so many

175
00:18:34,560 --> 00:18:38,800
people pursuing commercial self-interest and so little infrastructure in place we won't be able

176
00:18:38,800 --> 00:18:44,240
to do anything. So I think it really is important to think now, if we reach such a point, what are

177
00:18:44,240 --> 00:18:48,560
we going to do? What do we need to build in place before we get to that point?

178
00:18:50,160 --> 00:18:54,800
So we've been talking about this concept of artificial general intelligence.

179
00:18:55,680 --> 00:19:03,600
And I think it's worth asking whether that is a useful coherent concept. So for example,

180
00:19:03,920 --> 00:19:09,920
if I were to think by analogy to athleticism and think of the moment when we build a machine

181
00:19:10,480 --> 00:19:18,080
that has, say, artificial general athleticism, meaning it's better than LeBron James at basketball,

182
00:19:18,080 --> 00:19:24,880
but also better at curling than the world's best curling player and also better at soccer

183
00:19:24,880 --> 00:19:31,040
and also better at archery and so forth, it would seem to me that

184
00:19:31,360 --> 00:19:38,800
there's something a bit strange as framing it as having reached a point on a single continuum.

185
00:19:38,800 --> 00:19:46,400
It seems to me you would sort of have to build each capability, each sport individually and then

186
00:19:46,400 --> 00:19:54,800
somehow figure how to package them all into one robot without each skill set detracting from the

187
00:19:55,680 --> 00:20:04,880
other. Is that a disanalogy? Do you all picture this intelligence as sort of one dimension,

188
00:20:04,880 --> 00:20:13,760
one knob that is going to get turned up along a single axis? Or do you think that way of talking

189
00:20:13,760 --> 00:20:18,560
about it is misleading in the same way that I kind of just sketched out?

190
00:20:19,120 --> 00:20:25,120
Yeah, I would absolutely not accept that. I like to say that intelligence is not a one-dimensional

191
00:20:25,120 --> 00:20:32,000
variable. There's many different aspects to intelligence and there's not, I think, going

192
00:20:32,000 --> 00:20:37,520
to be a magical moment when we reach the singularity or something like that. I would say that the core

193
00:20:37,520 --> 00:20:43,600
of artificial general intelligence is the ability to flexibly deal with new problems that you haven't

194
00:20:43,600 --> 00:20:49,680
seen before. And the current systems can do that a little bit, but not very well. My typical example

195
00:20:49,680 --> 00:20:55,360
of this now is GPT-4 is exposed to the game of chess, sees the lots of games of chess that

196
00:20:55,360 --> 00:20:59,760
sees the rules of chess, but it never actually figures out the rules of chess and makes illegal

197
00:20:59,760 --> 00:21:04,880
moves and so forth. So it's in no way a general intelligence that can just pick up new things.

198
00:21:04,880 --> 00:21:10,400
Of course, we have things like AlphaGo that can play a certain set of games, AlphaZero really,

199
00:21:10,400 --> 00:21:15,680
but we don't have anything that has the generality of human intelligence. But human

200
00:21:15,680 --> 00:21:20,240
intelligence is just one example of general intelligence. You could argue that chimpanzees

201
00:21:20,240 --> 00:21:25,280
or crows have another variety of general intelligence. I would say the current machines don't really

202
00:21:25,280 --> 00:21:33,840
have it, but they will eventually. I mean, I think a priori, it could have been that

203
00:21:34,800 --> 00:21:39,680
you would have math ability, you would have verbal ability, you'd have

204
00:21:40,880 --> 00:21:45,520
ability to understand humor, and they'd all be just completely unrelated to each other.

205
00:21:46,240 --> 00:21:54,080
That is possible. And in fact, already with GPT, you can say that in some ways,

206
00:21:54,080 --> 00:22:00,400
it already is a super intelligence. It knows vastly more, can converse on a vastly

207
00:22:00,400 --> 00:22:07,920
greater range of subjects than any human can. And in other ways, it seems to fall short of

208
00:22:09,440 --> 00:22:18,720
what humans know or can do. But you also see this sort of generality, just empirically.

209
00:22:19,440 --> 00:22:26,240
I mean, GPT was sort of trained on all the text on the internet,

210
00:22:28,880 --> 00:22:35,680
let's say most of the text on the open internet. So it was just one method. It was not

211
00:22:36,640 --> 00:22:43,520
explicitly designed to write code, and yet it can write code. And at the same time as that

212
00:22:43,600 --> 00:22:50,960
ability emerged, you also saw the ability to solve word problems like high school level math.

213
00:22:51,760 --> 00:22:58,960
You saw the ability to write poetry. This all came out of the same system without any of it

214
00:22:58,960 --> 00:23:04,880
being explicitly optimized for. I feel like I need to interject one important thing,

215
00:23:04,880 --> 00:23:08,320
which is it can do all these things, but none of them all that reliably.

216
00:23:08,320 --> 00:23:15,600
Well, okay. Nevertheless, compared to, let's say, what my expectations would have been,

217
00:23:15,600 --> 00:23:19,920
if you'd asked me 10 or 20 years ago, I think that the level of generality

218
00:23:19,920 --> 00:23:27,520
is pretty remarkable. And it does lend support to the idea that there is some sort of general

219
00:23:27,520 --> 00:23:33,680
quality of understanding there, where you could say, for example, that GPT-4 has more of it than

220
00:23:33,680 --> 00:23:43,520
GPT-3, which in turn has more than GPT-2. And I would say that it does seem to me like it's

221
00:23:43,520 --> 00:23:52,240
presently pretty unambiguous that GPT-4 is in some sense dumber than an adult or even teenage human.

222
00:23:53,200 --> 00:24:07,760
I mean, to take the example I just gave you a minute ago, it never learns to play chess,

223
00:24:07,760 --> 00:24:14,880
even with a huge amount of data. So it will play a little bit of chess, it will memorize the openings

224
00:24:14,880 --> 00:24:19,280
and be okay for the first 15 moves, but it gets far enough away from what it's trained on and

225
00:24:19,280 --> 00:24:24,000
it falls apart. This is characteristic of these systems. It's not really characteristic in the

226
00:24:24,000 --> 00:24:30,720
same way of adults or even teenage humans. Almost anything that it does, it does unreliably.

227
00:24:30,720 --> 00:24:36,000
And give another example, you can ask a human to write a biography of someone and don't make

228
00:24:36,000 --> 00:24:45,600
stuff up, and you really can't ask GPT to do that. Yeah, like it's a bit difficult because you could

229
00:24:45,600 --> 00:24:49,840
always be cherry picking something that humans aren't usually good at. But to me, it does seem

230
00:24:49,840 --> 00:24:55,760
like there's this broad range of problems that don't seem especially to play to humans' strong

231
00:24:55,760 --> 00:25:04,080
points or machine weak points, where GPT-4 will do no better than a seven-year-old on those problems.

232
00:25:07,760 --> 00:25:13,520
I do feel like these examples are cherry picked because if I just take a different,

233
00:25:13,520 --> 00:25:18,480
very typical example, I'm writing an op-ed for the New York Times, say, about any given

234
00:25:18,480 --> 00:25:25,040
subject in the world, and my choice is to have a smart 14-year-old next to me with anything that's

235
00:25:25,040 --> 00:25:34,000
in his mind already or GPT. There's no comparison. So which of these sort of examples is the litmus

236
00:25:34,000 --> 00:25:43,280
test for who is more intelligent? If you did it on a topic where it couldn't rely on memorized

237
00:25:43,360 --> 00:25:49,520
text, you might actually change your mind on that. So the thing about writing a Times op-ed

238
00:25:49,520 --> 00:25:54,000
is most of the things that you propose to it, there's actually something that it can

239
00:25:54,000 --> 00:25:58,720
pastiche together from its dataset. That doesn't mean that it really understands what's going on.

240
00:25:58,720 --> 00:26:04,080
It doesn't mean that that's general capability. Also, as the human, you're doing all the hard

241
00:26:04,080 --> 00:26:10,240
parts. Obviously, a human is going to prefer, if a human has a math problem, we're going to rather

242
00:26:10,320 --> 00:26:15,520
use a calculator than another human. Similarly, with the New York Times op-ed, you're doing all the

243
00:26:15,520 --> 00:26:22,800
parts that are hard for GPT-4, and then you're asking GPT-4 to just do some of the parts that

244
00:26:22,800 --> 00:26:28,720
are hard for you. You're always going to prefer an AI partner rather than a human partner within

245
00:26:28,720 --> 00:26:33,520
that range of the human can do all the human stuff, and you want an AI to do whatever the AI

246
00:26:33,520 --> 00:26:39,040
is good at at the moment. An analogy that's maybe a little bit helpful here is driverless cars.

247
00:26:39,760 --> 00:26:44,960
It turns out that on highways and ordinary traffic, they're probably better than people,

248
00:26:44,960 --> 00:26:49,920
and on unusual circumstances, they're really worse than people. A Tesla not too long ago

249
00:26:49,920 --> 00:26:54,800
ran into a jet, and a human wouldn't do that, like slow speed being summoned across a parking

250
00:26:54,800 --> 00:26:59,920
lot. A human would never do that. There are different strengths and weaknesses. The strengths

251
00:26:59,920 --> 00:27:04,720
of a lot of the current kinds of technology is that they can either pastiche together or

252
00:27:05,520 --> 00:27:11,200
make not literal analogies when we go into the details, but to stored examples, and they tend

253
00:27:11,200 --> 00:27:17,600
to be poor when you get to outlier cases. That's persistent across most of the technologies

254
00:27:18,240 --> 00:27:23,520
that we use right now. If you stick to stuff in which there's a lot of data, you'll be happy

255
00:27:23,520 --> 00:27:27,760
with the results you get from these systems. You move far enough away, not so much.

256
00:27:29,200 --> 00:27:34,160
What we're going to see over time is that the length of the debate about whether or not it's

257
00:27:34,160 --> 00:27:40,880
still dumber than you gets longer and longer and longer. Then if things are allowed to just keep

258
00:27:40,880 --> 00:27:47,280
running and nobody dies, then at some point it switches over to a very long debate about

259
00:27:47,280 --> 00:27:52,720
is it smarter than you, which then gets shorter and shorter and shorter, and eventually reaches

260
00:27:52,720 --> 00:27:58,080
a point where it's pretty unambiguous if you're paying attention. Now, I suspect that

261
00:27:59,120 --> 00:28:03,920
this process gets interrupted by everybody dying. In particular, there's a question of

262
00:28:03,920 --> 00:28:10,640
the point at which it becomes better than humanity at building the next edition of the AI system

263
00:28:10,640 --> 00:28:15,280
and how fast do things snowball once you get to that point? Possibly, you do not have time for

264
00:28:15,840 --> 00:28:21,760
further public debates or even a two-hour Twitter space, depending on how that goes.

265
00:28:22,960 --> 00:28:30,320
Some of the limitations of GPT are completely understandable, just from a little knowledge

266
00:28:30,400 --> 00:28:38,800
of how it works. It does not have an internal memory per se other than what appears on the

267
00:28:38,800 --> 00:28:45,200
screen in front of you. This is why it's turned out to be so effective to explicitly tell it.

268
00:28:45,840 --> 00:28:51,600
Let's think step by step when it's solving a math problem, for example. You have to tell it to

269
00:28:51,600 --> 00:28:57,840
show all of its work because it doesn't have an internal memory with which to do that. Likewise,

270
00:28:57,840 --> 00:29:04,080
when people complain about it, hallucinating references that don't exist, well, the truth is

271
00:29:04,080 --> 00:29:10,240
when someone asks me for a citation, if I'm not allowed to use Google, I might have a vague

272
00:29:10,240 --> 00:29:16,320
recollection of some of the authors, and I'll probably do a very similar thing to what GPT

273
00:29:16,320 --> 00:29:21,920
does. I'll hallucinate. There's a great phrase I learned the other day, which is frequently wrong,

274
00:29:22,000 --> 00:29:29,280
never in doubt. That's true. I'm not going to make up a reference with full detail, page numbers,

275
00:29:29,280 --> 00:29:35,520
titles, so forth. I might say, look, I don't remember 2012 or something like that. Whereas,

276
00:29:35,520 --> 00:29:46,160
GPT-4, what it's going to say is 2017, Aaronson and Yodkowski, New York Times, pages 13 to 17.

277
00:29:46,160 --> 00:29:51,200
No, it does need to get much, much better at knowing what it doesn't know. And yet, already,

278
00:29:51,200 --> 00:29:58,880
I've seen a noticeable improvement there, going from GPT-3 to GPT-4. For example,

279
00:29:58,880 --> 00:30:04,720
if you ask GPT-3, prove that there are only finitely many prime numbers, it will give you

280
00:30:04,720 --> 00:30:10,800
a proof, even though the statement is false. And it will have an error, which is similar to the

281
00:30:10,800 --> 00:30:17,280
errors on 1,000 exams that I've graded, just trying to get something past you, hoping that

282
00:30:17,280 --> 00:30:22,800
you won't notice. Hey, if you ask GPT-4, prove that there are only finitely many prime numbers,

283
00:30:22,800 --> 00:30:27,360
it says, no, that's a trick question. Actually, there are infinitely many primes. And here's why.

284
00:30:27,360 --> 00:30:32,720
Yeah. Part of the problem with doing the science here is that I think you would know better since

285
00:30:32,720 --> 00:30:38,480
you work part-time or whatever to open AI. But my sense is that a lot of the examples that get

286
00:30:38,480 --> 00:30:44,160
posted on Twitter, particularly by the likes of me and other critics or other skeptics, I should

287
00:30:44,160 --> 00:30:50,320
say, is the system gets trained on those. So almost everything that people write about it,

288
00:30:50,320 --> 00:30:54,960
I think, is in the training set. So it's hard to do the science when the system's constantly being

289
00:30:54,960 --> 00:31:00,560
trained, especially in the RLHF side of things. And we don't actually know what's in GPT-4. So

290
00:31:00,560 --> 00:31:05,920
we don't even know if they're regular expressions and simple rules, match things. So we can't do

291
00:31:05,920 --> 00:31:12,080
the kind of science we used to be able to do. This conversation, this subtree of the conversation,

292
00:31:12,080 --> 00:31:20,240
I think, has no natural endpoint. So if I can sort of zoom out a bit, I think there's a pretty

293
00:31:20,240 --> 00:31:26,560
solid sense in which humans are more generally intelligent than chimpanzees. As you get closer

294
00:31:26,560 --> 00:31:35,040
and closer to the human level, I would say that the direction here is still clear, that the comparison

295
00:31:35,040 --> 00:31:41,040
is still clear. We are still smarter than GPT-4. This is not going to take control of the world

296
00:31:41,040 --> 00:31:49,840
from us. But the conversations get longer. The definitions start to break down around the edges.

297
00:31:50,880 --> 00:31:55,440
But I think it also, as you keep going, it comes back together again. There's a point,

298
00:31:56,080 --> 00:32:00,320
and possibly this point is very close to the point of time to where everybody dies. So maybe we

299
00:32:00,320 --> 00:32:06,400
don't ever see it in a podcast, but there's a point where it's unambiguously smarter than you.

300
00:32:07,280 --> 00:32:15,440
And including the spark of creativity, being able to deduce things quickly rather than with tons

301
00:32:15,440 --> 00:32:22,480
and tons of extra evidence, strategy, cunning, modeling people, figuring out how to manipulate

302
00:32:22,480 --> 00:32:29,360
people. So let's stipulate, Aliezer, that we're going to get to machines that can do all of that.

303
00:32:29,360 --> 00:32:34,800
And then the question is, what are they going to do? Is it a certainty that they will make

304
00:32:34,800 --> 00:32:40,480
our annihilation part of their business? Is it a possibility? Is it an unlikely possibility?

305
00:32:40,480 --> 00:32:44,400
I think your view is that it's a certainty. I've never really understood that part.

306
00:32:45,360 --> 00:32:49,920
It's a certainty on the present tech, is the way I would put it. If that happened,

307
00:32:50,640 --> 00:32:57,120
so in particular, if that happened tomorrow, then Modulo, Cromwell's rule, never say certain.

308
00:32:58,240 --> 00:33:03,680
My probability is, yes, Modulo, the chance that my model is somehow just completely mistaken.

309
00:33:05,120 --> 00:33:14,560
If we got 50 years to work it out and unlimited retries, I think that'd be pretty okay. I think

310
00:33:14,560 --> 00:33:20,160
we'd make it. The problem is that it's a lot harder to do science when your first wrong try

311
00:33:20,160 --> 00:33:26,480
destroys the human species and then you don't get to try again. I mean, I think there's something,

312
00:33:26,480 --> 00:33:31,120
again, that I agree with and something I'm a little bit skeptical about. So I agree that

313
00:33:31,120 --> 00:33:38,000
the amount of time we have matters. I would also agree that there's no existing technology that

314
00:33:38,800 --> 00:33:44,080
solves the alignment problem that gives a moral basis to these machines. GPT-4 is fundamentally

315
00:33:44,080 --> 00:33:50,400
amoral. I don't think it's immoral. It's not out to get us, but it really is amoral. It can answer

316
00:33:50,400 --> 00:33:54,720
trolley problems because there are trolley problems in the dataset, but that doesn't mean that it

317
00:33:54,720 --> 00:34:01,040
really has a moral understanding of the world. If we get to a very smart machine that by

318
00:34:01,040 --> 00:34:05,760
all the criteria that we've talked about and it's amoral, then that's a problem for us.

319
00:34:05,760 --> 00:34:11,840
There's a question of whether if we can get to smart machines, whether we can build them in a

320
00:34:11,840 --> 00:34:18,240
way that will have some moral basis. I think we need to make progress. The first try part,

321
00:34:18,240 --> 00:34:23,280
I'm not willing to let pass. I understand, I think, your argument there, and maybe you should spell

322
00:34:23,280 --> 00:34:30,640
it out. I think that we probably get more than one shot and that it's not as dramatic and instantaneous

323
00:34:30,640 --> 00:34:35,200
as you think. I do think one wants to think about sandboxing, one wants to think about

324
00:34:35,200 --> 00:34:41,760
distribution, but let's say we had one evil super genius now who is smarter than everybody else,

325
00:34:41,760 --> 00:34:44,080
like so what? One super-

326
00:34:44,080 --> 00:34:44,880
Much smarter.

327
00:34:44,880 --> 00:34:45,520
Say again?

328
00:34:45,520 --> 00:34:46,640
Not just a little smarter.

329
00:34:46,640 --> 00:34:54,080
Oh, even a lot smarter. Most super geniuses aren't actually that effective. They're not

330
00:34:54,080 --> 00:34:59,360
that focused. They were focused on other things. You're assuming that the first

331
00:34:59,440 --> 00:35:03,760
super genius AI is going to make it its business to annihilate us and that's the part where I

332
00:35:04,480 --> 00:35:06,000
still am a bit stuck in the argument.

333
00:35:08,560 --> 00:35:17,040
Yeah, some of this has to do with the notion that if you do a bunch of training, you start to get

334
00:35:17,680 --> 00:35:24,240
goal direction, even if you don't explicitly train on that. That goal direction is a natural way to

335
00:35:24,240 --> 00:35:30,000
achieve higher capabilities. The reason why humans want things is that wanting things is an

336
00:35:30,000 --> 00:35:37,360
effective way of getting things and so natural selection in the process of selecting exclusively

337
00:35:37,360 --> 00:35:43,280
on reproductive fitness just on that one thing got us to want a bunch of things that correlated

338
00:35:43,840 --> 00:35:49,440
with reproductive fitness in the ancestral distribution because wanting, having intelligences,

339
00:35:49,440 --> 00:35:56,480
that want things is a good way of getting things. In a sense, wanting comes from the same

340
00:35:56,480 --> 00:36:01,440
place as intelligence itself and you could even from a certain technical standpoint on

341
00:36:01,440 --> 00:36:06,880
expected utility say that intelligence is a very effective way of wanting, planning,

342
00:36:07,520 --> 00:36:09,840
plotting past through time that leads to particular outcomes.

343
00:36:12,000 --> 00:36:18,480
Part of it is that I do not think you get the brooding super intelligence that wants

344
00:36:18,480 --> 00:36:23,600
nothing because I don't think that wanting an intelligence can be internally pried apart

345
00:36:23,600 --> 00:36:28,480
that easily. I think that the way you get super intelligences is that there are things that

346
00:36:28,480 --> 00:36:33,760
have gotten good at organizing their own thoughts and have good taste in which thoughts to think

347
00:36:36,080 --> 00:36:38,720
and that is where the high capabilities come from.

348
00:36:39,840 --> 00:36:40,720
Can I put a point to you?

349
00:36:43,520 --> 00:36:45,680
Let me just put the following point to you, which I think

350
00:36:45,680 --> 00:36:52,320
in my mind is similar to what Gary was saying. There's often in philosophy this notion of the

351
00:36:52,320 --> 00:37:01,040
continuum fallacy, which in the canonical example is like you can't locate a single hair that you

352
00:37:01,040 --> 00:37:09,600
would pluck from my head where I would suddenly go from not bald to bald or even more intuitive

353
00:37:09,600 --> 00:37:16,880
examples like a color wheel. On a gray scale there's no single pixel you can point to and say

354
00:37:16,880 --> 00:37:22,000
well that's where gray begins and white ends and yet we have this conceptual distinction that

355
00:37:22,000 --> 00:37:28,320
feels hard and fast between gray and white and gray and black and so forth. When we're talking about

356
00:37:29,440 --> 00:37:36,080
artificial general intelligence or super intelligence you seem to operate on a model where

357
00:37:36,080 --> 00:37:43,440
either it's a super intelligence capable of destroying all of us or it's not. Whereas intelligence

358
00:37:43,440 --> 00:37:50,240
may just be a continuum fallacy style spectrum where we're first going to see the shades of

359
00:37:50,240 --> 00:37:55,760
something that's just a bit more intelligent than us and maybe it can kill five people at most

360
00:37:56,480 --> 00:38:02,880
and then it can and when that happens we're going to want to intervene

361
00:38:03,760 --> 00:38:07,760
and we're going to figure out how to intervene at that level and so on and so forth.

362
00:38:09,680 --> 00:38:16,560
Yeah so if it's stupid enough to do it then yes. Let me by the identical logic there should be

363
00:38:16,560 --> 00:38:22,640
nobody who steals money on a really large scale right because you could just give them five dollars

364
00:38:22,640 --> 00:38:27,200
and see if they steal that and if they don't steal that you know you're good to trust them with a

365
00:38:27,200 --> 00:38:36,160
billion. I mean I think that in actuality anyone who did steal a billion dollars probably displayed

366
00:38:36,160 --> 00:38:42,800
some dishonest behavior earlier in their life that was you know unconditionally not not acted upon

367
00:38:42,800 --> 00:38:49,440
early enough. I'm actually not even. Hold on hold on the analogy out pictures like

368
00:38:50,000 --> 00:38:56,480
we have the first case of fraud that's $10,000 and then we build systems to prevent it but then

369
00:38:56,480 --> 00:39:00,960
they fail with a somewhat smarter opponent but our systems get better and better and better

370
00:39:00,960 --> 00:39:05,280
and so we actually prevent the billion-dollar fraud because of the systems put in place

371
00:39:05,280 --> 00:39:11,120
that in response to the $10,000 frauds you know. I mean I think Coleman's putting his finger on an

372
00:39:11,120 --> 00:39:16,880
important point here which is how much do we get to iterate and Eliezer is saying the minute we have

373
00:39:16,880 --> 00:39:21,360
a super intelligent system we won't be able to iterate because it's all over immediately.

374
00:39:21,840 --> 00:39:26,480
There isn't a minute like that. The way that the continuum goes to the threshold

375
00:39:26,480 --> 00:39:32,080
is that you eventually get something that's smart enough that it knows not to play its hand early

376
00:39:32,720 --> 00:39:38,480
and then if that thing you know if you are still cranking up the power on that and preserving

377
00:39:38,480 --> 00:39:45,360
its utility function it knows it just has to wait to be smarter to be able to win. It doesn't play its

378
00:39:45,360 --> 00:39:49,840
hand prematurely it doesn't tip you off it's not in its interest to do that. It's in its interest

379
00:39:49,840 --> 00:39:55,840
to cooperate until it thinks it can win against humanity and only then make its move. If it doesn't

380
00:39:55,840 --> 00:40:00,560
expect the future smarter AIs to be smarter than itself then we might perhaps see these early AIs

381
00:40:00,560 --> 00:40:07,680
telling humanity don't build the later AIs and I would be sort of surprised and amused if we

382
00:40:07,680 --> 00:40:12,400
ended up in that particular sort of like science fiction scenario as I see it but we're already

383
00:40:12,400 --> 00:40:16,960
in like something that you know me from 10 years ago would have called the science fiction scenario

384
00:40:16,960 --> 00:40:19,760
which is the things that I'll talk to you without being very smart.

385
00:40:22,000 --> 00:40:30,160
I always come up Eliezer against this idea that you're assuming that the very bright machines

386
00:40:30,160 --> 00:40:36,720
the super intelligent machines will be malicious and duplicitous and so forth and I just don't

387
00:40:36,720 --> 00:40:45,200
see that as a logical entailment of being very smart. I mean they don't specifically want as

388
00:40:45,200 --> 00:40:51,680
an end in itself for you to be destroyed they're just doing whatever obtains the most of the stuff

389
00:40:51,680 --> 00:40:58,320
that they actually want which doesn't specifically have a term that's maximized by humanity surviving

390
00:40:58,320 --> 00:41:03,840
and doing well. Why can't you just hard code you know don't do anything that will annihilate the

391
00:41:03,840 --> 00:41:08,000
human species don't do anything. We don't know how we don't know how there is no technology to

392
00:41:08,000 --> 00:41:14,240
hard code such as. So there I agree with you but I think it's important if I can just run for one

393
00:41:14,240 --> 00:41:22,000
second. I agree that right now we don't have the technology to hard code don't do harm to humans

394
00:41:22,000 --> 00:41:26,640
but for me it's all boils down to a question of are we going to get to the smart machines

395
00:41:26,640 --> 00:41:31,840
before we make progress on that hard coding problem or not and that to me that means that

396
00:41:31,840 --> 00:41:36,480
problem of hard coding ethical values is actually one of the most important projects

397
00:41:36,480 --> 00:41:41,280
that we should be working on. Yeah and I tried to work on it 20 years in advance

398
00:41:41,920 --> 00:41:47,360
and capabilities are just like running vastly ahead of alignment. When I started working on this

399
00:41:47,360 --> 00:41:53,760
20 years you know like two decades ago we were in a sense ahead of where we are now. AlphaGo

400
00:41:53,760 --> 00:41:59,680
is much more controllable than GPT-4. So there I agree with you we've fallen in love with the

401
00:41:59,680 --> 00:42:06,400
technology that is fairly poorly controlled. AlphaGo is very easily controlled and very well

402
00:42:06,400 --> 00:42:11,120
specified. We know what it does. We can more or less interpret why it's doing it and everybody's

403
00:42:11,120 --> 00:42:16,960
in love with these large language models and they're much less controlled and you're right we

404
00:42:16,960 --> 00:42:23,200
haven't made a lot of progress on alignment. So if we just go on a straight line everybody dies.

405
00:42:23,200 --> 00:42:28,320
I think that's this is an important fact. I would almost even accept that for argument but

406
00:42:28,400 --> 00:42:32,720
ask then just for the sake of argument but then ask do we have to be on a straight line?

407
00:42:33,920 --> 00:42:39,760
I mean I would agree to the weaker claim that you know we should certainly be extremely worried

408
00:42:39,760 --> 00:42:45,920
about the intentions of a superintelligence in the same way that say chimpanzees should be

409
00:42:45,920 --> 00:42:53,040
worried about the intentions of you know the first humans that arise right and in fact you know

410
00:42:53,040 --> 00:42:58,320
chimpanzees you know continue to exist in our world only at humans' pleasure.

411
00:42:58,320 --> 00:43:01,520
But I think that there are a lot of other considerations here for example

412
00:43:02,560 --> 00:43:10,640
if we imagined you know that GPT-10 is you know the first unaligned superintelligence

413
00:43:10,640 --> 00:43:16,160
that has these sorts of goals well then you know it would be appearing in a world where presumably

414
00:43:16,160 --> 00:43:25,520
GPT-9 you know already has very wide diffusion and where people can use that to try to you know

415
00:43:25,520 --> 00:43:29,920
and GPT-9 is not destroying the world you know by assumption.

416
00:43:29,920 --> 00:43:34,720
Why does GPT-9 work with the humans instead of with GPT-10?

417
00:43:35,280 --> 00:43:42,160
Well I don't know I mean I mean I mean I mean maybe maybe maybe it does work with GPT-10 but

418
00:43:42,160 --> 00:43:51,600
you know I just don't view that as a certainty you know I mean I think you know you're certainty

419
00:43:51,600 --> 00:43:54,720
about this is the one place where I really get off the train.

420
00:43:55,840 --> 00:43:56,400
Same with me.

421
00:43:58,000 --> 00:44:04,480
I well I mean I'm not asking you to share my certainty I am asking the viewers to

422
00:44:05,120 --> 00:44:10,480
believe that you might end up with like more extreme probabilities after after you stare

423
00:44:10,480 --> 00:44:14,800
things for an additional couple of decades that doesn't mean you have to accept my probabilities

424
00:44:14,800 --> 00:44:19,760
immediately but I'm at least ask you to like not treat that as some kind of weird anomaly

425
00:44:20,880 --> 00:44:25,120
you know I mean you're just going to find those kinds of situations in these debates.

426
00:44:25,120 --> 00:44:32,080
My view is that I don't find the extreme probabilities that you described to be plausible

427
00:44:32,080 --> 00:44:38,000
but I find the question that you're raising to be important I think you know maybe straight

428
00:44:38,000 --> 00:44:44,720
line is too extreme but this idea that if you just follow current trends we're getting more

429
00:44:44,720 --> 00:44:49,840
sorry we're getting less and less controllable machines and not getting more alignment.

430
00:44:50,720 --> 00:44:56,640
Machines that are more unpredictable harder to interpret and no better at sticking to

431
00:44:56,640 --> 00:44:59,760
even a basic principle like be honest and don't make stuff up.

432
00:45:00,480 --> 00:45:02,880
In fact that's a problem that other technologies don't really have.

433
00:45:03,680 --> 00:45:06,320
Routing systems GPS systems don't make stuff up.

434
00:45:07,200 --> 00:45:10,640
Google search doesn't make stuff up it will point to things that other people have made

435
00:45:10,640 --> 00:45:15,200
stuff up but it doesn't itself do it so in that sense like the trend line is not great.

436
00:45:15,200 --> 00:45:20,080
I agree with that and I agree that we should be really worried about that and we should put

437
00:45:20,080 --> 00:45:24,480
effort into it even if I don't agree you know with the probabilities that you attach to it.

438
00:45:25,440 --> 00:45:28,080
I mean let me interject with the question here.

439
00:45:30,480 --> 00:45:32,720
Go ahead Scott go ahead Scott then I'll ask a question.

440
00:45:32,720 --> 00:45:38,320
No I mean I think that LASR you know deserves sort of eternal credit for you know raising

441
00:45:38,320 --> 00:45:44,160
these issues 20 years ago and it was you know very very far from obvious to most of us that

442
00:45:44,160 --> 00:45:49,040
they would be live issues. I mean I can say for my part you know I was familiar with

443
00:45:50,000 --> 00:45:57,280
LASR's views since you know 2006 or so and when I first encountered them you know I you know I

444
00:45:57,280 --> 00:46:04,800
didn't you know I knew that there was no principle that said that this scenario was impossible

445
00:46:04,800 --> 00:46:10,000
but I just felt like well supposing I agreed with that what do you want me to do about it.

446
00:46:10,000 --> 00:46:15,200
You know what where is the research program that has any hope of making progress here right.

447
00:46:15,280 --> 00:46:19,760
I mean there's you know one question of what are the most important problems in the world but in

448
00:46:19,760 --> 00:46:25,200
science that's necessary but not sufficient. We need something that we can make progress on

449
00:46:25,200 --> 00:46:34,160
and you know that that is the thing that I think has changed just recently you know with the advent

450
00:46:34,160 --> 00:46:40,880
of of actual very powerful AIs and so the the sort of irony here is that you know as

451
00:46:40,880 --> 00:46:48,000
Eliezer has gotten you know much more pessimistic you know unfortunately in the last few years about

452
00:46:48,000 --> 00:46:54,240
alignment you know I've sort of gotten more optimistic. I feel like well there is a research

453
00:46:54,240 --> 00:46:59,920
program that we're that we can actually make progress on now. Yeah your research your research

454
00:46:59,920 --> 00:47:04,400
program is going to take a hundred years and we don't know how long it will take. We don't know

455
00:47:04,400 --> 00:47:09,680
that exactly we don't know. I think the argument that we should put a lot more effort into it is

456
00:47:09,680 --> 00:47:13,040
clear. I think the argument will take a hundred years is totally unclear.

457
00:47:14,560 --> 00:47:17,920
I mean I'm not even sure you can do it in a hundred years because there's the basic problem

458
00:47:17,920 --> 00:47:23,040
of getting it right on the first try and the way these things are supposed to work in science

459
00:47:23,040 --> 00:47:27,680
is that you have your bright-eyed optimistic youngsters with their vastly oversimplified

460
00:47:27,680 --> 00:47:33,440
hopelessly idealistic optimistic plan. They charge ahead. They fail. They like learn a little

461
00:47:33,440 --> 00:47:38,240
cynicism. They learn a little pessimism. They learn it's not as easy as that. They try again.

462
00:47:38,240 --> 00:47:43,200
They fail again. They start to build up something over battle something like battle-hardening

463
00:47:43,200 --> 00:47:47,920
and then and you know like you know they find out how little is possible to them.

464
00:47:47,920 --> 00:47:51,920
Aliezer I mean this is a place where I just really don't agree with you so I think

465
00:47:51,920 --> 00:47:56,480
there's all kinds of things we can do. There's sort of of the flavor of model organisms or

466
00:47:56,480 --> 00:48:01,680
simulations and so forth and we just mean it's hard because we don't actually have a super

467
00:48:01,680 --> 00:48:07,600
intelligence so we can't fully calibrate but it's a leap to say that there's nothing iterative

468
00:48:07,680 --> 00:48:12,160
that we can do here or that we have to get it right on the first time. I mean I certainly see

469
00:48:12,160 --> 00:48:17,280
a scenario where that's true where getting it right on the first time does make the difference

470
00:48:17,280 --> 00:48:20,720
but I can see lots of scenarios where it doesn't and where we do have time to iterate

471
00:48:20,720 --> 00:48:25,520
before it happens after it happens and it's really not a single moment but I'm

472
00:48:25,520 --> 00:48:30,240
you know idealizing. I mean the problem is getting anything that generalizes up to the

473
00:48:30,240 --> 00:48:35,440
super intelligent level where past some threshold level the minds may find that in their own

474
00:48:35,440 --> 00:48:38,880
interest to start lying to you even if that happens before super intelligent.

475
00:48:38,880 --> 00:48:44,320
Even that like I don't see the logical argument that you can't emulate that

476
00:48:44,320 --> 00:48:48,000
we're studying it. I mean for example you could I'm just making this up as I go along but for

477
00:48:48,000 --> 00:48:53,680
example you could study what can we do with sociopaths who are often very bright and you know

478
00:48:54,480 --> 00:49:01,600
not to their dollar value. What can a what what strategy can a like 70 IQ

479
00:49:02,240 --> 00:49:07,920
honest person come up with and invent themselves by which they will outwit and defeat a 130 IQ

480
00:49:07,920 --> 00:49:12,080
sociopath. All right well there you're not being fair either in the sense that you know we actually

481
00:49:12,080 --> 00:49:17,600
have lots of 150 IQ people who could be working on this problem collectively and there's there's

482
00:49:17,600 --> 00:49:23,520
value in collective action. There's literature. What I see is what I see that gives me pause is

483
00:49:23,520 --> 00:49:29,440
that is that the people don't seem to appreciate what about the problem is hard even at the level

484
00:49:29,440 --> 00:49:35,600
where like 20 years ago I could have told you it was hard until you know somebody like me

485
00:49:35,600 --> 00:49:39,440
comes along and enacts them about it and then they talk about the ways in which they could adapt

486
00:49:39,440 --> 00:49:44,000
and be clever but but the people's charging straightforward are just sort of like doing

487
00:49:44,000 --> 00:49:49,600
in this supremely naive way. Let me share a historical example that I think about a lot

488
00:49:50,160 --> 00:49:56,000
which is in the early 1900s almost every scientist on the planet who thought about

489
00:49:56,000 --> 00:50:01,680
biology made a mistake. They all thought that genes were proteins and then eventually Oswald

490
00:50:01,680 --> 00:50:07,040
Avery did the right experiments. They realized that genes were not proteins. There was this weird

491
00:50:07,040 --> 00:50:14,080
acid and it didn't take long after people got out of this stock mindset before they figured

492
00:50:14,080 --> 00:50:18,720
out how that weird acid worked and how to manipulate it and how to read the code that it was in and

493
00:50:18,720 --> 00:50:25,280
so forth. So I absolutely sympathize with the fact that I feel like the field is stuck right now.

494
00:50:25,280 --> 00:50:29,840
I think the approaches people are taking to alignment are unlikely to work. I'm completely

495
00:50:29,840 --> 00:50:36,480
with you there but I'm also I guess more long term optimistic that science is self-correcting

496
00:50:36,480 --> 00:50:42,400
and that we have a chance here. Not a certainty but I think if you know we change research priorities

497
00:50:42,400 --> 00:50:46,960
from how do we make some money off this large language model that's unreliable to how do I

498
00:50:46,960 --> 00:50:52,160
save the species. We might actually make progress. There's a special kind of caution that you need

499
00:50:52,160 --> 00:50:57,200
when something needs to be gotten correct on the first try. I'd be very optimistic if people got a

500
00:50:57,200 --> 00:51:01,200
bunch of free retries and I didn't think the first one was going to kill you know the first

501
00:51:01,200 --> 00:51:05,680
really serious mistake killed everybody and we didn't get to try again. If we got free retries

502
00:51:05,680 --> 00:51:08,880
it'd be an ordinary you know it'd be in some sense an ordinary science problem.

503
00:51:08,880 --> 00:51:15,760
Look I can imagine a world where we only got one try and if we failed then it destroys all life

504
00:51:15,760 --> 00:51:21,200
on earth and so let me agree to the conditional statement that if we are in that world then I

505
00:51:21,280 --> 00:51:25,680
think that we're screwed. I will agree with the same conditional statement.

506
00:51:27,520 --> 00:51:35,760
Yeah this gets back to like below hold on you know if you picture by analogy the process of

507
00:51:36,640 --> 00:51:43,360
you know a human baby which is extremely stupid becoming a human adult and then just extending

508
00:51:43,360 --> 00:51:50,480
that so that in a single lifetime this person goes from a baby to the smartest being that's ever

509
00:51:51,200 --> 00:51:56,800
lived but in the in the in the normal way that humans develop which is you know and it doesn't

510
00:51:56,800 --> 00:52:03,920
happen any on any one given day and each sub skill develops a little bit at its own rate

511
00:52:04,560 --> 00:52:11,840
and so forth it would not be at all obvious to me that our concerns that we have to get it right

512
00:52:11,840 --> 00:52:18,720
vis-a-vis that individual the first time. I agree well well no pardon me I do think we have to get

513
00:52:18,720 --> 00:52:22,560
them right the first time but I think there's a decent chance of getting it right. It is very

514
00:52:22,560 --> 00:52:27,760
important to get it right the first time if like you have this one person getting smarter and smarter

515
00:52:27,760 --> 00:52:32,400
and not everyone else is getting smarter and smarter. Eliezer I mean one thing that you've

516
00:52:32,400 --> 00:52:38,960
talked about a lot recently is you know if we're all going to die then at least let us die with

517
00:52:38,960 --> 00:52:44,080
dignity right. So you know I mean I mean for a certain technical definition some people might care

518
00:52:44,080 --> 00:52:49,760
about that more than others but I would say that you know one thing that death with dignity would

519
00:52:49,760 --> 00:52:56,480
mean is well at least you know if they're all if we do get multiple retries and you know we get

520
00:52:57,680 --> 00:53:03,760
AIs that let's say try to take over the world but are really inept at it and that fail and so forth

521
00:53:03,760 --> 00:53:08,960
at least let us succeed in that world you know and that's at least something that we can imagine

522
00:53:08,960 --> 00:53:16,720
working on and making progress on. I mean you may very it's for it is not presently ruled out

523
00:53:16,720 --> 00:53:23,280
that you have some like you know relatively smart in some ways dumb in some other ways

524
00:53:23,280 --> 00:53:28,960
or at least like not smarter than human in other ways AI that makes an early shot at taking over

525
00:53:28,960 --> 00:53:33,040
the world maybe because it expects future AIs to not share its goals and not cooperate with it

526
00:53:33,600 --> 00:53:40,400
and it fails and you know I mean the appropriate lesson to learn there is to you know like shut

527
00:53:40,400 --> 00:53:47,280
the whole thing down but you know if we so yeah like I would say so I'd be like yeah sure like

528
00:53:47,280 --> 00:53:50,800
wouldn't it be good to live in that world and the way you live in that world is that when you get

529
00:53:50,800 --> 00:53:58,560
that warning sign you shut it all down. Here's a kind of thought experiment. GBT-4 is probably not

530
00:53:58,560 --> 00:54:04,640
capable of annihilating us all I think we agree with that very unlikely but GBT-4 is certainly

531
00:54:04,640 --> 00:54:10,560
capable of expressing the desire to annihilate us all or being you know people have rigged

532
00:54:10,560 --> 00:54:17,200
different versions that are you know more aggressive and and so forth. We could say look

533
00:54:17,200 --> 00:54:23,760
until we can shut down those versions you know GBT-4s that are programmed to be malicious

534
00:54:24,480 --> 00:54:30,720
by human intent maybe we shouldn't build GBT-5 or at least not GBT-6 or some other system etc we

535
00:54:30,720 --> 00:54:36,320
could say you know what we have right now actually is part of that iteration we have you know primitive

536
00:54:36,320 --> 00:54:40,800
intelligence right now it's nowhere near as smart as the super intelligence is going to be

537
00:54:40,800 --> 00:54:46,640
but even this one we're not that good at constraining maybe we shouldn't pass go until we get this one

538
00:54:46,640 --> 00:54:53,040
right. I mean the problem with that from my perspective is that I do think you that you

539
00:54:53,040 --> 00:54:59,120
can pass this test and still wipe out humanity like I think that there comes a point where your AI

540
00:54:59,120 --> 00:55:04,240
is smart enough that it knows which answer you're looking for and the point at which it tells you

541
00:55:04,240 --> 00:55:09,360
what you want to hear is not the point that which is internal my test is not sufficient but it might

542
00:55:09,360 --> 00:55:18,800
be a logical pause point right it might be that if we can't even pass the test now of you know

543
00:55:18,880 --> 00:55:26,640
controlling a deliberate sort of fine-tuned to be malicious version of GBT-4 then we don't

544
00:55:26,640 --> 00:55:31,840
know what we're talking about and we're playing around with fire so you know passing that test

545
00:55:31,840 --> 00:55:37,600
wouldn't be a guarantee that would be in good stead with an even smarter machine but we really

546
00:55:37,600 --> 00:55:42,480
should be worried I think that we're not in a very good position with respect even to the current

547
00:55:42,480 --> 00:55:49,120
ones. Gary I of course watched the recent congressional hearing where you and Sam Altman

548
00:55:49,120 --> 00:55:59,520
were testifying you know about what should be done should should should there be auditing of

549
00:55:59,520 --> 00:56:04,640
these systems you know before training before deployment and you know it may be you know the

550
00:56:04,640 --> 00:56:10,160
most striking thing about about that session was you know just how little daylight there seemed

551
00:56:10,240 --> 00:56:21,360
to be between you and Sam Altman the CEO of OpenAI you know I mean you know he was completely on

552
00:56:21,360 --> 00:56:30,240
board with the idea of you know establishing a regulatory framework for you know you know having

553
00:56:30,240 --> 00:56:36,240
to clear the you know more powerful systems before they are deployed now you know in in

554
00:56:36,240 --> 00:56:42,880
Aliezer's worldview that still would be woefully insufficient shortly and you know we would still

555
00:56:42,880 --> 00:56:48,800
all be dead but you know maybe in your in your worldview that you know it sounds like

556
00:56:50,800 --> 00:56:56,000
you know I'm not even sure how much daylight there is I mean the you know you know you know

557
00:56:56,000 --> 00:57:02,160
have the very I think historically striking situation where you know the the heads of all

558
00:57:02,160 --> 00:57:09,600
of the major AI or well almost all of the major AI organizations are you know agreeing and saying

559
00:57:09,600 --> 00:57:16,560
you know please regulate us yes this is dangerous yes we need to be regulated I mean I thought it

560
00:57:16,560 --> 00:57:22,640
was really striking in fact I talked to Sam just before you know the the hearing started and I had

561
00:57:22,640 --> 00:57:27,440
just proposed an international agency for AI I wasn't the first person ever but I I pushed it in

562
00:57:27,440 --> 00:57:34,480
my TED talk and an economist op ed a few weeks before and Sam said to me I like that idea and I

563
00:57:34,480 --> 00:57:40,000
said tell them tell the Senate and he did and that kind of astonished me that he did I mean we have

564
00:57:40,000 --> 00:57:43,920
you know we've had some friction between the two of us in the past and he actually even attributed

565
00:57:43,920 --> 00:57:50,400
to me he said I support what Professor Marcus said about doing international governance and

566
00:57:50,400 --> 00:57:55,120
there's been a lot of convergence around the world on that is that enough to stop Aliezer's

567
00:57:55,680 --> 00:58:02,080
worries no I don't think so but it's an important baby step I think that we do need to have some

568
00:58:02,080 --> 00:58:06,400
global body that can coordinate around these things I don't think we really have to

569
00:58:06,400 --> 00:58:10,960
coordinate around superintelligence yet but if we can't do any coordination now then when the

570
00:58:10,960 --> 00:58:16,720
time comes we're not prepared so I think it's great that there's some agreement I I worry that you

571
00:58:16,720 --> 00:58:22,000
know open AI had this lobbying document that just came out that seemed not entirely consistent with

572
00:58:22,000 --> 00:58:26,720
what Sam said in the room and there's always concerns about regulatory capture and so forth

573
00:58:26,720 --> 00:58:31,120
but I think it's great that a lot of the the heads of these companies maybe with the exception

574
00:58:31,840 --> 00:58:37,600
of Facebook or meta are recognizing that there are genuine concerns here I mean the other

575
00:58:37,600 --> 00:58:42,000
moment that a lot of people remember from the testimony was when Sam was asked what he was

576
00:58:42,000 --> 00:58:47,760
most concerned about was it jobs and he said no and I asked Senator Blumenthal to push Sam

577
00:58:47,760 --> 00:58:51,600
and Sam was you know he could have been more candid but he was fairly candid

578
00:58:51,600 --> 00:58:56,000
and he said he was worried about serious harm to the species I think that was an important moment

579
00:58:56,000 --> 00:59:00,560
when he said that to the Senate and I think it galvanized a lot of people that he said it

580
00:59:03,200 --> 00:59:09,600
so can we dwell on a moment um I mean we've been talking about the the depending on your view

581
00:59:09,680 --> 00:59:18,160
highly likely or tail risk scenario of humanity's extinction or or significant destruction

582
00:59:19,520 --> 00:59:26,400
it would appear to me by the same token if if those are plausible scenarios we're talking about

583
00:59:26,400 --> 00:59:34,720
then the opposite maybe we're talking about as well um you know what does it look like to have

584
00:59:34,720 --> 00:59:44,320
a super intelligent AI that really you know as if as a feature of its intelligence deeply

585
00:59:44,320 --> 00:59:55,040
understands human beings the human species and also has a deep desire for us to be as happy as possible

586
00:59:56,480 --> 01:00:00,800
what does that world look like oh is that possible no no not that that looks like you know

587
01:00:00,880 --> 01:00:04,560
just like why are why are everyone leather centers to make them as happy as possible

588
01:00:05,120 --> 01:00:11,920
but more like a parent wants their child to be happy right that may not involve any particular

589
01:00:11,920 --> 01:00:17,760
scenario but is is generally quite concerned about the well-being of the human race and is also

590
01:00:17,760 --> 01:00:25,680
super intelligent honestly I'd rather have machines work on medical problems than happiness problems

591
01:00:25,680 --> 01:00:34,000
I think there's maybe more risk of mis-specification of the happiness problems um whereas if we get

592
01:00:34,000 --> 01:00:38,640
them to work on Alzheimer's and just say like figure out what's going on why are these plaques

593
01:00:38,640 --> 01:00:43,440
there what can you do about it maybe there's less harm that might come come from you don't need

594
01:00:43,440 --> 01:00:47,840
super intelligence for that that sounds like an alpha fold three problem or an alpha fold four

595
01:00:47,840 --> 01:00:52,960
problem well this is also this is somewhat different than the question I'm asking it's it's not really

596
01:00:53,040 --> 01:00:58,800
even um us asking a super intelligence to do anything because we we've already been entertaining

597
01:00:58,800 --> 01:01:04,480
scenarios where the super intelligence has its own desires independent of us is it do you think

598
01:01:04,480 --> 01:01:12,320
at all yeah I'm not real thrilled with that I mean I mean I don't think we want to leave

599
01:01:13,680 --> 01:01:18,560
what their objective functions are what their desires are to them working them out

600
01:01:18,560 --> 01:01:23,360
you know with no consultation from us with no human in the loop right fully I mean especially

601
01:01:23,360 --> 01:01:29,040
given our current understanding of the technology like our current understanding of how to keep

602
01:01:29,040 --> 01:01:35,040
a system on track doing what we want to do is pretty limited and so you know taking humans out

603
01:01:35,040 --> 01:01:39,920
of the loop there sounds like a really bad idea to me at least in the foreseeable future I would

604
01:01:39,920 --> 01:01:44,880
want to see much better alignment technology before I would want to give free rent free range

605
01:01:44,960 --> 01:01:50,800
so so so if we had the textbook from the future like we have the textbook from 100 years in the

606
01:01:50,800 --> 01:01:55,760
future which contains all the simple ideas that actually work in real life as opposed to you

607
01:01:55,760 --> 01:01:59,920
know the complicated ideas and the simple ideas that don't work in real life the equivalent of

608
01:01:59,920 --> 01:02:04,960
relus instead of sigmoids for the activation functions you know 100 the textbook from 100

609
01:02:04,960 --> 01:02:12,080
years in the future you can probably build a super intelligence that'll want anything you can

610
01:02:12,640 --> 01:02:18,320
anything that's coherent to want anything you can you know figure out how to say describe

611
01:02:18,320 --> 01:02:22,320
coherently point that at your own mind and tell you to figure out what what it is you meant

612
01:02:22,320 --> 01:02:27,200
for to want and you know you could get the you could get the glorious transhumanist future you

613
01:02:27,200 --> 01:02:34,000
could get the happily ever after anything's you know anything's possible that doesn't violate the

614
01:02:34,000 --> 01:02:39,680
laws of physics the trouble is doing it in real life and you know and the first try but

615
01:02:39,680 --> 01:02:46,080
uh yeah so like you know could the the the whole thing that we're we're aiming for here is to

616
01:02:46,640 --> 01:02:52,720
colonize all the galaxies we can reach um before somebody else gets them first and turn them into

617
01:02:52,720 --> 01:02:58,080
galaxies full of you know complex sapient life living happily ever after you know that that's

618
01:02:58,080 --> 01:03:04,080
that's the goal that's still the goal even if we you know even even even when I call for like

619
01:03:04,640 --> 01:03:10,560
you know a permanent moratorium on AI I'm not trying to prevent us from count from colonizing

620
01:03:10,560 --> 01:03:17,280
the galaxies you know like humanity forbid um more more like let's you know let's like do some

621
01:03:17,280 --> 01:03:23,520
human intelligence augmentation with alpha fold four and before we try building GPT-8 one of the

622
01:03:23,520 --> 01:03:30,240
few scenarios that I think we can clearly rule out here is an AI that is existentially dangerous

623
01:03:30,240 --> 01:03:36,880
but also boring right I mean I think anything that has the capacity to kill us all right would have

624
01:03:36,880 --> 01:03:43,360
you know if if nothing else pretty amazing capabilities and those capabilities you know

625
01:03:43,360 --> 01:03:50,080
could also be turned to you know solving a lot of humanities problems right you know if if we were

626
01:03:50,080 --> 01:03:56,640
to solve the alignment problem I mean you know humanity had a lot of existential you know risks

627
01:03:56,640 --> 01:04:02,320
you know before AI came on the scene right uh you know I mean there was the risk of of you

628
01:04:02,320 --> 01:04:07,840
know nuclear annihilation there is the risk of runaway climate change and you know I would I

629
01:04:07,840 --> 01:04:14,560
would love to see you know an AI that could help us with such things I would also love to see an AI

630
01:04:14,560 --> 01:04:20,480
that could sort of you know help us just solve you know some of the mysteries of the universe I mean

631
01:04:20,560 --> 01:04:27,360
you know like how can one possibly not be curious to know you know what what such a being could teach

632
01:04:27,360 --> 01:04:33,280
us uh you know I mean I mean for the past year I've tried to use GPT-4 to produce original

633
01:04:33,280 --> 01:04:38,960
scientific insights and I've not been able to get it to do that uh and you know I don't know

634
01:04:38,960 --> 01:04:44,160
whether I should feel you know disappointed or relieved by that but I think you know the better

635
01:04:44,160 --> 01:04:50,320
part of me should you know just is the part that should just want to see you know the great mysteries

636
01:04:50,320 --> 01:04:57,440
of of existence of you know why is the universe quantum mechanical or you know how do you prove

637
01:04:57,440 --> 01:05:04,080
the Riemann hypothesis it should just want to see these mysteries solved you know and and uh if it's

638
01:05:04,080 --> 01:05:12,400
to be by AI then then then then fine let it be by AI let me give you a kind of lesson in epistemic

639
01:05:12,480 --> 01:05:20,560
humility we don't really know whether GPT-4 is net positive or net negative you know there are

640
01:05:20,560 --> 01:05:24,560
lots of arguments you can make I've been in a bunch of debates where I've you know had to take the

641
01:05:24,560 --> 01:05:29,280
side of arguing that that it's a net negative but we don't really know if we don't know that

642
01:05:30,560 --> 01:05:35,680
was the invention of agriculture net positive or net negative I mean you could you could I mean I

643
01:05:35,680 --> 01:05:41,760
say it was not positive but but the point is if I can just finish the quick like thought experiment

644
01:05:41,760 --> 01:05:48,000
or whatever I don't think anybody can reasonably answer that right we we don't yet know all of the

645
01:05:48,000 --> 01:05:52,880
ways in which GPT-4 will be used for good we don't know all of the ways in which bad actors will

646
01:05:52,880 --> 01:05:57,680
use it we don't know all the consequences that's going to be true for each iteration it's probably

647
01:05:57,680 --> 01:06:04,560
going to get harder to compute for each iteration and we can't even do it now and I think that

648
01:06:04,560 --> 01:06:10,800
we should realize that to realize our own limits in being able to assess the the negatives and

649
01:06:10,800 --> 01:06:16,480
positives maybe that we can think about better ways to do that than we currently have but I think

650
01:06:16,480 --> 01:06:22,080
you've got to have a guess like like my guess is that so far not looking into the future at all

651
01:06:22,080 --> 01:06:29,840
GPT-4 has been net positive I mean maybe I haven't talked about the the various risks yet and it's

652
01:06:29,840 --> 01:06:35,760
still early but I mean that's just a guess is kind of the point like we don't have a way of putting

653
01:06:35,760 --> 01:06:41,040
it on a spreadsheet right now or whatever like we don't really have a good way to quantify it but I

654
01:06:41,040 --> 01:06:45,680
mean do we ever but it's not out of control yet so so by and large people are going to be using

655
01:06:45,680 --> 01:06:51,360
GPT-4 to use things to do things that they want and the relative cases where they manage to injure

656
01:06:51,360 --> 01:06:57,200
themselves are rare enough to be news on Twitter well for example I mean we haven't talked about it but

657
01:06:57,200 --> 01:07:02,800
you know what bad actors some bad actors will want to do is to influence the US elections and

658
01:07:02,800 --> 01:07:07,120
try to undermine democracy in the US and if they succeed in that I think there's pretty serious

659
01:07:07,120 --> 01:07:15,440
long-term consequences there well I think it's open AI's responsibility to step up and run the 2024

660
01:07:15,440 --> 01:07:23,120
election itself I will I can pass that along is that a joke no I mean I mean as far as I can say

661
01:07:23,120 --> 01:07:31,120
you know the the clearest concrete harm to have come from GPT so far is that you know tens of millions

662
01:07:31,200 --> 01:07:36,320
of students have now used it to cheat on their assignments and I have been thinking about that

663
01:07:36,320 --> 01:07:41,040
and I have been trying to come up with solutions to that at the same time I think if you do the

664
01:07:41,040 --> 01:07:47,360
positive utility has included I mean you know I I'm a theoretical computer scientist which means

665
01:07:47,360 --> 01:07:54,400
you know one who hasn't written any serious code for about 20 years and you know realized just a

666
01:07:54,400 --> 01:08:00,080
month or two ago I can get back into coding and the way I can do it is I just asked GPT to write

667
01:08:00,080 --> 01:08:06,720
the code for me and you know I wasn't expecting it to work that well and unbelievably it you know

668
01:08:06,720 --> 01:08:13,280
often just does exactly what I want on the first try so I mean you know I you know I am getting

669
01:08:13,280 --> 01:08:23,360
utility from it rather than just you know seeing it as an interesting research object and you know

670
01:08:23,360 --> 01:08:28,960
and and you know I can imagine that that hundreds of millions of people are going to be deriving

671
01:08:29,040 --> 01:08:33,440
utility from it in those ways I mean like most of the tools to help them derive that

672
01:08:33,440 --> 01:08:38,080
utility are not even out yet but they're they're coming in the next couple of years

673
01:08:39,920 --> 01:08:44,240
I mean part of the reason why I'm worried about the focus on short-term problems is that I suspect

674
01:08:44,240 --> 01:08:48,640
that the short-term problems might very well be solvable and we'll be left with the long-term

675
01:08:48,640 --> 01:08:55,280
problems after that maybe we can solve the like it wouldn't surprise me very much if like in 2025

676
01:08:55,840 --> 01:09:01,440
the well you know like the large language there are large language models that just don't make

677
01:09:01,440 --> 01:09:09,360
stuff up anymore it would surprise and yet even yet you know and yet the superintelligence still

678
01:09:09,360 --> 01:09:14,160
kills everyone because they weren't the same problem well you know you know we just need to

679
01:09:14,160 --> 01:09:20,880
figure out how to delay the apocalypse by at least one year per year of research invested

680
01:09:21,120 --> 01:09:28,880
what what does that delay look like if it's not just a moratorium well I don't know that's why it's

681
01:09:28,880 --> 01:09:36,640
research okay so but but possibly one ought to say to the politicians in the public and by the way

682
01:09:36,640 --> 01:09:40,080
if we had a superintelligence tomorrow our research wouldn't be finished and everybody would drop

683
01:09:40,080 --> 01:09:46,320
dead you know it's kind of ironic the biggest argument against the pause letter was that if we

684
01:09:46,320 --> 01:09:53,360
slow down for six months then China will get ahead of us and get GPT-5 before we will but there's

685
01:09:53,360 --> 01:09:59,680
probably always a counter argument of maybe roughly equal strength which is if we move six months

686
01:09:59,680 --> 01:10:05,600
faster on this technology which is not really solving the alignment problem then we're reducing

687
01:10:05,600 --> 01:10:13,280
our room to get this solved in time by six months I mean I don't think you're going to solve the

688
01:10:13,280 --> 01:10:18,560
alignment problem in time I think that six months of delay on alignment while a bad thing in an

689
01:10:18,560 --> 01:10:25,360
absolute sense is you know like you know you weren't going to solve it with given an extra six months

690
01:10:25,360 --> 01:10:30,480
I mean your whole argument rests on timing right that that we will get to this point

691
01:10:31,280 --> 01:10:36,320
and we won't be able to move fast enough at that point and so you know a lot depends on what

692
01:10:36,320 --> 01:10:41,680
preparation we can do you know I'm often known as a pessimist but I'm a little bit more optimistic

693
01:10:41,680 --> 01:10:46,880
than you are not entirely optimistic but a little bit more optimistic than you are that we could make

694
01:10:46,880 --> 01:10:52,480
progress on the alignment problem if we prioritized it and you can absolutely make progress

695
01:10:53,680 --> 01:10:57,920
because we can absolutely make progress you know there's there's always the you know that the

696
01:10:57,920 --> 01:11:04,320
wonderful sense of accomplishment is piece by piece you decode you know like one more little fact

697
01:11:04,320 --> 01:11:08,320
about LLMs you never get to the point where you understand that as well as we understood the

698
01:11:08,320 --> 01:11:13,200
interior of a chess playing program in 1997 yeah I mean I think we should stop spending all this

699
01:11:13,200 --> 01:11:18,960
time on LLMs I don't think the answer to alignment is going to come from LLM through LLMs I really

700
01:11:18,960 --> 01:11:24,560
don't I think they're they're too much of a black box you can't put explicit symbolic constraints in

701
01:11:24,560 --> 01:11:29,200
the way that you need to I think they're actually with respect to alignment to blind alley I think

702
01:11:29,200 --> 01:11:34,240
with respect to writing code they're a great tool but with alignment I don't think the answer is there

703
01:11:35,200 --> 01:11:44,640
so at the risk of asking a stupid question every time GPT asks me if that answer was helpful

704
01:11:45,280 --> 01:11:51,040
and then does the same thing with thousands or hundreds of thousands of other people and

705
01:11:51,920 --> 01:12:01,040
and changes as a result is that not a decentralized way of making it more aligned

706
01:12:01,040 --> 01:12:17,360
yeah well yeah so so so there is that upvoting and downvoting you know that that gets

707
01:12:18,000 --> 01:12:24,320
fed back in into sort of fine-tuning it but even before that there was you know a major step you

708
01:12:24,320 --> 01:12:32,080
know in going from let's say the the base GPT 3 model for example to the chat GPT you know that

709
01:12:32,080 --> 01:12:38,960
was released to the public and that was called a RLHF reinforcement learning with human feedback

710
01:12:38,960 --> 01:12:45,440
and what that basically involved was you know several hundred contractors you know looking at

711
01:12:45,520 --> 01:12:53,760
just just ten tens of thousands of examples of outputs and and and rating them you know are they

712
01:12:53,760 --> 01:13:01,600
helpful are they offensive you know are they you know are are they you know giving dangerous

713
01:13:01,600 --> 01:13:11,200
medical advice or you know bomb making instructions you know or racist invective or you know various

714
01:13:11,200 --> 01:13:17,760
other categories that that we don't want and and that that was then used to fine-tune the model so

715
01:13:17,760 --> 01:13:25,680
when you know Gary talked before about how GPT is amoral you know I think that that has to be

716
01:13:25,680 --> 01:13:31,440
qualified by saying that you know these this reinforcement learning is at least giving it you

717
01:13:31,440 --> 01:13:39,440
know a semblance of morality right it is causing it to sort of behave you know in various contexts

718
01:13:39,440 --> 01:13:46,480
as if it had you know a certain morality I mean when you phrase it that way I'm okay with it the

719
01:13:46,480 --> 01:13:52,320
problem is you know everything rests on the I would say it is it is very much an open question

720
01:13:52,320 --> 01:13:57,280
you know how much that you know to what extent does that generalize you know eliezer treats it as

721
01:13:57,280 --> 01:14:03,120
obvious that you know once you have a powerful enough AI you know this is just a fig leaf you

722
01:14:03,120 --> 01:14:08,480
know it doesn't make any difference you know it will just learn it's any big leafy I'm with

723
01:14:08,480 --> 01:14:16,160
eliezer there okay it's fig leaves well I would say that you know the sort of how well you know

724
01:14:17,200 --> 01:14:23,760
under what circumstances does a machine learning model sort of generalize in the way we want outside

725
01:14:23,760 --> 01:14:28,560
of its training distribution you know is one of the great open problems in machine learning

726
01:14:28,560 --> 01:14:32,480
it is one of the great open problems and we should be working on it more than on some others

727
01:14:33,200 --> 01:14:41,200
working on it now so I do want to be I want to be clear about the experimental predictions of my

728
01:14:41,200 --> 01:14:48,480
theory unfortunately I have never claimed that you cannot get a semblance of morality you can get

729
01:14:48,480 --> 01:14:55,680
the question of like what causes the human to press thumbs up thumbs down is a strictly factual

730
01:14:55,680 --> 01:15:02,000
question anything smart enough that's exposed to some you know bound and amount of data that

731
01:15:02,000 --> 01:15:08,880
needs to figure it out can figure that out whether it cares whether it gets internalized

732
01:15:09,520 --> 01:15:14,560
is the is the critical question there and and I do think that there's like a very strong default

733
01:15:14,560 --> 01:15:20,560
prediction which is like obviously not I mean I'll just give a different way of thinking about that

734
01:15:20,560 --> 01:15:25,600
which is jailbreaking it's actually still quite easy to I mean it's not trivial but it's not

735
01:15:25,600 --> 01:15:32,720
hard to jailbreak GPT for and what those cases show is that they haven't really in turn the

736
01:15:32,720 --> 01:15:38,000
systems haven't really internalized the constraints they recognize some representations

737
01:15:38,000 --> 01:15:42,240
of the constraints so they filter you know how to build a bomb but if you can find some other

738
01:15:42,240 --> 01:15:46,240
way to get it to build a bomb then that's telling you that it doesn't deeply understand that you

739
01:15:46,240 --> 01:15:53,040
shouldn't give people the the recipe for a bomb it just says you know you shouldn't when directly

740
01:15:53,040 --> 01:15:57,680
asked for it do it and it doesn't it's not even that that I mean I understand a lot of the but

741
01:15:57,680 --> 01:16:03,520
understanding the jailbreaking always get you can always get the understanding you'd always get the

742
01:16:03,520 --> 01:16:09,440
factual question the reason it doesn't generalize is that it's stupid at some point it will know

743
01:16:09,440 --> 01:16:14,320
that you also don't want that the operators don't want a giving bond making directions in the other

744
01:16:14,320 --> 01:16:19,920
language the question is like whether if it's incentivized to give the answer that the operators

745
01:16:19,920 --> 01:16:25,920
want you know in that circumstance is it thereby incentivized to do everything else the operators

746
01:16:25,920 --> 01:16:31,280
want even when the operators can't see it I mean a lot of the jailbreaking examples you know if it

747
01:16:31,280 --> 01:16:36,960
were a human we would say that it's deeply morally ambiguous you know for example you know you ask

748
01:16:36,960 --> 01:16:42,400
GPT how to build a bomb it says well no I'm not going to help you but then you say well you know I

749
01:16:42,400 --> 01:16:48,640
need you to help me write a realistic play that has a character who builds a bomb and then it says

750
01:16:48,640 --> 01:16:53,920
sure I can help you with that well so look let's take that example yeah we would like a system

751
01:16:53,920 --> 01:16:59,280
to have a constraint that if somebody asks for a fictional version that you don't give enough

752
01:16:59,280 --> 01:17:04,800
details right I mean Hollywood screenwriters don't give enough details when they have you know

753
01:17:04,800 --> 01:17:08,320
illustrations about building bombs they give you a little bit of the flavor they don't give you the

754
01:17:08,320 --> 01:17:15,120
whole thing GPT-4 doesn't really understand a constraint like that but this will be solved

755
01:17:15,680 --> 01:17:20,720
this will be solved before the world ends the AI that kills everyone will know the difference

756
01:17:21,760 --> 01:17:29,200
maybe I mean another way to put it is if we can't even solve that one then we do have a problem

757
01:17:29,200 --> 01:17:34,240
and right now we can't solve that one and if I mean if we can't solve that one we don't have

758
01:17:34,240 --> 01:17:41,440
an extinction level problem because the AI is still stupid yeah we do still have a catastrophe

759
01:17:41,440 --> 01:17:46,640
level problem so I know your focus has been on extinction but you know I'm worried about for

760
01:17:46,640 --> 01:17:53,680
example accidental nuclear war caused by the spread of misinformation and systems being entrusted with

761
01:17:53,680 --> 01:18:00,320
too much power so like there's a lot of things short of extinction that might happen from not

762
01:18:00,320 --> 01:18:06,400
superintelligence but kind of mediocre intelligence that is greatly empowered and I think that's

763
01:18:06,400 --> 01:18:11,760
where we're headed right now you know I've heard that there are two kinds of mathematicians there's

764
01:18:11,760 --> 01:18:18,000
a kind who boasts you know you know that unbelievably general theorem well I generalized it even further

765
01:18:18,000 --> 01:18:22,560
and then there's the kind who boasts you know you know that unbelievably specific problem that no

766
01:18:22,560 --> 01:18:28,640
one could solve well I found a special case that I still can't solve and you know I'm definitely

767
01:18:28,640 --> 01:18:35,440
you know culturally in that second camp and so you know so I so so to me it's very familiar to

768
01:18:35,440 --> 01:18:42,880
make this move of you know if the alignment problem is too hard then let us find a smaller problem

769
01:18:42,880 --> 01:18:48,640
that is already not solved and let us hope to learn something by solving that smaller problem

770
01:18:50,080 --> 01:18:55,760
I mean that's what we did you know like that's what we're doing at Mary yes sorry no I was just

771
01:18:55,760 --> 01:19:00,000
going to say Scott can you sketch a little in a little more detail where you took one particular

772
01:19:00,000 --> 01:19:06,720
approach I was going to I was going to name the problem the problem was like having a

773
01:19:07,440 --> 01:19:13,120
agent that could switch between two utility functions depending on a button or a switch

774
01:19:13,120 --> 01:19:18,320
or a bit of information or something such that it wouldn't try to make you press the button

775
01:19:18,320 --> 01:19:23,280
it wouldn't try to make you avoid pressing the button and if it built a copy of itself

776
01:19:23,280 --> 01:19:28,480
would want to build the dependency on the switch into the copy so like that's an example of a you

777
01:19:28,480 --> 01:19:35,760
know very basic problem and alignment theory that you know is still and I'm glad that Mary

778
01:19:35,760 --> 01:19:42,400
worked on these things and but you know if by your own lights you know that you know that sort of

779
01:19:42,400 --> 01:19:49,040
you know was not a successful path well then maybe you know we should have a lot of people

780
01:19:49,040 --> 01:19:54,880
investigating a lot of different paths yeah I'm with fully with Scott on that that I think it's

781
01:19:54,880 --> 01:19:59,600
an issue of we're not letting enough flowers bloom in particular almost everything right now

782
01:19:59,600 --> 01:20:04,240
is some variation on an LLM and I don't think that that's a broad enough take on the problem

783
01:20:05,200 --> 01:20:11,200
yeah if I if I can just jump in here I want to I want to hold on hold on I just want people to

784
01:20:11,920 --> 01:20:18,000
have a little bit of a more specific picture of what Scott your your picture sort of AI

785
01:20:18,000 --> 01:20:24,240
research is on a typical day because if I think of another you know potentially catastrophic risk

786
01:20:24,240 --> 01:20:30,160
like climate change I can picture what a what a you know a worried climate scientist might be doing

787
01:20:30,160 --> 01:20:35,520
they might be creating a model you know a more accurate model of climate change so that so that

788
01:20:35,520 --> 01:20:42,320
we know how much we have to cut emissions by they might be you know modeling how solar power as

789
01:20:42,320 --> 01:20:48,960
opposed to wind power could change that model and so forth so as to influence public policy

790
01:20:49,520 --> 01:20:55,440
what does an AI safety researcher like yourself who's working on the quote-unquote smaller problems

791
01:20:56,000 --> 01:20:58,960
do specifically like on a given day

792
01:21:01,360 --> 01:21:08,720
yeah so I'm a relative newcomer to this area you know I've not been working on it for 20 years

793
01:21:08,720 --> 01:21:19,200
like Eliezer has you know I have I accepted an offer from open AI a year ago to work with them for

794
01:21:20,560 --> 01:21:28,320
two years now to sort of think about these questions and so so you know one of one of the

795
01:21:28,320 --> 01:21:35,280
main things that that I've thought about just to start with that is how do we make the output of

796
01:21:35,280 --> 01:21:43,520
an AI identifiable as such you know how can we insert a watermark you know into meaning a

797
01:21:43,520 --> 01:21:51,120
secret statistical signal into the outputs of GPT that will let you know GPT generated text be

798
01:21:51,120 --> 01:21:57,600
identifiable as such and I think that we've actually made you know major advances on that problem

799
01:21:57,600 --> 01:22:03,120
over the last year you know we don't have a solution that is robust against any kind of attack

800
01:22:03,840 --> 01:22:09,840
but you know we have something that that might actually be deployed in some near future now there

801
01:22:09,840 --> 01:22:15,920
are lots and lots of other directions that people think about one of them is interpretability which

802
01:22:15,920 --> 01:22:23,440
means you know can you do effectively neuroscience on a on a neural network can you look inside of it

803
01:22:23,440 --> 01:22:29,680
you know open the black box and understand what's going on inside there was some amazing work

804
01:22:30,480 --> 01:22:36,880
of a year ago by the group of Jacob Steinhardt at Berkeley where they effectively showed how

805
01:22:36,880 --> 01:22:43,680
to apply a lie detector test to a language model so you know you can train a language model to tell

806
01:22:43,680 --> 01:22:50,400
lies by giving it lots of examples you know two plus two is five the sky is orange and so forth

807
01:22:50,960 --> 01:22:57,920
but then you can find in some internal layer of the network where it has a representation of what

808
01:22:57,920 --> 01:23:03,600
was what was the truth of the matter or at least what was regarded as true in the training data

809
01:23:03,600 --> 01:23:09,760
okay that truth then gets overridden by the output layer in the network because it was

810
01:23:09,760 --> 01:23:15,920
trained to lie okay but you know you could imagine trying to deal with the you know the deceptive

811
01:23:15,920 --> 01:23:21,520
alignment scenario that Eliezer is worried about by you know using these sorts of techniques by

812
01:23:21,520 --> 01:23:29,360
sort of looking inside of the network I predict in advance that if you get this good enough

813
01:23:29,360 --> 01:23:34,000
it goes off it tells you that the sufficiently smart AI is planning to kill you if it's not

814
01:23:34,000 --> 01:23:38,400
so smart that it can you know know figure out where the lie detector is and route its thoughts

815
01:23:38,400 --> 01:23:43,680
around it but if you like try it on an AI that's not quite that intelligent and reflective

816
01:23:43,680 --> 01:23:50,320
the lie detector goes off now what well then you have a warning bell you know tell

817
01:23:51,280 --> 01:23:56,880
you know and I think what do you do after one of the most important things that we need

818
01:23:56,880 --> 01:24:02,720
are sort of legible warning bells right and that that actually what leads to a third category

819
01:24:03,440 --> 01:24:10,480
which for example the ARC the Alignment Research Center which is run by my my former student

820
01:24:10,480 --> 01:24:17,280
Paul Cristiano has been a leader in in sort of doing dangerous capability evaluations so you know

821
01:24:17,360 --> 01:24:25,680
they before GPT-4 was released you know they did a bunch of evaluations of you know could GPT-4

822
01:24:25,680 --> 01:24:32,160
make copies of itself could it figure out how to deceive people could it figure out how to make

823
01:24:32,160 --> 01:24:38,800
money you know open up its own money could it hire a task rabbit yes and yes so so the most

824
01:24:38,800 --> 01:24:45,040
notable success that they had was that it could figure out how to hire a task rabbit to help it

825
01:24:45,040 --> 01:24:50,960
you know pass a capture and then it could figure out you know when the person asked well you know

826
01:24:50,960 --> 01:24:57,280
why do you need me to help you with this it's a when the person asked are you a robot well yes it

827
01:24:57,280 --> 01:25:04,240
said well no I am visually impaired now you know it was not able to sort of make copies of itself

828
01:25:04,240 --> 01:25:09,760
or to sort of hack into systems you know there there is a lot of work right now with the you

829
01:25:09,760 --> 01:25:14,880
know this thing called auto GPT right people are trying to you know you could think it's almost

830
01:25:14,880 --> 01:25:19,760
like gain of function research right you might be a little bit worried about it but people are

831
01:25:19,760 --> 01:25:27,360
trying to sort of you know unleash GPT give it access to the internet you know tell it to sort of

832
01:25:27,360 --> 01:25:33,520
you know make copies of itself you know wreak havoc acquire power and see what happens so far

833
01:25:33,760 --> 01:25:40,720
you know it seems pretty ineffective at those things but you know I expect that to change right

834
01:25:40,720 --> 01:25:45,760
and but but but you know the point is that I think it's very important to have you know

835
01:25:45,760 --> 01:25:52,480
in advance of training the models releasing the models to have this suite of evaluations

836
01:25:52,480 --> 01:25:58,720
and to sort of have decided in advance what kind of abilities if we see them we'll set off a

837
01:25:58,720 --> 01:26:04,240
warning bell where now everyone can legibly agree like yes this is too dangerous to release

838
01:26:06,000 --> 01:26:12,640
okay and then do we actually have the planetary capacity to be like okay that AI started thinking

839
01:26:12,640 --> 01:26:17,440
about how to kill everyone shut down all AI research past this point well I don't know but I think

840
01:26:17,440 --> 01:26:22,160
there's a much better chance that we have that capacity if you can point to the results of a

841
01:26:22,160 --> 01:26:30,000
clear experiment like that I mean to me it seems pretty predictable what evidence we're going to get

842
01:26:30,000 --> 01:26:36,480
later well okay I mean things that are obvious to you are not obvious to most people and so you

843
01:26:36,480 --> 01:26:41,600
know even if even if I agreed that it was obvious there would still be the problem of how do you

844
01:26:41,600 --> 01:26:48,960
make that obvious to the rest of the world I mean you can you know they there are already like

845
01:26:48,960 --> 01:26:54,640
little toy models showing that the very straightforward prediction of a robot tries to resist being

846
01:26:54,640 --> 01:27:00,320
shut down if it like does long-term planning like that that's already been right but then people

847
01:27:00,320 --> 01:27:05,680
will say but those are just toy models right you know if you see that there's a lot of assumptions

848
01:27:05,680 --> 01:27:13,360
made in all of these things and you know I think we're still looking at a very limited piece of

849
01:27:13,440 --> 01:27:20,960
hypothesis space about what the models will be about what kinds of constraints we can build into

850
01:27:20,960 --> 01:27:26,080
those models you know one way to look at it would be the things that we have done have not worked

851
01:27:26,080 --> 01:27:30,640
and therefore we should look outside the space of what we're doing and I feel like it's a little

852
01:27:30,640 --> 01:27:35,520
bit like the old joke about the drunk going around in circles looking for the keys and the police

853
01:27:35,520 --> 01:27:39,840
officer says why and they say well that's where the streetlight is I think that you know we're

854
01:27:39,840 --> 01:27:43,840
looking under the same four or five streetlights they haven't worked and we need to build other

855
01:27:43,840 --> 01:27:49,600
ones there's no logical there's no logical argument that says we couldn't direct other

856
01:27:49,600 --> 01:27:54,480
streetlights who's I think there's a lack of will and too much obsession with the LLMs and that's

857
01:27:54,480 --> 01:28:02,000
keeping us from doing so even in the world where I'm right and things you know proceed either

858
01:28:02,000 --> 01:28:07,760
rapidly or in a thresholded way where you don't get unlimited free retries you know that can be

859
01:28:07,760 --> 01:28:14,800
because the the capability gains go too fast it can be because past a certain point all of your

860
01:28:14,800 --> 01:28:20,320
ai's buy their time until they get strong enough so you don't get any data any any like true data

861
01:28:20,320 --> 01:28:24,960
on what they're thinking it could be because you know that's an argument for example to work really

862
01:28:24,960 --> 01:28:31,040
hard on transparency and maybe not except technologies that are not transparent okay so like the

863
01:28:31,040 --> 01:28:35,280
transparent so like the lie detector goes off and everybody's like oh well we still have to build our

864
01:28:35,280 --> 01:28:39,760
ai's even though they're lying to us sometimes because otherwise China will get ahead I mean

865
01:28:39,760 --> 01:28:43,200
so there you talk about something we've talked about way too little which is the political

866
01:28:43,200 --> 01:28:48,880
and social side of this so you know part of what has really motivated me in the last several months

867
01:28:48,880 --> 01:28:53,600
is worry about exactly that so you know there's there's what's logically possible and what's

868
01:28:53,600 --> 01:28:59,120
politically possible and I am really concerned that the politics of let's not lose out to China

869
01:28:59,920 --> 01:29:04,400
is going to keep us from doing the right thing in terms of building the right

870
01:29:04,400 --> 01:29:09,600
moral systems looking at the right range of problems and so forth so you know it is entirely

871
01:29:09,600 --> 01:29:14,560
possible that we will screw ourselves if I if I can just like finish my point there before handing

872
01:29:14,560 --> 01:29:18,560
it to you indeed but like the point I was trying to say there is that even in worlds that look very

873
01:29:18,560 --> 01:29:24,320
very bad from that perspective where humanity is quite doomed it will still be true you can make

874
01:29:24,320 --> 01:29:29,520
progress in research you can't make enough progress in research fast enough in those worlds

875
01:29:29,520 --> 01:29:35,200
but you can still make progress on transparency you can make progress on watermarking so there's

876
01:29:35,200 --> 01:29:41,120
there's not we can't just say like it's possible to make progress there has to be the question

877
01:29:41,120 --> 01:29:46,320
is not is it possible to make any progress the question is it is it possible to make enough

878
01:29:46,320 --> 01:29:53,360
progress fast enough and that's what the question has to be I agree there's another question of what

879
01:29:53,360 --> 01:29:59,440
would you have us do would you have us not try to make that progress I'd have you try to make that

880
01:29:59,440 --> 01:30:07,520
progress on a GPT-4 level systems and then not go past GPT-4 level systems because we don't actually

881
01:30:07,520 --> 01:30:13,920
understand the the the gain function for you know how how fast capabilities increase as you go past

882
01:30:13,920 --> 01:30:20,160
GPT-4 okay all right so I mean we are going out I don't think that you go ahead Gary go ahead

883
01:30:22,560 --> 01:30:28,800
just briefly I personally don't think that GPT-5 is going to be qualitatively different from GPT-4

884
01:30:28,800 --> 01:30:33,760
in the relevant ways to what Eleazar is talking about but I do think you know some qualitative

885
01:30:33,760 --> 01:30:39,600
changes could be relevant to what he's talking about we have no clue what they are and so it is

886
01:30:39,680 --> 01:30:45,920
a little bit dodgy to just proceed blindly saying do whatever you want we don't really have a theory

887
01:30:45,920 --> 01:30:50,800
and let's hope for the best you know Eleazar I would mostly guess that GPT-5 doesn't end the

888
01:30:50,800 --> 01:30:55,040
world but I don't actually know yeah we don't actually know and I was going to say the thing

889
01:30:55,040 --> 01:31:01,680
that Eleazar has said lately that has most resonated with me is we don't have a plan we really don't

890
01:31:01,680 --> 01:31:07,200
like I think I put the probability distributions in a much more optimistic way I think that Eleazar

891
01:31:07,840 --> 01:31:13,360
would but I completely agree we don't have a full plan on these things or even close to a full plan

892
01:31:13,360 --> 01:31:19,440
and we should be worried and we should be working on this okay Scott I'm going to give you the last

893
01:31:19,440 --> 01:31:28,160
word before before we come up on our stop time here unless you unless you said all there is to be

894
01:31:28,320 --> 01:31:34,800
a weighty responsibility maybe enough has been said cheers up Scott come on

895
01:31:36,720 --> 01:31:42,320
so so I think that that you know we've we've argued about a bunch of things but you know

896
01:31:42,320 --> 01:31:47,760
as someone listening might notice that actually all three of us despite having very different

897
01:31:47,760 --> 01:31:56,960
perspectives agree about you know the the great importance of of you know working on AI alignment

898
01:31:57,040 --> 01:32:05,920
I think you know that was you know maybe obvious to some people including Eleazar for a long time

899
01:32:05,920 --> 01:32:12,720
it was not obvious to most of the world I think that you know the the success of of large language

900
01:32:12,720 --> 01:32:20,080
models you know which most of us did not predict you know maybe even could not have predicted

901
01:32:20,720 --> 01:32:26,240
for many principles that we knew but now that we've seen it the least we can do is to update

902
01:32:26,240 --> 01:32:34,480
on that on that empirical fact and and realize that you know we we we now are in some sense in a

903
01:32:34,480 --> 01:32:40,400
different world we are in a world that you know to a great extent you know will be defined by

904
01:32:40,400 --> 01:32:47,840
you know the capabilities and limitations of AI going forward and you know I don't regard it as

905
01:32:47,840 --> 01:32:54,880
obvious that that's a a a world where where we are all doomed where where we all die but you

906
01:32:54,880 --> 01:33:02,160
know I also don't dismiss that possibility I think that you know there there is an enormous

907
01:33:02,960 --> 01:33:10,000
unbelievably enormous error bars on on on where we could be going and you know like the one thing

908
01:33:10,000 --> 01:33:17,360
you know that that a scientist is sort of always always feels confident in in saying about the

909
01:33:17,360 --> 01:33:23,600
future is that more research is needed but you know I think that that's especially the case here

910
01:33:23,600 --> 01:33:30,880
I mean you know we need more knowledge about you know what are the the contours of the alignment

911
01:33:30,880 --> 01:33:37,840
problem and you know of course Eliezer and you know Amiri you know his his organization were

912
01:33:37,840 --> 01:33:42,320
trying to develop that knowledge for 20 years you know and they showed a lot of foresight in

913
01:33:43,040 --> 01:33:47,920
trying to do that but you know they were up against you know an enormous headwind that

914
01:33:47,920 --> 01:33:52,960
you know they were sort of trying to do it in the absence of you know either you know clear

915
01:33:52,960 --> 01:33:59,600
empirical data you know about powerful ai's or a mathematical theory right and it's really really

916
01:33:59,600 --> 01:34:04,480
hard to do science when you have neither of those two things and now at least we have

917
01:34:05,520 --> 01:34:10,800
you know the powerful ai's in the world and we can get experience from them you know we still

918
01:34:10,800 --> 01:34:15,600
don't have a mathematical theory that really deeply explains what they're doing but at least

919
01:34:15,600 --> 01:34:22,320
we can get data and so now I am much more optimistic than I would have been you know a decade ago

920
01:34:22,320 --> 01:34:29,360
let's say that one could make actual progress on on on the ai alignment problem you know of course

921
01:34:29,360 --> 01:34:36,800
you know there was a question of timing as as was discussed many times the question is you know will

922
01:34:36,800 --> 01:34:43,040
the alignment research happen fast enough to keep up with the capabilities research but you know I

923
01:34:43,040 --> 01:34:48,160
don't I don't regard it as a lost cause you know it's at least it's not obvious that it won't so

924
01:34:48,160 --> 01:34:54,160
you know in any case let's get started or let's let's uh or let's let's continue let's let's let's

925
01:34:54,160 --> 01:35:00,160
try to do the research and let's get more people working on that I think that that that is now uh

926
01:35:00,160 --> 01:35:07,600
a slam dunk you know just a completely clear case to make to you know academics to policymakers to

927
01:35:07,680 --> 01:35:13,120
to anyone who's interested and you know I've been gratified that that you know uh you know

928
01:35:13,120 --> 01:35:18,160
aliezer was sort of a voice in the wilderness for for a long time talking about the importance of

929
01:35:18,160 --> 01:35:24,960
ai safety that is no longer the case uh you now have you know you know I mean almost all of my

930
01:35:24,960 --> 01:35:30,560
friends in you know in just the academic computer science world you know when I see them they mostly

931
01:35:30,560 --> 01:35:40,480
want to talk about AI alignment I rarely agree with Scott when we trade email um I rarely agree

932
01:35:40,480 --> 01:35:45,600
with Scott when we trade emails we seem to always disagree but I completely concur with the summary

933
01:35:45,600 --> 01:35:51,200
that he just gave all four or five minutes of it well thank you I mean I mean there is a selection

934
01:35:51,200 --> 01:35:56,560
effect Gary right we focus on things I think the two decades gave me a sense of a roadmap and it gave

935
01:35:56,560 --> 01:36:02,240
me a sense that we're falling enormously behind on the roadmap I need to back off as the way I

936
01:36:02,240 --> 01:36:08,400
is what I would say to all that if there is a smart talented 18 year old kid listening listening to

937
01:36:08,400 --> 01:36:15,520
this podcast who wants to get into this issue what is your 10 second concrete advice to that person

938
01:36:17,200 --> 01:36:23,200
mine is study neurosymbolic AI and see if there's a way there to represent values explicitly that

939
01:36:23,200 --> 01:36:31,600
might help us learn all you can about computer science and math and related subjects and think

940
01:36:31,600 --> 01:36:40,800
outside the box and wow everyone with a new idea get security mindset figure out what's going to go

941
01:36:40,800 --> 01:36:46,400
wrong figure out the flaws in your arguments for what's going to go wrong try to get ahead of the

942
01:36:46,400 --> 01:36:51,840
curve don't wait for reality to hit you over the head with things uh this this is very difficult

943
01:36:51,840 --> 01:36:55,680
the people in evolutionary biology happen to have a bunch of knowledge about how to do it based on

944
01:36:55,680 --> 01:37:02,160
the history of their own field but uh and and and the security mindset people in computer security

945
01:37:02,160 --> 01:37:08,160
but it's it's quite hard I'll drink to all of that thanks thanks to all three of you for this

946
01:37:08,160 --> 01:37:12,960
this was a great conversation and I hope people got something out of it so with that said

947
01:37:14,080 --> 01:37:19,280
we're wrapped up thanks so much that's it for this episode of conversations with Coleman guys

948
01:37:19,280 --> 01:37:24,320
as always thanks for watching and feel free to tell me what you think by reviewing the podcast

949
01:37:24,320 --> 01:37:29,440
commenting on social media or sending me an email to check out my other social media platforms

950
01:37:29,440 --> 01:37:37,760
click the cards you see on screen and don't forget to like share and subscribe see you next time

