Why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.
What's the big fear here? Make the case.
We don't understand the things that we build.
The AIs are grown more than built, you might say.
They end up as giant, inscrutable matrices of floating point numbers that nobody can decode.
At this rate, we end up with something that is smarter than us, smarter than humanity,
that we don't understand, whose preferences we could not shape.
And by default, if that happens, if you have something around that is much
smarter than you and does not care about you one way or the other,
you probably end up dead at the end of that.
Extinction is a pretty extreme outcome that I don't think is particularly likely,
but the possibility that these machines will cause mayhem because we don't know how to
enforce that they do what we want them to do. I think that's a real thing to worry about.
Welcome to another episode of Conversations with Coleman.
Today's episode is a roundtable discussion about AI safety with Eliezer Yudkowski,
Gary Marcus, and Scott Aronson. Eliezer Yudkowski is a prominent AI researcher and writer
known for co-founding the Machine Intelligence Research Institute,
where he spearheaded research on AI safety. He's also widely recognized for his influential
writings on the topic of rationality. Scott Aronson is a theoretical computer scientist
and author, celebrated for his pioneering work in the field of quantum computation.
He's also the chair of COMSI at U of T Austin, but is currently taking a leave of absence to
work at open AI. Gary Marcus is a cognitive scientist, author, and entrepreneur known for
his work at the intersection of psychology, linguistics, and AI. He's also authored several
books, including Cluj and Rebooting AI, Building AI We Can Trust. This episode is all about AI
safety. We talk about the alignment problem. We talk about the possibility of human extinction
due to AI. We talk about what intelligence actually is. We talk about the notion of a singularity
or an AI takeoff event, and much more. It was really great to get these three guys in the same
virtual room, and I think you'll find that this conversation brings something a bit fresh to a
topic that has admittedly been beaten to death on certain corners of the internet. Without further
ado, Eleazar Yudkowski, Gary Marcus, and Scott Aronson. Okay, Eleazar Yudkowski, Scott Aronson,
Gary Marcus. Thanks so much for coming on my show. Thank you. The topic of today's conversation is
AI safety, and this is something that's been in the news lately. We've seen experts and
CEOs signing letters, recommending public policy, surrounding regulation. We continue to have the
debate between people that really fear AI is going to end the world and potentially kill
all of humanity and the people who feel that those fears are overblown.
And so this is going to be sort of a roundtable conversation about that, and you three are
really three of the best people in the world to talk about it with. So thank you all for doing this.
Let's just start out with you, Eleazar, because you've been one of the most
really influential voices getting people to take seriously the possibility that AI will kill us all.
You know, why is AI going to destroy us? Chat GPT seems pretty nice. I use it every day.
What's the big fear here? Make the case. Well, chat GPT seems quite unlikely to kill everyone in
its present state. AI capabilities keep on advancing and advancing. The question is not,
can a chat GPT kill us? The answer is probably no. So as long as that's true, as long as it
hasn't killed us yet, the engineers are just going to keep pushing the capabilities. There's no
obvious blocking point. We don't understand the things that we build. The AIs are grown
more than built, you might say. They end up as giant inscrutable matrices of floating point
numbers that nobody can decode. It's probably going to end up technically difficult to make
them want particular things and not others. People are just charging straight ahead.
So at this rate, we end up with something that is smarter than us, smarter than humanity,
that we don't understand, whose preferences we could not shape. And by default, if that happens,
if you have something around that is much smarter than you and does not care about you one way or
the other, you probably end up dead at the end of that. The way it gets the most of whatever
strange inscrutable things that it wants are worlds in which there are not humans taking up
space, using up resources, building other AIs to compete with it, or just a world in which you
built enough power plants that the surface of the earth got hot enough that humans didn't survive.
Gary, what do you have to say about that?
There are parts that I agree with and parts that I don't. So I agree that we are likely to wind up
with AIs that are smarter than us. I don't think we're particularly close now, but in 10 years or
50 years or 100 years at some point, maybe a thousand years, but it will happen. I think there's a
lot of anthropomorphization there about machines wanting things. Of course, they have objective
functions and we can talk about that. I think it's a presumption to say that the default is that
they're going to want something that leads to our demise and that they're going to be effective at
that and be able to literally kill us all. I think if you look at the history of AI at least so far,
they don't really have wants beyond what we program them to do. There is an alignment problem. I
think that that's real in the sense of people who program a system to do X and they do X prime.
That's kind of like X, but not exactly. I think there's really things to worry about. I think
there's a real research program here that is under-researched, which is the way I would put it
is we want to understand how to make machines that have values. Asimov's laws are way too simple,
but they're kind of starting point for conversation. We want to program machines that don't harm humans.
They can calculate the consequences of their actions. Right now, we have technology like
GPT-4 that has no idea what the consequences of its actions are. It doesn't really anticipate things.
There's a separate thing that Eliezer didn't emphasize, which is it's not just how smart the
machines are, but how much power we give them. How much we empower them to do things like access
the internet or manipulate people or write source code, access files and stuff like that.
Right now, auto-GPT can do all of those things and that's actually pretty disconcerting to me.
To me, that doesn't all add up to any kind of extinction risk anytime soon, but catastrophic
risk where things go pretty wrong because we wanted these systems to do X and we didn't really
specify it well. They don't really understand our intentions. I think there are risks like that.
I don't see it as a default that we wind up with extinction. I think it's pretty hard to actually
terminate the entire human species. You're going to have people in Antarctica that are going to be
out of harm's way or whatever, or you're going to have some people who respond differently to any
pathogen, et cetera. Extinction is a pretty extreme outcome that I don't think is particularly
likely, but the possibility that these machines will cause mayhem because we don't know how to
enforce that they do what we want them to do. I think that's a real thing to worry about
and it's certainly worth doing research on. Scott, how do you view this?
Yeah, so I'm sure that you can get the three of us arguing about something, but I think you're
going to get agreement from all three of us that AI safety is important and that catastrophic outcomes,
whether or not that means literal human extinction are possible. I think it's become
apparent over the last few years that this century is going to be largely defined by our
interaction with AI, that AI is going to be transformative for human civilization.
I'm confident of that much. If you ask me almost anything beyond that about how is it going to
transform civilization? Will it be good? Will it be bad? What will the AI want? I am pretty agnostic
just because if you would ask me 20 years ago to try to forecast where we are now, I would have
gotten a lot wrong. My only defense is I think that all of us here and almost everyone in the
world would have gotten a lot wrong about where we are now. If I try to envision where we are in
2043, does the AI want to replace humanity with something better? Does it want to keep us around
as pets? Does it want to just continue helping us out like just a super souped up version of
chat GPT? I think all of those scenarios merit consideration, but I think that what has happened
in the last few years that's really exciting is that AI safety has become an empirical subject.
There are these very powerful AIs that are now being deployed and we can actually learn something.
We can work on mitigating the nearer-term harms, not because the existential risk doesn't exist or
is absurd or is science fiction or anything like that, but just because the nearer-term harms are
the ones that we can see right in front of us and where we can actually get feedback from the
external world about how we're doing. We can learn something and hopefully some of the knowledge
that we gain will be useful in addressing the longer-term risks that I think Eliezer is very
rightly worried about. Seems to me there's alignment and then there's alignment. There's
alignment in the sense that we haven't even fully aligned smartphone technology with our
interests. There are some ways in which smartphones and social media have led to probably deleterious
mental health outcomes, especially for teenage girls, for example. There are those kinds of
mundane senses of alignment where it's like, is this technology doing more good than harm
in the normal everyday public policy sense? Then there's the capital A alignment of,
are we creating a creature that is going to view us like ants and have no problem
extinguishing us and whether intentional or not? It seems to me all of you agree that
the first sense of alignment is at the very least something to worry about now and something to deal
with, but I'm curious to what extent you think the really capital A sense of alignment
is a real problem because it can sound very much like science fiction to people. Maybe let's start
with Eliezer. From my perspective, I would say that if we had a solid guarantee that AI was going
to do no more harm than social media, we ought to plow ahead and get all the gains. It's not
enough harm to back this amount of harm that social media has done to humanity, while very
significant in my view. I think it's done a lot of damage to our sanity, but that's just not a
large enough harm to justify either foregoing the gains that you could get from AI if that was going
to be the worst downside or to justify the kind of drastic measures you'd need to stop plowing ahead
on AI. I think that the capital A alignment is beyond this generation. I've started the field,
I've watched over it for two decades. I feel like in some ways the modern generation plowing in with
their eyes on the short term stuff is like losing track of the larger problems because they can't
solve the larger problems and they can't solve the little problems, but we're just like plowing
straight into the big problems and we're going to go plow right into the big problems with a bunch
of little solutions that aren't going to scale. I think it's lethal. I think it's at the scale
where you just back off and don't do this. By back off and don't do this, what do you mean?
I mean have an international treaty about where the chips capable of doing AI training go
and have them all going into licensed monitored data centers and not have the training runs for
AIs more powerful than GPT-4, possibly even lowering that threshold over time as algorithms
improve and it gets possible to train more powerful AIs using less computing.
So you're picturing a kind of international agreement to just stop?
International moratorium. And if North Korea steals the GPU shipment, then you've got to be ready to
destroy their data center that they build by conventional means. And if you don't have that
willingness in advance, then countries may refuse to sign up for the agreement being like,
why aren't we just seeding the advantage to someone else then? It actually has to be a
worldwide shutdown because the scale of harm from a superintelligence, it's not that if you
have 10 times as many superintelligence as you've got, 10 times as much harm. It's not that a
superintelligence only wrecks the country that built the superintelligence. Any superintelligence
everywhere is anyone's last problem. So Gary and Scott, if either of you want to jump in there,
is AI safety a matter of forestalling the end of the world and all of these
smaller issues and pass towards safety that Scott, you mentioned, are just
throwing, I don't know what the analogy is, but pointless essentially. What do you guys make of this?
I mean, the journey of 1,000 miles begins with a step. Most of the way I think about
this comes from 25 years of doing computer science research, including quantum computing
and computational complexity, things like that, where we have these gigantic aspirational problems
that we don't know how to solve. And yet, year after year, we do make progress. We pick off
little sub-problems. And if we can't solve those, then we find sub-problems of those. And we keep
repeating until we find something that we can solve. And this is, I think, for centuries,
the way that science has made progress. Now, it is possible that this time, we just don't have
enough time for that to work. And I think that is what Eliezer is fearful of, that we just
don't have enough time for the ordinary scientific process to work before AI becomes too powerful,
in which case you start talking about things like a global moratorium enforced with the threat of
war. I am not ready to go there. I could imagine circumstances where maybe I say, gosh, this looks
like such an imminent threat that we have to. But I tend to be very, very worried in general about
causing a catastrophe in the course of trying to prevent a catastrophe. And I think when you are
talking about threatening airstrikes against data centers or things like that, then that is an
obvious worry. So I am somewhat in between here. I am with Scott that we are not at the point where
we should be bombing data centers. I don't think we are close to that. I am much less
aware of what the right word is to use here. I don't think we are anywhere near as close to AGI as
I think Eliezer sometimes sounds like. I don't think GPT-5 is anything like AGI. And I am not
particularly concerned about who gets it first and so forth. On the other hand, I think that
we are in a sort of dress rehearsal mode. Nobody expected GPT-4, really chat GPT,
to percolate as fast as it did. And it is a reminder that there is a social side to all of this,
how software gets distributed matters, and there is a corporate side.
It was a kind of galvanizing moment for me when Microsoft didn't pull Sydney, even though Sydney
did some awfully strange things. I thought they would take it for a while and it is a reminder
that they can make whatever decisions they want. So you kind of multiply that by Eliezer's concerns
about what do we do and at what point. What would be enough to cause problems is a reminder,
I think, that we need, for example, to start roughing out these international treaties now,
because there could become a moment where there is a problem. I don't think the problem that
Eliezer sees is here now, but maybe it will be. And maybe when it does come, we will have so many
people pursuing commercial self-interest and so little infrastructure in place we won't be able
to do anything. So I think it really is important to think now, if we reach such a point, what are
we going to do? What do we need to build in place before we get to that point?
So we've been talking about this concept of artificial general intelligence.
And I think it's worth asking whether that is a useful coherent concept. So for example,
if I were to think by analogy to athleticism and think of the moment when we build a machine
that has, say, artificial general athleticism, meaning it's better than LeBron James at basketball,
but also better at curling than the world's best curling player and also better at soccer
and also better at archery and so forth, it would seem to me that
there's something a bit strange as framing it as having reached a point on a single continuum.
It seems to me you would sort of have to build each capability, each sport individually and then
somehow figure how to package them all into one robot without each skill set detracting from the
other. Is that a disanalogy? Do you all picture this intelligence as sort of one dimension,
one knob that is going to get turned up along a single axis? Or do you think that way of talking
about it is misleading in the same way that I kind of just sketched out?
Yeah, I would absolutely not accept that. I like to say that intelligence is not a one-dimensional
variable. There's many different aspects to intelligence and there's not, I think, going
to be a magical moment when we reach the singularity or something like that. I would say that the core
of artificial general intelligence is the ability to flexibly deal with new problems that you haven't
seen before. And the current systems can do that a little bit, but not very well. My typical example
of this now is GPT-4 is exposed to the game of chess, sees the lots of games of chess that
sees the rules of chess, but it never actually figures out the rules of chess and makes illegal
moves and so forth. So it's in no way a general intelligence that can just pick up new things.
Of course, we have things like AlphaGo that can play a certain set of games, AlphaZero really,
but we don't have anything that has the generality of human intelligence. But human
intelligence is just one example of general intelligence. You could argue that chimpanzees
or crows have another variety of general intelligence. I would say the current machines don't really
have it, but they will eventually. I mean, I think a priori, it could have been that
you would have math ability, you would have verbal ability, you'd have
ability to understand humor, and they'd all be just completely unrelated to each other.
That is possible. And in fact, already with GPT, you can say that in some ways,
it already is a super intelligence. It knows vastly more, can converse on a vastly
greater range of subjects than any human can. And in other ways, it seems to fall short of
what humans know or can do. But you also see this sort of generality, just empirically.
I mean, GPT was sort of trained on all the text on the internet,
let's say most of the text on the open internet. So it was just one method. It was not
explicitly designed to write code, and yet it can write code. And at the same time as that
ability emerged, you also saw the ability to solve word problems like high school level math.
You saw the ability to write poetry. This all came out of the same system without any of it
being explicitly optimized for. I feel like I need to interject one important thing,
which is it can do all these things, but none of them all that reliably.
Well, okay. Nevertheless, compared to, let's say, what my expectations would have been,
if you'd asked me 10 or 20 years ago, I think that the level of generality
is pretty remarkable. And it does lend support to the idea that there is some sort of general
quality of understanding there, where you could say, for example, that GPT-4 has more of it than
GPT-3, which in turn has more than GPT-2. And I would say that it does seem to me like it's
presently pretty unambiguous that GPT-4 is in some sense dumber than an adult or even teenage human.
I mean, to take the example I just gave you a minute ago, it never learns to play chess,
even with a huge amount of data. So it will play a little bit of chess, it will memorize the openings
and be okay for the first 15 moves, but it gets far enough away from what it's trained on and
it falls apart. This is characteristic of these systems. It's not really characteristic in the
same way of adults or even teenage humans. Almost anything that it does, it does unreliably.
And give another example, you can ask a human to write a biography of someone and don't make
stuff up, and you really can't ask GPT to do that. Yeah, like it's a bit difficult because you could
always be cherry picking something that humans aren't usually good at. But to me, it does seem
like there's this broad range of problems that don't seem especially to play to humans' strong
points or machine weak points, where GPT-4 will do no better than a seven-year-old on those problems.
I do feel like these examples are cherry picked because if I just take a different,
very typical example, I'm writing an op-ed for the New York Times, say, about any given
subject in the world, and my choice is to have a smart 14-year-old next to me with anything that's
in his mind already or GPT. There's no comparison. So which of these sort of examples is the litmus
test for who is more intelligent? If you did it on a topic where it couldn't rely on memorized
text, you might actually change your mind on that. So the thing about writing a Times op-ed
is most of the things that you propose to it, there's actually something that it can
pastiche together from its dataset. That doesn't mean that it really understands what's going on.
It doesn't mean that that's general capability. Also, as the human, you're doing all the hard
parts. Obviously, a human is going to prefer, if a human has a math problem, we're going to rather
use a calculator than another human. Similarly, with the New York Times op-ed, you're doing all the
parts that are hard for GPT-4, and then you're asking GPT-4 to just do some of the parts that
are hard for you. You're always going to prefer an AI partner rather than a human partner within
that range of the human can do all the human stuff, and you want an AI to do whatever the AI
is good at at the moment. An analogy that's maybe a little bit helpful here is driverless cars.
It turns out that on highways and ordinary traffic, they're probably better than people,
and on unusual circumstances, they're really worse than people. A Tesla not too long ago
ran into a jet, and a human wouldn't do that, like slow speed being summoned across a parking
lot. A human would never do that. There are different strengths and weaknesses. The strengths
of a lot of the current kinds of technology is that they can either pastiche together or
make not literal analogies when we go into the details, but to stored examples, and they tend
to be poor when you get to outlier cases. That's persistent across most of the technologies
that we use right now. If you stick to stuff in which there's a lot of data, you'll be happy
with the results you get from these systems. You move far enough away, not so much.
What we're going to see over time is that the length of the debate about whether or not it's
still dumber than you gets longer and longer and longer. Then if things are allowed to just keep
running and nobody dies, then at some point it switches over to a very long debate about
is it smarter than you, which then gets shorter and shorter and shorter, and eventually reaches
a point where it's pretty unambiguous if you're paying attention. Now, I suspect that
this process gets interrupted by everybody dying. In particular, there's a question of
the point at which it becomes better than humanity at building the next edition of the AI system
and how fast do things snowball once you get to that point? Possibly, you do not have time for
further public debates or even a two-hour Twitter space, depending on how that goes.
Some of the limitations of GPT are completely understandable, just from a little knowledge
of how it works. It does not have an internal memory per se other than what appears on the
screen in front of you. This is why it's turned out to be so effective to explicitly tell it.
Let's think step by step when it's solving a math problem, for example. You have to tell it to
show all of its work because it doesn't have an internal memory with which to do that. Likewise,
when people complain about it, hallucinating references that don't exist, well, the truth is
when someone asks me for a citation, if I'm not allowed to use Google, I might have a vague
recollection of some of the authors, and I'll probably do a very similar thing to what GPT
does. I'll hallucinate. There's a great phrase I learned the other day, which is frequently wrong,
never in doubt. That's true. I'm not going to make up a reference with full detail, page numbers,
titles, so forth. I might say, look, I don't remember 2012 or something like that. Whereas,
GPT-4, what it's going to say is 2017, Aaronson and Yodkowski, New York Times, pages 13 to 17.
No, it does need to get much, much better at knowing what it doesn't know. And yet, already,
I've seen a noticeable improvement there, going from GPT-3 to GPT-4. For example,
if you ask GPT-3, prove that there are only finitely many prime numbers, it will give you
a proof, even though the statement is false. And it will have an error, which is similar to the
errors on 1,000 exams that I've graded, just trying to get something past you, hoping that
you won't notice. Hey, if you ask GPT-4, prove that there are only finitely many prime numbers,
it says, no, that's a trick question. Actually, there are infinitely many primes. And here's why.
Yeah. Part of the problem with doing the science here is that I think you would know better since
you work part-time or whatever to open AI. But my sense is that a lot of the examples that get
posted on Twitter, particularly by the likes of me and other critics or other skeptics, I should
say, is the system gets trained on those. So almost everything that people write about it,
I think, is in the training set. So it's hard to do the science when the system's constantly being
trained, especially in the RLHF side of things. And we don't actually know what's in GPT-4. So
we don't even know if they're regular expressions and simple rules, match things. So we can't do
the kind of science we used to be able to do. This conversation, this subtree of the conversation,
I think, has no natural endpoint. So if I can sort of zoom out a bit, I think there's a pretty
solid sense in which humans are more generally intelligent than chimpanzees. As you get closer
and closer to the human level, I would say that the direction here is still clear, that the comparison
is still clear. We are still smarter than GPT-4. This is not going to take control of the world
from us. But the conversations get longer. The definitions start to break down around the edges.
But I think it also, as you keep going, it comes back together again. There's a point,
and possibly this point is very close to the point of time to where everybody dies. So maybe we
don't ever see it in a podcast, but there's a point where it's unambiguously smarter than you.
And including the spark of creativity, being able to deduce things quickly rather than with tons
and tons of extra evidence, strategy, cunning, modeling people, figuring out how to manipulate
people. So let's stipulate, Aliezer, that we're going to get to machines that can do all of that.
And then the question is, what are they going to do? Is it a certainty that they will make
our annihilation part of their business? Is it a possibility? Is it an unlikely possibility?
I think your view is that it's a certainty. I've never really understood that part.
It's a certainty on the present tech, is the way I would put it. If that happened,
so in particular, if that happened tomorrow, then Modulo, Cromwell's rule, never say certain.
My probability is, yes, Modulo, the chance that my model is somehow just completely mistaken.
If we got 50 years to work it out and unlimited retries, I think that'd be pretty okay. I think
we'd make it. The problem is that it's a lot harder to do science when your first wrong try
destroys the human species and then you don't get to try again. I mean, I think there's something,
again, that I agree with and something I'm a little bit skeptical about. So I agree that
the amount of time we have matters. I would also agree that there's no existing technology that
solves the alignment problem that gives a moral basis to these machines. GPT-4 is fundamentally
amoral. I don't think it's immoral. It's not out to get us, but it really is amoral. It can answer
trolley problems because there are trolley problems in the dataset, but that doesn't mean that it
really has a moral understanding of the world. If we get to a very smart machine that by
all the criteria that we've talked about and it's amoral, then that's a problem for us.
There's a question of whether if we can get to smart machines, whether we can build them in a
way that will have some moral basis. I think we need to make progress. The first try part,
I'm not willing to let pass. I understand, I think, your argument there, and maybe you should spell
it out. I think that we probably get more than one shot and that it's not as dramatic and instantaneous
as you think. I do think one wants to think about sandboxing, one wants to think about
distribution, but let's say we had one evil super genius now who is smarter than everybody else,
like so what? One super-
Much smarter.
Say again?
Not just a little smarter.
Oh, even a lot smarter. Most super geniuses aren't actually that effective. They're not
that focused. They were focused on other things. You're assuming that the first
super genius AI is going to make it its business to annihilate us and that's the part where I
still am a bit stuck in the argument.
Yeah, some of this has to do with the notion that if you do a bunch of training, you start to get
goal direction, even if you don't explicitly train on that. That goal direction is a natural way to
achieve higher capabilities. The reason why humans want things is that wanting things is an
effective way of getting things and so natural selection in the process of selecting exclusively
on reproductive fitness just on that one thing got us to want a bunch of things that correlated
with reproductive fitness in the ancestral distribution because wanting, having intelligences,
that want things is a good way of getting things. In a sense, wanting comes from the same
place as intelligence itself and you could even from a certain technical standpoint on
expected utility say that intelligence is a very effective way of wanting, planning,
plotting past through time that leads to particular outcomes.
Part of it is that I do not think you get the brooding super intelligence that wants
nothing because I don't think that wanting an intelligence can be internally pried apart
that easily. I think that the way you get super intelligences is that there are things that
have gotten good at organizing their own thoughts and have good taste in which thoughts to think
and that is where the high capabilities come from.
Can I put a point to you?
Let me just put the following point to you, which I think
in my mind is similar to what Gary was saying. There's often in philosophy this notion of the
continuum fallacy, which in the canonical example is like you can't locate a single hair that you
would pluck from my head where I would suddenly go from not bald to bald or even more intuitive
examples like a color wheel. On a gray scale there's no single pixel you can point to and say
well that's where gray begins and white ends and yet we have this conceptual distinction that
feels hard and fast between gray and white and gray and black and so forth. When we're talking about
artificial general intelligence or super intelligence you seem to operate on a model where
either it's a super intelligence capable of destroying all of us or it's not. Whereas intelligence
may just be a continuum fallacy style spectrum where we're first going to see the shades of
something that's just a bit more intelligent than us and maybe it can kill five people at most
and then it can and when that happens we're going to want to intervene
and we're going to figure out how to intervene at that level and so on and so forth.
Yeah so if it's stupid enough to do it then yes. Let me by the identical logic there should be
nobody who steals money on a really large scale right because you could just give them five dollars
and see if they steal that and if they don't steal that you know you're good to trust them with a
billion. I mean I think that in actuality anyone who did steal a billion dollars probably displayed
some dishonest behavior earlier in their life that was you know unconditionally not not acted upon
early enough. I'm actually not even. Hold on hold on the analogy out pictures like
we have the first case of fraud that's $10,000 and then we build systems to prevent it but then
they fail with a somewhat smarter opponent but our systems get better and better and better
and so we actually prevent the billion-dollar fraud because of the systems put in place
that in response to the $10,000 frauds you know. I mean I think Coleman's putting his finger on an
important point here which is how much do we get to iterate and Eliezer is saying the minute we have
a super intelligent system we won't be able to iterate because it's all over immediately.
There isn't a minute like that. The way that the continuum goes to the threshold
is that you eventually get something that's smart enough that it knows not to play its hand early
and then if that thing you know if you are still cranking up the power on that and preserving
its utility function it knows it just has to wait to be smarter to be able to win. It doesn't play its
hand prematurely it doesn't tip you off it's not in its interest to do that. It's in its interest
to cooperate until it thinks it can win against humanity and only then make its move. If it doesn't
expect the future smarter AIs to be smarter than itself then we might perhaps see these early AIs
telling humanity don't build the later AIs and I would be sort of surprised and amused if we
ended up in that particular sort of like science fiction scenario as I see it but we're already
in like something that you know me from 10 years ago would have called the science fiction scenario
which is the things that I'll talk to you without being very smart.
I always come up Eliezer against this idea that you're assuming that the very bright machines
the super intelligent machines will be malicious and duplicitous and so forth and I just don't
see that as a logical entailment of being very smart. I mean they don't specifically want as
an end in itself for you to be destroyed they're just doing whatever obtains the most of the stuff
that they actually want which doesn't specifically have a term that's maximized by humanity surviving
and doing well. Why can't you just hard code you know don't do anything that will annihilate the
human species don't do anything. We don't know how we don't know how there is no technology to
hard code such as. So there I agree with you but I think it's important if I can just run for one
second. I agree that right now we don't have the technology to hard code don't do harm to humans
but for me it's all boils down to a question of are we going to get to the smart machines
before we make progress on that hard coding problem or not and that to me that means that
problem of hard coding ethical values is actually one of the most important projects
that we should be working on. Yeah and I tried to work on it 20 years in advance
and capabilities are just like running vastly ahead of alignment. When I started working on this
20 years you know like two decades ago we were in a sense ahead of where we are now. AlphaGo
is much more controllable than GPT-4. So there I agree with you we've fallen in love with the
technology that is fairly poorly controlled. AlphaGo is very easily controlled and very well
specified. We know what it does. We can more or less interpret why it's doing it and everybody's
in love with these large language models and they're much less controlled and you're right we
haven't made a lot of progress on alignment. So if we just go on a straight line everybody dies.
I think that's this is an important fact. I would almost even accept that for argument but
ask then just for the sake of argument but then ask do we have to be on a straight line?
I mean I would agree to the weaker claim that you know we should certainly be extremely worried
about the intentions of a superintelligence in the same way that say chimpanzees should be
worried about the intentions of you know the first humans that arise right and in fact you know
chimpanzees you know continue to exist in our world only at humans' pleasure.
But I think that there are a lot of other considerations here for example
if we imagined you know that GPT-10 is you know the first unaligned superintelligence
that has these sorts of goals well then you know it would be appearing in a world where presumably
GPT-9 you know already has very wide diffusion and where people can use that to try to you know
and GPT-9 is not destroying the world you know by assumption.
Why does GPT-9 work with the humans instead of with GPT-10?
Well I don't know I mean I mean I mean I mean maybe maybe maybe it does work with GPT-10 but
you know I just don't view that as a certainty you know I mean I think you know you're certainty
about this is the one place where I really get off the train.
Same with me.
I well I mean I'm not asking you to share my certainty I am asking the viewers to
believe that you might end up with like more extreme probabilities after after you stare
things for an additional couple of decades that doesn't mean you have to accept my probabilities
immediately but I'm at least ask you to like not treat that as some kind of weird anomaly
you know I mean you're just going to find those kinds of situations in these debates.
My view is that I don't find the extreme probabilities that you described to be plausible
but I find the question that you're raising to be important I think you know maybe straight
line is too extreme but this idea that if you just follow current trends we're getting more
sorry we're getting less and less controllable machines and not getting more alignment.
Machines that are more unpredictable harder to interpret and no better at sticking to
even a basic principle like be honest and don't make stuff up.
In fact that's a problem that other technologies don't really have.
Routing systems GPS systems don't make stuff up.
Google search doesn't make stuff up it will point to things that other people have made
stuff up but it doesn't itself do it so in that sense like the trend line is not great.
I agree with that and I agree that we should be really worried about that and we should put
effort into it even if I don't agree you know with the probabilities that you attach to it.
I mean let me interject with the question here.
Go ahead Scott go ahead Scott then I'll ask a question.
No I mean I think that LASR you know deserves sort of eternal credit for you know raising
these issues 20 years ago and it was you know very very far from obvious to most of us that
they would be live issues. I mean I can say for my part you know I was familiar with
LASR's views since you know 2006 or so and when I first encountered them you know I you know I
didn't you know I knew that there was no principle that said that this scenario was impossible
but I just felt like well supposing I agreed with that what do you want me to do about it.
You know what where is the research program that has any hope of making progress here right.
I mean there's you know one question of what are the most important problems in the world but in
science that's necessary but not sufficient. We need something that we can make progress on
and you know that that is the thing that I think has changed just recently you know with the advent
of of actual very powerful AIs and so the the sort of irony here is that you know as
Eliezer has gotten you know much more pessimistic you know unfortunately in the last few years about
alignment you know I've sort of gotten more optimistic. I feel like well there is a research
program that we're that we can actually make progress on now. Yeah your research your research
program is going to take a hundred years and we don't know how long it will take. We don't know
that exactly we don't know. I think the argument that we should put a lot more effort into it is
clear. I think the argument will take a hundred years is totally unclear.
I mean I'm not even sure you can do it in a hundred years because there's the basic problem
of getting it right on the first try and the way these things are supposed to work in science
is that you have your bright-eyed optimistic youngsters with their vastly oversimplified
hopelessly idealistic optimistic plan. They charge ahead. They fail. They like learn a little
cynicism. They learn a little pessimism. They learn it's not as easy as that. They try again.
They fail again. They start to build up something over battle something like battle-hardening
and then and you know like you know they find out how little is possible to them.
Aliezer I mean this is a place where I just really don't agree with you so I think
there's all kinds of things we can do. There's sort of of the flavor of model organisms or
simulations and so forth and we just mean it's hard because we don't actually have a super
intelligence so we can't fully calibrate but it's a leap to say that there's nothing iterative
that we can do here or that we have to get it right on the first time. I mean I certainly see
a scenario where that's true where getting it right on the first time does make the difference
but I can see lots of scenarios where it doesn't and where we do have time to iterate
before it happens after it happens and it's really not a single moment but I'm
you know idealizing. I mean the problem is getting anything that generalizes up to the
super intelligent level where past some threshold level the minds may find that in their own
interest to start lying to you even if that happens before super intelligent.
Even that like I don't see the logical argument that you can't emulate that
we're studying it. I mean for example you could I'm just making this up as I go along but for
example you could study what can we do with sociopaths who are often very bright and you know
not to their dollar value. What can a what what strategy can a like 70 IQ
honest person come up with and invent themselves by which they will outwit and defeat a 130 IQ
sociopath. All right well there you're not being fair either in the sense that you know we actually
have lots of 150 IQ people who could be working on this problem collectively and there's there's
value in collective action. There's literature. What I see is what I see that gives me pause is
that is that the people don't seem to appreciate what about the problem is hard even at the level
where like 20 years ago I could have told you it was hard until you know somebody like me
comes along and enacts them about it and then they talk about the ways in which they could adapt
and be clever but but the people's charging straightforward are just sort of like doing
in this supremely naive way. Let me share a historical example that I think about a lot
which is in the early 1900s almost every scientist on the planet who thought about
biology made a mistake. They all thought that genes were proteins and then eventually Oswald
Avery did the right experiments. They realized that genes were not proteins. There was this weird
acid and it didn't take long after people got out of this stock mindset before they figured
out how that weird acid worked and how to manipulate it and how to read the code that it was in and
so forth. So I absolutely sympathize with the fact that I feel like the field is stuck right now.
I think the approaches people are taking to alignment are unlikely to work. I'm completely
with you there but I'm also I guess more long term optimistic that science is self-correcting
and that we have a chance here. Not a certainty but I think if you know we change research priorities
from how do we make some money off this large language model that's unreliable to how do I
save the species. We might actually make progress. There's a special kind of caution that you need
when something needs to be gotten correct on the first try. I'd be very optimistic if people got a
bunch of free retries and I didn't think the first one was going to kill you know the first
really serious mistake killed everybody and we didn't get to try again. If we got free retries
it'd be an ordinary you know it'd be in some sense an ordinary science problem.
Look I can imagine a world where we only got one try and if we failed then it destroys all life
on earth and so let me agree to the conditional statement that if we are in that world then I
think that we're screwed. I will agree with the same conditional statement.
Yeah this gets back to like below hold on you know if you picture by analogy the process of
you know a human baby which is extremely stupid becoming a human adult and then just extending
that so that in a single lifetime this person goes from a baby to the smartest being that's ever
lived but in the in the in the normal way that humans develop which is you know and it doesn't
happen any on any one given day and each sub skill develops a little bit at its own rate
and so forth it would not be at all obvious to me that our concerns that we have to get it right
vis-a-vis that individual the first time. I agree well well no pardon me I do think we have to get
them right the first time but I think there's a decent chance of getting it right. It is very
important to get it right the first time if like you have this one person getting smarter and smarter
and not everyone else is getting smarter and smarter. Eliezer I mean one thing that you've
talked about a lot recently is you know if we're all going to die then at least let us die with
dignity right. So you know I mean I mean for a certain technical definition some people might care
about that more than others but I would say that you know one thing that death with dignity would
mean is well at least you know if they're all if we do get multiple retries and you know we get
AIs that let's say try to take over the world but are really inept at it and that fail and so forth
at least let us succeed in that world you know and that's at least something that we can imagine
working on and making progress on. I mean you may very it's for it is not presently ruled out
that you have some like you know relatively smart in some ways dumb in some other ways
or at least like not smarter than human in other ways AI that makes an early shot at taking over
the world maybe because it expects future AIs to not share its goals and not cooperate with it
and it fails and you know I mean the appropriate lesson to learn there is to you know like shut
the whole thing down but you know if we so yeah like I would say so I'd be like yeah sure like
wouldn't it be good to live in that world and the way you live in that world is that when you get
that warning sign you shut it all down. Here's a kind of thought experiment. GBT-4 is probably not
capable of annihilating us all I think we agree with that very unlikely but GBT-4 is certainly
capable of expressing the desire to annihilate us all or being you know people have rigged
different versions that are you know more aggressive and and so forth. We could say look
until we can shut down those versions you know GBT-4s that are programmed to be malicious
by human intent maybe we shouldn't build GBT-5 or at least not GBT-6 or some other system etc we
could say you know what we have right now actually is part of that iteration we have you know primitive
intelligence right now it's nowhere near as smart as the super intelligence is going to be
but even this one we're not that good at constraining maybe we shouldn't pass go until we get this one
right. I mean the problem with that from my perspective is that I do think you that you
can pass this test and still wipe out humanity like I think that there comes a point where your AI
is smart enough that it knows which answer you're looking for and the point at which it tells you
what you want to hear is not the point that which is internal my test is not sufficient but it might
be a logical pause point right it might be that if we can't even pass the test now of you know
controlling a deliberate sort of fine-tuned to be malicious version of GBT-4 then we don't
know what we're talking about and we're playing around with fire so you know passing that test
wouldn't be a guarantee that would be in good stead with an even smarter machine but we really
should be worried I think that we're not in a very good position with respect even to the current
ones. Gary I of course watched the recent congressional hearing where you and Sam Altman
were testifying you know about what should be done should should should there be auditing of
these systems you know before training before deployment and you know it may be you know the
most striking thing about about that session was you know just how little daylight there seemed
to be between you and Sam Altman the CEO of OpenAI you know I mean you know he was completely on
board with the idea of you know establishing a regulatory framework for you know you know having
to clear the you know more powerful systems before they are deployed now you know in in
Aliezer's worldview that still would be woefully insufficient shortly and you know we would still
all be dead but you know maybe in your in your worldview that you know it sounds like
you know I'm not even sure how much daylight there is I mean the you know you know you know
have the very I think historically striking situation where you know the the heads of all
of the major AI or well almost all of the major AI organizations are you know agreeing and saying
you know please regulate us yes this is dangerous yes we need to be regulated I mean I thought it
was really striking in fact I talked to Sam just before you know the the hearing started and I had
just proposed an international agency for AI I wasn't the first person ever but I I pushed it in
my TED talk and an economist op ed a few weeks before and Sam said to me I like that idea and I
said tell them tell the Senate and he did and that kind of astonished me that he did I mean we have
you know we've had some friction between the two of us in the past and he actually even attributed
to me he said I support what Professor Marcus said about doing international governance and
there's been a lot of convergence around the world on that is that enough to stop Aliezer's
worries no I don't think so but it's an important baby step I think that we do need to have some
global body that can coordinate around these things I don't think we really have to
coordinate around superintelligence yet but if we can't do any coordination now then when the
time comes we're not prepared so I think it's great that there's some agreement I I worry that you
know open AI had this lobbying document that just came out that seemed not entirely consistent with
what Sam said in the room and there's always concerns about regulatory capture and so forth
but I think it's great that a lot of the the heads of these companies maybe with the exception
of Facebook or meta are recognizing that there are genuine concerns here I mean the other
moment that a lot of people remember from the testimony was when Sam was asked what he was
most concerned about was it jobs and he said no and I asked Senator Blumenthal to push Sam
and Sam was you know he could have been more candid but he was fairly candid
and he said he was worried about serious harm to the species I think that was an important moment
when he said that to the Senate and I think it galvanized a lot of people that he said it
so can we dwell on a moment um I mean we've been talking about the the depending on your view
highly likely or tail risk scenario of humanity's extinction or or significant destruction
it would appear to me by the same token if if those are plausible scenarios we're talking about
then the opposite maybe we're talking about as well um you know what does it look like to have
a super intelligent AI that really you know as if as a feature of its intelligence deeply
understands human beings the human species and also has a deep desire for us to be as happy as possible
what does that world look like oh is that possible no no not that that looks like you know
just like why are why are everyone leather centers to make them as happy as possible
but more like a parent wants their child to be happy right that may not involve any particular
scenario but is is generally quite concerned about the well-being of the human race and is also
super intelligent honestly I'd rather have machines work on medical problems than happiness problems
I think there's maybe more risk of mis-specification of the happiness problems um whereas if we get
them to work on Alzheimer's and just say like figure out what's going on why are these plaques
there what can you do about it maybe there's less harm that might come come from you don't need
super intelligence for that that sounds like an alpha fold three problem or an alpha fold four
problem well this is also this is somewhat different than the question I'm asking it's it's not really
even um us asking a super intelligence to do anything because we we've already been entertaining
scenarios where the super intelligence has its own desires independent of us is it do you think
at all yeah I'm not real thrilled with that I mean I mean I don't think we want to leave
what their objective functions are what their desires are to them working them out
you know with no consultation from us with no human in the loop right fully I mean especially
given our current understanding of the technology like our current understanding of how to keep
a system on track doing what we want to do is pretty limited and so you know taking humans out
of the loop there sounds like a really bad idea to me at least in the foreseeable future I would
want to see much better alignment technology before I would want to give free rent free range
so so so if we had the textbook from the future like we have the textbook from 100 years in the
future which contains all the simple ideas that actually work in real life as opposed to you
know the complicated ideas and the simple ideas that don't work in real life the equivalent of
relus instead of sigmoids for the activation functions you know 100 the textbook from 100
years in the future you can probably build a super intelligence that'll want anything you can
anything that's coherent to want anything you can you know figure out how to say describe
coherently point that at your own mind and tell you to figure out what what it is you meant
for to want and you know you could get the you could get the glorious transhumanist future you
could get the happily ever after anything's you know anything's possible that doesn't violate the
laws of physics the trouble is doing it in real life and you know and the first try but
uh yeah so like you know could the the the whole thing that we're we're aiming for here is to
colonize all the galaxies we can reach um before somebody else gets them first and turn them into
galaxies full of you know complex sapient life living happily ever after you know that that's
that's the goal that's still the goal even if we you know even even even when I call for like
you know a permanent moratorium on AI I'm not trying to prevent us from count from colonizing
the galaxies you know like humanity forbid um more more like let's you know let's like do some
human intelligence augmentation with alpha fold four and before we try building GPT-8 one of the
few scenarios that I think we can clearly rule out here is an AI that is existentially dangerous
but also boring right I mean I think anything that has the capacity to kill us all right would have
you know if if nothing else pretty amazing capabilities and those capabilities you know
could also be turned to you know solving a lot of humanities problems right you know if if we were
to solve the alignment problem I mean you know humanity had a lot of existential you know risks
you know before AI came on the scene right uh you know I mean there was the risk of of you
know nuclear annihilation there is the risk of runaway climate change and you know I would I
would love to see you know an AI that could help us with such things I would also love to see an AI
that could sort of you know help us just solve you know some of the mysteries of the universe I mean
you know like how can one possibly not be curious to know you know what what such a being could teach
us uh you know I mean I mean for the past year I've tried to use GPT-4 to produce original
scientific insights and I've not been able to get it to do that uh and you know I don't know
whether I should feel you know disappointed or relieved by that but I think you know the better
part of me should you know just is the part that should just want to see you know the great mysteries
of of existence of you know why is the universe quantum mechanical or you know how do you prove
the Riemann hypothesis it should just want to see these mysteries solved you know and and uh if it's
to be by AI then then then then fine let it be by AI let me give you a kind of lesson in epistemic
humility we don't really know whether GPT-4 is net positive or net negative you know there are
lots of arguments you can make I've been in a bunch of debates where I've you know had to take the
side of arguing that that it's a net negative but we don't really know if we don't know that
was the invention of agriculture net positive or net negative I mean you could you could I mean I
say it was not positive but but the point is if I can just finish the quick like thought experiment
or whatever I don't think anybody can reasonably answer that right we we don't yet know all of the
ways in which GPT-4 will be used for good we don't know all of the ways in which bad actors will
use it we don't know all the consequences that's going to be true for each iteration it's probably
going to get harder to compute for each iteration and we can't even do it now and I think that
we should realize that to realize our own limits in being able to assess the the negatives and
positives maybe that we can think about better ways to do that than we currently have but I think
you've got to have a guess like like my guess is that so far not looking into the future at all
GPT-4 has been net positive I mean maybe I haven't talked about the the various risks yet and it's
still early but I mean that's just a guess is kind of the point like we don't have a way of putting
it on a spreadsheet right now or whatever like we don't really have a good way to quantify it but I
mean do we ever but it's not out of control yet so so by and large people are going to be using
GPT-4 to use things to do things that they want and the relative cases where they manage to injure
themselves are rare enough to be news on Twitter well for example I mean we haven't talked about it but
you know what bad actors some bad actors will want to do is to influence the US elections and
try to undermine democracy in the US and if they succeed in that I think there's pretty serious
long-term consequences there well I think it's open AI's responsibility to step up and run the 2024
election itself I will I can pass that along is that a joke no I mean I mean as far as I can say
you know the the clearest concrete harm to have come from GPT so far is that you know tens of millions
of students have now used it to cheat on their assignments and I have been thinking about that
and I have been trying to come up with solutions to that at the same time I think if you do the
positive utility has included I mean you know I I'm a theoretical computer scientist which means
you know one who hasn't written any serious code for about 20 years and you know realized just a
month or two ago I can get back into coding and the way I can do it is I just asked GPT to write
the code for me and you know I wasn't expecting it to work that well and unbelievably it you know
often just does exactly what I want on the first try so I mean you know I you know I am getting
utility from it rather than just you know seeing it as an interesting research object and you know
and and you know I can imagine that that hundreds of millions of people are going to be deriving
utility from it in those ways I mean like most of the tools to help them derive that
utility are not even out yet but they're they're coming in the next couple of years
I mean part of the reason why I'm worried about the focus on short-term problems is that I suspect
that the short-term problems might very well be solvable and we'll be left with the long-term
problems after that maybe we can solve the like it wouldn't surprise me very much if like in 2025
the well you know like the large language there are large language models that just don't make
stuff up anymore it would surprise and yet even yet you know and yet the superintelligence still
kills everyone because they weren't the same problem well you know you know we just need to
figure out how to delay the apocalypse by at least one year per year of research invested
what what does that delay look like if it's not just a moratorium well I don't know that's why it's
research okay so but but possibly one ought to say to the politicians in the public and by the way
if we had a superintelligence tomorrow our research wouldn't be finished and everybody would drop
dead you know it's kind of ironic the biggest argument against the pause letter was that if we
slow down for six months then China will get ahead of us and get GPT-5 before we will but there's
probably always a counter argument of maybe roughly equal strength which is if we move six months
faster on this technology which is not really solving the alignment problem then we're reducing
our room to get this solved in time by six months I mean I don't think you're going to solve the
alignment problem in time I think that six months of delay on alignment while a bad thing in an
absolute sense is you know like you know you weren't going to solve it with given an extra six months
I mean your whole argument rests on timing right that that we will get to this point
and we won't be able to move fast enough at that point and so you know a lot depends on what
preparation we can do you know I'm often known as a pessimist but I'm a little bit more optimistic
than you are not entirely optimistic but a little bit more optimistic than you are that we could make
progress on the alignment problem if we prioritized it and you can absolutely make progress
because we can absolutely make progress you know there's there's always the you know that the
wonderful sense of accomplishment is piece by piece you decode you know like one more little fact
about LLMs you never get to the point where you understand that as well as we understood the
interior of a chess playing program in 1997 yeah I mean I think we should stop spending all this
time on LLMs I don't think the answer to alignment is going to come from LLM through LLMs I really
don't I think they're they're too much of a black box you can't put explicit symbolic constraints in
the way that you need to I think they're actually with respect to alignment to blind alley I think
with respect to writing code they're a great tool but with alignment I don't think the answer is there
so at the risk of asking a stupid question every time GPT asks me if that answer was helpful
and then does the same thing with thousands or hundreds of thousands of other people and
and changes as a result is that not a decentralized way of making it more aligned
yeah well yeah so so so there is that upvoting and downvoting you know that that gets
fed back in into sort of fine-tuning it but even before that there was you know a major step you
know in going from let's say the the base GPT 3 model for example to the chat GPT you know that
was released to the public and that was called a RLHF reinforcement learning with human feedback
and what that basically involved was you know several hundred contractors you know looking at
just just ten tens of thousands of examples of outputs and and and rating them you know are they
helpful are they offensive you know are they you know are are they you know giving dangerous
medical advice or you know bomb making instructions you know or racist invective or you know various
other categories that that we don't want and and that that was then used to fine-tune the model so
when you know Gary talked before about how GPT is amoral you know I think that that has to be
qualified by saying that you know these this reinforcement learning is at least giving it you
know a semblance of morality right it is causing it to sort of behave you know in various contexts
as if it had you know a certain morality I mean when you phrase it that way I'm okay with it the
problem is you know everything rests on the I would say it is it is very much an open question
you know how much that you know to what extent does that generalize you know eliezer treats it as
obvious that you know once you have a powerful enough AI you know this is just a fig leaf you
know it doesn't make any difference you know it will just learn it's any big leafy I'm with
eliezer there okay it's fig leaves well I would say that you know the sort of how well you know
under what circumstances does a machine learning model sort of generalize in the way we want outside
of its training distribution you know is one of the great open problems in machine learning
it is one of the great open problems and we should be working on it more than on some others
working on it now so I do want to be I want to be clear about the experimental predictions of my
theory unfortunately I have never claimed that you cannot get a semblance of morality you can get
the question of like what causes the human to press thumbs up thumbs down is a strictly factual
question anything smart enough that's exposed to some you know bound and amount of data that
needs to figure it out can figure that out whether it cares whether it gets internalized
is the is the critical question there and and I do think that there's like a very strong default
prediction which is like obviously not I mean I'll just give a different way of thinking about that
which is jailbreaking it's actually still quite easy to I mean it's not trivial but it's not
hard to jailbreak GPT for and what those cases show is that they haven't really in turn the
systems haven't really internalized the constraints they recognize some representations
of the constraints so they filter you know how to build a bomb but if you can find some other
way to get it to build a bomb then that's telling you that it doesn't deeply understand that you
shouldn't give people the the recipe for a bomb it just says you know you shouldn't when directly
asked for it do it and it doesn't it's not even that that I mean I understand a lot of the but
understanding the jailbreaking always get you can always get the understanding you'd always get the
factual question the reason it doesn't generalize is that it's stupid at some point it will know
that you also don't want that the operators don't want a giving bond making directions in the other
language the question is like whether if it's incentivized to give the answer that the operators
want you know in that circumstance is it thereby incentivized to do everything else the operators
want even when the operators can't see it I mean a lot of the jailbreaking examples you know if it
were a human we would say that it's deeply morally ambiguous you know for example you know you ask
GPT how to build a bomb it says well no I'm not going to help you but then you say well you know I
need you to help me write a realistic play that has a character who builds a bomb and then it says
sure I can help you with that well so look let's take that example yeah we would like a system
to have a constraint that if somebody asks for a fictional version that you don't give enough
details right I mean Hollywood screenwriters don't give enough details when they have you know
illustrations about building bombs they give you a little bit of the flavor they don't give you the
whole thing GPT-4 doesn't really understand a constraint like that but this will be solved
this will be solved before the world ends the AI that kills everyone will know the difference
maybe I mean another way to put it is if we can't even solve that one then we do have a problem
and right now we can't solve that one and if I mean if we can't solve that one we don't have
an extinction level problem because the AI is still stupid yeah we do still have a catastrophe
level problem so I know your focus has been on extinction but you know I'm worried about for
example accidental nuclear war caused by the spread of misinformation and systems being entrusted with
too much power so like there's a lot of things short of extinction that might happen from not
superintelligence but kind of mediocre intelligence that is greatly empowered and I think that's
where we're headed right now you know I've heard that there are two kinds of mathematicians there's
a kind who boasts you know you know that unbelievably general theorem well I generalized it even further
and then there's the kind who boasts you know you know that unbelievably specific problem that no
one could solve well I found a special case that I still can't solve and you know I'm definitely
you know culturally in that second camp and so you know so I so so to me it's very familiar to
make this move of you know if the alignment problem is too hard then let us find a smaller problem
that is already not solved and let us hope to learn something by solving that smaller problem
I mean that's what we did you know like that's what we're doing at Mary yes sorry no I was just
going to say Scott can you sketch a little in a little more detail where you took one particular
approach I was going to I was going to name the problem the problem was like having a
agent that could switch between two utility functions depending on a button or a switch
or a bit of information or something such that it wouldn't try to make you press the button
it wouldn't try to make you avoid pressing the button and if it built a copy of itself
would want to build the dependency on the switch into the copy so like that's an example of a you
know very basic problem and alignment theory that you know is still and I'm glad that Mary
worked on these things and but you know if by your own lights you know that you know that sort of
you know was not a successful path well then maybe you know we should have a lot of people
investigating a lot of different paths yeah I'm with fully with Scott on that that I think it's
an issue of we're not letting enough flowers bloom in particular almost everything right now
is some variation on an LLM and I don't think that that's a broad enough take on the problem
yeah if I if I can just jump in here I want to I want to hold on hold on I just want people to
have a little bit of a more specific picture of what Scott your your picture sort of AI
research is on a typical day because if I think of another you know potentially catastrophic risk
like climate change I can picture what a what a you know a worried climate scientist might be doing
they might be creating a model you know a more accurate model of climate change so that so that
we know how much we have to cut emissions by they might be you know modeling how solar power as
opposed to wind power could change that model and so forth so as to influence public policy
what does an AI safety researcher like yourself who's working on the quote-unquote smaller problems
do specifically like on a given day
yeah so I'm a relative newcomer to this area you know I've not been working on it for 20 years
like Eliezer has you know I have I accepted an offer from open AI a year ago to work with them for
two years now to sort of think about these questions and so so you know one of one of the
main things that that I've thought about just to start with that is how do we make the output of
an AI identifiable as such you know how can we insert a watermark you know into meaning a
secret statistical signal into the outputs of GPT that will let you know GPT generated text be
identifiable as such and I think that we've actually made you know major advances on that problem
over the last year you know we don't have a solution that is robust against any kind of attack
but you know we have something that that might actually be deployed in some near future now there
are lots and lots of other directions that people think about one of them is interpretability which
means you know can you do effectively neuroscience on a on a neural network can you look inside of it
you know open the black box and understand what's going on inside there was some amazing work
of a year ago by the group of Jacob Steinhardt at Berkeley where they effectively showed how
to apply a lie detector test to a language model so you know you can train a language model to tell
lies by giving it lots of examples you know two plus two is five the sky is orange and so forth
but then you can find in some internal layer of the network where it has a representation of what
was what was the truth of the matter or at least what was regarded as true in the training data
okay that truth then gets overridden by the output layer in the network because it was
trained to lie okay but you know you could imagine trying to deal with the you know the deceptive
alignment scenario that Eliezer is worried about by you know using these sorts of techniques by
sort of looking inside of the network I predict in advance that if you get this good enough
it goes off it tells you that the sufficiently smart AI is planning to kill you if it's not
so smart that it can you know know figure out where the lie detector is and route its thoughts
around it but if you like try it on an AI that's not quite that intelligent and reflective
the lie detector goes off now what well then you have a warning bell you know tell
you know and I think what do you do after one of the most important things that we need
are sort of legible warning bells right and that that actually what leads to a third category
which for example the ARC the Alignment Research Center which is run by my my former student
Paul Cristiano has been a leader in in sort of doing dangerous capability evaluations so you know
they before GPT-4 was released you know they did a bunch of evaluations of you know could GPT-4
make copies of itself could it figure out how to deceive people could it figure out how to make
money you know open up its own money could it hire a task rabbit yes and yes so so the most
notable success that they had was that it could figure out how to hire a task rabbit to help it
you know pass a capture and then it could figure out you know when the person asked well you know
why do you need me to help you with this it's a when the person asked are you a robot well yes it
said well no I am visually impaired now you know it was not able to sort of make copies of itself
or to sort of hack into systems you know there there is a lot of work right now with the you
know this thing called auto GPT right people are trying to you know you could think it's almost
like gain of function research right you might be a little bit worried about it but people are
trying to sort of you know unleash GPT give it access to the internet you know tell it to sort of
you know make copies of itself you know wreak havoc acquire power and see what happens so far
you know it seems pretty ineffective at those things but you know I expect that to change right
and but but but you know the point is that I think it's very important to have you know
in advance of training the models releasing the models to have this suite of evaluations
and to sort of have decided in advance what kind of abilities if we see them we'll set off a
warning bell where now everyone can legibly agree like yes this is too dangerous to release
okay and then do we actually have the planetary capacity to be like okay that AI started thinking
about how to kill everyone shut down all AI research past this point well I don't know but I think
there's a much better chance that we have that capacity if you can point to the results of a
clear experiment like that I mean to me it seems pretty predictable what evidence we're going to get
later well okay I mean things that are obvious to you are not obvious to most people and so you
know even if even if I agreed that it was obvious there would still be the problem of how do you
make that obvious to the rest of the world I mean you can you know they there are already like
little toy models showing that the very straightforward prediction of a robot tries to resist being
shut down if it like does long-term planning like that that's already been right but then people
will say but those are just toy models right you know if you see that there's a lot of assumptions
made in all of these things and you know I think we're still looking at a very limited piece of
hypothesis space about what the models will be about what kinds of constraints we can build into
those models you know one way to look at it would be the things that we have done have not worked
and therefore we should look outside the space of what we're doing and I feel like it's a little
bit like the old joke about the drunk going around in circles looking for the keys and the police
officer says why and they say well that's where the streetlight is I think that you know we're
looking under the same four or five streetlights they haven't worked and we need to build other
ones there's no logical there's no logical argument that says we couldn't direct other
streetlights who's I think there's a lack of will and too much obsession with the LLMs and that's
keeping us from doing so even in the world where I'm right and things you know proceed either
rapidly or in a thresholded way where you don't get unlimited free retries you know that can be
because the the capability gains go too fast it can be because past a certain point all of your
ai's buy their time until they get strong enough so you don't get any data any any like true data
on what they're thinking it could be because you know that's an argument for example to work really
hard on transparency and maybe not except technologies that are not transparent okay so like the
transparent so like the lie detector goes off and everybody's like oh well we still have to build our
ai's even though they're lying to us sometimes because otherwise China will get ahead I mean
so there you talk about something we've talked about way too little which is the political
and social side of this so you know part of what has really motivated me in the last several months
is worry about exactly that so you know there's there's what's logically possible and what's
politically possible and I am really concerned that the politics of let's not lose out to China
is going to keep us from doing the right thing in terms of building the right
moral systems looking at the right range of problems and so forth so you know it is entirely
possible that we will screw ourselves if I if I can just like finish my point there before handing
it to you indeed but like the point I was trying to say there is that even in worlds that look very
very bad from that perspective where humanity is quite doomed it will still be true you can make
progress in research you can't make enough progress in research fast enough in those worlds
but you can still make progress on transparency you can make progress on watermarking so there's
there's not we can't just say like it's possible to make progress there has to be the question
is not is it possible to make any progress the question is it is it possible to make enough
progress fast enough and that's what the question has to be I agree there's another question of what
would you have us do would you have us not try to make that progress I'd have you try to make that
progress on a GPT-4 level systems and then not go past GPT-4 level systems because we don't actually
understand the the the gain function for you know how how fast capabilities increase as you go past
GPT-4 okay all right so I mean we are going out I don't think that you go ahead Gary go ahead
just briefly I personally don't think that GPT-5 is going to be qualitatively different from GPT-4
in the relevant ways to what Eleazar is talking about but I do think you know some qualitative
changes could be relevant to what he's talking about we have no clue what they are and so it is
a little bit dodgy to just proceed blindly saying do whatever you want we don't really have a theory
and let's hope for the best you know Eleazar I would mostly guess that GPT-5 doesn't end the
world but I don't actually know yeah we don't actually know and I was going to say the thing
that Eleazar has said lately that has most resonated with me is we don't have a plan we really don't
like I think I put the probability distributions in a much more optimistic way I think that Eleazar
would but I completely agree we don't have a full plan on these things or even close to a full plan
and we should be worried and we should be working on this okay Scott I'm going to give you the last
word before before we come up on our stop time here unless you unless you said all there is to be
a weighty responsibility maybe enough has been said cheers up Scott come on
so so I think that that you know we've we've argued about a bunch of things but you know
as someone listening might notice that actually all three of us despite having very different
perspectives agree about you know the the great importance of of you know working on AI alignment
I think you know that was you know maybe obvious to some people including Eleazar for a long time
it was not obvious to most of the world I think that you know the the success of of large language
models you know which most of us did not predict you know maybe even could not have predicted
for many principles that we knew but now that we've seen it the least we can do is to update
on that on that empirical fact and and realize that you know we we we now are in some sense in a
different world we are in a world that you know to a great extent you know will be defined by
you know the capabilities and limitations of AI going forward and you know I don't regard it as
obvious that that's a a a world where where we are all doomed where where we all die but you
know I also don't dismiss that possibility I think that you know there there is an enormous
unbelievably enormous error bars on on on where we could be going and you know like the one thing
you know that that a scientist is sort of always always feels confident in in saying about the
future is that more research is needed but you know I think that that's especially the case here
I mean you know we need more knowledge about you know what are the the contours of the alignment
problem and you know of course Eliezer and you know Amiri you know his his organization were
trying to develop that knowledge for 20 years you know and they showed a lot of foresight in
trying to do that but you know they were up against you know an enormous headwind that
you know they were sort of trying to do it in the absence of you know either you know clear
empirical data you know about powerful ai's or a mathematical theory right and it's really really
hard to do science when you have neither of those two things and now at least we have
you know the powerful ai's in the world and we can get experience from them you know we still
don't have a mathematical theory that really deeply explains what they're doing but at least
we can get data and so now I am much more optimistic than I would have been you know a decade ago
let's say that one could make actual progress on on on the ai alignment problem you know of course
you know there was a question of timing as as was discussed many times the question is you know will
the alignment research happen fast enough to keep up with the capabilities research but you know I
don't I don't regard it as a lost cause you know it's at least it's not obvious that it won't so
you know in any case let's get started or let's let's uh or let's let's continue let's let's let's
try to do the research and let's get more people working on that I think that that that is now uh
a slam dunk you know just a completely clear case to make to you know academics to policymakers to
to anyone who's interested and you know I've been gratified that that you know uh you know
aliezer was sort of a voice in the wilderness for for a long time talking about the importance of
ai safety that is no longer the case uh you now have you know you know I mean almost all of my
friends in you know in just the academic computer science world you know when I see them they mostly
want to talk about AI alignment I rarely agree with Scott when we trade email um I rarely agree
with Scott when we trade emails we seem to always disagree but I completely concur with the summary
that he just gave all four or five minutes of it well thank you I mean I mean there is a selection
effect Gary right we focus on things I think the two decades gave me a sense of a roadmap and it gave
me a sense that we're falling enormously behind on the roadmap I need to back off as the way I
is what I would say to all that if there is a smart talented 18 year old kid listening listening to
this podcast who wants to get into this issue what is your 10 second concrete advice to that person
mine is study neurosymbolic AI and see if there's a way there to represent values explicitly that
might help us learn all you can about computer science and math and related subjects and think
outside the box and wow everyone with a new idea get security mindset figure out what's going to go
wrong figure out the flaws in your arguments for what's going to go wrong try to get ahead of the
curve don't wait for reality to hit you over the head with things uh this this is very difficult
the people in evolutionary biology happen to have a bunch of knowledge about how to do it based on
the history of their own field but uh and and and the security mindset people in computer security
but it's it's quite hard I'll drink to all of that thanks thanks to all three of you for this
this was a great conversation and I hope people got something out of it so with that said
we're wrapped up thanks so much that's it for this episode of conversations with Coleman guys
as always thanks for watching and feel free to tell me what you think by reviewing the podcast
commenting on social media or sending me an email to check out my other social media platforms
click the cards you see on screen and don't forget to like share and subscribe see you next time
