1
00:00:00,000 --> 00:00:10,000
Well, everybody, great to be with you again. My name is Michael Millerman. And today we're going to go over this article called why AI will save the world.

2
00:00:10,000 --> 00:00:23,000
Reading it together with you for the first time, although the author here was on the Lex Friedman show recently and it was a clip from their interview I saw that referred to this article and intrigued me enough to want to read it together with you.

3
00:00:24,000 --> 00:00:31,000
So in just a minute, we'll do that. Like, share, subscribe, visit millermanschool.com for courses on philosophy and politics.

4
00:00:31,000 --> 00:00:42,000
More and more of my students are talking to me these days about the implications of artificial intelligence for sovereignty and for the common good for our basic ideas of political order.

5
00:00:42,000 --> 00:00:53,000
And therefore it's a good idea for us to get more into the details of how some people are speculating about the political and human significance of these new systems.

6
00:00:53,000 --> 00:01:01,000
Okay, so let's begin. The era of artificial intelligence is here and boy are people freaking out.

7
00:01:01,000 --> 00:01:09,000
Fortunately, I'm here to bring the good news. AI will not destroy the world and in fact may save it.

8
00:01:09,000 --> 00:01:22,000
First, the short description of what AI is the application of mathematics and software code to teach computers how to understand, synthesize and generate knowledge in ways similar to how people do it.

9
00:01:22,000 --> 00:01:28,000
AI is a computer program like any other it runs takes input processes and generates output.

10
00:01:28,000 --> 00:01:41,000
AI's output is useful across a wide range of fields, ranging from coding to medicine to law to the creative arts. It is owned by people and controlled by people like any other technology.

11
00:01:41,000 --> 00:01:51,000
So you see very quickly and early on here guarding against the idea that there's something autonomous or sovereign or tyrannical about AI.

12
00:01:51,000 --> 00:01:56,000
It's a tool and like any other tool it works in the hands of people.

13
00:01:56,000 --> 00:02:10,000
He says, let's see a shorter description of what AI isn't killer software and robots that will spring to life and decide to murder the human race, or otherwise ruin everything like you see in the movies.

14
00:02:10,000 --> 00:02:17,000
And even shorter description of what AI could be a way to make everything we care about better.

15
00:02:17,000 --> 00:02:26,000
Okay, so it's not going to be a runaway killer technology, but it might be something that we use to make the world a better place.

16
00:02:26,000 --> 00:02:40,000
Why AI can make everything we care about better. The most validated core conclusion of social science across many decades and thousands of studies is that human intelligence makes a very broad range of life outcomes better.

17
00:02:40,000 --> 00:03:06,000
Smarter people have better outcomes in almost every domain of activity, academic achievement, job performance, occupational status, income, creativity, physical health, longevity, learning new skills, managing complex tasks, leadership, entrepreneurial success, conflict resolution, reading comprehension, financial decision making, understanding others perspectives, creative arts, parenting outcomes, and life satisfaction.

18
00:03:06,000 --> 00:03:23,000
Further, human intelligence is the lever that we have used for millennia to create the world we live in today. Science, technology, math, physics, chemistry, medicine, energy, construction, transportation, communication, art, music, culture, philosophy, ethics, morality.

19
00:03:23,000 --> 00:03:41,000
Without the application of intelligence on all these domains, we would all still be living in mud huts, scratching out a meager existence of subsistence farming. Instead, we have used our intelligence to raise our standard of living on the order of 10,000 times over the last 4,000 years.

20
00:03:41,000 --> 00:03:57,000
What AI offers us is the opportunity to profoundly augment human intelligence, to make all of these outcomes of intelligence and many others from the creation of new medicines, to ways to solve climate change, to technologies to reach the stars, much, much better from here.

21
00:03:57,000 --> 00:04:17,000
AI augmentation of human intelligence has already started. AI is already around us in the form of computer control systems of many kinds, and is now rapidly escalating with AI large language models like chat GPT, and will accelerate very quickly from here if we let it.

22
00:04:17,000 --> 00:04:38,000
In our new era of AI, and here's I guess the vision of what it could be if it is developed to be helpful to humanity as a tool that augments our intelligence and proves our quality of life in our new era of AI, every child will have an AI tutor that is infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely helpful.

23
00:04:38,000 --> 00:04:48,000
I should add though, not if it's been trained on human experience in that case, I think it will get won't have these characteristics, certainly won't have any basis on which to model them.

24
00:04:48,000 --> 00:05:02,000
Okay, infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely helpful. The AI tutor will be by each child's side, every step of their development, helping them maximize their potential with the machine version of infinite love.

25
00:05:03,000 --> 00:05:17,000
I have to think about that because I have have some kids of my own. And one of them right now is with his grandma, and they're doing complicated math, you know, Russian, Soviet Jewish grandma type.

26
00:05:17,000 --> 00:05:35,000
Of course, the child is doing math, well, beyond his ears. And I wonder, you know, it's under the pressure somehow, well, when he doesn't crack under the pressure of something other than infinitely compassionate, infinitely patient, infinitely helpful, infinite love.

27
00:05:35,000 --> 00:05:51,000
He develops his mathematical skills and I've seen kids before in swim school, and they train with a teacher who's not infinitely patient and all of that but rather, whose strictness somehow is correlated with their development.

28
00:05:51,000 --> 00:06:05,000
So I just wonder, you know, I have some experience of human tutors and human teachers producing excellent results in their students. And you just wonder, will infinite love and infinite compassion really produce the best outcomes.

29
00:06:05,000 --> 00:06:18,000
It can't doesn't go without saying but it does sound good. Okay, next, every person will have an AI assistant coach mentor trainer advisor therapist, that is infinitely patient infinitely compassionate infinitely knowledgeable and infinitely helpful.

30
00:06:18,000 --> 00:06:26,000
The AI assistant will be present through all of life's opportunities and challenges maximizing every person's outcomes.

31
00:06:26,000 --> 00:06:35,000
Hmm. How does that sound to you? Does that sound plausible and AI assistant that can help you with everything is super compassionate knows you very well.

32
00:06:35,000 --> 00:06:42,000
And you have any questions about anything. What should I do in my job? What should I do with my wife? What should I do with my friends? What should I do with my business?

33
00:06:43,000 --> 00:06:57,000
And the AI is going to maximize your outcomes through all of your opportunities and challenges because of its infinite capacities. It's going to be like a little pocket Jesus there with you all the time to help you in everything that you are doing.

34
00:06:57,000 --> 00:07:15,000
Every scientist will have an AI assistant collaborator partner that will greatly expand their scope of scientific research and achievement. Every artist, every engineer, every business person, every doctor, every caregiver will have the same in their worlds.

35
00:07:15,000 --> 00:07:30,000
Every leader of a people. Okay, obviously this is a strong pitch for the fact that everybody on earth can benefit from AI, but let's see. Nevertheless, how it spun out every leader of people CEO government official non profit president athletic coach teacher will have the same.

36
00:07:30,000 --> 00:07:39,000
The magnification effects of better decisions by leaders across the people they lead are enormous. So this intelligence augmentation may be the most important of all.

37
00:07:40,000 --> 00:07:45,000
You know, this is as somebody who studies and teaches the history of political philosophy and political theory.

38
00:07:45,000 --> 00:08:05,000
It is the question of the relationship of artificial intelligence systems to governance and to ideas about ruling that interest me and that students of mine bring up to me in discussions, more so than for example the significance of AI systems for artists and engineers, and so on.

39
00:08:05,000 --> 00:08:18,000
So this is an interesting field here how AI systems would improve or could improve or whether they would have a detrimental effect on rule.

40
00:08:18,000 --> 00:08:30,000
Okay, productivity growth throughout the economy will accelerate dramatically driving economic growth creation of new industries creation of new jobs and wage growth and resulting in a new era of heightened material prosperity across the planet.

41
00:08:30,000 --> 00:08:45,000
Sounds optimistic. Obviously is optimistic questions whether it's accurate and plausible scientific breakthroughs and new technologies and medicines will dramatically expand as AI helps us further decode the laws of nature and harvest them for our benefit.

42
00:08:45,000 --> 00:08:58,000
The creative arts will enter a golden age, as AI augmented artists musicians writers and filmmakers gain the ability to realize their visions far faster and at greater scale than ever before again.

43
00:08:58,000 --> 00:09:09,000
I'm not sure that speed and scale are the right criterion for the status or the level of development of the creative arts to you.

44
00:09:10,000 --> 00:09:22,000
Like, I don't know. When you think about the golden age of architecture or something like that or the golden age of music, the golden age of painting, the golden age of film.

45
00:09:22,000 --> 00:09:36,000
Is it going to be better just because it's faster and at greater scale. Everybody can just plug in there. I don't know they had a strange dream at night they plug it into their AI system and the next day they have what a 3D printed.

46
00:09:36,000 --> 00:09:47,000
Who knows what. So I wonder, okay, but fine. There are smaller scale creative arts that might benefit. Listen, when I was younger, I used to be a big fan of Oasis, the rock and roll band.

47
00:09:47,000 --> 00:09:59,000
And recently I've heard AI augmented artists write songs that they have the lead singer of Oasis, Liam Gallagher, his voice on even though he didn't sing it.

48
00:09:59,000 --> 00:10:09,000
So AI augmented music where you can hear an artist you like singing a song that he didn't write that you wrote for him or something like that. And frankly, I thought that was pretty good. I enjoyed it.

49
00:10:09,000 --> 00:10:16,000
So there is something there that can benefit music and I assume also these other artistic disciplines too.

50
00:10:16,000 --> 00:10:24,000
I even think he continues AI is going to improve warfare when it has to happen by reducing wartime death rates dramatically.

51
00:10:24,000 --> 00:10:32,000
Every war is characterized by terrible decisions made under intense pressure and with sharply limited information by very limited human leaders.

52
00:10:32,000 --> 00:10:42,000
Now military commanders and political leaders will have AI advisors that will help them make much better strategic and tactical decisions minimizing risk, error and unnecessary bloodshed.

53
00:10:42,000 --> 00:10:46,000
Okay, but what if the AI says you have to be more brutal than you were planning to be.

54
00:10:46,000 --> 00:10:53,000
In short, anything that people do with their natural intelligence today can be done much better with AI.

55
00:10:53,000 --> 00:11:01,000
And we will be able to take on new by the way the assumption there obviously is that anything, you know, more intelligence means you can do things better.

56
00:11:01,000 --> 00:11:07,000
More natural intelligence means you can do things better. And if you augment your natural intelligence also means you can do things better.

57
00:11:07,000 --> 00:11:16,000
We will be able to take on new challenges that have been impossible to tackle without AI from curing all diseases to achieving interstellar travel.

58
00:11:17,000 --> 00:11:25,000
Sounds like promising the moon, in my opinion, but okay, and this isn't just about intelligence, perhaps the most underestimated quality of AI is how humanizing it can be.

59
00:11:25,000 --> 00:11:32,000
AI art gives people who otherwise lack technical skills, the freedom to create and share their artistic ideas.

60
00:11:32,000 --> 00:11:38,000
Talking to an empathetic AI friend really does improve their ability to handle adversity.

61
00:11:38,000 --> 00:11:44,000
You guys probably know or you may know the movie her speaking of an empathetic AI friend.

62
00:11:44,000 --> 00:11:57,000
And AI medical chatbots are already more empathetic than their human counterparts, rather than making the world harsher and more mechanistic infinitely patient and sympathetic AI will make the world warmer and nicer.

63
00:11:57,000 --> 00:12:07,000
Okay, can you guarantee I stop here to ask many questions you may have questions of your own but can you guarantee that the AI is going to be infinitely patient and sympathetic.

64
00:12:07,000 --> 00:12:15,000
Can you ensure at the outset that it's going to want to make the world warmer and nicer.

65
00:12:15,000 --> 00:12:30,000
That it won't be unintended consequences and unexpected suggestions and all the other kinds of things that could surprise somebody here if they're dealing with an intelligence system that's more intelligent than any human intelligence.

66
00:12:30,000 --> 00:12:47,000
It's kind of like you know people in theology and philosophy of religion who say there's a kind of similarity between the intelligence of man and God's intelligence but it's not a similarity that you can really understand or exploit and it's almost even blasphemous to think about because

67
00:12:47,000 --> 00:12:59,000
God is so much superior to man. Why should there be any part of us that coincides with or call less is with something so qualitatively superior.

68
00:12:59,000 --> 00:13:09,000
So you can't really even understand. It's only in some strange and analogical form that you can say human intelligence and divine intelligence call us at any point.

69
00:13:09,000 --> 00:13:23,000
Well OK fine if these artificial intelligence systems really reach whatever they're speculating in level of IQ and capacity. Why do we assume it will have any similarity to the way that we think.

70
00:13:23,000 --> 00:13:33,000
You know maybe it will recognize you know what the world doesn't need to be warmer and nicer. That's a low IQ thought that the world should be warm and nice.

71
00:13:34,000 --> 00:13:43,000
You know what I mean. Like how do you know when you by the time you get to whatever 220 IQ or wherever these things go if the systems continue to advance.

72
00:13:43,000 --> 00:13:53,000
It's like when I create an amazing AI chess player is going to play chess exactly the same way as I do but a little bit better or it's going to play go exactly the same way that I do but better.

73
00:13:53,000 --> 00:14:01,000
No it might find a completely new way of playing the game destroy you every time. So a lot of assumptions built in here about the relationship between.

74
00:14:01,000 --> 00:14:10,000
Like well intelligence become qualitatively different as it becomes quantitatively superior the stakes here are high the opportunities are profound AI is quite possibly.

75
00:14:10,000 --> 00:14:19,000
The most important and best thing our civilization has ever created certainly on par with electricity and microchips and probably beyond those.

76
00:14:19,000 --> 00:14:28,000
The development and proliferation of AI far from a risk that we should fear is a moral obligation that we have to ourselves to our children into our future.

77
00:14:29,000 --> 00:14:33,000
We should be living in a much better world with AI and now we can.

78
00:14:33,000 --> 00:14:48,000
Okay so there's the strong first part of the paper strong case for the idea that AI will make the world a better place in all respects in the arts in the sciences in medicine.

79
00:14:48,000 --> 00:14:50,000
In government.

80
00:14:50,000 --> 00:14:55,000
A golden age of humanity because AI will make everything we care about better.

81
00:14:55,000 --> 00:15:06,000
But as I'm sure you know there are some people who don't think so and who are worried and who are asking to pause the development of AI and so on so he naturally transitions to the question why the panic.

82
00:15:06,000 --> 00:15:14,000
In contrast to this positive view the public conversation about AI is presently shot through with hysterical fear and paranoia.

83
00:15:14,000 --> 00:15:23,000
We hear claims that AI will variously kill us all ruin our society take all our jobs cause crippling inequality and enable bad people to do awful things.

84
00:15:23,000 --> 00:15:29,000
What explains this divergence and potential outcomes from near utopia to horrifying dystopia.

85
00:15:29,000 --> 00:15:42,000
Historically every new technology that matters from electric lighting to automobiles to radio to the internet has sparked a moral panic a social contagion that convinces people the new technology is going to destroy the world or society or both.

86
00:15:42,000 --> 00:15:52,000
The fine folks at pessimists archive have documented these technology driven moral panics over the decades their history makes the pattern vividly clear.

87
00:15:52,000 --> 00:15:56,000
It turns out this present panic is not even the first for AI.

88
00:15:56,000 --> 00:16:00,000
Now it is certainly the case that many new technologies have led to bad outcomes.

89
00:16:00,000 --> 00:16:05,000
Often the same technologies that have been otherwise enormously beneficial to our welfare.

90
00:16:05,000 --> 00:16:10,000
So it's not that the mere existence of a moral panic means there's nothing to be concerned about.

91
00:16:10,000 --> 00:16:13,000
But a moral panic is by its very nature irrational.

92
00:16:13,000 --> 00:16:21,000
It takes what may be a legitimate concern and inflates it into a level of hysteria that ironically makes it harder to confront actually serious concerns.

93
00:16:21,000 --> 00:16:32,000
I want to say here not that I not that this is the last word but a moral panic may be irrational and may take what's legitimate and inflate it.

94
00:16:32,000 --> 00:16:50,000
But so too is a kind of zealous over optimism and over optimism that takes what may be a legitimate hope and inflates it into a level of not hysteria but you know euphoria or mania.

95
00:16:50,000 --> 00:16:52,000
That makes it harder to confront actually serious matters.

96
00:16:52,000 --> 00:17:04,000
So there's a there's a flip side to this the irrationality of moral panic but also the possibly the irrationality of over optimistic over zealous hopefulness that we shouldn't forget.

97
00:17:04,000 --> 00:17:05,000
Okay.

98
00:17:05,000 --> 00:17:08,000
And wow do we have a full blown moral panic about AI right now.

99
00:17:08,000 --> 00:17:16,000
Let's just see where does this link to.

100
00:17:16,000 --> 00:17:21,000
The AI arms race is changing everything time magazine February 17 2023.

101
00:17:21,000 --> 00:17:23,000
Can we look at that later.

102
00:17:23,000 --> 00:17:33,000
The moral panic is already being used as a motivating force by a variety of actors to demand policy action new AI restrictions regulations and laws.

103
00:17:33,000 --> 00:17:44,000
These actors who are making extremely dramatic public statements about the dangers of AI feeding on and further inflaming moral panic all present themselves as selfless champions of the public good.

104
00:17:44,000 --> 00:17:48,000
But are they and are they right or wrong.

105
00:17:48,000 --> 00:18:01,000
You see always shows you just how important whatever technological era we are in so much of politics is a debate over what's good over the public good over the common good.

106
00:18:01,000 --> 00:18:08,000
Is AI going to contribute to the common good the public good or is AI going to be detrimental to the common good.

107
00:18:08,000 --> 00:18:12,000
Well for sure we have to look into the arguments like he's about to do.

108
00:18:12,000 --> 00:18:21,000
And then we also have to have a broad understanding of what is meant by the public good what is good politically what is the good society what's the good human life.

109
00:18:21,000 --> 00:18:30,000
And those questions are old questions in the history of political philosophy from Plato's Republic to Aristotle's politics all the way down to Machiavelli and each and so on.

110
00:18:30,000 --> 00:18:40,000
So if you ever want to go to millerman school dot com and you'll see I have courses on the history of political philosophy but right now we continue with this the Baptists and bootleggers of AI.

111
00:18:40,000 --> 00:18:45,000
Economists have observed a long standing pattern and reform movements of this kind.

112
00:18:45,000 --> 00:18:56,000
And actors within movements like these fall into two categories Baptists and bootleggers drawing on the historical example of the prohibition of alcohol in the United States in the 1920s.

113
00:18:56,000 --> 00:19:08,000
Baptists are the true believer social reformers who legitimately feel deeply and emotionally if not rationally that new restrictions regulations and laws are required to prevent societal disaster.

114
00:19:08,000 --> 00:19:17,000
All prohibition these actors were often literally devout Christians who felt that alcohol was destroying the moral fabric of society for AI risk.

115
00:19:17,000 --> 00:19:24,000
These actors are true believers that AI presents one or another existential risks strap them to a polygraph they really mean it.

116
00:19:24,000 --> 00:19:28,000
So these are the AI skeptics are the ones who want to deprive you of the pleasures of alcohol.

117
00:19:29,000 --> 00:19:40,000
They're going to be the prohibitionist bootleggers are the self interested opportunists who stand to financially profit by the imposition of new restrictions regulations and laws that insulate them from competitors.

118
00:19:40,000 --> 00:19:48,000
For alcohol prohibition these were the literal bootleggers who made a fortune selling illicit alcohol to Americans when legitimate alcohol sales were banned.

119
00:19:49,000 --> 00:19:52,000
For AI risk these are CEOs who stand to make more money.

120
00:19:52,000 --> 00:20:03,000
If regulatory barriers are erected that form a cartel of government blessed AI vendors protected from new startup and open source competition the software version of too big to fail banks.

121
00:20:03,000 --> 00:20:14,000
Okay, a cynic would suggest that some of the apparent Baptists are also bootleggers specifically the ones paid to attack AI by their universities think tanks activist groups and media outlets.

122
00:20:14,000 --> 00:20:24,000
If you are paid a salary or receive grants to foster AI panic you're probably a bootlegger in other words you benefit from the from this from restriction of AI.

123
00:20:24,000 --> 00:20:30,000
The problem with bootleggers is that they win the Baptists are naive ideologues the bootleggers are cynical operators.

124
00:20:30,000 --> 00:20:39,000
And so the result of reform movements like these is often that bootleggers get what they want regulatory capture insulation from competition the formation of a cartel.

125
00:20:39,000 --> 00:20:45,000
And the Baptists are left wondering where their drive for social improvement went so wrong.

126
00:20:46,000 --> 00:20:52,000
We've just lived through a stunning example of this banking reform after the 2008 global financial crisis.

127
00:20:52,000 --> 00:21:00,000
The Baptists told us that we needed new laws and regulations to break up the too big to fail banks to prevent such a crisis from ever happening again.

128
00:21:00,000 --> 00:21:10,000
So Congress passed the Dodd-Frank Act of 2010, which was marketed as satisfying the Baptist school but in reality was co-opted by the bootleggers, the big banks.

129
00:21:10,000 --> 00:21:15,000
The result is that the same banks that were too big to fail in 2008 are much, much larger now.

130
00:21:15,000 --> 00:21:24,000
So in practice, even when the Baptists are genuine and even when the Baptists are right, they are used as cover by manipulative and venal bootleggers that benefit themselves.

131
00:21:24,000 --> 00:21:28,000
And this is what is happening in the drive for AI regulation right now.

132
00:21:28,000 --> 00:21:33,000
However, it isn't sufficient to simply identify the actors and impugn their motives like he's just done in this section.

133
00:21:33,000 --> 00:21:38,000
We should consider the arguments of both the Baptists and the bootleggers on their merits.

134
00:21:38,000 --> 00:21:42,000
So now he's going to turn to AI risk. Number one, will AI kill us all?

135
00:21:42,000 --> 00:21:44,000
Hope not.

136
00:21:44,000 --> 00:21:53,000
We're reading this article by Mark and Andreessen called Why AI Will Save the World.

137
00:21:53,000 --> 00:21:57,000
This was published not too long ago, I think just earlier this month.

138
00:21:57,000 --> 00:22:01,000
And I saw it come up very briefly on my YouTube feed today.

139
00:22:01,000 --> 00:22:05,000
A clip of somebody else discussing it and I thought, you know, I haven't read it. It looks pretty intriguing. Let's go over it.

140
00:22:05,000 --> 00:22:08,000
So thanks for being here. Like, share, subscribe and all the rest of it.

141
00:22:08,000 --> 00:22:15,000
Feel free to be active in the chat if you have thoughts on these topics and we're going to continue on once I find our spot here.

142
00:22:15,000 --> 00:22:17,000
Here we go. Will AI kill us all?

143
00:22:18,000 --> 00:22:24,000
The first and original AI do more risk is that AI will decide to literally kill humanity.

144
00:22:24,000 --> 00:22:31,000
The fear that technology of our own creation will rise up and destroy us is deeply coded into our culture.

145
00:22:31,000 --> 00:22:34,000
The Greeks expressed this fear in the Prometheus myth.

146
00:22:34,000 --> 00:22:40,000
Prometheus brought the destructive power of fire and more generally technology, technique, to man,

147
00:22:40,000 --> 00:22:44,000
for which Prometheus was condemned to perpetual torture by the gods.

148
00:22:44,000 --> 00:22:50,000
Later, Mary Shelley gave us moderns, our own version of this myth in her novel Frankenstein or the modern Prometheus,

149
00:22:50,000 --> 00:22:56,000
in which we developed the technology for eternal life, which then rises up and seeks to destroy us.

150
00:22:56,000 --> 00:23:06,000
And of course, no AI panic newspaper story is complete without a still image of a gleaming red-eyed killer robot from James Cameron's Terminator films.

151
00:23:06,000 --> 00:23:14,000
The presumed evolutionary purpose of this mythology is to motivate us to seriously consider potential risks of new technologies.

152
00:23:14,000 --> 00:23:19,000
Fire, after all, can indeed be used to burn down entire cities.

153
00:23:19,000 --> 00:23:27,000
But just as fire was also the foundation of modern civilization, as used to keep us warm and safe in cold and hostile world,

154
00:23:27,000 --> 00:23:32,000
this mythology ignores the far greater upside of most or all new technologies,

155
00:23:32,000 --> 00:23:36,000
and in practice inflames destructive emotion rather than reasoned analysis.

156
00:23:36,000 --> 00:23:41,000
Just because pre-modern man, freaked out like this, doesn't mean we have to.

157
00:23:41,000 --> 00:23:45,000
We can apply rationality instead.

158
00:23:45,000 --> 00:23:50,000
My view is that the AI will decide, excuse me, my view, he writes,

159
00:23:50,000 --> 00:23:57,000
is that the idea that AI will decide to literally kill humanity is a profound category error.

160
00:23:57,000 --> 00:24:06,000
AI is not a living being that has been primed by billions of years of evolution to participate in the battle for the survival of the fittest as animals are and as we are.

161
00:24:06,000 --> 00:24:13,000
It is math, code, computers, built by people owned by people used by people controlled by people.

162
00:24:13,000 --> 00:24:23,000
The idea that it will at some point develop a mind of its own and decide that it has motivations that lead it to try to kill us is a superstitious hand wave.

163
00:24:23,000 --> 00:24:28,000
I think largely from what I've heard anyway, this is somehow the crux of the debate.

164
00:24:28,000 --> 00:24:35,000
Will a superintelligence develop some sense of itself and of its own motivations?

165
00:24:35,000 --> 00:24:39,000
Will it, you know, become unleashed?

166
00:24:39,000 --> 00:24:43,000
Or will it always be a tool in the hands of the people who are using it?

167
00:24:43,000 --> 00:24:47,000
And even as a tool, you know, it may have become a powerful tool of destruction.

168
00:24:47,000 --> 00:24:51,000
But anyway, in short, AI doesn't want, it doesn't have goals.

169
00:24:51,000 --> 00:24:55,000
It doesn't want to kill you because it's not alive and AI is a machine.

170
00:24:55,000 --> 00:25:00,000
It's not going to come alive anymore than your toaster will.

171
00:25:00,000 --> 00:25:11,000
We guys think about that seems slightly disingenuous because on one hand he's saying that AI is a technology that's completely unprecedented that can help improve every aspect of life.

172
00:25:11,000 --> 00:25:14,000
That's going to augment human intelligence in every domain.

173
00:25:14,000 --> 00:25:18,000
Nobody thinks that a toaster will do those things.

174
00:25:18,000 --> 00:25:35,000
Now, obviously, there are true believers in killer AI, Baptists, who are gaining a suddenly stratospheric amount of media coverage for their terrifying warnings, some of whom claim to have been studying the topic for decades and say that they are now scared out of their minds by what they have learned.

175
00:25:35,000 --> 00:25:38,000
Some of these true believers are even actual innovators of the technology.

176
00:25:38,000 --> 00:25:49,000
These actors are arguing for a variety of bizarre and extreme restrictions on AI, ranging from a ban on AI development, all the way up to military airstrikes on data centers and nuclear war.

177
00:25:49,000 --> 00:25:50,000
Well, that's kind of crazy.

178
00:25:50,000 --> 00:25:52,000
I didn't hear about that.

179
00:25:52,000 --> 00:25:53,000
What is this?

180
00:25:53,000 --> 00:25:55,000
Pausing AI developments isn't enough.

181
00:25:55,000 --> 00:25:58,000
We need to shut it all down.

182
00:25:58,000 --> 00:25:59,000
Huh.

183
00:25:59,000 --> 00:26:04,000
Yadkowski is a decision theorist from the US, leads research at the Machine Intelligence Research Institute.

184
00:26:05,000 --> 00:26:10,000
Did he call for nuclear for military strikes on data centers?

185
00:26:10,000 --> 00:26:13,000
That's kind of got to look at that later.

186
00:26:13,000 --> 00:26:14,000
Let's go back here.

187
00:26:14,000 --> 00:26:30,000
They argue that because people like me cannot rule out future catastrophic consequences of AI catastrophic consequences of AI that we must assume a precautionary stance that may require large amounts of physical violence and death in order to prevent potential existential risk.

188
00:26:30,000 --> 00:26:34,000
My response, Andreessen writes, is that their position is non-scientific.

189
00:26:34,000 --> 00:26:36,000
What is the testable hypothesis?

190
00:26:36,000 --> 00:26:38,000
What would falsify the hypothesis?

191
00:26:38,000 --> 00:26:40,000
How do we know when we're getting into a danger zone?

192
00:26:40,000 --> 00:26:44,000
These questions go mainly unanswered apart from you can't prove it won't happen.

193
00:26:44,000 --> 00:26:57,000
In fact, these Baptist position is so non-scientific and so extreme, a conspiracy theory about math and code, and is already calling for physical violence that I will do something I would normally not do in question their motives as well.

194
00:26:57,000 --> 00:27:00,000
Okay, I'm going to continue in a minute, but this I don't like.

195
00:27:00,000 --> 00:27:04,000
This I don't like.

196
00:27:04,000 --> 00:27:06,000
You can't.

197
00:27:06,000 --> 00:27:24,000
How could you put it, you can't deflate the concerns by presenting AI as merely math and code, which makes a sound kind of banal, and therefore makes a conspiracy theory about a seem kind of ridiculous.

198
00:27:24,000 --> 00:27:39,000
And at the same time, say that this math and code will improve every aspect of human life in unprecedented ways that are truly infinitely loving and infinitely compassionate.

199
00:27:39,000 --> 00:27:49,000
So how can math and code be infinitely patient, infinitely knowledgeable, infinitely helpful, helpful, helpful, a machine version of infinite love.

200
00:27:49,000 --> 00:27:59,000
So when you're talking about the positive side, you can really exploit an anthropomorphized version of math and code.

201
00:27:59,000 --> 00:28:04,000
But when it comes to the concerns, you said, come on, don't be a conspiracy theorist.

202
00:28:04,000 --> 00:28:05,000
It's just math.

203
00:28:05,000 --> 00:28:07,000
Math can't hurt you.

204
00:28:07,000 --> 00:28:08,000
You know what I'm saying?

205
00:28:08,000 --> 00:28:10,000
I don't like that.

206
00:28:10,000 --> 00:28:16,000
I don't know whether he understands the sort of duplicity here.

207
00:28:16,000 --> 00:28:18,000
It should be fine if he does or doesn't.

208
00:28:18,000 --> 00:28:19,000
It doesn't matter.

209
00:28:19,000 --> 00:28:20,000
Still an interesting article.

210
00:28:20,000 --> 00:28:23,000
We're going through it, but you can't have it both ways.

211
00:28:23,000 --> 00:28:38,000
If it's just math, and it can be infinitely loving, then it's not ridiculous to wonder whether it can also have other less desirable emotional characteristics.

212
00:28:38,000 --> 00:28:39,000
Okay.

213
00:28:39,000 --> 00:28:40,000
Where were we?

214
00:28:40,000 --> 00:28:44,000
Sorry about that one second.

215
00:28:44,000 --> 00:28:47,000
I think we were.

216
00:28:47,000 --> 00:28:49,000
We're still up here, right?

217
00:28:49,000 --> 00:28:51,000
Okay, wait.

218
00:28:51,000 --> 00:28:52,000
Yeah, here we go.

219
00:28:52,000 --> 00:28:53,000
Okay.

220
00:28:53,000 --> 00:28:55,000
So what's going on, he says, what's, what's going on?

221
00:28:55,000 --> 00:28:57,000
Specifically, I think three things are going on.

222
00:28:57,000 --> 00:29:10,000
First, recall that John von Neumann responded to Robert Oppenheimer's famous hand wringing about his role creating nuclear weapons, which helped end World War Two and prevent World War Three with quote,

223
00:29:11,000 --> 00:29:15,000
Which is a nice line.

224
00:29:15,000 --> 00:29:21,000
What is the most dramatic way one can claim credit for the importance of one's work without sounding overly boastful?

225
00:29:21,000 --> 00:29:26,000
This explains the mismatch between the words and actions of the Baptist who are actually building and funding AI.

226
00:29:26,000 --> 00:29:29,000
Watch their actions, not their words.

227
00:29:29,000 --> 00:29:31,000
Truman was harsher after meeting with Oppenheimer.

228
00:29:31,000 --> 00:29:33,000
Don't let that cry baby in here again.

229
00:29:34,000 --> 00:29:42,000
Second, some of the Baptists, in other words, some of the people who are warning about the dangers of AI are actually bootleggers, meaning those who are going to profit from its restriction.

230
00:29:42,000 --> 00:29:47,000
There's a whole profession of AI safety expert, AI ethicist, AI risk researcher.

231
00:29:47,000 --> 00:29:53,000
They're paid to be doomers and their statements should be processed appropriately.

232
00:29:53,000 --> 00:30:06,000
Third, California is justifiably famous for our many thousands of cults, from Est, incidentally, interestingly influenced by Heidegger, to the People's Temple, from Heaven's Gate to the Manson family.

233
00:30:06,000 --> 00:30:19,000
Many, although not all of these cults are harmless and maybe even serve a purpose for alienated people who find homes in them, but some are very dangerous indeed and cults have a notoriously hard time straddling the line that ultimately leads to violence and death.

234
00:30:19,000 --> 00:30:30,000
And the reality, which is obvious to everyone in the Bay Area, but probably not outside of it, is that AI risk has developed into a cult, which has suddenly emerged into the daylight of global press attention and the public conversation.

235
00:30:30,000 --> 00:30:45,000
The cult has pulled in not just fringe characters, but also some actual industry experts and not a small number of wealthy donors, including until recently Sam Bankman Freed, and it's developed a full panoply of cult behaviors and beliefs.

236
00:30:45,000 --> 00:30:59,000
This cult is why there are a set of AI risk doomers who sound so extreme. It's not that they actually have secret knowledge that make their extremism logical, it's that they've whipped themselves into a frenzy and really are extremely extreme.

237
00:30:59,000 --> 00:31:16,000
It turns out that this type of cult isn't new. There's a longstanding Western tradition of millenarianism, which generates apocalypse cults. The AI risk cult has all the hallmarks of a millenarian apocalypse cult from Wikipedia with additions by me, quote.

238
00:31:16,000 --> 00:31:29,000
Millenarianism is the belief by a group or movement AI risk doomers in a coming fundamental transformation of society, the arrival of AI, after which all things will be changed AI utopia dystopia or end of the world.

239
00:31:29,000 --> 00:31:44,000
Only dramatic events, AI bands, airstrikes on data centers, nuclear strikes on unregulated AI are seen as able to change the world, prevent AI, and the change is anticipated to be brought about or survived by a group of the devout and dedicated.

240
00:31:44,000 --> 00:32:00,000
In most millenarian societies, excuse me scenarios, the disaster or battle to come AI apocalypse or its prevention will be followed by a new purified world AI bands in which the believers will be rewarded or at least acknowledged to have been correct all along.

241
00:32:01,000 --> 00:32:13,000
This apocalypse cult pattern is so obvious that I'm surprised more people don't see it. Don't get me wrong, cults are fun to hear about their written material is often creative and fascinating, and their members are engaging at dinner parties and on TV.

242
00:32:13,000 --> 00:32:19,000
But their extreme beliefs should not determine the future laws, the future of laws in society, obviously not.

243
00:32:20,000 --> 00:32:41,000
Okay, so to the first possible criticism of AI, the fear that it's going to kill us all. Mark Andreessen writes that no, that is not founded in anything other than this fear of new technologies, which may have an evolutionary purpose and which has precedence back in the myth of Prometheus and the story of

244
00:32:42,000 --> 00:32:54,000
But AI can't want to kill people because AI doesn't want anything doesn't have goals isn't alive, and is not much more than a toaster in terms of its intentions.

245
00:32:54,000 --> 00:33:01,000
And I already made my comments on that. So let's go to the next one. Will AI ruin our society.

246
00:33:01,000 --> 00:33:16,000
Okay, a quick pause. This is an article by Mark Andreessen, why AI will save the world published earlier this month that we're going through pretty interesting so far I hope you like it you should comment for sure in the chat or comment on the video if you're watching later.

247
00:33:16,000 --> 00:33:30,000
Like, share, subscribe. We definitely do live streams on politics and philosophy and more and more I think the arguments over the significance the political and social significance of these new technologies should be a part of our conversation because it

248
00:33:30,000 --> 00:33:39,000
implicates us in questions of sovereignty and the good society, the meaning of a human life, and all all the rest of it.

249
00:33:39,000 --> 00:33:43,000
So it's a nice topic to include in our conversations.

250
00:33:43,000 --> 00:34:02,000
Okay, will AI ruin our society. The second widely mooted AI risk is that AI will ruin our society by generating outputs that will be so harmful to use the nomenclature of this kind of doomer as to cause profound damage to humanity,

251
00:34:02,000 --> 00:34:05,000
even if we're not literally killed.

252
00:34:06,000 --> 00:34:22,000
Short version if the murder robots don't get us the hate speech and misinformation will. Okay, so I guess here is like you're going to have all kinds of fake chatbots and all kinds of propaganda farms, pushing out by the, you know, infinite stream of misinformation technologies.

253
00:34:23,000 --> 00:34:30,000
This is a relatively recent doomer concern that branched off from and somewhat took over the AI risk movement that I described above.

254
00:34:30,000 --> 00:34:44,000
In fact, the terminology of AI risk recently changed from AI safety, the term used by people who are worried that AI would literally kill us to AI alignment, the term used by people who are worried about societal harms.

255
00:34:44,000 --> 00:34:59,000
The original AI safety people are frustrated by this shift, although they don't know how to put it back in the box. They now advocate that the actual AI risk topic be renamed AI not kill everyone ism, which has not yet been widely adopted but is at least clear.

256
00:34:59,000 --> 00:35:10,000
The tip off to the nature of the AI societal risk claim is its own term AI alignment alignment with what human values, whose human values are that's where things get tricky.

257
00:35:10,000 --> 00:35:21,000
Okay, I'm interested to see where he takes us because remember what he said above. I pointed out he was invoking some human values of his own infinite compassion, infinite patience, infinite love.

258
00:35:22,000 --> 00:35:28,000
As it happens, I have had a front row seat to an analogous situation, the social media trust and safety wars.

259
00:35:28,000 --> 00:35:39,000
As is now obvious social media services have been under massive pressure from governments and activists to ban restrict sensor and otherwise suppress a wide range of content for many years.

260
00:35:39,000 --> 00:35:51,000
And the same concerns of quote unquote eight speech and its mathematical counterpart algorithmic bias and misinformation are being directly transferred from the social media context to the new frontier of AI alignment.

261
00:35:52,000 --> 00:35:58,000
My big learnings he writes from the social media wars are on the one hand there is no absolutist free speech position.

262
00:35:58,000 --> 00:36:02,000
First, every country including the US makes at least some content illegal.

263
00:36:02,000 --> 00:36:12,000
Second, there are certain kinds of content like child pornography and incitements to real world violence that are nearly universally agreed to be off limits legal or not by virtually every society.

264
00:36:13,000 --> 00:36:19,000
So any technological platform that facilitates or generates content speech is going to have some restrictions.

265
00:36:20,000 --> 00:36:24,000
On the other hand the slippery slope is not a fallacy it's an inevitability.

266
00:36:24,000 --> 00:36:34,000
Once a framework for restricting even egregiously terrible content is in place, for example for hate speech a specific hurtful word or for misinformation obviously false claims like the Pope is dead.

267
00:36:35,000 --> 00:36:50,000
A shockingly broad range of government agencies and activist pressure groups and and non governmental entities will kick into gear and demand ever greater levels of censorship and suppression of whatever speech they view as threatening to society and their own personal preferences.

268
00:36:51,000 --> 00:37:00,000
Just want to remind you on this topic of censorship we did a stream a couple of weeks ago Alexander Dugan on the metaphysics of censorship could be interesting to think about in relationship to AI alignment.

269
00:37:01,000 --> 00:37:05,000
They will do this up to and including in ways that are nakedly felony crimes.

270
00:37:06,000 --> 00:37:13,000
The cycle and practice can run apparently forever with the enthusiastic support of authoritarian hall monitors installed throughout our elite power structures.

271
00:37:13,000 --> 00:37:20,000
This has been cascading for a decade in social media and with only certain exceptions continues to get more fervent all the time.

272
00:37:21,000 --> 00:37:35,000
And so this is the dynamic that is formed around AI alignment now its proponents claim the wisdom to engineer AI generated speech and thought that are good for society and to ban AI generated speech and thoughts.

273
00:37:36,000 --> 00:37:42,000
Sorry that are bad for society and to ban AI generated yeah it's been generated.

274
00:37:43,000 --> 00:37:58,000
So basically you permit the good stuff you ban the bad stuff its opponents claim that the thought police are breathtakingly arrogant and presumptuous and often outright criminal at least in the US and in factors seeking to be kind to become a new kind of fused government corporate academic

275
00:37:58,000 --> 00:38:03,000
authoritarian speech dictatorship ripped straight from the pages of Orwell's 1984.

276
00:38:04,000 --> 00:38:12,000
Because I'm sure you have heard about this concern in fact Jordan Peterson maybe a couple of days ago or sometime last week posted on his Twitter.

277
00:38:13,000 --> 00:38:24,000
I think it was the UN maybe or some institution like that that said they were now going to be having you know AI automated misinformation censorship something like that.

278
00:38:24,000 --> 00:38:34,000
So that's the concern OK suddenly you're going to have ideologically supercharged AI systems that are designed to police speech and label everything they don't like hate speech.

279
00:38:35,000 --> 00:38:52,000
So like exactly what happens now but augmented by so AI augmented ideological censorship as the proponents of both trust and safety and AI alignment are clustered into the very narrow slice of the global population that characterizes the American Coastal elites,

280
00:38:52,000 --> 00:38:55,000
which includes many of the people who work in and right about the tech industry.

281
00:38:56,000 --> 00:39:04,000
Many of my readers will find yourselves primed to argue that dramatic restrictions on AI output are required to avoid destroying society.

282
00:39:05,000 --> 00:39:07,000
I will not attempt to talk you out of this now.

283
00:39:08,000 --> 00:39:16,000
He writes I will simply state that this is the nature of the demand and that most people in the world neither agree with your ideology nor want to see you win.

284
00:39:17,000 --> 00:39:18,000
We have here.

285
00:39:21,000 --> 00:39:25,000
America has a free what's what's he referring to it's behind the paywall look at it another day.

286
00:39:26,000 --> 00:39:40,000
Okay so most people in the world do not agree with the ideology of the American Coastal elites and don't want to see them win and therefore don't want to see an AI aligned with their ideology, especially with their sensorious limitations on free speech.

287
00:39:40,000 --> 00:39:56,000
If you don't agree with the prevailing niche morality that is being imposed on both social media and AI via ever intensifying speech codes, you should also realize that the fight over what AI is allowed to say or generate will be even more important by a lot than the fight over social media censorship.

288
00:39:57,000 --> 00:40:00,000
AI is highly likely to be the control layer for everything in the world.

289
00:40:01,000 --> 00:40:02,000
How is it allowed?

290
00:40:02,000 --> 00:40:07,000
How it is allowed to operate is going to matter perhaps more than anything else has ever mattered.

291
00:40:10,000 --> 00:40:17,000
I don't know how you feel about it when you see statements like that seem a little bit on the exaggerated side but okay I guess you have to consider it.

292
00:40:18,000 --> 00:40:27,000
You should be aware of how a small and isolated coterie of partisan social engineers are trying to determine that right now under cover of the age old claim that they're protecting you.

293
00:40:28,000 --> 00:40:31,000
In short, don't let the thought police suppress AI.

294
00:40:32,000 --> 00:40:33,000
Okay, so you had another argument here.

295
00:40:34,000 --> 00:40:39,000
AI is going to ruin our society because it's going to be allowed to be a racist homophobic and fascist.

296
00:40:40,000 --> 00:40:41,000
You know what I mean, right?

297
00:40:41,000 --> 00:40:53,000
It's going to be allowed to say things that don't pass the test of political correctness or whatever and that it should be made to answer to the dogmas of the day in order to be safe.

298
00:40:54,000 --> 00:40:56,000
And he says, no, don't let the thought police suppress AI.

299
00:40:57,000 --> 00:40:58,000
Okay, AI risk number three.

300
00:40:58,000 --> 00:41:00,000
Will AI take all our jobs?

301
00:41:02,000 --> 00:41:06,000
Well, if you don't have a job, you're safe.

302
00:41:07,000 --> 00:41:19,000
The fear of job loss due variously to mechanization, automation and computerization or AI has been a recurring panic for hundreds of years since the original onset of machinery, such as the mechanical loom.

303
00:41:20,000 --> 00:41:28,000
Even though every new major technology has led to more jobs at higher wages throughout history, each wave of this panic is accompanied by claims that this time it is different.

304
00:41:28,000 --> 00:41:29,000
This is the time it will finally happen.

305
00:41:30,000 --> 00:41:33,000
This is the technology that will finally deliver the hammer blow to human labor.

306
00:41:34,000 --> 00:41:35,000
And yet it never happens.

307
00:41:36,000 --> 00:41:40,000
I hear someone interrupt the article just to point out that there's a flip side of this argument, right?

308
00:41:40,000 --> 00:41:46,000
So he says every time there's a panic, the panic says this time it's different, but it's always the same.

309
00:41:47,000 --> 00:41:52,000
But we should point out that the flip side is he's also saying this time it's different on the optimistic side of things.

310
00:41:52,000 --> 00:41:54,000
Remember what he put here, right?

311
00:41:54,000 --> 00:42:01,000
He said, how you're going to operate AI may matter more than anything else has ever mattered.

312
00:42:02,000 --> 00:42:03,000
Same sort of idea.

313
00:42:03,000 --> 00:42:13,000
This is the technology that's really going to be so different from all the technologies that have come before, none of which could be infinitely loving, infinitely compassionate, infinitely patient and so on.

314
00:42:14,000 --> 00:42:17,000
So just keep in mind, you want to make sure you don't lose sight of that.

315
00:42:17,000 --> 00:42:21,000
He's not allowed to overstate the benefits and understate the harms.

316
00:42:21,000 --> 00:42:22,000
So you're just going to keep an eye on it.

317
00:42:23,000 --> 00:42:35,000
Okay, so we've been through two such technology driven unemployment panic cycles in our recent past, the outsourcing panic of the 2000s and the automation panic of the 2010s.

318
00:42:35,000 --> 00:42:43,000
Notwithstanding many talking heads, pundits and even tech industry executives pounding the table throughout both decades that mass unemployment was near.

319
00:42:43,000 --> 00:42:52,000
By late 2019, right before the onset of COVID, the world had more jobs at higher wages than ever in history.

320
00:42:52,000 --> 00:42:55,000
Nevertheless, this mistaken idea will not die.

321
00:42:55,000 --> 00:42:57,000
And sure enough, it's back.

322
00:42:57,000 --> 00:43:04,000
This time we finally have the technology that's going to take all the jobs and render human workers superfluous, real AI.

323
00:43:04,000 --> 00:43:11,000
Surely this time history won't repeat and AI will cause mass unemployment and not rapid economic job and wage growth, right?

324
00:43:11,000 --> 00:43:13,000
No, that's not going to happen.

325
00:43:13,000 --> 00:43:26,000
And in fact, AI, if allowed to develop and proliferate throughout the economy may cause the most dramatic and sustained economic boom of all time with correspondingly record job and wage growth, the exact opposite of the fear.

326
00:43:26,000 --> 00:43:27,000
And here's why.

327
00:43:27,000 --> 00:43:28,000
Okay, so good.

328
00:43:28,000 --> 00:43:32,000
We have the claim it's going to destroy job growth and we have the counter claim.

329
00:43:32,000 --> 00:43:33,000
No, it's not.

330
00:43:33,000 --> 00:43:34,000
It's going to lead to a boom.

331
00:43:34,000 --> 00:43:36,000
Well, that's the claim.

332
00:43:36,000 --> 00:43:37,000
What's the argument?

333
00:43:37,000 --> 00:43:44,000
The core mistake the automation kills jobs doomers keep making is called the lump of labor fallacy.

334
00:43:44,000 --> 00:43:53,000
This fallacy is the incorrect notion that there's a fixed amount of labor to be done in the economy at any given time and either machines do it or people do it.

335
00:43:53,000 --> 00:43:56,000
And if machines do it, there will be no work for people to do.

336
00:43:56,000 --> 00:44:03,000
The lump of labor fallacy flows naturally from naive intuition, but naive intuition here is wrong.

337
00:44:03,000 --> 00:44:11,000
When technology is applied to production, we get productivity growth and increase in output generated by a reduction in inputs.

338
00:44:11,000 --> 00:44:14,000
The result is lower prices for goods and services.

339
00:44:14,000 --> 00:44:22,000
As prices for goods and services fall, we pay less for them, meaning that we now have extra spending power with which to buy other things.

340
00:44:22,000 --> 00:44:32,000
This increases demand in the economy, which drives the creation of new production, including new products and new industries, which then creates new jobs for the people who are replaced by machines and prior jobs.

341
00:44:33,000 --> 00:44:40,000
The result is a larger economy with higher material prosperity, more industries, more products and more jobs.

342
00:44:40,000 --> 00:44:42,000
But the good news doesn't stop there.

343
00:44:42,000 --> 00:44:44,000
We also get higher wages.

344
00:44:44,000 --> 00:44:53,000
This is because at the level of the individual worker, the marketplace sets compensation as a function of the marginal productivity of the worker.

345
00:44:53,000 --> 00:44:58,000
A worker in a technology infused business will be more productive than a worker in a traditional business.

346
00:44:58,000 --> 00:45:04,000
The employer will either pay that worker more money as he is now more productive, or another employer will purely out of self interest.

347
00:45:04,000 --> 00:45:11,000
The result is that technology introduced into an industry generally not only increases the number of jobs in the industry, but also increases wages.

348
00:45:11,000 --> 00:45:18,000
You guys see that technology is going to make you marginally more productive and therefore is going to increase your wages if wages are related to marginal productivity.

349
00:45:18,000 --> 00:45:23,000
To summarize, technology empowers people to be more productive.

350
00:45:23,000 --> 00:45:27,000
This causes the prices for existing goods and services to fall and for wages to rise.

351
00:45:27,000 --> 00:45:33,000
This in turn causes economic growth and job growth while motivating the creation of new jobs and new industries.

352
00:45:33,000 --> 00:45:42,000
If a market economy is allowed to function normally and if technology is allowed to be introduced freely, this is a perpetual upward cycle that never ends.

353
00:45:42,000 --> 00:45:47,000
For, as Milton Friedman observed, human wants and needs are endless.

354
00:45:47,000 --> 00:45:51,000
Incidentally also something that Aristotle observed in the politics.

355
00:45:51,000 --> 00:45:55,000
We always want more than we have and Plato in the Republic.

356
00:45:55,000 --> 00:46:03,000
A technology infused market economy is the way we get closer to delivering everything everyone could conceivably want, but never all the way there.

357
00:46:03,000 --> 00:46:09,000
And that is why technology doesn't destroy jobs and never will.

358
00:46:09,000 --> 00:46:11,000
Okay, continuing.

359
00:46:11,000 --> 00:46:18,000
These are such mind blowing ideas for people who have not been exposed to them that it may take you some time to wrap your head around them, but I swear I'm not making them up.

360
00:46:18,000 --> 00:46:22,000
In fact, you can read about them in standard economic textbooks.

361
00:46:22,000 --> 00:46:37,000
I recommend the chapter The Curse of Machinery in Henry Haslett's Economics in One Lesson and Frederick Bastiat's satirical candle makers petition to blot out the sun due to its unfair competition with the lighting industry here modernized for our times.

362
00:46:37,000 --> 00:46:39,000
But this time is different to your thinking.

363
00:46:39,000 --> 00:46:43,000
This time with AI we have the technology that can replace all human labor.

364
00:46:43,000 --> 00:46:51,000
By using the principles I described above, he writes, think of what it would mean for literally all existing human labor to be replaced by machines.

365
00:46:51,000 --> 00:46:58,000
It would mean a take off rate of economic productivity growth that would be absolutely stratospheric far beyond any historical precedent.

366
00:46:58,000 --> 00:47:02,000
Prices of existing goods and services would drop across the board to virtually zero.

367
00:47:02,000 --> 00:47:04,000
Consumer welfare would skyrocket.

368
00:47:04,000 --> 00:47:06,000
Consumer spending power would skyrocket.

369
00:47:06,000 --> 00:47:08,000
New demand in the economy would explode.

370
00:47:08,000 --> 00:47:18,000
Entrepreneurs would create dizzying arrays of new industries, products and services and employ as many people and AI as they could as fast as possible to meet all the new demand.

371
00:47:18,000 --> 00:47:22,000
Suppose AI once again replaces that labor, the cycle would repeat, etc.

372
00:47:22,000 --> 00:47:28,000
It would be a straight spiral up to material utopia that neither Adam Smith nor Karl Marx ever dreamed of.

373
00:47:28,000 --> 00:47:30,000
We should be so lucky.

374
00:47:30,000 --> 00:47:34,000
Okay, so let's just review where we are at in this article.

375
00:47:34,000 --> 00:47:36,000
Oops, sorry.

376
00:47:36,000 --> 00:47:42,000
So, first we had the claim that AI is going to make everything we care about better.

377
00:47:42,000 --> 00:47:44,000
Everything.

378
00:47:44,000 --> 00:47:54,000
Because you're going to have augmented intelligence and intelligence is such a lead indicator of where you're going to be successful that augmented intelligence is going to make everything better.

379
00:47:54,000 --> 00:47:59,000
Children, parents, leaders, scientists, artists, you name it.

380
00:47:59,000 --> 00:48:07,000
But most people are arguing that AI is going to kill us all, take our jobs and do all kinds of other crazy things.

381
00:48:07,000 --> 00:48:21,000
And he had some analysis of why that is in part because when you ask for the regulation of AI, you may also benefit from that regulation like the bootleggers who made a fortune selling illicit alcohol to Americans on legitimate alcohol sales were banned.

382
00:48:21,000 --> 00:48:30,000
But besides the economic possible upside of arguing for a regulation of AI, he also wanted to examine their arguments on their merits.

383
00:48:30,000 --> 00:48:33,000
Will AI kill us all? We just discussed a moment ago.

384
00:48:33,000 --> 00:48:39,000
It won't because it doesn't want to kill anybody doesn't want to do anything. It's just a tool in the hands of people.

385
00:48:39,000 --> 00:48:45,000
It has no personality, no intention, no agency and no malice as it were.

386
00:48:45,000 --> 00:48:54,000
Then we have the idea that AI will ruin our society because it's going to be freely allowed to say all kinds of things that mess up our social conversation.

387
00:48:54,000 --> 00:48:58,000
Therefore, it should be made to be politically correct in effect.

388
00:48:58,000 --> 00:49:02,000
And he said, no, do not let the thought police suppress AI.

389
00:49:02,000 --> 00:49:07,000
The third risk, it's going to take all our jobs. You said far from it, it's going to lead to a boom.

390
00:49:07,000 --> 00:49:15,000
More jobs, more higher paying jobs, a golden age of economic prosperity.

391
00:49:15,000 --> 00:49:23,000
So we're at AI risk number four now. Will AI lead to crippling inequality?

392
00:49:23,000 --> 00:49:29,000
Speaking of Karl Marx, the concern about AI taking jobs, segues?

393
00:49:29,000 --> 00:49:32,000
How do you say that word? I think I always say segues.

394
00:49:32,000 --> 00:49:37,000
Okay, the concern about AI taking jobs, segues directly into the next claim.

395
00:49:37,000 --> 00:49:44,000
AI risk, the next claimed AI risk, which is, okay, Mark, suppose AI does take all the jobs, either for bad or for good.

396
00:49:44,000 --> 00:49:49,000
Won't that result in massive and crippling wealth inequality?

397
00:49:49,000 --> 00:49:53,000
As the owners of AI reap all the economic rewards and regular people get nothing.

398
00:49:53,000 --> 00:49:59,000
By the way, once upon a time, I can't vouch for it because I don't remember it well enough, but I read a book called.

399
00:49:59,000 --> 00:50:04,000
What's that guy's name? Georgiania, right? Georgiania.

400
00:50:04,000 --> 00:50:11,000
The book was called Prometheism, I think. Prometheism, something like that.

401
00:50:11,000 --> 00:50:19,000
And it argues sort of like this, that as technology develops, as it gets to the, and it should develop, as the Promethean spirit develops and it should develop,

402
00:50:19,000 --> 00:50:33,000
it's going to produce massive social inequality between the basically subhuman, like humanity will branch into a technologically superhuman overlords and subhuman slaves.

403
00:50:33,000 --> 00:50:37,000
But okay, speaking of Karl Marx, yeah, okay, the wealth inequality.

404
00:50:37,000 --> 00:50:46,000
As it happens, this was a central claim of Marxism that the owners of the means of production, the bourgeoisie, would inevitably steal all societal wealth from the people who do the actual work, the proletariat.

405
00:50:46,000 --> 00:50:54,000
This is another fallacy that simply will not die, no matter how often it's disproved by reality, but let's drive a stake through its heart anyway.

406
00:50:54,000 --> 00:51:01,000
The flaw in this theory is that as the owner of a piece of technology, it's not in your interest to keep it to yourself, in fact, the opposite.

407
00:51:01,000 --> 00:51:06,000
It's in your own interest to sell it to as many customers as possible.

408
00:51:06,000 --> 00:51:10,000
The largest market in the world for any product is the entire world, all eight billion of us.

409
00:51:11,000 --> 00:51:24,000
And so in reality, every new technology, even ones that start by selling to the rarefied air of high-paying big companies or wealthy customers, rapidly proliferates until it's in the hands of the largest possible mass market, ultimately everyone on the planet.

410
00:51:24,000 --> 00:51:31,000
The classic example of this was Elon Musk's so-called secret plan, which he naturally published openly for Tesla in 2006.

411
00:51:31,000 --> 00:51:33,000
Step one, build expensive sports car.

412
00:51:33,000 --> 00:51:35,000
Step two, use that money to build an affordable car.

413
00:51:35,000 --> 00:51:38,000
Step three, use that money to build an even more affordable car.

414
00:51:38,000 --> 00:51:43,000
Which is exactly what he's done becoming the richest man in the world as a result.

415
00:51:43,000 --> 00:51:45,000
The last point is key.

416
00:51:45,000 --> 00:51:49,000
Would Elon be even richer if he only sold cars to rich people today?

417
00:51:49,000 --> 00:51:50,000
No.

418
00:51:50,000 --> 00:51:53,000
Would he be even richer than that if he only made cars for himself?

419
00:51:53,000 --> 00:51:54,000
Of course not.

420
00:51:54,000 --> 00:51:59,000
No, he maximizes his own profit by selling through the largest possible market, the world.

421
00:52:00,000 --> 00:52:08,000
In short, everyone gets the thing, as we saw in the past with not just cars, but also electricity, radio, computers, the internet, mobile phones, and search engines.

422
00:52:08,000 --> 00:52:14,000
The makers of such technologies are highly motivated to drive down their prices until everyone on the planet can afford them.

423
00:52:14,000 --> 00:52:17,000
This is precisely what is already happening in AI.

424
00:52:17,000 --> 00:52:24,000
It's why you can use state-of-the-art generative AI, not just at low cost, but even for free today, in the form of Microsoft being in Google Bard.

425
00:52:24,000 --> 00:52:26,000
And it is what will continue to happen.

426
00:52:26,000 --> 00:52:31,000
Not because such vendors are foolish or generous, but precisely because they are greedy.

427
00:52:31,000 --> 00:52:35,000
They want to maximize the size of their market, which maximizes their profits.

428
00:52:38,000 --> 00:52:39,000
I'm not going to lie.

429
00:52:39,000 --> 00:52:41,000
I can relate to that, millermanschool.com.

430
00:52:41,000 --> 00:52:49,000
If the whole world was my customer, you know, if everybody in the world was taking the Plato course or the Heidegger course or the Dugan course, wouldn't necessarily be so bad.

431
00:52:49,000 --> 00:52:52,000
What happens is the opposite of technology driving centralization of wealth.

432
00:52:52,000 --> 00:53:00,000
Individual customers of the technology, ultimately including everyone on the planet, are empowered instead and capture most of the generated value.

433
00:53:00,000 --> 00:53:07,000
As with prior technologies, the companies that build AI, assuming they have to function in a free market, will compete furiously to make this happen.

434
00:53:07,000 --> 00:53:10,000
Marx was wrong then, and he's wrong now.

435
00:53:10,000 --> 00:53:13,000
This is not to say that inequality is not an issue in our society.

436
00:53:13,000 --> 00:53:14,000
It is.

437
00:53:14,000 --> 00:53:16,000
It's just not being driven by technology.

438
00:53:16,000 --> 00:53:18,000
It's being driven by the reverse.

439
00:53:18,000 --> 00:53:30,000
By the sectors of the economy that are the most resistant to new technology that have the most government intervention to prevent the adoption of new technology like AI, specifically housing education and healthcare.

440
00:53:30,000 --> 00:53:42,000
The actual risk of AI and inequality is not that AI will cause more inequality, but rather that we will not allow AI to be used to reduce inequality.

441
00:53:42,000 --> 00:53:47,000
Okay, so not only will it not lead to crippling inequality.

442
00:53:47,000 --> 00:53:53,000
It's somehow our best hope for producing more genuine equality.

443
00:53:53,000 --> 00:53:57,000
Okay, or for reducing inequality.

444
00:53:57,000 --> 00:53:59,000
AI risk number five.

445
00:53:59,000 --> 00:54:02,000
Let me just see how many more do we have to go through.

446
00:54:02,000 --> 00:54:04,000
So one more on the numbered list.

447
00:54:04,000 --> 00:54:05,000
Let's have a look.

448
00:54:05,000 --> 00:54:08,000
Will AI lead to bad people doing bad things?

449
00:54:08,000 --> 00:54:13,000
So far, I've explained why four of the five most often proposed risks of AI are not actually real.

450
00:54:13,000 --> 00:54:15,000
AI will not come to life and kill us.

451
00:54:15,000 --> 00:54:17,000
AI will not ruin our society.

452
00:54:17,000 --> 00:54:22,000
AI will not cause mass unemployment, and AI will not cause a ruinous increase in inequality.

453
00:54:22,000 --> 00:54:27,000
But now let's address the fifth, the one I actually agree with.

454
00:54:27,000 --> 00:54:30,000
AI will make it easier for bad people to do bad things.

455
00:54:30,000 --> 00:54:33,000
In some sense, this is a tautology.

456
00:54:33,000 --> 00:54:34,000
Technology is a tool.

457
00:54:34,000 --> 00:54:41,000
Tools starting with fire and rocks can be used to do good things, cook food and build houses and bad things, burn people and bludgeon people.

458
00:54:41,000 --> 00:54:44,000
Any technology can be used for good or bad, fair enough.

459
00:54:44,000 --> 00:54:50,000
And AI will make it easier for criminals, terrorists and hostile governments to do bad things.

460
00:54:50,000 --> 00:54:51,000
No question.

461
00:54:51,000 --> 00:54:53,000
This causes some people to propose.

462
00:54:53,000 --> 00:54:56,000
Well, in that case, let's not take the risk.

463
00:54:56,000 --> 00:54:59,000
Let's ban AI now before this can happen.

464
00:54:59,000 --> 00:55:04,000
Unfortunately, AI is not some esoteric physical matter that is hard to come by like plutonium.

465
00:55:04,000 --> 00:55:05,000
It's the opposite.

466
00:55:05,000 --> 00:55:09,000
It's the easiest material in the world to come by math and code.

467
00:55:09,000 --> 00:55:12,000
The AI cat is obviously already out of the bag.

468
00:55:12,000 --> 00:55:19,000
You can learn how to build AI from thousands of free online courses, books, papers and videos.

469
00:55:19,000 --> 00:55:23,000
And there are outstanding open source implementations proliferating by the day.

470
00:55:23,000 --> 00:55:24,000
AI is like air.

471
00:55:24,000 --> 00:55:25,000
It will be everywhere.

472
00:55:25,000 --> 00:55:34,000
The level of totalitarian oppression that would be required to arrest that would be so draconian, a world government monitoring and controlling all computers.

473
00:55:34,000 --> 00:55:41,000
Jack booted thugs and black helicopters seizing rogue GPUs that we would not have a society left to protect.

474
00:55:41,000 --> 00:55:47,000
So instead, there are two very straightforward ways to address the risk of bad people doing bad things with AI.

475
00:55:47,000 --> 00:55:50,000
And these are precisely what we should focus on.

476
00:55:50,000 --> 00:55:56,000
First, we have the laws on the books to criminalize most of the bad things that anyone is going to do with AI.

477
00:55:56,000 --> 00:55:57,000
Hack into the Pentagon.

478
00:55:57,000 --> 00:55:58,000
That's a crime.

479
00:55:58,000 --> 00:55:59,000
Steal money from a bank.

480
00:55:59,000 --> 00:56:00,000
That's a crime.

481
00:56:00,000 --> 00:56:01,000
Create a bio weapon.

482
00:56:01,000 --> 00:56:02,000
That's a crime.

483
00:56:02,000 --> 00:56:03,000
Commit a terrorist act.

484
00:56:03,000 --> 00:56:04,000
That's a crime.

485
00:56:04,000 --> 00:56:09,000
We can simply focus on preventing those crimes when we can and prosecuting them when we cannot.

486
00:56:09,000 --> 00:56:10,000
We don't even need new laws.

487
00:56:10,000 --> 00:56:14,000
I'm not aware of a single actual bad use for AI that's been proposed.

488
00:56:14,000 --> 00:56:16,000
That's not already illegal.

489
00:56:16,000 --> 00:56:22,000
And if a new bad use is identified, we ban that use.

490
00:56:22,000 --> 00:56:24,000
But you'll notice what I slipped in there.

491
00:56:24,000 --> 00:56:28,000
I said we should focus first on preventing AI assisted crimes before they happen.

492
00:56:28,000 --> 00:56:31,000
Wouldn't such prevention mean banning AI?

493
00:56:31,000 --> 00:56:35,000
There's another way to prevent such actions, and that's by using AI as a defensive tool.

494
00:56:35,000 --> 00:56:39,000
The same capabilities that make AI dangerous in the hands of bad guys with bad goals,

495
00:56:39,000 --> 00:56:42,000
make it powerful in the hands of good guys with good goals,

496
00:56:42,000 --> 00:56:46,000
specifically the good guys whose job it is to prevent bad things from happening.

497
00:56:46,000 --> 00:56:50,000
For example, if you're worried about AI generating fake people and fake videos,

498
00:56:50,000 --> 00:56:57,000
the answer is to build new systems where people can verify themselves and real content via cryptographic signatures.

499
00:56:57,000 --> 00:57:01,000
Digital creation and alteration of both real and fake content was already here before AI.

500
00:57:01,000 --> 00:57:05,000
The answer is not to ban word processors in Photoshop or AI,

501
00:57:05,000 --> 00:57:10,000
but to use technology to build a system that actually solves the problem.

502
00:57:10,000 --> 00:57:15,000
And so second, let's mount major efforts to use AI for good, legitimate defensive purposes.

503
00:57:15,000 --> 00:57:19,000
Let's put AI to work in cyber defense and biological defense and hunting terrorists

504
00:57:19,000 --> 00:57:24,000
and everything else that we want to do to keep ourselves, our communities and our nation safe.

505
00:57:24,000 --> 00:57:28,000
There are already many smart people in and out of government doing exactly this, of course,

506
00:57:28,000 --> 00:57:35,000
but if we apply all of the effort and brainpower that's currently fixated on the futile prospect of banning AI

507
00:57:35,000 --> 00:57:39,000
to using AI to protect against bad people doing bad things,

508
00:57:39,000 --> 00:57:44,000
I think there's no question a world infused with AI will be much safer than the world we live in today.

509
00:57:44,000 --> 00:57:48,000
Okay, we're getting close to the end here. Let's see what we have next.

510
00:57:48,000 --> 00:57:52,000
The actual risk of not pursuing AI with maximum force and speed.

511
00:57:52,000 --> 00:57:56,000
There is one final and real AI risk that's probably the scariest of all.

512
00:57:56,000 --> 00:58:01,000
AI isn't just being developed in the relatively free societies of the West.

513
00:58:01,000 --> 00:58:06,000
It's also being developed by the Communist Party of the People's Republic of China.

514
00:58:06,000 --> 00:58:11,000
China has a vastly different version, excuse me, vision for AI than we do.

515
00:58:11,000 --> 00:58:15,000
They view it as a mechanism for authoritarian population control full stop.

516
00:58:15,000 --> 00:58:18,000
They're not even being secretive about this. They're very clear about it,

517
00:58:18,000 --> 00:58:21,000
and they are already pursuing their agenda.

518
00:58:21,000 --> 00:58:25,000
And they do not intend to limit their AI strategy to China.

519
00:58:25,000 --> 00:58:28,000
They intend to proliferate it across the world.

520
00:58:28,000 --> 00:58:33,000
Everywhere they're powering 5G networks, everywhere they're loaning belt and road money,

521
00:58:33,000 --> 00:58:37,000
everywhere they're providing friendly consumer apps like TikTok

522
00:58:37,000 --> 00:58:41,000
that serve as front ends to their centralized command and control AI.

523
00:58:41,000 --> 00:58:46,000
The single greatest risk of AI is that China wins global AI dominance,

524
00:58:46,000 --> 00:58:50,000
and we, the United States and the West, do not.

525
00:58:50,000 --> 00:58:53,000
AI, he writes, propose a simple strategy for what to do about this.

526
00:58:53,000 --> 00:58:58,000
In fact, the same strategy President Ronald Reagan used to win the first Cold War

527
00:58:58,000 --> 00:59:02,000
with the Soviet Union. We win, they lose.

528
00:59:02,000 --> 00:59:06,000
Rather than allowing ungrounded panics around killer AI, harmful AI,

529
00:59:06,000 --> 00:59:11,000
job destroying AI, and inequality generating AI to put us on our back feet,

530
00:59:11,000 --> 00:59:16,000
we in the United States and the West should lean into AI as hard as we possibly can.

531
00:59:16,000 --> 00:59:20,000
We should seek to win the race to global AI technological superiority

532
00:59:20,000 --> 00:59:23,000
and ensure that China does not.

533
00:59:23,000 --> 00:59:28,000
In the process, we should drive AI into our economy and society as fast and hard as we possibly can

534
00:59:28,000 --> 00:59:32,000
in order to maximize its gains for economic productivity and human potential.

535
00:59:32,000 --> 00:59:36,000
This is the best way both to offset the real AI risks

536
00:59:36,000 --> 00:59:42,000
and to ensure that our way of life is not displaced by the much darker Chinese version.

537
00:59:43,000 --> 00:59:45,000
What is to be done?

538
00:59:45,000 --> 00:59:48,000
He asks, I propose a simple plan.

539
00:59:48,000 --> 00:59:52,000
Big AI companies should be allowed to build AI as fast and aggressively as they can,

540
00:59:52,000 --> 00:59:55,000
but not allowed to achieve regulatory capture,

541
00:59:55,000 --> 00:59:58,000
not allowed to establish a government protected cartel

542
00:59:58,000 --> 01:00:02,000
that is insulated from market competition due to incorrect claims of AI risk.

543
01:00:02,000 --> 01:00:05,000
This will maximize the technological and societal payoff

544
01:00:05,000 --> 01:00:10,000
from the amazing capabilities of these companies, which are jewels of modern capitalism.

545
01:00:10,000 --> 01:00:15,000
Startup AI companies should be allowed to build AI as fast and aggressively as they can.

546
01:00:15,000 --> 01:00:18,000
They should neither confront government-granted protection of big companies

547
01:00:18,000 --> 01:00:20,000
nor should they receive government assistance.

548
01:00:20,000 --> 01:00:22,000
They should simply be allowed to compete.

549
01:00:22,000 --> 01:00:25,000
If and as startups don't succeed,

550
01:00:25,000 --> 01:00:30,000
their presence in the market will also continuously motivate big companies to be their best.

551
01:00:30,000 --> 01:00:33,000
Our economies and societies win either way.

552
01:00:33,000 --> 01:00:38,000
Open source AI should be allowed to freely proliferate

553
01:00:38,000 --> 01:00:41,000
and compete with both big AI companies and startups.

554
01:00:41,000 --> 01:00:45,000
There should be no regulatory barriers to open source whatsoever.

555
01:00:45,000 --> 01:00:47,000
Even when open source does not beat companies,

556
01:00:47,000 --> 01:00:50,000
its widespread availability is a boon to students all over the world

557
01:00:50,000 --> 01:00:53,000
who want to learn how to build and use AI

558
01:00:53,000 --> 01:00:55,000
to become part of the technological future

559
01:00:55,000 --> 01:00:58,000
and will ensure that AI is available to everyone who can benefit from it

560
01:00:58,000 --> 01:01:01,000
no matter who they are or how much money they have.

561
01:01:01,000 --> 01:01:04,000
To offset the risk of bad people doing bad things with AI,

562
01:01:04,000 --> 01:01:08,000
governments working in partnership with the private sector should vigorously engage

563
01:01:08,000 --> 01:01:14,000
in each area of potential risk to use AI to maximize society's defensive capabilities.

564
01:01:14,000 --> 01:01:17,000
This shouldn't be limited to AI-enabled risks,

565
01:01:17,000 --> 01:01:20,000
but also more general problems such as malnutrition, disease, and climate.

566
01:01:20,000 --> 01:01:23,000
AI can be an incredibly powerful tool for solving problems,

567
01:01:23,000 --> 01:01:25,000
and we should embrace it as such.

568
01:01:25,000 --> 01:01:27,000
To prevent the risk of China achieving global AI dominance,

569
01:01:27,000 --> 01:01:30,000
we should use the full power of our private sector,

570
01:01:30,000 --> 01:01:33,000
our scientific establishment, and our governments

571
01:01:33,000 --> 01:01:36,000
in concert to drive American and Western AI to absolute global dominance,

572
01:01:36,000 --> 01:01:39,000
including ultimately inside China itself.

573
01:01:39,000 --> 01:01:43,000
We win, they lose, and that is how we use AI to save the world.

574
01:01:43,000 --> 01:01:45,000
It's time to build.

575
01:01:45,000 --> 01:01:47,000
And then what do we have here?

576
01:01:47,000 --> 01:01:49,000
Just two paragraphs to go.

577
01:01:49,000 --> 01:01:50,000
Legends and heroes.

578
01:01:50,000 --> 01:01:51,000
I close.

579
01:01:51,000 --> 01:01:53,000
Mark Andreessen writes with two simple statements.

580
01:01:53,000 --> 01:01:56,000
The development of AI started in the 1940s,

581
01:01:56,000 --> 01:01:58,000
simultaneous with the invention of the computer.

582
01:01:58,000 --> 01:02:01,000
The first scientific paper on neural networks,

583
01:02:01,000 --> 01:02:03,000
the architecture of the AI we have today,

584
01:02:03,000 --> 01:02:05,000
was published in 1943.

585
01:02:05,000 --> 01:02:09,000
Entire generations of AI scientists over the last 80 years were born,

586
01:02:09,000 --> 01:02:12,000
went to school, worked, and in many cases passed away

587
01:02:12,000 --> 01:02:14,000
without seeing the payoff that we are receiving now.

588
01:02:14,000 --> 01:02:16,000
They are legends, everyone.

589
01:02:16,000 --> 01:02:19,000
Today, growing legions of engineers, many of whom are young

590
01:02:19,000 --> 01:02:22,000
and may have had grandparents or even great-grandparents

591
01:02:22,000 --> 01:02:25,000
involved in the creation of the ideas behind AI,

592
01:02:25,000 --> 01:02:28,000
looking to make AI a reality against a wall of fear,

593
01:02:28,000 --> 01:02:31,000
mongering, and demerism that is attempting to paint them

594
01:02:31,000 --> 01:02:33,000
as reckless villains.

595
01:02:33,000 --> 01:02:35,000
I do not believe they are reckless or villains.

596
01:02:35,000 --> 01:02:37,000
They are heroes, everyone.

597
01:02:37,000 --> 01:02:40,000
My firm and I are thrilled to back as many of them as we can,

598
01:02:40,000 --> 01:02:44,000
and we will stand alongside them in their work 100%.

599
01:02:44,000 --> 01:02:46,000
Okay, so there you go.

600
01:02:46,000 --> 01:02:48,000
That was an interesting article.

601
01:02:48,000 --> 01:02:50,000
I hope that you learned something from it,

602
01:02:50,000 --> 01:02:52,000
and maybe you would not have read it otherwise,

603
01:02:52,000 --> 01:02:54,000
so I'm glad we had a chance to go over it.

604
01:02:54,000 --> 01:02:56,000
Let's save the world a strong argument here

605
01:02:56,000 --> 01:03:03,000
in favor of AI maximalism going all-in on AI, fast and hard,

606
01:03:03,000 --> 01:03:06,000
and specifically overcoming some of the more common

607
01:03:06,000 --> 01:03:10,000
or responding to some of the more common objections to AI,

608
01:03:10,000 --> 01:03:13,000
that it will kill us all,

609
01:03:13,000 --> 01:03:18,000
that it's going to have detrimental effects on our society,

610
01:03:18,000 --> 01:03:20,000
that it's going to take all our jobs,

611
01:03:20,000 --> 01:03:23,000
that it's going to lead to crippling inequality,

612
01:03:23,000 --> 01:03:26,000
and that people will do bad things with it,

613
01:03:26,000 --> 01:03:31,000
which is the risk that he said actually is the most likely.

614
01:03:31,000 --> 01:03:34,000
But you can use AI to stop the bad people

615
01:03:34,000 --> 01:03:36,000
who want to do bad things with it

616
01:03:36,000 --> 01:03:39,000
if you just use AI for detection purposes

617
01:03:39,000 --> 01:03:42,000
and you enforce the laws that are already in place.

618
01:03:42,000 --> 01:03:43,000
So there you go.

619
01:03:43,000 --> 01:03:44,000
I hope that you enjoyed that.

620
01:03:44,000 --> 01:03:47,000
As I said, we should find more occasion to discuss

621
01:03:47,000 --> 01:03:50,000
the significance of new AI technologies

622
01:03:50,000 --> 01:03:53,000
and the rest of it for the question of the good society,

623
01:03:53,000 --> 01:03:57,000
of political sovereignty, of progressivism,

624
01:03:57,000 --> 01:04:00,000
and the relationship of technology, freedom, tyranny,

625
01:04:00,000 --> 01:04:01,000
all of these kinds of things.

626
01:04:01,000 --> 01:04:03,000
I've been discussing those topics more and more

627
01:04:03,000 --> 01:04:05,000
with my students in private tutoring

628
01:04:05,000 --> 01:04:07,000
who asked me to walk them through

629
01:04:07,000 --> 01:04:10,000
what the tradition of political philosophy has had to say

630
01:04:10,000 --> 01:04:12,000
so that they who are working in fields

631
01:04:12,000 --> 01:04:15,000
of technological entrepreneurship and development

632
01:04:15,000 --> 01:04:17,000
can think more clearly about the underlying

633
01:04:17,000 --> 01:04:22,000
political theory dimension of the new technologies.

634
01:04:22,000 --> 01:04:23,000
And it's great.

635
01:04:23,000 --> 01:04:24,000
I'm glad that that's happening.

636
01:04:24,000 --> 01:04:27,000
I encourage you to check out MillermanSchool.com,

637
01:04:27,000 --> 01:04:29,000
like, share, subscribe to this channel,

638
01:04:29,000 --> 01:04:31,000
comment, and all the rest of this

639
01:04:31,000 --> 01:04:34,000
so these videos can be picked up by the algorithm shared

640
01:04:34,000 --> 01:04:35,000
and the channel can grow.

641
01:04:35,000 --> 01:04:36,000
Okay.

642
01:04:36,000 --> 01:04:37,000
Thanks everybody.

643
01:04:37,000 --> 01:04:38,000
Great being with you.

644
01:04:38,000 --> 01:04:39,000
Have a nice day.

645
01:04:39,000 --> 01:04:40,000
Until the next video, take care.

646
01:04:40,000 --> 01:04:41,000
Goodbye.

