{"text": " the Q&A button, or raise your hand after the talk and we can call on you to ask a question by video or audio. A little bit of logistics, we still have an open slot on December 8th, so if you have a talk at any length, it can be less than an hour or a full hour, and you want to share with the BCS Cog Launch community, just get in touch with me by email, you can respond to that announcement email that you got for this event, and we can work that out. With that, let's get to our main content. So, Sue, we're telling us today about rapid generalization of knowledge and structured domains. Take it away, Sue. All right. So, as John mentioned, I'm Sue. I'm a third-year PhD student in the BCS department, and I'm co-advised by Professor Ela Feed and Professor Josh Tenenbaum. And in general, the question I'm interested in is, how do people generalize their learning to novel situations? And in any given domain, if the underlying space is structured, we might learn those underlying structures independently of the sensory observations, and that might in turn help us generalize to novel situations. So, what I'm presenting today is a step towards answering this question. I'll start with motivation, and then I'll make a case for why hippocampal entorhinal system is an important system to study if you're interested in generalization. And then I've divided the rest of my talk in three parts, and I'll give you a brief overview of those three parts before I go into the details of each of those. So, imagine you go to Costco in Waltham, since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know where the bakery section is or where the fruit section is, and now imagine you go to a completely different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there, Costco might have the same layout, or it might have a layout which is some transformation of the original layout. For instance, it might be a reflective version of the original, or there might be minor changes. And despite of that, you're still able to find the things you're looking for using your previous knowledge of the map of Costco. Another example is roundabouts. So, if you learn to go about a roundabout in Cambridge, then even if you go to any other country or city, then you will be able to use your previous knowledge to actually navigate through that roundabout. So, generally, we learn novel environments as compositions of spatial structures that we've already seen before, and that allows us to quickly generalize and learn new spatial environments. For instance, when you go to a new city, you might encounter Costco again, you might encounter a roundabout again, and you know which map to pull out when you are in Costco, and which map to pull out when you are navigating around a roundabout. And so, here's another example where this is a hotel which has symmetric left and right wings, and if one has explored the left wing of this hotel, then they might be very quickly able to generalize their learning to the right wing and make inferences about the right wing, even if they haven't really directly explored the right wing. So, humans are actually very good at making these complex inferences from just very sparse observations, and this ability has been suggested to be a result of a systematic organization of knowledge called the cognitive map. And hippocampal entorhinal system is known to be important for the construction of this cognitive map. So, for instance, rodents when they explore a 2D spatial environment, it has been found that hippocampus has these place cells which code locations in the 2D environment, and there are good cells in the entorhinal cortex which show this hexagonally symmetric firing fields, which are periodic, and which have been thought to encode location, but also cell motion-based euclidean displacement. And these codings are important for the construction of cognitive map because they provide an allocentric representation. So, given that the hippocampal entorhinal system encodes spatial variables, the next question is, can this also be used to represent other continuous task variables other than space? And the answer to this question is yes, and I'll give one example. So, here in this experiment has been found that cells in hippocampus and entorhinal cortex respond to task-relevant variables like sound frequency. So, in this task, rodents pull this lever, and as they pull the lever, the frequency of the sound coming from this sound source actually keeps increasing, and they have to release the lever when this frequency is in this target zone. And what is found is that in hippocampus and entorhinal cortex, there are cells that fire for specific frequencies during the sound modulation task. So, that shows that entorhinal cortex is a system that is able to represent continuous task variables, even other than space. And so, our next question is, could this system also help us organize and navigate discrete knowledge? And family trees are one example of discrete knowledge. So, in family trees, there's an underlying hierarchical structure that we learn. And once we know that structure, then we can apply that structure to my family tree or to your family tree or anyone's family tree, and we can generalize that knowledge. So, we can make inferences like this, because Olivia is Emily's sister and Sam is Emily's son, Sam must be Olivia's nephew. So, even though we haven't directly observed this relationship, but just by mere observation of these two relationships, we are able to infer this relationship, because we know the underlying hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system might allow organization of this kind of a discrete knowledge, but that's only possible if it allows encoding non-Equidian relationships. With that, I'll go back to the spatial domain, and I'll talk about how spatial knowledge might be organized. And when I talk about organization, I'll point to the fact that even in a continuous domain like space, it might be possible that we have both Euclidean and non-Euclidean components to represent space itself, and thus making the system generalizable to even discrete domains. So, here's an experiment conducted by Bill Warren, where they show that people actually do not learn a global Euclidean map of space. So, in this experiment, they constructed virtual environments, and they basically, the task was for people, for human subjects. So, this was a human behavioral experiment, and human subjects were asked to go to different landmarks in this spatial environment, and they also built counterparts of the spatial environment, which were non-Euclidean, in these environments. So, when you enter one part of the wormhole, you seamlessly exit from the other end of the wormhole, and subjects were not aware of the existence of these wormholes. And basically, what they found through various manipulations of the experiment was that people do not actually learn a global Euclidean map, but rather a labeled graph, like representation, where the nodes represent places, the edges represent approximate distances between these places, and the node labels, which are angles, represent approximate angles between these places. And so, we built on this representation, and we proposed that people might be representing topometric maps, which are locally metric or Euclidean, but globally topological. And the main advantage of this kind of representation is that it allows us to combine accurate local maps into a global map, which might be inconsistent, but it still provides enough sufficient information for navigation. So, the next question is, can this kind of a topometric map be implemented in the brain? And if so, how? And next, I'm providing a theoretical framework for how it might be possible to represent such topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex. So, this is a topometric representation of space. You can see that these are metric maps connected topologically by these connections. And here, on this side, I'm showing a grid coding space, which is dense coding space with large capacity. And here, I'm showing place coding space, which also has large capacity, but it spars, so it can receive sensory inputs and form conjunctive representations. And so, in this schematic, small changes, sorry, large changes in contextual input from our spatial domain was remapping in the place cells, which in turn trigger remapping in the grid cells, enabling the formation of these local metric maps that can be reused. And so, on this schematic, we can really take any subpart of this schematic and call it a sub map. And this allows us to compose sub maps, because once we have learned a particular sub map, then we can actually encounter the sub map in a completely novel situation and still be able to spatially navigate and reason through it. Another thing that it allows us is learning non-ecredient relationships, because place cells actually encode topological relationships, enabling the representation of non-ecredient relations. So, now I'm going to describe the three parts in which I've divided the rest of the talk. So, in the first part, I'm probing whether sub maps drive past learning in complex spaces using human behavioral experiments. In the second part, I will talk about determining which principles might guide fragmentation of a space into sub maps. And finally, in the third part, I'll propose a framework for building a neural model of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast learning in complex spaces, specifically in humans. And this work is in collaboration with Marta Krivind, who's a postdoc in Tenenbaum Lab, and Kevin, who's an undergrad in the CS department. So, here I hypothesize that humans learn adaptable and compositional sub maps of spatial structures. So, for instance, this is a baseline environment, and this is a top-down view showing an environment with four rooms. And here I've shown certain transformations of this environment generated by small generator programs. And you can see this is the same environment rotated, because now you're entering from this point, so it might appear rotated to you. And here's a reflection of the same environment. Here is a transformation where we've removed the wall and added a shortcut, and here we've added a wall. And this is just the repetition of the same environments. And there's another transformation which is scaling, where you can imagine this environment scaled up to a bigger size, but having the same geometrical layout. And what I'm suggesting is that once people have learned the map of the space line environment, their representations might be adaptable to some or all of these transformations. And furthermore, people might represent richer spaces by combining these maps and their transformations, leading to quick generalization and learning. So, this can be modeled using Bayesian program learning framework, where concepts are represented as simple programs, and rich concepts can be built compositionally from them using a higher-level generative model. So, there is neural evidence for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab, and they show that when a rodent goes through this environment in four labs, there are cells which fire specifically for particular places in this environment, but there are also cells which encode, which are event-specific and encode specific labs. So, for instance, there are cells which show increased fighting rates as you go from lab one to lab four, and there are also cells which only fire specifically on lab one or on lab two and so on. And so, what I'm suggesting is that when we have repetitions of the same environment, there might be cells that encode the basic map of this environment, which, which is consistent across these occurrences, but there might be a second set of cells, which are event-specific and might encode which instance of this environment we are on. Here's another example, where this is an example of scaling, where the rodent actually just explores this circular environment, and this is the place field found in that circular environment, and when the circular environment is scaled up to a bigger size, the place field also scales according to the size of the environment. So, this shows that the map which the rodent has learned of this environment is actually adaptable to this transformation of scaling to a bigger size, and map also scales proportionately with the size of the environment. And here's a third example, where rodents form different maps, place maps in these different environments, and when these environments are composed by connecting them through a corridor, rodents end up using the same maps which they had learned before for these environments. And furthermore, if I replace this environment with one of the previous environments seen before, then remapping is only observed in this part of the environment, and this part of the environment actually stays the same using the same previous map. So, this provides some evidence in support of composition of independent local sub-maps. So, in my experiment, I aim to assess whether people learn sub-maps on spatial structures and use them rationally in exploration, and I hypothesized that people might learn sub-maps that are adaptable and compositional, and my first alternate hypothesis is that they might learn sub-maps of spatial structures that might not be adaptable or compositional to certain transformations, and the last alternate hypothesis is that people might just learn a global representation of environments without learning any sub-maps. So, in order to test this hypothesis, we're building this task where we are building 3D virtual environments using Unity, and these environments have this repeating structure, and the task is for the subjects to find maximum amount of diamonds embedded in these environments in a limited amount of time given to them, and in order to test adaptability and composition, we can also have these repetitions be transformations of each other, for instance, here it's a reflection, or we can also have these environments composed of different structures to see whether people can compose their representations or structures that they've already seen. So, here's an example. Here you see that a person is navigating through corridor and enters a structure, and they go to one of the rooms and they do not find anything there, and then they decide to go to the other room, and they end up finding a reward there, and now they're going back to the corridor and they continue exploring the environment, and when they enter another section, if they show a preferential navigation strategy towards the room that has a reward, then that indicates that they have realized that there's a repeating structure in the environment and indicates a possibility that people might be learning sub-maps and identifying sub-maps as they're navigating the spatial environments, and furthermore, if we do find through the experiment that people actually learn sub-maps, then we can use similar environments to design experiments where we can test for adaptability to transformations of environments and also for composition of different spatial structures. So, that takes me to the next part of the talk, which is determining which principles guide fragmentation into sub-maps. So, since we are seeing that sub-maps drive fast learning, the next natural question becomes what determines this fragmentation of a spatial environment into sub-maps, and this work is in collaboration with Mirko Klukas, who is a post-doc in the feed lab. So, here my hypothesis is that neural remapping is a signature of sub-map reconstruction. So, here I'll explain what remapping is. So, basically, this is a 2D environment, and when the animal just explores this 2D environment, we find hexagonally periodic grid fields in the entorhinal cortex, and when you actually insert these walls in this environment, then what is observed is that when the animal turns, then this grid field which is formed either re-orients or shifts, and this is called remapping, when the grid field actually re-orients or shifts from its original orientation. And so, what is observed is that animals actually end up using the same grid maps in alternate arms. So, this indicates the use of maps. And here's another example of the use of maps. So, basically, this is a 2-room environment, and animals explore this environment, and it is seen that eventually the map formed in environment A is the same as the map formed in environment B, over short time scales. So, this is another example of the fact that animals are reusing the maps in both the rooms which look very similar. So, what I'm suggesting is that this field repetition doesn't result from localization error or purely due to disorientation, because even when you use transparent walls in this environment, you still see that the grid maps are being reused in alternate arms, even though the animal can see through these transparent walls. Furthermore, if you extend this 2-room environment to a 4-room environment, you still see field repetition in all of these rooms, which suggests that animals are actually reusing sub-maps in a calculated way for efficient representation, rather than just being disoriented. So, next I talk about existing models of remapping, and there are two classes of models. One class of model suggests that remapping is driven by sensory ambiguity. So, for instance, if you are in an environment that looks similar to an environment you've been before, either in terms of its geometry or its visual observations, then you might end up using the same map that you had learned from a previous environment. Then there's another class of models that suggests that remapping is based on environment topology, instead of just sensory ambiguity. So, here, each state in the environment is represented in terms of its successor states, and it's called a successor representation. And this successor representation actually ends up looking similar to place feeds, and if you do an identity composition on these successor representations, then you get fields that are very similar to grid fields. And this successor representation encapsulates inherent dynamics of the environment, as well as the policy that the agent is following. However, there are other approaches like the graph-leplacian approach, which is policy independent. So, what are some of the limitations of these models? So, the models that are based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment like this, these two regions actually look very different. These models will not have any map fragmentation or remapping. However, on the other hand, models which are based on environment topology, actually, it's not very clear how remapping would happen on first visit in these environments, because you need to build up the successor representation or the transition matrix of the environment before you can observe the grid fields. So, in our model, we address these limitations, and we have remapping on first visit, and we also have remapping without sensory ambiguity. And next, I'll go into the details of our model. So, we interpret grid remapping as fragmentation into submaps. Why is this a useful interpretation? That's because remapping enables topological representation. So, for instance, if we are dividing this environment into submaps, then we also need to store the relationships between these submaps, and this enables a compact topological representation, which is beneficial for planning. Second reason is that remapping reduces path integration errors. So, if you try to learn a global map, it can very quickly become inconsistent because of accumulation of path integration errors. But if you divide the environment in submaps, then it becomes easier to map the environment. And this has been shown by using Atlas framework in robotics, where they divide the environment into submaps in order to map the environment, and it works very well for large environments. And the third reason is that remapping enables representation of abstract cognitive spaces, because it allows representation of non-euclidean structures. So, in our model, we have two possibilities. Either we can extend an old map, or we can decide to remap. And when we decide to remap, we can either remap to a new map or remap to an existing map. So, for instance, in the experiments we saw that in the square environment without walls, the map that the animal learns is always extended. But when we insert these walls in this environment, then the map is extended within lane one. But when you turn from lane one to lane two, you actually remap to a new map. And when you turn from lane two to lane three, you end up remapping to an existing map, which is the same as lane one. And similarly, in the two room experiment, we saw that when you go from room one to the corridor, you end up mapping to a new map. And when you go from corridor to room two, you actually end up remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going to extend a map or whether we should be remapping. So, in our model, remapping is based on the notion of contiguous regions. And a contiguous region is a region such that when I stay within that region, my visual observations change very little. And these contiguous regions are connected by these bottleneck states. And this is aligned with the experimental data, which we have seen, which suggests special rule of doorways and corridors. So, now we formalize the concept of contiguous regions by defining a measure of similarity. So, we define similarity as the ability to predict observations at one pose from the observations made at another pose. And so, the overlap between the observations made at two poses actually is a notion of similarity. And this formalizes the concept of contiguous regions as a region where any two points are similar. And here, I'm showing that similarity actually decreases when you transition between contiguous regions. So, if you look at points which are within this contiguous region, their similarity is high with respect to this point. But for points which are in other regions, the similarity is pretty low as compared to this point. So, then we can use this notion of similarity to define density in order to do density-based clustering. And here, we define density as a similarity between any pose X and its mth nearest neighbor. And this notion can be used with any greedy algorithm like optics to generate fragmentations of the environment. And here, I'm showing one example of fragmentations of the environment where it gives four different clusters corresponding to these four different colors shown here. So, given that contiguity is a local property, we can also try to compute segmentations online by predicting current observations from the past. And in this case, observations can be represented by boundary vector cells. And we can implement a short-term memory which stores exponential moving average of boundary vector cell activations to approximate the similarity. So, for instance, our short-term memory at a previous time step can be used to predict observations at a current time step to compute the similarity between two poses. And another component which we need to add to our model is the long-term memory component, which helps us decide whether we should be remapping to an existing map or we should be remapping to a new map. So, for all of these environments, our model makes the correct predictions which are in line with the experimental data observed. And these experiments have been done and we have neural data for them. This is a new prediction that our model makes for amorphous naturalistic environments. We predict that even in these environments, the map will be segmented and grid fields will realign when going from one contiguous region to the other. And we do not predict any map fragmentations in these spiral mesas. So, given that we built or proposed an algorithmic model for map fragmentation, the next question is, how can map fragmentation be implemented on a neural level? So, we want to provide a framework for building a neural circuit model of map fragmentation. And this work is in collaboration with Sarthak and Murko, who are both postdocs in FEDLA. So, going back to our theoretical framework, we had suggested that place cells might encode topological relationships between metric maps that might be represented by the grid space. And now I'm going to talk about how we can implement that at a neural level. So, at the neural level, we start with factorized representations in which different aspects of knowledge are represented separately and can then be flexibly recombined. So, for instance, in this case, location information from grid cells and contextual information from sensory cells form this conjunctive representation in place cells. Here grid cells can enable path integration and can be thought of as implementing an affine vector space or an impedance space. The recurrent wiring between these place cell population encodes neighborhood relationships or topology. And here, large changes in contextual input cause remapping in place cells, which in turn cause remapping in grid cells through these back projections. And remapping here corresponds to transitioning from one local map to another. So, most of the previous work on interplay of grid and place circuits focuses on maintaining firing properties of one population based on the inputs from another. So, for instance, successor representation suggests that grid cells are a low-dimensional representation of place cells that stabilize place cell activity. Similarly, here's a model which implements non-negative PCA of place cells. So, place cells are at the input. The weights are learned through heavy learning and a non-negativity constraint. And this network does PCU on the inputs, and the outputs end up converging to grid-like fields, again, suggested that grid cells might be a low-dimensional representation of place cells. Another set of work suggests that inputs from border cells to grid cells could be used for error-correcting grid cells. So, here I'm showing a one-day schematic just to make my point. So, this is a rodent at a specific location in space, and this is the grid activity profile that represents that location. And when the rodent explores the environment and comes back to this location, the representation of this location has drifted with respect to the original, and there's some error in the representation. And if the border cell activations are provided as input to grid cells, then they activate the current subset of neurons doing error correction and pulling back the representation to the original representation. And this is what this looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell representations are not very stable, but with border cell inputs, your grid cell representations are fairly stable. So, I also want to point out the fact that place cells are thought to store neighborhood relationships in their reference synapses, and therefore they could implement a topological navigation strategy. And many models of place cell-based navigation have actually emphasized this view. So, they've suggested that recurrence synapses encode either spatial or temporal connectivity, as suggested by Blum and Abbott, or they encode transition probability, as suggested by the successor representation work. So, given all these insights, our goal is to build a comprehensive neural circuit model of premapping. And we start with these two questions. Does high-capacity grid code, when projected to place cells, also lead to high capacity? And given these conjunctive representations between the location input and the sensory input, can we learn neighborhood relationships between place codes? And before I go into the details of capacity, I just want to point out that traditionally, Hopfield networks have been used for storing memories and patterns. And it has been observed that the maximum patterns that these networks can store is n, where n is the total number of neurons in the network. And modern Hopfield networks, also known as dense associative memories, have an exponential capacity, but they use many body interaction terms, which are not biologically plausible. And in our model, we stick to using two interaction terms in the weight computations, and we still get exponential capacities. So, this is the architecture of our model. The model has different grid modules, which have different scales or periods, and the binary grid code is projected to place code randomly. And the back projections from place cells to grid cells are learned through associative Hebbian learning. And we observe that when we perturb these place cells with a noisy version of place code representations, then the network is able to successfully reconstruct all the patterns it's trained on. So, the network is fairly robust to noise. Furthermore, this network has exponential capacity that grows much faster than a non-modular network where the grid cells are non-modular. Also, the network generalizes stored inputs to create stable attractor states around every pattern in the grid coding space, despite training only over a vanishing fraction of contiguous grid coding space. So, for instance, if my grid coding space has around 10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns, and the network is still able to robustly reconstruct all the 10,000 patterns in the grid coding space, which is pretty striking. Furthermore, next we add these heterosciitiative learning on the recurrent connections on place cells to see if they can encode neighborhood relationships or 1D sequences. And what we find is that the product of these weights converges to this transition matrix, which is actually the analytical matrix, an analytical transition matrix that relates contiguous grid codes in the grid coding space. Furthermore, this network actually has perfect sequence recall given enough number of place cells to approximate this transition matrix. And again, training on only a subset of the sequence is enough to recall the entire sequence. So, for instance, if I train, if I have a sequence of length 500, and I train the network on only first 150 patterns in the sequence, the network is robustly able to reconstruct all 500 patterns in the sequence without having seen all of them before. So, the next step in this network is to introduce sensory input. And the sensory input would project randomly to place cells and back projections from place cells to sensory input would be learned through associative hybrid learning. And here grid cells would form a basis, and hippocampal places would link that basis with arbitrary sensory input. And this combination of structured inputs and unstructured inputs could potentially enable the storage and robust recollection of a large number of arbitrary sensory patterns from this partial use. So, how does this connect to map fragmentation? So, in part two, we talked about map fragmentation based on the notion of contiguous regions. And here I'm positing that when you transition between different contiguous regions that actually corresponds to a large contextual change, which when provided as input to this network would trigger remapping in place cells, which would in turn cause remapping in grid cells, thus leading to the formation of local sub maps. And how does this connect to part one, where we saw that sub maps might enable quick learning and generalization in humans. So, this network actually enables us to anchor grid maps to external cues through these conjunctive representations in place cells. And this anchoring to external cues actually enables the alignment of grid maps, even when points of departure in an environment are different, leading to adaptable representations. Furthermore, if you are in an environment that is composed of previously seen spatial structures, then this anchoring still enables you to pull out the right map when you're navigating through that composed spatial structure. So, to summarize, our global hypothesis was that cognitive map is organized as a globally topological and locally metric or Euclidean map. So, this is one illustration of a topometric map. And we said that any sub part of this map can be termed as a sub map. In part one, we suggested that learning sub maps that are adaptable and compositional may drive fast learning in complex spaces. And we proposed to conduct human behavioral experiments to test this hypothesis. In second part, we proposed an algorithmic model of map fragmentation into sub maps based on the notion of contiguous regions. And in the third part, we proposed a framework for a neural circuit model of map fragmentation. And overall, we're suggesting that sub maps enable humans to build up a knowledge base of spatial structures that they can continuously enrich and refine throughout their life by combining their existing spatial knowledge with their new experiences. So, that's all for my talk. And before I end, I want to acknowledge my collaborators again, Marta, Kevin, Merko and Sata. And I also want to thank my supervisors, Ila and Josh for their continued feedback and support and discussions on these projects. I also want to thank Matt Wilson for his feedback and valuable suggestions during my committee meetings and the members of FeedLab and TenBomb in general for their support. And before I end, I also want to thank the VCS department and McGowan Institute for their continued support and for providing an environment that's conducive to research, even in the face of this pandemic. Thank you. And I can take any questions now. Thanks, Sue, for the great talk. We already have a question from Marta Griffin. Great talk. I have a question about partitioning a space to sub maps. What do you think will happen if you ask humans to intuitively partition an environment to regions? How would there be segmentation compared to the maps given by your similarity measure? So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively try to segment a map, how would that compare to the segmentations which we have proposed in part two of my talk where we have given an algorithmic model for map segmentation. So, basically, the way I view it is that in the algorithmic model of map segmentation, I basically took one metric map and one small portion of the map. And we said that that can be decomposed into various fragments of sub maps. But humans are actually able to reason in much richer environments than the ones which we saw in this algorithm. And so, there we can view it as something like this where you might have even small spaces being segmented into multiple maps. So, let me just see if I can, yeah, so even if you have something like this, so even within this, so I was calling this a sub map, but even within this sub map, we might have multiple maps. So, based on the notion of contiguous regions, you might have a local map here, a local map here, and all these maps might be connected. And so, the algorithmic model would predict those kinds of map segmentations, but then even a combination of those map segmentations can be termed as a sub map. And then that is what I was talking about in part one where I'm talking about composing the sub map which is even itself composed of smaller maps. And we can take this representation of this map and then compose them in various ways to build richer environments. I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret sub maps. How humans would interpret? Well, it's a little bit unclear to me what you mean by interpret sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret segmenting this environment versus maybe a more complex environment. And I would say that currently, this algorithm doesn't predict anything about how humans might interpret grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could look at it in the future. And I think some of my work in part one would potentially address that going forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli. Yeah. Hi. Can you hear me? Yes. Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird. Great talk. Can you talk a little bit more about how your model handles time? Or I think you mentioned that it was able to handle like replay of different sequences of states through some map. How would it be able to handle different trajectories through the same space that might activate play cells in different sequences? So the version of the model that I presented, that is trained on discrete patterns, right? So the sequences here, the sequences which are trained on here through heterospecific learning are composed of these discrete patterns. So if I say the sequences of length 500, there are 500 discrete patterns in that sequence. And when you say time, time is something continuous. And so we have worked on extrapolating this model to more continuous domains. And in a continuous domain, what would happen is that your sequence, instead of being discrete patterns, would be composed of these continuous stream. And in that case, we haven't explored what the network performance would be, but that's something ongoing. And definitely in 2D spaces, we would like to train this model on 2D sequences and then see, so right now, these results are pertaining to 1D sequences. But we do want to extrapolate the results and train this model on 2D sequences to see how it performs in 2D. And I think that would potentially address then your questions about time, because that pertains to continuous domains. Okay, yeah, thank you. Okay, we've got a big stream of questions coming in. The first is from Adam Eisen. Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map segmentation and topological association could be extended to non-spatial domains? That's a very good question. So in FeedLab, a small group of us have been thinking about how this could be extended. And so the idea is, I'm just going to go back to, let's see if I can just quickly hop back to my theoretical framework slide. So really, we're building up on this theoretical framework. And when we're thinking about non-spatial domains, we go back to this schematic picture. And here, basically, what we're saying is, within the spatial domain, I said that large changes in contextual input can drive remapping in place cells, which would drive remapping in the grid coding space. So similarly, in relational domains or in discrete domains, as we're thinking about it, we are referring back to this schematic picture. And we think that the phenomenon of remapping actually would enable us to encode these non-eclidean relationships. So first thing is that here, we're suggesting topological relationships. So that already enables us to encode non-eclidean relationships. But this phenomenon of remapping also allows us to make jumps. So if you think about a family tree, if you might try to encode it in an eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact, almost impossible to encode it in an eclidean space. But then if we provide a framework where we allow topological representations and we allow jumps in this eclidean space through this phenomenon of remapping through place cells, then that can allow us to potentially encode non-eclidean relationships. And that's the way we are thinking. And this is still work in progress. And we actively think about it. Next question from Nancy Camusher. How does your system decide when and where to carve the world in the sub-maps, especially in non-built environments where the divisions may be less obvious? So at least in part two, where I talk about fragmentation of maps into sub-maps. Let me just go back. So in these kinds of environments, we basically describe the principle of contiguous regions, where we have suggested that when you are navigating through this environment, if you are in an environment where your visual observations stay more or less consistent, then you will keep extending a map. But when you go from this region to another region, let's say you navigate from here to here, where your visual information here is completely distinct from your visual information here, then in that case, you will decide to segment the space. And that's what we are predicting. We're predicting that in this case, when you travel from one contiguous region to another, you will segment the space. And that's how you divide the space. So that's one principle, which we are describing for map segmentation. And that might not be the only principle, but that principle explains some of the experimental results that have been observed newly. And there might be another principle, for instance, path integration error, where if your path integration builds up and it reaches a certain threshold amount, then you automatically might start a new map. But I haven't illustrated that here, but that's also one of the other driving forces for us to actually split a space into multiple sub-maps so that we can have more efficient representations. Okay, next question from Chen. I was wondering if you can give some intuition about the mechanism for composition of sub-maps. So really, this is from the happy and learning mechanism. And secondly, can you comment on the relationship between this and the Hopfield mechanism? So since we're talking about composition of sub-maps here, in this case, in this model, what I'm suggesting is that we might end up forming local sub-maps by using the phenomenon of premapping. So here I'm suggesting, again, if you have large changes in contextual input, so for instance, let me just go back to the slide, which kind of connects this. So here we basically suggested how this environment can be segmented into different sub-maps. And on the mechanistic level, I'm trying to suggest that any large contextual input is going to cause remapping in place cells, which means the cells which are firing would be changed and they would represent a new map. And this remapping would actually enable also remapping in grid cells through associative learning because each place cell representation is associated with a certain grid cell representation. So when you have changes in firing fields of place cells, that also triggers certain corresponding changes in the grid coding phase, which is basically called remapping in grid phase. And so here I'm interpreting, when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you go from one contiguous region to another, that actually corresponds to a large contextual change and that contextual change triggers remapping in this model and this remapping in place cells then triggers remapping in grid cells phase. And that corresponds to the formation of local sub-maps. And these topological connections from place cells to themselves might actually represent how these local sub-maps are connected to each other. And there's also, going back to the anatomy, there's also a possibility of splitting this population actually into different populations. One that might just encode conjunctive representations and another one that might have these recurrent topological connections similar to CA1 and CA3 distinction in anatomy. But that's how I'm thinking about it at the moment. I want to point out a quick addendum from Ila who said to Nancy's question, we consider online segmentation decisions driven by regions or points of high surprise affordance changes and PIR. Okay, another question from Anya, even over. Do you think the way humans form space maps depends on whether their language uses directions that are egocentric left and right or allocentric north and south? That's a great question. So there were experiments in both animals as well as humans which have shown that we actually have both kinds of representations. We have egocentric representations and we also have allocentric representations. And the representations in the hippocampal inter-animal system are usually allocentric, but the thalamus is the part of the brain which is responsible for egocentric representations. So really, when we're navigating spaces, we are probably using mixed strategies. We do use our allocentric representations, but in some cases we might be using egocentric representations. And that's mostly also true for routes which we are traversing very frequently. So if there's a route which I take every day, let's say going from my home to BCS, then that converges to root learning. But if I've had very less experience in any environment, then I'm mostly using allocentric representations to actually make my way through that environment. And hence, allocentric representations are actually attributed to being able to make novel inferences. And when I'm building circuits here using grid cells and place cells, I'm mostly talking about allocentric representations. But then when I was talking about submaps in human experiments, I'm kind of agnostic to the fact that whether those representations are egocentric or allocentric because even if you learn an egocentric strategy, even in that case, if you determine that there's a repeating structure to the environment and you recall that this is the same environment that I've seen before, then you can still apply the same egocentric strategy or an allocentric strategy depending on which one you decide to use at that point. But going back to the main question, the experimental data shows that we're actually basically learning both kinds of strategies. Okay, Senke's Pellevan says, great talk. What are the large scale, sorry, where are the large scale topological relations coded in the network? So in the network model, our focus, so in this case, we are basically encoding topological relationships using the recurrent connections on the place cells. But for large scale topological relationships, what we are moving towards is splitting this population into two distinct populations. So one population that will only have conjunctive representations, and perhaps another population which will have the recurrent connections similar to the distinction between CA1 and CA3 in the brain. And it's true that in this network model, we don't really distinguish between small scale and large scale topological relationships. But by introducing this additional population, we hope to abstract away the large scale relationships. Also, even in this case, if we have minute relationships between spaces, then we can also extract away the large scale relationships. But again, adding another population might make that easier to make that abstraction. Okay. Nancy has another comment. There must be some way to represent globally metric information. For example, I know the approximate distance and angle from my current position to far away locations in my world. Do I understand right that you would say this information has to be pieced together from just the topological relationship between the sub maps that connect those locations? So that's a great question. I also sometimes wonder because like when I'm here, I can kind of look very far in space and be able to tell that, okay, I need to go in this direction. And that's definitely metric information. And so going back to the labeled graph hypothesis, which was suggested by Bill Warren. So even in this hypothesis, I'm just saying that this is we are learning metric representations here, which are consistent Euclidean representations, which means that they actually obey all the postulates of Euclidean geometry. And the connections between them still have metric information. There's still information about angles and directions, but they might not obey all postulates of Euclidean geometry. So for instance, you might have some inconsistent representations. You might not know exactly what distances and displacements you have to go in order to reach a goal, but you know approximately which distance and which angle. So even the topometric representation is not contradicting the fact that you might have approximate distances and angles. But what it's saying is that you have very accurate metric information about small pieces of space, but then for faraway pieces of space, you might still have approximate distances, angles, which help you navigate. Okay, another question from Sean Chen. Sorry for my pronunciations. Great talk. Could you elaborate the mechanism of exponential number of memory capacity in your model? So here's the, here's the network, right, where I'm showing that the network has exponential capacity. So basically in this network, we have different grid modules which have different periods and this is the grid code is actually binary. And we project this grid code randomly using random weight matrix to place cells. And the number of place cells in this case is much larger than the number of grid cells. And now we learn these back projections from places to grid cells using Hebbian learning. So it's just simple Hebbian learning. I don't have the equation. Basically, this is the equation, right? You can consider these as place cell patterns. And this is how we compute the weight matrix for the weights going back from place cells to grid cells. And once we've done that, then the way we test for memory is basically by perturbing the place code patterns. So we put up the place cells by a noisy version of the place code pattern, right? So in this graph, I'm showing that if you perturb them with 20% noise, then with 300 place cells, you get an output noise of zero, which means you get perfect reconstruction of the patterns that you're perturbing the network with. And that's how I'm defining exponential memory, right? And when I say exponential memory, what I'm suggesting is that you can, as you increase the number of cells in the network, the number of patterns that you can reconstruct from the noisy versions actually grows exponentially. Okay, I'd like to ask a question as well. I'm curious to hear your thoughts on sort of computational level descriptions here. So it seems like your model is largely tuned towards sort of prediction, accurate prediction of your local spatial environment and motive mechanisms for doing this well. And I wonder what you think about computational level accounts of this to be more, to ask a more concrete question. Do you think there's a role of reward or something other than accurate, just accurate prediction in the shaping of neural representations of space? Okay, so you're asking whether there is a role for reward in shaping the representations of space in the mechanistic model? I mean, more generally, what's the objective driving this algorithm? Is it about accurate prediction or is there something more than prediction? Okay, so you're asking what is kind of like the motivation for building the mechanistic model? Yeah, yeah, I mean, some of the graphs you're showing here about, some of the graphs you showed, for example, were about recall. And recall, accurate reconstruction of an environment might be one objective. But it seems like you could say that this is a mechanism evolved for some purpose, but it's just that or different than that. So, right, so these two questions, which I started with, which are related to capacity of the network, and whether we can do, whether we can learn sequences, these are basically questions which I started with, because I wanted to explore the theoretical properties of these circuits, right, given the coding properties that we know of grid cells and place cells and given biologically plausible learning tools through have been learning, what are the coding properties that these networks can exhibit, right, how much capacity they have, how much information can they store. And so, yes, even in these models, since we are talking about reconstruction, basically says that if you have some noise, if you have some noisy estimate of where you are in space, then this kind of a network can help you clean that estimate, right, and reach a cleaner version of where you might be in space and help you localize in space. And also, these temporal connectivity on the place cells kind of have a predictive component to them, right, because you can import sequences. So, when you are at a particular position, you might be able to predict what's coming next. And so, these are kind of small components, which I'm using to go towards building a model of remapping in space, right, because ultimately my goal is to be able to figure out how these neural circuits might lead to the formation of these sub-maps. And by knowing the properties of these circuits, then I'll be better able to construct and build those networks, which actually can build small local sub-maps of environments. I'm reading out a comment from Ila. To my question, you could view this as a structure learning even without reward. A reinforcement learner, including the brain, could use these learning structures. Cool. Well, I think we're actually over time now. That was a great talk, Sue. Thanks for sharing with us. Of course, thank you. So, we'll be back next week for another talk from Eli Pollock. Until then, have a good Thanksgiving, and I'll see you next week.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.640000000000001, "text": " the Q&A button, or raise your hand after the talk and we can call on you to ask a question", "tokens": [50364, 264, 1249, 5, 32, 2960, 11, 420, 5300, 428, 1011, 934, 264, 751, 293, 321, 393, 818, 322, 291, 281, 1029, 257, 1168, 50696], "temperature": 0.0, "avg_logprob": -0.10822515565205396, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.1326732337474823}, {"id": 1, "seek": 0, "start": 6.640000000000001, "end": 12.72, "text": " by video or audio. A little bit of logistics, we still have an open slot on December 8th,", "tokens": [50696, 538, 960, 420, 6278, 13, 316, 707, 857, 295, 27420, 11, 321, 920, 362, 364, 1269, 14747, 322, 7687, 1649, 392, 11, 51000], "temperature": 0.0, "avg_logprob": -0.10822515565205396, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.1326732337474823}, {"id": 2, "seek": 0, "start": 12.72, "end": 17.6, "text": " so if you have a talk at any length, it can be less than an hour or a full hour,", "tokens": [51000, 370, 498, 291, 362, 257, 751, 412, 604, 4641, 11, 309, 393, 312, 1570, 813, 364, 1773, 420, 257, 1577, 1773, 11, 51244], "temperature": 0.0, "avg_logprob": -0.10822515565205396, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.1326732337474823}, {"id": 3, "seek": 0, "start": 17.6, "end": 22.96, "text": " and you want to share with the BCS Cog Launch community, just get in touch with me by email,", "tokens": [51244, 293, 291, 528, 281, 2073, 365, 264, 14359, 50, 383, 664, 28119, 1768, 11, 445, 483, 294, 2557, 365, 385, 538, 3796, 11, 51512], "temperature": 0.0, "avg_logprob": -0.10822515565205396, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.1326732337474823}, {"id": 4, "seek": 0, "start": 22.96, "end": 27.52, "text": " you can respond to that announcement email that you got for this event, and we can work that out.", "tokens": [51512, 291, 393, 4196, 281, 300, 12847, 3796, 300, 291, 658, 337, 341, 2280, 11, 293, 321, 393, 589, 300, 484, 13, 51740], "temperature": 0.0, "avg_logprob": -0.10822515565205396, "compression_ratio": 1.6376811594202898, "no_speech_prob": 0.1326732337474823}, {"id": 5, "seek": 2752, "start": 27.919999999999998, "end": 33.84, "text": " With that, let's get to our main content. So, Sue, we're telling us today about rapid", "tokens": [50384, 2022, 300, 11, 718, 311, 483, 281, 527, 2135, 2701, 13, 407, 11, 25332, 11, 321, 434, 3585, 505, 965, 466, 7558, 50680], "temperature": 0.0, "avg_logprob": -0.24080323009956173, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.01922079734504223}, {"id": 6, "seek": 2752, "start": 33.84, "end": 43.44, "text": " generalization of knowledge and structured domains. Take it away, Sue.", "tokens": [50680, 2674, 2144, 295, 3601, 293, 18519, 25514, 13, 3664, 309, 1314, 11, 25332, 13, 51160], "temperature": 0.0, "avg_logprob": -0.24080323009956173, "compression_ratio": 1.2580645161290323, "no_speech_prob": 0.01922079734504223}, {"id": 7, "seek": 5752, "start": 57.52, "end": 73.84, "text": " All right. So, as John mentioned, I'm Sue. I'm a third-year PhD student in the BCS department,", "tokens": [50364, 1057, 558, 13, 407, 11, 382, 2619, 2835, 11, 286, 478, 25332, 13, 286, 478, 257, 2636, 12, 5294, 14476, 3107, 294, 264, 14359, 50, 5882, 11, 51180], "temperature": 0.0, "avg_logprob": -0.22123432159423828, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0032182838767766953}, {"id": 8, "seek": 5752, "start": 73.84, "end": 79.28, "text": " and I'm co-advised by Professor Ela Feed and Professor Josh Tenenbaum. And in general,", "tokens": [51180, 293, 286, 478, 598, 12, 345, 24420, 538, 8419, 17637, 33720, 293, 8419, 9785, 9380, 268, 46641, 13, 400, 294, 2674, 11, 51452], "temperature": 0.0, "avg_logprob": -0.22123432159423828, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0032182838767766953}, {"id": 9, "seek": 5752, "start": 79.28, "end": 84.0, "text": " the question I'm interested in is, how do people generalize their learning to novel situations?", "tokens": [51452, 264, 1168, 286, 478, 3102, 294, 307, 11, 577, 360, 561, 2674, 1125, 641, 2539, 281, 7613, 6851, 30, 51688], "temperature": 0.0, "avg_logprob": -0.22123432159423828, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0032182838767766953}, {"id": 10, "seek": 8400, "start": 84.0, "end": 88.32, "text": " And in any given domain, if the underlying space is structured, we might learn those", "tokens": [50364, 400, 294, 604, 2212, 9274, 11, 498, 264, 14217, 1901, 307, 18519, 11, 321, 1062, 1466, 729, 50580], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 11, "seek": 8400, "start": 88.32, "end": 92.8, "text": " underlying structures independently of the sensory observations, and that might in turn", "tokens": [50580, 14217, 9227, 21761, 295, 264, 27233, 18163, 11, 293, 300, 1062, 294, 1261, 50804], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 12, "seek": 8400, "start": 92.8, "end": 98.08, "text": " help us generalize to novel situations. So, what I'm presenting today is a step towards", "tokens": [50804, 854, 505, 2674, 1125, 281, 7613, 6851, 13, 407, 11, 437, 286, 478, 15578, 965, 307, 257, 1823, 3030, 51068], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 13, "seek": 8400, "start": 98.08, "end": 103.2, "text": " answering this question. I'll start with motivation, and then I'll make a case for why", "tokens": [51068, 13430, 341, 1168, 13, 286, 603, 722, 365, 12335, 11, 293, 550, 286, 603, 652, 257, 1389, 337, 983, 51324], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 14, "seek": 8400, "start": 103.2, "end": 108.0, "text": " hippocampal entorhinal system is an important system to study if you're interested in generalization.", "tokens": [51324, 27745, 905, 1215, 304, 948, 284, 71, 2071, 1185, 307, 364, 1021, 1185, 281, 2979, 498, 291, 434, 3102, 294, 2674, 2144, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 15, "seek": 8400, "start": 108.56, "end": 112.72, "text": " And then I've divided the rest of my talk in three parts, and I'll give you a brief overview", "tokens": [51592, 400, 550, 286, 600, 6666, 264, 1472, 295, 452, 751, 294, 1045, 3166, 11, 293, 286, 603, 976, 291, 257, 5353, 12492, 51800], "temperature": 0.0, "avg_logprob": -0.10687312178724394, "compression_ratio": 1.742765273311897, "no_speech_prob": 0.002181697404012084}, {"id": 16, "seek": 11272, "start": 112.72, "end": 118.88, "text": " of those three parts before I go into the details of each of those. So, imagine you go to Costco", "tokens": [50364, 295, 729, 1045, 3166, 949, 286, 352, 666, 264, 4365, 295, 1184, 295, 729, 13, 407, 11, 3811, 291, 352, 281, 43453, 50672], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 17, "seek": 11272, "start": 118.88, "end": 124.32, "text": " in Waltham, since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know", "tokens": [50672, 294, 343, 1302, 335, 11, 1670, 456, 311, 572, 43453, 294, 24876, 11, 293, 291, 1466, 264, 4471, 295, 43453, 13, 407, 11, 586, 291, 458, 50944], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 18, "seek": 11272, "start": 124.32, "end": 128.8, "text": " where the bakery section is or where the fruit section is, and now imagine you go to a completely", "tokens": [50944, 689, 264, 37519, 3541, 307, 420, 689, 264, 6773, 3541, 307, 11, 293, 586, 3811, 291, 352, 281, 257, 2584, 51168], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 19, "seek": 11272, "start": 128.8, "end": 134.16, "text": " different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there,", "tokens": [51168, 819, 1941, 11, 718, 311, 584, 6309, 11, 291, 352, 281, 8772, 38511, 11, 293, 291, 352, 281, 43453, 456, 13, 2754, 456, 11, 51436], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 20, "seek": 11272, "start": 134.16, "end": 138.24, "text": " Costco might have the same layout, or it might have a layout which is some transformation of the", "tokens": [51436, 43453, 1062, 362, 264, 912, 13333, 11, 420, 309, 1062, 362, 257, 13333, 597, 307, 512, 9887, 295, 264, 51640], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 21, "seek": 11272, "start": 138.24, "end": 141.92, "text": " original layout. For instance, it might be a reflective version of the original, or there", "tokens": [51640, 3380, 13333, 13, 1171, 5197, 11, 309, 1062, 312, 257, 28931, 3037, 295, 264, 3380, 11, 420, 456, 51824], "temperature": 0.0, "avg_logprob": -0.09377875694861779, "compression_ratio": 1.959322033898305, "no_speech_prob": 0.0006067832582630217}, {"id": 22, "seek": 14192, "start": 141.92, "end": 146.55999999999997, "text": " might be minor changes. And despite of that, you're still able to find the things you're looking for", "tokens": [50364, 1062, 312, 6696, 2962, 13, 400, 7228, 295, 300, 11, 291, 434, 920, 1075, 281, 915, 264, 721, 291, 434, 1237, 337, 50596], "temperature": 0.0, "avg_logprob": -0.06564743302085183, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0002866879221983254}, {"id": 23, "seek": 14192, "start": 146.55999999999997, "end": 152.23999999999998, "text": " using your previous knowledge of the map of Costco. Another example is roundabouts. So, if you learn", "tokens": [50596, 1228, 428, 3894, 3601, 295, 264, 4471, 295, 43453, 13, 3996, 1365, 307, 3098, 41620, 13, 407, 11, 498, 291, 1466, 50880], "temperature": 0.0, "avg_logprob": -0.06564743302085183, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0002866879221983254}, {"id": 24, "seek": 14192, "start": 152.23999999999998, "end": 157.92, "text": " to go about a roundabout in Cambridge, then even if you go to any other country or city,", "tokens": [50880, 281, 352, 466, 257, 3098, 21970, 294, 24876, 11, 550, 754, 498, 291, 352, 281, 604, 661, 1941, 420, 2307, 11, 51164], "temperature": 0.0, "avg_logprob": -0.06564743302085183, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0002866879221983254}, {"id": 25, "seek": 14192, "start": 157.92, "end": 162.72, "text": " then you will be able to use your previous knowledge to actually navigate through that roundabout.", "tokens": [51164, 550, 291, 486, 312, 1075, 281, 764, 428, 3894, 3601, 281, 767, 12350, 807, 300, 3098, 21970, 13, 51404], "temperature": 0.0, "avg_logprob": -0.06564743302085183, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0002866879221983254}, {"id": 26, "seek": 14192, "start": 163.44, "end": 167.67999999999998, "text": " So, generally, we learn novel environments as compositions of spatial structures that we've", "tokens": [51440, 407, 11, 5101, 11, 321, 1466, 7613, 12388, 382, 43401, 295, 23598, 9227, 300, 321, 600, 51652], "temperature": 0.0, "avg_logprob": -0.06564743302085183, "compression_ratio": 1.7683823529411764, "no_speech_prob": 0.0002866879221983254}, {"id": 27, "seek": 16768, "start": 167.68, "end": 172.32, "text": " already seen before, and that allows us to quickly generalize and learn new spatial", "tokens": [50364, 1217, 1612, 949, 11, 293, 300, 4045, 505, 281, 2661, 2674, 1125, 293, 1466, 777, 23598, 50596], "temperature": 0.0, "avg_logprob": -0.08737906035002288, "compression_ratio": 1.8359375, "no_speech_prob": 0.0009249646100215614}, {"id": 28, "seek": 16768, "start": 172.32, "end": 176.56, "text": " environments. For instance, when you go to a new city, you might encounter Costco again,", "tokens": [50596, 12388, 13, 1171, 5197, 11, 562, 291, 352, 281, 257, 777, 2307, 11, 291, 1062, 8593, 43453, 797, 11, 50808], "temperature": 0.0, "avg_logprob": -0.08737906035002288, "compression_ratio": 1.8359375, "no_speech_prob": 0.0009249646100215614}, {"id": 29, "seek": 16768, "start": 176.56, "end": 181.04000000000002, "text": " you might encounter a roundabout again, and you know which map to pull out when you are in Costco,", "tokens": [50808, 291, 1062, 8593, 257, 3098, 21970, 797, 11, 293, 291, 458, 597, 4471, 281, 2235, 484, 562, 291, 366, 294, 43453, 11, 51032], "temperature": 0.0, "avg_logprob": -0.08737906035002288, "compression_ratio": 1.8359375, "no_speech_prob": 0.0009249646100215614}, {"id": 30, "seek": 16768, "start": 181.04000000000002, "end": 188.8, "text": " and which map to pull out when you are navigating around a roundabout. And so, here's another example", "tokens": [51032, 293, 597, 4471, 281, 2235, 484, 562, 291, 366, 32054, 926, 257, 3098, 21970, 13, 400, 370, 11, 510, 311, 1071, 1365, 51420], "temperature": 0.0, "avg_logprob": -0.08737906035002288, "compression_ratio": 1.8359375, "no_speech_prob": 0.0009249646100215614}, {"id": 31, "seek": 16768, "start": 188.8, "end": 194.88, "text": " where this is a hotel which has symmetric left and right wings, and if one has explored the left", "tokens": [51420, 689, 341, 307, 257, 7622, 597, 575, 32330, 1411, 293, 558, 11405, 11, 293, 498, 472, 575, 24016, 264, 1411, 51724], "temperature": 0.0, "avg_logprob": -0.08737906035002288, "compression_ratio": 1.8359375, "no_speech_prob": 0.0009249646100215614}, {"id": 32, "seek": 19488, "start": 194.88, "end": 199.6, "text": " wing of this hotel, then they might be very quickly able to generalize their learning to the", "tokens": [50364, 11162, 295, 341, 7622, 11, 550, 436, 1062, 312, 588, 2661, 1075, 281, 2674, 1125, 641, 2539, 281, 264, 50600], "temperature": 0.0, "avg_logprob": -0.08843123471295392, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.000666330917738378}, {"id": 33, "seek": 19488, "start": 199.6, "end": 203.6, "text": " right wing and make inferences about the right wing, even if they haven't really directly explored", "tokens": [50600, 558, 11162, 293, 652, 13596, 2667, 466, 264, 558, 11162, 11, 754, 498, 436, 2378, 380, 534, 3838, 24016, 50800], "temperature": 0.0, "avg_logprob": -0.08843123471295392, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.000666330917738378}, {"id": 34, "seek": 19488, "start": 203.6, "end": 208.79999999999998, "text": " the right wing. So, humans are actually very good at making these complex inferences from", "tokens": [50800, 264, 558, 11162, 13, 407, 11, 6255, 366, 767, 588, 665, 412, 1455, 613, 3997, 13596, 2667, 490, 51060], "temperature": 0.0, "avg_logprob": -0.08843123471295392, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.000666330917738378}, {"id": 35, "seek": 19488, "start": 208.79999999999998, "end": 214.4, "text": " just very sparse observations, and this ability has been suggested to be a result of a systematic", "tokens": [51060, 445, 588, 637, 11668, 18163, 11, 293, 341, 3485, 575, 668, 10945, 281, 312, 257, 1874, 295, 257, 27249, 51340], "temperature": 0.0, "avg_logprob": -0.08843123471295392, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.000666330917738378}, {"id": 36, "seek": 19488, "start": 214.4, "end": 219.92, "text": " organization of knowledge called the cognitive map. And hippocampal entorhinal system is known to", "tokens": [51340, 4475, 295, 3601, 1219, 264, 15605, 4471, 13, 400, 27745, 905, 1215, 304, 948, 284, 71, 2071, 1185, 307, 2570, 281, 51616], "temperature": 0.0, "avg_logprob": -0.08843123471295392, "compression_ratio": 1.697508896797153, "no_speech_prob": 0.000666330917738378}, {"id": 37, "seek": 21992, "start": 219.92, "end": 225.44, "text": " be important for the construction of this cognitive map. So, for instance, rodents when", "tokens": [50364, 312, 1021, 337, 264, 6435, 295, 341, 15605, 4471, 13, 407, 11, 337, 5197, 11, 8685, 791, 562, 50640], "temperature": 0.0, "avg_logprob": -0.13043018687855112, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0008827297715470195}, {"id": 38, "seek": 21992, "start": 225.44, "end": 231.35999999999999, "text": " they explore a 2D spatial environment, it has been found that hippocampus has these place cells", "tokens": [50640, 436, 6839, 257, 568, 35, 23598, 2823, 11, 309, 575, 668, 1352, 300, 27745, 905, 1215, 301, 575, 613, 1081, 5438, 50936], "temperature": 0.0, "avg_logprob": -0.13043018687855112, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0008827297715470195}, {"id": 39, "seek": 21992, "start": 231.35999999999999, "end": 237.11999999999998, "text": " which code locations in the 2D environment, and there are good cells in the entorhinal cortex", "tokens": [50936, 597, 3089, 9253, 294, 264, 568, 35, 2823, 11, 293, 456, 366, 665, 5438, 294, 264, 948, 284, 71, 2071, 33312, 51224], "temperature": 0.0, "avg_logprob": -0.13043018687855112, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0008827297715470195}, {"id": 40, "seek": 21992, "start": 237.11999999999998, "end": 242.56, "text": " which show this hexagonally symmetric firing fields, which are periodic, and which have been", "tokens": [51224, 597, 855, 341, 23291, 6709, 379, 32330, 16045, 7909, 11, 597, 366, 27790, 11, 293, 597, 362, 668, 51496], "temperature": 0.0, "avg_logprob": -0.13043018687855112, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0008827297715470195}, {"id": 41, "seek": 21992, "start": 242.56, "end": 247.76, "text": " thought to encode location, but also cell motion-based euclidean displacement. And these", "tokens": [51496, 1194, 281, 2058, 1429, 4914, 11, 457, 611, 2815, 5394, 12, 6032, 308, 1311, 31264, 282, 21899, 13, 400, 613, 51756], "temperature": 0.0, "avg_logprob": -0.13043018687855112, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0008827297715470195}, {"id": 42, "seek": 24776, "start": 247.76, "end": 252.16, "text": " codings are important for the construction of cognitive map because they provide an allocentric", "tokens": [50364, 17656, 1109, 366, 1021, 337, 264, 6435, 295, 15605, 4471, 570, 436, 2893, 364, 439, 905, 32939, 50584], "temperature": 0.0, "avg_logprob": -0.06326130520213734, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0002377717028139159}, {"id": 43, "seek": 24776, "start": 252.16, "end": 258.15999999999997, "text": " representation. So, given that the hippocampal entorhinal system encodes spatial variables,", "tokens": [50584, 10290, 13, 407, 11, 2212, 300, 264, 27745, 905, 1215, 304, 948, 284, 71, 2071, 1185, 2058, 4789, 23598, 9102, 11, 50884], "temperature": 0.0, "avg_logprob": -0.06326130520213734, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0002377717028139159}, {"id": 44, "seek": 24776, "start": 258.15999999999997, "end": 263.12, "text": " the next question is, can this also be used to represent other continuous task variables", "tokens": [50884, 264, 958, 1168, 307, 11, 393, 341, 611, 312, 1143, 281, 2906, 661, 10957, 5633, 9102, 51132], "temperature": 0.0, "avg_logprob": -0.06326130520213734, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0002377717028139159}, {"id": 45, "seek": 24776, "start": 263.12, "end": 269.44, "text": " other than space? And the answer to this question is yes, and I'll give one example. So, here in", "tokens": [51132, 661, 813, 1901, 30, 400, 264, 1867, 281, 341, 1168, 307, 2086, 11, 293, 286, 603, 976, 472, 1365, 13, 407, 11, 510, 294, 51448], "temperature": 0.0, "avg_logprob": -0.06326130520213734, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0002377717028139159}, {"id": 46, "seek": 24776, "start": 269.44, "end": 273.59999999999997, "text": " this experiment has been found that cells in hippocampus and entorhinal cortex respond to", "tokens": [51448, 341, 5120, 575, 668, 1352, 300, 5438, 294, 27745, 905, 1215, 301, 293, 948, 284, 71, 2071, 33312, 4196, 281, 51656], "temperature": 0.0, "avg_logprob": -0.06326130520213734, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0002377717028139159}, {"id": 47, "seek": 27360, "start": 273.6, "end": 279.12, "text": " task-relevant variables like sound frequency. So, in this task, rodents pull this lever,", "tokens": [50364, 5633, 12, 265, 25638, 9102, 411, 1626, 7893, 13, 407, 11, 294, 341, 5633, 11, 8685, 791, 2235, 341, 12451, 11, 50640], "temperature": 0.0, "avg_logprob": -0.07331273266088183, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0008829479920677841}, {"id": 48, "seek": 27360, "start": 279.12, "end": 283.52000000000004, "text": " and as they pull the lever, the frequency of the sound coming from this sound source", "tokens": [50640, 293, 382, 436, 2235, 264, 12451, 11, 264, 7893, 295, 264, 1626, 1348, 490, 341, 1626, 4009, 50860], "temperature": 0.0, "avg_logprob": -0.07331273266088183, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0008829479920677841}, {"id": 49, "seek": 27360, "start": 283.52000000000004, "end": 287.36, "text": " actually keeps increasing, and they have to release the lever when this frequency is in", "tokens": [50860, 767, 5965, 5662, 11, 293, 436, 362, 281, 4374, 264, 12451, 562, 341, 7893, 307, 294, 51052], "temperature": 0.0, "avg_logprob": -0.07331273266088183, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0008829479920677841}, {"id": 50, "seek": 27360, "start": 287.36, "end": 292.56, "text": " this target zone. And what is found is that in hippocampus and entorhinal cortex, there are cells", "tokens": [51052, 341, 3779, 6668, 13, 400, 437, 307, 1352, 307, 300, 294, 27745, 905, 1215, 301, 293, 948, 284, 71, 2071, 33312, 11, 456, 366, 5438, 51312], "temperature": 0.0, "avg_logprob": -0.07331273266088183, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0008829479920677841}, {"id": 51, "seek": 27360, "start": 292.56, "end": 299.68, "text": " that fire for specific frequencies during the sound modulation task. So, that shows that", "tokens": [51312, 300, 2610, 337, 2685, 20250, 1830, 264, 1626, 42288, 5633, 13, 407, 11, 300, 3110, 300, 51668], "temperature": 0.0, "avg_logprob": -0.07331273266088183, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.0008829479920677841}, {"id": 52, "seek": 29968, "start": 300.64, "end": 305.68, "text": " entorhinal cortex is a system that is able to represent continuous task variables,", "tokens": [50412, 948, 284, 71, 2071, 33312, 307, 257, 1185, 300, 307, 1075, 281, 2906, 10957, 5633, 9102, 11, 50664], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 53, "seek": 29968, "start": 305.68, "end": 310.48, "text": " even other than space. And so, our next question is, could this system also help us organize", "tokens": [50664, 754, 661, 813, 1901, 13, 400, 370, 11, 527, 958, 1168, 307, 11, 727, 341, 1185, 611, 854, 505, 13859, 50904], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 54, "seek": 29968, "start": 310.48, "end": 315.44, "text": " and navigate discrete knowledge? And family trees are one example of discrete knowledge.", "tokens": [50904, 293, 12350, 27706, 3601, 30, 400, 1605, 5852, 366, 472, 1365, 295, 27706, 3601, 13, 51152], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 55, "seek": 29968, "start": 315.44, "end": 320.08, "text": " So, in family trees, there's an underlying hierarchical structure that we learn. And once we", "tokens": [51152, 407, 11, 294, 1605, 5852, 11, 456, 311, 364, 14217, 35250, 804, 3877, 300, 321, 1466, 13, 400, 1564, 321, 51384], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 56, "seek": 29968, "start": 320.08, "end": 323.68, "text": " know that structure, then we can apply that structure to my family tree or to your family", "tokens": [51384, 458, 300, 3877, 11, 550, 321, 393, 3079, 300, 3877, 281, 452, 1605, 4230, 420, 281, 428, 1605, 51564], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 57, "seek": 29968, "start": 323.68, "end": 327.84000000000003, "text": " tree or anyone's family tree, and we can generalize that knowledge. So, we can make inferences like", "tokens": [51564, 4230, 420, 2878, 311, 1605, 4230, 11, 293, 321, 393, 2674, 1125, 300, 3601, 13, 407, 11, 321, 393, 652, 13596, 2667, 411, 51772], "temperature": 0.0, "avg_logprob": -0.07025345479409525, "compression_ratio": 1.8862068965517242, "no_speech_prob": 0.0007552167517133057}, {"id": 58, "seek": 32784, "start": 327.84, "end": 333.59999999999997, "text": " this, because Olivia is Emily's sister and Sam is Emily's son, Sam must be Olivia's nephew.", "tokens": [50364, 341, 11, 570, 26023, 307, 15034, 311, 4892, 293, 4832, 307, 15034, 311, 1872, 11, 4832, 1633, 312, 26023, 311, 30799, 13, 50652], "temperature": 0.0, "avg_logprob": -0.06467029640266488, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0009251181618310511}, {"id": 59, "seek": 32784, "start": 333.59999999999997, "end": 338.08, "text": " So, even though we haven't directly observed this relationship, but just by mere observation of", "tokens": [50652, 407, 11, 754, 1673, 321, 2378, 380, 3838, 13095, 341, 2480, 11, 457, 445, 538, 8401, 14816, 295, 50876], "temperature": 0.0, "avg_logprob": -0.06467029640266488, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0009251181618310511}, {"id": 60, "seek": 32784, "start": 338.08, "end": 342.08, "text": " these two relationships, we are able to infer this relationship, because we know the underlying", "tokens": [50876, 613, 732, 6159, 11, 321, 366, 1075, 281, 13596, 341, 2480, 11, 570, 321, 458, 264, 14217, 51076], "temperature": 0.0, "avg_logprob": -0.06467029640266488, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0009251181618310511}, {"id": 61, "seek": 32784, "start": 342.08, "end": 347.44, "text": " hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system", "tokens": [51076, 35250, 804, 3877, 295, 341, 1605, 4230, 13, 407, 11, 309, 307, 1944, 300, 27745, 905, 1215, 304, 948, 284, 71, 2071, 1185, 51344], "temperature": 0.0, "avg_logprob": -0.06467029640266488, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0009251181618310511}, {"id": 62, "seek": 32784, "start": 347.44, "end": 352.4, "text": " might allow organization of this kind of a discrete knowledge, but that's only possible if it allows", "tokens": [51344, 1062, 2089, 4475, 295, 341, 733, 295, 257, 27706, 3601, 11, 457, 300, 311, 787, 1944, 498, 309, 4045, 51592], "temperature": 0.0, "avg_logprob": -0.06467029640266488, "compression_ratio": 1.7591240875912408, "no_speech_prob": 0.0009251181618310511}, {"id": 63, "seek": 35240, "start": 352.4, "end": 358.0, "text": " encoding non-Equidian relationships. With that, I'll go back to the spatial domain,", "tokens": [50364, 43430, 2107, 12, 36, 358, 34681, 6159, 13, 2022, 300, 11, 286, 603, 352, 646, 281, 264, 23598, 9274, 11, 50644], "temperature": 0.0, "avg_logprob": -0.101084107334174, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.0012838523834943771}, {"id": 64, "seek": 35240, "start": 358.0, "end": 363.35999999999996, "text": " and I'll talk about how spatial knowledge might be organized. And when I talk about organization,", "tokens": [50644, 293, 286, 603, 751, 466, 577, 23598, 3601, 1062, 312, 9983, 13, 400, 562, 286, 751, 466, 4475, 11, 50912], "temperature": 0.0, "avg_logprob": -0.101084107334174, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.0012838523834943771}, {"id": 65, "seek": 35240, "start": 363.35999999999996, "end": 368.47999999999996, "text": " I'll point to the fact that even in a continuous domain like space, it might be possible that", "tokens": [50912, 286, 603, 935, 281, 264, 1186, 300, 754, 294, 257, 10957, 9274, 411, 1901, 11, 309, 1062, 312, 1944, 300, 51168], "temperature": 0.0, "avg_logprob": -0.101084107334174, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.0012838523834943771}, {"id": 66, "seek": 35240, "start": 368.47999999999996, "end": 373.12, "text": " we have both Euclidean and non-Euclidean components to represent space itself,", "tokens": [51168, 321, 362, 1293, 462, 1311, 31264, 282, 293, 2107, 12, 36, 1311, 31264, 282, 6677, 281, 2906, 1901, 2564, 11, 51400], "temperature": 0.0, "avg_logprob": -0.101084107334174, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.0012838523834943771}, {"id": 67, "seek": 35240, "start": 373.12, "end": 377.52, "text": " and thus making the system generalizable to even discrete domains.", "tokens": [51400, 293, 8807, 1455, 264, 1185, 2674, 22395, 281, 754, 27706, 25514, 13, 51620], "temperature": 0.0, "avg_logprob": -0.101084107334174, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.0012838523834943771}, {"id": 68, "seek": 37752, "start": 378.24, "end": 386.4, "text": " So, here's an experiment conducted by Bill Warren, where they show that people actually do not learn", "tokens": [50400, 407, 11, 510, 311, 364, 5120, 13809, 538, 5477, 20538, 11, 689, 436, 855, 300, 561, 767, 360, 406, 1466, 50808], "temperature": 0.0, "avg_logprob": -0.17944279232540647, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0006360470433719456}, {"id": 69, "seek": 37752, "start": 386.4, "end": 392.08, "text": " a global Euclidean map of space. So, in this experiment, they constructed virtual environments,", "tokens": [50808, 257, 4338, 462, 1311, 31264, 282, 4471, 295, 1901, 13, 407, 11, 294, 341, 5120, 11, 436, 17083, 6374, 12388, 11, 51092], "temperature": 0.0, "avg_logprob": -0.17944279232540647, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0006360470433719456}, {"id": 70, "seek": 37752, "start": 392.08, "end": 398.64, "text": " and they basically, the task was for people, for human subjects. So, this was a human behavioral", "tokens": [51092, 293, 436, 1936, 11, 264, 5633, 390, 337, 561, 11, 337, 1952, 13066, 13, 407, 11, 341, 390, 257, 1952, 19124, 51420], "temperature": 0.0, "avg_logprob": -0.17944279232540647, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0006360470433719456}, {"id": 71, "seek": 37752, "start": 398.64, "end": 403.2, "text": " experiment, and human subjects were asked to go to different landmarks in this spatial environment,", "tokens": [51420, 5120, 11, 293, 1952, 13066, 645, 2351, 281, 352, 281, 819, 26962, 82, 294, 341, 23598, 2823, 11, 51648], "temperature": 0.0, "avg_logprob": -0.17944279232540647, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0006360470433719456}, {"id": 72, "seek": 37752, "start": 403.2, "end": 407.03999999999996, "text": " and they also built counterparts of the spatial environment, which were non-Euclidean,", "tokens": [51648, 293, 436, 611, 3094, 33287, 295, 264, 23598, 2823, 11, 597, 645, 2107, 12, 36, 1311, 31264, 282, 11, 51840], "temperature": 0.0, "avg_logprob": -0.17944279232540647, "compression_ratio": 1.8250950570342206, "no_speech_prob": 0.0006360470433719456}, {"id": 73, "seek": 40704, "start": 407.20000000000005, "end": 412.40000000000003, "text": " in these environments. So, when you enter one part of the wormhole, you seamlessly exit from the", "tokens": [50372, 294, 613, 12388, 13, 407, 11, 562, 291, 3242, 472, 644, 295, 264, 23835, 14094, 11, 291, 38083, 11043, 490, 264, 50632], "temperature": 0.0, "avg_logprob": -0.08913098492668671, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0010000047041103244}, {"id": 74, "seek": 40704, "start": 412.40000000000003, "end": 417.52000000000004, "text": " other end of the wormhole, and subjects were not aware of the existence of these wormholes.", "tokens": [50632, 661, 917, 295, 264, 23835, 14094, 11, 293, 13066, 645, 406, 3650, 295, 264, 9123, 295, 613, 23835, 37894, 13, 50888], "temperature": 0.0, "avg_logprob": -0.08913098492668671, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0010000047041103244}, {"id": 75, "seek": 40704, "start": 418.08000000000004, "end": 423.52000000000004, "text": " And basically, what they found through various manipulations of the experiment was that people", "tokens": [50916, 400, 1936, 11, 437, 436, 1352, 807, 3683, 9258, 4136, 295, 264, 5120, 390, 300, 561, 51188], "temperature": 0.0, "avg_logprob": -0.08913098492668671, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0010000047041103244}, {"id": 76, "seek": 40704, "start": 423.52000000000004, "end": 428.96000000000004, "text": " do not actually learn a global Euclidean map, but rather a labeled graph, like representation,", "tokens": [51188, 360, 406, 767, 1466, 257, 4338, 462, 1311, 31264, 282, 4471, 11, 457, 2831, 257, 21335, 4295, 11, 411, 10290, 11, 51460], "temperature": 0.0, "avg_logprob": -0.08913098492668671, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0010000047041103244}, {"id": 77, "seek": 40704, "start": 428.96000000000004, "end": 434.08000000000004, "text": " where the nodes represent places, the edges represent approximate distances between these", "tokens": [51460, 689, 264, 13891, 2906, 3190, 11, 264, 8819, 2906, 30874, 22182, 1296, 613, 51716], "temperature": 0.0, "avg_logprob": -0.08913098492668671, "compression_ratio": 1.7462686567164178, "no_speech_prob": 0.0010000047041103244}, {"id": 78, "seek": 43408, "start": 434.08, "end": 439.91999999999996, "text": " places, and the node labels, which are angles, represent approximate angles between these places.", "tokens": [50364, 3190, 11, 293, 264, 9984, 16949, 11, 597, 366, 14708, 11, 2906, 30874, 14708, 1296, 613, 3190, 13, 50656], "temperature": 0.0, "avg_logprob": -0.06516934886123195, "compression_ratio": 1.8353909465020577, "no_speech_prob": 0.00030530794174410403}, {"id": 79, "seek": 43408, "start": 440.8, "end": 444.47999999999996, "text": " And so, we built on this representation, and we proposed that people might be", "tokens": [50700, 400, 370, 11, 321, 3094, 322, 341, 10290, 11, 293, 321, 10348, 300, 561, 1062, 312, 50884], "temperature": 0.0, "avg_logprob": -0.06516934886123195, "compression_ratio": 1.8353909465020577, "no_speech_prob": 0.00030530794174410403}, {"id": 80, "seek": 43408, "start": 444.47999999999996, "end": 450.79999999999995, "text": " representing topometric maps, which are locally metric or Euclidean, but globally topological.", "tokens": [50884, 13460, 1192, 29470, 11317, 11, 597, 366, 16143, 20678, 420, 462, 1311, 31264, 282, 11, 457, 18958, 1192, 4383, 13, 51200], "temperature": 0.0, "avg_logprob": -0.06516934886123195, "compression_ratio": 1.8353909465020577, "no_speech_prob": 0.00030530794174410403}, {"id": 81, "seek": 43408, "start": 450.79999999999995, "end": 455.03999999999996, "text": " And the main advantage of this kind of representation is that it allows us to combine", "tokens": [51200, 400, 264, 2135, 5002, 295, 341, 733, 295, 10290, 307, 300, 309, 4045, 505, 281, 10432, 51412], "temperature": 0.0, "avg_logprob": -0.06516934886123195, "compression_ratio": 1.8353909465020577, "no_speech_prob": 0.00030530794174410403}, {"id": 82, "seek": 43408, "start": 455.03999999999996, "end": 460.56, "text": " accurate local maps into a global map, which might be inconsistent, but it still provides", "tokens": [51412, 8559, 2654, 11317, 666, 257, 4338, 4471, 11, 597, 1062, 312, 36891, 11, 457, 309, 920, 6417, 51688], "temperature": 0.0, "avg_logprob": -0.06516934886123195, "compression_ratio": 1.8353909465020577, "no_speech_prob": 0.00030530794174410403}, {"id": 83, "seek": 46056, "start": 460.56, "end": 465.6, "text": " enough sufficient information for navigation. So, the next question is,", "tokens": [50364, 1547, 11563, 1589, 337, 17346, 13, 407, 11, 264, 958, 1168, 307, 11, 50616], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 84, "seek": 46056, "start": 466.4, "end": 471.12, "text": " can this kind of a topometric map be implemented in the brain? And if so, how?", "tokens": [50656, 393, 341, 733, 295, 257, 1192, 29470, 4471, 312, 12270, 294, 264, 3567, 30, 400, 498, 370, 11, 577, 30, 50892], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 85, "seek": 46056, "start": 471.12, "end": 475.12, "text": " And next, I'm providing a theoretical framework for how it might be possible to represent such", "tokens": [50892, 400, 958, 11, 286, 478, 6530, 257, 20864, 8388, 337, 577, 309, 1062, 312, 1944, 281, 2906, 1270, 51092], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 86, "seek": 46056, "start": 475.12, "end": 480.48, "text": " topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex.", "tokens": [51092, 1192, 29470, 11317, 1228, 264, 1081, 5438, 293, 10748, 5438, 1352, 294, 27745, 905, 1215, 304, 917, 284, 71, 2071, 33312, 13, 51360], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 87, "seek": 46056, "start": 480.48, "end": 484.32, "text": " So, this is a topometric representation of space. You can see that these are metric maps", "tokens": [51360, 407, 11, 341, 307, 257, 1192, 29470, 10290, 295, 1901, 13, 509, 393, 536, 300, 613, 366, 20678, 11317, 51552], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 88, "seek": 46056, "start": 484.32, "end": 489.36, "text": " connected topologically by these connections. And here, on this side, I'm showing a grid coding", "tokens": [51552, 4582, 1192, 17157, 538, 613, 9271, 13, 400, 510, 11, 322, 341, 1252, 11, 286, 478, 4099, 257, 10748, 17720, 51804], "temperature": 0.0, "avg_logprob": -0.10975301076495458, "compression_ratio": 1.760942760942761, "no_speech_prob": 0.00023775306181050837}, {"id": 89, "seek": 48936, "start": 489.36, "end": 494.72, "text": " space, which is dense coding space with large capacity. And here, I'm showing place coding", "tokens": [50364, 1901, 11, 597, 307, 18011, 17720, 1901, 365, 2416, 6042, 13, 400, 510, 11, 286, 478, 4099, 1081, 17720, 50632], "temperature": 0.0, "avg_logprob": -0.08784821828206381, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0006877414416521788}, {"id": 90, "seek": 48936, "start": 494.72, "end": 500.16, "text": " space, which also has large capacity, but it spars, so it can receive sensory inputs and form", "tokens": [50632, 1901, 11, 597, 611, 575, 2416, 6042, 11, 457, 309, 637, 685, 11, 370, 309, 393, 4774, 27233, 15743, 293, 1254, 50904], "temperature": 0.0, "avg_logprob": -0.08784821828206381, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0006877414416521788}, {"id": 91, "seek": 48936, "start": 500.16, "end": 506.16, "text": " conjunctive representations. And so, in this schematic, small changes, sorry, large changes", "tokens": [50904, 18244, 20221, 33358, 13, 400, 370, 11, 294, 341, 44739, 11, 1359, 2962, 11, 2597, 11, 2416, 2962, 51204], "temperature": 0.0, "avg_logprob": -0.08784821828206381, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0006877414416521788}, {"id": 92, "seek": 48936, "start": 506.16, "end": 513.12, "text": " in contextual input from our spatial domain was remapping in the place cells, which in turn", "tokens": [51204, 294, 35526, 4846, 490, 527, 23598, 9274, 390, 890, 10534, 294, 264, 1081, 5438, 11, 597, 294, 1261, 51552], "temperature": 0.0, "avg_logprob": -0.08784821828206381, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0006877414416521788}, {"id": 93, "seek": 48936, "start": 513.12, "end": 517.28, "text": " trigger remapping in the grid cells, enabling the formation of these local metric maps that", "tokens": [51552, 7875, 890, 10534, 294, 264, 10748, 5438, 11, 23148, 264, 11723, 295, 613, 2654, 20678, 11317, 300, 51760], "temperature": 0.0, "avg_logprob": -0.08784821828206381, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.0006877414416521788}, {"id": 94, "seek": 51728, "start": 517.28, "end": 524.16, "text": " can be reused. And so, on this schematic, we can really take any subpart of this schematic and", "tokens": [50364, 393, 312, 319, 4717, 13, 400, 370, 11, 322, 341, 44739, 11, 321, 393, 534, 747, 604, 1422, 6971, 295, 341, 44739, 293, 50708], "temperature": 0.0, "avg_logprob": -0.09408824340156886, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.0002611200325191021}, {"id": 95, "seek": 51728, "start": 524.16, "end": 530.4, "text": " call it a sub map. And this allows us to compose sub maps, because once we have learned a particular", "tokens": [50708, 818, 309, 257, 1422, 4471, 13, 400, 341, 4045, 505, 281, 35925, 1422, 11317, 11, 570, 1564, 321, 362, 3264, 257, 1729, 51020], "temperature": 0.0, "avg_logprob": -0.09408824340156886, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.0002611200325191021}, {"id": 96, "seek": 51728, "start": 530.4, "end": 535.68, "text": " sub map, then we can actually encounter the sub map in a completely novel situation and still be able", "tokens": [51020, 1422, 4471, 11, 550, 321, 393, 767, 8593, 264, 1422, 4471, 294, 257, 2584, 7613, 2590, 293, 920, 312, 1075, 51284], "temperature": 0.0, "avg_logprob": -0.09408824340156886, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.0002611200325191021}, {"id": 97, "seek": 51728, "start": 535.68, "end": 545.1999999999999, "text": " to spatially navigate and reason through it. Another thing that it allows us is learning", "tokens": [51284, 281, 15000, 2270, 12350, 293, 1778, 807, 309, 13, 3996, 551, 300, 309, 4045, 505, 307, 2539, 51760], "temperature": 0.0, "avg_logprob": -0.09408824340156886, "compression_ratio": 1.7625570776255708, "no_speech_prob": 0.0002611200325191021}, {"id": 98, "seek": 54520, "start": 545.2, "end": 550.1600000000001, "text": " non-ecredient relationships, because place cells actually encode topological relationships, enabling", "tokens": [50364, 2107, 12, 3045, 986, 1196, 6159, 11, 570, 1081, 5438, 767, 2058, 1429, 1192, 4383, 6159, 11, 23148, 50612], "temperature": 0.0, "avg_logprob": -0.11875361148442064, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.0009396409732289612}, {"id": 99, "seek": 54520, "start": 550.88, "end": 556.24, "text": " the representation of non-ecredient relations. So, now I'm going to describe the three parts", "tokens": [50648, 264, 10290, 295, 2107, 12, 3045, 986, 1196, 2299, 13, 407, 11, 586, 286, 478, 516, 281, 6786, 264, 1045, 3166, 50916], "temperature": 0.0, "avg_logprob": -0.11875361148442064, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.0009396409732289612}, {"id": 100, "seek": 54520, "start": 556.24, "end": 561.76, "text": " in which I've divided the rest of the talk. So, in the first part, I'm probing whether", "tokens": [50916, 294, 597, 286, 600, 6666, 264, 1472, 295, 264, 751, 13, 407, 11, 294, 264, 700, 644, 11, 286, 478, 1239, 278, 1968, 51192], "temperature": 0.0, "avg_logprob": -0.11875361148442064, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.0009396409732289612}, {"id": 101, "seek": 54520, "start": 561.76, "end": 568.08, "text": " sub maps drive past learning in complex spaces using human behavioral experiments. In the second part,", "tokens": [51192, 1422, 11317, 3332, 1791, 2539, 294, 3997, 7673, 1228, 1952, 19124, 12050, 13, 682, 264, 1150, 644, 11, 51508], "temperature": 0.0, "avg_logprob": -0.11875361148442064, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.0009396409732289612}, {"id": 102, "seek": 54520, "start": 568.6400000000001, "end": 573.6, "text": " I will talk about determining which principles might guide fragmentation of a space into", "tokens": [51536, 286, 486, 751, 466, 23751, 597, 9156, 1062, 5934, 9241, 19631, 295, 257, 1901, 666, 51784], "temperature": 0.0, "avg_logprob": -0.11875361148442064, "compression_ratio": 1.7481481481481482, "no_speech_prob": 0.0009396409732289612}, {"id": 103, "seek": 57360, "start": 573.6, "end": 579.2, "text": " sub maps. And finally, in the third part, I'll propose a framework for building a neural model", "tokens": [50364, 1422, 11317, 13, 400, 2721, 11, 294, 264, 2636, 644, 11, 286, 603, 17421, 257, 8388, 337, 2390, 257, 18161, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.13715034876114282, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.00027364277048036456}, {"id": 104, "seek": 57360, "start": 579.2, "end": 586.8000000000001, "text": " of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast learning", "tokens": [50644, 295, 4471, 9241, 19631, 13, 407, 11, 294, 264, 700, 644, 11, 286, 478, 1239, 278, 1968, 1422, 11317, 3332, 2370, 2539, 51024], "temperature": 0.0, "avg_logprob": -0.13715034876114282, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.00027364277048036456}, {"id": 105, "seek": 57360, "start": 586.8000000000001, "end": 591.2, "text": " in complex spaces, specifically in humans. And this work is in collaboration with Marta Krivind,", "tokens": [51024, 294, 3997, 7673, 11, 4682, 294, 6255, 13, 400, 341, 589, 307, 294, 9363, 365, 5807, 64, 591, 29994, 471, 11, 51244], "temperature": 0.0, "avg_logprob": -0.13715034876114282, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.00027364277048036456}, {"id": 106, "seek": 57360, "start": 591.2, "end": 597.0400000000001, "text": " who's a postdoc in Tenenbaum Lab, and Kevin, who's an undergrad in the CS department.", "tokens": [51244, 567, 311, 257, 2183, 39966, 294, 9380, 268, 46641, 10137, 11, 293, 9954, 11, 567, 311, 364, 14295, 294, 264, 9460, 5882, 13, 51536], "temperature": 0.0, "avg_logprob": -0.13715034876114282, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.00027364277048036456}, {"id": 107, "seek": 57360, "start": 597.9200000000001, "end": 603.12, "text": " So, here I hypothesize that humans learn adaptable and compositional sub maps of spatial", "tokens": [51580, 407, 11, 510, 286, 14276, 1125, 300, 6255, 1466, 6231, 712, 293, 10199, 2628, 1422, 11317, 295, 23598, 51840], "temperature": 0.0, "avg_logprob": -0.13715034876114282, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.00027364277048036456}, {"id": 108, "seek": 60312, "start": 603.12, "end": 608.0, "text": " structures. So, for instance, this is a baseline environment, and this is a top-down view showing", "tokens": [50364, 9227, 13, 407, 11, 337, 5197, 11, 341, 307, 257, 20518, 2823, 11, 293, 341, 307, 257, 1192, 12, 5093, 1910, 4099, 50608], "temperature": 0.0, "avg_logprob": -0.10090234738971116, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0021132342517375946}, {"id": 109, "seek": 60312, "start": 608.0, "end": 613.6, "text": " an environment with four rooms. And here I've shown certain transformations of this environment", "tokens": [50608, 364, 2823, 365, 1451, 9396, 13, 400, 510, 286, 600, 4898, 1629, 34852, 295, 341, 2823, 50888], "temperature": 0.0, "avg_logprob": -0.10090234738971116, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0021132342517375946}, {"id": 110, "seek": 60312, "start": 613.6, "end": 619.44, "text": " generated by small generator programs. And you can see this is the same environment rotated,", "tokens": [50888, 10833, 538, 1359, 19265, 4268, 13, 400, 291, 393, 536, 341, 307, 264, 912, 2823, 42146, 11, 51180], "temperature": 0.0, "avg_logprob": -0.10090234738971116, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0021132342517375946}, {"id": 111, "seek": 60312, "start": 619.44, "end": 624.24, "text": " because now you're entering from this point, so it might appear rotated to you. And here's a reflection", "tokens": [51180, 570, 586, 291, 434, 11104, 490, 341, 935, 11, 370, 309, 1062, 4204, 42146, 281, 291, 13, 400, 510, 311, 257, 12914, 51420], "temperature": 0.0, "avg_logprob": -0.10090234738971116, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0021132342517375946}, {"id": 112, "seek": 60312, "start": 624.24, "end": 630.72, "text": " of the same environment. Here is a transformation where we've removed the wall and added a shortcut,", "tokens": [51420, 295, 264, 912, 2823, 13, 1692, 307, 257, 9887, 689, 321, 600, 7261, 264, 2929, 293, 3869, 257, 24822, 11, 51744], "temperature": 0.0, "avg_logprob": -0.10090234738971116, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0021132342517375946}, {"id": 113, "seek": 63072, "start": 630.72, "end": 635.2, "text": " and here we've added a wall. And this is just the repetition of the same environments.", "tokens": [50364, 293, 510, 321, 600, 3869, 257, 2929, 13, 400, 341, 307, 445, 264, 30432, 295, 264, 912, 12388, 13, 50588], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 114, "seek": 63072, "start": 635.2, "end": 639.28, "text": " And there's another transformation which is scaling, where you can imagine this environment", "tokens": [50588, 400, 456, 311, 1071, 9887, 597, 307, 21589, 11, 689, 291, 393, 3811, 341, 2823, 50792], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 115, "seek": 63072, "start": 639.28, "end": 644.48, "text": " scaled up to a bigger size, but having the same geometrical layout. And what I'm suggesting is", "tokens": [50792, 36039, 493, 281, 257, 3801, 2744, 11, 457, 1419, 264, 912, 12956, 15888, 13333, 13, 400, 437, 286, 478, 18094, 307, 51052], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 116, "seek": 63072, "start": 644.48, "end": 649.12, "text": " that once people have learned the map of the space line environment, their representations", "tokens": [51052, 300, 1564, 561, 362, 3264, 264, 4471, 295, 264, 1901, 1622, 2823, 11, 641, 33358, 51284], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 117, "seek": 63072, "start": 649.12, "end": 654.1600000000001, "text": " might be adaptable to some or all of these transformations. And furthermore, people might", "tokens": [51284, 1062, 312, 6231, 712, 281, 512, 420, 439, 295, 613, 34852, 13, 400, 3052, 3138, 11, 561, 1062, 51536], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 118, "seek": 63072, "start": 654.1600000000001, "end": 659.36, "text": " represent richer spaces by combining these maps and their transformations, leading to quick", "tokens": [51536, 2906, 29021, 7673, 538, 21928, 613, 11317, 293, 641, 34852, 11, 5775, 281, 1702, 51796], "temperature": 0.0, "avg_logprob": -0.0869663090541445, "compression_ratio": 1.863481228668942, "no_speech_prob": 0.0003101097827311605}, {"id": 119, "seek": 65936, "start": 659.36, "end": 665.36, "text": " generalization and learning. So, this can be modeled using Bayesian program learning framework,", "tokens": [50364, 2674, 2144, 293, 2539, 13, 407, 11, 341, 393, 312, 37140, 1228, 7840, 42434, 1461, 2539, 8388, 11, 50664], "temperature": 0.0, "avg_logprob": -0.12390579950241816, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0002694150898605585}, {"id": 120, "seek": 65936, "start": 665.36, "end": 669.52, "text": " where concepts are represented as simple programs, and rich concepts can be built", "tokens": [50664, 689, 10392, 366, 10379, 382, 2199, 4268, 11, 293, 4593, 10392, 393, 312, 3094, 50872], "temperature": 0.0, "avg_logprob": -0.12390579950241816, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0002694150898605585}, {"id": 121, "seek": 65936, "start": 669.52, "end": 675.84, "text": " compositionally from them using a higher-level generative model. So, there is neural evidence", "tokens": [50872, 12686, 379, 490, 552, 1228, 257, 2946, 12, 12418, 1337, 1166, 2316, 13, 407, 11, 456, 307, 18161, 4467, 51188], "temperature": 0.0, "avg_logprob": -0.12390579950241816, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0002694150898605585}, {"id": 122, "seek": 65936, "start": 675.84, "end": 681.92, "text": " for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab, and they show", "tokens": [51188, 337, 341, 17291, 13, 407, 11, 294, 341, 11, 341, 307, 364, 5120, 538, 264, 11385, 514, 10449, 10137, 11, 293, 436, 855, 51492], "temperature": 0.0, "avg_logprob": -0.12390579950241816, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0002694150898605585}, {"id": 123, "seek": 65936, "start": 681.92, "end": 687.6, "text": " that when a rodent goes through this environment in four labs, there are cells which fire specifically", "tokens": [51492, 300, 562, 257, 8685, 317, 1709, 807, 341, 2823, 294, 1451, 20339, 11, 456, 366, 5438, 597, 2610, 4682, 51776], "temperature": 0.0, "avg_logprob": -0.12390579950241816, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.0002694150898605585}, {"id": 124, "seek": 68760, "start": 687.6, "end": 692.5600000000001, "text": " for particular places in this environment, but there are also cells which encode, which are", "tokens": [50364, 337, 1729, 3190, 294, 341, 2823, 11, 457, 456, 366, 611, 5438, 597, 2058, 1429, 11, 597, 366, 50612], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 125, "seek": 68760, "start": 692.5600000000001, "end": 697.2, "text": " event-specific and encode specific labs. So, for instance, there are cells which show increased", "tokens": [50612, 2280, 12, 29258, 293, 2058, 1429, 2685, 20339, 13, 407, 11, 337, 5197, 11, 456, 366, 5438, 597, 855, 6505, 50844], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 126, "seek": 68760, "start": 697.2, "end": 701.44, "text": " fighting rates as you go from lab one to lab four, and there are also cells which only fire", "tokens": [50844, 5237, 6846, 382, 291, 352, 490, 2715, 472, 281, 2715, 1451, 11, 293, 456, 366, 611, 5438, 597, 787, 2610, 51056], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 127, "seek": 68760, "start": 701.44, "end": 706.24, "text": " specifically on lab one or on lab two and so on. And so, what I'm suggesting is that when we have", "tokens": [51056, 4682, 322, 2715, 472, 420, 322, 2715, 732, 293, 370, 322, 13, 400, 370, 11, 437, 286, 478, 18094, 307, 300, 562, 321, 362, 51296], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 128, "seek": 68760, "start": 706.24, "end": 711.44, "text": " repetitions of the same environment, there might be cells that encode the basic map of this environment,", "tokens": [51296, 13645, 2451, 295, 264, 912, 2823, 11, 456, 1062, 312, 5438, 300, 2058, 1429, 264, 3875, 4471, 295, 341, 2823, 11, 51556], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 129, "seek": 68760, "start": 711.44, "end": 716.5600000000001, "text": " which, which is consistent across these occurrences, but there might be a second set of cells,", "tokens": [51556, 597, 11, 597, 307, 8398, 2108, 613, 5160, 38983, 11, 457, 456, 1062, 312, 257, 1150, 992, 295, 5438, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1006176593529917, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.00034054386196658015}, {"id": 130, "seek": 71656, "start": 716.56, "end": 721.1199999999999, "text": " which are event-specific and might encode which instance of this environment we are on.", "tokens": [50364, 597, 366, 2280, 12, 29258, 293, 1062, 2058, 1429, 597, 5197, 295, 341, 2823, 321, 366, 322, 13, 50592], "temperature": 0.0, "avg_logprob": -0.06883462575765756, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.00016601619427092373}, {"id": 131, "seek": 71656, "start": 722.16, "end": 729.1999999999999, "text": " Here's another example, where this is an example of scaling, where the rodent actually just explores", "tokens": [50644, 1692, 311, 1071, 1365, 11, 689, 341, 307, 364, 1365, 295, 21589, 11, 689, 264, 8685, 317, 767, 445, 45473, 50996], "temperature": 0.0, "avg_logprob": -0.06883462575765756, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.00016601619427092373}, {"id": 132, "seek": 71656, "start": 729.1999999999999, "end": 734.0, "text": " this circular environment, and this is the place field found in that circular environment,", "tokens": [50996, 341, 16476, 2823, 11, 293, 341, 307, 264, 1081, 2519, 1352, 294, 300, 16476, 2823, 11, 51236], "temperature": 0.0, "avg_logprob": -0.06883462575765756, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.00016601619427092373}, {"id": 133, "seek": 71656, "start": 734.0, "end": 738.2399999999999, "text": " and when the circular environment is scaled up to a bigger size, the place field also scales", "tokens": [51236, 293, 562, 264, 16476, 2823, 307, 36039, 493, 281, 257, 3801, 2744, 11, 264, 1081, 2519, 611, 17408, 51448], "temperature": 0.0, "avg_logprob": -0.06883462575765756, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.00016601619427092373}, {"id": 134, "seek": 71656, "start": 738.2399999999999, "end": 742.64, "text": " according to the size of the environment. So, this shows that the map which the rodent has", "tokens": [51448, 4650, 281, 264, 2744, 295, 264, 2823, 13, 407, 11, 341, 3110, 300, 264, 4471, 597, 264, 8685, 317, 575, 51668], "temperature": 0.0, "avg_logprob": -0.06883462575765756, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.00016601619427092373}, {"id": 135, "seek": 74264, "start": 742.64, "end": 747.1999999999999, "text": " learned of this environment is actually adaptable to this transformation of scaling to a bigger", "tokens": [50364, 3264, 295, 341, 2823, 307, 767, 6231, 712, 281, 341, 9887, 295, 21589, 281, 257, 3801, 50592], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 136, "seek": 74264, "start": 747.1999999999999, "end": 752.24, "text": " size, and map also scales proportionately with the size of the environment. And here's a third", "tokens": [50592, 2744, 11, 293, 4471, 611, 17408, 16068, 1592, 365, 264, 2744, 295, 264, 2823, 13, 400, 510, 311, 257, 2636, 50844], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 137, "seek": 74264, "start": 752.24, "end": 758.48, "text": " example, where rodents form different maps, place maps in these different environments,", "tokens": [50844, 1365, 11, 689, 8685, 791, 1254, 819, 11317, 11, 1081, 11317, 294, 613, 819, 12388, 11, 51156], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 138, "seek": 74264, "start": 758.48, "end": 762.08, "text": " and when these environments are composed by connecting them through a corridor,", "tokens": [51156, 293, 562, 613, 12388, 366, 18204, 538, 11015, 552, 807, 257, 25602, 11, 51336], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 139, "seek": 74264, "start": 762.08, "end": 766.56, "text": " rodents end up using the same maps which they had learned before for these environments. And", "tokens": [51336, 8685, 791, 917, 493, 1228, 264, 912, 11317, 597, 436, 632, 3264, 949, 337, 613, 12388, 13, 400, 51560], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 140, "seek": 74264, "start": 766.56, "end": 771.36, "text": " furthermore, if I replace this environment with one of the previous environments seen before,", "tokens": [51560, 3052, 3138, 11, 498, 286, 7406, 341, 2823, 365, 472, 295, 264, 3894, 12388, 1612, 949, 11, 51800], "temperature": 0.0, "avg_logprob": -0.06472018727085047, "compression_ratio": 1.9464285714285714, "no_speech_prob": 0.00041076901834458113}, {"id": 141, "seek": 77136, "start": 771.36, "end": 776.4, "text": " then remapping is only observed in this part of the environment, and this part of the environment", "tokens": [50364, 550, 890, 10534, 307, 787, 13095, 294, 341, 644, 295, 264, 2823, 11, 293, 341, 644, 295, 264, 2823, 50616], "temperature": 0.0, "avg_logprob": -0.08773091009684972, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.00018517619173508137}, {"id": 142, "seek": 77136, "start": 776.4, "end": 782.8000000000001, "text": " actually stays the same using the same previous map. So, this provides some evidence in support of", "tokens": [50616, 767, 10834, 264, 912, 1228, 264, 912, 3894, 4471, 13, 407, 11, 341, 6417, 512, 4467, 294, 1406, 295, 50936], "temperature": 0.0, "avg_logprob": -0.08773091009684972, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.00018517619173508137}, {"id": 143, "seek": 77136, "start": 782.8000000000001, "end": 788.8000000000001, "text": " composition of independent local sub-maps. So, in my experiment, I aim to assess whether people", "tokens": [50936, 12686, 295, 6695, 2654, 1422, 12, 76, 2382, 13, 407, 11, 294, 452, 5120, 11, 286, 5939, 281, 5877, 1968, 561, 51236], "temperature": 0.0, "avg_logprob": -0.08773091009684972, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.00018517619173508137}, {"id": 144, "seek": 77136, "start": 788.8000000000001, "end": 794.08, "text": " learn sub-maps on spatial structures and use them rationally in exploration, and I hypothesized that", "tokens": [51236, 1466, 1422, 12, 76, 2382, 322, 23598, 9227, 293, 764, 552, 24258, 379, 294, 16197, 11, 293, 286, 14276, 1602, 300, 51500], "temperature": 0.0, "avg_logprob": -0.08773091009684972, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.00018517619173508137}, {"id": 145, "seek": 77136, "start": 794.08, "end": 799.12, "text": " people might learn sub-maps that are adaptable and compositional, and my first alternate hypothesis", "tokens": [51500, 561, 1062, 1466, 1422, 12, 76, 2382, 300, 366, 6231, 712, 293, 10199, 2628, 11, 293, 452, 700, 18873, 17291, 51752], "temperature": 0.0, "avg_logprob": -0.08773091009684972, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.00018517619173508137}, {"id": 146, "seek": 79912, "start": 799.2, "end": 802.5600000000001, "text": " is that they might learn sub-maps of spatial structures that might not be adaptable or", "tokens": [50368, 307, 300, 436, 1062, 1466, 1422, 12, 76, 2382, 295, 23598, 9227, 300, 1062, 406, 312, 6231, 712, 420, 50536], "temperature": 0.0, "avg_logprob": -0.08490314382187863, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.0005612802342511714}, {"id": 147, "seek": 79912, "start": 802.5600000000001, "end": 808.0, "text": " compositional to certain transformations, and the last alternate hypothesis is that people", "tokens": [50536, 10199, 2628, 281, 1629, 34852, 11, 293, 264, 1036, 18873, 17291, 307, 300, 561, 50808], "temperature": 0.0, "avg_logprob": -0.08490314382187863, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.0005612802342511714}, {"id": 148, "seek": 79912, "start": 808.0, "end": 812.88, "text": " might just learn a global representation of environments without learning any sub-maps.", "tokens": [50808, 1062, 445, 1466, 257, 4338, 10290, 295, 12388, 1553, 2539, 604, 1422, 12, 76, 2382, 13, 51052], "temperature": 0.0, "avg_logprob": -0.08490314382187863, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.0005612802342511714}, {"id": 149, "seek": 79912, "start": 812.88, "end": 819.2, "text": " So, in order to test this hypothesis, we're building this task where we are building 3D", "tokens": [51052, 407, 11, 294, 1668, 281, 1500, 341, 17291, 11, 321, 434, 2390, 341, 5633, 689, 321, 366, 2390, 805, 35, 51368], "temperature": 0.0, "avg_logprob": -0.08490314382187863, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.0005612802342511714}, {"id": 150, "seek": 79912, "start": 819.2, "end": 824.64, "text": " virtual environments using Unity, and these environments have this repeating structure,", "tokens": [51368, 6374, 12388, 1228, 27913, 11, 293, 613, 12388, 362, 341, 18617, 3877, 11, 51640], "temperature": 0.0, "avg_logprob": -0.08490314382187863, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.0005612802342511714}, {"id": 151, "seek": 82464, "start": 824.72, "end": 830.24, "text": " and the task is for the subjects to find maximum amount of diamonds embedded in these environments", "tokens": [50368, 293, 264, 5633, 307, 337, 264, 13066, 281, 915, 6674, 2372, 295, 22612, 16741, 294, 613, 12388, 50644], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 152, "seek": 82464, "start": 830.24, "end": 835.12, "text": " in a limited amount of time given to them, and in order to test adaptability and composition,", "tokens": [50644, 294, 257, 5567, 2372, 295, 565, 2212, 281, 552, 11, 293, 294, 1668, 281, 1500, 6231, 2310, 293, 12686, 11, 50888], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 153, "seek": 82464, "start": 835.12, "end": 839.12, "text": " we can also have these repetitions be transformations of each other, for instance,", "tokens": [50888, 321, 393, 611, 362, 613, 13645, 2451, 312, 34852, 295, 1184, 661, 11, 337, 5197, 11, 51088], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 154, "seek": 82464, "start": 839.12, "end": 843.36, "text": " here it's a reflection, or we can also have these environments composed of different structures to", "tokens": [51088, 510, 309, 311, 257, 12914, 11, 420, 321, 393, 611, 362, 613, 12388, 18204, 295, 819, 9227, 281, 51300], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 155, "seek": 82464, "start": 843.36, "end": 847.6, "text": " see whether people can compose their representations or structures that they've already seen.", "tokens": [51300, 536, 1968, 561, 393, 35925, 641, 33358, 420, 9227, 300, 436, 600, 1217, 1612, 13, 51512], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 156, "seek": 82464, "start": 848.56, "end": 853.84, "text": " So, here's an example. Here you see that a person is navigating through corridor and enters a", "tokens": [51560, 407, 11, 510, 311, 364, 1365, 13, 1692, 291, 536, 300, 257, 954, 307, 32054, 807, 25602, 293, 18780, 257, 51824], "temperature": 0.0, "avg_logprob": -0.08794205983479818, "compression_ratio": 1.8547854785478548, "no_speech_prob": 0.00042363847023807466}, {"id": 157, "seek": 85384, "start": 853.84, "end": 858.72, "text": " structure, and they go to one of the rooms and they do not find anything there, and then they", "tokens": [50364, 3877, 11, 293, 436, 352, 281, 472, 295, 264, 9396, 293, 436, 360, 406, 915, 1340, 456, 11, 293, 550, 436, 50608], "temperature": 0.0, "avg_logprob": -0.07955054702045762, "compression_ratio": 2.068085106382979, "no_speech_prob": 0.001523770159110427}, {"id": 158, "seek": 85384, "start": 858.72, "end": 864.1600000000001, "text": " decide to go to the other room, and they end up finding a reward there, and now they're going", "tokens": [50608, 4536, 281, 352, 281, 264, 661, 1808, 11, 293, 436, 917, 493, 5006, 257, 7782, 456, 11, 293, 586, 436, 434, 516, 50880], "temperature": 0.0, "avg_logprob": -0.07955054702045762, "compression_ratio": 2.068085106382979, "no_speech_prob": 0.001523770159110427}, {"id": 159, "seek": 85384, "start": 864.1600000000001, "end": 870.48, "text": " back to the corridor and they continue exploring the environment, and when they enter another section,", "tokens": [50880, 646, 281, 264, 25602, 293, 436, 2354, 12736, 264, 2823, 11, 293, 562, 436, 3242, 1071, 3541, 11, 51196], "temperature": 0.0, "avg_logprob": -0.07955054702045762, "compression_ratio": 2.068085106382979, "no_speech_prob": 0.001523770159110427}, {"id": 160, "seek": 85384, "start": 870.48, "end": 875.9200000000001, "text": " if they show a preferential navigation strategy towards the room that has a reward, then that", "tokens": [51196, 498, 436, 855, 257, 4382, 2549, 17346, 5206, 3030, 264, 1808, 300, 575, 257, 7782, 11, 550, 300, 51468], "temperature": 0.0, "avg_logprob": -0.07955054702045762, "compression_ratio": 2.068085106382979, "no_speech_prob": 0.001523770159110427}, {"id": 161, "seek": 85384, "start": 875.9200000000001, "end": 880.64, "text": " indicates that they have realized that there's a repeating structure in the environment and indicates", "tokens": [51468, 16203, 300, 436, 362, 5334, 300, 456, 311, 257, 18617, 3877, 294, 264, 2823, 293, 16203, 51704], "temperature": 0.0, "avg_logprob": -0.07955054702045762, "compression_ratio": 2.068085106382979, "no_speech_prob": 0.001523770159110427}, {"id": 162, "seek": 88064, "start": 880.64, "end": 884.96, "text": " a possibility that people might be learning sub-maps and identifying sub-maps as they're", "tokens": [50364, 257, 7959, 300, 561, 1062, 312, 2539, 1422, 12, 76, 2382, 293, 16696, 1422, 12, 76, 2382, 382, 436, 434, 50580], "temperature": 0.0, "avg_logprob": -0.09516112639172243, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.0004652489733416587}, {"id": 163, "seek": 88064, "start": 884.96, "end": 890.56, "text": " navigating the spatial environments, and furthermore, if we do find through the experiment that people", "tokens": [50580, 32054, 264, 23598, 12388, 11, 293, 3052, 3138, 11, 498, 321, 360, 915, 807, 264, 5120, 300, 561, 50860], "temperature": 0.0, "avg_logprob": -0.09516112639172243, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.0004652489733416587}, {"id": 164, "seek": 88064, "start": 890.56, "end": 896.16, "text": " actually learn sub-maps, then we can use similar environments to design experiments where we can", "tokens": [50860, 767, 1466, 1422, 12, 76, 2382, 11, 550, 321, 393, 764, 2531, 12388, 281, 1715, 12050, 689, 321, 393, 51140], "temperature": 0.0, "avg_logprob": -0.09516112639172243, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.0004652489733416587}, {"id": 165, "seek": 88064, "start": 896.16, "end": 903.12, "text": " test for adaptability to transformations of environments and also for composition of", "tokens": [51140, 1500, 337, 6231, 2310, 281, 34852, 295, 12388, 293, 611, 337, 12686, 295, 51488], "temperature": 0.0, "avg_logprob": -0.09516112639172243, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.0004652489733416587}, {"id": 166, "seek": 88064, "start": 903.12, "end": 910.4, "text": " different spatial structures. So, that takes me to the next part of the talk, which is", "tokens": [51488, 819, 23598, 9227, 13, 407, 11, 300, 2516, 385, 281, 264, 958, 644, 295, 264, 751, 11, 597, 307, 51852], "temperature": 0.0, "avg_logprob": -0.09516112639172243, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.0004652489733416587}, {"id": 167, "seek": 91040, "start": 910.48, "end": 914.64, "text": " determining which principles guide fragmentation into sub-maps. So, since we are seeing that", "tokens": [50368, 23751, 597, 9156, 5934, 9241, 19631, 666, 1422, 12, 76, 2382, 13, 407, 11, 1670, 321, 366, 2577, 300, 50576], "temperature": 0.0, "avg_logprob": -0.1600126516623575, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0007790559320710599}, {"id": 168, "seek": 91040, "start": 914.64, "end": 920.8, "text": " sub-maps drive fast learning, the next natural question becomes what determines this fragmentation", "tokens": [50576, 1422, 12, 76, 2382, 3332, 2370, 2539, 11, 264, 958, 3303, 1168, 3643, 437, 24799, 341, 9241, 19631, 50884], "temperature": 0.0, "avg_logprob": -0.1600126516623575, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0007790559320710599}, {"id": 169, "seek": 91040, "start": 920.8, "end": 925.76, "text": " of a spatial environment into sub-maps, and this work is in collaboration with Mirko Klukas,", "tokens": [50884, 295, 257, 23598, 2823, 666, 1422, 12, 76, 2382, 11, 293, 341, 589, 307, 294, 9363, 365, 9421, 4093, 591, 2781, 32876, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1600126516623575, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0007790559320710599}, {"id": 170, "seek": 91040, "start": 925.76, "end": 932.56, "text": " who is a post-doc in the feed lab. So, here my hypothesis is that neural remapping is a signature", "tokens": [51132, 567, 307, 257, 2183, 12, 39966, 294, 264, 3154, 2715, 13, 407, 11, 510, 452, 17291, 307, 300, 18161, 890, 10534, 307, 257, 13397, 51472], "temperature": 0.0, "avg_logprob": -0.1600126516623575, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0007790559320710599}, {"id": 171, "seek": 91040, "start": 932.56, "end": 938.96, "text": " of sub-map reconstruction. So, here I'll explain what remapping is. So, basically, this is a 2D", "tokens": [51472, 295, 1422, 12, 24223, 31565, 13, 407, 11, 510, 286, 603, 2903, 437, 890, 10534, 307, 13, 407, 11, 1936, 11, 341, 307, 257, 568, 35, 51792], "temperature": 0.0, "avg_logprob": -0.1600126516623575, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.0007790559320710599}, {"id": 172, "seek": 93896, "start": 939.0400000000001, "end": 946.32, "text": " environment, and when the animal just explores this 2D environment, we find hexagonally periodic", "tokens": [50368, 2823, 11, 293, 562, 264, 5496, 445, 45473, 341, 568, 35, 2823, 11, 321, 915, 23291, 6709, 379, 27790, 50732], "temperature": 0.0, "avg_logprob": -0.10768731096957593, "compression_ratio": 1.9174757281553398, "no_speech_prob": 0.0013664457947015762}, {"id": 173, "seek": 93896, "start": 946.32, "end": 954.24, "text": " grid fields in the entorhinal cortex, and when you actually insert these walls in this environment,", "tokens": [50732, 10748, 7909, 294, 264, 948, 284, 71, 2071, 33312, 11, 293, 562, 291, 767, 8969, 613, 7920, 294, 341, 2823, 11, 51128], "temperature": 0.0, "avg_logprob": -0.10768731096957593, "compression_ratio": 1.9174757281553398, "no_speech_prob": 0.0013664457947015762}, {"id": 174, "seek": 93896, "start": 954.24, "end": 959.52, "text": " then what is observed is that when the animal turns, then this grid field which is formed either", "tokens": [51128, 550, 437, 307, 13095, 307, 300, 562, 264, 5496, 4523, 11, 550, 341, 10748, 2519, 597, 307, 8693, 2139, 51392], "temperature": 0.0, "avg_logprob": -0.10768731096957593, "compression_ratio": 1.9174757281553398, "no_speech_prob": 0.0013664457947015762}, {"id": 175, "seek": 93896, "start": 959.52, "end": 965.2, "text": " re-orients or shifts, and this is called remapping, when the grid field actually re-orients or shifts", "tokens": [51392, 319, 12, 284, 2448, 420, 19201, 11, 293, 341, 307, 1219, 890, 10534, 11, 562, 264, 10748, 2519, 767, 319, 12, 284, 2448, 420, 19201, 51676], "temperature": 0.0, "avg_logprob": -0.10768731096957593, "compression_ratio": 1.9174757281553398, "no_speech_prob": 0.0013664457947015762}, {"id": 176, "seek": 96520, "start": 965.2, "end": 970.6400000000001, "text": " from its original orientation. And so, what is observed is that animals actually end up using", "tokens": [50364, 490, 1080, 3380, 14764, 13, 400, 370, 11, 437, 307, 13095, 307, 300, 4882, 767, 917, 493, 1228, 50636], "temperature": 0.0, "avg_logprob": -0.09312698060432367, "compression_ratio": 1.9748953974895398, "no_speech_prob": 0.0008039060630835593}, {"id": 177, "seek": 96520, "start": 970.6400000000001, "end": 976.8000000000001, "text": " the same grid maps in alternate arms. So, this indicates the use of maps. And here's another", "tokens": [50636, 264, 912, 10748, 11317, 294, 18873, 5812, 13, 407, 11, 341, 16203, 264, 764, 295, 11317, 13, 400, 510, 311, 1071, 50944], "temperature": 0.0, "avg_logprob": -0.09312698060432367, "compression_ratio": 1.9748953974895398, "no_speech_prob": 0.0008039060630835593}, {"id": 178, "seek": 96520, "start": 976.8000000000001, "end": 981.76, "text": " example of the use of maps. So, basically, this is a 2-room environment, and animals explore", "tokens": [50944, 1365, 295, 264, 764, 295, 11317, 13, 407, 11, 1936, 11, 341, 307, 257, 568, 12, 2861, 2823, 11, 293, 4882, 6839, 51192], "temperature": 0.0, "avg_logprob": -0.09312698060432367, "compression_ratio": 1.9748953974895398, "no_speech_prob": 0.0008039060630835593}, {"id": 179, "seek": 96520, "start": 981.76, "end": 989.76, "text": " this environment, and it is seen that eventually the map formed in environment A is the same as the", "tokens": [51192, 341, 2823, 11, 293, 309, 307, 1612, 300, 4728, 264, 4471, 8693, 294, 2823, 316, 307, 264, 912, 382, 264, 51592], "temperature": 0.0, "avg_logprob": -0.09312698060432367, "compression_ratio": 1.9748953974895398, "no_speech_prob": 0.0008039060630835593}, {"id": 180, "seek": 96520, "start": 989.76, "end": 994.88, "text": " map formed in environment B, over short time scales. So, this is another example of the fact", "tokens": [51592, 4471, 8693, 294, 2823, 363, 11, 670, 2099, 565, 17408, 13, 407, 11, 341, 307, 1071, 1365, 295, 264, 1186, 51848], "temperature": 0.0, "avg_logprob": -0.09312698060432367, "compression_ratio": 1.9748953974895398, "no_speech_prob": 0.0008039060630835593}, {"id": 181, "seek": 99488, "start": 994.88, "end": 1000.48, "text": " that animals are reusing the maps in both the rooms which look very similar. So, what I'm suggesting", "tokens": [50364, 300, 4882, 366, 319, 7981, 264, 11317, 294, 1293, 264, 9396, 597, 574, 588, 2531, 13, 407, 11, 437, 286, 478, 18094, 50644], "temperature": 0.0, "avg_logprob": -0.06900961615822532, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00013550101721193641}, {"id": 182, "seek": 99488, "start": 1000.48, "end": 1006.08, "text": " is that this field repetition doesn't result from localization error or purely due to disorientation,", "tokens": [50644, 307, 300, 341, 2519, 30432, 1177, 380, 1874, 490, 2654, 2144, 6713, 420, 17491, 3462, 281, 717, 19521, 399, 11, 50924], "temperature": 0.0, "avg_logprob": -0.06900961615822532, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00013550101721193641}, {"id": 183, "seek": 99488, "start": 1006.08, "end": 1012.32, "text": " because even when you use transparent walls in this environment, you still see that the grid", "tokens": [50924, 570, 754, 562, 291, 764, 12737, 7920, 294, 341, 2823, 11, 291, 920, 536, 300, 264, 10748, 51236], "temperature": 0.0, "avg_logprob": -0.06900961615822532, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00013550101721193641}, {"id": 184, "seek": 99488, "start": 1012.32, "end": 1017.68, "text": " maps are being reused in alternate arms, even though the animal can see through these transparent", "tokens": [51236, 11317, 366, 885, 319, 4717, 294, 18873, 5812, 11, 754, 1673, 264, 5496, 393, 536, 807, 613, 12737, 51504], "temperature": 0.0, "avg_logprob": -0.06900961615822532, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00013550101721193641}, {"id": 185, "seek": 99488, "start": 1017.68, "end": 1024.24, "text": " walls. Furthermore, if you extend this 2-room environment to a 4-room environment, you still", "tokens": [51504, 7920, 13, 23999, 11, 498, 291, 10101, 341, 568, 12, 2861, 2823, 281, 257, 1017, 12, 2861, 2823, 11, 291, 920, 51832], "temperature": 0.0, "avg_logprob": -0.06900961615822532, "compression_ratio": 1.7672727272727273, "no_speech_prob": 0.00013550101721193641}, {"id": 186, "seek": 102424, "start": 1024.24, "end": 1029.36, "text": " see field repetition in all of these rooms, which suggests that animals are actually reusing", "tokens": [50364, 536, 2519, 30432, 294, 439, 295, 613, 9396, 11, 597, 13409, 300, 4882, 366, 767, 319, 7981, 50620], "temperature": 0.0, "avg_logprob": -0.08242909413463664, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00015840784180909395}, {"id": 187, "seek": 102424, "start": 1029.36, "end": 1033.92, "text": " sub-maps in a calculated way for efficient representation, rather than just being disoriented.", "tokens": [50620, 1422, 12, 76, 2382, 294, 257, 15598, 636, 337, 7148, 10290, 11, 2831, 813, 445, 885, 717, 27414, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08242909413463664, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00015840784180909395}, {"id": 188, "seek": 102424, "start": 1038.48, "end": 1043.1200000000001, "text": " So, next I talk about existing models of remapping, and there are two classes of models.", "tokens": [51076, 407, 11, 958, 286, 751, 466, 6741, 5245, 295, 890, 10534, 11, 293, 456, 366, 732, 5359, 295, 5245, 13, 51308], "temperature": 0.0, "avg_logprob": -0.08242909413463664, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00015840784180909395}, {"id": 189, "seek": 102424, "start": 1043.1200000000001, "end": 1048.4, "text": " One class of model suggests that remapping is driven by sensory ambiguity. So, for instance,", "tokens": [51308, 1485, 1508, 295, 2316, 13409, 300, 890, 10534, 307, 9555, 538, 27233, 46519, 13, 407, 11, 337, 5197, 11, 51572], "temperature": 0.0, "avg_logprob": -0.08242909413463664, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00015840784180909395}, {"id": 190, "seek": 102424, "start": 1048.4, "end": 1053.1200000000001, "text": " if you are in an environment that looks similar to an environment you've been before, either in", "tokens": [51572, 498, 291, 366, 294, 364, 2823, 300, 1542, 2531, 281, 364, 2823, 291, 600, 668, 949, 11, 2139, 294, 51808], "temperature": 0.0, "avg_logprob": -0.08242909413463664, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.00015840784180909395}, {"id": 191, "seek": 105312, "start": 1053.12, "end": 1058.8, "text": " terms of its geometry or its visual observations, then you might end up using the same map that", "tokens": [50364, 2115, 295, 1080, 18426, 420, 1080, 5056, 18163, 11, 550, 291, 1062, 917, 493, 1228, 264, 912, 4471, 300, 50648], "temperature": 0.0, "avg_logprob": -0.11683666344844934, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0002453098422847688}, {"id": 192, "seek": 105312, "start": 1058.8, "end": 1063.4399999999998, "text": " you had learned from a previous environment. Then there's another class of models that suggests", "tokens": [50648, 291, 632, 3264, 490, 257, 3894, 2823, 13, 1396, 456, 311, 1071, 1508, 295, 5245, 300, 13409, 50880], "temperature": 0.0, "avg_logprob": -0.11683666344844934, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0002453098422847688}, {"id": 193, "seek": 105312, "start": 1063.4399999999998, "end": 1068.08, "text": " that remapping is based on environment topology, instead of just sensory ambiguity. So, here,", "tokens": [50880, 300, 890, 10534, 307, 2361, 322, 2823, 1192, 1793, 11, 2602, 295, 445, 27233, 46519, 13, 407, 11, 510, 11, 51112], "temperature": 0.0, "avg_logprob": -0.11683666344844934, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0002453098422847688}, {"id": 194, "seek": 105312, "start": 1068.7199999999998, "end": 1072.8, "text": " each state in the environment is represented in terms of its successor states, and it's called", "tokens": [51144, 1184, 1785, 294, 264, 2823, 307, 10379, 294, 2115, 295, 1080, 31864, 4368, 11, 293, 309, 311, 1219, 51348], "temperature": 0.0, "avg_logprob": -0.11683666344844934, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0002453098422847688}, {"id": 195, "seek": 105312, "start": 1072.8, "end": 1078.08, "text": " a successor representation. And this successor representation actually ends up looking similar", "tokens": [51348, 257, 31864, 10290, 13, 400, 341, 31864, 10290, 767, 5314, 493, 1237, 2531, 51612], "temperature": 0.0, "avg_logprob": -0.11683666344844934, "compression_ratio": 1.7992424242424243, "no_speech_prob": 0.0002453098422847688}, {"id": 196, "seek": 107808, "start": 1078.08, "end": 1082.6399999999999, "text": " to place feeds, and if you do an identity composition on these successor representations,", "tokens": [50364, 281, 1081, 23712, 11, 293, 498, 291, 360, 364, 6575, 12686, 322, 613, 31864, 33358, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1181018866744696, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0006262489478103817}, {"id": 197, "seek": 107808, "start": 1082.6399999999999, "end": 1088.8, "text": " then you get fields that are very similar to grid fields. And this successor representation", "tokens": [50592, 550, 291, 483, 7909, 300, 366, 588, 2531, 281, 10748, 7909, 13, 400, 341, 31864, 10290, 50900], "temperature": 0.0, "avg_logprob": -0.1181018866744696, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0006262489478103817}, {"id": 198, "seek": 107808, "start": 1089.28, "end": 1093.84, "text": " encapsulates inherent dynamics of the environment, as well as the policy that the agent is following.", "tokens": [50924, 38745, 26192, 26387, 15679, 295, 264, 2823, 11, 382, 731, 382, 264, 3897, 300, 264, 9461, 307, 3480, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1181018866744696, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0006262489478103817}, {"id": 199, "seek": 107808, "start": 1093.84, "end": 1099.6799999999998, "text": " However, there are other approaches like the graph-leplacian approach, which is policy independent.", "tokens": [51152, 2908, 11, 456, 366, 661, 11587, 411, 264, 4295, 12, 306, 564, 326, 952, 3109, 11, 597, 307, 3897, 6695, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1181018866744696, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0006262489478103817}, {"id": 200, "seek": 107808, "start": 1101.12, "end": 1104.96, "text": " So, what are some of the limitations of these models? So, the models that are", "tokens": [51516, 407, 11, 437, 366, 512, 295, 264, 15705, 295, 613, 5245, 30, 407, 11, 264, 5245, 300, 366, 51708], "temperature": 0.0, "avg_logprob": -0.1181018866744696, "compression_ratio": 1.7662835249042146, "no_speech_prob": 0.0006262489478103817}, {"id": 201, "seek": 110496, "start": 1104.96, "end": 1110.16, "text": " based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment", "tokens": [50364, 2361, 322, 27233, 46519, 360, 406, 362, 890, 10534, 1553, 27233, 46519, 13, 407, 11, 294, 364, 2823, 50624], "temperature": 0.0, "avg_logprob": -0.06436825261532682, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0005192378885112703}, {"id": 202, "seek": 110496, "start": 1110.16, "end": 1116.64, "text": " like this, these two regions actually look very different. These models will not have any map", "tokens": [50624, 411, 341, 11, 613, 732, 10682, 767, 574, 588, 819, 13, 1981, 5245, 486, 406, 362, 604, 4471, 50948], "temperature": 0.0, "avg_logprob": -0.06436825261532682, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0005192378885112703}, {"id": 203, "seek": 110496, "start": 1116.64, "end": 1125.52, "text": " fragmentation or remapping. However, on the other hand, models which are based on environment topology,", "tokens": [50948, 9241, 19631, 420, 890, 10534, 13, 2908, 11, 322, 264, 661, 1011, 11, 5245, 597, 366, 2361, 322, 2823, 1192, 1793, 11, 51392], "temperature": 0.0, "avg_logprob": -0.06436825261532682, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0005192378885112703}, {"id": 204, "seek": 110496, "start": 1125.52, "end": 1130.88, "text": " actually, it's not very clear how remapping would happen on first visit in these environments,", "tokens": [51392, 767, 11, 309, 311, 406, 588, 1850, 577, 890, 10534, 576, 1051, 322, 700, 3441, 294, 613, 12388, 11, 51660], "temperature": 0.0, "avg_logprob": -0.06436825261532682, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0005192378885112703}, {"id": 205, "seek": 110496, "start": 1130.88, "end": 1134.24, "text": " because you need to build up the successor representation or the transition matrix of", "tokens": [51660, 570, 291, 643, 281, 1322, 493, 264, 31864, 10290, 420, 264, 6034, 8141, 295, 51828], "temperature": 0.0, "avg_logprob": -0.06436825261532682, "compression_ratio": 1.7962264150943397, "no_speech_prob": 0.0005192378885112703}, {"id": 206, "seek": 113424, "start": 1134.24, "end": 1141.2, "text": " the environment before you can observe the grid fields. So, in our model, we address these limitations,", "tokens": [50364, 264, 2823, 949, 291, 393, 11441, 264, 10748, 7909, 13, 407, 11, 294, 527, 2316, 11, 321, 2985, 613, 15705, 11, 50712], "temperature": 0.0, "avg_logprob": -0.06626344176958192, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00031990886782296}, {"id": 207, "seek": 113424, "start": 1141.2, "end": 1145.36, "text": " and we have remapping on first visit, and we also have remapping without sensory ambiguity.", "tokens": [50712, 293, 321, 362, 890, 10534, 322, 700, 3441, 11, 293, 321, 611, 362, 890, 10534, 1553, 27233, 46519, 13, 50920], "temperature": 0.0, "avg_logprob": -0.06626344176958192, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00031990886782296}, {"id": 208, "seek": 113424, "start": 1145.36, "end": 1150.16, "text": " And next, I'll go into the details of our model. So, we interpret grid remapping as", "tokens": [50920, 400, 958, 11, 286, 603, 352, 666, 264, 4365, 295, 527, 2316, 13, 407, 11, 321, 7302, 10748, 890, 10534, 382, 51160], "temperature": 0.0, "avg_logprob": -0.06626344176958192, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00031990886782296}, {"id": 209, "seek": 113424, "start": 1150.16, "end": 1154.96, "text": " fragmentation into submaps. Why is this a useful interpretation? That's because", "tokens": [51160, 9241, 19631, 666, 8286, 2382, 13, 1545, 307, 341, 257, 4420, 14174, 30, 663, 311, 570, 51400], "temperature": 0.0, "avg_logprob": -0.06626344176958192, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00031990886782296}, {"id": 210, "seek": 113424, "start": 1154.96, "end": 1159.52, "text": " remapping enables topological representation. So, for instance, if we are dividing this", "tokens": [51400, 890, 10534, 17077, 1192, 4383, 10290, 13, 407, 11, 337, 5197, 11, 498, 321, 366, 26764, 341, 51628], "temperature": 0.0, "avg_logprob": -0.06626344176958192, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.00031990886782296}, {"id": 211, "seek": 115952, "start": 1159.52, "end": 1164.4, "text": " environment into submaps, then we also need to store the relationships between these submaps,", "tokens": [50364, 2823, 666, 8286, 2382, 11, 550, 321, 611, 643, 281, 3531, 264, 6159, 1296, 613, 8286, 2382, 11, 50608], "temperature": 0.0, "avg_logprob": -0.06765836715698242, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0004172114422544837}, {"id": 212, "seek": 115952, "start": 1164.96, "end": 1169.28, "text": " and this enables a compact topological representation, which is beneficial for planning.", "tokens": [50636, 293, 341, 17077, 257, 14679, 1192, 4383, 10290, 11, 597, 307, 14072, 337, 5038, 13, 50852], "temperature": 0.0, "avg_logprob": -0.06765836715698242, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0004172114422544837}, {"id": 213, "seek": 115952, "start": 1170.96, "end": 1175.2, "text": " Second reason is that remapping reduces path integration errors. So, if you try to learn a", "tokens": [50936, 5736, 1778, 307, 300, 890, 10534, 18081, 3100, 10980, 13603, 13, 407, 11, 498, 291, 853, 281, 1466, 257, 51148], "temperature": 0.0, "avg_logprob": -0.06765836715698242, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0004172114422544837}, {"id": 214, "seek": 115952, "start": 1175.2, "end": 1180.72, "text": " global map, it can very quickly become inconsistent because of accumulation of path integration errors.", "tokens": [51148, 4338, 4471, 11, 309, 393, 588, 2661, 1813, 36891, 570, 295, 35647, 295, 3100, 10980, 13603, 13, 51424], "temperature": 0.0, "avg_logprob": -0.06765836715698242, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0004172114422544837}, {"id": 215, "seek": 115952, "start": 1180.72, "end": 1185.52, "text": " But if you divide the environment in submaps, then it becomes easier to map the environment.", "tokens": [51424, 583, 498, 291, 9845, 264, 2823, 294, 8286, 2382, 11, 550, 309, 3643, 3571, 281, 4471, 264, 2823, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06765836715698242, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0004172114422544837}, {"id": 216, "seek": 118552, "start": 1185.52, "end": 1190.8799999999999, "text": " And this has been shown by using Atlas framework in robotics, where they divide the environment", "tokens": [50364, 400, 341, 575, 668, 4898, 538, 1228, 32485, 8388, 294, 34145, 11, 689, 436, 9845, 264, 2823, 50632], "temperature": 0.0, "avg_logprob": -0.10069192780388726, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.00011957521928707138}, {"id": 217, "seek": 118552, "start": 1190.8799999999999, "end": 1195.44, "text": " into submaps in order to map the environment, and it works very well for large environments.", "tokens": [50632, 666, 8286, 2382, 294, 1668, 281, 4471, 264, 2823, 11, 293, 309, 1985, 588, 731, 337, 2416, 12388, 13, 50860], "temperature": 0.0, "avg_logprob": -0.10069192780388726, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.00011957521928707138}, {"id": 218, "seek": 118552, "start": 1196.96, "end": 1201.44, "text": " And the third reason is that remapping enables representation of abstract cognitive spaces,", "tokens": [50936, 400, 264, 2636, 1778, 307, 300, 890, 10534, 17077, 10290, 295, 12649, 15605, 7673, 11, 51160], "temperature": 0.0, "avg_logprob": -0.10069192780388726, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.00011957521928707138}, {"id": 219, "seek": 118552, "start": 1201.44, "end": 1207.2, "text": " because it allows representation of non-euclidean structures. So, in our model, we have two", "tokens": [51160, 570, 309, 4045, 10290, 295, 2107, 12, 68, 1311, 31264, 282, 9227, 13, 407, 11, 294, 527, 2316, 11, 321, 362, 732, 51448], "temperature": 0.0, "avg_logprob": -0.10069192780388726, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.00011957521928707138}, {"id": 220, "seek": 118552, "start": 1207.2, "end": 1212.4, "text": " possibilities. Either we can extend an old map, or we can decide to remap. And when we decide to", "tokens": [51448, 12178, 13, 13746, 321, 393, 10101, 364, 1331, 4471, 11, 420, 321, 393, 4536, 281, 890, 569, 13, 400, 562, 321, 4536, 281, 51708], "temperature": 0.0, "avg_logprob": -0.10069192780388726, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.00011957521928707138}, {"id": 221, "seek": 121240, "start": 1212.4, "end": 1217.44, "text": " remap, we can either remap to a new map or remap to an existing map. So, for instance,", "tokens": [50364, 890, 569, 11, 321, 393, 2139, 890, 569, 281, 257, 777, 4471, 420, 890, 569, 281, 364, 6741, 4471, 13, 407, 11, 337, 5197, 11, 50616], "temperature": 0.0, "avg_logprob": -0.06842565536499023, "compression_ratio": 1.9723502304147464, "no_speech_prob": 0.0013881855411455035}, {"id": 222, "seek": 121240, "start": 1217.44, "end": 1224.3200000000002, "text": " in the experiments we saw that in the square environment without walls, the map that the", "tokens": [50616, 294, 264, 12050, 321, 1866, 300, 294, 264, 3732, 2823, 1553, 7920, 11, 264, 4471, 300, 264, 50960], "temperature": 0.0, "avg_logprob": -0.06842565536499023, "compression_ratio": 1.9723502304147464, "no_speech_prob": 0.0013881855411455035}, {"id": 223, "seek": 121240, "start": 1224.3200000000002, "end": 1230.4, "text": " animal learns is always extended. But when we insert these walls in this environment,", "tokens": [50960, 5496, 27152, 307, 1009, 10913, 13, 583, 562, 321, 8969, 613, 7920, 294, 341, 2823, 11, 51264], "temperature": 0.0, "avg_logprob": -0.06842565536499023, "compression_ratio": 1.9723502304147464, "no_speech_prob": 0.0013881855411455035}, {"id": 224, "seek": 121240, "start": 1230.4, "end": 1235.44, "text": " then the map is extended within lane one. But when you turn from lane one to lane two,", "tokens": [51264, 550, 264, 4471, 307, 10913, 1951, 12705, 472, 13, 583, 562, 291, 1261, 490, 12705, 472, 281, 12705, 732, 11, 51516], "temperature": 0.0, "avg_logprob": -0.06842565536499023, "compression_ratio": 1.9723502304147464, "no_speech_prob": 0.0013881855411455035}, {"id": 225, "seek": 121240, "start": 1235.44, "end": 1239.52, "text": " you actually remap to a new map. And when you turn from lane two to lane three,", "tokens": [51516, 291, 767, 890, 569, 281, 257, 777, 4471, 13, 400, 562, 291, 1261, 490, 12705, 732, 281, 12705, 1045, 11, 51720], "temperature": 0.0, "avg_logprob": -0.06842565536499023, "compression_ratio": 1.9723502304147464, "no_speech_prob": 0.0013881855411455035}, {"id": 226, "seek": 123952, "start": 1239.52, "end": 1243.36, "text": " you end up remapping to an existing map, which is the same as lane one.", "tokens": [50364, 291, 917, 493, 890, 10534, 281, 364, 6741, 4471, 11, 597, 307, 264, 912, 382, 12705, 472, 13, 50556], "temperature": 0.0, "avg_logprob": -0.06664088489563484, "compression_ratio": 1.944915254237288, "no_speech_prob": 0.00037990574492141604}, {"id": 227, "seek": 123952, "start": 1244.56, "end": 1250.24, "text": " And similarly, in the two room experiment, we saw that when you go from room one to the corridor,", "tokens": [50616, 400, 14138, 11, 294, 264, 732, 1808, 5120, 11, 321, 1866, 300, 562, 291, 352, 490, 1808, 472, 281, 264, 25602, 11, 50900], "temperature": 0.0, "avg_logprob": -0.06664088489563484, "compression_ratio": 1.944915254237288, "no_speech_prob": 0.00037990574492141604}, {"id": 228, "seek": 123952, "start": 1250.24, "end": 1254.8799999999999, "text": " you end up mapping to a new map. And when you go from corridor to room two, you actually end up", "tokens": [50900, 291, 917, 493, 18350, 281, 257, 777, 4471, 13, 400, 562, 291, 352, 490, 25602, 281, 1808, 732, 11, 291, 767, 917, 493, 51132], "temperature": 0.0, "avg_logprob": -0.06664088489563484, "compression_ratio": 1.944915254237288, "no_speech_prob": 0.00037990574492141604}, {"id": 229, "seek": 123952, "start": 1254.8799999999999, "end": 1259.68, "text": " remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going", "tokens": [51132, 890, 10534, 281, 364, 6741, 4471, 13, 407, 11, 958, 11, 286, 478, 516, 281, 751, 466, 577, 321, 4536, 1968, 321, 366, 516, 51372], "temperature": 0.0, "avg_logprob": -0.06664088489563484, "compression_ratio": 1.944915254237288, "no_speech_prob": 0.00037990574492141604}, {"id": 230, "seek": 123952, "start": 1259.68, "end": 1265.84, "text": " to extend a map or whether we should be remapping. So, in our model, remapping is based on the", "tokens": [51372, 281, 10101, 257, 4471, 420, 1968, 321, 820, 312, 890, 10534, 13, 407, 11, 294, 527, 2316, 11, 890, 10534, 307, 2361, 322, 264, 51680], "temperature": 0.0, "avg_logprob": -0.06664088489563484, "compression_ratio": 1.944915254237288, "no_speech_prob": 0.00037990574492141604}, {"id": 231, "seek": 126584, "start": 1265.84, "end": 1270.8, "text": " notion of contiguous regions. And a contiguous region is a region such that when I stay within", "tokens": [50364, 10710, 295, 660, 30525, 10682, 13, 400, 257, 660, 30525, 4458, 307, 257, 4458, 1270, 300, 562, 286, 1754, 1951, 50612], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 232, "seek": 126584, "start": 1270.8, "end": 1274.8799999999999, "text": " that region, my visual observations change very little. And these contiguous regions are connected", "tokens": [50612, 300, 4458, 11, 452, 5056, 18163, 1319, 588, 707, 13, 400, 613, 660, 30525, 10682, 366, 4582, 50816], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 233, "seek": 126584, "start": 1274.8799999999999, "end": 1279.4399999999998, "text": " by these bottleneck states. And this is aligned with the experimental data, which we have seen,", "tokens": [50816, 538, 613, 44641, 547, 4368, 13, 400, 341, 307, 17962, 365, 264, 17069, 1412, 11, 597, 321, 362, 1612, 11, 51044], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 234, "seek": 126584, "start": 1279.4399999999998, "end": 1285.28, "text": " which suggests special rule of doorways and corridors. So, now we formalize the concept", "tokens": [51044, 597, 13409, 2121, 4978, 295, 2853, 942, 293, 46920, 13, 407, 11, 586, 321, 9860, 1125, 264, 3410, 51336], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 235, "seek": 126584, "start": 1285.28, "end": 1290.24, "text": " of contiguous regions by defining a measure of similarity. So, we define similarity as the ability", "tokens": [51336, 295, 660, 30525, 10682, 538, 17827, 257, 3481, 295, 32194, 13, 407, 11, 321, 6964, 32194, 382, 264, 3485, 51584], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 236, "seek": 126584, "start": 1290.24, "end": 1295.1999999999998, "text": " to predict observations at one pose from the observations made at another pose. And so,", "tokens": [51584, 281, 6069, 18163, 412, 472, 10774, 490, 264, 18163, 1027, 412, 1071, 10774, 13, 400, 370, 11, 51832], "temperature": 0.0, "avg_logprob": -0.07522698974609375, "compression_ratio": 1.9315068493150684, "no_speech_prob": 0.0006562208291143179}, {"id": 237, "seek": 129520, "start": 1295.2, "end": 1299.92, "text": " the overlap between the observations made at two poses actually is a notion of similarity.", "tokens": [50364, 264, 19959, 1296, 264, 18163, 1027, 412, 732, 26059, 767, 307, 257, 10710, 295, 32194, 13, 50600], "temperature": 0.0, "avg_logprob": -0.05242302894592285, "compression_ratio": 1.8925619834710743, "no_speech_prob": 9.169145050691441e-05}, {"id": 238, "seek": 129520, "start": 1299.92, "end": 1304.64, "text": " And this formalizes the concept of contiguous regions as a region where any two points are", "tokens": [50600, 400, 341, 9860, 5660, 264, 3410, 295, 660, 30525, 10682, 382, 257, 4458, 689, 604, 732, 2793, 366, 50836], "temperature": 0.0, "avg_logprob": -0.05242302894592285, "compression_ratio": 1.8925619834710743, "no_speech_prob": 9.169145050691441e-05}, {"id": 239, "seek": 129520, "start": 1304.64, "end": 1309.52, "text": " similar. And here, I'm showing that similarity actually decreases when you transition between", "tokens": [50836, 2531, 13, 400, 510, 11, 286, 478, 4099, 300, 32194, 767, 24108, 562, 291, 6034, 1296, 51080], "temperature": 0.0, "avg_logprob": -0.05242302894592285, "compression_ratio": 1.8925619834710743, "no_speech_prob": 9.169145050691441e-05}, {"id": 240, "seek": 129520, "start": 1309.52, "end": 1313.92, "text": " contiguous regions. So, if you look at points which are within this contiguous region,", "tokens": [51080, 660, 30525, 10682, 13, 407, 11, 498, 291, 574, 412, 2793, 597, 366, 1951, 341, 660, 30525, 4458, 11, 51300], "temperature": 0.0, "avg_logprob": -0.05242302894592285, "compression_ratio": 1.8925619834710743, "no_speech_prob": 9.169145050691441e-05}, {"id": 241, "seek": 129520, "start": 1313.92, "end": 1319.1200000000001, "text": " their similarity is high with respect to this point. But for points which are in other regions,", "tokens": [51300, 641, 32194, 307, 1090, 365, 3104, 281, 341, 935, 13, 583, 337, 2793, 597, 366, 294, 661, 10682, 11, 51560], "temperature": 0.0, "avg_logprob": -0.05242302894592285, "compression_ratio": 1.8925619834710743, "no_speech_prob": 9.169145050691441e-05}, {"id": 242, "seek": 131912, "start": 1320.08, "end": 1322.8, "text": " the similarity is pretty low as compared to this point.", "tokens": [50412, 264, 32194, 307, 1238, 2295, 382, 5347, 281, 341, 935, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11409411479517356, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.0003799436381086707}, {"id": 243, "seek": 131912, "start": 1324.8, "end": 1329.4399999999998, "text": " So, then we can use this notion of similarity to define density in order to do density-based", "tokens": [50648, 407, 11, 550, 321, 393, 764, 341, 10710, 295, 32194, 281, 6964, 10305, 294, 1668, 281, 360, 10305, 12, 6032, 50880], "temperature": 0.0, "avg_logprob": -0.11409411479517356, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.0003799436381086707}, {"id": 244, "seek": 131912, "start": 1329.4399999999998, "end": 1334.8, "text": " clustering. And here, we define density as a similarity between any pose X and its", "tokens": [50880, 596, 48673, 13, 400, 510, 11, 321, 6964, 10305, 382, 257, 32194, 1296, 604, 10774, 1783, 293, 1080, 51148], "temperature": 0.0, "avg_logprob": -0.11409411479517356, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.0003799436381086707}, {"id": 245, "seek": 131912, "start": 1334.8, "end": 1342.0, "text": " mth nearest neighbor. And this notion can be used with any greedy algorithm like optics to", "tokens": [51148, 275, 392, 23831, 5987, 13, 400, 341, 10710, 393, 312, 1143, 365, 604, 28228, 9284, 411, 42599, 281, 51508], "temperature": 0.0, "avg_logprob": -0.11409411479517356, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.0003799436381086707}, {"id": 246, "seek": 131912, "start": 1342.0, "end": 1346.08, "text": " generate fragmentations of the environment. And here, I'm showing one example of fragmentations", "tokens": [51508, 8460, 26424, 763, 295, 264, 2823, 13, 400, 510, 11, 286, 478, 4099, 472, 1365, 295, 26424, 763, 51712], "temperature": 0.0, "avg_logprob": -0.11409411479517356, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.0003799436381086707}, {"id": 247, "seek": 134608, "start": 1346.08, "end": 1350.24, "text": " of the environment where it gives four different clusters corresponding to these four different", "tokens": [50364, 295, 264, 2823, 689, 309, 2709, 1451, 819, 23313, 11760, 281, 613, 1451, 819, 50572], "temperature": 0.0, "avg_logprob": -0.05623322661205005, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.00021651991119142622}, {"id": 248, "seek": 134608, "start": 1350.24, "end": 1358.8, "text": " colors shown here. So, given that contiguity is a local property, we can also try to compute", "tokens": [50572, 4577, 4898, 510, 13, 407, 11, 2212, 300, 660, 16397, 507, 307, 257, 2654, 4707, 11, 321, 393, 611, 853, 281, 14722, 51000], "temperature": 0.0, "avg_logprob": -0.05623322661205005, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.00021651991119142622}, {"id": 249, "seek": 134608, "start": 1358.8, "end": 1364.0, "text": " segmentations online by predicting current observations from the past. And in this case,", "tokens": [51000, 9469, 763, 2950, 538, 32884, 2190, 18163, 490, 264, 1791, 13, 400, 294, 341, 1389, 11, 51260], "temperature": 0.0, "avg_logprob": -0.05623322661205005, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.00021651991119142622}, {"id": 250, "seek": 134608, "start": 1364.0, "end": 1368.8, "text": " observations can be represented by boundary vector cells. And we can implement a short-term memory", "tokens": [51260, 18163, 393, 312, 10379, 538, 12866, 8062, 5438, 13, 400, 321, 393, 4445, 257, 2099, 12, 7039, 4675, 51500], "temperature": 0.0, "avg_logprob": -0.05623322661205005, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.00021651991119142622}, {"id": 251, "seek": 134608, "start": 1368.8, "end": 1372.8, "text": " which stores exponential moving average of boundary vector cell activations", "tokens": [51500, 597, 9512, 21510, 2684, 4274, 295, 12866, 8062, 2815, 2430, 763, 51700], "temperature": 0.0, "avg_logprob": -0.05623322661205005, "compression_ratio": 1.7587548638132295, "no_speech_prob": 0.00021651991119142622}, {"id": 252, "seek": 137280, "start": 1372.8, "end": 1377.6, "text": " to approximate the similarity. So, for instance, our short-term memory at a previous time step", "tokens": [50364, 281, 30874, 264, 32194, 13, 407, 11, 337, 5197, 11, 527, 2099, 12, 7039, 4675, 412, 257, 3894, 565, 1823, 50604], "temperature": 0.0, "avg_logprob": -0.05058120798181604, "compression_ratio": 1.856, "no_speech_prob": 0.0005273676360957325}, {"id": 253, "seek": 137280, "start": 1377.6, "end": 1382.32, "text": " can be used to predict observations at a current time step to compute the similarity between two", "tokens": [50604, 393, 312, 1143, 281, 6069, 18163, 412, 257, 2190, 565, 1823, 281, 14722, 264, 32194, 1296, 732, 50840], "temperature": 0.0, "avg_logprob": -0.05058120798181604, "compression_ratio": 1.856, "no_speech_prob": 0.0005273676360957325}, {"id": 254, "seek": 137280, "start": 1382.32, "end": 1386.8799999999999, "text": " poses. And another component which we need to add to our model is the long-term memory component,", "tokens": [50840, 26059, 13, 400, 1071, 6542, 597, 321, 643, 281, 909, 281, 527, 2316, 307, 264, 938, 12, 7039, 4675, 6542, 11, 51068], "temperature": 0.0, "avg_logprob": -0.05058120798181604, "compression_ratio": 1.856, "no_speech_prob": 0.0005273676360957325}, {"id": 255, "seek": 137280, "start": 1386.8799999999999, "end": 1390.72, "text": " which helps us decide whether we should be remapping to an existing map or we should be", "tokens": [51068, 597, 3665, 505, 4536, 1968, 321, 820, 312, 890, 10534, 281, 364, 6741, 4471, 420, 321, 820, 312, 51260], "temperature": 0.0, "avg_logprob": -0.05058120798181604, "compression_ratio": 1.856, "no_speech_prob": 0.0005273676360957325}, {"id": 256, "seek": 137280, "start": 1390.72, "end": 1398.72, "text": " remapping to a new map. So, for all of these environments, our model makes the correct", "tokens": [51260, 890, 10534, 281, 257, 777, 4471, 13, 407, 11, 337, 439, 295, 613, 12388, 11, 527, 2316, 1669, 264, 3006, 51660], "temperature": 0.0, "avg_logprob": -0.05058120798181604, "compression_ratio": 1.856, "no_speech_prob": 0.0005273676360957325}, {"id": 257, "seek": 139872, "start": 1398.72, "end": 1404.48, "text": " predictions which are in line with the experimental data observed. And these experiments have been", "tokens": [50364, 21264, 597, 366, 294, 1622, 365, 264, 17069, 1412, 13095, 13, 400, 613, 12050, 362, 668, 50652], "temperature": 0.0, "avg_logprob": -0.09625017166137695, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.0013246499001979828}, {"id": 258, "seek": 139872, "start": 1404.48, "end": 1409.92, "text": " done and we have neural data for them. This is a new prediction that our model makes for amorphous", "tokens": [50652, 1096, 293, 321, 362, 18161, 1412, 337, 552, 13, 639, 307, 257, 777, 17630, 300, 527, 2316, 1669, 337, 15543, 950, 563, 50924], "temperature": 0.0, "avg_logprob": -0.09625017166137695, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.0013246499001979828}, {"id": 259, "seek": 139872, "start": 1409.92, "end": 1414.64, "text": " naturalistic environments. We predict that even in these environments, the map will be segmented", "tokens": [50924, 3303, 3142, 12388, 13, 492, 6069, 300, 754, 294, 613, 12388, 11, 264, 4471, 486, 312, 9469, 292, 51160], "temperature": 0.0, "avg_logprob": -0.09625017166137695, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.0013246499001979828}, {"id": 260, "seek": 139872, "start": 1414.64, "end": 1418.8, "text": " and grid fields will realign when going from one contiguous region to the other.", "tokens": [51160, 293, 10748, 7909, 486, 957, 788, 562, 516, 490, 472, 660, 30525, 4458, 281, 264, 661, 13, 51368], "temperature": 0.0, "avg_logprob": -0.09625017166137695, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.0013246499001979828}, {"id": 261, "seek": 139872, "start": 1418.8, "end": 1422.32, "text": " And we do not predict any map fragmentations in these spiral mesas.", "tokens": [51368, 400, 321, 360, 406, 6069, 604, 4471, 26424, 763, 294, 613, 25165, 3813, 296, 13, 51544], "temperature": 0.0, "avg_logprob": -0.09625017166137695, "compression_ratio": 1.7791164658634537, "no_speech_prob": 0.0013246499001979828}, {"id": 262, "seek": 142232, "start": 1422.48, "end": 1431.52, "text": " So, given that we built or proposed an algorithmic model for map fragmentation, the next question is,", "tokens": [50372, 407, 11, 2212, 300, 321, 3094, 420, 10348, 364, 9284, 299, 2316, 337, 4471, 9241, 19631, 11, 264, 958, 1168, 307, 11, 50824], "temperature": 0.0, "avg_logprob": -0.151744922838713, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0004303337773308158}, {"id": 263, "seek": 142232, "start": 1431.52, "end": 1436.8799999999999, "text": " how can map fragmentation be implemented on a neural level? So, we want to provide a framework", "tokens": [50824, 577, 393, 4471, 9241, 19631, 312, 12270, 322, 257, 18161, 1496, 30, 407, 11, 321, 528, 281, 2893, 257, 8388, 51092], "temperature": 0.0, "avg_logprob": -0.151744922838713, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0004303337773308158}, {"id": 264, "seek": 142232, "start": 1436.8799999999999, "end": 1441.36, "text": " for building a neural circuit model of map fragmentation. And this work is in collaboration", "tokens": [51092, 337, 2390, 257, 18161, 9048, 2316, 295, 4471, 9241, 19631, 13, 400, 341, 589, 307, 294, 9363, 51316], "temperature": 0.0, "avg_logprob": -0.151744922838713, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0004303337773308158}, {"id": 265, "seek": 142232, "start": 1441.36, "end": 1448.56, "text": " with Sarthak and Murko, who are both postdocs in FEDLA. So, going back to our theoretical", "tokens": [51316, 365, 6894, 392, 514, 293, 9373, 4093, 11, 567, 366, 1293, 2183, 39966, 82, 294, 479, 4731, 11435, 13, 407, 11, 516, 646, 281, 527, 20864, 51676], "temperature": 0.0, "avg_logprob": -0.151744922838713, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0004303337773308158}, {"id": 266, "seek": 144856, "start": 1448.56, "end": 1454.8799999999999, "text": " framework, we had suggested that place cells might encode topological relationships between", "tokens": [50364, 8388, 11, 321, 632, 10945, 300, 1081, 5438, 1062, 2058, 1429, 1192, 4383, 6159, 1296, 50680], "temperature": 0.0, "avg_logprob": -0.06388280438441857, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001345567754469812}, {"id": 267, "seek": 144856, "start": 1454.8799999999999, "end": 1460.08, "text": " metric maps that might be represented by the grid space. And now I'm going to talk about", "tokens": [50680, 20678, 11317, 300, 1062, 312, 10379, 538, 264, 10748, 1901, 13, 400, 586, 286, 478, 516, 281, 751, 466, 50940], "temperature": 0.0, "avg_logprob": -0.06388280438441857, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001345567754469812}, {"id": 268, "seek": 144856, "start": 1460.08, "end": 1465.44, "text": " how we can implement that at a neural level. So, at the neural level, we start with factorized", "tokens": [50940, 577, 321, 393, 4445, 300, 412, 257, 18161, 1496, 13, 407, 11, 412, 264, 18161, 1496, 11, 321, 722, 365, 5952, 1602, 51208], "temperature": 0.0, "avg_logprob": -0.06388280438441857, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001345567754469812}, {"id": 269, "seek": 144856, "start": 1465.44, "end": 1469.6, "text": " representations in which different aspects of knowledge are represented separately and can", "tokens": [51208, 33358, 294, 597, 819, 7270, 295, 3601, 366, 10379, 14759, 293, 393, 51416], "temperature": 0.0, "avg_logprob": -0.06388280438441857, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001345567754469812}, {"id": 270, "seek": 144856, "start": 1469.6, "end": 1474.8799999999999, "text": " then be flexibly recombined. So, for instance, in this case, location information from grid cells", "tokens": [51416, 550, 312, 5896, 3545, 850, 3548, 2001, 13, 407, 11, 337, 5197, 11, 294, 341, 1389, 11, 4914, 1589, 490, 10748, 5438, 51680], "temperature": 0.0, "avg_logprob": -0.06388280438441857, "compression_ratio": 1.669064748201439, "no_speech_prob": 0.001345567754469812}, {"id": 271, "seek": 147488, "start": 1474.88, "end": 1479.7600000000002, "text": " and contextual information from sensory cells form this conjunctive representation in place cells.", "tokens": [50364, 293, 35526, 1589, 490, 27233, 5438, 1254, 341, 18244, 20221, 10290, 294, 1081, 5438, 13, 50608], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 272, "seek": 147488, "start": 1480.4, "end": 1484.8000000000002, "text": " Here grid cells can enable path integration and can be thought of as implementing an affine vector", "tokens": [50640, 1692, 10748, 5438, 393, 9528, 3100, 10980, 293, 393, 312, 1194, 295, 382, 18114, 364, 2096, 533, 8062, 50860], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 273, "seek": 147488, "start": 1484.8000000000002, "end": 1490.0800000000002, "text": " space or an impedance space. The recurrent wiring between these place cell population encodes", "tokens": [50860, 1901, 420, 364, 36264, 1901, 13, 440, 18680, 1753, 27520, 1296, 613, 1081, 2815, 4415, 2058, 4789, 51124], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 274, "seek": 147488, "start": 1490.0800000000002, "end": 1495.2800000000002, "text": " neighborhood relationships or topology. And here, large changes in contextual input cause", "tokens": [51124, 7630, 6159, 420, 1192, 1793, 13, 400, 510, 11, 2416, 2962, 294, 35526, 4846, 3082, 51384], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 275, "seek": 147488, "start": 1495.2800000000002, "end": 1501.0400000000002, "text": " remapping in place cells, which in turn cause remapping in grid cells through these back projections.", "tokens": [51384, 890, 10534, 294, 1081, 5438, 11, 597, 294, 1261, 3082, 890, 10534, 294, 10748, 5438, 807, 613, 646, 32371, 13, 51672], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 276, "seek": 147488, "start": 1501.0400000000002, "end": 1504.4, "text": " And remapping here corresponds to transitioning from one local map to another.", "tokens": [51672, 400, 890, 10534, 510, 23249, 281, 33777, 490, 472, 2654, 4471, 281, 1071, 13, 51840], "temperature": 0.0, "avg_logprob": -0.13501847641808645, "compression_ratio": 1.8306188925081432, "no_speech_prob": 0.0005356048932299018}, {"id": 277, "seek": 150488, "start": 1505.2800000000002, "end": 1511.1200000000001, "text": " So, most of the previous work on interplay of grid and place circuits focuses on maintaining", "tokens": [50384, 407, 11, 881, 295, 264, 3894, 589, 322, 728, 2858, 295, 10748, 293, 1081, 26354, 16109, 322, 14916, 50676], "temperature": 0.0, "avg_logprob": -0.1294418681751598, "compression_ratio": 1.7011494252873562, "no_speech_prob": 5.7367371482541785e-05}, {"id": 278, "seek": 150488, "start": 1511.1200000000001, "end": 1516.16, "text": " firing properties of one population based on the inputs from another. So, for instance,", "tokens": [50676, 16045, 7221, 295, 472, 4415, 2361, 322, 264, 15743, 490, 1071, 13, 407, 11, 337, 5197, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1294418681751598, "compression_ratio": 1.7011494252873562, "no_speech_prob": 5.7367371482541785e-05}, {"id": 279, "seek": 150488, "start": 1517.2800000000002, "end": 1521.1200000000001, "text": " successor representation suggests that grid cells are a low-dimensional representation", "tokens": [50984, 31864, 10290, 13409, 300, 10748, 5438, 366, 257, 2295, 12, 18759, 10290, 51176], "temperature": 0.0, "avg_logprob": -0.1294418681751598, "compression_ratio": 1.7011494252873562, "no_speech_prob": 5.7367371482541785e-05}, {"id": 280, "seek": 150488, "start": 1521.1200000000001, "end": 1527.5200000000002, "text": " of place cells that stabilize place cell activity. Similarly, here's a model which", "tokens": [51176, 295, 1081, 5438, 300, 31870, 1081, 2815, 5191, 13, 13157, 11, 510, 311, 257, 2316, 597, 51496], "temperature": 0.0, "avg_logprob": -0.1294418681751598, "compression_ratio": 1.7011494252873562, "no_speech_prob": 5.7367371482541785e-05}, {"id": 281, "seek": 150488, "start": 1527.5200000000002, "end": 1532.4, "text": " implements non-negative PCA of place cells. So, place cells are at the input. The weights are", "tokens": [51496, 704, 17988, 2107, 12, 28561, 1166, 6465, 32, 295, 1081, 5438, 13, 407, 11, 1081, 5438, 366, 412, 264, 4846, 13, 440, 17443, 366, 51740], "temperature": 0.0, "avg_logprob": -0.1294418681751598, "compression_ratio": 1.7011494252873562, "no_speech_prob": 5.7367371482541785e-05}, {"id": 282, "seek": 153240, "start": 1532.4, "end": 1537.8400000000001, "text": " learned through heavy learning and a non-negativity constraint. And this network does PCU on the", "tokens": [50364, 3264, 807, 4676, 2539, 293, 257, 2107, 12, 28561, 30142, 25534, 13, 400, 341, 3209, 775, 6465, 52, 322, 264, 50636], "temperature": 0.0, "avg_logprob": -0.13042518827650282, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.00013550641597248614}, {"id": 283, "seek": 153240, "start": 1537.8400000000001, "end": 1544.0, "text": " inputs, and the outputs end up converging to grid-like fields, again, suggested that grid", "tokens": [50636, 15743, 11, 293, 264, 23930, 917, 493, 9652, 3249, 281, 10748, 12, 4092, 7909, 11, 797, 11, 10945, 300, 10748, 50944], "temperature": 0.0, "avg_logprob": -0.13042518827650282, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.00013550641597248614}, {"id": 284, "seek": 153240, "start": 1544.0, "end": 1546.72, "text": " cells might be a low-dimensional representation of place cells.", "tokens": [50944, 5438, 1062, 312, 257, 2295, 12, 18759, 10290, 295, 1081, 5438, 13, 51080], "temperature": 0.0, "avg_logprob": -0.13042518827650282, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.00013550641597248614}, {"id": 285, "seek": 153240, "start": 1551.2800000000002, "end": 1556.16, "text": " Another set of work suggests that inputs from border cells to grid cells could be used for", "tokens": [51308, 3996, 992, 295, 589, 13409, 300, 15743, 490, 7838, 5438, 281, 10748, 5438, 727, 312, 1143, 337, 51552], "temperature": 0.0, "avg_logprob": -0.13042518827650282, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.00013550641597248614}, {"id": 286, "seek": 153240, "start": 1556.16, "end": 1562.16, "text": " error-correcting grid cells. So, here I'm showing a one-day schematic just to make my point. So,", "tokens": [51552, 6713, 12, 19558, 2554, 278, 10748, 5438, 13, 407, 11, 510, 286, 478, 4099, 257, 472, 12, 810, 44739, 445, 281, 652, 452, 935, 13, 407, 11, 51852], "temperature": 0.0, "avg_logprob": -0.13042518827650282, "compression_ratio": 1.6282527881040891, "no_speech_prob": 0.00013550641597248614}, {"id": 287, "seek": 156216, "start": 1562.16, "end": 1568.5600000000002, "text": " this is a rodent at a specific location in space, and this is the grid activity profile", "tokens": [50364, 341, 307, 257, 8685, 317, 412, 257, 2685, 4914, 294, 1901, 11, 293, 341, 307, 264, 10748, 5191, 7964, 50684], "temperature": 0.0, "avg_logprob": -0.06481511116027833, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.00021646686946041882}, {"id": 288, "seek": 156216, "start": 1569.28, "end": 1573.44, "text": " that represents that location. And when the rodent explores the environment and comes back to this", "tokens": [50720, 300, 8855, 300, 4914, 13, 400, 562, 264, 8685, 317, 45473, 264, 2823, 293, 1487, 646, 281, 341, 50928], "temperature": 0.0, "avg_logprob": -0.06481511116027833, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.00021646686946041882}, {"id": 289, "seek": 156216, "start": 1573.44, "end": 1577.92, "text": " location, the representation of this location has drifted with respect to the original,", "tokens": [50928, 4914, 11, 264, 10290, 295, 341, 4914, 575, 19699, 292, 365, 3104, 281, 264, 3380, 11, 51152], "temperature": 0.0, "avg_logprob": -0.06481511116027833, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.00021646686946041882}, {"id": 290, "seek": 156216, "start": 1577.92, "end": 1583.1200000000001, "text": " and there's some error in the representation. And if the border cell activations are provided", "tokens": [51152, 293, 456, 311, 512, 6713, 294, 264, 10290, 13, 400, 498, 264, 7838, 2815, 2430, 763, 366, 5649, 51412], "temperature": 0.0, "avg_logprob": -0.06481511116027833, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.00021646686946041882}, {"id": 291, "seek": 156216, "start": 1583.1200000000001, "end": 1588.5600000000002, "text": " as input to grid cells, then they activate the current subset of neurons doing error correction", "tokens": [51412, 382, 4846, 281, 10748, 5438, 11, 550, 436, 13615, 264, 2190, 25993, 295, 22027, 884, 6713, 19984, 51684], "temperature": 0.0, "avg_logprob": -0.06481511116027833, "compression_ratio": 1.9094650205761317, "no_speech_prob": 0.00021646686946041882}, {"id": 292, "seek": 158856, "start": 1588.56, "end": 1593.6799999999998, "text": " and pulling back the representation to the original representation. And this is what this", "tokens": [50364, 293, 8407, 646, 264, 10290, 281, 264, 3380, 10290, 13, 400, 341, 307, 437, 341, 50620], "temperature": 0.0, "avg_logprob": -0.06063834978983952, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.00017951549671124667}, {"id": 293, "seek": 158856, "start": 1593.6799999999998, "end": 1598.32, "text": " looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell", "tokens": [50620, 1542, 411, 294, 568, 35, 13, 407, 11, 294, 568, 35, 11, 498, 291, 360, 406, 362, 604, 7838, 2815, 15743, 11, 550, 428, 10748, 2815, 50852], "temperature": 0.0, "avg_logprob": -0.06063834978983952, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.00017951549671124667}, {"id": 294, "seek": 158856, "start": 1598.32, "end": 1602.3999999999999, "text": " representations are not very stable, but with border cell inputs, your grid cell representations", "tokens": [50852, 33358, 366, 406, 588, 8351, 11, 457, 365, 7838, 2815, 15743, 11, 428, 10748, 2815, 33358, 51056], "temperature": 0.0, "avg_logprob": -0.06063834978983952, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.00017951549671124667}, {"id": 295, "seek": 158856, "start": 1602.3999999999999, "end": 1610.72, "text": " are fairly stable. So, I also want to point out the fact that place cells are thought to store", "tokens": [51056, 366, 6457, 8351, 13, 407, 11, 286, 611, 528, 281, 935, 484, 264, 1186, 300, 1081, 5438, 366, 1194, 281, 3531, 51472], "temperature": 0.0, "avg_logprob": -0.06063834978983952, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.00017951549671124667}, {"id": 296, "seek": 158856, "start": 1610.72, "end": 1614.32, "text": " neighborhood relationships in their reference synapses, and therefore they could implement", "tokens": [51472, 7630, 6159, 294, 641, 6408, 5451, 2382, 279, 11, 293, 4412, 436, 727, 4445, 51652], "temperature": 0.0, "avg_logprob": -0.06063834978983952, "compression_ratio": 1.848605577689243, "no_speech_prob": 0.00017951549671124667}, {"id": 297, "seek": 161432, "start": 1614.32, "end": 1618.72, "text": " a topological navigation strategy. And many models of place cell-based navigation have", "tokens": [50364, 257, 1192, 4383, 17346, 5206, 13, 400, 867, 5245, 295, 1081, 2815, 12, 6032, 17346, 362, 50584], "temperature": 0.0, "avg_logprob": -0.09238657296872606, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0008294236031360924}, {"id": 298, "seek": 161432, "start": 1618.72, "end": 1623.76, "text": " actually emphasized this view. So, they've suggested that recurrence synapses encode either", "tokens": [50584, 767, 34068, 341, 1910, 13, 407, 11, 436, 600, 10945, 300, 18680, 10760, 5451, 2382, 279, 2058, 1429, 2139, 50836], "temperature": 0.0, "avg_logprob": -0.09238657296872606, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0008294236031360924}, {"id": 299, "seek": 161432, "start": 1623.76, "end": 1628.72, "text": " spatial or temporal connectivity, as suggested by Blum and Abbott, or they encode transition", "tokens": [50836, 23598, 420, 30881, 21095, 11, 382, 10945, 538, 2177, 449, 293, 32673, 1521, 11, 420, 436, 2058, 1429, 6034, 51084], "temperature": 0.0, "avg_logprob": -0.09238657296872606, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0008294236031360924}, {"id": 300, "seek": 161432, "start": 1628.72, "end": 1634.6399999999999, "text": " probability, as suggested by the successor representation work. So, given all these insights,", "tokens": [51084, 8482, 11, 382, 10945, 538, 264, 31864, 10290, 589, 13, 407, 11, 2212, 439, 613, 14310, 11, 51380], "temperature": 0.0, "avg_logprob": -0.09238657296872606, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0008294236031360924}, {"id": 301, "seek": 161432, "start": 1634.6399999999999, "end": 1639.4399999999998, "text": " our goal is to build a comprehensive neural circuit model of premapping. And we start with these", "tokens": [51380, 527, 3387, 307, 281, 1322, 257, 13914, 18161, 9048, 2316, 295, 5624, 10534, 13, 400, 321, 722, 365, 613, 51620], "temperature": 0.0, "avg_logprob": -0.09238657296872606, "compression_ratio": 1.7047970479704797, "no_speech_prob": 0.0008294236031360924}, {"id": 302, "seek": 163944, "start": 1639.44, "end": 1644.0, "text": " two questions. Does high-capacity grid code, when projected to place cells, also lead to high", "tokens": [50364, 732, 1651, 13, 4402, 1090, 12, 9485, 19008, 10748, 3089, 11, 562, 26231, 281, 1081, 5438, 11, 611, 1477, 281, 1090, 50592], "temperature": 0.0, "avg_logprob": -0.08679558532406585, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0012254135217517614}, {"id": 303, "seek": 163944, "start": 1644.0, "end": 1649.44, "text": " capacity? And given these conjunctive representations between the location input and the sensory input,", "tokens": [50592, 6042, 30, 400, 2212, 613, 18244, 20221, 33358, 1296, 264, 4914, 4846, 293, 264, 27233, 4846, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08679558532406585, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0012254135217517614}, {"id": 304, "seek": 163944, "start": 1649.44, "end": 1655.28, "text": " can we learn neighborhood relationships between place codes? And before I go into the details", "tokens": [50864, 393, 321, 1466, 7630, 6159, 1296, 1081, 14211, 30, 400, 949, 286, 352, 666, 264, 4365, 51156], "temperature": 0.0, "avg_logprob": -0.08679558532406585, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0012254135217517614}, {"id": 305, "seek": 163944, "start": 1655.28, "end": 1659.76, "text": " of capacity, I just want to point out that traditionally, Hopfield networks have been", "tokens": [51156, 295, 6042, 11, 286, 445, 528, 281, 935, 484, 300, 19067, 11, 13438, 7610, 9590, 362, 668, 51380], "temperature": 0.0, "avg_logprob": -0.08679558532406585, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0012254135217517614}, {"id": 306, "seek": 163944, "start": 1659.76, "end": 1665.04, "text": " used for storing memories and patterns. And it has been observed that the maximum patterns", "tokens": [51380, 1143, 337, 26085, 8495, 293, 8294, 13, 400, 309, 575, 668, 13095, 300, 264, 6674, 8294, 51644], "temperature": 0.0, "avg_logprob": -0.08679558532406585, "compression_ratio": 1.683453237410072, "no_speech_prob": 0.0012254135217517614}, {"id": 307, "seek": 166504, "start": 1665.04, "end": 1669.36, "text": " that these networks can store is n, where n is the total number of neurons in the network.", "tokens": [50364, 300, 613, 9590, 393, 3531, 307, 297, 11, 689, 297, 307, 264, 3217, 1230, 295, 22027, 294, 264, 3209, 13, 50580], "temperature": 0.0, "avg_logprob": -0.08836413846157565, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0005032223416492343}, {"id": 308, "seek": 166504, "start": 1670.0, "end": 1674.6399999999999, "text": " And modern Hopfield networks, also known as dense associative memories, have an exponential", "tokens": [50612, 400, 4363, 13438, 7610, 9590, 11, 611, 2570, 382, 18011, 4180, 1166, 8495, 11, 362, 364, 21510, 50844], "temperature": 0.0, "avg_logprob": -0.08836413846157565, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0005032223416492343}, {"id": 309, "seek": 166504, "start": 1674.6399999999999, "end": 1680.3999999999999, "text": " capacity, but they use many body interaction terms, which are not biologically plausible.", "tokens": [50844, 6042, 11, 457, 436, 764, 867, 1772, 9285, 2115, 11, 597, 366, 406, 3228, 17157, 39925, 13, 51132], "temperature": 0.0, "avg_logprob": -0.08836413846157565, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0005032223416492343}, {"id": 310, "seek": 166504, "start": 1680.3999999999999, "end": 1684.3999999999999, "text": " And in our model, we stick to using two interaction terms in the weight computations,", "tokens": [51132, 400, 294, 527, 2316, 11, 321, 2897, 281, 1228, 732, 9285, 2115, 294, 264, 3364, 2807, 763, 11, 51332], "temperature": 0.0, "avg_logprob": -0.08836413846157565, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0005032223416492343}, {"id": 311, "seek": 166504, "start": 1684.3999999999999, "end": 1694.48, "text": " and we still get exponential capacities. So, this is the architecture of our model.", "tokens": [51332, 293, 321, 920, 483, 21510, 39396, 13, 407, 11, 341, 307, 264, 9482, 295, 527, 2316, 13, 51836], "temperature": 0.0, "avg_logprob": -0.08836413846157565, "compression_ratio": 1.7333333333333334, "no_speech_prob": 0.0005032223416492343}, {"id": 312, "seek": 169448, "start": 1694.48, "end": 1699.1200000000001, "text": " The model has different grid modules, which have different scales or periods,", "tokens": [50364, 440, 2316, 575, 819, 10748, 16679, 11, 597, 362, 819, 17408, 420, 13804, 11, 50596], "temperature": 0.0, "avg_logprob": -0.12044343640727381, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.00034054365823976696}, {"id": 313, "seek": 169448, "start": 1699.1200000000001, "end": 1705.44, "text": " and the binary grid code is projected to place code randomly. And the back projections from", "tokens": [50596, 293, 264, 17434, 10748, 3089, 307, 26231, 281, 1081, 3089, 16979, 13, 400, 264, 646, 32371, 490, 50912], "temperature": 0.0, "avg_logprob": -0.12044343640727381, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.00034054365823976696}, {"id": 314, "seek": 169448, "start": 1705.44, "end": 1710.16, "text": " place cells to grid cells are learned through associative Hebbian learning. And we observe", "tokens": [50912, 1081, 5438, 281, 10748, 5438, 366, 3264, 807, 4180, 1166, 634, 6692, 952, 2539, 13, 400, 321, 11441, 51148], "temperature": 0.0, "avg_logprob": -0.12044343640727381, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.00034054365823976696}, {"id": 315, "seek": 169448, "start": 1710.16, "end": 1715.44, "text": " that when we perturb these place cells with a noisy version of place code representations,", "tokens": [51148, 300, 562, 321, 40468, 613, 1081, 5438, 365, 257, 24518, 3037, 295, 1081, 3089, 33358, 11, 51412], "temperature": 0.0, "avg_logprob": -0.12044343640727381, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.00034054365823976696}, {"id": 316, "seek": 169448, "start": 1715.44, "end": 1720.32, "text": " then the network is able to successfully reconstruct all the patterns it's trained on.", "tokens": [51412, 550, 264, 3209, 307, 1075, 281, 10727, 31499, 439, 264, 8294, 309, 311, 8895, 322, 13, 51656], "temperature": 0.0, "avg_logprob": -0.12044343640727381, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.00034054365823976696}, {"id": 317, "seek": 172032, "start": 1720.32, "end": 1725.76, "text": " So, the network is fairly robust to noise. Furthermore, this network has exponential", "tokens": [50364, 407, 11, 264, 3209, 307, 6457, 13956, 281, 5658, 13, 23999, 11, 341, 3209, 575, 21510, 50636], "temperature": 0.0, "avg_logprob": -0.11078997611999512, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0002959314442705363}, {"id": 318, "seek": 172032, "start": 1725.76, "end": 1730.3999999999999, "text": " capacity that grows much faster than a non-modular network where the grid cells are non-modular.", "tokens": [50636, 6042, 300, 13156, 709, 4663, 813, 257, 2107, 12, 8014, 1040, 3209, 689, 264, 10748, 5438, 366, 2107, 12, 8014, 1040, 13, 50868], "temperature": 0.0, "avg_logprob": -0.11078997611999512, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0002959314442705363}, {"id": 319, "seek": 172032, "start": 1733.76, "end": 1737.6, "text": " Also, the network generalizes stored inputs to create stable attractor", "tokens": [51036, 2743, 11, 264, 3209, 2674, 5660, 12187, 15743, 281, 1884, 8351, 5049, 284, 51228], "temperature": 0.0, "avg_logprob": -0.11078997611999512, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0002959314442705363}, {"id": 320, "seek": 172032, "start": 1737.6, "end": 1742.24, "text": " states around every pattern in the grid coding space, despite training only over a vanishing", "tokens": [51228, 4368, 926, 633, 5102, 294, 264, 10748, 17720, 1901, 11, 7228, 3097, 787, 670, 257, 3161, 3807, 51460], "temperature": 0.0, "avg_logprob": -0.11078997611999512, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0002959314442705363}, {"id": 321, "seek": 172032, "start": 1742.24, "end": 1747.6, "text": " fraction of contiguous grid coding space. So, for instance, if my grid coding space has around", "tokens": [51460, 14135, 295, 660, 30525, 10748, 17720, 1901, 13, 407, 11, 337, 5197, 11, 498, 452, 10748, 17720, 1901, 575, 926, 51728], "temperature": 0.0, "avg_logprob": -0.11078997611999512, "compression_ratio": 1.7813765182186234, "no_speech_prob": 0.0002959314442705363}, {"id": 322, "seek": 174760, "start": 1747.6, "end": 1753.1999999999998, "text": " 10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns,", "tokens": [50364, 1266, 11, 1360, 8294, 11, 286, 393, 3847, 264, 3209, 322, 787, 926, 2331, 8294, 11, 700, 2331, 8294, 11, 50644], "temperature": 0.0, "avg_logprob": -0.10973746754298701, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.00028236015350557864}, {"id": 323, "seek": 174760, "start": 1753.1999999999998, "end": 1757.4399999999998, "text": " and the network is still able to robustly reconstruct all the 10,000 patterns in the grid", "tokens": [50644, 293, 264, 3209, 307, 920, 1075, 281, 13956, 356, 31499, 439, 264, 1266, 11, 1360, 8294, 294, 264, 10748, 50856], "temperature": 0.0, "avg_logprob": -0.10973746754298701, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.00028236015350557864}, {"id": 324, "seek": 174760, "start": 1757.4399999999998, "end": 1764.8, "text": " coding space, which is pretty striking. Furthermore, next we add these heterosciitiative", "tokens": [50856, 17720, 1901, 11, 597, 307, 1238, 18559, 13, 23999, 11, 958, 321, 909, 613, 20789, 329, 537, 8707, 1166, 51224], "temperature": 0.0, "avg_logprob": -0.10973746754298701, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.00028236015350557864}, {"id": 325, "seek": 174760, "start": 1764.8, "end": 1768.7199999999998, "text": " learning on the recurrent connections on place cells to see if they can encode neighborhood", "tokens": [51224, 2539, 322, 264, 18680, 1753, 9271, 322, 1081, 5438, 281, 536, 498, 436, 393, 2058, 1429, 7630, 51420], "temperature": 0.0, "avg_logprob": -0.10973746754298701, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.00028236015350557864}, {"id": 326, "seek": 174760, "start": 1768.7199999999998, "end": 1775.76, "text": " relationships or 1D sequences. And what we find is that the product of these weights converges to", "tokens": [51420, 6159, 420, 502, 35, 22978, 13, 400, 437, 321, 915, 307, 300, 264, 1674, 295, 613, 17443, 9652, 2880, 281, 51772], "temperature": 0.0, "avg_logprob": -0.10973746754298701, "compression_ratio": 1.7026022304832713, "no_speech_prob": 0.00028236015350557864}, {"id": 327, "seek": 177576, "start": 1775.76, "end": 1780.48, "text": " this transition matrix, which is actually the analytical matrix, an analytical transition", "tokens": [50364, 341, 6034, 8141, 11, 597, 307, 767, 264, 29579, 8141, 11, 364, 29579, 6034, 50600], "temperature": 0.0, "avg_logprob": -0.08912015468516249, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.00015354879724327475}, {"id": 328, "seek": 177576, "start": 1780.48, "end": 1785.92, "text": " matrix that relates contiguous grid codes in the grid coding space. Furthermore,", "tokens": [50600, 8141, 300, 16155, 660, 30525, 10748, 14211, 294, 264, 10748, 17720, 1901, 13, 23999, 11, 50872], "temperature": 0.0, "avg_logprob": -0.08912015468516249, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.00015354879724327475}, {"id": 329, "seek": 177576, "start": 1785.92, "end": 1789.92, "text": " this network actually has perfect sequence recall given enough number of place cells to", "tokens": [50872, 341, 3209, 767, 575, 2176, 8310, 9901, 2212, 1547, 1230, 295, 1081, 5438, 281, 51072], "temperature": 0.0, "avg_logprob": -0.08912015468516249, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.00015354879724327475}, {"id": 330, "seek": 177576, "start": 1789.92, "end": 1795.52, "text": " approximate this transition matrix. And again, training on only a subset of the sequence is", "tokens": [51072, 30874, 341, 6034, 8141, 13, 400, 797, 11, 3097, 322, 787, 257, 25993, 295, 264, 8310, 307, 51352], "temperature": 0.0, "avg_logprob": -0.08912015468516249, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.00015354879724327475}, {"id": 331, "seek": 177576, "start": 1795.52, "end": 1799.84, "text": " enough to recall the entire sequence. So, for instance, if I train, if I have a sequence of", "tokens": [51352, 1547, 281, 9901, 264, 2302, 8310, 13, 407, 11, 337, 5197, 11, 498, 286, 3847, 11, 498, 286, 362, 257, 8310, 295, 51568], "temperature": 0.0, "avg_logprob": -0.08912015468516249, "compression_ratio": 1.8114754098360655, "no_speech_prob": 0.00015354879724327475}, {"id": 332, "seek": 179984, "start": 1799.9199999999998, "end": 1806.0, "text": " length 500, and I train the network on only first 150 patterns in the sequence, the network", "tokens": [50368, 4641, 5923, 11, 293, 286, 3847, 264, 3209, 322, 787, 700, 8451, 8294, 294, 264, 8310, 11, 264, 3209, 50672], "temperature": 0.0, "avg_logprob": -0.10364323971318264, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0004044214147143066}, {"id": 333, "seek": 179984, "start": 1806.0, "end": 1811.12, "text": " is robustly able to reconstruct all 500 patterns in the sequence without having seen all of them", "tokens": [50672, 307, 13956, 356, 1075, 281, 31499, 439, 5923, 8294, 294, 264, 8310, 1553, 1419, 1612, 439, 295, 552, 50928], "temperature": 0.0, "avg_logprob": -0.10364323971318264, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0004044214147143066}, {"id": 334, "seek": 179984, "start": 1811.12, "end": 1818.24, "text": " before. So, the next step in this network is to introduce sensory input. And the sensory input", "tokens": [50928, 949, 13, 407, 11, 264, 958, 1823, 294, 341, 3209, 307, 281, 5366, 27233, 4846, 13, 400, 264, 27233, 4846, 51284], "temperature": 0.0, "avg_logprob": -0.10364323971318264, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0004044214147143066}, {"id": 335, "seek": 179984, "start": 1818.24, "end": 1822.24, "text": " would project randomly to place cells and back projections from place cells to sensory input", "tokens": [51284, 576, 1716, 16979, 281, 1081, 5438, 293, 646, 32371, 490, 1081, 5438, 281, 27233, 4846, 51484], "temperature": 0.0, "avg_logprob": -0.10364323971318264, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0004044214147143066}, {"id": 336, "seek": 179984, "start": 1822.24, "end": 1827.4399999999998, "text": " would be learned through associative hybrid learning. And here grid cells would form a basis,", "tokens": [51484, 576, 312, 3264, 807, 4180, 1166, 13051, 2539, 13, 400, 510, 10748, 5438, 576, 1254, 257, 5143, 11, 51744], "temperature": 0.0, "avg_logprob": -0.10364323971318264, "compression_ratio": 1.8431372549019607, "no_speech_prob": 0.0004044214147143066}, {"id": 337, "seek": 182744, "start": 1827.44, "end": 1831.04, "text": " and hippocampal places would link that basis with arbitrary sensory input.", "tokens": [50364, 293, 27745, 905, 1215, 304, 3190, 576, 2113, 300, 5143, 365, 23211, 27233, 4846, 13, 50544], "temperature": 0.0, "avg_logprob": -0.10155512362110372, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.00031495909206569195}, {"id": 338, "seek": 182744, "start": 1831.68, "end": 1836.0800000000002, "text": " And this combination of structured inputs and unstructured inputs could potentially enable", "tokens": [50576, 400, 341, 6562, 295, 18519, 15743, 293, 18799, 46847, 15743, 727, 7263, 9528, 50796], "temperature": 0.0, "avg_logprob": -0.10155512362110372, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.00031495909206569195}, {"id": 339, "seek": 182744, "start": 1836.0800000000002, "end": 1840.24, "text": " the storage and robust recollection of a large number of arbitrary sensory patterns from this", "tokens": [50796, 264, 6725, 293, 13956, 39495, 10183, 295, 257, 2416, 1230, 295, 23211, 27233, 8294, 490, 341, 51004], "temperature": 0.0, "avg_logprob": -0.10155512362110372, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.00031495909206569195}, {"id": 340, "seek": 182744, "start": 1840.24, "end": 1847.8400000000001, "text": " partial use. So, how does this connect to map fragmentation? So, in part two, we talked about", "tokens": [51004, 14641, 764, 13, 407, 11, 577, 775, 341, 1745, 281, 4471, 9241, 19631, 30, 407, 11, 294, 644, 732, 11, 321, 2825, 466, 51384], "temperature": 0.0, "avg_logprob": -0.10155512362110372, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.00031495909206569195}, {"id": 341, "seek": 182744, "start": 1847.8400000000001, "end": 1854.24, "text": " map fragmentation based on the notion of contiguous regions. And here I'm positing that", "tokens": [51384, 4471, 9241, 19631, 2361, 322, 264, 10710, 295, 660, 30525, 10682, 13, 400, 510, 286, 478, 1366, 1748, 300, 51704], "temperature": 0.0, "avg_logprob": -0.10155512362110372, "compression_ratio": 1.7710843373493976, "no_speech_prob": 0.00031495909206569195}, {"id": 342, "seek": 185424, "start": 1854.24, "end": 1859.52, "text": " when you transition between different contiguous regions that actually corresponds to a large", "tokens": [50364, 562, 291, 6034, 1296, 819, 660, 30525, 10682, 300, 767, 23249, 281, 257, 2416, 50628], "temperature": 0.0, "avg_logprob": -0.08259301092110428, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.00047276169061660767}, {"id": 343, "seek": 185424, "start": 1859.52, "end": 1865.6, "text": " contextual change, which when provided as input to this network would trigger remapping in place", "tokens": [50628, 35526, 1319, 11, 597, 562, 5649, 382, 4846, 281, 341, 3209, 576, 7875, 890, 10534, 294, 1081, 50932], "temperature": 0.0, "avg_logprob": -0.08259301092110428, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.00047276169061660767}, {"id": 344, "seek": 185424, "start": 1865.6, "end": 1870.88, "text": " cells, which would in turn cause remapping in grid cells, thus leading to the formation of local", "tokens": [50932, 5438, 11, 597, 576, 294, 1261, 3082, 890, 10534, 294, 10748, 5438, 11, 8807, 5775, 281, 264, 11723, 295, 2654, 51196], "temperature": 0.0, "avg_logprob": -0.08259301092110428, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.00047276169061660767}, {"id": 345, "seek": 185424, "start": 1870.88, "end": 1878.8, "text": " sub maps. And how does this connect to part one, where we saw that sub maps might enable quick", "tokens": [51196, 1422, 11317, 13, 400, 577, 775, 341, 1745, 281, 644, 472, 11, 689, 321, 1866, 300, 1422, 11317, 1062, 9528, 1702, 51592], "temperature": 0.0, "avg_logprob": -0.08259301092110428, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.00047276169061660767}, {"id": 346, "seek": 185424, "start": 1878.8, "end": 1884.0, "text": " learning and generalization in humans. So, this network actually enables us to anchor grid maps", "tokens": [51592, 2539, 293, 2674, 2144, 294, 6255, 13, 407, 11, 341, 3209, 767, 17077, 505, 281, 18487, 10748, 11317, 51852], "temperature": 0.0, "avg_logprob": -0.08259301092110428, "compression_ratio": 1.7381818181818183, "no_speech_prob": 0.00047276169061660767}, {"id": 347, "seek": 188400, "start": 1884.0, "end": 1889.44, "text": " to external cues through these conjunctive representations in place cells. And this anchoring", "tokens": [50364, 281, 8320, 32192, 807, 613, 18244, 20221, 33358, 294, 1081, 5438, 13, 400, 341, 12723, 3662, 50636], "temperature": 0.0, "avg_logprob": -0.06788563979299445, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.00047264964086934924}, {"id": 348, "seek": 188400, "start": 1889.44, "end": 1894.96, "text": " to external cues actually enables the alignment of grid maps, even when points of departure in an", "tokens": [50636, 281, 8320, 32192, 767, 17077, 264, 18515, 295, 10748, 11317, 11, 754, 562, 2793, 295, 25866, 294, 364, 50912], "temperature": 0.0, "avg_logprob": -0.06788563979299445, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.00047264964086934924}, {"id": 349, "seek": 188400, "start": 1894.96, "end": 1901.92, "text": " environment are different, leading to adaptable representations. Furthermore, if you are in an", "tokens": [50912, 2823, 366, 819, 11, 5775, 281, 6231, 712, 33358, 13, 23999, 11, 498, 291, 366, 294, 364, 51260], "temperature": 0.0, "avg_logprob": -0.06788563979299445, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.00047264964086934924}, {"id": 350, "seek": 188400, "start": 1901.92, "end": 1907.6, "text": " environment that is composed of previously seen spatial structures, then this anchoring still", "tokens": [51260, 2823, 300, 307, 18204, 295, 8046, 1612, 23598, 9227, 11, 550, 341, 12723, 3662, 920, 51544], "temperature": 0.0, "avg_logprob": -0.06788563979299445, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.00047264964086934924}, {"id": 351, "seek": 188400, "start": 1907.6, "end": 1913.44, "text": " enables you to pull out the right map when you're navigating through that composed spatial structure.", "tokens": [51544, 17077, 291, 281, 2235, 484, 264, 558, 4471, 562, 291, 434, 32054, 807, 300, 18204, 23598, 3877, 13, 51836], "temperature": 0.0, "avg_logprob": -0.06788563979299445, "compression_ratio": 1.853846153846154, "no_speech_prob": 0.00047264964086934924}, {"id": 352, "seek": 191400, "start": 1914.48, "end": 1920.8, "text": " So, to summarize, our global hypothesis was that cognitive map is organized as a globally", "tokens": [50388, 407, 11, 281, 20858, 11, 527, 4338, 17291, 390, 300, 15605, 4471, 307, 9983, 382, 257, 18958, 50704], "temperature": 0.0, "avg_logprob": -0.14301574364137115, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0001071759543265216}, {"id": 353, "seek": 191400, "start": 1920.8, "end": 1927.84, "text": " topological and locally metric or Euclidean map. So, this is one illustration of a topometric map.", "tokens": [50704, 1192, 4383, 293, 16143, 20678, 420, 462, 1311, 31264, 282, 4471, 13, 407, 11, 341, 307, 472, 22645, 295, 257, 1192, 29470, 4471, 13, 51056], "temperature": 0.0, "avg_logprob": -0.14301574364137115, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0001071759543265216}, {"id": 354, "seek": 191400, "start": 1927.84, "end": 1932.88, "text": " And we said that any sub part of this map can be termed as a sub map.", "tokens": [51056, 400, 321, 848, 300, 604, 1422, 644, 295, 341, 4471, 393, 312, 1433, 292, 382, 257, 1422, 4471, 13, 51308], "temperature": 0.0, "avg_logprob": -0.14301574364137115, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0001071759543265216}, {"id": 355, "seek": 191400, "start": 1934.96, "end": 1939.68, "text": " In part one, we suggested that learning sub maps that are adaptable and compositional may drive", "tokens": [51412, 682, 644, 472, 11, 321, 10945, 300, 2539, 1422, 11317, 300, 366, 6231, 712, 293, 10199, 2628, 815, 3332, 51648], "temperature": 0.0, "avg_logprob": -0.14301574364137115, "compression_ratio": 1.6465116279069767, "no_speech_prob": 0.0001071759543265216}, {"id": 356, "seek": 193968, "start": 1939.68, "end": 1943.3600000000001, "text": " fast learning in complex spaces. And we proposed to conduct human behavioral", "tokens": [50364, 2370, 2539, 294, 3997, 7673, 13, 400, 321, 10348, 281, 6018, 1952, 19124, 50548], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 357, "seek": 193968, "start": 1943.3600000000001, "end": 1948.0800000000002, "text": " experiments to test this hypothesis. In second part, we proposed an algorithmic model of map", "tokens": [50548, 12050, 281, 1500, 341, 17291, 13, 682, 1150, 644, 11, 321, 10348, 364, 9284, 299, 2316, 295, 4471, 50784], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 358, "seek": 193968, "start": 1948.0800000000002, "end": 1952.72, "text": " fragmentation into sub maps based on the notion of contiguous regions. And in the third part,", "tokens": [50784, 9241, 19631, 666, 1422, 11317, 2361, 322, 264, 10710, 295, 660, 30525, 10682, 13, 400, 294, 264, 2636, 644, 11, 51016], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 359, "seek": 193968, "start": 1952.72, "end": 1957.68, "text": " we proposed a framework for a neural circuit model of map fragmentation. And overall,", "tokens": [51016, 321, 10348, 257, 8388, 337, 257, 18161, 9048, 2316, 295, 4471, 9241, 19631, 13, 400, 4787, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 360, "seek": 193968, "start": 1957.68, "end": 1962.24, "text": " we're suggesting that sub maps enable humans to build up a knowledge base of spatial structures", "tokens": [51264, 321, 434, 18094, 300, 1422, 11317, 9528, 6255, 281, 1322, 493, 257, 3601, 3096, 295, 23598, 9227, 51492], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 361, "seek": 193968, "start": 1962.24, "end": 1967.2, "text": " that they can continuously enrich and refine throughout their life by combining their existing", "tokens": [51492, 300, 436, 393, 15684, 18849, 293, 33906, 3710, 641, 993, 538, 21928, 641, 6741, 51740], "temperature": 0.0, "avg_logprob": -0.07952543275546184, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.0005701743066310883}, {"id": 362, "seek": 196720, "start": 1967.28, "end": 1973.1200000000001, "text": " spatial knowledge with their new experiences. So, that's all for my talk. And before I end,", "tokens": [50368, 23598, 3601, 365, 641, 777, 5235, 13, 407, 11, 300, 311, 439, 337, 452, 751, 13, 400, 949, 286, 917, 11, 50660], "temperature": 0.0, "avg_logprob": -0.16441605307839133, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0018092666286975145}, {"id": 363, "seek": 196720, "start": 1973.1200000000001, "end": 1978.8, "text": " I want to acknowledge my collaborators again, Marta, Kevin, Merko and Sata. And I also want to", "tokens": [50660, 286, 528, 281, 10692, 452, 39789, 797, 11, 5807, 64, 11, 9954, 11, 6124, 4093, 293, 318, 3274, 13, 400, 286, 611, 528, 281, 50944], "temperature": 0.0, "avg_logprob": -0.16441605307839133, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0018092666286975145}, {"id": 364, "seek": 196720, "start": 1978.8, "end": 1984.0800000000002, "text": " thank my supervisors, Ila and Josh for their continued feedback and support and discussions", "tokens": [50944, 1309, 452, 42218, 11, 286, 875, 293, 9785, 337, 641, 7014, 5824, 293, 1406, 293, 11088, 51208], "temperature": 0.0, "avg_logprob": -0.16441605307839133, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0018092666286975145}, {"id": 365, "seek": 196720, "start": 1984.0800000000002, "end": 1989.8400000000001, "text": " on these projects. I also want to thank Matt Wilson for his feedback and valuable suggestions", "tokens": [51208, 322, 613, 4455, 13, 286, 611, 528, 281, 1309, 7397, 15388, 337, 702, 5824, 293, 8263, 13396, 51496], "temperature": 0.0, "avg_logprob": -0.16441605307839133, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0018092666286975145}, {"id": 366, "seek": 196720, "start": 1989.8400000000001, "end": 1994.96, "text": " during my committee meetings and the members of FeedLab and TenBomb in general for their support.", "tokens": [51496, 1830, 452, 7482, 8410, 293, 264, 2679, 295, 33720, 37880, 293, 9380, 33, 3548, 294, 2674, 337, 641, 1406, 13, 51752], "temperature": 0.0, "avg_logprob": -0.16441605307839133, "compression_ratio": 1.6906474820143884, "no_speech_prob": 0.0018092666286975145}, {"id": 367, "seek": 199496, "start": 1995.52, "end": 2002.08, "text": " And before I end, I also want to thank the VCS department and McGowan Institute for their", "tokens": [50392, 400, 949, 286, 917, 11, 286, 611, 528, 281, 1309, 264, 691, 26283, 5882, 293, 21865, 37345, 9446, 337, 641, 50720], "temperature": 0.0, "avg_logprob": -0.14160185325436475, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.0006063589244149625}, {"id": 368, "seek": 199496, "start": 2002.08, "end": 2007.28, "text": " continued support and for providing an environment that's conducive to research,", "tokens": [50720, 7014, 1406, 293, 337, 6530, 364, 2823, 300, 311, 45095, 488, 281, 2132, 11, 50980], "temperature": 0.0, "avg_logprob": -0.14160185325436475, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.0006063589244149625}, {"id": 369, "seek": 199496, "start": 2007.28, "end": 2011.8400000000001, "text": " even in the face of this pandemic. Thank you. And I can take any questions now.", "tokens": [50980, 754, 294, 264, 1851, 295, 341, 5388, 13, 1044, 291, 13, 400, 286, 393, 747, 604, 1651, 586, 13, 51208], "temperature": 0.0, "avg_logprob": -0.14160185325436475, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.0006063589244149625}, {"id": 370, "seek": 199496, "start": 2014.56, "end": 2020.0, "text": " Thanks, Sue, for the great talk. We already have a question from Marta Griffin.", "tokens": [51344, 2561, 11, 25332, 11, 337, 264, 869, 751, 13, 492, 1217, 362, 257, 1168, 490, 5807, 64, 39188, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14160185325436475, "compression_ratio": 1.4864864864864864, "no_speech_prob": 0.0006063589244149625}, {"id": 371, "seek": 202000, "start": 2020.8, "end": 2024.0, "text": " Great talk. I have a question about partitioning a space to sub maps.", "tokens": [50404, 3769, 751, 13, 286, 362, 257, 1168, 466, 24808, 278, 257, 1901, 281, 1422, 11317, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12872005689262164, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0026716310530900955}, {"id": 372, "seek": 202000, "start": 2024.64, "end": 2029.12, "text": " What do you think will happen if you ask humans to intuitively partition an environment to regions?", "tokens": [50596, 708, 360, 291, 519, 486, 1051, 498, 291, 1029, 6255, 281, 46506, 24808, 364, 2823, 281, 10682, 30, 50820], "temperature": 0.0, "avg_logprob": -0.12872005689262164, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0026716310530900955}, {"id": 373, "seek": 202000, "start": 2029.12, "end": 2032.96, "text": " How would there be segmentation compared to the maps given by your similarity measure?", "tokens": [50820, 1012, 576, 456, 312, 9469, 399, 5347, 281, 264, 11317, 2212, 538, 428, 32194, 3481, 30, 51012], "temperature": 0.0, "avg_logprob": -0.12872005689262164, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0026716310530900955}, {"id": 374, "seek": 202000, "start": 2035.28, "end": 2043.04, "text": " So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively", "tokens": [51128, 407, 11, 291, 914, 264, 3997, 30, 15585, 11, 370, 11, 1392, 11, 370, 264, 1168, 307, 300, 498, 6255, 46506, 51516], "temperature": 0.0, "avg_logprob": -0.12872005689262164, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0026716310530900955}, {"id": 375, "seek": 202000, "start": 2043.04, "end": 2047.44, "text": " try to segment a map, how would that compare to the segmentations which we have proposed", "tokens": [51516, 853, 281, 9469, 257, 4471, 11, 577, 576, 300, 6794, 281, 264, 9469, 763, 597, 321, 362, 10348, 51736], "temperature": 0.0, "avg_logprob": -0.12872005689262164, "compression_ratio": 1.7233201581027668, "no_speech_prob": 0.0026716310530900955}, {"id": 376, "seek": 204744, "start": 2047.52, "end": 2053.76, "text": " in part two of my talk where we have given an algorithmic model for map segmentation.", "tokens": [50368, 294, 644, 732, 295, 452, 751, 689, 321, 362, 2212, 364, 9284, 299, 2316, 337, 4471, 9469, 399, 13, 50680], "temperature": 0.0, "avg_logprob": -0.138348547260413, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0008551737992092967}, {"id": 377, "seek": 204744, "start": 2055.92, "end": 2061.2000000000003, "text": " So, basically, the way I view it is that in the algorithmic model of map segmentation,", "tokens": [50788, 407, 11, 1936, 11, 264, 636, 286, 1910, 309, 307, 300, 294, 264, 9284, 299, 2316, 295, 4471, 9469, 399, 11, 51052], "temperature": 0.0, "avg_logprob": -0.138348547260413, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0008551737992092967}, {"id": 378, "seek": 204744, "start": 2061.2000000000003, "end": 2068.08, "text": " I basically took one metric map and one small portion of the map. And we said that that can be", "tokens": [51052, 286, 1936, 1890, 472, 20678, 4471, 293, 472, 1359, 8044, 295, 264, 4471, 13, 400, 321, 848, 300, 300, 393, 312, 51396], "temperature": 0.0, "avg_logprob": -0.138348547260413, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0008551737992092967}, {"id": 379, "seek": 204744, "start": 2068.08, "end": 2073.2000000000003, "text": " decomposed into various fragments of sub maps. But humans are actually able to reason in much", "tokens": [51396, 22867, 1744, 666, 3683, 29197, 295, 1422, 11317, 13, 583, 6255, 366, 767, 1075, 281, 1778, 294, 709, 51652], "temperature": 0.0, "avg_logprob": -0.138348547260413, "compression_ratio": 1.6409090909090909, "no_speech_prob": 0.0008551737992092967}, {"id": 380, "seek": 207320, "start": 2073.2, "end": 2079.68, "text": " richer environments than the ones which we saw in this algorithm. And so, there we can", "tokens": [50364, 29021, 12388, 813, 264, 2306, 597, 321, 1866, 294, 341, 9284, 13, 400, 370, 11, 456, 321, 393, 50688], "temperature": 0.0, "avg_logprob": -0.11843809059688024, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0029329610988497734}, {"id": 381, "seek": 207320, "start": 2079.68, "end": 2086.0, "text": " view it as something like this where you might have even small spaces being segmented into", "tokens": [50688, 1910, 309, 382, 746, 411, 341, 689, 291, 1062, 362, 754, 1359, 7673, 885, 9469, 292, 666, 51004], "temperature": 0.0, "avg_logprob": -0.11843809059688024, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0029329610988497734}, {"id": 382, "seek": 207320, "start": 2086.0, "end": 2092.48, "text": " multiple maps. So, let me just see if I can, yeah, so even if you have something like this,", "tokens": [51004, 3866, 11317, 13, 407, 11, 718, 385, 445, 536, 498, 286, 393, 11, 1338, 11, 370, 754, 498, 291, 362, 746, 411, 341, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11843809059688024, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0029329610988497734}, {"id": 383, "seek": 207320, "start": 2093.04, "end": 2096.96, "text": " so even within this, so I was calling this a sub map, but even within this sub map,", "tokens": [51356, 370, 754, 1951, 341, 11, 370, 286, 390, 5141, 341, 257, 1422, 4471, 11, 457, 754, 1951, 341, 1422, 4471, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11843809059688024, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0029329610988497734}, {"id": 384, "seek": 207320, "start": 2096.96, "end": 2100.8799999999997, "text": " we might have multiple maps. So, based on the notion of contiguous regions, you might have", "tokens": [51552, 321, 1062, 362, 3866, 11317, 13, 407, 11, 2361, 322, 264, 10710, 295, 660, 30525, 10682, 11, 291, 1062, 362, 51748], "temperature": 0.0, "avg_logprob": -0.11843809059688024, "compression_ratio": 1.88135593220339, "no_speech_prob": 0.0029329610988497734}, {"id": 385, "seek": 210088, "start": 2100.88, "end": 2105.2000000000003, "text": " a local map here, a local map here, and all these maps might be connected.", "tokens": [50364, 257, 2654, 4471, 510, 11, 257, 2654, 4471, 510, 11, 293, 439, 613, 11317, 1062, 312, 4582, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09983284385116012, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0012061666930094361}, {"id": 386, "seek": 210088, "start": 2105.2000000000003, "end": 2111.52, "text": " And so, the algorithmic model would predict those kinds of map segmentations, but then even", "tokens": [50580, 400, 370, 11, 264, 9284, 299, 2316, 576, 6069, 729, 3685, 295, 4471, 9469, 763, 11, 457, 550, 754, 50896], "temperature": 0.0, "avg_logprob": -0.09983284385116012, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0012061666930094361}, {"id": 387, "seek": 210088, "start": 2111.52, "end": 2116.6400000000003, "text": " a combination of those map segmentations can be termed as a sub map. And then that is what I", "tokens": [50896, 257, 6562, 295, 729, 4471, 9469, 763, 393, 312, 1433, 292, 382, 257, 1422, 4471, 13, 400, 550, 300, 307, 437, 286, 51152], "temperature": 0.0, "avg_logprob": -0.09983284385116012, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0012061666930094361}, {"id": 388, "seek": 210088, "start": 2116.6400000000003, "end": 2123.52, "text": " was talking about in part one where I'm talking about composing the sub map which is even itself", "tokens": [51152, 390, 1417, 466, 294, 644, 472, 689, 286, 478, 1417, 466, 715, 6110, 264, 1422, 4471, 597, 307, 754, 2564, 51496], "temperature": 0.0, "avg_logprob": -0.09983284385116012, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0012061666930094361}, {"id": 389, "seek": 210088, "start": 2123.52, "end": 2130.0, "text": " composed of smaller maps. And we can take this representation of this map and then compose", "tokens": [51496, 18204, 295, 4356, 11317, 13, 400, 321, 393, 747, 341, 10290, 295, 341, 4471, 293, 550, 35925, 51820], "temperature": 0.0, "avg_logprob": -0.09983284385116012, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0012061666930094361}, {"id": 390, "seek": 213000, "start": 2130.0, "end": 2138.96, "text": " them in various ways to build richer environments.", "tokens": [50364, 552, 294, 3683, 2098, 281, 1322, 29021, 12388, 13, 50812], "temperature": 0.0, "avg_logprob": -0.24960199083600726, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.001806732383556664}, {"id": 391, "seek": 213000, "start": 2150.24, "end": 2154.56, "text": " I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret", "tokens": [51376, 286, 603, 1524, 493, 490, 5807, 64, 13, 286, 478, 6369, 498, 264, 9284, 393, 6069, 577, 6255, 576, 7302, 51592], "temperature": 0.0, "avg_logprob": -0.24960199083600726, "compression_ratio": 1.2608695652173914, "no_speech_prob": 0.001806732383556664}, {"id": 392, "seek": 215456, "start": 2154.56, "end": 2160.88, "text": " sub maps. How humans would interpret?", "tokens": [50364, 1422, 11317, 13, 1012, 6255, 576, 7302, 30, 50680], "temperature": 0.0, "avg_logprob": -0.18772970955326873, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.0009995547588914633}, {"id": 393, "seek": 215456, "start": 2166.64, "end": 2170.72, "text": " Well, it's a little bit unclear to me what you mean by interpret", "tokens": [50968, 1042, 11, 309, 311, 257, 707, 857, 25636, 281, 385, 437, 291, 914, 538, 7302, 51172], "temperature": 0.0, "avg_logprob": -0.18772970955326873, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.0009995547588914633}, {"id": 394, "seek": 215456, "start": 2171.68, "end": 2180.32, "text": " sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret", "tokens": [51220, 1422, 11317, 11, 457, 286, 914, 11, 286, 478, 11926, 300, 291, 434, 18094, 11, 291, 458, 11, 577, 6255, 576, 7302, 51652], "temperature": 0.0, "avg_logprob": -0.18772970955326873, "compression_ratio": 1.4887218045112782, "no_speech_prob": 0.0009995547588914633}, {"id": 395, "seek": 218032, "start": 2181.2000000000003, "end": 2186.0800000000004, "text": " segmenting this environment versus maybe a more complex environment. And I would say that", "tokens": [50408, 9469, 278, 341, 2823, 5717, 1310, 257, 544, 3997, 2823, 13, 400, 286, 576, 584, 300, 50652], "temperature": 0.0, "avg_logprob": -0.11120762143816267, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0029780103359371424}, {"id": 396, "seek": 218032, "start": 2186.0800000000004, "end": 2191.44, "text": " currently, this algorithm doesn't predict anything about how humans might interpret", "tokens": [50652, 4362, 11, 341, 9284, 1177, 380, 6069, 1340, 466, 577, 6255, 1062, 7302, 50920], "temperature": 0.0, "avg_logprob": -0.11120762143816267, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0029780103359371424}, {"id": 397, "seek": 218032, "start": 2192.2400000000002, "end": 2198.0, "text": " grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could", "tokens": [50960, 10748, 11317, 13, 583, 300, 311, 11, 286, 914, 11, 321, 727, 574, 412, 309, 13, 663, 311, 364, 1880, 3513, 293, 321, 727, 51248], "temperature": 0.0, "avg_logprob": -0.11120762143816267, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0029780103359371424}, {"id": 398, "seek": 218032, "start": 2198.0, "end": 2204.48, "text": " look at it in the future. And I think some of my work in part one would potentially address that", "tokens": [51248, 574, 412, 309, 294, 264, 2027, 13, 400, 286, 519, 512, 295, 452, 589, 294, 644, 472, 576, 7263, 2985, 300, 51572], "temperature": 0.0, "avg_logprob": -0.11120762143816267, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.0029780103359371424}, {"id": 399, "seek": 220448, "start": 2204.48, "end": 2212.96, "text": " going forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli.", "tokens": [50364, 516, 2128, 13, 1057, 558, 13, 3087, 1168, 490, 16943, 31304, 1560, 13, 509, 393, 41445, 1803, 11, 16943, 13, 50788], "temperature": 0.0, "avg_logprob": -0.21572067488485308, "compression_ratio": 1.3544303797468353, "no_speech_prob": 0.0033177242148667574}, {"id": 400, "seek": 220448, "start": 2216.56, "end": 2218.48, "text": " Yeah. Hi. Can you hear me? Yes.", "tokens": [50968, 865, 13, 2421, 13, 1664, 291, 1568, 385, 30, 1079, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21572067488485308, "compression_ratio": 1.3544303797468353, "no_speech_prob": 0.0033177242148667574}, {"id": 401, "seek": 220448, "start": 2222.0, "end": 2229.28, "text": " Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird.", "tokens": [51240, 2425, 13, 1079, 11, 286, 393, 1568, 291, 13, 4919, 13, 1033, 13, 1033, 13, 8561, 13, 1079, 13, 4919, 13, 1222, 4705, 311, 257, 707, 3657, 13, 51604], "temperature": 0.0, "avg_logprob": -0.21572067488485308, "compression_ratio": 1.3544303797468353, "no_speech_prob": 0.0033177242148667574}, {"id": 402, "seek": 222928, "start": 2230.1600000000003, "end": 2235.44, "text": " Great talk. Can you talk a little bit more about how your model handles time?", "tokens": [50408, 3769, 751, 13, 1664, 291, 751, 257, 707, 857, 544, 466, 577, 428, 2316, 18722, 565, 30, 50672], "temperature": 0.0, "avg_logprob": -0.10784645875295003, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.002395839663222432}, {"id": 403, "seek": 222928, "start": 2236.0800000000004, "end": 2243.1200000000003, "text": " Or I think you mentioned that it was able to handle like replay of different sequences of", "tokens": [50704, 1610, 286, 519, 291, 2835, 300, 309, 390, 1075, 281, 4813, 411, 23836, 295, 819, 22978, 295, 51056], "temperature": 0.0, "avg_logprob": -0.10784645875295003, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.002395839663222432}, {"id": 404, "seek": 222928, "start": 2243.1200000000003, "end": 2252.96, "text": " states through some map. How would it be able to handle different trajectories through the same", "tokens": [51056, 4368, 807, 512, 4471, 13, 1012, 576, 309, 312, 1075, 281, 4813, 819, 18257, 2083, 807, 264, 912, 51548], "temperature": 0.0, "avg_logprob": -0.10784645875295003, "compression_ratio": 1.5380116959064327, "no_speech_prob": 0.002395839663222432}, {"id": 405, "seek": 225296, "start": 2253.04, "end": 2261.12, "text": " space that might activate play cells in different sequences? So the version of the model that I", "tokens": [50368, 1901, 300, 1062, 13615, 862, 5438, 294, 819, 22978, 30, 407, 264, 3037, 295, 264, 2316, 300, 286, 50772], "temperature": 0.0, "avg_logprob": -0.1827373504638672, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.0010160824749618769}, {"id": 406, "seek": 225296, "start": 2261.12, "end": 2268.32, "text": " presented, that is trained on discrete patterns, right? So the sequences here, the sequences which", "tokens": [50772, 8212, 11, 300, 307, 8895, 322, 27706, 8294, 11, 558, 30, 407, 264, 22978, 510, 11, 264, 22978, 597, 51132], "temperature": 0.0, "avg_logprob": -0.1827373504638672, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.0010160824749618769}, {"id": 407, "seek": 225296, "start": 2268.32, "end": 2273.44, "text": " are trained on here through heterospecific learning are composed of these discrete patterns. So if I", "tokens": [51132, 366, 8895, 322, 510, 807, 20789, 329, 494, 22856, 2539, 366, 18204, 295, 613, 27706, 8294, 13, 407, 498, 286, 51388], "temperature": 0.0, "avg_logprob": -0.1827373504638672, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.0010160824749618769}, {"id": 408, "seek": 225296, "start": 2273.44, "end": 2280.16, "text": " say the sequences of length 500, there are 500 discrete patterns in that sequence. And when you", "tokens": [51388, 584, 264, 22978, 295, 4641, 5923, 11, 456, 366, 5923, 27706, 8294, 294, 300, 8310, 13, 400, 562, 291, 51724], "temperature": 0.0, "avg_logprob": -0.1827373504638672, "compression_ratio": 1.8271028037383177, "no_speech_prob": 0.0010160824749618769}, {"id": 409, "seek": 228016, "start": 2280.24, "end": 2286.64, "text": " say time, time is something continuous. And so we have worked on extrapolating this model to", "tokens": [50368, 584, 565, 11, 565, 307, 746, 10957, 13, 400, 370, 321, 362, 2732, 322, 48224, 990, 341, 2316, 281, 50688], "temperature": 0.0, "avg_logprob": -0.09188795543852307, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0018372471677139401}, {"id": 410, "seek": 228016, "start": 2286.64, "end": 2290.64, "text": " more continuous domains. And in a continuous domain, what would happen is that your sequence,", "tokens": [50688, 544, 10957, 25514, 13, 400, 294, 257, 10957, 9274, 11, 437, 576, 1051, 307, 300, 428, 8310, 11, 50888], "temperature": 0.0, "avg_logprob": -0.09188795543852307, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0018372471677139401}, {"id": 411, "seek": 228016, "start": 2290.64, "end": 2295.92, "text": " instead of being discrete patterns, would be composed of these continuous stream. And in that", "tokens": [50888, 2602, 295, 885, 27706, 8294, 11, 576, 312, 18204, 295, 613, 10957, 4309, 13, 400, 294, 300, 51152], "temperature": 0.0, "avg_logprob": -0.09188795543852307, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0018372471677139401}, {"id": 412, "seek": 228016, "start": 2295.92, "end": 2300.64, "text": " case, we haven't explored what the network performance would be, but that's something", "tokens": [51152, 1389, 11, 321, 2378, 380, 24016, 437, 264, 3209, 3389, 576, 312, 11, 457, 300, 311, 746, 51388], "temperature": 0.0, "avg_logprob": -0.09188795543852307, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0018372471677139401}, {"id": 413, "seek": 228016, "start": 2300.64, "end": 2307.68, "text": " ongoing. And definitely in 2D spaces, we would like to train this model on 2D sequences and then", "tokens": [51388, 10452, 13, 400, 2138, 294, 568, 35, 7673, 11, 321, 576, 411, 281, 3847, 341, 2316, 322, 568, 35, 22978, 293, 550, 51740], "temperature": 0.0, "avg_logprob": -0.09188795543852307, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.0018372471677139401}, {"id": 414, "seek": 230768, "start": 2307.68, "end": 2313.12, "text": " see, so right now, these results are pertaining to 1D sequences. But we do want to extrapolate", "tokens": [50364, 536, 11, 370, 558, 586, 11, 613, 3542, 366, 49582, 281, 502, 35, 22978, 13, 583, 321, 360, 528, 281, 48224, 473, 50636], "temperature": 0.0, "avg_logprob": -0.12438155636929049, "compression_ratio": 1.5896414342629481, "no_speech_prob": 0.0009844090091064572}, {"id": 415, "seek": 230768, "start": 2313.68, "end": 2319.6, "text": " the results and train this model on 2D sequences to see how it performs in 2D. And I think that would", "tokens": [50664, 264, 3542, 293, 3847, 341, 2316, 322, 568, 35, 22978, 281, 536, 577, 309, 26213, 294, 568, 35, 13, 400, 286, 519, 300, 576, 50960], "temperature": 0.0, "avg_logprob": -0.12438155636929049, "compression_ratio": 1.5896414342629481, "no_speech_prob": 0.0009844090091064572}, {"id": 416, "seek": 230768, "start": 2320.3999999999996, "end": 2325.44, "text": " potentially address then your questions about time, because that pertains to continuous domains.", "tokens": [51000, 7263, 2985, 550, 428, 1651, 466, 565, 11, 570, 300, 13269, 2315, 281, 10957, 25514, 13, 51252], "temperature": 0.0, "avg_logprob": -0.12438155636929049, "compression_ratio": 1.5896414342629481, "no_speech_prob": 0.0009844090091064572}, {"id": 417, "seek": 230768, "start": 2326.16, "end": 2327.12, "text": " Okay, yeah, thank you.", "tokens": [51288, 1033, 11, 1338, 11, 1309, 291, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12438155636929049, "compression_ratio": 1.5896414342629481, "no_speech_prob": 0.0009844090091064572}, {"id": 418, "seek": 230768, "start": 2331.12, "end": 2335.2, "text": " Okay, we've got a big stream of questions coming in. The first is from Adam Eisen.", "tokens": [51536, 1033, 11, 321, 600, 658, 257, 955, 4309, 295, 1651, 1348, 294, 13, 440, 700, 307, 490, 7938, 35619, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12438155636929049, "compression_ratio": 1.5896414342629481, "no_speech_prob": 0.0009844090091064572}, {"id": 419, "seek": 233520, "start": 2335.8399999999997, "end": 2339.2799999999997, "text": " Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map", "tokens": [50396, 2561, 337, 257, 10343, 751, 13, 3560, 291, 1194, 412, 439, 466, 577, 341, 8388, 337, 1422, 12, 24223, 50568], "temperature": 0.0, "avg_logprob": -0.10530161606638055, "compression_ratio": 1.540084388185654, "no_speech_prob": 0.0009248406277038157}, {"id": 420, "seek": 233520, "start": 2339.2799999999997, "end": 2344.0, "text": " segmentation and topological association could be extended to non-spatial domains?", "tokens": [50568, 9469, 399, 293, 1192, 4383, 14598, 727, 312, 10913, 281, 2107, 12, 4952, 267, 831, 25514, 30, 50804], "temperature": 0.0, "avg_logprob": -0.10530161606638055, "compression_ratio": 1.540084388185654, "no_speech_prob": 0.0009248406277038157}, {"id": 421, "seek": 233520, "start": 2345.4399999999996, "end": 2352.8799999999997, "text": " That's a very good question. So in FeedLab, a small group of us have been thinking about how", "tokens": [50876, 663, 311, 257, 588, 665, 1168, 13, 407, 294, 33720, 37880, 11, 257, 1359, 1594, 295, 505, 362, 668, 1953, 466, 577, 51248], "temperature": 0.0, "avg_logprob": -0.10530161606638055, "compression_ratio": 1.540084388185654, "no_speech_prob": 0.0009248406277038157}, {"id": 422, "seek": 233520, "start": 2352.8799999999997, "end": 2359.3599999999997, "text": " this could be extended. And so the idea is, I'm just going to go back to, let's see if I can just", "tokens": [51248, 341, 727, 312, 10913, 13, 400, 370, 264, 1558, 307, 11, 286, 478, 445, 516, 281, 352, 646, 281, 11, 718, 311, 536, 498, 286, 393, 445, 51572], "temperature": 0.0, "avg_logprob": -0.10530161606638055, "compression_ratio": 1.540084388185654, "no_speech_prob": 0.0009248406277038157}, {"id": 423, "seek": 235936, "start": 2359.44, "end": 2365.1200000000003, "text": " quickly hop back to my theoretical framework slide. So really, we're building up on this", "tokens": [50368, 2661, 3818, 646, 281, 452, 20864, 8388, 4137, 13, 407, 534, 11, 321, 434, 2390, 493, 322, 341, 50652], "temperature": 0.0, "avg_logprob": -0.12634353102924667, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0010478015756234527}, {"id": 424, "seek": 235936, "start": 2365.1200000000003, "end": 2369.2000000000003, "text": " theoretical framework. And when we're thinking about non-spatial domains, we go back to this", "tokens": [50652, 20864, 8388, 13, 400, 562, 321, 434, 1953, 466, 2107, 12, 4952, 267, 831, 25514, 11, 321, 352, 646, 281, 341, 50856], "temperature": 0.0, "avg_logprob": -0.12634353102924667, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0010478015756234527}, {"id": 425, "seek": 235936, "start": 2370.0, "end": 2375.6, "text": " schematic picture. And here, basically, what we're saying is, within the spatial domain, I said that", "tokens": [50896, 44739, 3036, 13, 400, 510, 11, 1936, 11, 437, 321, 434, 1566, 307, 11, 1951, 264, 23598, 9274, 11, 286, 848, 300, 51176], "temperature": 0.0, "avg_logprob": -0.12634353102924667, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0010478015756234527}, {"id": 426, "seek": 235936, "start": 2375.6, "end": 2379.92, "text": " large changes in contextual input can drive remapping in place cells, which would drive", "tokens": [51176, 2416, 2962, 294, 35526, 4846, 393, 3332, 890, 10534, 294, 1081, 5438, 11, 597, 576, 3332, 51392], "temperature": 0.0, "avg_logprob": -0.12634353102924667, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0010478015756234527}, {"id": 427, "seek": 235936, "start": 2380.48, "end": 2388.56, "text": " remapping in the grid coding space. So similarly, in relational domains or in discrete domains,", "tokens": [51420, 890, 10534, 294, 264, 10748, 17720, 1901, 13, 407, 14138, 11, 294, 38444, 25514, 420, 294, 27706, 25514, 11, 51824], "temperature": 0.0, "avg_logprob": -0.12634353102924667, "compression_ratio": 1.806201550387597, "no_speech_prob": 0.0010478015756234527}, {"id": 428, "seek": 238936, "start": 2389.84, "end": 2394.56, "text": " as we're thinking about it, we are referring back to this schematic picture. And we think that", "tokens": [50388, 382, 321, 434, 1953, 466, 309, 11, 321, 366, 13761, 646, 281, 341, 44739, 3036, 13, 400, 321, 519, 300, 50624], "temperature": 0.0, "avg_logprob": -0.1073608820417286, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.00025706517044454813}, {"id": 429, "seek": 238936, "start": 2395.6, "end": 2401.44, "text": " the phenomenon of remapping actually would enable us to encode these non-eclidean relationships.", "tokens": [50676, 264, 14029, 295, 890, 10534, 767, 576, 9528, 505, 281, 2058, 1429, 613, 2107, 12, 3045, 31264, 282, 6159, 13, 50968], "temperature": 0.0, "avg_logprob": -0.1073608820417286, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.00025706517044454813}, {"id": 430, "seek": 238936, "start": 2401.44, "end": 2406.32, "text": " So first thing is that here, we're suggesting topological relationships. So that already", "tokens": [50968, 407, 700, 551, 307, 300, 510, 11, 321, 434, 18094, 1192, 4383, 6159, 13, 407, 300, 1217, 51212], "temperature": 0.0, "avg_logprob": -0.1073608820417286, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.00025706517044454813}, {"id": 431, "seek": 238936, "start": 2406.32, "end": 2411.28, "text": " enables us to encode non-eclidean relationships. But this phenomenon of remapping also allows", "tokens": [51212, 17077, 505, 281, 2058, 1429, 2107, 12, 3045, 31264, 282, 6159, 13, 583, 341, 14029, 295, 890, 10534, 611, 4045, 51460], "temperature": 0.0, "avg_logprob": -0.1073608820417286, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.00025706517044454813}, {"id": 432, "seek": 238936, "start": 2411.28, "end": 2416.88, "text": " us to make jumps. So if you think about a family tree, if you might try to encode it in an", "tokens": [51460, 505, 281, 652, 16704, 13, 407, 498, 291, 519, 466, 257, 1605, 4230, 11, 498, 291, 1062, 853, 281, 2058, 1429, 309, 294, 364, 51740], "temperature": 0.0, "avg_logprob": -0.1073608820417286, "compression_ratio": 1.9135802469135803, "no_speech_prob": 0.00025706517044454813}, {"id": 433, "seek": 241688, "start": 2416.96, "end": 2422.1600000000003, "text": " eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact,", "tokens": [50368, 11437, 31264, 282, 1901, 11, 550, 291, 603, 362, 19807, 13, 400, 309, 311, 588, 2252, 281, 2058, 1429, 309, 13, 682, 1186, 11, 50628], "temperature": 0.0, "avg_logprob": -0.07201902275411491, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0006981798214837909}, {"id": 434, "seek": 241688, "start": 2422.1600000000003, "end": 2428.48, "text": " almost impossible to encode it in an eclidean space. But then if we provide a framework where", "tokens": [50628, 1920, 6243, 281, 2058, 1429, 309, 294, 364, 11437, 31264, 282, 1901, 13, 583, 550, 498, 321, 2893, 257, 8388, 689, 50944], "temperature": 0.0, "avg_logprob": -0.07201902275411491, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0006981798214837909}, {"id": 435, "seek": 241688, "start": 2428.48, "end": 2433.2000000000003, "text": " we allow topological representations and we allow jumps in this eclidean space through this phenomenon", "tokens": [50944, 321, 2089, 1192, 4383, 33358, 293, 321, 2089, 16704, 294, 341, 11437, 31264, 282, 1901, 807, 341, 14029, 51180], "temperature": 0.0, "avg_logprob": -0.07201902275411491, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0006981798214837909}, {"id": 436, "seek": 241688, "start": 2433.2000000000003, "end": 2437.92, "text": " of remapping through place cells, then that can allow us to potentially encode non-eclidean", "tokens": [51180, 295, 890, 10534, 807, 1081, 5438, 11, 550, 300, 393, 2089, 505, 281, 7263, 2058, 1429, 2107, 12, 3045, 31264, 282, 51416], "temperature": 0.0, "avg_logprob": -0.07201902275411491, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0006981798214837909}, {"id": 437, "seek": 241688, "start": 2437.92, "end": 2442.32, "text": " relationships. And that's the way we are thinking. And this is still work in progress. And we", "tokens": [51416, 6159, 13, 400, 300, 311, 264, 636, 321, 366, 1953, 13, 400, 341, 307, 920, 589, 294, 4205, 13, 400, 321, 51636], "temperature": 0.0, "avg_logprob": -0.07201902275411491, "compression_ratio": 1.7849056603773585, "no_speech_prob": 0.0006981798214837909}, {"id": 438, "seek": 244232, "start": 2442.32, "end": 2453.1200000000003, "text": " actively think about it. Next question from Nancy Camusher. How does your system decide when and", "tokens": [50364, 13022, 519, 466, 309, 13, 3087, 1168, 490, 18154, 6886, 301, 511, 13, 1012, 775, 428, 1185, 4536, 562, 293, 50904], "temperature": 0.0, "avg_logprob": -0.16338864494772518, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0010982590029016137}, {"id": 439, "seek": 244232, "start": 2453.1200000000003, "end": 2457.36, "text": " where to carve the world in the sub-maps, especially in non-built environments where the", "tokens": [50904, 689, 281, 33832, 264, 1002, 294, 264, 1422, 12, 76, 2382, 11, 2318, 294, 2107, 12, 23018, 12388, 689, 264, 51116], "temperature": 0.0, "avg_logprob": -0.16338864494772518, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0010982590029016137}, {"id": 440, "seek": 244232, "start": 2457.36, "end": 2468.0800000000004, "text": " divisions may be less obvious? So at least in part two, where I talk about fragmentation of", "tokens": [51116, 24328, 815, 312, 1570, 6322, 30, 407, 412, 1935, 294, 644, 732, 11, 689, 286, 751, 466, 9241, 19631, 295, 51652], "temperature": 0.0, "avg_logprob": -0.16338864494772518, "compression_ratio": 1.4427083333333333, "no_speech_prob": 0.0010982590029016137}, {"id": 441, "seek": 246808, "start": 2468.08, "end": 2477.12, "text": " maps into sub-maps. Let me just go back. So in these kinds of environments, we basically describe", "tokens": [50364, 11317, 666, 1422, 12, 76, 2382, 13, 961, 385, 445, 352, 646, 13, 407, 294, 613, 3685, 295, 12388, 11, 321, 1936, 6786, 50816], "temperature": 0.0, "avg_logprob": -0.07869896563616666, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.003591065062209964}, {"id": 442, "seek": 246808, "start": 2477.12, "end": 2482.16, "text": " the principle of contiguous regions, where we have suggested that when you are navigating through", "tokens": [50816, 264, 8665, 295, 660, 30525, 10682, 11, 689, 321, 362, 10945, 300, 562, 291, 366, 32054, 807, 51068], "temperature": 0.0, "avg_logprob": -0.07869896563616666, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.003591065062209964}, {"id": 443, "seek": 246808, "start": 2482.16, "end": 2487.92, "text": " this environment, if you are in an environment where your visual observations stay more or less", "tokens": [51068, 341, 2823, 11, 498, 291, 366, 294, 364, 2823, 689, 428, 5056, 18163, 1754, 544, 420, 1570, 51356], "temperature": 0.0, "avg_logprob": -0.07869896563616666, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.003591065062209964}, {"id": 444, "seek": 246808, "start": 2487.92, "end": 2494.0, "text": " consistent, then you will keep extending a map. But when you go from this region to another region,", "tokens": [51356, 8398, 11, 550, 291, 486, 1066, 24360, 257, 4471, 13, 583, 562, 291, 352, 490, 341, 4458, 281, 1071, 4458, 11, 51660], "temperature": 0.0, "avg_logprob": -0.07869896563616666, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.003591065062209964}, {"id": 445, "seek": 249400, "start": 2494.0, "end": 2499.04, "text": " let's say you navigate from here to here, where your visual information here is completely distinct", "tokens": [50364, 718, 311, 584, 291, 12350, 490, 510, 281, 510, 11, 689, 428, 5056, 1589, 510, 307, 2584, 10644, 50616], "temperature": 0.0, "avg_logprob": -0.09911765371050153, "compression_ratio": 1.9793388429752066, "no_speech_prob": 0.004129380919039249}, {"id": 446, "seek": 249400, "start": 2499.04, "end": 2503.6, "text": " from your visual information here, then in that case, you will decide to segment the space. And", "tokens": [50616, 490, 428, 5056, 1589, 510, 11, 550, 294, 300, 1389, 11, 291, 486, 4536, 281, 9469, 264, 1901, 13, 400, 50844], "temperature": 0.0, "avg_logprob": -0.09911765371050153, "compression_ratio": 1.9793388429752066, "no_speech_prob": 0.004129380919039249}, {"id": 447, "seek": 249400, "start": 2504.16, "end": 2509.04, "text": " that's what we are predicting. We're predicting that in this case, when you travel from one", "tokens": [50872, 300, 311, 437, 321, 366, 32884, 13, 492, 434, 32884, 300, 294, 341, 1389, 11, 562, 291, 3147, 490, 472, 51116], "temperature": 0.0, "avg_logprob": -0.09911765371050153, "compression_ratio": 1.9793388429752066, "no_speech_prob": 0.004129380919039249}, {"id": 448, "seek": 249400, "start": 2509.04, "end": 2514.48, "text": " contiguous region to another, you will segment the space. And that's how you divide the space.", "tokens": [51116, 660, 30525, 4458, 281, 1071, 11, 291, 486, 9469, 264, 1901, 13, 400, 300, 311, 577, 291, 9845, 264, 1901, 13, 51388], "temperature": 0.0, "avg_logprob": -0.09911765371050153, "compression_ratio": 1.9793388429752066, "no_speech_prob": 0.004129380919039249}, {"id": 449, "seek": 249400, "start": 2514.48, "end": 2519.92, "text": " So that's one principle, which we are describing for map segmentation. And that might not be the", "tokens": [51388, 407, 300, 311, 472, 8665, 11, 597, 321, 366, 16141, 337, 4471, 9469, 399, 13, 400, 300, 1062, 406, 312, 264, 51660], "temperature": 0.0, "avg_logprob": -0.09911765371050153, "compression_ratio": 1.9793388429752066, "no_speech_prob": 0.004129380919039249}, {"id": 450, "seek": 251992, "start": 2519.92, "end": 2525.84, "text": " only principle, but that principle explains some of the experimental results that have been", "tokens": [50364, 787, 8665, 11, 457, 300, 8665, 13948, 512, 295, 264, 17069, 3542, 300, 362, 668, 50660], "temperature": 0.0, "avg_logprob": -0.08762901961201369, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0009103473857976496}, {"id": 451, "seek": 251992, "start": 2525.84, "end": 2530.64, "text": " observed newly. And there might be another principle, for instance, path integration error,", "tokens": [50660, 13095, 15109, 13, 400, 456, 1062, 312, 1071, 8665, 11, 337, 5197, 11, 3100, 10980, 6713, 11, 50900], "temperature": 0.0, "avg_logprob": -0.08762901961201369, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0009103473857976496}, {"id": 452, "seek": 251992, "start": 2530.64, "end": 2536.48, "text": " where if your path integration builds up and it reaches a certain threshold amount,", "tokens": [50900, 689, 498, 428, 3100, 10980, 15182, 493, 293, 309, 14235, 257, 1629, 14678, 2372, 11, 51192], "temperature": 0.0, "avg_logprob": -0.08762901961201369, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0009103473857976496}, {"id": 453, "seek": 251992, "start": 2536.48, "end": 2542.48, "text": " then you automatically might start a new map. But I haven't illustrated that here, but that's", "tokens": [51192, 550, 291, 6772, 1062, 722, 257, 777, 4471, 13, 583, 286, 2378, 380, 33875, 300, 510, 11, 457, 300, 311, 51492], "temperature": 0.0, "avg_logprob": -0.08762901961201369, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0009103473857976496}, {"id": 454, "seek": 251992, "start": 2542.48, "end": 2547.12, "text": " also one of the other driving forces for us to actually split a space into multiple sub-maps", "tokens": [51492, 611, 472, 295, 264, 661, 4840, 5874, 337, 505, 281, 767, 7472, 257, 1901, 666, 3866, 1422, 12, 76, 2382, 51724], "temperature": 0.0, "avg_logprob": -0.08762901961201369, "compression_ratio": 1.7132075471698114, "no_speech_prob": 0.0009103473857976496}, {"id": 455, "seek": 254712, "start": 2547.12, "end": 2558.7999999999997, "text": " so that we can have more efficient representations. Okay, next question from Chen.", "tokens": [50364, 370, 300, 321, 393, 362, 544, 7148, 33358, 13, 1033, 11, 958, 1168, 490, 13682, 13, 50948], "temperature": 0.0, "avg_logprob": -0.17699232737223308, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.0008263536728918552}, {"id": 456, "seek": 254712, "start": 2559.92, "end": 2563.44, "text": " I was wondering if you can give some intuition about the mechanism for composition of", "tokens": [51004, 286, 390, 6359, 498, 291, 393, 976, 512, 24002, 466, 264, 7513, 337, 12686, 295, 51180], "temperature": 0.0, "avg_logprob": -0.17699232737223308, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.0008263536728918552}, {"id": 457, "seek": 254712, "start": 2563.44, "end": 2568.48, "text": " sub-maps. So really, this is from the happy and learning mechanism. And secondly,", "tokens": [51180, 1422, 12, 76, 2382, 13, 407, 534, 11, 341, 307, 490, 264, 2055, 293, 2539, 7513, 13, 400, 26246, 11, 51432], "temperature": 0.0, "avg_logprob": -0.17699232737223308, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.0008263536728918552}, {"id": 458, "seek": 254712, "start": 2568.48, "end": 2571.6, "text": " can you comment on the relationship between this and the Hopfield mechanism?", "tokens": [51432, 393, 291, 2871, 322, 264, 2480, 1296, 341, 293, 264, 13438, 7610, 7513, 30, 51588], "temperature": 0.0, "avg_logprob": -0.17699232737223308, "compression_ratio": 1.5424528301886793, "no_speech_prob": 0.0008263536728918552}, {"id": 459, "seek": 257160, "start": 2571.6, "end": 2587.6, "text": " So since we're talking about composition of sub-maps here, in this case, in this model,", "tokens": [50364, 407, 1670, 321, 434, 1417, 466, 12686, 295, 1422, 12, 76, 2382, 510, 11, 294, 341, 1389, 11, 294, 341, 2316, 11, 51164], "temperature": 0.0, "avg_logprob": -0.15160391987233923, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.0008164526661857963}, {"id": 460, "seek": 257160, "start": 2587.6, "end": 2593.2, "text": " what I'm suggesting is that we might end up forming local sub-maps by using the", "tokens": [51164, 437, 286, 478, 18094, 307, 300, 321, 1062, 917, 493, 15745, 2654, 1422, 12, 76, 2382, 538, 1228, 264, 51444], "temperature": 0.0, "avg_logprob": -0.15160391987233923, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.0008164526661857963}, {"id": 461, "seek": 257160, "start": 2593.2, "end": 2598.72, "text": " phenomenon of premapping. So here I'm suggesting, again, if you have large changes in contextual", "tokens": [51444, 14029, 295, 5624, 10534, 13, 407, 510, 286, 478, 18094, 11, 797, 11, 498, 291, 362, 2416, 2962, 294, 35526, 51720], "temperature": 0.0, "avg_logprob": -0.15160391987233923, "compression_ratio": 1.543859649122807, "no_speech_prob": 0.0008164526661857963}, {"id": 462, "seek": 259872, "start": 2598.72, "end": 2606.08, "text": " input, so for instance, let me just go back to the slide, which kind of connects this.", "tokens": [50364, 4846, 11, 370, 337, 5197, 11, 718, 385, 445, 352, 646, 281, 264, 4137, 11, 597, 733, 295, 16967, 341, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1068918228149414, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.0008691629627719522}, {"id": 463, "seek": 259872, "start": 2607.52, "end": 2615.04, "text": " So here we basically suggested how this environment can be segmented into different", "tokens": [50804, 407, 510, 321, 1936, 10945, 577, 341, 2823, 393, 312, 9469, 292, 666, 819, 51180], "temperature": 0.0, "avg_logprob": -0.1068918228149414, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.0008691629627719522}, {"id": 464, "seek": 259872, "start": 2615.04, "end": 2621.3599999999997, "text": " sub-maps. And on the mechanistic level, I'm trying to suggest that any large contextual input", "tokens": [51180, 1422, 12, 76, 2382, 13, 400, 322, 264, 4236, 3142, 1496, 11, 286, 478, 1382, 281, 3402, 300, 604, 2416, 35526, 4846, 51496], "temperature": 0.0, "avg_logprob": -0.1068918228149414, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.0008691629627719522}, {"id": 465, "seek": 259872, "start": 2621.3599999999997, "end": 2626.7999999999997, "text": " is going to cause remapping in place cells, which means the cells which are firing would be", "tokens": [51496, 307, 516, 281, 3082, 890, 10534, 294, 1081, 5438, 11, 597, 1355, 264, 5438, 597, 366, 16045, 576, 312, 51768], "temperature": 0.0, "avg_logprob": -0.1068918228149414, "compression_ratio": 1.575221238938053, "no_speech_prob": 0.0008691629627719522}, {"id": 466, "seek": 262872, "start": 2628.9599999999996, "end": 2635.3599999999997, "text": " changed and they would represent a new map. And this remapping would actually enable also", "tokens": [50376, 3105, 293, 436, 576, 2906, 257, 777, 4471, 13, 400, 341, 890, 10534, 576, 767, 9528, 611, 50696], "temperature": 0.0, "avg_logprob": -0.11378248532613118, "compression_ratio": 1.800995024875622, "no_speech_prob": 0.001956232590600848}, {"id": 467, "seek": 262872, "start": 2635.3599999999997, "end": 2640.7999999999997, "text": " remapping in grid cells through associative learning because each place cell representation", "tokens": [50696, 890, 10534, 294, 10748, 5438, 807, 4180, 1166, 2539, 570, 1184, 1081, 2815, 10290, 50968], "temperature": 0.0, "avg_logprob": -0.11378248532613118, "compression_ratio": 1.800995024875622, "no_speech_prob": 0.001956232590600848}, {"id": 468, "seek": 262872, "start": 2640.7999999999997, "end": 2647.2, "text": " is associated with a certain grid cell representation. So when you have changes in", "tokens": [50968, 307, 6615, 365, 257, 1629, 10748, 2815, 10290, 13, 407, 562, 291, 362, 2962, 294, 51288], "temperature": 0.0, "avg_logprob": -0.11378248532613118, "compression_ratio": 1.800995024875622, "no_speech_prob": 0.001956232590600848}, {"id": 469, "seek": 262872, "start": 2647.2, "end": 2651.6, "text": " firing fields of place cells, that also triggers certain corresponding changes in the grid coding", "tokens": [51288, 16045, 7909, 295, 1081, 5438, 11, 300, 611, 22827, 1629, 11760, 2962, 294, 264, 10748, 17720, 51508], "temperature": 0.0, "avg_logprob": -0.11378248532613118, "compression_ratio": 1.800995024875622, "no_speech_prob": 0.001956232590600848}, {"id": 470, "seek": 265160, "start": 2651.6, "end": 2659.44, "text": " phase, which is basically called remapping in grid phase. And so here I'm interpreting,", "tokens": [50364, 5574, 11, 597, 307, 1936, 1219, 890, 10534, 294, 10748, 5574, 13, 400, 370, 510, 286, 478, 37395, 11, 50756], "temperature": 0.0, "avg_logprob": -0.07877205138982729, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0015008804621174932}, {"id": 471, "seek": 265160, "start": 2659.44, "end": 2664.4, "text": " when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you", "tokens": [50756, 562, 286, 574, 412, 452, 9284, 299, 2316, 293, 286, 574, 412, 452, 4236, 3142, 2316, 11, 286, 478, 1566, 300, 562, 291, 51004], "temperature": 0.0, "avg_logprob": -0.07877205138982729, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0015008804621174932}, {"id": 472, "seek": 265160, "start": 2664.4, "end": 2669.92, "text": " go from one contiguous region to another, that actually corresponds to a large contextual change", "tokens": [51004, 352, 490, 472, 660, 30525, 4458, 281, 1071, 11, 300, 767, 23249, 281, 257, 2416, 35526, 1319, 51280], "temperature": 0.0, "avg_logprob": -0.07877205138982729, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0015008804621174932}, {"id": 473, "seek": 265160, "start": 2669.92, "end": 2676.7999999999997, "text": " and that contextual change triggers remapping in this model and this remapping in place cells", "tokens": [51280, 293, 300, 35526, 1319, 22827, 890, 10534, 294, 341, 2316, 293, 341, 890, 10534, 294, 1081, 5438, 51624], "temperature": 0.0, "avg_logprob": -0.07877205138982729, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0015008804621174932}, {"id": 474, "seek": 267680, "start": 2676.8, "end": 2681.84, "text": " then triggers remapping in grid cells phase. And that corresponds to the formation of local", "tokens": [50364, 550, 22827, 890, 10534, 294, 10748, 5438, 5574, 13, 400, 300, 23249, 281, 264, 11723, 295, 2654, 50616], "temperature": 0.0, "avg_logprob": -0.1108543596769634, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.000842481735162437}, {"id": 475, "seek": 267680, "start": 2681.84, "end": 2687.1200000000003, "text": " sub-maps. And these topological connections from place cells to themselves might actually", "tokens": [50616, 1422, 12, 76, 2382, 13, 400, 613, 1192, 4383, 9271, 490, 1081, 5438, 281, 2969, 1062, 767, 50880], "temperature": 0.0, "avg_logprob": -0.1108543596769634, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.000842481735162437}, {"id": 476, "seek": 267680, "start": 2687.76, "end": 2692.8, "text": " represent how these local sub-maps are connected to each other. And there's also,", "tokens": [50912, 2906, 577, 613, 2654, 1422, 12, 76, 2382, 366, 4582, 281, 1184, 661, 13, 400, 456, 311, 611, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1108543596769634, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.000842481735162437}, {"id": 477, "seek": 267680, "start": 2693.44, "end": 2696.8, "text": " going back to the anatomy, there's also a possibility of splitting this population", "tokens": [51196, 516, 646, 281, 264, 31566, 11, 456, 311, 611, 257, 7959, 295, 30348, 341, 4415, 51364], "temperature": 0.0, "avg_logprob": -0.1108543596769634, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.000842481735162437}, {"id": 478, "seek": 267680, "start": 2696.8, "end": 2701.6000000000004, "text": " actually into different populations. One that might just encode conjunctive representations and", "tokens": [51364, 767, 666, 819, 12822, 13, 1485, 300, 1062, 445, 2058, 1429, 18244, 20221, 33358, 293, 51604], "temperature": 0.0, "avg_logprob": -0.1108543596769634, "compression_ratio": 1.8040816326530613, "no_speech_prob": 0.000842481735162437}, {"id": 479, "seek": 270160, "start": 2701.6, "end": 2708.7999999999997, "text": " another one that might have these recurrent topological connections similar to CA1 and", "tokens": [50364, 1071, 472, 300, 1062, 362, 613, 18680, 1753, 1192, 4383, 9271, 2531, 281, 22852, 16, 293, 50724], "temperature": 0.0, "avg_logprob": -0.1976622801560622, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00032484999974258244}, {"id": 480, "seek": 270160, "start": 2708.7999999999997, "end": 2713.36, "text": " CA3 distinction in anatomy. But that's how I'm thinking about it at the moment.", "tokens": [50724, 22852, 18, 16844, 294, 31566, 13, 583, 300, 311, 577, 286, 478, 1953, 466, 309, 412, 264, 1623, 13, 50952], "temperature": 0.0, "avg_logprob": -0.1976622801560622, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00032484999974258244}, {"id": 481, "seek": 270160, "start": 2723.2799999999997, "end": 2729.7599999999998, "text": " I want to point out a quick addendum from Ila who said to Nancy's question, we consider online", "tokens": [51448, 286, 528, 281, 935, 484, 257, 1702, 909, 27574, 490, 286, 875, 567, 848, 281, 18154, 311, 1168, 11, 321, 1949, 2950, 51772], "temperature": 0.0, "avg_logprob": -0.1976622801560622, "compression_ratio": 1.4184782608695652, "no_speech_prob": 0.00032484999974258244}, {"id": 482, "seek": 272976, "start": 2729.76, "end": 2735.0400000000004, "text": " segmentation decisions driven by regions or points of high surprise affordance changes and PIR.", "tokens": [50364, 9469, 399, 5327, 9555, 538, 10682, 420, 2793, 295, 1090, 6365, 6157, 719, 2962, 293, 430, 7740, 13, 50628], "temperature": 0.0, "avg_logprob": -0.19246192311131677, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.0009244869579561055}, {"id": 483, "seek": 272976, "start": 2741.2000000000003, "end": 2746.6400000000003, "text": " Okay, another question from Anya, even over. Do you think the way humans form space maps depends", "tokens": [50936, 1033, 11, 1071, 1168, 490, 2639, 64, 11, 754, 670, 13, 1144, 291, 519, 264, 636, 6255, 1254, 1901, 11317, 5946, 51208], "temperature": 0.0, "avg_logprob": -0.19246192311131677, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.0009244869579561055}, {"id": 484, "seek": 272976, "start": 2746.6400000000003, "end": 2751.5200000000004, "text": " on whether their language uses directions that are egocentric left and right or allocentric", "tokens": [51208, 322, 1968, 641, 2856, 4960, 11095, 300, 366, 24263, 905, 32939, 1411, 293, 558, 420, 439, 905, 32939, 51452], "temperature": 0.0, "avg_logprob": -0.19246192311131677, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.0009244869579561055}, {"id": 485, "seek": 272976, "start": 2751.5200000000004, "end": 2759.5200000000004, "text": " north and south? That's a great question. So there were experiments in both animals as well", "tokens": [51452, 6830, 293, 7377, 30, 663, 311, 257, 869, 1168, 13, 407, 456, 645, 12050, 294, 1293, 4882, 382, 731, 51852], "temperature": 0.0, "avg_logprob": -0.19246192311131677, "compression_ratio": 1.5222672064777327, "no_speech_prob": 0.0009244869579561055}, {"id": 486, "seek": 275952, "start": 2759.52, "end": 2764.08, "text": " as humans which have shown that we actually have both kinds of representations. We have", "tokens": [50364, 382, 6255, 597, 362, 4898, 300, 321, 767, 362, 1293, 3685, 295, 33358, 13, 492, 362, 50592], "temperature": 0.0, "avg_logprob": -0.13673501790956008, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0007669563638046384}, {"id": 487, "seek": 275952, "start": 2764.08, "end": 2770.56, "text": " egocentric representations and we also have allocentric representations. And the representations", "tokens": [50592, 24263, 905, 32939, 33358, 293, 321, 611, 362, 439, 905, 32939, 33358, 13, 400, 264, 33358, 50916], "temperature": 0.0, "avg_logprob": -0.13673501790956008, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0007669563638046384}, {"id": 488, "seek": 275952, "start": 2770.56, "end": 2779.36, "text": " in the hippocampal inter-animal system are usually allocentric, but the thalamus is the part of the", "tokens": [50916, 294, 264, 27745, 905, 1215, 304, 728, 12, 282, 10650, 1185, 366, 2673, 439, 905, 32939, 11, 457, 264, 258, 23819, 301, 307, 264, 644, 295, 264, 51356], "temperature": 0.0, "avg_logprob": -0.13673501790956008, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0007669563638046384}, {"id": 489, "seek": 275952, "start": 2779.36, "end": 2785.04, "text": " brain which is responsible for egocentric representations. So really, when we're navigating", "tokens": [51356, 3567, 597, 307, 6250, 337, 24263, 905, 32939, 33358, 13, 407, 534, 11, 562, 321, 434, 32054, 51640], "temperature": 0.0, "avg_logprob": -0.13673501790956008, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.0007669563638046384}, {"id": 490, "seek": 278504, "start": 2785.04, "end": 2790.4, "text": " spaces, we are probably using mixed strategies. We do use our allocentric representations,", "tokens": [50364, 7673, 11, 321, 366, 1391, 1228, 7467, 9029, 13, 492, 360, 764, 527, 439, 905, 32939, 33358, 11, 50632], "temperature": 0.0, "avg_logprob": -0.08376112154551915, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.008571401238441467}, {"id": 491, "seek": 278504, "start": 2790.4, "end": 2795.92, "text": " but in some cases we might be using egocentric representations. And that's mostly also true", "tokens": [50632, 457, 294, 512, 3331, 321, 1062, 312, 1228, 24263, 905, 32939, 33358, 13, 400, 300, 311, 5240, 611, 2074, 50908], "temperature": 0.0, "avg_logprob": -0.08376112154551915, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.008571401238441467}, {"id": 492, "seek": 278504, "start": 2795.92, "end": 2801.04, "text": " for routes which we are traversing very frequently. So if there's a route which I take every day,", "tokens": [50908, 337, 18242, 597, 321, 366, 23149, 278, 588, 10374, 13, 407, 498, 456, 311, 257, 7955, 597, 286, 747, 633, 786, 11, 51164], "temperature": 0.0, "avg_logprob": -0.08376112154551915, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.008571401238441467}, {"id": 493, "seek": 278504, "start": 2801.04, "end": 2807.7599999999998, "text": " let's say going from my home to BCS, then that converges to root learning. But if I've had very", "tokens": [51164, 718, 311, 584, 516, 490, 452, 1280, 281, 14359, 50, 11, 550, 300, 9652, 2880, 281, 5593, 2539, 13, 583, 498, 286, 600, 632, 588, 51500], "temperature": 0.0, "avg_logprob": -0.08376112154551915, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.008571401238441467}, {"id": 494, "seek": 278504, "start": 2807.7599999999998, "end": 2813.2, "text": " less experience in any environment, then I'm mostly using allocentric representations to", "tokens": [51500, 1570, 1752, 294, 604, 2823, 11, 550, 286, 478, 5240, 1228, 439, 905, 32939, 33358, 281, 51772], "temperature": 0.0, "avg_logprob": -0.08376112154551915, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.008571401238441467}, {"id": 495, "seek": 281320, "start": 2813.2, "end": 2817.6, "text": " actually make my way through that environment. And hence, allocentric representations are", "tokens": [50364, 767, 652, 452, 636, 807, 300, 2823, 13, 400, 16678, 11, 439, 905, 32939, 33358, 366, 50584], "temperature": 0.0, "avg_logprob": -0.08561350374805685, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0007790583185851574}, {"id": 496, "seek": 281320, "start": 2817.6, "end": 2825.12, "text": " actually attributed to being able to make novel inferences. And when I'm building circuits here", "tokens": [50584, 767, 30976, 281, 885, 1075, 281, 652, 7613, 13596, 2667, 13, 400, 562, 286, 478, 2390, 26354, 510, 50960], "temperature": 0.0, "avg_logprob": -0.08561350374805685, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0007790583185851574}, {"id": 497, "seek": 281320, "start": 2825.12, "end": 2829.4399999999996, "text": " using grid cells and place cells, I'm mostly talking about allocentric representations.", "tokens": [50960, 1228, 10748, 5438, 293, 1081, 5438, 11, 286, 478, 5240, 1417, 466, 439, 905, 32939, 33358, 13, 51176], "temperature": 0.0, "avg_logprob": -0.08561350374805685, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0007790583185851574}, {"id": 498, "seek": 281320, "start": 2829.4399999999996, "end": 2835.7599999999998, "text": " But then when I was talking about submaps in human experiments, I'm kind of", "tokens": [51176, 583, 550, 562, 286, 390, 1417, 466, 8286, 2382, 294, 1952, 12050, 11, 286, 478, 733, 295, 51492], "temperature": 0.0, "avg_logprob": -0.08561350374805685, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0007790583185851574}, {"id": 499, "seek": 281320, "start": 2835.7599999999998, "end": 2840.3999999999996, "text": " agnostic to the fact that whether those representations are egocentric or allocentric", "tokens": [51492, 623, 77, 19634, 281, 264, 1186, 300, 1968, 729, 33358, 366, 24263, 905, 32939, 420, 439, 905, 32939, 51724], "temperature": 0.0, "avg_logprob": -0.08561350374805685, "compression_ratio": 1.8354430379746836, "no_speech_prob": 0.0007790583185851574}, {"id": 500, "seek": 284040, "start": 2840.4, "end": 2847.36, "text": " because even if you learn an egocentric strategy, even in that case, if you determine that there's", "tokens": [50364, 570, 754, 498, 291, 1466, 364, 24263, 905, 32939, 5206, 11, 754, 294, 300, 1389, 11, 498, 291, 6997, 300, 456, 311, 50712], "temperature": 0.0, "avg_logprob": -0.06898339589436848, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0003857597475871444}, {"id": 501, "seek": 284040, "start": 2847.36, "end": 2852.64, "text": " a repeating structure to the environment and you recall that this is the same environment that I've", "tokens": [50712, 257, 18617, 3877, 281, 264, 2823, 293, 291, 9901, 300, 341, 307, 264, 912, 2823, 300, 286, 600, 50976], "temperature": 0.0, "avg_logprob": -0.06898339589436848, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0003857597475871444}, {"id": 502, "seek": 284040, "start": 2852.64, "end": 2858.0, "text": " seen before, then you can still apply the same egocentric strategy or an allocentric strategy", "tokens": [50976, 1612, 949, 11, 550, 291, 393, 920, 3079, 264, 912, 24263, 905, 32939, 5206, 420, 364, 439, 905, 32939, 5206, 51244], "temperature": 0.0, "avg_logprob": -0.06898339589436848, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0003857597475871444}, {"id": 503, "seek": 284040, "start": 2858.0, "end": 2864.32, "text": " depending on which one you decide to use at that point. But going back to the main question,", "tokens": [51244, 5413, 322, 597, 472, 291, 4536, 281, 764, 412, 300, 935, 13, 583, 516, 646, 281, 264, 2135, 1168, 11, 51560], "temperature": 0.0, "avg_logprob": -0.06898339589436848, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0003857597475871444}, {"id": 504, "seek": 286432, "start": 2865.2000000000003, "end": 2871.6800000000003, "text": " the experimental data shows that we're actually basically learning both kinds of strategies.", "tokens": [50408, 264, 17069, 1412, 3110, 300, 321, 434, 767, 1936, 2539, 1293, 3685, 295, 9029, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2882251186647277, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0022832422982901335}, {"id": 505, "seek": 286432, "start": 2877.92, "end": 2883.84, "text": " Okay, Senke's Pellevan says, great talk. What are the large scale, sorry, where are the large", "tokens": [51044, 1033, 11, 3862, 330, 311, 430, 4434, 9768, 1619, 11, 869, 751, 13, 708, 366, 264, 2416, 4373, 11, 2597, 11, 689, 366, 264, 2416, 51340], "temperature": 0.0, "avg_logprob": -0.2882251186647277, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0022832422982901335}, {"id": 506, "seek": 286432, "start": 2883.84, "end": 2893.92, "text": " scale topological relations coded in the network? So in the network model, our focus, so in this", "tokens": [51340, 4373, 1192, 4383, 2299, 34874, 294, 264, 3209, 30, 407, 294, 264, 3209, 2316, 11, 527, 1879, 11, 370, 294, 341, 51844], "temperature": 0.0, "avg_logprob": -0.2882251186647277, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.0022832422982901335}, {"id": 507, "seek": 289392, "start": 2893.92, "end": 2898.48, "text": " case, we are basically encoding topological relationships using the recurrent connections", "tokens": [50364, 1389, 11, 321, 366, 1936, 43430, 1192, 4383, 6159, 1228, 264, 18680, 1753, 9271, 50592], "temperature": 0.0, "avg_logprob": -0.06337999800841014, "compression_ratio": 1.8764940239043826, "no_speech_prob": 0.0007319235592149198}, {"id": 508, "seek": 289392, "start": 2898.48, "end": 2905.36, "text": " on the place cells. But for large scale topological relationships, what we are moving towards is", "tokens": [50592, 322, 264, 1081, 5438, 13, 583, 337, 2416, 4373, 1192, 4383, 6159, 11, 437, 321, 366, 2684, 3030, 307, 50936], "temperature": 0.0, "avg_logprob": -0.06337999800841014, "compression_ratio": 1.8764940239043826, "no_speech_prob": 0.0007319235592149198}, {"id": 509, "seek": 289392, "start": 2905.36, "end": 2910.16, "text": " splitting this population into two distinct populations. So one population that will only", "tokens": [50936, 30348, 341, 4415, 666, 732, 10644, 12822, 13, 407, 472, 4415, 300, 486, 787, 51176], "temperature": 0.0, "avg_logprob": -0.06337999800841014, "compression_ratio": 1.8764940239043826, "no_speech_prob": 0.0007319235592149198}, {"id": 510, "seek": 289392, "start": 2910.16, "end": 2915.52, "text": " have conjunctive representations, and perhaps another population which will have the recurrent", "tokens": [51176, 362, 18244, 20221, 33358, 11, 293, 4317, 1071, 4415, 597, 486, 362, 264, 18680, 1753, 51444], "temperature": 0.0, "avg_logprob": -0.06337999800841014, "compression_ratio": 1.8764940239043826, "no_speech_prob": 0.0007319235592149198}, {"id": 511, "seek": 289392, "start": 2915.52, "end": 2921.6, "text": " connections similar to the distinction between CA1 and CA3 in the brain. And it's true that in this", "tokens": [51444, 9271, 2531, 281, 264, 16844, 1296, 22852, 16, 293, 22852, 18, 294, 264, 3567, 13, 400, 309, 311, 2074, 300, 294, 341, 51748], "temperature": 0.0, "avg_logprob": -0.06337999800841014, "compression_ratio": 1.8764940239043826, "no_speech_prob": 0.0007319235592149198}, {"id": 512, "seek": 292160, "start": 2921.6, "end": 2926.48, "text": " network model, we don't really distinguish between small scale and large scale topological", "tokens": [50364, 3209, 2316, 11, 321, 500, 380, 534, 20206, 1296, 1359, 4373, 293, 2416, 4373, 1192, 4383, 50608], "temperature": 0.0, "avg_logprob": -0.07146599649012773, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.000423440127633512}, {"id": 513, "seek": 292160, "start": 2926.48, "end": 2930.3199999999997, "text": " relationships. But by introducing this additional population, we hope to abstract away the large", "tokens": [50608, 6159, 13, 583, 538, 15424, 341, 4497, 4415, 11, 321, 1454, 281, 12649, 1314, 264, 2416, 50800], "temperature": 0.0, "avg_logprob": -0.07146599649012773, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.000423440127633512}, {"id": 514, "seek": 292160, "start": 2930.3199999999997, "end": 2938.24, "text": " scale relationships. Also, even in this case, if we have minute relationships between spaces,", "tokens": [50800, 4373, 6159, 13, 2743, 11, 754, 294, 341, 1389, 11, 498, 321, 362, 3456, 6159, 1296, 7673, 11, 51196], "temperature": 0.0, "avg_logprob": -0.07146599649012773, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.000423440127633512}, {"id": 515, "seek": 292160, "start": 2938.24, "end": 2943.44, "text": " then we can also extract away the large scale relationships. But again, adding another population", "tokens": [51196, 550, 321, 393, 611, 8947, 1314, 264, 2416, 4373, 6159, 13, 583, 797, 11, 5127, 1071, 4415, 51456], "temperature": 0.0, "avg_logprob": -0.07146599649012773, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.000423440127633512}, {"id": 516, "seek": 292160, "start": 2944.3199999999997, "end": 2947.44, "text": " might make that easier to make that abstraction.", "tokens": [51500, 1062, 652, 300, 3571, 281, 652, 300, 37765, 13, 51656], "temperature": 0.0, "avg_logprob": -0.07146599649012773, "compression_ratio": 1.9454545454545455, "no_speech_prob": 0.000423440127633512}, {"id": 517, "seek": 295160, "start": 2952.56, "end": 2958.56, "text": " Okay. Nancy has another comment. There must be some way to represent globally metric information.", "tokens": [50412, 1033, 13, 18154, 575, 1071, 2871, 13, 821, 1633, 312, 512, 636, 281, 2906, 18958, 20678, 1589, 13, 50712], "temperature": 0.0, "avg_logprob": -0.11744604016294574, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0011684303171932697}, {"id": 518, "seek": 295160, "start": 2959.12, "end": 2962.72, "text": " For example, I know the approximate distance and angle from my current position to far away", "tokens": [50740, 1171, 1365, 11, 286, 458, 264, 30874, 4560, 293, 5802, 490, 452, 2190, 2535, 281, 1400, 1314, 50920], "temperature": 0.0, "avg_logprob": -0.11744604016294574, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0011684303171932697}, {"id": 519, "seek": 295160, "start": 2962.72, "end": 2966.48, "text": " locations in my world. Do I understand right that you would say this information has to be", "tokens": [50920, 9253, 294, 452, 1002, 13, 1144, 286, 1223, 558, 300, 291, 576, 584, 341, 1589, 575, 281, 312, 51108], "temperature": 0.0, "avg_logprob": -0.11744604016294574, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0011684303171932697}, {"id": 520, "seek": 295160, "start": 2966.48, "end": 2970.08, "text": " pieced together from just the topological relationship between the sub maps that connect", "tokens": [51108, 1730, 1232, 1214, 490, 445, 264, 1192, 4383, 2480, 1296, 264, 1422, 11317, 300, 1745, 51288], "temperature": 0.0, "avg_logprob": -0.11744604016294574, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0011684303171932697}, {"id": 521, "seek": 295160, "start": 2970.08, "end": 2977.2799999999997, "text": " those locations? So that's a great question. I also sometimes wonder because like when I'm here,", "tokens": [51288, 729, 9253, 30, 407, 300, 311, 257, 869, 1168, 13, 286, 611, 2171, 2441, 570, 411, 562, 286, 478, 510, 11, 51648], "temperature": 0.0, "avg_logprob": -0.11744604016294574, "compression_ratio": 1.6013745704467355, "no_speech_prob": 0.0011684303171932697}, {"id": 522, "seek": 297728, "start": 2978.1600000000003, "end": 2983.52, "text": " I can kind of look very far in space and be able to tell that, okay, I need to go in this direction.", "tokens": [50408, 286, 393, 733, 295, 574, 588, 1400, 294, 1901, 293, 312, 1075, 281, 980, 300, 11, 1392, 11, 286, 643, 281, 352, 294, 341, 3513, 13, 50676], "temperature": 0.0, "avg_logprob": -0.14175776360740125, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0012061374727636576}, {"id": 523, "seek": 297728, "start": 2984.48, "end": 2991.76, "text": " And that's definitely metric information. And so going back to the labeled graph hypothesis,", "tokens": [50724, 400, 300, 311, 2138, 20678, 1589, 13, 400, 370, 516, 646, 281, 264, 21335, 4295, 17291, 11, 51088], "temperature": 0.0, "avg_logprob": -0.14175776360740125, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0012061374727636576}, {"id": 524, "seek": 297728, "start": 2992.7200000000003, "end": 3002.7200000000003, "text": " which was suggested by Bill Warren. So even in this hypothesis, I'm just saying that this is", "tokens": [51136, 597, 390, 10945, 538, 5477, 20538, 13, 407, 754, 294, 341, 17291, 11, 286, 478, 445, 1566, 300, 341, 307, 51636], "temperature": 0.0, "avg_logprob": -0.14175776360740125, "compression_ratio": 1.5052631578947369, "no_speech_prob": 0.0012061374727636576}, {"id": 525, "seek": 300272, "start": 3002.7999999999997, "end": 3008.3199999999997, "text": " we are learning metric representations here, which are consistent Euclidean representations,", "tokens": [50368, 321, 366, 2539, 20678, 33358, 510, 11, 597, 366, 8398, 462, 1311, 31264, 282, 33358, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08549771559865851, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.0018671383149921894}, {"id": 526, "seek": 300272, "start": 3008.3199999999997, "end": 3012.9599999999996, "text": " which means that they actually obey all the postulates of Euclidean geometry.", "tokens": [50644, 597, 1355, 300, 436, 767, 19297, 439, 264, 2183, 26192, 295, 462, 1311, 31264, 282, 18426, 13, 50876], "temperature": 0.0, "avg_logprob": -0.08549771559865851, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.0018671383149921894}, {"id": 527, "seek": 300272, "start": 3013.9199999999996, "end": 3019.6, "text": " And the connections between them still have metric information. There's still information", "tokens": [50924, 400, 264, 9271, 1296, 552, 920, 362, 20678, 1589, 13, 821, 311, 920, 1589, 51208], "temperature": 0.0, "avg_logprob": -0.08549771559865851, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.0018671383149921894}, {"id": 528, "seek": 300272, "start": 3019.6, "end": 3027.3599999999997, "text": " about angles and directions, but they might not obey all postulates of Euclidean geometry. So for", "tokens": [51208, 466, 14708, 293, 11095, 11, 457, 436, 1062, 406, 19297, 439, 2183, 26192, 295, 462, 1311, 31264, 282, 18426, 13, 407, 337, 51596], "temperature": 0.0, "avg_logprob": -0.08549771559865851, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.0018671383149921894}, {"id": 529, "seek": 300272, "start": 3027.3599999999997, "end": 3031.68, "text": " instance, you might have some inconsistent representations. You might not know exactly", "tokens": [51596, 5197, 11, 291, 1062, 362, 512, 36891, 33358, 13, 509, 1062, 406, 458, 2293, 51812], "temperature": 0.0, "avg_logprob": -0.08549771559865851, "compression_ratio": 1.934782608695652, "no_speech_prob": 0.0018671383149921894}, {"id": 530, "seek": 303168, "start": 3031.68, "end": 3036.3999999999996, "text": " what distances and displacements you have to go in order to reach a goal, but you know approximately", "tokens": [50364, 437, 22182, 293, 14996, 41140, 291, 362, 281, 352, 294, 1668, 281, 2524, 257, 3387, 11, 457, 291, 458, 10447, 50600], "temperature": 0.0, "avg_logprob": -0.10601091384887695, "compression_ratio": 1.8987854251012146, "no_speech_prob": 0.0002340487262699753}, {"id": 531, "seek": 303168, "start": 3036.3999999999996, "end": 3041.7599999999998, "text": " which distance and which angle. So even the topometric representation is not contradicting", "tokens": [50600, 597, 4560, 293, 597, 5802, 13, 407, 754, 264, 1192, 29470, 10290, 307, 406, 15858, 21490, 50868], "temperature": 0.0, "avg_logprob": -0.10601091384887695, "compression_ratio": 1.8987854251012146, "no_speech_prob": 0.0002340487262699753}, {"id": 532, "seek": 303168, "start": 3041.7599999999998, "end": 3046.64, "text": " the fact that you might have approximate distances and angles. But what it's saying is that you have", "tokens": [50868, 264, 1186, 300, 291, 1062, 362, 30874, 22182, 293, 14708, 13, 583, 437, 309, 311, 1566, 307, 300, 291, 362, 51112], "temperature": 0.0, "avg_logprob": -0.10601091384887695, "compression_ratio": 1.8987854251012146, "no_speech_prob": 0.0002340487262699753}, {"id": 533, "seek": 303168, "start": 3046.64, "end": 3053.2, "text": " very accurate metric information about small pieces of space, but then for faraway pieces of", "tokens": [51112, 588, 8559, 20678, 1589, 466, 1359, 3755, 295, 1901, 11, 457, 550, 337, 1400, 10318, 3755, 295, 51440], "temperature": 0.0, "avg_logprob": -0.10601091384887695, "compression_ratio": 1.8987854251012146, "no_speech_prob": 0.0002340487262699753}, {"id": 534, "seek": 303168, "start": 3053.2, "end": 3056.8799999999997, "text": " space, you might still have approximate distances, angles, which help you navigate.", "tokens": [51440, 1901, 11, 291, 1062, 920, 362, 30874, 22182, 11, 14708, 11, 597, 854, 291, 12350, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10601091384887695, "compression_ratio": 1.8987854251012146, "no_speech_prob": 0.0002340487262699753}, {"id": 535, "seek": 306168, "start": 3062.64, "end": 3069.04, "text": " Okay, another question from Sean Chen. Sorry for my pronunciations. Great talk.", "tokens": [50412, 1033, 11, 1071, 1168, 490, 14839, 13682, 13, 4919, 337, 452, 7569, 11228, 763, 13, 3769, 751, 13, 50732], "temperature": 0.0, "avg_logprob": -0.2547519591546828, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0011327526299282908}, {"id": 536, "seek": 306168, "start": 3069.04, "end": 3072.64, "text": " Could you elaborate the mechanism of exponential number of memory capacity in your model?", "tokens": [50732, 7497, 291, 20945, 264, 7513, 295, 21510, 1230, 295, 4675, 6042, 294, 428, 2316, 30, 50912], "temperature": 0.0, "avg_logprob": -0.2547519591546828, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0011327526299282908}, {"id": 537, "seek": 306168, "start": 3080.72, "end": 3087.44, "text": " So here's the, here's the network, right, where I'm showing that the network has exponential", "tokens": [51316, 407, 510, 311, 264, 11, 510, 311, 264, 3209, 11, 558, 11, 689, 286, 478, 4099, 300, 264, 3209, 575, 21510, 51652], "temperature": 0.0, "avg_logprob": -0.2547519591546828, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0011327526299282908}, {"id": 538, "seek": 308744, "start": 3087.44, "end": 3094.2400000000002, "text": " capacity. So basically in this network, we have different grid modules which have different periods", "tokens": [50364, 6042, 13, 407, 1936, 294, 341, 3209, 11, 321, 362, 819, 10748, 16679, 597, 362, 819, 13804, 50704], "temperature": 0.0, "avg_logprob": -0.12950927870614187, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.004680883139371872}, {"id": 539, "seek": 308744, "start": 3094.2400000000002, "end": 3099.68, "text": " and this is the grid code is actually binary. And we project this grid code randomly using", "tokens": [50704, 293, 341, 307, 264, 10748, 3089, 307, 767, 17434, 13, 400, 321, 1716, 341, 10748, 3089, 16979, 1228, 50976], "temperature": 0.0, "avg_logprob": -0.12950927870614187, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.004680883139371872}, {"id": 540, "seek": 308744, "start": 3099.68, "end": 3104.88, "text": " random weight matrix to place cells. And the number of place cells in this case is much larger than", "tokens": [50976, 4974, 3364, 8141, 281, 1081, 5438, 13, 400, 264, 1230, 295, 1081, 5438, 294, 341, 1389, 307, 709, 4833, 813, 51236], "temperature": 0.0, "avg_logprob": -0.12950927870614187, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.004680883139371872}, {"id": 541, "seek": 308744, "start": 3104.88, "end": 3111.36, "text": " the number of grid cells. And now we learn these back projections from places to grid cells using", "tokens": [51236, 264, 1230, 295, 10748, 5438, 13, 400, 586, 321, 1466, 613, 646, 32371, 490, 3190, 281, 10748, 5438, 1228, 51560], "temperature": 0.0, "avg_logprob": -0.12950927870614187, "compression_ratio": 1.838862559241706, "no_speech_prob": 0.004680883139371872}, {"id": 542, "seek": 311136, "start": 3111.36, "end": 3116.1600000000003, "text": " Hebbian learning. So it's just simple Hebbian learning. I don't have the equation. Basically,", "tokens": [50364, 634, 6692, 952, 2539, 13, 407, 309, 311, 445, 2199, 634, 6692, 952, 2539, 13, 286, 500, 380, 362, 264, 5367, 13, 8537, 11, 50604], "temperature": 0.0, "avg_logprob": -0.10438210846947842, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.004537913482636213}, {"id": 543, "seek": 311136, "start": 3116.1600000000003, "end": 3121.2000000000003, "text": " this is the equation, right? You can consider these as place cell patterns. And this is how we", "tokens": [50604, 341, 307, 264, 5367, 11, 558, 30, 509, 393, 1949, 613, 382, 1081, 2815, 8294, 13, 400, 341, 307, 577, 321, 50856], "temperature": 0.0, "avg_logprob": -0.10438210846947842, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.004537913482636213}, {"id": 544, "seek": 311136, "start": 3121.2000000000003, "end": 3127.76, "text": " compute the weight matrix for the weights going back from place cells to grid cells. And once we've", "tokens": [50856, 14722, 264, 3364, 8141, 337, 264, 17443, 516, 646, 490, 1081, 5438, 281, 10748, 5438, 13, 400, 1564, 321, 600, 51184], "temperature": 0.0, "avg_logprob": -0.10438210846947842, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.004537913482636213}, {"id": 545, "seek": 311136, "start": 3127.76, "end": 3132.88, "text": " done that, then the way we test for memory is basically by perturbing the place code patterns.", "tokens": [51184, 1096, 300, 11, 550, 264, 636, 321, 1500, 337, 4675, 307, 1936, 538, 13269, 374, 4324, 264, 1081, 3089, 8294, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10438210846947842, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.004537913482636213}, {"id": 546, "seek": 311136, "start": 3132.88, "end": 3138.56, "text": " So we put up the place cells by a noisy version of the place code pattern, right? So in this graph,", "tokens": [51440, 407, 321, 829, 493, 264, 1081, 5438, 538, 257, 24518, 3037, 295, 264, 1081, 3089, 5102, 11, 558, 30, 407, 294, 341, 4295, 11, 51724], "temperature": 0.0, "avg_logprob": -0.10438210846947842, "compression_ratio": 1.8295454545454546, "no_speech_prob": 0.004537913482636213}, {"id": 547, "seek": 313856, "start": 3138.56, "end": 3146.08, "text": " I'm showing that if you perturb them with 20% noise, then with 300 place cells, you get an output", "tokens": [50364, 286, 478, 4099, 300, 498, 291, 40468, 552, 365, 945, 4, 5658, 11, 550, 365, 6641, 1081, 5438, 11, 291, 483, 364, 5598, 50740], "temperature": 0.0, "avg_logprob": -0.061518708500293416, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.00020020289230160415}, {"id": 548, "seek": 313856, "start": 3146.08, "end": 3150.0, "text": " noise of zero, which means you get perfect reconstruction of the patterns that you're", "tokens": [50740, 5658, 295, 4018, 11, 597, 1355, 291, 483, 2176, 31565, 295, 264, 8294, 300, 291, 434, 50936], "temperature": 0.0, "avg_logprob": -0.061518708500293416, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.00020020289230160415}, {"id": 549, "seek": 313856, "start": 3150.0, "end": 3155.04, "text": " perturbing the network with. And that's how I'm defining exponential memory, right? And", "tokens": [50936, 13269, 374, 4324, 264, 3209, 365, 13, 400, 300, 311, 577, 286, 478, 17827, 21510, 4675, 11, 558, 30, 400, 51188], "temperature": 0.0, "avg_logprob": -0.061518708500293416, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.00020020289230160415}, {"id": 550, "seek": 313856, "start": 3155.04, "end": 3160.7999999999997, "text": " when I say exponential memory, what I'm suggesting is that you can, as you increase the number of", "tokens": [51188, 562, 286, 584, 21510, 4675, 11, 437, 286, 478, 18094, 307, 300, 291, 393, 11, 382, 291, 3488, 264, 1230, 295, 51476], "temperature": 0.0, "avg_logprob": -0.061518708500293416, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.00020020289230160415}, {"id": 551, "seek": 313856, "start": 3160.7999999999997, "end": 3166.0, "text": " cells in the network, the number of patterns that you can reconstruct from the noisy versions", "tokens": [51476, 5438, 294, 264, 3209, 11, 264, 1230, 295, 8294, 300, 291, 393, 31499, 490, 264, 24518, 9606, 51736], "temperature": 0.0, "avg_logprob": -0.061518708500293416, "compression_ratio": 1.8300395256916997, "no_speech_prob": 0.00020020289230160415}, {"id": 552, "seek": 316600, "start": 3166.0, "end": 3167.44, "text": " actually grows exponentially.", "tokens": [50364, 767, 13156, 37330, 13, 50436], "temperature": 0.0, "avg_logprob": -0.12906970369054915, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.000410669221309945}, {"id": 553, "seek": 316600, "start": 3179.44, "end": 3185.6, "text": " Okay, I'd like to ask a question as well. I'm curious to hear your thoughts on sort of", "tokens": [51036, 1033, 11, 286, 1116, 411, 281, 1029, 257, 1168, 382, 731, 13, 286, 478, 6369, 281, 1568, 428, 4598, 322, 1333, 295, 51344], "temperature": 0.0, "avg_logprob": -0.12906970369054915, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.000410669221309945}, {"id": 554, "seek": 316600, "start": 3185.6, "end": 3192.64, "text": " computational level descriptions here. So it seems like your model is largely", "tokens": [51344, 28270, 1496, 24406, 510, 13, 407, 309, 2544, 411, 428, 2316, 307, 11611, 51696], "temperature": 0.0, "avg_logprob": -0.12906970369054915, "compression_ratio": 1.3566433566433567, "no_speech_prob": 0.000410669221309945}, {"id": 555, "seek": 319264, "start": 3192.64, "end": 3197.92, "text": " tuned towards sort of prediction, accurate prediction of your local spatial environment", "tokens": [50364, 10870, 3030, 1333, 295, 17630, 11, 8559, 17630, 295, 428, 2654, 23598, 2823, 50628], "temperature": 0.0, "avg_logprob": -0.14836620029650235, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.00554644875228405}, {"id": 556, "seek": 319264, "start": 3200.64, "end": 3206.7999999999997, "text": " and motive mechanisms for doing this well. And I wonder what you think about computational level", "tokens": [50764, 293, 28827, 15902, 337, 884, 341, 731, 13, 400, 286, 2441, 437, 291, 519, 466, 28270, 1496, 51072], "temperature": 0.0, "avg_logprob": -0.14836620029650235, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.00554644875228405}, {"id": 557, "seek": 319264, "start": 3207.7599999999998, "end": 3212.96, "text": " accounts of this to be more, to ask a more concrete question. Do you think there's a role", "tokens": [51120, 9402, 295, 341, 281, 312, 544, 11, 281, 1029, 257, 544, 9859, 1168, 13, 1144, 291, 519, 456, 311, 257, 3090, 51380], "temperature": 0.0, "avg_logprob": -0.14836620029650235, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.00554644875228405}, {"id": 558, "seek": 319264, "start": 3212.96, "end": 3218.56, "text": " of reward or something other than accurate, just accurate prediction in the shaping of", "tokens": [51380, 295, 7782, 420, 746, 661, 813, 8559, 11, 445, 8559, 17630, 294, 264, 25945, 295, 51660], "temperature": 0.0, "avg_logprob": -0.14836620029650235, "compression_ratio": 1.663594470046083, "no_speech_prob": 0.00554644875228405}, {"id": 559, "seek": 321856, "start": 3218.56, "end": 3224.48, "text": " neural representations of space? Okay, so you're asking whether there is a role for reward", "tokens": [50364, 18161, 33358, 295, 1901, 30, 1033, 11, 370, 291, 434, 3365, 1968, 456, 307, 257, 3090, 337, 7782, 50660], "temperature": 0.0, "avg_logprob": -0.16823518098290288, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.0006459581200033426}, {"id": 560, "seek": 321856, "start": 3225.36, "end": 3229.7599999999998, "text": " in shaping the representations of space in the mechanistic model?", "tokens": [50704, 294, 25945, 264, 33358, 295, 1901, 294, 264, 4236, 3142, 2316, 30, 50924], "temperature": 0.0, "avg_logprob": -0.16823518098290288, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.0006459581200033426}, {"id": 561, "seek": 321856, "start": 3231.84, "end": 3238.0, "text": " I mean, more generally, what's the objective driving this algorithm? Is it about", "tokens": [51028, 286, 914, 11, 544, 5101, 11, 437, 311, 264, 10024, 4840, 341, 9284, 30, 1119, 309, 466, 51336], "temperature": 0.0, "avg_logprob": -0.16823518098290288, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.0006459581200033426}, {"id": 562, "seek": 321856, "start": 3238.0, "end": 3240.24, "text": " accurate prediction or is there something more than prediction?", "tokens": [51336, 8559, 17630, 420, 307, 456, 746, 544, 813, 17630, 30, 51448], "temperature": 0.0, "avg_logprob": -0.16823518098290288, "compression_ratio": 1.601063829787234, "no_speech_prob": 0.0006459581200033426}, {"id": 563, "seek": 324024, "start": 3241.2, "end": 3252.72, "text": " Okay, so you're asking what is kind of like the motivation for building the mechanistic model?", "tokens": [50412, 1033, 11, 370, 291, 434, 3365, 437, 307, 733, 295, 411, 264, 12335, 337, 2390, 264, 4236, 3142, 2316, 30, 50988], "temperature": 0.0, "avg_logprob": -0.19390335083007812, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.0019867452792823315}, {"id": 564, "seek": 324024, "start": 3254.4799999999996, "end": 3257.12, "text": " Yeah, yeah, I mean, some of the graphs you're showing here about,", "tokens": [51076, 865, 11, 1338, 11, 286, 914, 11, 512, 295, 264, 24877, 291, 434, 4099, 510, 466, 11, 51208], "temperature": 0.0, "avg_logprob": -0.19390335083007812, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.0019867452792823315}, {"id": 565, "seek": 324024, "start": 3258.72, "end": 3263.8399999999997, "text": " some of the graphs you showed, for example, were about recall. And recall, accurate reconstruction", "tokens": [51288, 512, 295, 264, 24877, 291, 4712, 11, 337, 1365, 11, 645, 466, 9901, 13, 400, 9901, 11, 8559, 31565, 51544], "temperature": 0.0, "avg_logprob": -0.19390335083007812, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.0019867452792823315}, {"id": 566, "seek": 324024, "start": 3263.8399999999997, "end": 3269.7599999999998, "text": " of an environment might be one objective. But it seems like you could say that this is a", "tokens": [51544, 295, 364, 2823, 1062, 312, 472, 10024, 13, 583, 309, 2544, 411, 291, 727, 584, 300, 341, 307, 257, 51840], "temperature": 0.0, "avg_logprob": -0.19390335083007812, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.0019867452792823315}, {"id": 567, "seek": 326976, "start": 3269.84, "end": 3274.0800000000004, "text": " mechanism evolved for some purpose, but it's just that or different than that.", "tokens": [50368, 7513, 14178, 337, 512, 4334, 11, 457, 309, 311, 445, 300, 420, 819, 813, 300, 13, 50580], "temperature": 0.0, "avg_logprob": -0.1683464239139368, "compression_ratio": 1.8, "no_speech_prob": 0.0002912707859650254}, {"id": 568, "seek": 326976, "start": 3274.88, "end": 3283.2000000000003, "text": " So, right, so these two questions, which I started with, which are related to capacity of the network,", "tokens": [50620, 407, 11, 558, 11, 370, 613, 732, 1651, 11, 597, 286, 1409, 365, 11, 597, 366, 4077, 281, 6042, 295, 264, 3209, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1683464239139368, "compression_ratio": 1.8, "no_speech_prob": 0.0002912707859650254}, {"id": 569, "seek": 326976, "start": 3284.1600000000003, "end": 3288.48, "text": " and whether we can do, whether we can learn sequences, these are basically questions which I", "tokens": [51084, 293, 1968, 321, 393, 360, 11, 1968, 321, 393, 1466, 22978, 11, 613, 366, 1936, 1651, 597, 286, 51300], "temperature": 0.0, "avg_logprob": -0.1683464239139368, "compression_ratio": 1.8, "no_speech_prob": 0.0002912707859650254}, {"id": 570, "seek": 326976, "start": 3288.48, "end": 3293.2000000000003, "text": " started with, because I wanted to explore the theoretical properties of these circuits, right,", "tokens": [51300, 1409, 365, 11, 570, 286, 1415, 281, 6839, 264, 20864, 7221, 295, 613, 26354, 11, 558, 11, 51536], "temperature": 0.0, "avg_logprob": -0.1683464239139368, "compression_ratio": 1.8, "no_speech_prob": 0.0002912707859650254}, {"id": 571, "seek": 326976, "start": 3293.2000000000003, "end": 3297.6800000000003, "text": " given the coding properties that we know of grid cells and place cells and given", "tokens": [51536, 2212, 264, 17720, 7221, 300, 321, 458, 295, 10748, 5438, 293, 1081, 5438, 293, 2212, 51760], "temperature": 0.0, "avg_logprob": -0.1683464239139368, "compression_ratio": 1.8, "no_speech_prob": 0.0002912707859650254}, {"id": 572, "seek": 329768, "start": 3297.68, "end": 3303.9199999999996, "text": " biologically plausible learning tools through have been learning, what are the coding properties", "tokens": [50364, 3228, 17157, 39925, 2539, 3873, 807, 362, 668, 2539, 11, 437, 366, 264, 17720, 7221, 50676], "temperature": 0.0, "avg_logprob": -0.11250185516645324, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0004801696049980819}, {"id": 573, "seek": 329768, "start": 3303.9199999999996, "end": 3308.64, "text": " that these networks can exhibit, right, how much capacity they have, how much information can they", "tokens": [50676, 300, 613, 9590, 393, 20487, 11, 558, 11, 577, 709, 6042, 436, 362, 11, 577, 709, 1589, 393, 436, 50912], "temperature": 0.0, "avg_logprob": -0.11250185516645324, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0004801696049980819}, {"id": 574, "seek": 329768, "start": 3308.64, "end": 3314.7999999999997, "text": " store. And so, yes, even in these models, since we are talking about reconstruction,", "tokens": [50912, 3531, 13, 400, 370, 11, 2086, 11, 754, 294, 613, 5245, 11, 1670, 321, 366, 1417, 466, 31565, 11, 51220], "temperature": 0.0, "avg_logprob": -0.11250185516645324, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0004801696049980819}, {"id": 575, "seek": 329768, "start": 3315.68, "end": 3320.3199999999997, "text": " basically says that if you have some noise, if you have some noisy estimate of where you are in", "tokens": [51264, 1936, 1619, 300, 498, 291, 362, 512, 5658, 11, 498, 291, 362, 512, 24518, 12539, 295, 689, 291, 366, 294, 51496], "temperature": 0.0, "avg_logprob": -0.11250185516645324, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0004801696049980819}, {"id": 576, "seek": 329768, "start": 3320.3199999999997, "end": 3326.72, "text": " space, then this kind of a network can help you clean that estimate, right, and reach a cleaner", "tokens": [51496, 1901, 11, 550, 341, 733, 295, 257, 3209, 393, 854, 291, 2541, 300, 12539, 11, 558, 11, 293, 2524, 257, 16532, 51816], "temperature": 0.0, "avg_logprob": -0.11250185516645324, "compression_ratio": 1.8084291187739463, "no_speech_prob": 0.0004801696049980819}, {"id": 577, "seek": 332672, "start": 3326.72, "end": 3333.12, "text": " version of where you might be in space and help you localize in space. And also, these temporal", "tokens": [50364, 3037, 295, 689, 291, 1062, 312, 294, 1901, 293, 854, 291, 2654, 1125, 294, 1901, 13, 400, 611, 11, 613, 30881, 50684], "temperature": 0.0, "avg_logprob": -0.08740596433656406, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0005973191582597792}, {"id": 578, "seek": 332672, "start": 3333.12, "end": 3337.68, "text": " connectivity on the place cells kind of have a predictive component to them, right, because you", "tokens": [50684, 21095, 322, 264, 1081, 5438, 733, 295, 362, 257, 35521, 6542, 281, 552, 11, 558, 11, 570, 291, 50912], "temperature": 0.0, "avg_logprob": -0.08740596433656406, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0005973191582597792}, {"id": 579, "seek": 332672, "start": 3337.68, "end": 3341.4399999999996, "text": " can import sequences. So, when you are at a particular position, you might be able to predict", "tokens": [50912, 393, 974, 22978, 13, 407, 11, 562, 291, 366, 412, 257, 1729, 2535, 11, 291, 1062, 312, 1075, 281, 6069, 51100], "temperature": 0.0, "avg_logprob": -0.08740596433656406, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0005973191582597792}, {"id": 580, "seek": 332672, "start": 3341.4399999999996, "end": 3348.56, "text": " what's coming next. And so, these are kind of small components, which I'm using to go towards", "tokens": [51100, 437, 311, 1348, 958, 13, 400, 370, 11, 613, 366, 733, 295, 1359, 6677, 11, 597, 286, 478, 1228, 281, 352, 3030, 51456], "temperature": 0.0, "avg_logprob": -0.08740596433656406, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0005973191582597792}, {"id": 581, "seek": 332672, "start": 3348.56, "end": 3353.8399999999997, "text": " building a model of remapping in space, right, because ultimately my goal is to be able to", "tokens": [51456, 2390, 257, 2316, 295, 890, 10534, 294, 1901, 11, 558, 11, 570, 6284, 452, 3387, 307, 281, 312, 1075, 281, 51720], "temperature": 0.0, "avg_logprob": -0.08740596433656406, "compression_ratio": 1.7870722433460076, "no_speech_prob": 0.0005973191582597792}, {"id": 582, "seek": 335384, "start": 3353.84, "end": 3358.32, "text": " figure out how these neural circuits might lead to the formation of these sub-maps. And by knowing", "tokens": [50364, 2573, 484, 577, 613, 18161, 26354, 1062, 1477, 281, 264, 11723, 295, 613, 1422, 12, 76, 2382, 13, 400, 538, 5276, 50588], "temperature": 0.0, "avg_logprob": -0.1418275459139955, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0004509659775067121}, {"id": 583, "seek": 335384, "start": 3358.32, "end": 3363.44, "text": " the properties of these circuits, then I'll be better able to construct and build those networks,", "tokens": [50588, 264, 7221, 295, 613, 26354, 11, 550, 286, 603, 312, 1101, 1075, 281, 7690, 293, 1322, 729, 9590, 11, 50844], "temperature": 0.0, "avg_logprob": -0.1418275459139955, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0004509659775067121}, {"id": 584, "seek": 335384, "start": 3363.44, "end": 3367.36, "text": " which actually can build small local sub-maps of environments.", "tokens": [50844, 597, 767, 393, 1322, 1359, 2654, 1422, 12, 76, 2382, 295, 12388, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1418275459139955, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0004509659775067121}, {"id": 585, "seek": 335384, "start": 3372.2400000000002, "end": 3377.6800000000003, "text": " I'm reading out a comment from Ila. To my question, you could view this as a structure", "tokens": [51284, 286, 478, 3760, 484, 257, 2871, 490, 286, 875, 13, 1407, 452, 1168, 11, 291, 727, 1910, 341, 382, 257, 3877, 51556], "temperature": 0.0, "avg_logprob": -0.1418275459139955, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0004509659775067121}, {"id": 586, "seek": 335384, "start": 3377.6800000000003, "end": 3381.6800000000003, "text": " learning even without reward. A reinforcement learner, including the brain, could use these", "tokens": [51556, 2539, 754, 1553, 7782, 13, 316, 29280, 33347, 11, 3009, 264, 3567, 11, 727, 764, 613, 51756], "temperature": 0.0, "avg_logprob": -0.1418275459139955, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.0004509659775067121}, {"id": 587, "seek": 338168, "start": 3381.68, "end": 3390.8799999999997, "text": " learning structures. Cool. Well, I think we're actually over time now. That was a great talk,", "tokens": [50364, 2539, 9227, 13, 8561, 13, 1042, 11, 286, 519, 321, 434, 767, 670, 565, 586, 13, 663, 390, 257, 869, 751, 11, 50824], "temperature": 0.0, "avg_logprob": -0.18918118110069862, "compression_ratio": 1.3055555555555556, "no_speech_prob": 0.002925540553405881}, {"id": 588, "seek": 338168, "start": 3390.8799999999997, "end": 3398.7999999999997, "text": " Sue. Thanks for sharing with us. Of course, thank you. So, we'll be back next week for another", "tokens": [50824, 25332, 13, 2561, 337, 5414, 365, 505, 13, 2720, 1164, 11, 1309, 291, 13, 407, 11, 321, 603, 312, 646, 958, 1243, 337, 1071, 51220], "temperature": 0.0, "avg_logprob": -0.18918118110069862, "compression_ratio": 1.3055555555555556, "no_speech_prob": 0.002925540553405881}, {"id": 589, "seek": 339880, "start": 3398.8, "end": 3411.36, "text": " talk from Eli Pollock. Until then, have a good Thanksgiving, and I'll see you next week.", "tokens": [50392, 751, 490, 16943, 31304, 1560, 13, 9088, 550, 11, 362, 257, 665, 21230, 11, 293, 286, 603, 536, 291, 958, 1243, 13, 50992], "temperature": 0.0, "avg_logprob": -0.2520242118835449, "compression_ratio": 1.0232558139534884, "no_speech_prob": 0.049965232610702515}], "language": "en"}