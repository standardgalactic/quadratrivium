start	end	text
0	6640	the Q&A button, or raise your hand after the talk and we can call on you to ask a question
6640	12720	by video or audio. A little bit of logistics, we still have an open slot on December 8th,
12720	17600	so if you have a talk at any length, it can be less than an hour or a full hour,
17600	22960	and you want to share with the BCS Cog Launch community, just get in touch with me by email,
22960	27520	you can respond to that announcement email that you got for this event, and we can work that out.
27920	33840	With that, let's get to our main content. So, Sue, we're telling us today about rapid
33840	43440	generalization of knowledge and structured domains. Take it away, Sue.
57520	73840	All right. So, as John mentioned, I'm Sue. I'm a third-year PhD student in the BCS department,
73840	79280	and I'm co-advised by Professor Ela Feed and Professor Josh Tenenbaum. And in general,
79280	84000	the question I'm interested in is, how do people generalize their learning to novel situations?
84000	88320	And in any given domain, if the underlying space is structured, we might learn those
88320	92800	underlying structures independently of the sensory observations, and that might in turn
92800	98080	help us generalize to novel situations. So, what I'm presenting today is a step towards
98080	103200	answering this question. I'll start with motivation, and then I'll make a case for why
103200	108000	hippocampal entorhinal system is an important system to study if you're interested in generalization.
108560	112720	And then I've divided the rest of my talk in three parts, and I'll give you a brief overview
112720	118880	of those three parts before I go into the details of each of those. So, imagine you go to Costco
118880	124320	in Waltham, since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know
124320	128800	where the bakery section is or where the fruit section is, and now imagine you go to a completely
128800	134160	different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there,
134160	138240	Costco might have the same layout, or it might have a layout which is some transformation of the
138240	141920	original layout. For instance, it might be a reflective version of the original, or there
141920	146560	might be minor changes. And despite of that, you're still able to find the things you're looking for
146560	152240	using your previous knowledge of the map of Costco. Another example is roundabouts. So, if you learn
152240	157920	to go about a roundabout in Cambridge, then even if you go to any other country or city,
157920	162720	then you will be able to use your previous knowledge to actually navigate through that roundabout.
163440	167680	So, generally, we learn novel environments as compositions of spatial structures that we've
167680	172320	already seen before, and that allows us to quickly generalize and learn new spatial
172320	176560	environments. For instance, when you go to a new city, you might encounter Costco again,
176560	181040	you might encounter a roundabout again, and you know which map to pull out when you are in Costco,
181040	188800	and which map to pull out when you are navigating around a roundabout. And so, here's another example
188800	194880	where this is a hotel which has symmetric left and right wings, and if one has explored the left
194880	199600	wing of this hotel, then they might be very quickly able to generalize their learning to the
199600	203600	right wing and make inferences about the right wing, even if they haven't really directly explored
203600	208800	the right wing. So, humans are actually very good at making these complex inferences from
208800	214400	just very sparse observations, and this ability has been suggested to be a result of a systematic
214400	219920	organization of knowledge called the cognitive map. And hippocampal entorhinal system is known to
219920	225440	be important for the construction of this cognitive map. So, for instance, rodents when
225440	231360	they explore a 2D spatial environment, it has been found that hippocampus has these place cells
231360	237120	which code locations in the 2D environment, and there are good cells in the entorhinal cortex
237120	242560	which show this hexagonally symmetric firing fields, which are periodic, and which have been
242560	247760	thought to encode location, but also cell motion-based euclidean displacement. And these
247760	252160	codings are important for the construction of cognitive map because they provide an allocentric
252160	258160	representation. So, given that the hippocampal entorhinal system encodes spatial variables,
258160	263120	the next question is, can this also be used to represent other continuous task variables
263120	269440	other than space? And the answer to this question is yes, and I'll give one example. So, here in
269440	273600	this experiment has been found that cells in hippocampus and entorhinal cortex respond to
273600	279120	task-relevant variables like sound frequency. So, in this task, rodents pull this lever,
279120	283520	and as they pull the lever, the frequency of the sound coming from this sound source
283520	287360	actually keeps increasing, and they have to release the lever when this frequency is in
287360	292560	this target zone. And what is found is that in hippocampus and entorhinal cortex, there are cells
292560	299680	that fire for specific frequencies during the sound modulation task. So, that shows that
300640	305680	entorhinal cortex is a system that is able to represent continuous task variables,
305680	310480	even other than space. And so, our next question is, could this system also help us organize
310480	315440	and navigate discrete knowledge? And family trees are one example of discrete knowledge.
315440	320080	So, in family trees, there's an underlying hierarchical structure that we learn. And once we
320080	323680	know that structure, then we can apply that structure to my family tree or to your family
323680	327840	tree or anyone's family tree, and we can generalize that knowledge. So, we can make inferences like
327840	333600	this, because Olivia is Emily's sister and Sam is Emily's son, Sam must be Olivia's nephew.
333600	338080	So, even though we haven't directly observed this relationship, but just by mere observation of
338080	342080	these two relationships, we are able to infer this relationship, because we know the underlying
342080	347440	hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system
347440	352400	might allow organization of this kind of a discrete knowledge, but that's only possible if it allows
352400	358000	encoding non-Equidian relationships. With that, I'll go back to the spatial domain,
358000	363360	and I'll talk about how spatial knowledge might be organized. And when I talk about organization,
363360	368480	I'll point to the fact that even in a continuous domain like space, it might be possible that
368480	373120	we have both Euclidean and non-Euclidean components to represent space itself,
373120	377520	and thus making the system generalizable to even discrete domains.
378240	386400	So, here's an experiment conducted by Bill Warren, where they show that people actually do not learn
386400	392080	a global Euclidean map of space. So, in this experiment, they constructed virtual environments,
392080	398640	and they basically, the task was for people, for human subjects. So, this was a human behavioral
398640	403200	experiment, and human subjects were asked to go to different landmarks in this spatial environment,
403200	407040	and they also built counterparts of the spatial environment, which were non-Euclidean,
407200	412400	in these environments. So, when you enter one part of the wormhole, you seamlessly exit from the
412400	417520	other end of the wormhole, and subjects were not aware of the existence of these wormholes.
418080	423520	And basically, what they found through various manipulations of the experiment was that people
423520	428960	do not actually learn a global Euclidean map, but rather a labeled graph, like representation,
428960	434080	where the nodes represent places, the edges represent approximate distances between these
434080	439920	places, and the node labels, which are angles, represent approximate angles between these places.
440800	444480	And so, we built on this representation, and we proposed that people might be
444480	450800	representing topometric maps, which are locally metric or Euclidean, but globally topological.
450800	455040	And the main advantage of this kind of representation is that it allows us to combine
455040	460560	accurate local maps into a global map, which might be inconsistent, but it still provides
460560	465600	enough sufficient information for navigation. So, the next question is,
466400	471120	can this kind of a topometric map be implemented in the brain? And if so, how?
471120	475120	And next, I'm providing a theoretical framework for how it might be possible to represent such
475120	480480	topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex.
480480	484320	So, this is a topometric representation of space. You can see that these are metric maps
484320	489360	connected topologically by these connections. And here, on this side, I'm showing a grid coding
489360	494720	space, which is dense coding space with large capacity. And here, I'm showing place coding
494720	500160	space, which also has large capacity, but it spars, so it can receive sensory inputs and form
500160	506160	conjunctive representations. And so, in this schematic, small changes, sorry, large changes
506160	513120	in contextual input from our spatial domain was remapping in the place cells, which in turn
513120	517280	trigger remapping in the grid cells, enabling the formation of these local metric maps that
517280	524160	can be reused. And so, on this schematic, we can really take any subpart of this schematic and
524160	530400	call it a sub map. And this allows us to compose sub maps, because once we have learned a particular
530400	535680	sub map, then we can actually encounter the sub map in a completely novel situation and still be able
535680	545200	to spatially navigate and reason through it. Another thing that it allows us is learning
545200	550160	non-ecredient relationships, because place cells actually encode topological relationships, enabling
550880	556240	the representation of non-ecredient relations. So, now I'm going to describe the three parts
556240	561760	in which I've divided the rest of the talk. So, in the first part, I'm probing whether
561760	568080	sub maps drive past learning in complex spaces using human behavioral experiments. In the second part,
568640	573600	I will talk about determining which principles might guide fragmentation of a space into
573600	579200	sub maps. And finally, in the third part, I'll propose a framework for building a neural model
579200	586800	of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast learning
586800	591200	in complex spaces, specifically in humans. And this work is in collaboration with Marta Krivind,
591200	597040	who's a postdoc in Tenenbaum Lab, and Kevin, who's an undergrad in the CS department.
597920	603120	So, here I hypothesize that humans learn adaptable and compositional sub maps of spatial
603120	608000	structures. So, for instance, this is a baseline environment, and this is a top-down view showing
608000	613600	an environment with four rooms. And here I've shown certain transformations of this environment
613600	619440	generated by small generator programs. And you can see this is the same environment rotated,
619440	624240	because now you're entering from this point, so it might appear rotated to you. And here's a reflection
624240	630720	of the same environment. Here is a transformation where we've removed the wall and added a shortcut,
630720	635200	and here we've added a wall. And this is just the repetition of the same environments.
635200	639280	And there's another transformation which is scaling, where you can imagine this environment
639280	644480	scaled up to a bigger size, but having the same geometrical layout. And what I'm suggesting is
644480	649120	that once people have learned the map of the space line environment, their representations
649120	654160	might be adaptable to some or all of these transformations. And furthermore, people might
654160	659360	represent richer spaces by combining these maps and their transformations, leading to quick
659360	665360	generalization and learning. So, this can be modeled using Bayesian program learning framework,
665360	669520	where concepts are represented as simple programs, and rich concepts can be built
669520	675840	compositionally from them using a higher-level generative model. So, there is neural evidence
675840	681920	for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab, and they show
681920	687600	that when a rodent goes through this environment in four labs, there are cells which fire specifically
687600	692560	for particular places in this environment, but there are also cells which encode, which are
692560	697200	event-specific and encode specific labs. So, for instance, there are cells which show increased
697200	701440	fighting rates as you go from lab one to lab four, and there are also cells which only fire
701440	706240	specifically on lab one or on lab two and so on. And so, what I'm suggesting is that when we have
706240	711440	repetitions of the same environment, there might be cells that encode the basic map of this environment,
711440	716560	which, which is consistent across these occurrences, but there might be a second set of cells,
716560	721120	which are event-specific and might encode which instance of this environment we are on.
722160	729200	Here's another example, where this is an example of scaling, where the rodent actually just explores
729200	734000	this circular environment, and this is the place field found in that circular environment,
734000	738240	and when the circular environment is scaled up to a bigger size, the place field also scales
738240	742640	according to the size of the environment. So, this shows that the map which the rodent has
742640	747200	learned of this environment is actually adaptable to this transformation of scaling to a bigger
747200	752240	size, and map also scales proportionately with the size of the environment. And here's a third
752240	758480	example, where rodents form different maps, place maps in these different environments,
758480	762080	and when these environments are composed by connecting them through a corridor,
762080	766560	rodents end up using the same maps which they had learned before for these environments. And
766560	771360	furthermore, if I replace this environment with one of the previous environments seen before,
771360	776400	then remapping is only observed in this part of the environment, and this part of the environment
776400	782800	actually stays the same using the same previous map. So, this provides some evidence in support of
782800	788800	composition of independent local sub-maps. So, in my experiment, I aim to assess whether people
788800	794080	learn sub-maps on spatial structures and use them rationally in exploration, and I hypothesized that
794080	799120	people might learn sub-maps that are adaptable and compositional, and my first alternate hypothesis
799200	802560	is that they might learn sub-maps of spatial structures that might not be adaptable or
802560	808000	compositional to certain transformations, and the last alternate hypothesis is that people
808000	812880	might just learn a global representation of environments without learning any sub-maps.
812880	819200	So, in order to test this hypothesis, we're building this task where we are building 3D
819200	824640	virtual environments using Unity, and these environments have this repeating structure,
824720	830240	and the task is for the subjects to find maximum amount of diamonds embedded in these environments
830240	835120	in a limited amount of time given to them, and in order to test adaptability and composition,
835120	839120	we can also have these repetitions be transformations of each other, for instance,
839120	843360	here it's a reflection, or we can also have these environments composed of different structures to
843360	847600	see whether people can compose their representations or structures that they've already seen.
848560	853840	So, here's an example. Here you see that a person is navigating through corridor and enters a
853840	858720	structure, and they go to one of the rooms and they do not find anything there, and then they
858720	864160	decide to go to the other room, and they end up finding a reward there, and now they're going
864160	870480	back to the corridor and they continue exploring the environment, and when they enter another section,
870480	875920	if they show a preferential navigation strategy towards the room that has a reward, then that
875920	880640	indicates that they have realized that there's a repeating structure in the environment and indicates
880640	884960	a possibility that people might be learning sub-maps and identifying sub-maps as they're
884960	890560	navigating the spatial environments, and furthermore, if we do find through the experiment that people
890560	896160	actually learn sub-maps, then we can use similar environments to design experiments where we can
896160	903120	test for adaptability to transformations of environments and also for composition of
903120	910400	different spatial structures. So, that takes me to the next part of the talk, which is
910480	914640	determining which principles guide fragmentation into sub-maps. So, since we are seeing that
914640	920800	sub-maps drive fast learning, the next natural question becomes what determines this fragmentation
920800	925760	of a spatial environment into sub-maps, and this work is in collaboration with Mirko Klukas,
925760	932560	who is a post-doc in the feed lab. So, here my hypothesis is that neural remapping is a signature
932560	938960	of sub-map reconstruction. So, here I'll explain what remapping is. So, basically, this is a 2D
939040	946320	environment, and when the animal just explores this 2D environment, we find hexagonally periodic
946320	954240	grid fields in the entorhinal cortex, and when you actually insert these walls in this environment,
954240	959520	then what is observed is that when the animal turns, then this grid field which is formed either
959520	965200	re-orients or shifts, and this is called remapping, when the grid field actually re-orients or shifts
965200	970640	from its original orientation. And so, what is observed is that animals actually end up using
970640	976800	the same grid maps in alternate arms. So, this indicates the use of maps. And here's another
976800	981760	example of the use of maps. So, basically, this is a 2-room environment, and animals explore
981760	989760	this environment, and it is seen that eventually the map formed in environment A is the same as the
989760	994880	map formed in environment B, over short time scales. So, this is another example of the fact
994880	1000480	that animals are reusing the maps in both the rooms which look very similar. So, what I'm suggesting
1000480	1006080	is that this field repetition doesn't result from localization error or purely due to disorientation,
1006080	1012320	because even when you use transparent walls in this environment, you still see that the grid
1012320	1017680	maps are being reused in alternate arms, even though the animal can see through these transparent
1017680	1024240	walls. Furthermore, if you extend this 2-room environment to a 4-room environment, you still
1024240	1029360	see field repetition in all of these rooms, which suggests that animals are actually reusing
1029360	1033920	sub-maps in a calculated way for efficient representation, rather than just being disoriented.
1038480	1043120	So, next I talk about existing models of remapping, and there are two classes of models.
1043120	1048400	One class of model suggests that remapping is driven by sensory ambiguity. So, for instance,
1048400	1053120	if you are in an environment that looks similar to an environment you've been before, either in
1053120	1058800	terms of its geometry or its visual observations, then you might end up using the same map that
1058800	1063440	you had learned from a previous environment. Then there's another class of models that suggests
1063440	1068080	that remapping is based on environment topology, instead of just sensory ambiguity. So, here,
1068720	1072800	each state in the environment is represented in terms of its successor states, and it's called
1072800	1078080	a successor representation. And this successor representation actually ends up looking similar
1078080	1082640	to place feeds, and if you do an identity composition on these successor representations,
1082640	1088800	then you get fields that are very similar to grid fields. And this successor representation
1089280	1093840	encapsulates inherent dynamics of the environment, as well as the policy that the agent is following.
1093840	1099680	However, there are other approaches like the graph-leplacian approach, which is policy independent.
1101120	1104960	So, what are some of the limitations of these models? So, the models that are
1104960	1110160	based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment
1110160	1116640	like this, these two regions actually look very different. These models will not have any map
1116640	1125520	fragmentation or remapping. However, on the other hand, models which are based on environment topology,
1125520	1130880	actually, it's not very clear how remapping would happen on first visit in these environments,
1130880	1134240	because you need to build up the successor representation or the transition matrix of
1134240	1141200	the environment before you can observe the grid fields. So, in our model, we address these limitations,
1141200	1145360	and we have remapping on first visit, and we also have remapping without sensory ambiguity.
1145360	1150160	And next, I'll go into the details of our model. So, we interpret grid remapping as
1150160	1154960	fragmentation into submaps. Why is this a useful interpretation? That's because
1154960	1159520	remapping enables topological representation. So, for instance, if we are dividing this
1159520	1164400	environment into submaps, then we also need to store the relationships between these submaps,
1164960	1169280	and this enables a compact topological representation, which is beneficial for planning.
1170960	1175200	Second reason is that remapping reduces path integration errors. So, if you try to learn a
1175200	1180720	global map, it can very quickly become inconsistent because of accumulation of path integration errors.
1180720	1185520	But if you divide the environment in submaps, then it becomes easier to map the environment.
1185520	1190880	And this has been shown by using Atlas framework in robotics, where they divide the environment
1190880	1195440	into submaps in order to map the environment, and it works very well for large environments.
1196960	1201440	And the third reason is that remapping enables representation of abstract cognitive spaces,
1201440	1207200	because it allows representation of non-euclidean structures. So, in our model, we have two
1207200	1212400	possibilities. Either we can extend an old map, or we can decide to remap. And when we decide to
1212400	1217440	remap, we can either remap to a new map or remap to an existing map. So, for instance,
1217440	1224320	in the experiments we saw that in the square environment without walls, the map that the
1224320	1230400	animal learns is always extended. But when we insert these walls in this environment,
1230400	1235440	then the map is extended within lane one. But when you turn from lane one to lane two,
1235440	1239520	you actually remap to a new map. And when you turn from lane two to lane three,
1239520	1243360	you end up remapping to an existing map, which is the same as lane one.
1244560	1250240	And similarly, in the two room experiment, we saw that when you go from room one to the corridor,
1250240	1254880	you end up mapping to a new map. And when you go from corridor to room two, you actually end up
1254880	1259680	remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going
1259680	1265840	to extend a map or whether we should be remapping. So, in our model, remapping is based on the
1265840	1270800	notion of contiguous regions. And a contiguous region is a region such that when I stay within
1270800	1274880	that region, my visual observations change very little. And these contiguous regions are connected
1274880	1279440	by these bottleneck states. And this is aligned with the experimental data, which we have seen,
1279440	1285280	which suggests special rule of doorways and corridors. So, now we formalize the concept
1285280	1290240	of contiguous regions by defining a measure of similarity. So, we define similarity as the ability
1290240	1295200	to predict observations at one pose from the observations made at another pose. And so,
1295200	1299920	the overlap between the observations made at two poses actually is a notion of similarity.
1299920	1304640	And this formalizes the concept of contiguous regions as a region where any two points are
1304640	1309520	similar. And here, I'm showing that similarity actually decreases when you transition between
1309520	1313920	contiguous regions. So, if you look at points which are within this contiguous region,
1313920	1319120	their similarity is high with respect to this point. But for points which are in other regions,
1320080	1322800	the similarity is pretty low as compared to this point.
1324800	1329440	So, then we can use this notion of similarity to define density in order to do density-based
1329440	1334800	clustering. And here, we define density as a similarity between any pose X and its
1334800	1342000	mth nearest neighbor. And this notion can be used with any greedy algorithm like optics to
1342000	1346080	generate fragmentations of the environment. And here, I'm showing one example of fragmentations
1346080	1350240	of the environment where it gives four different clusters corresponding to these four different
1350240	1358800	colors shown here. So, given that contiguity is a local property, we can also try to compute
1358800	1364000	segmentations online by predicting current observations from the past. And in this case,
1364000	1368800	observations can be represented by boundary vector cells. And we can implement a short-term memory
1368800	1372800	which stores exponential moving average of boundary vector cell activations
1372800	1377600	to approximate the similarity. So, for instance, our short-term memory at a previous time step
1377600	1382320	can be used to predict observations at a current time step to compute the similarity between two
1382320	1386880	poses. And another component which we need to add to our model is the long-term memory component,
1386880	1390720	which helps us decide whether we should be remapping to an existing map or we should be
1390720	1398720	remapping to a new map. So, for all of these environments, our model makes the correct
1398720	1404480	predictions which are in line with the experimental data observed. And these experiments have been
1404480	1409920	done and we have neural data for them. This is a new prediction that our model makes for amorphous
1409920	1414640	naturalistic environments. We predict that even in these environments, the map will be segmented
1414640	1418800	and grid fields will realign when going from one contiguous region to the other.
1418800	1422320	And we do not predict any map fragmentations in these spiral mesas.
1422480	1431520	So, given that we built or proposed an algorithmic model for map fragmentation, the next question is,
1431520	1436880	how can map fragmentation be implemented on a neural level? So, we want to provide a framework
1436880	1441360	for building a neural circuit model of map fragmentation. And this work is in collaboration
1441360	1448560	with Sarthak and Murko, who are both postdocs in FEDLA. So, going back to our theoretical
1448560	1454880	framework, we had suggested that place cells might encode topological relationships between
1454880	1460080	metric maps that might be represented by the grid space. And now I'm going to talk about
1460080	1465440	how we can implement that at a neural level. So, at the neural level, we start with factorized
1465440	1469600	representations in which different aspects of knowledge are represented separately and can
1469600	1474880	then be flexibly recombined. So, for instance, in this case, location information from grid cells
1474880	1479760	and contextual information from sensory cells form this conjunctive representation in place cells.
1480400	1484800	Here grid cells can enable path integration and can be thought of as implementing an affine vector
1484800	1490080	space or an impedance space. The recurrent wiring between these place cell population encodes
1490080	1495280	neighborhood relationships or topology. And here, large changes in contextual input cause
1495280	1501040	remapping in place cells, which in turn cause remapping in grid cells through these back projections.
1501040	1504400	And remapping here corresponds to transitioning from one local map to another.
1505280	1511120	So, most of the previous work on interplay of grid and place circuits focuses on maintaining
1511120	1516160	firing properties of one population based on the inputs from another. So, for instance,
1517280	1521120	successor representation suggests that grid cells are a low-dimensional representation
1521120	1527520	of place cells that stabilize place cell activity. Similarly, here's a model which
1527520	1532400	implements non-negative PCA of place cells. So, place cells are at the input. The weights are
1532400	1537840	learned through heavy learning and a non-negativity constraint. And this network does PCU on the
1537840	1544000	inputs, and the outputs end up converging to grid-like fields, again, suggested that grid
1544000	1546720	cells might be a low-dimensional representation of place cells.
1551280	1556160	Another set of work suggests that inputs from border cells to grid cells could be used for
1556160	1562160	error-correcting grid cells. So, here I'm showing a one-day schematic just to make my point. So,
1562160	1568560	this is a rodent at a specific location in space, and this is the grid activity profile
1569280	1573440	that represents that location. And when the rodent explores the environment and comes back to this
1573440	1577920	location, the representation of this location has drifted with respect to the original,
1577920	1583120	and there's some error in the representation. And if the border cell activations are provided
1583120	1588560	as input to grid cells, then they activate the current subset of neurons doing error correction
1588560	1593680	and pulling back the representation to the original representation. And this is what this
1593680	1598320	looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell
1598320	1602400	representations are not very stable, but with border cell inputs, your grid cell representations
1602400	1610720	are fairly stable. So, I also want to point out the fact that place cells are thought to store
1610720	1614320	neighborhood relationships in their reference synapses, and therefore they could implement
1614320	1618720	a topological navigation strategy. And many models of place cell-based navigation have
1618720	1623760	actually emphasized this view. So, they've suggested that recurrence synapses encode either
1623760	1628720	spatial or temporal connectivity, as suggested by Blum and Abbott, or they encode transition
1628720	1634640	probability, as suggested by the successor representation work. So, given all these insights,
1634640	1639440	our goal is to build a comprehensive neural circuit model of premapping. And we start with these
1639440	1644000	two questions. Does high-capacity grid code, when projected to place cells, also lead to high
1644000	1649440	capacity? And given these conjunctive representations between the location input and the sensory input,
1649440	1655280	can we learn neighborhood relationships between place codes? And before I go into the details
1655280	1659760	of capacity, I just want to point out that traditionally, Hopfield networks have been
1659760	1665040	used for storing memories and patterns. And it has been observed that the maximum patterns
1665040	1669360	that these networks can store is n, where n is the total number of neurons in the network.
1670000	1674640	And modern Hopfield networks, also known as dense associative memories, have an exponential
1674640	1680400	capacity, but they use many body interaction terms, which are not biologically plausible.
1680400	1684400	And in our model, we stick to using two interaction terms in the weight computations,
1684400	1694480	and we still get exponential capacities. So, this is the architecture of our model.
1694480	1699120	The model has different grid modules, which have different scales or periods,
1699120	1705440	and the binary grid code is projected to place code randomly. And the back projections from
1705440	1710160	place cells to grid cells are learned through associative Hebbian learning. And we observe
1710160	1715440	that when we perturb these place cells with a noisy version of place code representations,
1715440	1720320	then the network is able to successfully reconstruct all the patterns it's trained on.
1720320	1725760	So, the network is fairly robust to noise. Furthermore, this network has exponential
1725760	1730400	capacity that grows much faster than a non-modular network where the grid cells are non-modular.
1733760	1737600	Also, the network generalizes stored inputs to create stable attractor
1737600	1742240	states around every pattern in the grid coding space, despite training only over a vanishing
1742240	1747600	fraction of contiguous grid coding space. So, for instance, if my grid coding space has around
1747600	1753200	10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns,
1753200	1757440	and the network is still able to robustly reconstruct all the 10,000 patterns in the grid
1757440	1764800	coding space, which is pretty striking. Furthermore, next we add these heterosciitiative
1764800	1768720	learning on the recurrent connections on place cells to see if they can encode neighborhood
1768720	1775760	relationships or 1D sequences. And what we find is that the product of these weights converges to
1775760	1780480	this transition matrix, which is actually the analytical matrix, an analytical transition
1780480	1785920	matrix that relates contiguous grid codes in the grid coding space. Furthermore,
1785920	1789920	this network actually has perfect sequence recall given enough number of place cells to
1789920	1795520	approximate this transition matrix. And again, training on only a subset of the sequence is
1795520	1799840	enough to recall the entire sequence. So, for instance, if I train, if I have a sequence of
1799920	1806000	length 500, and I train the network on only first 150 patterns in the sequence, the network
1806000	1811120	is robustly able to reconstruct all 500 patterns in the sequence without having seen all of them
1811120	1818240	before. So, the next step in this network is to introduce sensory input. And the sensory input
1818240	1822240	would project randomly to place cells and back projections from place cells to sensory input
1822240	1827440	would be learned through associative hybrid learning. And here grid cells would form a basis,
1827440	1831040	and hippocampal places would link that basis with arbitrary sensory input.
1831680	1836080	And this combination of structured inputs and unstructured inputs could potentially enable
1836080	1840240	the storage and robust recollection of a large number of arbitrary sensory patterns from this
1840240	1847840	partial use. So, how does this connect to map fragmentation? So, in part two, we talked about
1847840	1854240	map fragmentation based on the notion of contiguous regions. And here I'm positing that
1854240	1859520	when you transition between different contiguous regions that actually corresponds to a large
1859520	1865600	contextual change, which when provided as input to this network would trigger remapping in place
1865600	1870880	cells, which would in turn cause remapping in grid cells, thus leading to the formation of local
1870880	1878800	sub maps. And how does this connect to part one, where we saw that sub maps might enable quick
1878800	1884000	learning and generalization in humans. So, this network actually enables us to anchor grid maps
1884000	1889440	to external cues through these conjunctive representations in place cells. And this anchoring
1889440	1894960	to external cues actually enables the alignment of grid maps, even when points of departure in an
1894960	1901920	environment are different, leading to adaptable representations. Furthermore, if you are in an
1901920	1907600	environment that is composed of previously seen spatial structures, then this anchoring still
1907600	1913440	enables you to pull out the right map when you're navigating through that composed spatial structure.
1914480	1920800	So, to summarize, our global hypothesis was that cognitive map is organized as a globally
1920800	1927840	topological and locally metric or Euclidean map. So, this is one illustration of a topometric map.
1927840	1932880	And we said that any sub part of this map can be termed as a sub map.
1934960	1939680	In part one, we suggested that learning sub maps that are adaptable and compositional may drive
1939680	1943360	fast learning in complex spaces. And we proposed to conduct human behavioral
1943360	1948080	experiments to test this hypothesis. In second part, we proposed an algorithmic model of map
1948080	1952720	fragmentation into sub maps based on the notion of contiguous regions. And in the third part,
1952720	1957680	we proposed a framework for a neural circuit model of map fragmentation. And overall,
1957680	1962240	we're suggesting that sub maps enable humans to build up a knowledge base of spatial structures
1962240	1967200	that they can continuously enrich and refine throughout their life by combining their existing
1967280	1973120	spatial knowledge with their new experiences. So, that's all for my talk. And before I end,
1973120	1978800	I want to acknowledge my collaborators again, Marta, Kevin, Merko and Sata. And I also want to
1978800	1984080	thank my supervisors, Ila and Josh for their continued feedback and support and discussions
1984080	1989840	on these projects. I also want to thank Matt Wilson for his feedback and valuable suggestions
1989840	1994960	during my committee meetings and the members of FeedLab and TenBomb in general for their support.
1995520	2002080	And before I end, I also want to thank the VCS department and McGowan Institute for their
2002080	2007280	continued support and for providing an environment that's conducive to research,
2007280	2011840	even in the face of this pandemic. Thank you. And I can take any questions now.
2014560	2020000	Thanks, Sue, for the great talk. We already have a question from Marta Griffin.
2020800	2024000	Great talk. I have a question about partitioning a space to sub maps.
2024640	2029120	What do you think will happen if you ask humans to intuitively partition an environment to regions?
2029120	2032960	How would there be segmentation compared to the maps given by your similarity measure?
2035280	2043040	So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively
2043040	2047440	try to segment a map, how would that compare to the segmentations which we have proposed
2047520	2053760	in part two of my talk where we have given an algorithmic model for map segmentation.
2055920	2061200	So, basically, the way I view it is that in the algorithmic model of map segmentation,
2061200	2068080	I basically took one metric map and one small portion of the map. And we said that that can be
2068080	2073200	decomposed into various fragments of sub maps. But humans are actually able to reason in much
2073200	2079680	richer environments than the ones which we saw in this algorithm. And so, there we can
2079680	2086000	view it as something like this where you might have even small spaces being segmented into
2086000	2092480	multiple maps. So, let me just see if I can, yeah, so even if you have something like this,
2093040	2096960	so even within this, so I was calling this a sub map, but even within this sub map,
2096960	2100880	we might have multiple maps. So, based on the notion of contiguous regions, you might have
2100880	2105200	a local map here, a local map here, and all these maps might be connected.
2105200	2111520	And so, the algorithmic model would predict those kinds of map segmentations, but then even
2111520	2116640	a combination of those map segmentations can be termed as a sub map. And then that is what I
2116640	2123520	was talking about in part one where I'm talking about composing the sub map which is even itself
2123520	2130000	composed of smaller maps. And we can take this representation of this map and then compose
2130000	2138960	them in various ways to build richer environments.
2150240	2154560	I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret
2154560	2160880	sub maps. How humans would interpret?
2166640	2170720	Well, it's a little bit unclear to me what you mean by interpret
2171680	2180320	sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret
2181200	2186080	segmenting this environment versus maybe a more complex environment. And I would say that
2186080	2191440	currently, this algorithm doesn't predict anything about how humans might interpret
2192240	2198000	grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could
2198000	2204480	look at it in the future. And I think some of my work in part one would potentially address that
2204480	2212960	going forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli.
2216560	2218480	Yeah. Hi. Can you hear me? Yes.
2222000	2229280	Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird.
2230160	2235440	Great talk. Can you talk a little bit more about how your model handles time?
2236080	2243120	Or I think you mentioned that it was able to handle like replay of different sequences of
2243120	2252960	states through some map. How would it be able to handle different trajectories through the same
2253040	2261120	space that might activate play cells in different sequences? So the version of the model that I
2261120	2268320	presented, that is trained on discrete patterns, right? So the sequences here, the sequences which
2268320	2273440	are trained on here through heterospecific learning are composed of these discrete patterns. So if I
2273440	2280160	say the sequences of length 500, there are 500 discrete patterns in that sequence. And when you
2280240	2286640	say time, time is something continuous. And so we have worked on extrapolating this model to
2286640	2290640	more continuous domains. And in a continuous domain, what would happen is that your sequence,
2290640	2295920	instead of being discrete patterns, would be composed of these continuous stream. And in that
2295920	2300640	case, we haven't explored what the network performance would be, but that's something
2300640	2307680	ongoing. And definitely in 2D spaces, we would like to train this model on 2D sequences and then
2307680	2313120	see, so right now, these results are pertaining to 1D sequences. But we do want to extrapolate
2313680	2319600	the results and train this model on 2D sequences to see how it performs in 2D. And I think that would
2320400	2325440	potentially address then your questions about time, because that pertains to continuous domains.
2326160	2327120	Okay, yeah, thank you.
2331120	2335200	Okay, we've got a big stream of questions coming in. The first is from Adam Eisen.
2335840	2339280	Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map
2339280	2344000	segmentation and topological association could be extended to non-spatial domains?
2345440	2352880	That's a very good question. So in FeedLab, a small group of us have been thinking about how
2352880	2359360	this could be extended. And so the idea is, I'm just going to go back to, let's see if I can just
2359440	2365120	quickly hop back to my theoretical framework slide. So really, we're building up on this
2365120	2369200	theoretical framework. And when we're thinking about non-spatial domains, we go back to this
2370000	2375600	schematic picture. And here, basically, what we're saying is, within the spatial domain, I said that
2375600	2379920	large changes in contextual input can drive remapping in place cells, which would drive
2380480	2388560	remapping in the grid coding space. So similarly, in relational domains or in discrete domains,
2389840	2394560	as we're thinking about it, we are referring back to this schematic picture. And we think that
2395600	2401440	the phenomenon of remapping actually would enable us to encode these non-eclidean relationships.
2401440	2406320	So first thing is that here, we're suggesting topological relationships. So that already
2406320	2411280	enables us to encode non-eclidean relationships. But this phenomenon of remapping also allows
2411280	2416880	us to make jumps. So if you think about a family tree, if you might try to encode it in an
2416960	2422160	eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact,
2422160	2428480	almost impossible to encode it in an eclidean space. But then if we provide a framework where
2428480	2433200	we allow topological representations and we allow jumps in this eclidean space through this phenomenon
2433200	2437920	of remapping through place cells, then that can allow us to potentially encode non-eclidean
2437920	2442320	relationships. And that's the way we are thinking. And this is still work in progress. And we
2442320	2453120	actively think about it. Next question from Nancy Camusher. How does your system decide when and
2453120	2457360	where to carve the world in the sub-maps, especially in non-built environments where the
2457360	2468080	divisions may be less obvious? So at least in part two, where I talk about fragmentation of
2468080	2477120	maps into sub-maps. Let me just go back. So in these kinds of environments, we basically describe
2477120	2482160	the principle of contiguous regions, where we have suggested that when you are navigating through
2482160	2487920	this environment, if you are in an environment where your visual observations stay more or less
2487920	2494000	consistent, then you will keep extending a map. But when you go from this region to another region,
2494000	2499040	let's say you navigate from here to here, where your visual information here is completely distinct
2499040	2503600	from your visual information here, then in that case, you will decide to segment the space. And
2504160	2509040	that's what we are predicting. We're predicting that in this case, when you travel from one
2509040	2514480	contiguous region to another, you will segment the space. And that's how you divide the space.
2514480	2519920	So that's one principle, which we are describing for map segmentation. And that might not be the
2519920	2525840	only principle, but that principle explains some of the experimental results that have been
2525840	2530640	observed newly. And there might be another principle, for instance, path integration error,
2530640	2536480	where if your path integration builds up and it reaches a certain threshold amount,
2536480	2542480	then you automatically might start a new map. But I haven't illustrated that here, but that's
2542480	2547120	also one of the other driving forces for us to actually split a space into multiple sub-maps
2547120	2558800	so that we can have more efficient representations. Okay, next question from Chen.
2559920	2563440	I was wondering if you can give some intuition about the mechanism for composition of
2563440	2568480	sub-maps. So really, this is from the happy and learning mechanism. And secondly,
2568480	2571600	can you comment on the relationship between this and the Hopfield mechanism?
2571600	2587600	So since we're talking about composition of sub-maps here, in this case, in this model,
2587600	2593200	what I'm suggesting is that we might end up forming local sub-maps by using the
2593200	2598720	phenomenon of premapping. So here I'm suggesting, again, if you have large changes in contextual
2598720	2606080	input, so for instance, let me just go back to the slide, which kind of connects this.
2607520	2615040	So here we basically suggested how this environment can be segmented into different
2615040	2621360	sub-maps. And on the mechanistic level, I'm trying to suggest that any large contextual input
2621360	2626800	is going to cause remapping in place cells, which means the cells which are firing would be
2628960	2635360	changed and they would represent a new map. And this remapping would actually enable also
2635360	2640800	remapping in grid cells through associative learning because each place cell representation
2640800	2647200	is associated with a certain grid cell representation. So when you have changes in
2647200	2651600	firing fields of place cells, that also triggers certain corresponding changes in the grid coding
2651600	2659440	phase, which is basically called remapping in grid phase. And so here I'm interpreting,
2659440	2664400	when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you
2664400	2669920	go from one contiguous region to another, that actually corresponds to a large contextual change
2669920	2676800	and that contextual change triggers remapping in this model and this remapping in place cells
2676800	2681840	then triggers remapping in grid cells phase. And that corresponds to the formation of local
2681840	2687120	sub-maps. And these topological connections from place cells to themselves might actually
2687760	2692800	represent how these local sub-maps are connected to each other. And there's also,
2693440	2696800	going back to the anatomy, there's also a possibility of splitting this population
2696800	2701600	actually into different populations. One that might just encode conjunctive representations and
2701600	2708800	another one that might have these recurrent topological connections similar to CA1 and
2708800	2713360	CA3 distinction in anatomy. But that's how I'm thinking about it at the moment.
2723280	2729760	I want to point out a quick addendum from Ila who said to Nancy's question, we consider online
2729760	2735040	segmentation decisions driven by regions or points of high surprise affordance changes and PIR.
2741200	2746640	Okay, another question from Anya, even over. Do you think the way humans form space maps depends
2746640	2751520	on whether their language uses directions that are egocentric left and right or allocentric
2751520	2759520	north and south? That's a great question. So there were experiments in both animals as well
2759520	2764080	as humans which have shown that we actually have both kinds of representations. We have
2764080	2770560	egocentric representations and we also have allocentric representations. And the representations
2770560	2779360	in the hippocampal inter-animal system are usually allocentric, but the thalamus is the part of the
2779360	2785040	brain which is responsible for egocentric representations. So really, when we're navigating
2785040	2790400	spaces, we are probably using mixed strategies. We do use our allocentric representations,
2790400	2795920	but in some cases we might be using egocentric representations. And that's mostly also true
2795920	2801040	for routes which we are traversing very frequently. So if there's a route which I take every day,
2801040	2807760	let's say going from my home to BCS, then that converges to root learning. But if I've had very
2807760	2813200	less experience in any environment, then I'm mostly using allocentric representations to
2813200	2817600	actually make my way through that environment. And hence, allocentric representations are
2817600	2825120	actually attributed to being able to make novel inferences. And when I'm building circuits here
2825120	2829440	using grid cells and place cells, I'm mostly talking about allocentric representations.
2829440	2835760	But then when I was talking about submaps in human experiments, I'm kind of
2835760	2840400	agnostic to the fact that whether those representations are egocentric or allocentric
2840400	2847360	because even if you learn an egocentric strategy, even in that case, if you determine that there's
2847360	2852640	a repeating structure to the environment and you recall that this is the same environment that I've
2852640	2858000	seen before, then you can still apply the same egocentric strategy or an allocentric strategy
2858000	2864320	depending on which one you decide to use at that point. But going back to the main question,
2865200	2871680	the experimental data shows that we're actually basically learning both kinds of strategies.
2877920	2883840	Okay, Senke's Pellevan says, great talk. What are the large scale, sorry, where are the large
2883840	2893920	scale topological relations coded in the network? So in the network model, our focus, so in this
2893920	2898480	case, we are basically encoding topological relationships using the recurrent connections
2898480	2905360	on the place cells. But for large scale topological relationships, what we are moving towards is
2905360	2910160	splitting this population into two distinct populations. So one population that will only
2910160	2915520	have conjunctive representations, and perhaps another population which will have the recurrent
2915520	2921600	connections similar to the distinction between CA1 and CA3 in the brain. And it's true that in this
2921600	2926480	network model, we don't really distinguish between small scale and large scale topological
2926480	2930320	relationships. But by introducing this additional population, we hope to abstract away the large
2930320	2938240	scale relationships. Also, even in this case, if we have minute relationships between spaces,
2938240	2943440	then we can also extract away the large scale relationships. But again, adding another population
2944320	2947440	might make that easier to make that abstraction.
2952560	2958560	Okay. Nancy has another comment. There must be some way to represent globally metric information.
2959120	2962720	For example, I know the approximate distance and angle from my current position to far away
2962720	2966480	locations in my world. Do I understand right that you would say this information has to be
2966480	2970080	pieced together from just the topological relationship between the sub maps that connect
2970080	2977280	those locations? So that's a great question. I also sometimes wonder because like when I'm here,
2978160	2983520	I can kind of look very far in space and be able to tell that, okay, I need to go in this direction.
2984480	2991760	And that's definitely metric information. And so going back to the labeled graph hypothesis,
2992720	3002720	which was suggested by Bill Warren. So even in this hypothesis, I'm just saying that this is
3002800	3008320	we are learning metric representations here, which are consistent Euclidean representations,
3008320	3012960	which means that they actually obey all the postulates of Euclidean geometry.
3013920	3019600	And the connections between them still have metric information. There's still information
3019600	3027360	about angles and directions, but they might not obey all postulates of Euclidean geometry. So for
3027360	3031680	instance, you might have some inconsistent representations. You might not know exactly
3031680	3036400	what distances and displacements you have to go in order to reach a goal, but you know approximately
3036400	3041760	which distance and which angle. So even the topometric representation is not contradicting
3041760	3046640	the fact that you might have approximate distances and angles. But what it's saying is that you have
3046640	3053200	very accurate metric information about small pieces of space, but then for faraway pieces of
3053200	3056880	space, you might still have approximate distances, angles, which help you navigate.
3062640	3069040	Okay, another question from Sean Chen. Sorry for my pronunciations. Great talk.
3069040	3072640	Could you elaborate the mechanism of exponential number of memory capacity in your model?
3080720	3087440	So here's the, here's the network, right, where I'm showing that the network has exponential
3087440	3094240	capacity. So basically in this network, we have different grid modules which have different periods
3094240	3099680	and this is the grid code is actually binary. And we project this grid code randomly using
3099680	3104880	random weight matrix to place cells. And the number of place cells in this case is much larger than
3104880	3111360	the number of grid cells. And now we learn these back projections from places to grid cells using
3111360	3116160	Hebbian learning. So it's just simple Hebbian learning. I don't have the equation. Basically,
3116160	3121200	this is the equation, right? You can consider these as place cell patterns. And this is how we
3121200	3127760	compute the weight matrix for the weights going back from place cells to grid cells. And once we've
3127760	3132880	done that, then the way we test for memory is basically by perturbing the place code patterns.
3132880	3138560	So we put up the place cells by a noisy version of the place code pattern, right? So in this graph,
3138560	3146080	I'm showing that if you perturb them with 20% noise, then with 300 place cells, you get an output
3146080	3150000	noise of zero, which means you get perfect reconstruction of the patterns that you're
3150000	3155040	perturbing the network with. And that's how I'm defining exponential memory, right? And
3155040	3160800	when I say exponential memory, what I'm suggesting is that you can, as you increase the number of
3160800	3166000	cells in the network, the number of patterns that you can reconstruct from the noisy versions
3166000	3167440	actually grows exponentially.
3179440	3185600	Okay, I'd like to ask a question as well. I'm curious to hear your thoughts on sort of
3185600	3192640	computational level descriptions here. So it seems like your model is largely
3192640	3197920	tuned towards sort of prediction, accurate prediction of your local spatial environment
3200640	3206800	and motive mechanisms for doing this well. And I wonder what you think about computational level
3207760	3212960	accounts of this to be more, to ask a more concrete question. Do you think there's a role
3212960	3218560	of reward or something other than accurate, just accurate prediction in the shaping of
3218560	3224480	neural representations of space? Okay, so you're asking whether there is a role for reward
3225360	3229760	in shaping the representations of space in the mechanistic model?
3231840	3238000	I mean, more generally, what's the objective driving this algorithm? Is it about
3238000	3240240	accurate prediction or is there something more than prediction?
3241200	3252720	Okay, so you're asking what is kind of like the motivation for building the mechanistic model?
3254480	3257120	Yeah, yeah, I mean, some of the graphs you're showing here about,
3258720	3263840	some of the graphs you showed, for example, were about recall. And recall, accurate reconstruction
3263840	3269760	of an environment might be one objective. But it seems like you could say that this is a
3269840	3274080	mechanism evolved for some purpose, but it's just that or different than that.
3274880	3283200	So, right, so these two questions, which I started with, which are related to capacity of the network,
3284160	3288480	and whether we can do, whether we can learn sequences, these are basically questions which I
3288480	3293200	started with, because I wanted to explore the theoretical properties of these circuits, right,
3293200	3297680	given the coding properties that we know of grid cells and place cells and given
3297680	3303920	biologically plausible learning tools through have been learning, what are the coding properties
3303920	3308640	that these networks can exhibit, right, how much capacity they have, how much information can they
3308640	3314800	store. And so, yes, even in these models, since we are talking about reconstruction,
3315680	3320320	basically says that if you have some noise, if you have some noisy estimate of where you are in
3320320	3326720	space, then this kind of a network can help you clean that estimate, right, and reach a cleaner
3326720	3333120	version of where you might be in space and help you localize in space. And also, these temporal
3333120	3337680	connectivity on the place cells kind of have a predictive component to them, right, because you
3337680	3341440	can import sequences. So, when you are at a particular position, you might be able to predict
3341440	3348560	what's coming next. And so, these are kind of small components, which I'm using to go towards
3348560	3353840	building a model of remapping in space, right, because ultimately my goal is to be able to
3353840	3358320	figure out how these neural circuits might lead to the formation of these sub-maps. And by knowing
3358320	3363440	the properties of these circuits, then I'll be better able to construct and build those networks,
3363440	3367360	which actually can build small local sub-maps of environments.
3372240	3377680	I'm reading out a comment from Ila. To my question, you could view this as a structure
3377680	3381680	learning even without reward. A reinforcement learner, including the brain, could use these
3381680	3390880	learning structures. Cool. Well, I think we're actually over time now. That was a great talk,
3390880	3398800	Sue. Thanks for sharing with us. Of course, thank you. So, we'll be back next week for another
3398800	3411360	talk from Eli Pollock. Until then, have a good Thanksgiving, and I'll see you next week.
