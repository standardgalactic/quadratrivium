1
00:00:00,000 --> 00:00:06,640
the Q&A button, or raise your hand after the talk and we can call on you to ask a question

2
00:00:06,640 --> 00:00:12,720
by video or audio. A little bit of logistics, we still have an open slot on December 8th,

3
00:00:12,720 --> 00:00:17,600
so if you have a talk at any length, it can be less than an hour or a full hour,

4
00:00:17,600 --> 00:00:22,960
and you want to share with the BCS Cog Launch community, just get in touch with me by email,

5
00:00:22,960 --> 00:00:27,520
you can respond to that announcement email that you got for this event, and we can work that out.

6
00:00:27,920 --> 00:00:33,840
With that, let's get to our main content. So, Sue, we're telling us today about rapid

7
00:00:33,840 --> 00:00:43,440
generalization of knowledge and structured domains. Take it away, Sue.

8
00:00:57,520 --> 00:01:13,840
All right. So, as John mentioned, I'm Sue. I'm a third-year PhD student in the BCS department,

9
00:01:13,840 --> 00:01:19,280
and I'm co-advised by Professor Ela Feed and Professor Josh Tenenbaum. And in general,

10
00:01:19,280 --> 00:01:24,000
the question I'm interested in is, how do people generalize their learning to novel situations?

11
00:01:24,000 --> 00:01:28,320
And in any given domain, if the underlying space is structured, we might learn those

12
00:01:28,320 --> 00:01:32,800
underlying structures independently of the sensory observations, and that might in turn

13
00:01:32,800 --> 00:01:38,080
help us generalize to novel situations. So, what I'm presenting today is a step towards

14
00:01:38,080 --> 00:01:43,200
answering this question. I'll start with motivation, and then I'll make a case for why

15
00:01:43,200 --> 00:01:48,000
hippocampal entorhinal system is an important system to study if you're interested in generalization.

16
00:01:48,560 --> 00:01:52,720
And then I've divided the rest of my talk in three parts, and I'll give you a brief overview

17
00:01:52,720 --> 00:01:58,880
of those three parts before I go into the details of each of those. So, imagine you go to Costco

18
00:01:58,880 --> 00:02:04,320
in Waltham, since there's no Costco in Cambridge, and you learn the map of Costco. So, now you know

19
00:02:04,320 --> 00:02:08,800
where the bakery section is or where the fruit section is, and now imagine you go to a completely

20
00:02:08,800 --> 00:02:14,160
different country, let's say Canada, you go to Waterloo, and you go to Costco there. Even there,

21
00:02:14,160 --> 00:02:18,240
Costco might have the same layout, or it might have a layout which is some transformation of the

22
00:02:18,240 --> 00:02:21,920
original layout. For instance, it might be a reflective version of the original, or there

23
00:02:21,920 --> 00:02:26,560
might be minor changes. And despite of that, you're still able to find the things you're looking for

24
00:02:26,560 --> 00:02:32,240
using your previous knowledge of the map of Costco. Another example is roundabouts. So, if you learn

25
00:02:32,240 --> 00:02:37,920
to go about a roundabout in Cambridge, then even if you go to any other country or city,

26
00:02:37,920 --> 00:02:42,720
then you will be able to use your previous knowledge to actually navigate through that roundabout.

27
00:02:43,440 --> 00:02:47,680
So, generally, we learn novel environments as compositions of spatial structures that we've

28
00:02:47,680 --> 00:02:52,320
already seen before, and that allows us to quickly generalize and learn new spatial

29
00:02:52,320 --> 00:02:56,560
environments. For instance, when you go to a new city, you might encounter Costco again,

30
00:02:56,560 --> 00:03:01,040
you might encounter a roundabout again, and you know which map to pull out when you are in Costco,

31
00:03:01,040 --> 00:03:08,800
and which map to pull out when you are navigating around a roundabout. And so, here's another example

32
00:03:08,800 --> 00:03:14,880
where this is a hotel which has symmetric left and right wings, and if one has explored the left

33
00:03:14,880 --> 00:03:19,600
wing of this hotel, then they might be very quickly able to generalize their learning to the

34
00:03:19,600 --> 00:03:23,600
right wing and make inferences about the right wing, even if they haven't really directly explored

35
00:03:23,600 --> 00:03:28,800
the right wing. So, humans are actually very good at making these complex inferences from

36
00:03:28,800 --> 00:03:34,400
just very sparse observations, and this ability has been suggested to be a result of a systematic

37
00:03:34,400 --> 00:03:39,920
organization of knowledge called the cognitive map. And hippocampal entorhinal system is known to

38
00:03:39,920 --> 00:03:45,440
be important for the construction of this cognitive map. So, for instance, rodents when

39
00:03:45,440 --> 00:03:51,360
they explore a 2D spatial environment, it has been found that hippocampus has these place cells

40
00:03:51,360 --> 00:03:57,120
which code locations in the 2D environment, and there are good cells in the entorhinal cortex

41
00:03:57,120 --> 00:04:02,560
which show this hexagonally symmetric firing fields, which are periodic, and which have been

42
00:04:02,560 --> 00:04:07,760
thought to encode location, but also cell motion-based euclidean displacement. And these

43
00:04:07,760 --> 00:04:12,160
codings are important for the construction of cognitive map because they provide an allocentric

44
00:04:12,160 --> 00:04:18,160
representation. So, given that the hippocampal entorhinal system encodes spatial variables,

45
00:04:18,160 --> 00:04:23,120
the next question is, can this also be used to represent other continuous task variables

46
00:04:23,120 --> 00:04:29,440
other than space? And the answer to this question is yes, and I'll give one example. So, here in

47
00:04:29,440 --> 00:04:33,600
this experiment has been found that cells in hippocampus and entorhinal cortex respond to

48
00:04:33,600 --> 00:04:39,120
task-relevant variables like sound frequency. So, in this task, rodents pull this lever,

49
00:04:39,120 --> 00:04:43,520
and as they pull the lever, the frequency of the sound coming from this sound source

50
00:04:43,520 --> 00:04:47,360
actually keeps increasing, and they have to release the lever when this frequency is in

51
00:04:47,360 --> 00:04:52,560
this target zone. And what is found is that in hippocampus and entorhinal cortex, there are cells

52
00:04:52,560 --> 00:04:59,680
that fire for specific frequencies during the sound modulation task. So, that shows that

53
00:05:00,640 --> 00:05:05,680
entorhinal cortex is a system that is able to represent continuous task variables,

54
00:05:05,680 --> 00:05:10,480
even other than space. And so, our next question is, could this system also help us organize

55
00:05:10,480 --> 00:05:15,440
and navigate discrete knowledge? And family trees are one example of discrete knowledge.

56
00:05:15,440 --> 00:05:20,080
So, in family trees, there's an underlying hierarchical structure that we learn. And once we

57
00:05:20,080 --> 00:05:23,680
know that structure, then we can apply that structure to my family tree or to your family

58
00:05:23,680 --> 00:05:27,840
tree or anyone's family tree, and we can generalize that knowledge. So, we can make inferences like

59
00:05:27,840 --> 00:05:33,600
this, because Olivia is Emily's sister and Sam is Emily's son, Sam must be Olivia's nephew.

60
00:05:33,600 --> 00:05:38,080
So, even though we haven't directly observed this relationship, but just by mere observation of

61
00:05:38,080 --> 00:05:42,080
these two relationships, we are able to infer this relationship, because we know the underlying

62
00:05:42,080 --> 00:05:47,440
hierarchical structure of this family tree. So, it is possible that hippocampal entorhinal system

63
00:05:47,440 --> 00:05:52,400
might allow organization of this kind of a discrete knowledge, but that's only possible if it allows

64
00:05:52,400 --> 00:05:58,000
encoding non-Equidian relationships. With that, I'll go back to the spatial domain,

65
00:05:58,000 --> 00:06:03,360
and I'll talk about how spatial knowledge might be organized. And when I talk about organization,

66
00:06:03,360 --> 00:06:08,480
I'll point to the fact that even in a continuous domain like space, it might be possible that

67
00:06:08,480 --> 00:06:13,120
we have both Euclidean and non-Euclidean components to represent space itself,

68
00:06:13,120 --> 00:06:17,520
and thus making the system generalizable to even discrete domains.

69
00:06:18,240 --> 00:06:26,400
So, here's an experiment conducted by Bill Warren, where they show that people actually do not learn

70
00:06:26,400 --> 00:06:32,080
a global Euclidean map of space. So, in this experiment, they constructed virtual environments,

71
00:06:32,080 --> 00:06:38,640
and they basically, the task was for people, for human subjects. So, this was a human behavioral

72
00:06:38,640 --> 00:06:43,200
experiment, and human subjects were asked to go to different landmarks in this spatial environment,

73
00:06:43,200 --> 00:06:47,040
and they also built counterparts of the spatial environment, which were non-Euclidean,

74
00:06:47,200 --> 00:06:52,400
in these environments. So, when you enter one part of the wormhole, you seamlessly exit from the

75
00:06:52,400 --> 00:06:57,520
other end of the wormhole, and subjects were not aware of the existence of these wormholes.

76
00:06:58,080 --> 00:07:03,520
And basically, what they found through various manipulations of the experiment was that people

77
00:07:03,520 --> 00:07:08,960
do not actually learn a global Euclidean map, but rather a labeled graph, like representation,

78
00:07:08,960 --> 00:07:14,080
where the nodes represent places, the edges represent approximate distances between these

79
00:07:14,080 --> 00:07:19,920
places, and the node labels, which are angles, represent approximate angles between these places.

80
00:07:20,800 --> 00:07:24,480
And so, we built on this representation, and we proposed that people might be

81
00:07:24,480 --> 00:07:30,800
representing topometric maps, which are locally metric or Euclidean, but globally topological.

82
00:07:30,800 --> 00:07:35,040
And the main advantage of this kind of representation is that it allows us to combine

83
00:07:35,040 --> 00:07:40,560
accurate local maps into a global map, which might be inconsistent, but it still provides

84
00:07:40,560 --> 00:07:45,600
enough sufficient information for navigation. So, the next question is,

85
00:07:46,400 --> 00:07:51,120
can this kind of a topometric map be implemented in the brain? And if so, how?

86
00:07:51,120 --> 00:07:55,120
And next, I'm providing a theoretical framework for how it might be possible to represent such

87
00:07:55,120 --> 00:08:00,480
topometric maps using the place cells and grid cells found in hippocampal endorhinal cortex.

88
00:08:00,480 --> 00:08:04,320
So, this is a topometric representation of space. You can see that these are metric maps

89
00:08:04,320 --> 00:08:09,360
connected topologically by these connections. And here, on this side, I'm showing a grid coding

90
00:08:09,360 --> 00:08:14,720
space, which is dense coding space with large capacity. And here, I'm showing place coding

91
00:08:14,720 --> 00:08:20,160
space, which also has large capacity, but it spars, so it can receive sensory inputs and form

92
00:08:20,160 --> 00:08:26,160
conjunctive representations. And so, in this schematic, small changes, sorry, large changes

93
00:08:26,160 --> 00:08:33,120
in contextual input from our spatial domain was remapping in the place cells, which in turn

94
00:08:33,120 --> 00:08:37,280
trigger remapping in the grid cells, enabling the formation of these local metric maps that

95
00:08:37,280 --> 00:08:44,160
can be reused. And so, on this schematic, we can really take any subpart of this schematic and

96
00:08:44,160 --> 00:08:50,400
call it a sub map. And this allows us to compose sub maps, because once we have learned a particular

97
00:08:50,400 --> 00:08:55,680
sub map, then we can actually encounter the sub map in a completely novel situation and still be able

98
00:08:55,680 --> 00:09:05,200
to spatially navigate and reason through it. Another thing that it allows us is learning

99
00:09:05,200 --> 00:09:10,160
non-ecredient relationships, because place cells actually encode topological relationships, enabling

100
00:09:10,880 --> 00:09:16,240
the representation of non-ecredient relations. So, now I'm going to describe the three parts

101
00:09:16,240 --> 00:09:21,760
in which I've divided the rest of the talk. So, in the first part, I'm probing whether

102
00:09:21,760 --> 00:09:28,080
sub maps drive past learning in complex spaces using human behavioral experiments. In the second part,

103
00:09:28,640 --> 00:09:33,600
I will talk about determining which principles might guide fragmentation of a space into

104
00:09:33,600 --> 00:09:39,200
sub maps. And finally, in the third part, I'll propose a framework for building a neural model

105
00:09:39,200 --> 00:09:46,800
of map fragmentation. So, in the first part, I'm probing whether sub maps drive fast learning

106
00:09:46,800 --> 00:09:51,200
in complex spaces, specifically in humans. And this work is in collaboration with Marta Krivind,

107
00:09:51,200 --> 00:09:57,040
who's a postdoc in Tenenbaum Lab, and Kevin, who's an undergrad in the CS department.

108
00:09:57,920 --> 00:10:03,120
So, here I hypothesize that humans learn adaptable and compositional sub maps of spatial

109
00:10:03,120 --> 00:10:08,000
structures. So, for instance, this is a baseline environment, and this is a top-down view showing

110
00:10:08,000 --> 00:10:13,600
an environment with four rooms. And here I've shown certain transformations of this environment

111
00:10:13,600 --> 00:10:19,440
generated by small generator programs. And you can see this is the same environment rotated,

112
00:10:19,440 --> 00:10:24,240
because now you're entering from this point, so it might appear rotated to you. And here's a reflection

113
00:10:24,240 --> 00:10:30,720
of the same environment. Here is a transformation where we've removed the wall and added a shortcut,

114
00:10:30,720 --> 00:10:35,200
and here we've added a wall. And this is just the repetition of the same environments.

115
00:10:35,200 --> 00:10:39,280
And there's another transformation which is scaling, where you can imagine this environment

116
00:10:39,280 --> 00:10:44,480
scaled up to a bigger size, but having the same geometrical layout. And what I'm suggesting is

117
00:10:44,480 --> 00:10:49,120
that once people have learned the map of the space line environment, their representations

118
00:10:49,120 --> 00:10:54,160
might be adaptable to some or all of these transformations. And furthermore, people might

119
00:10:54,160 --> 00:10:59,360
represent richer spaces by combining these maps and their transformations, leading to quick

120
00:10:59,360 --> 00:11:05,360
generalization and learning. So, this can be modeled using Bayesian program learning framework,

121
00:11:05,360 --> 00:11:09,520
where concepts are represented as simple programs, and rich concepts can be built

122
00:11:09,520 --> 00:11:15,840
compositionally from them using a higher-level generative model. So, there is neural evidence

123
00:11:15,840 --> 00:11:21,920
for this hypothesis. So, in this, this is an experiment by the Tonakawa Lab, and they show

124
00:11:21,920 --> 00:11:27,600
that when a rodent goes through this environment in four labs, there are cells which fire specifically

125
00:11:27,600 --> 00:11:32,560
for particular places in this environment, but there are also cells which encode, which are

126
00:11:32,560 --> 00:11:37,200
event-specific and encode specific labs. So, for instance, there are cells which show increased

127
00:11:37,200 --> 00:11:41,440
fighting rates as you go from lab one to lab four, and there are also cells which only fire

128
00:11:41,440 --> 00:11:46,240
specifically on lab one or on lab two and so on. And so, what I'm suggesting is that when we have

129
00:11:46,240 --> 00:11:51,440
repetitions of the same environment, there might be cells that encode the basic map of this environment,

130
00:11:51,440 --> 00:11:56,560
which, which is consistent across these occurrences, but there might be a second set of cells,

131
00:11:56,560 --> 00:12:01,120
which are event-specific and might encode which instance of this environment we are on.

132
00:12:02,160 --> 00:12:09,200
Here's another example, where this is an example of scaling, where the rodent actually just explores

133
00:12:09,200 --> 00:12:14,000
this circular environment, and this is the place field found in that circular environment,

134
00:12:14,000 --> 00:12:18,240
and when the circular environment is scaled up to a bigger size, the place field also scales

135
00:12:18,240 --> 00:12:22,640
according to the size of the environment. So, this shows that the map which the rodent has

136
00:12:22,640 --> 00:12:27,200
learned of this environment is actually adaptable to this transformation of scaling to a bigger

137
00:12:27,200 --> 00:12:32,240
size, and map also scales proportionately with the size of the environment. And here's a third

138
00:12:32,240 --> 00:12:38,480
example, where rodents form different maps, place maps in these different environments,

139
00:12:38,480 --> 00:12:42,080
and when these environments are composed by connecting them through a corridor,

140
00:12:42,080 --> 00:12:46,560
rodents end up using the same maps which they had learned before for these environments. And

141
00:12:46,560 --> 00:12:51,360
furthermore, if I replace this environment with one of the previous environments seen before,

142
00:12:51,360 --> 00:12:56,400
then remapping is only observed in this part of the environment, and this part of the environment

143
00:12:56,400 --> 00:13:02,800
actually stays the same using the same previous map. So, this provides some evidence in support of

144
00:13:02,800 --> 00:13:08,800
composition of independent local sub-maps. So, in my experiment, I aim to assess whether people

145
00:13:08,800 --> 00:13:14,080
learn sub-maps on spatial structures and use them rationally in exploration, and I hypothesized that

146
00:13:14,080 --> 00:13:19,120
people might learn sub-maps that are adaptable and compositional, and my first alternate hypothesis

147
00:13:19,200 --> 00:13:22,560
is that they might learn sub-maps of spatial structures that might not be adaptable or

148
00:13:22,560 --> 00:13:28,000
compositional to certain transformations, and the last alternate hypothesis is that people

149
00:13:28,000 --> 00:13:32,880
might just learn a global representation of environments without learning any sub-maps.

150
00:13:32,880 --> 00:13:39,200
So, in order to test this hypothesis, we're building this task where we are building 3D

151
00:13:39,200 --> 00:13:44,640
virtual environments using Unity, and these environments have this repeating structure,

152
00:13:44,720 --> 00:13:50,240
and the task is for the subjects to find maximum amount of diamonds embedded in these environments

153
00:13:50,240 --> 00:13:55,120
in a limited amount of time given to them, and in order to test adaptability and composition,

154
00:13:55,120 --> 00:13:59,120
we can also have these repetitions be transformations of each other, for instance,

155
00:13:59,120 --> 00:14:03,360
here it's a reflection, or we can also have these environments composed of different structures to

156
00:14:03,360 --> 00:14:07,600
see whether people can compose their representations or structures that they've already seen.

157
00:14:08,560 --> 00:14:13,840
So, here's an example. Here you see that a person is navigating through corridor and enters a

158
00:14:13,840 --> 00:14:18,720
structure, and they go to one of the rooms and they do not find anything there, and then they

159
00:14:18,720 --> 00:14:24,160
decide to go to the other room, and they end up finding a reward there, and now they're going

160
00:14:24,160 --> 00:14:30,480
back to the corridor and they continue exploring the environment, and when they enter another section,

161
00:14:30,480 --> 00:14:35,920
if they show a preferential navigation strategy towards the room that has a reward, then that

162
00:14:35,920 --> 00:14:40,640
indicates that they have realized that there's a repeating structure in the environment and indicates

163
00:14:40,640 --> 00:14:44,960
a possibility that people might be learning sub-maps and identifying sub-maps as they're

164
00:14:44,960 --> 00:14:50,560
navigating the spatial environments, and furthermore, if we do find through the experiment that people

165
00:14:50,560 --> 00:14:56,160
actually learn sub-maps, then we can use similar environments to design experiments where we can

166
00:14:56,160 --> 00:15:03,120
test for adaptability to transformations of environments and also for composition of

167
00:15:03,120 --> 00:15:10,400
different spatial structures. So, that takes me to the next part of the talk, which is

168
00:15:10,480 --> 00:15:14,640
determining which principles guide fragmentation into sub-maps. So, since we are seeing that

169
00:15:14,640 --> 00:15:20,800
sub-maps drive fast learning, the next natural question becomes what determines this fragmentation

170
00:15:20,800 --> 00:15:25,760
of a spatial environment into sub-maps, and this work is in collaboration with Mirko Klukas,

171
00:15:25,760 --> 00:15:32,560
who is a post-doc in the feed lab. So, here my hypothesis is that neural remapping is a signature

172
00:15:32,560 --> 00:15:38,960
of sub-map reconstruction. So, here I'll explain what remapping is. So, basically, this is a 2D

173
00:15:39,040 --> 00:15:46,320
environment, and when the animal just explores this 2D environment, we find hexagonally periodic

174
00:15:46,320 --> 00:15:54,240
grid fields in the entorhinal cortex, and when you actually insert these walls in this environment,

175
00:15:54,240 --> 00:15:59,520
then what is observed is that when the animal turns, then this grid field which is formed either

176
00:15:59,520 --> 00:16:05,200
re-orients or shifts, and this is called remapping, when the grid field actually re-orients or shifts

177
00:16:05,200 --> 00:16:10,640
from its original orientation. And so, what is observed is that animals actually end up using

178
00:16:10,640 --> 00:16:16,800
the same grid maps in alternate arms. So, this indicates the use of maps. And here's another

179
00:16:16,800 --> 00:16:21,760
example of the use of maps. So, basically, this is a 2-room environment, and animals explore

180
00:16:21,760 --> 00:16:29,760
this environment, and it is seen that eventually the map formed in environment A is the same as the

181
00:16:29,760 --> 00:16:34,880
map formed in environment B, over short time scales. So, this is another example of the fact

182
00:16:34,880 --> 00:16:40,480
that animals are reusing the maps in both the rooms which look very similar. So, what I'm suggesting

183
00:16:40,480 --> 00:16:46,080
is that this field repetition doesn't result from localization error or purely due to disorientation,

184
00:16:46,080 --> 00:16:52,320
because even when you use transparent walls in this environment, you still see that the grid

185
00:16:52,320 --> 00:16:57,680
maps are being reused in alternate arms, even though the animal can see through these transparent

186
00:16:57,680 --> 00:17:04,240
walls. Furthermore, if you extend this 2-room environment to a 4-room environment, you still

187
00:17:04,240 --> 00:17:09,360
see field repetition in all of these rooms, which suggests that animals are actually reusing

188
00:17:09,360 --> 00:17:13,920
sub-maps in a calculated way for efficient representation, rather than just being disoriented.

189
00:17:18,480 --> 00:17:23,120
So, next I talk about existing models of remapping, and there are two classes of models.

190
00:17:23,120 --> 00:17:28,400
One class of model suggests that remapping is driven by sensory ambiguity. So, for instance,

191
00:17:28,400 --> 00:17:33,120
if you are in an environment that looks similar to an environment you've been before, either in

192
00:17:33,120 --> 00:17:38,800
terms of its geometry or its visual observations, then you might end up using the same map that

193
00:17:38,800 --> 00:17:43,440
you had learned from a previous environment. Then there's another class of models that suggests

194
00:17:43,440 --> 00:17:48,080
that remapping is based on environment topology, instead of just sensory ambiguity. So, here,

195
00:17:48,720 --> 00:17:52,800
each state in the environment is represented in terms of its successor states, and it's called

196
00:17:52,800 --> 00:17:58,080
a successor representation. And this successor representation actually ends up looking similar

197
00:17:58,080 --> 00:18:02,640
to place feeds, and if you do an identity composition on these successor representations,

198
00:18:02,640 --> 00:18:08,800
then you get fields that are very similar to grid fields. And this successor representation

199
00:18:09,280 --> 00:18:13,840
encapsulates inherent dynamics of the environment, as well as the policy that the agent is following.

200
00:18:13,840 --> 00:18:19,680
However, there are other approaches like the graph-leplacian approach, which is policy independent.

201
00:18:21,120 --> 00:18:24,960
So, what are some of the limitations of these models? So, the models that are

202
00:18:24,960 --> 00:18:30,160
based on sensory ambiguity do not have remapping without sensory ambiguity. So, in an environment

203
00:18:30,160 --> 00:18:36,640
like this, these two regions actually look very different. These models will not have any map

204
00:18:36,640 --> 00:18:45,520
fragmentation or remapping. However, on the other hand, models which are based on environment topology,

205
00:18:45,520 --> 00:18:50,880
actually, it's not very clear how remapping would happen on first visit in these environments,

206
00:18:50,880 --> 00:18:54,240
because you need to build up the successor representation or the transition matrix of

207
00:18:54,240 --> 00:19:01,200
the environment before you can observe the grid fields. So, in our model, we address these limitations,

208
00:19:01,200 --> 00:19:05,360
and we have remapping on first visit, and we also have remapping without sensory ambiguity.

209
00:19:05,360 --> 00:19:10,160
And next, I'll go into the details of our model. So, we interpret grid remapping as

210
00:19:10,160 --> 00:19:14,960
fragmentation into submaps. Why is this a useful interpretation? That's because

211
00:19:14,960 --> 00:19:19,520
remapping enables topological representation. So, for instance, if we are dividing this

212
00:19:19,520 --> 00:19:24,400
environment into submaps, then we also need to store the relationships between these submaps,

213
00:19:24,960 --> 00:19:29,280
and this enables a compact topological representation, which is beneficial for planning.

214
00:19:30,960 --> 00:19:35,200
Second reason is that remapping reduces path integration errors. So, if you try to learn a

215
00:19:35,200 --> 00:19:40,720
global map, it can very quickly become inconsistent because of accumulation of path integration errors.

216
00:19:40,720 --> 00:19:45,520
But if you divide the environment in submaps, then it becomes easier to map the environment.

217
00:19:45,520 --> 00:19:50,880
And this has been shown by using Atlas framework in robotics, where they divide the environment

218
00:19:50,880 --> 00:19:55,440
into submaps in order to map the environment, and it works very well for large environments.

219
00:19:56,960 --> 00:20:01,440
And the third reason is that remapping enables representation of abstract cognitive spaces,

220
00:20:01,440 --> 00:20:07,200
because it allows representation of non-euclidean structures. So, in our model, we have two

221
00:20:07,200 --> 00:20:12,400
possibilities. Either we can extend an old map, or we can decide to remap. And when we decide to

222
00:20:12,400 --> 00:20:17,440
remap, we can either remap to a new map or remap to an existing map. So, for instance,

223
00:20:17,440 --> 00:20:24,320
in the experiments we saw that in the square environment without walls, the map that the

224
00:20:24,320 --> 00:20:30,400
animal learns is always extended. But when we insert these walls in this environment,

225
00:20:30,400 --> 00:20:35,440
then the map is extended within lane one. But when you turn from lane one to lane two,

226
00:20:35,440 --> 00:20:39,520
you actually remap to a new map. And when you turn from lane two to lane three,

227
00:20:39,520 --> 00:20:43,360
you end up remapping to an existing map, which is the same as lane one.

228
00:20:44,560 --> 00:20:50,240
And similarly, in the two room experiment, we saw that when you go from room one to the corridor,

229
00:20:50,240 --> 00:20:54,880
you end up mapping to a new map. And when you go from corridor to room two, you actually end up

230
00:20:54,880 --> 00:20:59,680
remapping to an existing map. So, next, I'm going to talk about how we decide whether we are going

231
00:20:59,680 --> 00:21:05,840
to extend a map or whether we should be remapping. So, in our model, remapping is based on the

232
00:21:05,840 --> 00:21:10,800
notion of contiguous regions. And a contiguous region is a region such that when I stay within

233
00:21:10,800 --> 00:21:14,880
that region, my visual observations change very little. And these contiguous regions are connected

234
00:21:14,880 --> 00:21:19,440
by these bottleneck states. And this is aligned with the experimental data, which we have seen,

235
00:21:19,440 --> 00:21:25,280
which suggests special rule of doorways and corridors. So, now we formalize the concept

236
00:21:25,280 --> 00:21:30,240
of contiguous regions by defining a measure of similarity. So, we define similarity as the ability

237
00:21:30,240 --> 00:21:35,200
to predict observations at one pose from the observations made at another pose. And so,

238
00:21:35,200 --> 00:21:39,920
the overlap between the observations made at two poses actually is a notion of similarity.

239
00:21:39,920 --> 00:21:44,640
And this formalizes the concept of contiguous regions as a region where any two points are

240
00:21:44,640 --> 00:21:49,520
similar. And here, I'm showing that similarity actually decreases when you transition between

241
00:21:49,520 --> 00:21:53,920
contiguous regions. So, if you look at points which are within this contiguous region,

242
00:21:53,920 --> 00:21:59,120
their similarity is high with respect to this point. But for points which are in other regions,

243
00:22:00,080 --> 00:22:02,800
the similarity is pretty low as compared to this point.

244
00:22:04,800 --> 00:22:09,440
So, then we can use this notion of similarity to define density in order to do density-based

245
00:22:09,440 --> 00:22:14,800
clustering. And here, we define density as a similarity between any pose X and its

246
00:22:14,800 --> 00:22:22,000
mth nearest neighbor. And this notion can be used with any greedy algorithm like optics to

247
00:22:22,000 --> 00:22:26,080
generate fragmentations of the environment. And here, I'm showing one example of fragmentations

248
00:22:26,080 --> 00:22:30,240
of the environment where it gives four different clusters corresponding to these four different

249
00:22:30,240 --> 00:22:38,800
colors shown here. So, given that contiguity is a local property, we can also try to compute

250
00:22:38,800 --> 00:22:44,000
segmentations online by predicting current observations from the past. And in this case,

251
00:22:44,000 --> 00:22:48,800
observations can be represented by boundary vector cells. And we can implement a short-term memory

252
00:22:48,800 --> 00:22:52,800
which stores exponential moving average of boundary vector cell activations

253
00:22:52,800 --> 00:22:57,600
to approximate the similarity. So, for instance, our short-term memory at a previous time step

254
00:22:57,600 --> 00:23:02,320
can be used to predict observations at a current time step to compute the similarity between two

255
00:23:02,320 --> 00:23:06,880
poses. And another component which we need to add to our model is the long-term memory component,

256
00:23:06,880 --> 00:23:10,720
which helps us decide whether we should be remapping to an existing map or we should be

257
00:23:10,720 --> 00:23:18,720
remapping to a new map. So, for all of these environments, our model makes the correct

258
00:23:18,720 --> 00:23:24,480
predictions which are in line with the experimental data observed. And these experiments have been

259
00:23:24,480 --> 00:23:29,920
done and we have neural data for them. This is a new prediction that our model makes for amorphous

260
00:23:29,920 --> 00:23:34,640
naturalistic environments. We predict that even in these environments, the map will be segmented

261
00:23:34,640 --> 00:23:38,800
and grid fields will realign when going from one contiguous region to the other.

262
00:23:38,800 --> 00:23:42,320
And we do not predict any map fragmentations in these spiral mesas.

263
00:23:42,480 --> 00:23:51,520
So, given that we built or proposed an algorithmic model for map fragmentation, the next question is,

264
00:23:51,520 --> 00:23:56,880
how can map fragmentation be implemented on a neural level? So, we want to provide a framework

265
00:23:56,880 --> 00:24:01,360
for building a neural circuit model of map fragmentation. And this work is in collaboration

266
00:24:01,360 --> 00:24:08,560
with Sarthak and Murko, who are both postdocs in FEDLA. So, going back to our theoretical

267
00:24:08,560 --> 00:24:14,880
framework, we had suggested that place cells might encode topological relationships between

268
00:24:14,880 --> 00:24:20,080
metric maps that might be represented by the grid space. And now I'm going to talk about

269
00:24:20,080 --> 00:24:25,440
how we can implement that at a neural level. So, at the neural level, we start with factorized

270
00:24:25,440 --> 00:24:29,600
representations in which different aspects of knowledge are represented separately and can

271
00:24:29,600 --> 00:24:34,880
then be flexibly recombined. So, for instance, in this case, location information from grid cells

272
00:24:34,880 --> 00:24:39,760
and contextual information from sensory cells form this conjunctive representation in place cells.

273
00:24:40,400 --> 00:24:44,800
Here grid cells can enable path integration and can be thought of as implementing an affine vector

274
00:24:44,800 --> 00:24:50,080
space or an impedance space. The recurrent wiring between these place cell population encodes

275
00:24:50,080 --> 00:24:55,280
neighborhood relationships or topology. And here, large changes in contextual input cause

276
00:24:55,280 --> 00:25:01,040
remapping in place cells, which in turn cause remapping in grid cells through these back projections.

277
00:25:01,040 --> 00:25:04,400
And remapping here corresponds to transitioning from one local map to another.

278
00:25:05,280 --> 00:25:11,120
So, most of the previous work on interplay of grid and place circuits focuses on maintaining

279
00:25:11,120 --> 00:25:16,160
firing properties of one population based on the inputs from another. So, for instance,

280
00:25:17,280 --> 00:25:21,120
successor representation suggests that grid cells are a low-dimensional representation

281
00:25:21,120 --> 00:25:27,520
of place cells that stabilize place cell activity. Similarly, here's a model which

282
00:25:27,520 --> 00:25:32,400
implements non-negative PCA of place cells. So, place cells are at the input. The weights are

283
00:25:32,400 --> 00:25:37,840
learned through heavy learning and a non-negativity constraint. And this network does PCU on the

284
00:25:37,840 --> 00:25:44,000
inputs, and the outputs end up converging to grid-like fields, again, suggested that grid

285
00:25:44,000 --> 00:25:46,720
cells might be a low-dimensional representation of place cells.

286
00:25:51,280 --> 00:25:56,160
Another set of work suggests that inputs from border cells to grid cells could be used for

287
00:25:56,160 --> 00:26:02,160
error-correcting grid cells. So, here I'm showing a one-day schematic just to make my point. So,

288
00:26:02,160 --> 00:26:08,560
this is a rodent at a specific location in space, and this is the grid activity profile

289
00:26:09,280 --> 00:26:13,440
that represents that location. And when the rodent explores the environment and comes back to this

290
00:26:13,440 --> 00:26:17,920
location, the representation of this location has drifted with respect to the original,

291
00:26:17,920 --> 00:26:23,120
and there's some error in the representation. And if the border cell activations are provided

292
00:26:23,120 --> 00:26:28,560
as input to grid cells, then they activate the current subset of neurons doing error correction

293
00:26:28,560 --> 00:26:33,680
and pulling back the representation to the original representation. And this is what this

294
00:26:33,680 --> 00:26:38,320
looks like in 2D. So, in 2D, if you do not have any border cell inputs, then your grid cell

295
00:26:38,320 --> 00:26:42,400
representations are not very stable, but with border cell inputs, your grid cell representations

296
00:26:42,400 --> 00:26:50,720
are fairly stable. So, I also want to point out the fact that place cells are thought to store

297
00:26:50,720 --> 00:26:54,320
neighborhood relationships in their reference synapses, and therefore they could implement

298
00:26:54,320 --> 00:26:58,720
a topological navigation strategy. And many models of place cell-based navigation have

299
00:26:58,720 --> 00:27:03,760
actually emphasized this view. So, they've suggested that recurrence synapses encode either

300
00:27:03,760 --> 00:27:08,720
spatial or temporal connectivity, as suggested by Blum and Abbott, or they encode transition

301
00:27:08,720 --> 00:27:14,640
probability, as suggested by the successor representation work. So, given all these insights,

302
00:27:14,640 --> 00:27:19,440
our goal is to build a comprehensive neural circuit model of premapping. And we start with these

303
00:27:19,440 --> 00:27:24,000
two questions. Does high-capacity grid code, when projected to place cells, also lead to high

304
00:27:24,000 --> 00:27:29,440
capacity? And given these conjunctive representations between the location input and the sensory input,

305
00:27:29,440 --> 00:27:35,280
can we learn neighborhood relationships between place codes? And before I go into the details

306
00:27:35,280 --> 00:27:39,760
of capacity, I just want to point out that traditionally, Hopfield networks have been

307
00:27:39,760 --> 00:27:45,040
used for storing memories and patterns. And it has been observed that the maximum patterns

308
00:27:45,040 --> 00:27:49,360
that these networks can store is n, where n is the total number of neurons in the network.

309
00:27:50,000 --> 00:27:54,640
And modern Hopfield networks, also known as dense associative memories, have an exponential

310
00:27:54,640 --> 00:28:00,400
capacity, but they use many body interaction terms, which are not biologically plausible.

311
00:28:00,400 --> 00:28:04,400
And in our model, we stick to using two interaction terms in the weight computations,

312
00:28:04,400 --> 00:28:14,480
and we still get exponential capacities. So, this is the architecture of our model.

313
00:28:14,480 --> 00:28:19,120
The model has different grid modules, which have different scales or periods,

314
00:28:19,120 --> 00:28:25,440
and the binary grid code is projected to place code randomly. And the back projections from

315
00:28:25,440 --> 00:28:30,160
place cells to grid cells are learned through associative Hebbian learning. And we observe

316
00:28:30,160 --> 00:28:35,440
that when we perturb these place cells with a noisy version of place code representations,

317
00:28:35,440 --> 00:28:40,320
then the network is able to successfully reconstruct all the patterns it's trained on.

318
00:28:40,320 --> 00:28:45,760
So, the network is fairly robust to noise. Furthermore, this network has exponential

319
00:28:45,760 --> 00:28:50,400
capacity that grows much faster than a non-modular network where the grid cells are non-modular.

320
00:28:53,760 --> 00:28:57,600
Also, the network generalizes stored inputs to create stable attractor

321
00:28:57,600 --> 00:29:02,240
states around every pattern in the grid coding space, despite training only over a vanishing

322
00:29:02,240 --> 00:29:07,600
fraction of contiguous grid coding space. So, for instance, if my grid coding space has around

323
00:29:07,600 --> 00:29:13,200
10,000 patterns, I can train the network on only around 200 patterns, first 200 patterns,

324
00:29:13,200 --> 00:29:17,440
and the network is still able to robustly reconstruct all the 10,000 patterns in the grid

325
00:29:17,440 --> 00:29:24,800
coding space, which is pretty striking. Furthermore, next we add these heterosciitiative

326
00:29:24,800 --> 00:29:28,720
learning on the recurrent connections on place cells to see if they can encode neighborhood

327
00:29:28,720 --> 00:29:35,760
relationships or 1D sequences. And what we find is that the product of these weights converges to

328
00:29:35,760 --> 00:29:40,480
this transition matrix, which is actually the analytical matrix, an analytical transition

329
00:29:40,480 --> 00:29:45,920
matrix that relates contiguous grid codes in the grid coding space. Furthermore,

330
00:29:45,920 --> 00:29:49,920
this network actually has perfect sequence recall given enough number of place cells to

331
00:29:49,920 --> 00:29:55,520
approximate this transition matrix. And again, training on only a subset of the sequence is

332
00:29:55,520 --> 00:29:59,840
enough to recall the entire sequence. So, for instance, if I train, if I have a sequence of

333
00:29:59,920 --> 00:30:06,000
length 500, and I train the network on only first 150 patterns in the sequence, the network

334
00:30:06,000 --> 00:30:11,120
is robustly able to reconstruct all 500 patterns in the sequence without having seen all of them

335
00:30:11,120 --> 00:30:18,240
before. So, the next step in this network is to introduce sensory input. And the sensory input

336
00:30:18,240 --> 00:30:22,240
would project randomly to place cells and back projections from place cells to sensory input

337
00:30:22,240 --> 00:30:27,440
would be learned through associative hybrid learning. And here grid cells would form a basis,

338
00:30:27,440 --> 00:30:31,040
and hippocampal places would link that basis with arbitrary sensory input.

339
00:30:31,680 --> 00:30:36,080
And this combination of structured inputs and unstructured inputs could potentially enable

340
00:30:36,080 --> 00:30:40,240
the storage and robust recollection of a large number of arbitrary sensory patterns from this

341
00:30:40,240 --> 00:30:47,840
partial use. So, how does this connect to map fragmentation? So, in part two, we talked about

342
00:30:47,840 --> 00:30:54,240
map fragmentation based on the notion of contiguous regions. And here I'm positing that

343
00:30:54,240 --> 00:30:59,520
when you transition between different contiguous regions that actually corresponds to a large

344
00:30:59,520 --> 00:31:05,600
contextual change, which when provided as input to this network would trigger remapping in place

345
00:31:05,600 --> 00:31:10,880
cells, which would in turn cause remapping in grid cells, thus leading to the formation of local

346
00:31:10,880 --> 00:31:18,800
sub maps. And how does this connect to part one, where we saw that sub maps might enable quick

347
00:31:18,800 --> 00:31:24,000
learning and generalization in humans. So, this network actually enables us to anchor grid maps

348
00:31:24,000 --> 00:31:29,440
to external cues through these conjunctive representations in place cells. And this anchoring

349
00:31:29,440 --> 00:31:34,960
to external cues actually enables the alignment of grid maps, even when points of departure in an

350
00:31:34,960 --> 00:31:41,920
environment are different, leading to adaptable representations. Furthermore, if you are in an

351
00:31:41,920 --> 00:31:47,600
environment that is composed of previously seen spatial structures, then this anchoring still

352
00:31:47,600 --> 00:31:53,440
enables you to pull out the right map when you're navigating through that composed spatial structure.

353
00:31:54,480 --> 00:32:00,800
So, to summarize, our global hypothesis was that cognitive map is organized as a globally

354
00:32:00,800 --> 00:32:07,840
topological and locally metric or Euclidean map. So, this is one illustration of a topometric map.

355
00:32:07,840 --> 00:32:12,880
And we said that any sub part of this map can be termed as a sub map.

356
00:32:14,960 --> 00:32:19,680
In part one, we suggested that learning sub maps that are adaptable and compositional may drive

357
00:32:19,680 --> 00:32:23,360
fast learning in complex spaces. And we proposed to conduct human behavioral

358
00:32:23,360 --> 00:32:28,080
experiments to test this hypothesis. In second part, we proposed an algorithmic model of map

359
00:32:28,080 --> 00:32:32,720
fragmentation into sub maps based on the notion of contiguous regions. And in the third part,

360
00:32:32,720 --> 00:32:37,680
we proposed a framework for a neural circuit model of map fragmentation. And overall,

361
00:32:37,680 --> 00:32:42,240
we're suggesting that sub maps enable humans to build up a knowledge base of spatial structures

362
00:32:42,240 --> 00:32:47,200
that they can continuously enrich and refine throughout their life by combining their existing

363
00:32:47,280 --> 00:32:53,120
spatial knowledge with their new experiences. So, that's all for my talk. And before I end,

364
00:32:53,120 --> 00:32:58,800
I want to acknowledge my collaborators again, Marta, Kevin, Merko and Sata. And I also want to

365
00:32:58,800 --> 00:33:04,080
thank my supervisors, Ila and Josh for their continued feedback and support and discussions

366
00:33:04,080 --> 00:33:09,840
on these projects. I also want to thank Matt Wilson for his feedback and valuable suggestions

367
00:33:09,840 --> 00:33:14,960
during my committee meetings and the members of FeedLab and TenBomb in general for their support.

368
00:33:15,520 --> 00:33:22,080
And before I end, I also want to thank the VCS department and McGowan Institute for their

369
00:33:22,080 --> 00:33:27,280
continued support and for providing an environment that's conducive to research,

370
00:33:27,280 --> 00:33:31,840
even in the face of this pandemic. Thank you. And I can take any questions now.

371
00:33:34,560 --> 00:33:40,000
Thanks, Sue, for the great talk. We already have a question from Marta Griffin.

372
00:33:40,800 --> 00:33:44,000
Great talk. I have a question about partitioning a space to sub maps.

373
00:33:44,640 --> 00:33:49,120
What do you think will happen if you ask humans to intuitively partition an environment to regions?

374
00:33:49,120 --> 00:33:52,960
How would there be segmentation compared to the maps given by your similarity measure?

375
00:33:55,280 --> 00:34:03,040
So, you mean the complex? Anyways, so, okay, so the question is that if humans intuitively

376
00:34:03,040 --> 00:34:07,440
try to segment a map, how would that compare to the segmentations which we have proposed

377
00:34:07,520 --> 00:34:13,760
in part two of my talk where we have given an algorithmic model for map segmentation.

378
00:34:15,920 --> 00:34:21,200
So, basically, the way I view it is that in the algorithmic model of map segmentation,

379
00:34:21,200 --> 00:34:28,080
I basically took one metric map and one small portion of the map. And we said that that can be

380
00:34:28,080 --> 00:34:33,200
decomposed into various fragments of sub maps. But humans are actually able to reason in much

381
00:34:33,200 --> 00:34:39,680
richer environments than the ones which we saw in this algorithm. And so, there we can

382
00:34:39,680 --> 00:34:46,000
view it as something like this where you might have even small spaces being segmented into

383
00:34:46,000 --> 00:34:52,480
multiple maps. So, let me just see if I can, yeah, so even if you have something like this,

384
00:34:53,040 --> 00:34:56,960
so even within this, so I was calling this a sub map, but even within this sub map,

385
00:34:56,960 --> 00:35:00,880
we might have multiple maps. So, based on the notion of contiguous regions, you might have

386
00:35:00,880 --> 00:35:05,200
a local map here, a local map here, and all these maps might be connected.

387
00:35:05,200 --> 00:35:11,520
And so, the algorithmic model would predict those kinds of map segmentations, but then even

388
00:35:11,520 --> 00:35:16,640
a combination of those map segmentations can be termed as a sub map. And then that is what I

389
00:35:16,640 --> 00:35:23,520
was talking about in part one where I'm talking about composing the sub map which is even itself

390
00:35:23,520 --> 00:35:30,000
composed of smaller maps. And we can take this representation of this map and then compose

391
00:35:30,000 --> 00:35:38,960
them in various ways to build richer environments.

392
00:35:50,240 --> 00:35:54,560
I'll follow up from Marta. I'm curious if the algorithm can predict how humans would interpret

393
00:35:54,560 --> 00:36:00,880
sub maps. How humans would interpret?

394
00:36:06,640 --> 00:36:10,720
Well, it's a little bit unclear to me what you mean by interpret

395
00:36:11,680 --> 00:36:20,320
sub maps, but I mean, I'm assuming that you're suggesting, you know, how humans would interpret

396
00:36:21,200 --> 00:36:26,080
segmenting this environment versus maybe a more complex environment. And I would say that

397
00:36:26,080 --> 00:36:31,440
currently, this algorithm doesn't predict anything about how humans might interpret

398
00:36:32,240 --> 00:36:38,000
grid maps. But that's, I mean, we could look at it. That's an interesting direction and we could

399
00:36:38,000 --> 00:36:44,480
look at it in the future. And I think some of my work in part one would potentially address that

400
00:36:44,480 --> 00:36:52,960
going forward. All right. Next question from Eli Pollock. You can unmute yourself, Eli.

401
00:36:56,560 --> 00:36:58,480
Yeah. Hi. Can you hear me? Yes.

402
00:37:02,000 --> 00:37:09,280
Hello. Yes, I can hear you. Sorry. Okay. Okay. Cool. Yes. Sorry. My internet's a little weird.

403
00:37:10,160 --> 00:37:15,440
Great talk. Can you talk a little bit more about how your model handles time?

404
00:37:16,080 --> 00:37:23,120
Or I think you mentioned that it was able to handle like replay of different sequences of

405
00:37:23,120 --> 00:37:32,960
states through some map. How would it be able to handle different trajectories through the same

406
00:37:33,040 --> 00:37:41,120
space that might activate play cells in different sequences? So the version of the model that I

407
00:37:41,120 --> 00:37:48,320
presented, that is trained on discrete patterns, right? So the sequences here, the sequences which

408
00:37:48,320 --> 00:37:53,440
are trained on here through heterospecific learning are composed of these discrete patterns. So if I

409
00:37:53,440 --> 00:38:00,160
say the sequences of length 500, there are 500 discrete patterns in that sequence. And when you

410
00:38:00,240 --> 00:38:06,640
say time, time is something continuous. And so we have worked on extrapolating this model to

411
00:38:06,640 --> 00:38:10,640
more continuous domains. And in a continuous domain, what would happen is that your sequence,

412
00:38:10,640 --> 00:38:15,920
instead of being discrete patterns, would be composed of these continuous stream. And in that

413
00:38:15,920 --> 00:38:20,640
case, we haven't explored what the network performance would be, but that's something

414
00:38:20,640 --> 00:38:27,680
ongoing. And definitely in 2D spaces, we would like to train this model on 2D sequences and then

415
00:38:27,680 --> 00:38:33,120
see, so right now, these results are pertaining to 1D sequences. But we do want to extrapolate

416
00:38:33,680 --> 00:38:39,600
the results and train this model on 2D sequences to see how it performs in 2D. And I think that would

417
00:38:40,400 --> 00:38:45,440
potentially address then your questions about time, because that pertains to continuous domains.

418
00:38:46,160 --> 00:38:47,120
Okay, yeah, thank you.

419
00:38:51,120 --> 00:38:55,200
Okay, we've got a big stream of questions coming in. The first is from Adam Eisen.

420
00:38:55,840 --> 00:38:59,280
Thanks for a fascinating talk. Have you thought at all about how this framework for sub-map

421
00:38:59,280 --> 00:39:04,000
segmentation and topological association could be extended to non-spatial domains?

422
00:39:05,440 --> 00:39:12,880
That's a very good question. So in FeedLab, a small group of us have been thinking about how

423
00:39:12,880 --> 00:39:19,360
this could be extended. And so the idea is, I'm just going to go back to, let's see if I can just

424
00:39:19,440 --> 00:39:25,120
quickly hop back to my theoretical framework slide. So really, we're building up on this

425
00:39:25,120 --> 00:39:29,200
theoretical framework. And when we're thinking about non-spatial domains, we go back to this

426
00:39:30,000 --> 00:39:35,600
schematic picture. And here, basically, what we're saying is, within the spatial domain, I said that

427
00:39:35,600 --> 00:39:39,920
large changes in contextual input can drive remapping in place cells, which would drive

428
00:39:40,480 --> 00:39:48,560
remapping in the grid coding space. So similarly, in relational domains or in discrete domains,

429
00:39:49,840 --> 00:39:54,560
as we're thinking about it, we are referring back to this schematic picture. And we think that

430
00:39:55,600 --> 00:40:01,440
the phenomenon of remapping actually would enable us to encode these non-eclidean relationships.

431
00:40:01,440 --> 00:40:06,320
So first thing is that here, we're suggesting topological relationships. So that already

432
00:40:06,320 --> 00:40:11,280
enables us to encode non-eclidean relationships. But this phenomenon of remapping also allows

433
00:40:11,280 --> 00:40:16,880
us to make jumps. So if you think about a family tree, if you might try to encode it in an

434
00:40:16,960 --> 00:40:22,160
eclidean space, then you'll have conflicts. And it's very difficult to encode it. In fact,

435
00:40:22,160 --> 00:40:28,480
almost impossible to encode it in an eclidean space. But then if we provide a framework where

436
00:40:28,480 --> 00:40:33,200
we allow topological representations and we allow jumps in this eclidean space through this phenomenon

437
00:40:33,200 --> 00:40:37,920
of remapping through place cells, then that can allow us to potentially encode non-eclidean

438
00:40:37,920 --> 00:40:42,320
relationships. And that's the way we are thinking. And this is still work in progress. And we

439
00:40:42,320 --> 00:40:53,120
actively think about it. Next question from Nancy Camusher. How does your system decide when and

440
00:40:53,120 --> 00:40:57,360
where to carve the world in the sub-maps, especially in non-built environments where the

441
00:40:57,360 --> 00:41:08,080
divisions may be less obvious? So at least in part two, where I talk about fragmentation of

442
00:41:08,080 --> 00:41:17,120
maps into sub-maps. Let me just go back. So in these kinds of environments, we basically describe

443
00:41:17,120 --> 00:41:22,160
the principle of contiguous regions, where we have suggested that when you are navigating through

444
00:41:22,160 --> 00:41:27,920
this environment, if you are in an environment where your visual observations stay more or less

445
00:41:27,920 --> 00:41:34,000
consistent, then you will keep extending a map. But when you go from this region to another region,

446
00:41:34,000 --> 00:41:39,040
let's say you navigate from here to here, where your visual information here is completely distinct

447
00:41:39,040 --> 00:41:43,600
from your visual information here, then in that case, you will decide to segment the space. And

448
00:41:44,160 --> 00:41:49,040
that's what we are predicting. We're predicting that in this case, when you travel from one

449
00:41:49,040 --> 00:41:54,480
contiguous region to another, you will segment the space. And that's how you divide the space.

450
00:41:54,480 --> 00:41:59,920
So that's one principle, which we are describing for map segmentation. And that might not be the

451
00:41:59,920 --> 00:42:05,840
only principle, but that principle explains some of the experimental results that have been

452
00:42:05,840 --> 00:42:10,640
observed newly. And there might be another principle, for instance, path integration error,

453
00:42:10,640 --> 00:42:16,480
where if your path integration builds up and it reaches a certain threshold amount,

454
00:42:16,480 --> 00:42:22,480
then you automatically might start a new map. But I haven't illustrated that here, but that's

455
00:42:22,480 --> 00:42:27,120
also one of the other driving forces for us to actually split a space into multiple sub-maps

456
00:42:27,120 --> 00:42:38,800
so that we can have more efficient representations. Okay, next question from Chen.

457
00:42:39,920 --> 00:42:43,440
I was wondering if you can give some intuition about the mechanism for composition of

458
00:42:43,440 --> 00:42:48,480
sub-maps. So really, this is from the happy and learning mechanism. And secondly,

459
00:42:48,480 --> 00:42:51,600
can you comment on the relationship between this and the Hopfield mechanism?

460
00:42:51,600 --> 00:43:07,600
So since we're talking about composition of sub-maps here, in this case, in this model,

461
00:43:07,600 --> 00:43:13,200
what I'm suggesting is that we might end up forming local sub-maps by using the

462
00:43:13,200 --> 00:43:18,720
phenomenon of premapping. So here I'm suggesting, again, if you have large changes in contextual

463
00:43:18,720 --> 00:43:26,080
input, so for instance, let me just go back to the slide, which kind of connects this.

464
00:43:27,520 --> 00:43:35,040
So here we basically suggested how this environment can be segmented into different

465
00:43:35,040 --> 00:43:41,360
sub-maps. And on the mechanistic level, I'm trying to suggest that any large contextual input

466
00:43:41,360 --> 00:43:46,800
is going to cause remapping in place cells, which means the cells which are firing would be

467
00:43:48,960 --> 00:43:55,360
changed and they would represent a new map. And this remapping would actually enable also

468
00:43:55,360 --> 00:44:00,800
remapping in grid cells through associative learning because each place cell representation

469
00:44:00,800 --> 00:44:07,200
is associated with a certain grid cell representation. So when you have changes in

470
00:44:07,200 --> 00:44:11,600
firing fields of place cells, that also triggers certain corresponding changes in the grid coding

471
00:44:11,600 --> 00:44:19,440
phase, which is basically called remapping in grid phase. And so here I'm interpreting,

472
00:44:19,440 --> 00:44:24,400
when I look at my algorithmic model and I look at my mechanistic model, I'm saying that when you

473
00:44:24,400 --> 00:44:29,920
go from one contiguous region to another, that actually corresponds to a large contextual change

474
00:44:29,920 --> 00:44:36,800
and that contextual change triggers remapping in this model and this remapping in place cells

475
00:44:36,800 --> 00:44:41,840
then triggers remapping in grid cells phase. And that corresponds to the formation of local

476
00:44:41,840 --> 00:44:47,120
sub-maps. And these topological connections from place cells to themselves might actually

477
00:44:47,760 --> 00:44:52,800
represent how these local sub-maps are connected to each other. And there's also,

478
00:44:53,440 --> 00:44:56,800
going back to the anatomy, there's also a possibility of splitting this population

479
00:44:56,800 --> 00:45:01,600
actually into different populations. One that might just encode conjunctive representations and

480
00:45:01,600 --> 00:45:08,800
another one that might have these recurrent topological connections similar to CA1 and

481
00:45:08,800 --> 00:45:13,360
CA3 distinction in anatomy. But that's how I'm thinking about it at the moment.

482
00:45:23,280 --> 00:45:29,760
I want to point out a quick addendum from Ila who said to Nancy's question, we consider online

483
00:45:29,760 --> 00:45:35,040
segmentation decisions driven by regions or points of high surprise affordance changes and PIR.

484
00:45:41,200 --> 00:45:46,640
Okay, another question from Anya, even over. Do you think the way humans form space maps depends

485
00:45:46,640 --> 00:45:51,520
on whether their language uses directions that are egocentric left and right or allocentric

486
00:45:51,520 --> 00:45:59,520
north and south? That's a great question. So there were experiments in both animals as well

487
00:45:59,520 --> 00:46:04,080
as humans which have shown that we actually have both kinds of representations. We have

488
00:46:04,080 --> 00:46:10,560
egocentric representations and we also have allocentric representations. And the representations

489
00:46:10,560 --> 00:46:19,360
in the hippocampal inter-animal system are usually allocentric, but the thalamus is the part of the

490
00:46:19,360 --> 00:46:25,040
brain which is responsible for egocentric representations. So really, when we're navigating

491
00:46:25,040 --> 00:46:30,400
spaces, we are probably using mixed strategies. We do use our allocentric representations,

492
00:46:30,400 --> 00:46:35,920
but in some cases we might be using egocentric representations. And that's mostly also true

493
00:46:35,920 --> 00:46:41,040
for routes which we are traversing very frequently. So if there's a route which I take every day,

494
00:46:41,040 --> 00:46:47,760
let's say going from my home to BCS, then that converges to root learning. But if I've had very

495
00:46:47,760 --> 00:46:53,200
less experience in any environment, then I'm mostly using allocentric representations to

496
00:46:53,200 --> 00:46:57,600
actually make my way through that environment. And hence, allocentric representations are

497
00:46:57,600 --> 00:47:05,120
actually attributed to being able to make novel inferences. And when I'm building circuits here

498
00:47:05,120 --> 00:47:09,440
using grid cells and place cells, I'm mostly talking about allocentric representations.

499
00:47:09,440 --> 00:47:15,760
But then when I was talking about submaps in human experiments, I'm kind of

500
00:47:15,760 --> 00:47:20,400
agnostic to the fact that whether those representations are egocentric or allocentric

501
00:47:20,400 --> 00:47:27,360
because even if you learn an egocentric strategy, even in that case, if you determine that there's

502
00:47:27,360 --> 00:47:32,640
a repeating structure to the environment and you recall that this is the same environment that I've

503
00:47:32,640 --> 00:47:38,000
seen before, then you can still apply the same egocentric strategy or an allocentric strategy

504
00:47:38,000 --> 00:47:44,320
depending on which one you decide to use at that point. But going back to the main question,

505
00:47:45,200 --> 00:47:51,680
the experimental data shows that we're actually basically learning both kinds of strategies.

506
00:47:57,920 --> 00:48:03,840
Okay, Senke's Pellevan says, great talk. What are the large scale, sorry, where are the large

507
00:48:03,840 --> 00:48:13,920
scale topological relations coded in the network? So in the network model, our focus, so in this

508
00:48:13,920 --> 00:48:18,480
case, we are basically encoding topological relationships using the recurrent connections

509
00:48:18,480 --> 00:48:25,360
on the place cells. But for large scale topological relationships, what we are moving towards is

510
00:48:25,360 --> 00:48:30,160
splitting this population into two distinct populations. So one population that will only

511
00:48:30,160 --> 00:48:35,520
have conjunctive representations, and perhaps another population which will have the recurrent

512
00:48:35,520 --> 00:48:41,600
connections similar to the distinction between CA1 and CA3 in the brain. And it's true that in this

513
00:48:41,600 --> 00:48:46,480
network model, we don't really distinguish between small scale and large scale topological

514
00:48:46,480 --> 00:48:50,320
relationships. But by introducing this additional population, we hope to abstract away the large

515
00:48:50,320 --> 00:48:58,240
scale relationships. Also, even in this case, if we have minute relationships between spaces,

516
00:48:58,240 --> 00:49:03,440
then we can also extract away the large scale relationships. But again, adding another population

517
00:49:04,320 --> 00:49:07,440
might make that easier to make that abstraction.

518
00:49:12,560 --> 00:49:18,560
Okay. Nancy has another comment. There must be some way to represent globally metric information.

519
00:49:19,120 --> 00:49:22,720
For example, I know the approximate distance and angle from my current position to far away

520
00:49:22,720 --> 00:49:26,480
locations in my world. Do I understand right that you would say this information has to be

521
00:49:26,480 --> 00:49:30,080
pieced together from just the topological relationship between the sub maps that connect

522
00:49:30,080 --> 00:49:37,280
those locations? So that's a great question. I also sometimes wonder because like when I'm here,

523
00:49:38,160 --> 00:49:43,520
I can kind of look very far in space and be able to tell that, okay, I need to go in this direction.

524
00:49:44,480 --> 00:49:51,760
And that's definitely metric information. And so going back to the labeled graph hypothesis,

525
00:49:52,720 --> 00:50:02,720
which was suggested by Bill Warren. So even in this hypothesis, I'm just saying that this is

526
00:50:02,800 --> 00:50:08,320
we are learning metric representations here, which are consistent Euclidean representations,

527
00:50:08,320 --> 00:50:12,960
which means that they actually obey all the postulates of Euclidean geometry.

528
00:50:13,920 --> 00:50:19,600
And the connections between them still have metric information. There's still information

529
00:50:19,600 --> 00:50:27,360
about angles and directions, but they might not obey all postulates of Euclidean geometry. So for

530
00:50:27,360 --> 00:50:31,680
instance, you might have some inconsistent representations. You might not know exactly

531
00:50:31,680 --> 00:50:36,400
what distances and displacements you have to go in order to reach a goal, but you know approximately

532
00:50:36,400 --> 00:50:41,760
which distance and which angle. So even the topometric representation is not contradicting

533
00:50:41,760 --> 00:50:46,640
the fact that you might have approximate distances and angles. But what it's saying is that you have

534
00:50:46,640 --> 00:50:53,200
very accurate metric information about small pieces of space, but then for faraway pieces of

535
00:50:53,200 --> 00:50:56,880
space, you might still have approximate distances, angles, which help you navigate.

536
00:51:02,640 --> 00:51:09,040
Okay, another question from Sean Chen. Sorry for my pronunciations. Great talk.

537
00:51:09,040 --> 00:51:12,640
Could you elaborate the mechanism of exponential number of memory capacity in your model?

538
00:51:20,720 --> 00:51:27,440
So here's the, here's the network, right, where I'm showing that the network has exponential

539
00:51:27,440 --> 00:51:34,240
capacity. So basically in this network, we have different grid modules which have different periods

540
00:51:34,240 --> 00:51:39,680
and this is the grid code is actually binary. And we project this grid code randomly using

541
00:51:39,680 --> 00:51:44,880
random weight matrix to place cells. And the number of place cells in this case is much larger than

542
00:51:44,880 --> 00:51:51,360
the number of grid cells. And now we learn these back projections from places to grid cells using

543
00:51:51,360 --> 00:51:56,160
Hebbian learning. So it's just simple Hebbian learning. I don't have the equation. Basically,

544
00:51:56,160 --> 00:52:01,200
this is the equation, right? You can consider these as place cell patterns. And this is how we

545
00:52:01,200 --> 00:52:07,760
compute the weight matrix for the weights going back from place cells to grid cells. And once we've

546
00:52:07,760 --> 00:52:12,880
done that, then the way we test for memory is basically by perturbing the place code patterns.

547
00:52:12,880 --> 00:52:18,560
So we put up the place cells by a noisy version of the place code pattern, right? So in this graph,

548
00:52:18,560 --> 00:52:26,080
I'm showing that if you perturb them with 20% noise, then with 300 place cells, you get an output

549
00:52:26,080 --> 00:52:30,000
noise of zero, which means you get perfect reconstruction of the patterns that you're

550
00:52:30,000 --> 00:52:35,040
perturbing the network with. And that's how I'm defining exponential memory, right? And

551
00:52:35,040 --> 00:52:40,800
when I say exponential memory, what I'm suggesting is that you can, as you increase the number of

552
00:52:40,800 --> 00:52:46,000
cells in the network, the number of patterns that you can reconstruct from the noisy versions

553
00:52:46,000 --> 00:52:47,440
actually grows exponentially.

554
00:52:59,440 --> 00:53:05,600
Okay, I'd like to ask a question as well. I'm curious to hear your thoughts on sort of

555
00:53:05,600 --> 00:53:12,640
computational level descriptions here. So it seems like your model is largely

556
00:53:12,640 --> 00:53:17,920
tuned towards sort of prediction, accurate prediction of your local spatial environment

557
00:53:20,640 --> 00:53:26,800
and motive mechanisms for doing this well. And I wonder what you think about computational level

558
00:53:27,760 --> 00:53:32,960
accounts of this to be more, to ask a more concrete question. Do you think there's a role

559
00:53:32,960 --> 00:53:38,560
of reward or something other than accurate, just accurate prediction in the shaping of

560
00:53:38,560 --> 00:53:44,480
neural representations of space? Okay, so you're asking whether there is a role for reward

561
00:53:45,360 --> 00:53:49,760
in shaping the representations of space in the mechanistic model?

562
00:53:51,840 --> 00:53:58,000
I mean, more generally, what's the objective driving this algorithm? Is it about

563
00:53:58,000 --> 00:54:00,240
accurate prediction or is there something more than prediction?

564
00:54:01,200 --> 00:54:12,720
Okay, so you're asking what is kind of like the motivation for building the mechanistic model?

565
00:54:14,480 --> 00:54:17,120
Yeah, yeah, I mean, some of the graphs you're showing here about,

566
00:54:18,720 --> 00:54:23,840
some of the graphs you showed, for example, were about recall. And recall, accurate reconstruction

567
00:54:23,840 --> 00:54:29,760
of an environment might be one objective. But it seems like you could say that this is a

568
00:54:29,840 --> 00:54:34,080
mechanism evolved for some purpose, but it's just that or different than that.

569
00:54:34,880 --> 00:54:43,200
So, right, so these two questions, which I started with, which are related to capacity of the network,

570
00:54:44,160 --> 00:54:48,480
and whether we can do, whether we can learn sequences, these are basically questions which I

571
00:54:48,480 --> 00:54:53,200
started with, because I wanted to explore the theoretical properties of these circuits, right,

572
00:54:53,200 --> 00:54:57,680
given the coding properties that we know of grid cells and place cells and given

573
00:54:57,680 --> 00:55:03,920
biologically plausible learning tools through have been learning, what are the coding properties

574
00:55:03,920 --> 00:55:08,640
that these networks can exhibit, right, how much capacity they have, how much information can they

575
00:55:08,640 --> 00:55:14,800
store. And so, yes, even in these models, since we are talking about reconstruction,

576
00:55:15,680 --> 00:55:20,320
basically says that if you have some noise, if you have some noisy estimate of where you are in

577
00:55:20,320 --> 00:55:26,720
space, then this kind of a network can help you clean that estimate, right, and reach a cleaner

578
00:55:26,720 --> 00:55:33,120
version of where you might be in space and help you localize in space. And also, these temporal

579
00:55:33,120 --> 00:55:37,680
connectivity on the place cells kind of have a predictive component to them, right, because you

580
00:55:37,680 --> 00:55:41,440
can import sequences. So, when you are at a particular position, you might be able to predict

581
00:55:41,440 --> 00:55:48,560
what's coming next. And so, these are kind of small components, which I'm using to go towards

582
00:55:48,560 --> 00:55:53,840
building a model of remapping in space, right, because ultimately my goal is to be able to

583
00:55:53,840 --> 00:55:58,320
figure out how these neural circuits might lead to the formation of these sub-maps. And by knowing

584
00:55:58,320 --> 00:56:03,440
the properties of these circuits, then I'll be better able to construct and build those networks,

585
00:56:03,440 --> 00:56:07,360
which actually can build small local sub-maps of environments.

586
00:56:12,240 --> 00:56:17,680
I'm reading out a comment from Ila. To my question, you could view this as a structure

587
00:56:17,680 --> 00:56:21,680
learning even without reward. A reinforcement learner, including the brain, could use these

588
00:56:21,680 --> 00:56:30,880
learning structures. Cool. Well, I think we're actually over time now. That was a great talk,

589
00:56:30,880 --> 00:56:38,800
Sue. Thanks for sharing with us. Of course, thank you. So, we'll be back next week for another

590
00:56:38,800 --> 00:56:51,360
talk from Eli Pollock. Until then, have a good Thanksgiving, and I'll see you next week.

