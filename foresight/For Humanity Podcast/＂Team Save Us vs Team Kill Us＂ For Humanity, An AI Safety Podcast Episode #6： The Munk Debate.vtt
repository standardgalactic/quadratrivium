WEBVTT

00:00.000 --> 00:02.560
In biotech, it's exactly the other way around.

00:02.560 --> 00:04.400
If you had come up with a new medicine and said,

00:04.400 --> 00:06.400
this cures cancer, it's awesome,

00:06.400 --> 00:09.040
you can't just go sell it in the supermarket.

00:09.040 --> 00:11.840
The monk debate focused on AI safety,

00:11.840 --> 00:14.640
and it was absolutely extraordinary.

00:14.640 --> 00:18.880
Claiming that AI is an existential threat is itself harmful.

00:18.880 --> 00:22.000
It misleads people about the current state

00:22.000 --> 00:23.600
and likely future of AI.

00:23.600 --> 00:26.640
I've been excited to put together this show for some time now.

00:26.640 --> 00:27.800
It's spicy.

00:27.800 --> 00:30.200
Apologize I'm going to be cursing more in this show

00:30.200 --> 00:32.600
than I normally would.

00:32.600 --> 00:37.320
Such sensationalist claims deflect attention

00:37.320 --> 00:39.640
for real immediate risks.

00:39.640 --> 00:43.840
One set of emotions that we can hardwire into them

00:43.840 --> 00:45.480
is subservience.

00:45.480 --> 00:49.600
The most flamboyantly obnoxious cavalier.

00:49.600 --> 00:52.000
Shit on my leg and tell me it's raining.

00:52.000 --> 00:54.360
I'm going to cheer as the world ends.

00:54.360 --> 00:56.080
Risk means nothing to me.

00:56.080 --> 01:00.160
Fuck your kid's future lunatic.

01:00.160 --> 01:04.160
Is the director of AI research at Metta.

01:04.160 --> 01:06.360
His name is Jan LeCun.

01:06.360 --> 01:09.200
Of course, if it's not safe, we're not going to build it, right?

01:14.480 --> 01:17.760
I mean, will you build a bomb that just blows up randomly?

01:17.760 --> 01:20.720
No, right?

01:20.720 --> 01:24.200
If it's not safe, we're not going to build it.

01:24.200 --> 01:27.320
What a total load of shit.

01:27.320 --> 01:29.840
How the fuck would you know if it's not safe

01:29.840 --> 01:32.600
until it's too late to stop it?

01:32.600 --> 01:35.000
That's the whole fundamental fucking problem

01:35.000 --> 01:36.080
with this situation.

01:44.280 --> 01:48.200
Welcome to For Humanity, an AI safety podcast,

01:48.200 --> 01:50.480
episode six, The Monk Debate.

01:50.480 --> 01:51.960
I'm John Sherman, your host.

01:51.960 --> 01:53.640
Thanks so much for joining me.

01:53.640 --> 01:56.840
This is the AI safety podcast for the general public.

01:56.840 --> 01:58.640
No tech background required.

01:58.640 --> 02:02.680
This show is exclusively about the threat of human extinction

02:02.680 --> 02:05.000
from artificial intelligence.

02:05.000 --> 02:07.640
Please, right now, if you would, hit like

02:07.640 --> 02:11.680
and hit that subscribe button if you haven't hit it already.

02:11.680 --> 02:14.920
Repost these videos on your personal social channels

02:14.920 --> 02:17.760
and please tell people about this show.

02:17.760 --> 02:19.960
Those are some ways that we can help spread the word

02:19.960 --> 02:21.320
about these issues.

02:21.320 --> 02:24.560
Last week, I challenged you to convince someone in your life

02:24.560 --> 02:28.000
that the risk of human extinction

02:28.000 --> 02:30.200
from artificial intelligence is an issue

02:30.200 --> 02:32.160
that requires their attention.

02:32.160 --> 02:33.520
This week, I want to go one further

02:33.520 --> 02:34.480
and challenge you again.

02:34.480 --> 02:36.040
This week, I want to challenge you

02:36.040 --> 02:37.920
to talk to someone about these issues

02:37.920 --> 02:40.160
that is totally outside of this debate,

02:40.160 --> 02:43.720
someone that has no clue that AI is a threat to them.

02:44.760 --> 02:47.080
Here's an easy place, I think, to start.

02:47.960 --> 02:52.080
Every mom and dad out there needs to know about this.

02:52.080 --> 02:59.080
Moms and dads, fellow moms and dads, AI is the biggest threat

02:59.080 --> 03:02.240
to our children in the short term.

03:02.240 --> 03:06.640
It's not fentanyl, it's not some creeper in the park,

03:06.640 --> 03:08.640
it's not drunk driving.

03:09.640 --> 03:14.480
AI is the biggest short term threat to our children.

03:15.480 --> 03:17.480
Parents will do anything for their kids.

03:17.480 --> 03:19.720
So let's start with parents.

03:19.720 --> 03:22.400
Maybe this can be their wake-up call.

03:22.400 --> 03:25.120
I have been genuinely encouraged by the reaction

03:25.120 --> 03:28.200
I've been getting to the show online and in person,

03:28.200 --> 03:31.400
but we can get this message out much further.

03:31.400 --> 03:32.760
And if you like what I'm doing,

03:32.760 --> 03:35.200
I'm asking you to step out of your comfort zone,

03:35.200 --> 03:38.760
share this very serious, very unsettling news

03:38.760 --> 03:40.360
with someone you know.

03:40.360 --> 03:42.160
I'm going to tell you this,

03:42.240 --> 03:45.240
it is not going to be a fun experience.

03:45.240 --> 03:49.440
It sucks to be talking about this stuff.

03:49.440 --> 03:52.960
I want you to know that I hate sitting here

03:52.960 --> 03:54.920
and talking about this stuff,

03:54.920 --> 03:59.440
as much as you probably do sitting there and hearing it.

03:59.440 --> 04:01.880
It fucking sucks.

04:04.360 --> 04:07.280
It unthinkably, absolutely fucking sucks

04:07.280 --> 04:09.880
to seriously talking about this stuff.

04:10.000 --> 04:15.000
It fucking sucks to seriously talking about human extinction.

04:16.520 --> 04:21.520
But we are really threatened by AI killing all humans

04:22.960 --> 04:24.720
in as soon as two years.

04:24.720 --> 04:26.680
That is not hyperbole.

04:26.680 --> 04:29.360
That is literally what the AI companies

04:29.360 --> 04:34.360
and AI safety experts are openly admitting and telling us.

04:35.800 --> 04:39.560
So I can't just go about my normal life anymore.

04:39.560 --> 04:41.440
And if you believe what I've been telling you

04:41.440 --> 04:43.440
and what they've been telling you,

04:43.440 --> 04:45.120
you really can't either.

04:46.120 --> 04:48.280
So I'm going to sit here every week

04:48.280 --> 04:49.920
and try to get the word out.

04:49.920 --> 04:51.080
And I need your help,

04:51.080 --> 04:53.120
and I need the help of everyone you know.

04:53.120 --> 04:54.640
What if one year from today,

04:54.640 --> 04:57.760
everyone on earth was of the understanding

04:57.760 --> 05:00.120
that AI is a lethal threat to their family,

05:00.120 --> 05:01.120
to their children,

05:01.120 --> 05:03.880
to their children's yet unborn children,

05:03.880 --> 05:05.940
to every living thing on earth.

05:06.780 --> 05:09.860
In just a minute, we'll get today's show going.

05:09.860 --> 05:12.180
And I think you're going to really love it.

05:12.180 --> 05:15.380
I've been excited to put together this show for some time now.

05:15.380 --> 05:16.540
It's spicy.

05:16.540 --> 05:18.940
Apologize, I'm going to be cursing more in this show

05:18.940 --> 05:20.540
than I normally would.

05:20.540 --> 05:23.540
But first, I want you to hear from some important people

05:23.540 --> 05:27.540
in the news who are openly, recently discussing

05:27.540 --> 05:31.420
totally unthinkable things about artificial intelligence.

05:31.420 --> 05:35.420
So PDOOM is a technical term used for the purpose

05:36.020 --> 05:37.860
of using artificial intelligence.

05:37.860 --> 05:42.340
And it's also used for the percentage chance we all die.

05:42.340 --> 05:47.340
Anthropic CEO Dario Amadei recently put it at 10 to 25%.

05:48.820 --> 05:52.180
AI safety research king, Elias Ryukowski,

05:52.180 --> 05:54.180
puts it at 98%.

05:56.380 --> 05:58.340
In the last week or so in the news,

05:58.340 --> 06:01.380
enter Lena Kahn, the commissioner,

06:01.380 --> 06:04.940
excuse me, the chairman of the Federal Trade Commission,

06:05.220 --> 06:09.420
personal PDOOM, the percentage chance

06:09.420 --> 06:12.100
we all die from AI.

06:12.100 --> 06:14.780
What is your PDOOM, Lena Kahn?

06:14.780 --> 06:18.620
What is your probability that AI will kill us all?

06:19.860 --> 06:21.780
I have to stay an optimist on this one.

06:21.780 --> 06:25.580
So I'm going to hedge on the side of lower risk there.

06:25.580 --> 06:27.100
So are you zero?

06:27.100 --> 06:30.180
No, no, not zero, maybe like 15%.

06:30.180 --> 06:31.020
Oh, all right.

06:31.020 --> 06:31.860
Yeah.

06:31.860 --> 06:34.220
And they say there are no techno optimists in the government.

06:34.500 --> 06:39.500
This is a very high ranking U.S. government official.

06:40.660 --> 06:42.900
Did she go running back to her office

06:42.900 --> 06:46.500
and say, stop everything, we have to do something about this?

06:46.500 --> 06:49.620
No, not at all.

06:49.620 --> 06:54.620
Somehow, a 15% chance of every last human on Earth

06:56.020 --> 07:01.020
being murdered by AI is somehow at a very high level.

07:04.220 --> 07:05.340
Acceptable.

07:07.540 --> 07:09.820
This is the cost of doing business?

07:12.340 --> 07:15.100
There is no fucking business if we're all dead.

07:19.860 --> 07:22.180
There was a ton of drama at OpenAI recently,

07:22.180 --> 07:23.180
as I'm sure you know.

07:23.180 --> 07:28.180
It seems very clear that money and speed won

07:28.620 --> 07:31.860
and safety lost, but for three days,

07:31.860 --> 07:35.220
OpenAI had an interim CEO named Emmett Shear,

07:35.220 --> 07:37.460
the former CEO of Twitch.

07:39.180 --> 07:42.420
This is an important Silicon Valley leader

07:42.420 --> 07:44.580
who clearly gets this stuff.

07:44.580 --> 07:46.020
Listen.

07:46.020 --> 07:47.740
Generally, I am very pro-technology

07:47.740 --> 07:49.260
and I really believe the upsides

07:49.260 --> 07:50.860
usually outweigh the downsides.

07:50.860 --> 07:52.900
Every technology can be misused.

07:52.900 --> 07:54.980
Regulating early is usually a mistake.

07:54.980 --> 07:56.900
I have a very specific concern about AI.

07:56.900 --> 07:57.940
We built an intelligence.

07:57.940 --> 07:58.780
It's kind of amazing, actually.

07:58.780 --> 07:59.900
It may not be the smartest intelligence,

07:59.900 --> 08:01.340
but it is unintelligence.

08:01.340 --> 08:04.220
It can solve problems and make arbitrary plans.

08:05.340 --> 08:08.300
At some point, as it gets better,

08:08.300 --> 08:09.980
the kinds of problems it will be able to solve

08:09.980 --> 08:14.060
will include programming, chip design, material science,

08:14.060 --> 08:17.020
power of production, all of the things you would need

08:17.020 --> 08:19.700
to design an artificial intelligence.

08:19.700 --> 08:21.580
At that point, you will be able to point

08:21.580 --> 08:24.140
the thing we've built back at itself.

08:24.140 --> 08:25.940
And this will happen before you get that point

08:25.940 --> 08:26.780
with humans in the loop.

08:26.780 --> 08:28.420
It already is happening with humans in the loop,

08:28.420 --> 08:31.320
but that loop will get tighter and tighter and tighter.

08:31.320 --> 08:32.720
And faster and faster and faster

08:32.720 --> 08:35.880
until it can fully self-improve itself.

08:35.880 --> 08:40.520
At which point, it will get very fast very quickly.

08:40.520 --> 08:41.440
And that kind of intelligence

08:41.440 --> 08:43.640
is just an intrinsically very dangerous thing

08:43.640 --> 08:45.800
because intelligence is power.

08:45.800 --> 08:48.320
Human beings are the dominant far alive on this planet

08:48.320 --> 08:49.840
pretty much entirely because we were smarter

08:49.840 --> 08:50.600
than the other creatures.

08:50.600 --> 08:53.280
Now, I just laid out a chain of argument

08:53.280 --> 08:55.960
with a lot of if this, then this, if this, then this,

08:55.960 --> 08:57.560
if this, then this.

08:57.560 --> 09:01.240
I know Eliza thinks that like we're all dooms for sure.

09:03.440 --> 09:06.480
I buy his doom argument, I buy the chain and the logic,

09:06.480 --> 09:10.160
like my P-doom, my probability of doom is like,

09:10.160 --> 09:11.560
my bid-ask spread and that's pretty high

09:11.560 --> 09:12.440
because I have a lot of uncertainty,

09:12.440 --> 09:17.440
but I would say it's like between like five and 50.

09:18.920 --> 09:20.800
So there's a wide spread.

09:20.800 --> 09:22.800
I think Paul Cristiano 50, you know.

09:22.800 --> 09:26.080
Paul Cristiano who handled a lot of the stuff

09:26.120 --> 09:29.360
with an open AI, I think said 25 to 50.

09:29.360 --> 09:32.720
It seems like if you talk to most AI researchers,

09:32.720 --> 09:35.200
there's some preponderance of people that give some percentage.

09:35.200 --> 09:37.160
That should cause you to shit your pants.

09:38.240 --> 09:39.960
I think it's important for you to hear

09:39.960 --> 09:41.840
just how little of a secret

09:41.840 --> 09:45.600
the existential threat we face is.

09:46.480 --> 09:50.200
This is not fringe conspiracy shit in any way.

09:50.200 --> 09:51.700
This is not opinion.

09:52.700 --> 09:56.700
I'm just connecting the dots of the facts

09:57.620 --> 10:00.340
that the makers of AI and many others

10:00.340 --> 10:03.060
are laying out there openly saying.

10:03.940 --> 10:06.300
So on to today's show.

10:06.300 --> 10:08.500
All right, wouldn't it be great

10:08.500 --> 10:13.060
if AI safety was substantively debated

10:13.060 --> 10:15.820
on American television every night?

10:15.820 --> 10:19.340
Talk about everything else under the sun, so much crap.

10:19.340 --> 10:20.660
Wouldn't it be great if we talked about

10:20.660 --> 10:22.260
really the important things?

10:23.140 --> 10:26.340
Well, in Toronto, there's a wonderful organization

10:26.340 --> 10:29.460
that runs what's called the Monk Debates.

10:29.460 --> 10:33.100
They're a series of exceptionally well done televised debates

10:33.100 --> 10:36.680
on a wide variety of topics affecting public life.

10:36.680 --> 10:39.460
The Monk Debate focused on AI safety

10:39.460 --> 10:42.740
and it was absolutely extraordinary.

10:42.740 --> 10:45.960
Each debate is framed around a single statement

10:45.960 --> 10:49.040
and it's two on two, pro versus con.

10:49.920 --> 10:52.640
The statement for the AI safety debate was,

10:53.560 --> 10:56.880
be it resolved, AI research and development

10:56.880 --> 10:59.000
poses an existential threat.

11:00.440 --> 11:02.440
Be it resolved, AI research and development

11:02.440 --> 11:04.680
poses an existential threat.

11:04.680 --> 11:07.640
Well, as you should know from watching this show

11:07.640 --> 11:11.240
for humanity, that statement should not be controversial.

11:11.240 --> 11:15.680
It is a widely accepted fact in most circles.

11:15.680 --> 11:18.840
It's something nearly everyone who's anyone in big AI

11:18.840 --> 11:22.880
signed a statement saying that AI is an existential threat

11:22.880 --> 11:26.720
along with the lines of pandemic and nuclear war.

11:26.720 --> 11:28.440
So the whole premise of this debate

11:28.440 --> 11:32.920
is something nearly the whole industry already concedes,

11:32.920 --> 11:37.920
but not everyone, not nearly, sadly.

11:39.440 --> 11:41.120
For my non-tech audience out there,

11:41.120 --> 11:43.120
it may shock you to know that there is a small

11:43.120 --> 11:47.600
but extremely powerful group of people in Silicon Valley

11:47.640 --> 11:51.480
who call themselves effective accelerationists.

11:53.480 --> 11:55.840
They believe that we will solve humanity's problems

11:55.840 --> 12:00.040
by racing towards technological advances.

12:00.040 --> 12:01.160
For the purpose of this show,

12:01.160 --> 12:03.760
we're just gonna call them accelerationists.

12:05.000 --> 12:08.460
Since I opened the For Humanity X account

12:08.460 --> 12:10.240
more than a month ago,

12:10.240 --> 12:12.320
I've been in there mixing it up with a lot of people

12:12.320 --> 12:14.440
and I have gotten a real palpable sense

12:14.480 --> 12:17.960
of these folks and their attitudes.

12:17.960 --> 12:20.000
It's my personal opinion.

12:20.000 --> 12:22.960
They're obnoxious, they're bullies,

12:23.840 --> 12:26.480
they're know-it-alls of the worst kind

12:26.480 --> 12:30.620
and they somehow have the audacity and arrogance

12:30.620 --> 12:34.640
to believe that you and I and everyone else on earth

12:34.640 --> 12:39.640
has given them permission to risk ending the future

12:40.640 --> 12:44.180
for our families forever.

12:46.520 --> 12:48.440
They believe we've authorized this,

12:50.080 --> 12:52.120
but their desire to accelerate technology

12:52.120 --> 12:54.960
affects everyone, not just them.

12:54.960 --> 12:59.960
So the single biggest asshole among them,

13:00.280 --> 13:04.440
the most flamboyantly obnoxious cavalier,

13:04.440 --> 13:06.840
shit on my leg and tell me it's raining,

13:06.840 --> 13:09.200
I'm going to cheer as the world ends,

13:09.200 --> 13:13.240
risk means nothing to me, fuck your kid's future, lunatic,

13:14.960 --> 13:18.960
is the director of AI research at Metta.

13:18.960 --> 13:22.880
His name is Yann Lecun, he's French

13:22.880 --> 13:25.480
and that kind of kills me because I love the French

13:25.480 --> 13:27.240
and hearing him say all this bullshit

13:27.240 --> 13:29.080
in that beautiful French accent

13:29.080 --> 13:30.920
makes it even more painful.

13:31.960 --> 13:34.320
Yann is one half of Team Khan

13:34.320 --> 13:37.160
in the monk debate on AI safety.

13:37.160 --> 13:39.480
Listen closely as you hear him speak,

13:39.480 --> 13:41.800
you'll hear no specifics.

13:41.800 --> 13:44.160
He's gonna tell you that something much smarter

13:44.160 --> 13:49.160
than a human is going to be subservient to humans.

13:51.000 --> 13:53.840
Ladies and gentlemen, with no further ado,

13:53.840 --> 13:57.280
presenting the one and only Yann Lecun.

13:58.800 --> 14:02.100
Those systems are controllable, they can be made safe

14:02.140 --> 14:06.460
as long as we implement the safety objectives.

14:07.580 --> 14:10.460
And the surprising thing is that they will have emotions,

14:10.460 --> 14:13.940
they will have empathy, they will have all the things

14:13.940 --> 14:18.060
that we require entities in the world to have

14:18.060 --> 14:20.340
if we want them to behave properly.

14:20.340 --> 14:23.580
So I do not believe that we can achieve anything close

14:23.580 --> 14:27.700
to human level intelligence without endowing AI systems

14:27.740 --> 14:31.940
with this kind of emotions,

14:32.940 --> 14:34.540
similar to human emotions.

14:34.540 --> 14:36.420
This will be the way to control them.

14:36.420 --> 14:41.420
Now, one set of emotions that we can hardware into them

14:42.100 --> 14:43.660
is subservience.

14:43.660 --> 14:47.180
So I have a positive view, as you can tell,

14:47.180 --> 14:50.940
and I think there is a very efficient way

14:50.940 --> 14:53.580
or good way of making AI systems safe

14:53.580 --> 14:56.140
is gonna be arduous engineering,

14:56.140 --> 15:00.060
just like making turbojet safe, it took decades.

15:01.140 --> 15:03.340
And it's hard engineering, but it's doable.

15:04.500 --> 15:08.540
Just like making a turbojet, says Yann Lecun.

15:08.540 --> 15:10.880
That is deliberately misleading.

15:12.780 --> 15:15.220
Giving them emotions will allow us to control them,

15:15.220 --> 15:17.180
says Yann Lecun.

15:17.180 --> 15:21.540
That is incredibly optimistic speculation at best.

15:21.540 --> 15:26.540
We can hardwire them into subservience, says Yann Lecun.

15:26.860 --> 15:30.380
That is simply ridiculous to claim.

15:30.380 --> 15:31.540
We might be able to do that,

15:31.540 --> 15:35.020
as many safety researchers say, though, that we won't.

15:35.020 --> 15:37.020
But even those who say we might be able

15:37.020 --> 15:40.340
to make them subservient says it would take decades,

15:40.340 --> 15:43.240
like three to five decades, to do the research

15:43.240 --> 15:44.740
to figure out how to do that.

15:44.740 --> 15:48.700
And as you know, the timeline is two to 10 years

15:48.700 --> 15:53.340
for AGI, the Singularity and the Unknown Future after that.

15:53.340 --> 15:57.940
These two timelines are catastrophically misaligned.

15:57.940 --> 16:00.460
But let's get back to the debate.

16:00.460 --> 16:04.500
So Yann's tag team partner is Melanie Mitchell.

16:04.500 --> 16:08.460
She has been working on AI research since the 1980s

16:08.460 --> 16:10.220
as a college student.

16:10.220 --> 16:13.020
I will let Melanie Mitchell introduce herself,

16:13.020 --> 16:15.580
but I will say, holy shit, she is wild.

16:15.580 --> 16:16.500
She's pissed off.

16:16.500 --> 16:19.380
We're even talking about existential risk.

16:20.700 --> 16:23.420
First, I'll argue that the possible scenarios

16:23.420 --> 16:26.900
that people have dreamed up for AI existential threats

16:26.900 --> 16:30.220
are all based on unfounded speculations

16:30.220 --> 16:34.180
rather than on science or empirical evidence.

16:34.180 --> 16:36.980
Second, well, we can all acknowledge

16:36.980 --> 16:40.820
that AI presents many risks and harms.

16:40.820 --> 16:45.620
None of them rise to the extreme level of existential,

16:45.620 --> 16:49.420
saying that AI literally threatens human extinction,

16:49.420 --> 16:51.980
says a very high bar.

16:51.980 --> 16:56.100
Finally, claiming that AI is an existential threat

16:56.100 --> 16:58.060
is itself harmful.

16:58.060 --> 17:01.220
It misleads people about the current state

17:01.220 --> 17:03.420
and likely future of AI.

17:03.420 --> 17:08.140
Such sensationalist claims deflect attention

17:08.140 --> 17:11.180
for real immediate risks.

17:11.180 --> 17:13.860
And further might result in blocking the potential benefits

17:13.860 --> 17:17.460
that we could reap from technological progress.

17:17.460 --> 17:19.140
Holy fucking hell.

17:19.140 --> 17:20.900
All right, so I have not done a great job

17:20.900 --> 17:23.020
in the first five shows of letting you hear

17:23.020 --> 17:25.580
from the other side of this debate,

17:25.580 --> 17:27.620
but that's going to change right now.

17:27.620 --> 17:30.260
I haven't really included the other side of the AI safety

17:30.260 --> 17:32.780
debate so far because objectively,

17:32.780 --> 17:36.340
I find their case so incredibly weak.

17:36.340 --> 17:38.700
But you're going to need to make up your own mind on this.

17:38.700 --> 17:42.020
So today, we're going to show you a lot of Jan and Melanie,

17:42.020 --> 17:45.580
team acceleration, team, we're willing to risk

17:45.580 --> 17:48.700
whether your kids get to have kids,

17:48.700 --> 17:51.140
and we're going to do so without even attempting

17:51.140 --> 17:53.220
to gain your consent.

17:56.060 --> 17:58.220
Here's some more of Jan Lacoon laying out his case

17:58.220 --> 18:03.220
as to why AI is in no way a threat of human extinction.

18:03.460 --> 18:04.980
I hope you'll enjoy as I do

18:04.980 --> 18:08.300
when the monk debates moderator jumps in to correct

18:08.300 --> 18:11.540
the completely disingenuous misleading frame

18:11.540 --> 18:15.580
that Jan puts around the question of why an AI system

18:15.580 --> 18:18.300
would cause human extinction.

18:18.300 --> 18:20.260
As you well know by now,

18:20.260 --> 18:22.820
human extinction would not come from an AI system

18:22.820 --> 18:25.460
with a desire to arbitrarily dominate.

18:25.460 --> 18:28.860
These systems won't be mad at us or vindictive.

18:28.860 --> 18:31.700
AI safety experts believe they will just have

18:31.700 --> 18:33.820
different goals than ours.

18:33.820 --> 18:38.220
But here's Jan spinning his deceptive case

18:38.220 --> 18:39.660
and getting called on it.

18:39.660 --> 18:42.500
Because we are social animals, we are a social species,

18:42.500 --> 18:46.060
and nature has evolved us to organize ourselves

18:46.060 --> 18:48.380
hierarchically like baboons, like chimpanzees,

18:49.460 --> 18:50.380
not orangutans.

18:50.380 --> 18:52.380
Orangutans have no desire to dominate anybody

18:52.380 --> 18:54.340
because they're not a social species.

18:54.340 --> 18:58.900
So this desire to dominate has nothing to do with intelligence.

18:58.900 --> 19:01.780
They're almost as smart as we are, by the way.

19:01.780 --> 19:03.580
So this is nothing to do with intelligence.

19:03.580 --> 19:07.300
We can make intelligent machines that are superior to us,

19:07.300 --> 19:08.900
but have no desire to dominate.

19:10.460 --> 19:11.940
I lead a research lab,

19:11.940 --> 19:13.980
I only hire people who are smarter than me,

19:13.980 --> 19:15.820
none of them want my job.

19:15.820 --> 19:16.660
Now.

19:16.660 --> 19:17.500
So.

19:18.380 --> 19:20.380
But Jan, is it just a second point?

19:20.380 --> 19:21.700
Just to refer to the other side of the debate,

19:21.700 --> 19:22.980
because it's something I think the audience

19:22.980 --> 19:24.340
would appreciate understanding.

19:24.340 --> 19:26.660
It's not so much the desire to dominate,

19:26.660 --> 19:28.180
it's the control problem.

19:28.180 --> 19:30.540
It's that you've set them some goals,

19:30.540 --> 19:32.740
maybe very noble and great goals,

19:32.740 --> 19:35.660
but they start doing other things to achieve those goals,

19:35.660 --> 19:37.460
which are antithetical to our interests.

19:37.460 --> 19:39.540
It's not that they're trying to dominate,

19:39.540 --> 19:42.980
it's that there is a tragedy of the commons that goes on.

19:42.980 --> 19:43.820
It's the same thing with companies.

19:43.820 --> 19:45.420
This is the goal and I'm not a problem.

19:45.420 --> 19:47.660
So how do we design goals for machines

19:47.660 --> 19:49.380
so that they behave properly?

19:49.380 --> 19:51.140
And again, this is something that's,

19:51.140 --> 19:53.380
you know, a difficult engineering problem,

19:53.380 --> 19:56.420
but this is not a problem that we are unfamiliar with,

19:56.420 --> 20:00.220
because as societies, we've been doing this for millennia.

20:00.220 --> 20:01.740
This is called making laws.

20:02.740 --> 20:06.700
Sure, we'll just make some laws, problem solved.

20:08.180 --> 20:11.620
We've been solving problems like this for millennia.

20:13.660 --> 20:16.380
No, we have not.

20:16.380 --> 20:21.380
No problem ever before in our past is in any way

20:21.740 --> 20:24.940
like the challenge of artificial intelligence,

20:24.940 --> 20:28.300
but don't worry, I don't have to make the counterarguments.

20:28.300 --> 20:32.700
We have an all-star duo on the other side of this debate.

20:32.700 --> 20:35.300
Please say hello to our old friend

20:35.300 --> 20:38.540
and powerhouse AI safety researcher,

20:38.540 --> 20:41.460
MIT professor, Max Tegmark.

20:41.460 --> 20:43.540
Back in the Stone Age with a rock,

20:43.540 --> 20:45.820
maybe someone could kill five people.

20:45.820 --> 20:49.060
300 years ago with a bomb, maybe a hundred people.

20:50.700 --> 20:55.740
In 1945 with a couple of nukes, 250,000 people

20:55.740 --> 20:58.100
with bioweapons, even more,

20:58.100 --> 21:01.180
with nuclear winter, according to a recent science article,

21:02.060 --> 21:03.740
over five billion people.

21:03.740 --> 21:08.740
Now the blast radius has risen up to about 60% of humanity.

21:09.540 --> 21:12.140
And since, as we'll argue,

21:12.140 --> 21:14.020
superhuman intelligence is going to be

21:14.020 --> 21:16.540
way more powerful than any of this.

21:16.540 --> 21:20.820
Its blast radius can easily be 100% of humanity,

21:20.820 --> 21:24.300
giving it the potential power to really wipe us out.

21:24.300 --> 21:29.300
Again, this is different than anything else ever before.

21:29.420 --> 21:34.300
Nuclear bombs cannot make more nuclear bombs by themselves.

21:34.300 --> 21:37.380
Nuclear bombs cannot decide to detonate themselves

21:37.380 --> 21:41.100
at a location and time of their choosing.

21:42.060 --> 21:45.620
This is different than anything else ever before.

21:45.620 --> 21:48.660
Max, please tell the people.

21:48.660 --> 21:50.940
It can do all the intelligent things

21:50.940 --> 21:53.020
that we humans can do just better.

21:54.060 --> 21:57.900
For example, it can do goal-oriented behavior.

21:57.900 --> 22:02.380
It can persuade, manipulate, hire people,

22:02.380 --> 22:07.380
start companies, build robots, do scientific research.

22:08.540 --> 22:09.500
It could, for example,

22:09.500 --> 22:11.660
research how to make more powerful bioweapons

22:12.780 --> 22:16.020
or how to make even more intelligent systems

22:16.020 --> 22:19.660
so it could recursively self-improve itself.

22:19.660 --> 22:23.340
It also could do things that we humans cannot do at all.

22:23.340 --> 22:25.500
It could very easily make copies of itself.

22:25.900 --> 22:30.260
If you imagine for a moment a super human AI

22:30.260 --> 22:33.180
that can think, say a thousand times faster

22:33.180 --> 22:34.140
than a human researcher,

22:34.140 --> 22:38.740
so it can in nine hours do years worth of research,

22:38.740 --> 22:41.140
but now instead think of a million of those,

22:41.140 --> 22:44.340
a swarm of a million super intelligent AIs

22:44.340 --> 22:46.420
where as soon as one of them discovers something new,

22:46.420 --> 22:49.540
it can instantly share that new skill with all of them.

22:49.540 --> 22:52.780
That's the kind of power we're talking about here.

22:52.820 --> 22:57.620
And finally, super human AI will probably be

22:57.620 --> 22:59.860
a very alien kind of intelligence

22:59.860 --> 23:03.580
that lacks anything like human emotions or empathy.

23:05.380 --> 23:08.380
I could listen to Max Tagmark talk all day.

23:08.380 --> 23:10.180
I really hope to have him on this show

23:10.180 --> 23:11.620
sometime in the near future.

23:12.620 --> 23:15.140
In biotech, it's exactly the other way around.

23:15.140 --> 23:16.660
If you had come up with a new medicine

23:16.660 --> 23:18.980
and said, this cure is cancer, it's awesome,

23:18.980 --> 23:21.460
you can't just go sell it in the supermarket

23:21.460 --> 23:22.780
until someone proves it is there.

23:22.780 --> 23:25.900
It's your job to convince the Food and Drug Administration,

23:25.900 --> 23:28.140
the US or the Canadian authorities or whatever

23:28.140 --> 23:30.860
that this is safe and that the benefits,

23:30.860 --> 23:33.060
yes, the benefits outweigh the risks.

23:33.060 --> 23:35.660
It should be the responsibility of the companies

23:35.660 --> 23:37.540
that the first prove that this is safe

23:37.540 --> 23:38.820
before it gets deployed.

23:38.820 --> 23:41.740
We need to become like biotech.

23:41.740 --> 23:45.300
In this monk debate, Max is hardly alone.

23:45.300 --> 23:46.220
This is kind of fun,

23:46.220 --> 23:48.380
got a little bit of soap opera drama for you.

23:48.380 --> 23:50.620
So the highest prize in computer science

23:50.660 --> 23:52.100
is called the Touring Award,

23:52.100 --> 23:54.420
named after Alan Turing, the father of AI

23:54.420 --> 23:57.060
who we talked about a little bit last week.

23:57.060 --> 24:02.060
In 2018, the Touring Award went to three AI researchers

24:02.220 --> 24:04.580
for their breakthrough work on neural networks

24:04.580 --> 24:05.900
and artificial intelligence,

24:05.900 --> 24:08.860
which was one of the two major advancements in AI

24:08.860 --> 24:10.980
that got us to where we are today.

24:11.860 --> 24:15.580
The three Touring Award winners in 2018 were Jeffrey Hinton,

24:15.580 --> 24:17.580
who you've met in previous shows,

24:17.580 --> 24:21.500
the Quick Google, famously to voice his concerns

24:21.500 --> 24:24.780
about AI and human extinction.

24:24.780 --> 24:27.220
The other two winners were two of his former students

24:27.220 --> 24:29.300
from the University of Toronto.

24:31.060 --> 24:33.660
So on our stage at the monk debate,

24:33.660 --> 24:36.940
we have two of those former students,

24:36.940 --> 24:40.560
the two former students who won the Touring Award

24:40.560 --> 24:41.980
with Professor Hinton.

24:44.180 --> 24:47.380
One of them is Yan Lacoon.

24:48.580 --> 24:52.420
He now says his old mentor and professor, Jeffrey Hinton,

24:52.420 --> 24:56.400
is a doomer who's totally wrong about AI risk.

24:57.900 --> 25:00.820
The other of Professor Hinton's old students

25:00.820 --> 25:04.260
is Maxis' teammate for the debate,

25:04.260 --> 25:06.740
AI researcher, Yoshua Benjiro.

25:08.180 --> 25:11.060
Once we have machines that have a self-preservation goal,

25:11.060 --> 25:13.780
well, we are in trouble.

25:14.620 --> 25:18.580
You know, think about what happens when you want to survive.

25:18.580 --> 25:22.220
You don't want others to turn you off, right?

25:22.220 --> 25:24.740
And you need to be able to control your environment,

25:24.740 --> 25:26.260
which means control humans.

25:26.260 --> 25:30.820
So existential risk isn't just, well, we all disappear.

25:30.820 --> 25:33.620
It might be that we're all disempowered,

25:33.620 --> 25:36.980
that we are not anymore in the control of our destiny.

25:36.980 --> 25:39.820
And I don't think this is something we want.

25:39.820 --> 25:42.660
Not extinct, but not in charge.

25:42.660 --> 25:44.780
That is not something we want at all.

25:44.780 --> 25:46.980
But this debate, just like this show,

25:46.980 --> 25:50.580
we're gonna focus only on human extinction risk.

25:50.580 --> 25:55.580
Yan Lacoon thinks there's no extinction risk at all.

25:55.700 --> 26:00.460
Watch as he mocks this grave concern

26:00.460 --> 26:04.580
as mere science fiction and marvel

26:04.580 --> 26:09.580
as he backs up his bold claims with absolutely nothing.

26:10.500 --> 26:12.460
As fiction scenarios of, you know,

26:12.460 --> 26:15.300
the earth being wiped out, humanity being wiped out,

26:15.300 --> 26:17.460
this sounds like a James Bond movie, right?

26:17.460 --> 26:19.860
It's like the supervillain who goes in space

26:19.860 --> 26:22.620
and then kind of puts some deadly gas

26:22.620 --> 26:24.340
and eliminates all of humanity.

26:24.340 --> 26:25.620
It's a James Bond movie.

26:25.620 --> 26:26.860
And I can't disprove it.

26:26.860 --> 26:28.460
The same way, if I tell you,

26:28.460 --> 26:30.900
I used the Bertrand Russell idea,

26:30.900 --> 26:33.180
if I tell you there is a teapot flying

26:33.180 --> 26:37.460
between the orbits of Jupiter and Saturn,

26:37.460 --> 26:39.100
you're gonna tell me I'm crazy.

26:39.100 --> 26:41.260
But you can't disprove me, right?

26:41.260 --> 26:43.060
You can't disprove that assertion.

26:43.060 --> 26:46.100
It's gonna cost you a huge amount of resources to do this.

26:46.100 --> 26:49.420
So it's kind of the same thing with those doomsynarios.

26:49.420 --> 26:52.220
They're sci-fi, but I can't prove that they're wrong.

26:53.340 --> 26:55.100
But the risk is negligible.

26:55.100 --> 26:58.660
And the reason it's negligible of extension

26:58.660 --> 27:01.140
is because we build those things.

27:01.140 --> 27:01.980
We build them.

27:01.980 --> 27:03.580
We have agency.

27:03.580 --> 27:06.380
This is not superhuman intelligence.

27:06.380 --> 27:08.100
It's not something that's gonna just happen.

27:08.100 --> 27:09.420
So, of course, if it's not safe,

27:09.420 --> 27:11.340
we're not gonna build it, right?

27:16.420 --> 27:17.580
I mean, will you build a bomb

27:17.580 --> 27:19.700
that just blows up randomly?

27:19.700 --> 27:20.980
No, right?

27:22.220 --> 27:24.500
If it's not safe, we're not going to build it.

27:25.700 --> 27:28.820
What a total load of shit.

27:28.820 --> 27:31.340
How the fuck would you know if it's not safe

27:31.340 --> 27:34.100
until it's too late to stop it?

27:34.100 --> 27:36.500
That's the whole fundamental fucking problem

27:36.500 --> 27:37.780
with this situation.

27:37.780 --> 27:40.860
But, Jan, you know this much better than I do.

27:40.860 --> 27:43.460
So seriously, this is it, right?

27:43.460 --> 27:45.100
It would be reasonable after watching

27:45.100 --> 27:49.260
the first five episodes of this podcast to say,

27:49.260 --> 27:52.300
I'd like to hear more from the other side about this.

27:52.300 --> 27:54.620
So this is your chance to evaluate

27:54.620 --> 27:56.180
the strength of their logic.

27:57.420 --> 28:00.900
I went into this hoping to be persuaded by them,

28:00.900 --> 28:05.140
hoping to be persuaded that everything is fine.

28:05.140 --> 28:07.580
But now it's your turn.

28:07.580 --> 28:10.860
Let's do a little Jan and Max mode debate,

28:10.860 --> 28:14.780
soundbite ping pong, and then you decide for yourself.

28:14.780 --> 28:18.220
If you think everything is just fine.

28:18.220 --> 28:20.900
Remember what people were saying just before year 2000?

28:22.340 --> 28:24.020
Satellites were gonna fall out of the sky

28:24.020 --> 28:25.300
and crash into cities,

28:25.300 --> 28:28.100
and the phone system was gonna crash

28:28.100 --> 28:29.500
and civilization will end.

28:32.140 --> 28:33.700
It didn't happen.

28:33.700 --> 28:34.540
We're still here.

28:35.540 --> 28:36.780
So I think there's a little bit

28:36.780 --> 28:40.620
of the same kind of feeling of uncertainty.

28:40.620 --> 28:42.020
A lot of people have the feeling

28:42.020 --> 28:43.420
that bad things are gonna happen

28:43.420 --> 28:44.940
because they're not in control.

28:46.300 --> 28:48.860
They have the feeling that AI is just gonna happen,

28:48.860 --> 28:50.220
and there's nothing they can do about it,

28:50.220 --> 28:51.260
and that creates fear.

28:51.260 --> 28:53.700
And I can completely understand that.

28:53.700 --> 28:57.900
But some of us, in what could be construed

28:57.900 --> 29:01.900
as a driver's seat, there are plans

29:01.900 --> 29:02.980
to make those things safe.

29:03.020 --> 29:05.580
Stock traders will tell you the past performance

29:05.580 --> 29:08.300
is not an indicator of future performance,

29:08.300 --> 29:10.900
future results, yeah, future results.

29:10.900 --> 29:13.140
And it would be a huge mistake

29:13.140 --> 29:15.060
in an exponential technological growth

29:15.060 --> 29:18.020
to assume that just because something happened one way

29:18.020 --> 29:20.980
in the past is gonna continue being this way.

29:20.980 --> 29:24.580
AI is gonna be subservient to human.

29:24.580 --> 29:26.020
It's gonna be smarter than us,

29:26.020 --> 29:27.780
but it's not gonna reduce our agency.

29:27.780 --> 29:30.140
On the contrary, it's going to empower us.

29:31.140 --> 29:32.340
It's like having a staff

29:32.340 --> 29:34.420
of really smart people working for you.

29:34.420 --> 29:37.540
It's naive to think that just because you make something smart,

29:37.540 --> 29:39.740
it's only gonna suddenly care about humans.

29:39.740 --> 29:42.340
Ask some wooly mammoths

29:42.340 --> 29:45.780
if they feel so reassured that we're smarter than them.

29:45.780 --> 29:49.620
Then therefore, we would automatically adopt mammoth ethics

29:49.620 --> 29:51.420
and do things that were good for the mammoths

29:51.420 --> 29:52.420
that we didn't.

29:52.420 --> 29:55.380
That's why you probably haven't met any.

29:55.380 --> 29:57.660
I'll tell you what I think.

29:57.740 --> 29:59.900
I think Jan is full of shit

29:59.900 --> 30:03.300
and Max makes all the sense in the world.

30:04.620 --> 30:06.580
I really wish it was the other way around.

30:06.580 --> 30:10.980
I really, really do, but he just isn't at all.

30:12.940 --> 30:14.900
There are some great moments in the monk debate

30:14.900 --> 30:17.620
on AI safety where the teams really start

30:17.620 --> 30:19.380
to mix it up with each other

30:19.380 --> 30:21.820
and there are some really incredible exchanges

30:21.820 --> 30:22.820
I wanna show you.

30:22.820 --> 30:27.460
First, here are the 2018 Turing Award winners.

30:27.460 --> 30:30.820
Once united by science, now divided

30:30.820 --> 30:35.180
by the potentially existential results of their work.

30:35.180 --> 30:39.460
Mixing it up, Jan Lacoon goes for some classic

30:39.460 --> 30:42.260
historical comparison bullshit

30:42.260 --> 30:44.340
and Yoshua Bengio responds

30:44.340 --> 30:47.220
with what I think is force and logic.

30:47.220 --> 30:48.700
Socrates was against writing.

30:48.700 --> 30:53.100
He said, people are going to lose their memory, right?

30:54.060 --> 30:56.980
The Catholic Church was against printing press

30:57.020 --> 31:00.260
saying they would lose control of the dogma,

31:00.260 --> 31:03.740
which they did, they could do nothing about it.

31:03.740 --> 31:07.540
The Ottoman Empire banned the printing press

31:07.540 --> 31:09.060
and according to some historian,

31:09.060 --> 31:12.500
that's what accelerated their decline.

31:12.500 --> 31:16.940
And so every technology that makes people smarter

31:17.860 --> 31:20.820
or enables communication between people

31:20.820 --> 31:25.820
facilitates education, again, is interestingly good.

31:26.180 --> 31:28.380
And AI is kind of a new version of this.

31:28.380 --> 31:30.100
It's a new printing press.

31:30.100 --> 31:33.260
So long as it doesn't blow up in our face.

31:33.260 --> 31:35.700
And because that can match your hand

31:35.700 --> 31:37.300
with a printing press, right?

31:37.300 --> 31:41.220
Yes, if it's the problem is the scale, right?

31:41.220 --> 31:45.460
So long as we built technologies that could be harmful

31:45.460 --> 31:48.500
but on a small scale, the goods,

31:48.500 --> 31:51.900
the benefits overwhelm the dangers.

31:51.900 --> 31:55.060
Now we're talking about building technologies

31:55.100 --> 31:57.140
unlike any other technology.

31:57.140 --> 31:57.980
No.

31:57.980 --> 32:00.460
Because it's technology that can design its own technology.

32:00.460 --> 32:01.300
No.

32:01.300 --> 32:02.140
Yes.

32:02.140 --> 32:02.980
No.

32:02.980 --> 32:03.860
I don't think about super human AI.

32:03.860 --> 32:05.180
This is the subject.

32:05.180 --> 32:06.380
It's under control.

32:06.380 --> 32:07.220
It's under control.

32:07.220 --> 32:08.540
And we remain under control.

32:08.540 --> 32:11.140
It's very much like previous technologies.

32:11.140 --> 32:12.980
It's not quite relatively different.

32:12.980 --> 32:14.620
The experts have been studying this question,

32:14.620 --> 32:16.980
say it's going to be very hard to keep it under control.

32:16.980 --> 32:18.660
And that is why I'm here today.

32:20.780 --> 32:23.820
Melanie Mitchell is so special.

32:24.820 --> 32:26.740
She's pissed off we're even talking

32:26.740 --> 32:30.300
about existential risks from AI.

32:31.580 --> 32:33.860
Seriously, throughout the month debate,

32:33.860 --> 32:36.500
she sounds off on why it is bad for us

32:36.500 --> 32:38.740
to even talk about existential risk from AI

32:38.740 --> 32:40.580
when there are in her eyes

32:40.580 --> 32:45.180
so many more important, urgent threats

32:45.180 --> 32:48.420
that need our attention, such as disinformation

32:48.420 --> 32:52.460
and fake photos and videos.

32:52.660 --> 32:54.100
It takes our attention away

32:54.100 --> 32:56.660
from some of the real immediate risks,

32:56.660 --> 33:00.660
like disinformation and bias.

33:00.660 --> 33:03.460
So are you saying it's a 0% risk?

33:03.460 --> 33:05.340
0% existential risk?

33:05.340 --> 33:06.340
Is that your claim?

33:07.260 --> 33:12.260
Do I think there's a 0% risk of any scenario?

33:12.260 --> 33:13.500
No, of course not.

33:13.500 --> 33:14.820
What do you think it is then?

33:14.820 --> 33:15.660
1%?

33:15.660 --> 33:16.500
We're really debating.

33:16.500 --> 33:17.900
I can't put a number on it.

33:17.900 --> 33:19.540
I think it's quite low.

33:19.540 --> 33:20.620
And I think what we're debating

33:20.620 --> 33:24.900
is there a reasonable existential risk,

33:24.900 --> 33:28.380
a risk of end of civilization in the reasonable future?

33:28.380 --> 33:29.460
Otherwise, we wouldn't be up here to be.

33:29.460 --> 33:30.500
How high is the high?

33:30.500 --> 33:34.180
I'm not gonna like say 0.0000001.

33:34.180 --> 33:35.020
I mean, I can't say that.

33:35.020 --> 33:36.980
How high is too high for you then?

33:36.980 --> 33:37.820
How high is?

33:37.820 --> 33:41.300
How high a probability of a risk is too high for you?

33:41.300 --> 33:43.700
I don't think we can put a probability on it.

33:43.700 --> 33:44.700
We don't know.

33:44.700 --> 33:46.340
We don't know enough.

33:46.340 --> 33:48.020
No, I mean, it's acceptable.

33:48.020 --> 33:53.020
Okay, it's lower than yours being wiped out by meteor.

33:54.300 --> 33:56.380
And by the way, I can help with that problem.

33:56.380 --> 33:58.340
This is the essential question for anyone

33:58.340 --> 34:01.660
who wants to accelerate artificial intelligence.

34:01.660 --> 34:05.860
What percentage existential risk to humans

34:05.860 --> 34:07.940
is acceptable to you?

34:10.380 --> 34:12.940
In my opinion, when you are talking about

34:12.940 --> 34:16.900
a potential future with zero human beings,

34:16.900 --> 34:21.900
the only acceptable risk percentage is also zero.

34:25.020 --> 34:27.900
Here are the two former award-winning partners.

34:27.900 --> 34:30.220
Now on opposite sides of the question of

34:30.220 --> 34:32.820
don't you just need some good guys with good AI

34:32.820 --> 34:35.580
to beat some bad guys with bad AI?

34:35.580 --> 34:38.580
Decide for yourself if you think Yan Lacun

34:38.580 --> 34:43.580
is being appropriately optimistic or stunningly blind.

34:43.660 --> 34:47.580
Bad guys can use AI for bad things.

34:47.580 --> 34:49.500
There's many more good guys who can use the same

34:49.500 --> 34:52.220
more powerful AI to counteract it.

34:52.220 --> 34:53.220
So in the end,

34:53.220 --> 34:54.260
Those good guys are gonna win.

34:54.260 --> 34:56.620
Sometimes the attacker has the advantage.

34:56.620 --> 34:58.740
There is absolutely no reason to believe

34:58.740 --> 35:00.180
that's the case for AI.

35:00.180 --> 35:03.020
In fact, we are facing this situation right now.

35:03.020 --> 35:03.860
How do you know?

35:03.860 --> 35:05.820
There's never anything that is completely perfect.

35:05.820 --> 35:06.660
Well, that's the issue.

35:06.660 --> 35:08.780
That's what we need to do more than what we're doing now.

35:08.780 --> 35:12.580
Again, it's the good guys' AI, which is superior,

35:12.580 --> 35:15.100
to the bad guys' AI.

35:15.100 --> 35:18.460
That's like saying that the way to stop a bad guy

35:18.460 --> 35:21.860
with a bio-weapon is to have a good guy with a bio-weapon.

35:21.860 --> 35:23.340
That's not what you do.

35:23.340 --> 35:26.700
The way you stop a bio-weapon attack is with vaccines

35:26.700 --> 35:28.420
and banning bio-weapons

35:28.420 --> 35:31.740
and having various forms of regulation and control.

35:31.740 --> 35:36.740
Next, Max asks Melanie what I think is a reasonable question.

35:36.900 --> 35:40.540
Why the product maker isn't responsible

35:40.580 --> 35:42.660
for proving their product is safe

35:42.660 --> 35:45.380
before they release it to the general public?

35:45.380 --> 35:48.180
But why is it our role to explain to you why it's dangerous?

35:48.180 --> 35:49.500
You still haven't answered my question.

35:49.500 --> 35:50.980
I've never said it wasn't dangerous.

35:50.980 --> 35:52.220
I've asked you twice.

35:52.220 --> 35:54.700
What is your plan for avoiding misuse?

35:54.700 --> 35:55.700
You haven't told me.

35:55.700 --> 35:56.540
You haven't asked you twice.

35:56.540 --> 35:58.980
What's your plan for solving the alignment problem?

35:58.980 --> 36:00.060
You haven't told me.

36:00.060 --> 36:00.900
I've asked you twice.

36:00.900 --> 36:03.980
Can we just finish to tell me what your plan is

36:03.980 --> 36:06.100
for avoiding the scenario where we get out-competed

36:06.100 --> 36:06.940
in disempowerment?

36:06.940 --> 36:07.940
You've said nothing.

36:08.780 --> 36:11.820
Unfortunately, I don't get paid enough

36:11.820 --> 36:13.900
to solve all these problems about AI policy.

36:13.900 --> 36:15.660
But if you either fill in medicine to have it,

36:15.660 --> 36:16.980
you'd have to have a plan.

36:16.980 --> 36:18.980
I don't have to explain why.

36:18.980 --> 36:21.500
I think the AI community is developing plans

36:21.500 --> 36:23.140
to mitigate risks.

36:24.100 --> 36:29.100
I think the community is developing plans

36:29.100 --> 36:33.220
to mitigate risks, she says.

36:33.220 --> 36:34.860
Is developing plans?

36:35.780 --> 36:37.460
This is like saying we'll figure out

36:37.460 --> 36:40.060
how to make landing gear mid-flight.

36:40.060 --> 36:43.980
We are already currently cruising as a world

36:43.980 --> 36:47.260
at 30,000 feet in a plane with no landing gear.

36:47.260 --> 36:50.020
The pilots already said, fuck it, let's just take off.

36:50.020 --> 36:51.860
We'll figure out landing gear mid-flight.

36:53.300 --> 36:56.580
We're hurtling through the air, no landing gear,

36:56.580 --> 37:00.380
and no knowledge of when we actually need to land,

37:00.380 --> 37:02.620
no knowledge of the timing.

37:02.660 --> 37:05.060
But the pilots are saying, please stay calm,

37:05.060 --> 37:08.580
do enjoy your snack and beverage service.

37:08.580 --> 37:09.620
It's nuts.

37:10.900 --> 37:13.860
Let's get back to the debate where Yoshua channels

37:13.860 --> 37:17.780
my inner spirit and tells Melanie, you're wrong.

37:18.780 --> 37:22.780
They might want to preserve their existence.

37:22.780 --> 37:25.340
They don't want to do anything, they're not alive.

37:25.340 --> 37:26.700
No, that's very easy to do.

37:26.700 --> 37:28.740
No, you're wrong, you're wrong.

37:28.740 --> 37:31.260
The systems that, for example,

37:32.100 --> 37:34.540
ChadGPT is essentially like an oracle.

37:34.540 --> 37:35.780
It doesn't really have a want,

37:35.780 --> 37:37.100
although literally it wants to please us

37:37.100 --> 37:38.700
because of the reinforcement learning.

37:38.700 --> 37:40.700
Yes, it's been trying to please us.

37:40.700 --> 37:44.140
Then it's actually easy to put a wrapper around them,

37:44.140 --> 37:47.260
to turn them into agents that have goals.

37:47.260 --> 37:50.580
It's actually easy to do that humans gave them.

37:50.580 --> 37:51.700
Yes, that's their own goal.

37:51.700 --> 37:53.940
Yes, and in order to achieve those goals,

37:53.940 --> 37:57.060
they're gonna have sub-goals and those sub-goals,

37:57.060 --> 38:00.620
for example, may include things like deception.

38:00.620 --> 38:04.020
Okay, finally, in one last-ditch effort,

38:04.020 --> 38:05.780
Melanie Mitchell tries to drop.

38:05.780 --> 38:09.500
Well, the people I talk to all think just like I do,

38:09.500 --> 38:12.220
and they all think everything is fine too.

38:12.220 --> 38:17.220
You say the people you talk to in AI have this belief.

38:17.420 --> 38:20.020
Well, the people I talk to in AI don't have that belief.

38:20.020 --> 38:23.620
I think the field is quite split, I would say.

38:23.620 --> 38:25.420
And I think there's quite a...

38:25.420 --> 38:27.980
Did it coin at 50% that we all get a dime?

38:28.980 --> 38:29.980
Well...

38:29.980 --> 38:31.700
Are any of the people who sign this letter

38:31.700 --> 38:34.500
saying that this is an existential risk?

38:34.500 --> 38:37.540
There's many people who have signed letters

38:37.540 --> 38:39.140
saying they think it's an existential risk.

38:39.140 --> 38:41.140
They don't say over what time scale.

38:41.140 --> 38:42.940
Yeah, but this was Jeff Hinton.

38:42.940 --> 38:46.660
Yeah, Jeff Hinton is a godfather.

38:46.660 --> 38:48.020
But he doesn't know everything.

38:48.020 --> 38:48.860
You don't talk to them.

38:48.860 --> 38:52.460
Yeah, I think that he's a smart guy,

38:52.460 --> 38:57.420
but I think that a lot of people have way over-hyped

38:57.420 --> 38:58.580
the risk of these things,

38:58.580 --> 39:01.140
and that's really convinced a lot of the general public

39:01.140 --> 39:03.740
that this is what we should be focusing on,

39:03.740 --> 39:06.660
not the more immediate harms of AI.

39:06.660 --> 39:07.500
Over-hyped.

39:10.300 --> 39:11.340
Science fiction.

39:13.460 --> 39:14.900
Just another Y2K.

39:16.980 --> 39:19.460
Just like the invention of the typewriter.

39:21.740 --> 39:23.620
Please, tell me in the comments,

39:23.620 --> 39:26.420
if you think these arguments made by Melanie and Jan

39:26.420 --> 39:28.500
are not total bullshit.

39:28.500 --> 39:30.420
Maybe I'm missing something,

39:30.420 --> 39:33.100
but I find them not only to be incredibly weak,

39:33.100 --> 39:36.060
I find them to be deceptive and manipulative.

39:37.260 --> 39:39.180
The audience at the end of the month debate

39:39.180 --> 39:41.780
was asked to vote for who they thought won.

39:41.780 --> 39:43.620
By a score of 67 to 33,

39:43.620 --> 39:47.300
they decided that Max and Yoshua were more convincing

39:47.300 --> 39:49.900
than Melanie and Jan.

39:49.900 --> 39:52.620
So I hope you got a lot out of that debate.

39:52.620 --> 39:55.300
I would encourage you to go to the Monk Debates website.

39:55.300 --> 39:56.900
They have a variety of debates

39:56.900 --> 39:58.740
on a lot of really interesting subjects,

39:58.740 --> 40:01.140
and they're all exceptionally well done,

40:01.140 --> 40:03.740
just like that debate was.

40:03.740 --> 40:05.300
So for six weeks now,

40:05.300 --> 40:07.260
I've been putting out a podcast per week,

40:07.260 --> 40:08.940
and as I said, I'm really encouraged

40:08.940 --> 40:11.460
by the response I've been getting.

40:11.460 --> 40:16.380
The question is, who will win this battle for humanity?

40:16.380 --> 40:19.300
The accelerationists, like Jan Lacoon,

40:19.300 --> 40:21.820
Melanie Mitchell and Sam Malt,

40:21.820 --> 40:24.460
or the decelerationists like Max Tegmark.

40:25.620 --> 40:28.460
Currently, if it was an NFL football game,

40:28.460 --> 40:29.980
I think the score would be something

40:29.980 --> 40:33.340
like 45 to three accelerationists.

40:34.220 --> 40:38.580
Acceleration is dominating in funding progress and talent.

40:39.820 --> 40:42.100
The game is in the second half,

40:42.100 --> 40:44.940
but we don't know when the whistle is gonna blow

40:44.940 --> 40:47.200
to say that the game is over.

40:48.060 --> 40:49.380
The odds could not be worse,

40:49.380 --> 40:51.220
the situation could not be more dire.

40:51.220 --> 40:55.380
You, me, and everyone else alive right now

40:55.380 --> 40:58.260
can either be blindly let off a cliff,

40:58.260 --> 41:01.900
or we can work to have a voice in this,

41:01.900 --> 41:04.900
work to learn what is happening and what is to come,

41:04.900 --> 41:08.100
and do anything and everything we can

41:08.100 --> 41:10.100
to give our children a future.

41:11.180 --> 41:15.060
So next week, we're gonna do something really different.

41:15.060 --> 41:16.820
Our show aims to move this debate

41:16.860 --> 41:19.260
from Tech, YouTube, and X

41:19.260 --> 41:22.620
into the family dinner table conversation.

41:22.620 --> 41:25.060
I think the voice of parents

41:25.060 --> 41:27.820
is sorely missing from this debate.

41:27.820 --> 41:29.460
As a parent myself, I can tell you,

41:29.460 --> 41:31.740
I've often said that if I was standing on a street corner

41:31.740 --> 41:33.580
and my son or daughter was next to me

41:33.580 --> 41:34.860
and a bus was coming at us

41:34.860 --> 41:36.620
and they were falling into the path of the bus

41:36.620 --> 41:37.820
and I could pull them back

41:37.820 --> 41:40.260
and I would have to fall and be killed by the bus

41:40.260 --> 41:42.420
instead of them, that I would see them

41:42.420 --> 41:44.700
falling towards the street corner to safety.

41:44.700 --> 41:46.580
I would fall myself into the street

41:46.580 --> 41:47.900
knowing the bus was about to hit me

41:47.900 --> 41:50.580
and I would do so with a big smile on my face,

41:50.580 --> 41:53.580
knowing I'd saved the life of my child, right?

41:53.580 --> 41:55.380
That is how parents are.

41:55.380 --> 41:58.860
Parents will do anything to protect their kids.

41:58.860 --> 42:02.260
Well, my fellow parents,

42:02.260 --> 42:04.420
it hurts me deeply to tell you

42:04.420 --> 42:08.420
that your children are under a threat more grave

42:08.420 --> 42:13.420
than any threat faced by any children ever before.

42:13.900 --> 42:14.820
Mine are too.

42:16.780 --> 42:20.540
Parents, there is an intruder in your home.

42:21.740 --> 42:24.100
There is an intruder in your school.

42:25.260 --> 42:27.620
It came from Silicon Valley.

42:27.620 --> 42:32.620
It is an alien and it doesn't care about anything

42:32.620 --> 42:33.780
that you care about.

42:39.620 --> 42:41.700
So next week, I'm gonna talk to three moms

42:41.700 --> 42:44.340
who are totally new to this AI safety debate,

42:44.340 --> 42:45.740
just three regular moms

42:45.740 --> 42:48.380
who through this podcast have become aware

42:48.380 --> 42:52.500
of this most grave threat to their families.

42:52.500 --> 42:55.540
I wanna know how it feels to know

42:55.540 --> 42:58.520
that this biggest threat hasn't even been on their radar.

42:59.620 --> 43:02.180
What they think about a few thousand people

43:02.180 --> 43:05.700
in Silicon Valley who are more than comfortable

43:05.700 --> 43:09.300
risking the mass murder of every child on earth

43:11.960 --> 43:14.220
and how these moms plan to go forward

43:14.220 --> 43:16.000
knowing what they now know.

43:17.180 --> 43:19.700
Should be an interesting show for sure.

43:19.700 --> 43:21.660
Thank you so much for watching.

43:21.660 --> 43:23.820
For Humanity, I'm John Sherman.

43:23.820 --> 43:25.420
I'll see you back here next week.

43:25.420 --> 43:31.860
Repeat Twitter, Instagram, Facebook, YouTube.

