1
00:00:00,000 --> 00:00:02,560
In biotech, it's exactly the other way around.

2
00:00:02,560 --> 00:00:04,400
If you had come up with a new medicine and said,

3
00:00:04,400 --> 00:00:06,400
this cures cancer, it's awesome,

4
00:00:06,400 --> 00:00:09,040
you can't just go sell it in the supermarket.

5
00:00:09,040 --> 00:00:11,840
The monk debate focused on AI safety,

6
00:00:11,840 --> 00:00:14,640
and it was absolutely extraordinary.

7
00:00:14,640 --> 00:00:18,880
Claiming that AI is an existential threat is itself harmful.

8
00:00:18,880 --> 00:00:22,000
It misleads people about the current state

9
00:00:22,000 --> 00:00:23,600
and likely future of AI.

10
00:00:23,600 --> 00:00:26,640
I've been excited to put together this show for some time now.

11
00:00:26,640 --> 00:00:27,800
It's spicy.

12
00:00:27,800 --> 00:00:30,200
Apologize I'm going to be cursing more in this show

13
00:00:30,200 --> 00:00:32,600
than I normally would.

14
00:00:32,600 --> 00:00:37,320
Such sensationalist claims deflect attention

15
00:00:37,320 --> 00:00:39,640
for real immediate risks.

16
00:00:39,640 --> 00:00:43,840
One set of emotions that we can hardwire into them

17
00:00:43,840 --> 00:00:45,480
is subservience.

18
00:00:45,480 --> 00:00:49,600
The most flamboyantly obnoxious cavalier.

19
00:00:49,600 --> 00:00:52,000
Shit on my leg and tell me it's raining.

20
00:00:52,000 --> 00:00:54,360
I'm going to cheer as the world ends.

21
00:00:54,360 --> 00:00:56,080
Risk means nothing to me.

22
00:00:56,080 --> 00:01:00,160
Fuck your kid's future lunatic.

23
00:01:00,160 --> 00:01:04,160
Is the director of AI research at Metta.

24
00:01:04,160 --> 00:01:06,360
His name is Jan LeCun.

25
00:01:06,360 --> 00:01:09,200
Of course, if it's not safe, we're not going to build it, right?

26
00:01:14,480 --> 00:01:17,760
I mean, will you build a bomb that just blows up randomly?

27
00:01:17,760 --> 00:01:20,720
No, right?

28
00:01:20,720 --> 00:01:24,200
If it's not safe, we're not going to build it.

29
00:01:24,200 --> 00:01:27,320
What a total load of shit.

30
00:01:27,320 --> 00:01:29,840
How the fuck would you know if it's not safe

31
00:01:29,840 --> 00:01:32,600
until it's too late to stop it?

32
00:01:32,600 --> 00:01:35,000
That's the whole fundamental fucking problem

33
00:01:35,000 --> 00:01:36,080
with this situation.

34
00:01:44,280 --> 00:01:48,200
Welcome to For Humanity, an AI safety podcast,

35
00:01:48,200 --> 00:01:50,480
episode six, The Monk Debate.

36
00:01:50,480 --> 00:01:51,960
I'm John Sherman, your host.

37
00:01:51,960 --> 00:01:53,640
Thanks so much for joining me.

38
00:01:53,640 --> 00:01:56,840
This is the AI safety podcast for the general public.

39
00:01:56,840 --> 00:01:58,640
No tech background required.

40
00:01:58,640 --> 00:02:02,680
This show is exclusively about the threat of human extinction

41
00:02:02,680 --> 00:02:05,000
from artificial intelligence.

42
00:02:05,000 --> 00:02:07,640
Please, right now, if you would, hit like

43
00:02:07,640 --> 00:02:11,680
and hit that subscribe button if you haven't hit it already.

44
00:02:11,680 --> 00:02:14,920
Repost these videos on your personal social channels

45
00:02:14,920 --> 00:02:17,760
and please tell people about this show.

46
00:02:17,760 --> 00:02:19,960
Those are some ways that we can help spread the word

47
00:02:19,960 --> 00:02:21,320
about these issues.

48
00:02:21,320 --> 00:02:24,560
Last week, I challenged you to convince someone in your life

49
00:02:24,560 --> 00:02:28,000
that the risk of human extinction

50
00:02:28,000 --> 00:02:30,200
from artificial intelligence is an issue

51
00:02:30,200 --> 00:02:32,160
that requires their attention.

52
00:02:32,160 --> 00:02:33,520
This week, I want to go one further

53
00:02:33,520 --> 00:02:34,480
and challenge you again.

54
00:02:34,480 --> 00:02:36,040
This week, I want to challenge you

55
00:02:36,040 --> 00:02:37,920
to talk to someone about these issues

56
00:02:37,920 --> 00:02:40,160
that is totally outside of this debate,

57
00:02:40,160 --> 00:02:43,720
someone that has no clue that AI is a threat to them.

58
00:02:44,760 --> 00:02:47,080
Here's an easy place, I think, to start.

59
00:02:47,960 --> 00:02:52,080
Every mom and dad out there needs to know about this.

60
00:02:52,080 --> 00:02:59,080
Moms and dads, fellow moms and dads, AI is the biggest threat

61
00:02:59,080 --> 00:03:02,240
to our children in the short term.

62
00:03:02,240 --> 00:03:06,640
It's not fentanyl, it's not some creeper in the park,

63
00:03:06,640 --> 00:03:08,640
it's not drunk driving.

64
00:03:09,640 --> 00:03:14,480
AI is the biggest short term threat to our children.

65
00:03:15,480 --> 00:03:17,480
Parents will do anything for their kids.

66
00:03:17,480 --> 00:03:19,720
So let's start with parents.

67
00:03:19,720 --> 00:03:22,400
Maybe this can be their wake-up call.

68
00:03:22,400 --> 00:03:25,120
I have been genuinely encouraged by the reaction

69
00:03:25,120 --> 00:03:28,200
I've been getting to the show online and in person,

70
00:03:28,200 --> 00:03:31,400
but we can get this message out much further.

71
00:03:31,400 --> 00:03:32,760
And if you like what I'm doing,

72
00:03:32,760 --> 00:03:35,200
I'm asking you to step out of your comfort zone,

73
00:03:35,200 --> 00:03:38,760
share this very serious, very unsettling news

74
00:03:38,760 --> 00:03:40,360
with someone you know.

75
00:03:40,360 --> 00:03:42,160
I'm going to tell you this,

76
00:03:42,240 --> 00:03:45,240
it is not going to be a fun experience.

77
00:03:45,240 --> 00:03:49,440
It sucks to be talking about this stuff.

78
00:03:49,440 --> 00:03:52,960
I want you to know that I hate sitting here

79
00:03:52,960 --> 00:03:54,920
and talking about this stuff,

80
00:03:54,920 --> 00:03:59,440
as much as you probably do sitting there and hearing it.

81
00:03:59,440 --> 00:04:01,880
It fucking sucks.

82
00:04:04,360 --> 00:04:07,280
It unthinkably, absolutely fucking sucks

83
00:04:07,280 --> 00:04:09,880
to seriously talking about this stuff.

84
00:04:10,000 --> 00:04:15,000
It fucking sucks to seriously talking about human extinction.

85
00:04:16,520 --> 00:04:21,520
But we are really threatened by AI killing all humans

86
00:04:22,960 --> 00:04:24,720
in as soon as two years.

87
00:04:24,720 --> 00:04:26,680
That is not hyperbole.

88
00:04:26,680 --> 00:04:29,360
That is literally what the AI companies

89
00:04:29,360 --> 00:04:34,360
and AI safety experts are openly admitting and telling us.

90
00:04:35,800 --> 00:04:39,560
So I can't just go about my normal life anymore.

91
00:04:39,560 --> 00:04:41,440
And if you believe what I've been telling you

92
00:04:41,440 --> 00:04:43,440
and what they've been telling you,

93
00:04:43,440 --> 00:04:45,120
you really can't either.

94
00:04:46,120 --> 00:04:48,280
So I'm going to sit here every week

95
00:04:48,280 --> 00:04:49,920
and try to get the word out.

96
00:04:49,920 --> 00:04:51,080
And I need your help,

97
00:04:51,080 --> 00:04:53,120
and I need the help of everyone you know.

98
00:04:53,120 --> 00:04:54,640
What if one year from today,

99
00:04:54,640 --> 00:04:57,760
everyone on earth was of the understanding

100
00:04:57,760 --> 00:05:00,120
that AI is a lethal threat to their family,

101
00:05:00,120 --> 00:05:01,120
to their children,

102
00:05:01,120 --> 00:05:03,880
to their children's yet unborn children,

103
00:05:03,880 --> 00:05:05,940
to every living thing on earth.

104
00:05:06,780 --> 00:05:09,860
In just a minute, we'll get today's show going.

105
00:05:09,860 --> 00:05:12,180
And I think you're going to really love it.

106
00:05:12,180 --> 00:05:15,380
I've been excited to put together this show for some time now.

107
00:05:15,380 --> 00:05:16,540
It's spicy.

108
00:05:16,540 --> 00:05:18,940
Apologize, I'm going to be cursing more in this show

109
00:05:18,940 --> 00:05:20,540
than I normally would.

110
00:05:20,540 --> 00:05:23,540
But first, I want you to hear from some important people

111
00:05:23,540 --> 00:05:27,540
in the news who are openly, recently discussing

112
00:05:27,540 --> 00:05:31,420
totally unthinkable things about artificial intelligence.

113
00:05:31,420 --> 00:05:35,420
So PDOOM is a technical term used for the purpose

114
00:05:36,020 --> 00:05:37,860
of using artificial intelligence.

115
00:05:37,860 --> 00:05:42,340
And it's also used for the percentage chance we all die.

116
00:05:42,340 --> 00:05:47,340
Anthropic CEO Dario Amadei recently put it at 10 to 25%.

117
00:05:48,820 --> 00:05:52,180
AI safety research king, Elias Ryukowski,

118
00:05:52,180 --> 00:05:54,180
puts it at 98%.

119
00:05:56,380 --> 00:05:58,340
In the last week or so in the news,

120
00:05:58,340 --> 00:06:01,380
enter Lena Kahn, the commissioner,

121
00:06:01,380 --> 00:06:04,940
excuse me, the chairman of the Federal Trade Commission,

122
00:06:05,220 --> 00:06:09,420
personal PDOOM, the percentage chance

123
00:06:09,420 --> 00:06:12,100
we all die from AI.

124
00:06:12,100 --> 00:06:14,780
What is your PDOOM, Lena Kahn?

125
00:06:14,780 --> 00:06:18,620
What is your probability that AI will kill us all?

126
00:06:19,860 --> 00:06:21,780
I have to stay an optimist on this one.

127
00:06:21,780 --> 00:06:25,580
So I'm going to hedge on the side of lower risk there.

128
00:06:25,580 --> 00:06:27,100
So are you zero?

129
00:06:27,100 --> 00:06:30,180
No, no, not zero, maybe like 15%.

130
00:06:30,180 --> 00:06:31,020
Oh, all right.

131
00:06:31,020 --> 00:06:31,860
Yeah.

132
00:06:31,860 --> 00:06:34,220
And they say there are no techno optimists in the government.

133
00:06:34,500 --> 00:06:39,500
This is a very high ranking U.S. government official.

134
00:06:40,660 --> 00:06:42,900
Did she go running back to her office

135
00:06:42,900 --> 00:06:46,500
and say, stop everything, we have to do something about this?

136
00:06:46,500 --> 00:06:49,620
No, not at all.

137
00:06:49,620 --> 00:06:54,620
Somehow, a 15% chance of every last human on Earth

138
00:06:56,020 --> 00:07:01,020
being murdered by AI is somehow at a very high level.

139
00:07:04,220 --> 00:07:05,340
Acceptable.

140
00:07:07,540 --> 00:07:09,820
This is the cost of doing business?

141
00:07:12,340 --> 00:07:15,100
There is no fucking business if we're all dead.

142
00:07:19,860 --> 00:07:22,180
There was a ton of drama at OpenAI recently,

143
00:07:22,180 --> 00:07:23,180
as I'm sure you know.

144
00:07:23,180 --> 00:07:28,180
It seems very clear that money and speed won

145
00:07:28,620 --> 00:07:31,860
and safety lost, but for three days,

146
00:07:31,860 --> 00:07:35,220
OpenAI had an interim CEO named Emmett Shear,

147
00:07:35,220 --> 00:07:37,460
the former CEO of Twitch.

148
00:07:39,180 --> 00:07:42,420
This is an important Silicon Valley leader

149
00:07:42,420 --> 00:07:44,580
who clearly gets this stuff.

150
00:07:44,580 --> 00:07:46,020
Listen.

151
00:07:46,020 --> 00:07:47,740
Generally, I am very pro-technology

152
00:07:47,740 --> 00:07:49,260
and I really believe the upsides

153
00:07:49,260 --> 00:07:50,860
usually outweigh the downsides.

154
00:07:50,860 --> 00:07:52,900
Every technology can be misused.

155
00:07:52,900 --> 00:07:54,980
Regulating early is usually a mistake.

156
00:07:54,980 --> 00:07:56,900
I have a very specific concern about AI.

157
00:07:56,900 --> 00:07:57,940
We built an intelligence.

158
00:07:57,940 --> 00:07:58,780
It's kind of amazing, actually.

159
00:07:58,780 --> 00:07:59,900
It may not be the smartest intelligence,

160
00:07:59,900 --> 00:08:01,340
but it is unintelligence.

161
00:08:01,340 --> 00:08:04,220
It can solve problems and make arbitrary plans.

162
00:08:05,340 --> 00:08:08,300
At some point, as it gets better,

163
00:08:08,300 --> 00:08:09,980
the kinds of problems it will be able to solve

164
00:08:09,980 --> 00:08:14,060
will include programming, chip design, material science,

165
00:08:14,060 --> 00:08:17,020
power of production, all of the things you would need

166
00:08:17,020 --> 00:08:19,700
to design an artificial intelligence.

167
00:08:19,700 --> 00:08:21,580
At that point, you will be able to point

168
00:08:21,580 --> 00:08:24,140
the thing we've built back at itself.

169
00:08:24,140 --> 00:08:25,940
And this will happen before you get that point

170
00:08:25,940 --> 00:08:26,780
with humans in the loop.

171
00:08:26,780 --> 00:08:28,420
It already is happening with humans in the loop,

172
00:08:28,420 --> 00:08:31,320
but that loop will get tighter and tighter and tighter.

173
00:08:31,320 --> 00:08:32,720
And faster and faster and faster

174
00:08:32,720 --> 00:08:35,880
until it can fully self-improve itself.

175
00:08:35,880 --> 00:08:40,520
At which point, it will get very fast very quickly.

176
00:08:40,520 --> 00:08:41,440
And that kind of intelligence

177
00:08:41,440 --> 00:08:43,640
is just an intrinsically very dangerous thing

178
00:08:43,640 --> 00:08:45,800
because intelligence is power.

179
00:08:45,800 --> 00:08:48,320
Human beings are the dominant far alive on this planet

180
00:08:48,320 --> 00:08:49,840
pretty much entirely because we were smarter

181
00:08:49,840 --> 00:08:50,600
than the other creatures.

182
00:08:50,600 --> 00:08:53,280
Now, I just laid out a chain of argument

183
00:08:53,280 --> 00:08:55,960
with a lot of if this, then this, if this, then this,

184
00:08:55,960 --> 00:08:57,560
if this, then this.

185
00:08:57,560 --> 00:09:01,240
I know Eliza thinks that like we're all dooms for sure.

186
00:09:03,440 --> 00:09:06,480
I buy his doom argument, I buy the chain and the logic,

187
00:09:06,480 --> 00:09:10,160
like my P-doom, my probability of doom is like,

188
00:09:10,160 --> 00:09:11,560
my bid-ask spread and that's pretty high

189
00:09:11,560 --> 00:09:12,440
because I have a lot of uncertainty,

190
00:09:12,440 --> 00:09:17,440
but I would say it's like between like five and 50.

191
00:09:18,920 --> 00:09:20,800
So there's a wide spread.

192
00:09:20,800 --> 00:09:22,800
I think Paul Cristiano 50, you know.

193
00:09:22,800 --> 00:09:26,080
Paul Cristiano who handled a lot of the stuff

194
00:09:26,120 --> 00:09:29,360
with an open AI, I think said 25 to 50.

195
00:09:29,360 --> 00:09:32,720
It seems like if you talk to most AI researchers,

196
00:09:32,720 --> 00:09:35,200
there's some preponderance of people that give some percentage.

197
00:09:35,200 --> 00:09:37,160
That should cause you to shit your pants.

198
00:09:38,240 --> 00:09:39,960
I think it's important for you to hear

199
00:09:39,960 --> 00:09:41,840
just how little of a secret

200
00:09:41,840 --> 00:09:45,600
the existential threat we face is.

201
00:09:46,480 --> 00:09:50,200
This is not fringe conspiracy shit in any way.

202
00:09:50,200 --> 00:09:51,700
This is not opinion.

203
00:09:52,700 --> 00:09:56,700
I'm just connecting the dots of the facts

204
00:09:57,620 --> 00:10:00,340
that the makers of AI and many others

205
00:10:00,340 --> 00:10:03,060
are laying out there openly saying.

206
00:10:03,940 --> 00:10:06,300
So on to today's show.

207
00:10:06,300 --> 00:10:08,500
All right, wouldn't it be great

208
00:10:08,500 --> 00:10:13,060
if AI safety was substantively debated

209
00:10:13,060 --> 00:10:15,820
on American television every night?

210
00:10:15,820 --> 00:10:19,340
Talk about everything else under the sun, so much crap.

211
00:10:19,340 --> 00:10:20,660
Wouldn't it be great if we talked about

212
00:10:20,660 --> 00:10:22,260
really the important things?

213
00:10:23,140 --> 00:10:26,340
Well, in Toronto, there's a wonderful organization

214
00:10:26,340 --> 00:10:29,460
that runs what's called the Monk Debates.

215
00:10:29,460 --> 00:10:33,100
They're a series of exceptionally well done televised debates

216
00:10:33,100 --> 00:10:36,680
on a wide variety of topics affecting public life.

217
00:10:36,680 --> 00:10:39,460
The Monk Debate focused on AI safety

218
00:10:39,460 --> 00:10:42,740
and it was absolutely extraordinary.

219
00:10:42,740 --> 00:10:45,960
Each debate is framed around a single statement

220
00:10:45,960 --> 00:10:49,040
and it's two on two, pro versus con.

221
00:10:49,920 --> 00:10:52,640
The statement for the AI safety debate was,

222
00:10:53,560 --> 00:10:56,880
be it resolved, AI research and development

223
00:10:56,880 --> 00:10:59,000
poses an existential threat.

224
00:11:00,440 --> 00:11:02,440
Be it resolved, AI research and development

225
00:11:02,440 --> 00:11:04,680
poses an existential threat.

226
00:11:04,680 --> 00:11:07,640
Well, as you should know from watching this show

227
00:11:07,640 --> 00:11:11,240
for humanity, that statement should not be controversial.

228
00:11:11,240 --> 00:11:15,680
It is a widely accepted fact in most circles.

229
00:11:15,680 --> 00:11:18,840
It's something nearly everyone who's anyone in big AI

230
00:11:18,840 --> 00:11:22,880
signed a statement saying that AI is an existential threat

231
00:11:22,880 --> 00:11:26,720
along with the lines of pandemic and nuclear war.

232
00:11:26,720 --> 00:11:28,440
So the whole premise of this debate

233
00:11:28,440 --> 00:11:32,920
is something nearly the whole industry already concedes,

234
00:11:32,920 --> 00:11:37,920
but not everyone, not nearly, sadly.

235
00:11:39,440 --> 00:11:41,120
For my non-tech audience out there,

236
00:11:41,120 --> 00:11:43,120
it may shock you to know that there is a small

237
00:11:43,120 --> 00:11:47,600
but extremely powerful group of people in Silicon Valley

238
00:11:47,640 --> 00:11:51,480
who call themselves effective accelerationists.

239
00:11:53,480 --> 00:11:55,840
They believe that we will solve humanity's problems

240
00:11:55,840 --> 00:12:00,040
by racing towards technological advances.

241
00:12:00,040 --> 00:12:01,160
For the purpose of this show,

242
00:12:01,160 --> 00:12:03,760
we're just gonna call them accelerationists.

243
00:12:05,000 --> 00:12:08,460
Since I opened the For Humanity X account

244
00:12:08,460 --> 00:12:10,240
more than a month ago,

245
00:12:10,240 --> 00:12:12,320
I've been in there mixing it up with a lot of people

246
00:12:12,320 --> 00:12:14,440
and I have gotten a real palpable sense

247
00:12:14,480 --> 00:12:17,960
of these folks and their attitudes.

248
00:12:17,960 --> 00:12:20,000
It's my personal opinion.

249
00:12:20,000 --> 00:12:22,960
They're obnoxious, they're bullies,

250
00:12:23,840 --> 00:12:26,480
they're know-it-alls of the worst kind

251
00:12:26,480 --> 00:12:30,620
and they somehow have the audacity and arrogance

252
00:12:30,620 --> 00:12:34,640
to believe that you and I and everyone else on earth

253
00:12:34,640 --> 00:12:39,640
has given them permission to risk ending the future

254
00:12:40,640 --> 00:12:44,180
for our families forever.

255
00:12:46,520 --> 00:12:48,440
They believe we've authorized this,

256
00:12:50,080 --> 00:12:52,120
but their desire to accelerate technology

257
00:12:52,120 --> 00:12:54,960
affects everyone, not just them.

258
00:12:54,960 --> 00:12:59,960
So the single biggest asshole among them,

259
00:13:00,280 --> 00:13:04,440
the most flamboyantly obnoxious cavalier,

260
00:13:04,440 --> 00:13:06,840
shit on my leg and tell me it's raining,

261
00:13:06,840 --> 00:13:09,200
I'm going to cheer as the world ends,

262
00:13:09,200 --> 00:13:13,240
risk means nothing to me, fuck your kid's future, lunatic,

263
00:13:14,960 --> 00:13:18,960
is the director of AI research at Metta.

264
00:13:18,960 --> 00:13:22,880
His name is Yann Lecun, he's French

265
00:13:22,880 --> 00:13:25,480
and that kind of kills me because I love the French

266
00:13:25,480 --> 00:13:27,240
and hearing him say all this bullshit

267
00:13:27,240 --> 00:13:29,080
in that beautiful French accent

268
00:13:29,080 --> 00:13:30,920
makes it even more painful.

269
00:13:31,960 --> 00:13:34,320
Yann is one half of Team Khan

270
00:13:34,320 --> 00:13:37,160
in the monk debate on AI safety.

271
00:13:37,160 --> 00:13:39,480
Listen closely as you hear him speak,

272
00:13:39,480 --> 00:13:41,800
you'll hear no specifics.

273
00:13:41,800 --> 00:13:44,160
He's gonna tell you that something much smarter

274
00:13:44,160 --> 00:13:49,160
than a human is going to be subservient to humans.

275
00:13:51,000 --> 00:13:53,840
Ladies and gentlemen, with no further ado,

276
00:13:53,840 --> 00:13:57,280
presenting the one and only Yann Lecun.

277
00:13:58,800 --> 00:14:02,100
Those systems are controllable, they can be made safe

278
00:14:02,140 --> 00:14:06,460
as long as we implement the safety objectives.

279
00:14:07,580 --> 00:14:10,460
And the surprising thing is that they will have emotions,

280
00:14:10,460 --> 00:14:13,940
they will have empathy, they will have all the things

281
00:14:13,940 --> 00:14:18,060
that we require entities in the world to have

282
00:14:18,060 --> 00:14:20,340
if we want them to behave properly.

283
00:14:20,340 --> 00:14:23,580
So I do not believe that we can achieve anything close

284
00:14:23,580 --> 00:14:27,700
to human level intelligence without endowing AI systems

285
00:14:27,740 --> 00:14:31,940
with this kind of emotions,

286
00:14:32,940 --> 00:14:34,540
similar to human emotions.

287
00:14:34,540 --> 00:14:36,420
This will be the way to control them.

288
00:14:36,420 --> 00:14:41,420
Now, one set of emotions that we can hardware into them

289
00:14:42,100 --> 00:14:43,660
is subservience.

290
00:14:43,660 --> 00:14:47,180
So I have a positive view, as you can tell,

291
00:14:47,180 --> 00:14:50,940
and I think there is a very efficient way

292
00:14:50,940 --> 00:14:53,580
or good way of making AI systems safe

293
00:14:53,580 --> 00:14:56,140
is gonna be arduous engineering,

294
00:14:56,140 --> 00:15:00,060
just like making turbojet safe, it took decades.

295
00:15:01,140 --> 00:15:03,340
And it's hard engineering, but it's doable.

296
00:15:04,500 --> 00:15:08,540
Just like making a turbojet, says Yann Lecun.

297
00:15:08,540 --> 00:15:10,880
That is deliberately misleading.

298
00:15:12,780 --> 00:15:15,220
Giving them emotions will allow us to control them,

299
00:15:15,220 --> 00:15:17,180
says Yann Lecun.

300
00:15:17,180 --> 00:15:21,540
That is incredibly optimistic speculation at best.

301
00:15:21,540 --> 00:15:26,540
We can hardwire them into subservience, says Yann Lecun.

302
00:15:26,860 --> 00:15:30,380
That is simply ridiculous to claim.

303
00:15:30,380 --> 00:15:31,540
We might be able to do that,

304
00:15:31,540 --> 00:15:35,020
as many safety researchers say, though, that we won't.

305
00:15:35,020 --> 00:15:37,020
But even those who say we might be able

306
00:15:37,020 --> 00:15:40,340
to make them subservient says it would take decades,

307
00:15:40,340 --> 00:15:43,240
like three to five decades, to do the research

308
00:15:43,240 --> 00:15:44,740
to figure out how to do that.

309
00:15:44,740 --> 00:15:48,700
And as you know, the timeline is two to 10 years

310
00:15:48,700 --> 00:15:53,340
for AGI, the Singularity and the Unknown Future after that.

311
00:15:53,340 --> 00:15:57,940
These two timelines are catastrophically misaligned.

312
00:15:57,940 --> 00:16:00,460
But let's get back to the debate.

313
00:16:00,460 --> 00:16:04,500
So Yann's tag team partner is Melanie Mitchell.

314
00:16:04,500 --> 00:16:08,460
She has been working on AI research since the 1980s

315
00:16:08,460 --> 00:16:10,220
as a college student.

316
00:16:10,220 --> 00:16:13,020
I will let Melanie Mitchell introduce herself,

317
00:16:13,020 --> 00:16:15,580
but I will say, holy shit, she is wild.

318
00:16:15,580 --> 00:16:16,500
She's pissed off.

319
00:16:16,500 --> 00:16:19,380
We're even talking about existential risk.

320
00:16:20,700 --> 00:16:23,420
First, I'll argue that the possible scenarios

321
00:16:23,420 --> 00:16:26,900
that people have dreamed up for AI existential threats

322
00:16:26,900 --> 00:16:30,220
are all based on unfounded speculations

323
00:16:30,220 --> 00:16:34,180
rather than on science or empirical evidence.

324
00:16:34,180 --> 00:16:36,980
Second, well, we can all acknowledge

325
00:16:36,980 --> 00:16:40,820
that AI presents many risks and harms.

326
00:16:40,820 --> 00:16:45,620
None of them rise to the extreme level of existential,

327
00:16:45,620 --> 00:16:49,420
saying that AI literally threatens human extinction,

328
00:16:49,420 --> 00:16:51,980
says a very high bar.

329
00:16:51,980 --> 00:16:56,100
Finally, claiming that AI is an existential threat

330
00:16:56,100 --> 00:16:58,060
is itself harmful.

331
00:16:58,060 --> 00:17:01,220
It misleads people about the current state

332
00:17:01,220 --> 00:17:03,420
and likely future of AI.

333
00:17:03,420 --> 00:17:08,140
Such sensationalist claims deflect attention

334
00:17:08,140 --> 00:17:11,180
for real immediate risks.

335
00:17:11,180 --> 00:17:13,860
And further might result in blocking the potential benefits

336
00:17:13,860 --> 00:17:17,460
that we could reap from technological progress.

337
00:17:17,460 --> 00:17:19,140
Holy fucking hell.

338
00:17:19,140 --> 00:17:20,900
All right, so I have not done a great job

339
00:17:20,900 --> 00:17:23,020
in the first five shows of letting you hear

340
00:17:23,020 --> 00:17:25,580
from the other side of this debate,

341
00:17:25,580 --> 00:17:27,620
but that's going to change right now.

342
00:17:27,620 --> 00:17:30,260
I haven't really included the other side of the AI safety

343
00:17:30,260 --> 00:17:32,780
debate so far because objectively,

344
00:17:32,780 --> 00:17:36,340
I find their case so incredibly weak.

345
00:17:36,340 --> 00:17:38,700
But you're going to need to make up your own mind on this.

346
00:17:38,700 --> 00:17:42,020
So today, we're going to show you a lot of Jan and Melanie,

347
00:17:42,020 --> 00:17:45,580
team acceleration, team, we're willing to risk

348
00:17:45,580 --> 00:17:48,700
whether your kids get to have kids,

349
00:17:48,700 --> 00:17:51,140
and we're going to do so without even attempting

350
00:17:51,140 --> 00:17:53,220
to gain your consent.

351
00:17:56,060 --> 00:17:58,220
Here's some more of Jan Lacoon laying out his case

352
00:17:58,220 --> 00:18:03,220
as to why AI is in no way a threat of human extinction.

353
00:18:03,460 --> 00:18:04,980
I hope you'll enjoy as I do

354
00:18:04,980 --> 00:18:08,300
when the monk debates moderator jumps in to correct

355
00:18:08,300 --> 00:18:11,540
the completely disingenuous misleading frame

356
00:18:11,540 --> 00:18:15,580
that Jan puts around the question of why an AI system

357
00:18:15,580 --> 00:18:18,300
would cause human extinction.

358
00:18:18,300 --> 00:18:20,260
As you well know by now,

359
00:18:20,260 --> 00:18:22,820
human extinction would not come from an AI system

360
00:18:22,820 --> 00:18:25,460
with a desire to arbitrarily dominate.

361
00:18:25,460 --> 00:18:28,860
These systems won't be mad at us or vindictive.

362
00:18:28,860 --> 00:18:31,700
AI safety experts believe they will just have

363
00:18:31,700 --> 00:18:33,820
different goals than ours.

364
00:18:33,820 --> 00:18:38,220
But here's Jan spinning his deceptive case

365
00:18:38,220 --> 00:18:39,660
and getting called on it.

366
00:18:39,660 --> 00:18:42,500
Because we are social animals, we are a social species,

367
00:18:42,500 --> 00:18:46,060
and nature has evolved us to organize ourselves

368
00:18:46,060 --> 00:18:48,380
hierarchically like baboons, like chimpanzees,

369
00:18:49,460 --> 00:18:50,380
not orangutans.

370
00:18:50,380 --> 00:18:52,380
Orangutans have no desire to dominate anybody

371
00:18:52,380 --> 00:18:54,340
because they're not a social species.

372
00:18:54,340 --> 00:18:58,900
So this desire to dominate has nothing to do with intelligence.

373
00:18:58,900 --> 00:19:01,780
They're almost as smart as we are, by the way.

374
00:19:01,780 --> 00:19:03,580
So this is nothing to do with intelligence.

375
00:19:03,580 --> 00:19:07,300
We can make intelligent machines that are superior to us,

376
00:19:07,300 --> 00:19:08,900
but have no desire to dominate.

377
00:19:10,460 --> 00:19:11,940
I lead a research lab,

378
00:19:11,940 --> 00:19:13,980
I only hire people who are smarter than me,

379
00:19:13,980 --> 00:19:15,820
none of them want my job.

380
00:19:15,820 --> 00:19:16,660
Now.

381
00:19:16,660 --> 00:19:17,500
So.

382
00:19:18,380 --> 00:19:20,380
But Jan, is it just a second point?

383
00:19:20,380 --> 00:19:21,700
Just to refer to the other side of the debate,

384
00:19:21,700 --> 00:19:22,980
because it's something I think the audience

385
00:19:22,980 --> 00:19:24,340
would appreciate understanding.

386
00:19:24,340 --> 00:19:26,660
It's not so much the desire to dominate,

387
00:19:26,660 --> 00:19:28,180
it's the control problem.

388
00:19:28,180 --> 00:19:30,540
It's that you've set them some goals,

389
00:19:30,540 --> 00:19:32,740
maybe very noble and great goals,

390
00:19:32,740 --> 00:19:35,660
but they start doing other things to achieve those goals,

391
00:19:35,660 --> 00:19:37,460
which are antithetical to our interests.

392
00:19:37,460 --> 00:19:39,540
It's not that they're trying to dominate,

393
00:19:39,540 --> 00:19:42,980
it's that there is a tragedy of the commons that goes on.

394
00:19:42,980 --> 00:19:43,820
It's the same thing with companies.

395
00:19:43,820 --> 00:19:45,420
This is the goal and I'm not a problem.

396
00:19:45,420 --> 00:19:47,660
So how do we design goals for machines

397
00:19:47,660 --> 00:19:49,380
so that they behave properly?

398
00:19:49,380 --> 00:19:51,140
And again, this is something that's,

399
00:19:51,140 --> 00:19:53,380
you know, a difficult engineering problem,

400
00:19:53,380 --> 00:19:56,420
but this is not a problem that we are unfamiliar with,

401
00:19:56,420 --> 00:20:00,220
because as societies, we've been doing this for millennia.

402
00:20:00,220 --> 00:20:01,740
This is called making laws.

403
00:20:02,740 --> 00:20:06,700
Sure, we'll just make some laws, problem solved.

404
00:20:08,180 --> 00:20:11,620
We've been solving problems like this for millennia.

405
00:20:13,660 --> 00:20:16,380
No, we have not.

406
00:20:16,380 --> 00:20:21,380
No problem ever before in our past is in any way

407
00:20:21,740 --> 00:20:24,940
like the challenge of artificial intelligence,

408
00:20:24,940 --> 00:20:28,300
but don't worry, I don't have to make the counterarguments.

409
00:20:28,300 --> 00:20:32,700
We have an all-star duo on the other side of this debate.

410
00:20:32,700 --> 00:20:35,300
Please say hello to our old friend

411
00:20:35,300 --> 00:20:38,540
and powerhouse AI safety researcher,

412
00:20:38,540 --> 00:20:41,460
MIT professor, Max Tegmark.

413
00:20:41,460 --> 00:20:43,540
Back in the Stone Age with a rock,

414
00:20:43,540 --> 00:20:45,820
maybe someone could kill five people.

415
00:20:45,820 --> 00:20:49,060
300 years ago with a bomb, maybe a hundred people.

416
00:20:50,700 --> 00:20:55,740
In 1945 with a couple of nukes, 250,000 people

417
00:20:55,740 --> 00:20:58,100
with bioweapons, even more,

418
00:20:58,100 --> 00:21:01,180
with nuclear winter, according to a recent science article,

419
00:21:02,060 --> 00:21:03,740
over five billion people.

420
00:21:03,740 --> 00:21:08,740
Now the blast radius has risen up to about 60% of humanity.

421
00:21:09,540 --> 00:21:12,140
And since, as we'll argue,

422
00:21:12,140 --> 00:21:14,020
superhuman intelligence is going to be

423
00:21:14,020 --> 00:21:16,540
way more powerful than any of this.

424
00:21:16,540 --> 00:21:20,820
Its blast radius can easily be 100% of humanity,

425
00:21:20,820 --> 00:21:24,300
giving it the potential power to really wipe us out.

426
00:21:24,300 --> 00:21:29,300
Again, this is different than anything else ever before.

427
00:21:29,420 --> 00:21:34,300
Nuclear bombs cannot make more nuclear bombs by themselves.

428
00:21:34,300 --> 00:21:37,380
Nuclear bombs cannot decide to detonate themselves

429
00:21:37,380 --> 00:21:41,100
at a location and time of their choosing.

430
00:21:42,060 --> 00:21:45,620
This is different than anything else ever before.

431
00:21:45,620 --> 00:21:48,660
Max, please tell the people.

432
00:21:48,660 --> 00:21:50,940
It can do all the intelligent things

433
00:21:50,940 --> 00:21:53,020
that we humans can do just better.

434
00:21:54,060 --> 00:21:57,900
For example, it can do goal-oriented behavior.

435
00:21:57,900 --> 00:22:02,380
It can persuade, manipulate, hire people,

436
00:22:02,380 --> 00:22:07,380
start companies, build robots, do scientific research.

437
00:22:08,540 --> 00:22:09,500
It could, for example,

438
00:22:09,500 --> 00:22:11,660
research how to make more powerful bioweapons

439
00:22:12,780 --> 00:22:16,020
or how to make even more intelligent systems

440
00:22:16,020 --> 00:22:19,660
so it could recursively self-improve itself.

441
00:22:19,660 --> 00:22:23,340
It also could do things that we humans cannot do at all.

442
00:22:23,340 --> 00:22:25,500
It could very easily make copies of itself.

443
00:22:25,900 --> 00:22:30,260
If you imagine for a moment a super human AI

444
00:22:30,260 --> 00:22:33,180
that can think, say a thousand times faster

445
00:22:33,180 --> 00:22:34,140
than a human researcher,

446
00:22:34,140 --> 00:22:38,740
so it can in nine hours do years worth of research,

447
00:22:38,740 --> 00:22:41,140
but now instead think of a million of those,

448
00:22:41,140 --> 00:22:44,340
a swarm of a million super intelligent AIs

449
00:22:44,340 --> 00:22:46,420
where as soon as one of them discovers something new,

450
00:22:46,420 --> 00:22:49,540
it can instantly share that new skill with all of them.

451
00:22:49,540 --> 00:22:52,780
That's the kind of power we're talking about here.

452
00:22:52,820 --> 00:22:57,620
And finally, super human AI will probably be

453
00:22:57,620 --> 00:22:59,860
a very alien kind of intelligence

454
00:22:59,860 --> 00:23:03,580
that lacks anything like human emotions or empathy.

455
00:23:05,380 --> 00:23:08,380
I could listen to Max Tagmark talk all day.

456
00:23:08,380 --> 00:23:10,180
I really hope to have him on this show

457
00:23:10,180 --> 00:23:11,620
sometime in the near future.

458
00:23:12,620 --> 00:23:15,140
In biotech, it's exactly the other way around.

459
00:23:15,140 --> 00:23:16,660
If you had come up with a new medicine

460
00:23:16,660 --> 00:23:18,980
and said, this cure is cancer, it's awesome,

461
00:23:18,980 --> 00:23:21,460
you can't just go sell it in the supermarket

462
00:23:21,460 --> 00:23:22,780
until someone proves it is there.

463
00:23:22,780 --> 00:23:25,900
It's your job to convince the Food and Drug Administration,

464
00:23:25,900 --> 00:23:28,140
the US or the Canadian authorities or whatever

465
00:23:28,140 --> 00:23:30,860
that this is safe and that the benefits,

466
00:23:30,860 --> 00:23:33,060
yes, the benefits outweigh the risks.

467
00:23:33,060 --> 00:23:35,660
It should be the responsibility of the companies

468
00:23:35,660 --> 00:23:37,540
that the first prove that this is safe

469
00:23:37,540 --> 00:23:38,820
before it gets deployed.

470
00:23:38,820 --> 00:23:41,740
We need to become like biotech.

471
00:23:41,740 --> 00:23:45,300
In this monk debate, Max is hardly alone.

472
00:23:45,300 --> 00:23:46,220
This is kind of fun,

473
00:23:46,220 --> 00:23:48,380
got a little bit of soap opera drama for you.

474
00:23:48,380 --> 00:23:50,620
So the highest prize in computer science

475
00:23:50,660 --> 00:23:52,100
is called the Touring Award,

476
00:23:52,100 --> 00:23:54,420
named after Alan Turing, the father of AI

477
00:23:54,420 --> 00:23:57,060
who we talked about a little bit last week.

478
00:23:57,060 --> 00:24:02,060
In 2018, the Touring Award went to three AI researchers

479
00:24:02,220 --> 00:24:04,580
for their breakthrough work on neural networks

480
00:24:04,580 --> 00:24:05,900
and artificial intelligence,

481
00:24:05,900 --> 00:24:08,860
which was one of the two major advancements in AI

482
00:24:08,860 --> 00:24:10,980
that got us to where we are today.

483
00:24:11,860 --> 00:24:15,580
The three Touring Award winners in 2018 were Jeffrey Hinton,

484
00:24:15,580 --> 00:24:17,580
who you've met in previous shows,

485
00:24:17,580 --> 00:24:21,500
the Quick Google, famously to voice his concerns

486
00:24:21,500 --> 00:24:24,780
about AI and human extinction.

487
00:24:24,780 --> 00:24:27,220
The other two winners were two of his former students

488
00:24:27,220 --> 00:24:29,300
from the University of Toronto.

489
00:24:31,060 --> 00:24:33,660
So on our stage at the monk debate,

490
00:24:33,660 --> 00:24:36,940
we have two of those former students,

491
00:24:36,940 --> 00:24:40,560
the two former students who won the Touring Award

492
00:24:40,560 --> 00:24:41,980
with Professor Hinton.

493
00:24:44,180 --> 00:24:47,380
One of them is Yan Lacoon.

494
00:24:48,580 --> 00:24:52,420
He now says his old mentor and professor, Jeffrey Hinton,

495
00:24:52,420 --> 00:24:56,400
is a doomer who's totally wrong about AI risk.

496
00:24:57,900 --> 00:25:00,820
The other of Professor Hinton's old students

497
00:25:00,820 --> 00:25:04,260
is Maxis' teammate for the debate,

498
00:25:04,260 --> 00:25:06,740
AI researcher, Yoshua Benjiro.

499
00:25:08,180 --> 00:25:11,060
Once we have machines that have a self-preservation goal,

500
00:25:11,060 --> 00:25:13,780
well, we are in trouble.

501
00:25:14,620 --> 00:25:18,580
You know, think about what happens when you want to survive.

502
00:25:18,580 --> 00:25:22,220
You don't want others to turn you off, right?

503
00:25:22,220 --> 00:25:24,740
And you need to be able to control your environment,

504
00:25:24,740 --> 00:25:26,260
which means control humans.

505
00:25:26,260 --> 00:25:30,820
So existential risk isn't just, well, we all disappear.

506
00:25:30,820 --> 00:25:33,620
It might be that we're all disempowered,

507
00:25:33,620 --> 00:25:36,980
that we are not anymore in the control of our destiny.

508
00:25:36,980 --> 00:25:39,820
And I don't think this is something we want.

509
00:25:39,820 --> 00:25:42,660
Not extinct, but not in charge.

510
00:25:42,660 --> 00:25:44,780
That is not something we want at all.

511
00:25:44,780 --> 00:25:46,980
But this debate, just like this show,

512
00:25:46,980 --> 00:25:50,580
we're gonna focus only on human extinction risk.

513
00:25:50,580 --> 00:25:55,580
Yan Lacoon thinks there's no extinction risk at all.

514
00:25:55,700 --> 00:26:00,460
Watch as he mocks this grave concern

515
00:26:00,460 --> 00:26:04,580
as mere science fiction and marvel

516
00:26:04,580 --> 00:26:09,580
as he backs up his bold claims with absolutely nothing.

517
00:26:10,500 --> 00:26:12,460
As fiction scenarios of, you know,

518
00:26:12,460 --> 00:26:15,300
the earth being wiped out, humanity being wiped out,

519
00:26:15,300 --> 00:26:17,460
this sounds like a James Bond movie, right?

520
00:26:17,460 --> 00:26:19,860
It's like the supervillain who goes in space

521
00:26:19,860 --> 00:26:22,620
and then kind of puts some deadly gas

522
00:26:22,620 --> 00:26:24,340
and eliminates all of humanity.

523
00:26:24,340 --> 00:26:25,620
It's a James Bond movie.

524
00:26:25,620 --> 00:26:26,860
And I can't disprove it.

525
00:26:26,860 --> 00:26:28,460
The same way, if I tell you,

526
00:26:28,460 --> 00:26:30,900
I used the Bertrand Russell idea,

527
00:26:30,900 --> 00:26:33,180
if I tell you there is a teapot flying

528
00:26:33,180 --> 00:26:37,460
between the orbits of Jupiter and Saturn,

529
00:26:37,460 --> 00:26:39,100
you're gonna tell me I'm crazy.

530
00:26:39,100 --> 00:26:41,260
But you can't disprove me, right?

531
00:26:41,260 --> 00:26:43,060
You can't disprove that assertion.

532
00:26:43,060 --> 00:26:46,100
It's gonna cost you a huge amount of resources to do this.

533
00:26:46,100 --> 00:26:49,420
So it's kind of the same thing with those doomsynarios.

534
00:26:49,420 --> 00:26:52,220
They're sci-fi, but I can't prove that they're wrong.

535
00:26:53,340 --> 00:26:55,100
But the risk is negligible.

536
00:26:55,100 --> 00:26:58,660
And the reason it's negligible of extension

537
00:26:58,660 --> 00:27:01,140
is because we build those things.

538
00:27:01,140 --> 00:27:01,980
We build them.

539
00:27:01,980 --> 00:27:03,580
We have agency.

540
00:27:03,580 --> 00:27:06,380
This is not superhuman intelligence.

541
00:27:06,380 --> 00:27:08,100
It's not something that's gonna just happen.

542
00:27:08,100 --> 00:27:09,420
So, of course, if it's not safe,

543
00:27:09,420 --> 00:27:11,340
we're not gonna build it, right?

544
00:27:16,420 --> 00:27:17,580
I mean, will you build a bomb

545
00:27:17,580 --> 00:27:19,700
that just blows up randomly?

546
00:27:19,700 --> 00:27:20,980
No, right?

547
00:27:22,220 --> 00:27:24,500
If it's not safe, we're not going to build it.

548
00:27:25,700 --> 00:27:28,820
What a total load of shit.

549
00:27:28,820 --> 00:27:31,340
How the fuck would you know if it's not safe

550
00:27:31,340 --> 00:27:34,100
until it's too late to stop it?

551
00:27:34,100 --> 00:27:36,500
That's the whole fundamental fucking problem

552
00:27:36,500 --> 00:27:37,780
with this situation.

553
00:27:37,780 --> 00:27:40,860
But, Jan, you know this much better than I do.

554
00:27:40,860 --> 00:27:43,460
So seriously, this is it, right?

555
00:27:43,460 --> 00:27:45,100
It would be reasonable after watching

556
00:27:45,100 --> 00:27:49,260
the first five episodes of this podcast to say,

557
00:27:49,260 --> 00:27:52,300
I'd like to hear more from the other side about this.

558
00:27:52,300 --> 00:27:54,620
So this is your chance to evaluate

559
00:27:54,620 --> 00:27:56,180
the strength of their logic.

560
00:27:57,420 --> 00:28:00,900
I went into this hoping to be persuaded by them,

561
00:28:00,900 --> 00:28:05,140
hoping to be persuaded that everything is fine.

562
00:28:05,140 --> 00:28:07,580
But now it's your turn.

563
00:28:07,580 --> 00:28:10,860
Let's do a little Jan and Max mode debate,

564
00:28:10,860 --> 00:28:14,780
soundbite ping pong, and then you decide for yourself.

565
00:28:14,780 --> 00:28:18,220
If you think everything is just fine.

566
00:28:18,220 --> 00:28:20,900
Remember what people were saying just before year 2000?

567
00:28:22,340 --> 00:28:24,020
Satellites were gonna fall out of the sky

568
00:28:24,020 --> 00:28:25,300
and crash into cities,

569
00:28:25,300 --> 00:28:28,100
and the phone system was gonna crash

570
00:28:28,100 --> 00:28:29,500
and civilization will end.

571
00:28:32,140 --> 00:28:33,700
It didn't happen.

572
00:28:33,700 --> 00:28:34,540
We're still here.

573
00:28:35,540 --> 00:28:36,780
So I think there's a little bit

574
00:28:36,780 --> 00:28:40,620
of the same kind of feeling of uncertainty.

575
00:28:40,620 --> 00:28:42,020
A lot of people have the feeling

576
00:28:42,020 --> 00:28:43,420
that bad things are gonna happen

577
00:28:43,420 --> 00:28:44,940
because they're not in control.

578
00:28:46,300 --> 00:28:48,860
They have the feeling that AI is just gonna happen,

579
00:28:48,860 --> 00:28:50,220
and there's nothing they can do about it,

580
00:28:50,220 --> 00:28:51,260
and that creates fear.

581
00:28:51,260 --> 00:28:53,700
And I can completely understand that.

582
00:28:53,700 --> 00:28:57,900
But some of us, in what could be construed

583
00:28:57,900 --> 00:29:01,900
as a driver's seat, there are plans

584
00:29:01,900 --> 00:29:02,980
to make those things safe.

585
00:29:03,020 --> 00:29:05,580
Stock traders will tell you the past performance

586
00:29:05,580 --> 00:29:08,300
is not an indicator of future performance,

587
00:29:08,300 --> 00:29:10,900
future results, yeah, future results.

588
00:29:10,900 --> 00:29:13,140
And it would be a huge mistake

589
00:29:13,140 --> 00:29:15,060
in an exponential technological growth

590
00:29:15,060 --> 00:29:18,020
to assume that just because something happened one way

591
00:29:18,020 --> 00:29:20,980
in the past is gonna continue being this way.

592
00:29:20,980 --> 00:29:24,580
AI is gonna be subservient to human.

593
00:29:24,580 --> 00:29:26,020
It's gonna be smarter than us,

594
00:29:26,020 --> 00:29:27,780
but it's not gonna reduce our agency.

595
00:29:27,780 --> 00:29:30,140
On the contrary, it's going to empower us.

596
00:29:31,140 --> 00:29:32,340
It's like having a staff

597
00:29:32,340 --> 00:29:34,420
of really smart people working for you.

598
00:29:34,420 --> 00:29:37,540
It's naive to think that just because you make something smart,

599
00:29:37,540 --> 00:29:39,740
it's only gonna suddenly care about humans.

600
00:29:39,740 --> 00:29:42,340
Ask some wooly mammoths

601
00:29:42,340 --> 00:29:45,780
if they feel so reassured that we're smarter than them.

602
00:29:45,780 --> 00:29:49,620
Then therefore, we would automatically adopt mammoth ethics

603
00:29:49,620 --> 00:29:51,420
and do things that were good for the mammoths

604
00:29:51,420 --> 00:29:52,420
that we didn't.

605
00:29:52,420 --> 00:29:55,380
That's why you probably haven't met any.

606
00:29:55,380 --> 00:29:57,660
I'll tell you what I think.

607
00:29:57,740 --> 00:29:59,900
I think Jan is full of shit

608
00:29:59,900 --> 00:30:03,300
and Max makes all the sense in the world.

609
00:30:04,620 --> 00:30:06,580
I really wish it was the other way around.

610
00:30:06,580 --> 00:30:10,980
I really, really do, but he just isn't at all.

611
00:30:12,940 --> 00:30:14,900
There are some great moments in the monk debate

612
00:30:14,900 --> 00:30:17,620
on AI safety where the teams really start

613
00:30:17,620 --> 00:30:19,380
to mix it up with each other

614
00:30:19,380 --> 00:30:21,820
and there are some really incredible exchanges

615
00:30:21,820 --> 00:30:22,820
I wanna show you.

616
00:30:22,820 --> 00:30:27,460
First, here are the 2018 Turing Award winners.

617
00:30:27,460 --> 00:30:30,820
Once united by science, now divided

618
00:30:30,820 --> 00:30:35,180
by the potentially existential results of their work.

619
00:30:35,180 --> 00:30:39,460
Mixing it up, Jan Lacoon goes for some classic

620
00:30:39,460 --> 00:30:42,260
historical comparison bullshit

621
00:30:42,260 --> 00:30:44,340
and Yoshua Bengio responds

622
00:30:44,340 --> 00:30:47,220
with what I think is force and logic.

623
00:30:47,220 --> 00:30:48,700
Socrates was against writing.

624
00:30:48,700 --> 00:30:53,100
He said, people are going to lose their memory, right?

625
00:30:54,060 --> 00:30:56,980
The Catholic Church was against printing press

626
00:30:57,020 --> 00:31:00,260
saying they would lose control of the dogma,

627
00:31:00,260 --> 00:31:03,740
which they did, they could do nothing about it.

628
00:31:03,740 --> 00:31:07,540
The Ottoman Empire banned the printing press

629
00:31:07,540 --> 00:31:09,060
and according to some historian,

630
00:31:09,060 --> 00:31:12,500
that's what accelerated their decline.

631
00:31:12,500 --> 00:31:16,940
And so every technology that makes people smarter

632
00:31:17,860 --> 00:31:20,820
or enables communication between people

633
00:31:20,820 --> 00:31:25,820
facilitates education, again, is interestingly good.

634
00:31:26,180 --> 00:31:28,380
And AI is kind of a new version of this.

635
00:31:28,380 --> 00:31:30,100
It's a new printing press.

636
00:31:30,100 --> 00:31:33,260
So long as it doesn't blow up in our face.

637
00:31:33,260 --> 00:31:35,700
And because that can match your hand

638
00:31:35,700 --> 00:31:37,300
with a printing press, right?

639
00:31:37,300 --> 00:31:41,220
Yes, if it's the problem is the scale, right?

640
00:31:41,220 --> 00:31:45,460
So long as we built technologies that could be harmful

641
00:31:45,460 --> 00:31:48,500
but on a small scale, the goods,

642
00:31:48,500 --> 00:31:51,900
the benefits overwhelm the dangers.

643
00:31:51,900 --> 00:31:55,060
Now we're talking about building technologies

644
00:31:55,100 --> 00:31:57,140
unlike any other technology.

645
00:31:57,140 --> 00:31:57,980
No.

646
00:31:57,980 --> 00:32:00,460
Because it's technology that can design its own technology.

647
00:32:00,460 --> 00:32:01,300
No.

648
00:32:01,300 --> 00:32:02,140
Yes.

649
00:32:02,140 --> 00:32:02,980
No.

650
00:32:02,980 --> 00:32:03,860
I don't think about super human AI.

651
00:32:03,860 --> 00:32:05,180
This is the subject.

652
00:32:05,180 --> 00:32:06,380
It's under control.

653
00:32:06,380 --> 00:32:07,220
It's under control.

654
00:32:07,220 --> 00:32:08,540
And we remain under control.

655
00:32:08,540 --> 00:32:11,140
It's very much like previous technologies.

656
00:32:11,140 --> 00:32:12,980
It's not quite relatively different.

657
00:32:12,980 --> 00:32:14,620
The experts have been studying this question,

658
00:32:14,620 --> 00:32:16,980
say it's going to be very hard to keep it under control.

659
00:32:16,980 --> 00:32:18,660
And that is why I'm here today.

660
00:32:20,780 --> 00:32:23,820
Melanie Mitchell is so special.

661
00:32:24,820 --> 00:32:26,740
She's pissed off we're even talking

662
00:32:26,740 --> 00:32:30,300
about existential risks from AI.

663
00:32:31,580 --> 00:32:33,860
Seriously, throughout the month debate,

664
00:32:33,860 --> 00:32:36,500
she sounds off on why it is bad for us

665
00:32:36,500 --> 00:32:38,740
to even talk about existential risk from AI

666
00:32:38,740 --> 00:32:40,580
when there are in her eyes

667
00:32:40,580 --> 00:32:45,180
so many more important, urgent threats

668
00:32:45,180 --> 00:32:48,420
that need our attention, such as disinformation

669
00:32:48,420 --> 00:32:52,460
and fake photos and videos.

670
00:32:52,660 --> 00:32:54,100
It takes our attention away

671
00:32:54,100 --> 00:32:56,660
from some of the real immediate risks,

672
00:32:56,660 --> 00:33:00,660
like disinformation and bias.

673
00:33:00,660 --> 00:33:03,460
So are you saying it's a 0% risk?

674
00:33:03,460 --> 00:33:05,340
0% existential risk?

675
00:33:05,340 --> 00:33:06,340
Is that your claim?

676
00:33:07,260 --> 00:33:12,260
Do I think there's a 0% risk of any scenario?

677
00:33:12,260 --> 00:33:13,500
No, of course not.

678
00:33:13,500 --> 00:33:14,820
What do you think it is then?

679
00:33:14,820 --> 00:33:15,660
1%?

680
00:33:15,660 --> 00:33:16,500
We're really debating.

681
00:33:16,500 --> 00:33:17,900
I can't put a number on it.

682
00:33:17,900 --> 00:33:19,540
I think it's quite low.

683
00:33:19,540 --> 00:33:20,620
And I think what we're debating

684
00:33:20,620 --> 00:33:24,900
is there a reasonable existential risk,

685
00:33:24,900 --> 00:33:28,380
a risk of end of civilization in the reasonable future?

686
00:33:28,380 --> 00:33:29,460
Otherwise, we wouldn't be up here to be.

687
00:33:29,460 --> 00:33:30,500
How high is the high?

688
00:33:30,500 --> 00:33:34,180
I'm not gonna like say 0.0000001.

689
00:33:34,180 --> 00:33:35,020
I mean, I can't say that.

690
00:33:35,020 --> 00:33:36,980
How high is too high for you then?

691
00:33:36,980 --> 00:33:37,820
How high is?

692
00:33:37,820 --> 00:33:41,300
How high a probability of a risk is too high for you?

693
00:33:41,300 --> 00:33:43,700
I don't think we can put a probability on it.

694
00:33:43,700 --> 00:33:44,700
We don't know.

695
00:33:44,700 --> 00:33:46,340
We don't know enough.

696
00:33:46,340 --> 00:33:48,020
No, I mean, it's acceptable.

697
00:33:48,020 --> 00:33:53,020
Okay, it's lower than yours being wiped out by meteor.

698
00:33:54,300 --> 00:33:56,380
And by the way, I can help with that problem.

699
00:33:56,380 --> 00:33:58,340
This is the essential question for anyone

700
00:33:58,340 --> 00:34:01,660
who wants to accelerate artificial intelligence.

701
00:34:01,660 --> 00:34:05,860
What percentage existential risk to humans

702
00:34:05,860 --> 00:34:07,940
is acceptable to you?

703
00:34:10,380 --> 00:34:12,940
In my opinion, when you are talking about

704
00:34:12,940 --> 00:34:16,900
a potential future with zero human beings,

705
00:34:16,900 --> 00:34:21,900
the only acceptable risk percentage is also zero.

706
00:34:25,020 --> 00:34:27,900
Here are the two former award-winning partners.

707
00:34:27,900 --> 00:34:30,220
Now on opposite sides of the question of

708
00:34:30,220 --> 00:34:32,820
don't you just need some good guys with good AI

709
00:34:32,820 --> 00:34:35,580
to beat some bad guys with bad AI?

710
00:34:35,580 --> 00:34:38,580
Decide for yourself if you think Yan Lacun

711
00:34:38,580 --> 00:34:43,580
is being appropriately optimistic or stunningly blind.

712
00:34:43,660 --> 00:34:47,580
Bad guys can use AI for bad things.

713
00:34:47,580 --> 00:34:49,500
There's many more good guys who can use the same

714
00:34:49,500 --> 00:34:52,220
more powerful AI to counteract it.

715
00:34:52,220 --> 00:34:53,220
So in the end,

716
00:34:53,220 --> 00:34:54,260
Those good guys are gonna win.

717
00:34:54,260 --> 00:34:56,620
Sometimes the attacker has the advantage.

718
00:34:56,620 --> 00:34:58,740
There is absolutely no reason to believe

719
00:34:58,740 --> 00:35:00,180
that's the case for AI.

720
00:35:00,180 --> 00:35:03,020
In fact, we are facing this situation right now.

721
00:35:03,020 --> 00:35:03,860
How do you know?

722
00:35:03,860 --> 00:35:05,820
There's never anything that is completely perfect.

723
00:35:05,820 --> 00:35:06,660
Well, that's the issue.

724
00:35:06,660 --> 00:35:08,780
That's what we need to do more than what we're doing now.

725
00:35:08,780 --> 00:35:12,580
Again, it's the good guys' AI, which is superior,

726
00:35:12,580 --> 00:35:15,100
to the bad guys' AI.

727
00:35:15,100 --> 00:35:18,460
That's like saying that the way to stop a bad guy

728
00:35:18,460 --> 00:35:21,860
with a bio-weapon is to have a good guy with a bio-weapon.

729
00:35:21,860 --> 00:35:23,340
That's not what you do.

730
00:35:23,340 --> 00:35:26,700
The way you stop a bio-weapon attack is with vaccines

731
00:35:26,700 --> 00:35:28,420
and banning bio-weapons

732
00:35:28,420 --> 00:35:31,740
and having various forms of regulation and control.

733
00:35:31,740 --> 00:35:36,740
Next, Max asks Melanie what I think is a reasonable question.

734
00:35:36,900 --> 00:35:40,540
Why the product maker isn't responsible

735
00:35:40,580 --> 00:35:42,660
for proving their product is safe

736
00:35:42,660 --> 00:35:45,380
before they release it to the general public?

737
00:35:45,380 --> 00:35:48,180
But why is it our role to explain to you why it's dangerous?

738
00:35:48,180 --> 00:35:49,500
You still haven't answered my question.

739
00:35:49,500 --> 00:35:50,980
I've never said it wasn't dangerous.

740
00:35:50,980 --> 00:35:52,220
I've asked you twice.

741
00:35:52,220 --> 00:35:54,700
What is your plan for avoiding misuse?

742
00:35:54,700 --> 00:35:55,700
You haven't told me.

743
00:35:55,700 --> 00:35:56,540
You haven't asked you twice.

744
00:35:56,540 --> 00:35:58,980
What's your plan for solving the alignment problem?

745
00:35:58,980 --> 00:36:00,060
You haven't told me.

746
00:36:00,060 --> 00:36:00,900
I've asked you twice.

747
00:36:00,900 --> 00:36:03,980
Can we just finish to tell me what your plan is

748
00:36:03,980 --> 00:36:06,100
for avoiding the scenario where we get out-competed

749
00:36:06,100 --> 00:36:06,940
in disempowerment?

750
00:36:06,940 --> 00:36:07,940
You've said nothing.

751
00:36:08,780 --> 00:36:11,820
Unfortunately, I don't get paid enough

752
00:36:11,820 --> 00:36:13,900
to solve all these problems about AI policy.

753
00:36:13,900 --> 00:36:15,660
But if you either fill in medicine to have it,

754
00:36:15,660 --> 00:36:16,980
you'd have to have a plan.

755
00:36:16,980 --> 00:36:18,980
I don't have to explain why.

756
00:36:18,980 --> 00:36:21,500
I think the AI community is developing plans

757
00:36:21,500 --> 00:36:23,140
to mitigate risks.

758
00:36:24,100 --> 00:36:29,100
I think the community is developing plans

759
00:36:29,100 --> 00:36:33,220
to mitigate risks, she says.

760
00:36:33,220 --> 00:36:34,860
Is developing plans?

761
00:36:35,780 --> 00:36:37,460
This is like saying we'll figure out

762
00:36:37,460 --> 00:36:40,060
how to make landing gear mid-flight.

763
00:36:40,060 --> 00:36:43,980
We are already currently cruising as a world

764
00:36:43,980 --> 00:36:47,260
at 30,000 feet in a plane with no landing gear.

765
00:36:47,260 --> 00:36:50,020
The pilots already said, fuck it, let's just take off.

766
00:36:50,020 --> 00:36:51,860
We'll figure out landing gear mid-flight.

767
00:36:53,300 --> 00:36:56,580
We're hurtling through the air, no landing gear,

768
00:36:56,580 --> 00:37:00,380
and no knowledge of when we actually need to land,

769
00:37:00,380 --> 00:37:02,620
no knowledge of the timing.

770
00:37:02,660 --> 00:37:05,060
But the pilots are saying, please stay calm,

771
00:37:05,060 --> 00:37:08,580
do enjoy your snack and beverage service.

772
00:37:08,580 --> 00:37:09,620
It's nuts.

773
00:37:10,900 --> 00:37:13,860
Let's get back to the debate where Yoshua channels

774
00:37:13,860 --> 00:37:17,780
my inner spirit and tells Melanie, you're wrong.

775
00:37:18,780 --> 00:37:22,780
They might want to preserve their existence.

776
00:37:22,780 --> 00:37:25,340
They don't want to do anything, they're not alive.

777
00:37:25,340 --> 00:37:26,700
No, that's very easy to do.

778
00:37:26,700 --> 00:37:28,740
No, you're wrong, you're wrong.

779
00:37:28,740 --> 00:37:31,260
The systems that, for example,

780
00:37:32,100 --> 00:37:34,540
ChadGPT is essentially like an oracle.

781
00:37:34,540 --> 00:37:35,780
It doesn't really have a want,

782
00:37:35,780 --> 00:37:37,100
although literally it wants to please us

783
00:37:37,100 --> 00:37:38,700
because of the reinforcement learning.

784
00:37:38,700 --> 00:37:40,700
Yes, it's been trying to please us.

785
00:37:40,700 --> 00:37:44,140
Then it's actually easy to put a wrapper around them,

786
00:37:44,140 --> 00:37:47,260
to turn them into agents that have goals.

787
00:37:47,260 --> 00:37:50,580
It's actually easy to do that humans gave them.

788
00:37:50,580 --> 00:37:51,700
Yes, that's their own goal.

789
00:37:51,700 --> 00:37:53,940
Yes, and in order to achieve those goals,

790
00:37:53,940 --> 00:37:57,060
they're gonna have sub-goals and those sub-goals,

791
00:37:57,060 --> 00:38:00,620
for example, may include things like deception.

792
00:38:00,620 --> 00:38:04,020
Okay, finally, in one last-ditch effort,

793
00:38:04,020 --> 00:38:05,780
Melanie Mitchell tries to drop.

794
00:38:05,780 --> 00:38:09,500
Well, the people I talk to all think just like I do,

795
00:38:09,500 --> 00:38:12,220
and they all think everything is fine too.

796
00:38:12,220 --> 00:38:17,220
You say the people you talk to in AI have this belief.

797
00:38:17,420 --> 00:38:20,020
Well, the people I talk to in AI don't have that belief.

798
00:38:20,020 --> 00:38:23,620
I think the field is quite split, I would say.

799
00:38:23,620 --> 00:38:25,420
And I think there's quite a...

800
00:38:25,420 --> 00:38:27,980
Did it coin at 50% that we all get a dime?

801
00:38:28,980 --> 00:38:29,980
Well...

802
00:38:29,980 --> 00:38:31,700
Are any of the people who sign this letter

803
00:38:31,700 --> 00:38:34,500
saying that this is an existential risk?

804
00:38:34,500 --> 00:38:37,540
There's many people who have signed letters

805
00:38:37,540 --> 00:38:39,140
saying they think it's an existential risk.

806
00:38:39,140 --> 00:38:41,140
They don't say over what time scale.

807
00:38:41,140 --> 00:38:42,940
Yeah, but this was Jeff Hinton.

808
00:38:42,940 --> 00:38:46,660
Yeah, Jeff Hinton is a godfather.

809
00:38:46,660 --> 00:38:48,020
But he doesn't know everything.

810
00:38:48,020 --> 00:38:48,860
You don't talk to them.

811
00:38:48,860 --> 00:38:52,460
Yeah, I think that he's a smart guy,

812
00:38:52,460 --> 00:38:57,420
but I think that a lot of people have way over-hyped

813
00:38:57,420 --> 00:38:58,580
the risk of these things,

814
00:38:58,580 --> 00:39:01,140
and that's really convinced a lot of the general public

815
00:39:01,140 --> 00:39:03,740
that this is what we should be focusing on,

816
00:39:03,740 --> 00:39:06,660
not the more immediate harms of AI.

817
00:39:06,660 --> 00:39:07,500
Over-hyped.

818
00:39:10,300 --> 00:39:11,340
Science fiction.

819
00:39:13,460 --> 00:39:14,900
Just another Y2K.

820
00:39:16,980 --> 00:39:19,460
Just like the invention of the typewriter.

821
00:39:21,740 --> 00:39:23,620
Please, tell me in the comments,

822
00:39:23,620 --> 00:39:26,420
if you think these arguments made by Melanie and Jan

823
00:39:26,420 --> 00:39:28,500
are not total bullshit.

824
00:39:28,500 --> 00:39:30,420
Maybe I'm missing something,

825
00:39:30,420 --> 00:39:33,100
but I find them not only to be incredibly weak,

826
00:39:33,100 --> 00:39:36,060
I find them to be deceptive and manipulative.

827
00:39:37,260 --> 00:39:39,180
The audience at the end of the month debate

828
00:39:39,180 --> 00:39:41,780
was asked to vote for who they thought won.

829
00:39:41,780 --> 00:39:43,620
By a score of 67 to 33,

830
00:39:43,620 --> 00:39:47,300
they decided that Max and Yoshua were more convincing

831
00:39:47,300 --> 00:39:49,900
than Melanie and Jan.

832
00:39:49,900 --> 00:39:52,620
So I hope you got a lot out of that debate.

833
00:39:52,620 --> 00:39:55,300
I would encourage you to go to the Monk Debates website.

834
00:39:55,300 --> 00:39:56,900
They have a variety of debates

835
00:39:56,900 --> 00:39:58,740
on a lot of really interesting subjects,

836
00:39:58,740 --> 00:40:01,140
and they're all exceptionally well done,

837
00:40:01,140 --> 00:40:03,740
just like that debate was.

838
00:40:03,740 --> 00:40:05,300
So for six weeks now,

839
00:40:05,300 --> 00:40:07,260
I've been putting out a podcast per week,

840
00:40:07,260 --> 00:40:08,940
and as I said, I'm really encouraged

841
00:40:08,940 --> 00:40:11,460
by the response I've been getting.

842
00:40:11,460 --> 00:40:16,380
The question is, who will win this battle for humanity?

843
00:40:16,380 --> 00:40:19,300
The accelerationists, like Jan Lacoon,

844
00:40:19,300 --> 00:40:21,820
Melanie Mitchell and Sam Malt,

845
00:40:21,820 --> 00:40:24,460
or the decelerationists like Max Tegmark.

846
00:40:25,620 --> 00:40:28,460
Currently, if it was an NFL football game,

847
00:40:28,460 --> 00:40:29,980
I think the score would be something

848
00:40:29,980 --> 00:40:33,340
like 45 to three accelerationists.

849
00:40:34,220 --> 00:40:38,580
Acceleration is dominating in funding progress and talent.

850
00:40:39,820 --> 00:40:42,100
The game is in the second half,

851
00:40:42,100 --> 00:40:44,940
but we don't know when the whistle is gonna blow

852
00:40:44,940 --> 00:40:47,200
to say that the game is over.

853
00:40:48,060 --> 00:40:49,380
The odds could not be worse,

854
00:40:49,380 --> 00:40:51,220
the situation could not be more dire.

855
00:40:51,220 --> 00:40:55,380
You, me, and everyone else alive right now

856
00:40:55,380 --> 00:40:58,260
can either be blindly let off a cliff,

857
00:40:58,260 --> 00:41:01,900
or we can work to have a voice in this,

858
00:41:01,900 --> 00:41:04,900
work to learn what is happening and what is to come,

859
00:41:04,900 --> 00:41:08,100
and do anything and everything we can

860
00:41:08,100 --> 00:41:10,100
to give our children a future.

861
00:41:11,180 --> 00:41:15,060
So next week, we're gonna do something really different.

862
00:41:15,060 --> 00:41:16,820
Our show aims to move this debate

863
00:41:16,860 --> 00:41:19,260
from Tech, YouTube, and X

864
00:41:19,260 --> 00:41:22,620
into the family dinner table conversation.

865
00:41:22,620 --> 00:41:25,060
I think the voice of parents

866
00:41:25,060 --> 00:41:27,820
is sorely missing from this debate.

867
00:41:27,820 --> 00:41:29,460
As a parent myself, I can tell you,

868
00:41:29,460 --> 00:41:31,740
I've often said that if I was standing on a street corner

869
00:41:31,740 --> 00:41:33,580
and my son or daughter was next to me

870
00:41:33,580 --> 00:41:34,860
and a bus was coming at us

871
00:41:34,860 --> 00:41:36,620
and they were falling into the path of the bus

872
00:41:36,620 --> 00:41:37,820
and I could pull them back

873
00:41:37,820 --> 00:41:40,260
and I would have to fall and be killed by the bus

874
00:41:40,260 --> 00:41:42,420
instead of them, that I would see them

875
00:41:42,420 --> 00:41:44,700
falling towards the street corner to safety.

876
00:41:44,700 --> 00:41:46,580
I would fall myself into the street

877
00:41:46,580 --> 00:41:47,900
knowing the bus was about to hit me

878
00:41:47,900 --> 00:41:50,580
and I would do so with a big smile on my face,

879
00:41:50,580 --> 00:41:53,580
knowing I'd saved the life of my child, right?

880
00:41:53,580 --> 00:41:55,380
That is how parents are.

881
00:41:55,380 --> 00:41:58,860
Parents will do anything to protect their kids.

882
00:41:58,860 --> 00:42:02,260
Well, my fellow parents,

883
00:42:02,260 --> 00:42:04,420
it hurts me deeply to tell you

884
00:42:04,420 --> 00:42:08,420
that your children are under a threat more grave

885
00:42:08,420 --> 00:42:13,420
than any threat faced by any children ever before.

886
00:42:13,900 --> 00:42:14,820
Mine are too.

887
00:42:16,780 --> 00:42:20,540
Parents, there is an intruder in your home.

888
00:42:21,740 --> 00:42:24,100
There is an intruder in your school.

889
00:42:25,260 --> 00:42:27,620
It came from Silicon Valley.

890
00:42:27,620 --> 00:42:32,620
It is an alien and it doesn't care about anything

891
00:42:32,620 --> 00:42:33,780
that you care about.

892
00:42:39,620 --> 00:42:41,700
So next week, I'm gonna talk to three moms

893
00:42:41,700 --> 00:42:44,340
who are totally new to this AI safety debate,

894
00:42:44,340 --> 00:42:45,740
just three regular moms

895
00:42:45,740 --> 00:42:48,380
who through this podcast have become aware

896
00:42:48,380 --> 00:42:52,500
of this most grave threat to their families.

897
00:42:52,500 --> 00:42:55,540
I wanna know how it feels to know

898
00:42:55,540 --> 00:42:58,520
that this biggest threat hasn't even been on their radar.

899
00:42:59,620 --> 00:43:02,180
What they think about a few thousand people

900
00:43:02,180 --> 00:43:05,700
in Silicon Valley who are more than comfortable

901
00:43:05,700 --> 00:43:09,300
risking the mass murder of every child on earth

902
00:43:11,960 --> 00:43:14,220
and how these moms plan to go forward

903
00:43:14,220 --> 00:43:16,000
knowing what they now know.

904
00:43:17,180 --> 00:43:19,700
Should be an interesting show for sure.

905
00:43:19,700 --> 00:43:21,660
Thank you so much for watching.

906
00:43:21,660 --> 00:43:23,820
For Humanity, I'm John Sherman.

907
00:43:23,820 --> 00:43:25,420
I'll see you back here next week.

908
00:43:25,420 --> 00:43:31,860
Repeat Twitter, Instagram, Facebook, YouTube.

