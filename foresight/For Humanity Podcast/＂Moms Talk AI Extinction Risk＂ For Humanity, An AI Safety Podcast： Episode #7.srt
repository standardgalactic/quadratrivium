1
00:00:00,000 --> 00:00:03,000
I don't hear as much about, like, end of the world as I do.

2
00:00:03,000 --> 00:00:05,380
What is the application in the school system?

3
00:00:05,380 --> 00:00:07,300
And then how is my child learning?

4
00:00:07,300 --> 00:00:11,020
When you first heard the notion of existential risk,

5
00:00:11,020 --> 00:00:14,980
that, like, within our lifetimes,

6
00:00:14,980 --> 00:00:19,860
there could be a world where there is no life on Earth.

7
00:00:19,860 --> 00:00:24,420
We are expected lifetimes, I guess.

8
00:00:24,420 --> 00:00:25,380
How did that hit you?

9
00:00:25,380 --> 00:00:27,140
What was your first thought?

10
00:00:30,880 --> 00:00:36,040
I think I believed.

11
00:00:36,040 --> 00:00:38,240
I mean, I'd sound so crazy.

12
00:00:38,240 --> 00:00:41,320
I feel like I've found a little crazy to say, like,

13
00:00:41,320 --> 00:00:42,800
I needed to do my research.

14
00:00:42,800 --> 00:00:46,320
Like, I needed to know who was saying there.

15
00:00:46,320 --> 00:00:48,640
Were those people credible?

16
00:00:48,640 --> 00:00:52,400
And what's been really interesting, the camps, right,

17
00:00:52,400 --> 00:00:57,840
being the dancers, versus the juniors, or there need camp.

18
00:00:57,840 --> 00:01:01,720
And each camp has their expert that

19
00:01:01,720 --> 00:01:03,720
went to this Ivy League institution

20
00:01:03,720 --> 00:01:07,880
and worked at this, you know, this big tech firm,

21
00:01:07,880 --> 00:01:09,520
or used to work at the big tech firm.

22
00:01:09,520 --> 00:01:13,960
Like, both sides of the camp have their, you know,

23
00:01:13,960 --> 00:01:15,560
qualified experts.

24
00:01:15,560 --> 00:01:18,480
Like, human extinction, like, you know,

25
00:01:18,480 --> 00:01:21,960
society has been trying to, like, eliminate me my entire life.

26
00:01:21,960 --> 00:01:24,280
Nothing will be able to, like, overpower us

27
00:01:24,280 --> 00:01:27,200
because of what we've been able to endure

28
00:01:27,200 --> 00:01:30,160
and what we've been able to survive.

29
00:01:30,160 --> 00:01:33,560
So for me, I'm like, OK, come at me, AI.

30
00:01:33,560 --> 00:01:37,920
Like, you know, you know, it's just like, like, come at me.

31
00:01:37,920 --> 00:01:41,120
Throwing down the, she's throwing down the bell.

32
00:01:41,120 --> 00:01:43,240
Like, like, like, come at me, tech bros.

33
00:01:43,240 --> 00:01:44,120
Like, I got you.

34
00:01:44,120 --> 00:01:49,160
Like, there's no way that you can, like, completely, like,

35
00:01:49,160 --> 00:01:51,680
you know, erase me because, like, I've been through so much.

36
00:01:51,680 --> 00:01:53,200
I think it means a little brand.

37
00:01:53,200 --> 00:01:56,080
I think the term AI, so to speak, is boring.

38
00:01:56,080 --> 00:01:58,280
I don't think anybody gets excited about it.

39
00:01:58,280 --> 00:02:00,120
I don't think they know what it means.

40
00:02:00,120 --> 00:02:01,720
It sounds like a fake news moment.

41
00:02:01,720 --> 00:02:02,400
Like, what?

42
00:02:02,400 --> 00:02:04,720
Like, AI, fake news, snoozer.

43
00:02:04,720 --> 00:02:09,160
I think the scary part for me was that the people that created it

44
00:02:09,160 --> 00:02:12,320
were warning us and said, we don't know how to control it

45
00:02:12,320 --> 00:02:13,000
at this point.

46
00:02:13,000 --> 00:02:16,400
And so when you have a creator that doesn't understand it,

47
00:02:16,400 --> 00:02:19,240
it doesn't understand what it's doing, that's very scary.

48
00:02:19,240 --> 00:02:23,000
I was stunned to know that the people who make AI

49
00:02:23,000 --> 00:02:29,320
have no idea why it does, what it does, or how to control it.

50
00:02:29,320 --> 00:02:32,400
What did you think when you all first heard that?

51
00:02:44,600 --> 00:02:49,440
Welcome to For Humanity, an AI safety podcast, episode 7,

52
00:02:49,440 --> 00:02:52,160
Mom's Talk AI Extinction Risks.

53
00:02:52,200 --> 00:02:53,360
I'm John Sherman, your host.

54
00:02:53,360 --> 00:02:55,400
Thank you so much for joining me.

55
00:02:55,400 --> 00:02:58,200
This is the AI Safety Podcast for the general public,

56
00:02:58,200 --> 00:03:00,000
no tech background required.

57
00:03:00,000 --> 00:03:02,280
As you know, this show is exclusively

58
00:03:02,280 --> 00:03:05,800
about the threat of extinction from artificial intelligence.

59
00:03:05,800 --> 00:03:09,640
Please like, subscribe, and tell a friend about this show.

60
00:03:09,640 --> 00:03:11,440
And if you are on X and want to hear more

61
00:03:11,440 --> 00:03:13,320
about these issues on a daily basis,

62
00:03:13,320 --> 00:03:16,080
please follow at For Humanity Pod.

63
00:03:16,080 --> 00:03:17,960
OK.

64
00:03:17,960 --> 00:03:20,040
So the whole point of this podcast

65
00:03:20,040 --> 00:03:22,680
has been to move this debate from the Silicon Valley

66
00:03:22,680 --> 00:03:25,880
boardroom to the family dinner table.

67
00:03:25,880 --> 00:03:29,480
I also, as a parent, have been wondering

68
00:03:29,480 --> 00:03:34,640
how anyone who works in AI could do this to their children.

69
00:03:34,640 --> 00:03:39,200
And that if maybe awakening the parental maternal instincts

70
00:03:39,200 --> 00:03:42,000
that are so foreign to this AI safety debate

71
00:03:42,000 --> 00:03:46,720
could be possible and maybe even powerful.

72
00:03:46,720 --> 00:03:48,440
So I thought, what a better way to do that

73
00:03:48,560 --> 00:03:50,600
than to just do that.

74
00:03:50,600 --> 00:03:52,920
So I asked three moms to come on the show

75
00:03:52,920 --> 00:03:57,560
and discuss AI as an immediate threat to their children.

76
00:03:57,560 --> 00:03:59,960
I want to give a big thanks to Stephanie, Jen,

77
00:03:59,960 --> 00:04:02,040
and Crystal, our three moms.

78
00:04:02,040 --> 00:04:04,440
I asked them all to watch the first episode

79
00:04:04,440 --> 00:04:06,720
of this podcast before we met.

80
00:04:06,720 --> 00:04:09,240
I think this is a fascinating conversation,

81
00:04:09,240 --> 00:04:13,240
and I am thrilled to share it with you.

82
00:04:13,240 --> 00:04:15,040
All right.

83
00:04:15,040 --> 00:04:16,840
Moms, I think we made it.

84
00:04:16,840 --> 00:04:20,120
We made it for the Gauntlet Riverside Zooms

85
00:04:20,120 --> 00:04:22,800
and getting on a video conference.

86
00:04:22,800 --> 00:04:25,160
So nice to see you all.

87
00:04:25,160 --> 00:04:27,520
Nice to see you.

88
00:04:27,520 --> 00:04:28,480
Awesome.

89
00:04:28,480 --> 00:04:33,080
So let's just take it from the top here a little bit, I guess.

90
00:04:33,080 --> 00:04:35,520
So it's an interesting thing in your life

91
00:04:35,520 --> 00:04:37,720
when you start to talk about these AI safety issues.

92
00:04:37,720 --> 00:04:40,160
They resonate with certain people and not with other people.

93
00:04:40,160 --> 00:04:43,800
And Stephanie has been a friend of mine for many years.

94
00:04:43,800 --> 00:04:47,200
And it's an issue that resonated with her.

95
00:04:47,200 --> 00:04:49,400
And we got to talking about it.

96
00:04:49,400 --> 00:04:50,920
And one of the things we're talking about

97
00:04:50,920 --> 00:04:54,720
is where is the parental attitude in this?

98
00:04:54,720 --> 00:04:59,040
Where is the maternal attitude in this?

99
00:04:59,040 --> 00:05:03,520
And so I've been trying to take this debate as much as I can

100
00:05:03,520 --> 00:05:05,640
from tech people to regular people.

101
00:05:05,640 --> 00:05:07,320
So why not just do it and take it right

102
00:05:07,320 --> 00:05:11,840
to the most regular people of all, the moms?

103
00:05:11,840 --> 00:05:18,560
I will say as a father, as a son, husband, all these things

104
00:05:18,560 --> 00:05:20,520
that moms, the world go around.

105
00:05:20,520 --> 00:05:24,600
And so I salute to all the moms everywhere.

106
00:05:24,600 --> 00:05:26,560
I thought we could just start off right at the top.

107
00:05:26,560 --> 00:05:31,160
Maybe let's just go around the horn, say who you are.

108
00:05:31,160 --> 00:05:32,600
Tell me a little bit about your kids

109
00:05:32,600 --> 00:05:34,560
and maybe what you do outside of kids.

110
00:05:34,560 --> 00:05:36,120
And we'll just quick shoot around the horn

111
00:05:36,120 --> 00:05:38,200
and then come back up to the top.

112
00:05:38,200 --> 00:05:39,960
Stephanie, why don't you start us off?

113
00:05:40,000 --> 00:05:41,800
So I'm Stephanie Rano.

114
00:05:41,800 --> 00:05:46,320
I am a mom of 30 kiddos.

115
00:05:46,320 --> 00:05:48,440
My oldest is 13.

116
00:05:48,440 --> 00:05:51,480
My middle is 10 and my youngest is 8.

117
00:05:51,480 --> 00:05:54,600
So kind of early, but definitely that 13-year-old has

118
00:05:54,600 --> 00:05:55,640
opinions.

119
00:05:55,640 --> 00:05:58,400
Two of my kids are neurodivergent.

120
00:05:58,400 --> 00:06:00,720
And that is much a part of our lives.

121
00:06:00,720 --> 00:06:02,680
I'm really open about talking about it

122
00:06:02,680 --> 00:06:07,840
with anybody who will listen just around their gifts

123
00:06:07,840 --> 00:06:10,520
and the place if they need support.

124
00:06:10,520 --> 00:06:14,720
And what I do and I'm not being a mom is I do recruiting.

125
00:06:14,720 --> 00:06:17,760
So I do sales for a recruiting company.

126
00:06:17,760 --> 00:06:20,160
I've been doing stocking and recruiting for almost 20

127
00:06:20,160 --> 00:06:21,120
years.

128
00:06:21,120 --> 00:06:24,000
And I'm really excited for this conversation.

129
00:06:24,000 --> 00:06:26,760
And excited to share the podcast, hopefully with my kids.

130
00:06:26,760 --> 00:06:29,520
So watch your language, Jeff.

131
00:06:29,520 --> 00:06:30,120
Thank you.

132
00:06:30,120 --> 00:06:31,480
Thank you, Stephanie.

133
00:06:31,480 --> 00:06:33,680
I did make a promise to our group here

134
00:06:33,680 --> 00:06:36,480
that I would clean up my act for the show in an effort.

135
00:06:36,520 --> 00:06:38,200
Maybe we can show it to the kids afterwards.

136
00:06:38,200 --> 00:06:39,360
So that would be great.

137
00:06:39,360 --> 00:06:41,080
Jen, please introduce yourself.

138
00:06:41,080 --> 00:06:41,640
Yes.

139
00:06:41,640 --> 00:06:43,120
Hello, everyone.

140
00:06:43,120 --> 00:06:46,480
My name is Jen White-Johnson.

141
00:06:46,480 --> 00:06:50,280
I do identify as someone who is neurodivergent.

142
00:06:50,280 --> 00:06:55,600
I do have ADHD and also anxiety and also an autoimmune

143
00:06:55,600 --> 00:06:56,080
disorder.

144
00:06:56,080 --> 00:07:00,360
So I live along the intersections of being black

145
00:07:00,360 --> 00:07:02,560
and having a disability.

146
00:07:02,560 --> 00:07:04,880
And my son is also autistic.

147
00:07:04,920 --> 00:07:07,520
And my husband is also neurodivergent.

148
00:07:07,520 --> 00:07:13,320
So we are very neurodivergent, affirming family.

149
00:07:13,320 --> 00:07:16,360
And it intersects with my teaching.

150
00:07:16,360 --> 00:07:21,440
And I also teach graphic design within these past few years

151
00:07:21,440 --> 00:07:23,240
since my son was diagnosed.

152
00:07:23,240 --> 00:07:28,360
I do a lot of disability advocacy work specifically

153
00:07:28,360 --> 00:07:34,200
under the influence of art and where

154
00:07:34,200 --> 00:07:37,560
that can be used as a tool to really uplift

155
00:07:37,560 --> 00:07:40,720
the conversation of disability justice.

156
00:07:40,720 --> 00:07:42,760
So thank you for having me.

157
00:07:42,760 --> 00:07:44,280
Awesome, awesome thrill to have you.

158
00:07:44,280 --> 00:07:46,520
And Crystal, please introduce yourself.

159
00:07:46,520 --> 00:07:47,480
Hi, everyone.

160
00:07:47,480 --> 00:07:49,960
My name is Crystal Putman Garcia.

161
00:07:49,960 --> 00:07:53,320
So I am also a mom of three, like Stephanie.

162
00:07:53,320 --> 00:07:55,680
I have twins that are nine.

163
00:07:55,680 --> 00:07:59,280
And then I have an almost seven-year-old, so six right now.

164
00:07:59,280 --> 00:08:01,920
So they're a little bit on the younger side.

165
00:08:01,960 --> 00:08:04,280
But I do have a neurodivergent son who

166
00:08:04,280 --> 00:08:07,480
is obsessed with technology.

167
00:08:07,480 --> 00:08:12,360
He could hack into our Amazon accounts when he was five.

168
00:08:12,360 --> 00:08:13,760
I think it's one of his gifts.

169
00:08:13,760 --> 00:08:18,360
And so definitely try to balance when is it OK

170
00:08:18,360 --> 00:08:21,320
to give your kids technology because we live in a world

171
00:08:21,320 --> 00:08:22,200
with technology.

172
00:08:22,200 --> 00:08:24,840
And so we want kids that are comfortable with it.

173
00:08:24,840 --> 00:08:26,960
But where do you draw the line?

174
00:08:26,960 --> 00:08:28,200
And I'm not just a mom.

175
00:08:28,200 --> 00:08:30,320
I also work for a tech company.

176
00:08:30,360 --> 00:08:35,920
So I work for a policy and global intelligence company

177
00:08:35,920 --> 00:08:37,280
in half or several years.

178
00:08:37,280 --> 00:08:40,840
And I also, in the past, have worked

179
00:08:40,840 --> 00:08:44,520
to make sure that there's an intersection between consumers

180
00:08:44,520 --> 00:08:45,960
and internet privacy.

181
00:08:45,960 --> 00:08:48,840
And so this is an area that's particularly of interest to me.

182
00:08:48,840 --> 00:08:50,960
And one of the reasons why I thought this group would be

183
00:08:50,960 --> 00:08:57,360
really great to get together is you all put so much work

184
00:08:57,360 --> 00:08:59,200
into everything you're doing.

185
00:08:59,200 --> 00:09:02,160
You are badasses professionally.

186
00:09:02,160 --> 00:09:07,760
And you're also incredibly devoted to your kids, as am I.

187
00:09:07,760 --> 00:09:09,680
I'll just introduce myself as well for a second.

188
00:09:09,680 --> 00:09:10,640
I'm John.

189
00:09:10,640 --> 00:09:13,880
I do video production.

190
00:09:13,880 --> 00:09:17,000
I do a podcast that you may be watching.

191
00:09:17,000 --> 00:09:22,520
And I have two kids, Boy Girl Twins, who are 18 years old.

192
00:09:22,520 --> 00:09:23,600
I can't believe it.

193
00:09:23,600 --> 00:09:28,760
They are seniors in high school and getting ready to leave us.

194
00:09:28,800 --> 00:09:31,000
And it's a whole lot to take.

195
00:09:31,000 --> 00:09:36,000
But I certainly, as we think about these AI existential

196
00:09:36,000 --> 00:09:39,440
issues, I certainly make kids are always at the top of my mind.

197
00:09:39,440 --> 00:09:42,400
So why don't we go around the world one more time?

198
00:09:42,400 --> 00:09:47,040
And let's talk about our experience thus far with AI.

199
00:09:47,040 --> 00:09:49,280
We're going to talk a lot about existential risk

200
00:09:49,280 --> 00:09:50,600
and in the podcast.

201
00:09:50,600 --> 00:09:54,960
But before you became aware of this existential risk

202
00:09:54,960 --> 00:09:57,320
conversation, what were your impressions?

203
00:09:57,320 --> 00:09:59,560
And just sort of briefly, what were your dealings with it?

204
00:09:59,560 --> 00:10:02,280
I know Jen and I don't know, Stephanie, either.

205
00:10:02,280 --> 00:10:05,800
You all have been doing some really interesting things

206
00:10:05,800 --> 00:10:09,680
with neurodivergence and kids and AI, which

207
00:10:09,680 --> 00:10:12,280
has some promise in that space.

208
00:10:12,280 --> 00:10:14,240
So Jen, you want to start off and just talk maybe

209
00:10:14,240 --> 00:10:17,640
a little bit about what you've been doing with AI in this area?

210
00:10:17,640 --> 00:10:19,280
Sure.

211
00:10:19,280 --> 00:10:25,680
I mean, just as a designer specifically using graphic design

212
00:10:25,720 --> 00:10:29,440
as my trade and my weapon of choice,

213
00:10:29,440 --> 00:10:36,440
it's always tech and the intersection of how we use it

214
00:10:36,440 --> 00:10:41,920
to kind of emote joy has always been a part of my world.

215
00:10:41,920 --> 00:10:49,520
And so I really use AI to just amplify good.

216
00:10:49,520 --> 00:10:52,280
Crystal, in the marketing world, I'm in the marketing world.

217
00:10:52,280 --> 00:10:55,960
I mean, you must hit every day with 65 new things AI will do

218
00:10:55,960 --> 00:10:59,240
for you that are fantastic that you need to learn yesterday.

219
00:10:59,240 --> 00:11:00,680
Yes, absolutely.

220
00:11:00,680 --> 00:11:04,000
And there's this worry that you're

221
00:11:04,000 --> 00:11:07,640
not going to need marketers anymore because genitive AI is

222
00:11:07,640 --> 00:11:08,880
going to get so good.

223
00:11:08,880 --> 00:11:10,400
I still believe that humans are better.

224
00:11:10,400 --> 00:11:12,080
It still needs oversight.

225
00:11:12,080 --> 00:11:14,760
There's a lot of pros, but a lot of cons.

226
00:11:14,760 --> 00:11:17,800
It's like the smartest, stupid thing around.

227
00:11:17,800 --> 00:11:19,080
Stephanie, how about you?

228
00:11:19,080 --> 00:11:22,160
AI up until we had this conversation.

229
00:11:22,200 --> 00:11:24,480
Yeah, it's funny.

230
00:11:24,480 --> 00:11:31,680
Back this time last year, I was sent a link by a consultant

231
00:11:31,680 --> 00:11:34,000
that we work with, a tech consultant that we work with

232
00:11:34,000 --> 00:11:38,600
who also does some strategy with our to chat to you.

233
00:11:38,600 --> 00:11:41,240
And he said, I know you're working on your presentation

234
00:11:41,240 --> 00:11:44,720
for your big company retreat.

235
00:11:44,720 --> 00:11:45,920
Check this out.

236
00:11:45,920 --> 00:11:49,040
What I ended up doing, Rick, I used it to create the outliering,

237
00:11:49,040 --> 00:11:52,040
to create the agenda, to help me come up with activities,

238
00:11:52,040 --> 00:11:55,160
specific work activities.

239
00:11:55,160 --> 00:11:56,520
And it was awesome.

240
00:11:56,520 --> 00:11:58,360
And I was like, oh, my gosh, this is great.

241
00:11:58,360 --> 00:12:00,920
But I think it's different.

242
00:12:00,920 --> 00:12:04,960
The permission we're in now with generative AI

243
00:12:04,960 --> 00:12:09,120
and the rames to get to general, what is it?

244
00:12:09,120 --> 00:12:11,160
Generalized artificial intelligence?

245
00:12:11,160 --> 00:12:13,400
Yeah, AI, artificial intelligence, yeah.

246
00:12:13,400 --> 00:12:15,120
Artificial general intelligence.

247
00:12:15,120 --> 00:12:18,360
That, to me, is a different story.

248
00:12:18,360 --> 00:12:21,360
And that's where I start to get nervous.

249
00:12:21,400 --> 00:12:27,640
And that's where my kids start to call me a ladderer.

250
00:12:27,640 --> 00:12:30,040
Well, I think stuff you do to your husband, they have.

251
00:12:30,040 --> 00:12:31,760
If you cut their head.

252
00:12:31,760 --> 00:12:32,240
They have.

253
00:12:32,240 --> 00:12:34,320
And it's the access, right?

254
00:12:34,320 --> 00:12:35,160
So you've used it.

255
00:12:35,160 --> 00:12:37,520
It's been embedded in the things that you've been using.

256
00:12:37,520 --> 00:12:40,560
But now they have direct access to it

257
00:12:40,560 --> 00:12:42,880
in a way that's different, right?

258
00:12:42,880 --> 00:12:46,480
Yeah, I mean, I think when we all actually got to use it,

259
00:12:46,480 --> 00:12:48,440
you know, like it was one thing to hear about this thing,

260
00:12:48,440 --> 00:12:49,840
artificial intelligence.

261
00:12:49,840 --> 00:12:53,280
And then chat GPT-4 came out and we all,

262
00:12:53,280 --> 00:12:55,800
some friend of us talked us into logging into it

263
00:12:55,800 --> 00:12:57,640
and signing up and checking it out.

264
00:12:57,640 --> 00:13:00,760
And you were like, oh, my God, this is fundamentally different

265
00:13:00,760 --> 00:13:03,560
from anything ever before.

266
00:13:03,560 --> 00:13:05,840
So that will lead me into my next question.

267
00:13:05,840 --> 00:13:07,760
And this can go to anyone.

268
00:13:07,760 --> 00:13:11,560
When you first heard the notion of existential risk,

269
00:13:11,560 --> 00:13:15,520
that like, within our lifetimes,

270
00:13:15,520 --> 00:13:19,760
there could be a world where there is no life on Earth.

271
00:13:20,440 --> 00:13:24,960
We are expected lifetimes, I guess.

272
00:13:24,960 --> 00:13:25,920
How did that hit you?

273
00:13:25,920 --> 00:13:27,680
What was your first thought?

274
00:13:33,600 --> 00:13:36,560
I think I believed.

275
00:13:36,560 --> 00:13:38,760
I mean, I sound so crazy.

276
00:13:38,760 --> 00:13:41,800
I feel like I sound a little crazy to throw like,

277
00:13:41,800 --> 00:13:43,360
I needed to do my research.

278
00:13:43,360 --> 00:13:46,880
Like, I needed to know who was saying this.

279
00:13:46,880 --> 00:13:49,120
Were those people credible?

280
00:13:49,160 --> 00:13:51,520
And where it's been really interesting,

281
00:13:51,520 --> 00:13:52,960
I think of the camps, right?

282
00:13:52,960 --> 00:13:57,960
The advanceers versus the Jumers or their new camp.

283
00:13:58,360 --> 00:14:02,860
And each camp has their expert that went to this

284
00:14:02,860 --> 00:14:06,800
Ivy League institution and work at this, you know,

285
00:14:06,800 --> 00:14:10,040
this big tech firm or used to work at the big tech firm.

286
00:14:10,040 --> 00:14:14,480
Like, both sides of the camp have their, you know,

287
00:14:14,480 --> 00:14:16,640
qualified experts.

288
00:14:16,640 --> 00:14:19,400
So, you know, it's really like sitting through,

289
00:14:19,400 --> 00:14:23,640
I think, for me and going like, well, in my guts,

290
00:14:23,640 --> 00:14:26,760
because we all, as parents, as mom,

291
00:14:26,760 --> 00:14:30,600
we all have the guts in the same thing, which says,

292
00:14:30,600 --> 00:14:33,240
you don't know, something does it doing right.

293
00:14:34,240 --> 00:14:37,720
And then when you start to see the onion peel away,

294
00:14:37,720 --> 00:14:41,240
like with that mess with them all then,

295
00:14:41,240 --> 00:14:44,160
like when you start to go like, wait a minute,

296
00:14:44,160 --> 00:14:47,880
like what's it going on with good, no offense,

297
00:14:47,880 --> 00:14:50,080
like bull even their toy.

298
00:14:50,080 --> 00:14:52,840
That's all about the green meat and all about the money in,

299
00:14:52,840 --> 00:14:55,400
oh, it used to be a non-profit, but no, it's not.

300
00:14:55,400 --> 00:14:56,800
Now we're focused on profit.

301
00:14:56,800 --> 00:14:59,960
Well, wait a minute, like wasn't your origin

302
00:14:59,960 --> 00:15:02,960
of your company focused on the good of the Nermit?

303
00:15:02,960 --> 00:15:07,960
So I guess I'm still a little bit, I'm slow with like,

304
00:15:08,680 --> 00:15:11,980
I will always continue to just kind of try and pull

305
00:15:11,980 --> 00:15:14,940
from various sources to make sense.

306
00:15:14,940 --> 00:15:18,380
And then frankly, trust my gut to go,

307
00:15:18,380 --> 00:15:20,100
I need to take this seriously.

308
00:15:20,100 --> 00:15:21,700
I didn't take the pandemic seriously

309
00:15:21,700 --> 00:15:22,540
when I first came out.

310
00:15:22,540 --> 00:15:26,340
My husband and my boss were like, canary in the coal mine.

311
00:15:26,340 --> 00:15:27,880
We were like, this is gonna be bad.

312
00:15:27,880 --> 00:15:29,740
I'm like, I'm going to the conference.

313
00:15:29,740 --> 00:15:32,100
You know, like, don't go to the conference in March.

314
00:15:32,100 --> 00:15:33,940
I was like, I'll be fine.

315
00:15:33,940 --> 00:15:36,420
I was like, don't go and then the conference back.

316
00:15:36,420 --> 00:15:39,700
But this is like my canary moment, I think.

317
00:15:40,700 --> 00:15:41,660
Yeah.

318
00:15:41,660 --> 00:15:42,500
Yeah.

319
00:15:42,500 --> 00:15:43,340
Excellent.

320
00:15:43,340 --> 00:15:44,780
Crystal, Jen, what do you think?

321
00:15:44,780 --> 00:15:46,740
Human extinction.

322
00:15:46,740 --> 00:15:50,180
I think extinction is a strong word

323
00:15:50,180 --> 00:15:51,860
and over what period of time.

324
00:15:51,860 --> 00:15:55,500
So I think, you know, I think that's the hard part about,

325
00:15:55,500 --> 00:15:56,740
well, there's two different things.

326
00:15:56,740 --> 00:16:00,100
There's the scary part is, I think going back

327
00:16:00,100 --> 00:16:02,380
to what Stephanie said, who is saying this

328
00:16:02,380 --> 00:16:04,220
and based on what information,

329
00:16:04,220 --> 00:16:06,740
I think the scary part for me was that the people

330
00:16:06,740 --> 00:16:09,580
that created it were warning us

331
00:16:10,420 --> 00:16:12,540
and said, we don't know how to control it at this point.

332
00:16:12,540 --> 00:16:15,700
And so when you have a creator that doesn't understand it,

333
00:16:15,700 --> 00:16:17,300
it doesn't understand what it's doing.

334
00:16:17,300 --> 00:16:20,260
That's very scary to me.

335
00:16:20,260 --> 00:16:22,140
That's the first thing that worried me.

336
00:16:22,140 --> 00:16:23,980
The second thing that worried me is you look

337
00:16:23,980 --> 00:16:28,540
at social media, et cetera, and these things were created

338
00:16:28,540 --> 00:16:31,140
so that you cannot, like as humans,

339
00:16:31,140 --> 00:16:32,780
like you can't win against it, right?

340
00:16:32,780 --> 00:16:35,300
Like you become addicted to it.

341
00:16:35,300 --> 00:16:36,140
You can't help it.

342
00:16:36,140 --> 00:16:38,860
And so I think those are the two things

343
00:16:38,860 --> 00:16:40,380
that concern me as you can see

344
00:16:40,380 --> 00:16:44,580
how social media spreads misinformation, right?

345
00:16:44,580 --> 00:16:46,900
So if you think about it, something that the people

346
00:16:46,900 --> 00:16:51,100
that created it can't control and you have this,

347
00:16:51,100 --> 00:16:52,740
and then the people that created it

348
00:16:52,740 --> 00:16:54,540
are warning you about it.

349
00:16:54,540 --> 00:16:57,260
And then you've seen just implications

350
00:16:57,260 --> 00:17:00,380
from not generative AI, but a tool like social media

351
00:17:00,380 --> 00:17:03,100
that does use it, that's spreading misinformation.

352
00:17:03,100 --> 00:17:05,860
You can see how the risks could be there.

353
00:17:05,900 --> 00:17:07,580
So I'm not fully doom and gloom

354
00:17:07,580 --> 00:17:10,740
like the human race is going away,

355
00:17:10,740 --> 00:17:12,300
but I think it's very concerning.

356
00:17:12,300 --> 00:17:15,420
And if you don't have people stepping in

357
00:17:15,420 --> 00:17:17,860
to take a stronger role in managing against it,

358
00:17:17,860 --> 00:17:19,900
then it becomes scarier.

359
00:17:19,900 --> 00:17:21,420
Yeah, for sure, Jen.

360
00:17:22,300 --> 00:17:26,420
Yeah, I mean, I still think that it takes us as humans

361
00:17:26,420 --> 00:17:30,260
to be able to make a lot of these tools

362
00:17:30,260 --> 00:17:32,460
do what we want them to do.

363
00:17:32,460 --> 00:17:34,220
I've watched my friends.

364
00:17:34,220 --> 00:17:36,740
I've watched other artists, my husband,

365
00:17:36,740 --> 00:17:38,420
use them to the fullest extent.

366
00:17:38,420 --> 00:17:42,660
And there's still essences of the person,

367
00:17:42,660 --> 00:17:46,300
the human that exists within the creations.

368
00:17:46,300 --> 00:17:49,180
And that's what I really love the most

369
00:17:49,180 --> 00:17:53,580
is how can you kind of coexist in the world

370
00:17:53,580 --> 00:17:57,540
that essentially we've built sort of.

371
00:17:57,540 --> 00:17:59,260
Yeah, yeah, I like it.

372
00:17:59,260 --> 00:18:01,500
Even in this group of four,

373
00:18:01,540 --> 00:18:04,620
we have, I think, four unique takes

374
00:18:04,620 --> 00:18:09,380
on almost the same set of information, right?

375
00:18:09,380 --> 00:18:12,580
Which is, and so Stephanie, to your point,

376
00:18:12,580 --> 00:18:15,100
first of all, are we Flat Earthers?

377
00:18:15,100 --> 00:18:19,740
Are we like Y2K people wrapping our houses in duct tape?

378
00:18:21,580 --> 00:18:22,700
The answer's no.

379
00:18:22,700 --> 00:18:25,060
I wish we were, honestly, I wish we were.

380
00:18:25,060 --> 00:18:26,380
I hope we are.

381
00:18:26,380 --> 00:18:27,220
How about this?

382
00:18:27,220 --> 00:18:28,060
I hope we are.

383
00:18:28,060 --> 00:18:30,620
I hope that this podcast is literally

384
00:18:30,620 --> 00:18:33,220
a duct tape wrap around my house.

385
00:18:33,220 --> 00:18:36,740
And I'll wake up on the January 1st morning

386
00:18:36,740 --> 00:18:39,460
and I'll be like, oh, AGI happened.

387
00:18:39,460 --> 00:18:41,460
And we're all fine and everything's fine.

388
00:18:42,500 --> 00:18:46,700
I just, that is not my read of the academics

389
00:18:46,700 --> 00:18:47,540
in the field.

390
00:18:48,780 --> 00:18:50,420
I saw something on Twitter that I'm gonna send you

391
00:18:50,420 --> 00:18:53,420
the other day, it's a list of 125 professors

392
00:18:53,420 --> 00:18:55,980
in like a string of heads of departments

393
00:18:55,980 --> 00:18:59,100
who are basically saying this same thing

394
00:18:59,100 --> 00:19:04,100
that they have grave concern for existential risk

395
00:19:05,460 --> 00:19:09,020
for all life on Earth if we continue to proceed

396
00:19:09,020 --> 00:19:12,300
with developing artificial intelligence systems

397
00:19:12,300 --> 00:19:14,980
that are not aligned with human goals and values

398
00:19:14,980 --> 00:19:16,740
and that we don't know how they work.

399
00:19:16,740 --> 00:19:18,460
So let's go to those two things,

400
00:19:19,340 --> 00:19:21,540
alignment and interpretability.

401
00:19:21,540 --> 00:19:25,380
I was stunned to know that the people who make AI

402
00:19:25,380 --> 00:19:29,420
have no idea why it does, what it does,

403
00:19:29,420 --> 00:19:31,660
or how to control it.

404
00:19:31,660 --> 00:19:34,500
What did you think when you all first heard that?

405
00:19:37,700 --> 00:19:39,140
I mean, it's not a,

406
00:19:41,660 --> 00:19:43,060
anyone who wants that one.

407
00:19:44,220 --> 00:19:46,860
I mean, Crystal, I'll go with you because,

408
00:19:46,860 --> 00:19:47,940
if someone came to you and said,

409
00:19:47,940 --> 00:19:50,620
hey, I wanna get this new product for our department,

410
00:19:51,860 --> 00:19:53,280
it has tremendous power.

411
00:19:54,280 --> 00:19:56,160
It could make us the best company on Earth

412
00:19:56,160 --> 00:19:58,400
or kill everyone in the building.

413
00:19:58,400 --> 00:19:59,640
I don't know how it works.

414
00:20:00,800 --> 00:20:01,640
And I, you know.

415
00:20:03,640 --> 00:20:06,080
That's the scary part that I had mentioned earlier

416
00:20:06,080 --> 00:20:07,920
is that the people that created it

417
00:20:07,920 --> 00:20:09,720
don't fully understand it.

418
00:20:09,720 --> 00:20:11,240
And they don't know necessarily

419
00:20:11,240 --> 00:20:13,360
how to necessarily stop parts of it.

420
00:20:13,360 --> 00:20:16,000
And I think that that's the most concerning part.

421
00:20:16,000 --> 00:20:18,800
I think that goes back to, I believe, Jen's part.

422
00:20:18,800 --> 00:20:21,400
You have to have a human interaction

423
00:20:21,400 --> 00:20:25,880
and you have to create rules around the use of it,

424
00:20:25,880 --> 00:20:28,720
or else it's going to go into the hands of bad players.

425
00:20:28,720 --> 00:20:30,960
You can't say that everyone's gonna use it for good things

426
00:20:30,960 --> 00:20:32,280
because that's just not true.

427
00:20:32,280 --> 00:20:36,400
And so how are we, as human beings, as companies,

428
00:20:36,400 --> 00:20:39,680
as governments, as moms, as human beings,

429
00:20:39,680 --> 00:20:41,600
going to all work together to make sure

430
00:20:41,600 --> 00:20:44,200
it's being used for the right reasons?

431
00:20:44,200 --> 00:20:47,440
Because we've all experienced the good in it.

432
00:20:47,440 --> 00:20:50,360
We've also seen how it can fail, right?

433
00:20:50,360 --> 00:20:52,760
You look at it, you're like, well, that's not right.

434
00:20:52,760 --> 00:20:56,240
And so that's where I think the human mind

435
00:20:56,240 --> 00:20:58,240
currently is different is we can differentiate

436
00:20:58,240 --> 00:20:59,800
between those two things.

437
00:20:59,800 --> 00:21:01,800
But I think there's a point where it's hard to do that.

438
00:21:01,800 --> 00:21:03,480
And that's what concerns me.

439
00:21:04,560 --> 00:21:05,400
Yeah.

440
00:21:05,400 --> 00:21:06,920
Yeah, Stephanie?

441
00:21:06,920 --> 00:21:08,400
Yeah, no, I agree.

442
00:21:09,400 --> 00:21:14,280
Definitely, I think it's shocking, but not shocking

443
00:21:14,280 --> 00:21:18,880
that people would want to run fast toward advancement

444
00:21:18,880 --> 00:21:20,840
and towards being the first,

445
00:21:20,840 --> 00:21:23,400
like being the first company, the first person,

446
00:21:23,400 --> 00:21:27,240
the first team, you know, it's interesting, right?

447
00:21:27,240 --> 00:21:30,800
I almost wish we had someone that may be described

448
00:21:30,800 --> 00:21:33,080
to more of like in certain value years.

449
00:21:33,080 --> 00:21:37,400
I don't know, like, and maybe you guys do, I don't, you know.

450
00:21:37,400 --> 00:21:40,640
But in listening, John had recommended Mo Galdet,

451
00:21:40,640 --> 00:21:42,680
who has a book called Scary Smart.

452
00:21:42,680 --> 00:21:44,640
He was with, let's do the act,

453
00:21:44,640 --> 00:21:46,760
but the time is only 10 years.

454
00:21:46,760 --> 00:21:49,000
And she, you know, he even says,

455
00:21:49,000 --> 00:21:53,160
like kind of the Western drive to make more money

456
00:21:53,160 --> 00:21:57,280
and just succeed by any measure and get to the top.

457
00:21:57,280 --> 00:21:59,800
And it doesn't matter if you leave in your weight.

458
00:21:59,800 --> 00:22:02,080
We want to say that that's not the case,

459
00:22:02,080 --> 00:22:05,600
but it's so very much the case.

460
00:22:05,600 --> 00:22:09,880
And so I think as moms, too, it's hard to,

461
00:22:10,720 --> 00:22:15,720
like, to parent kids when it's, you know,

462
00:22:15,760 --> 00:22:17,640
when you've got the pressure,

463
00:22:17,640 --> 00:22:20,040
social media or of your friends' kids

464
00:22:20,040 --> 00:22:24,160
or of a man on what school, and, you know,

465
00:22:24,160 --> 00:22:27,880
you're fighting that all, already, all the time.

466
00:22:27,880 --> 00:22:32,480
And then you sort of, you blow that out,

467
00:22:32,480 --> 00:22:34,240
you know, you miss, blow the AI.

468
00:22:34,240 --> 00:22:38,720
And how else are their systems supposed to learn

469
00:22:38,720 --> 00:22:40,240
other than our inputs?

470
00:22:40,240 --> 00:22:44,800
And if our inputs are all for like the basis parts

471
00:22:44,840 --> 00:22:48,200
of who we are as humans, then that's not great.

472
00:22:48,200 --> 00:22:51,520
Like that's only setting up the system

473
00:22:51,520 --> 00:22:54,880
to be based in the way that they pursue.

474
00:22:54,880 --> 00:22:58,800
And so it really worries me.

475
00:22:58,800 --> 00:23:00,800
You know, I also am aware that, like,

476
00:23:00,800 --> 00:23:05,080
we're cognitively discriminant people as humans.

477
00:23:05,080 --> 00:23:06,800
I don't know that AI systems

478
00:23:06,800 --> 00:23:08,280
or maybe they'll be able to do that.

479
00:23:08,280 --> 00:23:09,120
I don't know.

480
00:23:09,120 --> 00:23:10,840
I feel like that might be a human, like, win.

481
00:23:10,840 --> 00:23:14,640
Like, hey, I can hold two very opposing ideas

482
00:23:14,640 --> 00:23:15,480
at the same time.

483
00:23:15,480 --> 00:23:18,640
Like, I am a practicing Catholic.

484
00:23:18,640 --> 00:23:22,560
And I also am really worried about existential crisis,

485
00:23:22,560 --> 00:23:24,280
but I'm not worrying to a point

486
00:23:24,280 --> 00:23:26,120
of where I can't get out of bed

487
00:23:26,120 --> 00:23:30,080
or where I can't go to work or where I can't parent my kids.

488
00:23:30,080 --> 00:23:32,680
So, because I think at the core,

489
00:23:32,680 --> 00:23:36,400
I have this thing that, like, is my belief

490
00:23:36,400 --> 00:23:41,200
and says, like, you can't, like, offer that up.

491
00:23:41,200 --> 00:23:43,320
You can't worry about that.

492
00:23:43,320 --> 00:23:45,760
So, I don't know if that makes sense.

493
00:23:45,760 --> 00:23:49,600
I mean, like, the, I'm very concerned.

494
00:23:49,600 --> 00:23:52,640
And also, I can't really, or,

495
00:23:52,640 --> 00:23:54,280
day in or day out,

496
00:23:54,280 --> 00:23:56,040
I'm up late 16th through another year,

497
00:23:56,040 --> 00:23:56,880
or we're this way.

498
00:23:58,360 --> 00:23:59,960
Yeah, no, absolutely.

499
00:23:59,960 --> 00:24:02,280
Elon Musk was asked about it recently.

500
00:24:02,280 --> 00:24:07,400
And he said, you know, he has existential fears of AI

501
00:24:07,400 --> 00:24:10,120
and says it openly and says, in 2016,

502
00:24:10,120 --> 00:24:12,960
he had spent a lot of sleepless nights worried about

503
00:24:13,000 --> 00:24:14,360
what I'm worried about right now,

504
00:24:14,360 --> 00:24:15,760
but that he has come to peace with it

505
00:24:15,760 --> 00:24:18,320
because he's decided that there's no more interesting time

506
00:24:18,320 --> 00:24:20,360
to be alive as a human.

507
00:24:20,360 --> 00:24:23,960
So, he will sacrifice the potential for extinction

508
00:24:23,960 --> 00:24:27,560
with the excitement of living the most exciting life

509
00:24:27,560 --> 00:24:29,560
of any human generation.

510
00:24:29,560 --> 00:24:31,880
I don't know that I can quite get there,

511
00:24:31,880 --> 00:24:33,320
but I did find something in that,

512
00:24:33,320 --> 00:24:35,600
something in that that was a little helpful.

513
00:24:35,600 --> 00:24:37,080
Here's the question I have for you all,

514
00:24:37,080 --> 00:24:38,960
and then I'll throw it to you, Jen.

515
00:24:38,960 --> 00:24:41,320
Where are the women in this whole thing?

516
00:24:41,320 --> 00:24:43,480
You know, a lot of people have seen my first podcast.

517
00:24:43,480 --> 00:24:45,120
I appreciate that you all watched it.

518
00:24:45,120 --> 00:24:50,120
And, you know, this is a very male-dominated thing

519
00:24:50,560 --> 00:24:52,880
that is happening to the world.

520
00:24:54,560 --> 00:24:57,360
Where are the women and what are your thoughts about it, Jen?

521
00:24:59,280 --> 00:25:02,480
Well, I mean, there's a ton of women

522
00:25:02,480 --> 00:25:06,000
that are really advocating specifically from, you know,

523
00:25:06,000 --> 00:25:09,360
the black and brown perspective when it comes to ethical

524
00:25:09,360 --> 00:25:10,760
and responsible AI.

525
00:25:10,760 --> 00:25:14,200
Oveta Samson is a really amazing voice.

526
00:25:15,520 --> 00:25:18,280
You know, Gerald Thomas,

527
00:25:18,280 --> 00:25:21,680
who are specifically working to address

528
00:25:21,680 --> 00:25:24,600
the conversation on representation,

529
00:25:24,600 --> 00:25:27,640
because like I feel like the conversation shifts

530
00:25:27,640 --> 00:25:30,600
when you're asking, you know, black and brown folks to say,

531
00:25:30,600 --> 00:25:32,560
okay, well, what will you do with AI?

532
00:25:32,560 --> 00:25:36,480
What will you do with these tools

533
00:25:36,480 --> 00:25:39,280
and how will you make them radical?

534
00:25:39,280 --> 00:25:41,600
And how will you, what will they look like

535
00:25:41,600 --> 00:25:43,520
when they're kind of placed in the hands

536
00:25:43,520 --> 00:25:46,000
of multi-pre-marginalized people

537
00:25:46,000 --> 00:25:48,920
who have always been denied access

538
00:25:48,920 --> 00:25:52,200
for these specific tools to create

539
00:25:52,200 --> 00:25:55,040
and build the worlds that they specifically want to see.

540
00:25:55,040 --> 00:25:58,120
You know, the tools that they want to see, you know,

541
00:25:58,120 --> 00:26:00,400
that have been, you know, kind of denied.

542
00:26:00,400 --> 00:26:03,240
And so I feel like making sure

543
00:26:03,240 --> 00:26:05,760
that we can kind of have space,

544
00:26:06,040 --> 00:26:09,240
and I have like my LinkedIn open and, you know,

545
00:26:09,240 --> 00:26:12,280
being able to just, you know, like, okay, yeah,

546
00:26:12,280 --> 00:26:14,840
well, who's having these conversations?

547
00:26:14,840 --> 00:26:19,040
And I really love that the language specifically

548
00:26:19,040 --> 00:26:24,040
has been evolved into incorporating responsibility,

549
00:26:24,840 --> 00:26:29,240
you know, equity, being able to,

550
00:26:29,240 --> 00:26:31,040
and I love that, you know,

551
00:26:31,080 --> 00:26:35,880
black women who are in, invested in machine learning

552
00:26:35,880 --> 00:26:38,640
and, you know, gen AI are like,

553
00:26:38,640 --> 00:26:40,960
are leading those conversations.

554
00:26:41,840 --> 00:26:43,200
So, you know, it, like I said,

555
00:26:43,200 --> 00:26:47,000
like it begins to shift when, you know,

556
00:26:47,000 --> 00:26:49,120
when the conversation is put in the hands

557
00:26:49,120 --> 00:26:52,560
of the multi-pre-marginalized.

558
00:26:52,560 --> 00:26:53,840
Sure, sure.

559
00:26:53,840 --> 00:26:54,680
Thank you.

560
00:26:54,680 --> 00:26:55,520
And I feel like

561
00:26:55,520 --> 00:27:00,520
there's a maternal attitude missing to this whole thing.

562
00:27:01,480 --> 00:27:04,720
Like it's like some 30-year-old guys are like,

563
00:27:04,720 --> 00:27:07,200
hey, I got this car, it goes super fast.

564
00:27:07,200 --> 00:27:10,200
I'm gonna go race it at 300 miles on the highway.

565
00:27:10,200 --> 00:27:12,800
And nobody's being like, you might hurt someone.

566
00:27:12,800 --> 00:27:15,280
Don't, you know, you might want to think twice about that.

567
00:27:15,280 --> 00:27:18,680
Have you thought about the other people?

568
00:27:20,200 --> 00:27:23,640
Crystal, Stephanie, any thoughts about the male domination

569
00:27:23,640 --> 00:27:26,480
of this suicide cult?

570
00:27:26,480 --> 00:27:29,800
So it's funny, John, because I, as you asked this,

571
00:27:29,800 --> 00:27:32,080
last week I saw her film in Danny Herrera,

572
00:27:32,080 --> 00:27:34,200
she was like, it's an AI advocate.

573
00:27:34,200 --> 00:27:37,480
She posted about the New York Times article.

574
00:27:37,480 --> 00:27:40,800
So I think, you know, there is complicity in,

575
00:27:40,800 --> 00:27:44,920
particularly in media, that if you're not gonna cover,

576
00:27:44,920 --> 00:27:47,240
if you're gonna cover like the 12 game changers

577
00:27:47,240 --> 00:27:49,520
from leaders in AI, you can't find a woman

578
00:27:49,520 --> 00:27:52,520
and she would like, lazy, and it's totally right.

579
00:27:52,560 --> 00:27:54,920
Lazy reporting, lazy coverage.

580
00:27:54,920 --> 00:27:57,120
I'm not calling you crazy, John, I promise.

581
00:27:57,120 --> 00:27:59,280
But it's like, because you were like,

582
00:27:59,280 --> 00:28:00,360
I didn't have any in mind.

583
00:28:00,360 --> 00:28:01,200
And I was like-

584
00:28:01,200 --> 00:28:02,040
I looked hard.

585
00:28:02,040 --> 00:28:03,120
That really, like, yeah.

586
00:28:03,120 --> 00:28:08,120
But she posted this and she had over 600 comments of people

587
00:28:09,360 --> 00:28:11,760
and out of those, like, probably let's say,

588
00:28:11,760 --> 00:28:13,520
let's just think of like half,

589
00:28:13,520 --> 00:28:17,640
would-listing women leaders in AI.

590
00:28:17,640 --> 00:28:18,960
And you know what's really interesting?

591
00:28:18,960 --> 00:28:20,880
Just a couple that she mentioned.

592
00:28:21,000 --> 00:28:26,000
Lee, Rana L. Touloubi, Margaret Mitchell, Tim Gibrew,

593
00:28:27,400 --> 00:28:31,640
Siren Snyder, Vanilla Braga, Joy Bouladini.

594
00:28:31,640 --> 00:28:33,960
All these women, the stuff they're doing

595
00:28:33,960 --> 00:28:38,240
is around gender bias and around like ethics

596
00:28:38,240 --> 00:28:41,040
and around same food and around all this stuff.

597
00:28:41,040 --> 00:28:43,000
And you're like, yeah.

598
00:28:43,000 --> 00:28:46,920
So then cover things that we're saying

599
00:28:46,920 --> 00:28:49,080
and like that are coordinated with this conversation

600
00:28:49,080 --> 00:28:51,000
around how do we protect our, you know,

601
00:28:51,000 --> 00:28:56,000
I think there's a real issue when it comes to coverage

602
00:28:56,120 --> 00:28:58,360
and people and the New York Times got flammed

603
00:28:58,360 --> 00:29:01,760
and as they said, right.

604
00:29:01,760 --> 00:29:04,200
You know a little hot about that one.

605
00:29:04,200 --> 00:29:06,200
I need to swear words, but I got hot.

606
00:29:07,720 --> 00:29:10,680
I think, I do think there are a lot of women

607
00:29:10,680 --> 00:29:11,520
that are doing things.

608
00:29:11,520 --> 00:29:13,720
I don't think they're getting the coverage

609
00:29:14,520 --> 00:29:16,200
to Stephanie's point earlier.

610
00:29:16,200 --> 00:29:18,880
I also think there's an interesting part to it

611
00:29:18,880 --> 00:29:20,040
about geography.

612
00:29:21,200 --> 00:29:24,960
And so you see women and men, right in the EU,

613
00:29:24,960 --> 00:29:26,840
it's always more kind of risk a burst

614
00:29:26,840 --> 00:29:28,960
when it comes to technology.

615
00:29:28,960 --> 00:29:32,720
If you look at privacy, so I have a background in privacy.

616
00:29:32,720 --> 00:29:35,280
The strongest privacy laws are in the EU.

617
00:29:35,280 --> 00:29:38,960
And then in the US, it usually goes to California next

618
00:29:38,960 --> 00:29:41,960
and then it might go federal at that point.

619
00:29:41,960 --> 00:29:43,560
I think you're seeing that in AI.

620
00:29:44,840 --> 00:29:46,520
And so you're seeing, so I think you also

621
00:29:46,520 --> 00:29:48,720
have to look geographically.

622
00:29:48,720 --> 00:29:51,720
In the US, we are capitalistic.

623
00:29:51,720 --> 00:29:53,480
We're gonna go for whoever's gonna make

624
00:29:53,480 --> 00:29:54,440
the most money quickly.

625
00:29:54,440 --> 00:29:58,840
I think the EU has more of a familial kind of community sense

626
00:29:58,840 --> 00:30:01,960
than the US and then you're gonna see that play out.

627
00:30:01,960 --> 00:30:04,920
So I'm curious how the EU standards

628
00:30:04,920 --> 00:30:08,360
are gonna impact the US in other parts of the world.

629
00:30:08,360 --> 00:30:11,160
So I'm not sure, I think there's a male versus female

630
00:30:11,160 --> 00:30:14,320
US coverage, but there's a really interesting work

631
00:30:14,360 --> 00:30:18,080
happening geographically as well.

632
00:30:18,080 --> 00:30:19,320
Yeah, that's super interesting.

633
00:30:19,320 --> 00:30:22,240
I absolutely believe there are women

634
00:30:22,240 --> 00:30:24,000
doing incredible things in AI.

635
00:30:24,000 --> 00:30:25,560
I think they're not getting any coverage.

636
00:30:25,560 --> 00:30:28,400
And I think that it's also just really hard

637
00:30:28,400 --> 00:30:31,040
for those sort of stories to break through

638
00:30:32,120 --> 00:30:34,360
because it's so male dominated at the top

639
00:30:34,360 --> 00:30:38,240
that it's a real problem and it's a problem in bias.

640
00:30:38,240 --> 00:30:43,240
And it's a problem in what I wanna talk about today

641
00:30:43,800 --> 00:30:47,480
which is some 30 year old guys believing

642
00:30:47,480 --> 00:30:51,280
that they have been authorized to exercise

643
00:30:51,280 --> 00:30:54,080
existential risk on behalf of us.

644
00:30:54,080 --> 00:30:59,080
Like they go to work today thinking that somehow

645
00:30:59,360 --> 00:31:02,760
it's reasonable and appropriate for them to toy

646
00:31:02,760 --> 00:31:04,880
with all of us dying.

647
00:31:06,680 --> 00:31:09,960
And I just can't get over how that's possible

648
00:31:09,960 --> 00:31:12,800
for people to get up then and say,

649
00:31:12,840 --> 00:31:16,520
I'm gonna come back tomorrow for another day of this.

650
00:31:18,720 --> 00:31:20,920
Crystal, I wanna get at one thing you talked about

651
00:31:20,920 --> 00:31:23,040
a little bit earlier, cause I feel like even in this group

652
00:31:23,040 --> 00:31:25,960
of four we may have some differences of opinion.

653
00:31:25,960 --> 00:31:29,680
I feel like Stephanie, I feel like you can picture

654
00:31:29,680 --> 00:31:31,760
human extinction a little bit.

655
00:31:31,760 --> 00:31:33,840
Like we've had some conversations and I feel like

656
00:31:33,840 --> 00:31:36,760
it's a what, like think about your street.

657
00:31:36,760 --> 00:31:41,360
Like your literal street, what does your street look like

658
00:31:41,400 --> 00:31:46,400
the morning after all life on earth is eliminated.

659
00:31:48,680 --> 00:31:53,680
It is laughable, like it's so uncomfortable that,

660
00:31:56,600 --> 00:31:58,600
you know, it's really hard to contemplate.

661
00:31:58,600 --> 00:32:02,000
I did not think it possible in my life

662
00:32:02,000 --> 00:32:03,640
that I would be contemplating these things.

663
00:32:03,640 --> 00:32:06,600
And yet I find that I'm doing it on a daily basis.

664
00:32:06,600 --> 00:32:08,560
So Crystal, I feel like, and Jen, I don't know

665
00:32:08,560 --> 00:32:11,280
where you are on it, but I feel like Crystal,

666
00:32:11,960 --> 00:32:15,560
well, I'm a black woman in America.

667
00:32:15,560 --> 00:32:20,120
So like I feel like I'm at risk every single day,

668
00:32:20,120 --> 00:32:22,000
like walking in the street.

669
00:32:22,000 --> 00:32:24,560
I mean, that's why like I have like,

670
00:32:24,560 --> 00:32:27,280
you know, if we really wanna get deep with it,

671
00:32:27,280 --> 00:32:30,200
like human extinction, like, you know,

672
00:32:30,200 --> 00:32:33,520
society has been trying to like eliminate me my entire life.

673
00:32:33,520 --> 00:32:34,800
You know?

674
00:32:34,800 --> 00:32:39,320
So it's just, to me, it doesn't take tech,

675
00:32:40,240 --> 00:32:42,160
technology to do that.

676
00:32:42,160 --> 00:32:43,840
It doesn't take technology to do that.

677
00:32:43,840 --> 00:32:46,440
It just takes, you know, being erased

678
00:32:46,440 --> 00:32:50,600
and having like my culture become erased and appropriated.

679
00:32:50,600 --> 00:32:53,680
And, you know, if anything, you know,

680
00:32:53,680 --> 00:32:55,560
just eliminated from the conversation,

681
00:32:55,560 --> 00:32:59,440
which is why it's so important for us to kind of take up

682
00:32:59,440 --> 00:33:02,560
as much space as we can within the AI space,

683
00:33:02,560 --> 00:33:06,720
within, you know, ethical conversations on responsibility

684
00:33:06,760 --> 00:33:09,560
and what that means within artificial intelligence.

685
00:33:09,560 --> 00:33:12,840
Because, you know, I mean, at the core, like, you know,

686
00:33:12,840 --> 00:33:17,040
as, you know, black people have been, we are the oracles.

687
00:33:17,040 --> 00:33:21,080
I mean, we have been guiding people through the path of,

688
00:33:21,080 --> 00:33:24,640
you know, of survival for centuries.

689
00:33:24,640 --> 00:33:27,960
Like, you know, Harriet Tubman, I mean,

690
00:33:27,960 --> 00:33:30,360
she was literally using astronomy

691
00:33:30,360 --> 00:33:33,480
and her own disability to just to guide people

692
00:33:34,480 --> 00:33:37,360
to make sure that they can actually survive, you know?

693
00:33:37,360 --> 00:33:40,600
Yeah, it's really super interesting to think about it,

694
00:33:40,600 --> 00:33:43,480
like that, Jen, to think about different people

695
00:33:43,480 --> 00:33:44,880
perceiving it in different ways

696
00:33:44,880 --> 00:33:47,200
based on their own personal experience.

697
00:33:49,040 --> 00:33:51,200
Yeah, I'm so glad that you joined.

698
00:33:51,200 --> 00:33:54,080
Bet we have these perspectives, that we are not.

699
00:33:54,080 --> 00:33:54,920
You know what I mean?

700
00:33:54,920 --> 00:33:57,920
Like, I'm really glad that you brought that up

701
00:33:57,920 --> 00:33:58,840
because I think,

702
00:33:59,680 --> 00:34:04,680
I think within, you know, wake up in fear

703
00:34:06,600 --> 00:34:08,160
and fear for your study,

704
00:34:11,160 --> 00:34:13,840
that's something that get within your bones.

705
00:34:14,800 --> 00:34:16,480
And it's probably been, you know,

706
00:34:16,480 --> 00:34:18,960
in your family and in your bones forever.

707
00:34:18,960 --> 00:34:23,840
And, you know, it's a privilege that we don't have that.

708
00:34:23,840 --> 00:34:26,800
And now we're thinking about it.

709
00:34:26,800 --> 00:34:28,960
And I also, my next brain, you know,

710
00:34:28,960 --> 00:34:31,400
brain knowing of one brain, two mouths,

711
00:34:31,400 --> 00:34:32,840
maybe I have some other weird brain

712
00:34:32,840 --> 00:34:34,520
fitting some plate though.

713
00:34:34,520 --> 00:34:38,000
But, I don't even know, Monday.

714
00:34:39,160 --> 00:34:40,480
Well, my next thought though is like,

715
00:34:40,480 --> 00:34:45,040
and let's not reach the system that way.

716
00:34:45,040 --> 00:34:47,800
Like, maybe we have the chance

717
00:34:47,800 --> 00:34:51,840
to make the, it's a generalized AI,

718
00:34:51,840 --> 00:34:53,640
we need there's a chance

719
00:34:53,640 --> 00:34:56,480
that it can actually better than us

720
00:34:56,520 --> 00:34:59,000
as humans in that way.

721
00:34:59,000 --> 00:35:03,600
Like, we have the chance right now to teach it.

722
00:35:03,600 --> 00:35:04,440
Yeah.

723
00:35:04,440 --> 00:35:05,880
So, like they teach our kids,

724
00:35:05,880 --> 00:35:07,280
like Murgale Devka, right?

725
00:35:07,280 --> 00:35:09,000
Like, if they're in their infancy

726
00:35:09,000 --> 00:35:11,280
or they're toddler famous right now,

727
00:35:11,280 --> 00:35:13,840
which they are, and like,

728
00:35:13,840 --> 00:35:18,600
I don't need like, all of the good people,

729
00:35:18,600 --> 00:35:22,400
which is a lot more than the bad people in this world,

730
00:35:22,400 --> 00:35:25,000
all the billions of really good people

731
00:35:25,000 --> 00:35:26,040
who comes on all kinds,

732
00:35:26,040 --> 00:35:31,040
how do we get people that don't even have internet

733
00:35:31,440 --> 00:35:33,520
to be able to participate in chats

734
00:35:33,520 --> 00:35:35,600
with, you know, in a generative AI

735
00:35:35,600 --> 00:35:37,760
so that it had that perspective?

736
00:35:38,840 --> 00:35:41,800
You know, it's glad to have those inputs.

737
00:35:41,800 --> 00:35:45,400
It can have the influence of 3000 dudes

738
00:35:45,400 --> 00:35:46,400
and a couple of ladies.

739
00:35:46,400 --> 00:35:47,240
In San Francisco.

740
00:35:47,240 --> 00:35:48,080
Yeah.

741
00:35:48,080 --> 00:35:50,440
In San Francisco, to your point about John Birkin,

742
00:35:50,440 --> 00:35:52,480
it needs like the input.

743
00:35:52,480 --> 00:35:55,720
There's a people in my neighborhood,

744
00:35:55,720 --> 00:35:57,120
friends of my kids' friends,

745
00:35:57,120 --> 00:35:58,480
and they're starting to be like,

746
00:35:58,480 --> 00:36:01,000
oh, you want to talk AI, talk to me.

747
00:36:01,960 --> 00:36:05,760
But one of the mums, it's a lot of mums,

748
00:36:05,760 --> 00:36:09,240
and soon as we're both using it,

749
00:36:09,240 --> 00:36:12,840
and I do, I used it to draft content

750
00:36:12,840 --> 00:36:17,200
for like, an invite for a party.

751
00:36:17,200 --> 00:36:18,600
Like, that sounds silly,

752
00:36:18,600 --> 00:36:21,800
but I was sitting at a basketball practice for my son,

753
00:36:21,800 --> 00:36:24,360
and I was like, oh, he's trying to get us,

754
00:36:24,400 --> 00:36:27,960
there's one thing, we did this two seconds, right?

755
00:36:27,960 --> 00:36:29,520
So I could actually be mostly present

756
00:36:29,520 --> 00:36:31,880
during the basketball practice.

757
00:36:31,880 --> 00:36:33,920
And I gave it a quick prompt,

758
00:36:33,920 --> 00:36:35,280
and then I revised it twice,

759
00:36:35,280 --> 00:36:37,760
I gave it three, and I thought it's very great.

760
00:36:37,760 --> 00:36:39,440
I feel better now, if I got it.

761
00:36:40,480 --> 00:36:42,720
But I use it, and for some people,

762
00:36:42,720 --> 00:36:44,400
it's like, well, you shouldn't use it

763
00:36:44,400 --> 00:36:46,440
if you're worried about the end of the financial trip

764
00:36:46,440 --> 00:36:49,040
with other, you got it, right?

765
00:36:49,040 --> 00:36:51,160
Right, absolutely, and that's what I was talking, yes.

766
00:36:51,160 --> 00:36:55,480
I firmly believe that 99% of the AI out there

767
00:36:55,480 --> 00:36:58,240
is totally safe, and should be used,

768
00:36:58,240 --> 00:37:00,160
and there's a lot of incredible benefits

769
00:37:00,160 --> 00:37:01,000
we could get from it.

770
00:37:01,000 --> 00:37:03,120
Like, a lot of the safety research experts say,

771
00:37:03,120 --> 00:37:04,600
if we just pause for 20 years

772
00:37:04,600 --> 00:37:07,000
and just dealt with the tech we have now,

773
00:37:07,000 --> 00:37:09,520
for 20 years, we could get incredible benefits

774
00:37:09,520 --> 00:37:13,120
for health and medical and scientific breakthroughs

775
00:37:13,120 --> 00:37:14,600
and all these kinds of things.

776
00:37:15,600 --> 00:37:18,880
But we appear to be racing towards it

777
00:37:18,880 --> 00:37:20,720
much more quickly than that.

778
00:37:20,760 --> 00:37:23,400
What do you all hear in your conversations

779
00:37:23,400 --> 00:37:25,200
with your friends, with other moms out there

780
00:37:25,200 --> 00:37:28,120
at the park, at the water cooler, whatever?

781
00:37:28,120 --> 00:37:32,280
What is the tenor and tone of the conversation

782
00:37:32,280 --> 00:37:35,560
at this moment here in December of 2023,

783
00:37:35,560 --> 00:37:40,560
the year AI came out, and we all learned about it?

784
00:37:41,360 --> 00:37:43,160
How do you feel like people are feeling?

785
00:37:44,360 --> 00:37:48,280
I feel like when it comes up in my group of friends,

786
00:37:48,280 --> 00:37:50,040
it's less around human extinction,

787
00:37:50,040 --> 00:37:52,200
it's more like how the kids are gonna use it.

788
00:37:52,200 --> 00:37:55,560
So like, are my kids going to properly learn

789
00:37:55,560 --> 00:37:57,920
how to write an essay, or are they just gonna feed

790
00:37:57,920 --> 00:37:59,960
the data into chat GBT?

791
00:37:59,960 --> 00:38:02,640
And so where I hear it more, it's less around

792
00:38:02,640 --> 00:38:04,360
are humans going to go extinct?

793
00:38:04,360 --> 00:38:09,120
It's more around how is my child going to be learning

794
00:38:09,120 --> 00:38:10,840
to use it or to not use it?

795
00:38:10,840 --> 00:38:14,360
How am I gonna make sure that my child has the right skills

796
00:38:14,360 --> 00:38:16,600
so that when they do go into the workforce,

797
00:38:17,480 --> 00:38:19,880
they're able to kind of do the job.

798
00:38:19,880 --> 00:38:23,440
You're seeing schools now have kids take tests

799
00:38:23,440 --> 00:38:25,480
with a pen and paper again.

800
00:38:25,480 --> 00:38:27,520
I'm seeing several universities doing that.

801
00:38:27,520 --> 00:38:30,280
Who would have thought that because they wanna make sure

802
00:38:30,280 --> 00:38:32,000
they can still write an essay?

803
00:38:32,000 --> 00:38:36,160
So I think it's gonna be interesting as parents,

804
00:38:36,160 --> 00:38:38,080
and then the education system as well.

805
00:38:38,080 --> 00:38:40,560
So how do you make sure your kids are learning?

806
00:38:40,560 --> 00:38:44,160
So I don't hear as much about like end of the world as I do.

807
00:38:44,160 --> 00:38:46,520
What is the application in the school system?

808
00:38:46,520 --> 00:38:48,960
And then how is my child learning?

809
00:38:49,080 --> 00:38:49,920
Exactly.

810
00:38:49,920 --> 00:38:51,240
Yeah, yeah.

811
00:38:51,240 --> 00:38:52,280
No, I hear a lot of that.

812
00:38:52,280 --> 00:38:54,840
And I'm actually gonna circle right back

813
00:38:54,840 --> 00:38:56,800
actually to what Jen was talking about just a second ago

814
00:38:56,800 --> 00:38:58,280
because I had something I wanna talk about,

815
00:38:58,280 --> 00:39:03,200
which is, so I started this podcast seven weeks ago

816
00:39:03,200 --> 00:39:05,320
was working on it for a couple of months before that.

817
00:39:05,320 --> 00:39:07,960
And it was right around, it was all coming together

818
00:39:07,960 --> 00:39:11,720
right around the October 7th attacks in Israel.

819
00:39:11,720 --> 00:39:15,160
And kind of, you know, to what you're saying, Jen,

820
00:39:15,160 --> 00:39:20,160
it's like people feel directly threatened

821
00:39:22,000 --> 00:39:26,360
on a personal, emotional level in different ways

822
00:39:26,360 --> 00:39:27,200
at all times.

823
00:39:27,200 --> 00:39:30,240
And it's really hard for this very abstract issue

824
00:39:30,240 --> 00:39:34,080
of AI safety, something inside a computer system

825
00:39:34,080 --> 00:39:38,000
to emotionally resonate in the way these very personal,

826
00:39:38,000 --> 00:39:40,760
very direct threats do.

827
00:39:40,760 --> 00:39:43,080
And I sort of don't know the way around that.

828
00:39:43,080 --> 00:39:46,880
Like I don't know the way this really amorphous thing

829
00:39:46,880 --> 00:39:49,080
can compete with these other causes

830
00:39:49,080 --> 00:39:52,040
that are so visceral for people.

831
00:39:52,040 --> 00:39:53,560
And I don't know the answer.

832
00:39:53,560 --> 00:39:54,520
I'm just putting it out there.

833
00:39:54,520 --> 00:39:56,240
You know, and I don't know that there is a good answer.

834
00:39:56,240 --> 00:39:58,840
Like, I think people that are focused on issues

835
00:39:58,840 --> 00:40:01,520
of immediate human suffering and human condition,

836
00:40:04,240 --> 00:40:05,280
I can't come in and say,

837
00:40:05,280 --> 00:40:07,320
hey, everybody should drop what they're doing

838
00:40:07,320 --> 00:40:11,360
and go work on AI safety and forget about

839
00:40:11,360 --> 00:40:13,640
any stigmatism and racism and all the horrible things

840
00:40:13,640 --> 00:40:16,200
out there, hate and all the horrible things on the planet.

841
00:40:16,200 --> 00:40:20,680
Like people working on those causes have to continue,

842
00:40:20,680 --> 00:40:24,880
but it's like we as a society have some sort of like limit

843
00:40:24,880 --> 00:40:26,760
for cause stuff.

844
00:40:26,760 --> 00:40:30,040
And I feel like we're tapped out

845
00:40:30,040 --> 00:40:33,280
and AI is not gonna get, you know,

846
00:40:33,280 --> 00:40:37,960
the attention that things that affect people

847
00:40:37,960 --> 00:40:40,400
more directly and personally immediately do.

848
00:40:41,680 --> 00:40:46,400
Well, then I also think you have to look at the inputs, right?

849
00:40:46,400 --> 00:40:50,080
Going into AI as well.

850
00:40:50,080 --> 00:40:53,040
So you have deeply personal things to people,

851
00:40:53,040 --> 00:40:55,400
but people have very different views on things.

852
00:40:55,400 --> 00:40:58,720
And you have all those disparate inputs

853
00:40:58,720 --> 00:40:59,920
going into generative AI.

854
00:40:59,920 --> 00:41:01,440
And so then what comes out of that,

855
00:41:01,440 --> 00:41:04,520
I think is an interesting question.

856
00:41:04,520 --> 00:41:06,680
I think AI can see and I'll just say it.

857
00:41:06,680 --> 00:41:08,240
I think it means a new brand.

858
00:41:08,240 --> 00:41:11,120
I think the term AI safety was boring.

859
00:41:11,120 --> 00:41:12,520
I don't think anybody's guessing.

860
00:41:12,520 --> 00:41:13,360
Life is about it.

861
00:41:13,360 --> 00:41:15,160
I don't think they know what it means.

862
00:41:15,160 --> 00:41:16,800
Well, it sounds like a safety poll.

863
00:41:16,800 --> 00:41:17,640
Like what?

864
00:41:17,640 --> 00:41:20,080
Like AI safety is sooner or later.

865
00:41:20,080 --> 00:41:21,160
Let me ask this.

866
00:41:22,760 --> 00:41:25,800
Show of hands, and I think I'm gonna know the show of hands.

867
00:41:25,800 --> 00:41:28,080
I think it's me and Stephanie and Crystal and Jen

868
00:41:28,080 --> 00:41:29,080
on two sides.

869
00:41:29,960 --> 00:41:33,000
Could you actually imagine the end

870
00:41:33,000 --> 00:41:34,640
of all living things on earth?

871
00:41:37,160 --> 00:41:38,880
Because of AI or in general?

872
00:41:38,880 --> 00:41:40,440
Yes, because of AI.

873
00:41:40,440 --> 00:41:41,400
Because of AI.

874
00:41:44,120 --> 00:41:45,920
No, I can't.

875
00:41:45,920 --> 00:41:47,600
I mean, can't.

876
00:41:47,600 --> 00:41:50,200
No, I mean, just because like I know how to use it.

877
00:41:50,200 --> 00:41:53,440
Like I know how to use it for like,

878
00:41:53,440 --> 00:41:57,600
I just know how to be radical with it.

879
00:41:57,600 --> 00:41:58,440
And I know how.

880
00:41:58,440 --> 00:42:01,520
The concern is when it starts using itself, right?

881
00:42:01,520 --> 00:42:03,840
When it starts recursively improving itself,

882
00:42:03,840 --> 00:42:06,280
setting its own goals, becoming its own agent.

883
00:42:07,280 --> 00:42:09,640
You know, it goes off on its own

884
00:42:09,640 --> 00:42:11,600
and it's no longer checking with us.

885
00:42:11,600 --> 00:42:15,440
That to me is very conceivable, if not likely.

886
00:42:15,440 --> 00:42:17,680
But how is it gonna do it though?

887
00:42:17,680 --> 00:42:20,840
Like, and this is where we need to get very specific

888
00:42:20,840 --> 00:42:21,960
about the tools.

889
00:42:21,960 --> 00:42:24,960
Like, okay, so what tools are we specifically talking about?

890
00:42:24,960 --> 00:42:26,320
Like what AI?

891
00:42:26,320 --> 00:42:27,880
Like mid-journey, for instance.

892
00:42:27,880 --> 00:42:31,840
Like mid-journey cannot create problems on its own.

893
00:42:31,840 --> 00:42:33,400
Totally fine, not a problem.

894
00:42:33,400 --> 00:42:35,640
And anything that's out there that consumers are using

895
00:42:35,640 --> 00:42:36,760
is not a problem at all.

896
00:42:36,760 --> 00:42:39,360
It's the 1% of the frontier level work

897
00:42:39,360 --> 00:42:42,360
that open AI, Google, Microsoft,

898
00:42:42,360 --> 00:42:44,880
DeepMind, Anthropic are doing

899
00:42:44,880 --> 00:42:48,360
that is pushing the frontier boundary of the systems

900
00:42:48,360 --> 00:42:52,000
where it starts to recursively improve itself.

901
00:42:52,000 --> 00:42:55,280
And nobody knows what happens after that point.

902
00:42:55,280 --> 00:42:57,160
So, yeah, Crystal.

903
00:42:57,160 --> 00:42:58,640
The two things that I think about,

904
00:42:58,640 --> 00:43:02,080
I have a hard time thinking humans become extinct.

905
00:43:02,080 --> 00:43:03,720
That's like, I have a hard time there.

906
00:43:04,280 --> 00:43:07,800
Where I can see is questioning,

907
00:43:07,800 --> 00:43:09,680
what does it mean to be human?

908
00:43:09,680 --> 00:43:11,760
And Stephanie and I had a conversation around this

909
00:43:11,760 --> 00:43:16,760
because at some point, do we make generative AI so smart

910
00:43:17,000 --> 00:43:19,160
or kind of robots or whatever

911
00:43:19,160 --> 00:43:23,040
that they become citizens, humans, et cetera?

912
00:43:23,040 --> 00:43:24,600
And then you have this,

913
00:43:24,600 --> 00:43:28,160
then you have a kind of question on who makes decisions.

914
00:43:28,160 --> 00:43:30,360
Because if generative AI keeps getting smarter,

915
00:43:30,360 --> 00:43:31,600
then you can see over time,

916
00:43:31,600 --> 00:43:34,600
humans kind of move into a different role within society.

917
00:43:34,600 --> 00:43:39,600
So I can see some interesting things happening

918
00:43:40,120 --> 00:43:45,040
if you look into the future of AI becoming human

919
00:43:45,040 --> 00:43:47,600
or how we define human is one aspect.

920
00:43:47,600 --> 00:43:49,600
But the thing I keep thinking that generative AI

921
00:43:49,600 --> 00:43:54,080
can do really well more quickly is cause absolute chaos.

922
00:43:54,080 --> 00:43:56,720
Because you have disinformation, lies,

923
00:43:56,720 --> 00:43:58,520
and then you lose trust in society.

924
00:43:58,520 --> 00:44:00,480
And once you lose trust in society,

925
00:44:00,520 --> 00:44:01,960
that's where a lot of bad things happen.

926
00:44:01,960 --> 00:44:04,320
That's where war happens.

927
00:44:04,320 --> 00:44:07,040
You have groups of people, again,

928
00:44:07,040 --> 00:44:09,200
fighting different interpretations.

929
00:44:09,200 --> 00:44:12,880
And so the world I see that's more plausible more quickly

930
00:44:12,880 --> 00:44:17,040
is just sheer chaos with disinformation

931
00:44:17,040 --> 00:44:18,960
because of generative AI.

932
00:44:18,960 --> 00:44:22,240
And I don't disagree with any of that stuff,

933
00:44:22,240 --> 00:44:24,520
but I just wanna pick at it a little bit further, right?

934
00:44:24,520 --> 00:44:26,400
So with Crystal and Jen,

935
00:44:26,400 --> 00:44:29,440
so you guys watched the first episode of the podcast

936
00:44:29,440 --> 00:44:31,440
where you know that there's this 22-word statement

937
00:44:31,440 --> 00:44:34,040
that everybody in AI put out that says

938
00:44:34,040 --> 00:44:37,880
that artificial intelligence is an extinction risk

939
00:44:37,880 --> 00:44:41,120
along the lines of pandemic and nuclear war, right?

940
00:44:41,120 --> 00:44:46,120
And so my question is if they're saying it,

941
00:44:47,560 --> 00:44:50,000
if other people, if the literal people

942
00:44:50,000 --> 00:44:52,000
who are making it are saying it,

943
00:44:52,000 --> 00:44:57,000
why do you think you and the 99% of the public

944
00:44:58,000 --> 00:45:03,000
is like, ah, yeah, but like they don't really mean it.

945
00:45:03,480 --> 00:45:05,080
Like they said extinction,

946
00:45:05,080 --> 00:45:07,040
but they really meant something else.

947
00:45:09,040 --> 00:45:14,040
Like I just, I feel like I could be extinct without it.

948
00:45:15,800 --> 00:45:17,240
Like who I am as a person,

949
00:45:17,240 --> 00:45:21,720
like I could be extinct without it, you know?

950
00:45:21,720 --> 00:45:26,720
Like I was like my people, my culture, you know,

951
00:45:27,680 --> 00:45:30,000
without technology, I mean, you know,

952
00:45:30,000 --> 00:45:35,000
literally they were using colonialism and racism

953
00:45:35,080 --> 00:45:38,400
and they were using black and brown bodies

954
00:45:38,400 --> 00:45:43,400
as they were harvesting like enslaved people,

955
00:45:43,400 --> 00:45:46,400
in terms of, so I feel like it's gonna take more

956
00:45:46,400 --> 00:45:49,680
than technology, more than technology

957
00:45:49,680 --> 00:45:53,240
to like make us extinct as a people.

958
00:45:53,240 --> 00:45:56,200
I still feel like, you know, black people,

959
00:45:56,200 --> 00:46:00,280
multi-multiply marginalized people, like we will never,

960
00:46:00,280 --> 00:46:03,680
like nothing will be able to like overpower us

961
00:46:03,680 --> 00:46:06,560
because of what we've been able to endure

962
00:46:06,560 --> 00:46:09,560
and what we've been able to survive.

963
00:46:09,560 --> 00:46:14,560
So for me, I'm like, okay, come at me AI, like, you know,

964
00:46:14,640 --> 00:46:17,360
you know, it's just like, like, come at me.

965
00:46:17,360 --> 00:46:20,480
Throwing down the, she's throwing down the bell.

966
00:46:20,480 --> 00:46:24,160
Like, come at me tech bros, like, I got you, like,

967
00:46:24,160 --> 00:46:27,440
there's no way that you can like completely like,

968
00:46:28,560 --> 00:46:30,480
you know, erase me because like I've been through

969
00:46:30,480 --> 00:46:34,760
so much more than technology can, you know.

970
00:46:37,320 --> 00:46:38,680
Yeah.

971
00:46:38,680 --> 00:46:40,560
That's kind of like where I'm at.

972
00:46:41,520 --> 00:46:42,360
Yeah.

973
00:46:42,360 --> 00:46:44,480
I think it's generally hard to think

974
00:46:44,480 --> 00:46:48,080
about existential problems, like,

975
00:46:48,120 --> 00:46:50,280
and it's really hard then to think

976
00:46:50,280 --> 00:46:53,720
about existential problems with your kids.

977
00:46:53,720 --> 00:46:56,400
Like, as it relates to your kids because,

978
00:46:56,400 --> 00:47:00,040
and I love your shirt, Jen, for free and human.

979
00:47:00,040 --> 00:47:03,440
And it's like, maybe that could be like the,

980
00:47:03,440 --> 00:47:07,360
like, is it like a combo of raising good humans

981
00:47:07,360 --> 00:47:12,360
and just raising, I mean, again,

982
00:47:14,440 --> 00:47:16,200
is we haven't read Mo Galback's book,

983
00:47:16,200 --> 00:47:17,560
Scary Smiles Through Day to Play,

984
00:47:17,560 --> 00:47:19,600
called the AI, we have malware like cobblers

985
00:47:19,600 --> 00:47:22,160
and we should raise them like they raise our kids.

986
00:47:22,160 --> 00:47:26,040
So we kind of, we show them what it means

987
00:47:26,040 --> 00:47:27,240
to be a good person.

988
00:47:27,240 --> 00:47:30,800
Like, what do we mean when we're a family person, right?

989
00:47:30,800 --> 00:47:33,080
What do we mean when we're saying, I think human?

990
00:47:33,080 --> 00:47:36,360
And how all the, you know, the interactions

991
00:47:36,360 --> 00:47:38,240
you've had in real life, you know,

992
00:47:38,240 --> 00:47:40,800
digitally living with that face.

993
00:47:40,800 --> 00:47:41,920
Do you feel sorry?

994
00:47:41,920 --> 00:47:45,120
Do you feel mean or do you try and do better?

995
00:47:45,160 --> 00:47:49,600
Like, if that's, well, I don't know.

996
00:47:49,600 --> 00:47:53,160
Like, John, even in that philosophy we talked about,

997
00:47:53,160 --> 00:47:56,200
like, as humans, we can agree on what we mean

998
00:47:56,200 --> 00:47:58,080
as to be a good human.

999
00:47:58,080 --> 00:48:02,520
And then, you know, so if we can agree as humans

1000
00:48:02,520 --> 00:48:04,680
what it means to be good,

1001
00:48:04,680 --> 00:48:08,160
how will we going to teach a toddler?

1002
00:48:08,160 --> 00:48:10,200
Me do though, me do though.

1003
00:48:11,280 --> 00:48:13,680
If the systems are set,

1004
00:48:13,680 --> 00:48:17,880
and the intelligence is set to be generatory,

1005
00:48:17,880 --> 00:48:19,200
not to be generatory,

1006
00:48:20,520 --> 00:48:22,800
not to be generate, right?

1007
00:48:22,800 --> 00:48:25,480
I get what generative and other way it's like,

1008
00:48:25,480 --> 00:48:26,720
let's just break it down.

1009
00:48:26,720 --> 00:48:30,800
Like, if it is supposed to, like,

1010
00:48:30,800 --> 00:48:35,800
it's needed to create and not be destructive,

1011
00:48:36,160 --> 00:48:39,920
then let's have more people do that.

1012
00:48:39,920 --> 00:48:42,080
Well, I don't mind a real general,

1013
00:48:42,080 --> 00:48:43,680
but some things that just went into that,

1014
00:48:43,680 --> 00:48:45,800
like, you know, just more,

1015
00:48:45,800 --> 00:48:47,280
there's more inputs,

1016
00:48:47,280 --> 00:48:51,160
but more, more people

1017
00:48:51,160 --> 00:48:54,160
that are this one, two, kind of that.

1018
00:48:54,160 --> 00:48:55,560
Well, in some ways,

1019
00:48:55,560 --> 00:48:57,480
that's why I don't stop using AI,

1020
00:48:57,480 --> 00:48:58,880
like, let's get generative,

1021
00:48:58,880 --> 00:49:00,480
like, let's get that generative,

1022
00:49:00,480 --> 00:49:02,120
that I particularly worry about.

1023
00:49:02,120 --> 00:49:06,120
Let's get more people using it in a good way.

1024
00:49:06,120 --> 00:49:09,440
I'm a little bit more bullish, like, John,

1025
00:49:09,440 --> 00:49:13,080
I believe that you have 250 people that have signed this,

1026
00:49:13,080 --> 00:49:14,560
oh, who own a lot of the companies

1027
00:49:14,560 --> 00:49:16,520
that are working on this.

1028
00:49:16,520 --> 00:49:18,880
So the people that are saying it's a risk

1029
00:49:18,880 --> 00:49:21,640
also have the power to put guardrails

1030
00:49:21,640 --> 00:49:24,000
and to service leaders, and I urge you.

1031
00:49:24,000 --> 00:49:25,360
And I also believe that,

1032
00:49:25,360 --> 00:49:27,280
so if they say that's a problem

1033
00:49:27,280 --> 00:49:29,640
and we want to do something about it,

1034
00:49:29,640 --> 00:49:30,480
that's a good thing.

1035
00:49:30,480 --> 00:49:32,080
You've got, you know, you know,

1036
00:49:32,080 --> 00:49:33,920
you're always going to have the regulation come in.

1037
00:49:33,920 --> 00:49:36,120
Governments tend to be slower,

1038
00:49:36,120 --> 00:49:38,200
but I do believe you've got people that can stop it.

1039
00:49:38,200 --> 00:49:40,400
It's in a toddler stage.

1040
00:49:40,400 --> 00:49:44,080
As women, as moms, it's all of our responsibility

1041
00:49:44,080 --> 00:49:46,240
to make sure we have diversity of thought

1042
00:49:46,240 --> 00:49:48,920
in all the products that we're building,

1043
00:49:48,920 --> 00:49:51,320
in our marketing that we're doing,

1044
00:49:51,320 --> 00:49:54,400
making sure that we're hiring the right people for the roles,

1045
00:49:54,400 --> 00:49:59,320
and so we're not powerless if we all are raising good humans

1046
00:49:59,320 --> 00:50:00,560
and we're being good humans,

1047
00:50:00,560 --> 00:50:03,160
and we're making sure we're getting the right voices

1048
00:50:03,160 --> 00:50:06,360
at the table, then I believe that we can,

1049
00:50:06,360 --> 00:50:07,400
we're not going to stop it,

1050
00:50:07,400 --> 00:50:10,000
but I think we can use it for generative

1051
00:50:10,000 --> 00:50:11,640
and not degenerative purposes.

1052
00:50:11,640 --> 00:50:14,040
You're always going to have the nefarious folks

1053
00:50:14,040 --> 00:50:15,480
trying to do things,

1054
00:50:15,480 --> 00:50:17,680
but I do believe that these tech companies

1055
00:50:17,680 --> 00:50:20,280
know what they're doing and because they're funding.

1056
00:50:21,280 --> 00:50:23,440
And if they can actually start to say,

1057
00:50:23,440 --> 00:50:25,240
why don't we get a group together?

1058
00:50:26,840 --> 00:50:27,680
You can do something.

1059
00:50:27,680 --> 00:50:28,800
So I'm bullish.

1060
00:50:29,960 --> 00:50:32,240
I can see where it can go in dangerous places,

1061
00:50:32,240 --> 00:50:35,520
but I have to believe that humans can be better,

1062
00:50:35,520 --> 00:50:38,000
smarter to solve this.

1063
00:50:38,000 --> 00:50:40,320
Okay, so what do you think?

1064
00:50:40,320 --> 00:50:43,160
I've watched a ton of AI safety podcasts,

1065
00:50:43,160 --> 00:50:46,800
but I have never seen one quite like that before.

1066
00:50:46,800 --> 00:50:48,600
Real quick, I think Stephanie is right,

1067
00:50:48,600 --> 00:50:51,240
that AI safety is the wrong term.

1068
00:50:51,240 --> 00:50:53,960
It's boring and it's soft.

1069
00:50:53,960 --> 00:50:56,680
Let's talk about alternative terms to AI safety

1070
00:50:56,680 --> 00:51:00,520
in the comments, and honestly, I was surprised

1071
00:51:00,520 --> 00:51:02,600
and maybe even a bit disappointed

1072
00:51:02,600 --> 00:51:07,200
that all three moms did not fully buy my case

1073
00:51:07,200 --> 00:51:09,200
even after having heard it,

1074
00:51:09,200 --> 00:51:11,800
that human extinction from AI

1075
00:51:11,800 --> 00:51:14,600
is the most urgent threat we face.

1076
00:51:15,640 --> 00:51:18,260
I did not fully make the sale.

1077
00:51:19,840 --> 00:51:22,800
That was a little hard to take, but it's okay.

1078
00:51:22,800 --> 00:51:24,440
This takes time.

1079
00:51:25,400 --> 00:51:27,840
Quickly, my own mother, who I love dearly

1080
00:51:27,840 --> 00:51:30,800
and I think is an incredibly smart woman,

1081
00:51:30,800 --> 00:51:33,920
watched the first three episodes of this podcast

1082
00:51:33,920 --> 00:51:36,720
and was still telling me she didn't really fully get it,

1083
00:51:36,720 --> 00:51:38,200
but then after episode four,

1084
00:51:38,200 --> 00:51:41,160
she told me it started to make sense.

1085
00:51:41,160 --> 00:51:43,960
So that's how this is gonna go.

1086
00:51:43,960 --> 00:51:46,800
We, the ambassadors of this message,

1087
00:51:46,800 --> 00:51:48,840
let us not be discouraged

1088
00:51:48,840 --> 00:51:52,040
when someone is not immediately convinced

1089
00:51:52,040 --> 00:51:53,600
of what you're saying.

1090
00:51:53,600 --> 00:51:56,400
The idea of no life on earth at all

1091
00:51:56,400 --> 00:51:59,360
is something that is very difficult to process

1092
00:51:59,400 --> 00:52:01,640
for all of us, myself included,

1093
00:52:01,640 --> 00:52:04,840
but if we are to have a chance,

1094
00:52:04,840 --> 00:52:07,120
if our children are to have a chance,

1095
00:52:07,120 --> 00:52:09,360
we must go through the process,

1096
00:52:09,360 --> 00:52:12,760
we must try to save ourselves.

1097
00:52:13,800 --> 00:52:15,960
Thank you so much for watching.

1098
00:52:15,960 --> 00:52:18,800
Next week, we're gonna start pointing fingers

1099
00:52:18,800 --> 00:52:20,680
and naming names.

1100
00:52:20,680 --> 00:52:23,720
I'm ready to blame some people for all this.

1101
00:52:23,720 --> 00:52:25,240
Our next episode will be called

1102
00:52:25,240 --> 00:52:27,740
the top three doomers in AI,

1103
00:52:27,740 --> 00:52:29,380
and here's a hint,

1104
00:52:29,380 --> 00:52:32,820
they are not AI safety researchers at all.

1105
00:52:34,660 --> 00:52:37,020
For Humanity, I'm John Sherman.

1106
00:52:37,020 --> 00:52:38,620
I'll see you back here next week.

