WEBVTT

00:00.000 --> 00:03.000
I don't hear as much about, like, end of the world as I do.

00:03.000 --> 00:05.380
What is the application in the school system?

00:05.380 --> 00:07.300
And then how is my child learning?

00:07.300 --> 00:11.020
When you first heard the notion of existential risk,

00:11.020 --> 00:14.980
that, like, within our lifetimes,

00:14.980 --> 00:19.860
there could be a world where there is no life on Earth.

00:19.860 --> 00:24.420
We are expected lifetimes, I guess.

00:24.420 --> 00:25.380
How did that hit you?

00:25.380 --> 00:27.140
What was your first thought?

00:30.880 --> 00:36.040
I think I believed.

00:36.040 --> 00:38.240
I mean, I'd sound so crazy.

00:38.240 --> 00:41.320
I feel like I've found a little crazy to say, like,

00:41.320 --> 00:42.800
I needed to do my research.

00:42.800 --> 00:46.320
Like, I needed to know who was saying there.

00:46.320 --> 00:48.640
Were those people credible?

00:48.640 --> 00:52.400
And what's been really interesting, the camps, right,

00:52.400 --> 00:57.840
being the dancers, versus the juniors, or there need camp.

00:57.840 --> 01:01.720
And each camp has their expert that

01:01.720 --> 01:03.720
went to this Ivy League institution

01:03.720 --> 01:07.880
and worked at this, you know, this big tech firm,

01:07.880 --> 01:09.520
or used to work at the big tech firm.

01:09.520 --> 01:13.960
Like, both sides of the camp have their, you know,

01:13.960 --> 01:15.560
qualified experts.

01:15.560 --> 01:18.480
Like, human extinction, like, you know,

01:18.480 --> 01:21.960
society has been trying to, like, eliminate me my entire life.

01:21.960 --> 01:24.280
Nothing will be able to, like, overpower us

01:24.280 --> 01:27.200
because of what we've been able to endure

01:27.200 --> 01:30.160
and what we've been able to survive.

01:30.160 --> 01:33.560
So for me, I'm like, OK, come at me, AI.

01:33.560 --> 01:37.920
Like, you know, you know, it's just like, like, come at me.

01:37.920 --> 01:41.120
Throwing down the, she's throwing down the bell.

01:41.120 --> 01:43.240
Like, like, like, come at me, tech bros.

01:43.240 --> 01:44.120
Like, I got you.

01:44.120 --> 01:49.160
Like, there's no way that you can, like, completely, like,

01:49.160 --> 01:51.680
you know, erase me because, like, I've been through so much.

01:51.680 --> 01:53.200
I think it means a little brand.

01:53.200 --> 01:56.080
I think the term AI, so to speak, is boring.

01:56.080 --> 01:58.280
I don't think anybody gets excited about it.

01:58.280 --> 02:00.120
I don't think they know what it means.

02:00.120 --> 02:01.720
It sounds like a fake news moment.

02:01.720 --> 02:02.400
Like, what?

02:02.400 --> 02:04.720
Like, AI, fake news, snoozer.

02:04.720 --> 02:09.160
I think the scary part for me was that the people that created it

02:09.160 --> 02:12.320
were warning us and said, we don't know how to control it

02:12.320 --> 02:13.000
at this point.

02:13.000 --> 02:16.400
And so when you have a creator that doesn't understand it,

02:16.400 --> 02:19.240
it doesn't understand what it's doing, that's very scary.

02:19.240 --> 02:23.000
I was stunned to know that the people who make AI

02:23.000 --> 02:29.320
have no idea why it does, what it does, or how to control it.

02:29.320 --> 02:32.400
What did you think when you all first heard that?

02:44.600 --> 02:49.440
Welcome to For Humanity, an AI safety podcast, episode 7,

02:49.440 --> 02:52.160
Mom's Talk AI Extinction Risks.

02:52.200 --> 02:53.360
I'm John Sherman, your host.

02:53.360 --> 02:55.400
Thank you so much for joining me.

02:55.400 --> 02:58.200
This is the AI Safety Podcast for the general public,

02:58.200 --> 03:00.000
no tech background required.

03:00.000 --> 03:02.280
As you know, this show is exclusively

03:02.280 --> 03:05.800
about the threat of extinction from artificial intelligence.

03:05.800 --> 03:09.640
Please like, subscribe, and tell a friend about this show.

03:09.640 --> 03:11.440
And if you are on X and want to hear more

03:11.440 --> 03:13.320
about these issues on a daily basis,

03:13.320 --> 03:16.080
please follow at For Humanity Pod.

03:16.080 --> 03:17.960
OK.

03:17.960 --> 03:20.040
So the whole point of this podcast

03:20.040 --> 03:22.680
has been to move this debate from the Silicon Valley

03:22.680 --> 03:25.880
boardroom to the family dinner table.

03:25.880 --> 03:29.480
I also, as a parent, have been wondering

03:29.480 --> 03:34.640
how anyone who works in AI could do this to their children.

03:34.640 --> 03:39.200
And that if maybe awakening the parental maternal instincts

03:39.200 --> 03:42.000
that are so foreign to this AI safety debate

03:42.000 --> 03:46.720
could be possible and maybe even powerful.

03:46.720 --> 03:48.440
So I thought, what a better way to do that

03:48.560 --> 03:50.600
than to just do that.

03:50.600 --> 03:52.920
So I asked three moms to come on the show

03:52.920 --> 03:57.560
and discuss AI as an immediate threat to their children.

03:57.560 --> 03:59.960
I want to give a big thanks to Stephanie, Jen,

03:59.960 --> 04:02.040
and Crystal, our three moms.

04:02.040 --> 04:04.440
I asked them all to watch the first episode

04:04.440 --> 04:06.720
of this podcast before we met.

04:06.720 --> 04:09.240
I think this is a fascinating conversation,

04:09.240 --> 04:13.240
and I am thrilled to share it with you.

04:13.240 --> 04:15.040
All right.

04:15.040 --> 04:16.840
Moms, I think we made it.

04:16.840 --> 04:20.120
We made it for the Gauntlet Riverside Zooms

04:20.120 --> 04:22.800
and getting on a video conference.

04:22.800 --> 04:25.160
So nice to see you all.

04:25.160 --> 04:27.520
Nice to see you.

04:27.520 --> 04:28.480
Awesome.

04:28.480 --> 04:33.080
So let's just take it from the top here a little bit, I guess.

04:33.080 --> 04:35.520
So it's an interesting thing in your life

04:35.520 --> 04:37.720
when you start to talk about these AI safety issues.

04:37.720 --> 04:40.160
They resonate with certain people and not with other people.

04:40.160 --> 04:43.800
And Stephanie has been a friend of mine for many years.

04:43.800 --> 04:47.200
And it's an issue that resonated with her.

04:47.200 --> 04:49.400
And we got to talking about it.

04:49.400 --> 04:50.920
And one of the things we're talking about

04:50.920 --> 04:54.720
is where is the parental attitude in this?

04:54.720 --> 04:59.040
Where is the maternal attitude in this?

04:59.040 --> 05:03.520
And so I've been trying to take this debate as much as I can

05:03.520 --> 05:05.640
from tech people to regular people.

05:05.640 --> 05:07.320
So why not just do it and take it right

05:07.320 --> 05:11.840
to the most regular people of all, the moms?

05:11.840 --> 05:18.560
I will say as a father, as a son, husband, all these things

05:18.560 --> 05:20.520
that moms, the world go around.

05:20.520 --> 05:24.600
And so I salute to all the moms everywhere.

05:24.600 --> 05:26.560
I thought we could just start off right at the top.

05:26.560 --> 05:31.160
Maybe let's just go around the horn, say who you are.

05:31.160 --> 05:32.600
Tell me a little bit about your kids

05:32.600 --> 05:34.560
and maybe what you do outside of kids.

05:34.560 --> 05:36.120
And we'll just quick shoot around the horn

05:36.120 --> 05:38.200
and then come back up to the top.

05:38.200 --> 05:39.960
Stephanie, why don't you start us off?

05:40.000 --> 05:41.800
So I'm Stephanie Rano.

05:41.800 --> 05:46.320
I am a mom of 30 kiddos.

05:46.320 --> 05:48.440
My oldest is 13.

05:48.440 --> 05:51.480
My middle is 10 and my youngest is 8.

05:51.480 --> 05:54.600
So kind of early, but definitely that 13-year-old has

05:54.600 --> 05:55.640
opinions.

05:55.640 --> 05:58.400
Two of my kids are neurodivergent.

05:58.400 --> 06:00.720
And that is much a part of our lives.

06:00.720 --> 06:02.680
I'm really open about talking about it

06:02.680 --> 06:07.840
with anybody who will listen just around their gifts

06:07.840 --> 06:10.520
and the place if they need support.

06:10.520 --> 06:14.720
And what I do and I'm not being a mom is I do recruiting.

06:14.720 --> 06:17.760
So I do sales for a recruiting company.

06:17.760 --> 06:20.160
I've been doing stocking and recruiting for almost 20

06:20.160 --> 06:21.120
years.

06:21.120 --> 06:24.000
And I'm really excited for this conversation.

06:24.000 --> 06:26.760
And excited to share the podcast, hopefully with my kids.

06:26.760 --> 06:29.520
So watch your language, Jeff.

06:29.520 --> 06:30.120
Thank you.

06:30.120 --> 06:31.480
Thank you, Stephanie.

06:31.480 --> 06:33.680
I did make a promise to our group here

06:33.680 --> 06:36.480
that I would clean up my act for the show in an effort.

06:36.520 --> 06:38.200
Maybe we can show it to the kids afterwards.

06:38.200 --> 06:39.360
So that would be great.

06:39.360 --> 06:41.080
Jen, please introduce yourself.

06:41.080 --> 06:41.640
Yes.

06:41.640 --> 06:43.120
Hello, everyone.

06:43.120 --> 06:46.480
My name is Jen White-Johnson.

06:46.480 --> 06:50.280
I do identify as someone who is neurodivergent.

06:50.280 --> 06:55.600
I do have ADHD and also anxiety and also an autoimmune

06:55.600 --> 06:56.080
disorder.

06:56.080 --> 07:00.360
So I live along the intersections of being black

07:00.360 --> 07:02.560
and having a disability.

07:02.560 --> 07:04.880
And my son is also autistic.

07:04.920 --> 07:07.520
And my husband is also neurodivergent.

07:07.520 --> 07:13.320
So we are very neurodivergent, affirming family.

07:13.320 --> 07:16.360
And it intersects with my teaching.

07:16.360 --> 07:21.440
And I also teach graphic design within these past few years

07:21.440 --> 07:23.240
since my son was diagnosed.

07:23.240 --> 07:28.360
I do a lot of disability advocacy work specifically

07:28.360 --> 07:34.200
under the influence of art and where

07:34.200 --> 07:37.560
that can be used as a tool to really uplift

07:37.560 --> 07:40.720
the conversation of disability justice.

07:40.720 --> 07:42.760
So thank you for having me.

07:42.760 --> 07:44.280
Awesome, awesome thrill to have you.

07:44.280 --> 07:46.520
And Crystal, please introduce yourself.

07:46.520 --> 07:47.480
Hi, everyone.

07:47.480 --> 07:49.960
My name is Crystal Putman Garcia.

07:49.960 --> 07:53.320
So I am also a mom of three, like Stephanie.

07:53.320 --> 07:55.680
I have twins that are nine.

07:55.680 --> 07:59.280
And then I have an almost seven-year-old, so six right now.

07:59.280 --> 08:01.920
So they're a little bit on the younger side.

08:01.960 --> 08:04.280
But I do have a neurodivergent son who

08:04.280 --> 08:07.480
is obsessed with technology.

08:07.480 --> 08:12.360
He could hack into our Amazon accounts when he was five.

08:12.360 --> 08:13.760
I think it's one of his gifts.

08:13.760 --> 08:18.360
And so definitely try to balance when is it OK

08:18.360 --> 08:21.320
to give your kids technology because we live in a world

08:21.320 --> 08:22.200
with technology.

08:22.200 --> 08:24.840
And so we want kids that are comfortable with it.

08:24.840 --> 08:26.960
But where do you draw the line?

08:26.960 --> 08:28.200
And I'm not just a mom.

08:28.200 --> 08:30.320
I also work for a tech company.

08:30.360 --> 08:35.920
So I work for a policy and global intelligence company

08:35.920 --> 08:37.280
in half or several years.

08:37.280 --> 08:40.840
And I also, in the past, have worked

08:40.840 --> 08:44.520
to make sure that there's an intersection between consumers

08:44.520 --> 08:45.960
and internet privacy.

08:45.960 --> 08:48.840
And so this is an area that's particularly of interest to me.

08:48.840 --> 08:50.960
And one of the reasons why I thought this group would be

08:50.960 --> 08:57.360
really great to get together is you all put so much work

08:57.360 --> 08:59.200
into everything you're doing.

08:59.200 --> 09:02.160
You are badasses professionally.

09:02.160 --> 09:07.760
And you're also incredibly devoted to your kids, as am I.

09:07.760 --> 09:09.680
I'll just introduce myself as well for a second.

09:09.680 --> 09:10.640
I'm John.

09:10.640 --> 09:13.880
I do video production.

09:13.880 --> 09:17.000
I do a podcast that you may be watching.

09:17.000 --> 09:22.520
And I have two kids, Boy Girl Twins, who are 18 years old.

09:22.520 --> 09:23.600
I can't believe it.

09:23.600 --> 09:28.760
They are seniors in high school and getting ready to leave us.

09:28.800 --> 09:31.000
And it's a whole lot to take.

09:31.000 --> 09:36.000
But I certainly, as we think about these AI existential

09:36.000 --> 09:39.440
issues, I certainly make kids are always at the top of my mind.

09:39.440 --> 09:42.400
So why don't we go around the world one more time?

09:42.400 --> 09:47.040
And let's talk about our experience thus far with AI.

09:47.040 --> 09:49.280
We're going to talk a lot about existential risk

09:49.280 --> 09:50.600
and in the podcast.

09:50.600 --> 09:54.960
But before you became aware of this existential risk

09:54.960 --> 09:57.320
conversation, what were your impressions?

09:57.320 --> 09:59.560
And just sort of briefly, what were your dealings with it?

09:59.560 --> 10:02.280
I know Jen and I don't know, Stephanie, either.

10:02.280 --> 10:05.800
You all have been doing some really interesting things

10:05.800 --> 10:09.680
with neurodivergence and kids and AI, which

10:09.680 --> 10:12.280
has some promise in that space.

10:12.280 --> 10:14.240
So Jen, you want to start off and just talk maybe

10:14.240 --> 10:17.640
a little bit about what you've been doing with AI in this area?

10:17.640 --> 10:19.280
Sure.

10:19.280 --> 10:25.680
I mean, just as a designer specifically using graphic design

10:25.720 --> 10:29.440
as my trade and my weapon of choice,

10:29.440 --> 10:36.440
it's always tech and the intersection of how we use it

10:36.440 --> 10:41.920
to kind of emote joy has always been a part of my world.

10:41.920 --> 10:49.520
And so I really use AI to just amplify good.

10:49.520 --> 10:52.280
Crystal, in the marketing world, I'm in the marketing world.

10:52.280 --> 10:55.960
I mean, you must hit every day with 65 new things AI will do

10:55.960 --> 10:59.240
for you that are fantastic that you need to learn yesterday.

10:59.240 --> 11:00.680
Yes, absolutely.

11:00.680 --> 11:04.000
And there's this worry that you're

11:04.000 --> 11:07.640
not going to need marketers anymore because genitive AI is

11:07.640 --> 11:08.880
going to get so good.

11:08.880 --> 11:10.400
I still believe that humans are better.

11:10.400 --> 11:12.080
It still needs oversight.

11:12.080 --> 11:14.760
There's a lot of pros, but a lot of cons.

11:14.760 --> 11:17.800
It's like the smartest, stupid thing around.

11:17.800 --> 11:19.080
Stephanie, how about you?

11:19.080 --> 11:22.160
AI up until we had this conversation.

11:22.200 --> 11:24.480
Yeah, it's funny.

11:24.480 --> 11:31.680
Back this time last year, I was sent a link by a consultant

11:31.680 --> 11:34.000
that we work with, a tech consultant that we work with

11:34.000 --> 11:38.600
who also does some strategy with our to chat to you.

11:38.600 --> 11:41.240
And he said, I know you're working on your presentation

11:41.240 --> 11:44.720
for your big company retreat.

11:44.720 --> 11:45.920
Check this out.

11:45.920 --> 11:49.040
What I ended up doing, Rick, I used it to create the outliering,

11:49.040 --> 11:52.040
to create the agenda, to help me come up with activities,

11:52.040 --> 11:55.160
specific work activities.

11:55.160 --> 11:56.520
And it was awesome.

11:56.520 --> 11:58.360
And I was like, oh, my gosh, this is great.

11:58.360 --> 12:00.920
But I think it's different.

12:00.920 --> 12:04.960
The permission we're in now with generative AI

12:04.960 --> 12:09.120
and the rames to get to general, what is it?

12:09.120 --> 12:11.160
Generalized artificial intelligence?

12:11.160 --> 12:13.400
Yeah, AI, artificial intelligence, yeah.

12:13.400 --> 12:15.120
Artificial general intelligence.

12:15.120 --> 12:18.360
That, to me, is a different story.

12:18.360 --> 12:21.360
And that's where I start to get nervous.

12:21.400 --> 12:27.640
And that's where my kids start to call me a ladderer.

12:27.640 --> 12:30.040
Well, I think stuff you do to your husband, they have.

12:30.040 --> 12:31.760
If you cut their head.

12:31.760 --> 12:32.240
They have.

12:32.240 --> 12:34.320
And it's the access, right?

12:34.320 --> 12:35.160
So you've used it.

12:35.160 --> 12:37.520
It's been embedded in the things that you've been using.

12:37.520 --> 12:40.560
But now they have direct access to it

12:40.560 --> 12:42.880
in a way that's different, right?

12:42.880 --> 12:46.480
Yeah, I mean, I think when we all actually got to use it,

12:46.480 --> 12:48.440
you know, like it was one thing to hear about this thing,

12:48.440 --> 12:49.840
artificial intelligence.

12:49.840 --> 12:53.280
And then chat GPT-4 came out and we all,

12:53.280 --> 12:55.800
some friend of us talked us into logging into it

12:55.800 --> 12:57.640
and signing up and checking it out.

12:57.640 --> 13:00.760
And you were like, oh, my God, this is fundamentally different

13:00.760 --> 13:03.560
from anything ever before.

13:03.560 --> 13:05.840
So that will lead me into my next question.

13:05.840 --> 13:07.760
And this can go to anyone.

13:07.760 --> 13:11.560
When you first heard the notion of existential risk,

13:11.560 --> 13:15.520
that like, within our lifetimes,

13:15.520 --> 13:19.760
there could be a world where there is no life on Earth.

13:20.440 --> 13:24.960
We are expected lifetimes, I guess.

13:24.960 --> 13:25.920
How did that hit you?

13:25.920 --> 13:27.680
What was your first thought?

13:33.600 --> 13:36.560
I think I believed.

13:36.560 --> 13:38.760
I mean, I sound so crazy.

13:38.760 --> 13:41.800
I feel like I sound a little crazy to throw like,

13:41.800 --> 13:43.360
I needed to do my research.

13:43.360 --> 13:46.880
Like, I needed to know who was saying this.

13:46.880 --> 13:49.120
Were those people credible?

13:49.160 --> 13:51.520
And where it's been really interesting,

13:51.520 --> 13:52.960
I think of the camps, right?

13:52.960 --> 13:57.960
The advanceers versus the Jumers or their new camp.

13:58.360 --> 14:02.860
And each camp has their expert that went to this

14:02.860 --> 14:06.800
Ivy League institution and work at this, you know,

14:06.800 --> 14:10.040
this big tech firm or used to work at the big tech firm.

14:10.040 --> 14:14.480
Like, both sides of the camp have their, you know,

14:14.480 --> 14:16.640
qualified experts.

14:16.640 --> 14:19.400
So, you know, it's really like sitting through,

14:19.400 --> 14:23.640
I think, for me and going like, well, in my guts,

14:23.640 --> 14:26.760
because we all, as parents, as mom,

14:26.760 --> 14:30.600
we all have the guts in the same thing, which says,

14:30.600 --> 14:33.240
you don't know, something does it doing right.

14:34.240 --> 14:37.720
And then when you start to see the onion peel away,

14:37.720 --> 14:41.240
like with that mess with them all then,

14:41.240 --> 14:44.160
like when you start to go like, wait a minute,

14:44.160 --> 14:47.880
like what's it going on with good, no offense,

14:47.880 --> 14:50.080
like bull even their toy.

14:50.080 --> 14:52.840
That's all about the green meat and all about the money in,

14:52.840 --> 14:55.400
oh, it used to be a non-profit, but no, it's not.

14:55.400 --> 14:56.800
Now we're focused on profit.

14:56.800 --> 14:59.960
Well, wait a minute, like wasn't your origin

14:59.960 --> 15:02.960
of your company focused on the good of the Nermit?

15:02.960 --> 15:07.960
So I guess I'm still a little bit, I'm slow with like,

15:08.680 --> 15:11.980
I will always continue to just kind of try and pull

15:11.980 --> 15:14.940
from various sources to make sense.

15:14.940 --> 15:18.380
And then frankly, trust my gut to go,

15:18.380 --> 15:20.100
I need to take this seriously.

15:20.100 --> 15:21.700
I didn't take the pandemic seriously

15:21.700 --> 15:22.540
when I first came out.

15:22.540 --> 15:26.340
My husband and my boss were like, canary in the coal mine.

15:26.340 --> 15:27.880
We were like, this is gonna be bad.

15:27.880 --> 15:29.740
I'm like, I'm going to the conference.

15:29.740 --> 15:32.100
You know, like, don't go to the conference in March.

15:32.100 --> 15:33.940
I was like, I'll be fine.

15:33.940 --> 15:36.420
I was like, don't go and then the conference back.

15:36.420 --> 15:39.700
But this is like my canary moment, I think.

15:40.700 --> 15:41.660
Yeah.

15:41.660 --> 15:42.500
Yeah.

15:42.500 --> 15:43.340
Excellent.

15:43.340 --> 15:44.780
Crystal, Jen, what do you think?

15:44.780 --> 15:46.740
Human extinction.

15:46.740 --> 15:50.180
I think extinction is a strong word

15:50.180 --> 15:51.860
and over what period of time.

15:51.860 --> 15:55.500
So I think, you know, I think that's the hard part about,

15:55.500 --> 15:56.740
well, there's two different things.

15:56.740 --> 16:00.100
There's the scary part is, I think going back

16:00.100 --> 16:02.380
to what Stephanie said, who is saying this

16:02.380 --> 16:04.220
and based on what information,

16:04.220 --> 16:06.740
I think the scary part for me was that the people

16:06.740 --> 16:09.580
that created it were warning us

16:10.420 --> 16:12.540
and said, we don't know how to control it at this point.

16:12.540 --> 16:15.700
And so when you have a creator that doesn't understand it,

16:15.700 --> 16:17.300
it doesn't understand what it's doing.

16:17.300 --> 16:20.260
That's very scary to me.

16:20.260 --> 16:22.140
That's the first thing that worried me.

16:22.140 --> 16:23.980
The second thing that worried me is you look

16:23.980 --> 16:28.540
at social media, et cetera, and these things were created

16:28.540 --> 16:31.140
so that you cannot, like as humans,

16:31.140 --> 16:32.780
like you can't win against it, right?

16:32.780 --> 16:35.300
Like you become addicted to it.

16:35.300 --> 16:36.140
You can't help it.

16:36.140 --> 16:38.860
And so I think those are the two things

16:38.860 --> 16:40.380
that concern me as you can see

16:40.380 --> 16:44.580
how social media spreads misinformation, right?

16:44.580 --> 16:46.900
So if you think about it, something that the people

16:46.900 --> 16:51.100
that created it can't control and you have this,

16:51.100 --> 16:52.740
and then the people that created it

16:52.740 --> 16:54.540
are warning you about it.

16:54.540 --> 16:57.260
And then you've seen just implications

16:57.260 --> 17:00.380
from not generative AI, but a tool like social media

17:00.380 --> 17:03.100
that does use it, that's spreading misinformation.

17:03.100 --> 17:05.860
You can see how the risks could be there.

17:05.900 --> 17:07.580
So I'm not fully doom and gloom

17:07.580 --> 17:10.740
like the human race is going away,

17:10.740 --> 17:12.300
but I think it's very concerning.

17:12.300 --> 17:15.420
And if you don't have people stepping in

17:15.420 --> 17:17.860
to take a stronger role in managing against it,

17:17.860 --> 17:19.900
then it becomes scarier.

17:19.900 --> 17:21.420
Yeah, for sure, Jen.

17:22.300 --> 17:26.420
Yeah, I mean, I still think that it takes us as humans

17:26.420 --> 17:30.260
to be able to make a lot of these tools

17:30.260 --> 17:32.460
do what we want them to do.

17:32.460 --> 17:34.220
I've watched my friends.

17:34.220 --> 17:36.740
I've watched other artists, my husband,

17:36.740 --> 17:38.420
use them to the fullest extent.

17:38.420 --> 17:42.660
And there's still essences of the person,

17:42.660 --> 17:46.300
the human that exists within the creations.

17:46.300 --> 17:49.180
And that's what I really love the most

17:49.180 --> 17:53.580
is how can you kind of coexist in the world

17:53.580 --> 17:57.540
that essentially we've built sort of.

17:57.540 --> 17:59.260
Yeah, yeah, I like it.

17:59.260 --> 18:01.500
Even in this group of four,

18:01.540 --> 18:04.620
we have, I think, four unique takes

18:04.620 --> 18:09.380
on almost the same set of information, right?

18:09.380 --> 18:12.580
Which is, and so Stephanie, to your point,

18:12.580 --> 18:15.100
first of all, are we Flat Earthers?

18:15.100 --> 18:19.740
Are we like Y2K people wrapping our houses in duct tape?

18:21.580 --> 18:22.700
The answer's no.

18:22.700 --> 18:25.060
I wish we were, honestly, I wish we were.

18:25.060 --> 18:26.380
I hope we are.

18:26.380 --> 18:27.220
How about this?

18:27.220 --> 18:28.060
I hope we are.

18:28.060 --> 18:30.620
I hope that this podcast is literally

18:30.620 --> 18:33.220
a duct tape wrap around my house.

18:33.220 --> 18:36.740
And I'll wake up on the January 1st morning

18:36.740 --> 18:39.460
and I'll be like, oh, AGI happened.

18:39.460 --> 18:41.460
And we're all fine and everything's fine.

18:42.500 --> 18:46.700
I just, that is not my read of the academics

18:46.700 --> 18:47.540
in the field.

18:48.780 --> 18:50.420
I saw something on Twitter that I'm gonna send you

18:50.420 --> 18:53.420
the other day, it's a list of 125 professors

18:53.420 --> 18:55.980
in like a string of heads of departments

18:55.980 --> 18:59.100
who are basically saying this same thing

18:59.100 --> 19:04.100
that they have grave concern for existential risk

19:05.460 --> 19:09.020
for all life on Earth if we continue to proceed

19:09.020 --> 19:12.300
with developing artificial intelligence systems

19:12.300 --> 19:14.980
that are not aligned with human goals and values

19:14.980 --> 19:16.740
and that we don't know how they work.

19:16.740 --> 19:18.460
So let's go to those two things,

19:19.340 --> 19:21.540
alignment and interpretability.

19:21.540 --> 19:25.380
I was stunned to know that the people who make AI

19:25.380 --> 19:29.420
have no idea why it does, what it does,

19:29.420 --> 19:31.660
or how to control it.

19:31.660 --> 19:34.500
What did you think when you all first heard that?

19:37.700 --> 19:39.140
I mean, it's not a,

19:41.660 --> 19:43.060
anyone who wants that one.

19:44.220 --> 19:46.860
I mean, Crystal, I'll go with you because,

19:46.860 --> 19:47.940
if someone came to you and said,

19:47.940 --> 19:50.620
hey, I wanna get this new product for our department,

19:51.860 --> 19:53.280
it has tremendous power.

19:54.280 --> 19:56.160
It could make us the best company on Earth

19:56.160 --> 19:58.400
or kill everyone in the building.

19:58.400 --> 19:59.640
I don't know how it works.

20:00.800 --> 20:01.640
And I, you know.

20:03.640 --> 20:06.080
That's the scary part that I had mentioned earlier

20:06.080 --> 20:07.920
is that the people that created it

20:07.920 --> 20:09.720
don't fully understand it.

20:09.720 --> 20:11.240
And they don't know necessarily

20:11.240 --> 20:13.360
how to necessarily stop parts of it.

20:13.360 --> 20:16.000
And I think that that's the most concerning part.

20:16.000 --> 20:18.800
I think that goes back to, I believe, Jen's part.

20:18.800 --> 20:21.400
You have to have a human interaction

20:21.400 --> 20:25.880
and you have to create rules around the use of it,

20:25.880 --> 20:28.720
or else it's going to go into the hands of bad players.

20:28.720 --> 20:30.960
You can't say that everyone's gonna use it for good things

20:30.960 --> 20:32.280
because that's just not true.

20:32.280 --> 20:36.400
And so how are we, as human beings, as companies,

20:36.400 --> 20:39.680
as governments, as moms, as human beings,

20:39.680 --> 20:41.600
going to all work together to make sure

20:41.600 --> 20:44.200
it's being used for the right reasons?

20:44.200 --> 20:47.440
Because we've all experienced the good in it.

20:47.440 --> 20:50.360
We've also seen how it can fail, right?

20:50.360 --> 20:52.760
You look at it, you're like, well, that's not right.

20:52.760 --> 20:56.240
And so that's where I think the human mind

20:56.240 --> 20:58.240
currently is different is we can differentiate

20:58.240 --> 20:59.800
between those two things.

20:59.800 --> 21:01.800
But I think there's a point where it's hard to do that.

21:01.800 --> 21:03.480
And that's what concerns me.

21:04.560 --> 21:05.400
Yeah.

21:05.400 --> 21:06.920
Yeah, Stephanie?

21:06.920 --> 21:08.400
Yeah, no, I agree.

21:09.400 --> 21:14.280
Definitely, I think it's shocking, but not shocking

21:14.280 --> 21:18.880
that people would want to run fast toward advancement

21:18.880 --> 21:20.840
and towards being the first,

21:20.840 --> 21:23.400
like being the first company, the first person,

21:23.400 --> 21:27.240
the first team, you know, it's interesting, right?

21:27.240 --> 21:30.800
I almost wish we had someone that may be described

21:30.800 --> 21:33.080
to more of like in certain value years.

21:33.080 --> 21:37.400
I don't know, like, and maybe you guys do, I don't, you know.

21:37.400 --> 21:40.640
But in listening, John had recommended Mo Galdet,

21:40.640 --> 21:42.680
who has a book called Scary Smart.

21:42.680 --> 21:44.640
He was with, let's do the act,

21:44.640 --> 21:46.760
but the time is only 10 years.

21:46.760 --> 21:49.000
And she, you know, he even says,

21:49.000 --> 21:53.160
like kind of the Western drive to make more money

21:53.160 --> 21:57.280
and just succeed by any measure and get to the top.

21:57.280 --> 21:59.800
And it doesn't matter if you leave in your weight.

21:59.800 --> 22:02.080
We want to say that that's not the case,

22:02.080 --> 22:05.600
but it's so very much the case.

22:05.600 --> 22:09.880
And so I think as moms, too, it's hard to,

22:10.720 --> 22:15.720
like, to parent kids when it's, you know,

22:15.760 --> 22:17.640
when you've got the pressure,

22:17.640 --> 22:20.040
social media or of your friends' kids

22:20.040 --> 22:24.160
or of a man on what school, and, you know,

22:24.160 --> 22:27.880
you're fighting that all, already, all the time.

22:27.880 --> 22:32.480
And then you sort of, you blow that out,

22:32.480 --> 22:34.240
you know, you miss, blow the AI.

22:34.240 --> 22:38.720
And how else are their systems supposed to learn

22:38.720 --> 22:40.240
other than our inputs?

22:40.240 --> 22:44.800
And if our inputs are all for like the basis parts

22:44.840 --> 22:48.200
of who we are as humans, then that's not great.

22:48.200 --> 22:51.520
Like that's only setting up the system

22:51.520 --> 22:54.880
to be based in the way that they pursue.

22:54.880 --> 22:58.800
And so it really worries me.

22:58.800 --> 23:00.800
You know, I also am aware that, like,

23:00.800 --> 23:05.080
we're cognitively discriminant people as humans.

23:05.080 --> 23:06.800
I don't know that AI systems

23:06.800 --> 23:08.280
or maybe they'll be able to do that.

23:08.280 --> 23:09.120
I don't know.

23:09.120 --> 23:10.840
I feel like that might be a human, like, win.

23:10.840 --> 23:14.640
Like, hey, I can hold two very opposing ideas

23:14.640 --> 23:15.480
at the same time.

23:15.480 --> 23:18.640
Like, I am a practicing Catholic.

23:18.640 --> 23:22.560
And I also am really worried about existential crisis,

23:22.560 --> 23:24.280
but I'm not worrying to a point

23:24.280 --> 23:26.120
of where I can't get out of bed

23:26.120 --> 23:30.080
or where I can't go to work or where I can't parent my kids.

23:30.080 --> 23:32.680
So, because I think at the core,

23:32.680 --> 23:36.400
I have this thing that, like, is my belief

23:36.400 --> 23:41.200
and says, like, you can't, like, offer that up.

23:41.200 --> 23:43.320
You can't worry about that.

23:43.320 --> 23:45.760
So, I don't know if that makes sense.

23:45.760 --> 23:49.600
I mean, like, the, I'm very concerned.

23:49.600 --> 23:52.640
And also, I can't really, or,

23:52.640 --> 23:54.280
day in or day out,

23:54.280 --> 23:56.040
I'm up late 16th through another year,

23:56.040 --> 23:56.880
or we're this way.

23:58.360 --> 23:59.960
Yeah, no, absolutely.

23:59.960 --> 24:02.280
Elon Musk was asked about it recently.

24:02.280 --> 24:07.400
And he said, you know, he has existential fears of AI

24:07.400 --> 24:10.120
and says it openly and says, in 2016,

24:10.120 --> 24:12.960
he had spent a lot of sleepless nights worried about

24:13.000 --> 24:14.360
what I'm worried about right now,

24:14.360 --> 24:15.760
but that he has come to peace with it

24:15.760 --> 24:18.320
because he's decided that there's no more interesting time

24:18.320 --> 24:20.360
to be alive as a human.

24:20.360 --> 24:23.960
So, he will sacrifice the potential for extinction

24:23.960 --> 24:27.560
with the excitement of living the most exciting life

24:27.560 --> 24:29.560
of any human generation.

24:29.560 --> 24:31.880
I don't know that I can quite get there,

24:31.880 --> 24:33.320
but I did find something in that,

24:33.320 --> 24:35.600
something in that that was a little helpful.

24:35.600 --> 24:37.080
Here's the question I have for you all,

24:37.080 --> 24:38.960
and then I'll throw it to you, Jen.

24:38.960 --> 24:41.320
Where are the women in this whole thing?

24:41.320 --> 24:43.480
You know, a lot of people have seen my first podcast.

24:43.480 --> 24:45.120
I appreciate that you all watched it.

24:45.120 --> 24:50.120
And, you know, this is a very male-dominated thing

24:50.560 --> 24:52.880
that is happening to the world.

24:54.560 --> 24:57.360
Where are the women and what are your thoughts about it, Jen?

24:59.280 --> 25:02.480
Well, I mean, there's a ton of women

25:02.480 --> 25:06.000
that are really advocating specifically from, you know,

25:06.000 --> 25:09.360
the black and brown perspective when it comes to ethical

25:09.360 --> 25:10.760
and responsible AI.

25:10.760 --> 25:14.200
Oveta Samson is a really amazing voice.

25:15.520 --> 25:18.280
You know, Gerald Thomas,

25:18.280 --> 25:21.680
who are specifically working to address

25:21.680 --> 25:24.600
the conversation on representation,

25:24.600 --> 25:27.640
because like I feel like the conversation shifts

25:27.640 --> 25:30.600
when you're asking, you know, black and brown folks to say,

25:30.600 --> 25:32.560
okay, well, what will you do with AI?

25:32.560 --> 25:36.480
What will you do with these tools

25:36.480 --> 25:39.280
and how will you make them radical?

25:39.280 --> 25:41.600
And how will you, what will they look like

25:41.600 --> 25:43.520
when they're kind of placed in the hands

25:43.520 --> 25:46.000
of multi-pre-marginalized people

25:46.000 --> 25:48.920
who have always been denied access

25:48.920 --> 25:52.200
for these specific tools to create

25:52.200 --> 25:55.040
and build the worlds that they specifically want to see.

25:55.040 --> 25:58.120
You know, the tools that they want to see, you know,

25:58.120 --> 26:00.400
that have been, you know, kind of denied.

26:00.400 --> 26:03.240
And so I feel like making sure

26:03.240 --> 26:05.760
that we can kind of have space,

26:06.040 --> 26:09.240
and I have like my LinkedIn open and, you know,

26:09.240 --> 26:12.280
being able to just, you know, like, okay, yeah,

26:12.280 --> 26:14.840
well, who's having these conversations?

26:14.840 --> 26:19.040
And I really love that the language specifically

26:19.040 --> 26:24.040
has been evolved into incorporating responsibility,

26:24.840 --> 26:29.240
you know, equity, being able to,

26:29.240 --> 26:31.040
and I love that, you know,

26:31.080 --> 26:35.880
black women who are in, invested in machine learning

26:35.880 --> 26:38.640
and, you know, gen AI are like,

26:38.640 --> 26:40.960
are leading those conversations.

26:41.840 --> 26:43.200
So, you know, it, like I said,

26:43.200 --> 26:47.000
like it begins to shift when, you know,

26:47.000 --> 26:49.120
when the conversation is put in the hands

26:49.120 --> 26:52.560
of the multi-pre-marginalized.

26:52.560 --> 26:53.840
Sure, sure.

26:53.840 --> 26:54.680
Thank you.

26:54.680 --> 26:55.520
And I feel like

26:55.520 --> 27:00.520
there's a maternal attitude missing to this whole thing.

27:01.480 --> 27:04.720
Like it's like some 30-year-old guys are like,

27:04.720 --> 27:07.200
hey, I got this car, it goes super fast.

27:07.200 --> 27:10.200
I'm gonna go race it at 300 miles on the highway.

27:10.200 --> 27:12.800
And nobody's being like, you might hurt someone.

27:12.800 --> 27:15.280
Don't, you know, you might want to think twice about that.

27:15.280 --> 27:18.680
Have you thought about the other people?

27:20.200 --> 27:23.640
Crystal, Stephanie, any thoughts about the male domination

27:23.640 --> 27:26.480
of this suicide cult?

27:26.480 --> 27:29.800
So it's funny, John, because I, as you asked this,

27:29.800 --> 27:32.080
last week I saw her film in Danny Herrera,

27:32.080 --> 27:34.200
she was like, it's an AI advocate.

27:34.200 --> 27:37.480
She posted about the New York Times article.

27:37.480 --> 27:40.800
So I think, you know, there is complicity in,

27:40.800 --> 27:44.920
particularly in media, that if you're not gonna cover,

27:44.920 --> 27:47.240
if you're gonna cover like the 12 game changers

27:47.240 --> 27:49.520
from leaders in AI, you can't find a woman

27:49.520 --> 27:52.520
and she would like, lazy, and it's totally right.

27:52.560 --> 27:54.920
Lazy reporting, lazy coverage.

27:54.920 --> 27:57.120
I'm not calling you crazy, John, I promise.

27:57.120 --> 27:59.280
But it's like, because you were like,

27:59.280 --> 28:00.360
I didn't have any in mind.

28:00.360 --> 28:01.200
And I was like-

28:01.200 --> 28:02.040
I looked hard.

28:02.040 --> 28:03.120
That really, like, yeah.

28:03.120 --> 28:08.120
But she posted this and she had over 600 comments of people

28:09.360 --> 28:11.760
and out of those, like, probably let's say,

28:11.760 --> 28:13.520
let's just think of like half,

28:13.520 --> 28:17.640
would-listing women leaders in AI.

28:17.640 --> 28:18.960
And you know what's really interesting?

28:18.960 --> 28:20.880
Just a couple that she mentioned.

28:21.000 --> 28:26.000
Lee, Rana L. Touloubi, Margaret Mitchell, Tim Gibrew,

28:27.400 --> 28:31.640
Siren Snyder, Vanilla Braga, Joy Bouladini.

28:31.640 --> 28:33.960
All these women, the stuff they're doing

28:33.960 --> 28:38.240
is around gender bias and around like ethics

28:38.240 --> 28:41.040
and around same food and around all this stuff.

28:41.040 --> 28:43.000
And you're like, yeah.

28:43.000 --> 28:46.920
So then cover things that we're saying

28:46.920 --> 28:49.080
and like that are coordinated with this conversation

28:49.080 --> 28:51.000
around how do we protect our, you know,

28:51.000 --> 28:56.000
I think there's a real issue when it comes to coverage

28:56.120 --> 28:58.360
and people and the New York Times got flammed

28:58.360 --> 29:01.760
and as they said, right.

29:01.760 --> 29:04.200
You know a little hot about that one.

29:04.200 --> 29:06.200
I need to swear words, but I got hot.

29:07.720 --> 29:10.680
I think, I do think there are a lot of women

29:10.680 --> 29:11.520
that are doing things.

29:11.520 --> 29:13.720
I don't think they're getting the coverage

29:14.520 --> 29:16.200
to Stephanie's point earlier.

29:16.200 --> 29:18.880
I also think there's an interesting part to it

29:18.880 --> 29:20.040
about geography.

29:21.200 --> 29:24.960
And so you see women and men, right in the EU,

29:24.960 --> 29:26.840
it's always more kind of risk a burst

29:26.840 --> 29:28.960
when it comes to technology.

29:28.960 --> 29:32.720
If you look at privacy, so I have a background in privacy.

29:32.720 --> 29:35.280
The strongest privacy laws are in the EU.

29:35.280 --> 29:38.960
And then in the US, it usually goes to California next

29:38.960 --> 29:41.960
and then it might go federal at that point.

29:41.960 --> 29:43.560
I think you're seeing that in AI.

29:44.840 --> 29:46.520
And so you're seeing, so I think you also

29:46.520 --> 29:48.720
have to look geographically.

29:48.720 --> 29:51.720
In the US, we are capitalistic.

29:51.720 --> 29:53.480
We're gonna go for whoever's gonna make

29:53.480 --> 29:54.440
the most money quickly.

29:54.440 --> 29:58.840
I think the EU has more of a familial kind of community sense

29:58.840 --> 30:01.960
than the US and then you're gonna see that play out.

30:01.960 --> 30:04.920
So I'm curious how the EU standards

30:04.920 --> 30:08.360
are gonna impact the US in other parts of the world.

30:08.360 --> 30:11.160
So I'm not sure, I think there's a male versus female

30:11.160 --> 30:14.320
US coverage, but there's a really interesting work

30:14.360 --> 30:18.080
happening geographically as well.

30:18.080 --> 30:19.320
Yeah, that's super interesting.

30:19.320 --> 30:22.240
I absolutely believe there are women

30:22.240 --> 30:24.000
doing incredible things in AI.

30:24.000 --> 30:25.560
I think they're not getting any coverage.

30:25.560 --> 30:28.400
And I think that it's also just really hard

30:28.400 --> 30:31.040
for those sort of stories to break through

30:32.120 --> 30:34.360
because it's so male dominated at the top

30:34.360 --> 30:38.240
that it's a real problem and it's a problem in bias.

30:38.240 --> 30:43.240
And it's a problem in what I wanna talk about today

30:43.800 --> 30:47.480
which is some 30 year old guys believing

30:47.480 --> 30:51.280
that they have been authorized to exercise

30:51.280 --> 30:54.080
existential risk on behalf of us.

30:54.080 --> 30:59.080
Like they go to work today thinking that somehow

30:59.360 --> 31:02.760
it's reasonable and appropriate for them to toy

31:02.760 --> 31:04.880
with all of us dying.

31:06.680 --> 31:09.960
And I just can't get over how that's possible

31:09.960 --> 31:12.800
for people to get up then and say,

31:12.840 --> 31:16.520
I'm gonna come back tomorrow for another day of this.

31:18.720 --> 31:20.920
Crystal, I wanna get at one thing you talked about

31:20.920 --> 31:23.040
a little bit earlier, cause I feel like even in this group

31:23.040 --> 31:25.960
of four we may have some differences of opinion.

31:25.960 --> 31:29.680
I feel like Stephanie, I feel like you can picture

31:29.680 --> 31:31.760
human extinction a little bit.

31:31.760 --> 31:33.840
Like we've had some conversations and I feel like

31:33.840 --> 31:36.760
it's a what, like think about your street.

31:36.760 --> 31:41.360
Like your literal street, what does your street look like

31:41.400 --> 31:46.400
the morning after all life on earth is eliminated.

31:48.680 --> 31:53.680
It is laughable, like it's so uncomfortable that,

31:56.600 --> 31:58.600
you know, it's really hard to contemplate.

31:58.600 --> 32:02.000
I did not think it possible in my life

32:02.000 --> 32:03.640
that I would be contemplating these things.

32:03.640 --> 32:06.600
And yet I find that I'm doing it on a daily basis.

32:06.600 --> 32:08.560
So Crystal, I feel like, and Jen, I don't know

32:08.560 --> 32:11.280
where you are on it, but I feel like Crystal,

32:11.960 --> 32:15.560
well, I'm a black woman in America.

32:15.560 --> 32:20.120
So like I feel like I'm at risk every single day,

32:20.120 --> 32:22.000
like walking in the street.

32:22.000 --> 32:24.560
I mean, that's why like I have like,

32:24.560 --> 32:27.280
you know, if we really wanna get deep with it,

32:27.280 --> 32:30.200
like human extinction, like, you know,

32:30.200 --> 32:33.520
society has been trying to like eliminate me my entire life.

32:33.520 --> 32:34.800
You know?

32:34.800 --> 32:39.320
So it's just, to me, it doesn't take tech,

32:40.240 --> 32:42.160
technology to do that.

32:42.160 --> 32:43.840
It doesn't take technology to do that.

32:43.840 --> 32:46.440
It just takes, you know, being erased

32:46.440 --> 32:50.600
and having like my culture become erased and appropriated.

32:50.600 --> 32:53.680
And, you know, if anything, you know,

32:53.680 --> 32:55.560
just eliminated from the conversation,

32:55.560 --> 32:59.440
which is why it's so important for us to kind of take up

32:59.440 --> 33:02.560
as much space as we can within the AI space,

33:02.560 --> 33:06.720
within, you know, ethical conversations on responsibility

33:06.760 --> 33:09.560
and what that means within artificial intelligence.

33:09.560 --> 33:12.840
Because, you know, I mean, at the core, like, you know,

33:12.840 --> 33:17.040
as, you know, black people have been, we are the oracles.

33:17.040 --> 33:21.080
I mean, we have been guiding people through the path of,

33:21.080 --> 33:24.640
you know, of survival for centuries.

33:24.640 --> 33:27.960
Like, you know, Harriet Tubman, I mean,

33:27.960 --> 33:30.360
she was literally using astronomy

33:30.360 --> 33:33.480
and her own disability to just to guide people

33:34.480 --> 33:37.360
to make sure that they can actually survive, you know?

33:37.360 --> 33:40.600
Yeah, it's really super interesting to think about it,

33:40.600 --> 33:43.480
like that, Jen, to think about different people

33:43.480 --> 33:44.880
perceiving it in different ways

33:44.880 --> 33:47.200
based on their own personal experience.

33:49.040 --> 33:51.200
Yeah, I'm so glad that you joined.

33:51.200 --> 33:54.080
Bet we have these perspectives, that we are not.

33:54.080 --> 33:54.920
You know what I mean?

33:54.920 --> 33:57.920
Like, I'm really glad that you brought that up

33:57.920 --> 33:58.840
because I think,

33:59.680 --> 34:04.680
I think within, you know, wake up in fear

34:06.600 --> 34:08.160
and fear for your study,

34:11.160 --> 34:13.840
that's something that get within your bones.

34:14.800 --> 34:16.480
And it's probably been, you know,

34:16.480 --> 34:18.960
in your family and in your bones forever.

34:18.960 --> 34:23.840
And, you know, it's a privilege that we don't have that.

34:23.840 --> 34:26.800
And now we're thinking about it.

34:26.800 --> 34:28.960
And I also, my next brain, you know,

34:28.960 --> 34:31.400
brain knowing of one brain, two mouths,

34:31.400 --> 34:32.840
maybe I have some other weird brain

34:32.840 --> 34:34.520
fitting some plate though.

34:34.520 --> 34:38.000
But, I don't even know, Monday.

34:39.160 --> 34:40.480
Well, my next thought though is like,

34:40.480 --> 34:45.040
and let's not reach the system that way.

34:45.040 --> 34:47.800
Like, maybe we have the chance

34:47.800 --> 34:51.840
to make the, it's a generalized AI,

34:51.840 --> 34:53.640
we need there's a chance

34:53.640 --> 34:56.480
that it can actually better than us

34:56.520 --> 34:59.000
as humans in that way.

34:59.000 --> 35:03.600
Like, we have the chance right now to teach it.

35:03.600 --> 35:04.440
Yeah.

35:04.440 --> 35:05.880
So, like they teach our kids,

35:05.880 --> 35:07.280
like Murgale Devka, right?

35:07.280 --> 35:09.000
Like, if they're in their infancy

35:09.000 --> 35:11.280
or they're toddler famous right now,

35:11.280 --> 35:13.840
which they are, and like,

35:13.840 --> 35:18.600
I don't need like, all of the good people,

35:18.600 --> 35:22.400
which is a lot more than the bad people in this world,

35:22.400 --> 35:25.000
all the billions of really good people

35:25.000 --> 35:26.040
who comes on all kinds,

35:26.040 --> 35:31.040
how do we get people that don't even have internet

35:31.440 --> 35:33.520
to be able to participate in chats

35:33.520 --> 35:35.600
with, you know, in a generative AI

35:35.600 --> 35:37.760
so that it had that perspective?

35:38.840 --> 35:41.800
You know, it's glad to have those inputs.

35:41.800 --> 35:45.400
It can have the influence of 3000 dudes

35:45.400 --> 35:46.400
and a couple of ladies.

35:46.400 --> 35:47.240
In San Francisco.

35:47.240 --> 35:48.080
Yeah.

35:48.080 --> 35:50.440
In San Francisco, to your point about John Birkin,

35:50.440 --> 35:52.480
it needs like the input.

35:52.480 --> 35:55.720
There's a people in my neighborhood,

35:55.720 --> 35:57.120
friends of my kids' friends,

35:57.120 --> 35:58.480
and they're starting to be like,

35:58.480 --> 36:01.000
oh, you want to talk AI, talk to me.

36:01.960 --> 36:05.760
But one of the mums, it's a lot of mums,

36:05.760 --> 36:09.240
and soon as we're both using it,

36:09.240 --> 36:12.840
and I do, I used it to draft content

36:12.840 --> 36:17.200
for like, an invite for a party.

36:17.200 --> 36:18.600
Like, that sounds silly,

36:18.600 --> 36:21.800
but I was sitting at a basketball practice for my son,

36:21.800 --> 36:24.360
and I was like, oh, he's trying to get us,

36:24.400 --> 36:27.960
there's one thing, we did this two seconds, right?

36:27.960 --> 36:29.520
So I could actually be mostly present

36:29.520 --> 36:31.880
during the basketball practice.

36:31.880 --> 36:33.920
And I gave it a quick prompt,

36:33.920 --> 36:35.280
and then I revised it twice,

36:35.280 --> 36:37.760
I gave it three, and I thought it's very great.

36:37.760 --> 36:39.440
I feel better now, if I got it.

36:40.480 --> 36:42.720
But I use it, and for some people,

36:42.720 --> 36:44.400
it's like, well, you shouldn't use it

36:44.400 --> 36:46.440
if you're worried about the end of the financial trip

36:46.440 --> 36:49.040
with other, you got it, right?

36:49.040 --> 36:51.160
Right, absolutely, and that's what I was talking, yes.

36:51.160 --> 36:55.480
I firmly believe that 99% of the AI out there

36:55.480 --> 36:58.240
is totally safe, and should be used,

36:58.240 --> 37:00.160
and there's a lot of incredible benefits

37:00.160 --> 37:01.000
we could get from it.

37:01.000 --> 37:03.120
Like, a lot of the safety research experts say,

37:03.120 --> 37:04.600
if we just pause for 20 years

37:04.600 --> 37:07.000
and just dealt with the tech we have now,

37:07.000 --> 37:09.520
for 20 years, we could get incredible benefits

37:09.520 --> 37:13.120
for health and medical and scientific breakthroughs

37:13.120 --> 37:14.600
and all these kinds of things.

37:15.600 --> 37:18.880
But we appear to be racing towards it

37:18.880 --> 37:20.720
much more quickly than that.

37:20.760 --> 37:23.400
What do you all hear in your conversations

37:23.400 --> 37:25.200
with your friends, with other moms out there

37:25.200 --> 37:28.120
at the park, at the water cooler, whatever?

37:28.120 --> 37:32.280
What is the tenor and tone of the conversation

37:32.280 --> 37:35.560
at this moment here in December of 2023,

37:35.560 --> 37:40.560
the year AI came out, and we all learned about it?

37:41.360 --> 37:43.160
How do you feel like people are feeling?

37:44.360 --> 37:48.280
I feel like when it comes up in my group of friends,

37:48.280 --> 37:50.040
it's less around human extinction,

37:50.040 --> 37:52.200
it's more like how the kids are gonna use it.

37:52.200 --> 37:55.560
So like, are my kids going to properly learn

37:55.560 --> 37:57.920
how to write an essay, or are they just gonna feed

37:57.920 --> 37:59.960
the data into chat GBT?

37:59.960 --> 38:02.640
And so where I hear it more, it's less around

38:02.640 --> 38:04.360
are humans going to go extinct?

38:04.360 --> 38:09.120
It's more around how is my child going to be learning

38:09.120 --> 38:10.840
to use it or to not use it?

38:10.840 --> 38:14.360
How am I gonna make sure that my child has the right skills

38:14.360 --> 38:16.600
so that when they do go into the workforce,

38:17.480 --> 38:19.880
they're able to kind of do the job.

38:19.880 --> 38:23.440
You're seeing schools now have kids take tests

38:23.440 --> 38:25.480
with a pen and paper again.

38:25.480 --> 38:27.520
I'm seeing several universities doing that.

38:27.520 --> 38:30.280
Who would have thought that because they wanna make sure

38:30.280 --> 38:32.000
they can still write an essay?

38:32.000 --> 38:36.160
So I think it's gonna be interesting as parents,

38:36.160 --> 38:38.080
and then the education system as well.

38:38.080 --> 38:40.560
So how do you make sure your kids are learning?

38:40.560 --> 38:44.160
So I don't hear as much about like end of the world as I do.

38:44.160 --> 38:46.520
What is the application in the school system?

38:46.520 --> 38:48.960
And then how is my child learning?

38:49.080 --> 38:49.920
Exactly.

38:49.920 --> 38:51.240
Yeah, yeah.

38:51.240 --> 38:52.280
No, I hear a lot of that.

38:52.280 --> 38:54.840
And I'm actually gonna circle right back

38:54.840 --> 38:56.800
actually to what Jen was talking about just a second ago

38:56.800 --> 38:58.280
because I had something I wanna talk about,

38:58.280 --> 39:03.200
which is, so I started this podcast seven weeks ago

39:03.200 --> 39:05.320
was working on it for a couple of months before that.

39:05.320 --> 39:07.960
And it was right around, it was all coming together

39:07.960 --> 39:11.720
right around the October 7th attacks in Israel.

39:11.720 --> 39:15.160
And kind of, you know, to what you're saying, Jen,

39:15.160 --> 39:20.160
it's like people feel directly threatened

39:22.000 --> 39:26.360
on a personal, emotional level in different ways

39:26.360 --> 39:27.200
at all times.

39:27.200 --> 39:30.240
And it's really hard for this very abstract issue

39:30.240 --> 39:34.080
of AI safety, something inside a computer system

39:34.080 --> 39:38.000
to emotionally resonate in the way these very personal,

39:38.000 --> 39:40.760
very direct threats do.

39:40.760 --> 39:43.080
And I sort of don't know the way around that.

39:43.080 --> 39:46.880
Like I don't know the way this really amorphous thing

39:46.880 --> 39:49.080
can compete with these other causes

39:49.080 --> 39:52.040
that are so visceral for people.

39:52.040 --> 39:53.560
And I don't know the answer.

39:53.560 --> 39:54.520
I'm just putting it out there.

39:54.520 --> 39:56.240
You know, and I don't know that there is a good answer.

39:56.240 --> 39:58.840
Like, I think people that are focused on issues

39:58.840 --> 40:01.520
of immediate human suffering and human condition,

40:04.240 --> 40:05.280
I can't come in and say,

40:05.280 --> 40:07.320
hey, everybody should drop what they're doing

40:07.320 --> 40:11.360
and go work on AI safety and forget about

40:11.360 --> 40:13.640
any stigmatism and racism and all the horrible things

40:13.640 --> 40:16.200
out there, hate and all the horrible things on the planet.

40:16.200 --> 40:20.680
Like people working on those causes have to continue,

40:20.680 --> 40:24.880
but it's like we as a society have some sort of like limit

40:24.880 --> 40:26.760
for cause stuff.

40:26.760 --> 40:30.040
And I feel like we're tapped out

40:30.040 --> 40:33.280
and AI is not gonna get, you know,

40:33.280 --> 40:37.960
the attention that things that affect people

40:37.960 --> 40:40.400
more directly and personally immediately do.

40:41.680 --> 40:46.400
Well, then I also think you have to look at the inputs, right?

40:46.400 --> 40:50.080
Going into AI as well.

40:50.080 --> 40:53.040
So you have deeply personal things to people,

40:53.040 --> 40:55.400
but people have very different views on things.

40:55.400 --> 40:58.720
And you have all those disparate inputs

40:58.720 --> 40:59.920
going into generative AI.

40:59.920 --> 41:01.440
And so then what comes out of that,

41:01.440 --> 41:04.520
I think is an interesting question.

41:04.520 --> 41:06.680
I think AI can see and I'll just say it.

41:06.680 --> 41:08.240
I think it means a new brand.

41:08.240 --> 41:11.120
I think the term AI safety was boring.

41:11.120 --> 41:12.520
I don't think anybody's guessing.

41:12.520 --> 41:13.360
Life is about it.

41:13.360 --> 41:15.160
I don't think they know what it means.

41:15.160 --> 41:16.800
Well, it sounds like a safety poll.

41:16.800 --> 41:17.640
Like what?

41:17.640 --> 41:20.080
Like AI safety is sooner or later.

41:20.080 --> 41:21.160
Let me ask this.

41:22.760 --> 41:25.800
Show of hands, and I think I'm gonna know the show of hands.

41:25.800 --> 41:28.080
I think it's me and Stephanie and Crystal and Jen

41:28.080 --> 41:29.080
on two sides.

41:29.960 --> 41:33.000
Could you actually imagine the end

41:33.000 --> 41:34.640
of all living things on earth?

41:37.160 --> 41:38.880
Because of AI or in general?

41:38.880 --> 41:40.440
Yes, because of AI.

41:40.440 --> 41:41.400
Because of AI.

41:44.120 --> 41:45.920
No, I can't.

41:45.920 --> 41:47.600
I mean, can't.

41:47.600 --> 41:50.200
No, I mean, just because like I know how to use it.

41:50.200 --> 41:53.440
Like I know how to use it for like,

41:53.440 --> 41:57.600
I just know how to be radical with it.

41:57.600 --> 41:58.440
And I know how.

41:58.440 --> 42:01.520
The concern is when it starts using itself, right?

42:01.520 --> 42:03.840
When it starts recursively improving itself,

42:03.840 --> 42:06.280
setting its own goals, becoming its own agent.

42:07.280 --> 42:09.640
You know, it goes off on its own

42:09.640 --> 42:11.600
and it's no longer checking with us.

42:11.600 --> 42:15.440
That to me is very conceivable, if not likely.

42:15.440 --> 42:17.680
But how is it gonna do it though?

42:17.680 --> 42:20.840
Like, and this is where we need to get very specific

42:20.840 --> 42:21.960
about the tools.

42:21.960 --> 42:24.960
Like, okay, so what tools are we specifically talking about?

42:24.960 --> 42:26.320
Like what AI?

42:26.320 --> 42:27.880
Like mid-journey, for instance.

42:27.880 --> 42:31.840
Like mid-journey cannot create problems on its own.

42:31.840 --> 42:33.400
Totally fine, not a problem.

42:33.400 --> 42:35.640
And anything that's out there that consumers are using

42:35.640 --> 42:36.760
is not a problem at all.

42:36.760 --> 42:39.360
It's the 1% of the frontier level work

42:39.360 --> 42:42.360
that open AI, Google, Microsoft,

42:42.360 --> 42:44.880
DeepMind, Anthropic are doing

42:44.880 --> 42:48.360
that is pushing the frontier boundary of the systems

42:48.360 --> 42:52.000
where it starts to recursively improve itself.

42:52.000 --> 42:55.280
And nobody knows what happens after that point.

42:55.280 --> 42:57.160
So, yeah, Crystal.

42:57.160 --> 42:58.640
The two things that I think about,

42:58.640 --> 43:02.080
I have a hard time thinking humans become extinct.

43:02.080 --> 43:03.720
That's like, I have a hard time there.

43:04.280 --> 43:07.800
Where I can see is questioning,

43:07.800 --> 43:09.680
what does it mean to be human?

43:09.680 --> 43:11.760
And Stephanie and I had a conversation around this

43:11.760 --> 43:16.760
because at some point, do we make generative AI so smart

43:17.000 --> 43:19.160
or kind of robots or whatever

43:19.160 --> 43:23.040
that they become citizens, humans, et cetera?

43:23.040 --> 43:24.600
And then you have this,

43:24.600 --> 43:28.160
then you have a kind of question on who makes decisions.

43:28.160 --> 43:30.360
Because if generative AI keeps getting smarter,

43:30.360 --> 43:31.600
then you can see over time,

43:31.600 --> 43:34.600
humans kind of move into a different role within society.

43:34.600 --> 43:39.600
So I can see some interesting things happening

43:40.120 --> 43:45.040
if you look into the future of AI becoming human

43:45.040 --> 43:47.600
or how we define human is one aspect.

43:47.600 --> 43:49.600
But the thing I keep thinking that generative AI

43:49.600 --> 43:54.080
can do really well more quickly is cause absolute chaos.

43:54.080 --> 43:56.720
Because you have disinformation, lies,

43:56.720 --> 43:58.520
and then you lose trust in society.

43:58.520 --> 44:00.480
And once you lose trust in society,

44:00.520 --> 44:01.960
that's where a lot of bad things happen.

44:01.960 --> 44:04.320
That's where war happens.

44:04.320 --> 44:07.040
You have groups of people, again,

44:07.040 --> 44:09.200
fighting different interpretations.

44:09.200 --> 44:12.880
And so the world I see that's more plausible more quickly

44:12.880 --> 44:17.040
is just sheer chaos with disinformation

44:17.040 --> 44:18.960
because of generative AI.

44:18.960 --> 44:22.240
And I don't disagree with any of that stuff,

44:22.240 --> 44:24.520
but I just wanna pick at it a little bit further, right?

44:24.520 --> 44:26.400
So with Crystal and Jen,

44:26.400 --> 44:29.440
so you guys watched the first episode of the podcast

44:29.440 --> 44:31.440
where you know that there's this 22-word statement

44:31.440 --> 44:34.040
that everybody in AI put out that says

44:34.040 --> 44:37.880
that artificial intelligence is an extinction risk

44:37.880 --> 44:41.120
along the lines of pandemic and nuclear war, right?

44:41.120 --> 44:46.120
And so my question is if they're saying it,

44:47.560 --> 44:50.000
if other people, if the literal people

44:50.000 --> 44:52.000
who are making it are saying it,

44:52.000 --> 44:57.000
why do you think you and the 99% of the public

44:58.000 --> 45:03.000
is like, ah, yeah, but like they don't really mean it.

45:03.480 --> 45:05.080
Like they said extinction,

45:05.080 --> 45:07.040
but they really meant something else.

45:09.040 --> 45:14.040
Like I just, I feel like I could be extinct without it.

45:15.800 --> 45:17.240
Like who I am as a person,

45:17.240 --> 45:21.720
like I could be extinct without it, you know?

45:21.720 --> 45:26.720
Like I was like my people, my culture, you know,

45:27.680 --> 45:30.000
without technology, I mean, you know,

45:30.000 --> 45:35.000
literally they were using colonialism and racism

45:35.080 --> 45:38.400
and they were using black and brown bodies

45:38.400 --> 45:43.400
as they were harvesting like enslaved people,

45:43.400 --> 45:46.400
in terms of, so I feel like it's gonna take more

45:46.400 --> 45:49.680
than technology, more than technology

45:49.680 --> 45:53.240
to like make us extinct as a people.

45:53.240 --> 45:56.200
I still feel like, you know, black people,

45:56.200 --> 46:00.280
multi-multiply marginalized people, like we will never,

46:00.280 --> 46:03.680
like nothing will be able to like overpower us

46:03.680 --> 46:06.560
because of what we've been able to endure

46:06.560 --> 46:09.560
and what we've been able to survive.

46:09.560 --> 46:14.560
So for me, I'm like, okay, come at me AI, like, you know,

46:14.640 --> 46:17.360
you know, it's just like, like, come at me.

46:17.360 --> 46:20.480
Throwing down the, she's throwing down the bell.

46:20.480 --> 46:24.160
Like, come at me tech bros, like, I got you, like,

46:24.160 --> 46:27.440
there's no way that you can like completely like,

46:28.560 --> 46:30.480
you know, erase me because like I've been through

46:30.480 --> 46:34.760
so much more than technology can, you know.

46:37.320 --> 46:38.680
Yeah.

46:38.680 --> 46:40.560
That's kind of like where I'm at.

46:41.520 --> 46:42.360
Yeah.

46:42.360 --> 46:44.480
I think it's generally hard to think

46:44.480 --> 46:48.080
about existential problems, like,

46:48.120 --> 46:50.280
and it's really hard then to think

46:50.280 --> 46:53.720
about existential problems with your kids.

46:53.720 --> 46:56.400
Like, as it relates to your kids because,

46:56.400 --> 47:00.040
and I love your shirt, Jen, for free and human.

47:00.040 --> 47:03.440
And it's like, maybe that could be like the,

47:03.440 --> 47:07.360
like, is it like a combo of raising good humans

47:07.360 --> 47:12.360
and just raising, I mean, again,

47:14.440 --> 47:16.200
is we haven't read Mo Galback's book,

47:16.200 --> 47:17.560
Scary Smiles Through Day to Play,

47:17.560 --> 47:19.600
called the AI, we have malware like cobblers

47:19.600 --> 47:22.160
and we should raise them like they raise our kids.

47:22.160 --> 47:26.040
So we kind of, we show them what it means

47:26.040 --> 47:27.240
to be a good person.

47:27.240 --> 47:30.800
Like, what do we mean when we're a family person, right?

47:30.800 --> 47:33.080
What do we mean when we're saying, I think human?

47:33.080 --> 47:36.360
And how all the, you know, the interactions

47:36.360 --> 47:38.240
you've had in real life, you know,

47:38.240 --> 47:40.800
digitally living with that face.

47:40.800 --> 47:41.920
Do you feel sorry?

47:41.920 --> 47:45.120
Do you feel mean or do you try and do better?

47:45.160 --> 47:49.600
Like, if that's, well, I don't know.

47:49.600 --> 47:53.160
Like, John, even in that philosophy we talked about,

47:53.160 --> 47:56.200
like, as humans, we can agree on what we mean

47:56.200 --> 47:58.080
as to be a good human.

47:58.080 --> 48:02.520
And then, you know, so if we can agree as humans

48:02.520 --> 48:04.680
what it means to be good,

48:04.680 --> 48:08.160
how will we going to teach a toddler?

48:08.160 --> 48:10.200
Me do though, me do though.

48:11.280 --> 48:13.680
If the systems are set,

48:13.680 --> 48:17.880
and the intelligence is set to be generatory,

48:17.880 --> 48:19.200
not to be generatory,

48:20.520 --> 48:22.800
not to be generate, right?

48:22.800 --> 48:25.480
I get what generative and other way it's like,

48:25.480 --> 48:26.720
let's just break it down.

48:26.720 --> 48:30.800
Like, if it is supposed to, like,

48:30.800 --> 48:35.800
it's needed to create and not be destructive,

48:36.160 --> 48:39.920
then let's have more people do that.

48:39.920 --> 48:42.080
Well, I don't mind a real general,

48:42.080 --> 48:43.680
but some things that just went into that,

48:43.680 --> 48:45.800
like, you know, just more,

48:45.800 --> 48:47.280
there's more inputs,

48:47.280 --> 48:51.160
but more, more people

48:51.160 --> 48:54.160
that are this one, two, kind of that.

48:54.160 --> 48:55.560
Well, in some ways,

48:55.560 --> 48:57.480
that's why I don't stop using AI,

48:57.480 --> 48:58.880
like, let's get generative,

48:58.880 --> 49:00.480
like, let's get that generative,

49:00.480 --> 49:02.120
that I particularly worry about.

49:02.120 --> 49:06.120
Let's get more people using it in a good way.

49:06.120 --> 49:09.440
I'm a little bit more bullish, like, John,

49:09.440 --> 49:13.080
I believe that you have 250 people that have signed this,

49:13.080 --> 49:14.560
oh, who own a lot of the companies

49:14.560 --> 49:16.520
that are working on this.

49:16.520 --> 49:18.880
So the people that are saying it's a risk

49:18.880 --> 49:21.640
also have the power to put guardrails

49:21.640 --> 49:24.000
and to service leaders, and I urge you.

49:24.000 --> 49:25.360
And I also believe that,

49:25.360 --> 49:27.280
so if they say that's a problem

49:27.280 --> 49:29.640
and we want to do something about it,

49:29.640 --> 49:30.480
that's a good thing.

49:30.480 --> 49:32.080
You've got, you know, you know,

49:32.080 --> 49:33.920
you're always going to have the regulation come in.

49:33.920 --> 49:36.120
Governments tend to be slower,

49:36.120 --> 49:38.200
but I do believe you've got people that can stop it.

49:38.200 --> 49:40.400
It's in a toddler stage.

49:40.400 --> 49:44.080
As women, as moms, it's all of our responsibility

49:44.080 --> 49:46.240
to make sure we have diversity of thought

49:46.240 --> 49:48.920
in all the products that we're building,

49:48.920 --> 49:51.320
in our marketing that we're doing,

49:51.320 --> 49:54.400
making sure that we're hiring the right people for the roles,

49:54.400 --> 49:59.320
and so we're not powerless if we all are raising good humans

49:59.320 --> 50:00.560
and we're being good humans,

50:00.560 --> 50:03.160
and we're making sure we're getting the right voices

50:03.160 --> 50:06.360
at the table, then I believe that we can,

50:06.360 --> 50:07.400
we're not going to stop it,

50:07.400 --> 50:10.000
but I think we can use it for generative

50:10.000 --> 50:11.640
and not degenerative purposes.

50:11.640 --> 50:14.040
You're always going to have the nefarious folks

50:14.040 --> 50:15.480
trying to do things,

50:15.480 --> 50:17.680
but I do believe that these tech companies

50:17.680 --> 50:20.280
know what they're doing and because they're funding.

50:21.280 --> 50:23.440
And if they can actually start to say,

50:23.440 --> 50:25.240
why don't we get a group together?

50:26.840 --> 50:27.680
You can do something.

50:27.680 --> 50:28.800
So I'm bullish.

50:29.960 --> 50:32.240
I can see where it can go in dangerous places,

50:32.240 --> 50:35.520
but I have to believe that humans can be better,

50:35.520 --> 50:38.000
smarter to solve this.

50:38.000 --> 50:40.320
Okay, so what do you think?

50:40.320 --> 50:43.160
I've watched a ton of AI safety podcasts,

50:43.160 --> 50:46.800
but I have never seen one quite like that before.

50:46.800 --> 50:48.600
Real quick, I think Stephanie is right,

50:48.600 --> 50:51.240
that AI safety is the wrong term.

50:51.240 --> 50:53.960
It's boring and it's soft.

50:53.960 --> 50:56.680
Let's talk about alternative terms to AI safety

50:56.680 --> 51:00.520
in the comments, and honestly, I was surprised

51:00.520 --> 51:02.600
and maybe even a bit disappointed

51:02.600 --> 51:07.200
that all three moms did not fully buy my case

51:07.200 --> 51:09.200
even after having heard it,

51:09.200 --> 51:11.800
that human extinction from AI

51:11.800 --> 51:14.600
is the most urgent threat we face.

51:15.640 --> 51:18.260
I did not fully make the sale.

51:19.840 --> 51:22.800
That was a little hard to take, but it's okay.

51:22.800 --> 51:24.440
This takes time.

51:25.400 --> 51:27.840
Quickly, my own mother, who I love dearly

51:27.840 --> 51:30.800
and I think is an incredibly smart woman,

51:30.800 --> 51:33.920
watched the first three episodes of this podcast

51:33.920 --> 51:36.720
and was still telling me she didn't really fully get it,

51:36.720 --> 51:38.200
but then after episode four,

51:38.200 --> 51:41.160
she told me it started to make sense.

51:41.160 --> 51:43.960
So that's how this is gonna go.

51:43.960 --> 51:46.800
We, the ambassadors of this message,

51:46.800 --> 51:48.840
let us not be discouraged

51:48.840 --> 51:52.040
when someone is not immediately convinced

51:52.040 --> 51:53.600
of what you're saying.

51:53.600 --> 51:56.400
The idea of no life on earth at all

51:56.400 --> 51:59.360
is something that is very difficult to process

51:59.400 --> 52:01.640
for all of us, myself included,

52:01.640 --> 52:04.840
but if we are to have a chance,

52:04.840 --> 52:07.120
if our children are to have a chance,

52:07.120 --> 52:09.360
we must go through the process,

52:09.360 --> 52:12.760
we must try to save ourselves.

52:13.800 --> 52:15.960
Thank you so much for watching.

52:15.960 --> 52:18.800
Next week, we're gonna start pointing fingers

52:18.800 --> 52:20.680
and naming names.

52:20.680 --> 52:23.720
I'm ready to blame some people for all this.

52:23.720 --> 52:25.240
Our next episode will be called

52:25.240 --> 52:27.740
the top three doomers in AI,

52:27.740 --> 52:29.380
and here's a hint,

52:29.380 --> 52:32.820
they are not AI safety researchers at all.

52:34.660 --> 52:37.020
For Humanity, I'm John Sherman.

52:37.020 --> 52:38.620
I'll see you back here next week.

