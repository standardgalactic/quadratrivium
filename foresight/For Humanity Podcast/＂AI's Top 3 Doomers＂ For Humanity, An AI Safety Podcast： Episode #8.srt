1
00:00:00,000 --> 00:00:06,500
GPT-4, you know, not a risk like you're talking about there, but how sure are we that GPT-9 won't be?

2
00:00:06,500 --> 00:00:14,500
How sure are we that Chad GPT-9 won't murder your whole family and mine, he asks?

3
00:00:14,500 --> 00:00:18,500
The bad case, and I think this is like important to say, is like lights out for all of us.

4
00:00:18,500 --> 00:00:24,500
The real doomers are the ones that could bring the fucking doom, yeah?

5
00:00:24,500 --> 00:00:37,500
You know, my chance that something goes, you know, really quite catastrophically wrong on this scale of, you know, human civilization, you know, might be somewhere between 10 and 25%.

6
00:00:37,500 --> 00:00:42,500
Who the fuck do you think you are?

7
00:00:42,500 --> 00:00:50,500
Who the fuck do you think gave you permission to endanger the lives of 8 billion of your neighbors?

8
00:00:50,500 --> 00:01:03,500
Seriously, who in their right mind would take a job at a company where your work creates a 10 to 25% chance of killing their family and all of their neighbors?

9
00:01:03,500 --> 00:01:07,500
Oh, and all the rest of the people in the whole world too.

10
00:01:07,500 --> 00:01:13,500
Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.

11
00:01:13,500 --> 00:01:16,500
I would like to see much, much more happening.

12
00:01:16,500 --> 00:01:24,500
Oh wow, Sam should try to find somebody who knows somebody who's in charge at OpenAI who can fix this.

13
00:01:24,500 --> 00:01:25,500
Come on.

14
00:01:25,500 --> 00:01:29,500
Some young person goes to a school and starts shooting people.

15
00:01:29,500 --> 00:01:33,500
You blame video games, right?

16
00:01:33,500 --> 00:01:39,500
Back in the old days people would blame comic books, they would blame jazz, they would blame TV, they would blame movies.

17
00:01:39,500 --> 00:01:42,500
Yeah, yeah, it's just like video games and school shootings.

18
00:01:42,500 --> 00:01:45,500
It's just like comic books or jazz.

19
00:01:45,500 --> 00:01:47,500
Get the fuck out of here.

20
00:01:47,500 --> 00:01:49,500
But who determines the goal?

21
00:01:49,500 --> 00:01:50,500
We do.

22
00:01:50,500 --> 00:01:52,500
You know better than that.

23
00:01:52,500 --> 00:01:57,500
You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals.

24
00:01:57,500 --> 00:02:04,500
You know that a human can set goals and AI systems must then set sub-goals to achieve those first goals.

25
00:02:04,500 --> 00:02:15,500
And you know those sub-goals when unaligned with human values and obscured from view hidden in a black box system as they are could kill us all.

26
00:02:22,500 --> 00:02:30,500
Welcome to For Humanity, an AI safety podcast, episode 8, AI's Top 3 Doomers.

27
00:02:30,500 --> 00:02:32,500
I'm John Sherman, your host.

28
00:02:32,500 --> 00:02:33,500
Thank you so much for joining me.

29
00:02:33,500 --> 00:02:36,500
This is the AI safety podcast for the general public.

30
00:02:36,500 --> 00:02:38,500
No tech background required.

31
00:02:38,500 --> 00:02:43,500
This podcast, as you know, is solely about the threat of human extinction from artificial intelligence.

32
00:02:43,500 --> 00:02:46,500
Please hit like and subscribe, tell a friend about the show.

33
00:02:46,500 --> 00:02:52,500
And if you're on X, follow us at For Humanity Pod and leave a comment if you will.

34
00:02:52,500 --> 00:02:54,500
I really, really appreciate those comments.

35
00:02:54,500 --> 00:02:59,500
And reach out to me by email if you'd like to at For Humanity podcast at gmail.com.

36
00:02:59,500 --> 00:03:00,500
I'd love to hear from you.

37
00:03:00,500 --> 00:03:04,500
Many of you have been reaching out to me and I want to say thank you for doing that.

38
00:03:04,500 --> 00:03:15,500
We are building a community of people here, a community of people who are unafraid to be ambassadors of AI extinction risk in Falkander.

39
00:03:15,500 --> 00:03:23,500
This show is a lot of work and every time one of you reaches out to me with some words of encouragement, it means a great deal.

40
00:03:23,500 --> 00:03:24,500
It really does.

41
00:03:24,500 --> 00:03:25,500
So thank you.

42
00:03:25,500 --> 00:03:31,500
I'm going to end today's show on something really positive, new and exciting coming up for 2024.

43
00:03:31,500 --> 00:03:35,500
But first, the moment we are in.

44
00:03:35,500 --> 00:03:40,500
Every day, more and more people are waking up to the grim truth.

45
00:03:40,500 --> 00:03:51,500
A bunch of 30-somethings in Silicon Valley are in a race to build technology that can cure all of our problems or kill us all before it has a chance to do that.

46
00:03:51,500 --> 00:03:56,500
Those both sound crazy, but they are both literally true.

47
00:03:56,500 --> 00:04:02,500
The future, utopia or doom will be decided by us this decade.

48
00:04:02,500 --> 00:04:10,500
Everything I've just said is almost consensus view among most of the AI thought leaders of all different stripes.

49
00:04:10,500 --> 00:04:19,500
I'm not religious at all, but I was thrilled this week to see Pope Francis come out with a forceful call for AI safety.

50
00:04:19,500 --> 00:04:22,500
Not sure if you saw it, check out this Washington Post headline.

51
00:04:22,500 --> 00:04:24,500
Warning of risk to survival.

52
00:04:24,500 --> 00:04:28,500
Pope calls for global treaty on AI.

53
00:04:28,500 --> 00:04:29,500
Wow.

54
00:04:29,500 --> 00:04:30,500
You got to love it.

55
00:04:30,500 --> 00:04:38,500
In the Pope's statement on AI, he said, I think very importantly and on target for our show topic today, the Pope wrote,

56
00:04:38,500 --> 00:04:48,500
artificial intelligence is not the responsibility of a few, but of the entire human family.

57
00:04:48,500 --> 00:04:54,500
Again, I don't believe in religion, but amen, Pope Francis, he is right.

58
00:04:54,500 --> 00:05:02,500
A handful of tech people responsible only to their shareholders cannot decide our survival.

59
00:05:02,500 --> 00:05:06,500
There are nearly 1.5 billion Catholics on earth.

60
00:05:06,500 --> 00:05:10,500
Catholics of the world hear Pope Francis.

61
00:05:10,500 --> 00:05:14,500
He's saying human survival is at risk from AI.

62
00:05:14,500 --> 00:05:17,500
Let's be clear.

63
00:05:17,500 --> 00:05:19,500
This is not metaphorical.

64
00:05:19,500 --> 00:05:24,500
He doesn't mean something like survival, but nicer.

65
00:05:24,500 --> 00:05:32,500
The leading academic researchers and the leaders of all the big tech AI companies are openly talking about their work.

66
00:05:32,500 --> 00:05:36,500
Killing everyone on earth knows survivors.

67
00:05:36,500 --> 00:05:40,500
That is literally what they are talking about.

68
00:05:40,500 --> 00:05:47,500
And the consensus of all the leading academic researchers on this subject is it's very likely, if it happens to happen,

69
00:05:47,500 --> 00:05:54,500
that it will happen within the next 2 to 10 years.

70
00:05:54,500 --> 00:06:02,500
If every Catholic believed the Pope's dire warning, that would be truly wonderful.

71
00:06:02,500 --> 00:06:09,500
If every major world religious leader came out with a similar statement of grave concern, that would be even better.

72
00:06:09,500 --> 00:06:18,500
What a potentially powerful, immediate impact that could have on global, worldwide opinion.

73
00:06:18,500 --> 00:06:27,500
AI could still cure cancer and poverty, solve climate change, even if we only developed narrow artificial intelligence.

74
00:06:27,500 --> 00:06:30,500
But we're not doing it that way.

75
00:06:30,500 --> 00:06:39,500
We're in a race to build artificial general intelligence, an AI that's better than humans in a wide variety of skills and areas of expertise.

76
00:06:39,500 --> 00:06:42,500
We are racing towards our own doom.

77
00:06:42,500 --> 00:06:46,500
That's what most of the leading AI safety researchers are saying.

78
00:06:46,500 --> 00:06:52,500
That's what the godfathers and creators of AI are shouting.

79
00:06:52,500 --> 00:06:55,500
They're saying slow down.

80
00:06:55,500 --> 00:06:57,500
They're saying stop.

81
00:06:57,500 --> 00:06:59,500
But the race continues.

82
00:06:59,500 --> 00:07:12,500
A handful of people are taking risks for the futures of 8 billion people without any consent whatsoever.

83
00:07:12,500 --> 00:07:15,500
Civil society is based on consent, right?

84
00:07:15,500 --> 00:07:17,500
You can't touch someone without their consent.

85
00:07:17,500 --> 00:07:25,500
Every time you sign something all day long and you're signing away your consent, we do it for the most trivial things and the most important things.

86
00:07:25,500 --> 00:07:34,500
If I want to cut down a tree in my yard that affects my neighbor's yard, I need to get my neighbor's consent.

87
00:07:34,500 --> 00:07:40,500
The big AI companies have no fucking right at all to do what they're doing without anyone's consent.

88
00:07:40,500 --> 00:07:49,500
We, the public, have not given them our consent for this at all.

89
00:07:50,500 --> 00:07:55,500
So on today's show, the top three doomers in AI.

90
00:07:55,500 --> 00:08:00,500
We're going to name names, point fingers, and lay down the blame.

91
00:08:00,500 --> 00:08:13,500
These three men are unconsensually risking doom for you, your family, me, my family, and every other living thing on this planet.

92
00:08:13,500 --> 00:08:17,500
The term doomer is misused.

93
00:08:17,500 --> 00:08:23,500
AI risk deniers are the real doomers.

94
00:08:23,500 --> 00:08:25,500
Let's be very clear.

95
00:08:25,500 --> 00:08:32,500
Dumerism equals AI risk denialism.

96
00:08:32,500 --> 00:08:39,500
The doomers are not the AI safety researchers like Elias or Yadkowski and Max Tegmark,

97
00:08:39,500 --> 00:08:44,500
or the converts like Jeffrey Hinton and Yoshua Benjiro.

98
00:08:44,500 --> 00:08:48,500
Those are heroes trying to save us.

99
00:08:48,500 --> 00:08:55,500
The real doomers are the ones that could bring the fucking doom, yeah?

100
00:08:55,500 --> 00:08:59,500
Yeah, that makes a whole lot more sense to me.

101
00:08:59,500 --> 00:09:05,500
So on today's show, the top three doomers in AI.

102
00:09:05,500 --> 00:09:09,500
These are the fucking assholes who are risking our doom.

103
00:09:09,500 --> 00:09:13,500
They're leading the charge to make it happen as fast as possible.

104
00:09:13,500 --> 00:09:25,500
So coming in at number three, the number three doomer in AI, it's anthropic CEO Dario Amade.

105
00:09:25,500 --> 00:09:32,500
Yeah.

106
00:09:32,500 --> 00:09:39,500
Dario's story is heartbreaking for those gravely concerned about AI risk.

107
00:09:39,500 --> 00:09:49,500
It's a tale of a life rooted in good intentions, tragically broken, corrupted by market forces, ego, and greed.

108
00:09:49,500 --> 00:09:58,500
Dario Amade, he worked at OpenAI from 2015 to 2020, leading the teams that built ChatGPT2 and ChatGPT3.

109
00:09:58,500 --> 00:10:08,500
He is, by all accounts, an incredible computer scientist, and honestly, he seems like a nice, very likable guy in the interviews you're about to see.

110
00:10:08,500 --> 00:10:14,500
He even runs his company with his sister. Aw.

111
00:10:14,500 --> 00:10:25,500
In 2020, though, Dario left OpenAI with some others to start their own company, Anthropic, that would be far more focused on safety than OpenAI.

112
00:10:25,500 --> 00:10:38,500
Today, Anthropic has only 160 employees, and it is widely considered the third most important AI company on the planet, just behind Microsoft's OpenAI and Google's DeepMind.

113
00:10:38,500 --> 00:10:50,500
AI safety was prized at Anthropic originally. The company was all about safety in a meaningful and genuine way, but things changed.

114
00:10:50,500 --> 00:11:01,500
Today, Anthropic has 160 humans whose daily work threatens the total slaughter of their 8 billion neighbors.

115
00:11:01,500 --> 00:11:06,500
It was a long New York Times article about Dario and Anthropic in July.

116
00:11:06,500 --> 00:11:15,500
In it, company leaders say to work on safety at the frontier level, you need to build systems that are at the dangerous frontier level.

117
00:11:15,500 --> 00:11:21,500
I believe there is some truth in that. So they are doing it.

118
00:11:21,500 --> 00:11:33,500
They got a $5 billion investment to build an AI model 10 times more capable than today's most capable models.

119
00:11:33,500 --> 00:11:41,500
That is quite literally the dangerous frontier work, the 1% that could kill us all.

120
00:11:42,500 --> 00:11:50,500
10 times smarter than today's AI could well be AGI, pushing us past the singularity.

121
00:11:50,500 --> 00:11:55,500
Well, maybe it won't.

122
00:11:55,500 --> 00:12:09,500
Here's Anthropic CEO Dario Amadei on the Logan Bartlett podcast, a great show, on how Anthropic changed after OpenAI released chat GPT to the general public.

123
00:12:09,500 --> 00:12:23,500
I didn't want our first act on the public stage after we put so much effort into being responsible to accelerate things so greatly.

124
00:12:23,500 --> 00:12:28,500
I generally feel like we made the right call there. I think it's actually pretty debatable.

125
00:12:28,500 --> 00:12:34,500
There's many pros, many cons, but I think overall we made the right call.

126
00:12:34,500 --> 00:12:41,500
And then certainly as soon as the other models were out and the gun had been fired, then we started putting these things out.

127
00:12:41,500 --> 00:12:50,500
We're like, okay, all right, now there's definitely a market in this people, people know about it, and so we should get out ahead.

128
00:12:50,500 --> 00:12:56,500
And indeed we've managed to put ourselves among the top two or three players in this space.

129
00:12:56,500 --> 00:13:06,500
Was that gun being fired and chat GPT sort of taking off? Was that similar to the maybe fear that you had of like, hey, this might start a race?

130
00:13:06,500 --> 00:13:20,500
Yeah, yeah, similar and in fact more so. So I think we saw it with Google's reaction to it, that there was definitely just judging from the public statements, a sense of fear and existential threat.

131
00:13:20,500 --> 00:13:26,500
And I think they responded in a very economically rational way. I don't blame them for it at all.

132
00:13:26,500 --> 00:13:34,500
But you put the two things together and it really created an environment where things were racing forward very quickly.

133
00:13:34,500 --> 00:13:41,500
And look, I love technology as much as the next person. There was something like super exciting about the whole make them dance.

134
00:13:41,500 --> 00:13:49,500
Oh, we're responding with something. I can get just as excited about this as everyone, but given the rate at which the technology is progressing,

135
00:13:49,500 --> 00:13:58,500
you know, there was a worrying aspect about this as well. And so in this case, I'm at least at least on balance clad that, you know, we weren't the ones who fired that starting gun.

136
00:13:58,500 --> 00:14:10,500
So he left open AI to start a new safer lab. But then once open AI released chat GPT, he gave up on being safer.

137
00:14:10,500 --> 00:14:15,500
He openly admits this. And he joined the arms race.

138
00:14:15,500 --> 00:14:26,500
Right there at the end of that sound bite we just played, he says he gets some consolation for not being the one to fire the gun to start the AGI arms race.

139
00:14:26,500 --> 00:14:31,500
That is pathetic. That's called caving in.

140
00:14:31,500 --> 00:14:38,500
When the AI safety company said, fuck it, we need to join the arms race or be left behind. So we're in.

141
00:14:38,500 --> 00:14:42,500
Humanities chances took a big hit.

142
00:14:42,500 --> 00:14:51,500
What happened that anthropic is a sickening window into the market forces at play here. Listen to me.

143
00:14:51,500 --> 00:14:58,500
The rush to make money and be first is far more important to the human race than ensuring survival.

144
00:14:58,500 --> 00:15:13,500
Full stop. That is simply the fact of how we are currently living. That is our status quo. That is what the actions of our society currently undeniably show.

145
00:15:13,500 --> 00:15:21,500
Our plan is to be so rich and so dead.

146
00:15:21,500 --> 00:15:28,500
So why is Dario Amadei the number three ranked AI doomer in the world?

147
00:15:28,500 --> 00:15:32,500
He's making the doom happen.

148
00:15:32,500 --> 00:15:38,500
We played the sound bite I'm going to play for you now a few shows ago. Listen to more of it here now.

149
00:15:38,500 --> 00:15:47,500
Here's Dario with Logan Bartlett giving the percentage chance the P doom that the work he's doing ends all life on earth.

150
00:15:47,500 --> 00:15:52,500
Do you think about percentage chance doom?

151
00:15:52,500 --> 00:16:04,500
I think it's popular to give these percentage numbers and the truth is that I'm not sure it's easy to put a number to it and if you forced me to it would fluctuate all the time.

152
00:16:04,500 --> 00:16:25,500
I think I've often said that my chance that something goes really quite catastrophically wrong on the scale of human civilization might be somewhere between 10 and 25 percent when you put together the risk of something going wrong with the model itself,

153
00:16:25,500 --> 00:16:40,500
with something going wrong with human people or organizations or nation states misusing the model or it kind of inducing conflict among them or just some way in which kind of society can't handle it.

154
00:16:40,500 --> 00:16:50,500
That said, what that means is that there's a 75 to 90 percent chance that this technology is developed and everything goes fine.

155
00:16:50,500 --> 00:17:03,500
If you don't believe that AI can kill us all very soon, ask yourself, what is it that you know that the CEO of the third most important AI company on earth doesn't know?

156
00:17:03,500 --> 00:17:11,500
If someone in your life doesn't believe that AI can kill them, play that soundbite for them.

157
00:17:11,500 --> 00:17:20,500
Here's Logan Bartlett asking a great question and Dario not answering it at all. It's like he doesn't even see the question.

158
00:17:20,500 --> 00:17:33,500
There's a box article that said something to the effect of an employee predicted there was a 20 percent chance that a rogue AI would destroy humanity within the next decade to the reporter, I guess, that was around.

159
00:17:33,500 --> 00:17:41,500
Does all this stuff weigh heavily on the organization on a daily basis or is it mostly consistent with a normal startup for the average employee?

160
00:17:41,500 --> 00:17:48,500
Yeah, so I don't know. I'll give my own experience and it's kind of the same thing that I recommend to others.

161
00:17:48,500 --> 00:18:06,500
So, you know, I really freaked out about this stuff in 2018 or 2019 or so when, you know, when I first believed that, you know, turned out to be at least in some ways correct, that the models would scale very rapidly and, you know, they would have this importance to the world.

162
00:18:06,500 --> 00:18:23,500
I'm sure Dario Amadei is a lot like the few thousand people working on the most dangerous frontier AI capabilities work. Incredibly smart, probably even well intentioned, but completely blind to the notion of consent.

163
00:18:23,500 --> 00:18:33,500
There can be no such thing as informed consent in this, not even the big AI companies know what AGI will do, when or how.

164
00:18:33,500 --> 00:18:43,500
And yet they race on, never even attempting to get anyone's consent. Dario mentions he knows the importance of his work to the world.

165
00:18:43,500 --> 00:18:52,500
So to be blunt, Mr. Amadei, who the fuck do you think you are?

166
00:18:52,500 --> 00:19:00,500
Who the fuck do you think gave you permission to endanger the lives of eight billion of your neighbors?

167
00:19:00,500 --> 00:19:13,500
Seriously, who in their right mind would take a job at a company where your work creates a 10 to 25% chance of killing their family and all of their neighbors?

168
00:19:13,500 --> 00:19:25,500
Oh, and all the rest of the people in the whole world too. What the fuck are we talking about here? Who the fuck do you think you are, sir?

169
00:19:25,500 --> 00:19:29,500
Okay, I'm gonna breathe for a moment.

170
00:19:29,500 --> 00:19:37,500
On to the number two doomer in AI. Oh man, this guy is just the absolute worst.

171
00:19:37,500 --> 00:19:47,500
The number two doomer in AI is Suicide Cult Leader and Director of AI Research at META, Yann LeCun.

172
00:19:55,500 --> 00:20:04,500
Yann LeCun is simply stunningly awful. Here is Yann at the World Science Festival in late November.

173
00:20:04,500 --> 00:20:09,500
You can have a very intelligent system that has no desire to dominate at all.

174
00:20:09,500 --> 00:20:17,500
And so the way we will design those systems is to be smart. In other words, you give them a goal, they can solve that goal for you.

175
00:20:17,500 --> 00:20:22,500
But who determines the goal? We do.

176
00:20:22,500 --> 00:20:29,500
You know better than that. You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals.

177
00:20:29,500 --> 00:20:36,500
You know that a human can set goals and AI systems must then set sub-goals to achieve those first goals.

178
00:20:36,500 --> 00:20:47,500
And you know those sub-goals when unaligned with human values and obscured from view hidden in a black box system as they are, could kill us all.

179
00:20:47,500 --> 00:20:57,500
But Yann LeCun, this NYU professor and META department leader, somehow has enough spare time on his hands to sit on ex-formerly Twitter

180
00:20:57,500 --> 00:21:05,500
and spew AI extinction risk denial bile through a fire hose.

181
00:21:05,500 --> 00:21:15,500
He not only denies any AI extinction risk whatsoever, he bullies and makes fun of anyone who does, calling them doomers,

182
00:21:15,500 --> 00:21:22,500
calling AI ex-risk sci-fi movie stuff, James Bond movie stuff and more.

183
00:21:22,500 --> 00:21:26,500
If I had Yann LeCun in a public debate, I would shut him down with one question.

184
00:21:26,500 --> 00:21:39,500
Yann, let's say you build AGI and let's say your interpretability sensor, which doesn't even currently exist, tells you that this AGI, which is smarter than any human, is plotting to kill you.

185
00:21:39,500 --> 00:21:44,500
What do you do next?

186
00:21:44,500 --> 00:21:50,500
There is no answer. The only answer is you die. So let's not get there, right?

187
00:21:50,500 --> 00:21:57,500
But Yann isn't here for the hard questions. His go-to move is openly disingenuous.

188
00:21:57,500 --> 00:22:01,500
I have to believe he's too smart to know this isn't bullshit.

189
00:22:01,500 --> 00:22:10,500
Yann LeCun always reaches for historical precedent. Oh, AI is just like the typewriter. It's just another printing press moment.

190
00:22:10,500 --> 00:22:21,500
But as you know, there is nothing in human history, absolutely nothing remotely like the invention of AI and the coming of AGI,

191
00:22:21,500 --> 00:22:27,500
and any attempt to find a historical comparison is genuinely dangerous.

192
00:22:27,500 --> 00:22:31,500
Just listen to the total bullshit that comes from this man's mouth.

193
00:22:31,500 --> 00:22:39,500
It's very easy to attribute cultural phenomena and social phenomena to the new thing that just happened, right?

194
00:22:39,500 --> 00:22:52,500
So if some young person goes to school and starts shooting people, you blame video games, right?

195
00:22:52,500 --> 00:22:59,500
Back in the old days, people would blame comic books, they would blame jazz, they would blame TV, they would blame movies, novels.

196
00:22:59,500 --> 00:23:07,500
The story goes back centuries. Whenever there is a new cultural phenomenon, whenever there is an effect on society,

197
00:23:07,500 --> 00:23:12,500
you blame the latest technology that appears, particularly communication technology.

198
00:23:12,500 --> 00:23:17,500
So it's natural to blame, for example, social networks, not just Facebook,

199
00:23:17,500 --> 00:23:22,500
just any social networks for political polarization, okay? That seems natural.

200
00:23:22,500 --> 00:23:27,500
People shout at each other on social network. That necessarily polarizes people.

201
00:23:27,500 --> 00:23:31,500
A very natural thing to do. That turns out to be completely false.

202
00:23:31,500 --> 00:23:41,500
Yeah, yeah, it's just like video games and school shootings. It's just like comic books or jazz. Get the fuck out of here.

203
00:23:41,500 --> 00:23:50,500
At the World Science Fair, the moderator asks Yana a great question, and his answer is just numbing, completely numbing.

204
00:23:50,500 --> 00:23:52,500
I do not understand this.

205
00:23:52,500 --> 00:23:56,500
For you to say, hey, let's really slow this down.

206
00:23:56,500 --> 00:24:04,500
Is there anything that would happen from these AI developments that would cause you to say, I want to slow this down?

207
00:24:04,500 --> 00:24:09,500
So you want me to imagine a scenario that I don't believe can happen?

208
00:24:09,500 --> 00:24:11,500
Yeah.

209
00:24:11,500 --> 00:24:13,500
Okay.

210
00:24:13,500 --> 00:24:28,500
So it's kind of funny because we're on a panel with a 60-something advocating for progress against a 30-something advocating for conservatism.

211
00:24:28,500 --> 00:24:37,500
Isn't that paradoxical? Anyway, I mean, we can imagine all kinds of catastrophe scenarios.

212
00:24:37,500 --> 00:24:49,500
Every sort of James Bond movie with a supervillain has some sort of catastrophe scenario where someone turns crazy and wants to, you know, eliminate humanity to recreate it to something or take over the world.

213
00:24:49,500 --> 00:24:54,500
All of science fiction is full of that. That's what makes it funny and interesting.

214
00:24:54,500 --> 00:25:00,500
I don't know if you noticed in those clips the facial expression of the guy with the beard two seats down from Jan Lacune.

215
00:25:00,500 --> 00:25:09,500
This guy is amazing. You're about to meet a new AI safety hero, new character alert. Please say hello to AI ethicist Tristan Harris.

216
00:25:09,500 --> 00:25:13,500
Find his stuff on the Internet. He really is incredible. Follow him.

217
00:25:13,500 --> 00:25:20,500
Listen to Tristan Harris due to Jan Lacune, what Lacune always tries to do but fails.

218
00:25:20,500 --> 00:25:26,500
Tristan finds a historical reference that actually makes sense.

219
00:25:26,500 --> 00:25:31,500
If you keep the scaling model, the scaling laws going and you start to get AI that starts to automate scientific processes

220
00:25:31,500 --> 00:25:40,500
where it's generating its own hypotheses and it has its own lab and starts to test those hypotheses and you start getting that kind of AI.

221
00:25:40,500 --> 00:25:44,500
You start getting AI that's making its own scientific discoveries.

222
00:25:44,500 --> 00:25:52,500
We have it going this fast. It feels like her metaphor was it's like the 24th century crashing down on the 21st century.

223
00:25:52,500 --> 00:25:59,500
A metaphor for you is imagine that the 20th century tech was crashing down on 16th century governance.

224
00:25:59,500 --> 00:26:11,500
The 16th century, you've got the king, you've got their advisors, but suddenly television, radio, the telegraph, video games, Nintendo,

225
00:26:11,500 --> 00:26:18,500
and thermonuclear weapons all show up. They just land in your society.

226
00:26:18,500 --> 00:26:25,500
But you're like, call the knights and the knights show up and you're like, what are you going to do?

227
00:26:25,500 --> 00:26:33,500
What are we going to do when technology hundreds of years advanced beyond our current moment is suddenly thrown in our hands?

228
00:26:33,500 --> 00:26:42,500
Nobody has any fucking clue. Just imagine medieval kings and knights waking up one day with nukes and cell phones.

229
00:26:42,500 --> 00:26:54,500
The odds that I'd be sitting here today talking to you like this after that seem very slim, which brings us to the number one doomer in AI.

230
00:26:54,500 --> 00:27:04,500
This was not a hard choice. It's the guy who did everything the original founders of AI swore they would never do.

231
00:27:05,500 --> 00:27:15,500
He chose to release his general AI models open source. He chose to release them on the open internet.

232
00:27:15,500 --> 00:27:21,500
He chose to release them with the ability to write their own code.

233
00:27:21,500 --> 00:27:32,500
This man's fingerprints are on the handle of the Pandora's box that is now open and threatening to murder everyone you've ever met.

234
00:27:32,500 --> 00:27:37,500
And also everyone you've never met too.

235
00:27:37,500 --> 00:27:46,500
He is the human starting gun in our profit fueled global race to mass suicide.

236
00:27:46,500 --> 00:27:52,500
The number one doomer in AI is open AI CEO Sam Altman.

237
00:27:52,500 --> 00:28:04,500
Number one doomer Sam Altman is the most dangerous human to ever live.

238
00:28:04,500 --> 00:28:14,500
I could and will tell you about why but he does a great job all by himself right here in this famous quote from January of this year.

239
00:28:14,500 --> 00:28:20,500
Right after he spent a minute detailing the wonders of the best case scenario for AI.

240
00:28:21,500 --> 00:28:28,500
Sam drop this bad case and I think this is like important to say is like lights out for all of us.

241
00:28:28,500 --> 00:28:32,500
I could end my case for Sam being the number one doomer right there.

242
00:28:32,500 --> 00:28:39,500
Who the fuck works on something they legitimately think could kill everyone on earth.

243
00:28:39,500 --> 00:28:45,500
In a highly controlled governmental military framework weapons of mass destruction are designed and built.

244
00:28:45,500 --> 00:28:53,500
This is not that the sale of a fucking egg is far more regulated than AI right now.

245
00:28:53,500 --> 00:29:02,500
And Sam Altman of Missouri believes he has the right the morning right to do the work he's doing.

246
00:29:02,500 --> 00:29:11,500
How is that fucking possible Sam you are the bringer of the doom Sam you are the fucking doomer.

247
00:29:11,500 --> 00:29:23,500
Not EleIzer not Max not Connor not Paul not Roman you Sam Altman are the number one doomer in AI.

248
00:29:23,500 --> 00:29:36,500
On that same January day Sam went on to talk about his safety concern being mostly with misuse and then he takes a full pot shot at the AI safety community just listen.

249
00:29:37,500 --> 00:29:49,500
I'm more worried about like an accidental misuse case in the short term where you know someone gets a super powerful like it's not like the AI wakes up and decides to be evil.

250
00:29:49,500 --> 00:29:59,500
And I think all of the sort of traditional AI safety thinkers reveal a lot about more about themselves than they mean to when they talk about what they think the AGI is going to be like.

251
00:29:59,500 --> 00:30:03,500
But but I can see the accidental misuse case clearly.

252
00:30:03,500 --> 00:30:04,500
I'm sorry.

253
00:30:04,500 --> 00:30:15,500
Right there the number one doomer in AI had the gall the audacity to take a shot at what he calls this sort of traditional AI safety thinkers.

254
00:30:15,500 --> 00:30:23,500
His burn is somehow that they reveal something clearly off about themselves when they talk about what AGI is going to be like.

255
00:30:23,500 --> 00:30:34,500
Does she mean that EleIzer Yadkowski and Max Tagmark think AGI is going to be bad and mean because they're really bad and mean deep down inside.

256
00:30:34,500 --> 00:30:37,500
What the actual fuck is he even talking about.

257
00:30:37,500 --> 00:30:45,500
AGI will be simply like every other intelligence anywhere that's not human.

258
00:30:45,500 --> 00:30:50,500
It won't share human goals and values.

259
00:30:50,500 --> 00:30:58,500
One more clip from that event here Sam bemoans the need for more safety and alignment work.

260
00:30:58,500 --> 00:31:10,500
From the guy who is the CEO of the leading AI company on Earth, a company that puts its money 99 cents on the dollar on capabilities and one cent on safety work.

261
00:31:10,500 --> 00:31:16,500
Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.

262
00:31:16,500 --> 00:31:19,500
I would like to see much, much more happening.

263
00:31:19,500 --> 00:31:26,500
Oh, wow, Sam should try to find somebody who knows somebody who's in charge at Open AI who can fix this.

264
00:31:26,500 --> 00:31:27,500
Come on.

265
00:31:27,500 --> 00:31:29,500
Honestly, it's so pathetic.

266
00:31:29,500 --> 00:31:30,500
He read.

267
00:31:30,500 --> 00:31:31,500
Wow.

268
00:31:31,500 --> 00:31:33,500
Vannan, it's been a pleasure so much.

269
00:31:33,500 --> 00:31:34,500
Thank you.

270
00:31:34,500 --> 00:31:40,500
Sam Altman, the world's most dangerous man and number one doomer is bankrolled by Microsoft.

271
00:31:40,500 --> 00:31:49,500
At Dev Day for Open AI, Microsoft CEO Sacha Nadala casually dropped in on his boy wonder, Sam the doomer.

272
00:31:49,500 --> 00:31:52,500
This is a sickening 20 seconds of video.

273
00:31:52,500 --> 00:32:01,500
In it, you'll see Sacha Nadala literally say, lastly, safety is super important because you know you always leave the most important things for last.

274
00:32:01,500 --> 00:32:08,500
And then you'll see Sam Altman say the doomiest words thus far uttered in human history.

275
00:32:08,500 --> 00:32:18,500
And then the last thing is, of course, we're very grounded in the fact that safety matters and safety is not something that you'd care about later, but it's something we do shift left on and we're very, very focused on that with you all.

276
00:32:18,500 --> 00:32:19,500
Great.

277
00:32:19,500 --> 00:32:20,500
Well, I think we have the best partnership in tech.

278
00:32:20,500 --> 00:32:22,500
I'm excited for us to build AI together.

279
00:32:22,500 --> 00:32:23,500
I'm really excited.

280
00:32:23,500 --> 00:32:24,500
Have a friend.

281
00:32:24,500 --> 00:32:25,500
Thank you very much for coming.

282
00:32:25,500 --> 00:32:28,500
It really is too much to take.

283
00:32:28,500 --> 00:32:30,500
He knows his work.

284
00:32:30,500 --> 00:32:34,500
His choices may kill us all.

285
00:32:34,500 --> 00:32:37,500
And yet he continues.

286
00:32:37,500 --> 00:32:42,500
Sam Altman was asked a great question about just this at a recent Bloomberg event.

287
00:32:42,500 --> 00:32:47,500
He signed a 22 word statement, warning about the dangers of AI.

288
00:32:47,500 --> 00:32:57,500
It reads, mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war.

289
00:32:57,500 --> 00:32:59,500
Connect the dots for us here.

290
00:32:59,500 --> 00:33:05,500
How do we get from a cool chat bot to the end of humanity?

291
00:33:05,500 --> 00:33:07,500
Well, we're planning not to.

292
00:33:07,500 --> 00:33:09,500
That's the hope.

293
00:33:09,500 --> 00:33:13,500
But there's also the fear.

294
00:33:13,500 --> 00:33:26,500
I mean, I think there's many ways it could go wrong, but we work with powerful technology that can be used in dangerous ways very frequently in the world.

295
00:33:26,500 --> 00:33:34,500
And I think we've developed over the decades good safety system practices in many categories.

296
00:33:34,500 --> 00:33:35,500
It's not perfect.

297
00:33:35,500 --> 00:33:36,500
And this won't be perfect either.

298
00:33:36,500 --> 00:33:37,500
Things will go wrong.

299
00:33:37,500 --> 00:33:42,500
But I think we'll be able to mitigate some of the worst scenarios you can imagine.

300
00:33:42,500 --> 00:33:47,500
You know, bioterrorists like a common example, cybersecurity is another like many more we could talk about.

301
00:33:47,500 --> 00:34:01,500
But as this technology, like the main thing that I feel is important about this technology is that we are on an exponential curve and a relatively steep one.

302
00:34:01,500 --> 00:34:07,500
Human intuition for exponential curves is like really bad in general.

303
00:34:07,500 --> 00:34:11,500
It clearly was not that important in our evolutionary history.

304
00:34:11,500 --> 00:34:21,500
And so I think we have to, given that we all have that weakness, I think we have to like really push ourselves to say, OK, GPT-4, you know, not a risk like you're talking about there.

305
00:34:21,500 --> 00:34:24,500
But how sure are we that GPT-9 won't be?

306
00:34:24,500 --> 00:34:32,500
How sure are we that chat GPT-9 won't murder your whole family and mine, he asks.

307
00:34:32,500 --> 00:34:37,500
That is why Sam Altman is the number one doomer in AI.

308
00:34:37,500 --> 00:34:51,500
He's not sure that his own product, a few versions from now, won't slaughter my family, your family, every family, and all their pets too.

309
00:34:51,500 --> 00:34:56,500
He says maybe it's GPT-9 that's a lethal threat.

310
00:34:56,500 --> 00:35:04,500
AI safety researchers say it could be GPT-5 or 6.

311
00:35:04,500 --> 00:35:10,500
Are we really just debating when not what?

312
00:35:10,500 --> 00:35:16,500
Is the only question left what version kills us?

313
00:35:16,500 --> 00:35:24,500
Have we really just conceded the doomers can make their deadly weapons systems that will no regulation, no oversight?

314
00:35:24,500 --> 00:35:31,500
No, we have not conceded that we have fucking not.

315
00:35:31,500 --> 00:35:37,500
OK, so it is almost the end of the year.

316
00:35:37,500 --> 00:35:45,500
I've been making these shows for eight weeks straight and it has honestly been as fulfilling as it has been hard work.

317
00:35:45,500 --> 00:35:53,500
I am so excited what we can do with this podcast together in 2024 as we build this movement and this community.

318
00:35:53,500 --> 00:36:01,500
It starts with each one of us convincing others one at a time that this is the only issue that really matters anymore.

319
00:36:01,500 --> 00:36:05,500
So for 2024 I have big plans for humanity.

320
00:36:05,500 --> 00:36:13,500
I have so many show ideas and this story is going to unfold in real time so we'll be reacting in real time as well.

321
00:36:13,500 --> 00:36:25,500
Let us build a community of people here who are unafraid to plainly convincingly evangelize for AI existential risk awareness.

322
00:36:25,500 --> 00:36:33,500
Let's use each other as resources, helping each other get better at convincing others by sharing our experiences.

323
00:36:33,500 --> 00:36:39,500
It's not easy to convince people and it's not fun and we could all be way better at it.

324
00:36:39,500 --> 00:36:47,500
OK, finally, I'm going to take a few days to put together the next show just to spend a little extra time to be with my family over the holidays.

325
00:36:47,500 --> 00:36:50,500
So the next show will come out right after the new year.

326
00:36:50,500 --> 00:36:53,500
So I want to end with a holiday message.

327
00:36:53,500 --> 00:36:58,500
2023 was a year of shock and disbelief.

328
00:36:58,500 --> 00:37:01,500
In 2024, let's get our optimism back.

329
00:37:01,500 --> 00:37:04,500
Let's get our joy back.

330
00:37:04,500 --> 00:37:13,500
If the worst is to come, I want to spend every second enjoying life as much as humanly possible.

331
00:37:13,500 --> 00:37:18,500
I will not let these doomers rob me of my time or of my joy.

332
00:37:18,500 --> 00:37:24,500
Let's make 2024 a year of celebration and loving life.

333
00:37:24,500 --> 00:37:27,500
Let's celebrate what we love about being humans.

334
00:37:27,500 --> 00:37:32,500
Let's celebrate what we love about living on Earth.

335
00:37:32,500 --> 00:37:36,500
Let's spend time with our friends and family more than ever before.

336
00:37:36,500 --> 00:37:39,500
Take that vacation, go out to that dinner.

337
00:37:39,500 --> 00:37:43,500
Let's live like every second is precious.

338
00:37:43,500 --> 00:37:51,500
And while we celebrate life and humanity, let's kick and scream and fight like crazy to survive.

339
00:37:51,500 --> 00:37:57,500
So each show next year will end with a celebration of something that is wonderful about life.

340
00:37:57,500 --> 00:38:01,500
I'm going to share from my life and I hope you'll share from yours.

341
00:38:01,500 --> 00:38:06,500
So that gives me something big to smile about and something I'm really looking forward to sharing with you in 2024.

342
00:38:06,500 --> 00:38:10,500
Please have a wonderful holiday season and a happy new year.

343
00:38:10,500 --> 00:38:15,500
Whatever, wherever you celebrate, hold on to your loved ones close.

344
00:38:15,500 --> 00:38:20,500
For Humanity, I'm John Sherman. I'll see you back here next year.

345
00:38:31,500 --> 00:38:33,500
Thank you.

