start	end	text
0	8800	Unelected, unvetted, and completely unaccountable, this one man has chosen to put all of our
8800	14880	lives in grave danger so that he can make money and fulfill his dreams of radically
14880	21960	altering human existence as we have never known it without even an effort to get any
21960	26400	consent from you or anyone else in the general public.
26800	30320	Here's the really crazy part of it.
30320	33280	He tells us all of this openly.
33280	34920	There is no deception.
34920	37720	He admits all of it.
37720	41920	You have an incredible amount of power at this moment in time.
41920	44920	Why should we trust you?
44920	45920	You shouldn't.
45920	46920	You shouldn't.
46920	47920	You shouldn't.
47920	48920	You shouldn't.
48920	51160	Let's be very clear.
51160	57160	The man who signed the 22-word statement admitting that his AI technology has the power to kill
57160	62760	all life on Earth and must be mitigated like nuclear arms and pandemic is also saying to
62760	67560	you directly that he should not be trusted.
67560	71720	When people show you who they are, believe them.
71720	72720	Yes.
72720	75480	In general, it's the process for the bigger question of safety.
75480	80040	How do you provide that layer that protects the model from doing crazy, dangerous things?
81040	88040	I think there will come a point where that's mostly what we think about the whole company.
88040	89040	Aha.
89040	90040	Hmm.
90040	95560	Okay, so at some point, the safety crisis will be so urgent the entire company will
95560	98640	need to deal with it.
98640	100280	Hear him.
100280	101840	Believe him.
101840	108360	The thing he's making will be very unsafe and at an unknown point, the guy who says,
108800	117800	Trust me wants us to trust him that he will at the exact right moment know to throw the
117800	121800	whole company into safety work.
121800	125000	But just not quite yet.
125000	131480	I thought at some point between when Open AI started and when we created AGI, there was
131480	135720	going to be something crazy and explosive that happened, but there may be more crazy
135720	145160	and explosive things still to happen.
145160	152240	Welcome to For Humanity and AI Safety Podcast, episode number 22, Nobody Elected, Sam Aldman.
152240	153720	I'm Jon Sherman, your host.
153720	155760	Thank you so much for joining me.
155760	158800	In America, we elect our leaders.
158800	164600	They audition their ideas and then our government of by and for the people votes to decide our
164600	168840	collective direction and decisions.
168840	173600	But the American making the most consequential decisions in human history right now as I
173600	179640	speak hasn't been elected by anyone nor would he be.
179640	184160	His ideas poll very poorly with the general public.
184160	191400	He says go the public says stop poll show more than 70% are opposed to building superhuman
191400	193200	intelligence.
193200	197640	So this week, we're going to focus on the most important human in AI who is also the
197640	203720	most important human who has ever lived sadly.
203720	206920	Today's show is going to point fingers and lay blame.
206920	213000	Sam Aldman is why we are at the most dangerous moment in human history that we are at right
213000	214640	now.
214640	217120	This is the AI Safety Podcast for the general public.
217120	218640	No tech background required.
218640	225320	This podcast is solely about the threat of human extinction from artificial intelligence.
225320	229480	At the end of today's show, we're going to talk about how hard it is to convince people
229480	236200	to believe that AI risk is so much of a risk, they are moved to take action.
236200	239080	I have some new insight into that.
239080	246160	But first, it is not hyperbole to say that open AI CEO Sam Aldman holds the life of every
246160	250880	single human on earth in his hands.
250880	261240	Your kids, your parents, your friends, his knife is at each of their throats.
261240	265120	Sam Aldman was on the Lex Friedman podcast very recently.
265120	270400	And so I felt compelled to break that down for you this week.
270400	275640	Sam was also in a very hard hitting piece in business insider this week.
275640	279960	There's a link in the description as there will be to the Lex podcast.
279960	286920	The title of the business insider article was some VCs are over the Sam Altman hype.
286920	288160	Sounds good to me.
288160	291520	Next subheading the platform of Sam.
291520	294320	This is what I want to read you from the article.
294320	298780	Some who know Altman paint a picture of a benevolent visionary, a thoughtful steward
298780	300740	of a promising technology.
300740	306480	They see his plan to build a far reaching AI empire that touches everything from nuclear
306480	312380	fusion to anti-aging technology as a leap forward for humanity.
312380	317940	Others say he's more interested in self-promotion than human advancement.
317940	324360	All agree he's a masterful storyteller who could sell sand in the Sahara.
324360	329460	But as Altman lays out a sweeping vision for the transformation of society, not everyone
329480	333120	in Silicon Valley is buying it quote.
333120	337280	There are holes a mile deep in this guy's resume, but he's managed to figure out how
337280	340840	to take his chess pieces and move them correctly.
340840	343880	An anonymous startup founder said.
343880	348240	And now one of the things went crazy.
348240	350480	And he's an AI expert.
350480	357980	Okay, so my point in reading that is this Sam Altman is a tech CEO from the sales side.
358000	360680	He knows how to sell ice in Alaska.
360680	365120	He ran a startup incubator and he had a lot of different bets.
365120	369120	And open AI is the one that hit big.
369120	371920	So this guy making all the decisions.
371920	373480	He's not an AI expert.
373480	375520	He does not write code.
375520	377760	He's a salesman, nothing against salesman.
377760	379640	There have to be salesmen in the world.
379640	382600	But I don't know that that's who you want making all the decisions.
382600	389460	It was way back in 2015 that Vanity Fair gushed over Sam Altman and Elon Musk launching a
389460	395780	nonprofit company to save the world from a dystopian future.
395780	404700	Fast forward to 2023 and Sam Altman, his nonprofit, then a soon to be blossoming profit center
404700	412800	of the largest corporation on earth was the one who decided to unleash chat GPT on the
412800	415280	world when he did.
415280	420840	It is widely reported that Google had a similar product to chat GPT six months earlier, but
420840	428040	they did not release it because they were concerned over safety.
428040	436380	Unelected, unvetted and completely unaccountable, this one man has chosen to put all of our
436380	442460	lives in grave danger so that he can make money and fulfill his dreams of radically
442460	449540	altering human existence as we have never known it without even an effort to get any
449540	454740	consent from you or anyone else in the general public.
454740	457940	Here's the really crazy part of it.
457940	460920	He tells us all of this openly.
460920	462520	There is no deception.
462520	465560	He admits all of it.
465560	467780	Here's the sad part.
467780	470380	No one takes him at his word.
470380	475120	No one is listening to his promises of doom.
475120	477500	It all just sounds so outrageous.
477500	483200	People dismiss it instantly and move on to whatever bullshit they were thinking of first.
483200	484360	Yeah.
484360	486600	I'm talking about this guy.
486620	489860	Who nine months ago told us this.
489860	495140	I think even you would acknowledge you have an incredible amount of power at this moment
495140	496140	in time.
496140	499300	Why should we trust you?
499300	500300	You shouldn't.
500300	501300	You shouldn't.
501300	502300	You shouldn't.
502300	503300	You shouldn't.
503300	505540	Let's be very clear.
505540	511540	The man who signed the 22 word statement admitting that his AI technology has the power to kill
511540	517080	all life on earth and must be mitigated like nuclear arms and pandemic is also saying to
517080	524600	you directly that he should not be trusted.
524600	530080	The man with his hand on the kill eight billion people button is telling you he cannot be
530080	532920	trusted with this responsibility.
532920	537400	When people show you who they are, believe them.
537400	538400	Yes.
538400	539400	Absolutely.
539400	551780	This one says to you, I'm selfish or I'm mean or I am unkind or I'm crazy or I'm
551780	552780	crazy.
552780	553780	Believe them.
553780	556580	They know themselves much better than you do.
556580	563780	But no, more often than not, those of us who don't trust life say don't say a thing
563780	564780	like that.
564780	567180	You're not really crazy.
567180	569800	You're not really unkind.
569800	570800	You're not really mean.
570800	579760	And as soon as you say that, the person that you know and shows you, I told you, I told
579760	582360	you I was unkind.
582360	586040	Thank you, Oprah and the late great Dr. Maya Angelou.
586040	589520	Sam keeps telling us not to trust him.
589520	593400	He says no one should have the power he has.
593400	595920	Maybe we should believe him.
595920	598540	Like no one person should be trusted here.
598540	601540	I don't have super voting shares.
601540	603340	Like I don't want them.
603340	604340	The board can fire me.
604340	605340	I think that's important.
605340	608740	The board can fire me and I think that's important.
608740	613500	He said back in June 2023 and I, yes.
613500	618620	So now we fast forward to that famous week in November 17th, 2023, less than six months
618620	626920	later when it seems indisputably a battle over AI safety and Sam Altman's honesty
626920	634560	around AI safety, the board, including his friend and co-founder, Ilya Sutskiver, do
634560	639160	exactly what he says the failsafe should be.
639160	641560	They fire him.
641560	648040	And within hours, after having attached his nonprofit intended to help humanity to Microsoft
648060	654380	the world's largest corporation, Microsoft naturally decided it could not lose its golden
654380	656380	goose.
656380	661700	So Microsoft led to the firing of the board members who drew the hard line on safety.
661700	665340	Safety lost, Sam won.
665340	671620	This was all a huge fucking deal and yet nobody other than a handful of people intimately
671620	678440	involved seems even to this day to have any idea what really happened, which is why Sam
678440	683840	Altman's appearance on the Lex Friedman show is really worth picking apart.
683840	688520	For the first time, Sam was interviewed in depth about what happened the weekend he got
688520	689840	fired.
689840	697320	What happened that weekend is a national security issue of grave seriousness.
697320	702780	And again, the guy building the tech that can end all life on Earth, the same tech he
702780	708700	says he cannot control the same tech that no one understands how it works or why it
708700	710820	does what it does.
710820	718440	That guy, when he reflects on that weekend, when his lack of regard for safety should
718440	726500	have put him out on the streets, he is consumed by one thing, not what happened, not the safety
726520	733000	concerns, not the fact he lied about the board being a check on him.
733000	742840	Sam Altman's take on the weekend that he got fired is all about his feelings.
742840	748920	Take me through the open AI board saga that started on Thursday, November 16th, maybe Friday,
748920	750880	November 17th for you.
750880	761620	That was definitely the most painful professional experience of my life and chaotic and shameful
761620	767660	and upsetting and a bunch of other negative things.
767660	774380	There were great things about it too, and I wish I wish it had not been in such an adrenaline
774380	777420	rush that I wasn't able to stop and appreciate them at the time.
777420	787840	But I came across this tweet of mine from that time period, which was kind of going
787840	793920	to your own eulogy, watching people say all these great things about you and just unbelievable
793920	798960	support from people I love and care about.
798960	801720	That was really nice.
801720	807420	The whole weekend I kind of felt, with one big exception, I felt like a great deal of
807420	818540	love and very little hate, even though it felt like I have no idea what's happening
818540	822620	and what's going to happen here and this feels really bad and there were definitely times
822620	829180	I thought it was going to be one of the worst things to ever happen for AI safety.
829180	836680	One of the worst things to happen for AI safety says the guy who started the race to AGI,
836680	841240	but don't worry, he says he saw this coming and that's just the start.
841240	846320	Well, I also think I'm happy that it happened relatively early.
846320	852800	I thought at some point between when OpenAI started and when we created AGI, there was
852800	857040	going to be something crazy and explosive that happened, but there may be more crazy
857040	860260	and explosive things still to happen.
860260	861260	Honestly I just can't.
861260	863820	I cannot take this man.
863820	870740	With a twinkle in his eye, a smirk on his face and the power to kill everyone I love,
870740	878060	it honestly a lot of the time feels like he's taunting me, like he's taunting us.
878060	884300	There is nothing about risking all life on earth without anyone's consent that is funny
884360	888000	or cute.
888000	892320	But enough for now about you and me and our families.
892320	898120	Let's focus on what's really important and still the biggest known about the AI safety
898120	900440	crisis to date.
900440	902360	Sam Altman's feelings.
902360	912240	It feels like something that was in the past that was really unpleasant and really difficult
912260	921100	and painful, but we're back to work and things are so busy and so intense that I don't spend
921100	922780	a lot of time thinking about it.
922780	929380	There was a time after, there was like this fugue state for kind of like the month after,
929380	935860	maybe 45 days after that was, I was just sort of like drifting through the days.
935860	938220	I was so out of it.
938220	940140	I was feeling so down.
940140	942140	Just at a personal psychological level.
942760	950240	Really painful and hard to like have to keep running open AI in the middle of that.
950240	955960	I just wanted to like crawl into a cave and kind of recover for a while, but now it's
955960	960760	like we're just back to working on the mission.
960760	970960	What's still useful to go back there and reflect on board structures, on power dynamics,
970980	977580	on how companies are run, the tension between research and product development and money
977580	984860	and all this kind of stuff, so that you who have a very high potential of building AGI
984860	990300	would do so in a slightly more organized, less dramatic way in the future.
990300	995980	There's value there to go, both the personal psychological aspects of you as a leader and
995980	1000580	also just the board structure and all this kind of messy stuff.
1000600	1010200	We've only learned a lot about structure and incentives and what we need out of a board.
1010200	1016520	Okay, so I'm sorry, but Lex did a terrible job in this interview, I'm sorry.
1016520	1020960	They talked for two hours and not once did we find out anything really new about what
1020960	1024600	actually happened the weekend he got fired.
1024600	1029720	He called it the worst thing possible for AI safety and Lex does not even follow up and
1029740	1036940	ask him what he means, what do you mean it's the worst thing possible for AI safety?
1036940	1042740	But he does ask about open AI's chief scientist, Ilya Sutskiver, who before the firing did
1042740	1048380	podcasts and made a lot of public appearances since the firing weekend, Ilya has not been
1048380	1052380	seen anywhere in public that I'm aware of.
1052380	1058700	He Ilya is also noted for his deep and genuine concern over AI safety, so does Lex try to
1058720	1061040	get the reel on this out of Sam?
1061040	1064360	Oh no, it's just more footsie.
1064360	1065360	So cute.
1065360	1070880	Let me ask you about Ilya, is he being held hostage in a secret nuclear facility?
1070880	1071880	No.
1071880	1073600	What about a regular secret facility?
1073600	1074600	No.
1074600	1076000	What about a nuclear non-secret facility?
1076000	1077720	Neither, not that either.
1077720	1083520	I mean, it's becoming a meme at some point, you've known Ilya for a long time, he's obviously
1083520	1088360	in part of this drama with the board and all that kind of stuff.
1088380	1091220	What's your relationship with him now?
1091220	1097140	I love Ilya, I have tremendous respect for Ilya, I don't know anything I can say about
1097140	1100900	his plans right now, that's a question for him.
1100900	1106340	But I really hope we work together for, you know, certainly the rest of my career.
1106340	1109900	He's a little bit younger than me, maybe he works a little bit longer.
1109900	1116420	You know, there's a meme that he saw something, like he maybe saw AGI and that gave him a
1116480	1119360	lot of worry internally.
1119360	1121160	What did Ilya see?
1121160	1129960	Ilya has not seen AGI, none of us have seen AGI, we have not built AGI.
1129960	1139400	I do think one of the many things that I really love about Ilya is he takes AGI and the safety
1139400	1144720	concerns broadly speaking, you know, including things like the impact this is going to have
1144720	1147940	on society very seriously.
1147940	1154780	And we, as we continue to make significant progress, Ilya is one of the people that I've
1154780	1159660	spent the most time over the last couple of years talking about what this is going to
1159660	1163940	mean, what we need to do to ensure we get it right, to ensure that we succeed at the
1163940	1165940	mission.
1165940	1180960	So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks
1180960	1184960	and worries about making sure we get this right.
1184960	1192960	So the credit to humanity most concerned about safety has been silenced and hidden.
1192980	1196980	And with that problem taken care of, the race is back on.
1196980	1202380	Next, Lex asked Sam about Elon Musk suing him.
1202380	1207020	And again, suicide Sam goes straight to his feelings.
1207020	1212100	I know he knows what it's like to have haters attack him and it makes me extra sad he's
1212100	1213100	doing it, Toss.
1213100	1216900	Yeah, he's one of the greatest builders of all time, potentially the greatest builder
1216900	1217900	of all time.
1217900	1218900	It makes me sad.
1218900	1221620	And I think it makes a lot of people sad, like there's a lot of people who've really
1221640	1224560	looked up to him for a long time and said this.
1224560	1228640	I said in some interview or something that I missed the old Elon and the number of messages
1228640	1231960	I got being like that exactly encapsulates how I feel.
1231960	1234360	I think he should just win.
1234360	1242360	He should just make X grok beat GPT and then GPT beats grok and it's just a competition.
1242360	1247160	What do you hope this goes with Elon?
1247160	1248520	This tension, this dance.
1248520	1249520	What do you hope this?
1249520	1255460	Like if we go one, two, three years from now, you're a relationship with him on a personal
1255460	1263460	level too, like friendship, friendly competition, just all this kind of stuff.
1263460	1272740	Yeah, I really respect Elon.
1272740	1276100	And I hope that years in the future we have an amicable relationship.
1276960	1282880	Yeah, I hope you guys have an amicable relationship like this month.
1282880	1288960	And just compete and win and explore these ideas together.
1288960	1296920	I do suppose there's competition for talent or whatever, but it should be friendly competition.
1296920	1297920	Just build.
1297920	1300240	Build cool shit.
1300240	1304280	And Elon is pretty good at building cool shit, but so are you.
1304280	1310420	Building cool shit with like a rowy tone of voice building cool shit.
1310420	1314740	Building cool shit is why we are all on course to die in the next 10 fucking years.
1314740	1317340	It is not cool.
1317340	1320940	Okay, this next one is wild.
1320940	1326460	Lex asks Sam about the basis for the New York Times versus open AI lawsuit.
1326460	1332220	Remember Sam's multi-billion dollar black box machine would not exist if it had not
1332240	1335080	been trained on internet data.
1335080	1341560	Sam already having harvested the data and made his billions has the gall to drop this
1341560	1342560	answer.
1342560	1346400	There's a lot of tough questions here.
1346400	1348200	You're dealing in a very tough space.
1348200	1353160	Do you think training AI should be or is fair use under copyright law?
1353160	1357600	I think the question behind that question is do people who create valuable data deserve
1357620	1362140	to have some way that they get compensated for use of it and that I think the answer
1362140	1363140	is yes.
1363140	1366420	I don't know yet what the answer is.
1366420	1368100	People have proposed a lot of different things.
1368100	1375700	We've tried some different models, but if I'm like an artist, for example, A, I would
1375700	1380980	like to be able to opt out of people generating art in my style and B, if they do generate
1380980	1385860	art in my style, I'd like to have some economic model associated with that.
1385880	1388760	What the actual fuck are you even talking about?
1388760	1391080	You already stole the training data.
1391080	1395240	All the writers and artists have had their work stolen.
1395240	1399760	This dude built a billion dollar business on stolen writing and images and now after
1399760	1404880	his business is flourishing after and the toothpaste seems impossible to put back in
1404880	1411360	the tube, he's like, um, yeah, somebody should figure out a way to make this fair.
1412180	1416720	There's a real theme with Sam Altman.
1416720	1421380	Every problem he causes is someone else's to solve.
1421380	1425580	So in general is the process for the bigger question of safety.
1425580	1430260	How do you provide that layer that protects the model from doing crazy dangerous things?
1430260	1437300	I think there will come a point where that's mostly what we think about the whole company
1437300	1440660	and it won't be like, it's not like you have one safety team.
1440660	1443520	It's like when we shipped GPT-4, that took the whole company thing with all these different
1443520	1448200	aspects and how they fit together and I think it's going to take that.
1448200	1452320	More and more of the company thinks about those issues all the time.
1452320	1458120	That's literally what humans will be thinking about the more powerful AI becomes.
1458120	1463080	So most of the employees that open AI will be thinking safety or at least to some degree.
1463080	1465600	Broadly defined, yes.
1465600	1468600	I wonder what are the full broad definition of that?
1468620	1470820	What are the different harms that could be caused?
1470820	1475740	Is this like on a technical level or is this almost like security threats?
1475740	1476740	It'll be all those things.
1476740	1481580	I was going to say it'll be people, state actors trying to steal the model.
1481580	1485020	It'll be all of the technical alignment work.
1485020	1491860	It'll be societal impacts, economic impacts.
1491860	1496100	It's not just like we have one team thinking about how to align the model and it's really
1496100	1500480	going to be like getting to be getting to the good outcome is going to take the whole
1500480	1501480	the whole effort.
1501480	1502480	Huh.
1502480	1503480	Hmm.
1503480	1504480	Okay.
1504480	1509800	So at some point, the safety crisis will be so urgent, the entire company will need
1509800	1512640	to deal with it.
1512640	1515840	Hear him, believe him.
1515840	1522400	The thing he's making will be very unsafe and at an unknown point, the guy who says,
1522400	1531820	trust me, wants us to trust him that he will at the exact right moment know to throw the
1531820	1538020	whole company into safety work, but just not quite yet.
1538020	1539340	Cool.
1539340	1541620	Seems like a plan.
1541620	1542620	Okay.
1542620	1544460	This next one is wild.
1544460	1551380	So, um, I spoke to a group of coders about AI risk last week, uh, a week or so ago, um,
1551380	1556280	outside of Philadelphia, and the number one question they had was how will AGI escape?
1556280	1560480	How does it go from the digital world to the physical world?
1560480	1562080	Hiring humans is an easy way.
1562080	1565640	There's lots of others, but here's an even easier way.
1565640	1567120	Go tell him, Sam.
1567120	1574040	Will we, uh, see human and robots or human and robot brains from open AI at some point?
1574040	1575560	At some point.
1575560	1578120	How important is embodied AI to you?
1578120	1583620	I think it's like sort of depressing if we have AGI and the only way to like get things
1583620	1587460	done in the physical world is like to make a human go do it.
1587460	1595260	So I, I really hope that as part of this transition, as this phase change, we also get, uh, we
1595260	1598420	also get human and robots or some sort of physical world robots.
1598420	1599660	Okay.
1599660	1601900	Let's add up the tab.
1601900	1604780	Sam can't control his AI.
1604780	1608640	Sam doesn't understand how his AI works.
1608640	1612840	Sam admits his AI can end all life on earth.
1612840	1614240	Sam is clear.
1614240	1616640	You should not trust him.
1616640	1621320	And one more.
1621320	1629800	Sam would be depressed if his AGI is not put in to robots.
1629800	1635180	For Bill's AGI first gets a lot of power.
1635180	1640220	Do you trust yourself with that much power?
1640220	1648100	When people show you who they are, believe them.
1648100	1650060	Yes.
1650060	1656980	Look, I, I was gonna, I'll just be very honest with this answer.
1656980	1662560	I was gonna say, and I still believe this, that it is important that I, nor any other
1662560	1669800	one person have total control over open AI or over AGI.
1669800	1674840	And I think you want a robust governance system.
1674840	1684040	Um, I can point out a whole bunch of things about all of our board drama from last year
1684040	1687980	about how I didn't fight it initially and was just like, yeah, that's, you know, the
1687980	1693660	will of the board, even though I think it's a really bad decision.
1693660	1696940	And then later I clearly did fight it and I can explain the nuance and why I think it
1696940	1699380	was okay for me to fight it later.
1699380	1711660	But as many people have observed, um, although the board had the legal ability to fire me
1711660	1716440	in practice, it didn't quite work.
1716440	1721480	And that is its own kind of governance failure.
1721480	1729000	Now, again, I, I feel like I can completely defend the specifics here.
1729000	1738480	And I think most people would agree with that, but it, it does make it harder for me to like
1738480	1741240	look you in the eye and say, Hey, the board can just fire me.
1741660	1743940	They know themselves much better than you do.
1747180	1753540	Okay, Sam, one last clip, please, most important person on earth, give us some reassurance.
1755380	1758540	Are you afraid of losing control of the AGI itself?
1758580	1763740	That's a lot of people who worry about existential risk, not because of state actors, not because
1763740	1766300	of security concerns, but because of the AI itself.
1766300	1767460	That is not my top worry.
1767820	1770620	As I currently see things, there have been times I've worried about that more than maybe
1770640	1773160	times again, in the future, that's my top worry.
1773680	1774760	It's not my top worry right now.
1774800	1776680	What's your intuition about it not being your worry?
1776680	1778760	Cause there's a lot of other stuff to worry about, essentially.
1781480	1782760	You think you could be surprised?
1782840	1784680	We for sure could be surprised.
1785200	1787600	Like saying it's not my top worry.
1787600	1790880	It doesn't mean I don't think we need, like, I think we need to work on it super hard.
1792320	1794840	I think we need to work on it super hard.
1796320	1797840	Um, are you fucking kidding?
1798020	1802860	That is not at all the same as we are working on it super hard.
1803620	1805100	Two very different things.
1806060	1809420	Sam Altman wants someone else to make his tech safe.
1809780	1813780	Sam Altman wants someone else to regulate his tech.
1814260	1819460	Sam Altman even wants someone else to figure out how to compensate the people whose
1819460	1823020	training data was stolen to build his death star.
1824300	1825820	No one elected Sam Altman.
1826640	1832600	No one has vetted Sam Altman, but he is in charge of whether or not your family
1832600	1834040	lives or dies.
1837120	1843600	No government oversight motivated by profit, ego, a boyish need to build cool
1843600	1851880	stuff and an unrequested desire to change how humans live fundamentally with
1851880	1854640	no plan as fast as possible.
1856760	1862560	That is why I believe we need to stop Sam Altman and all those like him.
1863160	1864360	Okay, but here's the thing.
1865720	1870800	The case against Sam and his peers is not hitting home with the general public yet.
1871120	1872680	Um, I want to share a story with you.
1872680	1877720	One of my closest and oldest friends on the planet and I had an exchange this week
1877720	1879800	that I think you can learn from.
1880080	1885640	So after a catch up zoom that unexpectedly drifted far into AI risk, he
1885640	1889340	told me in an email follow up that he believed everything I said.
1890380	1894380	So I, after some hesitation followed up, I just wanted to know really to help
1894380	1895620	better understand people.
1896100	1903020	Um, why, if he believed that his three young kids lives are threatened by AI, why
1903020	1907860	he has not taken action yet, why he doesn't feel compelled to do things like I
1907860	1909420	feel compelled to do things.
1909780	1910960	And here's what he wrote back.
1910960	1914220	Three bullet points that I think are so, so common.
1914220	1920200	I bet 90% of the inactive public would say the same three points.
1920600	1925080	He wrote, honestly, I don't have a good excuse.
1925600	1930280	Number one, I'm very busy with work and kids and life.
1930960	1935920	Number two, I'm only one person with zero influence on companies, individuals
1935920	1937960	and lawmakers who can impact what's happening.
1938420	1942000	And number three, I'm hopeful that doom is not imminent.
1942620	1946380	And that in the end, good outcomes will prevail.
1946940	1951380	That is literally what those of us who desperately want to spread AI risk
1951380	1955300	awareness and see action come from it are up against.
1956820	1960580	So on one hand, I was depressed at my first reading.
1961020	1963820	Those three points are so weak.
1964700	1965620	We're all busy.
1966180	1970660	We can all have influence and hope is not nearly enough.
1971600	1977120	But then I thought, wait a minute, those three points are so weak.
1979200	1981400	What we are up against is these three things.
1981440	1984600	Again, very busy with work and kids and life.
1985320	1986840	I don't have influence.
1987560	1989880	I hope that we get a good outcome.
1991280	1995480	The weaknesses in each of those is not our enemy.
1995840	1996960	It's our ally.
1997400	1999440	These are three winnable arguments.
1999980	2005660	So as you make for the, as you make the case for AI risk, keep those three
2005660	2010540	things in mind, that is what you and we are fighting against.
2011340	2012020	It's winnable.
2013140	2015860	Okay, friends, it's 2024.
2015860	2017740	AGI is coming at some point.
2017740	2021180	We don't know when we don't know how long we have to live.
2021300	2027700	So we celebrate every moment of every day, like it could be our last call
2027700	2030920	at the celebration of life for this week's celebration of life.
2030920	2033640	I want to share something that really moved me as a dad.
2034240	2039600	Um, there's not much that makes me feel thrilled to be alive more than
2039600	2042760	incredible vocal tone on a singing voice.
2043200	2047800	Um, when my daughter first shared with me, Billie Eilish, I was immediately
2047880	2050000	blown away by her vocal tone.
2050600	2055520	Um, and then as a dad of boy girl twins, it was so cool to learn
2055540	2059860	that her brother Phineas is her producer and musical creative partner.
2060020	2064860	Um, so I came across a performance that just really blew me away.
2064860	2069140	The two of them doing their song, um, what was I made for at the Grammy
2069140	2073340	awards, a live performance and as a dad of a boy and a girl and a fan of vocal
2073340	2076100	tone and songwriting, it just floored me.
2076180	2078500	Um, they are simply incredible.
2078700	2084700	And this song has some haunting reflections on our AI risk debate.
2084700	2086520	Please enjoy this is so good.
2205000	2209920	Don't tell my boy friend, it's not what he's made for.
2214160	2216640	But what was I made for?
2221520	2231480	Cause I, cause I, I don't know how to feel.
2231480	2255260	But I want to try, I don't know how to feel, but someday I might, someday I might.
2261480	2288260	I think I forgot how to be happy, something I'm not, but something I can't be, something I wait for, and something I'm made for.
2291780	2296980	Something I'm made for.
2305980	2307340	Just beautiful.
2308060	2309980	Okay, friends, that is all for this week.
2310220	2312780	Next week's show is going to be a little while.
2312820	2315620	So get ready for that for humanity.
2315660	2316460	I'm John Sherman.
2316460	2318300	I will see you right back here next week.
2318500	2320340	We have so much work to do.
