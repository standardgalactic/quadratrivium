start	end	text
0	6500	GPT-4, you know, not a risk like you're talking about there, but how sure are we that GPT-9 won't be?
6500	14500	How sure are we that Chad GPT-9 won't murder your whole family and mine, he asks?
14500	18500	The bad case, and I think this is like important to say, is like lights out for all of us.
18500	24500	The real doomers are the ones that could bring the fucking doom, yeah?
24500	37500	You know, my chance that something goes, you know, really quite catastrophically wrong on this scale of, you know, human civilization, you know, might be somewhere between 10 and 25%.
37500	42500	Who the fuck do you think you are?
42500	50500	Who the fuck do you think gave you permission to endanger the lives of 8 billion of your neighbors?
50500	63500	Seriously, who in their right mind would take a job at a company where your work creates a 10 to 25% chance of killing their family and all of their neighbors?
63500	67500	Oh, and all the rest of the people in the whole world too.
67500	73500	Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.
73500	76500	I would like to see much, much more happening.
76500	84500	Oh wow, Sam should try to find somebody who knows somebody who's in charge at OpenAI who can fix this.
84500	85500	Come on.
85500	89500	Some young person goes to a school and starts shooting people.
89500	93500	You blame video games, right?
93500	99500	Back in the old days people would blame comic books, they would blame jazz, they would blame TV, they would blame movies.
99500	102500	Yeah, yeah, it's just like video games and school shootings.
102500	105500	It's just like comic books or jazz.
105500	107500	Get the fuck out of here.
107500	109500	But who determines the goal?
109500	110500	We do.
110500	112500	You know better than that.
112500	117500	You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals.
117500	124500	You know that a human can set goals and AI systems must then set sub-goals to achieve those first goals.
124500	135500	And you know those sub-goals when unaligned with human values and obscured from view hidden in a black box system as they are could kill us all.
142500	150500	Welcome to For Humanity, an AI safety podcast, episode 8, AI's Top 3 Doomers.
150500	152500	I'm John Sherman, your host.
152500	153500	Thank you so much for joining me.
153500	156500	This is the AI safety podcast for the general public.
156500	158500	No tech background required.
158500	163500	This podcast, as you know, is solely about the threat of human extinction from artificial intelligence.
163500	166500	Please hit like and subscribe, tell a friend about the show.
166500	172500	And if you're on X, follow us at For Humanity Pod and leave a comment if you will.
172500	174500	I really, really appreciate those comments.
174500	179500	And reach out to me by email if you'd like to at For Humanity podcast at gmail.com.
179500	180500	I'd love to hear from you.
180500	184500	Many of you have been reaching out to me and I want to say thank you for doing that.
184500	195500	We are building a community of people here, a community of people who are unafraid to be ambassadors of AI extinction risk in Falkander.
195500	203500	This show is a lot of work and every time one of you reaches out to me with some words of encouragement, it means a great deal.
203500	204500	It really does.
204500	205500	So thank you.
205500	211500	I'm going to end today's show on something really positive, new and exciting coming up for 2024.
211500	215500	But first, the moment we are in.
215500	220500	Every day, more and more people are waking up to the grim truth.
220500	231500	A bunch of 30-somethings in Silicon Valley are in a race to build technology that can cure all of our problems or kill us all before it has a chance to do that.
231500	236500	Those both sound crazy, but they are both literally true.
236500	242500	The future, utopia or doom will be decided by us this decade.
242500	250500	Everything I've just said is almost consensus view among most of the AI thought leaders of all different stripes.
250500	259500	I'm not religious at all, but I was thrilled this week to see Pope Francis come out with a forceful call for AI safety.
259500	262500	Not sure if you saw it, check out this Washington Post headline.
262500	264500	Warning of risk to survival.
264500	268500	Pope calls for global treaty on AI.
268500	269500	Wow.
269500	270500	You got to love it.
270500	278500	In the Pope's statement on AI, he said, I think very importantly and on target for our show topic today, the Pope wrote,
278500	288500	artificial intelligence is not the responsibility of a few, but of the entire human family.
288500	294500	Again, I don't believe in religion, but amen, Pope Francis, he is right.
294500	302500	A handful of tech people responsible only to their shareholders cannot decide our survival.
302500	306500	There are nearly 1.5 billion Catholics on earth.
306500	310500	Catholics of the world hear Pope Francis.
310500	314500	He's saying human survival is at risk from AI.
314500	317500	Let's be clear.
317500	319500	This is not metaphorical.
319500	324500	He doesn't mean something like survival, but nicer.
324500	332500	The leading academic researchers and the leaders of all the big tech AI companies are openly talking about their work.
332500	336500	Killing everyone on earth knows survivors.
336500	340500	That is literally what they are talking about.
340500	347500	And the consensus of all the leading academic researchers on this subject is it's very likely, if it happens to happen,
347500	354500	that it will happen within the next 2 to 10 years.
354500	362500	If every Catholic believed the Pope's dire warning, that would be truly wonderful.
362500	369500	If every major world religious leader came out with a similar statement of grave concern, that would be even better.
369500	378500	What a potentially powerful, immediate impact that could have on global, worldwide opinion.
378500	387500	AI could still cure cancer and poverty, solve climate change, even if we only developed narrow artificial intelligence.
387500	390500	But we're not doing it that way.
390500	399500	We're in a race to build artificial general intelligence, an AI that's better than humans in a wide variety of skills and areas of expertise.
399500	402500	We are racing towards our own doom.
402500	406500	That's what most of the leading AI safety researchers are saying.
406500	412500	That's what the godfathers and creators of AI are shouting.
412500	415500	They're saying slow down.
415500	417500	They're saying stop.
417500	419500	But the race continues.
419500	432500	A handful of people are taking risks for the futures of 8 billion people without any consent whatsoever.
432500	435500	Civil society is based on consent, right?
435500	437500	You can't touch someone without their consent.
437500	445500	Every time you sign something all day long and you're signing away your consent, we do it for the most trivial things and the most important things.
445500	454500	If I want to cut down a tree in my yard that affects my neighbor's yard, I need to get my neighbor's consent.
454500	460500	The big AI companies have no fucking right at all to do what they're doing without anyone's consent.
460500	469500	We, the public, have not given them our consent for this at all.
470500	475500	So on today's show, the top three doomers in AI.
475500	480500	We're going to name names, point fingers, and lay down the blame.
480500	493500	These three men are unconsensually risking doom for you, your family, me, my family, and every other living thing on this planet.
493500	497500	The term doomer is misused.
497500	503500	AI risk deniers are the real doomers.
503500	505500	Let's be very clear.
505500	512500	Dumerism equals AI risk denialism.
512500	519500	The doomers are not the AI safety researchers like Elias or Yadkowski and Max Tegmark,
519500	524500	or the converts like Jeffrey Hinton and Yoshua Benjiro.
524500	528500	Those are heroes trying to save us.
528500	535500	The real doomers are the ones that could bring the fucking doom, yeah?
535500	539500	Yeah, that makes a whole lot more sense to me.
539500	545500	So on today's show, the top three doomers in AI.
545500	549500	These are the fucking assholes who are risking our doom.
549500	553500	They're leading the charge to make it happen as fast as possible.
553500	565500	So coming in at number three, the number three doomer in AI, it's anthropic CEO Dario Amade.
565500	572500	Yeah.
572500	579500	Dario's story is heartbreaking for those gravely concerned about AI risk.
579500	589500	It's a tale of a life rooted in good intentions, tragically broken, corrupted by market forces, ego, and greed.
589500	598500	Dario Amade, he worked at OpenAI from 2015 to 2020, leading the teams that built ChatGPT2 and ChatGPT3.
598500	608500	He is, by all accounts, an incredible computer scientist, and honestly, he seems like a nice, very likable guy in the interviews you're about to see.
608500	614500	He even runs his company with his sister. Aw.
614500	625500	In 2020, though, Dario left OpenAI with some others to start their own company, Anthropic, that would be far more focused on safety than OpenAI.
625500	638500	Today, Anthropic has only 160 employees, and it is widely considered the third most important AI company on the planet, just behind Microsoft's OpenAI and Google's DeepMind.
638500	650500	AI safety was prized at Anthropic originally. The company was all about safety in a meaningful and genuine way, but things changed.
650500	661500	Today, Anthropic has 160 humans whose daily work threatens the total slaughter of their 8 billion neighbors.
661500	666500	It was a long New York Times article about Dario and Anthropic in July.
666500	675500	In it, company leaders say to work on safety at the frontier level, you need to build systems that are at the dangerous frontier level.
675500	681500	I believe there is some truth in that. So they are doing it.
681500	693500	They got a $5 billion investment to build an AI model 10 times more capable than today's most capable models.
693500	701500	That is quite literally the dangerous frontier work, the 1% that could kill us all.
702500	710500	10 times smarter than today's AI could well be AGI, pushing us past the singularity.
710500	715500	Well, maybe it won't.
715500	729500	Here's Anthropic CEO Dario Amadei on the Logan Bartlett podcast, a great show, on how Anthropic changed after OpenAI released chat GPT to the general public.
729500	743500	I didn't want our first act on the public stage after we put so much effort into being responsible to accelerate things so greatly.
743500	748500	I generally feel like we made the right call there. I think it's actually pretty debatable.
748500	754500	There's many pros, many cons, but I think overall we made the right call.
754500	761500	And then certainly as soon as the other models were out and the gun had been fired, then we started putting these things out.
761500	770500	We're like, okay, all right, now there's definitely a market in this people, people know about it, and so we should get out ahead.
770500	776500	And indeed we've managed to put ourselves among the top two or three players in this space.
776500	786500	Was that gun being fired and chat GPT sort of taking off? Was that similar to the maybe fear that you had of like, hey, this might start a race?
786500	800500	Yeah, yeah, similar and in fact more so. So I think we saw it with Google's reaction to it, that there was definitely just judging from the public statements, a sense of fear and existential threat.
800500	806500	And I think they responded in a very economically rational way. I don't blame them for it at all.
806500	814500	But you put the two things together and it really created an environment where things were racing forward very quickly.
814500	821500	And look, I love technology as much as the next person. There was something like super exciting about the whole make them dance.
821500	829500	Oh, we're responding with something. I can get just as excited about this as everyone, but given the rate at which the technology is progressing,
829500	838500	you know, there was a worrying aspect about this as well. And so in this case, I'm at least at least on balance clad that, you know, we weren't the ones who fired that starting gun.
838500	850500	So he left open AI to start a new safer lab. But then once open AI released chat GPT, he gave up on being safer.
850500	855500	He openly admits this. And he joined the arms race.
855500	866500	Right there at the end of that sound bite we just played, he says he gets some consolation for not being the one to fire the gun to start the AGI arms race.
866500	871500	That is pathetic. That's called caving in.
871500	878500	When the AI safety company said, fuck it, we need to join the arms race or be left behind. So we're in.
878500	882500	Humanities chances took a big hit.
882500	891500	What happened that anthropic is a sickening window into the market forces at play here. Listen to me.
891500	898500	The rush to make money and be first is far more important to the human race than ensuring survival.
898500	913500	Full stop. That is simply the fact of how we are currently living. That is our status quo. That is what the actions of our society currently undeniably show.
913500	921500	Our plan is to be so rich and so dead.
921500	928500	So why is Dario Amadei the number three ranked AI doomer in the world?
928500	932500	He's making the doom happen.
932500	938500	We played the sound bite I'm going to play for you now a few shows ago. Listen to more of it here now.
938500	947500	Here's Dario with Logan Bartlett giving the percentage chance the P doom that the work he's doing ends all life on earth.
947500	952500	Do you think about percentage chance doom?
952500	964500	I think it's popular to give these percentage numbers and the truth is that I'm not sure it's easy to put a number to it and if you forced me to it would fluctuate all the time.
964500	985500	I think I've often said that my chance that something goes really quite catastrophically wrong on the scale of human civilization might be somewhere between 10 and 25 percent when you put together the risk of something going wrong with the model itself,
985500	1000500	with something going wrong with human people or organizations or nation states misusing the model or it kind of inducing conflict among them or just some way in which kind of society can't handle it.
1000500	1010500	That said, what that means is that there's a 75 to 90 percent chance that this technology is developed and everything goes fine.
1010500	1023500	If you don't believe that AI can kill us all very soon, ask yourself, what is it that you know that the CEO of the third most important AI company on earth doesn't know?
1023500	1031500	If someone in your life doesn't believe that AI can kill them, play that soundbite for them.
1031500	1040500	Here's Logan Bartlett asking a great question and Dario not answering it at all. It's like he doesn't even see the question.
1040500	1053500	There's a box article that said something to the effect of an employee predicted there was a 20 percent chance that a rogue AI would destroy humanity within the next decade to the reporter, I guess, that was around.
1053500	1061500	Does all this stuff weigh heavily on the organization on a daily basis or is it mostly consistent with a normal startup for the average employee?
1061500	1068500	Yeah, so I don't know. I'll give my own experience and it's kind of the same thing that I recommend to others.
1068500	1086500	So, you know, I really freaked out about this stuff in 2018 or 2019 or so when, you know, when I first believed that, you know, turned out to be at least in some ways correct, that the models would scale very rapidly and, you know, they would have this importance to the world.
1086500	1103500	I'm sure Dario Amadei is a lot like the few thousand people working on the most dangerous frontier AI capabilities work. Incredibly smart, probably even well intentioned, but completely blind to the notion of consent.
1103500	1113500	There can be no such thing as informed consent in this, not even the big AI companies know what AGI will do, when or how.
1113500	1123500	And yet they race on, never even attempting to get anyone's consent. Dario mentions he knows the importance of his work to the world.
1123500	1132500	So to be blunt, Mr. Amadei, who the fuck do you think you are?
1132500	1140500	Who the fuck do you think gave you permission to endanger the lives of eight billion of your neighbors?
1140500	1153500	Seriously, who in their right mind would take a job at a company where your work creates a 10 to 25% chance of killing their family and all of their neighbors?
1153500	1165500	Oh, and all the rest of the people in the whole world too. What the fuck are we talking about here? Who the fuck do you think you are, sir?
1165500	1169500	Okay, I'm gonna breathe for a moment.
1169500	1177500	On to the number two doomer in AI. Oh man, this guy is just the absolute worst.
1177500	1187500	The number two doomer in AI is Suicide Cult Leader and Director of AI Research at META, Yann LeCun.
1195500	1204500	Yann LeCun is simply stunningly awful. Here is Yann at the World Science Festival in late November.
1204500	1209500	You can have a very intelligent system that has no desire to dominate at all.
1209500	1217500	And so the way we will design those systems is to be smart. In other words, you give them a goal, they can solve that goal for you.
1217500	1222500	But who determines the goal? We do.
1222500	1229500	You know better than that. You know what the godfather of AI Jeffrey Hinton says about goals and sub-goals.
1229500	1236500	You know that a human can set goals and AI systems must then set sub-goals to achieve those first goals.
1236500	1247500	And you know those sub-goals when unaligned with human values and obscured from view hidden in a black box system as they are, could kill us all.
1247500	1257500	But Yann LeCun, this NYU professor and META department leader, somehow has enough spare time on his hands to sit on ex-formerly Twitter
1257500	1265500	and spew AI extinction risk denial bile through a fire hose.
1265500	1275500	He not only denies any AI extinction risk whatsoever, he bullies and makes fun of anyone who does, calling them doomers,
1275500	1282500	calling AI ex-risk sci-fi movie stuff, James Bond movie stuff and more.
1282500	1286500	If I had Yann LeCun in a public debate, I would shut him down with one question.
1286500	1299500	Yann, let's say you build AGI and let's say your interpretability sensor, which doesn't even currently exist, tells you that this AGI, which is smarter than any human, is plotting to kill you.
1299500	1304500	What do you do next?
1304500	1310500	There is no answer. The only answer is you die. So let's not get there, right?
1310500	1317500	But Yann isn't here for the hard questions. His go-to move is openly disingenuous.
1317500	1321500	I have to believe he's too smart to know this isn't bullshit.
1321500	1330500	Yann LeCun always reaches for historical precedent. Oh, AI is just like the typewriter. It's just another printing press moment.
1330500	1341500	But as you know, there is nothing in human history, absolutely nothing remotely like the invention of AI and the coming of AGI,
1341500	1347500	and any attempt to find a historical comparison is genuinely dangerous.
1347500	1351500	Just listen to the total bullshit that comes from this man's mouth.
1351500	1359500	It's very easy to attribute cultural phenomena and social phenomena to the new thing that just happened, right?
1359500	1372500	So if some young person goes to school and starts shooting people, you blame video games, right?
1372500	1379500	Back in the old days, people would blame comic books, they would blame jazz, they would blame TV, they would blame movies, novels.
1379500	1387500	The story goes back centuries. Whenever there is a new cultural phenomenon, whenever there is an effect on society,
1387500	1392500	you blame the latest technology that appears, particularly communication technology.
1392500	1397500	So it's natural to blame, for example, social networks, not just Facebook,
1397500	1402500	just any social networks for political polarization, okay? That seems natural.
1402500	1407500	People shout at each other on social network. That necessarily polarizes people.
1407500	1411500	A very natural thing to do. That turns out to be completely false.
1411500	1421500	Yeah, yeah, it's just like video games and school shootings. It's just like comic books or jazz. Get the fuck out of here.
1421500	1430500	At the World Science Fair, the moderator asks Yana a great question, and his answer is just numbing, completely numbing.
1430500	1432500	I do not understand this.
1432500	1436500	For you to say, hey, let's really slow this down.
1436500	1444500	Is there anything that would happen from these AI developments that would cause you to say, I want to slow this down?
1444500	1449500	So you want me to imagine a scenario that I don't believe can happen?
1449500	1451500	Yeah.
1451500	1453500	Okay.
1453500	1468500	So it's kind of funny because we're on a panel with a 60-something advocating for progress against a 30-something advocating for conservatism.
1468500	1477500	Isn't that paradoxical? Anyway, I mean, we can imagine all kinds of catastrophe scenarios.
1477500	1489500	Every sort of James Bond movie with a supervillain has some sort of catastrophe scenario where someone turns crazy and wants to, you know, eliminate humanity to recreate it to something or take over the world.
1489500	1494500	All of science fiction is full of that. That's what makes it funny and interesting.
1494500	1500500	I don't know if you noticed in those clips the facial expression of the guy with the beard two seats down from Jan Lacune.
1500500	1509500	This guy is amazing. You're about to meet a new AI safety hero, new character alert. Please say hello to AI ethicist Tristan Harris.
1509500	1513500	Find his stuff on the Internet. He really is incredible. Follow him.
1513500	1520500	Listen to Tristan Harris due to Jan Lacune, what Lacune always tries to do but fails.
1520500	1526500	Tristan finds a historical reference that actually makes sense.
1526500	1531500	If you keep the scaling model, the scaling laws going and you start to get AI that starts to automate scientific processes
1531500	1540500	where it's generating its own hypotheses and it has its own lab and starts to test those hypotheses and you start getting that kind of AI.
1540500	1544500	You start getting AI that's making its own scientific discoveries.
1544500	1552500	We have it going this fast. It feels like her metaphor was it's like the 24th century crashing down on the 21st century.
1552500	1559500	A metaphor for you is imagine that the 20th century tech was crashing down on 16th century governance.
1559500	1571500	The 16th century, you've got the king, you've got their advisors, but suddenly television, radio, the telegraph, video games, Nintendo,
1571500	1578500	and thermonuclear weapons all show up. They just land in your society.
1578500	1585500	But you're like, call the knights and the knights show up and you're like, what are you going to do?
1585500	1593500	What are we going to do when technology hundreds of years advanced beyond our current moment is suddenly thrown in our hands?
1593500	1602500	Nobody has any fucking clue. Just imagine medieval kings and knights waking up one day with nukes and cell phones.
1602500	1614500	The odds that I'd be sitting here today talking to you like this after that seem very slim, which brings us to the number one doomer in AI.
1614500	1624500	This was not a hard choice. It's the guy who did everything the original founders of AI swore they would never do.
1625500	1635500	He chose to release his general AI models open source. He chose to release them on the open internet.
1635500	1641500	He chose to release them with the ability to write their own code.
1641500	1652500	This man's fingerprints are on the handle of the Pandora's box that is now open and threatening to murder everyone you've ever met.
1652500	1657500	And also everyone you've never met too.
1657500	1666500	He is the human starting gun in our profit fueled global race to mass suicide.
1666500	1672500	The number one doomer in AI is open AI CEO Sam Altman.
1672500	1684500	Number one doomer Sam Altman is the most dangerous human to ever live.
1684500	1694500	I could and will tell you about why but he does a great job all by himself right here in this famous quote from January of this year.
1694500	1700500	Right after he spent a minute detailing the wonders of the best case scenario for AI.
1701500	1708500	Sam drop this bad case and I think this is like important to say is like lights out for all of us.
1708500	1712500	I could end my case for Sam being the number one doomer right there.
1712500	1719500	Who the fuck works on something they legitimately think could kill everyone on earth.
1719500	1725500	In a highly controlled governmental military framework weapons of mass destruction are designed and built.
1725500	1733500	This is not that the sale of a fucking egg is far more regulated than AI right now.
1733500	1742500	And Sam Altman of Missouri believes he has the right the morning right to do the work he's doing.
1742500	1751500	How is that fucking possible Sam you are the bringer of the doom Sam you are the fucking doomer.
1751500	1763500	Not EleIzer not Max not Connor not Paul not Roman you Sam Altman are the number one doomer in AI.
1763500	1776500	On that same January day Sam went on to talk about his safety concern being mostly with misuse and then he takes a full pot shot at the AI safety community just listen.
1777500	1789500	I'm more worried about like an accidental misuse case in the short term where you know someone gets a super powerful like it's not like the AI wakes up and decides to be evil.
1789500	1799500	And I think all of the sort of traditional AI safety thinkers reveal a lot about more about themselves than they mean to when they talk about what they think the AGI is going to be like.
1799500	1803500	But but I can see the accidental misuse case clearly.
1803500	1804500	I'm sorry.
1804500	1815500	Right there the number one doomer in AI had the gall the audacity to take a shot at what he calls this sort of traditional AI safety thinkers.
1815500	1823500	His burn is somehow that they reveal something clearly off about themselves when they talk about what AGI is going to be like.
1823500	1834500	Does she mean that EleIzer Yadkowski and Max Tagmark think AGI is going to be bad and mean because they're really bad and mean deep down inside.
1834500	1837500	What the actual fuck is he even talking about.
1837500	1845500	AGI will be simply like every other intelligence anywhere that's not human.
1845500	1850500	It won't share human goals and values.
1850500	1858500	One more clip from that event here Sam bemoans the need for more safety and alignment work.
1858500	1870500	From the guy who is the CEO of the leading AI company on Earth, a company that puts its money 99 cents on the dollar on capabilities and one cent on safety work.
1870500	1876500	Yeah, I think it's like impossible to overstate the importance of AI safety and alignment work.
1876500	1879500	I would like to see much, much more happening.
1879500	1886500	Oh, wow, Sam should try to find somebody who knows somebody who's in charge at Open AI who can fix this.
1886500	1887500	Come on.
1887500	1889500	Honestly, it's so pathetic.
1889500	1890500	He read.
1890500	1891500	Wow.
1891500	1893500	Vannan, it's been a pleasure so much.
1893500	1894500	Thank you.
1894500	1900500	Sam Altman, the world's most dangerous man and number one doomer is bankrolled by Microsoft.
1900500	1909500	At Dev Day for Open AI, Microsoft CEO Sacha Nadala casually dropped in on his boy wonder, Sam the doomer.
1909500	1912500	This is a sickening 20 seconds of video.
1912500	1921500	In it, you'll see Sacha Nadala literally say, lastly, safety is super important because you know you always leave the most important things for last.
1921500	1928500	And then you'll see Sam Altman say the doomiest words thus far uttered in human history.
1928500	1938500	And then the last thing is, of course, we're very grounded in the fact that safety matters and safety is not something that you'd care about later, but it's something we do shift left on and we're very, very focused on that with you all.
1938500	1939500	Great.
1939500	1940500	Well, I think we have the best partnership in tech.
1940500	1942500	I'm excited for us to build AI together.
1942500	1943500	I'm really excited.
1943500	1944500	Have a friend.
1944500	1945500	Thank you very much for coming.
1945500	1948500	It really is too much to take.
1948500	1950500	He knows his work.
1950500	1954500	His choices may kill us all.
1954500	1957500	And yet he continues.
1957500	1962500	Sam Altman was asked a great question about just this at a recent Bloomberg event.
1962500	1967500	He signed a 22 word statement, warning about the dangers of AI.
1967500	1977500	It reads, mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war.
1977500	1979500	Connect the dots for us here.
1979500	1985500	How do we get from a cool chat bot to the end of humanity?
1985500	1987500	Well, we're planning not to.
1987500	1989500	That's the hope.
1989500	1993500	But there's also the fear.
1993500	2006500	I mean, I think there's many ways it could go wrong, but we work with powerful technology that can be used in dangerous ways very frequently in the world.
2006500	2014500	And I think we've developed over the decades good safety system practices in many categories.
2014500	2015500	It's not perfect.
2015500	2016500	And this won't be perfect either.
2016500	2017500	Things will go wrong.
2017500	2022500	But I think we'll be able to mitigate some of the worst scenarios you can imagine.
2022500	2027500	You know, bioterrorists like a common example, cybersecurity is another like many more we could talk about.
2027500	2041500	But as this technology, like the main thing that I feel is important about this technology is that we are on an exponential curve and a relatively steep one.
2041500	2047500	Human intuition for exponential curves is like really bad in general.
2047500	2051500	It clearly was not that important in our evolutionary history.
2051500	2061500	And so I think we have to, given that we all have that weakness, I think we have to like really push ourselves to say, OK, GPT-4, you know, not a risk like you're talking about there.
2061500	2064500	But how sure are we that GPT-9 won't be?
2064500	2072500	How sure are we that chat GPT-9 won't murder your whole family and mine, he asks.
2072500	2077500	That is why Sam Altman is the number one doomer in AI.
2077500	2091500	He's not sure that his own product, a few versions from now, won't slaughter my family, your family, every family, and all their pets too.
2091500	2096500	He says maybe it's GPT-9 that's a lethal threat.
2096500	2104500	AI safety researchers say it could be GPT-5 or 6.
2104500	2110500	Are we really just debating when not what?
2110500	2116500	Is the only question left what version kills us?
2116500	2124500	Have we really just conceded the doomers can make their deadly weapons systems that will no regulation, no oversight?
2124500	2131500	No, we have not conceded that we have fucking not.
2131500	2137500	OK, so it is almost the end of the year.
2137500	2145500	I've been making these shows for eight weeks straight and it has honestly been as fulfilling as it has been hard work.
2145500	2153500	I am so excited what we can do with this podcast together in 2024 as we build this movement and this community.
2153500	2161500	It starts with each one of us convincing others one at a time that this is the only issue that really matters anymore.
2161500	2165500	So for 2024 I have big plans for humanity.
2165500	2173500	I have so many show ideas and this story is going to unfold in real time so we'll be reacting in real time as well.
2173500	2185500	Let us build a community of people here who are unafraid to plainly convincingly evangelize for AI existential risk awareness.
2185500	2193500	Let's use each other as resources, helping each other get better at convincing others by sharing our experiences.
2193500	2199500	It's not easy to convince people and it's not fun and we could all be way better at it.
2199500	2207500	OK, finally, I'm going to take a few days to put together the next show just to spend a little extra time to be with my family over the holidays.
2207500	2210500	So the next show will come out right after the new year.
2210500	2213500	So I want to end with a holiday message.
2213500	2218500	2023 was a year of shock and disbelief.
2218500	2221500	In 2024, let's get our optimism back.
2221500	2224500	Let's get our joy back.
2224500	2233500	If the worst is to come, I want to spend every second enjoying life as much as humanly possible.
2233500	2238500	I will not let these doomers rob me of my time or of my joy.
2238500	2244500	Let's make 2024 a year of celebration and loving life.
2244500	2247500	Let's celebrate what we love about being humans.
2247500	2252500	Let's celebrate what we love about living on Earth.
2252500	2256500	Let's spend time with our friends and family more than ever before.
2256500	2259500	Take that vacation, go out to that dinner.
2259500	2263500	Let's live like every second is precious.
2263500	2271500	And while we celebrate life and humanity, let's kick and scream and fight like crazy to survive.
2271500	2277500	So each show next year will end with a celebration of something that is wonderful about life.
2277500	2281500	I'm going to share from my life and I hope you'll share from yours.
2281500	2286500	So that gives me something big to smile about and something I'm really looking forward to sharing with you in 2024.
2286500	2290500	Please have a wonderful holiday season and a happy new year.
2290500	2295500	Whatever, wherever you celebrate, hold on to your loved ones close.
2295500	2300500	For Humanity, I'm John Sherman. I'll see you back here next year.
2311500	2313500	Thank you.
