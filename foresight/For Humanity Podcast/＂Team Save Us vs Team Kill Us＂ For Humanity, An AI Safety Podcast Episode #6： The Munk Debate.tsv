start	end	text
0	2560	In biotech, it's exactly the other way around.
2560	4400	If you had come up with a new medicine and said,
4400	6400	this cures cancer, it's awesome,
6400	9040	you can't just go sell it in the supermarket.
9040	11840	The monk debate focused on AI safety,
11840	14640	and it was absolutely extraordinary.
14640	18880	Claiming that AI is an existential threat is itself harmful.
18880	22000	It misleads people about the current state
22000	23600	and likely future of AI.
23600	26640	I've been excited to put together this show for some time now.
26640	27800	It's spicy.
27800	30200	Apologize I'm going to be cursing more in this show
30200	32600	than I normally would.
32600	37320	Such sensationalist claims deflect attention
37320	39640	for real immediate risks.
39640	43840	One set of emotions that we can hardwire into them
43840	45480	is subservience.
45480	49600	The most flamboyantly obnoxious cavalier.
49600	52000	Shit on my leg and tell me it's raining.
52000	54360	I'm going to cheer as the world ends.
54360	56080	Risk means nothing to me.
56080	60160	Fuck your kid's future lunatic.
60160	64160	Is the director of AI research at Metta.
64160	66360	His name is Jan LeCun.
66360	69200	Of course, if it's not safe, we're not going to build it, right?
74480	77760	I mean, will you build a bomb that just blows up randomly?
77760	80720	No, right?
80720	84200	If it's not safe, we're not going to build it.
84200	87320	What a total load of shit.
87320	89840	How the fuck would you know if it's not safe
89840	92600	until it's too late to stop it?
92600	95000	That's the whole fundamental fucking problem
95000	96080	with this situation.
104280	108200	Welcome to For Humanity, an AI safety podcast,
108200	110480	episode six, The Monk Debate.
110480	111960	I'm John Sherman, your host.
111960	113640	Thanks so much for joining me.
113640	116840	This is the AI safety podcast for the general public.
116840	118640	No tech background required.
118640	122680	This show is exclusively about the threat of human extinction
122680	125000	from artificial intelligence.
125000	127640	Please, right now, if you would, hit like
127640	131680	and hit that subscribe button if you haven't hit it already.
131680	134920	Repost these videos on your personal social channels
134920	137760	and please tell people about this show.
137760	139960	Those are some ways that we can help spread the word
139960	141320	about these issues.
141320	144560	Last week, I challenged you to convince someone in your life
144560	148000	that the risk of human extinction
148000	150200	from artificial intelligence is an issue
150200	152160	that requires their attention.
152160	153520	This week, I want to go one further
153520	154480	and challenge you again.
154480	156040	This week, I want to challenge you
156040	157920	to talk to someone about these issues
157920	160160	that is totally outside of this debate,
160160	163720	someone that has no clue that AI is a threat to them.
164760	167080	Here's an easy place, I think, to start.
167960	172080	Every mom and dad out there needs to know about this.
172080	179080	Moms and dads, fellow moms and dads, AI is the biggest threat
179080	182240	to our children in the short term.
182240	186640	It's not fentanyl, it's not some creeper in the park,
186640	188640	it's not drunk driving.
189640	194480	AI is the biggest short term threat to our children.
195480	197480	Parents will do anything for their kids.
197480	199720	So let's start with parents.
199720	202400	Maybe this can be their wake-up call.
202400	205120	I have been genuinely encouraged by the reaction
205120	208200	I've been getting to the show online and in person,
208200	211400	but we can get this message out much further.
211400	212760	And if you like what I'm doing,
212760	215200	I'm asking you to step out of your comfort zone,
215200	218760	share this very serious, very unsettling news
218760	220360	with someone you know.
220360	222160	I'm going to tell you this,
222240	225240	it is not going to be a fun experience.
225240	229440	It sucks to be talking about this stuff.
229440	232960	I want you to know that I hate sitting here
232960	234920	and talking about this stuff,
234920	239440	as much as you probably do sitting there and hearing it.
239440	241880	It fucking sucks.
244360	247280	It unthinkably, absolutely fucking sucks
247280	249880	to seriously talking about this stuff.
250000	255000	It fucking sucks to seriously talking about human extinction.
256520	261520	But we are really threatened by AI killing all humans
262960	264720	in as soon as two years.
264720	266680	That is not hyperbole.
266680	269360	That is literally what the AI companies
269360	274360	and AI safety experts are openly admitting and telling us.
275800	279560	So I can't just go about my normal life anymore.
279560	281440	And if you believe what I've been telling you
281440	283440	and what they've been telling you,
283440	285120	you really can't either.
286120	288280	So I'm going to sit here every week
288280	289920	and try to get the word out.
289920	291080	And I need your help,
291080	293120	and I need the help of everyone you know.
293120	294640	What if one year from today,
294640	297760	everyone on earth was of the understanding
297760	300120	that AI is a lethal threat to their family,
300120	301120	to their children,
301120	303880	to their children's yet unborn children,
303880	305940	to every living thing on earth.
306780	309860	In just a minute, we'll get today's show going.
309860	312180	And I think you're going to really love it.
312180	315380	I've been excited to put together this show for some time now.
315380	316540	It's spicy.
316540	318940	Apologize, I'm going to be cursing more in this show
318940	320540	than I normally would.
320540	323540	But first, I want you to hear from some important people
323540	327540	in the news who are openly, recently discussing
327540	331420	totally unthinkable things about artificial intelligence.
331420	335420	So PDOOM is a technical term used for the purpose
336020	337860	of using artificial intelligence.
337860	342340	And it's also used for the percentage chance we all die.
342340	347340	Anthropic CEO Dario Amadei recently put it at 10 to 25%.
348820	352180	AI safety research king, Elias Ryukowski,
352180	354180	puts it at 98%.
356380	358340	In the last week or so in the news,
358340	361380	enter Lena Kahn, the commissioner,
361380	364940	excuse me, the chairman of the Federal Trade Commission,
365220	369420	personal PDOOM, the percentage chance
369420	372100	we all die from AI.
372100	374780	What is your PDOOM, Lena Kahn?
374780	378620	What is your probability that AI will kill us all?
379860	381780	I have to stay an optimist on this one.
381780	385580	So I'm going to hedge on the side of lower risk there.
385580	387100	So are you zero?
387100	390180	No, no, not zero, maybe like 15%.
390180	391020	Oh, all right.
391020	391860	Yeah.
391860	394220	And they say there are no techno optimists in the government.
394500	399500	This is a very high ranking U.S. government official.
400660	402900	Did she go running back to her office
402900	406500	and say, stop everything, we have to do something about this?
406500	409620	No, not at all.
409620	414620	Somehow, a 15% chance of every last human on Earth
416020	421020	being murdered by AI is somehow at a very high level.
424220	425340	Acceptable.
427540	429820	This is the cost of doing business?
432340	435100	There is no fucking business if we're all dead.
439860	442180	There was a ton of drama at OpenAI recently,
442180	443180	as I'm sure you know.
443180	448180	It seems very clear that money and speed won
448620	451860	and safety lost, but for three days,
451860	455220	OpenAI had an interim CEO named Emmett Shear,
455220	457460	the former CEO of Twitch.
459180	462420	This is an important Silicon Valley leader
462420	464580	who clearly gets this stuff.
464580	466020	Listen.
466020	467740	Generally, I am very pro-technology
467740	469260	and I really believe the upsides
469260	470860	usually outweigh the downsides.
470860	472900	Every technology can be misused.
472900	474980	Regulating early is usually a mistake.
474980	476900	I have a very specific concern about AI.
476900	477940	We built an intelligence.
477940	478780	It's kind of amazing, actually.
478780	479900	It may not be the smartest intelligence,
479900	481340	but it is unintelligence.
481340	484220	It can solve problems and make arbitrary plans.
485340	488300	At some point, as it gets better,
488300	489980	the kinds of problems it will be able to solve
489980	494060	will include programming, chip design, material science,
494060	497020	power of production, all of the things you would need
497020	499700	to design an artificial intelligence.
499700	501580	At that point, you will be able to point
501580	504140	the thing we've built back at itself.
504140	505940	And this will happen before you get that point
505940	506780	with humans in the loop.
506780	508420	It already is happening with humans in the loop,
508420	511320	but that loop will get tighter and tighter and tighter.
511320	512720	And faster and faster and faster
512720	515880	until it can fully self-improve itself.
515880	520520	At which point, it will get very fast very quickly.
520520	521440	And that kind of intelligence
521440	523640	is just an intrinsically very dangerous thing
523640	525800	because intelligence is power.
525800	528320	Human beings are the dominant far alive on this planet
528320	529840	pretty much entirely because we were smarter
529840	530600	than the other creatures.
530600	533280	Now, I just laid out a chain of argument
533280	535960	with a lot of if this, then this, if this, then this,
535960	537560	if this, then this.
537560	541240	I know Eliza thinks that like we're all dooms for sure.
543440	546480	I buy his doom argument, I buy the chain and the logic,
546480	550160	like my P-doom, my probability of doom is like,
550160	551560	my bid-ask spread and that's pretty high
551560	552440	because I have a lot of uncertainty,
552440	557440	but I would say it's like between like five and 50.
558920	560800	So there's a wide spread.
560800	562800	I think Paul Cristiano 50, you know.
562800	566080	Paul Cristiano who handled a lot of the stuff
566120	569360	with an open AI, I think said 25 to 50.
569360	572720	It seems like if you talk to most AI researchers,
572720	575200	there's some preponderance of people that give some percentage.
575200	577160	That should cause you to shit your pants.
578240	579960	I think it's important for you to hear
579960	581840	just how little of a secret
581840	585600	the existential threat we face is.
586480	590200	This is not fringe conspiracy shit in any way.
590200	591700	This is not opinion.
592700	596700	I'm just connecting the dots of the facts
597620	600340	that the makers of AI and many others
600340	603060	are laying out there openly saying.
603940	606300	So on to today's show.
606300	608500	All right, wouldn't it be great
608500	613060	if AI safety was substantively debated
613060	615820	on American television every night?
615820	619340	Talk about everything else under the sun, so much crap.
619340	620660	Wouldn't it be great if we talked about
620660	622260	really the important things?
623140	626340	Well, in Toronto, there's a wonderful organization
626340	629460	that runs what's called the Monk Debates.
629460	633100	They're a series of exceptionally well done televised debates
633100	636680	on a wide variety of topics affecting public life.
636680	639460	The Monk Debate focused on AI safety
639460	642740	and it was absolutely extraordinary.
642740	645960	Each debate is framed around a single statement
645960	649040	and it's two on two, pro versus con.
649920	652640	The statement for the AI safety debate was,
653560	656880	be it resolved, AI research and development
656880	659000	poses an existential threat.
660440	662440	Be it resolved, AI research and development
662440	664680	poses an existential threat.
664680	667640	Well, as you should know from watching this show
667640	671240	for humanity, that statement should not be controversial.
671240	675680	It is a widely accepted fact in most circles.
675680	678840	It's something nearly everyone who's anyone in big AI
678840	682880	signed a statement saying that AI is an existential threat
682880	686720	along with the lines of pandemic and nuclear war.
686720	688440	So the whole premise of this debate
688440	692920	is something nearly the whole industry already concedes,
692920	697920	but not everyone, not nearly, sadly.
699440	701120	For my non-tech audience out there,
701120	703120	it may shock you to know that there is a small
703120	707600	but extremely powerful group of people in Silicon Valley
707640	711480	who call themselves effective accelerationists.
713480	715840	They believe that we will solve humanity's problems
715840	720040	by racing towards technological advances.
720040	721160	For the purpose of this show,
721160	723760	we're just gonna call them accelerationists.
725000	728460	Since I opened the For Humanity X account
728460	730240	more than a month ago,
730240	732320	I've been in there mixing it up with a lot of people
732320	734440	and I have gotten a real palpable sense
734480	737960	of these folks and their attitudes.
737960	740000	It's my personal opinion.
740000	742960	They're obnoxious, they're bullies,
743840	746480	they're know-it-alls of the worst kind
746480	750620	and they somehow have the audacity and arrogance
750620	754640	to believe that you and I and everyone else on earth
754640	759640	has given them permission to risk ending the future
760640	764180	for our families forever.
766520	768440	They believe we've authorized this,
770080	772120	but their desire to accelerate technology
772120	774960	affects everyone, not just them.
774960	779960	So the single biggest asshole among them,
780280	784440	the most flamboyantly obnoxious cavalier,
784440	786840	shit on my leg and tell me it's raining,
786840	789200	I'm going to cheer as the world ends,
789200	793240	risk means nothing to me, fuck your kid's future, lunatic,
794960	798960	is the director of AI research at Metta.
798960	802880	His name is Yann Lecun, he's French
802880	805480	and that kind of kills me because I love the French
805480	807240	and hearing him say all this bullshit
807240	809080	in that beautiful French accent
809080	810920	makes it even more painful.
811960	814320	Yann is one half of Team Khan
814320	817160	in the monk debate on AI safety.
817160	819480	Listen closely as you hear him speak,
819480	821800	you'll hear no specifics.
821800	824160	He's gonna tell you that something much smarter
824160	829160	than a human is going to be subservient to humans.
831000	833840	Ladies and gentlemen, with no further ado,
833840	837280	presenting the one and only Yann Lecun.
838800	842100	Those systems are controllable, they can be made safe
842140	846460	as long as we implement the safety objectives.
847580	850460	And the surprising thing is that they will have emotions,
850460	853940	they will have empathy, they will have all the things
853940	858060	that we require entities in the world to have
858060	860340	if we want them to behave properly.
860340	863580	So I do not believe that we can achieve anything close
863580	867700	to human level intelligence without endowing AI systems
867740	871940	with this kind of emotions,
872940	874540	similar to human emotions.
874540	876420	This will be the way to control them.
876420	881420	Now, one set of emotions that we can hardware into them
882100	883660	is subservience.
883660	887180	So I have a positive view, as you can tell,
887180	890940	and I think there is a very efficient way
890940	893580	or good way of making AI systems safe
893580	896140	is gonna be arduous engineering,
896140	900060	just like making turbojet safe, it took decades.
901140	903340	And it's hard engineering, but it's doable.
904500	908540	Just like making a turbojet, says Yann Lecun.
908540	910880	That is deliberately misleading.
912780	915220	Giving them emotions will allow us to control them,
915220	917180	says Yann Lecun.
917180	921540	That is incredibly optimistic speculation at best.
921540	926540	We can hardwire them into subservience, says Yann Lecun.
926860	930380	That is simply ridiculous to claim.
930380	931540	We might be able to do that,
931540	935020	as many safety researchers say, though, that we won't.
935020	937020	But even those who say we might be able
937020	940340	to make them subservient says it would take decades,
940340	943240	like three to five decades, to do the research
943240	944740	to figure out how to do that.
944740	948700	And as you know, the timeline is two to 10 years
948700	953340	for AGI, the Singularity and the Unknown Future after that.
953340	957940	These two timelines are catastrophically misaligned.
957940	960460	But let's get back to the debate.
960460	964500	So Yann's tag team partner is Melanie Mitchell.
964500	968460	She has been working on AI research since the 1980s
968460	970220	as a college student.
970220	973020	I will let Melanie Mitchell introduce herself,
973020	975580	but I will say, holy shit, she is wild.
975580	976500	She's pissed off.
976500	979380	We're even talking about existential risk.
980700	983420	First, I'll argue that the possible scenarios
983420	986900	that people have dreamed up for AI existential threats
986900	990220	are all based on unfounded speculations
990220	994180	rather than on science or empirical evidence.
994180	996980	Second, well, we can all acknowledge
996980	1000820	that AI presents many risks and harms.
1000820	1005620	None of them rise to the extreme level of existential,
1005620	1009420	saying that AI literally threatens human extinction,
1009420	1011980	says a very high bar.
1011980	1016100	Finally, claiming that AI is an existential threat
1016100	1018060	is itself harmful.
1018060	1021220	It misleads people about the current state
1021220	1023420	and likely future of AI.
1023420	1028140	Such sensationalist claims deflect attention
1028140	1031180	for real immediate risks.
1031180	1033860	And further might result in blocking the potential benefits
1033860	1037460	that we could reap from technological progress.
1037460	1039140	Holy fucking hell.
1039140	1040900	All right, so I have not done a great job
1040900	1043020	in the first five shows of letting you hear
1043020	1045580	from the other side of this debate,
1045580	1047620	but that's going to change right now.
1047620	1050260	I haven't really included the other side of the AI safety
1050260	1052780	debate so far because objectively,
1052780	1056340	I find their case so incredibly weak.
1056340	1058700	But you're going to need to make up your own mind on this.
1058700	1062020	So today, we're going to show you a lot of Jan and Melanie,
1062020	1065580	team acceleration, team, we're willing to risk
1065580	1068700	whether your kids get to have kids,
1068700	1071140	and we're going to do so without even attempting
1071140	1073220	to gain your consent.
1076060	1078220	Here's some more of Jan Lacoon laying out his case
1078220	1083220	as to why AI is in no way a threat of human extinction.
1083460	1084980	I hope you'll enjoy as I do
1084980	1088300	when the monk debates moderator jumps in to correct
1088300	1091540	the completely disingenuous misleading frame
1091540	1095580	that Jan puts around the question of why an AI system
1095580	1098300	would cause human extinction.
1098300	1100260	As you well know by now,
1100260	1102820	human extinction would not come from an AI system
1102820	1105460	with a desire to arbitrarily dominate.
1105460	1108860	These systems won't be mad at us or vindictive.
1108860	1111700	AI safety experts believe they will just have
1111700	1113820	different goals than ours.
1113820	1118220	But here's Jan spinning his deceptive case
1118220	1119660	and getting called on it.
1119660	1122500	Because we are social animals, we are a social species,
1122500	1126060	and nature has evolved us to organize ourselves
1126060	1128380	hierarchically like baboons, like chimpanzees,
1129460	1130380	not orangutans.
1130380	1132380	Orangutans have no desire to dominate anybody
1132380	1134340	because they're not a social species.
1134340	1138900	So this desire to dominate has nothing to do with intelligence.
1138900	1141780	They're almost as smart as we are, by the way.
1141780	1143580	So this is nothing to do with intelligence.
1143580	1147300	We can make intelligent machines that are superior to us,
1147300	1148900	but have no desire to dominate.
1150460	1151940	I lead a research lab,
1151940	1153980	I only hire people who are smarter than me,
1153980	1155820	none of them want my job.
1155820	1156660	Now.
1156660	1157500	So.
1158380	1160380	But Jan, is it just a second point?
1160380	1161700	Just to refer to the other side of the debate,
1161700	1162980	because it's something I think the audience
1162980	1164340	would appreciate understanding.
1164340	1166660	It's not so much the desire to dominate,
1166660	1168180	it's the control problem.
1168180	1170540	It's that you've set them some goals,
1170540	1172740	maybe very noble and great goals,
1172740	1175660	but they start doing other things to achieve those goals,
1175660	1177460	which are antithetical to our interests.
1177460	1179540	It's not that they're trying to dominate,
1179540	1182980	it's that there is a tragedy of the commons that goes on.
1182980	1183820	It's the same thing with companies.
1183820	1185420	This is the goal and I'm not a problem.
1185420	1187660	So how do we design goals for machines
1187660	1189380	so that they behave properly?
1189380	1191140	And again, this is something that's,
1191140	1193380	you know, a difficult engineering problem,
1193380	1196420	but this is not a problem that we are unfamiliar with,
1196420	1200220	because as societies, we've been doing this for millennia.
1200220	1201740	This is called making laws.
1202740	1206700	Sure, we'll just make some laws, problem solved.
1208180	1211620	We've been solving problems like this for millennia.
1213660	1216380	No, we have not.
1216380	1221380	No problem ever before in our past is in any way
1221740	1224940	like the challenge of artificial intelligence,
1224940	1228300	but don't worry, I don't have to make the counterarguments.
1228300	1232700	We have an all-star duo on the other side of this debate.
1232700	1235300	Please say hello to our old friend
1235300	1238540	and powerhouse AI safety researcher,
1238540	1241460	MIT professor, Max Tegmark.
1241460	1243540	Back in the Stone Age with a rock,
1243540	1245820	maybe someone could kill five people.
1245820	1249060	300 years ago with a bomb, maybe a hundred people.
1250700	1255740	In 1945 with a couple of nukes, 250,000 people
1255740	1258100	with bioweapons, even more,
1258100	1261180	with nuclear winter, according to a recent science article,
1262060	1263740	over five billion people.
1263740	1268740	Now the blast radius has risen up to about 60% of humanity.
1269540	1272140	And since, as we'll argue,
1272140	1274020	superhuman intelligence is going to be
1274020	1276540	way more powerful than any of this.
1276540	1280820	Its blast radius can easily be 100% of humanity,
1280820	1284300	giving it the potential power to really wipe us out.
1284300	1289300	Again, this is different than anything else ever before.
1289420	1294300	Nuclear bombs cannot make more nuclear bombs by themselves.
1294300	1297380	Nuclear bombs cannot decide to detonate themselves
1297380	1301100	at a location and time of their choosing.
1302060	1305620	This is different than anything else ever before.
1305620	1308660	Max, please tell the people.
1308660	1310940	It can do all the intelligent things
1310940	1313020	that we humans can do just better.
1314060	1317900	For example, it can do goal-oriented behavior.
1317900	1322380	It can persuade, manipulate, hire people,
1322380	1327380	start companies, build robots, do scientific research.
1328540	1329500	It could, for example,
1329500	1331660	research how to make more powerful bioweapons
1332780	1336020	or how to make even more intelligent systems
1336020	1339660	so it could recursively self-improve itself.
1339660	1343340	It also could do things that we humans cannot do at all.
1343340	1345500	It could very easily make copies of itself.
1345900	1350260	If you imagine for a moment a super human AI
1350260	1353180	that can think, say a thousand times faster
1353180	1354140	than a human researcher,
1354140	1358740	so it can in nine hours do years worth of research,
1358740	1361140	but now instead think of a million of those,
1361140	1364340	a swarm of a million super intelligent AIs
1364340	1366420	where as soon as one of them discovers something new,
1366420	1369540	it can instantly share that new skill with all of them.
1369540	1372780	That's the kind of power we're talking about here.
1372820	1377620	And finally, super human AI will probably be
1377620	1379860	a very alien kind of intelligence
1379860	1383580	that lacks anything like human emotions or empathy.
1385380	1388380	I could listen to Max Tagmark talk all day.
1388380	1390180	I really hope to have him on this show
1390180	1391620	sometime in the near future.
1392620	1395140	In biotech, it's exactly the other way around.
1395140	1396660	If you had come up with a new medicine
1396660	1398980	and said, this cure is cancer, it's awesome,
1398980	1401460	you can't just go sell it in the supermarket
1401460	1402780	until someone proves it is there.
1402780	1405900	It's your job to convince the Food and Drug Administration,
1405900	1408140	the US or the Canadian authorities or whatever
1408140	1410860	that this is safe and that the benefits,
1410860	1413060	yes, the benefits outweigh the risks.
1413060	1415660	It should be the responsibility of the companies
1415660	1417540	that the first prove that this is safe
1417540	1418820	before it gets deployed.
1418820	1421740	We need to become like biotech.
1421740	1425300	In this monk debate, Max is hardly alone.
1425300	1426220	This is kind of fun,
1426220	1428380	got a little bit of soap opera drama for you.
1428380	1430620	So the highest prize in computer science
1430660	1432100	is called the Touring Award,
1432100	1434420	named after Alan Turing, the father of AI
1434420	1437060	who we talked about a little bit last week.
1437060	1442060	In 2018, the Touring Award went to three AI researchers
1442220	1444580	for their breakthrough work on neural networks
1444580	1445900	and artificial intelligence,
1445900	1448860	which was one of the two major advancements in AI
1448860	1450980	that got us to where we are today.
1451860	1455580	The three Touring Award winners in 2018 were Jeffrey Hinton,
1455580	1457580	who you've met in previous shows,
1457580	1461500	the Quick Google, famously to voice his concerns
1461500	1464780	about AI and human extinction.
1464780	1467220	The other two winners were two of his former students
1467220	1469300	from the University of Toronto.
1471060	1473660	So on our stage at the monk debate,
1473660	1476940	we have two of those former students,
1476940	1480560	the two former students who won the Touring Award
1480560	1481980	with Professor Hinton.
1484180	1487380	One of them is Yan Lacoon.
1488580	1492420	He now says his old mentor and professor, Jeffrey Hinton,
1492420	1496400	is a doomer who's totally wrong about AI risk.
1497900	1500820	The other of Professor Hinton's old students
1500820	1504260	is Maxis' teammate for the debate,
1504260	1506740	AI researcher, Yoshua Benjiro.
1508180	1511060	Once we have machines that have a self-preservation goal,
1511060	1513780	well, we are in trouble.
1514620	1518580	You know, think about what happens when you want to survive.
1518580	1522220	You don't want others to turn you off, right?
1522220	1524740	And you need to be able to control your environment,
1524740	1526260	which means control humans.
1526260	1530820	So existential risk isn't just, well, we all disappear.
1530820	1533620	It might be that we're all disempowered,
1533620	1536980	that we are not anymore in the control of our destiny.
1536980	1539820	And I don't think this is something we want.
1539820	1542660	Not extinct, but not in charge.
1542660	1544780	That is not something we want at all.
1544780	1546980	But this debate, just like this show,
1546980	1550580	we're gonna focus only on human extinction risk.
1550580	1555580	Yan Lacoon thinks there's no extinction risk at all.
1555700	1560460	Watch as he mocks this grave concern
1560460	1564580	as mere science fiction and marvel
1564580	1569580	as he backs up his bold claims with absolutely nothing.
1570500	1572460	As fiction scenarios of, you know,
1572460	1575300	the earth being wiped out, humanity being wiped out,
1575300	1577460	this sounds like a James Bond movie, right?
1577460	1579860	It's like the supervillain who goes in space
1579860	1582620	and then kind of puts some deadly gas
1582620	1584340	and eliminates all of humanity.
1584340	1585620	It's a James Bond movie.
1585620	1586860	And I can't disprove it.
1586860	1588460	The same way, if I tell you,
1588460	1590900	I used the Bertrand Russell idea,
1590900	1593180	if I tell you there is a teapot flying
1593180	1597460	between the orbits of Jupiter and Saturn,
1597460	1599100	you're gonna tell me I'm crazy.
1599100	1601260	But you can't disprove me, right?
1601260	1603060	You can't disprove that assertion.
1603060	1606100	It's gonna cost you a huge amount of resources to do this.
1606100	1609420	So it's kind of the same thing with those doomsynarios.
1609420	1612220	They're sci-fi, but I can't prove that they're wrong.
1613340	1615100	But the risk is negligible.
1615100	1618660	And the reason it's negligible of extension
1618660	1621140	is because we build those things.
1621140	1621980	We build them.
1621980	1623580	We have agency.
1623580	1626380	This is not superhuman intelligence.
1626380	1628100	It's not something that's gonna just happen.
1628100	1629420	So, of course, if it's not safe,
1629420	1631340	we're not gonna build it, right?
1636420	1637580	I mean, will you build a bomb
1637580	1639700	that just blows up randomly?
1639700	1640980	No, right?
1642220	1644500	If it's not safe, we're not going to build it.
1645700	1648820	What a total load of shit.
1648820	1651340	How the fuck would you know if it's not safe
1651340	1654100	until it's too late to stop it?
1654100	1656500	That's the whole fundamental fucking problem
1656500	1657780	with this situation.
1657780	1660860	But, Jan, you know this much better than I do.
1660860	1663460	So seriously, this is it, right?
1663460	1665100	It would be reasonable after watching
1665100	1669260	the first five episodes of this podcast to say,
1669260	1672300	I'd like to hear more from the other side about this.
1672300	1674620	So this is your chance to evaluate
1674620	1676180	the strength of their logic.
1677420	1680900	I went into this hoping to be persuaded by them,
1680900	1685140	hoping to be persuaded that everything is fine.
1685140	1687580	But now it's your turn.
1687580	1690860	Let's do a little Jan and Max mode debate,
1690860	1694780	soundbite ping pong, and then you decide for yourself.
1694780	1698220	If you think everything is just fine.
1698220	1700900	Remember what people were saying just before year 2000?
1702340	1704020	Satellites were gonna fall out of the sky
1704020	1705300	and crash into cities,
1705300	1708100	and the phone system was gonna crash
1708100	1709500	and civilization will end.
1712140	1713700	It didn't happen.
1713700	1714540	We're still here.
1715540	1716780	So I think there's a little bit
1716780	1720620	of the same kind of feeling of uncertainty.
1720620	1722020	A lot of people have the feeling
1722020	1723420	that bad things are gonna happen
1723420	1724940	because they're not in control.
1726300	1728860	They have the feeling that AI is just gonna happen,
1728860	1730220	and there's nothing they can do about it,
1730220	1731260	and that creates fear.
1731260	1733700	And I can completely understand that.
1733700	1737900	But some of us, in what could be construed
1737900	1741900	as a driver's seat, there are plans
1741900	1742980	to make those things safe.
1743020	1745580	Stock traders will tell you the past performance
1745580	1748300	is not an indicator of future performance,
1748300	1750900	future results, yeah, future results.
1750900	1753140	And it would be a huge mistake
1753140	1755060	in an exponential technological growth
1755060	1758020	to assume that just because something happened one way
1758020	1760980	in the past is gonna continue being this way.
1760980	1764580	AI is gonna be subservient to human.
1764580	1766020	It's gonna be smarter than us,
1766020	1767780	but it's not gonna reduce our agency.
1767780	1770140	On the contrary, it's going to empower us.
1771140	1772340	It's like having a staff
1772340	1774420	of really smart people working for you.
1774420	1777540	It's naive to think that just because you make something smart,
1777540	1779740	it's only gonna suddenly care about humans.
1779740	1782340	Ask some wooly mammoths
1782340	1785780	if they feel so reassured that we're smarter than them.
1785780	1789620	Then therefore, we would automatically adopt mammoth ethics
1789620	1791420	and do things that were good for the mammoths
1791420	1792420	that we didn't.
1792420	1795380	That's why you probably haven't met any.
1795380	1797660	I'll tell you what I think.
1797740	1799900	I think Jan is full of shit
1799900	1803300	and Max makes all the sense in the world.
1804620	1806580	I really wish it was the other way around.
1806580	1810980	I really, really do, but he just isn't at all.
1812940	1814900	There are some great moments in the monk debate
1814900	1817620	on AI safety where the teams really start
1817620	1819380	to mix it up with each other
1819380	1821820	and there are some really incredible exchanges
1821820	1822820	I wanna show you.
1822820	1827460	First, here are the 2018 Turing Award winners.
1827460	1830820	Once united by science, now divided
1830820	1835180	by the potentially existential results of their work.
1835180	1839460	Mixing it up, Jan Lacoon goes for some classic
1839460	1842260	historical comparison bullshit
1842260	1844340	and Yoshua Bengio responds
1844340	1847220	with what I think is force and logic.
1847220	1848700	Socrates was against writing.
1848700	1853100	He said, people are going to lose their memory, right?
1854060	1856980	The Catholic Church was against printing press
1857020	1860260	saying they would lose control of the dogma,
1860260	1863740	which they did, they could do nothing about it.
1863740	1867540	The Ottoman Empire banned the printing press
1867540	1869060	and according to some historian,
1869060	1872500	that's what accelerated their decline.
1872500	1876940	And so every technology that makes people smarter
1877860	1880820	or enables communication between people
1880820	1885820	facilitates education, again, is interestingly good.
1886180	1888380	And AI is kind of a new version of this.
1888380	1890100	It's a new printing press.
1890100	1893260	So long as it doesn't blow up in our face.
1893260	1895700	And because that can match your hand
1895700	1897300	with a printing press, right?
1897300	1901220	Yes, if it's the problem is the scale, right?
1901220	1905460	So long as we built technologies that could be harmful
1905460	1908500	but on a small scale, the goods,
1908500	1911900	the benefits overwhelm the dangers.
1911900	1915060	Now we're talking about building technologies
1915100	1917140	unlike any other technology.
1917140	1917980	No.
1917980	1920460	Because it's technology that can design its own technology.
1920460	1921300	No.
1921300	1922140	Yes.
1922140	1922980	No.
1922980	1923860	I don't think about super human AI.
1923860	1925180	This is the subject.
1925180	1926380	It's under control.
1926380	1927220	It's under control.
1927220	1928540	And we remain under control.
1928540	1931140	It's very much like previous technologies.
1931140	1932980	It's not quite relatively different.
1932980	1934620	The experts have been studying this question,
1934620	1936980	say it's going to be very hard to keep it under control.
1936980	1938660	And that is why I'm here today.
1940780	1943820	Melanie Mitchell is so special.
1944820	1946740	She's pissed off we're even talking
1946740	1950300	about existential risks from AI.
1951580	1953860	Seriously, throughout the month debate,
1953860	1956500	she sounds off on why it is bad for us
1956500	1958740	to even talk about existential risk from AI
1958740	1960580	when there are in her eyes
1960580	1965180	so many more important, urgent threats
1965180	1968420	that need our attention, such as disinformation
1968420	1972460	and fake photos and videos.
1972660	1974100	It takes our attention away
1974100	1976660	from some of the real immediate risks,
1976660	1980660	like disinformation and bias.
1980660	1983460	So are you saying it's a 0% risk?
1983460	1985340	0% existential risk?
1985340	1986340	Is that your claim?
1987260	1992260	Do I think there's a 0% risk of any scenario?
1992260	1993500	No, of course not.
1993500	1994820	What do you think it is then?
1994820	1995660	1%?
1995660	1996500	We're really debating.
1996500	1997900	I can't put a number on it.
1997900	1999540	I think it's quite low.
1999540	2000620	And I think what we're debating
2000620	2004900	is there a reasonable existential risk,
2004900	2008380	a risk of end of civilization in the reasonable future?
2008380	2009460	Otherwise, we wouldn't be up here to be.
2009460	2010500	How high is the high?
2010500	2014180	I'm not gonna like say 0.0000001.
2014180	2015020	I mean, I can't say that.
2015020	2016980	How high is too high for you then?
2016980	2017820	How high is?
2017820	2021300	How high a probability of a risk is too high for you?
2021300	2023700	I don't think we can put a probability on it.
2023700	2024700	We don't know.
2024700	2026340	We don't know enough.
2026340	2028020	No, I mean, it's acceptable.
2028020	2033020	Okay, it's lower than yours being wiped out by meteor.
2034300	2036380	And by the way, I can help with that problem.
2036380	2038340	This is the essential question for anyone
2038340	2041660	who wants to accelerate artificial intelligence.
2041660	2045860	What percentage existential risk to humans
2045860	2047940	is acceptable to you?
2050380	2052940	In my opinion, when you are talking about
2052940	2056900	a potential future with zero human beings,
2056900	2061900	the only acceptable risk percentage is also zero.
2065020	2067900	Here are the two former award-winning partners.
2067900	2070220	Now on opposite sides of the question of
2070220	2072820	don't you just need some good guys with good AI
2072820	2075580	to beat some bad guys with bad AI?
2075580	2078580	Decide for yourself if you think Yan Lacun
2078580	2083580	is being appropriately optimistic or stunningly blind.
2083660	2087580	Bad guys can use AI for bad things.
2087580	2089500	There's many more good guys who can use the same
2089500	2092220	more powerful AI to counteract it.
2092220	2093220	So in the end,
2093220	2094260	Those good guys are gonna win.
2094260	2096620	Sometimes the attacker has the advantage.
2096620	2098740	There is absolutely no reason to believe
2098740	2100180	that's the case for AI.
2100180	2103020	In fact, we are facing this situation right now.
2103020	2103860	How do you know?
2103860	2105820	There's never anything that is completely perfect.
2105820	2106660	Well, that's the issue.
2106660	2108780	That's what we need to do more than what we're doing now.
2108780	2112580	Again, it's the good guys' AI, which is superior,
2112580	2115100	to the bad guys' AI.
2115100	2118460	That's like saying that the way to stop a bad guy
2118460	2121860	with a bio-weapon is to have a good guy with a bio-weapon.
2121860	2123340	That's not what you do.
2123340	2126700	The way you stop a bio-weapon attack is with vaccines
2126700	2128420	and banning bio-weapons
2128420	2131740	and having various forms of regulation and control.
2131740	2136740	Next, Max asks Melanie what I think is a reasonable question.
2136900	2140540	Why the product maker isn't responsible
2140580	2142660	for proving their product is safe
2142660	2145380	before they release it to the general public?
2145380	2148180	But why is it our role to explain to you why it's dangerous?
2148180	2149500	You still haven't answered my question.
2149500	2150980	I've never said it wasn't dangerous.
2150980	2152220	I've asked you twice.
2152220	2154700	What is your plan for avoiding misuse?
2154700	2155700	You haven't told me.
2155700	2156540	You haven't asked you twice.
2156540	2158980	What's your plan for solving the alignment problem?
2158980	2160060	You haven't told me.
2160060	2160900	I've asked you twice.
2160900	2163980	Can we just finish to tell me what your plan is
2163980	2166100	for avoiding the scenario where we get out-competed
2166100	2166940	in disempowerment?
2166940	2167940	You've said nothing.
2168780	2171820	Unfortunately, I don't get paid enough
2171820	2173900	to solve all these problems about AI policy.
2173900	2175660	But if you either fill in medicine to have it,
2175660	2176980	you'd have to have a plan.
2176980	2178980	I don't have to explain why.
2178980	2181500	I think the AI community is developing plans
2181500	2183140	to mitigate risks.
2184100	2189100	I think the community is developing plans
2189100	2193220	to mitigate risks, she says.
2193220	2194860	Is developing plans?
2195780	2197460	This is like saying we'll figure out
2197460	2200060	how to make landing gear mid-flight.
2200060	2203980	We are already currently cruising as a world
2203980	2207260	at 30,000 feet in a plane with no landing gear.
2207260	2210020	The pilots already said, fuck it, let's just take off.
2210020	2211860	We'll figure out landing gear mid-flight.
2213300	2216580	We're hurtling through the air, no landing gear,
2216580	2220380	and no knowledge of when we actually need to land,
2220380	2222620	no knowledge of the timing.
2222660	2225060	But the pilots are saying, please stay calm,
2225060	2228580	do enjoy your snack and beverage service.
2228580	2229620	It's nuts.
2230900	2233860	Let's get back to the debate where Yoshua channels
2233860	2237780	my inner spirit and tells Melanie, you're wrong.
2238780	2242780	They might want to preserve their existence.
2242780	2245340	They don't want to do anything, they're not alive.
2245340	2246700	No, that's very easy to do.
2246700	2248740	No, you're wrong, you're wrong.
2248740	2251260	The systems that, for example,
2252100	2254540	ChadGPT is essentially like an oracle.
2254540	2255780	It doesn't really have a want,
2255780	2257100	although literally it wants to please us
2257100	2258700	because of the reinforcement learning.
2258700	2260700	Yes, it's been trying to please us.
2260700	2264140	Then it's actually easy to put a wrapper around them,
2264140	2267260	to turn them into agents that have goals.
2267260	2270580	It's actually easy to do that humans gave them.
2270580	2271700	Yes, that's their own goal.
2271700	2273940	Yes, and in order to achieve those goals,
2273940	2277060	they're gonna have sub-goals and those sub-goals,
2277060	2280620	for example, may include things like deception.
2280620	2284020	Okay, finally, in one last-ditch effort,
2284020	2285780	Melanie Mitchell tries to drop.
2285780	2289500	Well, the people I talk to all think just like I do,
2289500	2292220	and they all think everything is fine too.
2292220	2297220	You say the people you talk to in AI have this belief.
2297420	2300020	Well, the people I talk to in AI don't have that belief.
2300020	2303620	I think the field is quite split, I would say.
2303620	2305420	And I think there's quite a...
2305420	2307980	Did it coin at 50% that we all get a dime?
2308980	2309980	Well...
2309980	2311700	Are any of the people who sign this letter
2311700	2314500	saying that this is an existential risk?
2314500	2317540	There's many people who have signed letters
2317540	2319140	saying they think it's an existential risk.
2319140	2321140	They don't say over what time scale.
2321140	2322940	Yeah, but this was Jeff Hinton.
2322940	2326660	Yeah, Jeff Hinton is a godfather.
2326660	2328020	But he doesn't know everything.
2328020	2328860	You don't talk to them.
2328860	2332460	Yeah, I think that he's a smart guy,
2332460	2337420	but I think that a lot of people have way over-hyped
2337420	2338580	the risk of these things,
2338580	2341140	and that's really convinced a lot of the general public
2341140	2343740	that this is what we should be focusing on,
2343740	2346660	not the more immediate harms of AI.
2346660	2347500	Over-hyped.
2350300	2351340	Science fiction.
2353460	2354900	Just another Y2K.
2356980	2359460	Just like the invention of the typewriter.
2361740	2363620	Please, tell me in the comments,
2363620	2366420	if you think these arguments made by Melanie and Jan
2366420	2368500	are not total bullshit.
2368500	2370420	Maybe I'm missing something,
2370420	2373100	but I find them not only to be incredibly weak,
2373100	2376060	I find them to be deceptive and manipulative.
2377260	2379180	The audience at the end of the month debate
2379180	2381780	was asked to vote for who they thought won.
2381780	2383620	By a score of 67 to 33,
2383620	2387300	they decided that Max and Yoshua were more convincing
2387300	2389900	than Melanie and Jan.
2389900	2392620	So I hope you got a lot out of that debate.
2392620	2395300	I would encourage you to go to the Monk Debates website.
2395300	2396900	They have a variety of debates
2396900	2398740	on a lot of really interesting subjects,
2398740	2401140	and they're all exceptionally well done,
2401140	2403740	just like that debate was.
2403740	2405300	So for six weeks now,
2405300	2407260	I've been putting out a podcast per week,
2407260	2408940	and as I said, I'm really encouraged
2408940	2411460	by the response I've been getting.
2411460	2416380	The question is, who will win this battle for humanity?
2416380	2419300	The accelerationists, like Jan Lacoon,
2419300	2421820	Melanie Mitchell and Sam Malt,
2421820	2424460	or the decelerationists like Max Tegmark.
2425620	2428460	Currently, if it was an NFL football game,
2428460	2429980	I think the score would be something
2429980	2433340	like 45 to three accelerationists.
2434220	2438580	Acceleration is dominating in funding progress and talent.
2439820	2442100	The game is in the second half,
2442100	2444940	but we don't know when the whistle is gonna blow
2444940	2447200	to say that the game is over.
2448060	2449380	The odds could not be worse,
2449380	2451220	the situation could not be more dire.
2451220	2455380	You, me, and everyone else alive right now
2455380	2458260	can either be blindly let off a cliff,
2458260	2461900	or we can work to have a voice in this,
2461900	2464900	work to learn what is happening and what is to come,
2464900	2468100	and do anything and everything we can
2468100	2470100	to give our children a future.
2471180	2475060	So next week, we're gonna do something really different.
2475060	2476820	Our show aims to move this debate
2476860	2479260	from Tech, YouTube, and X
2479260	2482620	into the family dinner table conversation.
2482620	2485060	I think the voice of parents
2485060	2487820	is sorely missing from this debate.
2487820	2489460	As a parent myself, I can tell you,
2489460	2491740	I've often said that if I was standing on a street corner
2491740	2493580	and my son or daughter was next to me
2493580	2494860	and a bus was coming at us
2494860	2496620	and they were falling into the path of the bus
2496620	2497820	and I could pull them back
2497820	2500260	and I would have to fall and be killed by the bus
2500260	2502420	instead of them, that I would see them
2502420	2504700	falling towards the street corner to safety.
2504700	2506580	I would fall myself into the street
2506580	2507900	knowing the bus was about to hit me
2507900	2510580	and I would do so with a big smile on my face,
2510580	2513580	knowing I'd saved the life of my child, right?
2513580	2515380	That is how parents are.
2515380	2518860	Parents will do anything to protect their kids.
2518860	2522260	Well, my fellow parents,
2522260	2524420	it hurts me deeply to tell you
2524420	2528420	that your children are under a threat more grave
2528420	2533420	than any threat faced by any children ever before.
2533900	2534820	Mine are too.
2536780	2540540	Parents, there is an intruder in your home.
2541740	2544100	There is an intruder in your school.
2545260	2547620	It came from Silicon Valley.
2547620	2552620	It is an alien and it doesn't care about anything
2552620	2553780	that you care about.
2559620	2561700	So next week, I'm gonna talk to three moms
2561700	2564340	who are totally new to this AI safety debate,
2564340	2565740	just three regular moms
2565740	2568380	who through this podcast have become aware
2568380	2572500	of this most grave threat to their families.
2572500	2575540	I wanna know how it feels to know
2575540	2578520	that this biggest threat hasn't even been on their radar.
2579620	2582180	What they think about a few thousand people
2582180	2585700	in Silicon Valley who are more than comfortable
2585700	2589300	risking the mass murder of every child on earth
2591960	2594220	and how these moms plan to go forward
2594220	2596000	knowing what they now know.
2597180	2599700	Should be an interesting show for sure.
2599700	2601660	Thank you so much for watching.
2601660	2603820	For Humanity, I'm John Sherman.
2603820	2605420	I'll see you back here next week.
2605420	2611860	Repeat Twitter, Instagram, Facebook, YouTube.
