1
00:00:00,000 --> 00:00:08,800
Unelected, unvetted, and completely unaccountable, this one man has chosen to put all of our

2
00:00:08,800 --> 00:00:14,880
lives in grave danger so that he can make money and fulfill his dreams of radically

3
00:00:14,880 --> 00:00:21,960
altering human existence as we have never known it without even an effort to get any

4
00:00:21,960 --> 00:00:26,400
consent from you or anyone else in the general public.

5
00:00:26,800 --> 00:00:30,320
Here's the really crazy part of it.

6
00:00:30,320 --> 00:00:33,280
He tells us all of this openly.

7
00:00:33,280 --> 00:00:34,920
There is no deception.

8
00:00:34,920 --> 00:00:37,720
He admits all of it.

9
00:00:37,720 --> 00:00:41,920
You have an incredible amount of power at this moment in time.

10
00:00:41,920 --> 00:00:44,920
Why should we trust you?

11
00:00:44,920 --> 00:00:45,920
You shouldn't.

12
00:00:45,920 --> 00:00:46,920
You shouldn't.

13
00:00:46,920 --> 00:00:47,920
You shouldn't.

14
00:00:47,920 --> 00:00:48,920
You shouldn't.

15
00:00:48,920 --> 00:00:51,160
Let's be very clear.

16
00:00:51,160 --> 00:00:57,160
The man who signed the 22-word statement admitting that his AI technology has the power to kill

17
00:00:57,160 --> 00:01:02,760
all life on Earth and must be mitigated like nuclear arms and pandemic is also saying to

18
00:01:02,760 --> 00:01:07,560
you directly that he should not be trusted.

19
00:01:07,560 --> 00:01:11,720
When people show you who they are, believe them.

20
00:01:11,720 --> 00:01:12,720
Yes.

21
00:01:12,720 --> 00:01:15,480
In general, it's the process for the bigger question of safety.

22
00:01:15,480 --> 00:01:20,040
How do you provide that layer that protects the model from doing crazy, dangerous things?

23
00:01:21,040 --> 00:01:28,040
I think there will come a point where that's mostly what we think about the whole company.

24
00:01:28,040 --> 00:01:29,040
Aha.

25
00:01:29,040 --> 00:01:30,040
Hmm.

26
00:01:30,040 --> 00:01:35,560
Okay, so at some point, the safety crisis will be so urgent the entire company will

27
00:01:35,560 --> 00:01:38,640
need to deal with it.

28
00:01:38,640 --> 00:01:40,280
Hear him.

29
00:01:40,280 --> 00:01:41,840
Believe him.

30
00:01:41,840 --> 00:01:48,360
The thing he's making will be very unsafe and at an unknown point, the guy who says,

31
00:01:48,800 --> 00:01:57,800
Trust me wants us to trust him that he will at the exact right moment know to throw the

32
00:01:57,800 --> 00:02:01,800
whole company into safety work.

33
00:02:01,800 --> 00:02:05,000
But just not quite yet.

34
00:02:05,000 --> 00:02:11,480
I thought at some point between when Open AI started and when we created AGI, there was

35
00:02:11,480 --> 00:02:15,720
going to be something crazy and explosive that happened, but there may be more crazy

36
00:02:15,720 --> 00:02:25,160
and explosive things still to happen.

37
00:02:25,160 --> 00:02:32,240
Welcome to For Humanity and AI Safety Podcast, episode number 22, Nobody Elected, Sam Aldman.

38
00:02:32,240 --> 00:02:33,720
I'm Jon Sherman, your host.

39
00:02:33,720 --> 00:02:35,760
Thank you so much for joining me.

40
00:02:35,760 --> 00:02:38,800
In America, we elect our leaders.

41
00:02:38,800 --> 00:02:44,600
They audition their ideas and then our government of by and for the people votes to decide our

42
00:02:44,600 --> 00:02:48,840
collective direction and decisions.

43
00:02:48,840 --> 00:02:53,600
But the American making the most consequential decisions in human history right now as I

44
00:02:53,600 --> 00:02:59,640
speak hasn't been elected by anyone nor would he be.

45
00:02:59,640 --> 00:03:04,160
His ideas poll very poorly with the general public.

46
00:03:04,160 --> 00:03:11,400
He says go the public says stop poll show more than 70% are opposed to building superhuman

47
00:03:11,400 --> 00:03:13,200
intelligence.

48
00:03:13,200 --> 00:03:17,640
So this week, we're going to focus on the most important human in AI who is also the

49
00:03:17,640 --> 00:03:23,720
most important human who has ever lived sadly.

50
00:03:23,720 --> 00:03:26,920
Today's show is going to point fingers and lay blame.

51
00:03:26,920 --> 00:03:33,000
Sam Aldman is why we are at the most dangerous moment in human history that we are at right

52
00:03:33,000 --> 00:03:34,640
now.

53
00:03:34,640 --> 00:03:37,120
This is the AI Safety Podcast for the general public.

54
00:03:37,120 --> 00:03:38,640
No tech background required.

55
00:03:38,640 --> 00:03:45,320
This podcast is solely about the threat of human extinction from artificial intelligence.

56
00:03:45,320 --> 00:03:49,480
At the end of today's show, we're going to talk about how hard it is to convince people

57
00:03:49,480 --> 00:03:56,200
to believe that AI risk is so much of a risk, they are moved to take action.

58
00:03:56,200 --> 00:03:59,080
I have some new insight into that.

59
00:03:59,080 --> 00:04:06,160
But first, it is not hyperbole to say that open AI CEO Sam Aldman holds the life of every

60
00:04:06,160 --> 00:04:10,880
single human on earth in his hands.

61
00:04:10,880 --> 00:04:21,240
Your kids, your parents, your friends, his knife is at each of their throats.

62
00:04:21,240 --> 00:04:25,120
Sam Aldman was on the Lex Friedman podcast very recently.

63
00:04:25,120 --> 00:04:30,400
And so I felt compelled to break that down for you this week.

64
00:04:30,400 --> 00:04:35,640
Sam was also in a very hard hitting piece in business insider this week.

65
00:04:35,640 --> 00:04:39,960
There's a link in the description as there will be to the Lex podcast.

66
00:04:39,960 --> 00:04:46,920
The title of the business insider article was some VCs are over the Sam Altman hype.

67
00:04:46,920 --> 00:04:48,160
Sounds good to me.

68
00:04:48,160 --> 00:04:51,520
Next subheading the platform of Sam.

69
00:04:51,520 --> 00:04:54,320
This is what I want to read you from the article.

70
00:04:54,320 --> 00:04:58,780
Some who know Altman paint a picture of a benevolent visionary, a thoughtful steward

71
00:04:58,780 --> 00:05:00,740
of a promising technology.

72
00:05:00,740 --> 00:05:06,480
They see his plan to build a far reaching AI empire that touches everything from nuclear

73
00:05:06,480 --> 00:05:12,380
fusion to anti-aging technology as a leap forward for humanity.

74
00:05:12,380 --> 00:05:17,940
Others say he's more interested in self-promotion than human advancement.

75
00:05:17,940 --> 00:05:24,360
All agree he's a masterful storyteller who could sell sand in the Sahara.

76
00:05:24,360 --> 00:05:29,460
But as Altman lays out a sweeping vision for the transformation of society, not everyone

77
00:05:29,480 --> 00:05:33,120
in Silicon Valley is buying it quote.

78
00:05:33,120 --> 00:05:37,280
There are holes a mile deep in this guy's resume, but he's managed to figure out how

79
00:05:37,280 --> 00:05:40,840
to take his chess pieces and move them correctly.

80
00:05:40,840 --> 00:05:43,880
An anonymous startup founder said.

81
00:05:43,880 --> 00:05:48,240
And now one of the things went crazy.

82
00:05:48,240 --> 00:05:50,480
And he's an AI expert.

83
00:05:50,480 --> 00:05:57,980
Okay, so my point in reading that is this Sam Altman is a tech CEO from the sales side.

84
00:05:58,000 --> 00:06:00,680
He knows how to sell ice in Alaska.

85
00:06:00,680 --> 00:06:05,120
He ran a startup incubator and he had a lot of different bets.

86
00:06:05,120 --> 00:06:09,120
And open AI is the one that hit big.

87
00:06:09,120 --> 00:06:11,920
So this guy making all the decisions.

88
00:06:11,920 --> 00:06:13,480
He's not an AI expert.

89
00:06:13,480 --> 00:06:15,520
He does not write code.

90
00:06:15,520 --> 00:06:17,760
He's a salesman, nothing against salesman.

91
00:06:17,760 --> 00:06:19,640
There have to be salesmen in the world.

92
00:06:19,640 --> 00:06:22,600
But I don't know that that's who you want making all the decisions.

93
00:06:22,600 --> 00:06:29,460
It was way back in 2015 that Vanity Fair gushed over Sam Altman and Elon Musk launching a

94
00:06:29,460 --> 00:06:35,780
nonprofit company to save the world from a dystopian future.

95
00:06:35,780 --> 00:06:44,700
Fast forward to 2023 and Sam Altman, his nonprofit, then a soon to be blossoming profit center

96
00:06:44,700 --> 00:06:52,800
of the largest corporation on earth was the one who decided to unleash chat GPT on the

97
00:06:52,800 --> 00:06:55,280
world when he did.

98
00:06:55,280 --> 00:07:00,840
It is widely reported that Google had a similar product to chat GPT six months earlier, but

99
00:07:00,840 --> 00:07:08,040
they did not release it because they were concerned over safety.

100
00:07:08,040 --> 00:07:16,380
Unelected, unvetted and completely unaccountable, this one man has chosen to put all of our

101
00:07:16,380 --> 00:07:22,460
lives in grave danger so that he can make money and fulfill his dreams of radically

102
00:07:22,460 --> 00:07:29,540
altering human existence as we have never known it without even an effort to get any

103
00:07:29,540 --> 00:07:34,740
consent from you or anyone else in the general public.

104
00:07:34,740 --> 00:07:37,940
Here's the really crazy part of it.

105
00:07:37,940 --> 00:07:40,920
He tells us all of this openly.

106
00:07:40,920 --> 00:07:42,520
There is no deception.

107
00:07:42,520 --> 00:07:45,560
He admits all of it.

108
00:07:45,560 --> 00:07:47,780
Here's the sad part.

109
00:07:47,780 --> 00:07:50,380
No one takes him at his word.

110
00:07:50,380 --> 00:07:55,120
No one is listening to his promises of doom.

111
00:07:55,120 --> 00:07:57,500
It all just sounds so outrageous.

112
00:07:57,500 --> 00:08:03,200
People dismiss it instantly and move on to whatever bullshit they were thinking of first.

113
00:08:03,200 --> 00:08:04,360
Yeah.

114
00:08:04,360 --> 00:08:06,600
I'm talking about this guy.

115
00:08:06,620 --> 00:08:09,860
Who nine months ago told us this.

116
00:08:09,860 --> 00:08:15,140
I think even you would acknowledge you have an incredible amount of power at this moment

117
00:08:15,140 --> 00:08:16,140
in time.

118
00:08:16,140 --> 00:08:19,300
Why should we trust you?

119
00:08:19,300 --> 00:08:20,300
You shouldn't.

120
00:08:20,300 --> 00:08:21,300
You shouldn't.

121
00:08:21,300 --> 00:08:22,300
You shouldn't.

122
00:08:22,300 --> 00:08:23,300
You shouldn't.

123
00:08:23,300 --> 00:08:25,540
Let's be very clear.

124
00:08:25,540 --> 00:08:31,540
The man who signed the 22 word statement admitting that his AI technology has the power to kill

125
00:08:31,540 --> 00:08:37,080
all life on earth and must be mitigated like nuclear arms and pandemic is also saying to

126
00:08:37,080 --> 00:08:44,600
you directly that he should not be trusted.

127
00:08:44,600 --> 00:08:50,080
The man with his hand on the kill eight billion people button is telling you he cannot be

128
00:08:50,080 --> 00:08:52,920
trusted with this responsibility.

129
00:08:52,920 --> 00:08:57,400
When people show you who they are, believe them.

130
00:08:57,400 --> 00:08:58,400
Yes.

131
00:08:58,400 --> 00:08:59,400
Absolutely.

132
00:08:59,400 --> 00:09:11,780
This one says to you, I'm selfish or I'm mean or I am unkind or I'm crazy or I'm

133
00:09:11,780 --> 00:09:12,780
crazy.

134
00:09:12,780 --> 00:09:13,780
Believe them.

135
00:09:13,780 --> 00:09:16,580
They know themselves much better than you do.

136
00:09:16,580 --> 00:09:23,780
But no, more often than not, those of us who don't trust life say don't say a thing

137
00:09:23,780 --> 00:09:24,780
like that.

138
00:09:24,780 --> 00:09:27,180
You're not really crazy.

139
00:09:27,180 --> 00:09:29,800
You're not really unkind.

140
00:09:29,800 --> 00:09:30,800
You're not really mean.

141
00:09:30,800 --> 00:09:39,760
And as soon as you say that, the person that you know and shows you, I told you, I told

142
00:09:39,760 --> 00:09:42,360
you I was unkind.

143
00:09:42,360 --> 00:09:46,040
Thank you, Oprah and the late great Dr. Maya Angelou.

144
00:09:46,040 --> 00:09:49,520
Sam keeps telling us not to trust him.

145
00:09:49,520 --> 00:09:53,400
He says no one should have the power he has.

146
00:09:53,400 --> 00:09:55,920
Maybe we should believe him.

147
00:09:55,920 --> 00:09:58,540
Like no one person should be trusted here.

148
00:09:58,540 --> 00:10:01,540
I don't have super voting shares.

149
00:10:01,540 --> 00:10:03,340
Like I don't want them.

150
00:10:03,340 --> 00:10:04,340
The board can fire me.

151
00:10:04,340 --> 00:10:05,340
I think that's important.

152
00:10:05,340 --> 00:10:08,740
The board can fire me and I think that's important.

153
00:10:08,740 --> 00:10:13,500
He said back in June 2023 and I, yes.

154
00:10:13,500 --> 00:10:18,620
So now we fast forward to that famous week in November 17th, 2023, less than six months

155
00:10:18,620 --> 00:10:26,920
later when it seems indisputably a battle over AI safety and Sam Altman's honesty

156
00:10:26,920 --> 00:10:34,560
around AI safety, the board, including his friend and co-founder, Ilya Sutskiver, do

157
00:10:34,560 --> 00:10:39,160
exactly what he says the failsafe should be.

158
00:10:39,160 --> 00:10:41,560
They fire him.

159
00:10:41,560 --> 00:10:48,040
And within hours, after having attached his nonprofit intended to help humanity to Microsoft

160
00:10:48,060 --> 00:10:54,380
the world's largest corporation, Microsoft naturally decided it could not lose its golden

161
00:10:54,380 --> 00:10:56,380
goose.

162
00:10:56,380 --> 00:11:01,700
So Microsoft led to the firing of the board members who drew the hard line on safety.

163
00:11:01,700 --> 00:11:05,340
Safety lost, Sam won.

164
00:11:05,340 --> 00:11:11,620
This was all a huge fucking deal and yet nobody other than a handful of people intimately

165
00:11:11,620 --> 00:11:18,440
involved seems even to this day to have any idea what really happened, which is why Sam

166
00:11:18,440 --> 00:11:23,840
Altman's appearance on the Lex Friedman show is really worth picking apart.

167
00:11:23,840 --> 00:11:28,520
For the first time, Sam was interviewed in depth about what happened the weekend he got

168
00:11:28,520 --> 00:11:29,840
fired.

169
00:11:29,840 --> 00:11:37,320
What happened that weekend is a national security issue of grave seriousness.

170
00:11:37,320 --> 00:11:42,780
And again, the guy building the tech that can end all life on Earth, the same tech he

171
00:11:42,780 --> 00:11:48,700
says he cannot control the same tech that no one understands how it works or why it

172
00:11:48,700 --> 00:11:50,820
does what it does.

173
00:11:50,820 --> 00:11:58,440
That guy, when he reflects on that weekend, when his lack of regard for safety should

174
00:11:58,440 --> 00:12:06,500
have put him out on the streets, he is consumed by one thing, not what happened, not the safety

175
00:12:06,520 --> 00:12:13,000
concerns, not the fact he lied about the board being a check on him.

176
00:12:13,000 --> 00:12:22,840
Sam Altman's take on the weekend that he got fired is all about his feelings.

177
00:12:22,840 --> 00:12:28,920
Take me through the open AI board saga that started on Thursday, November 16th, maybe Friday,

178
00:12:28,920 --> 00:12:30,880
November 17th for you.

179
00:12:30,880 --> 00:12:41,620
That was definitely the most painful professional experience of my life and chaotic and shameful

180
00:12:41,620 --> 00:12:47,660
and upsetting and a bunch of other negative things.

181
00:12:47,660 --> 00:12:54,380
There were great things about it too, and I wish I wish it had not been in such an adrenaline

182
00:12:54,380 --> 00:12:57,420
rush that I wasn't able to stop and appreciate them at the time.

183
00:12:57,420 --> 00:13:07,840
But I came across this tweet of mine from that time period, which was kind of going

184
00:13:07,840 --> 00:13:13,920
to your own eulogy, watching people say all these great things about you and just unbelievable

185
00:13:13,920 --> 00:13:18,960
support from people I love and care about.

186
00:13:18,960 --> 00:13:21,720
That was really nice.

187
00:13:21,720 --> 00:13:27,420
The whole weekend I kind of felt, with one big exception, I felt like a great deal of

188
00:13:27,420 --> 00:13:38,540
love and very little hate, even though it felt like I have no idea what's happening

189
00:13:38,540 --> 00:13:42,620
and what's going to happen here and this feels really bad and there were definitely times

190
00:13:42,620 --> 00:13:49,180
I thought it was going to be one of the worst things to ever happen for AI safety.

191
00:13:49,180 --> 00:13:56,680
One of the worst things to happen for AI safety says the guy who started the race to AGI,

192
00:13:56,680 --> 00:14:01,240
but don't worry, he says he saw this coming and that's just the start.

193
00:14:01,240 --> 00:14:06,320
Well, I also think I'm happy that it happened relatively early.

194
00:14:06,320 --> 00:14:12,800
I thought at some point between when OpenAI started and when we created AGI, there was

195
00:14:12,800 --> 00:14:17,040
going to be something crazy and explosive that happened, but there may be more crazy

196
00:14:17,040 --> 00:14:20,260
and explosive things still to happen.

197
00:14:20,260 --> 00:14:21,260
Honestly I just can't.

198
00:14:21,260 --> 00:14:23,820
I cannot take this man.

199
00:14:23,820 --> 00:14:30,740
With a twinkle in his eye, a smirk on his face and the power to kill everyone I love,

200
00:14:30,740 --> 00:14:38,060
it honestly a lot of the time feels like he's taunting me, like he's taunting us.

201
00:14:38,060 --> 00:14:44,300
There is nothing about risking all life on earth without anyone's consent that is funny

202
00:14:44,360 --> 00:14:48,000
or cute.

203
00:14:48,000 --> 00:14:52,320
But enough for now about you and me and our families.

204
00:14:52,320 --> 00:14:58,120
Let's focus on what's really important and still the biggest known about the AI safety

205
00:14:58,120 --> 00:15:00,440
crisis to date.

206
00:15:00,440 --> 00:15:02,360
Sam Altman's feelings.

207
00:15:02,360 --> 00:15:12,240
It feels like something that was in the past that was really unpleasant and really difficult

208
00:15:12,260 --> 00:15:21,100
and painful, but we're back to work and things are so busy and so intense that I don't spend

209
00:15:21,100 --> 00:15:22,780
a lot of time thinking about it.

210
00:15:22,780 --> 00:15:29,380
There was a time after, there was like this fugue state for kind of like the month after,

211
00:15:29,380 --> 00:15:35,860
maybe 45 days after that was, I was just sort of like drifting through the days.

212
00:15:35,860 --> 00:15:38,220
I was so out of it.

213
00:15:38,220 --> 00:15:40,140
I was feeling so down.

214
00:15:40,140 --> 00:15:42,140
Just at a personal psychological level.

215
00:15:42,760 --> 00:15:50,240
Really painful and hard to like have to keep running open AI in the middle of that.

216
00:15:50,240 --> 00:15:55,960
I just wanted to like crawl into a cave and kind of recover for a while, but now it's

217
00:15:55,960 --> 00:16:00,760
like we're just back to working on the mission.

218
00:16:00,760 --> 00:16:10,960
What's still useful to go back there and reflect on board structures, on power dynamics,

219
00:16:10,980 --> 00:16:17,580
on how companies are run, the tension between research and product development and money

220
00:16:17,580 --> 00:16:24,860
and all this kind of stuff, so that you who have a very high potential of building AGI

221
00:16:24,860 --> 00:16:30,300
would do so in a slightly more organized, less dramatic way in the future.

222
00:16:30,300 --> 00:16:35,980
There's value there to go, both the personal psychological aspects of you as a leader and

223
00:16:35,980 --> 00:16:40,580
also just the board structure and all this kind of messy stuff.

224
00:16:40,600 --> 00:16:50,200
We've only learned a lot about structure and incentives and what we need out of a board.

225
00:16:50,200 --> 00:16:56,520
Okay, so I'm sorry, but Lex did a terrible job in this interview, I'm sorry.

226
00:16:56,520 --> 00:17:00,960
They talked for two hours and not once did we find out anything really new about what

227
00:17:00,960 --> 00:17:04,600
actually happened the weekend he got fired.

228
00:17:04,600 --> 00:17:09,720
He called it the worst thing possible for AI safety and Lex does not even follow up and

229
00:17:09,740 --> 00:17:16,940
ask him what he means, what do you mean it's the worst thing possible for AI safety?

230
00:17:16,940 --> 00:17:22,740
But he does ask about open AI's chief scientist, Ilya Sutskiver, who before the firing did

231
00:17:22,740 --> 00:17:28,380
podcasts and made a lot of public appearances since the firing weekend, Ilya has not been

232
00:17:28,380 --> 00:17:32,380
seen anywhere in public that I'm aware of.

233
00:17:32,380 --> 00:17:38,700
He Ilya is also noted for his deep and genuine concern over AI safety, so does Lex try to

234
00:17:38,720 --> 00:17:41,040
get the reel on this out of Sam?

235
00:17:41,040 --> 00:17:44,360
Oh no, it's just more footsie.

236
00:17:44,360 --> 00:17:45,360
So cute.

237
00:17:45,360 --> 00:17:50,880
Let me ask you about Ilya, is he being held hostage in a secret nuclear facility?

238
00:17:50,880 --> 00:17:51,880
No.

239
00:17:51,880 --> 00:17:53,600
What about a regular secret facility?

240
00:17:53,600 --> 00:17:54,600
No.

241
00:17:54,600 --> 00:17:56,000
What about a nuclear non-secret facility?

242
00:17:56,000 --> 00:17:57,720
Neither, not that either.

243
00:17:57,720 --> 00:18:03,520
I mean, it's becoming a meme at some point, you've known Ilya for a long time, he's obviously

244
00:18:03,520 --> 00:18:08,360
in part of this drama with the board and all that kind of stuff.

245
00:18:08,380 --> 00:18:11,220
What's your relationship with him now?

246
00:18:11,220 --> 00:18:17,140
I love Ilya, I have tremendous respect for Ilya, I don't know anything I can say about

247
00:18:17,140 --> 00:18:20,900
his plans right now, that's a question for him.

248
00:18:20,900 --> 00:18:26,340
But I really hope we work together for, you know, certainly the rest of my career.

249
00:18:26,340 --> 00:18:29,900
He's a little bit younger than me, maybe he works a little bit longer.

250
00:18:29,900 --> 00:18:36,420
You know, there's a meme that he saw something, like he maybe saw AGI and that gave him a

251
00:18:36,480 --> 00:18:39,360
lot of worry internally.

252
00:18:39,360 --> 00:18:41,160
What did Ilya see?

253
00:18:41,160 --> 00:18:49,960
Ilya has not seen AGI, none of us have seen AGI, we have not built AGI.

254
00:18:49,960 --> 00:18:59,400
I do think one of the many things that I really love about Ilya is he takes AGI and the safety

255
00:18:59,400 --> 00:19:04,720
concerns broadly speaking, you know, including things like the impact this is going to have

256
00:19:04,720 --> 00:19:07,940
on society very seriously.

257
00:19:07,940 --> 00:19:14,780
And we, as we continue to make significant progress, Ilya is one of the people that I've

258
00:19:14,780 --> 00:19:19,660
spent the most time over the last couple of years talking about what this is going to

259
00:19:19,660 --> 00:19:23,940
mean, what we need to do to ensure we get it right, to ensure that we succeed at the

260
00:19:23,940 --> 00:19:25,940
mission.

261
00:19:25,940 --> 00:19:40,960
So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks

262
00:19:40,960 --> 00:19:44,960
and worries about making sure we get this right.

263
00:19:44,960 --> 00:19:52,960
So the credit to humanity most concerned about safety has been silenced and hidden.

264
00:19:52,980 --> 00:19:56,980
And with that problem taken care of, the race is back on.

265
00:19:56,980 --> 00:20:02,380
Next, Lex asked Sam about Elon Musk suing him.

266
00:20:02,380 --> 00:20:07,020
And again, suicide Sam goes straight to his feelings.

267
00:20:07,020 --> 00:20:12,100
I know he knows what it's like to have haters attack him and it makes me extra sad he's

268
00:20:12,100 --> 00:20:13,100
doing it, Toss.

269
00:20:13,100 --> 00:20:16,900
Yeah, he's one of the greatest builders of all time, potentially the greatest builder

270
00:20:16,900 --> 00:20:17,900
of all time.

271
00:20:17,900 --> 00:20:18,900
It makes me sad.

272
00:20:18,900 --> 00:20:21,620
And I think it makes a lot of people sad, like there's a lot of people who've really

273
00:20:21,640 --> 00:20:24,560
looked up to him for a long time and said this.

274
00:20:24,560 --> 00:20:28,640
I said in some interview or something that I missed the old Elon and the number of messages

275
00:20:28,640 --> 00:20:31,960
I got being like that exactly encapsulates how I feel.

276
00:20:31,960 --> 00:20:34,360
I think he should just win.

277
00:20:34,360 --> 00:20:42,360
He should just make X grok beat GPT and then GPT beats grok and it's just a competition.

278
00:20:42,360 --> 00:20:47,160
What do you hope this goes with Elon?

279
00:20:47,160 --> 00:20:48,520
This tension, this dance.

280
00:20:48,520 --> 00:20:49,520
What do you hope this?

281
00:20:49,520 --> 00:20:55,460
Like if we go one, two, three years from now, you're a relationship with him on a personal

282
00:20:55,460 --> 00:21:03,460
level too, like friendship, friendly competition, just all this kind of stuff.

283
00:21:03,460 --> 00:21:12,740
Yeah, I really respect Elon.

284
00:21:12,740 --> 00:21:16,100
And I hope that years in the future we have an amicable relationship.

285
00:21:16,960 --> 00:21:22,880
Yeah, I hope you guys have an amicable relationship like this month.

286
00:21:22,880 --> 00:21:28,960
And just compete and win and explore these ideas together.

287
00:21:28,960 --> 00:21:36,920
I do suppose there's competition for talent or whatever, but it should be friendly competition.

288
00:21:36,920 --> 00:21:37,920
Just build.

289
00:21:37,920 --> 00:21:40,240
Build cool shit.

290
00:21:40,240 --> 00:21:44,280
And Elon is pretty good at building cool shit, but so are you.

291
00:21:44,280 --> 00:21:50,420
Building cool shit with like a rowy tone of voice building cool shit.

292
00:21:50,420 --> 00:21:54,740
Building cool shit is why we are all on course to die in the next 10 fucking years.

293
00:21:54,740 --> 00:21:57,340
It is not cool.

294
00:21:57,340 --> 00:22:00,940
Okay, this next one is wild.

295
00:22:00,940 --> 00:22:06,460
Lex asks Sam about the basis for the New York Times versus open AI lawsuit.

296
00:22:06,460 --> 00:22:12,220
Remember Sam's multi-billion dollar black box machine would not exist if it had not

297
00:22:12,240 --> 00:22:15,080
been trained on internet data.

298
00:22:15,080 --> 00:22:21,560
Sam already having harvested the data and made his billions has the gall to drop this

299
00:22:21,560 --> 00:22:22,560
answer.

300
00:22:22,560 --> 00:22:26,400
There's a lot of tough questions here.

301
00:22:26,400 --> 00:22:28,200
You're dealing in a very tough space.

302
00:22:28,200 --> 00:22:33,160
Do you think training AI should be or is fair use under copyright law?

303
00:22:33,160 --> 00:22:37,600
I think the question behind that question is do people who create valuable data deserve

304
00:22:37,620 --> 00:22:42,140
to have some way that they get compensated for use of it and that I think the answer

305
00:22:42,140 --> 00:22:43,140
is yes.

306
00:22:43,140 --> 00:22:46,420
I don't know yet what the answer is.

307
00:22:46,420 --> 00:22:48,100
People have proposed a lot of different things.

308
00:22:48,100 --> 00:22:55,700
We've tried some different models, but if I'm like an artist, for example, A, I would

309
00:22:55,700 --> 00:23:00,980
like to be able to opt out of people generating art in my style and B, if they do generate

310
00:23:00,980 --> 00:23:05,860
art in my style, I'd like to have some economic model associated with that.

311
00:23:05,880 --> 00:23:08,760
What the actual fuck are you even talking about?

312
00:23:08,760 --> 00:23:11,080
You already stole the training data.

313
00:23:11,080 --> 00:23:15,240
All the writers and artists have had their work stolen.

314
00:23:15,240 --> 00:23:19,760
This dude built a billion dollar business on stolen writing and images and now after

315
00:23:19,760 --> 00:23:24,880
his business is flourishing after and the toothpaste seems impossible to put back in

316
00:23:24,880 --> 00:23:31,360
the tube, he's like, um, yeah, somebody should figure out a way to make this fair.

317
00:23:32,180 --> 00:23:36,720
There's a real theme with Sam Altman.

318
00:23:36,720 --> 00:23:41,380
Every problem he causes is someone else's to solve.

319
00:23:41,380 --> 00:23:45,580
So in general is the process for the bigger question of safety.

320
00:23:45,580 --> 00:23:50,260
How do you provide that layer that protects the model from doing crazy dangerous things?

321
00:23:50,260 --> 00:23:57,300
I think there will come a point where that's mostly what we think about the whole company

322
00:23:57,300 --> 00:24:00,660
and it won't be like, it's not like you have one safety team.

323
00:24:00,660 --> 00:24:03,520
It's like when we shipped GPT-4, that took the whole company thing with all these different

324
00:24:03,520 --> 00:24:08,200
aspects and how they fit together and I think it's going to take that.

325
00:24:08,200 --> 00:24:12,320
More and more of the company thinks about those issues all the time.

326
00:24:12,320 --> 00:24:18,120
That's literally what humans will be thinking about the more powerful AI becomes.

327
00:24:18,120 --> 00:24:23,080
So most of the employees that open AI will be thinking safety or at least to some degree.

328
00:24:23,080 --> 00:24:25,600
Broadly defined, yes.

329
00:24:25,600 --> 00:24:28,600
I wonder what are the full broad definition of that?

330
00:24:28,620 --> 00:24:30,820
What are the different harms that could be caused?

331
00:24:30,820 --> 00:24:35,740
Is this like on a technical level or is this almost like security threats?

332
00:24:35,740 --> 00:24:36,740
It'll be all those things.

333
00:24:36,740 --> 00:24:41,580
I was going to say it'll be people, state actors trying to steal the model.

334
00:24:41,580 --> 00:24:45,020
It'll be all of the technical alignment work.

335
00:24:45,020 --> 00:24:51,860
It'll be societal impacts, economic impacts.

336
00:24:51,860 --> 00:24:56,100
It's not just like we have one team thinking about how to align the model and it's really

337
00:24:56,100 --> 00:25:00,480
going to be like getting to be getting to the good outcome is going to take the whole

338
00:25:00,480 --> 00:25:01,480
the whole effort.

339
00:25:01,480 --> 00:25:02,480
Huh.

340
00:25:02,480 --> 00:25:03,480
Hmm.

341
00:25:03,480 --> 00:25:04,480
Okay.

342
00:25:04,480 --> 00:25:09,800
So at some point, the safety crisis will be so urgent, the entire company will need

343
00:25:09,800 --> 00:25:12,640
to deal with it.

344
00:25:12,640 --> 00:25:15,840
Hear him, believe him.

345
00:25:15,840 --> 00:25:22,400
The thing he's making will be very unsafe and at an unknown point, the guy who says,

346
00:25:22,400 --> 00:25:31,820
trust me, wants us to trust him that he will at the exact right moment know to throw the

347
00:25:31,820 --> 00:25:38,020
whole company into safety work, but just not quite yet.

348
00:25:38,020 --> 00:25:39,340
Cool.

349
00:25:39,340 --> 00:25:41,620
Seems like a plan.

350
00:25:41,620 --> 00:25:42,620
Okay.

351
00:25:42,620 --> 00:25:44,460
This next one is wild.

352
00:25:44,460 --> 00:25:51,380
So, um, I spoke to a group of coders about AI risk last week, uh, a week or so ago, um,

353
00:25:51,380 --> 00:25:56,280
outside of Philadelphia, and the number one question they had was how will AGI escape?

354
00:25:56,280 --> 00:26:00,480
How does it go from the digital world to the physical world?

355
00:26:00,480 --> 00:26:02,080
Hiring humans is an easy way.

356
00:26:02,080 --> 00:26:05,640
There's lots of others, but here's an even easier way.

357
00:26:05,640 --> 00:26:07,120
Go tell him, Sam.

358
00:26:07,120 --> 00:26:14,040
Will we, uh, see human and robots or human and robot brains from open AI at some point?

359
00:26:14,040 --> 00:26:15,560
At some point.

360
00:26:15,560 --> 00:26:18,120
How important is embodied AI to you?

361
00:26:18,120 --> 00:26:23,620
I think it's like sort of depressing if we have AGI and the only way to like get things

362
00:26:23,620 --> 00:26:27,460
done in the physical world is like to make a human go do it.

363
00:26:27,460 --> 00:26:35,260
So I, I really hope that as part of this transition, as this phase change, we also get, uh, we

364
00:26:35,260 --> 00:26:38,420
also get human and robots or some sort of physical world robots.

365
00:26:38,420 --> 00:26:39,660
Okay.

366
00:26:39,660 --> 00:26:41,900
Let's add up the tab.

367
00:26:41,900 --> 00:26:44,780
Sam can't control his AI.

368
00:26:44,780 --> 00:26:48,640
Sam doesn't understand how his AI works.

369
00:26:48,640 --> 00:26:52,840
Sam admits his AI can end all life on earth.

370
00:26:52,840 --> 00:26:54,240
Sam is clear.

371
00:26:54,240 --> 00:26:56,640
You should not trust him.

372
00:26:56,640 --> 00:27:01,320
And one more.

373
00:27:01,320 --> 00:27:09,800
Sam would be depressed if his AGI is not put in to robots.

374
00:27:09,800 --> 00:27:15,180
For Bill's AGI first gets a lot of power.

375
00:27:15,180 --> 00:27:20,220
Do you trust yourself with that much power?

376
00:27:20,220 --> 00:27:28,100
When people show you who they are, believe them.

377
00:27:28,100 --> 00:27:30,060
Yes.

378
00:27:30,060 --> 00:27:36,980
Look, I, I was gonna, I'll just be very honest with this answer.

379
00:27:36,980 --> 00:27:42,560
I was gonna say, and I still believe this, that it is important that I, nor any other

380
00:27:42,560 --> 00:27:49,800
one person have total control over open AI or over AGI.

381
00:27:49,800 --> 00:27:54,840
And I think you want a robust governance system.

382
00:27:54,840 --> 00:28:04,040
Um, I can point out a whole bunch of things about all of our board drama from last year

383
00:28:04,040 --> 00:28:07,980
about how I didn't fight it initially and was just like, yeah, that's, you know, the

384
00:28:07,980 --> 00:28:13,660
will of the board, even though I think it's a really bad decision.

385
00:28:13,660 --> 00:28:16,940
And then later I clearly did fight it and I can explain the nuance and why I think it

386
00:28:16,940 --> 00:28:19,380
was okay for me to fight it later.

387
00:28:19,380 --> 00:28:31,660
But as many people have observed, um, although the board had the legal ability to fire me

388
00:28:31,660 --> 00:28:36,440
in practice, it didn't quite work.

389
00:28:36,440 --> 00:28:41,480
And that is its own kind of governance failure.

390
00:28:41,480 --> 00:28:49,000
Now, again, I, I feel like I can completely defend the specifics here.

391
00:28:49,000 --> 00:28:58,480
And I think most people would agree with that, but it, it does make it harder for me to like

392
00:28:58,480 --> 00:29:01,240
look you in the eye and say, Hey, the board can just fire me.

393
00:29:01,660 --> 00:29:03,940
They know themselves much better than you do.

394
00:29:07,180 --> 00:29:13,540
Okay, Sam, one last clip, please, most important person on earth, give us some reassurance.

395
00:29:15,380 --> 00:29:18,540
Are you afraid of losing control of the AGI itself?

396
00:29:18,580 --> 00:29:23,740
That's a lot of people who worry about existential risk, not because of state actors, not because

397
00:29:23,740 --> 00:29:26,300
of security concerns, but because of the AI itself.

398
00:29:26,300 --> 00:29:27,460
That is not my top worry.

399
00:29:27,820 --> 00:29:30,620
As I currently see things, there have been times I've worried about that more than maybe

400
00:29:30,640 --> 00:29:33,160
times again, in the future, that's my top worry.

401
00:29:33,680 --> 00:29:34,760
It's not my top worry right now.

402
00:29:34,800 --> 00:29:36,680
What's your intuition about it not being your worry?

403
00:29:36,680 --> 00:29:38,760
Cause there's a lot of other stuff to worry about, essentially.

404
00:29:41,480 --> 00:29:42,760
You think you could be surprised?

405
00:29:42,840 --> 00:29:44,680
We for sure could be surprised.

406
00:29:45,200 --> 00:29:47,600
Like saying it's not my top worry.

407
00:29:47,600 --> 00:29:50,880
It doesn't mean I don't think we need, like, I think we need to work on it super hard.

408
00:29:52,320 --> 00:29:54,840
I think we need to work on it super hard.

409
00:29:56,320 --> 00:29:57,840
Um, are you fucking kidding?

410
00:29:58,020 --> 00:30:02,860
That is not at all the same as we are working on it super hard.

411
00:30:03,620 --> 00:30:05,100
Two very different things.

412
00:30:06,060 --> 00:30:09,420
Sam Altman wants someone else to make his tech safe.

413
00:30:09,780 --> 00:30:13,780
Sam Altman wants someone else to regulate his tech.

414
00:30:14,260 --> 00:30:19,460
Sam Altman even wants someone else to figure out how to compensate the people whose

415
00:30:19,460 --> 00:30:23,020
training data was stolen to build his death star.

416
00:30:24,300 --> 00:30:25,820
No one elected Sam Altman.

417
00:30:26,640 --> 00:30:32,600
No one has vetted Sam Altman, but he is in charge of whether or not your family

418
00:30:32,600 --> 00:30:34,040
lives or dies.

419
00:30:37,120 --> 00:30:43,600
No government oversight motivated by profit, ego, a boyish need to build cool

420
00:30:43,600 --> 00:30:51,880
stuff and an unrequested desire to change how humans live fundamentally with

421
00:30:51,880 --> 00:30:54,640
no plan as fast as possible.

422
00:30:56,760 --> 00:31:02,560
That is why I believe we need to stop Sam Altman and all those like him.

423
00:31:03,160 --> 00:31:04,360
Okay, but here's the thing.

424
00:31:05,720 --> 00:31:10,800
The case against Sam and his peers is not hitting home with the general public yet.

425
00:31:11,120 --> 00:31:12,680
Um, I want to share a story with you.

426
00:31:12,680 --> 00:31:17,720
One of my closest and oldest friends on the planet and I had an exchange this week

427
00:31:17,720 --> 00:31:19,800
that I think you can learn from.

428
00:31:20,080 --> 00:31:25,640
So after a catch up zoom that unexpectedly drifted far into AI risk, he

429
00:31:25,640 --> 00:31:29,340
told me in an email follow up that he believed everything I said.

430
00:31:30,380 --> 00:31:34,380
So I, after some hesitation followed up, I just wanted to know really to help

431
00:31:34,380 --> 00:31:35,620
better understand people.

432
00:31:36,100 --> 00:31:43,020
Um, why, if he believed that his three young kids lives are threatened by AI, why

433
00:31:43,020 --> 00:31:47,860
he has not taken action yet, why he doesn't feel compelled to do things like I

434
00:31:47,860 --> 00:31:49,420
feel compelled to do things.

435
00:31:49,780 --> 00:31:50,960
And here's what he wrote back.

436
00:31:50,960 --> 00:31:54,220
Three bullet points that I think are so, so common.

437
00:31:54,220 --> 00:32:00,200
I bet 90% of the inactive public would say the same three points.

438
00:32:00,600 --> 00:32:05,080
He wrote, honestly, I don't have a good excuse.

439
00:32:05,600 --> 00:32:10,280
Number one, I'm very busy with work and kids and life.

440
00:32:10,960 --> 00:32:15,920
Number two, I'm only one person with zero influence on companies, individuals

441
00:32:15,920 --> 00:32:17,960
and lawmakers who can impact what's happening.

442
00:32:18,420 --> 00:32:22,000
And number three, I'm hopeful that doom is not imminent.

443
00:32:22,620 --> 00:32:26,380
And that in the end, good outcomes will prevail.

444
00:32:26,940 --> 00:32:31,380
That is literally what those of us who desperately want to spread AI risk

445
00:32:31,380 --> 00:32:35,300
awareness and see action come from it are up against.

446
00:32:36,820 --> 00:32:40,580
So on one hand, I was depressed at my first reading.

447
00:32:41,020 --> 00:32:43,820
Those three points are so weak.

448
00:32:44,700 --> 00:32:45,620
We're all busy.

449
00:32:46,180 --> 00:32:50,660
We can all have influence and hope is not nearly enough.

450
00:32:51,600 --> 00:32:57,120
But then I thought, wait a minute, those three points are so weak.

451
00:32:59,200 --> 00:33:01,400
What we are up against is these three things.

452
00:33:01,440 --> 00:33:04,600
Again, very busy with work and kids and life.

453
00:33:05,320 --> 00:33:06,840
I don't have influence.

454
00:33:07,560 --> 00:33:09,880
I hope that we get a good outcome.

455
00:33:11,280 --> 00:33:15,480
The weaknesses in each of those is not our enemy.

456
00:33:15,840 --> 00:33:16,960
It's our ally.

457
00:33:17,400 --> 00:33:19,440
These are three winnable arguments.

458
00:33:19,980 --> 00:33:25,660
So as you make for the, as you make the case for AI risk, keep those three

459
00:33:25,660 --> 00:33:30,540
things in mind, that is what you and we are fighting against.

460
00:33:31,340 --> 00:33:32,020
It's winnable.

461
00:33:33,140 --> 00:33:35,860
Okay, friends, it's 2024.

462
00:33:35,860 --> 00:33:37,740
AGI is coming at some point.

463
00:33:37,740 --> 00:33:41,180
We don't know when we don't know how long we have to live.

464
00:33:41,300 --> 00:33:47,700
So we celebrate every moment of every day, like it could be our last call

465
00:33:47,700 --> 00:33:50,920
at the celebration of life for this week's celebration of life.

466
00:33:50,920 --> 00:33:53,640
I want to share something that really moved me as a dad.

467
00:33:54,240 --> 00:33:59,600
Um, there's not much that makes me feel thrilled to be alive more than

468
00:33:59,600 --> 00:34:02,760
incredible vocal tone on a singing voice.

469
00:34:03,200 --> 00:34:07,800
Um, when my daughter first shared with me, Billie Eilish, I was immediately

470
00:34:07,880 --> 00:34:10,000
blown away by her vocal tone.

471
00:34:10,600 --> 00:34:15,520
Um, and then as a dad of boy girl twins, it was so cool to learn

472
00:34:15,540 --> 00:34:19,860
that her brother Phineas is her producer and musical creative partner.

473
00:34:20,020 --> 00:34:24,860
Um, so I came across a performance that just really blew me away.

474
00:34:24,860 --> 00:34:29,140
The two of them doing their song, um, what was I made for at the Grammy

475
00:34:29,140 --> 00:34:33,340
awards, a live performance and as a dad of a boy and a girl and a fan of vocal

476
00:34:33,340 --> 00:34:36,100
tone and songwriting, it just floored me.

477
00:34:36,180 --> 00:34:38,500
Um, they are simply incredible.

478
00:34:38,700 --> 00:34:44,700
And this song has some haunting reflections on our AI risk debate.

479
00:34:44,700 --> 00:34:46,520
Please enjoy this is so good.

480
00:36:45,000 --> 00:36:49,920
Don't tell my boy friend, it's not what he's made for.

481
00:36:54,160 --> 00:36:56,640
But what was I made for?

482
00:37:01,520 --> 00:37:11,480
Cause I, cause I, I don't know how to feel.

483
00:37:11,480 --> 00:37:35,260
But I want to try, I don't know how to feel, but someday I might, someday I might.

484
00:37:41,480 --> 00:38:08,260
I think I forgot how to be happy, something I'm not, but something I can't be, something I wait for, and something I'm made for.

485
00:38:11,780 --> 00:38:16,980
Something I'm made for.

486
00:38:25,980 --> 00:38:27,340
Just beautiful.

487
00:38:28,060 --> 00:38:29,980
Okay, friends, that is all for this week.

488
00:38:30,220 --> 00:38:32,780
Next week's show is going to be a little while.

489
00:38:32,820 --> 00:38:35,620
So get ready for that for humanity.

490
00:38:35,660 --> 00:38:36,460
I'm John Sherman.

491
00:38:36,460 --> 00:38:38,300
I will see you right back here next week.

492
00:38:38,500 --> 00:38:40,340
We have so much work to do.

