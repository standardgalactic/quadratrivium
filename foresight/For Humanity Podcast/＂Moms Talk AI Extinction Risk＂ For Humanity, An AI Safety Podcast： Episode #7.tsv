start	end	text
0	3000	I don't hear as much about, like, end of the world as I do.
3000	5380	What is the application in the school system?
5380	7300	And then how is my child learning?
7300	11020	When you first heard the notion of existential risk,
11020	14980	that, like, within our lifetimes,
14980	19860	there could be a world where there is no life on Earth.
19860	24420	We are expected lifetimes, I guess.
24420	25380	How did that hit you?
25380	27140	What was your first thought?
30880	36040	I think I believed.
36040	38240	I mean, I'd sound so crazy.
38240	41320	I feel like I've found a little crazy to say, like,
41320	42800	I needed to do my research.
42800	46320	Like, I needed to know who was saying there.
46320	48640	Were those people credible?
48640	52400	And what's been really interesting, the camps, right,
52400	57840	being the dancers, versus the juniors, or there need camp.
57840	61720	And each camp has their expert that
61720	63720	went to this Ivy League institution
63720	67880	and worked at this, you know, this big tech firm,
67880	69520	or used to work at the big tech firm.
69520	73960	Like, both sides of the camp have their, you know,
73960	75560	qualified experts.
75560	78480	Like, human extinction, like, you know,
78480	81960	society has been trying to, like, eliminate me my entire life.
81960	84280	Nothing will be able to, like, overpower us
84280	87200	because of what we've been able to endure
87200	90160	and what we've been able to survive.
90160	93560	So for me, I'm like, OK, come at me, AI.
93560	97920	Like, you know, you know, it's just like, like, come at me.
97920	101120	Throwing down the, she's throwing down the bell.
101120	103240	Like, like, like, come at me, tech bros.
103240	104120	Like, I got you.
104120	109160	Like, there's no way that you can, like, completely, like,
109160	111680	you know, erase me because, like, I've been through so much.
111680	113200	I think it means a little brand.
113200	116080	I think the term AI, so to speak, is boring.
116080	118280	I don't think anybody gets excited about it.
118280	120120	I don't think they know what it means.
120120	121720	It sounds like a fake news moment.
121720	122400	Like, what?
122400	124720	Like, AI, fake news, snoozer.
124720	129160	I think the scary part for me was that the people that created it
129160	132320	were warning us and said, we don't know how to control it
132320	133000	at this point.
133000	136400	And so when you have a creator that doesn't understand it,
136400	139240	it doesn't understand what it's doing, that's very scary.
139240	143000	I was stunned to know that the people who make AI
143000	149320	have no idea why it does, what it does, or how to control it.
149320	152400	What did you think when you all first heard that?
164600	169440	Welcome to For Humanity, an AI safety podcast, episode 7,
169440	172160	Mom's Talk AI Extinction Risks.
172200	173360	I'm John Sherman, your host.
173360	175400	Thank you so much for joining me.
175400	178200	This is the AI Safety Podcast for the general public,
178200	180000	no tech background required.
180000	182280	As you know, this show is exclusively
182280	185800	about the threat of extinction from artificial intelligence.
185800	189640	Please like, subscribe, and tell a friend about this show.
189640	191440	And if you are on X and want to hear more
191440	193320	about these issues on a daily basis,
193320	196080	please follow at For Humanity Pod.
196080	197960	OK.
197960	200040	So the whole point of this podcast
200040	202680	has been to move this debate from the Silicon Valley
202680	205880	boardroom to the family dinner table.
205880	209480	I also, as a parent, have been wondering
209480	214640	how anyone who works in AI could do this to their children.
214640	219200	And that if maybe awakening the parental maternal instincts
219200	222000	that are so foreign to this AI safety debate
222000	226720	could be possible and maybe even powerful.
226720	228440	So I thought, what a better way to do that
228560	230600	than to just do that.
230600	232920	So I asked three moms to come on the show
232920	237560	and discuss AI as an immediate threat to their children.
237560	239960	I want to give a big thanks to Stephanie, Jen,
239960	242040	and Crystal, our three moms.
242040	244440	I asked them all to watch the first episode
244440	246720	of this podcast before we met.
246720	249240	I think this is a fascinating conversation,
249240	253240	and I am thrilled to share it with you.
253240	255040	All right.
255040	256840	Moms, I think we made it.
256840	260120	We made it for the Gauntlet Riverside Zooms
260120	262800	and getting on a video conference.
262800	265160	So nice to see you all.
265160	267520	Nice to see you.
267520	268480	Awesome.
268480	273080	So let's just take it from the top here a little bit, I guess.
273080	275520	So it's an interesting thing in your life
275520	277720	when you start to talk about these AI safety issues.
277720	280160	They resonate with certain people and not with other people.
280160	283800	And Stephanie has been a friend of mine for many years.
283800	287200	And it's an issue that resonated with her.
287200	289400	And we got to talking about it.
289400	290920	And one of the things we're talking about
290920	294720	is where is the parental attitude in this?
294720	299040	Where is the maternal attitude in this?
299040	303520	And so I've been trying to take this debate as much as I can
303520	305640	from tech people to regular people.
305640	307320	So why not just do it and take it right
307320	311840	to the most regular people of all, the moms?
311840	318560	I will say as a father, as a son, husband, all these things
318560	320520	that moms, the world go around.
320520	324600	And so I salute to all the moms everywhere.
324600	326560	I thought we could just start off right at the top.
326560	331160	Maybe let's just go around the horn, say who you are.
331160	332600	Tell me a little bit about your kids
332600	334560	and maybe what you do outside of kids.
334560	336120	And we'll just quick shoot around the horn
336120	338200	and then come back up to the top.
338200	339960	Stephanie, why don't you start us off?
340000	341800	So I'm Stephanie Rano.
341800	346320	I am a mom of 30 kiddos.
346320	348440	My oldest is 13.
348440	351480	My middle is 10 and my youngest is 8.
351480	354600	So kind of early, but definitely that 13-year-old has
354600	355640	opinions.
355640	358400	Two of my kids are neurodivergent.
358400	360720	And that is much a part of our lives.
360720	362680	I'm really open about talking about it
362680	367840	with anybody who will listen just around their gifts
367840	370520	and the place if they need support.
370520	374720	And what I do and I'm not being a mom is I do recruiting.
374720	377760	So I do sales for a recruiting company.
377760	380160	I've been doing stocking and recruiting for almost 20
380160	381120	years.
381120	384000	And I'm really excited for this conversation.
384000	386760	And excited to share the podcast, hopefully with my kids.
386760	389520	So watch your language, Jeff.
389520	390120	Thank you.
390120	391480	Thank you, Stephanie.
391480	393680	I did make a promise to our group here
393680	396480	that I would clean up my act for the show in an effort.
396520	398200	Maybe we can show it to the kids afterwards.
398200	399360	So that would be great.
399360	401080	Jen, please introduce yourself.
401080	401640	Yes.
401640	403120	Hello, everyone.
403120	406480	My name is Jen White-Johnson.
406480	410280	I do identify as someone who is neurodivergent.
410280	415600	I do have ADHD and also anxiety and also an autoimmune
415600	416080	disorder.
416080	420360	So I live along the intersections of being black
420360	422560	and having a disability.
422560	424880	And my son is also autistic.
424920	427520	And my husband is also neurodivergent.
427520	433320	So we are very neurodivergent, affirming family.
433320	436360	And it intersects with my teaching.
436360	441440	And I also teach graphic design within these past few years
441440	443240	since my son was diagnosed.
443240	448360	I do a lot of disability advocacy work specifically
448360	454200	under the influence of art and where
454200	457560	that can be used as a tool to really uplift
457560	460720	the conversation of disability justice.
460720	462760	So thank you for having me.
462760	464280	Awesome, awesome thrill to have you.
464280	466520	And Crystal, please introduce yourself.
466520	467480	Hi, everyone.
467480	469960	My name is Crystal Putman Garcia.
469960	473320	So I am also a mom of three, like Stephanie.
473320	475680	I have twins that are nine.
475680	479280	And then I have an almost seven-year-old, so six right now.
479280	481920	So they're a little bit on the younger side.
481960	484280	But I do have a neurodivergent son who
484280	487480	is obsessed with technology.
487480	492360	He could hack into our Amazon accounts when he was five.
492360	493760	I think it's one of his gifts.
493760	498360	And so definitely try to balance when is it OK
498360	501320	to give your kids technology because we live in a world
501320	502200	with technology.
502200	504840	And so we want kids that are comfortable with it.
504840	506960	But where do you draw the line?
506960	508200	And I'm not just a mom.
508200	510320	I also work for a tech company.
510360	515920	So I work for a policy and global intelligence company
515920	517280	in half or several years.
517280	520840	And I also, in the past, have worked
520840	524520	to make sure that there's an intersection between consumers
524520	525960	and internet privacy.
525960	528840	And so this is an area that's particularly of interest to me.
528840	530960	And one of the reasons why I thought this group would be
530960	537360	really great to get together is you all put so much work
537360	539200	into everything you're doing.
539200	542160	You are badasses professionally.
542160	547760	And you're also incredibly devoted to your kids, as am I.
547760	549680	I'll just introduce myself as well for a second.
549680	550640	I'm John.
550640	553880	I do video production.
553880	557000	I do a podcast that you may be watching.
557000	562520	And I have two kids, Boy Girl Twins, who are 18 years old.
562520	563600	I can't believe it.
563600	568760	They are seniors in high school and getting ready to leave us.
568800	571000	And it's a whole lot to take.
571000	576000	But I certainly, as we think about these AI existential
576000	579440	issues, I certainly make kids are always at the top of my mind.
579440	582400	So why don't we go around the world one more time?
582400	587040	And let's talk about our experience thus far with AI.
587040	589280	We're going to talk a lot about existential risk
589280	590600	and in the podcast.
590600	594960	But before you became aware of this existential risk
594960	597320	conversation, what were your impressions?
597320	599560	And just sort of briefly, what were your dealings with it?
599560	602280	I know Jen and I don't know, Stephanie, either.
602280	605800	You all have been doing some really interesting things
605800	609680	with neurodivergence and kids and AI, which
609680	612280	has some promise in that space.
612280	614240	So Jen, you want to start off and just talk maybe
614240	617640	a little bit about what you've been doing with AI in this area?
617640	619280	Sure.
619280	625680	I mean, just as a designer specifically using graphic design
625720	629440	as my trade and my weapon of choice,
629440	636440	it's always tech and the intersection of how we use it
636440	641920	to kind of emote joy has always been a part of my world.
641920	649520	And so I really use AI to just amplify good.
649520	652280	Crystal, in the marketing world, I'm in the marketing world.
652280	655960	I mean, you must hit every day with 65 new things AI will do
655960	659240	for you that are fantastic that you need to learn yesterday.
659240	660680	Yes, absolutely.
660680	664000	And there's this worry that you're
664000	667640	not going to need marketers anymore because genitive AI is
667640	668880	going to get so good.
668880	670400	I still believe that humans are better.
670400	672080	It still needs oversight.
672080	674760	There's a lot of pros, but a lot of cons.
674760	677800	It's like the smartest, stupid thing around.
677800	679080	Stephanie, how about you?
679080	682160	AI up until we had this conversation.
682200	684480	Yeah, it's funny.
684480	691680	Back this time last year, I was sent a link by a consultant
691680	694000	that we work with, a tech consultant that we work with
694000	698600	who also does some strategy with our to chat to you.
698600	701240	And he said, I know you're working on your presentation
701240	704720	for your big company retreat.
704720	705920	Check this out.
705920	709040	What I ended up doing, Rick, I used it to create the outliering,
709040	712040	to create the agenda, to help me come up with activities,
712040	715160	specific work activities.
715160	716520	And it was awesome.
716520	718360	And I was like, oh, my gosh, this is great.
718360	720920	But I think it's different.
720920	724960	The permission we're in now with generative AI
724960	729120	and the rames to get to general, what is it?
729120	731160	Generalized artificial intelligence?
731160	733400	Yeah, AI, artificial intelligence, yeah.
733400	735120	Artificial general intelligence.
735120	738360	That, to me, is a different story.
738360	741360	And that's where I start to get nervous.
741400	747640	And that's where my kids start to call me a ladderer.
747640	750040	Well, I think stuff you do to your husband, they have.
750040	751760	If you cut their head.
751760	752240	They have.
752240	754320	And it's the access, right?
754320	755160	So you've used it.
755160	757520	It's been embedded in the things that you've been using.
757520	760560	But now they have direct access to it
760560	762880	in a way that's different, right?
762880	766480	Yeah, I mean, I think when we all actually got to use it,
766480	768440	you know, like it was one thing to hear about this thing,
768440	769840	artificial intelligence.
769840	773280	And then chat GPT-4 came out and we all,
773280	775800	some friend of us talked us into logging into it
775800	777640	and signing up and checking it out.
777640	780760	And you were like, oh, my God, this is fundamentally different
780760	783560	from anything ever before.
783560	785840	So that will lead me into my next question.
785840	787760	And this can go to anyone.
787760	791560	When you first heard the notion of existential risk,
791560	795520	that like, within our lifetimes,
795520	799760	there could be a world where there is no life on Earth.
800440	804960	We are expected lifetimes, I guess.
804960	805920	How did that hit you?
805920	807680	What was your first thought?
813600	816560	I think I believed.
816560	818760	I mean, I sound so crazy.
818760	821800	I feel like I sound a little crazy to throw like,
821800	823360	I needed to do my research.
823360	826880	Like, I needed to know who was saying this.
826880	829120	Were those people credible?
829160	831520	And where it's been really interesting,
831520	832960	I think of the camps, right?
832960	837960	The advanceers versus the Jumers or their new camp.
838360	842860	And each camp has their expert that went to this
842860	846800	Ivy League institution and work at this, you know,
846800	850040	this big tech firm or used to work at the big tech firm.
850040	854480	Like, both sides of the camp have their, you know,
854480	856640	qualified experts.
856640	859400	So, you know, it's really like sitting through,
859400	863640	I think, for me and going like, well, in my guts,
863640	866760	because we all, as parents, as mom,
866760	870600	we all have the guts in the same thing, which says,
870600	873240	you don't know, something does it doing right.
874240	877720	And then when you start to see the onion peel away,
877720	881240	like with that mess with them all then,
881240	884160	like when you start to go like, wait a minute,
884160	887880	like what's it going on with good, no offense,
887880	890080	like bull even their toy.
890080	892840	That's all about the green meat and all about the money in,
892840	895400	oh, it used to be a non-profit, but no, it's not.
895400	896800	Now we're focused on profit.
896800	899960	Well, wait a minute, like wasn't your origin
899960	902960	of your company focused on the good of the Nermit?
902960	907960	So I guess I'm still a little bit, I'm slow with like,
908680	911980	I will always continue to just kind of try and pull
911980	914940	from various sources to make sense.
914940	918380	And then frankly, trust my gut to go,
918380	920100	I need to take this seriously.
920100	921700	I didn't take the pandemic seriously
921700	922540	when I first came out.
922540	926340	My husband and my boss were like, canary in the coal mine.
926340	927880	We were like, this is gonna be bad.
927880	929740	I'm like, I'm going to the conference.
929740	932100	You know, like, don't go to the conference in March.
932100	933940	I was like, I'll be fine.
933940	936420	I was like, don't go and then the conference back.
936420	939700	But this is like my canary moment, I think.
940700	941660	Yeah.
941660	942500	Yeah.
942500	943340	Excellent.
943340	944780	Crystal, Jen, what do you think?
944780	946740	Human extinction.
946740	950180	I think extinction is a strong word
950180	951860	and over what period of time.
951860	955500	So I think, you know, I think that's the hard part about,
955500	956740	well, there's two different things.
956740	960100	There's the scary part is, I think going back
960100	962380	to what Stephanie said, who is saying this
962380	964220	and based on what information,
964220	966740	I think the scary part for me was that the people
966740	969580	that created it were warning us
970420	972540	and said, we don't know how to control it at this point.
972540	975700	And so when you have a creator that doesn't understand it,
975700	977300	it doesn't understand what it's doing.
977300	980260	That's very scary to me.
980260	982140	That's the first thing that worried me.
982140	983980	The second thing that worried me is you look
983980	988540	at social media, et cetera, and these things were created
988540	991140	so that you cannot, like as humans,
991140	992780	like you can't win against it, right?
992780	995300	Like you become addicted to it.
995300	996140	You can't help it.
996140	998860	And so I think those are the two things
998860	1000380	that concern me as you can see
1000380	1004580	how social media spreads misinformation, right?
1004580	1006900	So if you think about it, something that the people
1006900	1011100	that created it can't control and you have this,
1011100	1012740	and then the people that created it
1012740	1014540	are warning you about it.
1014540	1017260	And then you've seen just implications
1017260	1020380	from not generative AI, but a tool like social media
1020380	1023100	that does use it, that's spreading misinformation.
1023100	1025860	You can see how the risks could be there.
1025900	1027580	So I'm not fully doom and gloom
1027580	1030740	like the human race is going away,
1030740	1032300	but I think it's very concerning.
1032300	1035420	And if you don't have people stepping in
1035420	1037860	to take a stronger role in managing against it,
1037860	1039900	then it becomes scarier.
1039900	1041420	Yeah, for sure, Jen.
1042300	1046420	Yeah, I mean, I still think that it takes us as humans
1046420	1050260	to be able to make a lot of these tools
1050260	1052460	do what we want them to do.
1052460	1054220	I've watched my friends.
1054220	1056740	I've watched other artists, my husband,
1056740	1058420	use them to the fullest extent.
1058420	1062660	And there's still essences of the person,
1062660	1066300	the human that exists within the creations.
1066300	1069180	And that's what I really love the most
1069180	1073580	is how can you kind of coexist in the world
1073580	1077540	that essentially we've built sort of.
1077540	1079260	Yeah, yeah, I like it.
1079260	1081500	Even in this group of four,
1081540	1084620	we have, I think, four unique takes
1084620	1089380	on almost the same set of information, right?
1089380	1092580	Which is, and so Stephanie, to your point,
1092580	1095100	first of all, are we Flat Earthers?
1095100	1099740	Are we like Y2K people wrapping our houses in duct tape?
1101580	1102700	The answer's no.
1102700	1105060	I wish we were, honestly, I wish we were.
1105060	1106380	I hope we are.
1106380	1107220	How about this?
1107220	1108060	I hope we are.
1108060	1110620	I hope that this podcast is literally
1110620	1113220	a duct tape wrap around my house.
1113220	1116740	And I'll wake up on the January 1st morning
1116740	1119460	and I'll be like, oh, AGI happened.
1119460	1121460	And we're all fine and everything's fine.
1122500	1126700	I just, that is not my read of the academics
1126700	1127540	in the field.
1128780	1130420	I saw something on Twitter that I'm gonna send you
1130420	1133420	the other day, it's a list of 125 professors
1133420	1135980	in like a string of heads of departments
1135980	1139100	who are basically saying this same thing
1139100	1144100	that they have grave concern for existential risk
1145460	1149020	for all life on Earth if we continue to proceed
1149020	1152300	with developing artificial intelligence systems
1152300	1154980	that are not aligned with human goals and values
1154980	1156740	and that we don't know how they work.
1156740	1158460	So let's go to those two things,
1159340	1161540	alignment and interpretability.
1161540	1165380	I was stunned to know that the people who make AI
1165380	1169420	have no idea why it does, what it does,
1169420	1171660	or how to control it.
1171660	1174500	What did you think when you all first heard that?
1177700	1179140	I mean, it's not a,
1181660	1183060	anyone who wants that one.
1184220	1186860	I mean, Crystal, I'll go with you because,
1186860	1187940	if someone came to you and said,
1187940	1190620	hey, I wanna get this new product for our department,
1191860	1193280	it has tremendous power.
1194280	1196160	It could make us the best company on Earth
1196160	1198400	or kill everyone in the building.
1198400	1199640	I don't know how it works.
1200800	1201640	And I, you know.
1203640	1206080	That's the scary part that I had mentioned earlier
1206080	1207920	is that the people that created it
1207920	1209720	don't fully understand it.
1209720	1211240	And they don't know necessarily
1211240	1213360	how to necessarily stop parts of it.
1213360	1216000	And I think that that's the most concerning part.
1216000	1218800	I think that goes back to, I believe, Jen's part.
1218800	1221400	You have to have a human interaction
1221400	1225880	and you have to create rules around the use of it,
1225880	1228720	or else it's going to go into the hands of bad players.
1228720	1230960	You can't say that everyone's gonna use it for good things
1230960	1232280	because that's just not true.
1232280	1236400	And so how are we, as human beings, as companies,
1236400	1239680	as governments, as moms, as human beings,
1239680	1241600	going to all work together to make sure
1241600	1244200	it's being used for the right reasons?
1244200	1247440	Because we've all experienced the good in it.
1247440	1250360	We've also seen how it can fail, right?
1250360	1252760	You look at it, you're like, well, that's not right.
1252760	1256240	And so that's where I think the human mind
1256240	1258240	currently is different is we can differentiate
1258240	1259800	between those two things.
1259800	1261800	But I think there's a point where it's hard to do that.
1261800	1263480	And that's what concerns me.
1264560	1265400	Yeah.
1265400	1266920	Yeah, Stephanie?
1266920	1268400	Yeah, no, I agree.
1269400	1274280	Definitely, I think it's shocking, but not shocking
1274280	1278880	that people would want to run fast toward advancement
1278880	1280840	and towards being the first,
1280840	1283400	like being the first company, the first person,
1283400	1287240	the first team, you know, it's interesting, right?
1287240	1290800	I almost wish we had someone that may be described
1290800	1293080	to more of like in certain value years.
1293080	1297400	I don't know, like, and maybe you guys do, I don't, you know.
1297400	1300640	But in listening, John had recommended Mo Galdet,
1300640	1302680	who has a book called Scary Smart.
1302680	1304640	He was with, let's do the act,
1304640	1306760	but the time is only 10 years.
1306760	1309000	And she, you know, he even says,
1309000	1313160	like kind of the Western drive to make more money
1313160	1317280	and just succeed by any measure and get to the top.
1317280	1319800	And it doesn't matter if you leave in your weight.
1319800	1322080	We want to say that that's not the case,
1322080	1325600	but it's so very much the case.
1325600	1329880	And so I think as moms, too, it's hard to,
1330720	1335720	like, to parent kids when it's, you know,
1335760	1337640	when you've got the pressure,
1337640	1340040	social media or of your friends' kids
1340040	1344160	or of a man on what school, and, you know,
1344160	1347880	you're fighting that all, already, all the time.
1347880	1352480	And then you sort of, you blow that out,
1352480	1354240	you know, you miss, blow the AI.
1354240	1358720	And how else are their systems supposed to learn
1358720	1360240	other than our inputs?
1360240	1364800	And if our inputs are all for like the basis parts
1364840	1368200	of who we are as humans, then that's not great.
1368200	1371520	Like that's only setting up the system
1371520	1374880	to be based in the way that they pursue.
1374880	1378800	And so it really worries me.
1378800	1380800	You know, I also am aware that, like,
1380800	1385080	we're cognitively discriminant people as humans.
1385080	1386800	I don't know that AI systems
1386800	1388280	or maybe they'll be able to do that.
1388280	1389120	I don't know.
1389120	1390840	I feel like that might be a human, like, win.
1390840	1394640	Like, hey, I can hold two very opposing ideas
1394640	1395480	at the same time.
1395480	1398640	Like, I am a practicing Catholic.
1398640	1402560	And I also am really worried about existential crisis,
1402560	1404280	but I'm not worrying to a point
1404280	1406120	of where I can't get out of bed
1406120	1410080	or where I can't go to work or where I can't parent my kids.
1410080	1412680	So, because I think at the core,
1412680	1416400	I have this thing that, like, is my belief
1416400	1421200	and says, like, you can't, like, offer that up.
1421200	1423320	You can't worry about that.
1423320	1425760	So, I don't know if that makes sense.
1425760	1429600	I mean, like, the, I'm very concerned.
1429600	1432640	And also, I can't really, or,
1432640	1434280	day in or day out,
1434280	1436040	I'm up late 16th through another year,
1436040	1436880	or we're this way.
1438360	1439960	Yeah, no, absolutely.
1439960	1442280	Elon Musk was asked about it recently.
1442280	1447400	And he said, you know, he has existential fears of AI
1447400	1450120	and says it openly and says, in 2016,
1450120	1452960	he had spent a lot of sleepless nights worried about
1453000	1454360	what I'm worried about right now,
1454360	1455760	but that he has come to peace with it
1455760	1458320	because he's decided that there's no more interesting time
1458320	1460360	to be alive as a human.
1460360	1463960	So, he will sacrifice the potential for extinction
1463960	1467560	with the excitement of living the most exciting life
1467560	1469560	of any human generation.
1469560	1471880	I don't know that I can quite get there,
1471880	1473320	but I did find something in that,
1473320	1475600	something in that that was a little helpful.
1475600	1477080	Here's the question I have for you all,
1477080	1478960	and then I'll throw it to you, Jen.
1478960	1481320	Where are the women in this whole thing?
1481320	1483480	You know, a lot of people have seen my first podcast.
1483480	1485120	I appreciate that you all watched it.
1485120	1490120	And, you know, this is a very male-dominated thing
1490560	1492880	that is happening to the world.
1494560	1497360	Where are the women and what are your thoughts about it, Jen?
1499280	1502480	Well, I mean, there's a ton of women
1502480	1506000	that are really advocating specifically from, you know,
1506000	1509360	the black and brown perspective when it comes to ethical
1509360	1510760	and responsible AI.
1510760	1514200	Oveta Samson is a really amazing voice.
1515520	1518280	You know, Gerald Thomas,
1518280	1521680	who are specifically working to address
1521680	1524600	the conversation on representation,
1524600	1527640	because like I feel like the conversation shifts
1527640	1530600	when you're asking, you know, black and brown folks to say,
1530600	1532560	okay, well, what will you do with AI?
1532560	1536480	What will you do with these tools
1536480	1539280	and how will you make them radical?
1539280	1541600	And how will you, what will they look like
1541600	1543520	when they're kind of placed in the hands
1543520	1546000	of multi-pre-marginalized people
1546000	1548920	who have always been denied access
1548920	1552200	for these specific tools to create
1552200	1555040	and build the worlds that they specifically want to see.
1555040	1558120	You know, the tools that they want to see, you know,
1558120	1560400	that have been, you know, kind of denied.
1560400	1563240	And so I feel like making sure
1563240	1565760	that we can kind of have space,
1566040	1569240	and I have like my LinkedIn open and, you know,
1569240	1572280	being able to just, you know, like, okay, yeah,
1572280	1574840	well, who's having these conversations?
1574840	1579040	And I really love that the language specifically
1579040	1584040	has been evolved into incorporating responsibility,
1584840	1589240	you know, equity, being able to,
1589240	1591040	and I love that, you know,
1591080	1595880	black women who are in, invested in machine learning
1595880	1598640	and, you know, gen AI are like,
1598640	1600960	are leading those conversations.
1601840	1603200	So, you know, it, like I said,
1603200	1607000	like it begins to shift when, you know,
1607000	1609120	when the conversation is put in the hands
1609120	1612560	of the multi-pre-marginalized.
1612560	1613840	Sure, sure.
1613840	1614680	Thank you.
1614680	1615520	And I feel like
1615520	1620520	there's a maternal attitude missing to this whole thing.
1621480	1624720	Like it's like some 30-year-old guys are like,
1624720	1627200	hey, I got this car, it goes super fast.
1627200	1630200	I'm gonna go race it at 300 miles on the highway.
1630200	1632800	And nobody's being like, you might hurt someone.
1632800	1635280	Don't, you know, you might want to think twice about that.
1635280	1638680	Have you thought about the other people?
1640200	1643640	Crystal, Stephanie, any thoughts about the male domination
1643640	1646480	of this suicide cult?
1646480	1649800	So it's funny, John, because I, as you asked this,
1649800	1652080	last week I saw her film in Danny Herrera,
1652080	1654200	she was like, it's an AI advocate.
1654200	1657480	She posted about the New York Times article.
1657480	1660800	So I think, you know, there is complicity in,
1660800	1664920	particularly in media, that if you're not gonna cover,
1664920	1667240	if you're gonna cover like the 12 game changers
1667240	1669520	from leaders in AI, you can't find a woman
1669520	1672520	and she would like, lazy, and it's totally right.
1672560	1674920	Lazy reporting, lazy coverage.
1674920	1677120	I'm not calling you crazy, John, I promise.
1677120	1679280	But it's like, because you were like,
1679280	1680360	I didn't have any in mind.
1680360	1681200	And I was like-
1681200	1682040	I looked hard.
1682040	1683120	That really, like, yeah.
1683120	1688120	But she posted this and she had over 600 comments of people
1689360	1691760	and out of those, like, probably let's say,
1691760	1693520	let's just think of like half,
1693520	1697640	would-listing women leaders in AI.
1697640	1698960	And you know what's really interesting?
1698960	1700880	Just a couple that she mentioned.
1701000	1706000	Lee, Rana L. Touloubi, Margaret Mitchell, Tim Gibrew,
1707400	1711640	Siren Snyder, Vanilla Braga, Joy Bouladini.
1711640	1713960	All these women, the stuff they're doing
1713960	1718240	is around gender bias and around like ethics
1718240	1721040	and around same food and around all this stuff.
1721040	1723000	And you're like, yeah.
1723000	1726920	So then cover things that we're saying
1726920	1729080	and like that are coordinated with this conversation
1729080	1731000	around how do we protect our, you know,
1731000	1736000	I think there's a real issue when it comes to coverage
1736120	1738360	and people and the New York Times got flammed
1738360	1741760	and as they said, right.
1741760	1744200	You know a little hot about that one.
1744200	1746200	I need to swear words, but I got hot.
1747720	1750680	I think, I do think there are a lot of women
1750680	1751520	that are doing things.
1751520	1753720	I don't think they're getting the coverage
1754520	1756200	to Stephanie's point earlier.
1756200	1758880	I also think there's an interesting part to it
1758880	1760040	about geography.
1761200	1764960	And so you see women and men, right in the EU,
1764960	1766840	it's always more kind of risk a burst
1766840	1768960	when it comes to technology.
1768960	1772720	If you look at privacy, so I have a background in privacy.
1772720	1775280	The strongest privacy laws are in the EU.
1775280	1778960	And then in the US, it usually goes to California next
1778960	1781960	and then it might go federal at that point.
1781960	1783560	I think you're seeing that in AI.
1784840	1786520	And so you're seeing, so I think you also
1786520	1788720	have to look geographically.
1788720	1791720	In the US, we are capitalistic.
1791720	1793480	We're gonna go for whoever's gonna make
1793480	1794440	the most money quickly.
1794440	1798840	I think the EU has more of a familial kind of community sense
1798840	1801960	than the US and then you're gonna see that play out.
1801960	1804920	So I'm curious how the EU standards
1804920	1808360	are gonna impact the US in other parts of the world.
1808360	1811160	So I'm not sure, I think there's a male versus female
1811160	1814320	US coverage, but there's a really interesting work
1814360	1818080	happening geographically as well.
1818080	1819320	Yeah, that's super interesting.
1819320	1822240	I absolutely believe there are women
1822240	1824000	doing incredible things in AI.
1824000	1825560	I think they're not getting any coverage.
1825560	1828400	And I think that it's also just really hard
1828400	1831040	for those sort of stories to break through
1832120	1834360	because it's so male dominated at the top
1834360	1838240	that it's a real problem and it's a problem in bias.
1838240	1843240	And it's a problem in what I wanna talk about today
1843800	1847480	which is some 30 year old guys believing
1847480	1851280	that they have been authorized to exercise
1851280	1854080	existential risk on behalf of us.
1854080	1859080	Like they go to work today thinking that somehow
1859360	1862760	it's reasonable and appropriate for them to toy
1862760	1864880	with all of us dying.
1866680	1869960	And I just can't get over how that's possible
1869960	1872800	for people to get up then and say,
1872840	1876520	I'm gonna come back tomorrow for another day of this.
1878720	1880920	Crystal, I wanna get at one thing you talked about
1880920	1883040	a little bit earlier, cause I feel like even in this group
1883040	1885960	of four we may have some differences of opinion.
1885960	1889680	I feel like Stephanie, I feel like you can picture
1889680	1891760	human extinction a little bit.
1891760	1893840	Like we've had some conversations and I feel like
1893840	1896760	it's a what, like think about your street.
1896760	1901360	Like your literal street, what does your street look like
1901400	1906400	the morning after all life on earth is eliminated.
1908680	1913680	It is laughable, like it's so uncomfortable that,
1916600	1918600	you know, it's really hard to contemplate.
1918600	1922000	I did not think it possible in my life
1922000	1923640	that I would be contemplating these things.
1923640	1926600	And yet I find that I'm doing it on a daily basis.
1926600	1928560	So Crystal, I feel like, and Jen, I don't know
1928560	1931280	where you are on it, but I feel like Crystal,
1931960	1935560	well, I'm a black woman in America.
1935560	1940120	So like I feel like I'm at risk every single day,
1940120	1942000	like walking in the street.
1942000	1944560	I mean, that's why like I have like,
1944560	1947280	you know, if we really wanna get deep with it,
1947280	1950200	like human extinction, like, you know,
1950200	1953520	society has been trying to like eliminate me my entire life.
1953520	1954800	You know?
1954800	1959320	So it's just, to me, it doesn't take tech,
1960240	1962160	technology to do that.
1962160	1963840	It doesn't take technology to do that.
1963840	1966440	It just takes, you know, being erased
1966440	1970600	and having like my culture become erased and appropriated.
1970600	1973680	And, you know, if anything, you know,
1973680	1975560	just eliminated from the conversation,
1975560	1979440	which is why it's so important for us to kind of take up
1979440	1982560	as much space as we can within the AI space,
1982560	1986720	within, you know, ethical conversations on responsibility
1986760	1989560	and what that means within artificial intelligence.
1989560	1992840	Because, you know, I mean, at the core, like, you know,
1992840	1997040	as, you know, black people have been, we are the oracles.
1997040	2001080	I mean, we have been guiding people through the path of,
2001080	2004640	you know, of survival for centuries.
2004640	2007960	Like, you know, Harriet Tubman, I mean,
2007960	2010360	she was literally using astronomy
2010360	2013480	and her own disability to just to guide people
2014480	2017360	to make sure that they can actually survive, you know?
2017360	2020600	Yeah, it's really super interesting to think about it,
2020600	2023480	like that, Jen, to think about different people
2023480	2024880	perceiving it in different ways
2024880	2027200	based on their own personal experience.
2029040	2031200	Yeah, I'm so glad that you joined.
2031200	2034080	Bet we have these perspectives, that we are not.
2034080	2034920	You know what I mean?
2034920	2037920	Like, I'm really glad that you brought that up
2037920	2038840	because I think,
2039680	2044680	I think within, you know, wake up in fear
2046600	2048160	and fear for your study,
2051160	2053840	that's something that get within your bones.
2054800	2056480	And it's probably been, you know,
2056480	2058960	in your family and in your bones forever.
2058960	2063840	And, you know, it's a privilege that we don't have that.
2063840	2066800	And now we're thinking about it.
2066800	2068960	And I also, my next brain, you know,
2068960	2071400	brain knowing of one brain, two mouths,
2071400	2072840	maybe I have some other weird brain
2072840	2074520	fitting some plate though.
2074520	2078000	But, I don't even know, Monday.
2079160	2080480	Well, my next thought though is like,
2080480	2085040	and let's not reach the system that way.
2085040	2087800	Like, maybe we have the chance
2087800	2091840	to make the, it's a generalized AI,
2091840	2093640	we need there's a chance
2093640	2096480	that it can actually better than us
2096520	2099000	as humans in that way.
2099000	2103600	Like, we have the chance right now to teach it.
2103600	2104440	Yeah.
2104440	2105880	So, like they teach our kids,
2105880	2107280	like Murgale Devka, right?
2107280	2109000	Like, if they're in their infancy
2109000	2111280	or they're toddler famous right now,
2111280	2113840	which they are, and like,
2113840	2118600	I don't need like, all of the good people,
2118600	2122400	which is a lot more than the bad people in this world,
2122400	2125000	all the billions of really good people
2125000	2126040	who comes on all kinds,
2126040	2131040	how do we get people that don't even have internet
2131440	2133520	to be able to participate in chats
2133520	2135600	with, you know, in a generative AI
2135600	2137760	so that it had that perspective?
2138840	2141800	You know, it's glad to have those inputs.
2141800	2145400	It can have the influence of 3000 dudes
2145400	2146400	and a couple of ladies.
2146400	2147240	In San Francisco.
2147240	2148080	Yeah.
2148080	2150440	In San Francisco, to your point about John Birkin,
2150440	2152480	it needs like the input.
2152480	2155720	There's a people in my neighborhood,
2155720	2157120	friends of my kids' friends,
2157120	2158480	and they're starting to be like,
2158480	2161000	oh, you want to talk AI, talk to me.
2161960	2165760	But one of the mums, it's a lot of mums,
2165760	2169240	and soon as we're both using it,
2169240	2172840	and I do, I used it to draft content
2172840	2177200	for like, an invite for a party.
2177200	2178600	Like, that sounds silly,
2178600	2181800	but I was sitting at a basketball practice for my son,
2181800	2184360	and I was like, oh, he's trying to get us,
2184400	2187960	there's one thing, we did this two seconds, right?
2187960	2189520	So I could actually be mostly present
2189520	2191880	during the basketball practice.
2191880	2193920	And I gave it a quick prompt,
2193920	2195280	and then I revised it twice,
2195280	2197760	I gave it three, and I thought it's very great.
2197760	2199440	I feel better now, if I got it.
2200480	2202720	But I use it, and for some people,
2202720	2204400	it's like, well, you shouldn't use it
2204400	2206440	if you're worried about the end of the financial trip
2206440	2209040	with other, you got it, right?
2209040	2211160	Right, absolutely, and that's what I was talking, yes.
2211160	2215480	I firmly believe that 99% of the AI out there
2215480	2218240	is totally safe, and should be used,
2218240	2220160	and there's a lot of incredible benefits
2220160	2221000	we could get from it.
2221000	2223120	Like, a lot of the safety research experts say,
2223120	2224600	if we just pause for 20 years
2224600	2227000	and just dealt with the tech we have now,
2227000	2229520	for 20 years, we could get incredible benefits
2229520	2233120	for health and medical and scientific breakthroughs
2233120	2234600	and all these kinds of things.
2235600	2238880	But we appear to be racing towards it
2238880	2240720	much more quickly than that.
2240760	2243400	What do you all hear in your conversations
2243400	2245200	with your friends, with other moms out there
2245200	2248120	at the park, at the water cooler, whatever?
2248120	2252280	What is the tenor and tone of the conversation
2252280	2255560	at this moment here in December of 2023,
2255560	2260560	the year AI came out, and we all learned about it?
2261360	2263160	How do you feel like people are feeling?
2264360	2268280	I feel like when it comes up in my group of friends,
2268280	2270040	it's less around human extinction,
2270040	2272200	it's more like how the kids are gonna use it.
2272200	2275560	So like, are my kids going to properly learn
2275560	2277920	how to write an essay, or are they just gonna feed
2277920	2279960	the data into chat GBT?
2279960	2282640	And so where I hear it more, it's less around
2282640	2284360	are humans going to go extinct?
2284360	2289120	It's more around how is my child going to be learning
2289120	2290840	to use it or to not use it?
2290840	2294360	How am I gonna make sure that my child has the right skills
2294360	2296600	so that when they do go into the workforce,
2297480	2299880	they're able to kind of do the job.
2299880	2303440	You're seeing schools now have kids take tests
2303440	2305480	with a pen and paper again.
2305480	2307520	I'm seeing several universities doing that.
2307520	2310280	Who would have thought that because they wanna make sure
2310280	2312000	they can still write an essay?
2312000	2316160	So I think it's gonna be interesting as parents,
2316160	2318080	and then the education system as well.
2318080	2320560	So how do you make sure your kids are learning?
2320560	2324160	So I don't hear as much about like end of the world as I do.
2324160	2326520	What is the application in the school system?
2326520	2328960	And then how is my child learning?
2329080	2329920	Exactly.
2329920	2331240	Yeah, yeah.
2331240	2332280	No, I hear a lot of that.
2332280	2334840	And I'm actually gonna circle right back
2334840	2336800	actually to what Jen was talking about just a second ago
2336800	2338280	because I had something I wanna talk about,
2338280	2343200	which is, so I started this podcast seven weeks ago
2343200	2345320	was working on it for a couple of months before that.
2345320	2347960	And it was right around, it was all coming together
2347960	2351720	right around the October 7th attacks in Israel.
2351720	2355160	And kind of, you know, to what you're saying, Jen,
2355160	2360160	it's like people feel directly threatened
2362000	2366360	on a personal, emotional level in different ways
2366360	2367200	at all times.
2367200	2370240	And it's really hard for this very abstract issue
2370240	2374080	of AI safety, something inside a computer system
2374080	2378000	to emotionally resonate in the way these very personal,
2378000	2380760	very direct threats do.
2380760	2383080	And I sort of don't know the way around that.
2383080	2386880	Like I don't know the way this really amorphous thing
2386880	2389080	can compete with these other causes
2389080	2392040	that are so visceral for people.
2392040	2393560	And I don't know the answer.
2393560	2394520	I'm just putting it out there.
2394520	2396240	You know, and I don't know that there is a good answer.
2396240	2398840	Like, I think people that are focused on issues
2398840	2401520	of immediate human suffering and human condition,
2404240	2405280	I can't come in and say,
2405280	2407320	hey, everybody should drop what they're doing
2407320	2411360	and go work on AI safety and forget about
2411360	2413640	any stigmatism and racism and all the horrible things
2413640	2416200	out there, hate and all the horrible things on the planet.
2416200	2420680	Like people working on those causes have to continue,
2420680	2424880	but it's like we as a society have some sort of like limit
2424880	2426760	for cause stuff.
2426760	2430040	And I feel like we're tapped out
2430040	2433280	and AI is not gonna get, you know,
2433280	2437960	the attention that things that affect people
2437960	2440400	more directly and personally immediately do.
2441680	2446400	Well, then I also think you have to look at the inputs, right?
2446400	2450080	Going into AI as well.
2450080	2453040	So you have deeply personal things to people,
2453040	2455400	but people have very different views on things.
2455400	2458720	And you have all those disparate inputs
2458720	2459920	going into generative AI.
2459920	2461440	And so then what comes out of that,
2461440	2464520	I think is an interesting question.
2464520	2466680	I think AI can see and I'll just say it.
2466680	2468240	I think it means a new brand.
2468240	2471120	I think the term AI safety was boring.
2471120	2472520	I don't think anybody's guessing.
2472520	2473360	Life is about it.
2473360	2475160	I don't think they know what it means.
2475160	2476800	Well, it sounds like a safety poll.
2476800	2477640	Like what?
2477640	2480080	Like AI safety is sooner or later.
2480080	2481160	Let me ask this.
2482760	2485800	Show of hands, and I think I'm gonna know the show of hands.
2485800	2488080	I think it's me and Stephanie and Crystal and Jen
2488080	2489080	on two sides.
2489960	2493000	Could you actually imagine the end
2493000	2494640	of all living things on earth?
2497160	2498880	Because of AI or in general?
2498880	2500440	Yes, because of AI.
2500440	2501400	Because of AI.
2504120	2505920	No, I can't.
2505920	2507600	I mean, can't.
2507600	2510200	No, I mean, just because like I know how to use it.
2510200	2513440	Like I know how to use it for like,
2513440	2517600	I just know how to be radical with it.
2517600	2518440	And I know how.
2518440	2521520	The concern is when it starts using itself, right?
2521520	2523840	When it starts recursively improving itself,
2523840	2526280	setting its own goals, becoming its own agent.
2527280	2529640	You know, it goes off on its own
2529640	2531600	and it's no longer checking with us.
2531600	2535440	That to me is very conceivable, if not likely.
2535440	2537680	But how is it gonna do it though?
2537680	2540840	Like, and this is where we need to get very specific
2540840	2541960	about the tools.
2541960	2544960	Like, okay, so what tools are we specifically talking about?
2544960	2546320	Like what AI?
2546320	2547880	Like mid-journey, for instance.
2547880	2551840	Like mid-journey cannot create problems on its own.
2551840	2553400	Totally fine, not a problem.
2553400	2555640	And anything that's out there that consumers are using
2555640	2556760	is not a problem at all.
2556760	2559360	It's the 1% of the frontier level work
2559360	2562360	that open AI, Google, Microsoft,
2562360	2564880	DeepMind, Anthropic are doing
2564880	2568360	that is pushing the frontier boundary of the systems
2568360	2572000	where it starts to recursively improve itself.
2572000	2575280	And nobody knows what happens after that point.
2575280	2577160	So, yeah, Crystal.
2577160	2578640	The two things that I think about,
2578640	2582080	I have a hard time thinking humans become extinct.
2582080	2583720	That's like, I have a hard time there.
2584280	2587800	Where I can see is questioning,
2587800	2589680	what does it mean to be human?
2589680	2591760	And Stephanie and I had a conversation around this
2591760	2596760	because at some point, do we make generative AI so smart
2597000	2599160	or kind of robots or whatever
2599160	2603040	that they become citizens, humans, et cetera?
2603040	2604600	And then you have this,
2604600	2608160	then you have a kind of question on who makes decisions.
2608160	2610360	Because if generative AI keeps getting smarter,
2610360	2611600	then you can see over time,
2611600	2614600	humans kind of move into a different role within society.
2614600	2619600	So I can see some interesting things happening
2620120	2625040	if you look into the future of AI becoming human
2625040	2627600	or how we define human is one aspect.
2627600	2629600	But the thing I keep thinking that generative AI
2629600	2634080	can do really well more quickly is cause absolute chaos.
2634080	2636720	Because you have disinformation, lies,
2636720	2638520	and then you lose trust in society.
2638520	2640480	And once you lose trust in society,
2640520	2641960	that's where a lot of bad things happen.
2641960	2644320	That's where war happens.
2644320	2647040	You have groups of people, again,
2647040	2649200	fighting different interpretations.
2649200	2652880	And so the world I see that's more plausible more quickly
2652880	2657040	is just sheer chaos with disinformation
2657040	2658960	because of generative AI.
2658960	2662240	And I don't disagree with any of that stuff,
2662240	2664520	but I just wanna pick at it a little bit further, right?
2664520	2666400	So with Crystal and Jen,
2666400	2669440	so you guys watched the first episode of the podcast
2669440	2671440	where you know that there's this 22-word statement
2671440	2674040	that everybody in AI put out that says
2674040	2677880	that artificial intelligence is an extinction risk
2677880	2681120	along the lines of pandemic and nuclear war, right?
2681120	2686120	And so my question is if they're saying it,
2687560	2690000	if other people, if the literal people
2690000	2692000	who are making it are saying it,
2692000	2697000	why do you think you and the 99% of the public
2698000	2703000	is like, ah, yeah, but like they don't really mean it.
2703480	2705080	Like they said extinction,
2705080	2707040	but they really meant something else.
2709040	2714040	Like I just, I feel like I could be extinct without it.
2715800	2717240	Like who I am as a person,
2717240	2721720	like I could be extinct without it, you know?
2721720	2726720	Like I was like my people, my culture, you know,
2727680	2730000	without technology, I mean, you know,
2730000	2735000	literally they were using colonialism and racism
2735080	2738400	and they were using black and brown bodies
2738400	2743400	as they were harvesting like enslaved people,
2743400	2746400	in terms of, so I feel like it's gonna take more
2746400	2749680	than technology, more than technology
2749680	2753240	to like make us extinct as a people.
2753240	2756200	I still feel like, you know, black people,
2756200	2760280	multi-multiply marginalized people, like we will never,
2760280	2763680	like nothing will be able to like overpower us
2763680	2766560	because of what we've been able to endure
2766560	2769560	and what we've been able to survive.
2769560	2774560	So for me, I'm like, okay, come at me AI, like, you know,
2774640	2777360	you know, it's just like, like, come at me.
2777360	2780480	Throwing down the, she's throwing down the bell.
2780480	2784160	Like, come at me tech bros, like, I got you, like,
2784160	2787440	there's no way that you can like completely like,
2788560	2790480	you know, erase me because like I've been through
2790480	2794760	so much more than technology can, you know.
2797320	2798680	Yeah.
2798680	2800560	That's kind of like where I'm at.
2801520	2802360	Yeah.
2802360	2804480	I think it's generally hard to think
2804480	2808080	about existential problems, like,
2808120	2810280	and it's really hard then to think
2810280	2813720	about existential problems with your kids.
2813720	2816400	Like, as it relates to your kids because,
2816400	2820040	and I love your shirt, Jen, for free and human.
2820040	2823440	And it's like, maybe that could be like the,
2823440	2827360	like, is it like a combo of raising good humans
2827360	2832360	and just raising, I mean, again,
2834440	2836200	is we haven't read Mo Galback's book,
2836200	2837560	Scary Smiles Through Day to Play,
2837560	2839600	called the AI, we have malware like cobblers
2839600	2842160	and we should raise them like they raise our kids.
2842160	2846040	So we kind of, we show them what it means
2846040	2847240	to be a good person.
2847240	2850800	Like, what do we mean when we're a family person, right?
2850800	2853080	What do we mean when we're saying, I think human?
2853080	2856360	And how all the, you know, the interactions
2856360	2858240	you've had in real life, you know,
2858240	2860800	digitally living with that face.
2860800	2861920	Do you feel sorry?
2861920	2865120	Do you feel mean or do you try and do better?
2865160	2869600	Like, if that's, well, I don't know.
2869600	2873160	Like, John, even in that philosophy we talked about,
2873160	2876200	like, as humans, we can agree on what we mean
2876200	2878080	as to be a good human.
2878080	2882520	And then, you know, so if we can agree as humans
2882520	2884680	what it means to be good,
2884680	2888160	how will we going to teach a toddler?
2888160	2890200	Me do though, me do though.
2891280	2893680	If the systems are set,
2893680	2897880	and the intelligence is set to be generatory,
2897880	2899200	not to be generatory,
2900520	2902800	not to be generate, right?
2902800	2905480	I get what generative and other way it's like,
2905480	2906720	let's just break it down.
2906720	2910800	Like, if it is supposed to, like,
2910800	2915800	it's needed to create and not be destructive,
2916160	2919920	then let's have more people do that.
2919920	2922080	Well, I don't mind a real general,
2922080	2923680	but some things that just went into that,
2923680	2925800	like, you know, just more,
2925800	2927280	there's more inputs,
2927280	2931160	but more, more people
2931160	2934160	that are this one, two, kind of that.
2934160	2935560	Well, in some ways,
2935560	2937480	that's why I don't stop using AI,
2937480	2938880	like, let's get generative,
2938880	2940480	like, let's get that generative,
2940480	2942120	that I particularly worry about.
2942120	2946120	Let's get more people using it in a good way.
2946120	2949440	I'm a little bit more bullish, like, John,
2949440	2953080	I believe that you have 250 people that have signed this,
2953080	2954560	oh, who own a lot of the companies
2954560	2956520	that are working on this.
2956520	2958880	So the people that are saying it's a risk
2958880	2961640	also have the power to put guardrails
2961640	2964000	and to service leaders, and I urge you.
2964000	2965360	And I also believe that,
2965360	2967280	so if they say that's a problem
2967280	2969640	and we want to do something about it,
2969640	2970480	that's a good thing.
2970480	2972080	You've got, you know, you know,
2972080	2973920	you're always going to have the regulation come in.
2973920	2976120	Governments tend to be slower,
2976120	2978200	but I do believe you've got people that can stop it.
2978200	2980400	It's in a toddler stage.
2980400	2984080	As women, as moms, it's all of our responsibility
2984080	2986240	to make sure we have diversity of thought
2986240	2988920	in all the products that we're building,
2988920	2991320	in our marketing that we're doing,
2991320	2994400	making sure that we're hiring the right people for the roles,
2994400	2999320	and so we're not powerless if we all are raising good humans
2999320	3000560	and we're being good humans,
3000560	3003160	and we're making sure we're getting the right voices
3003160	3006360	at the table, then I believe that we can,
3006360	3007400	we're not going to stop it,
3007400	3010000	but I think we can use it for generative
3010000	3011640	and not degenerative purposes.
3011640	3014040	You're always going to have the nefarious folks
3014040	3015480	trying to do things,
3015480	3017680	but I do believe that these tech companies
3017680	3020280	know what they're doing and because they're funding.
3021280	3023440	And if they can actually start to say,
3023440	3025240	why don't we get a group together?
3026840	3027680	You can do something.
3027680	3028800	So I'm bullish.
3029960	3032240	I can see where it can go in dangerous places,
3032240	3035520	but I have to believe that humans can be better,
3035520	3038000	smarter to solve this.
3038000	3040320	Okay, so what do you think?
3040320	3043160	I've watched a ton of AI safety podcasts,
3043160	3046800	but I have never seen one quite like that before.
3046800	3048600	Real quick, I think Stephanie is right,
3048600	3051240	that AI safety is the wrong term.
3051240	3053960	It's boring and it's soft.
3053960	3056680	Let's talk about alternative terms to AI safety
3056680	3060520	in the comments, and honestly, I was surprised
3060520	3062600	and maybe even a bit disappointed
3062600	3067200	that all three moms did not fully buy my case
3067200	3069200	even after having heard it,
3069200	3071800	that human extinction from AI
3071800	3074600	is the most urgent threat we face.
3075640	3078260	I did not fully make the sale.
3079840	3082800	That was a little hard to take, but it's okay.
3082800	3084440	This takes time.
3085400	3087840	Quickly, my own mother, who I love dearly
3087840	3090800	and I think is an incredibly smart woman,
3090800	3093920	watched the first three episodes of this podcast
3093920	3096720	and was still telling me she didn't really fully get it,
3096720	3098200	but then after episode four,
3098200	3101160	she told me it started to make sense.
3101160	3103960	So that's how this is gonna go.
3103960	3106800	We, the ambassadors of this message,
3106800	3108840	let us not be discouraged
3108840	3112040	when someone is not immediately convinced
3112040	3113600	of what you're saying.
3113600	3116400	The idea of no life on earth at all
3116400	3119360	is something that is very difficult to process
3119400	3121640	for all of us, myself included,
3121640	3124840	but if we are to have a chance,
3124840	3127120	if our children are to have a chance,
3127120	3129360	we must go through the process,
3129360	3132760	we must try to save ourselves.
3133800	3135960	Thank you so much for watching.
3135960	3138800	Next week, we're gonna start pointing fingers
3138800	3140680	and naming names.
3140680	3143720	I'm ready to blame some people for all this.
3143720	3145240	Our next episode will be called
3145240	3147740	the top three doomers in AI,
3147740	3149380	and here's a hint,
3149380	3152820	they are not AI safety researchers at all.
3154660	3157020	For Humanity, I'm John Sherman.
3157020	3158620	I'll see you back here next week.
