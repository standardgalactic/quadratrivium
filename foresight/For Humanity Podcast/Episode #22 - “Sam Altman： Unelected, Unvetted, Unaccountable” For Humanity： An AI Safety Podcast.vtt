WEBVTT

00:00.000 --> 00:08.800
Unelected, unvetted, and completely unaccountable, this one man has chosen to put all of our

00:08.800 --> 00:14.880
lives in grave danger so that he can make money and fulfill his dreams of radically

00:14.880 --> 00:21.960
altering human existence as we have never known it without even an effort to get any

00:21.960 --> 00:26.400
consent from you or anyone else in the general public.

00:26.800 --> 00:30.320
Here's the really crazy part of it.

00:30.320 --> 00:33.280
He tells us all of this openly.

00:33.280 --> 00:34.920
There is no deception.

00:34.920 --> 00:37.720
He admits all of it.

00:37.720 --> 00:41.920
You have an incredible amount of power at this moment in time.

00:41.920 --> 00:44.920
Why should we trust you?

00:44.920 --> 00:45.920
You shouldn't.

00:45.920 --> 00:46.920
You shouldn't.

00:46.920 --> 00:47.920
You shouldn't.

00:47.920 --> 00:48.920
You shouldn't.

00:48.920 --> 00:51.160
Let's be very clear.

00:51.160 --> 00:57.160
The man who signed the 22-word statement admitting that his AI technology has the power to kill

00:57.160 --> 01:02.760
all life on Earth and must be mitigated like nuclear arms and pandemic is also saying to

01:02.760 --> 01:07.560
you directly that he should not be trusted.

01:07.560 --> 01:11.720
When people show you who they are, believe them.

01:11.720 --> 01:12.720
Yes.

01:12.720 --> 01:15.480
In general, it's the process for the bigger question of safety.

01:15.480 --> 01:20.040
How do you provide that layer that protects the model from doing crazy, dangerous things?

01:21.040 --> 01:28.040
I think there will come a point where that's mostly what we think about the whole company.

01:28.040 --> 01:29.040
Aha.

01:29.040 --> 01:30.040
Hmm.

01:30.040 --> 01:35.560
Okay, so at some point, the safety crisis will be so urgent the entire company will

01:35.560 --> 01:38.640
need to deal with it.

01:38.640 --> 01:40.280
Hear him.

01:40.280 --> 01:41.840
Believe him.

01:41.840 --> 01:48.360
The thing he's making will be very unsafe and at an unknown point, the guy who says,

01:48.800 --> 01:57.800
Trust me wants us to trust him that he will at the exact right moment know to throw the

01:57.800 --> 02:01.800
whole company into safety work.

02:01.800 --> 02:05.000
But just not quite yet.

02:05.000 --> 02:11.480
I thought at some point between when Open AI started and when we created AGI, there was

02:11.480 --> 02:15.720
going to be something crazy and explosive that happened, but there may be more crazy

02:15.720 --> 02:25.160
and explosive things still to happen.

02:25.160 --> 02:32.240
Welcome to For Humanity and AI Safety Podcast, episode number 22, Nobody Elected, Sam Aldman.

02:32.240 --> 02:33.720
I'm Jon Sherman, your host.

02:33.720 --> 02:35.760
Thank you so much for joining me.

02:35.760 --> 02:38.800
In America, we elect our leaders.

02:38.800 --> 02:44.600
They audition their ideas and then our government of by and for the people votes to decide our

02:44.600 --> 02:48.840
collective direction and decisions.

02:48.840 --> 02:53.600
But the American making the most consequential decisions in human history right now as I

02:53.600 --> 02:59.640
speak hasn't been elected by anyone nor would he be.

02:59.640 --> 03:04.160
His ideas poll very poorly with the general public.

03:04.160 --> 03:11.400
He says go the public says stop poll show more than 70% are opposed to building superhuman

03:11.400 --> 03:13.200
intelligence.

03:13.200 --> 03:17.640
So this week, we're going to focus on the most important human in AI who is also the

03:17.640 --> 03:23.720
most important human who has ever lived sadly.

03:23.720 --> 03:26.920
Today's show is going to point fingers and lay blame.

03:26.920 --> 03:33.000
Sam Aldman is why we are at the most dangerous moment in human history that we are at right

03:33.000 --> 03:34.640
now.

03:34.640 --> 03:37.120
This is the AI Safety Podcast for the general public.

03:37.120 --> 03:38.640
No tech background required.

03:38.640 --> 03:45.320
This podcast is solely about the threat of human extinction from artificial intelligence.

03:45.320 --> 03:49.480
At the end of today's show, we're going to talk about how hard it is to convince people

03:49.480 --> 03:56.200
to believe that AI risk is so much of a risk, they are moved to take action.

03:56.200 --> 03:59.080
I have some new insight into that.

03:59.080 --> 04:06.160
But first, it is not hyperbole to say that open AI CEO Sam Aldman holds the life of every

04:06.160 --> 04:10.880
single human on earth in his hands.

04:10.880 --> 04:21.240
Your kids, your parents, your friends, his knife is at each of their throats.

04:21.240 --> 04:25.120
Sam Aldman was on the Lex Friedman podcast very recently.

04:25.120 --> 04:30.400
And so I felt compelled to break that down for you this week.

04:30.400 --> 04:35.640
Sam was also in a very hard hitting piece in business insider this week.

04:35.640 --> 04:39.960
There's a link in the description as there will be to the Lex podcast.

04:39.960 --> 04:46.920
The title of the business insider article was some VCs are over the Sam Altman hype.

04:46.920 --> 04:48.160
Sounds good to me.

04:48.160 --> 04:51.520
Next subheading the platform of Sam.

04:51.520 --> 04:54.320
This is what I want to read you from the article.

04:54.320 --> 04:58.780
Some who know Altman paint a picture of a benevolent visionary, a thoughtful steward

04:58.780 --> 05:00.740
of a promising technology.

05:00.740 --> 05:06.480
They see his plan to build a far reaching AI empire that touches everything from nuclear

05:06.480 --> 05:12.380
fusion to anti-aging technology as a leap forward for humanity.

05:12.380 --> 05:17.940
Others say he's more interested in self-promotion than human advancement.

05:17.940 --> 05:24.360
All agree he's a masterful storyteller who could sell sand in the Sahara.

05:24.360 --> 05:29.460
But as Altman lays out a sweeping vision for the transformation of society, not everyone

05:29.480 --> 05:33.120
in Silicon Valley is buying it quote.

05:33.120 --> 05:37.280
There are holes a mile deep in this guy's resume, but he's managed to figure out how

05:37.280 --> 05:40.840
to take his chess pieces and move them correctly.

05:40.840 --> 05:43.880
An anonymous startup founder said.

05:43.880 --> 05:48.240
And now one of the things went crazy.

05:48.240 --> 05:50.480
And he's an AI expert.

05:50.480 --> 05:57.980
Okay, so my point in reading that is this Sam Altman is a tech CEO from the sales side.

05:58.000 --> 06:00.680
He knows how to sell ice in Alaska.

06:00.680 --> 06:05.120
He ran a startup incubator and he had a lot of different bets.

06:05.120 --> 06:09.120
And open AI is the one that hit big.

06:09.120 --> 06:11.920
So this guy making all the decisions.

06:11.920 --> 06:13.480
He's not an AI expert.

06:13.480 --> 06:15.520
He does not write code.

06:15.520 --> 06:17.760
He's a salesman, nothing against salesman.

06:17.760 --> 06:19.640
There have to be salesmen in the world.

06:19.640 --> 06:22.600
But I don't know that that's who you want making all the decisions.

06:22.600 --> 06:29.460
It was way back in 2015 that Vanity Fair gushed over Sam Altman and Elon Musk launching a

06:29.460 --> 06:35.780
nonprofit company to save the world from a dystopian future.

06:35.780 --> 06:44.700
Fast forward to 2023 and Sam Altman, his nonprofit, then a soon to be blossoming profit center

06:44.700 --> 06:52.800
of the largest corporation on earth was the one who decided to unleash chat GPT on the

06:52.800 --> 06:55.280
world when he did.

06:55.280 --> 07:00.840
It is widely reported that Google had a similar product to chat GPT six months earlier, but

07:00.840 --> 07:08.040
they did not release it because they were concerned over safety.

07:08.040 --> 07:16.380
Unelected, unvetted and completely unaccountable, this one man has chosen to put all of our

07:16.380 --> 07:22.460
lives in grave danger so that he can make money and fulfill his dreams of radically

07:22.460 --> 07:29.540
altering human existence as we have never known it without even an effort to get any

07:29.540 --> 07:34.740
consent from you or anyone else in the general public.

07:34.740 --> 07:37.940
Here's the really crazy part of it.

07:37.940 --> 07:40.920
He tells us all of this openly.

07:40.920 --> 07:42.520
There is no deception.

07:42.520 --> 07:45.560
He admits all of it.

07:45.560 --> 07:47.780
Here's the sad part.

07:47.780 --> 07:50.380
No one takes him at his word.

07:50.380 --> 07:55.120
No one is listening to his promises of doom.

07:55.120 --> 07:57.500
It all just sounds so outrageous.

07:57.500 --> 08:03.200
People dismiss it instantly and move on to whatever bullshit they were thinking of first.

08:03.200 --> 08:04.360
Yeah.

08:04.360 --> 08:06.600
I'm talking about this guy.

08:06.620 --> 08:09.860
Who nine months ago told us this.

08:09.860 --> 08:15.140
I think even you would acknowledge you have an incredible amount of power at this moment

08:15.140 --> 08:16.140
in time.

08:16.140 --> 08:19.300
Why should we trust you?

08:19.300 --> 08:20.300
You shouldn't.

08:20.300 --> 08:21.300
You shouldn't.

08:21.300 --> 08:22.300
You shouldn't.

08:22.300 --> 08:23.300
You shouldn't.

08:23.300 --> 08:25.540
Let's be very clear.

08:25.540 --> 08:31.540
The man who signed the 22 word statement admitting that his AI technology has the power to kill

08:31.540 --> 08:37.080
all life on earth and must be mitigated like nuclear arms and pandemic is also saying to

08:37.080 --> 08:44.600
you directly that he should not be trusted.

08:44.600 --> 08:50.080
The man with his hand on the kill eight billion people button is telling you he cannot be

08:50.080 --> 08:52.920
trusted with this responsibility.

08:52.920 --> 08:57.400
When people show you who they are, believe them.

08:57.400 --> 08:58.400
Yes.

08:58.400 --> 08:59.400
Absolutely.

08:59.400 --> 09:11.780
This one says to you, I'm selfish or I'm mean or I am unkind or I'm crazy or I'm

09:11.780 --> 09:12.780
crazy.

09:12.780 --> 09:13.780
Believe them.

09:13.780 --> 09:16.580
They know themselves much better than you do.

09:16.580 --> 09:23.780
But no, more often than not, those of us who don't trust life say don't say a thing

09:23.780 --> 09:24.780
like that.

09:24.780 --> 09:27.180
You're not really crazy.

09:27.180 --> 09:29.800
You're not really unkind.

09:29.800 --> 09:30.800
You're not really mean.

09:30.800 --> 09:39.760
And as soon as you say that, the person that you know and shows you, I told you, I told

09:39.760 --> 09:42.360
you I was unkind.

09:42.360 --> 09:46.040
Thank you, Oprah and the late great Dr. Maya Angelou.

09:46.040 --> 09:49.520
Sam keeps telling us not to trust him.

09:49.520 --> 09:53.400
He says no one should have the power he has.

09:53.400 --> 09:55.920
Maybe we should believe him.

09:55.920 --> 09:58.540
Like no one person should be trusted here.

09:58.540 --> 10:01.540
I don't have super voting shares.

10:01.540 --> 10:03.340
Like I don't want them.

10:03.340 --> 10:04.340
The board can fire me.

10:04.340 --> 10:05.340
I think that's important.

10:05.340 --> 10:08.740
The board can fire me and I think that's important.

10:08.740 --> 10:13.500
He said back in June 2023 and I, yes.

10:13.500 --> 10:18.620
So now we fast forward to that famous week in November 17th, 2023, less than six months

10:18.620 --> 10:26.920
later when it seems indisputably a battle over AI safety and Sam Altman's honesty

10:26.920 --> 10:34.560
around AI safety, the board, including his friend and co-founder, Ilya Sutskiver, do

10:34.560 --> 10:39.160
exactly what he says the failsafe should be.

10:39.160 --> 10:41.560
They fire him.

10:41.560 --> 10:48.040
And within hours, after having attached his nonprofit intended to help humanity to Microsoft

10:48.060 --> 10:54.380
the world's largest corporation, Microsoft naturally decided it could not lose its golden

10:54.380 --> 10:56.380
goose.

10:56.380 --> 11:01.700
So Microsoft led to the firing of the board members who drew the hard line on safety.

11:01.700 --> 11:05.340
Safety lost, Sam won.

11:05.340 --> 11:11.620
This was all a huge fucking deal and yet nobody other than a handful of people intimately

11:11.620 --> 11:18.440
involved seems even to this day to have any idea what really happened, which is why Sam

11:18.440 --> 11:23.840
Altman's appearance on the Lex Friedman show is really worth picking apart.

11:23.840 --> 11:28.520
For the first time, Sam was interviewed in depth about what happened the weekend he got

11:28.520 --> 11:29.840
fired.

11:29.840 --> 11:37.320
What happened that weekend is a national security issue of grave seriousness.

11:37.320 --> 11:42.780
And again, the guy building the tech that can end all life on Earth, the same tech he

11:42.780 --> 11:48.700
says he cannot control the same tech that no one understands how it works or why it

11:48.700 --> 11:50.820
does what it does.

11:50.820 --> 11:58.440
That guy, when he reflects on that weekend, when his lack of regard for safety should

11:58.440 --> 12:06.500
have put him out on the streets, he is consumed by one thing, not what happened, not the safety

12:06.520 --> 12:13.000
concerns, not the fact he lied about the board being a check on him.

12:13.000 --> 12:22.840
Sam Altman's take on the weekend that he got fired is all about his feelings.

12:22.840 --> 12:28.920
Take me through the open AI board saga that started on Thursday, November 16th, maybe Friday,

12:28.920 --> 12:30.880
November 17th for you.

12:30.880 --> 12:41.620
That was definitely the most painful professional experience of my life and chaotic and shameful

12:41.620 --> 12:47.660
and upsetting and a bunch of other negative things.

12:47.660 --> 12:54.380
There were great things about it too, and I wish I wish it had not been in such an adrenaline

12:54.380 --> 12:57.420
rush that I wasn't able to stop and appreciate them at the time.

12:57.420 --> 13:07.840
But I came across this tweet of mine from that time period, which was kind of going

13:07.840 --> 13:13.920
to your own eulogy, watching people say all these great things about you and just unbelievable

13:13.920 --> 13:18.960
support from people I love and care about.

13:18.960 --> 13:21.720
That was really nice.

13:21.720 --> 13:27.420
The whole weekend I kind of felt, with one big exception, I felt like a great deal of

13:27.420 --> 13:38.540
love and very little hate, even though it felt like I have no idea what's happening

13:38.540 --> 13:42.620
and what's going to happen here and this feels really bad and there were definitely times

13:42.620 --> 13:49.180
I thought it was going to be one of the worst things to ever happen for AI safety.

13:49.180 --> 13:56.680
One of the worst things to happen for AI safety says the guy who started the race to AGI,

13:56.680 --> 14:01.240
but don't worry, he says he saw this coming and that's just the start.

14:01.240 --> 14:06.320
Well, I also think I'm happy that it happened relatively early.

14:06.320 --> 14:12.800
I thought at some point between when OpenAI started and when we created AGI, there was

14:12.800 --> 14:17.040
going to be something crazy and explosive that happened, but there may be more crazy

14:17.040 --> 14:20.260
and explosive things still to happen.

14:20.260 --> 14:21.260
Honestly I just can't.

14:21.260 --> 14:23.820
I cannot take this man.

14:23.820 --> 14:30.740
With a twinkle in his eye, a smirk on his face and the power to kill everyone I love,

14:30.740 --> 14:38.060
it honestly a lot of the time feels like he's taunting me, like he's taunting us.

14:38.060 --> 14:44.300
There is nothing about risking all life on earth without anyone's consent that is funny

14:44.360 --> 14:48.000
or cute.

14:48.000 --> 14:52.320
But enough for now about you and me and our families.

14:52.320 --> 14:58.120
Let's focus on what's really important and still the biggest known about the AI safety

14:58.120 --> 15:00.440
crisis to date.

15:00.440 --> 15:02.360
Sam Altman's feelings.

15:02.360 --> 15:12.240
It feels like something that was in the past that was really unpleasant and really difficult

15:12.260 --> 15:21.100
and painful, but we're back to work and things are so busy and so intense that I don't spend

15:21.100 --> 15:22.780
a lot of time thinking about it.

15:22.780 --> 15:29.380
There was a time after, there was like this fugue state for kind of like the month after,

15:29.380 --> 15:35.860
maybe 45 days after that was, I was just sort of like drifting through the days.

15:35.860 --> 15:38.220
I was so out of it.

15:38.220 --> 15:40.140
I was feeling so down.

15:40.140 --> 15:42.140
Just at a personal psychological level.

15:42.760 --> 15:50.240
Really painful and hard to like have to keep running open AI in the middle of that.

15:50.240 --> 15:55.960
I just wanted to like crawl into a cave and kind of recover for a while, but now it's

15:55.960 --> 16:00.760
like we're just back to working on the mission.

16:00.760 --> 16:10.960
What's still useful to go back there and reflect on board structures, on power dynamics,

16:10.980 --> 16:17.580
on how companies are run, the tension between research and product development and money

16:17.580 --> 16:24.860
and all this kind of stuff, so that you who have a very high potential of building AGI

16:24.860 --> 16:30.300
would do so in a slightly more organized, less dramatic way in the future.

16:30.300 --> 16:35.980
There's value there to go, both the personal psychological aspects of you as a leader and

16:35.980 --> 16:40.580
also just the board structure and all this kind of messy stuff.

16:40.600 --> 16:50.200
We've only learned a lot about structure and incentives and what we need out of a board.

16:50.200 --> 16:56.520
Okay, so I'm sorry, but Lex did a terrible job in this interview, I'm sorry.

16:56.520 --> 17:00.960
They talked for two hours and not once did we find out anything really new about what

17:00.960 --> 17:04.600
actually happened the weekend he got fired.

17:04.600 --> 17:09.720
He called it the worst thing possible for AI safety and Lex does not even follow up and

17:09.740 --> 17:16.940
ask him what he means, what do you mean it's the worst thing possible for AI safety?

17:16.940 --> 17:22.740
But he does ask about open AI's chief scientist, Ilya Sutskiver, who before the firing did

17:22.740 --> 17:28.380
podcasts and made a lot of public appearances since the firing weekend, Ilya has not been

17:28.380 --> 17:32.380
seen anywhere in public that I'm aware of.

17:32.380 --> 17:38.700
He Ilya is also noted for his deep and genuine concern over AI safety, so does Lex try to

17:38.720 --> 17:41.040
get the reel on this out of Sam?

17:41.040 --> 17:44.360
Oh no, it's just more footsie.

17:44.360 --> 17:45.360
So cute.

17:45.360 --> 17:50.880
Let me ask you about Ilya, is he being held hostage in a secret nuclear facility?

17:50.880 --> 17:51.880
No.

17:51.880 --> 17:53.600
What about a regular secret facility?

17:53.600 --> 17:54.600
No.

17:54.600 --> 17:56.000
What about a nuclear non-secret facility?

17:56.000 --> 17:57.720
Neither, not that either.

17:57.720 --> 18:03.520
I mean, it's becoming a meme at some point, you've known Ilya for a long time, he's obviously

18:03.520 --> 18:08.360
in part of this drama with the board and all that kind of stuff.

18:08.380 --> 18:11.220
What's your relationship with him now?

18:11.220 --> 18:17.140
I love Ilya, I have tremendous respect for Ilya, I don't know anything I can say about

18:17.140 --> 18:20.900
his plans right now, that's a question for him.

18:20.900 --> 18:26.340
But I really hope we work together for, you know, certainly the rest of my career.

18:26.340 --> 18:29.900
He's a little bit younger than me, maybe he works a little bit longer.

18:29.900 --> 18:36.420
You know, there's a meme that he saw something, like he maybe saw AGI and that gave him a

18:36.480 --> 18:39.360
lot of worry internally.

18:39.360 --> 18:41.160
What did Ilya see?

18:41.160 --> 18:49.960
Ilya has not seen AGI, none of us have seen AGI, we have not built AGI.

18:49.960 --> 18:59.400
I do think one of the many things that I really love about Ilya is he takes AGI and the safety

18:59.400 --> 19:04.720
concerns broadly speaking, you know, including things like the impact this is going to have

19:04.720 --> 19:07.940
on society very seriously.

19:07.940 --> 19:14.780
And we, as we continue to make significant progress, Ilya is one of the people that I've

19:14.780 --> 19:19.660
spent the most time over the last couple of years talking about what this is going to

19:19.660 --> 19:23.940
mean, what we need to do to ensure we get it right, to ensure that we succeed at the

19:23.940 --> 19:25.940
mission.

19:25.940 --> 19:40.960
So Ilya did not see AGI, but Ilya is a credit to humanity in terms of how much he thinks

19:40.960 --> 19:44.960
and worries about making sure we get this right.

19:44.960 --> 19:52.960
So the credit to humanity most concerned about safety has been silenced and hidden.

19:52.980 --> 19:56.980
And with that problem taken care of, the race is back on.

19:56.980 --> 20:02.380
Next, Lex asked Sam about Elon Musk suing him.

20:02.380 --> 20:07.020
And again, suicide Sam goes straight to his feelings.

20:07.020 --> 20:12.100
I know he knows what it's like to have haters attack him and it makes me extra sad he's

20:12.100 --> 20:13.100
doing it, Toss.

20:13.100 --> 20:16.900
Yeah, he's one of the greatest builders of all time, potentially the greatest builder

20:16.900 --> 20:17.900
of all time.

20:17.900 --> 20:18.900
It makes me sad.

20:18.900 --> 20:21.620
And I think it makes a lot of people sad, like there's a lot of people who've really

20:21.640 --> 20:24.560
looked up to him for a long time and said this.

20:24.560 --> 20:28.640
I said in some interview or something that I missed the old Elon and the number of messages

20:28.640 --> 20:31.960
I got being like that exactly encapsulates how I feel.

20:31.960 --> 20:34.360
I think he should just win.

20:34.360 --> 20:42.360
He should just make X grok beat GPT and then GPT beats grok and it's just a competition.

20:42.360 --> 20:47.160
What do you hope this goes with Elon?

20:47.160 --> 20:48.520
This tension, this dance.

20:48.520 --> 20:49.520
What do you hope this?

20:49.520 --> 20:55.460
Like if we go one, two, three years from now, you're a relationship with him on a personal

20:55.460 --> 21:03.460
level too, like friendship, friendly competition, just all this kind of stuff.

21:03.460 --> 21:12.740
Yeah, I really respect Elon.

21:12.740 --> 21:16.100
And I hope that years in the future we have an amicable relationship.

21:16.960 --> 21:22.880
Yeah, I hope you guys have an amicable relationship like this month.

21:22.880 --> 21:28.960
And just compete and win and explore these ideas together.

21:28.960 --> 21:36.920
I do suppose there's competition for talent or whatever, but it should be friendly competition.

21:36.920 --> 21:37.920
Just build.

21:37.920 --> 21:40.240
Build cool shit.

21:40.240 --> 21:44.280
And Elon is pretty good at building cool shit, but so are you.

21:44.280 --> 21:50.420
Building cool shit with like a rowy tone of voice building cool shit.

21:50.420 --> 21:54.740
Building cool shit is why we are all on course to die in the next 10 fucking years.

21:54.740 --> 21:57.340
It is not cool.

21:57.340 --> 22:00.940
Okay, this next one is wild.

22:00.940 --> 22:06.460
Lex asks Sam about the basis for the New York Times versus open AI lawsuit.

22:06.460 --> 22:12.220
Remember Sam's multi-billion dollar black box machine would not exist if it had not

22:12.240 --> 22:15.080
been trained on internet data.

22:15.080 --> 22:21.560
Sam already having harvested the data and made his billions has the gall to drop this

22:21.560 --> 22:22.560
answer.

22:22.560 --> 22:26.400
There's a lot of tough questions here.

22:26.400 --> 22:28.200
You're dealing in a very tough space.

22:28.200 --> 22:33.160
Do you think training AI should be or is fair use under copyright law?

22:33.160 --> 22:37.600
I think the question behind that question is do people who create valuable data deserve

22:37.620 --> 22:42.140
to have some way that they get compensated for use of it and that I think the answer

22:42.140 --> 22:43.140
is yes.

22:43.140 --> 22:46.420
I don't know yet what the answer is.

22:46.420 --> 22:48.100
People have proposed a lot of different things.

22:48.100 --> 22:55.700
We've tried some different models, but if I'm like an artist, for example, A, I would

22:55.700 --> 23:00.980
like to be able to opt out of people generating art in my style and B, if they do generate

23:00.980 --> 23:05.860
art in my style, I'd like to have some economic model associated with that.

23:05.880 --> 23:08.760
What the actual fuck are you even talking about?

23:08.760 --> 23:11.080
You already stole the training data.

23:11.080 --> 23:15.240
All the writers and artists have had their work stolen.

23:15.240 --> 23:19.760
This dude built a billion dollar business on stolen writing and images and now after

23:19.760 --> 23:24.880
his business is flourishing after and the toothpaste seems impossible to put back in

23:24.880 --> 23:31.360
the tube, he's like, um, yeah, somebody should figure out a way to make this fair.

23:32.180 --> 23:36.720
There's a real theme with Sam Altman.

23:36.720 --> 23:41.380
Every problem he causes is someone else's to solve.

23:41.380 --> 23:45.580
So in general is the process for the bigger question of safety.

23:45.580 --> 23:50.260
How do you provide that layer that protects the model from doing crazy dangerous things?

23:50.260 --> 23:57.300
I think there will come a point where that's mostly what we think about the whole company

23:57.300 --> 24:00.660
and it won't be like, it's not like you have one safety team.

24:00.660 --> 24:03.520
It's like when we shipped GPT-4, that took the whole company thing with all these different

24:03.520 --> 24:08.200
aspects and how they fit together and I think it's going to take that.

24:08.200 --> 24:12.320
More and more of the company thinks about those issues all the time.

24:12.320 --> 24:18.120
That's literally what humans will be thinking about the more powerful AI becomes.

24:18.120 --> 24:23.080
So most of the employees that open AI will be thinking safety or at least to some degree.

24:23.080 --> 24:25.600
Broadly defined, yes.

24:25.600 --> 24:28.600
I wonder what are the full broad definition of that?

24:28.620 --> 24:30.820
What are the different harms that could be caused?

24:30.820 --> 24:35.740
Is this like on a technical level or is this almost like security threats?

24:35.740 --> 24:36.740
It'll be all those things.

24:36.740 --> 24:41.580
I was going to say it'll be people, state actors trying to steal the model.

24:41.580 --> 24:45.020
It'll be all of the technical alignment work.

24:45.020 --> 24:51.860
It'll be societal impacts, economic impacts.

24:51.860 --> 24:56.100
It's not just like we have one team thinking about how to align the model and it's really

24:56.100 --> 25:00.480
going to be like getting to be getting to the good outcome is going to take the whole

25:00.480 --> 25:01.480
the whole effort.

25:01.480 --> 25:02.480
Huh.

25:02.480 --> 25:03.480
Hmm.

25:03.480 --> 25:04.480
Okay.

25:04.480 --> 25:09.800
So at some point, the safety crisis will be so urgent, the entire company will need

25:09.800 --> 25:12.640
to deal with it.

25:12.640 --> 25:15.840
Hear him, believe him.

25:15.840 --> 25:22.400
The thing he's making will be very unsafe and at an unknown point, the guy who says,

25:22.400 --> 25:31.820
trust me, wants us to trust him that he will at the exact right moment know to throw the

25:31.820 --> 25:38.020
whole company into safety work, but just not quite yet.

25:38.020 --> 25:39.340
Cool.

25:39.340 --> 25:41.620
Seems like a plan.

25:41.620 --> 25:42.620
Okay.

25:42.620 --> 25:44.460
This next one is wild.

25:44.460 --> 25:51.380
So, um, I spoke to a group of coders about AI risk last week, uh, a week or so ago, um,

25:51.380 --> 25:56.280
outside of Philadelphia, and the number one question they had was how will AGI escape?

25:56.280 --> 26:00.480
How does it go from the digital world to the physical world?

26:00.480 --> 26:02.080
Hiring humans is an easy way.

26:02.080 --> 26:05.640
There's lots of others, but here's an even easier way.

26:05.640 --> 26:07.120
Go tell him, Sam.

26:07.120 --> 26:14.040
Will we, uh, see human and robots or human and robot brains from open AI at some point?

26:14.040 --> 26:15.560
At some point.

26:15.560 --> 26:18.120
How important is embodied AI to you?

26:18.120 --> 26:23.620
I think it's like sort of depressing if we have AGI and the only way to like get things

26:23.620 --> 26:27.460
done in the physical world is like to make a human go do it.

26:27.460 --> 26:35.260
So I, I really hope that as part of this transition, as this phase change, we also get, uh, we

26:35.260 --> 26:38.420
also get human and robots or some sort of physical world robots.

26:38.420 --> 26:39.660
Okay.

26:39.660 --> 26:41.900
Let's add up the tab.

26:41.900 --> 26:44.780
Sam can't control his AI.

26:44.780 --> 26:48.640
Sam doesn't understand how his AI works.

26:48.640 --> 26:52.840
Sam admits his AI can end all life on earth.

26:52.840 --> 26:54.240
Sam is clear.

26:54.240 --> 26:56.640
You should not trust him.

26:56.640 --> 27:01.320
And one more.

27:01.320 --> 27:09.800
Sam would be depressed if his AGI is not put in to robots.

27:09.800 --> 27:15.180
For Bill's AGI first gets a lot of power.

27:15.180 --> 27:20.220
Do you trust yourself with that much power?

27:20.220 --> 27:28.100
When people show you who they are, believe them.

27:28.100 --> 27:30.060
Yes.

27:30.060 --> 27:36.980
Look, I, I was gonna, I'll just be very honest with this answer.

27:36.980 --> 27:42.560
I was gonna say, and I still believe this, that it is important that I, nor any other

27:42.560 --> 27:49.800
one person have total control over open AI or over AGI.

27:49.800 --> 27:54.840
And I think you want a robust governance system.

27:54.840 --> 28:04.040
Um, I can point out a whole bunch of things about all of our board drama from last year

28:04.040 --> 28:07.980
about how I didn't fight it initially and was just like, yeah, that's, you know, the

28:07.980 --> 28:13.660
will of the board, even though I think it's a really bad decision.

28:13.660 --> 28:16.940
And then later I clearly did fight it and I can explain the nuance and why I think it

28:16.940 --> 28:19.380
was okay for me to fight it later.

28:19.380 --> 28:31.660
But as many people have observed, um, although the board had the legal ability to fire me

28:31.660 --> 28:36.440
in practice, it didn't quite work.

28:36.440 --> 28:41.480
And that is its own kind of governance failure.

28:41.480 --> 28:49.000
Now, again, I, I feel like I can completely defend the specifics here.

28:49.000 --> 28:58.480
And I think most people would agree with that, but it, it does make it harder for me to like

28:58.480 --> 29:01.240
look you in the eye and say, Hey, the board can just fire me.

29:01.660 --> 29:03.940
They know themselves much better than you do.

29:07.180 --> 29:13.540
Okay, Sam, one last clip, please, most important person on earth, give us some reassurance.

29:15.380 --> 29:18.540
Are you afraid of losing control of the AGI itself?

29:18.580 --> 29:23.740
That's a lot of people who worry about existential risk, not because of state actors, not because

29:23.740 --> 29:26.300
of security concerns, but because of the AI itself.

29:26.300 --> 29:27.460
That is not my top worry.

29:27.820 --> 29:30.620
As I currently see things, there have been times I've worried about that more than maybe

29:30.640 --> 29:33.160
times again, in the future, that's my top worry.

29:33.680 --> 29:34.760
It's not my top worry right now.

29:34.800 --> 29:36.680
What's your intuition about it not being your worry?

29:36.680 --> 29:38.760
Cause there's a lot of other stuff to worry about, essentially.

29:41.480 --> 29:42.760
You think you could be surprised?

29:42.840 --> 29:44.680
We for sure could be surprised.

29:45.200 --> 29:47.600
Like saying it's not my top worry.

29:47.600 --> 29:50.880
It doesn't mean I don't think we need, like, I think we need to work on it super hard.

29:52.320 --> 29:54.840
I think we need to work on it super hard.

29:56.320 --> 29:57.840
Um, are you fucking kidding?

29:58.020 --> 30:02.860
That is not at all the same as we are working on it super hard.

30:03.620 --> 30:05.100
Two very different things.

30:06.060 --> 30:09.420
Sam Altman wants someone else to make his tech safe.

30:09.780 --> 30:13.780
Sam Altman wants someone else to regulate his tech.

30:14.260 --> 30:19.460
Sam Altman even wants someone else to figure out how to compensate the people whose

30:19.460 --> 30:23.020
training data was stolen to build his death star.

30:24.300 --> 30:25.820
No one elected Sam Altman.

30:26.640 --> 30:32.600
No one has vetted Sam Altman, but he is in charge of whether or not your family

30:32.600 --> 30:34.040
lives or dies.

30:37.120 --> 30:43.600
No government oversight motivated by profit, ego, a boyish need to build cool

30:43.600 --> 30:51.880
stuff and an unrequested desire to change how humans live fundamentally with

30:51.880 --> 30:54.640
no plan as fast as possible.

30:56.760 --> 31:02.560
That is why I believe we need to stop Sam Altman and all those like him.

31:03.160 --> 31:04.360
Okay, but here's the thing.

31:05.720 --> 31:10.800
The case against Sam and his peers is not hitting home with the general public yet.

31:11.120 --> 31:12.680
Um, I want to share a story with you.

31:12.680 --> 31:17.720
One of my closest and oldest friends on the planet and I had an exchange this week

31:17.720 --> 31:19.800
that I think you can learn from.

31:20.080 --> 31:25.640
So after a catch up zoom that unexpectedly drifted far into AI risk, he

31:25.640 --> 31:29.340
told me in an email follow up that he believed everything I said.

31:30.380 --> 31:34.380
So I, after some hesitation followed up, I just wanted to know really to help

31:34.380 --> 31:35.620
better understand people.

31:36.100 --> 31:43.020
Um, why, if he believed that his three young kids lives are threatened by AI, why

31:43.020 --> 31:47.860
he has not taken action yet, why he doesn't feel compelled to do things like I

31:47.860 --> 31:49.420
feel compelled to do things.

31:49.780 --> 31:50.960
And here's what he wrote back.

31:50.960 --> 31:54.220
Three bullet points that I think are so, so common.

31:54.220 --> 32:00.200
I bet 90% of the inactive public would say the same three points.

32:00.600 --> 32:05.080
He wrote, honestly, I don't have a good excuse.

32:05.600 --> 32:10.280
Number one, I'm very busy with work and kids and life.

32:10.960 --> 32:15.920
Number two, I'm only one person with zero influence on companies, individuals

32:15.920 --> 32:17.960
and lawmakers who can impact what's happening.

32:18.420 --> 32:22.000
And number three, I'm hopeful that doom is not imminent.

32:22.620 --> 32:26.380
And that in the end, good outcomes will prevail.

32:26.940 --> 32:31.380
That is literally what those of us who desperately want to spread AI risk

32:31.380 --> 32:35.300
awareness and see action come from it are up against.

32:36.820 --> 32:40.580
So on one hand, I was depressed at my first reading.

32:41.020 --> 32:43.820
Those three points are so weak.

32:44.700 --> 32:45.620
We're all busy.

32:46.180 --> 32:50.660
We can all have influence and hope is not nearly enough.

32:51.600 --> 32:57.120
But then I thought, wait a minute, those three points are so weak.

32:59.200 --> 33:01.400
What we are up against is these three things.

33:01.440 --> 33:04.600
Again, very busy with work and kids and life.

33:05.320 --> 33:06.840
I don't have influence.

33:07.560 --> 33:09.880
I hope that we get a good outcome.

33:11.280 --> 33:15.480
The weaknesses in each of those is not our enemy.

33:15.840 --> 33:16.960
It's our ally.

33:17.400 --> 33:19.440
These are three winnable arguments.

33:19.980 --> 33:25.660
So as you make for the, as you make the case for AI risk, keep those three

33:25.660 --> 33:30.540
things in mind, that is what you and we are fighting against.

33:31.340 --> 33:32.020
It's winnable.

33:33.140 --> 33:35.860
Okay, friends, it's 2024.

33:35.860 --> 33:37.740
AGI is coming at some point.

33:37.740 --> 33:41.180
We don't know when we don't know how long we have to live.

33:41.300 --> 33:47.700
So we celebrate every moment of every day, like it could be our last call

33:47.700 --> 33:50.920
at the celebration of life for this week's celebration of life.

33:50.920 --> 33:53.640
I want to share something that really moved me as a dad.

33:54.240 --> 33:59.600
Um, there's not much that makes me feel thrilled to be alive more than

33:59.600 --> 34:02.760
incredible vocal tone on a singing voice.

34:03.200 --> 34:07.800
Um, when my daughter first shared with me, Billie Eilish, I was immediately

34:07.880 --> 34:10.000
blown away by her vocal tone.

34:10.600 --> 34:15.520
Um, and then as a dad of boy girl twins, it was so cool to learn

34:15.540 --> 34:19.860
that her brother Phineas is her producer and musical creative partner.

34:20.020 --> 34:24.860
Um, so I came across a performance that just really blew me away.

34:24.860 --> 34:29.140
The two of them doing their song, um, what was I made for at the Grammy

34:29.140 --> 34:33.340
awards, a live performance and as a dad of a boy and a girl and a fan of vocal

34:33.340 --> 34:36.100
tone and songwriting, it just floored me.

34:36.180 --> 34:38.500
Um, they are simply incredible.

34:38.700 --> 34:44.700
And this song has some haunting reflections on our AI risk debate.

34:44.700 --> 34:46.520
Please enjoy this is so good.

36:45.000 --> 36:49.920
Don't tell my boy friend, it's not what he's made for.

36:54.160 --> 36:56.640
But what was I made for?

37:01.520 --> 37:11.480
Cause I, cause I, I don't know how to feel.

37:11.480 --> 37:35.260
But I want to try, I don't know how to feel, but someday I might, someday I might.

37:41.480 --> 38:08.260
I think I forgot how to be happy, something I'm not, but something I can't be, something I wait for, and something I'm made for.

38:11.780 --> 38:16.980
Something I'm made for.

38:25.980 --> 38:27.340
Just beautiful.

38:28.060 --> 38:29.980
Okay, friends, that is all for this week.

38:30.220 --> 38:32.780
Next week's show is going to be a little while.

38:32.820 --> 38:35.620
So get ready for that for humanity.

38:35.660 --> 38:36.460
I'm John Sherman.

38:36.460 --> 38:38.300
I will see you right back here next week.

38:38.500 --> 38:40.340
We have so much work to do.

