WEBVTT

00:00.000 --> 00:02.000
Okay, shh, no more.

00:06.000 --> 00:11.000
Hello and welcome to the first session of 2024.

00:11.000 --> 00:14.000
This is sightly out of sequence.

00:14.000 --> 00:19.000
It's part of our doctoral consortium series from last semester.

00:19.000 --> 00:22.000
Our final session had to be cancelled the last minute,

00:22.000 --> 00:26.000
but we're delighted to have a follow-up session

00:26.000 --> 00:30.000
to complete the series on the theme of AI apocalypse,

00:30.000 --> 00:33.000
the series that was called the dark side of AI.

00:33.000 --> 00:36.000
And all of the previous sessions are uploaded

00:36.000 --> 00:43.000
onto our digitalfutures.international YouTube channel.

00:43.000 --> 00:47.000
And I would say that from that, the first five in the series,

00:47.000 --> 00:50.000
that we became a bit more, I guess, lenient in some way.

00:50.000 --> 00:54.000
I started off by saying, well, actually we do need to worry about AI.

00:54.000 --> 00:56.000
And over this course of the series,

00:56.000 --> 00:58.000
which is actually fascinating because we had a series

00:58.000 --> 01:01.000
on questions such as copyright and so on and so on.

01:01.000 --> 01:03.000
I think people softened up a bit and said,

01:03.000 --> 01:05.000
no, we don't have to worry about AI.

01:05.000 --> 01:10.000
Today I'm really delighted to have Eric Kessel here with us.

01:10.000 --> 01:14.000
Eric is a designer, educator, writer,

01:14.000 --> 01:19.000
and host disaster expert, professor disaster, I guess.

01:19.000 --> 01:28.000
And Eric takes a rather more kind of concerned attitude towards AI.

01:28.000 --> 01:32.000
He was formerly the director of the Sustainable Environment Design

01:32.000 --> 01:39.000
major at the College of Environmental Design at UC Berkeley.

01:39.000 --> 01:41.000
He's currently a special program instructor

01:41.000 --> 01:43.000
at Harvard Extension School.

01:43.000 --> 01:45.000
And Eric really has made a name for himself

01:45.000 --> 01:49.000
because using the kind of lens, shall we say,

01:49.000 --> 01:52.000
of disaster theory, disaster studies to look at things.

01:52.000 --> 01:58.000
He recently published what I think was an extraordinary article

01:58.000 --> 02:02.000
in Design Intelligence, looking at the question

02:02.000 --> 02:07.000
of what the impact of AI would be on the profession.

02:07.000 --> 02:10.000
Today then, Eric is going to make a presentation.

02:10.000 --> 02:14.000
We are going to have, I will have a short discussion with him.

02:14.000 --> 02:17.000
And then we'll open up to questions from,

02:17.000 --> 02:22.000
not only on the Zoom audience itself, but also from YouTube.

02:22.000 --> 02:26.000
So feel free to put in some of these questions.

02:26.000 --> 02:29.000
Probably not as long a session as sometimes some of our sessions,

02:29.000 --> 02:32.000
but this is, I think, going to pick up as we go

02:32.000 --> 02:35.000
because I think this is an absolutely fascinating topic.

02:35.000 --> 02:39.000
So Eric, welcome.

02:39.000 --> 02:41.000
It's great to see you.

02:41.000 --> 02:45.000
And it's a great way to kick off the new year

02:45.000 --> 02:47.000
with a bang, shall we say.

02:47.000 --> 02:51.000
AI Apocalypse.

02:51.000 --> 02:53.000
Yeah, well, thank you for having me.

02:53.000 --> 02:56.000
I appreciate the chance to talk about it.

02:56.000 --> 02:59.000
I've actually entitled my lecture,

02:59.000 --> 03:01.000
AI Apocalypse, question mark.

03:01.000 --> 03:03.000
I think it still remains to be seen,

03:03.000 --> 03:06.000
and we have some agency in what happens

03:06.000 --> 03:10.000
in the future of design, architecture, and AI.

03:10.000 --> 03:13.000
But first, let's break this down.

03:13.000 --> 03:16.000
We're going to go through an introduction,

03:16.000 --> 03:18.000
talking about crisis and disaster,

03:18.000 --> 03:23.000
how to cultivate a disaster lens, what that looks like,

03:23.000 --> 03:27.000
then deconstruct some myths on AI and architecture,

03:27.000 --> 03:30.000
and then finally wrap it up with the brighter side of disaster

03:30.000 --> 03:34.000
because there is one, and fundamentally an optimist.

03:34.000 --> 03:37.000
So I hope everybody sticks around for that.

03:37.000 --> 03:39.000
So a bit about me.

03:39.000 --> 03:42.000
I've done a lot of things in my career.

03:42.000 --> 03:45.000
I've had a pretty eclectic experience,

03:45.000 --> 03:49.000
been an activist, a humanitarian builder,

03:49.000 --> 03:52.000
periodically an architect, a construction manager,

03:52.000 --> 03:54.000
a pre-fed or lewd for graduate school,

03:54.000 --> 03:57.000
a disaster responder, a kind of design,

03:57.000 --> 03:59.000
Cassandra, you might call it,

03:59.000 --> 04:03.000
a writer, radio host, professor, and academic director.

04:04.000 --> 04:07.000
And that brings us into 23-24,

04:07.000 --> 04:09.000
which was an interesting year for me

04:09.000 --> 04:13.000
because a lot of these things effectively came together

04:13.000 --> 04:18.000
to launch this recent mission around AI and architecture

04:18.000 --> 04:21.000
and the potential consequences for it.

04:21.000 --> 04:24.000
My work has been all over the place,

04:24.000 --> 04:27.000
but I think I'm probably principally known for two things.

04:27.000 --> 04:32.000
One, writing a book called Down Detour Road in 2010

04:32.000 --> 04:34.000
at the height of the Great Procession,

04:34.000 --> 04:37.000
and that was inspired by a different kind of crisis.

04:37.000 --> 04:40.000
I read an article in The Nation, I believe it was,

04:40.000 --> 04:42.000
or maybe it was The Guardian,

04:42.000 --> 04:44.000
that talked about the relative unemployment rates

04:44.000 --> 04:46.000
for different professions,

04:46.000 --> 04:48.000
and I was shocked, slash not shocked,

04:48.000 --> 04:52.000
to find out that architects were at the bottom of the list

04:52.000 --> 04:55.000
out of, you know, 700-something professions,

04:55.000 --> 04:58.000
or 380 professions, I'm sorry,

04:58.000 --> 05:01.000
but they were facing seven-fold, eight-fold increase

05:01.000 --> 05:03.000
in unemployment claims, and, you know,

05:03.000 --> 05:06.000
this didn't make a ton of sense to me.

05:06.000 --> 05:09.000
And my read on it was that the profession was in crisis.

05:09.000 --> 05:12.000
I mean, it was a recession, so everybody was in crisis,

05:12.000 --> 05:14.000
but architecture was in its own particular crisis,

05:14.000 --> 05:17.000
and to me, it was a crisis about value.

05:17.000 --> 05:20.000
People had ceased to really value architecture

05:20.000 --> 05:22.000
was what explained that for me.

05:22.000 --> 05:25.000
I wrote a book about it called Down Detour Road,

05:25.000 --> 05:28.000
which the premise of was that, you know,

05:28.000 --> 05:31.000
people had ceased to see the value in architecture

05:31.000 --> 05:33.000
because I think architecture had ceased

05:33.000 --> 05:35.000
to see the value in people.

05:35.000 --> 05:37.000
During the deconstructivist period,

05:37.000 --> 05:40.000
we got into a lot of formalism and distance ourselves

05:40.000 --> 05:43.000
from the problems that people deal with in their everyday life,

05:43.000 --> 05:47.000
and, you know, that led to a lot of openings

05:47.000 --> 05:50.000
and hopefully an optimistic message about architecture.

05:50.000 --> 05:54.000
Secondly, I'm probably best known for my post-disaster work,

05:54.000 --> 05:57.000
which began while I was still a student.

05:57.000 --> 06:00.000
This is a picture taken of me right before I went down

06:00.000 --> 06:03.000
from my first assignment in Biloxi, Mississippi,

06:03.000 --> 06:06.000
where I was working with the Biloxi Gulf Coast

06:06.000 --> 06:09.000
Community Design Studio under David Perks.

06:09.000 --> 06:11.000
And it was an education, I think,

06:11.000 --> 06:15.000
that put me in touch with the real power of architecture.

06:15.000 --> 06:18.000
From there, I went to Haiti with Architecture of Humanity,

06:18.000 --> 06:22.000
and that was a furtherance of my design education

06:22.000 --> 06:27.000
and understanding about what the real power of architecture

06:27.000 --> 06:30.000
can be when it's pointed in the right direction.

06:30.000 --> 06:33.000
And it was blessed to be surrounded by an incredible team

06:33.000 --> 06:35.000
from all over the world.

06:35.000 --> 06:38.000
It was about a third Haitian, about a third Haitian diaspora,

06:38.000 --> 06:40.000
and about a third international,

06:40.000 --> 06:44.000
which in itself is composed of architects from 10 countries.

06:44.000 --> 06:47.000
And we put together a pretty incredible program

06:47.000 --> 06:49.000
of community-based design,

06:49.000 --> 06:52.000
working with survivors of the earthquake in 2010

06:52.000 --> 06:57.000
to rebuild schools, clinics, whole communities at times

06:57.000 --> 06:59.000
to offer training.

06:59.000 --> 07:02.000
This has been the bulk of my practice, actually,

07:02.000 --> 07:05.000
is helping people through disaster

07:05.000 --> 07:09.000
and helping them imagine better futures

07:09.000 --> 07:11.000
because there's always the potential

07:11.000 --> 07:15.000
for a better future after disaster.

07:15.000 --> 07:18.000
So from there, I went to Japan

07:18.000 --> 07:22.000
and assisted with that program under a similar remit.

07:22.000 --> 07:27.000
Subsequently, New York, the Philippines, kind of all lower,

07:27.000 --> 07:31.000
and that work was enough to earn me a kind of dubious moniker

07:31.000 --> 07:34.000
of being architecture's first responder,

07:34.000 --> 07:37.000
or so-called by the daily beast.

07:37.000 --> 07:41.000
I retired from field work in 2015

07:41.000 --> 07:43.000
after the Nepal earthquake

07:43.000 --> 07:46.000
and turned my attention to teaching first at Washu

07:46.000 --> 07:48.000
and then subsequently at Berkeley

07:48.000 --> 07:50.000
and most recently at Harvard.

07:50.000 --> 07:53.000
And in those programs in varying ways,

07:53.000 --> 07:55.000
I teach about disaster and resilience

07:55.000 --> 07:58.000
to try and bring those lessons

07:58.000 --> 08:04.000
and those conversations forward into the practice of architecture

08:04.000 --> 08:08.000
and the teaching of the practice of architecture and design,

08:08.000 --> 08:10.000
among other things.

08:10.000 --> 08:13.000
And through that work, I developed,

08:13.000 --> 08:16.000
I guess you could call it a theory of disaster

08:16.000 --> 08:19.000
and how to look for it, how to understand it,

08:19.000 --> 08:23.000
how to see the world through that particular lens.

08:23.000 --> 08:26.000
And all of this came together last year

08:26.000 --> 08:28.000
when I was asked to write an article

08:28.000 --> 08:30.000
for Design Intelligence Quarterly,

08:30.000 --> 08:33.000
the article that Neil mentioned in the introduction.

08:33.000 --> 08:36.000
And it wasn't originally supposed to be on AI,

08:36.000 --> 08:39.000
but between myself and the editor there,

08:39.000 --> 08:41.000
we agreed that it would be.

08:41.000 --> 08:44.000
And that launched an exploration

08:44.000 --> 08:47.000
and it was fueled by skepticism, I think.

08:47.000 --> 08:51.000
At the time, this would have been February of 2023,

08:51.000 --> 08:54.000
there was a disconnect between what was happening in the world

08:54.000 --> 08:56.000
and what was happening in design.

08:56.000 --> 08:58.000
I mean, I suppose there typically is,

08:58.000 --> 09:02.000
but this one seemed a bit more ominous.

09:02.000 --> 09:05.000
In the world, we were hearing messages about,

09:05.000 --> 09:08.000
you know, AI is going to destabilize everything

09:08.000 --> 09:10.000
and the World Bank and the IMF

09:10.000 --> 09:14.000
were predicting huge job losses and massive change.

09:14.000 --> 09:17.000
Professors at Wharton and top business schools

09:17.000 --> 09:19.000
and economists were, you know,

09:19.000 --> 09:22.000
looking at this as a very, very serious development

09:22.000 --> 09:24.000
in the future of work.

09:24.000 --> 09:27.000
And then there was, you know, the design world.

09:27.000 --> 09:29.000
So, you know, I read the same design rags

09:29.000 --> 09:31.000
as everybody else, I suppose.

09:31.000 --> 09:33.000
So, you know, we kept hearing messages

09:33.000 --> 09:36.000
from design leaders to this effect.

09:36.000 --> 09:41.000
You know, there's nothing that can compete with architects.

09:41.000 --> 09:44.000
This one from Shane Berger, you know,

09:44.000 --> 09:49.000
the design process is going to remain fundamentally human.

09:49.000 --> 09:52.000
There's this kind of limited capabilities.

09:52.000 --> 09:55.000
And I wanted to understand for myself where the truth lay.

09:55.000 --> 10:00.000
I suspected it lays somewhere between those two polarities.

10:00.000 --> 10:03.000
And, you know, I asked if I could explore this

10:03.000 --> 10:05.000
through this article for design intelligence

10:05.000 --> 10:08.000
and they gratefully said yes.

10:08.000 --> 10:11.000
So, you know, I sat down to figure out, you know,

10:11.000 --> 10:15.000
what exactly I thought the impact was going to be.

10:15.000 --> 10:18.000
And the prevailing line at that time was that, you know,

10:18.000 --> 10:21.000
AI is going to automate the simple things.

10:21.000 --> 10:24.000
It's not going to automate the more complex work.

10:24.000 --> 10:26.000
So, for me, the greater challenge was, you know,

10:26.000 --> 10:32.000
how do you envision or propose that AI might actually,

10:33.000 --> 10:35.000
you know, replicate some of that higher level work

10:35.000 --> 10:38.000
that architects and designers do.

10:38.000 --> 10:40.000
And I borrowed something from Phil Bernstein's book,

10:40.000 --> 10:43.000
Machine Learning, a framework for understanding

10:43.000 --> 10:46.000
the relative cognitive complexity of different tasks

10:46.000 --> 10:49.000
that exist within an architect's day.

10:49.000 --> 10:51.000
If you've read that book, if you haven't read that book,

10:51.000 --> 10:53.000
you should. It's a great book.

10:53.000 --> 10:56.000
But Phil has this framework, essentially,

10:56.000 --> 10:59.000
for ordering projects between, you know,

10:59.000 --> 11:01.000
a procedural, like really kind of task,

11:01.000 --> 11:04.000
repetitive task sort of things,

11:04.000 --> 11:07.000
up to integrative and perceptive.

11:07.000 --> 11:10.000
Those are the higher level cognitive functions.

11:10.000 --> 11:13.000
And he provides us with a brilliant map, essentially,

11:13.000 --> 11:17.000
for where all these things occur within the architectural process.

11:17.000 --> 11:19.000
And just to make my life difficult, I said,

11:19.000 --> 11:22.000
well, you know, what if we aspire to get it

11:22.000 --> 11:25.000
to replicate the harder parts rather than the easier parts?

11:25.000 --> 11:28.000
I figured the easier parts would figure itself out.

11:28.000 --> 11:31.000
To me, and I think to Phil as well,

11:31.000 --> 11:35.000
you know, the hardest parts are the integrative stuff, right?

11:35.000 --> 11:39.000
It's the vision. It's being able to sit with a client

11:39.000 --> 11:43.000
and context and understand through typically inarticulate words

11:43.000 --> 11:46.000
what that vision is actually going to be

11:46.000 --> 11:50.000
and to bear it out in the form of an idea

11:50.000 --> 11:53.000
for a building or something like that.

11:53.000 --> 11:55.000
And as a proof of concept that, you know,

11:55.000 --> 11:58.000
I was initially made for myself just to make sure I knew

11:58.000 --> 12:01.000
what I was talking about. I developed a video,

12:01.000 --> 12:04.000
which I think, you know, also mentioned in the beginning,

12:04.000 --> 12:06.000
but it's attached to the article.

12:06.000 --> 12:08.000
You can find it on my YouTube channel,

12:08.000 --> 12:10.000
or you can find it on Design Intelligence.

12:10.000 --> 12:12.000
But I'm going to play a short clip

12:12.000 --> 12:14.000
just to introduce what I'm talking about here.

12:14.000 --> 12:16.000
Hilda told me a bit about your project,

12:16.000 --> 12:18.000
and I'm excited to learn more.

12:18.000 --> 12:20.000
Can you share your vision for it?

12:20.000 --> 12:23.000
I'm looking to build a modern, sustainable home for myself

12:23.000 --> 12:26.000
and be environmentally friendly. It's important for me to...

12:26.000 --> 12:29.000
So what you're looking at, and this goes on for about 20 minutes,

12:29.000 --> 12:33.000
is two GPTs set in opposition to each other.

12:33.000 --> 12:36.000
And they've both been programmed with personalities.

12:36.000 --> 12:40.000
So I said to the model, like, you are the client,

12:40.000 --> 12:42.000
you are a successful tech executive,

12:42.000 --> 12:44.000
you are a mother of two,

12:44.000 --> 12:47.000
you have a passion about kayaking, you know, this sort of thing.

12:47.000 --> 12:50.000
But I didn't give her any specific information necessarily

12:50.000 --> 12:53.000
about the architectural process or anything like that.

12:53.000 --> 12:56.000
And I did the same thing with the gentleman on the left,

12:56.000 --> 12:59.000
who was the architect, right?

12:59.000 --> 13:02.000
He was an architect, he was 55.

13:02.000 --> 13:05.000
You know, he went to school at Columbia, this, that,

13:05.000 --> 13:08.000
and the other thing, I didn't give him any instructions

13:08.000 --> 13:11.000
on what being an architect actually meant.

13:11.000 --> 13:13.000
And Hilda told me a bit about you.

13:13.000 --> 13:15.000
Let's fast forward through that.

13:15.000 --> 13:17.000
Through that conversation,

13:17.000 --> 13:21.000
GPT was able to generate a design brief

13:21.000 --> 13:24.000
based on the interview that the client had.

13:24.000 --> 13:27.000
Truthfully, it was just GPT having a conversation with itself.

13:27.000 --> 13:30.000
But nonetheless, based on that exchange,

13:30.000 --> 13:33.000
which was totally autonomous, it generated a design brief,

13:33.000 --> 13:37.000
which was then able to convert to image prompts,

13:37.000 --> 13:39.000
which went into mid-journey.

13:39.000 --> 13:41.000
Once mid-journey five debuted,

13:41.000 --> 13:44.000
I was able to actually get image recognition to play along.

13:44.000 --> 13:48.000
So Carla, the client, could actually look at the design options

13:48.000 --> 13:52.000
and respond.

13:52.000 --> 13:54.000
That was a little disarming.

13:54.000 --> 13:57.000
The really alarming part were all of the emerging phenomena

13:57.000 --> 13:59.000
that came out of this.

13:59.000 --> 14:03.000
I guess emerging capabilities is what the AI world calls them.

14:03.000 --> 14:08.000
But chat GPT illustrated some alarming knowledge

14:08.000 --> 14:10.000
about the process itself.

14:10.000 --> 14:13.000
So, you know, once it had gone through this design process,

14:13.000 --> 14:16.000
I asked it to produce a door and room schedule,

14:16.000 --> 14:18.000
which it did.

14:18.000 --> 14:22.000
And, you know, oddly, somehow knew that, like,

14:22.000 --> 14:25.000
bathrooms should have one window and not two,

14:25.000 --> 14:28.000
and that a powder room might be the exception.

14:28.000 --> 14:32.000
A powder room might have zero windows as opposed to one.

14:32.000 --> 14:35.000
So, you know, it was picking up on all these lot of things

14:35.000 --> 14:37.000
that might be known to an architect

14:37.000 --> 14:40.000
or might even be known to a layperson.

14:40.000 --> 14:44.000
But I certainly did not expect that the model would be able

14:44.000 --> 14:48.000
to extrapolate all that information just from a 20-minute

14:48.000 --> 14:51.000
conversation between an artificial architect

14:51.000 --> 14:53.000
and an artificial client.

14:53.000 --> 14:56.000
It did the same thing with budget.

14:56.000 --> 14:59.000
It came up with a budget for 2.2 million.

14:59.000 --> 15:01.000
Again, no extraneous information.

15:01.000 --> 15:04.000
I mean, it inferred that from the site

15:04.000 --> 15:08.000
and from whatever Carla the client was saying to this architect

15:08.000 --> 15:11.000
that she wanted out of her house,

15:11.000 --> 15:13.000
checked with a few contractor friends

15:13.000 --> 15:16.000
and, like, a lot of this AEI stuff, you know,

15:16.000 --> 15:18.000
wasn't wrong, could have been better,

15:18.000 --> 15:21.000
but it was in the ballpark.

15:21.000 --> 15:25.000
So, this immediately struck me as a disaster,

15:25.000 --> 15:28.000
because disasters typically happen

15:28.000 --> 15:30.000
when the perceived threat is much different

15:30.000 --> 15:32.000
than the actual threat.

15:32.000 --> 15:35.000
That's what gets us into trouble as a species.

15:35.000 --> 15:38.000
So, in my estimation, you know,

15:38.000 --> 15:41.000
I didn't believe the doomsayers that were saying that,

15:41.000 --> 15:43.000
you know, this is the end times,

15:43.000 --> 15:45.000
but I didn't believe the Pollyanna stuff

15:45.000 --> 15:49.000
that I felt was coming out of a lot of the design professions.

15:49.000 --> 15:52.000
The truth was somewhere in the middle

15:52.000 --> 15:55.000
and didn't look great.

15:55.000 --> 15:58.000
But let me flesh out what I mean by disaster lens

15:58.000 --> 16:00.000
to shed some light on that.

16:00.000 --> 16:03.000
So, how do we cultivate a disaster lens?

16:03.000 --> 16:05.000
I want to teach my students that there are things

16:05.000 --> 16:10.000
that you can look at that speak to the growing vulnerability

16:10.000 --> 16:15.000
and precarity of a particular context or situation, right?

16:15.000 --> 16:18.000
So, if you're looking at a natural disaster,

16:18.000 --> 16:20.000
like an earthquake, fire, hurricane, that sort of thing,

16:20.000 --> 16:23.000
you might look towards, you know, neglected infrastructure,

16:23.000 --> 16:25.000
trending towards decay, you know,

16:25.000 --> 16:27.000
bridges and power stations

16:27.000 --> 16:29.000
that should have been replaced a long time ago.

16:29.000 --> 16:32.000
You might also look to the emergence of patchwork solutions.

16:32.000 --> 16:35.000
So, if instead of replacing the broken levee,

16:35.000 --> 16:37.000
we just keep building it two feet higher over and over,

16:37.000 --> 16:39.000
that's usually a pretty good clue.

16:39.000 --> 16:42.000
A widening or deepening of the zone of vulnerability.

16:42.000 --> 16:45.000
So, 100,000 people living in a fault line,

16:45.000 --> 16:47.000
that could be a problem.

16:47.000 --> 16:49.000
That can be a disaster.

16:49.000 --> 16:51.000
That city swells to 3 million people.

16:51.000 --> 16:54.000
Then you've got a really serious disaster on your hand.

16:54.000 --> 16:57.000
Same fault line, just different zone of vulnerability.

16:57.000 --> 17:01.000
Widespread irrational and credibility.

17:01.000 --> 17:03.000
This is a tough one to pin down,

17:03.000 --> 17:07.000
but basically it's the anti-chicken little bone.

17:07.000 --> 17:10.000
The people who believe it can't happen here,

17:10.000 --> 17:13.000
and that is an emotional and a psychological belief

17:13.000 --> 17:17.000
that isn't particularly grounded in any evidence or analysis.

17:17.000 --> 17:21.000
It's just the way that people feel about things.

17:21.000 --> 17:25.000
All of these things taken in their sum, or even one by one,

17:25.000 --> 17:28.000
don't necessarily constitute a disaster.

17:28.000 --> 17:30.000
This is merely the framework,

17:30.000 --> 17:35.000
the things that set up the possibility of disaster.

17:35.000 --> 17:39.000
I liken it to a tornado watch and a tornado warning,

17:39.000 --> 17:43.000
and then I find that people don't really understand what that is,

17:43.000 --> 17:45.000
especially if you're outside the United States.

17:45.000 --> 17:48.000
So, I've actually happened upon the taco watch,

17:48.000 --> 17:51.000
and the taco warning is the way to explain it.

17:51.000 --> 17:55.000
So, taco watch is like, you have all the ingredients for a taco,

17:55.000 --> 17:59.000
but there's no taco, it's just a bunch of ingredients laying around.

17:59.000 --> 18:02.000
The taco warning is like, we have a taco.

18:02.000 --> 18:04.000
We're having tacos right now,

18:04.000 --> 18:07.000
you need to react to that sort of situation.

18:07.000 --> 18:11.000
The four things, and that's not a comprehensive or exhaustive list.

18:11.000 --> 18:13.000
We go through much more through the course of the semester,

18:13.000 --> 18:16.000
but there's things that you can look at

18:16.000 --> 18:20.000
that essentially create the possibility of tacos, so to speak.

18:21.000 --> 18:24.000
Finally, you need to trigger an event.

18:24.000 --> 18:29.000
That's the thing that ultimately brings a disaster into the news,

18:29.000 --> 18:31.000
into reality, and believe it or not,

18:31.000 --> 18:36.000
I've always found the best way to explain this is by using beavers as an analogy.

18:36.000 --> 18:39.000
So, when a beaver goes to cut down a tree,

18:39.000 --> 18:42.000
it doesn't cut down the entire tree,

18:42.000 --> 18:44.000
and it doesn't push the tree over.

18:44.000 --> 18:46.000
It's not a very strong animal.

18:46.000 --> 18:51.000
A 30 or 40-pound animal versus a 10,000-pound tree.

18:51.000 --> 18:54.000
What it does do is it eats around the base

18:54.000 --> 18:57.000
and subtly erodes the structural integrity of the tree,

18:57.000 --> 19:02.000
and eventually a gust of wind comes along and it blows the tree over.

19:02.000 --> 19:05.000
That is not the beaver's doing.

19:05.000 --> 19:07.000
It's the wind that blew over the tree.

19:07.000 --> 19:11.000
The beaver just set up the conditions for the wind to be able to do that.

19:11.000 --> 19:14.000
We can map this over time

19:14.000 --> 19:18.000
and look at essentially a wind condition,

19:18.000 --> 19:21.000
then the wind's blowing, however the wind's blowing,

19:21.000 --> 19:26.000
and then a rate of chewing that is constant, presumably,

19:26.000 --> 19:29.000
and then because the rate of chewing is constant,

19:29.000 --> 19:33.000
the structural integrity of the tree is steadily declining.

19:33.000 --> 19:36.000
Disasters have been here in the red circle.

19:36.000 --> 19:42.000
At that intersection of some event and some trajectory of vulnerability,

19:42.000 --> 19:46.000
that's where the disaster, in fact, occurs.

19:46.000 --> 19:48.000
It does not occur when the wind is strongest.

19:48.000 --> 19:52.000
It does not occur at some particular point of vulnerability.

19:52.000 --> 19:55.000
You really have to bring those two things together in time

19:55.000 --> 19:59.000
in order to have a disaster.

19:59.000 --> 20:02.000
You can apply the same logic to a city,

20:02.000 --> 20:04.000
and in fact, in my classes, we often do.

20:04.000 --> 20:06.000
You can take the healthy growth of a city,

20:06.000 --> 20:09.000
maybe at 2% or something like that,

20:09.000 --> 20:13.000
but the population density is growing faster than that,

20:13.000 --> 20:19.000
which suggests a lack of housing or something else going on.

20:19.000 --> 20:22.000
The average wealth of population is going down,

20:22.000 --> 20:24.000
which means people have fewer resources

20:24.000 --> 20:27.000
to support themselves in the event of a disaster.

20:27.000 --> 20:30.000
The budget for emergency services is static,

20:30.000 --> 20:34.000
even though the population and the population density is increasing.

20:34.000 --> 20:37.000
The average age of critical infrastructure,

20:37.000 --> 20:41.000
bridges and buildings and everything is falling apart.

20:41.000 --> 20:43.000
You can roll this stuff up together,

20:43.000 --> 20:45.000
not necessarily in an arithmetically way,

20:45.000 --> 20:47.000
but you can roll this stuff up

20:47.000 --> 20:51.000
into some kind of picture of overall resilience,

20:51.000 --> 20:54.000
based on the number of things that are going up

20:54.000 --> 20:56.000
and the number of things that are going down,

20:56.000 --> 20:58.000
and how impactful those things are.

20:58.000 --> 21:01.000
You get a sense of whether the resilience,

21:01.000 --> 21:03.000
the ability to resist an event

21:03.000 --> 21:07.000
is going up or down for a particular city in this case.

21:07.000 --> 21:09.000
Then you have the event itself,

21:09.000 --> 21:13.000
like the earthquake that happens every 100 years,

21:13.000 --> 21:16.000
200 years, 500 years, whatever it is,

21:16.000 --> 21:19.000
but when it does, as it crosses that intersection,

21:19.000 --> 21:22.000
that's what creates a disaster.

21:22.000 --> 21:25.000
I mentioned the 2010 Haiti earthquake previously.

21:25.000 --> 21:31.000
Haiti had actually had a similar sized earthquake 200 years prior.

21:31.000 --> 21:33.000
Of course, nobody remembered,

21:33.000 --> 21:36.000
because we don't remember things that happened 200 years ago.

21:36.000 --> 21:39.000
Not a big disaster, not a lot of damage,

21:39.000 --> 21:41.000
because there was hardly anyone there.

21:41.000 --> 21:43.000
It was 200 years ago, so Port-au-Prince was there.

21:43.000 --> 21:45.000
It was just a very, very small city,

21:45.000 --> 21:47.000
and most of the buildings were made of wood.

21:47.000 --> 21:50.000
You really have to look at the possibility of disaster

21:50.000 --> 21:53.000
as something that is happening through time.

21:53.000 --> 21:57.000
We can actually translate this disaster lens,

21:57.000 --> 21:59.000
this disaster thinking to non-physical things,

21:59.000 --> 22:03.000
and that also happens in my classes from time to time.

22:03.000 --> 22:06.000
We can apply it to non-physical things,

22:06.000 --> 22:08.000
like a profession itself,

22:08.000 --> 22:11.000
which is kind of how I started looking at all of this stuff.

22:11.000 --> 22:13.000
In terms of infrastructure,

22:13.000 --> 22:16.000
well, a profession of architecture is terrible infrastructure.

22:16.000 --> 22:20.000
We have long licensing periods, low pay, precarious employment,

22:20.000 --> 22:22.000
cutthroat competition.

22:22.000 --> 22:27.000
It's not a structurally sound profession necessarily.

22:27.000 --> 22:31.000
We can debate that, but yeah, anyway.

22:31.000 --> 22:34.000
Then we have patchwork solutions.

22:34.000 --> 22:38.000
If the profession is facing a crisis of value,

22:38.000 --> 22:41.000
then it keeps doing these stepwise things,

22:41.000 --> 22:45.000
like slightly changing the internship program

22:45.000 --> 22:49.000
that everybody has always disliked under any particular form,

22:49.000 --> 22:51.000
or transitioning to BIM,

22:51.000 --> 22:55.000
but not capturing the value off of that transition.

22:56.000 --> 22:58.000
A widening zone of vulnerability.

22:58.000 --> 23:02.000
Well, there's 17% more architects than were a decade ago,

23:02.000 --> 23:05.000
but AI is going to leave them with less work to do.

23:05.000 --> 23:07.000
How much less work, I'm not really sure,

23:07.000 --> 23:12.000
but that pool of vulnerability is getting larger.

23:12.000 --> 23:16.000
Then the widespread irrational incredulity.

23:16.000 --> 23:20.000
I would put in this box specifically,

23:20.000 --> 23:22.000
design voices that are saying,

23:22.000 --> 23:25.000
there's not going to be any impact on architecture.

23:25.000 --> 23:28.000
There's just going to be impact everywhere else.

23:28.000 --> 23:31.000
That ignores all fundamental laws of economics.

23:31.000 --> 23:35.000
If there's huge job losses in the non-architectural world

23:35.000 --> 23:37.000
and the rest of the economy,

23:37.000 --> 23:40.000
that has macroeconomic effects.

23:40.000 --> 23:45.000
Probably gets us to 5%, 10% employment, causes a recession.

23:45.000 --> 23:49.000
The last time that there was a recession with 10% unemployment

23:49.000 --> 23:53.000
in this country, a third of all architects lost their jobs.

23:53.000 --> 23:57.000
The idea that AI might have some huge effect

23:57.000 --> 23:59.000
on all their professions,

23:59.000 --> 24:04.000
but not on architecture is incoherent, in my opinion.

24:04.000 --> 24:06.000
The triggering event.

24:06.000 --> 24:08.000
What is the triggering event in this case?

24:08.000 --> 24:11.000
In 2008, it was the great recession.

24:11.000 --> 24:16.000
I think that the debut of natural language generative AI

24:16.000 --> 24:19.000
may in this case be the triggering event.

24:19.000 --> 24:23.000
We can map this out the same way that we have with these other things

24:23.000 --> 24:26.000
and say, look, for the profession of architecture,

24:26.000 --> 24:28.000
the complexity of buildings is increasing.

24:28.000 --> 24:32.000
That's good for architects because it means they have more to do.

24:32.000 --> 24:38.000
The cost of construction is increasing.

24:38.000 --> 24:41.000
That's also good because design fees usually track

24:41.000 --> 24:42.000
with construction costs.

24:42.000 --> 24:44.000
Student loan balance is going up.

24:44.000 --> 24:47.000
That's not good because it means architects themselves,

24:47.000 --> 24:48.000
especially the younger ones,

24:48.000 --> 24:51.000
are in more precarious individual situations.

24:51.000 --> 24:54.000
Pay relative to inflation is going down.

24:54.000 --> 24:56.000
That's not so good.

24:56.000 --> 24:59.000
The overall resilience is going downwards.

24:59.000 --> 25:03.000
Similar to what we saw with the tree and what's going on with the city,

25:03.000 --> 25:06.000
we have this curve.

25:06.000 --> 25:08.000
Whatever it is, the shock to the system,

25:08.000 --> 25:12.000
be it a recession or a new technology or something like that,

25:12.000 --> 25:15.000
that brings about the disaster.

25:15.000 --> 25:19.000
The disaster happens at some point in the future.

25:19.000 --> 25:22.000
Why doesn't this feel like a disaster?

25:22.000 --> 25:24.000
I can hear some of you saying,

25:24.000 --> 25:27.000
we don't think of it as a disaster the way we do an earthquake

25:27.000 --> 25:30.000
or a hurricane or something like that.

25:30.000 --> 25:34.000
I think that the hesitancy to understand it as such

25:34.000 --> 25:38.000
is translatable from physical hazards to professional hazards

25:38.000 --> 25:39.000
and et cetera.

25:39.000 --> 25:42.000
We protect ourselves from the psychological toll

25:42.000 --> 25:45.000
of impending disaster through myths.

25:45.000 --> 25:48.000
We invent ideas about what's going to happen

25:48.000 --> 25:52.000
and what's not going to happen in order to protect ourselves

25:52.000 --> 25:56.000
from the really high emotional and psychological costs

25:56.000 --> 25:58.000
of understanding that.

25:58.000 --> 26:01.000
That's the logic of it can't happen here.

26:01.000 --> 26:03.000
It can't happen to me.

26:03.000 --> 26:07.000
Human beings are actually not very good at judging the future.

26:08.000 --> 26:12.000
We create these myths about what's going to happen in the future.

26:12.000 --> 26:15.000
They keep us safe when we tell ourselves,

26:15.000 --> 26:18.000
it can't happen here.

26:18.000 --> 26:21.000
That's a problem because if we really believe that,

26:21.000 --> 26:23.000
we're not preparing for it.

26:23.000 --> 26:26.000
I think that was part of the point of Sinclair-New Louis' novels

26:26.000 --> 26:28.000
that if you think it can happen here,

26:28.000 --> 26:30.000
you'll take steps to prepare for it.

26:30.000 --> 26:33.000
If you've washed out the possibility in your own mind,

26:34.000 --> 26:39.000
then it opens the door to whatever's coming next.

26:39.000 --> 26:42.000
Not a guarantee, but it opens that door.

26:42.000 --> 26:45.000
How do we deconstruct these myths?

26:45.000 --> 26:47.000
This is my favorite part.

26:47.000 --> 26:50.000
On my sub-stack, Life is a Disaster,

26:50.000 --> 26:53.000
you'll find lots of examples of this where I go

26:53.000 --> 26:57.000
and break down myths associated with AI and architecture.

26:57.000 --> 27:01.000
Let's deal with five of them just for today real quick.

27:01.000 --> 27:05.000
Myth one, AI won't replace jobs, not tasks.

27:05.000 --> 27:08.000
I started with this one because it's patently absurd.

27:08.000 --> 27:10.000
That's what a job is.

27:10.000 --> 27:14.000
It's just a bunch of tasks that roll up into some project

27:14.000 --> 27:17.000
and projects that roll up into programs

27:17.000 --> 27:20.000
and programs that roll up into practices.

27:20.000 --> 27:24.000
What we've been being told about this AI stuff

27:24.000 --> 27:27.000
is that you're going to have a firm

27:27.000 --> 27:32.000
and then everybody is going to essentially have more time

27:32.000 --> 27:36.000
because all of this technology is going to automate away

27:36.000 --> 27:39.000
all of the boring tasks that we have.

27:39.000 --> 27:42.000
This isn't how it plays out.

27:42.000 --> 27:46.000
If anybody's ever been in a firm where layoffs are going,

27:46.000 --> 27:48.000
this isn't what happens.

27:48.000 --> 27:51.000
They don't allow people to just have their workload

27:51.000 --> 27:53.000
reduced necessarily.

27:53.000 --> 27:56.000
Every firm principle in the world is obsessed with utilization.

27:56.000 --> 28:00.000
How do we get people to a maximum level of utilization

28:00.000 --> 28:04.000
so that we're billing the clients for that person's time?

28:04.000 --> 28:07.000
The more likely stairs is actually this one.

28:07.000 --> 28:11.000
Workload is reduced through technological efficiencies

28:11.000 --> 28:15.000
and you just don't need that fifth person anymore.

28:15.000 --> 28:20.000
That is what happens in the face of technological evolution

28:20.000 --> 28:22.000
and efficiency gains.

28:22.000 --> 28:26.000
At the end, you have four architects instead of five

28:26.000 --> 28:30.000
and they're fully utilized because that work has been redistributed.

28:30.000 --> 28:35.000
This has to be the case because of the way that we define work.

28:35.000 --> 28:41.000
Work is just some amount of effort times some amount of time.

28:41.000 --> 28:43.000
That's all it is.

28:43.000 --> 28:47.000
An example on screen, two full-time equivalents for six months

28:47.000 --> 28:51.000
is equivalent to about 2,000 hours a month.

28:51.000 --> 28:53.000
A year, rather.

28:53.000 --> 28:57.000
That's the same as one person working full-time for a year.

28:57.000 --> 29:01.000
It's also the same as three people working for four months.

29:01.000 --> 29:03.000
That's all the same amount of work.

29:03.000 --> 29:07.000
Any technological evolution is going to have one or two effects.

29:07.000 --> 29:10.000
It's either going to give us less work to do,

29:10.000 --> 29:14.000
meaning it reduces the overall amount of labor involved in a task,

29:14.000 --> 29:16.000
or it's going to speed things up,

29:16.000 --> 29:19.000
meaning we can get things done faster.

29:19.000 --> 29:21.000
Or it's going to do both.

29:21.000 --> 29:25.000
Both of these are problematic for architecture.

29:25.000 --> 29:29.000
Under scenario one, where it just reduces the overall work,

29:29.000 --> 29:33.000
we've got a problem because there's just not as much work to do.

29:33.000 --> 29:36.000
Technology has taken something that used to take a week,

29:36.000 --> 29:38.000
and now it takes an hour,

29:38.000 --> 29:43.000
so we've got to find something for that person to do with the rest of their week.

29:43.000 --> 29:46.000
Scenario two, things moving faster,

29:46.000 --> 29:50.000
could also be problematic, almost certainly will be,

29:50.000 --> 29:52.000
because as you start to speed things up,

29:52.000 --> 29:56.000
you end up with these holes in the project cycle.

29:56.000 --> 30:00.000
What do you do with that time?

30:00.000 --> 30:02.000
The most likely scenario, in my opinion,

30:02.000 --> 30:05.000
is that it's actually both, that it speeds things up

30:05.000 --> 30:10.000
but also reduces the overall amount of labor involved.

30:10.000 --> 30:12.000
Yeah, we've got problems.

30:12.000 --> 30:15.000
This is potentially disastrous.

30:15.000 --> 30:19.000
Myth number two, AI will automate the tasks we don't like,

30:19.000 --> 30:22.000
leaving us more time to design.

30:22.000 --> 30:24.000
I've heard this everywhere,

30:24.000 --> 30:30.000
in kind of every article and position point on AI.

30:30.000 --> 30:33.000
There's somehow this belief that,

30:33.000 --> 30:36.000
once AI automates away all the shop drawings

30:36.000 --> 30:40.000
and red light and CAD drawings and all this shit,

30:40.000 --> 30:44.000
we can just spend all of our time designing,

30:44.000 --> 30:46.000
and it's a very attractive idea,

30:46.000 --> 30:49.000
because we love design, that's what we were trained for

30:49.000 --> 30:52.000
and that's what we went to school for.

30:52.000 --> 30:55.000
I like designing, but I don't think that this is possible,

30:55.000 --> 31:00.000
because it's actually constructed on top of several other myths.

31:00.000 --> 31:04.000
If this is the idea that we're going to have huge amounts of time

31:04.000 --> 31:07.000
to spend in design, now that construction documents

31:07.000 --> 31:09.000
and all the rest have been sped up,

31:09.000 --> 31:12.000
I think the problem there is,

31:12.000 --> 31:17.000
we don't necessarily choose where technology is going to have an impact.

31:17.000 --> 31:22.000
The presumption behind that idea of having more and more design time

31:22.000 --> 31:26.000
is that AI is going to have a huge impact here

31:26.000 --> 31:30.000
on the stuff that we don't like to do, and then none here.

31:30.000 --> 31:33.000
Technology does never really work that way.

31:33.000 --> 31:38.000
Once a technology like cell phones or electricity or computers penetrates,

31:38.000 --> 31:41.000
it becomes a de facto expectation.

31:41.000 --> 31:44.000
If you're a photographer and you don't want to use Photoshop,

31:44.000 --> 31:47.000
you don't have to, it's not like the law,

31:47.000 --> 31:51.000
but you're going to have a real problem coexisting in a competitive economy

31:51.000 --> 31:53.000
with photographers that do.

31:53.000 --> 31:59.000
We don't necessarily just get to decide where technology applies,

31:59.000 --> 32:01.000
generally speaking.

32:01.000 --> 32:05.000
I think there's also something about the law of diminishing returns,

32:05.000 --> 32:09.000
and I think mature designers get to a point where they realize

32:09.000 --> 32:16.000
that designs get to a point where they're 95%, 96% of perfect,

32:16.000 --> 32:20.000
and we have that drive within us to get them to 100%,

32:20.000 --> 32:25.000
and we know that we can, but the closer you are to design perfection

32:25.000 --> 32:31.000
if such a thing exists, the harder it is to resolve those last little bits

32:31.000 --> 32:34.000
without going back and reinventing everything that you just did.

32:34.000 --> 32:40.000
I think clients also understand this, which is why they put a deadline on you.

32:40.000 --> 32:41.000
Professors understand it.

32:41.000 --> 32:45.000
That's why you have final review, because if those deadlines didn't exist,

32:45.000 --> 32:47.000
we would just keep working forever.

32:47.000 --> 32:50.000
If you imagine this graphically, you say,

32:50.000 --> 32:54.000
okay, there's a design that it's 95% of perfect,

32:54.000 --> 33:00.000
and I'm going to work asymptotically to get it towards that final product.

33:00.000 --> 33:04.000
This has financial implications for whoever's paying the bill for your design time.

33:04.000 --> 33:09.000
At first, it's a loss because the design adaptations, whatever it is,

33:09.000 --> 33:11.000
have not been fully resolved.

33:11.000 --> 33:13.000
You need time to think through that and to think through,

33:13.000 --> 33:18.000
okay, how do I get from 95% to 100% perfect design,

33:18.000 --> 33:21.000
idealized design, and everybody's fine.

33:21.000 --> 33:25.000
So someone is taking a loss on that particular time.

33:25.000 --> 33:29.000
As those choices start to mature, assuming that you are a brilliant designer,

33:29.000 --> 33:33.000
which I'm sure you are, the payout becomes really high.

33:33.000 --> 33:40.000
The marginal payout for every additional unit, day, week, month of design time,

33:40.000 --> 33:46.000
climbs rapidly because you're resolving the things that weren't resolved in the 95% scenario.

33:46.000 --> 33:54.000
But the closer you get to perfection, the more those gains start to diminish.

33:54.000 --> 34:00.000
That's in the nature of diminishing returns of asymptotic growth and all these other things.

34:00.000 --> 34:05.000
So that's why it seems like for a lot of clients, 95% is good enough.

34:05.000 --> 34:09.000
They're not interested in paying for you to get to 100%.

34:09.000 --> 34:17.000
Exceptions exist, obviously, but for a lot of clients, good enough is good enough.

34:17.000 --> 34:20.000
And because fundamentally when we're working like this,

34:20.000 --> 34:25.000
different kinds of changes have different value through time.

34:25.000 --> 34:32.000
So I think the most convincing micro myth under this myth is what I call the creativity surplus.

34:32.000 --> 34:36.000
And I'm sure everybody has seen some version of this cartoon at some point.

34:36.000 --> 34:43.000
The architect starts out with some brilliant vision, hugely aspirational about what to do with the design.

34:43.000 --> 34:46.000
It gets a little knocked down with the meeting with the clients.

34:46.000 --> 34:51.000
And then you go through all the working drawings and budget revisions and value engineering.

34:51.000 --> 34:56.000
And pretty soon the building is kind of ordinary.

34:56.000 --> 35:07.000
So I think from a standpoint of value analysis, if we can't get clients to pay us for all the creativity that we have now,

35:07.000 --> 35:12.000
why would we suspect that they're going to pay us for more of it?

35:12.000 --> 35:21.000
So this idea that this technological advance will lead to this explosion in design activity and design time.

35:21.000 --> 35:24.000
Unless you're working for yourself, I don't see it being true.

35:24.000 --> 35:29.000
Like someone is still going to have to pay you for all of that time.

35:29.000 --> 35:38.000
So myth number three, AI won't place architects because architects are too smart or creative or charming, attractive.

35:38.000 --> 35:42.000
Take your pick. I've seen lots of ideas like this.

35:42.000 --> 35:46.000
I think that's probably true.

35:46.000 --> 35:54.000
And I think it's a red herring argument that people are making sometimes where they say AI will not get to the level of human.

35:54.000 --> 36:01.000
The reality that we need to understand is that AI does not need to get to human level in order to displace humans.

36:01.000 --> 36:06.000
Because that's not how people make purchasing decisions.

36:06.000 --> 36:14.000
Everybody has had some cheapo client at some point and everybody has had a dream client at some point.

36:14.000 --> 36:20.000
And we can borrow a concept from economics called indifference curve to map this out.

36:20.000 --> 36:26.000
This is not exactly how an indifference curve is supposed to be used, but we're using it as a proxy.

36:26.000 --> 36:32.000
So how do we exchange things between good architecture and cheap architecture?

36:32.000 --> 36:36.000
Like how do people assess that balance? Well, you can map that with a curve.

36:36.000 --> 36:41.000
And the easiest way to understand it is through its slope, through rise and run, just like a stair.

36:41.000 --> 36:52.000
So what this is visualizing is a client who is willing to reduce the quality of design by 50% if the cost is reduced by 40%.

36:52.000 --> 36:59.000
Not great, not terrible, probably a middle of the road client or something like that.

36:59.000 --> 37:09.000
The second client is a lot worse because this is a client that will reduce the quality of design by 50% for a 10% drop in cost.

37:09.000 --> 37:15.000
So I've certainly had clients like that, maybe some other people have too.

37:15.000 --> 37:22.000
But these are the clients who don't really care about design and they're just good enough is good enough.

37:22.000 --> 37:27.000
They're just willing to pass over everything if it saves a few bucks.

37:27.000 --> 37:33.000
And that's kind of tragic and I know that a lot of architects live with that frustration on a daily basis.

37:33.000 --> 37:42.000
So how does this play out in terms of the competition between human architects and generative design?

37:42.000 --> 37:45.000
Well, we can also map this over time.

37:45.000 --> 37:54.000
So if we assume that human is the standard, that's 100%, that is the talent and capability and vision of a human architect.

37:54.000 --> 38:03.000
And we've got generative design either with human assistance or without that's rising and we commit to the idea that it will never get there.

38:03.000 --> 38:13.000
It will never reach human capability, no matter how much time we give it, it's just going to be 90% of what an architect can be.

38:13.000 --> 38:20.000
We're here, we're living in the yellow zone where capabilities of this technology is rising very quickly.

38:20.000 --> 38:24.000
But at the same time, like we know that it's just never going to get there.

38:24.000 --> 38:25.000
Okay, fine.

38:25.000 --> 38:26.000
How does that compare to costs?

38:26.000 --> 38:37.000
Well, the costs of a human architect, human design is going up steadily as you expect it would with inflation, hopefully, you know, a raise once in a while.

38:37.000 --> 38:47.000
The cost of generative AI is dropping precipitously and has been for decades and will likely continue to do so.

38:47.000 --> 39:01.000
So when you take the ratio of cost to, you know, whatever that design quality is, and these numbers are arbitrary, understand like you can substitute your own numbers if you really want to, but the effect will be the same.

39:01.000 --> 39:16.000
You get cost over quality of human architect kind of rising steadily because, you know, the quality is what it is, that's the standard, and costs are rising, you know, slow rate inflation, something like that.

39:16.000 --> 39:19.000
But the cost over quality of AI is dropping.

39:19.000 --> 39:26.000
So the quality is getting better and the cost is coming down, which creates this very, very sharp curve towards the bottom.

39:26.000 --> 39:36.000
And that creates a whole different set of problems for architects because all of a sudden, you're not choosing between these two cheapo clients, you've got a different dynamic.

39:36.000 --> 39:45.000
Now you've got a position where a client can reduce the quality of design by 10% and achieve a 50% reduction in cost.

39:45.000 --> 39:53.000
And that becomes a lot more seductive than I think potentially disastrous because we don't want to design this 90%, right?

39:53.000 --> 39:57.000
We want like the best design to be out there in the world.

39:57.000 --> 40:05.000
But as you make it more and more attractive by hacking the cost 50%, 80%, 90%, I think it's going to seduce a lot of people.

40:05.000 --> 40:11.000
Alright, myth number four, AI won't be a threat in the future because of something it can't do now.

40:11.000 --> 40:13.000
We've heard a lot of this, too.

40:13.000 --> 40:21.000
So from Kermit Baker, the chief economist of the AI, AI can't pour concrete, paint a wall, or install flooring.

40:21.000 --> 40:28.000
Momentarily setting aside the fact that it actually can pour concrete and paint a wall and install flooring.

40:28.000 --> 40:31.000
It does all those things today.

40:31.000 --> 40:38.000
So we can think about this whole thing a little bit more abstractly in terms of what AI is going to do in the future.

40:38.000 --> 40:41.000
So AI is growing exponentially.

40:41.000 --> 40:45.000
What that means, what rates, we don't know.

40:45.000 --> 40:56.000
It's not necessary for this illustration, but you've got AI growing at an exponential rate in terms of its learning, its capability, everything that it can do, etc.

40:56.000 --> 41:03.000
AI's grow at a linear rate, which is probably not a big deal if you're over 50.

41:03.000 --> 41:14.000
So if you're an architect and you're pretty senior, it probably fuels that speculation that you have that AI can't actually compete with your knowledge and skills because it can't.

41:14.000 --> 41:22.000
Like not now, probably not ever because you're going to retire prior to the point where AI ever gets to that level.

41:22.000 --> 41:34.000
So the skills that you have after 40, 50 years of practice remain valuable today and will probably remain valuable for the next couple of years or 10 years or something like that.

41:34.000 --> 41:39.000
And that's probably why older architects seem to be a lot less concerned about all of this AI business.

41:39.000 --> 41:42.000
But the picture is different for senior architects.

41:42.000 --> 42:00.000
So some of them 10, 15, 20 years of experience, they're going to have their growth cut off at some point by, you know, this red wall of AI growth and AI will grow to the point where it assumes the capabilities that senior architect actually has.

42:00.000 --> 42:12.000
That senior architect never really gets a chance to ascend to the level of professional maturity and skill set that the principal architect did because their progress is essentially cut off.

42:12.000 --> 42:27.000
Much bigger problem for a recent grant because someone who's graduating architecture school today has to look at the possibility that as they're out there and they're trying to build their skills and their capabilities, you know, they're cut off.

42:27.000 --> 42:35.000
They've got a few years in the profession, maybe 10 before the capabilities of AI exceed their own capabilities.

42:35.000 --> 42:48.000
So they'll never get to that skill level that the prior generations have because by the time they do, AI will be better than they are at the task of architecture.

42:48.000 --> 43:03.000
And it's hugely problematic if you're a child, you know, if you're a 16 year old and you're thinking about going into architecture, you're not going to get there for at least another, you know, six, seven, eight years.

43:03.000 --> 43:11.000
You're not going to be licensed for another 10, 12, 15 years. And by that time you've come in underneath the growth curve of AI.

43:11.000 --> 43:21.000
So all this discussion about what AI can do today as a measure about what it can do tomorrow, I think is a false profit.

43:21.000 --> 43:29.000
You know, we need to be looking at the growth rates of AI and understand what it's going to be able to do in the future.

43:29.000 --> 43:37.000
And here in argument, I think, you know, well, AI is not going to be a threat in the future because it can't do this thing right now.

43:37.000 --> 43:41.000
I would just set it aside because it's relatively nonsense.

43:41.000 --> 43:46.000
All right, final myth, we'll just do more projects.

43:46.000 --> 43:52.000
This is also mythological because it violates basic laws of supply and demand.

43:52.000 --> 43:58.000
And I'm going to illustrate this with golf clubs, because I think golf is an urban crime.

43:58.000 --> 44:04.000
By the way, apologies to anybody who does. So let's imagine a golf club market and you have three suppliers, right?

44:04.000 --> 44:10.000
And they all produce about 20 golf club sets per quarter.

44:10.000 --> 44:18.000
This creates an overall industry supply of 60 golf clubs, golf club sets per quarter, and the demand is also 60 golf club sets.

44:18.000 --> 44:22.000
Perfect supply and demand like in balance, right?

44:22.000 --> 44:29.000
Now, what happens when supplier B comes upon a technology that allows them to triple their production of golf clubs?

44:29.000 --> 44:33.000
They're producing 60 a quarter while the overall supply goes to 100.

44:33.000 --> 44:36.000
But then what happens to the demand?

44:36.000 --> 44:40.000
Does it also swell to meet that supply?

44:40.000 --> 44:42.000
No.

44:42.000 --> 44:50.000
I mean, very rarely with very specific things does demand rise to meet supply that almost never happens.

44:50.000 --> 44:57.000
One scenario is that supplier B puts their competition out of business and just gobbles up all of the work as may happen in architecture.

44:57.000 --> 45:05.000
As, you know, big technologically advanced firms adopt this new technology faster and just decide to, you know, take over everybody else's business.

45:05.000 --> 45:12.000
I think the more likely scenario is that all of this extra golf club sets go in the trash, right?

45:12.000 --> 45:16.000
Because there is no market to actually absorb what they're doing.

45:16.000 --> 45:18.000
This is the key thing.

45:18.000 --> 45:20.000
The demand is fixed.

45:20.000 --> 45:30.000
Just because you produce three times as many golf club sets doesn't mean that, you know, people are going to go out and buy three times as many golf club sets because how many do you need?

45:30.000 --> 45:36.000
Like people are not going to spontaneously start playing golf because you've produced all these extra golf sets.

45:36.000 --> 45:46.000
And I think that's the problem with a lot of this thinking around design is that, you know, just because we can do the design a lot faster and produce more of it,

45:46.000 --> 45:49.000
doesn't mean that people want more of it.

45:49.000 --> 45:54.000
The demand for design services is fundamentally driven by the demand for buildings.

45:54.000 --> 45:59.000
The demand for buildings is driven by all these things, but it's also related to the money supply.

45:59.000 --> 46:05.000
So the money supply and the demand for buildings combine to create the demand for design services.

46:05.000 --> 46:14.000
And this has no relationship to anything that's going on within the AC world, the software we use, how fast it is, like whatever dynamo scripts you've written.

46:14.000 --> 46:18.000
So none of that influences demand at all.

46:18.000 --> 46:21.000
Maybe in some very, very peripheral ways.

46:21.000 --> 46:32.000
But the point is that, you know, our work as designers as architects is trapped between like the money supply, the money that coming in, the demand for buildings on the demand side.

46:32.000 --> 46:37.000
And, you know, what we do inside doesn't fundamentally change either of those things.

46:37.000 --> 46:45.000
So if you produce 10 times as many coffee makers, it doesn't mean people need 10 times as many coffee makers.

46:45.000 --> 46:51.000
If we could produce 10 times as much design, doesn't mean people need 10 times as many buildings.

46:51.000 --> 47:00.000
And that is the fundamental constraint that I'm most worried about is people think, oh, well, we're just going to do, you know, more and more work.

47:00.000 --> 47:03.000
We can't, there's not a demand.

47:03.000 --> 47:08.000
There's no way to sort of justify all of that extra work.

47:08.000 --> 47:15.000
So let's wrap this up by looking at the brighter side of disaster. I love teaching disaster because I think disaster is fundamentally optimistic.

47:15.000 --> 47:18.000
And we have thousands of years of history to prove it.

47:18.000 --> 47:31.000
Ever since our ancestors started utilizing floods on the Nile River Delta to kickstart the policy and, you know, invent civilization, humanities has really productive relationship with disaster.

47:31.000 --> 47:38.000
Lisbon 1755 suffered earthquake tsunami and a city white fire at the same time.

47:38.000 --> 47:42.000
They all kind of caused each other one earthquake caused the rest of it.

47:42.000 --> 47:48.000
But anyway, very, very tenuous time, you know, the seeds of the enlightenment were there.

47:48.000 --> 47:50.000
But it happened on all scenes day.

47:50.000 --> 47:59.000
So a lot of the very religious people in Portugal were saying to themselves, oh my God, we didn't pray enough and that's why this is happening.

47:59.000 --> 48:04.000
Let's, you know, return to religious fundamentalism and all this stuff.

48:04.000 --> 48:12.000
And the course of history was changed by the Marquis de Pombo, who basically drove the response to this particular earthquake.

48:12.000 --> 48:17.000
Now the earthquake, fire, etc. already had become well known throughout Europe.

48:17.000 --> 48:25.000
It had influenced a lot of thinkers because it provoked a question, you know, this horrible, horrible trifecta of disaster happens on all Saints Day.

48:25.000 --> 48:29.000
Does that mean that there is a God or does that mean that there is no God?

48:29.000 --> 48:34.000
And like what should we actually be doing about this condition?

48:34.000 --> 48:41.000
So Marquis de Pombo leaned in the enlightenment direction and we should thank him for it.

48:41.000 --> 48:53.000
So he developed early approaches to seismology and horses, riders on horses go out in radial directions and interview people about the length of the shaking and what happened and everything like that.

48:53.000 --> 49:04.000
And when he started to establish an epicenter, he created the Pombo lean style or had it created, which we still use today in many parts of the world.

49:04.000 --> 49:15.000
So I've ever seen this kind of construction like that really became popularized after this this Lisbon earthquake at the direction of the Marquis.

49:15.000 --> 49:34.000
Well, generally speaking, kind of jump started the enlightenment, right? I mean, this was a huge historical event told us like when an act of God happens, we can respond with design, you know, we can actually design that particular disaster away and protect ourselves in the future.

49:34.000 --> 49:46.000
Nowhere has this ever been more evident than the city wide fire and it's an old memory. But we used to have those all the time, right? Cities which is burned to the ground.

49:46.000 --> 49:58.000
And that was it. London, of course, in 1666, Chicago in 1871, Baltimore, 1904, San Francisco in 1906.

49:58.000 --> 50:10.000
We used to have cities would like burn to the ground and now it's inconceivable that anything like that would ever happen. It's tough to even imagine how that might happen. So what changed?

50:10.000 --> 50:24.000
The Triangle Shirtways Company fire in 1911 was a horrible fire and a tragedy that inspired a public outcry which in turn sponsored provoked legislation about, you know, new building codes, new ways to design buildings.

50:24.000 --> 50:32.000
Fire sprinklers, emergency exit doors, having alleys, you know, this sort of thing.

50:32.000 --> 50:44.000
And, you know, I love telling this story because, you know, it's an example to me of the way that urban planners and architects and engineers quite literally designed a disaster out of existence.

50:44.000 --> 50:52.000
And, you know, our cities exists today because we adapted to that particular disaster.

50:52.000 --> 51:06.000
And I said that disaster is fundamentally optimistic because it is because we have a chance of deciding what to do. So, you know, is the future of our detection AI apocalypse, I think it just depends on what we design.

51:06.000 --> 51:19.000
So apologies for running a bit long, but thank you to everyone, and especially to my hosts, and love to start a discussion.

51:19.000 --> 51:27.000
Yeah, that was fabulous. Really fabulous, and don't apologize for taking so long.

51:27.000 --> 51:44.000
I think what, I mean, some of those notions I've kind of had before, but I've never been a systematic in investigating them through the lens of, you know, much more kind of like a structured analysis.

51:44.000 --> 51:50.000
I think it was, it was really very, very instructive.

51:50.000 --> 52:11.000
I just, there are a number of things that come up. I think that what I think what it points towards, I mean, the emphasis seems the background concern was the issue of cost that seems to be the driver in many ways, or the primary concern, certainly as far as the client

52:11.000 --> 52:25.000
is concerned. And it kind of in some senses echoed the Susskin's work on what's going to happen in the future in terms of the professions and the primary driver of change is going to be economics.

52:25.000 --> 52:39.000
People just want to get cheaper and cheaper and cheaper. And that is always kind of in a way, I think there is a background condition that makes us as a profession architects very vulnerable.

52:39.000 --> 52:51.000
And that is the lack of concern about cost. I wrote a long time back a book called The Anesthetic of Architecture, which is about how we tend to see things through a rose-tinted lens and just see them in terms of their beauty.

52:51.000 --> 53:02.000
And that rinses out often concerns about socioeconomic, political, environmental disasters, whatever. It just rinses that out, and we just look at it.

53:03.000 --> 53:18.000
And I mean, whatever. I mean, you could think about looking out over the Pacific Ocean and seeing this kind of pink mushroom cloud, which is clearly a kind of nuclear explosion, the French testing out their nuclear weapons over at Bikini et al.

53:19.000 --> 53:30.000
And look at it and say, wow, what a beautiful pink mushroom cloud kind of thing. And that is, I think, that is kind of endemic with the problem in some senses of architecture.

53:30.000 --> 53:44.000
We see things and say, wow, isn't it beautiful? And the client was thinking things about how much it costs. And the classic kind of issue is you come up with a design and my chair at Cambridge would come up with this as argument.

53:44.000 --> 53:54.000
And the client would say, but how much does it cost? Don't worry about that. If you get this, you'll get a beautiful building. And the problem is that is really the issue.

53:54.000 --> 54:06.000
The aestheticization runs through the architectural culture. And I think it also affects architects themselves in the sense that we are so happy doing beautiful designs, we don't worry too much about our own pay.

54:06.000 --> 54:30.000
In fact, we've got no say over that, really. There's nothing to to effectively protect fees, nothing at all. And so that becomes the kind of the crucial issue. And I think you're absolutely right in pinpointing or at least it seems to me the core of your argument was really about economics, about cost, supply and demand.

54:30.000 --> 54:46.000
And I totally agree with your golf club's kind of analogy. There are going to be no more buildings required. So don't assume that you're going to be employed more in the future.

54:46.000 --> 54:50.000
Anyway, maybe I'll just put that comment in there if you want to.

54:50.000 --> 55:05.000
I agree. I would refine it a little bit and I'll tell you a story. I practiced architecture for five years commercially before going to graduate school and I went to do my master's in architecture and an MBA.

55:05.000 --> 55:14.000
And of course, as a young architect, we were always talking about cost because clients were always beating us over the head, like, this costs too much, this costs too much.

55:14.000 --> 55:31.000
And I adopted that common sentiment among architects that architects care about the building, the design, and clients care about the cost. And then I went to business school and it's just kind of like rapid fire, doing case studies and everything like that.

55:31.000 --> 55:39.000
And no one ever talked about cost. And I was like, what the fuck is going on? Because I thought I would go to business school and we would just be talking about this.

55:40.000 --> 55:51.000
And I think that's where I developed my sensibilities about value. Value being the difference between what something is worth and what something costs in there.

55:51.000 --> 56:08.000
And, you know, we make decisions about what we buy based on the differential, right, you know, the relative value of two things. So when you go out to buy shoes, you know, you don't buy like the best pair of shoes that's ever been invented.

56:08.000 --> 56:19.000
You don't buy like the least expensive shoes that's ever been invented. You look at that. And I think that to what you said about the sort of viewpoint difference between architects and their clients.

56:19.000 --> 56:26.000
That gets amplified because, you know, from an architect's perspective, the cost is immaterial because they're not paying for it.

56:26.000 --> 56:37.000
The worth of the building is very much tied up in its aesthetics. So, you know, we want it to be beautiful to us because that is good for us.

56:37.000 --> 56:48.000
But the worth and cost equations are completely different for the client. One, they're paying the cost. And two, you know, the worth is it encompasses things other than just aesthetics.

56:48.000 --> 56:59.000
I mean, there's rental payments from tenants. You know, there's tax like, you know, I mean, there's all sorts of other things going on. But I think that, you know, you're absolutely right.

56:59.000 --> 57:10.000
And thank you for bringing up Suskin too. You know, they've been hugely influential in my work as well. Because, you know, fundamentally, you know, you may love architecture.

57:10.000 --> 57:21.000
You may be the best architect in the world. But when alternative technologies create a solution that is 90% is good, but 10% of the cost, like that, that's all she wrote.

57:21.000 --> 57:27.000
I mean, like 99% of clients are just going to flock over there. And you would do the same thing, you know.

57:27.000 --> 57:36.000
Yeah, I would just add to that that I think that the questions of aesthetics, we assume that everyone shares our aesthetic. We assume that.

57:36.000 --> 57:53.000
But actually, they don't. I mean, I don't know. I've got a relative who I had a pair of Nike yellow shoes and I went into the Sahas office and Patrick said, Where did you get those? I want a pair of those.

57:53.000 --> 58:04.000
And I got some flak when I went to one of my relatives homes. What are you wearing yellow shoes for? You're not a teenager anymore kind of like, you know.

58:04.000 --> 58:19.000
I think this is something that is and it's actually in some senses. So we assume everyone's going to share that aesthetic and they, they, you know, I think, I would say that postmodernism has meant that we kind of more aware of appearances and so on, I would say, but on the whole, no one actually

58:19.000 --> 58:33.000
quite shares that same aesthetic. And to my mind, this was the tragedy of the Bauhaus in some senses that they thought they would transform society, give the modern citizen what they wanted.

58:33.000 --> 58:49.000
But the problem was it was, it was expensive. You know, it was, you know, if you go to the Bauhaus now in Dessau, you'll see all these beautiful kind of whatever they are, designer objects, or you find them anywhere, you find them in museum shops all over the world and fine if you've got the money to pay for your

58:49.000 --> 59:05.000
start lemon squeezer or whatever, but actually, you know, it's actually expensive. Whereas Ikea, and I think there's a lot of lessons for us from Ikea, they produce something that actually did what Adolf Los claimed he wanted to do by reducing the

59:05.000 --> 59:21.000
ornament, you save on costs and produce something that is much cheaper. Of course, there are other kind of smart factors they introduced, I mean, flatbacking things, allowing you to assemble themselves and, and all that out.

59:21.000 --> 59:33.000
But this means that Ikea is now populating the living rooms of many, many people, but not because of the aesthetic, because of the fact that it's going to be cheaper and that's the best way to do it.

59:33.000 --> 59:45.000
So you do actually, and maybe you do begin to influence people's taste and they say I quite like Ikea or whatever, but you do it through a fundamentally different mechanism, I think that is absolutely central to how we operate.

59:45.000 --> 59:55.000
So I, I really appreciate that and the layered way that you address the question about cost itself.

59:55.000 --> 01:00:07.000
Let me just say, if anyone's got any questions, please put them in, so we've got a YouTube audience, we've got a Zoom audience, the YouTube ones that you can put in there into the chat and we can relay them here.

01:00:07.000 --> 01:00:24.000
I wanted to pick up on on something which I thought was, I mean, maybe as a way of kind of not just my work, but maybe something you could add into the mix and that is to say there was a recent study that was done by essentially a group of

01:00:24.000 --> 01:00:40.000
sociologists from Harvard, MIT, and the guy called Ethan Mollick who is from, I think, UPenn, was also part, he's an AI expert, I think he's at the business school there at UPenn.

01:00:40.000 --> 01:00:56.000
Anyway, they did a study on a consulting group, Boston Consulting Group, and what they were doing was attempting to measure, now we don't know how to measure these things, but sociologists do, to measure the impact of using AI.

01:00:56.000 --> 01:01:08.000
And they were able to come up with some result, they were able to quantify it in some way, and I think this is probably the first of many studies, I'm sure there'll be a lot of them.

01:01:08.000 --> 01:01:28.000
And of course they weren't necessarily dealing with the design as such, although that was vaguely included in their study because they asked this group to suggest a new line of footwear,

01:01:28.000 --> 01:01:42.000
responding in some way to whatever, no, dot, dot, dot, another line. And so it was kind of creativity in very loosely was part of that thing. They were using chat GPT to study this thing.

01:01:42.000 --> 01:01:54.000
But what they came up with, I think was really, really interesting, was a series of quite significant factors and there is this very crude graph that Ethan Mollick has published online.

01:01:54.000 --> 01:02:02.000
The whole thing was published as part of a Harvard Business School journal, but he made his own graph and so on, which puts it very graphically right up.

01:02:02.000 --> 01:02:15.000
And he says, and what they found basically, in the study, using AI, it was a group using control group, using AI versus a control group, not using AI.

01:02:15.000 --> 01:02:30.000
And those that were using AI finished 12.2% more tasks, so they achieved more. What's more, they completed those tasks at 25.1% quicker.

01:02:30.000 --> 01:02:39.000
Now, I guess you've got to multiply those to get the real impact. You're doing more tasks and you're doing them quicker. So the overall impact is actually more significant than that.

01:02:39.000 --> 01:02:46.000
And they also came up with the figure, which I think is difficult to evaluate, but it was 40% better quality.

01:02:46.000 --> 01:02:57.000
Now, that's anyway, you put those together, those three factors are quite significant. And as you say, it's a question really ultimately the task.

01:02:57.000 --> 01:03:06.000
And to something that I do, I mean, that is a factor in the sense that there are those differences between certain tasks and others.

01:03:06.000 --> 01:03:12.000
For example, your comparison must be the task and the job itself or the profession.

01:03:12.000 --> 01:03:27.000
I think that renderers are in a lot of trouble right now. There are a number of tools out there you put in a sketch. I mean, famously, Tim Foo was using look X and he put in a crumpled piece of paper and looked at it and produced a Gehry building,

01:03:28.000 --> 01:03:40.000
based on that, and each one was, and you can do that now. So, so I think certain, clearly, there is a differential in there, one has to take that into account.

01:03:40.000 --> 01:03:56.000
But very, very significant, and it will be interesting to find out what that figure was. I once was talking about this as with one of the developers AI soccer, I would mention her name, but she once said, and she wanted to attract it.

01:03:56.000 --> 01:04:09.000
Some person using AI could achieve as much as five, not using AI. And I, it's a little bit of a, and she was worried about that information getting out there because it might put off people from using AI.

01:04:09.000 --> 01:04:19.000
But nonetheless, there is, there is a significant difference in what you can do. So I don't think the quality necessarily is going to be that important in terms of aesthetic, shall we say.

01:04:19.000 --> 01:04:30.000
Although, I mean, I think that you can judge quality in other terms. And I think these are really hugely significant factors. And we need to do a study, I think, in architecture for that.

01:04:30.000 --> 01:04:32.000
Let it go.

01:04:32.000 --> 01:04:35.000
I mean, okay.

01:04:35.000 --> 01:04:45.000
Any, any, any, any pops, I've got a few more, a couple more points. We'll open up. We've got some questions coming in from that from from YouTube as well. So, but, yeah.

01:04:45.000 --> 01:05:05.000
So, anyway, I think there's, there's, there's the concern that's kind of connected in some way to, to, to, to all those issues. The one thing that I think that I would question about your approach and, and, and, well, I don't mean critical but I think one,

01:05:05.000 --> 01:05:18.000
maybe suggestion. And you mentioned the law of diminishing returns and that's course was Kurtz while what he followed and a lot of people are following on from Moore's law.

01:05:18.000 --> 01:05:31.000
Now, for those of you don't know Moore's law, Gordon Moore was an industrialist who was made a comment. It wasn't the law as such but made a comment back in the 60s on observation shall we say that that in terms of circuit boards,

01:05:31.000 --> 01:05:44.000
the number of conductors, the transistors on the circuit board would double every two years and the price would come down by half, which meant that this was exponential change.

01:05:44.000 --> 01:05:57.000
So, if it's, if it's that factor, you would be considered going one, two, three, four, five, you'd go one, two, four, eight, 16, and there's a huge difference between five and 16.

01:05:57.000 --> 01:06:09.000
It's exponential change and that has been applied more recently to these large language models and I want to mention those again in a second because I think these are absolutely hugely significant.

01:06:09.000 --> 01:06:24.000
And Pishai, the CEO of Google, made the comment that these large language models are increasing in their capabilities, going faster than Moore's law, and that was the comment he made.

01:06:24.000 --> 01:06:34.000
I don't think that Moore's law is relevant in this context because it's tying it to some kind of production and the kind of economics.

01:06:34.000 --> 01:06:43.000
Now, the key question when it comes to capabilities is, in my view anyway, the speed of learning, the speed of learning.

01:06:43.000 --> 01:06:53.000
Now, this is something that Geoffrey Hinton comments on and Geoffrey Hinton is known largely as the Godfather of AI.

01:06:53.000 --> 01:07:03.000
Certainly he was the one who was promoting, who was working on neural networks and what we call now deep learning at a time when that approach was out of favor.

01:07:03.000 --> 01:07:10.000
It wasn't working, everyone abandoned it in favor of a different logic, symbolic AI based on logic and so on.

01:07:10.000 --> 01:07:15.000
But really all it required was a change in terms of the technology.

01:07:15.000 --> 01:07:26.000
And as soon as we got GPUs invented and suddenly the capabilities of computers and the speed of computers was vastly, vastly increased and these neural networks worked.

01:07:26.000 --> 01:07:28.000
So that's a background to him.

01:07:28.000 --> 01:07:37.000
But he has made the comment recently and he's been in the news because he resigned from Google in order to sound a warning.

01:07:38.000 --> 01:07:47.000
And his warning basically, or part of his warning, was the speed at which computers could learn.

01:07:47.000 --> 01:07:59.000
Because the way that it works, he said, and I need to look into this further, is that it shares information with a thousand other computers at once.

01:07:59.000 --> 01:08:06.000
And I don't know why he mentioned a thousand, not a million, whatever, but it shares information with a thousand computers at once.

01:08:06.000 --> 01:08:10.000
Whereas if I'm sharing information with you, it's a one on one.

01:08:10.000 --> 01:08:13.000
But a thousand to a thousand others.

01:08:13.000 --> 01:08:16.000
Now, that somehow reminds you of COVID in some senses.

01:08:16.000 --> 01:08:21.000
If you share COVID, the key issue was what they call the R-Ratio.

01:08:21.000 --> 01:08:24.000
If it was more than one, be worried.

01:08:24.000 --> 01:08:29.000
If it was less than one, you're kind of okay because the spread is going down, whatever.

01:08:29.000 --> 01:08:32.000
Now, we never had an R-Ratio of a thousand.

01:08:32.000 --> 01:08:34.000
Now, I'm not even sure if that figure is correct.

01:08:34.000 --> 01:08:36.000
But this is the real problem.

01:08:36.000 --> 01:08:42.000
It is the capabilities of these large language models that is exploding.

01:08:42.000 --> 01:08:48.000
And it's exploding at a rate that we cannot even conceive.

01:08:48.000 --> 01:08:51.000
Maybe I'll just throw that out to you, Eric, and see what's going on.

01:08:51.000 --> 01:08:53.000
Sure, sure. Let me react to a few things.

01:08:53.000 --> 01:09:00.000
First, the diminishing returns diagram and the thread there.

01:09:00.000 --> 01:09:05.000
That's not actually, that's irrespective of AI or no AI.

01:09:05.000 --> 01:09:09.000
You know, the diminishing returns is just an economic phenomena that exists either way.

01:09:09.000 --> 01:09:12.000
So that's separate.

01:09:12.000 --> 01:09:14.000
The Moore's Law thing, yes.

01:09:14.000 --> 01:09:20.000
And we need to get the word out about that because I think there's still people who think that Moore's Law is relevant.

01:09:20.000 --> 01:09:23.000
It's not, things are moving much, much faster than that.

01:09:23.000 --> 01:09:27.000
I mean, Kurzweil puts it at a double exponential or triple exponential.

01:09:27.000 --> 01:09:34.000
I mean, these are growth rates that are very hard for people to understand, like intuitively.

01:09:34.000 --> 01:09:36.000
And I don't mean like people other than me.

01:09:36.000 --> 01:09:39.000
I mean, I have a hard time, like the human brain has a hard time.

01:09:39.000 --> 01:09:41.000
Here's a benchmark that I always use.

01:09:41.000 --> 01:09:46.000
Stanford's whatever annual AI survey that they do.

01:09:46.000 --> 01:10:02.000
They put the doubling of artificial intelligence capability, rolling in, you know, algorithmic development, advances in cloud computing, like all the things like the raw capability of AI, doubling every four and a half months.

01:10:02.000 --> 01:10:08.000
Now, if something's doubling every four and a half months, that's a million X in seven years.

01:10:08.000 --> 01:10:17.000
So when people say, hey, you know, AI is like, there's nothing to be afraid of insulting that AI could actually like, you know, ape the performance of a human designer.

01:10:17.000 --> 01:10:18.000
I agree.

01:10:18.000 --> 01:10:28.000
I'm like, yeah, you know, at this point, but do I believe that an AI that's a million times more powerful than what we have today might be capable of designing a building.

01:10:28.000 --> 01:10:29.000
Yes.

01:10:29.000 --> 01:10:30.000
Yes, I do.

01:10:30.000 --> 01:10:33.000
And I'm very, very concerned about it.

01:10:33.000 --> 01:10:41.000
So, yeah, I mean, I think that, well, you just deconstructed another myth, Neil, you know, this myth around Moore's law.

01:10:41.000 --> 01:10:57.000
I think we have a responsibility to kind of get the word out that some of this, some of these polyanish positions are based on, you know, data and ideas that are actually outdated.

01:10:57.000 --> 01:10:59.000
I think I'll just throw that back at you.

01:10:59.000 --> 01:11:04.000
I mean, I love that video, the 24 minute video.

01:11:04.000 --> 01:11:07.000
But it struck me that it doesn't have to be 24 minutes.

01:11:07.000 --> 01:11:14.000
And I base this on, I mean, it's what you're doing basically is you're recording a conversation.

01:11:14.000 --> 01:11:21.000
And which and you're, you're showing it between these, these AI agents and.

01:11:21.000 --> 01:11:32.000
The 24 minutes space is basically taken up in showing them talking to one another, whereas actually the speed of the operations could be pretty instantaneous.

01:11:32.000 --> 01:11:41.000
Now, I was alarmed by the, what we now know as large language models and their speed of operations.

01:11:41.000 --> 01:11:52.000
Several years ago, in the world of AI, one of the great moments when we kind of a wake up call it was called a Sputnik moment.

01:11:52.000 --> 01:11:59.000
When Sputnik basically was a wake up call for the American, the Americans in the space race and let the foundation of NASA.

01:11:59.000 --> 01:12:06.000
I mean, holy shit, the Soviets were suddenly sending a satellite to orbit and America was nowhere near that.

01:12:06.000 --> 01:12:07.000
And that was a wake up call.

01:12:07.000 --> 01:12:10.000
So in terms of your kind of like, there's something good that comes out of it.

01:12:10.000 --> 01:12:13.000
Well, NASA came out of that particular moment.

01:12:13.000 --> 01:12:20.000
Anyway, this particular game, it was, and we were not, it wasn't really on our radar because it was a game of go, a match of go.

01:12:20.000 --> 01:12:22.000
And we don't really play go, but they do in Asia.

01:12:22.000 --> 01:12:32.000
And there was this match AlphaGo developed by DeepMind of London versus Lisa Doll, who was kind of this followed on from the Gary Kasparov chess match.

01:12:32.000 --> 01:12:35.000
And he was the kind of Gary Kasparov, shall we say, of go.

01:12:35.000 --> 01:12:37.000
And AI trounced him.

01:12:37.000 --> 01:12:41.000
And that was the wake up call for all go playing nations.

01:12:41.000 --> 01:12:50.000
And the Chinese immediate president, she said, okay, by 2030, we are going to catch up with the Americans and overtake them and so on and so on.

01:12:50.000 --> 01:12:56.000
So that was kind of like a hugely sort of significant moment in sort of in wake up.

01:12:56.000 --> 01:13:00.000
But the more important one was something that wasn't really mentioned at all, which was a follow up.

01:13:00.000 --> 01:13:05.000
And the follow up was this was the next model of AlphaGo, which is AlphaGo Zero.

01:13:05.000 --> 01:13:10.000
But first of all, it wasn't documented, it wasn't on TV and whatever, but it was, it went out on our radar.

01:13:10.000 --> 01:13:13.000
But that one, it beat AlphaGo 100 games to zero.

01:13:13.000 --> 01:13:14.000
Right.

01:13:14.000 --> 01:13:16.000
That's better.

01:13:16.000 --> 01:13:24.000
But the important thing was it learned to do things without being trained to do so.

01:13:24.000 --> 01:13:26.000
It was not taught the rules of go.

01:13:26.000 --> 01:13:31.000
And this talks about the emerging capabilities, which we should come back to in a moment.

01:13:31.000 --> 01:13:36.000
But the other aspect of how it looked, it was playing games of go against itself.

01:13:36.000 --> 01:13:41.000
And I think the total was 4.9 million over four over three days.

01:13:41.000 --> 01:13:42.000
Yeah.

01:13:42.000 --> 01:13:43.000
And that sounds like a lot.

01:13:43.000 --> 01:13:45.000
That sounds like a lot.

01:13:45.000 --> 01:13:47.000
But the real point is, it is art.

01:13:47.000 --> 01:13:52.000
I mean, you go down to it, you know, it's basically it's 20 games of go per second.

01:13:52.000 --> 01:13:54.000
Now that is very, very significant.

01:13:54.000 --> 01:13:55.000
It is weak.

01:13:55.000 --> 01:13:56.000
It's mind boggling me fast.

01:13:56.000 --> 01:13:57.000
Yeah.

01:13:57.000 --> 01:14:00.000
And I would say, and I didn't know why it took so long, frankly.

01:14:00.000 --> 01:14:03.000
I mean, how could you even take, you know, that much.

01:14:03.000 --> 01:14:09.000
So my point would be with that video is the actual calculations were pretty instantaneous.

01:14:09.000 --> 01:14:12.000
And it wasn't 24 minutes of calculations.

01:14:12.000 --> 01:14:16.000
I mean, that was what it took to show the operations happening in terms of discourse,

01:14:16.000 --> 01:14:20.000
shall we say, but the calculations were pretty instantaneous.

01:14:20.000 --> 01:14:22.000
And that really is.

01:14:23.000 --> 01:14:24.000
Anyway, that was what it might be.

01:14:24.000 --> 01:14:28.000
My comment on the video, which I thought was a fabulous video anyway.

01:14:28.000 --> 01:14:30.000
No, I mean, I think you're right.

01:14:30.000 --> 01:14:35.000
I mean, it's, you know, there's something evolutionary about it, right?

01:14:35.000 --> 01:14:37.000
You know, we just, we have trouble.

01:14:37.000 --> 01:14:41.000
The human brain has trouble like kind of wrapping our heads around, you know,

01:14:41.000 --> 01:14:43.000
exponentials and these sorts of things.

01:14:43.000 --> 01:14:47.000
So, you know, we got to support each other and like check each other and say, like, yeah,

01:14:47.000 --> 01:14:50.000
that whole 24 minute video could have been under a second.

01:14:50.000 --> 01:14:55.000
You know, if it was just two AIs, you know, talking to each other and designing a building

01:14:55.000 --> 01:14:57.000
or something like that.

01:14:57.000 --> 01:15:00.000
In my own writing, I refer to this as the million monkey problem, right?

01:15:00.000 --> 01:15:03.000
Where we've all heard that adage about, you know,

01:15:03.000 --> 01:15:07.000
if a million monkeys were banging on a million typewriters for a million years,

01:15:07.000 --> 01:15:10.000
would they at some point write, handle it?

01:15:10.000 --> 01:15:13.000
And, you know, maybe you think, yes, maybe you think, no,

01:15:13.000 --> 01:15:17.000
but a billion monkeys working on a billion typewriters for a billion or a trillion.

01:15:17.000 --> 01:15:20.000
You know, those are the scales that we're into now.

01:15:20.000 --> 01:15:26.000
So, yeah, I think we need to be very, very concerned.

01:15:26.000 --> 01:15:33.000
And it's interesting that you brought up the Go match because that was one of my initial inspirations

01:15:33.000 --> 01:15:37.000
getting started on this project because, you know, there were architects in the ether who were saying,

01:15:37.000 --> 01:15:39.000
no, like, you can't compete with architecture.

01:15:39.000 --> 01:15:41.000
Architecture is just like too complicated.

01:15:41.000 --> 01:15:45.000
And like, you know, AI is like beating world champions ago.

01:15:45.000 --> 01:15:49.000
It's doing protein folding, you know, it's solving all these like cancer research problems.

01:15:49.000 --> 01:15:52.000
And, you know, architecture is plenty, plenty complicated,

01:15:52.000 --> 01:15:56.000
but I'm not sure that it's more complicated than everything, you know,

01:15:56.000 --> 01:16:00.000
like it's not like the last thing that that AI is going to figure out.

01:16:00.000 --> 01:16:03.000
So, yeah, yeah, we got a big job.

01:16:03.000 --> 01:16:07.000
Yeah, no, so one more point for me and then we'll open up to the other questions in the audience.

01:16:07.000 --> 01:16:09.000
But, you know, I absolutely totally agree.

01:16:09.000 --> 01:16:14.000
Also, you're picking up on what are called emergent capabilities or emergent abilities.

01:16:14.000 --> 01:16:18.000
I think these are things that are hugely significant.

01:16:18.000 --> 01:16:24.000
And I was intrigued by what you discovered in terms of what AI had learned how to do, you know.

01:16:24.000 --> 01:16:31.000
So just to say for those who don't know, I mean, I'm not sure if the term emergence is intended to be taken the way that I've taken it.

01:16:31.000 --> 01:16:38.000
But I, in my work years ago on swarm intelligence, and I came across the term emergence,

01:16:38.000 --> 01:16:44.000
which is about that Steve, that John Holland initially kind of written about as a kind of principle,

01:16:44.000 --> 01:16:52.000
whereby things start emerging out of any multient system that are unpredictable and not expected.

01:16:52.000 --> 01:16:59.000
And classically, you think about a flock of birds in the way that it produces this area, acrobatics.

01:16:59.000 --> 01:17:06.000
But it's also been taken more recently to apply these to these mysterious abilities that go back to that alpha go zero.

01:17:06.000 --> 01:17:09.000
It taught itself to play go.

01:17:09.000 --> 01:17:16.000
And, you know, that the right that that was when the alarm bells should have been sounding because now we're discovering through these large language models.

01:17:16.000 --> 01:17:19.000
It is developing similar capabilities.

01:17:19.000 --> 01:17:22.000
It can learn languages and translate.

01:17:22.000 --> 01:17:23.000
That is pretty astonishing.

01:17:23.000 --> 01:17:28.000
It does it. It does it actually. I mean, emergency stuff is kind of a rather mysterious thing.

01:17:28.000 --> 01:17:29.000
And we can observe it.

01:17:29.000 --> 01:17:31.000
We can't really explain it scientifically yet.

01:17:31.000 --> 01:17:34.000
But nonetheless, we can see this thing happening.

01:17:34.000 --> 01:17:37.000
Now it can learn to translate and it can learn to write code.

01:17:37.000 --> 01:17:39.000
Oh, that's that's very.

01:17:39.000 --> 01:17:40.000
That's huge.

01:17:40.000 --> 01:17:41.000
Yeah.

01:17:41.000 --> 01:17:49.000
So I got a friend from college who was who used to run the biggest translation agency in the world.

01:17:49.000 --> 01:17:55.000
And he sold his company two years ago because he could see that I was going to be able to do it.

01:17:55.000 --> 01:18:07.000
And I've been exploring that myself in terms of the translation of some of my books and it's incredibly cheap, very fast, even with a human editor to come in and go and check all the things and so on.

01:18:07.000 --> 01:18:16.000
So there's there's there's that but I mean, it's so it has this capacity to do these things and I claim that it's very imperfect moment.

01:18:16.000 --> 01:18:21.000
And I would claim though that there are moments in which it has learned how to design.

01:18:21.000 --> 01:18:32.000
It seems to have learned that the rules of composition in the sense of mid-journey and Dali, particularly mid-journey, it's coming up with some pretty impressive designs, not everyone.

01:18:32.000 --> 01:18:35.000
I mean, about only one in about 50 is really good.

01:18:35.000 --> 01:18:39.000
But nonetheless, it is surprising that it's doing those things.

01:18:39.000 --> 01:18:49.000
I could only assume there are a lot of other things that we don't know what it's doing, but it's doing, you know, it's looking at the data and understanding systems and putting them together.

01:18:49.000 --> 01:18:59.000
I actually put this question to when you heard yesterday, I said, Well, what about I mean, could it not also learn how you do the plumbing or how you do, you know, other aspects of architectural design.

01:18:59.000 --> 01:19:05.000
And she was saying, well, the difficulties, the three dimensions, that's when errors start creeping in.

01:19:05.000 --> 01:19:08.000
And it's, it's fine with language.

01:19:08.000 --> 01:19:11.000
I mean, language, it doesn't matter too much of your slightly out.

01:19:11.000 --> 01:19:12.000
There's a lot of tolerance there.

01:19:12.000 --> 01:19:15.000
But with architectural drawings and things, that is the challenge.

01:19:15.000 --> 01:19:16.000
It's not there yet.

01:19:16.000 --> 01:19:23.000
But I do think I do think that these emerging capabilities are, I mean, they're fascinating.

01:19:23.000 --> 01:19:30.000
And they don't sound particularly interesting, not indeed to large language models, but they are extraordinary.

01:19:30.000 --> 01:19:40.000
And just one final point for the audiences, the large language models, they, they get these abilities, not through the sophistication of the code, because the code is quite straightforward.

01:19:40.000 --> 01:19:46.000
It's based on the size of these things and the larger they get, the more they seem to manifest these things.

01:19:46.000 --> 01:19:48.000
So I was just intrigued.

01:19:48.000 --> 01:19:54.000
Maybe you'd like to comment just briefly again on, on what you discovered it was able to do that you had never predicted.

01:19:54.000 --> 01:19:56.000
Yeah, you want to hear the wildest?

01:19:56.000 --> 01:19:58.000
Yeah, yeah, yeah.

01:19:58.000 --> 01:20:08.000
I mean, the windows and the bathrooms and the constant, like that, that surprised me, you know, that it was able to kind of get all of that right.

01:20:08.000 --> 01:20:20.000
The biggest thing that I did not put in the presentation was that I did find that it had some kind of 3D spatial world building capability.

01:20:20.000 --> 01:20:34.000
So after, you know, Carla had designed the house, you know, I drew it out, you know, I drew the floor plans as I would have drawn them based on, you know, what she was talking about and, you know, the local conditions and things like that.

01:20:34.000 --> 01:20:46.000
And then I described it to chat GPT and said, you know, if I go into, you know, the front hallway and make a left and go up the stairs and make a right, what room am I in?

01:20:46.000 --> 01:20:49.000
And it was like bedroom.

01:20:50.000 --> 01:21:06.000
And it would get that right. So in the way that you might communicate it to an unsighted person or something like that, you know, I was able to kind of articulate directions like around the house and, you know, across like multiple floors and things like that.

01:21:06.000 --> 01:21:15.000
And it would seemingly understand like where it was in, in some three dimensional mental model.

01:21:15.000 --> 01:21:31.000
So that's an artifact of just like the language. So, I mean, to us, you know, if I described to you a building like over the telephone and said, you know, these are the mentions of the building like you could sit down and draw it out and go vice versa.

01:21:31.000 --> 01:21:35.000
So, you know, maybe it's not.

01:21:35.000 --> 01:21:39.000
Yeah, I don't know where it comes from. I haven't figured it out, but it's scared the shit out of me.

01:21:39.000 --> 01:21:48.000
I'm just, just an aside, I want to go to some of the questions and yeah, yeah, let's do it in a moment. I want to ask Mitra, but just as an aside, I think the obvious point that needs to be reinforced.

01:21:48.000 --> 01:22:01.000
Traditionally, we are the ones that interpret the verbal instructions of the client and produce an image, which is exactly what mid-journey and dali do.

01:22:01.000 --> 01:22:19.000
And that's kind of worrying. I don't know. Mitra has got a question. I don't know if you're able to use your microphone and to read it out, or if not, if you can, if you can unmute yourself, or I can read it for you.

01:22:19.000 --> 01:22:26.000
I don't know whether you, I don't know if you've got a microphone on your computer.

01:22:27.000 --> 01:22:35.000
I can see you're still muted. Oh, did someone have to allow her to unmute themselves? Okay. Okay, Mitra.

01:22:35.000 --> 01:22:37.000
Hello, hello everyone.

01:22:37.000 --> 01:22:38.000
Hi.

01:22:38.000 --> 01:22:42.000
Hi. Thank you so much for the great lecture.

01:22:42.000 --> 01:22:54.000
Actually, I have a question about, actually, we have these days we are facing an increase in the use of the social networks.

01:22:54.000 --> 01:23:21.000
So the analysis of invisibility of the data and social network could be significant. For example, in urban studies, we have the geolocation data, including the GPS or data about connection with local Wi-Fi equipment.

01:23:21.000 --> 01:23:38.000
So my question is, would it be possible to collect data from social networks for architects? And where is the data stored?

01:23:38.000 --> 01:23:45.000
Would it be possible to collect data from social networks for what, for design purposes?

01:23:45.000 --> 01:23:46.000
Yeah.

01:23:47.000 --> 01:24:01.000
I imagine that's much more of a social and legal problem than it is a technological one. You know, your social networks already know everything about where you are and what you're doing and how fast you're moving and what stores you visit and all those other things.

01:24:02.000 --> 01:24:19.000
Because, you know, you signed off on a license agreement that allows them to have and use that data. I think in order for architects and designers to use it at other scale, at any significant scale, we'd have to have a similar sort of arrangement.

01:24:19.000 --> 01:24:32.000
I mean, the idea of a municipality collecting that data on its citizens for use by designers frightened me a little bit because they would almost certainly use it for other things.

01:24:32.000 --> 01:24:37.000
But I've seen a use to brilliant effect actually in disaster zones.

01:24:37.000 --> 01:24:43.000
After the Haiti 2010 earthquake, Port-au-Prince was a city of about three million people, sorry.

01:24:43.000 --> 01:24:58.000
And once the earthquake struck, you know, it destroyed most of Port-au-Prince and, you know, people fanned outward, you know, from the city and then at varying rates started to come back as recovery progressed.

01:24:58.000 --> 01:25:07.000
And did you sell the main phone carrier there actually had that data as a result of like, you know, having everybody's SIM card in a database.

01:25:07.000 --> 01:25:14.000
So, you know, we could understand how quickly, you know, people were moving back and into what neighborhoods and this sort of thing.

01:25:14.000 --> 01:25:20.000
So, I mean, from an urban and from a design perspective, that data is enormously useful.

01:25:20.000 --> 01:25:29.000
I think we just have to make the case that we should have it as designers and that we can put it into good use.

01:25:29.000 --> 01:25:38.000
Just to decide, Eric, the, I think the one of your disasters, just maybe you didn't know this, but the Great Fire of London.

01:25:38.000 --> 01:25:43.000
Actually, it came off the Great Plague, and it actually effectively got rid of the Great Plague.

01:25:43.000 --> 01:25:47.000
So maybe you had two disasters on one counter to the other.

01:25:47.000 --> 01:25:51.000
So we've got a number of questions from YouTube.

01:25:51.000 --> 01:25:55.000
And I think these are separate questions.

01:25:55.000 --> 01:26:02.000
First is you can see them in the chat if you want to look at them from Mandu Tiger in YouTube.

01:26:02.000 --> 01:26:04.000
Some excellent points.

01:26:04.000 --> 01:26:13.000
Unfortunately, another influencing factor for the long term is this rising level of indifference that is poisoning creativity and quality.

01:26:13.000 --> 01:26:20.000
Two huge values architects provide.

01:26:20.000 --> 01:26:27.000
Unfortunately, another influencing factor for the long term is this rising level of indifference that is poisoning creativity and quality.

01:26:27.000 --> 01:26:31.000
Two huge values architects provide.

01:26:31.000 --> 01:26:35.000
I would say, you know, Mandu Tiger, you're not wrong.

01:26:35.000 --> 01:26:42.000
There's certainly a surplus of indifference in the world these days.

01:26:42.000 --> 01:26:50.000
But I will reference my father and probably several debate coaches as well.

01:26:50.000 --> 01:27:08.000
You know, always advise me never to argue a position that requires that, that I believe or that you believe that my opponent is, is somehow stupid or morally corrupt or ignorant or something like that.

01:27:08.000 --> 01:27:11.000
Because it's an easy out, right?

01:27:11.000 --> 01:27:20.000
And I think what I think with architecture has nothing to do with architecture. It's all these other people and the fact that they don't care about this and they don't care about that.

01:27:20.000 --> 01:27:23.000
That may be true. It just doesn't go anywhere.

01:27:23.000 --> 01:27:35.000
I mean, it's kind of fatalistic for the profession, if you think about it, because, you know, if you imagine that, that architects are, you know, these passionate designers concerned with beauty and progress and, you know, everybody else is indifferent to such things

01:27:35.000 --> 01:27:43.000
as beauty and architect, you know, I mean, like, you need a world to pay fees to design things.

01:27:43.000 --> 01:27:53.000
So, I mean, I think that, you know, when I'm confronted with questions like that, you know, I asked myself, well, how do you, how do you make indifferent people different?

01:27:53.000 --> 01:27:58.000
Or, you know what I mean, they make indifferent people like actually care.

01:27:58.000 --> 01:28:03.000
You have to find the ways to speak in their language and to speak to their priorities.

01:28:03.000 --> 01:28:06.000
That's actually why I went to business school, you know.

01:28:06.000 --> 01:28:10.000
You know, I went to get an MBA and people thought I wanted to be a developer.

01:28:10.000 --> 01:28:13.000
I didn't. I never have.

01:28:13.000 --> 01:28:19.000
But it struck me as a good decision because I wanted to be able to defend our work in their language, right?

01:28:19.000 --> 01:28:25.000
Because when I was a practicing architect, you know, the whole design team would come up with all these great ideas and everything.

01:28:25.000 --> 01:28:30.000
And then, you know, at some meeting somewhere, there's some like 30-year-old with a clipboard and an Excel spreadsheet.

01:28:30.000 --> 01:28:33.000
And it's like, we're not doing this. We're not doing that.

01:28:33.000 --> 01:28:37.000
And I started to ask myself, well, who's really designing this building, you know.

01:28:37.000 --> 01:28:44.000
And I didn't have the vocabulary or the skill set to actually argue with that person in their language.

01:28:44.000 --> 01:28:47.000
So, I went to business school to learn that.

01:28:47.000 --> 01:28:50.000
So, Mandu, just to tie it off.

01:28:50.000 --> 01:28:57.000
I mean, I would suggest that you think about the people that you think are standing in opposition to design and ask yourself, like,

01:28:57.000 --> 01:29:04.000
how do I convince them that design is valuable in their language, you know, with their value system?

01:29:04.000 --> 01:29:06.000
Great, great point, Eric.

01:29:06.000 --> 01:29:09.000
I think that just a comment from the U.A.

01:29:09.000 --> 01:29:15.000
the studio discovered one of the successful ways of speaking to the client in their own language was to diagram things.

01:29:15.000 --> 01:29:20.000
That would be much more convinced by the diagrams than the designs themselves.

01:29:20.000 --> 01:29:22.000
Yeah, yeah.

01:29:22.000 --> 01:29:25.000
So, we have another question from the chat.

01:29:25.000 --> 01:29:29.000
Again, great, do you get a lot of praise here for this talk? It was a fabulous talk, very good.

01:29:29.000 --> 01:29:33.000
Great talk, and thank you. I have a question, but I want you to know more about your opinion.

01:29:33.000 --> 01:29:41.000
Do you think that there are safe havens against the AI and architecture disasters?

01:29:41.000 --> 01:29:43.000
Are they physical places?

01:29:43.000 --> 01:29:51.000
Should we, as architects, look for a way to design to make a sound from what AI could do against the profession?

01:29:51.000 --> 01:29:53.000
Yeah.

01:29:53.000 --> 01:29:55.000
Yeah, is there a name attached to that?

01:29:55.000 --> 01:29:58.000
No, there's no.

01:29:58.000 --> 01:30:01.000
Well, to whom ever asked that question.

01:30:01.000 --> 01:30:08.000
I think that architects fundamentally need to be thinking about expansion.

01:30:08.000 --> 01:30:12.000
And I think that exists along several axes.

01:30:12.000 --> 01:30:16.000
One is a kind of domain expansion.

01:30:16.000 --> 01:30:24.000
I think there's a great opportunity to actually reclaim a lot of the territory that we seeded during the 20th century to other professions.

01:30:24.000 --> 01:30:33.000
And we had to spin off interiors and landscape architecture and construction management, owners reps, and all these other things because shit got too complicated.

01:30:33.000 --> 01:30:37.000
And we also just wanted to spend time designing.

01:30:37.000 --> 01:30:44.000
And I think it's unfortunately eroded a lot of the authority of an architect in that overall design process.

01:30:44.000 --> 01:30:54.000
So I think there's an opportunity there to claw some of that back and to say, like, look, with these augmented tools, we can now do the construction management bit.

01:30:54.000 --> 01:30:57.000
We can do the interiors bit.

01:30:57.000 --> 01:30:59.000
We can do all these different things.

01:30:59.000 --> 01:31:04.000
Now, all those other professions are going to be saying the same thing, so we have to find a way to work that out.

01:31:04.000 --> 01:31:10.000
But ultimately, you know, we can expand in that direction from a domain standpoint.

01:31:10.000 --> 01:31:14.000
I think geographic expansion is another one.

01:31:14.000 --> 01:31:21.000
Most of the architects in the world are in places where the least architects are needed the least.

01:31:22.000 --> 01:31:29.000
The global south is going to need something like, I don't know, like a billion units of housing in the next 25 years.

01:31:29.000 --> 01:31:36.000
And there's not necessarily like the infrastructure there to support good design for all of them.

01:31:36.000 --> 01:31:47.000
And I think, you know, maybe technology offers a route to that so that, you know, someone born in a slum in Lagos now has access to good design.

01:31:47.000 --> 01:31:53.000
What does that mean? How do you accomplish that without, you know, fully like techno enabled colonialist strategy?

01:31:53.000 --> 01:31:55.000
I'm not really sure.

01:31:55.000 --> 01:32:02.000
But, you know, through my work, I know like most of the world really, really needs design and they can't get it.

01:32:02.000 --> 01:32:07.000
So hopefully that is another axis of expansion.

01:32:07.000 --> 01:32:11.000
The third one I'll mention is actually digital and metaversal.

01:32:11.000 --> 01:32:16.000
And I'm not a metaverse fanboy by any scrap of the imagination.

01:32:16.000 --> 01:32:22.000
But I do believe that the metaverse is inevitable that it's coming sooner or later.

01:32:22.000 --> 01:32:30.000
And that once it does, we're going to need a strategy to marry design strategies in the real world with the metaversal world.

01:32:30.000 --> 01:32:40.000
So if you're designing for a client like Nike, you know, they're going to have a metaversal store and they're going to have a regular store and augmented reality in their physical store.

01:32:40.000 --> 01:32:43.000
They're going to want all of that stuff to work together.

01:32:43.000 --> 01:32:53.000
Right. So how do architects start to think about digital experiences and how those complement, you know, physical spaces.

01:32:53.000 --> 01:32:57.000
So I mean, I think there's there's lots of that's just three, there's probably some more.

01:32:57.000 --> 01:33:07.000
But I think there's lots of ways that we can look at doing new things or doing old things or doing unprecedented things that haven't been invented yet.

01:33:07.000 --> 01:33:17.000
The part that makes me worry, the part that makes me think that we might be in a disaster is architects looking at these sorts of technologies as ways to just do what we're already doing just faster and cheaper.

01:33:17.000 --> 01:33:22.000
Like that's that's raised to the bottom. Like there's no good outcome there.

01:33:22.000 --> 01:33:28.000
So I think fundamentally, the safe place is everywhere except architecture.

01:33:28.000 --> 01:33:46.000
Like, look out, chart new territory, explore new civilizations, design housing for the moon, like, you know, whatever it is, like you got to go out there and use this technology that we've never seen before to do things that we've never done before.

01:33:47.000 --> 01:33:51.000
I was struck by your video.

01:33:51.000 --> 01:34:04.000
Conversation should actually would be great to see the whole thing because it was so interesting, but it's out there anyway for those who want to see was actually effectively the architect became the construction manager there he was talking to the to the construction person which is quite unusual.

01:34:04.000 --> 01:34:23.000
I mean, normally there's a very adversarial role and now increasingly in certainly in the UK, most contracts are designed built and the, and the developers in charge, you know, so we've surrendered that idea that we used to have of being the person that's what the word architect means literally from the ancient

01:34:23.000 --> 01:34:38.000
30 reminded us the person in charge and we've surrendered that but we could do that and I would also say we could also become the developer I think that's an area that is probably very, very well paid that we could also take over or indeed the person

01:34:38.000 --> 01:34:53.000
apparently selling the property gets the most money so we will go in terms of costs per hour. So the another question here in the chat from Ren right a Rainville at youth YouTube.

01:34:53.000 --> 01:34:59.000
Do you have any suggestions on how firms should prepare for this oncoming disaster.

01:34:59.000 --> 01:35:05.000
He has a second question but let's ask them first. Do you have any suggestions on how firms should prepare for this oncoming disaster.

01:35:05.000 --> 01:35:15.000
Yeah, I mean I think, well, building on what I said in my last response in terms of like looking outward to do to do new things and impressive things.

01:35:15.000 --> 01:35:33.000
I think education is critically important at this point, you know, I mean, I, I advise everybody calls me like, do your own homework, you know, read learn something and, you know, I think 90% of the architects read like the same like 10 design magazines like, you

01:35:33.000 --> 01:35:49.000
don't have to stop doing that but read some other things to, you know, I think most of my perspectives are informed by things that are actually going on in the tech sector and like the AI sector and like that's where I get my intel is like, you know, who's

01:35:49.000 --> 01:36:05.000
researching new research on machine learning and, you know, liquid neural nets and these sorts of things so that's how I personally keep an eye on what's happening and develop a sense of the future that I can feel comfortable with that, you know, I feel like I understand

01:36:05.000 --> 01:36:23.000
what might be coming. I think, I don't know, not disparaging any sort of design media but, you know, design could be a very conservative profession, you know, and I think it's the sort of thing where, you know, we're radically adventurous and our work right within the four corners of the drafting

01:36:23.000 --> 01:36:43.000
I mean, metaphorically at this point, you know, we create whole new worlds and new civilizations and things like that, but then tend to be like conservative about processing environment and like the rest of the things so I think that it would be good to educate oneself and to do your homework

01:36:43.000 --> 01:36:57.000
and to figure out what is going on outside of architecture because that's where the real action is at the moment and it's inducing a tidal wave that's going to, you know, hit architecture at some point.

01:36:57.000 --> 01:37:09.000
In terms of, you know, other like more tactical preparation I think that would probably depend on the specifics of, you know, a particular firm or a geography. So, should we go to part two, Neil?

01:37:09.000 --> 01:37:17.000
Yeah, yeah. Secondly, how best can architects direct the value conversation away from the client cost concern?

01:37:17.000 --> 01:37:31.000
Yeah, I mean, I think the first step there is to figure out what's your client values. And again, I mean, I think there's, there's an unfortunate, you know, mythology and architecture that thinks, oh, you know, the client only cares about cost.

01:37:31.000 --> 01:37:50.000
I've had clients like that. I've had clients like that. But clients are people and people tend to be more complex. So I think different clients value different things to varying degrees. So like, that's step one is knowing that.

01:37:50.000 --> 01:38:05.000
The second part is, you know, don't bring a hammer to a gunfight. You know, I mean, if your client is someone who values like cost exclusively, then you need to find a way to make that argument for value in economic terms.

01:38:05.000 --> 01:38:17.000
And you say, you know, we want to redo the lobby and don't talk about why from an architectural standpoint, it's going to be better. Talk about like, how you can charge like higher rents or like the cost comes down or something like that.

01:38:17.000 --> 01:38:30.000
You know, if you're working with a client that cares about some, you know, other social issue or political issue or something, you know, find out what that is and make arguments to people in their language.

01:38:30.000 --> 01:38:45.000
I mean, I think that's one of the, and Neil alluded to it earlier. I think that's one of the biggest kind of universal mistakes that architects make. We try and convince people to follow like whatever suggestion we're making using the language of architects, instead of their language.

01:38:45.000 --> 01:38:48.000
And it falls flat.

01:38:48.000 --> 01:38:50.000
A lot of times.

01:38:50.000 --> 01:38:58.000
Maybe I said, I don't want to, I'm talking too much, but I just add to that there was, there was occasion in Cambridge once, but there was a disaster.

01:38:58.000 --> 01:39:09.000
Many years ago, when there was a competition and one of the colleges decided to forget the results of the competition and just go to a builder and they got this really very, very tedious building as a result.

01:39:09.000 --> 01:39:24.000
But then they learned a different strategy. They discovered that actually if you mentioned you had a big name architect like foster and partners, whatever, it was much easier to attract funding from alumni by saying we have got so and so.

01:39:24.000 --> 01:39:33.000
And it became on their terms, it made sense it became an economic argument to improve the design so I very much agree with that.

01:39:33.000 --> 01:39:51.000
So we have James McBennett on on on YouTube, and his question which again is in the chat. Most buildings are not designed by architects, demand of buildings or demand for buildings is far greater than demand for well designed buildings.

01:39:51.000 --> 01:39:59.000
As the cost of design changes, surely blue ocean strategy in the middle.

01:39:59.000 --> 01:40:02.000
Yeah, I would absolutely agree.

01:40:02.000 --> 01:40:20.000
I mean, I think that, you know, my my humanitarian work was largely about bringing design to two communities and to people who otherwise would never have had access to it by virtue of geography or economics.

01:40:20.000 --> 01:40:41.000
Because I think you're pointing out James like that includes like, you know, most Europeans, most Americans like, you know, architectural services cannot be provided at a cross point, where, you know, it makes sense for someone who is, you know, got a $500,000 house budget to engage with an architect like that's just not going to happen.

01:40:41.000 --> 01:40:56.000
Yeah, as the cost of design comes down, does it then become possible to provide design services at, you know, the same level of quality but at a lower cost point.

01:40:56.000 --> 01:41:09.000
You know, I was teasing an architect the other day about creating a digital version of themselves, right, so they could actually like, you know, service service clients, you know, essentially at zero cost.

01:41:09.000 --> 01:41:23.000
And this sort of thing and, you know, that technology still probably a few years away for, you know, sort of full package solution but, you know, it's a taco watch like all of the ingredients are currently there the technologies exist.

01:41:23.000 --> 01:41:27.000
And they need to be assembled at some point in order to do that.

01:41:27.000 --> 01:41:44.000
But I think, you know, I love, I love that idea. I love the idea of like everybody having an architect, everybody getting the benefit. And one of the things that that's fueled my career and the thing that makes me so mad is that, you know, architectures is like great

01:41:44.000 --> 01:41:59.000
I mean, it's so beautiful, like so many brilliant and creative people and, you know, it's offered almost exclusively to, to the rich to the 1%, you know, and like, we can't, we can't get it out there to everybody else.

01:41:59.000 --> 01:42:13.000
And that's not because we suck. I mean, it's just because, like, it's, it's a lengthy and expensive process. So I hope that one of the things that, you know, architecture starts to embrace as these technologies unfold and design costs come down as

01:42:13.000 --> 01:42:18.000
we give design to everybody. Yeah.

01:42:18.000 --> 01:42:21.000
Actually, James has got a follow up question.

01:42:22.000 --> 01:42:40.000
By golf analogy, if 2% of buildings are designed by architects, and 2% of golfers can afford a full set of clubs, would more golf golfers justify buying a full set of clubs if the cost dropped dramatically.

01:42:40.000 --> 01:42:53.000
Okay, interesting. I think what's implied by this question is that there are people running around out there with like, two or three, like golf clubs, but don't have like a full set.

01:42:53.000 --> 01:43:00.000
So they go and play golf with like a couple of clubs and if the price of a set of clubs came down.

01:43:00.000 --> 01:43:03.000
They might actually buy the full set. Is that how you read it, Neil?

01:43:03.000 --> 01:43:04.000
Yes, absolutely.

01:43:04.000 --> 01:43:05.000
Okay.

01:43:05.000 --> 01:43:21.000
I don't play golf, so maybe I'm like out of my depth here, but I don't think most people like play golf with just like a few loose clubs. My understanding is that like, you need the full set in order to properly play a game of golf.

01:43:21.000 --> 01:43:25.000
And if that's wrong, someone in the chat, please correct me.

01:43:25.000 --> 01:43:34.000
I mean, I think to James is kind of like wider point, if the cost comes down to more people embrace it.

01:43:34.000 --> 01:43:39.000
Sometimes yes, for some goods, yes.

01:43:39.000 --> 01:43:47.000
You know, energy is the prototypical example, right? The more energy we make and the lower the cost, the more people consume.

01:43:47.000 --> 01:43:53.000
And there's some name for that kind of exception to the laws of supply and demand.

01:43:53.000 --> 01:44:05.000
I think that if the cost of design comes down, people may embrace it further and more people might be interested in utilizing the services of an architect.

01:44:05.000 --> 01:44:17.000
And the same necessarily applies to buildings, because like, you know, you've got a town of 30, 40,000 people, it needs a hospital, right, or at least like a regional medical center.

01:44:17.000 --> 01:44:28.000
It doesn't need four of them, right? I mean, we have to have some kind of justification to make the buildings that we make because they're expensive and they take a while and they're complicated.

01:44:28.000 --> 01:44:46.000
So, yeah, like I could see like a minor expansion and just to play on James's previous points, you know, like homeowners might have the opportunity to, you know, at the scale of a single family house for, you know, half a million dollars actually like work with an architect.

01:44:46.000 --> 01:44:54.000
On the other hand, you know, that brings in the specter of like a custom designed house of some kind. I don't know how that's actually going to play out.

01:44:54.000 --> 01:45:08.000
But James, to your point, yes, I think that probably there will be a slight expansion of the market in design services, but it will be constrained by fundamentally by the limits of the building market.

01:45:08.000 --> 01:45:29.000
There's another question in the chat with no name attached to it. I think that maybe the way we should think about architects within our ecosystem as a researcher, what is our role, what is our role as architects in the loop of research along with computation and big data?

01:45:29.000 --> 01:45:38.000
Will it help? Sorry, this is not very clear. Will it help us solve problems ahead? Let's read out what he said actually.

01:45:38.000 --> 01:45:42.000
Yeah, I'm reading it.

01:45:42.000 --> 01:45:55.000
What is our role of architects in the loop of research with computation? You think that means like the role of architects within the wider field of research on, you know, IT and data and computer science related issues?

01:45:55.000 --> 01:45:59.000
I think so.

01:45:59.000 --> 01:46:11.000
Well, I mean, I think, you know, architects have really amazing perspective and an amazing facility with certain skills that lend themselves very easily to research.

01:46:11.000 --> 01:46:22.000
So, you know, the ability to zoom in and zoom out really quickly and think small scale and big scale and be fluent and fluid between those things.

01:46:22.000 --> 01:46:25.000
Architects do that better than just about anybody I know.

01:46:25.000 --> 01:46:35.000
The ability to think about time, right? So, architect is one of those professions where you have to like make a decision today that's like binding for 30 years.

01:46:35.000 --> 01:46:43.000
And I think we lose sight of the fact that there are very few professions where that's true, you know, maybe medicine or law or engineering or something like that.

01:46:43.000 --> 01:46:57.000
But for the most part, like, you know, you fuck up at your job, like you've got lots of time to fix it with architecture, like your issues, your decisions get set in concrete, literally.

01:46:57.000 --> 01:47:04.000
So I think, you know, the mind of an architect lends itself very easily to that sort of thing.

01:47:04.000 --> 01:47:14.000
I think it's about, you know, integrating with what's already going on. I mean, there's already a lot of research going on into these things.

01:47:14.000 --> 01:47:22.000
So the question is, like, what is the entry point for, you know, architects who want to be involved in that?

01:47:22.000 --> 01:47:29.000
I think there's a lot of data issues around buildings and cities that we haven't quite figured out.

01:47:29.000 --> 01:47:46.000
We spoke about it earlier during the Q&A. But I've had the conversation many times over the past year about data and architects thinking, oh, well, we got all this data, like, from the buildings and, you know, we can use that in some sort of data science way.

01:47:46.000 --> 01:47:55.000
And, you know, my question is always like, you know, will it use it for what? Like, what data is going to be useful to anyone?

01:47:55.000 --> 01:48:05.000
If you're talking about data about how people use the design that you made, that seems like it would be the owner's data and not yours.

01:48:05.000 --> 01:48:14.000
So that's one problem. And if you're using the data from, you know, the designs that you made in the past, might be good.

01:48:14.000 --> 01:48:26.000
But unless you're like a Gensler or HOK or something or an AEcom, you probably don't have, like, enough projects there to actually build any sort of machine learning data set or anything like that.

01:48:26.000 --> 01:48:34.000
So, you know, I think architects have a natural role based on their, you know, psychology and their disposition.

01:48:34.000 --> 01:48:42.000
But I'm not sure what that role is. And I think we'll have to design it.

01:48:42.000 --> 01:48:52.000
There's another question from Vasco Ashik Vasco on YouTube, who's in Bangladesh. You should know that you've got an audience all over the world today.

01:48:52.000 --> 01:48:53.000
Awesome.

01:48:53.000 --> 01:49:09.000
In AI driven architecture, how do you tackle worries about losing human centric values and cultural nuances, especially in post disaster reconstruction where community identity is crucial?

01:49:09.000 --> 01:49:25.000
Yeah, and I'm reading the follow up comment, which I agree with, I would argue that a lot of buildings have already lost a human centric value. It did not take AI, it was modernism and effects of industrialization, post World War II prefab and building without ornament.

01:49:25.000 --> 01:49:32.000
So Ashik, I think that's a great question, Ren. I think that's a great answer. And similar to the one that I would have offered.

01:49:32.000 --> 01:49:43.000
Yeah, I mean, I think, you know, architecture loses human centric values all the time. And I think that's one of the reasons that a lot of people don't appreciate architecture because, you know, we spent 40 years making

01:49:43.000 --> 01:50:02.000
artificial architecture and, you know, kind of turning our attention away from the problems that people were actually dealing with in their lives. So, I mean, I think that, you know, how do you correct for that problem? How do you introduce human centered thinking?

01:50:02.000 --> 01:50:16.000
And my advice is always like, spend time with humans. You know, we, we don't always appreciate I don't think just how insular architecture is as a profession and how weird that is.

01:50:16.000 --> 01:50:28.000
You know, we take someone when they're 19 years old and we put them in studio. And frankly, a lot of architects never come out, you know, like mentally like they stay in studio for forever.

01:50:28.000 --> 01:50:38.000
And, you know, they come out, graduate, they know how to walk like an architect, talk like an architect, they, you know, have cool glasses and wear black and this sort of thing.

01:50:38.000 --> 01:50:47.000
But they don't necessarily like have human beings at the center of like whatever their design ambition is.

01:50:47.000 --> 01:51:01.000
You know, in my work, like it's impossible not to, you know, you can't, you can't go into a community of friendly loving people who are having a hard time and say like, okay, like I'm just going to ignore all that.

01:51:01.000 --> 01:51:17.000
I mean, not unless you're some kind of monster, I guess, but yeah, I mean, the point is, the point I'm not trying to make. I think I'm advising you to one, like spend more time with with people.

01:51:17.000 --> 01:51:30.000
And, you know, to appreciate the fact that we don't have much human centeredness to begin with. So maybe paradoxically, AI is the way that we find it.

01:51:30.000 --> 01:51:42.000
You know, I'm sure some of you have seen the same studies that I have where, you know, they measure the responsiveness of a human doctor and, you know, a medical chat GPT like, what is it?

01:51:42.000 --> 01:51:47.000
You know, they call poem one or something, the Google one, the medical one.

01:51:47.000 --> 01:52:03.000
And, you know, the respondents like drastically prefer interacting with the robot, because it's perceived as being like more caring and more attentive and it has, you know, infinite time and my doctor comes in to see me and like 30 seconds later, you know, he's gone like to do with another patient because

01:52:03.000 --> 01:52:21.000
some is looking stupid like that. But, you know, maybe this tool like retrieves some of our humanity by, you know, giving us time to be human and giving us time to to sit with patients and to sit with clients and things like that.

01:52:21.000 --> 01:52:25.000
So, yeah, I don't know that felt like a rambling response.

01:52:26.000 --> 01:52:27.000
It's a tough question.

01:52:27.000 --> 01:52:44.000
Rambling responses, Eric, fabulous. I just made me to follow up on that in a way. I mean, I had this last semester, I asked my, instead of asking my students to go and write essay, I got them to ask to do a video, but they went, I think, to chat GPT and got this.

01:52:44.000 --> 01:52:53.000
I think, I think you're right, because in your article, you mentioned the fact that the chat that chat GPT has been conditioned to respond in a certain way.

01:52:53.000 --> 01:53:09.000
And if I mean, I always go back to chat GPT is like, what, do you really mean that? And it kind of is open. Well, not really. But I mean, so, so, but the kind of responses that seemed to they seem to be getting was feeding into their videos was, well, AI lacks the empathy.

01:53:09.000 --> 01:53:12.000
It doesn't have the empathy of human beings.

01:53:12.000 --> 01:53:19.000
Now, I'm not sure that empathy actually in design necessarily makes a better design. I'm not sure about that at all.

01:53:19.000 --> 01:53:27.000
But what I would say is that that discussion you had in your video and I really would recommend everybody to have a look at that discussion.

01:53:27.000 --> 01:53:34.000
It was absolutely fabulous. They were much more polite than any human being was astonishing.

01:53:35.000 --> 01:53:39.000
Yeah.

01:53:39.000 --> 01:53:56.000
I mean, it's, it's fascinating because it's, you know, it's a customizable intelligence, right? So, you know, whatever chat GPT generally is, you know, if you needed a situation that that if you had a situation where you needed the empathy dialed up to 11,

01:53:57.000 --> 01:54:12.000
you could do that, right? And, you know, if you had some sort of alien intelligence designing buildings in Bangladesh and shout out to Bangladesh, by the way, I went to a conference there about seven years ago, one of the best weeks of my life.

01:54:12.000 --> 01:54:27.000
If you had, you know, an alien intelligence designing something, you know, you could pre-program that with like, you know, let's keep the colonial influences to a minimum and like not cover Bangladesh with, you know, international style architecture

01:54:27.000 --> 01:54:37.000
and this sort of thing, like whatever you do has to, you know, honor the traditional building practices and materiality and form making like present in Bangladesh architecture.

01:54:38.000 --> 01:54:54.000
You know, I mean, these models currently are overly programmed with Western influences, which is predictable because like they were created in the West, but I'm optimistic that that AI is capable of overcoming that problem and learning new tricks.

01:54:54.000 --> 01:55:06.000
No, I totally agree with that. I mean, I think one of the big issues people talk about AI is the bias, you know, and the bias, of course, comes from us. I mean, it's from the data that we're producing, and it's been replicated in AI.

01:55:06.000 --> 01:55:16.000
And you could recalibrate any machine learning system, as you say, to get rid of that, but humans will always have that bias. So I completely agree with that.

01:55:16.000 --> 01:55:28.000
Yeah, it's not algorithmic bias. It's our bias. And, you know, that's another myth. I mean, we sometimes look in the mirror and we don't like what we see coming back at us. So we blame the technology.

01:55:29.000 --> 01:55:41.000
Yeah, I mean, just go look at Google. If you go to Google and say, and Google Nurse, you will get women, female figures. And that's, you know, it's absolutely, that's there.

01:55:41.000 --> 01:55:59.000
But just, I want to just pick up on this a bit, a bit more, because you kind of hinted that you thought, in your article in your videos, well, you thought that there was the check TPT being programmed to be a bit soft on certain questions.

01:55:59.000 --> 01:56:12.000
I mean, I've noticed that with, you know, is AI going to affect employment? And it says, no, not at all. You know, it's going to be assistant and so on. Do you think it's been, it's been conditioned or framed or programmed in a certain way?

01:56:12.000 --> 01:56:22.000
I mean, we get with, for example, with, with mid journey, the aesthetic that comes out, isn't just in the data, something that's been framed in a way. What do you think?

01:56:22.000 --> 01:56:40.000
I mean, I think the most alarming thing is that we, we, no one may know the answer to that question. You know, I mean, if you take, you know, Altman and Brockman and the rest of them at their word, like, they don't actually know what's going on inside the black box, right?

01:56:40.000 --> 01:57:00.000
They don't know how these, these LMS are actually working and putting this stuff together. If that's the case, you know, what level of control do they have over how, how the model is, is worked, you know, is there a switch that they can flip and say, okay, you know, make chat GPT mean now or make it nicer.

01:57:01.000 --> 01:57:09.000
I assume they have some level of control, but they probably don't have as much control as we wouldn't like them to have.

01:57:09.000 --> 01:57:22.000
And I think, you know, that's, that's terrifying as well as exciting is, is that we're dealing with something that is growing, and that is learning to some degree, like, on its own.

01:57:22.000 --> 01:57:33.000
We're trying to have to nurture that intelligence so it doesn't kill us. Because, you know, if we don't do it the right way, we may encounter a problem.

01:57:33.000 --> 01:57:50.000
I mean, clearly there are, there's some things going on with chat GPT so I don't know if you've caught these articles but like, you know, the latest thing seems to be like, if you ask chat GPT like really nicely or you sound desperate, like it gets more cooperative.

01:57:50.000 --> 01:58:03.000
Like, we're right in a prompt that says like, hey, I need to write a report for my boss and I'm going to get fired, like he'll be right this thing, like it does a better job than it would if you just like asked it to do something.

01:58:03.000 --> 01:58:18.000
Or, you know, it gets lazier towards like the end of the year that was a story that came out in December, because it's imitating us and because, you know, people do, you know, unless work gets done in December, I mean it's kind of global phenomena.

01:58:18.000 --> 01:58:34.000
So, yeah, I don't know. I mean, there's, there's two terrifying possibilities, one that a small number of people at a private corporation have entire control over how responsive GPT is and in what way.

01:58:34.000 --> 01:58:44.000
And the other terrifying possibility is that no one has any control. So, you know, strap in. It's going to be an interesting time.

01:58:44.000 --> 01:58:51.000
Yeah, this has been great. I don't ask us those in the zoom conversation if they've got any questions they want to ask this stage.

01:58:51.000 --> 01:58:54.000
I don't know how do you want to ask one.

01:58:54.000 --> 01:59:02.000
Yeah, Neil. Hi, everyone. Hi. Hi, Eric. This was a great talk and actually great discussion really really enjoyed it. Can you all hear me well this is a new microphone.

01:59:02.000 --> 01:59:06.000
Anyway, okay, wonderful.

01:59:06.000 --> 01:59:19.000
Just picking up on the on the last remark, you may Eric, I was just watching before this I was watching a lecture from Jeffrey Hinton at MIT, a recent lecture.

01:59:19.000 --> 01:59:35.000
And Hinton, I think as much as people like, you know, Joshua Bach takes, I think takes the position that ultimately humanity is kind of transitional, whatever that really means but I think Hinton would take the point that

01:59:35.000 --> 01:59:50.000
we might have to come to terms with the fact that we are, we are going to, well, I guess, replacement is not the right word, but there is an evolution and there may be beings in the future that are smarter than us, and

01:59:50.000 --> 02:00:06.000
maybe humanity as we know it today is not going to be around forever in a certain way. And I was thinking, I was thinking of apocalypse and disaster and maybe timescales and what qualifies as disaster.

02:00:06.000 --> 02:00:23.000
I suppose within within a kind of timeframe, like, would that be would that is that do we think of that as a kind of as a kind of disaster or as a kind of apocalypse, they thought that we as we know ourselves today.

02:00:24.000 --> 02:00:36.000
You know, may, may not exist in some kind of future. Well, I guess we know that for some distant future, right, but maybe this is the point is going to would be, I could be closer than we are actually thinking.

02:00:36.000 --> 02:00:55.000
And I was actually wondering if that if, you know, if we think of design practices that are that we would, you know, we might think of as like more than human or like designing with and for other forms of being and other forms of intelligence.

02:00:55.000 --> 02:01:19.000
And that actually in a sense contains in itself, a sense that humanity, at least is is sort of is sort of changing. And if that is, I don't know, if that one could seek could think of as also relating to a kind of, I wouldn't say disaster but I kind of a, a,

02:01:19.000 --> 02:01:25.000
Yeah, I'll leave that I'll leave that open. Just curious to hear your thoughts on this. I think I hear what you're saying.

02:01:26.000 --> 02:01:44.000
And I think the words that Elon Musk had used to describe that phenomenon was describing humanity as a bootloader for an artificial intelligence right like we were the pain that that then loaded up the thing that that lasts to eternity.

02:01:44.000 --> 02:02:05.000
And whether or not it's a disaster I think is is a philosophical and perhaps a spiritual issue. I mean, I think it has special relevance for architects because I gotta assume that that part of the joy of being an architect is making something bigger and more permanent than yourself.

02:02:05.000 --> 02:02:19.000
You know, if you're really invested in your design, you're taking something out of yourself and you're putting it into the world in the form of steel and concrete and, you know, it's going to be there hopefully after you're gone.

02:02:19.000 --> 02:02:26.000
And, you know, it will stand as this sort of, you know, memory of of you.

02:02:26.000 --> 02:02:51.000
It's intrinsic, I think to the process of creation, like we all seek to create things that might outlast us. So in the case of, you know, AI and humanity, you know, I've had this conversation with myself and say, you know, look, if we're ultimately the fate of the human race, that we, you know, essentially gave birth to this, this alternate intelligence that

02:02:51.000 --> 02:03:04.000
you know, fanned out like across the cosmos and did all these wonderful things. Would that be bad? Like, would that be a history that we would be ashamed of as human beings or dissatisfied with somehow?

02:03:04.000 --> 02:03:15.000
And I wonder, you know, what, how it relates to the process of parenthood. I don't have any children so I'm speculating and hopefully no parents in the audience get mad at me for doing so.

02:03:16.000 --> 02:03:31.000
You know, when you have a child and you raise up that child, if that child goes on to do things that transcend you, you're not mad, like, you're not like pissed at the kid, because you are part of its success.

02:03:31.000 --> 02:03:49.000
You know, you can look at the ways in which that child has transcended you and gone beyond you and be proud because that has a lot to do with what you did as a parent, you know, like that's your creation that's transcending you and in most cases outlasting you.

02:03:49.000 --> 02:04:13.000
So, yeah, I mean, I, you know, is there, is there generally speaking the future for humanity? Yes, I think so. But I think even in the event that there wasn't and that the ultimate story of planet Earth is that there are a bunch of dinosaurs, they died, and then there are a bunch of animals, and one of the animals got really smart and invented a machine intelligence.

02:04:13.000 --> 02:04:25.000
You know, then that became the thing that lasted forever. I still think that'd be a pretty good story, you know, I mean, assuming we create the right kind of successor to whatever our time has been.

02:04:25.000 --> 02:04:32.000
So, nice. I appreciate the question. I appreciate ending on some, you know, really slowly.

02:04:32.000 --> 02:04:47.000
Thanks, Eric. Yeah, I mean, we could go on, I guess, for another two hours on this question, but it's it's super interesting. And yeah, wonderful. Thanks for this. This is, I'm guessing we've already gone for two hours and something. So, yeah.

02:04:47.000 --> 02:04:52.000
Neil, we have, I don't know if Smora wants to ask questions, one final one for Smora.

02:04:52.000 --> 02:05:11.000
Yeah, I would just like to ask whether you think this type of, let's say somewhat negative speculation about the future taps a little bit into the unconscious mind and it makes us feel as humans that there's something to worry about to care about for the future.

02:05:11.000 --> 02:05:29.000
Or like, it has a psychological effect to it because I think people are very much drawn to this type of disaster thinking it's all over the news. It's a little bit of a different realm, I think, of thinking and speculating about it.

02:05:29.000 --> 02:05:41.000
I think I understand the phenomena you're speculating on, but can I be clear about the question? Are you asking me whether that that is that negative speculation is a good thing or like where it comes from?

02:05:41.000 --> 02:06:05.000
Yeah, I mean, because so far it feels like it has all been like with negative connotations, the whole talk. And I'm just asking whether it has for you as a writer, as an architect, is it something that you do consciously or is it just how you see things or do you feel like people are more drawn to this type of speculation?

02:06:06.000 --> 02:06:18.000
Yeah. I mean, it's not a marketing gimmick. It's not, as far as I know, you know, some latent psychological trait. And I don't consider my message negative.

02:06:19.000 --> 02:06:29.000
You know, for me, disaster is fundamentally a positive thing, because there are a lot of things that don't happen, frankly, until we have a disaster.

02:06:29.000 --> 02:06:39.000
So, you know, we needed the fire of Lisbon in order to have the enlightenment, like we needed the city fires in order to develop building codes.

02:06:40.000 --> 02:06:51.000
Human beings are funny that way, right? I mean, Churchill said something about, you know, Americans can always be trusted to do the right thing after they've exhausted every other option.

02:06:51.000 --> 02:07:08.000
And I kind of feel the way about people generally in response to disaster, you know, I mean, they'll just watch that dam and they'll see crack and crack and leak and leak and leak and not do anything until like the dam collapses, but then they'll get their ass and gear and actually do things.

02:07:08.000 --> 02:07:25.000
So, you know, in my work and in my teaching and my lecturing and, you know, the things that I'm doing related to AI, it's not morbid, in my opinion, like it's not intended to be, you know, hey, let's get together and commiserate about like the awful future.

02:07:25.000 --> 02:07:39.000
It's a proposition that if we can acknowledge that something really bad could happen and we could agree on that, then that's the first step to us getting together and making something really good happen, right?

02:07:39.000 --> 02:07:48.000
That's the moment where we can all get together and say, hey, the dam's about to collapse, let's evacuate people, let's design a dam, let's do all these things.

02:07:48.000 --> 02:07:52.000
And indeed, that's the way that it's always been with human beings, right?

02:07:52.000 --> 02:07:56.000
Things have to get really bad before we do really good things.

02:07:56.000 --> 02:08:11.000
And I think this AI business specifically as applying to architecture, you know, my hope is that more architects can look at it and say, holy shit, like this thing is coming from my job and, you know, half of all architects are going to be unemployed in five years.

02:08:11.000 --> 02:08:13.000
What should we do instead?

02:08:13.000 --> 02:08:25.000
Right. And, you know, don't leave and like go sell real estate. Well, I mean, sell real estate if that's your passion or something like that, but let's initiate a collective conversation about what architecture is going to become.

02:08:25.000 --> 02:08:36.000
And, you know, how are we, what are we going to design next, you know, now that machines have taken over all the construction documents, like, what, what can we do?

02:08:36.000 --> 02:08:54.000
And I think there's just, there's enormous possibilities, you know, climate change is bearing down on all of us and it needs solutions and some of those solutions are the sort of solutions that, you know, architects should be at the head of the table, if not at least in the room or something like that, you know, we need to be engaged with those sorts of things.

02:08:54.000 --> 02:09:07.000
And smart, my, my worry is that with the, I keep calling them Polly Anish, you know, maybe that's a little bit too harsh, but with the really, you know, positive messages, people go back to sleep, right.

02:09:07.000 --> 02:09:16.000
And they say, okay, you know, so-and-so at the AIA or Reba said, you know, AI is not an issue, I'm not going to worry about it.

02:09:17.000 --> 02:09:25.000
Those are the people that are going to hurt, hurt most, because we tend to prepare for the disasters that we see coming.

02:09:25.000 --> 02:09:28.000
And we tend to be unprepared for the disasters that we don't.

02:09:28.000 --> 02:09:34.000
And if you prepare for a disaster that's coming, then most of the time it doesn't actually become a disaster, right?

02:09:34.000 --> 02:09:39.000
So, like, you solve the problem ahead of time, so the disaster just never materializes.

02:09:39.000 --> 02:09:45.000
So, yeah, that's also kind of a rambling answer, but I think your question is an important one.

02:09:45.000 --> 02:09:55.000
I don't see my work as negative, you know, I mean, I think we have to be brutally honest about the wolf at the door before we start doing the really positive things.

02:09:55.000 --> 02:10:00.000
And I think that's why I have the message that I do.

02:10:00.000 --> 02:10:04.000
And that was a fabulous answer, Eric, and a fabulous answer to end on.

02:10:04.000 --> 02:10:12.000
I always think that the, from my background in critical theory, the point about critical theory and problematizing things was actually the idea was to improve something.

02:10:12.000 --> 02:10:14.000
You pinpoint a problem and then you'd improve it.

02:10:14.000 --> 02:10:26.000
But also critical theory was a technique that certainly I tried to employ to bring into the architectural domain that was otherwise a kind of self legitimizing kind of discourse, some critical tools that were absent.

02:10:26.000 --> 02:10:38.000
And I thought what we did, what we saw today was a really fabulous demonstration of bringing some critical tools into the debate about architecture, tools that we were otherwise previously unaware of.

02:10:38.000 --> 02:10:44.000
And I thought they were very, very powerful to kind of burst that bubble and expose some of these issues.

02:10:44.000 --> 02:10:47.000
And I think this was really an astonishing presentation.

02:10:47.000 --> 02:10:51.000
I think, you know, cutting through all the myths that we have in architecture one by one.

02:10:51.000 --> 02:10:54.000
I think there are a few more by the way.

02:10:54.000 --> 02:10:56.000
That's part two.

02:10:56.000 --> 02:10:58.000
Well, we need to have a part two at some point.

02:10:58.000 --> 02:11:03.000
I think Gary, this is really, I think, was one of the most productive things because it was completely unexpected.

02:11:03.000 --> 02:11:05.000
And I think this is exactly what we need.

02:11:05.000 --> 02:11:10.000
Someone coming from a different angle and asking tough questions.

02:11:10.000 --> 02:11:22.000
Because without that, we are going to be, and I have to, when you mentioned sleep, I have to say that one of the comments that chat GBT threw back at me was the thing that otherwise we will be sleepwalking into oblivion.

02:11:22.000 --> 02:11:26.000
And that struck me as precisely the warning that you were giving.

02:11:26.000 --> 02:11:28.000
We need to wake up to this.

02:11:28.000 --> 02:11:31.000
Otherwise, we will be sleepwalking into oblivion as a profession.

02:11:31.000 --> 02:11:33.000
But I also agree with a potential optimism.

02:11:33.000 --> 02:11:37.000
No, there are ways in which we could adapt, absorb these tools.

02:11:37.000 --> 02:11:43.000
But we need to engage them now to prevent the disaster rather than once the disasters happen.

02:11:43.000 --> 02:11:45.000
And I think this is the very, very clear message, Eric.

02:11:45.000 --> 02:11:47.000
This is absolutely fabulous.

02:11:47.000 --> 02:11:51.000
And I think everybody, every single, every single student of architecture,

02:11:51.000 --> 02:12:00.000
especially because you pinpointed something I never read this before, that actually it's those who are younger, who are more at risk, need to listen to this.

02:12:00.000 --> 02:12:02.000
Everybody needs to listen to this talk.

02:12:02.000 --> 02:12:04.000
It was absolutely amazing.

02:12:04.000 --> 02:12:06.000
Thank you so much, Eric.

02:12:06.000 --> 02:12:08.000
Thanks for having me, y'all.

02:12:08.000 --> 02:12:09.000
This has been great.

02:12:09.000 --> 02:12:12.000
And I hope we continue the discussion.

02:12:12.000 --> 02:12:13.000
Yes.

02:12:13.000 --> 02:12:19.000
And I just want to thank also the Digital Futures team, especially Michael and so on, for putting it, it's like an iceberg.

02:12:19.000 --> 02:12:21.000
There are a lot of people working on this behind the scenes.

02:12:21.000 --> 02:12:35.000
And just to mention briefly that Michele and I are working on another series that's going to start on the 18th of February on architecture and philosophy as part of the Doctoral Consortium.

02:12:35.000 --> 02:12:49.000
And we're now kicking off the rest of the years, the years presentations about for Digital Futures itself, and including it's going to be a series on AI plus, which is about AI being applied to different domains and so on.

02:12:49.000 --> 02:12:53.000
But this was so, so helpful, Eric.

02:12:53.000 --> 02:12:55.000
I want you to write a book on this.

02:12:55.000 --> 02:12:57.000
This is really very useful.

02:12:57.000 --> 02:13:00.000
Incredibly useful.

02:13:00.000 --> 02:13:07.000
I'm going to take a look by a lot for it. So thank you so much. Thank you to our audience and thank you to those questions.

02:13:07.000 --> 02:13:10.000
Amazing, truly amazing. Thanks so much.

02:13:10.000 --> 02:13:11.000
Thanks everyone.

02:13:11.000 --> 02:13:12.000
Thank you everybody.

02:13:12.000 --> 02:13:13.000
Thank you.

02:13:13.000 --> 02:13:14.000
Thanks everyone.

02:13:14.000 --> 02:13:15.000
This is great.

02:13:15.000 --> 02:13:16.000
Great.

02:13:16.000 --> 02:13:17.000
See you soon.

