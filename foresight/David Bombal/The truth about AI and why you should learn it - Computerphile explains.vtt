WEBVTT

00:00.000 --> 00:06.160
This is a fascinating story we have for you of a senior Google engineer who says one of

00:06.160 --> 00:11.080
the company's artificial intelligence systems has become a sentient being.

00:11.080 --> 00:16.160
He believed one of the company's artificial intelligence chatbots had become sentient.

00:16.160 --> 00:22.640
Engineer Blake Lemoine says a chatbot project he was working on called Lambda can express

00:22.640 --> 00:26.160
thoughts and feelings equivalent to that of a child.

00:26.240 --> 00:32.080
Google has rejected claims that one of its programs had advanced so much that it had become sentient.

00:32.080 --> 00:37.040
That's I think the big issue right is that a lot of people get bogged down deciding well what

00:37.040 --> 00:38.640
does sentient actually mean?

00:46.320 --> 00:49.520
Hey everyone it's David Bombal back with a very special guest Mike welcome.

00:49.520 --> 00:50.640
Oh thanks for having me.

00:50.640 --> 00:55.760
So Mike I've seen a lot of your videos on YouTube computer file millions of views on

00:55.760 --> 00:59.520
some of the topics that you've done but can you just introduce yourself to the audience

00:59.520 --> 01:02.960
for people who might not have seen those videos or don't know what you're doing because you're

01:02.960 --> 01:06.800
telling me offline YouTube isn't your main thing you do more than that.

01:06.800 --> 01:10.640
That's right yeah so actually in some sense YouTube isn't a side for me right it's just

01:10.640 --> 01:12.480
something I did because I thought it would be fun.

01:12.480 --> 01:17.680
I'm an academic at Nottingham associate professor and I work teaching security.

01:17.680 --> 01:20.480
I research teaching AI and computer vision.

01:20.480 --> 01:27.200
It just happened that we have some ties at Nottingham to Brady and Sean who do things like

01:27.200 --> 01:31.920
number file and computer file and so computer file was kind of fledgling it had a bit of it

01:31.920 --> 01:36.320
was a bit established when I when I started doing them and it kind of just took took off really.

01:37.520 --> 01:42.000
I think because I did topics on security and AI and things people thought those were interesting

01:42.000 --> 01:46.000
and so I get you know a lot of views on those now but it is still a bit peculiar when people

01:46.000 --> 01:51.600
say hello to me you know because I just turn up and do normal things the rest of the time.

01:51.600 --> 01:56.640
So you get stopped in the street and it has happened yeah my wife is never impressed when

01:56.640 --> 02:02.320
that happens she thinks this is ridiculous but you know it happens from time to time I do I do

02:02.320 --> 02:06.320
really enjoy it I get lots of emails from people saying thanks for your videos I enjoy them and

02:06.320 --> 02:10.080
that's that's why I do it that's that's what it's for for me.

02:10.080 --> 02:15.360
I loved what you said offline you know you someone said that they started computer science because

02:15.360 --> 02:19.920
of you. I've had a couple of emails like that and that's the those are the best emails right because

02:19.920 --> 02:24.720
I want people to learn about computer science I love computers I'm a massive geek basically

02:24.720 --> 02:30.480
I program for fun and the more people do that the more it's a win for me so you know if I can

02:30.480 --> 02:34.000
encourage a few people by doing videos but that's what I really want to do.

02:34.000 --> 02:38.800
That's fantastic one of the videos I watched obviously in preparation for this interview

02:38.800 --> 02:45.760
is this recent video that you put out about AI and that's going to be the topic that we

02:45.760 --> 02:51.520
want to talk about today so let me lead with this I get this these kind of emails all the time

02:52.080 --> 02:57.760
David is it worth me studying cyber security David is it worth me studying computers because AI

02:57.760 --> 03:03.600
are going to take all the jobs away and I think movies over the years like you know there's been

03:03.600 --> 03:10.000
so many of these movies where the robots take over and this can you talk about this and you can go

03:10.000 --> 03:16.240
into the details if you like but I think this this this sort of recent event that you spoke about

03:16.240 --> 03:20.800
in your video hasn't helped the conversation at all so can you tell us about that yeah in what your

03:20.800 --> 03:25.200
thoughts are about you know what happened yeah so no I absolutely agree but it didn't help the

03:25.200 --> 03:29.280
conversation about was okay I think in my video about was kind of what I tried to end with was

03:29.280 --> 03:34.720
basically it doesn't you know in some sense the nuances of where this AI is doesn't interest me that

03:34.720 --> 03:39.760
much all I know is it's not where they're suggesting it is at least that's you know that's what I think

03:39.760 --> 03:45.840
I suppose at the moment AI is very application driven right so a lot of it is supervised there

03:45.840 --> 03:49.760
are other ways of doing it but a lot of it's supervised which means that you have some kind

03:49.760 --> 03:53.440
of training set with some inputs and some outputs that you're trying to get the model to learn and

03:53.440 --> 03:58.320
then you just train the model until that happens that can work really really really well and so

03:58.400 --> 04:02.720
from my own research I do this on things like image segmentation where I'm trying to find objects

04:02.720 --> 04:07.840
in images and you know medical image segmentation and things like this but you know in practice if

04:07.840 --> 04:11.440
I then take that network and try and run it on street scenes it won't work because it's not

04:11.440 --> 04:16.240
trained on street scenes it doesn't know what they are it hasn't got any ability to go oh it's a street

04:16.240 --> 04:20.880
now you know and take what it's learned somewhere and apply it somewhere else you know retraining

04:20.880 --> 04:27.200
a network is really the only way to do it and that involves even more data right so I don't think

04:27.200 --> 04:30.880
at the moment it's realistic to suggest that there's going to be some general

04:30.880 --> 04:35.600
intelligence that can just do all of our jobs right you know you've seen github co-pilot that

04:35.600 --> 04:40.800
just produces text code and sometimes it will produce a useful function and sometimes it will

04:40.800 --> 04:45.040
produce a function full of bugs that you've got to then spend time fixing and have you actually

04:45.040 --> 04:50.720
saved any time I don't know the jury's out I think so I wouldn't worry at the moment I'm not worried

04:50.720 --> 04:56.080
I mean maybe designing things to replace myself is a huge mistake but I don't think we're there yet

04:56.080 --> 04:59.760
so I mean tell us just for people who haven't seen it um haven't seen your video and like haven't

04:59.760 --> 05:06.000
read perhaps what's going on there's this google person lambda yeah so what is lambda and what is

05:06.560 --> 05:11.760
what what what what was he basically saying um google lambda is a um it's what we call a large

05:11.760 --> 05:17.360
language model so it's basically a a very very large neural network designed in a certain way

05:17.360 --> 05:22.000
they're all designed in a very similar way and it has more parameters in it than we've ever seen

05:22.000 --> 05:27.040
really in a model right or GPT-3 is also very very big and so really what this brings to the

05:27.040 --> 05:32.320
table is not so much something new that we've never seen before in AI it's just it's just huge

05:32.320 --> 05:35.360
you know orders of magnitude bigger than the kind of networks I would use to do

05:36.080 --> 05:40.240
you know complex imaging tasks and what they've basically done is they've trained this this model

05:40.240 --> 05:45.200
to read a sentence and then predict what the next word will be and so you could imagine but if you

05:45.200 --> 05:48.800
wanted to do this by hand and you had infinite resources you could just look at every sentence

05:48.800 --> 05:54.240
that's ever been written by humans and work out for any given let's say 10 words what the next word

05:54.240 --> 05:59.040
will always be and if you did that and you had that list of all the possible inputs you do pretty

05:59.040 --> 06:03.440
well at generating sentences because at the end of the day that's what people say right this is

06:03.440 --> 06:07.360
you've got it on record as what they've said in the past you can just say those things again and so

06:07.360 --> 06:11.280
this model is one of those this model is one where you put in some sentences so you might put in a

06:11.280 --> 06:16.640
sentence that says what do you think about quantum physics and then what the model will do is predict

06:16.640 --> 06:23.440
the next likely word and it will probably say well I'm going to start by saying what I think is

06:23.440 --> 06:29.680
and then generate some plausible text on quantum physics because people have written about quantum

06:29.680 --> 06:34.720
physics before and that data is in the training set what it hasn't done is learned what quantum

06:34.720 --> 06:38.960
physics is or connected to an internet resource that has information on quantum physics and looked

06:38.960 --> 06:44.000
it up so in some sense it's a bit like the you know in Star Trek you've got the computer you can

06:44.000 --> 06:48.240
talk to and they often ask the computer to do things like you know put the shields up or whatever

06:48.240 --> 06:55.200
yeah computer dim lights it's like that but it's not connected to any kind of anything on the ship

06:55.200 --> 07:01.520
so it just talks to you and talks back but it never actions anything it never it never has you

07:01.520 --> 07:06.560
know it's just going from the text in the training set and I think that's something that's perhaps

07:06.560 --> 07:11.920
lost a bit in in when it when it's discussed it's but it's not connected to anything it doesn't even

07:11.920 --> 07:18.000
have a memory basically and so it can't reflect on past experience because it has no place to store

07:18.000 --> 07:23.120
past experience it has no record of those events and so when it produces sentences that look really

07:23.120 --> 07:28.160
really interesting they're actually just really interesting sounding sentences you know and I

07:28.160 --> 07:32.960
think so anyway I mean if I if I sort of digress slightly but you know in this particular case

07:32.960 --> 07:37.360
what happened was someone from Google who I think was in the ethics department I don't think he was

07:37.360 --> 07:43.120
actually responsible for developing this AI basically said look at this chat I've had with it

07:43.120 --> 07:47.920
don't you think it's sentient basically is what he said and the answer I think from me and pretty

07:47.920 --> 07:52.720
much everyone who understands these models was no it's no it's not and what and I think the thing

07:52.720 --> 07:57.120
that bothered me most about it was not particularly one person saying this because he's very entitled

07:57.120 --> 08:02.400
to his opinion right you know I think I think it was that the media took it massively seriously and

08:02.400 --> 08:07.520
it was all over everywhere is this the next thing and that bugs me somewhat because I don't think it

08:07.520 --> 08:13.120
helps for conversation like you say right people start people who don't know what a big a big model

08:13.120 --> 08:16.800
a big language model is are going to be a bit worried about this and there's really no reason

08:16.800 --> 08:21.680
at this time to be worried and like that bothers me slightly which is why I do my videos to try and

08:21.680 --> 08:27.840
tell people about it I mean the problem is the movies predict this happening and then people

08:27.840 --> 08:34.160
see this stuff in the news and it's like it's the end of the world and end of my job Arnold and the

08:34.160 --> 08:39.440
robots are going to take over so it really doesn't happen it doesn't it really doesn't help and I

08:39.440 --> 08:42.560
like what you said I mean in your video which I'll link below you said something that I thought was

08:42.560 --> 08:50.160
hilarious you said can python functions get lonely yeah can you explain what you were saying by that

08:50.160 --> 08:55.280
yeah so one of the one of the comments in the original chat transcript between this researcher

08:55.280 --> 09:01.040
and his friend and his colleague and this lambda AI was do you get lonely and it's spouted off a whole

09:01.040 --> 09:07.440
paragraph about how lonely it is and it doesn't make any sense because it's a function call right so

09:07.440 --> 09:12.560
you put in your words at the top it runs of what is essentially a big transformer network which is

09:12.560 --> 09:17.840
pre-trained on all this data and then it spits out words at the bottom which you read and then it

09:17.840 --> 09:22.160
stops executing right because there's no kind of ongoing process like there is in my I mean I like

09:22.240 --> 09:25.120
to think that when I'm not immediately saying something to you I'm still there's something

09:25.120 --> 09:30.720
going on in there right maybe I mean you know I can't prove it to you but but this is not the

09:30.720 --> 09:34.960
case you know and it's just like when you run I mean I made a joke about it but when you run

09:34.960 --> 09:39.200
you know reverse string in python you don't worry that it gets lonely the rest of time because it's

09:39.200 --> 09:44.160
not executing that's just some code that executed it finished executing and it just lies dormant

09:44.160 --> 09:49.520
in memory doing absolutely nothing of interest and that's for me kind of what this model is doing

09:50.080 --> 09:55.280
if they developed a model that was always on in some way like maybe it was always doing something

09:55.280 --> 10:00.240
and it had memory and it had storage I could I still probably would think I would need some

10:00.240 --> 10:05.120
convincing but it had any kind of you know higher level thought process but at least it would be

10:05.120 --> 10:09.760
plausible you know it would sort of think well at least it's got something going on in there

10:09.760 --> 10:15.120
but I just don't think it's designed that way it's designed as a very very big reverse string

10:15.120 --> 10:19.680
and you know I don't worry about those things being sentient yeah but it's crazy I mean I mean

10:19.680 --> 10:26.720
the well I mean in my opinion because they kind of they were implying that this AI or whatever

10:26.720 --> 10:30.400
was like a human or equivalent to a human and it seems like that's quite a stretch

10:30.400 --> 10:36.480
but in you know popular popular culture that's what people equate to AI it seems yeah that that

10:36.480 --> 10:41.680
really that that's I think the big issue right is is that a lot of people get bogged down deciding

10:41.680 --> 10:45.520
well what does sentient actually mean and that doesn't interest me because when anyone uses the

10:45.520 --> 10:49.760
word they're not using it in a different definition they're using it in the definition we think of

10:49.760 --> 10:55.040
as like termination and sky net right you know this this researcher wasn't saying I think it's

10:55.040 --> 10:59.680
sentient but I define sentience as something like a slightly combinated if statement right he was

10:59.680 --> 11:04.240
saying I think it's like a person and it's got memories and it's got experiences and it gets

11:04.240 --> 11:09.600
lonely and it needs a lot of feelings and yeah and and without with any with zero evidence to

11:09.600 --> 11:13.360
support this and indeed not so much evidence is just it doesn't even make sense so I think you have

11:13.360 --> 11:17.520
to be extremely careful using the word sentient not because you might have a different definition

11:17.520 --> 11:23.360
but because everyone has the actual same definition right which is actual you know human level

11:23.360 --> 11:28.320
cognitive ability but you know which so I don't spend a lot of time worrying about what the

11:28.320 --> 11:32.720
definition of sentience is because if I go to someone in the conversation and say this is sentient

11:32.720 --> 11:38.000
I think we both understand implicitly what that means to me to say that and so I don't I I think

11:38.000 --> 11:42.720
we're arguing about the definition is a bit silly because we actually all secretly agree on the

11:42.720 --> 11:47.920
definition yeah I mean I think for the general population I mean I'm not I'm not into the AI

11:47.920 --> 11:52.240
piece like you are and that's why you I want to talk to you about it you know I just think people

11:52.240 --> 11:57.040
go off movies and popular culture that's sort of what what people that's the impression they get

11:57.040 --> 12:03.200
and that's why it was so big on the news perhaps but can you explain AI versus machine learning and

12:03.200 --> 12:08.320
like what is machine learning what is AI and perhaps just take us down the road now yeah okay

12:08.320 --> 12:13.840
like teach us you know sort of the basics of the stuff yeah so AI is misused in the sense that it's

12:13.840 --> 12:19.200
now a catchall right and I will admit I do that to an extent myself and it's partly because I'm

12:19.200 --> 12:23.920
lazy right I think I think it's because it means I don't have to define the exact thing that I'm

12:23.920 --> 12:28.400
doing yeah at any given time car engines are slightly different but they all at the moment I

12:28.400 --> 12:33.760
mean I say they will the combustion engines all do much the same thing even though one's got more

12:33.760 --> 12:38.000
cylinders and one's got fewer cylinders and one has a turbo and one doesn't you don't say I you

12:38.000 --> 12:42.000
don't miss it well I don't go on about those details I just say I've got a car and it goes

12:42.000 --> 12:46.960
so AI I think is a catchall that includes machine learning so you've got AI as a big kind of

12:47.600 --> 12:52.800
thing of stuff with loads of stuff in it and even my maze solving video where I just do very simple

12:53.360 --> 12:59.040
looking around the corridors of the maze would be defined in some sense as AI right but the

12:59.040 --> 13:04.320
Dijkstra algorithm that we use to do network routing and things and other similar algorithms

13:04.320 --> 13:09.280
you could define them in some ways as AI because they adapt to messages coming in and they change

13:09.280 --> 13:15.120
weights and paths and things but we wouldn't go as far as to say they were you know anyway you know

13:15.120 --> 13:20.320
smart in some sense right so I think AI is quite a broad term and then there are things like genetic

13:20.320 --> 13:25.120
algorithms evolutionary algorithms which do slightly different things they are arguably less

13:25.120 --> 13:29.840
popular or less prevalent perhaps will be the right way to put it but they also come under the

13:29.840 --> 13:35.440
umbrella of AI so AI is this very big umbrella term which basically encompasses most most things

13:35.440 --> 13:40.160
where you could imagine it was sort of intelligence and then in that you've got machine learning and

13:40.160 --> 13:45.040
machine learning is just the idea that you want to try and program a computer without having to

13:45.040 --> 13:49.760
program it essentially you want to give it some examples or some other mechanism from which to

13:49.760 --> 13:55.920
learn and it comes up with its own rules for what it's going to do so a decision tree is a good

13:55.920 --> 14:01.680
example of a very simple you know conceptually simple machine learning approach where you have

14:01.680 --> 14:07.440
some kind of data and every time you make a decision you just you just split it in two so maybe you're

14:07.440 --> 14:12.400
trying to analyze financial data decide whether people get a new credit card right so the first

14:12.400 --> 14:17.280
decision you make is have they ever defaulted on a credit card yes goes this way no goes this way

14:17.360 --> 14:22.240
and then the next decision is okay what's their current credit limit it's it's above 7000 it goes

14:22.240 --> 14:27.440
this way below 7000 goes this way and you just split this data into two and two and two until at

14:27.440 --> 14:32.160
the end you get the actual nodes that have the decisions on right and it's machine learning

14:32.160 --> 14:36.560
because what you can do is you can you can basically create this tree but actually change

14:36.560 --> 14:42.320
the numbers and values in it and the decisions based on the data so you can say well actually

14:42.400 --> 14:48.320
maybe 7000 doesn't work that well we're going to have it at 6500 and change the thresholds and things

14:48.320 --> 14:52.480
and you can do this all automatically in the training process so that's the kind of thing

14:52.480 --> 14:57.120
we're talking about with machine learning now what happens of course is there's a big push in

14:57.120 --> 15:01.040
deep learning which I you know I can also talk about but yeah be great yeah because I mean we just

15:01.040 --> 15:04.480
hear the you know I just hear these buzzwords I mean preparing for this interview just like

15:04.480 --> 15:08.880
buzzword after buzzword after buzzword and I think a lot of us you know who are not in this sort of

15:08.880 --> 15:13.840
field but are interested in it you know so yeah if you can define as much and like yeah yeah sure so

15:13.840 --> 15:18.960
I mean so you've got yeah sorry you've got you've got ai right which is which is right here in some

15:18.960 --> 15:22.800
subset of actors machine learning which includes what I would kind of call traditional machine

15:22.800 --> 15:27.600
learning like support vector machines decision trees random forests right these are all linear

15:27.600 --> 15:32.480
regression even right where you're just fitting a line to us to some data and then we have things

15:32.480 --> 15:38.720
like slightly more complicated algorithms like artificial neural networks which they I mean they

15:38.720 --> 15:44.880
kind of take some inspiration from our brains but I would I would be very careful saying that to me

15:44.880 --> 15:52.160
I think you know to suggest it's like our brain is is is is iffy and so yeah yeah but they have

15:52.160 --> 15:56.800
their name yeah like that's what they're called and then what we've basically done recently

15:57.680 --> 16:01.600
is we've made them much much bigger right and we've introduced other terms like convolutional

16:01.600 --> 16:07.440
networks and transformers and things but for the sake of you know this sentence they're just bigger

16:07.440 --> 16:12.720
deeper networks that can learn more impressive functions so they can map that input to that

16:12.720 --> 16:17.120
output more effectively right because that's what you want to try and do you've got some data

16:17.120 --> 16:21.600
you've got some predictions you need to make on that data and your hope is that once you've trained

16:21.600 --> 16:26.320
it some new data comes along and you can make some good predictions right I mean let's think of an

16:26.320 --> 16:34.480
example suppose suppose I want to you know do um MRI segmentation for medical imaging right so I have

16:34.560 --> 16:38.480
50 patients some of whom unfortunately have some kind of illness some of whom don't

16:38.480 --> 16:43.440
and I train the network to try and find the ones that have illness my hope is that when I then

16:43.440 --> 16:48.960
sort of fix that network in place and bring in some new patients it will be able to say whether

16:48.960 --> 16:54.320
they have that illness or not that's the idea and we'll have done that by basically reconfiguring

16:54.320 --> 17:00.000
itself based on the examples I gave it to begin with so doing a technology example it could be

17:00.000 --> 17:05.200
something like spotting is this a virus or is it just yeah exactly right and in fact you know

17:05.200 --> 17:09.600
modern antiviruses will include some kind of machine learning element probably so you know you

17:09.600 --> 17:14.160
might have features derived from so what we what we usually put into the front of a network is

17:14.160 --> 17:20.560
something we call features which is um our way of just saying input data right so sometimes you've

17:20.560 --> 17:24.400
crafted those features like you've chosen what you think is interesting features to give the network

17:24.400 --> 17:28.800
and sometimes you'll just shove something in like you know in antivirus you could you could choose

17:28.880 --> 17:33.440
things like how many system calls does it make or you know how many how many bytes is the executable

17:33.440 --> 17:37.840
or how many of this particular character does it have in the executable and you could choose those

17:37.840 --> 17:42.880
features because you think they are indicative sometimes of malware or not malware you stick

17:42.880 --> 17:49.760
them in some kind of a machine learning approach with a load of examples and then say right now

17:49.760 --> 17:56.400
change your weights and change your rules internally so that on this training set your prediction is

17:56.480 --> 18:03.520
accurate as possible right and so let's say you do that you have 100 000 malware and regular samples

18:03.520 --> 18:08.400
you give it to your AI and you just over and over again say right you got that one wrong

18:09.280 --> 18:13.520
reconfigure yourself so that next time you get a bit better at predicting it you do that over

18:13.520 --> 18:18.080
and over again and the hope is then that when a new virus comes along that you've never seen

18:18.880 --> 18:24.400
those same sort of should we say suspicious things exist in it and the network flags that up

18:24.400 --> 18:29.200
that's the idea so the the training data is like the the stuff you give it initially which would be

18:29.200 --> 18:35.040
this 100 000 like virus and not virus yeah and then you when you say weights you like it's basically

18:35.040 --> 18:40.960
saying like if it makes like a hundred system calls rather than 10 or you said some kind of

18:40.960 --> 18:45.440
threshold is that right yeah so okay so in a decision tree or something like that that's what

18:45.440 --> 18:49.280
would happen there would be some kind of threshold decision based at some point during it for a

18:49.280 --> 18:52.320
neural network it's a little bit more complicated and it's what you actually do is you treat all

18:52.320 --> 18:56.640
these weights just as numbers and you just calculate mathematical functions based on those

18:56.640 --> 19:01.760
numbers so what you might do is multiply all of those numbers that come in by some weights

19:01.760 --> 19:06.640
let's say you multiply one of them by two and one of them by negative four one of them by a half

19:06.640 --> 19:10.400
and then you add them all up and what that does is take a different amount of each one

19:10.400 --> 19:14.720
and then you and then you repeat that process over and over again to try and basically learn a

19:14.720 --> 19:19.040
complicated mathematical function that's really the only thing it does you know you're essentially

19:19.040 --> 19:23.600
trying to fit a really complicated curve through the data essentially so that you can distinguish

19:23.600 --> 19:33.040
between real and fake malware or you know regular executables and malware and and so the weight

19:33.040 --> 19:37.200
when I say weight what I'm really talking about is the parameters of my model which influence

19:37.200 --> 19:42.560
this mathematical function so the and then you would adjust the the the weights and the mathematical

19:42.560 --> 19:48.560
functions based on the result did it get did it's correctly determine that this was malware

19:48.880 --> 19:53.360
exactly so so let's suppose we were doing malware right so we think one an output of one means it's

19:53.360 --> 19:58.480
definitely malware and an output of zero means it's definitely not malware an output of 0.5 is

19:58.480 --> 20:05.120
not very useful to us because we don't know um what we do is we put in a piece of malware or many

20:05.120 --> 20:10.000
pieces of malware we run through let's say our deep neural network or whatever it is we're running

20:10.000 --> 20:14.800
and it will produce a value between zero and one and then we say well look you gave us a value of

20:14.800 --> 20:21.120
0.7 but actually it was malware this time so you've got an error of 0.3 I wanted you to produce

20:21.120 --> 20:27.120
0.3 higher for that one than you did so can you adjust your mathematical function to next time when

20:27.120 --> 20:32.480
I put that malware in produce a value of one and not a value of 0.7 now if you do that for one malware

20:32.480 --> 20:36.720
sample it's going to be the worst machine learning ever because you're just going to give it something

20:36.720 --> 20:40.880
else and it's going to go I don't know what you mean right because this is nonsense so you have to

20:40.960 --> 20:45.680
give it a lot of data and and I guess what you're trying to do is calculate the best average

20:45.680 --> 20:51.840
mathematical function that does the best job it can in the general case of all of these malware's

20:51.840 --> 20:57.520
right massively optimizing one malware is not useful because it's not going to generalize it's

20:57.520 --> 21:03.600
not going to apply in real world to some new malware so you put in 10, 20, 100 different

21:03.600 --> 21:08.640
malware at the same time and all of them are trying to go to one or go to zero and you're

21:08.640 --> 21:13.360
trying to change the weights to simultaneously do all of those at the same time that's that's what

21:13.360 --> 21:18.800
machine learning does basically for a neural network the process for actually doing this it's

21:18.800 --> 21:24.240
it's complicated to describe but it's it's fairly intuitive what you do is you because all these

21:24.240 --> 21:28.800
weights are involved in the calculation you put your features in for your malware you go all the

21:28.800 --> 21:34.560
way forward through the deep learning or the network then you calculate your error and then

21:34.560 --> 21:39.680
you go backwards adjusting the weights based on what you just found out right essentially and so

21:39.680 --> 21:44.000
if a weight doesn't have any impact on the decision because let's say it just sets everything to zero

21:44.000 --> 21:47.520
you won't adjust that weight because it's not useful you will only adjust the ones because

21:47.520 --> 21:51.680
you're calculating the influence that each of these weights has on the error you adjust all

21:51.680 --> 21:57.840
the ones that have the biggest impact and so the network will kind of try and find its way

21:57.840 --> 22:02.800
towards a good function you know and we use a process called stochastic gradient descent often

22:02.800 --> 22:07.200
to train this so what we're doing is we're picking random malwares and putting them in

22:07.200 --> 22:11.200
and that will often it will often get them wrong right because it's never seen any of these things

22:11.200 --> 22:17.520
before and so over time maybe you just nudge it slightly in a better direction and then over many

22:17.520 --> 22:23.200
thousands of looks it slowly converges on something that actually makes reasonable decisions that's

22:23.200 --> 22:28.240
you know that's the idea so it's a long process and is this what you would call supervised or is it

22:28.960 --> 22:34.080
this is definitely supervised so supervised is where you have your your your label ground truth

22:34.080 --> 22:38.160
what we was you know our we have labels for data so we're putting our data in we have some labels

22:38.160 --> 22:42.800
against which we can compare and that means that we have some idea of how right or wrong

22:42.800 --> 22:47.360
the network is in any given case right and that's very very useful and the majority despite what

22:47.360 --> 22:53.200
people might say the majority of deep learning or machine learning is supervised learning because

22:53.200 --> 22:59.920
it gets results the quickest if I want to detect some illness in MRI having examples of that illness

23:00.560 --> 23:06.320
is going to be much much easier so Mike supervised learning if I understand it right is you giving

23:06.320 --> 23:13.600
it examples of like you said actual malware or actual like in your MRI scans problems yeah and

23:13.600 --> 23:18.080
then you're supervising that it got it right and then you're correcting it yeah and it makes

23:18.080 --> 23:22.800
things much easier right so the majority of machine learning is supervised because it is

23:22.800 --> 23:26.960
simpler and easier to do if you work in applied areas like me where you're trying to get things to

23:26.960 --> 23:31.040
work really really well if you work in industry a lot of what you're trying to do is just minimize

23:31.040 --> 23:35.840
that error term you're trying to get as close to good predictions in for the majority of cases

23:35.840 --> 23:40.560
so getting some examples is going to get you to converge on that much much more quickly this is

23:40.560 --> 23:44.640
you know distinct from something like weakly supervised or unsupervised learning and there's

23:44.640 --> 23:48.880
lots of different variants so unsupervised learning is you don't have any labels right maybe

23:48.880 --> 23:55.120
the data is too big or the data is too hard to annotate or no one can agree on what the labels are

23:55.120 --> 23:59.280
and so the best you're going to be able to do is kind of partition the data into plausible groups

23:59.920 --> 24:03.600
so you can say well look we don't know exactly what all these things are but we know that this

24:03.600 --> 24:07.760
group is distinct from this group and that's unsupervised so an example would be suppose

24:07.760 --> 24:12.560
suppose you work for an online shop and you have a load of data on what different customers have bought

24:13.200 --> 24:18.160
one thing you might do is start trying to group customers into some kind of plausible groups

24:18.160 --> 24:21.440
based on roughly the things they they're not all going to have bought the exact same thing right

24:21.440 --> 24:26.000
so it's not going to be trivial but they might have bought so someone's buying mostly dog related

24:26.000 --> 24:30.000
stuff and someone's buying mostly technical gadgets and then what you can do is say well look

24:30.000 --> 24:34.000
I put all these people in the tech group and this guy bought this really nice new microphone or

24:34.000 --> 24:38.640
news camera so I'm going to recommend that now to other people in the group and maybe I get a few

24:38.640 --> 24:43.760
hits and I and I sell a few cameras that way um you can get much more complicatedness but that is

24:43.760 --> 24:49.280
an example of perhaps unsupervised learning where you don't need to have some kind of label

24:49.280 --> 24:53.040
for everyone you don't need to have labeled me ahead of time as a tech enthusiast you just need

24:53.040 --> 24:57.440
to look at the stuff I've been buying I know it's the same as always other people and know

24:57.440 --> 25:01.680
that that's interesting right rather than we know exactly what it means that's a great example so in

25:01.680 --> 25:08.080
other words you didn't tell the machine who the people were it discovered that based on the

25:08.160 --> 25:12.400
patterns of data right yeah and it didn't really even discover who they were it mostly just grouped

25:12.400 --> 25:16.800
them and that allowed us to make decisions based on the fact they were grouped now as that happens

25:16.800 --> 25:21.520
I've given this group a label of tech enthusiasts but of course you don't need to even know that

25:21.520 --> 25:25.600
you just need to know but on average they buy more TVs than everyone else so maybe send them

25:25.600 --> 25:30.560
emails about TVs you know it's that kind of idea you can still do supervised learning and other

25:30.560 --> 25:35.200
forms of learning with stuff like marketing and and recommender systems and things but you might

25:35.200 --> 25:40.400
imagine that that could be one way you would do it and I think it's a good example the problem I see

25:40.400 --> 25:45.840
like from listening to you is reality versus the movies or reality versus the news cycle

25:45.840 --> 25:52.480
because you always hear about google doing like um like teaching a machine to play chess or whatever

25:52.480 --> 25:58.800
the games are and it just like magically gets this done um and it teaches itself kind of like not

25:58.800 --> 26:03.200
even knowing what the the rules of the game are so that is a that's something called reinforcement

26:03.200 --> 26:08.480
learning a lot of the time reinforcement learning is still supervised learning okay it's just that

26:08.480 --> 26:14.080
you get the labels as you go from playing the game so the way it works is you know what you might do

26:14.080 --> 26:18.880
is you play a random game of chess where you literally move at random right and you lose

26:18.880 --> 26:23.440
and so you get a strong suggestion that maybe next time don't do that like that was stupid

26:23.440 --> 26:27.840
so now you move slightly less at random than you did before but it's still pretty bad

26:27.840 --> 26:31.760
and you lose again but you know learn a bit and this is basically how they train it so what you

26:31.760 --> 26:36.320
do is you play millions and millions and millions of games of chess and every time it goes well

26:36.960 --> 26:41.440
you just learn a little something about what was better than that time than what's the time before

26:41.440 --> 26:45.600
we're still talking about a network which is a big mathematical function right so we're still

26:45.600 --> 26:50.560
talking about something that has weights that you adjust so that when you put an input state in

26:50.560 --> 26:55.360
you get the best desirable output state which in this case of course is you won more often than you

26:55.360 --> 27:00.160
didn't for me I mean these are fascinating because they're trained in a very different way to the

27:00.240 --> 27:04.800
way I would train the network I come up with labeled data and I put it in like and I use the

27:04.800 --> 27:10.000
examples with reinforcement learning you have to start trying to give it rewards which is where it

27:10.000 --> 27:17.440
gets its labeled data from so is it is it is it is it better that you go 25 moves in chess before

27:17.440 --> 27:21.600
you lose or is it better that you checkmate regardless of how long it takes right you know

27:21.600 --> 27:26.880
because you might end up in a stalemate you know there's things with playing chess where you might

27:26.880 --> 27:30.400
say well look these other goals are also important or something like this and so you can spend a lot

27:30.400 --> 27:34.800
of time thinking about different ways you could train the network which I think I think is really

27:34.800 --> 27:41.040
interesting perhaps I'm misinterpreting it but it sounds like the hype cycle versus reality

27:41.040 --> 27:47.040
there's a big disconnect like the people have this vision that the robots are going to take over

27:47.040 --> 27:51.520
but you don't think that's going to happen like anytime soon right yeah I mean I well the funny

27:51.520 --> 27:56.800
thing is like I did a lecture once where where I said to everyone you know the char one hash function

27:56.800 --> 28:03.360
is absolutely fine right and then the next the next day yeah google released their their two pdfs

28:03.360 --> 28:07.520
that had the same char one hash right now that's embarrassing when that happens as a lecturer you

28:07.520 --> 28:12.240
know um so you know I don't want to say you know I don't want to say it could never happen what I

28:12.240 --> 28:15.840
would say is that the something that's really really good at go or something that's really really

28:15.840 --> 28:23.040
good at chess is really really good at chess and that is it right it will do nothing else right

28:23.040 --> 28:28.800
as far as I can tell human chess players are also good at other things and we we don't have that

28:28.800 --> 28:36.640
generalizability yet is this the age the difference between like specialized knowledge and yeah I mean

28:36.640 --> 28:40.560
again we could get bought we could get bogged down and what what the definition means but I think

28:40.560 --> 28:45.120
that our official general's urgency to most people watching is just something that kind of is a bit

28:45.120 --> 28:51.040
like a human right and certainly is very very general so you could say right this now is a totally

28:51.040 --> 28:54.560
different game learn to play it and it will go off and play it and it would still remember how to

28:54.560 --> 28:58.560
play chess and it could play all the games you know and it's just super it's super impressive

28:58.560 --> 29:04.080
though that doesn't exist will it exist I don't know I mean I think that if we keep making these

29:04.080 --> 29:10.000
models bigger we'll probably get to a point within a few decades where they are very impressive at a

29:10.000 --> 29:16.320
lot of different tasks but I still am not convinced yet that we've got any real strategy to get past

29:16.320 --> 29:20.800
the idea of just you need to like have a load of data right or a load of play a load of games

29:20.800 --> 29:27.280
my daughter can have a go at playing a semi-coherent game of chess just having been told the rules of

29:27.280 --> 29:31.280
chess and she didn't you know let's say she's not winning any competitions right not yet

29:31.280 --> 29:34.960
but she didn't need to play a million games against herself to work out what to do right

29:34.960 --> 29:40.800
there's something that she is doing but is much much more impressive than what this AI is doing

29:40.800 --> 29:44.800
that isn't to say the AI isn't incredibly impressive it's just very different I do think

29:44.800 --> 29:49.200
that the hype cycle is very different to what we actually see on the ground which is that basically

29:49.200 --> 29:54.720
a lot of the time I mean you know aside from playing games and reinforcement learning and large

29:54.720 --> 29:59.520
language models the majority of what people are doing is trying to find objects segment images

30:00.240 --> 30:04.720
and these things are mostly done in a supervised way and they don't generalize but we don't

30:04.720 --> 30:08.560
care because we were trying to find those specific objects so that's good and if we need them to do

30:08.560 --> 30:12.080
something else we'll retrain them to do something else yeah because my next question I think you've

30:12.080 --> 30:16.400
you've already given us the answer and maybe you can just elaborate is what is AI really good at

30:16.400 --> 30:22.240
compared and you know it just seems like it's like automation automation has its place but you still

30:22.240 --> 30:26.720
it takes like it's just correct me if I'm wrong but it seems to take away like low level tasks that

30:26.720 --> 30:30.880
are boring and monotonous or difficult for a human to do and then humans can concentrate on other

30:30.880 --> 30:37.040
things yeah what is AI really good at and where do you see it going yeah so AI is that automation

30:37.040 --> 30:41.200
is exactly what you what what you write on but with the caveat that you've got to have found a good

30:41.200 --> 30:45.840
way to train it to automate it won't just automate stuff you can't just stick it on a on a on a

30:45.840 --> 30:50.720
production line and say automate that for me because it did we won't know what to do so yeah

30:50.720 --> 30:55.440
from my point of view what AI is really good at is so before I worked in you know at machine

30:55.440 --> 30:59.520
learning and deep learning this was a normal computer vision researcher right and so I was

30:59.600 --> 31:04.320
you know this is like you know early 2010 something like this time before I mean literally deep

31:04.320 --> 31:09.760
learning appeared in about 2014 and before that we didn't have it right there were some networks

31:09.760 --> 31:13.520
but no one was really paying attention to them and everyone was just doing normal stuff right and

31:13.520 --> 31:17.680
what I would describe as image processing so if I wanted to find something in an image what I would

31:17.680 --> 31:23.200
be trying to do is come up with rules in my head about what I needed to do to that image to find

31:23.200 --> 31:27.440
those objects and then I would implement those rules and code so I'd say okay first of all go

31:27.520 --> 31:31.280
like we're trying to find you know something in MRI so first find all the bright pixels

31:31.280 --> 31:37.360
now find all the bright pixels but form a continuous blob that's of this size you know and I start

31:37.360 --> 31:43.760
and I try and design an algorithm to find whatever it was I was finding through these if statements

31:43.760 --> 31:48.160
and rules right it's just code and what machine learning lets me do is not worry about the the

31:48.160 --> 31:52.560
rules because the problem you have if you do if you do it by just coding is you get stuck in edge

31:52.560 --> 31:58.880
cases you get stuck on the you know you solve 90% of the issues pretty quickly because 90%

31:58.880 --> 32:04.160
of the images are trivial and then that 10% you just will never solve because they're just they

32:04.160 --> 32:09.040
don't apply the normal rules that everything else does and you know if you're looking at a sort of

32:09.040 --> 32:14.800
medical diagnosis AI or program that's a huge problem but you're just going to miss 10% because

32:14.800 --> 32:20.320
you couldn't deal with the edge cases and so from my point of view coming from image analysis

32:20.320 --> 32:24.400
that was what it let us solve it allows you because this mathematical function is very very

32:24.400 --> 32:29.680
complicated it can learn the edge cases if you give it sufficient numbers of them so you just so

32:29.680 --> 32:34.640
actually a lot of the time when I work with biologists or medics and and and they present the

32:34.640 --> 32:38.800
images I'll say these are all very nice but have you got any worse ones have you got any really

32:38.800 --> 32:44.320
bad ones because the more because the more bad stuff we give it the better it will get at at

32:44.320 --> 32:50.320
working when those things come along if you train your AI on a on a 3 or a 7 tesla MRI scanner

32:50.320 --> 32:55.360
which is super clear it won't work when you run it on a 1.5 you know so maybe you want to get

32:55.360 --> 32:58.800
samples from all the different scanners you know what I mean there's these kind of decisions

32:58.800 --> 33:03.920
it actually means that the problem is no longer one of which if statements do I need to write to

33:03.920 --> 33:09.440
get this to work it's now what kind of data and how do I present the data to this network to get

33:09.440 --> 33:14.160
it to work all right and that so it becomes much more about the input and output problem

33:14.160 --> 33:18.320
than it becomes about what you do in the middle which it just learns that's great I mean I just

33:18.320 --> 33:22.560
wanted to see if I understand the terms I see terms like artificial intelligence machine learning

33:22.560 --> 33:26.480
neural networks and deep learning we've covered all of those is that right yeah so I mean to go into

33:26.480 --> 33:32.640
some deep learning what I would say in terms of definition of deep learning is you know earlier

33:32.640 --> 33:36.720
I said that you might do I features for your problem right so suppose you're trying to sell

33:36.960 --> 33:41.840
cars what you might do is you might come up with some properties of cars that are relevant to its

33:41.840 --> 33:46.240
purchase price so you might say okay how many cylinders has it got how many how much horsepower

33:46.240 --> 33:50.480
has it got has it got leather seats right as it got air conditioning and you would have all these

33:50.480 --> 33:54.240
features and you come up with a list of let's say a hundred different properties of a car and you

33:54.240 --> 33:59.280
would stick them in some AI decision tree neural network doesn't matter and then it would spit out

33:59.280 --> 34:03.600
a value for you and you would train it on a bunch of examples and you would hopefully have a system

34:03.600 --> 34:08.960
that could really nicely predict the value of cars right now the problem is that suppose I've

34:08.960 --> 34:14.000
missed out a feature that's absolutely crucial to the value of cars suppose I forgot to put in

34:14.000 --> 34:18.400
the engine size and it turns out that 90 percent of the car's value is on how big the engine is

34:18.400 --> 34:22.880
right and so I've given it bad data then right and and then I have to go back and have to put

34:22.880 --> 34:26.400
data in again and I have to train it all again and you know it's a waste of time and what will

34:26.400 --> 34:30.480
actually happen if you try to implement a system where you missed out features is it would never

34:30.480 --> 34:37.200
work as well as you hoped and a car would come along that looked good on the features I did give it

34:37.200 --> 34:41.440
but actually had a really small engine and it would massively overvalue it or something like this

34:41.440 --> 34:45.680
right or undervalue it and you give away a really nice car for almost free what deep learning does

34:45.680 --> 34:51.200
is something called representation learning that's the because it's deeper it has the power to also

34:51.200 --> 34:55.840
learn the features as well as the decision based on those features so you might say well I can't

34:55.840 --> 35:00.800
bother to decide to decide all these features so I'm just going to dump the raw specs or a picture

35:00.800 --> 35:06.960
of the car in at the front and have it determine for me the value right and it would be looking at

35:06.960 --> 35:12.000
the size and model shape the color the size of the wheels and it would do all this and it would

35:12.000 --> 35:18.000
extract the features first inside the network and then it would use that to make a decision so deep

35:18.000 --> 35:24.000
learning is often described as just the same network but deeper but actually it's a different I

35:24.000 --> 35:29.840
think a different paradigm where you're basically no longer hand crafting what you put in you're

35:29.840 --> 35:34.960
just shoving all of it in and it works out what's useful and what's not and so you've explained

35:34.960 --> 35:40.160
neural networks already is all right yeah I mean so a neural network yeah so I we talked about how

35:40.160 --> 35:46.240
a neural network calculates a weighted sum so it takes some features at one layer and it waits

35:46.240 --> 35:49.680
them and then it calculates the sum of those for the next layer and we have something called an

35:49.680 --> 35:54.480
activation function in there as well which allows the basically it makes the function a lot more

35:54.480 --> 36:00.480
complex right it makes it non-linear makes it learn more powerful things modern deep networks

36:00.480 --> 36:07.120
actually have additional operations like convolutions and pooling operations which work on

36:07.120 --> 36:11.920
grids of data often right it doesn't have to but you know often they do so what you might do

36:12.560 --> 36:17.760
is instead of calculating a weighted sum of all the features you might slide a filter over the

36:17.760 --> 36:24.720
image to calculate filters at every location and so it's like a sort of a map of activations

36:24.720 --> 36:29.600
and then you might repeat that process over and over again so what what um deep networks are capable

36:29.600 --> 36:35.920
of doing convolutional networks is determining features across the whole image right or across

36:35.920 --> 36:40.560
the whole of the data stream and then repeating that process over and over again that's how they

36:40.560 --> 36:45.360
develop that's how they develop their representation learning right they use the filters to create

36:45.360 --> 36:50.560
interesting information before they make a decision you teach security at university but

36:50.560 --> 36:56.240
you're doing a lot of the ai side ai stuff as well um i think the the question a lot of people will

36:56.240 --> 37:00.880
be asking including myself is do i need to learn some kind of programming language and which language

37:00.880 --> 37:04.880
would it be would you recommend and do i need to learn like a whole bunch of math because it sounds

37:04.880 --> 37:08.800
like you know math is one of the or maths as we say in the uk is something that you have to

37:09.600 --> 37:17.680
it because you have to learn is that right to you know having having some idea of what's going

37:17.680 --> 37:22.000
on mathematically helps you from an intuition point of view right because i understand the back

37:22.000 --> 37:25.920
propagation process which is how the actual weights have adjusted and that allows me to

37:25.920 --> 37:30.000
understand what would happen if i connect two bits of network together in a weird shape or

37:30.000 --> 37:35.600
something like this but in practice actually day to day running of a deep network doesn't really

37:35.600 --> 37:41.520
involve any maths and and and there is some disagreement in the community about whether

37:41.520 --> 37:45.360
you really need to know math at all right you know i'm i sort of go back and forth i sometimes

37:45.360 --> 37:48.720
think it's useful and i sometimes think it's not i certainly don't think people should be if they

37:48.720 --> 37:53.840
don't like maths should be put off from having a go because i'm always an advocate for have a go at

37:53.840 --> 37:58.800
something you might really enjoy it right what i would say is that actually running a neural network

37:58.800 --> 38:03.520
doesn't require a lot of maths it just requires a bit of python basically so that's the language

38:03.520 --> 38:08.320
you normally use python um i have a love hate relationship with python i think but sometimes

38:08.320 --> 38:12.640
sometimes i just want to declare what my types are and stop having runtime errors for half an hour

38:12.640 --> 38:16.720
into something but what what they've done is they've got a lot of libraries like tensorflow and

38:16.720 --> 38:22.960
pytorch that operate but sit in python and then they they very very quickly go go down into C

38:22.960 --> 38:28.320
and CUDA for fast matrix multiplications which is all the stuff that goes on behind the scenes

38:28.320 --> 38:32.560
in your neural network so they're very very quick because they're not implemented end to end in python

38:33.120 --> 38:39.520
but python gives you a very convenient and nice way of doing all this you know loading the images

38:39.520 --> 38:43.600
it just appears as a kind of array you know you might have a list of images that you use for

38:43.600 --> 38:48.400
your data set and then you put that into a network and so on right you know a lot of it's just inputting

38:48.400 --> 38:52.880
out putting lists and dictionaries like the rest of python and so it makes things quite easy to use

38:52.880 --> 38:57.040
you know you'll have had a look at python but python for me is is a is a nice enough language

38:57.040 --> 39:00.560
in the sense that it's fairly easy to pick up particularly if you already know a language

39:00.640 --> 39:05.360
it's often language people recommend you start with anyway because it's fairly relaxed about

39:05.360 --> 39:11.760
syntax and just you making a total mess of it so that's you know that's always good um but doing

39:11.760 --> 39:16.720
going from knowledge of python to having implemented a deep network will not take you very long you

39:16.720 --> 39:21.040
will understand everything the first time but you can get give it a go and you can watch it training

39:21.040 --> 39:25.680
and you can start to you can start to pick up on what's going on and then you can make a change

39:25.680 --> 39:29.520
to the network and maybe improve your performance slightly do you have to write it from scratch

39:29.520 --> 39:34.000
or is like it's it's TensorFlow or something that like Google have created exactly they do

39:34.000 --> 39:38.080
a huge amount of heavy lifting right which is one of the reasons why you can kind of get away

39:38.080 --> 39:45.040
with not having always mathematical background so I mean I use PyTorch mainly and in PyTorch

39:45.040 --> 39:51.200
it handles all the weights and learning for you so you say I want my network to have this many

39:51.200 --> 39:55.440
layers and I want my layers to be like this and I want it to take an image of this size and turn it

39:55.440 --> 40:01.200
into a 10 class classification problem where I'm picking cats and dogs and airplanes or what have

40:01.200 --> 40:06.960
you and then it just trots off and does it and it just goes it goes puts the images in it it

40:06.960 --> 40:11.120
retrains the network and it puts the images in it retrains the network and it iterates and you can

40:11.120 --> 40:16.800
watch your learning rate really watch your loss function go down as it gets better and better

40:16.800 --> 40:21.040
every iteration instead eventually you can then just deploy it in some sort of production code or

40:21.040 --> 40:27.280
whatever and maybe without maybe test it first so but um you know like it does a huge amount

40:27.280 --> 40:31.920
there's a lot of mathematics behind the scenes not all of it particularly complicated but it's

40:31.920 --> 40:38.160
definitely a lot of it and it's all massively parallelized on a GPU and you know so you can

40:38.160 --> 40:44.720
actually get away with a few dozen lines of code to get a pretty nifty neural network going

40:44.720 --> 40:48.000
let's see that that's good to hear because you know when you start talking about the ins and

40:48.000 --> 40:52.560
outs it's like this sounds so complicated so it's like PyTorch just a library or something that you

40:52.560 --> 40:57.200
would import and then just you just send some commands to it yeah Torch started off as a

40:57.200 --> 41:03.360
machine learning library in um well it was written in C presumably but and CUDA but it was it was for

41:03.360 --> 41:08.160
Lua and again that's another language I have a should we say a very strong mixed opinions about

41:08.880 --> 41:13.600
however since then TensorFlow came along in Python I think it was seen as Python's more

41:13.680 --> 41:19.200
convenient for the majority of developers and so PyTorch spawned off Torch basically and is now

41:19.200 --> 41:27.520
the dominant library for this so TensorFlow is Google and PyTorch is Facebook AI or meta AI I

41:27.520 --> 41:32.480
suppose it is now and that's the one you would start with here if you were starting I yeah so

41:32.480 --> 41:36.400
this is it people have different opinions on this I think that the just give us your opinion

41:36.400 --> 41:41.280
because you know we I just sorry to interrupt I just want to put it this way I like to have

41:41.280 --> 41:46.960
parts like when I talk to experts like yourself it's like okay I'm new now how do I go from like

41:46.960 --> 41:51.200
knowing nothing to like at least getting started if so if you anything you can help me yeah well I

41:51.200 --> 41:55.280
mean I tell you what knowledge whatever yeah yeah yeah I would start with PyTorch personally

41:55.280 --> 42:00.160
right from a research point of view PyTorch is more flexible which helps me but it also doesn't

42:00.160 --> 42:06.640
require a lot of lines of code um to get running and it also does a nice thing where it doesn't hide

42:06.640 --> 42:10.560
away all of the details there's just enough detail in there that you can kind of type away and it'll

42:10.560 --> 42:16.000
kind of work but you do see the network going forward and learning and optimizing the weights

42:16.000 --> 42:19.920
and things like this there's a few lines of code but do that that you can kind of look at and go

42:20.400 --> 42:26.480
and then you kind of pick these things up right it's not a case that you just type PyTorch.train

42:26.480 --> 42:30.960
and pass it your input data and then it just does it and you have no idea what happened

42:30.960 --> 42:36.880
which I like because that wouldn't be fun right but also you wouldn't learn anything so I like

42:36.880 --> 42:43.280
PyTorch from that for that reason it also has a load of examples so if you go on the um if you

42:43.280 --> 42:48.000
go on the github repository for PyTorch or Torchvision you get the the whole you've got all the like

42:48.000 --> 42:53.920
core networks that are big from the literature in there and you've also got some examples of simple

42:53.920 --> 42:57.600
data problems and things like this that you can run from end to end and just basically run the

42:57.600 --> 43:01.040
file and it will start training a network and then you can delve in and see what it is it's

43:01.040 --> 43:05.200
actually doing. Do you need I think you mentioned a GPU do you need specific hardware or can you

43:05.200 --> 43:12.000
just run this on your laptop? You need you really need a um so PyTorch uses CUDA right so you really

43:12.000 --> 43:16.960
could do with using a um I don't know if PyTorch supports OpenCL I can't remember ideally you

43:16.960 --> 43:23.040
would have access to a CUDA enabled GPU that would make this process much much faster so as I mentioned

43:23.040 --> 43:28.800
the back end of of PyTorch and most of these deep learning libraries is written in C and CUDA and it's

43:28.800 --> 43:34.000
just massively parallelized matrix modifications most of the time and that is something that you

43:34.000 --> 43:38.560
don't want to be doing on a CPU right you can for very small networks run it on a CPU so if you

43:38.560 --> 43:43.840
download the simplest PyTorch example when you run it on a CPU it will run okay and you'll be able to

43:43.840 --> 43:49.200
see what happens. Anything with images anything where the dimensionality is high you're going to be

43:49.200 --> 43:53.760
waiting half an hour for it just to finish one pass and it won't get anything done. One other

43:53.760 --> 44:01.440
thing you might like to try is Google Colab so Google Colab is um is Google's public Jupiter

44:01.440 --> 44:06.960
notebook style laboratory environment that actually provides limited time but fair use

44:06.960 --> 44:12.080
access to GPUs to to have a go at these things right it's a it's a great place to go and you

44:12.080 --> 44:17.200
can also download loads of Colab notebooks existing implementations to test them out that's a great

44:17.200 --> 44:22.320
place to start you know I'm a big fan of Google Colab I think that as a platform it's really really

44:22.320 --> 44:27.200
useful um and you can actually pay us I mean I'm not I don't work for Google Colab you can pay a

44:27.280 --> 44:33.920
small subscription to get access to higher access or more preference more um should we say higher

44:33.920 --> 44:39.040
priority access to GPUs right that's what you know you can get um so it's it's like fair use

44:39.040 --> 44:42.960
normally so if you if you use it a lot you might have to wait for half a day or something.

44:42.960 --> 44:47.840
I mean in the best case scenario I'd come and attend one of your classes um but not everyone's

44:47.840 --> 44:53.040
going to be able to do that um do you have books or online courses or stuff that you would personally

44:53.760 --> 44:58.560
yeah so I mean what I always recommend to people is Andrew Ung's Coursera course on

44:58.560 --> 45:03.360
machine learning is a great place to start right now it's lower it's lower level so Andrew Ung is

45:03.360 --> 45:08.400
is is very well known in the machine learning community he's you know he's he's done a load

45:08.400 --> 45:13.360
of great work um his Coursera course is really good it's quite mathematical right so that isn't

45:13.360 --> 45:17.200
necessarily a problem you just have to go in knowing that's going to happen right but what it

45:17.200 --> 45:22.240
does do is it gives you a lot of information on stuff that we haven't really talked about so things

45:22.240 --> 45:26.800
like watching your learning rate your loss your your loss function go down right so if you if you

45:26.800 --> 45:31.760
draw a graph of your loss which is your error at the end of your network over time what should

45:31.760 --> 45:35.680
happen is it gets better and better right it goes down but it might not go down it might

45:35.680 --> 45:39.760
sort of do this a lot of machine learning is understanding what that means and what you could

45:39.760 --> 45:43.520
try and do to rectify that problem you know for the first for your first day of machine learning

45:43.520 --> 45:48.160
it's not important but over time some of the concepts that you talk about in this machine

45:48.240 --> 45:54.160
learning course will come in handy and there's a book by Yoshua Benjo called Deep Learning which

45:54.160 --> 45:59.760
also again a lot of maths in it but it covers a lot of the core concepts personally i'm a kind of

45:59.760 --> 46:05.840
i've always been a kind of learn by doing kind of a person right and so in what i like to do is just

46:05.840 --> 46:10.000
get on the pie torch or the TensorFlow tutorials and just start running some stuff and see what

46:10.000 --> 46:14.720
happens and if you know python or you know any language that's even plausibly similar to python

46:14.720 --> 46:18.080
you're you know you're going to have you're going to have a great time doing that i think

46:18.640 --> 46:22.000
especially for a lot of the audience if they starting out with us let's say there's younger

46:22.000 --> 46:26.320
people who starting their careers and i spoke about this in the beginning about you know people

46:26.320 --> 46:30.960
are worrying that this will take their jobs away but i'm assuming there's whenever i see the hype

46:30.960 --> 46:37.920
cycle there seems to be a lot of demand for ai skills huge demand yeah yeah there's a huge

46:37.920 --> 46:41.040
demand so i would say there's there's kind of you know you've got your different levels of

46:41.040 --> 46:45.120
sort of data analysts right so you've got people who are pretty good with a spreadsheet up to people

46:45.120 --> 46:49.840
who are working trying to train self-driving cars and things i suppose if i'm being sort of a bit

46:49.840 --> 46:54.160
bit random in my choices of job description and you know you've got anywhere in between there's

46:54.160 --> 46:59.600
huge demand everywhere so you know if you have any kind of data analysis ability if you can look at

46:59.600 --> 47:04.320
a table of data and start to pick out patterns and start to work out what's going on and make

47:04.320 --> 47:09.360
predictions on that data that's a really useful skill to have in lots and lots of jobs it's a very

47:09.360 --> 47:14.720
very um very very popular thing that people have so a lot we have a lot of graduates who graduate

47:14.720 --> 47:19.760
with a few modules in machine learning and a few modules in data analysis and things like this and

47:19.760 --> 47:23.680
they and they're in a really strong position these things are not you know you can learn these things

47:23.680 --> 47:27.520
yourself so you know you can go in i've got a data analysis course it's not very long obviously

47:27.520 --> 47:32.160
because you know youtube videos but i have some data analysis videos there are lots of data analysis

47:32.160 --> 47:36.160
videos on your youtube channel yeah yeah on our youtube channel computer file we have like a 10

47:36.160 --> 47:40.480
part series on data analysis which is just kind of like a taster but you can have a go at that

47:40.480 --> 47:46.240
there's lots of stuff on data analysis data analysis and uh modeling and machine learning in some

47:46.240 --> 47:50.640
ways go hand in hand it's often good to have a little bit of a look at both of them because

47:51.360 --> 47:56.080
you know cleaning data for example like you get you get a spreadsheet of data but doesn't make any

47:56.080 --> 48:00.560
sense it's unwise just to stick that straight into a neural network and see what you get out because

48:00.560 --> 48:05.360
there could be some complete you know it could be missing values there could be errors they could

48:05.360 --> 48:10.080
all just have hugely different scales of data these are all things to think about so some knowledge

48:10.080 --> 48:14.640
of how to prepare that data for let's say a downstream task like machine learning it's a

48:14.640 --> 48:18.960
really useful thing to know how to do as well i love that you're teaching at the university you're

48:18.960 --> 48:24.480
teaching security cybersecurity type stuff but you're also doing AI so there you see that like

48:24.480 --> 48:29.040
that's a really good mix and i'm assuming based on what you've just said you know it's a really good

48:29.040 --> 48:33.200
idea if you if you if you're into cyber or want to get into cyber to you know add this to your

48:33.280 --> 48:37.120
skill set yeah i mean i would be hard pressed to find any career that wouldn't be at least helped

48:37.120 --> 48:40.960
a little bit by knowing some data analysis of machine learning because just it just comes up a

48:40.960 --> 48:47.120
lot right you know and and also i mean as you know we already spoke about how people can be misled

48:47.120 --> 48:52.640
by the hype cycle right and and you will be much more resistant to this if you understand how these

48:52.640 --> 48:58.320
things work and that's going to put you in a good position i yeah i i think that um so i as it happens

48:58.320 --> 49:02.240
i teach security i partly i find it really interesting so i try and cling on to that module

49:02.240 --> 49:08.560
with you know with a with a vice grip and not let anyone else have it um i also teach cryptography

49:08.560 --> 49:14.000
uh at at university as well and get you back for some more interviews man yeah right so yeah by

49:14.000 --> 49:19.040
all means but so those are subjects i find i don't actively research day to day but i do find very

49:19.040 --> 49:22.960
very interesting and i do have some collaborations with because we have actual security researchers

49:22.960 --> 49:26.960
working at nottingham in lots of places we have good collaborations with them there is obviously

49:26.960 --> 49:32.000
machine learning involved in quite a lot of security because it's a it's a it's one of many

49:32.000 --> 49:38.160
strategies for detecting malware or for anomaly detection or you know any smart system that's

49:38.160 --> 49:42.560
doing something but hopefully you don't have to program all the rules yourself so yeah it does it

49:42.560 --> 49:46.800
does help i've got i think i've got a project uh an undergraduate student starting who's going to

49:46.800 --> 49:50.560
look at malware detection with a bit of machine learning as well and so she can bring the knowledge

49:50.560 --> 49:55.200
of the malware i can bring the knowledge of the uh of mostly the ai you know and it'll be it'll be

49:55.200 --> 49:59.760
great mic i always like to ask this question um if you were talking to your younger self let's say

49:59.760 --> 50:05.360
you were 18 or you know i don't know let's say not everyone is 18 who watches these videos but let's

50:05.360 --> 50:10.160
say they were 25 30 whatever yeah what would you advise someone to do based on you know what you've

50:10.160 --> 50:15.440
seen i think if you're if you're really interested in it in a career in cyber security or a career in

50:15.440 --> 50:20.960
uh machine learning it's worth noting that not everyone has a degree that does those things and

50:20.960 --> 50:25.520
that's fine right it's also fine if you do have a degree i see people saying well you don't need a

50:25.520 --> 50:30.320
degree for this or you do need a degree for this i actually think learn the skills right and then

50:30.320 --> 50:34.400
you get a job based on your experience it's you know and you're going to have a great time i think

50:34.400 --> 50:37.760
that again it's not one of these debates i like to get into because everyone has their own career

50:37.760 --> 50:41.600
path that they want to follow if you're if you're if you did a degree in something completely different

50:41.600 --> 50:44.720
and you've worked in a job you're not really enjoying and you want to try something new i think

50:44.720 --> 50:50.720
that's absolutely fine have a go there's so many resources online that there weren't 20 30 years ago

50:51.280 --> 50:55.280
that there are you know people doing interviews and videos on different topics that you can just

50:55.280 --> 51:00.720
watch and learn about and as i say i'm a very hands-on person if i want to try and learn a skill

51:00.720 --> 51:06.480
i'm just going to try and do it and it will probably go really wrong the first time um so i think that

51:07.120 --> 51:13.120
practice right and this is true of coding as well i think i'm big big um believer but coding is

51:13.200 --> 51:16.960
mostly practice people say like how did you know that was going to be a bug because i've seen it

51:16.960 --> 51:21.600
so many times before you know like because it happens all the time i think yeah that would be

51:21.600 --> 51:26.400
what i would do find something you love doing and do more of that you know i program at home for fun

51:26.400 --> 51:30.640
and it's partly because i find it fun and also sometimes i want to learn something new i did a

51:30.640 --> 51:35.760
video a year or two ago on the enigma machine right i don't need to program the enigma machine

51:35.760 --> 51:39.440
for my job i just thought it was super interesting and i just sat at home and did it and i learned

51:39.440 --> 51:43.760
quite a lot actually about the whole process and the history of it by just having to implement the

51:43.760 --> 51:49.200
thing and so i think yeah i think crack on and learn would be what i would do i love that i mean

51:49.200 --> 51:56.400
and i just have to say this you are doctor uh mike you you you got phd yeah is that right in

51:56.400 --> 52:01.280
what in what was it in computer vision so i mean what i really love about this and um this is just

52:01.280 --> 52:08.080
my opinion so i don't want to put you on the spot but um i love that you as someone with a phd

52:08.880 --> 52:14.320
are not excluding excluding people who perhaps never had that opportunity and i love that you're

52:14.320 --> 52:19.840
encouraging everyone you know just to go for it don't let your limitations or yeah lack of resources

52:19.840 --> 52:24.240
stop you sorry i mean as it happens like i wasn't i was a pretty average student at school right i

52:24.240 --> 52:30.000
mean i i didn't i didn't do much much there wasn't much in terms of computer science um in at school

52:30.000 --> 52:35.600
when i was younger it was it was you know let's use microsoft word and and and let's try that out

52:35.600 --> 52:39.520
and so i didn't i barely did any computing at all i could only a little i could only a program

52:39.520 --> 52:45.200
a tiny bit when i arrived at university loads of people arrive at university with huge programming

52:45.200 --> 52:49.840
experience but and loads of people arrive with no programming experience and we always say to them

52:49.840 --> 52:54.640
you'll all be the same in the end right like that's the whole point of a degree and it's a whole

52:54.640 --> 52:59.280
point of what we teach i think that it's never too late to get to get into computers and learn

52:59.280 --> 53:02.800
about programming and stuff i try and teach people to program all the time i mean not all of them

53:02.800 --> 53:08.240
were interested which is annoying um but you know so like if it was up to me all my family

53:08.240 --> 53:11.760
would be able to program because i'd be giving them extra lessons but some of them want to do

53:11.760 --> 53:16.160
other things apparently but yeah i'm not i'm not a gate i don't want to be a gatekeeper because

53:16.960 --> 53:21.280
that's not going to get more people doing cool computer stuff there are some things where a

53:21.280 --> 53:25.600
massive specialism is important right you know i'm not proposing to go into a hospital and start

53:25.600 --> 53:29.440
surgery on people because you need a lot of training to do these things but i also think

53:29.440 --> 53:33.840
that if someone wanted to be a surgeon they should crack on and do the training right you know you

53:33.840 --> 53:39.760
know i think you can learn those skills um and you know we require if you're going to work at

53:39.760 --> 53:45.040
university we usually require a phd and that's something that universities require but there's

53:45.040 --> 53:48.720
a great deal i don't know about the real world and industry about people who are watching will

53:48.720 --> 53:53.600
know way more about than me and that's also fine right you know everyone's got their own expertise

53:53.600 --> 53:58.400
so i like to learn from those people and hope i can teach them a bit about the things i know about

53:58.400 --> 54:03.920
i love that i love that another thing i mean i i said 18 but i i get a lot of pushback sometimes

54:03.920 --> 54:09.120
on these videos and i'm not sure if you've heard this question before am i too old to start learning

54:09.120 --> 54:15.920
ai no um no i mean consider also that the majority of academics who are using ai aren't

54:16.560 --> 54:22.560
18 year old fresh graduates they are researchers that have been doing it for decades because you

54:22.560 --> 54:26.320
know so we've all had to learn it from scratch as well right like i say deep learning only appeared

54:26.320 --> 54:32.400
in 2014 so it's been a mad rush since then there's loads of scope to learn um and i don't think it

54:32.400 --> 54:36.480
takes to get a little bit going it doesn't take that many hours you know if you want to do something

54:36.480 --> 54:40.880
you know around your job or whatever it is your current life situation is i think it's doable

54:40.880 --> 54:46.720
i love that any closing thoughts no i think um i hope people found it interesting right and i

54:46.720 --> 54:51.840
happy to come back and talk about more topics in detail but i think that you know i love um i love

54:51.840 --> 54:55.680
my job and telling people about stuff that i think is interesting so i would encourage

54:55.680 --> 54:59.600
those people to go off and and look into it in a bit more detail and have a go to download a

54:59.600 --> 55:03.200
pie talk tutorial and start running it and you'll train a deep network and then when someone goes

55:03.200 --> 55:06.400
all this deep learning is a bit scary you can go well actually i did that last week and it wasn't

55:06.400 --> 55:11.280
that wasn't that difficult yeah that that's what i'd suggest so for everyone watching please put

55:11.280 --> 55:15.760
in the comments below topics that you would like us to discuss definitely want to try and get mic

55:15.760 --> 55:20.880
back so um let us know what you want us to talk about uh computer file has a lot of fantastic

55:20.880 --> 55:24.880
videos that mic has created um so go and have a look at those i'll i'll link some of those below

55:25.680 --> 55:36.800
please give us your feedback mic thanks so much thanks so much love to be here

