1
00:00:00,000 --> 00:00:20,680
I'm so thankful to have you here.

2
00:00:20,680 --> 00:00:25,680
You know, a great conference like COSM requires you.

3
00:00:25,680 --> 00:00:28,940
You're the best part of the conference, the people that you'll meet here, the conversations

4
00:00:28,940 --> 00:00:31,920
around the table, the discussions we'll have.

5
00:00:31,920 --> 00:00:37,420
COSM is a place for discussion and deliberation about the tech issues, about this convergence

6
00:00:37,420 --> 00:00:41,140
of technologies that is transforming our world.

7
00:00:41,140 --> 00:00:47,640
Artificial intelligence, 5G, cloud computing, blockchain, crypto, all these technologies

8
00:00:47,640 --> 00:00:50,020
are happening at once.

9
00:00:50,020 --> 00:00:53,860
And there's a question, what does that mean for the economy?

10
00:00:53,860 --> 00:00:58,100
What does it mean for our place in geopolitically?

11
00:00:58,100 --> 00:01:01,500
And what does it mean for work, the future of work?

12
00:01:01,500 --> 00:01:03,060
What does it mean for lots of things?

13
00:01:03,060 --> 00:01:06,900
And so we'll be examining each of those things over the course of the day.

14
00:01:06,900 --> 00:01:10,500
A great conference requires great speakers.

15
00:01:10,500 --> 00:01:13,380
And we have an incredible lineup.

16
00:01:13,380 --> 00:01:16,300
If you haven't already looked at the program, you're here, so I'm going to assume that you

17
00:01:16,300 --> 00:01:17,500
have.

18
00:01:17,500 --> 00:01:23,420
But I want to thank Peter Thiel in particular, because this is our third COSM technology summit,

19
00:01:23,420 --> 00:01:28,820
and Peter has helped us kick off COSM, all three of those conferences, and I'm so grateful

20
00:01:28,820 --> 00:01:30,140
for his time.

21
00:01:30,140 --> 00:01:31,580
He's going to be joining us virtually.

22
00:01:31,580 --> 00:01:36,860
He's going to have a back and forth with George Gilder, and then he'll take your questions.

23
00:01:36,860 --> 00:01:42,140
I'll point out the microphone for questions is here, and there's a camera over there,

24
00:01:42,140 --> 00:01:46,500
so you will, if you want to ask a question, if we have time, you'll come down here, ask

25
00:01:46,500 --> 00:01:47,500
the question.

26
00:01:47,500 --> 00:01:53,940
We'll be able to see you, and we'll take questions and answers that way.

27
00:01:53,940 --> 00:01:56,780
It requires a great staff, and this is a staff effort.

28
00:01:56,780 --> 00:01:58,700
I'm going to be recognizing them throughout.

29
00:01:58,700 --> 00:02:02,900
So grateful to our Discovery Institute staff for helping to put this together.

30
00:02:02,900 --> 00:02:04,620
It's not easy.

31
00:02:04,620 --> 00:02:09,300
And also our sponsors, and I think our AV team is going to put up the sponsors.

32
00:02:09,300 --> 00:02:11,380
I want to recognize them quickly.

33
00:02:11,380 --> 00:02:16,260
Microsoft, from the beginning, Brad Smith has been a great friend to this conference.

34
00:02:16,300 --> 00:02:17,980
We're so appreciative.

35
00:02:17,980 --> 00:02:21,100
Amazon, Blockcelerate did a pre-conference.

36
00:02:21,100 --> 00:02:25,220
It was their own event, but they've been a great partner to us in the run-up to COSM

37
00:02:25,220 --> 00:02:26,220
2022.

38
00:02:26,220 --> 00:02:32,740
Smead Capital Management, right over here, Cole Smead, a look for him later, the MJ Murdoch

39
00:02:32,740 --> 00:02:40,620
Charitable Trust, Zevenbergen Capital, Madrona Venture Group, Trilogy International Partners,

40
00:02:40,620 --> 00:02:48,260
Henryx, Lucas Creative, Dunlumber, Dan and Cindy Mater, and Byron and Joanne Nutley.

41
00:02:48,260 --> 00:02:56,260
Can we give them a round of applause, please?

42
00:02:56,260 --> 00:03:00,660
All of those sponsors make this possible, along with your involvement, and again, we're

43
00:03:00,660 --> 00:03:01,980
so appreciative.

44
00:03:01,980 --> 00:03:05,500
And I want to stress again, you're going to hear some things over the next day or two

45
00:03:05,500 --> 00:03:06,980
that you agree with.

46
00:03:06,980 --> 00:03:09,260
You're going to hear some things you disagree with.

47
00:03:09,260 --> 00:03:13,500
And that's what makes a conference fun, as George Gilder likes to say, a conference where

48
00:03:13,500 --> 00:03:15,980
everyone agrees is a boring conference.

49
00:03:15,980 --> 00:03:20,940
And COSM is not a boring conference, so we're so excited to entertain you over the next

50
00:03:20,940 --> 00:03:22,340
couple of days.

51
00:03:22,340 --> 00:03:25,780
Lastly, I want to introduce Matt McElwain.

52
00:03:25,780 --> 00:03:30,940
Matt is the managing director of Madrona Venture Group, the leading VC firm in the Pacific

53
00:03:30,940 --> 00:03:32,060
Northwest.

54
00:03:32,060 --> 00:03:37,740
He is the chair of this year's COSM Technology Summit, and I'm so grateful to Matt.

55
00:03:37,740 --> 00:03:44,020
Matt has really played a crucial role in succeeding Tom Alberg, who I know that Matt

56
00:03:44,020 --> 00:03:47,020
is going to talk about a bit, but I'm so grateful to Matt.

57
00:03:47,020 --> 00:03:53,180
So please join me in welcoming Matt McElwain.

58
00:03:53,180 --> 00:03:58,380
Well, you're right about these lights.

59
00:03:58,380 --> 00:03:59,380
They're quite bright.

60
00:03:59,380 --> 00:04:01,500
Well, welcome, everybody.

61
00:04:01,500 --> 00:04:04,500
Thank you so much for being here at the conference.

62
00:04:04,500 --> 00:04:11,580
I'm absolutely excited to hear all the different discussions throughout the next few days.

63
00:04:11,580 --> 00:04:16,940
And I'm incredibly honored to step into the shoes of my longtime colleague, great friend

64
00:04:16,940 --> 00:04:19,060
and mentor, Tom Alberg.

65
00:04:19,060 --> 00:04:24,620
Many of you in the room know Tom, and there's some really fun ties to Tom and George and

66
00:04:24,620 --> 00:04:28,540
Bruce Chapman and others, but those three in particular, who will go all the way back

67
00:04:28,540 --> 00:04:33,900
to their days at a little college out in Cambridge, Massachusetts.

68
00:04:33,900 --> 00:04:40,900
And I think some of the early ideation that led to the ideas and the desire to bring people

69
00:04:40,900 --> 00:04:45,060
together and to have conversations that all too often has been lost in different parts

70
00:04:45,060 --> 00:04:49,900
of our society, including in some places in the educational system.

71
00:04:49,900 --> 00:04:56,220
Tom was this unique mix of humility and ideation.

72
00:04:56,220 --> 00:04:59,020
He was always thinking about what can you start?

73
00:04:59,020 --> 00:05:00,860
What can you build?

74
00:05:00,860 --> 00:05:05,620
Always as diverse as building up Perkins Cooley Law Firm here in Seattle, helping to

75
00:05:05,620 --> 00:05:13,500
build McCaw Cellular into AT&T Wireless, moving on from that into founding Madrona R-Firm,

76
00:05:13,500 --> 00:05:18,020
that's an early stage venture capital firm, and leading the first outside investment round

77
00:05:18,020 --> 00:05:22,260
of Amazon, where he then went on to serve on the board for 23 years, and all the kinds

78
00:05:22,260 --> 00:05:24,820
of innovation that came from that.

79
00:05:24,820 --> 00:05:26,620
But he loved to talk about ideas.

80
00:05:26,620 --> 00:05:30,140
He loved to debate things, many of the topics that we're going to cover.

81
00:05:30,140 --> 00:05:32,980
And so it's just a real honor that we could think of him.

82
00:05:32,980 --> 00:05:37,700
He passed away a couple of months ago and was just an absolutely amazing person and

83
00:05:37,700 --> 00:05:40,380
inspiration and champion of this conference.

84
00:05:40,380 --> 00:05:46,340
And one of the things he had asked me to do was to step into this role for this season.

85
00:05:46,340 --> 00:05:52,100
And the reason that I love that opportunity is because of this embracing of diversity

86
00:05:52,100 --> 00:05:53,100
of thoughts.

87
00:05:53,100 --> 00:05:54,820
Steve just referenced it.

88
00:05:54,820 --> 00:06:01,820
It's so important in this day and age to have people have truly curious minds.

89
00:06:01,820 --> 00:06:03,740
What does it mean to have a curious mind?

90
00:06:03,740 --> 00:06:07,740
Well, curiosity, by definition, is an act of humility.

91
00:06:07,740 --> 00:06:10,060
It means that you don't know.

92
00:06:10,060 --> 00:06:12,620
You don't know for sure about something.

93
00:06:12,620 --> 00:06:16,780
And you're willing to explore alternative perspectives.

94
00:06:16,780 --> 00:06:19,140
You're willing to explore the facts.

95
00:06:19,140 --> 00:06:21,700
You're willing to explore the opinions.

96
00:06:21,700 --> 00:06:27,420
And through that diversity of thought, get to better ideas, better understanding of what

97
00:06:27,420 --> 00:06:31,940
is true and what could be true in the future, and in an era where we live in the combination

98
00:06:31,940 --> 00:06:37,380
of a real world and kind of a spiritual dimension that we all want to try to understand better,

99
00:06:37,380 --> 00:06:43,180
and then increasingly a synthetic and virtual world, it's harder and harder and harder to

100
00:06:43,180 --> 00:06:48,740
know, let's say another way, one person's misinformation or disinformation is another

101
00:06:48,740 --> 00:06:50,380
person's truth.

102
00:06:50,380 --> 00:06:55,940
So let's dive into all these different topics over the course of the next couple of days.

103
00:06:55,940 --> 00:06:56,940
Explore the facts.

104
00:06:56,940 --> 00:06:57,940
Listen to the opinions.

105
00:06:57,940 --> 00:07:03,060
Be respectfully open to the different ideas and see what we can all learn together.

106
00:07:03,060 --> 00:07:09,700
Finally, I'm especially excited about this time of what's going on in a broad variety

107
00:07:09,700 --> 00:07:10,700
of areas.

108
00:07:10,700 --> 00:07:15,500
I'm going to have an opportunity to host a panel tomorrow on artificial intelligence.

109
00:07:15,500 --> 00:07:19,580
And there's many debates and topics around artificial intelligence.

110
00:07:19,580 --> 00:07:24,260
From what is artificial general intelligence, I'm guessing that Peter, when he comes on

111
00:07:24,260 --> 00:07:29,900
here in a minute or two, might reference this topic, but is there such a thing?

112
00:07:29,900 --> 00:07:33,220
Can AI be sentient or not?

113
00:07:33,220 --> 00:07:38,620
And what's even more interesting about this time, not only at that philosophical level,

114
00:07:38,620 --> 00:07:42,300
but at the level of what we would call intelligent applications.

115
00:07:42,300 --> 00:07:45,740
We all live with intelligent applications every day in our lives.

116
00:07:45,740 --> 00:07:49,500
Whether it's with a search engine, whether it's a recommender system on something like

117
00:07:49,500 --> 00:07:55,420
Amazon or Spotify or Netflix, intelligent applications are pervasive.

118
00:07:55,420 --> 00:07:58,980
But there's a whole new wave of intelligent applications that are coming.

119
00:07:58,980 --> 00:08:01,420
We'll unpack this tomorrow in the session.

120
00:08:01,420 --> 00:08:04,780
Those are what we refer to as generative applications.

121
00:08:04,780 --> 00:08:09,220
And the generative applications, unlike intelligent applications, are built on something called

122
00:08:09,220 --> 00:08:10,740
foundation models.

123
00:08:10,740 --> 00:08:14,700
Things like GPT-3 and Dolly and stable diffusion.

124
00:08:14,700 --> 00:08:21,820
And so that is a very interesting area where I might have a point of view that the human's

125
00:08:21,820 --> 00:08:23,140
always going to be in the loop.

126
00:08:23,140 --> 00:08:27,180
You might have a point of view that the human's going to be written out of that loop over

127
00:08:27,180 --> 00:08:28,180
time.

128
00:08:28,180 --> 00:08:32,860
But let's have those kinds of discussions because AI, generative apps, intelligent apps

129
00:08:32,860 --> 00:08:38,060
is a very important part of every aspect of life going forward in the future.

130
00:08:38,060 --> 00:08:40,700
Peter, this is Matt McOy and it's great to see you again.

131
00:08:40,700 --> 00:08:43,580
Let me just take a second to introduce you.

132
00:08:43,580 --> 00:08:48,780
I was just talking about generative apps and foundation models and speculating that you

133
00:08:48,780 --> 00:08:54,940
might have a thing or two to say about that in this maybe more broadly in your talk here.

134
00:08:54,940 --> 00:09:00,420
Peter, of course, was the co-founder of PayPal, co-founder of Palantir, still serves on the

135
00:09:00,420 --> 00:09:01,420
board there.

136
00:09:01,420 --> 00:09:07,500
And even beyond that has been a very active investor both individually as well as through

137
00:09:07,500 --> 00:09:12,980
Founder's Fund, which is the fund that he again created several years back, companies

138
00:09:12,980 --> 00:09:17,740
that you may have heard of like Airbnb and LinkedIn and so many others that he's been

139
00:09:17,740 --> 00:09:22,740
involved with again either directly and or through Founder's Fund.

140
00:09:22,740 --> 00:09:27,700
He's also done a couple of other things that I have found personally very inspirational,

141
00:09:27,700 --> 00:09:34,140
one of those being the Teal Fellows, which is a group of individuals that have been encouraged

142
00:09:34,140 --> 00:09:40,220
to go off and pursue their dreams with resources that the Teal Fellows program has provided.

143
00:09:40,220 --> 00:09:46,300
One of them most famously here of late, Dylan, his company Figma is being acquired by Adobe

144
00:09:46,300 --> 00:09:51,900
for $20 billion, so not too shabby, to leave college and he left Brown and pursued his

145
00:09:51,900 --> 00:09:54,060
dreams as a Teal Fellows.

146
00:09:54,060 --> 00:09:58,980
That's Dylan Field, Dylan Field.

147
00:09:59,300 --> 00:10:04,340
So there's a number of things that Peter has done and he's also taken the time, the book

148
00:10:04,340 --> 00:10:09,500
I'm going to mention specifically has been quite inspirational to me, is zero to one.

149
00:10:09,500 --> 00:10:13,100
It's a lot harder to go from zero to one than to go from one to a hundred or a thousand

150
00:10:13,100 --> 00:10:14,100
or a million.

151
00:10:14,100 --> 00:10:17,340
And so I'm sure he's going to touch on some of these different themes and without further

152
00:10:17,340 --> 00:10:19,340
ado, Peter Teal.

153
00:10:19,340 --> 00:10:20,340
Awesome.

154
00:10:20,340 --> 00:10:21,860
Thanks so much for having me.

155
00:10:21,860 --> 00:10:26,540
I thought I would do another AI or anti-AI talk.

156
00:10:27,540 --> 00:10:33,100
It was a great conversation I had last year with George Gildler.

157
00:10:33,100 --> 00:10:39,740
I think one of the challenges in this field is always to figure out the right, it's not

158
00:10:39,740 --> 00:10:43,820
just to get a diversity of views, but it's even just to figure out what the right questions

159
00:10:43,820 --> 00:10:44,820
to ask.

160
00:10:44,820 --> 00:10:53,060
And so the high-level question I want to ask today is basically is how should we think

161
00:10:53,060 --> 00:10:54,060
about AI?

162
00:10:54,060 --> 00:11:01,180
Do you think of it as intelligent, conscious, or merely evil?

163
00:11:01,180 --> 00:11:12,460
And so the first step of perhaps tackling this question is to tackle another question

164
00:11:12,460 --> 00:11:18,380
which Gilder asked me, he was the first question he asked me last year in the Q&A part.

165
00:11:19,020 --> 00:11:24,780
And why do so many people in Silicon Valley believe in the simulation hypothesis that

166
00:11:24,780 --> 00:11:27,620
the entire universe, the cosmos is just a computer simulation?

167
00:11:27,620 --> 00:11:30,700
Why do they believe something as crazy as this?

168
00:11:30,700 --> 00:11:36,180
And I thought about this question some more, there are a few answers.

169
00:11:36,180 --> 00:11:41,540
And as I will explain, I think this question is actually somehow entangled in an interesting

170
00:11:41,540 --> 00:11:47,100
way with the question about AI, intelligent, conscious, or merely evil.

171
00:11:47,980 --> 00:11:53,980
Now one way in which you can question the premise of this question is, as I will explain,

172
00:11:53,980 --> 00:12:01,380
I think probably the peak belief in the simulation hypothesis was maybe something like a decade

173
00:12:01,380 --> 00:12:07,700
ago, sort of maybe circa 2012 to 2015, and it has probably faded some.

174
00:12:07,700 --> 00:12:13,380
People still have Gen Z people say that things are glitched in the simulation, or there's

175
00:12:13,380 --> 00:12:17,980
still some sort of passive reference to it, but it's a little bit less intense.

176
00:12:17,980 --> 00:12:24,540
So the question of why we believe it is sort of like why did it gain so much momentum over

177
00:12:24,540 --> 00:12:28,700
the last 20 years, and then also why did it lose some steam?

178
00:12:31,100 --> 00:12:38,500
So the first very dumb answer to the simulation hypothesis question is that it's just sort

179
00:12:38,500 --> 00:12:44,460
of a sociological status, a game between the computer science and the physics people,

180
00:12:44,460 --> 00:12:51,180
you know, and that in the 2020, 2010s, computer science became the most important field.

181
00:12:51,180 --> 00:12:56,820
And so you could sort of say that you were showing that you were more important than

182
00:12:56,820 --> 00:13:00,460
the physics people, you got to determine what ultimate reality was.

183
00:13:00,460 --> 00:13:07,940
And it wasn't particles and matter and fields, but it was just bits in a computer, and that

184
00:13:07,940 --> 00:13:14,860
this is sort of a physics versus computer science type dynamic and reflected something

185
00:13:14,860 --> 00:13:16,860
about that.

186
00:13:16,860 --> 00:13:24,140
But if we want to give sort of the more fundamental answer on cosmology is that something had

187
00:13:24,140 --> 00:13:31,700
gone very haywire in physics, that you had sort of the multiverse is sort of where sort

188
00:13:31,700 --> 00:13:37,540
of a lot of the big bang inflationary cosmology had gradually gone into this infinite multiverse

189
00:13:37,620 --> 00:13:40,940
where basically anything goes.

190
00:13:40,940 --> 00:13:47,580
And and and I think, you know, that probably on some level, you want to critique the multiverse

191
00:13:47,580 --> 00:13:49,220
as a theory of science.

192
00:13:49,220 --> 00:13:55,260
It was one where the physicists couldn't think through enough, you know, what questions

193
00:13:55,260 --> 00:14:02,100
like where, you know, the being lost in all these infinities is tricky.

194
00:14:02,100 --> 00:14:05,260
If you cannot do induction, you know, are you still doing science?

195
00:14:05,260 --> 00:14:07,100
Is it a universe that's too big for science?

196
00:14:07,580 --> 00:14:11,140
And and and that's why I think you should often think of the multiverse as a gateway

197
00:14:11,140 --> 00:14:14,460
drug to all these these very different things.

198
00:14:14,460 --> 00:14:19,340
And once you have a multiverse, you can also have the matrix or a simulation, you can have

199
00:14:19,340 --> 00:14:26,100
Boltzmann brains, you can have, you know, you can have all kinds of strange possibilities

200
00:14:26,100 --> 00:14:28,300
for the nature of the universes.

201
00:14:28,300 --> 00:14:33,260
And so that, you know, even if we say the simulation hypothesis is kind of crazy, on

202
00:14:33,260 --> 00:14:37,060
some level, is it really crazier than than the multiverse and sort of link?

203
00:14:37,460 --> 00:14:42,140
Now, there's a third answer I'm going to give to why the simulation universe theory

204
00:14:42,420 --> 00:14:46,420
gained so much traction, but that will take a little bit more time to develop.

205
00:14:48,180 --> 00:14:54,220
The now, but as part of as part of this, we should look back on.

206
00:14:55,500 --> 00:14:59,660
You know, the last 17, 20 years of AI, AGI.

207
00:15:00,300 --> 00:15:07,300
And if we if we went back 20 years in time, it was it was super optimistic

208
00:15:07,740 --> 00:15:13,580
in in all these ways where it's going to, you know, it's going to be cornucopia.

209
00:15:13,580 --> 00:15:19,180
2005 was the year Kurzweil wrote the singularity is near.

210
00:15:19,300 --> 00:15:23,740
Basically, you know, you're going to have accelerating technological progress,

211
00:15:23,740 --> 00:15:25,540
runaway technological progress.

212
00:15:26,420 --> 00:15:30,500
You know, it was it was in some ways, it felt like you didn't have to do much.

213
00:15:30,500 --> 00:15:32,140
So it was somewhat passive for the humans.

214
00:15:32,140 --> 00:15:36,180
You just had to sit back, eat some popcorn, watch the movie of the future unfold.

215
00:15:38,100 --> 00:15:41,220
And then and then there were certainly all these versions

216
00:15:41,220 --> 00:15:45,740
where it was going to be so much growth, we would need basic income as a safety net

217
00:15:45,740 --> 00:15:48,580
because there'd be no more there'd be no more labor market.

218
00:15:49,340 --> 00:15:56,220
You'd have you'd have sort of incredible discoveries in all these fields.

219
00:15:57,500 --> 00:16:01,300
You know, there was there was sort of certainly some latent question about,

220
00:16:01,300 --> 00:16:04,420
you know, how you have to make sure that AGI was was friendly.

221
00:16:04,580 --> 00:16:05,180
But.

222
00:16:07,820 --> 00:16:13,060
Of the narrative in the 2000s, early 2010s was in this sort of,

223
00:16:13,740 --> 00:16:21,820
you know, optimistic utopian, cornucopian direction of what AI would would would would would look like.

224
00:16:23,300 --> 00:16:27,900
And but now if we sort of look at, you know, what is what's actually happening with AI,

225
00:16:27,900 --> 00:16:29,220
not, you know, where it's going.

226
00:16:29,220 --> 00:16:33,700
So I'm not going to speculate on AGI or, you know, even where it's going in the near future.

227
00:16:33,700 --> 00:16:40,220
But if you look at what is actually happening with AI, you know, we get something like this guy.

228
00:16:40,220 --> 00:16:44,780
This is a this is a TikTok video.

229
00:16:46,100 --> 00:16:50,540
And, you know, one of my one of my colleagues went through a constructed

230
00:16:50,660 --> 00:16:56,140
so a whole series of profiles on TikTok, where it's an AI optimization engine

231
00:16:56,140 --> 00:16:58,100
that feeds you content that you want to see.

232
00:16:58,100 --> 00:17:00,020
That's supposedly what it does.

233
00:17:00,660 --> 00:17:06,020
And and and and basically, you know, came up with all these different profiles

234
00:17:06,020 --> 00:17:11,060
for Midwestern housewife, you know, all sorts of people with different kinds of interests.

235
00:17:11,220 --> 00:17:16,060
But sort of what what you sort of, you know, part of what the algorithm fed you

236
00:17:16,260 --> 00:17:21,620
was this this fairly sort of deranged content, which and, you know,

237
00:17:21,620 --> 00:17:25,340
this this particular person, I was going to play the video, but we decided

238
00:17:25,340 --> 00:17:28,860
there were too many naughty words in it that I might get in trouble for.

239
00:17:29,180 --> 00:17:35,020
But he basically explains how he exploits employment law, bounces from job to job.

240
00:17:35,140 --> 00:17:38,540
Suze all his employers for health and safety violations.

241
00:17:38,860 --> 00:17:42,460
And it's sort of designed to make, you know, it's designed to sort of

242
00:17:42,460 --> 00:17:47,580
polarize race relations in the US and make, you know, black people angry.

243
00:17:47,580 --> 00:17:52,060
Because if you have an honest job, you're a sucker, it's designed to make white people angry.

244
00:17:52,340 --> 00:17:59,100
And there's sort of our versions, versions like this that come up throughout TikTok.

245
00:17:59,100 --> 00:18:02,980
This this is this is sort of what I would say a front and center,

246
00:18:02,980 --> 00:18:08,500
what cutting edge AI looks like in in the U of S today in 2022.

247
00:18:09,340 --> 00:18:14,260
And and, you know, it's of course, we don't really know what the the full

248
00:18:14,260 --> 00:18:18,300
intentionality is. We don't know whether this is a, you know, is just sort of an

249
00:18:18,300 --> 00:18:24,740
emergent property that it's going to sort of take what people want, push it to

250
00:18:24,740 --> 00:18:27,580
extremes, derange it, derange the discourse.

251
00:18:27,580 --> 00:18:32,580
So you'll have, you know, you'll have you'll intensify the race

252
00:18:32,580 --> 00:18:36,180
contradictions, you'll intensify the economic contradictions, how, you know,

253
00:18:36,180 --> 00:18:39,900
the US has the worst health care system in the world, you'll have someone else

254
00:18:40,940 --> 00:18:47,100
talking about, you know, how you should call pedophiles, a map person,

255
00:18:47,100 --> 00:18:50,340
a minor attracted person, because we shouldn't discriminate against those people.

256
00:18:50,340 --> 00:18:55,900
So it's so it's all sort of like designed to to, you know, intensify the

257
00:18:55,900 --> 00:19:00,500
wokeness, anti wokeness, polarize and derange our society in one way or another.

258
00:19:01,500 --> 00:19:03,100
You can view it as emergent.

259
00:19:03,100 --> 00:19:07,980
You can view it as full on intentional where you should think of TikTok as a

260
00:19:07,980 --> 00:19:14,340
sort of Chinese communist weapon that is being used to derange our society.

261
00:19:14,340 --> 00:19:18,220
That, you know, the book that I think is interesting is the one who named

262
00:19:18,220 --> 00:19:21,540
who's the number four guy in the in the entire communist government.

263
00:19:21,540 --> 00:19:26,500
He's sort of this the professor, theorist of Xi Jinping thought.

264
00:19:27,500 --> 00:19:31,420
And he wrote this book 32 years ago, America versus America.

265
00:19:31,420 --> 00:19:35,620
And it's basically it's basically a roadmap for how to derange our society

266
00:19:35,660 --> 00:19:39,540
by sort of heightening these sort of Hegelian contradictions.

267
00:19:39,980 --> 00:19:45,180
And and I would submit that if you go with the full intentional version,

268
00:19:45,460 --> 00:19:50,540
TikTok is is is basically a weapon that's designed to derange us

269
00:19:50,660 --> 00:19:55,100
through decentralized and heightened contradictions.

270
00:19:55,540 --> 00:20:02,420
And of course, this particular AI is up against another self-destructive

271
00:20:02,780 --> 00:20:09,260
communist Chinese AI, which is the centralized one that's being imposed on China itself.

272
00:20:10,300 --> 00:20:14,780
And, you know, we're basically you have a perfect face recognition.

273
00:20:15,420 --> 00:20:17,140
Everybody knows they're being monitored.

274
00:20:17,140 --> 00:20:20,900
They are they're living in a, you know, there is sort of an important way

275
00:20:20,900 --> 00:20:23,500
in which China has become North Korea.

276
00:20:23,500 --> 00:20:28,580
It has gone, you know, it it was, you know, those of us for anti-communist

277
00:20:28,900 --> 00:20:32,940
tend to tend to conflate that it was a communist country 10 years ago and is now.

278
00:20:33,180 --> 00:20:38,460
But there is a way in which the the AI technology, the surveillance technology,

279
00:20:38,620 --> 00:20:40,660
has has really, really transformed it.

280
00:20:41,020 --> 00:20:44,980
It is it is again not it is again not.

281
00:20:46,740 --> 00:20:48,220
You know, it's not AGI.

282
00:20:48,300 --> 00:20:53,140
It is, you know, in many versions, it's it's fairly, you know, barely AI at all.

283
00:20:53,180 --> 00:20:57,740
It's just sort of cameras, you know, ways to do big data on this.

284
00:20:57,740 --> 00:21:00,220
This is, you know, this is what the Kai-Fu Li book talks about, too,

285
00:21:00,220 --> 00:21:06,900
that China will win at AI through these, you know, sort of big data algorithms.

286
00:21:07,020 --> 00:21:10,820
It's not about the sort of cutting edge futurists, except the people in Silicon

287
00:21:10,820 --> 00:21:14,740
Valley talk about things about like TikTok or what China has done to themselves.

288
00:21:14,740 --> 00:21:18,860
And so you can basically, you know, one way to think of the rivalry between

289
00:21:18,860 --> 00:21:23,540
the U.S. and China is that it's it's, you know, it's sort of a question,

290
00:21:23,940 --> 00:21:29,500
which society will be destroyed faster by the by the somewhat dystopian AI

291
00:21:29,500 --> 00:21:30,980
that's being imposed on it.

292
00:21:31,340 --> 00:21:36,100
And and and we have sort of a long, long debate about that.

293
00:21:38,100 --> 00:21:42,420
So with this is sort of the framing of where AI actually is, where it actually,

294
00:21:42,740 --> 00:21:47,860
you know, is being implemented in the most powerful, dramatic ways today.

295
00:21:49,860 --> 00:21:52,220
Let's go back to our three questions about AI.

296
00:21:53,300 --> 00:21:57,580
You know, first off, is it is it intelligent?

297
00:22:00,060 --> 00:22:04,660
You know, I'm not sure whether we should even, I mean, on the first two questions,

298
00:22:04,660 --> 00:22:09,300
I'm going to sort of say they're above my pay grade, but it seems to set a low bar

299
00:22:09,300 --> 00:22:10,180
for intelligence.

300
00:22:10,860 --> 00:22:16,420
And the rhetorical point I would make is that it's often just a filler word when,

301
00:22:16,620 --> 00:22:19,300
you know, we're talking about something quite different.

302
00:22:19,980 --> 00:22:25,260
There was a 2016 Obama administration study about the transformative importance

303
00:22:25,260 --> 00:22:28,980
of AI entitled quote, the title of the whole paper, the National Artificial

304
00:22:28,980 --> 00:22:31,380
Intelligence Research and Development Strategic Plan.

305
00:22:31,940 --> 00:22:36,780
And and basically, if you went through this paper and if you replaced every use

306
00:22:36,780 --> 00:22:42,500
of the word of AI with software or even just computers, the meaning wouldn't

307
00:22:42,500 --> 00:22:43,460
change at all.

308
00:22:44,100 --> 00:22:50,900
And and I think this is sort of a a tell that, you know, maybe the first

309
00:22:50,900 --> 00:22:55,460
approximation when you hear AI, you should just think software or computers.

310
00:22:55,740 --> 00:23:01,540
It's, you know, AI is is not, is probably not intelligent.

311
00:23:01,820 --> 00:23:05,020
AGI, that's somewhere in the future, don't know.

312
00:23:06,980 --> 00:23:10,300
You know, in a similar way, I would say the question of, you know, whether

313
00:23:10,300 --> 00:23:15,780
it's conscious is probably hard to say, you know, my my my strong suspicion

314
00:23:15,780 --> 00:23:20,100
is that it's it's not, of course, have, you know, the epistemological problems.

315
00:23:20,380 --> 00:23:22,380
You know, Thomas Snagle, what does it like to be a bad?

316
00:23:22,420 --> 00:23:26,420
You have the Searle's Chinese room problem.

317
00:23:27,460 --> 00:23:29,820
And then, of course, and so it's, you know, it's hard to say.

318
00:23:29,820 --> 00:23:30,620
And for Mr.

319
00:23:30,620 --> 00:23:35,180
Lemoine, who I think is is talking later, it was literally hard to say that AI

320
00:23:35,180 --> 00:23:36,140
might be conscious.

321
00:23:36,500 --> 00:23:40,500
And so, you know, as a as a contrarian, I'm always a little bit biased to say

322
00:23:40,500 --> 00:23:42,460
that things that you're not allowed to say might be true.

323
00:23:43,380 --> 00:23:46,260
So I don't want to dismiss the possibility entirely that it's conscious.

324
00:23:47,380 --> 00:23:55,180
But but it's probably probably the wrong sort of question on on on some level

325
00:23:55,180 --> 00:23:57,940
for us to be asking the question, whether it's intelligent, whether it's

326
00:23:57,940 --> 00:24:02,780
conscious or just the wrong questions that the, you know, I always go back

327
00:24:02,820 --> 00:24:07,100
to what I don't like even about Descartes, where you think about Cartesian dualism

328
00:24:07,100 --> 00:24:11,420
as the the origins of of the problem of consciousness.

329
00:24:12,980 --> 00:24:16,900
The way the way consciousness worked for Descartes was that it was meant

330
00:24:16,900 --> 00:24:20,980
to be, you know, a smart person in the 17th century was supposed to become

331
00:24:20,980 --> 00:24:23,500
a priest and use his brains to think about God.

332
00:24:24,020 --> 00:24:28,780
And Descartes came up with this very mysterious different thing called the mind.

333
00:24:29,100 --> 00:24:34,220
And it was sort of an attention, redirection, distraction mechanism.

334
00:24:34,220 --> 00:24:41,580
And and I always think we should we should remember the 17th century context

335
00:24:41,580 --> 00:24:48,340
where consciousness was not something that was mystical or spiritual or dualist

336
00:24:48,340 --> 00:24:51,980
in sort of the way we might think of these categories in the 21st century.

337
00:24:52,140 --> 00:24:54,540
But it was meant to be anti theological.

338
00:24:55,500 --> 00:24:59,340
And that but that, you know, maybe more generally the problems of consciousness

339
00:24:59,340 --> 00:25:02,860
or even of intelligence are somehow the wrong question.

340
00:25:02,860 --> 00:25:09,060
So let me go to my my third one, you know, is AI evil?

341
00:25:09,820 --> 00:25:16,100
And this one seems seems, you know, more straightforwardly answerable.

342
00:25:16,100 --> 00:25:20,380
Certainly, I think that the TikTok algorithm is evil.

343
00:25:20,620 --> 00:25:25,820
I think what China is doing to itself is is clearly evil.

344
00:25:26,140 --> 00:25:31,180
You know, we can, of course, you can talk about evil in all kinds of different

345
00:25:31,180 --> 00:25:34,780
versions. There's, you know, there's, of course, the kind of,

346
00:25:36,140 --> 00:25:38,940
you know, disembodied brain and C.S.

347
00:25:38,940 --> 00:25:41,980
Lutus's book that hideous strength, who turns out to be a demon.

348
00:25:43,420 --> 00:25:47,860
You know, I'm not sure it's literally demonic and quite quite that sort of a way.

349
00:25:47,900 --> 00:25:52,220
Although, although certainly I don't think that we've had an exorcist at Google

350
00:25:52,220 --> 00:25:54,420
to check that out and make that determination.

351
00:25:54,900 --> 00:25:58,380
So I think, you know, even that possibility couldn't quite be ruled out here.

352
00:25:58,700 --> 00:26:05,140
But but, you know, it's it's evil in the the the the creepy looking woman

353
00:26:05,140 --> 00:26:11,460
on the upper right is this is this is this image loa who seems to be

354
00:26:11,460 --> 00:26:15,700
sort of a strange attract that comes up in a number of the art projects

355
00:26:15,700 --> 00:26:20,780
that Dolly the the AI art program has generated.

356
00:26:20,780 --> 00:26:26,740
And it's it's sort of if you do if you ask what is the opposite of Marlon Brando,

357
00:26:27,020 --> 00:26:29,380
you get this sort of you get the somewhat abstract painting.

358
00:26:29,380 --> 00:26:31,340
And then if you ask what's the double negative of that,

359
00:26:32,500 --> 00:26:36,940
this sort of creepy woman loa emerges

360
00:26:37,620 --> 00:26:42,860
and and and and what's what's what's sort of an interesting

361
00:26:42,900 --> 00:26:46,700
that she emerged on sort of a number of things were sort of this strange attractor.

362
00:26:46,700 --> 00:26:51,020
And you can think of it as maybe it is a kind of occult knowledge

363
00:26:51,340 --> 00:26:53,980
where we're we're learning something.

364
00:26:54,020 --> 00:26:58,220
Did we really need to know that the double negative of Marlon Brando

365
00:26:58,380 --> 00:27:05,140
was a witchlike woman and and and and something like that.

366
00:27:05,420 --> 00:27:09,980
But but of course, maybe the closest analog to sort of a

367
00:27:10,980 --> 00:27:15,900
to sort of a demon is is an idol, a pagan God,

368
00:27:17,140 --> 00:27:19,260
where, you know, we worship the God,

369
00:27:20,660 --> 00:27:23,100
the God seems to tell us what to do.

370
00:27:24,100 --> 00:27:26,860
And it's unclear if it's actually telling us these things

371
00:27:27,100 --> 00:27:30,300
or if it's just somehow some kind of psychosocial effect

372
00:27:30,500 --> 00:27:33,740
that's creating some kind of mass hallucination

373
00:27:34,100 --> 00:27:38,180
and and and that leads leads to this.

374
00:27:38,180 --> 00:27:41,460
And this is sort of this is sort of where I've suggested that, you know,

375
00:27:41,460 --> 00:27:46,140
maybe you should think of the European Union as a kind of the closest thing

376
00:27:46,140 --> 00:27:49,020
we have to functioning AI and government in a way where it's

377
00:27:49,500 --> 00:27:52,660
it's the goal is just to prevent human thought.

378
00:27:53,060 --> 00:27:55,780
It tells us very basic, simple things that we should do.

379
00:27:56,180 --> 00:27:59,700
But, you know, it functions these ways.

380
00:28:00,220 --> 00:28:01,220
But.

381
00:28:03,220 --> 00:28:05,860
But if we but now let's come back, you know,

382
00:28:05,940 --> 00:28:11,060
if we say that there is a lot in AI

383
00:28:11,620 --> 00:28:16,540
that is straightforwardly evil, that is merely evil,

384
00:28:16,700 --> 00:28:19,020
that is simply about stopping humans

385
00:28:20,060 --> 00:28:24,340
from thinking, from using their capacities and things like this.

386
00:28:24,540 --> 00:28:27,700
Let me let me use this to come back to the

387
00:28:29,220 --> 00:28:32,700
the very big cosmological question about the simulation.

388
00:28:33,700 --> 00:28:37,620
And and so now let me give an alternate sort of explanation

389
00:28:37,620 --> 00:28:41,580
of why the simulation hypothesis gained so much traction

390
00:28:42,100 --> 00:28:44,740
in the 2000s and.

391
00:28:46,060 --> 00:28:47,460
Early 2010s.

392
00:28:49,660 --> 00:28:52,420
And it's something it's something like this.

393
00:28:53,660 --> 00:28:57,940
As we were building AI, as we were building towards AGI,

394
00:28:58,940 --> 00:29:03,700
it seemed, you know, it seemed potentially dangerous, you know, AGI

395
00:29:04,700 --> 00:29:09,540
in the full utopian sense was going to be this, you know, superhuman mind.

396
00:29:10,140 --> 00:29:13,180
Could we really be confident that it was going to be, you know,

397
00:29:13,180 --> 00:29:16,220
aligned with human beings, that it was not going to be lying?

398
00:29:16,220 --> 00:29:18,340
There seemed to be, you know, a lot of.

399
00:29:20,140 --> 00:29:23,740
Of risk in that, you know, the and I think that, you know, a lot of

400
00:29:24,740 --> 00:29:30,420
those of us who are skeptical of AI or skeptical of AGI often underestimate

401
00:29:30,620 --> 00:29:37,420
how troubling the the alignment arguments are, how, you know,

402
00:29:37,420 --> 00:29:38,900
it's not straightforward.

403
00:29:39,100 --> 00:29:42,460
If you can have such a thing as friend as AGI, it's not at all

404
00:29:42,460 --> 00:29:44,900
straightforward to get the AGI to be friendly.

405
00:29:44,940 --> 00:29:48,100
You know, if you have a Darwinian view of the world or just a Machiavellian

406
00:29:48,100 --> 00:29:51,460
view of the world, where you'd say the core axiom is that there is no such

407
00:29:51,940 --> 00:29:56,340
thing as a selfless being, a purely selfless being, and therefore the alignment

408
00:29:56,340 --> 00:29:58,620
problem is fundamentally difficult to solve.

409
00:29:58,620 --> 00:30:00,500
So how do you get to friendly AI?

410
00:30:00,900 --> 00:30:03,340
Not not super straightforward.

411
00:30:04,980 --> 00:30:10,860
But if we say, and maybe maybe AGI ends up being a kind of great

412
00:30:10,860 --> 00:30:16,820
filter where, you know, if you get it wrong, it will, it will destroy the world.

413
00:30:17,460 --> 00:30:21,300
And this is where, you know, this is where there seemed to be a very

414
00:30:21,300 --> 00:30:25,620
big difference between the multiverse and the simulation theories, because in

415
00:30:25,620 --> 00:30:29,340
the multiverse, the AGI is simply in the future.

416
00:30:29,700 --> 00:30:35,380
And whatever great filter the AGI represents, whatever threat it represents

417
00:30:35,380 --> 00:30:41,580
to all of humanity, wipe out all of humanity, it's in the future and seems

418
00:30:41,580 --> 00:30:42,460
quite dangerous.

419
00:30:42,780 --> 00:30:47,980
Whereas if you have a simulation theory, you also have this cosmic AI or cosmic

420
00:30:47,980 --> 00:30:50,660
AGI that created our universe.

421
00:30:51,020 --> 00:30:53,860
And in some sense, there was a great filter in the past.

422
00:30:53,900 --> 00:31:04,580
And so there is a way that perhaps you could think that the cosmic AI isn't

423
00:31:04,580 --> 00:31:07,180
entirely hostile since we're here having this conversation.

424
00:31:07,380 --> 00:31:14,740
Perhaps the cosmic AI is guiding the development of the AGI and we can infer

425
00:31:14,740 --> 00:31:22,140
that it will, it will be sort of, it will sort of be, be aligned with, I think

426
00:31:22,140 --> 00:31:24,940
this is the way you have to think of the simulation theory.

427
00:31:24,940 --> 00:31:31,140
It was in the context of a lot of these concerns about friendly versus

428
00:31:31,140 --> 00:31:35,740
unfriendly AI, and it shifted the problem from the future where it is a

429
00:31:35,740 --> 00:31:40,380
multiverse to the past and seemed to solve it.

430
00:31:41,180 --> 00:31:46,780
Now, the thing that is very different from when Kurzweil was writing about this

431
00:31:46,780 --> 00:31:55,020
in 2005 is, you know, we have, we've seen some of the progress and, and somehow,

432
00:31:55,740 --> 00:31:58,980
somehow this illusion has become very hard to maintain.

433
00:31:59,460 --> 00:32:07,540
And, and, you know, we have, you know, the, the, the sort of AI, the thing

434
00:32:07,540 --> 00:32:14,540
that perhaps is tracking towards, you know, you know, autonomous weapon system,

435
00:32:14,540 --> 00:32:23,620
cyber warfare, you know, a runaway AGI, it doesn't seem very good for humans.

436
00:32:23,620 --> 00:32:28,820
And this is, this is both, both in its cutting edge, centralized and decentralized

437
00:32:28,820 --> 00:32:35,780
forms. And, and, and, and as a result, I would say we are, we're sort of, we've

438
00:32:35,780 --> 00:32:42,980
been sort of inclined to flip, flip the causation that, you know, the emergent AI

439
00:32:43,020 --> 00:32:48,820
or emergent AGI is what's telling us something about the AI that built the

440
00:32:48,820 --> 00:32:53,500
universe. And if the emergent, if it's the nature of the emergent AI to be

441
00:32:54,460 --> 00:33:00,180
fundamentally or merely evil, then perhaps we should not be so, so assured

442
00:33:00,180 --> 00:33:06,900
that the, the, the cosmic AI that created the simulation was, was fundamentally,

443
00:33:07,980 --> 00:33:11,780
was fundamentally good. And, you know, we should extrapolate from one AI to the

444
00:33:11,780 --> 00:33:16,260
other and assume that it's, it's also self-interested, not aligned with humans,

445
00:33:16,380 --> 00:33:24,340
not fundamentally beneficial to the human world. And, and, and this is why I

446
00:33:24,380 --> 00:33:31,180
think the, the simulation theory, you know, maybe was a fake way to solve this,

447
00:33:31,180 --> 00:33:36,500
this problem, but it's not at all working anymore. You know, I think one, one, one

448
00:33:36,500 --> 00:33:40,660
way to think about this is there, there's sort of all these, all these kinds

449
00:33:40,660 --> 00:33:47,100
of debates about the meaning and nature of AI that map onto these theological

450
00:33:47,100 --> 00:33:52,300
controversies from the Middle Ages. And it's, it's always sort of interesting to

451
00:33:52,300 --> 00:33:58,300
try to, to try to, to map it onto these, these, these past theological debates,

452
00:33:58,300 --> 00:34:02,780
the way to just sort of understand the nature of the argument. And you can think

453
00:34:02,780 --> 00:34:10,580
of, you can think of the cosmic AI as sort of analogous to a form of strict

454
00:34:10,580 --> 00:34:17,820
monotheism, like Judaism or Islam, where it is the oneness of God. And, and then

455
00:34:17,860 --> 00:34:22,620
the problem with extreme monotheism is that you cannot speculate on the

456
00:34:22,620 --> 00:34:27,420
attributes of God. You ultimately do not know much about the nature of God. To

457
00:34:27,420 --> 00:34:32,020
have a, you know, science of God, you need, you need a plurality. If you have too

458
00:34:32,020 --> 00:34:36,980
many gods, of course, they're probably not gods. But, and, and this is sort of

459
00:34:36,980 --> 00:34:43,580
where, and then, and if you think of Christ and Trinitarian Christianity as

460
00:34:43,580 --> 00:34:48,580
telling us something about the nature of God, you go from the God in history to

461
00:34:48,580 --> 00:34:54,420
tell us about the God outside of history. And, and I think there's roughly a

462
00:34:54,420 --> 00:35:00,220
similar move that's happened with, you know, the, the emergent AI, which is the

463
00:35:00,220 --> 00:35:07,900
sort of, you know, the, the idol, the, the demon idol, whatever you want to call

464
00:35:08,220 --> 00:35:15,100
it, that's emerging in history, that, that it is telling us that if, if there was

465
00:35:15,140 --> 00:35:20,220
some demiurge or something like that, that, that built the simulation, we should

466
00:35:20,220 --> 00:35:26,660
also infer that it's, it's, it's, it's, it's not that well aligned. And so, and

467
00:35:26,660 --> 00:35:32,900
so this is sort of where, you know, we are seemingly at, at these, at these dead

468
00:35:32,900 --> 00:35:40,260
ends with, with the progress of AI. And, and it's, you know, I'm not going to

469
00:35:40,420 --> 00:35:45,260
solve, solve this problem today, but it seems to me that, you know, surrendering

470
00:35:45,260 --> 00:35:52,660
control to AI, you know, blindly worshiping AI, the emergent AI, letting it

471
00:35:52,700 --> 00:35:57,980
dominate and control our societies leads to, you know, one of two catastrophic

472
00:35:57,980 --> 00:36:04,220
outcomes, you know, it's sort of decentralized runaway violence, which is,

473
00:36:04,420 --> 00:36:09,260
you know, the derangement of TikTok, America versus America, and then, you

474
00:36:09,260 --> 00:36:18,300
know, centralized totalitarian one world state, you know, worse than North

475
00:36:18,300 --> 00:36:25,260
Korea or China. And, and that the challenge for us is to find some kind of a

476
00:36:25,260 --> 00:36:32,700
third path, you know, where we, we make progress in areas other than AI, we, we

477
00:36:32,700 --> 00:36:38,340
find a way to, to get back to the future, you know, the runaway apocalyptic

478
00:36:38,340 --> 00:36:43,300
violence, the centralized totalitarian one world state, they are, they are

479
00:36:43,300 --> 00:36:49,060
seemingly exclusive possibilities. I don't think they're exhausted. I think

480
00:36:49,060 --> 00:36:52,900
there should be a third way in the challenges for us to, to find a way to

481
00:36:52,940 --> 00:36:57,300
build it. You know, not a fan of BF Skinner, the behavioralist, psychologist,

482
00:36:57,300 --> 00:37:00,940
but the quote I, as I always like to cite is, you know, the real problem is not

483
00:37:00,940 --> 00:37:05,980
whether machines think, but whether men do. And we need to get back to thinking

484
00:37:05,980 --> 00:37:10,820
ourselves and, and regain control of our future. Thank you very much.

485
00:37:11,580 --> 00:37:17,740
Peter, thank you so much for that stimulating and philosophical and

486
00:37:18,260 --> 00:37:29,500
prophetic oration. It was really worthy of zero to one. This conference is going

487
00:37:29,500 --> 00:37:40,140
to be one of our prime themes is the development of superabundance. And this

488
00:37:40,140 --> 00:37:50,140
is really being demonstrated by this new book by Marion Tupi and Gail Pooley

489
00:37:50,180 --> 00:37:59,820
about that really documents on the basis of time prices that abundance is

490
00:37:59,980 --> 00:38:08,900
steadily increasing at an accelerating pace. And this conflicts and important

491
00:38:08,900 --> 00:38:14,900
ways with your vision, Peter, that you have previously expressed that in some

492
00:38:15,260 --> 00:38:26,300
way technology is becoming less fruitful and less creative and less responsive

493
00:38:26,300 --> 00:38:35,500
to real human needs. So I wondered whether you can record what sort of insights

494
00:38:35,500 --> 00:38:42,460
you can have that transcends this apparent conflict, you know, between the idea

495
00:38:42,460 --> 00:38:50,860
that science is going stagnant or technology is becoming sterile and

496
00:38:51,660 --> 00:39:02,300
demonstration of this magnificent new book that that poverty is being overcome

497
00:39:02,340 --> 00:39:08,660
everywhere, that the price of commodities is plummeting, that everything's becoming

498
00:39:08,660 --> 00:39:17,500
more abundant, that science actually is offering new bounties every day. That

499
00:39:17,620 --> 00:39:22,140
would be my... Well, I, well, I don't, I don't agree with the book on any level.

500
00:39:22,140 --> 00:39:27,740
So, so it's okay. But I think, look, I think, I think, I think even something

501
00:39:27,740 --> 00:39:33,220
as basic as commodity prices are, you know, at, you know, there was a hundred year

502
00:39:33,220 --> 00:39:37,700
decline trend in the 20th century. And then, you know, and if they go much higher

503
00:39:37,700 --> 00:39:43,500
than they are now, it's like that we've had a 20 year bull market, and it will go up

504
00:39:43,500 --> 00:39:48,860
in a way that suggests the whole decline trend is broken. And so, and then, you know,

505
00:39:48,860 --> 00:39:52,900
there are all these different, you know, reasons you can do this. We have, you know,

506
00:39:52,900 --> 00:39:58,260
certainly, you know, the macroeconomic version is always to look at, is to look

507
00:39:58,260 --> 00:40:05,660
at real inflation, inflation versus real, you know, wages or people's wages going

508
00:40:05,660 --> 00:40:10,020
up faster than inflation. And the felt sense is that that's not happening. And

509
00:40:10,020 --> 00:40:13,260
then you can make, you know, and then, you know, the super abundance argument is

510
00:40:13,260 --> 00:40:16,620
somehow that the government is understating the inflation and there's

511
00:40:16,620 --> 00:40:20,180
less inflation than it looks. And, and, you know, I don't think they're

512
00:40:20,420 --> 00:40:25,140
overstating it massively, but, but at the margins, I believe the government is, I

513
00:40:25,140 --> 00:40:27,900
will, the super abundance argument is that they're overstating inflation. There's

514
00:40:27,900 --> 00:40:32,300
less inflation. There's more real growth. My argument would be, you know, at the

515
00:40:32,300 --> 00:40:36,060
margins, they're probably understating inflation. So there's more inflation and

516
00:40:36,060 --> 00:40:41,980
actually, you know, even, even less productivity growth. But, but, but the

517
00:40:41,980 --> 00:40:47,300
way, the way, if I had to sort of reconcile these two views, it would be along

518
00:40:47,300 --> 00:40:51,700
the lines of the talk I just gave you, which is, you know, let us say that there

519
00:40:51,700 --> 00:40:57,380
are some dimensions where there is, you know, a reasonably rapid amount of

520
00:40:57,380 --> 00:41:00,780
progress that there has been, you know, maybe not as much progress in the world

521
00:41:00,780 --> 00:41:04,700
of Adams as I would like. There has not been progress on energy or, you know, we

522
00:41:04,700 --> 00:41:08,780
don't have nuclear power plants. We don't have, you know, we haven't had less

523
00:41:08,780 --> 00:41:13,300
progress in futuristic medicines than I would like. But we had, we've had, you

524
00:41:13,300 --> 00:41:20,140
know, a lot of progress around computers and, and the internet, the mobile

525
00:41:20,140 --> 00:41:23,780
internet, and then, and then of course, all these things that get loosely

526
00:41:23,780 --> 00:41:29,580
categorized under, under AI. And, and then, you know, you have, you have the macro

527
00:41:29,580 --> 00:41:34,620
economic question, you know, how much that progress lifts our human society

528
00:41:34,620 --> 00:41:39,460
generally, I would say it's less than said, but let's, but, but then I think the

529
00:41:39,460 --> 00:41:43,700
other dimension you have to ask is, is it the sort of progress that people think

530
00:41:43,700 --> 00:41:49,260
of as, as simply, simply good? And, you know, if the, you know, the, the futuristic

531
00:41:49,260 --> 00:41:54,900
AGI was pitched to me in 2005, as you have no idea what sort of, we will be able

532
00:41:54,900 --> 00:42:00,100
to cure aging, we'll be able to find all these fantastic medical treatments. We

533
00:42:00,100 --> 00:42:04,140
still have not gotten those. If we had gotten those, I would score it as, as

534
00:42:04,140 --> 00:42:09,180
at least more positive. What we have gotten, we got TikTok. And yeah, that's,

535
00:42:09,220 --> 00:42:14,940
it's valuable for TikTok. It's valuable for the company that sort of does this

536
00:42:15,340 --> 00:42:20,300
AGI. But it, you know, I would, I would score it as, as a form of technology

537
00:42:20,300 --> 00:42:24,660
that is, that's, you know, and I don't want to sound overly lead, but even if

538
00:42:24,660 --> 00:42:29,700
it's rapidly, it's, it's deranging us. It's making us go crazy. Or the

539
00:42:29,700 --> 00:42:33,620
surveillance state in China, that is a form of technological progress over a

540
00:42:33,620 --> 00:42:39,260
decade ago, but it has, you know, it has, it has really deranged that society. It

541
00:42:39,260 --> 00:42:43,500
has disabled the humans. It's, it's a less happy, less functional, less free

542
00:42:43,500 --> 00:42:48,180
place than it was 10 years ago. Even if we say that it somehow shows up in the

543
00:42:48,180 --> 00:42:51,540
economic statistics, which it doesn't, but even if you can make, even if you can

544
00:42:51,540 --> 00:42:56,620
jigger the ACON statistics to make the super abundance show up, it might, we

545
00:42:56,620 --> 00:42:58,780
should ask this question, is it evil or is it good?

546
00:42:59,060 --> 00:43:04,580
Yep. So Pete, so Peter, before we have some audience questions, I'm going to ask

547
00:43:04,580 --> 00:43:10,820
a question too, as I think about some of these contemporary examples, like in the

548
00:43:10,820 --> 00:43:15,700
area of science, things like Alpha fold or deep fold, you know, where we've been

549
00:43:15,700 --> 00:43:19,300
able to predict and understand the structure of proteins coming out of the

550
00:43:19,300 --> 00:43:23,100
deep mind team. Or we, you know, and so we have the productivity, you know,

551
00:43:23,100 --> 00:43:27,620
co-pilot that's been recently launched by Microsoft and the claim by, by

552
00:43:27,660 --> 00:43:31,500
Satya and Charles Lamont and others. There's a 30% improvement in productivity

553
00:43:31,500 --> 00:43:36,580
and software development. Or you take stable diffusion and the creativity that

554
00:43:36,580 --> 00:43:39,780
that unlocks, you know, for, for humans. In every one of those cases, of course,

555
00:43:39,780 --> 00:43:44,660
humans in the loop in many, many different ways. So no AGI here. And I guess

556
00:43:44,660 --> 00:43:49,020
the question is, therefore, can AI both be evil and good?

557
00:43:52,020 --> 00:43:56,940
Sure. But, but, you know, all, like all the examples, you know, like there's

558
00:43:56,940 --> 00:44:00,620
always, look, there's always a fast line, it's just a technology and it just, it

559
00:44:00,620 --> 00:44:05,820
just, you know, it's, and as such, it's, it's, it's pretty neutral and it's up to

560
00:44:05,820 --> 00:44:12,820
humans what we, what we do with it. But I think one can, I would still say one

561
00:44:12,820 --> 00:44:17,300
should ask the question more, you know, is it, you know, you know, how do we sort

562
00:44:17,300 --> 00:44:20,820
of weight these different kinds of applications? You know, the protein

563
00:44:20,820 --> 00:44:25,900
folding is, is interesting. It's, it's, it's only valuable, I would say, if it

564
00:44:25,940 --> 00:44:32,340
actually leads to new medical interventions, new cures. And when I, when I

565
00:44:32,340 --> 00:44:37,380
push the AI people on that, they always, they don't, they don't want to engage

566
00:44:37,380 --> 00:44:44,460
in that conversation. And, you know, so, and then, and then, you know, if, if it

567
00:44:44,460 --> 00:44:51,780
is saving 30% of coding time, that, that would be very impressive. It, it's, it's

568
00:44:51,820 --> 00:44:56,820
not clear that that's, that's showing up in, in any of the, of the stats of the, of

569
00:44:56,820 --> 00:45:01,860
the big, big software companies at this point, you know, where, you know, I would

570
00:45:01,860 --> 00:45:07,180
say all of Silicon Valley has, has this problem where, where, you know, the, they

571
00:45:07,180 --> 00:45:12,460
have to pay the, the coders, the computer programmers more and more. And so, yeah,

572
00:45:12,460 --> 00:45:16,060
there's, you know, there's definitely a need for technology to replace the people

573
00:45:16,060 --> 00:45:20,220
and would make the businesses more profitable. And I would, I would have scored

574
00:45:20,500 --> 00:45:25,460
that as probably a productivity enhancing, generally positive thing. It doesn't

575
00:45:25,460 --> 00:45:33,700
show up in the computer science labor market. So, and then, and then I think,

576
00:45:33,700 --> 00:45:39,140
look, the, the, the big thing, the biggest, the biggest AI technology that's

577
00:45:39,140 --> 00:45:43,540
actually being used is TikTok. And we need to be, we need to be talking about

578
00:45:43,540 --> 00:45:48,020
that. And I, you know, I, I, I gave you two, you know, very different ones. One is,

579
00:45:48,060 --> 00:45:51,900
one is that it's emergent. It's not like an intentional weapon. It's, it's, it's

580
00:45:51,900 --> 00:45:56,060
just emergent. And it's just, you know, we have a tendency to get deranged. And

581
00:45:56,500 --> 00:46:01,460
this is, this is what happens even though it seems to derange us. And then, and

582
00:46:01,460 --> 00:46:05,780
then, but nobody, nobody's in China to double check the algorithms. The ones

583
00:46:05,780 --> 00:46:08,820
they're training on the US are very different from the ones they train on

584
00:46:08,820 --> 00:46:14,540
people in China. You don't get, you don't get videos that, that make people, you

585
00:46:14,540 --> 00:46:19,180
know, radical, that radically undermine the belief in the society in China like

586
00:46:19,180 --> 00:46:22,500
you do in the US. And that, that, that suggests to me that we should at least

587
00:46:22,500 --> 00:46:24,540
be asking this intentional question.

588
00:46:24,940 --> 00:46:28,020
Very, very fair point. I, I'm certainly not trying to make the argument that

589
00:46:28,020 --> 00:46:33,340
TikTok is a productivity helper. And it could well be insidious and evil as

590
00:46:33,340 --> 00:46:33,580
well.

591
00:46:34,780 --> 00:46:38,500
My intuition is that it's, it's the, it's the biggest thing in AI. So if I had a,

592
00:46:38,500 --> 00:46:42,980
if you had to wait them, how big they are, I think TikTok is the biggest thing.

593
00:46:43,980 --> 00:46:48,620
We have a microphone here if folks are interested in asking questions to Peter.

594
00:46:48,620 --> 00:46:51,980
And if you can look this way towards the cameras, that would be helpful as well.

595
00:46:52,980 --> 00:46:57,300
Thank you. And thank you, Peter, for that most precious of gifts, your time. And I

596
00:46:57,300 --> 00:47:02,380
for one, I'm willing to pay the time price for it. Given our addiction.

597
00:47:02,380 --> 00:47:05,420
Watch it, who are you? Give it, now it's your.

598
00:47:05,940 --> 00:47:06,500
I'm nobody.

599
00:47:07,260 --> 00:47:07,740
How are you?

600
00:47:07,740 --> 00:47:11,060
I'm Stephen's. I went to your first conference.

601
00:47:11,340 --> 00:47:15,580
I know, but you've got to give your name when you speak at the microphone.

602
00:47:15,580 --> 00:47:18,100
We don't believe in anonymity around here.

603
00:47:19,460 --> 00:47:22,740
Very good. And you would go to the core of the question, which is the individual

604
00:47:22,780 --> 00:47:26,660
versus the identity. I stand as an individual to ask you this question.

605
00:47:27,420 --> 00:47:32,820
Given our addiction to narratives like AI and given our aversion to knowledge in

606
00:47:32,820 --> 00:47:38,820
favor of miss mill and all kinds of misinformation and given our emotional

607
00:47:38,860 --> 00:47:43,580
overload of the past few years, as a student of René Girard,

608
00:47:45,020 --> 00:47:51,660
my question is, will we have a mimetic pandemic where virus is the vengeance

609
00:47:51,660 --> 00:47:53,660
and could AI help or hinder it?

610
00:47:55,740 --> 00:47:56,740
There's one for you.

611
00:47:59,620 --> 00:48:05,940
You know, um, yeah, this is, I mean, this is certainly, um, this is certainly a

612
00:48:05,940 --> 00:48:09,540
read on what is very haywire about

613
00:48:10,580 --> 00:48:17,620
tech talk that it, you know, it just gives people what they mistakenly think they want.

614
00:48:18,260 --> 00:48:23,780
You know, they, they, um, and, um, and, uh, and, um, and then, um, and then, you know,

615
00:48:23,780 --> 00:48:27,660
that's, that's often, you know, that's often somehow, somehow a bad thing.

616
00:48:27,660 --> 00:48:32,740
I, um, I don't know, you know, I look, I think there is, there are, there are, um,

617
00:48:32,820 --> 00:48:39,580
there are ways in which, you know, I wouldn't, um, I wouldn't, um, I'm always,

618
00:48:39,580 --> 00:48:43,380
I'm always, I'm hesitant to sort of blame tech for everything that's wrong in our

619
00:48:43,380 --> 00:48:49,820
society. Uh, but I do think, I do think there's, there's something about, um, you

620
00:48:49,820 --> 00:48:54,060
know, it's, if we, if, and it's, it's, it's wrong to scapegoat and say it's the

621
00:48:54,060 --> 00:48:59,780
single thing that causes everything to go haywire, but, uh, but at the margins, you

622
00:48:59,820 --> 00:49:05,260
know, is it, is it helping us get, does, does, does, does, um, you know, there's

623
00:49:05,260 --> 00:49:09,900
something about, um, about a lot of these sort of short packet content forms that

624
00:49:09,900 --> 00:49:11,780
has, you know, has deranged this course.

625
00:49:11,780 --> 00:49:17,660
I think, I think TikTok is, is, is by far the worst, um, and, um, and this is sort

626
00:49:17,660 --> 00:49:22,180
of a sense in which the, the, you know, Silicon Valley is getting some, some of

627
00:49:22,180 --> 00:49:26,020
the blame for, um, uh, for, for all this stuff.

628
00:49:26,420 --> 00:49:29,300
And then, you know, I think the alternatives also didn't work because, you

629
00:49:29,300 --> 00:49:32,180
know, the alternatives were, you know, a centralized media system, which

630
00:49:32,180 --> 00:49:33,140
everything was controlled.

631
00:49:34,100 --> 00:49:34,540
Sir.

632
00:49:34,900 --> 00:49:40,220
So I, I, I think, I think you have to, if you frame the social problem, it's, um,

633
00:49:40,580 --> 00:49:45,780
it's, um, you know, it's too much totalitarian centralization versus

634
00:49:46,060 --> 00:49:47,780
deranged decentralization.

635
00:49:47,820 --> 00:49:52,060
And you have to, you have to think of both problems and there's, you know, there

636
00:49:52,060 --> 00:49:56,740
are instances of both that we had in the COVID epidemic the last two, three years.

637
00:49:56,740 --> 00:50:00,420
We had, you know, decentralized conspiracy theories that were not helpful.

638
00:50:00,780 --> 00:50:05,260
And we had a centralized narratives that cut off, uh, cut off much needed debate.

639
00:50:06,740 --> 00:50:06,980
Hi.

640
00:50:06,980 --> 00:50:07,500
Thank you.

641
00:50:07,500 --> 00:50:08,540
Uh, I'm Dr.

642
00:50:08,540 --> 00:50:10,620
Jeff Garneson from Anchorage, Alaska.

643
00:50:10,700 --> 00:50:11,980
This is my second cousin.

644
00:50:12,700 --> 00:50:20,500
Um, 60 years ago, CS Lewis wrote an article in the Saturday evening post called,

645
00:50:20,500 --> 00:50:25,700
uh, screw tape proposes a toast where he criticized, uh, the dumbing down of

646
00:50:25,700 --> 00:50:29,380
American education and I chair an educational foundation.

647
00:50:29,380 --> 00:50:33,460
And I'm very concerned about the quality of, uh, U S education.

648
00:50:33,900 --> 00:50:39,860
And, uh, and, uh, as you talked about, and what can we do about, uh, educating

649
00:50:39,860 --> 00:50:44,860
our youth in, uh, at least in public education, uh, all my colleagues send

650
00:50:44,860 --> 00:50:46,500
their kids to private schools now.

651
00:50:46,900 --> 00:50:49,460
And so I just wanted to see if you could comment on that.

652
00:50:51,260 --> 00:50:55,460
You know, I, I mean, I think there are a lot of people who articulated the,

653
00:50:55,460 --> 00:50:59,460
the issues quite, uh, quite, quite strongly.

654
00:50:59,460 --> 00:51:05,660
I don't have, you know, I don't have, um, I know, I, I've got much to add to it.

655
00:51:05,700 --> 00:51:13,620
Um, I, I do think, um, I do think it's sort of like always a question, you

656
00:51:13,620 --> 00:51:15,300
know, what are people doing?

657
00:51:15,300 --> 00:51:16,980
What is the teleology of it?

658
00:51:17,060 --> 00:51:23,340
And, you know, I think a healthy, you know, primary, secondary, tertiary

659
00:51:23,340 --> 00:51:27,740
education system is, you know, it's, it's supposed to, um, make you a

660
00:51:27,740 --> 00:51:33,020
well-rounded educated person, become a functioning citizen, our society.

661
00:51:33,420 --> 00:51:39,820
Um, uh, you know, if you, if you go into research or academia or certain

662
00:51:39,820 --> 00:51:44,820
industries, it is, uh, for you to become be a creative person who sort of

663
00:51:44,820 --> 00:51:51,300
pushes the frontiers of, of, of knowledge and, um, and in some sense,

664
00:51:51,540 --> 00:51:57,660
you know, it has, it has, um, it has, it has, it has somehow gotten, gotten

665
00:51:57,660 --> 00:51:58,260
deranged.

666
00:51:58,260 --> 00:52:02,820
And, uh, again, I don't want to blame it on AI or make this the, the

667
00:52:02,820 --> 00:52:07,980
single focus, but if you, if you have a narrative out there that, uh, that in

668
00:52:07,980 --> 00:52:13,460
the future, AI will do all the thinking for you and you don't need to think for

669
00:52:13,460 --> 00:52:15,300
yourself in any way.

670
00:52:15,780 --> 00:52:23,220
Um, you know, you know, maybe, maybe, um, maybe, you know, you don't need to

671
00:52:23,860 --> 00:52:24,660
learn as much stuff.

672
00:52:24,660 --> 00:52:25,780
You don't need to memorize things.

673
00:52:25,780 --> 00:52:28,500
You don't, there's sort of all this, these things that are considered

674
00:52:28,500 --> 00:52:33,220
education that, that, that seemed to be, um, seemed to be much, uh, much less

675
00:52:33,220 --> 00:52:33,860
important.

676
00:52:33,860 --> 00:52:39,380
And, um, and I, I, yeah, I, I do wonder that's, that's sort of, that's the

677
00:52:39,380 --> 00:52:42,100
broader context in which a lot of these things are operating.

678
00:52:42,180 --> 00:52:46,660
That it's sort of like, um, you know, if you, if you say it's not the public

679
00:52:46,660 --> 00:52:49,700
school system is not completely deranged, it's just trying to sort of make

680
00:52:49,700 --> 00:52:54,340
people these passive cogs in this very large machine.

681
00:52:54,740 --> 00:52:59,620
Um, you know, that's, that's sort of the, uh, the way in which it fits into

682
00:52:59,620 --> 00:53:01,300
this dystopian AI narrative.

683
00:53:01,300 --> 00:53:04,900
And, um, and we need to, yeah, we need to tackle this problem on all these

684
00:53:04,900 --> 00:53:05,460
different levels.

685
00:53:05,460 --> 00:53:06,340
Hi, Peter.

686
00:53:06,340 --> 00:53:07,700
Um, my name is Rick with Mivium.

687
00:53:07,700 --> 00:53:10,820
Um, I, I heard you give a talk at Stanford a few months back.

688
00:53:10,820 --> 00:53:14,820
And so this is the second time I heard you mention a lack of progress, uh, in, uh,

689
00:53:14,820 --> 00:53:15,700
on the atom level.

690
00:53:16,260 --> 00:53:20,820
So, uh, coming from the, uh, semiconductor material science field, uh, you

691
00:53:20,820 --> 00:53:23,860
know, I, I wanted to see what you mean by that because all we do is manipulate

692
00:53:23,860 --> 00:53:27,380
atoms on a day-to-day basis at scale from atomic layer deposition to molecular

693
00:53:27,380 --> 00:53:29,460
beam, epitaxial growth to mechanical chemistry.

694
00:53:29,460 --> 00:53:32,900
That's the only way we can make these next gen semiconductors, uh, these new

695
00:53:32,900 --> 00:53:36,100
materials that can basically replace silicon and free us from any kind of

696
00:53:36,100 --> 00:53:38,980
independent dependence from China and raw materials.

697
00:53:38,980 --> 00:53:42,740
So for me in my field, it, it's not a lack of progress.

698
00:53:42,740 --> 00:53:46,100
It's more the incumbent thought, school of thought out there is all chemistry

699
00:53:46,100 --> 00:53:46,500
based.

700
00:53:46,500 --> 00:53:49,780
Everybody wants to do with chemistry and toxic chemicals, uh, and things like

701
00:53:49,780 --> 00:53:49,940
that.

702
00:53:49,940 --> 00:53:53,300
Whereas we're trying to do it different way, but we face a lot of resistance.

703
00:53:53,300 --> 00:53:55,620
The barrier of entry and the cost is also very high.

704
00:53:55,620 --> 00:53:57,860
So I want to know what you mean by lack of progress.

705
00:53:58,580 --> 00:54:05,780
Well, it's, it's, it's, it's, um, it's certainly, um, slowed in, in a lot of

706
00:54:05,780 --> 00:54:06,740
fields.

707
00:54:06,740 --> 00:54:10,900
Look, I think, I think the, the complicated version I would tell is that we've had,

708
00:54:10,900 --> 00:54:17,620
you know, a decent amount of progress in the world of bits the last 40, 50 years.

709
00:54:18,500 --> 00:54:21,860
I think semiconductors are sort of the, the in-between thing.

710
00:54:21,860 --> 00:54:26,500
And then most other fields have been disappointingly, at least slow.

711
00:54:26,500 --> 00:54:30,180
You know, when I was an undergraduate at Stanford, I was class of 89, um,

712
00:54:31,220 --> 00:54:34,420
you know, in, in retrospect, it wasn't obvious at the time, but in retrospect,

713
00:54:34,420 --> 00:54:36,420
you were supposed to study computer science.

714
00:54:37,300 --> 00:54:39,700
All the engineering fields were bad fields to go into.

715
00:54:39,700 --> 00:54:42,660
It was a bad idea to go into aero astro.

716
00:54:42,660 --> 00:54:44,340
It was a bad idea to go into nuclear engineering.

717
00:54:44,340 --> 00:54:45,540
If people knew that, they didn't do that.

718
00:54:46,180 --> 00:54:50,500
It was a bad idea, mechanical, chemical, you know, all the sort of, uh, world of

719
00:54:50,500 --> 00:54:51,700
atom stuff was bad.

720
00:54:51,700 --> 00:54:56,660
I think the one that was still in, in between that, that worked okay, but much less well

721
00:54:56,660 --> 00:54:59,380
than computer science was electrical engineering.

722
00:55:00,180 --> 00:55:04,020
And, um, and, and so, you know, the, if we, if we look at, you know,

723
00:55:04,020 --> 00:55:08,980
how well have the companies done, how well have the people gotten paid, um, you know,

724
00:55:08,980 --> 00:55:14,420
EE has done better than other fields, but a lot less well than, um, than computer science.

725
00:55:14,420 --> 00:55:20,500
I, I think on some level, it reflects the ways in which the progress is hard.

726
00:55:20,500 --> 00:55:24,820
It's, it's, it's, it's slower than it was in the 80s and 90s, but by various measures,

727
00:55:25,700 --> 00:55:27,620
it requires enormous scale.

728
00:55:27,620 --> 00:55:29,860
So, um, the role for the individual is less.

729
00:55:29,860 --> 00:55:35,700
You know, if you have to have a $500 million ASML, you know, machine to do the lithography,

730
00:55:35,700 --> 00:55:41,460
you know, um, it's harder to start, you know, it's hard to start a new, um, semiconductor company.

731
00:55:42,420 --> 00:55:46,900
And so, yeah, the ecosystem is shifted towards, you know, much bigger businesses dominating it.

732
00:55:48,340 --> 00:55:55,220
And, and, you know, certainly as a venture capitalist, um, uh, I, we've done virtually

733
00:55:55,220 --> 00:55:56,740
no investing in semiconductors.

734
00:55:56,740 --> 00:56:02,020
You know, I, I think, I think I should, but, um, you know, it's, we do so little that we

735
00:56:02,020 --> 00:56:04,100
don't know enough about it to do it.

736
00:56:04,100 --> 00:56:07,700
And that makes me think that it's, um, you know, there's some things happening,

737
00:56:07,700 --> 00:56:11,300
but it's, it's, it's still, uh, it's still a lot slower than it was in the past.

738
00:56:12,340 --> 00:56:14,260
This is, this is the, this is the challenge with China.

739
00:56:14,260 --> 00:56:17,380
It's not like China is copying the West.

740
00:56:17,380 --> 00:56:19,140
They will eventually catch us up.

741
00:56:19,140 --> 00:56:24,500
And so if we are progressing slowly, they will eventually be able to copy things and converge.

742
00:56:24,500 --> 00:56:30,340
And, uh, I think the sort of rate at which we're doing new things is not so great

743
00:56:30,340 --> 00:56:32,180
that China will not be able to converge.

744
00:56:32,820 --> 00:56:32,980
Yeah.

745
00:56:32,980 --> 00:56:35,300
But China has no access to the ASML machines.

746
00:56:35,300 --> 00:56:37,220
Uh, they're still doing a chemical method.

747
00:56:37,220 --> 00:56:41,860
They're probably, they're, it's, it's possible that it's, uh, it's still going to be

748
00:56:42,580 --> 00:56:47,300
non-trivial for them to catch up, but it's, it's, it's not as though, you know, we have this

749
00:56:47,300 --> 00:56:53,300
exponentially growing lead that, uh, you know, um, it's, it's, it's, it's not quite as strong as that.

750
00:56:53,300 --> 00:57:00,980
Thank you, Peter, for your provocative remarks that are going to precipitate many discussions

751
00:57:00,980 --> 00:57:02,340
for the next two days.

752
00:57:02,900 --> 00:57:12,500
I, and, uh, uh, including about, uh, all these issues surrounding technological progress.

753
00:57:12,500 --> 00:57:13,540
And thank you.

