start	end	text
0	20680	I'm so thankful to have you here.
20680	25680	You know, a great conference like COSM requires you.
25680	28940	You're the best part of the conference, the people that you'll meet here, the conversations
28940	31920	around the table, the discussions we'll have.
31920	37420	COSM is a place for discussion and deliberation about the tech issues, about this convergence
37420	41140	of technologies that is transforming our world.
41140	47640	Artificial intelligence, 5G, cloud computing, blockchain, crypto, all these technologies
47640	50020	are happening at once.
50020	53860	And there's a question, what does that mean for the economy?
53860	58100	What does it mean for our place in geopolitically?
58100	61500	And what does it mean for work, the future of work?
61500	63060	What does it mean for lots of things?
63060	66900	And so we'll be examining each of those things over the course of the day.
66900	70500	A great conference requires great speakers.
70500	73380	And we have an incredible lineup.
73380	76300	If you haven't already looked at the program, you're here, so I'm going to assume that you
76300	77500	have.
77500	83420	But I want to thank Peter Thiel in particular, because this is our third COSM technology summit,
83420	88820	and Peter has helped us kick off COSM, all three of those conferences, and I'm so grateful
88820	90140	for his time.
90140	91580	He's going to be joining us virtually.
91580	96860	He's going to have a back and forth with George Gilder, and then he'll take your questions.
96860	102140	I'll point out the microphone for questions is here, and there's a camera over there,
102140	106500	so you will, if you want to ask a question, if we have time, you'll come down here, ask
106500	107500	the question.
107500	113940	We'll be able to see you, and we'll take questions and answers that way.
113940	116780	It requires a great staff, and this is a staff effort.
116780	118700	I'm going to be recognizing them throughout.
118700	122900	So grateful to our Discovery Institute staff for helping to put this together.
122900	124620	It's not easy.
124620	129300	And also our sponsors, and I think our AV team is going to put up the sponsors.
129300	131380	I want to recognize them quickly.
131380	136260	Microsoft, from the beginning, Brad Smith has been a great friend to this conference.
136300	137980	We're so appreciative.
137980	141100	Amazon, Blockcelerate did a pre-conference.
141100	145220	It was their own event, but they've been a great partner to us in the run-up to COSM
145220	146220	2022.
146220	152740	Smead Capital Management, right over here, Cole Smead, a look for him later, the MJ Murdoch
152740	160620	Charitable Trust, Zevenbergen Capital, Madrona Venture Group, Trilogy International Partners,
160620	168260	Henryx, Lucas Creative, Dunlumber, Dan and Cindy Mater, and Byron and Joanne Nutley.
168260	176260	Can we give them a round of applause, please?
176260	180660	All of those sponsors make this possible, along with your involvement, and again, we're
180660	181980	so appreciative.
181980	185500	And I want to stress again, you're going to hear some things over the next day or two
185500	186980	that you agree with.
186980	189260	You're going to hear some things you disagree with.
189260	193500	And that's what makes a conference fun, as George Gilder likes to say, a conference where
193500	195980	everyone agrees is a boring conference.
195980	200940	And COSM is not a boring conference, so we're so excited to entertain you over the next
200940	202340	couple of days.
202340	205780	Lastly, I want to introduce Matt McElwain.
205780	210940	Matt is the managing director of Madrona Venture Group, the leading VC firm in the Pacific
210940	212060	Northwest.
212060	217740	He is the chair of this year's COSM Technology Summit, and I'm so grateful to Matt.
217740	224020	Matt has really played a crucial role in succeeding Tom Alberg, who I know that Matt
224020	227020	is going to talk about a bit, but I'm so grateful to Matt.
227020	233180	So please join me in welcoming Matt McElwain.
233180	238380	Well, you're right about these lights.
238380	239380	They're quite bright.
239380	241500	Well, welcome, everybody.
241500	244500	Thank you so much for being here at the conference.
244500	251580	I'm absolutely excited to hear all the different discussions throughout the next few days.
251580	256940	And I'm incredibly honored to step into the shoes of my longtime colleague, great friend
256940	259060	and mentor, Tom Alberg.
259060	264620	Many of you in the room know Tom, and there's some really fun ties to Tom and George and
264620	268540	Bruce Chapman and others, but those three in particular, who will go all the way back
268540	273900	to their days at a little college out in Cambridge, Massachusetts.
273900	280900	And I think some of the early ideation that led to the ideas and the desire to bring people
280900	285060	together and to have conversations that all too often has been lost in different parts
285060	289900	of our society, including in some places in the educational system.
289900	296220	Tom was this unique mix of humility and ideation.
296220	299020	He was always thinking about what can you start?
299020	300860	What can you build?
300860	305620	Always as diverse as building up Perkins Cooley Law Firm here in Seattle, helping to
305620	313500	build McCaw Cellular into AT&T Wireless, moving on from that into founding Madrona R-Firm,
313500	318020	that's an early stage venture capital firm, and leading the first outside investment round
318020	322260	of Amazon, where he then went on to serve on the board for 23 years, and all the kinds
322260	324820	of innovation that came from that.
324820	326620	But he loved to talk about ideas.
326620	330140	He loved to debate things, many of the topics that we're going to cover.
330140	332980	And so it's just a real honor that we could think of him.
332980	337700	He passed away a couple of months ago and was just an absolutely amazing person and
337700	340380	inspiration and champion of this conference.
340380	346340	And one of the things he had asked me to do was to step into this role for this season.
346340	352100	And the reason that I love that opportunity is because of this embracing of diversity
352100	353100	of thoughts.
353100	354820	Steve just referenced it.
354820	361820	It's so important in this day and age to have people have truly curious minds.
361820	363740	What does it mean to have a curious mind?
363740	367740	Well, curiosity, by definition, is an act of humility.
367740	370060	It means that you don't know.
370060	372620	You don't know for sure about something.
372620	376780	And you're willing to explore alternative perspectives.
376780	379140	You're willing to explore the facts.
379140	381700	You're willing to explore the opinions.
381700	387420	And through that diversity of thought, get to better ideas, better understanding of what
387420	391940	is true and what could be true in the future, and in an era where we live in the combination
391940	397380	of a real world and kind of a spiritual dimension that we all want to try to understand better,
397380	403180	and then increasingly a synthetic and virtual world, it's harder and harder and harder to
403180	408740	know, let's say another way, one person's misinformation or disinformation is another
408740	410380	person's truth.
410380	415940	So let's dive into all these different topics over the course of the next couple of days.
415940	416940	Explore the facts.
416940	417940	Listen to the opinions.
417940	423060	Be respectfully open to the different ideas and see what we can all learn together.
423060	429700	Finally, I'm especially excited about this time of what's going on in a broad variety
429700	430700	of areas.
430700	435500	I'm going to have an opportunity to host a panel tomorrow on artificial intelligence.
435500	439580	And there's many debates and topics around artificial intelligence.
439580	444260	From what is artificial general intelligence, I'm guessing that Peter, when he comes on
444260	449900	here in a minute or two, might reference this topic, but is there such a thing?
449900	453220	Can AI be sentient or not?
453220	458620	And what's even more interesting about this time, not only at that philosophical level,
458620	462300	but at the level of what we would call intelligent applications.
462300	465740	We all live with intelligent applications every day in our lives.
465740	469500	Whether it's with a search engine, whether it's a recommender system on something like
469500	475420	Amazon or Spotify or Netflix, intelligent applications are pervasive.
475420	478980	But there's a whole new wave of intelligent applications that are coming.
478980	481420	We'll unpack this tomorrow in the session.
481420	484780	Those are what we refer to as generative applications.
484780	489220	And the generative applications, unlike intelligent applications, are built on something called
489220	490740	foundation models.
490740	494700	Things like GPT-3 and Dolly and stable diffusion.
494700	501820	And so that is a very interesting area where I might have a point of view that the human's
501820	503140	always going to be in the loop.
503140	507180	You might have a point of view that the human's going to be written out of that loop over
507180	508180	time.
508180	512860	But let's have those kinds of discussions because AI, generative apps, intelligent apps
512860	518060	is a very important part of every aspect of life going forward in the future.
518060	520700	Peter, this is Matt McOy and it's great to see you again.
520700	523580	Let me just take a second to introduce you.
523580	528780	I was just talking about generative apps and foundation models and speculating that you
528780	534940	might have a thing or two to say about that in this maybe more broadly in your talk here.
534940	540420	Peter, of course, was the co-founder of PayPal, co-founder of Palantir, still serves on the
540420	541420	board there.
541420	547500	And even beyond that has been a very active investor both individually as well as through
547500	552980	Founder's Fund, which is the fund that he again created several years back, companies
552980	557740	that you may have heard of like Airbnb and LinkedIn and so many others that he's been
557740	562740	involved with again either directly and or through Founder's Fund.
562740	567700	He's also done a couple of other things that I have found personally very inspirational,
567700	574140	one of those being the Teal Fellows, which is a group of individuals that have been encouraged
574140	580220	to go off and pursue their dreams with resources that the Teal Fellows program has provided.
580220	586300	One of them most famously here of late, Dylan, his company Figma is being acquired by Adobe
586300	591900	for $20 billion, so not too shabby, to leave college and he left Brown and pursued his
591900	594060	dreams as a Teal Fellows.
594060	598980	That's Dylan Field, Dylan Field.
599300	604340	So there's a number of things that Peter has done and he's also taken the time, the book
604340	609500	I'm going to mention specifically has been quite inspirational to me, is zero to one.
609500	613100	It's a lot harder to go from zero to one than to go from one to a hundred or a thousand
613100	614100	or a million.
614100	617340	And so I'm sure he's going to touch on some of these different themes and without further
617340	619340	ado, Peter Teal.
619340	620340	Awesome.
620340	621860	Thanks so much for having me.
621860	626540	I thought I would do another AI or anti-AI talk.
627540	633100	It was a great conversation I had last year with George Gildler.
633100	639740	I think one of the challenges in this field is always to figure out the right, it's not
639740	643820	just to get a diversity of views, but it's even just to figure out what the right questions
643820	644820	to ask.
644820	653060	And so the high-level question I want to ask today is basically is how should we think
653060	654060	about AI?
654060	661180	Do you think of it as intelligent, conscious, or merely evil?
661180	672460	And so the first step of perhaps tackling this question is to tackle another question
672460	678380	which Gilder asked me, he was the first question he asked me last year in the Q&A part.
679020	684780	And why do so many people in Silicon Valley believe in the simulation hypothesis that
684780	687620	the entire universe, the cosmos is just a computer simulation?
687620	690700	Why do they believe something as crazy as this?
690700	696180	And I thought about this question some more, there are a few answers.
696180	701540	And as I will explain, I think this question is actually somehow entangled in an interesting
701540	707100	way with the question about AI, intelligent, conscious, or merely evil.
707980	713980	Now one way in which you can question the premise of this question is, as I will explain,
713980	721380	I think probably the peak belief in the simulation hypothesis was maybe something like a decade
721380	727700	ago, sort of maybe circa 2012 to 2015, and it has probably faded some.
727700	733380	People still have Gen Z people say that things are glitched in the simulation, or there's
733380	737980	still some sort of passive reference to it, but it's a little bit less intense.
737980	744540	So the question of why we believe it is sort of like why did it gain so much momentum over
744540	748700	the last 20 years, and then also why did it lose some steam?
751100	758500	So the first very dumb answer to the simulation hypothesis question is that it's just sort
758500	764460	of a sociological status, a game between the computer science and the physics people,
764460	771180	you know, and that in the 2020, 2010s, computer science became the most important field.
771180	776820	And so you could sort of say that you were showing that you were more important than
776820	780460	the physics people, you got to determine what ultimate reality was.
780460	787940	And it wasn't particles and matter and fields, but it was just bits in a computer, and that
787940	794860	this is sort of a physics versus computer science type dynamic and reflected something
794860	796860	about that.
796860	804140	But if we want to give sort of the more fundamental answer on cosmology is that something had
804140	811700	gone very haywire in physics, that you had sort of the multiverse is sort of where sort
811700	817540	of a lot of the big bang inflationary cosmology had gradually gone into this infinite multiverse
817620	820940	where basically anything goes.
820940	827580	And and and I think, you know, that probably on some level, you want to critique the multiverse
827580	829220	as a theory of science.
829220	835260	It was one where the physicists couldn't think through enough, you know, what questions
835260	842100	like where, you know, the being lost in all these infinities is tricky.
842100	845260	If you cannot do induction, you know, are you still doing science?
845260	847100	Is it a universe that's too big for science?
847580	851140	And and and that's why I think you should often think of the multiverse as a gateway
851140	854460	drug to all these these very different things.
854460	859340	And once you have a multiverse, you can also have the matrix or a simulation, you can have
859340	866100	Boltzmann brains, you can have, you know, you can have all kinds of strange possibilities
866100	868300	for the nature of the universes.
868300	873260	And so that, you know, even if we say the simulation hypothesis is kind of crazy, on
873260	877060	some level, is it really crazier than than the multiverse and sort of link?
877460	882140	Now, there's a third answer I'm going to give to why the simulation universe theory
882420	886420	gained so much traction, but that will take a little bit more time to develop.
888180	894220	The now, but as part of as part of this, we should look back on.
895500	899660	You know, the last 17, 20 years of AI, AGI.
900300	907300	And if we if we went back 20 years in time, it was it was super optimistic
907740	913580	in in all these ways where it's going to, you know, it's going to be cornucopia.
913580	919180	2005 was the year Kurzweil wrote the singularity is near.
919300	923740	Basically, you know, you're going to have accelerating technological progress,
923740	925540	runaway technological progress.
926420	930500	You know, it was it was in some ways, it felt like you didn't have to do much.
930500	932140	So it was somewhat passive for the humans.
932140	936180	You just had to sit back, eat some popcorn, watch the movie of the future unfold.
938100	941220	And then and then there were certainly all these versions
941220	945740	where it was going to be so much growth, we would need basic income as a safety net
945740	948580	because there'd be no more there'd be no more labor market.
949340	956220	You'd have you'd have sort of incredible discoveries in all these fields.
957500	961300	You know, there was there was sort of certainly some latent question about,
961300	964420	you know, how you have to make sure that AGI was was friendly.
964580	965180	But.
967820	973060	Of the narrative in the 2000s, early 2010s was in this sort of,
973740	981820	you know, optimistic utopian, cornucopian direction of what AI would would would would would look like.
983300	987900	And but now if we sort of look at, you know, what is what's actually happening with AI,
987900	989220	not, you know, where it's going.
989220	993700	So I'm not going to speculate on AGI or, you know, even where it's going in the near future.
993700	1000220	But if you look at what is actually happening with AI, you know, we get something like this guy.
1000220	1004780	This is a this is a TikTok video.
1006100	1010540	And, you know, one of my one of my colleagues went through a constructed
1010660	1016140	so a whole series of profiles on TikTok, where it's an AI optimization engine
1016140	1018100	that feeds you content that you want to see.
1018100	1020020	That's supposedly what it does.
1020660	1026020	And and and and basically, you know, came up with all these different profiles
1026020	1031060	for Midwestern housewife, you know, all sorts of people with different kinds of interests.
1031220	1036060	But sort of what what you sort of, you know, part of what the algorithm fed you
1036260	1041620	was this this fairly sort of deranged content, which and, you know,
1041620	1045340	this this particular person, I was going to play the video, but we decided
1045340	1048860	there were too many naughty words in it that I might get in trouble for.
1049180	1055020	But he basically explains how he exploits employment law, bounces from job to job.
1055140	1058540	Suze all his employers for health and safety violations.
1058860	1062460	And it's sort of designed to make, you know, it's designed to sort of
1062460	1067580	polarize race relations in the US and make, you know, black people angry.
1067580	1072060	Because if you have an honest job, you're a sucker, it's designed to make white people angry.
1072340	1079100	And there's sort of our versions, versions like this that come up throughout TikTok.
1079100	1082980	This this is this is sort of what I would say a front and center,
1082980	1088500	what cutting edge AI looks like in in the U of S today in 2022.
1089340	1094260	And and, you know, it's of course, we don't really know what the the full
1094260	1098300	intentionality is. We don't know whether this is a, you know, is just sort of an
1098300	1104740	emergent property that it's going to sort of take what people want, push it to
1104740	1107580	extremes, derange it, derange the discourse.
1107580	1112580	So you'll have, you know, you'll have you'll intensify the race
1112580	1116180	contradictions, you'll intensify the economic contradictions, how, you know,
1116180	1119900	the US has the worst health care system in the world, you'll have someone else
1120940	1127100	talking about, you know, how you should call pedophiles, a map person,
1127100	1130340	a minor attracted person, because we shouldn't discriminate against those people.
1130340	1135900	So it's so it's all sort of like designed to to, you know, intensify the
1135900	1140500	wokeness, anti wokeness, polarize and derange our society in one way or another.
1141500	1143100	You can view it as emergent.
1143100	1147980	You can view it as full on intentional where you should think of TikTok as a
1147980	1154340	sort of Chinese communist weapon that is being used to derange our society.
1154340	1158220	That, you know, the book that I think is interesting is the one who named
1158220	1161540	who's the number four guy in the in the entire communist government.
1161540	1166500	He's sort of this the professor, theorist of Xi Jinping thought.
1167500	1171420	And he wrote this book 32 years ago, America versus America.
1171420	1175620	And it's basically it's basically a roadmap for how to derange our society
1175660	1179540	by sort of heightening these sort of Hegelian contradictions.
1179980	1185180	And and I would submit that if you go with the full intentional version,
1185460	1190540	TikTok is is is basically a weapon that's designed to derange us
1190660	1195100	through decentralized and heightened contradictions.
1195540	1202420	And of course, this particular AI is up against another self-destructive
1202780	1209260	communist Chinese AI, which is the centralized one that's being imposed on China itself.
1210300	1214780	And, you know, we're basically you have a perfect face recognition.
1215420	1217140	Everybody knows they're being monitored.
1217140	1220900	They are they're living in a, you know, there is sort of an important way
1220900	1223500	in which China has become North Korea.
1223500	1228580	It has gone, you know, it it was, you know, those of us for anti-communist
1228900	1232940	tend to tend to conflate that it was a communist country 10 years ago and is now.
1233180	1238460	But there is a way in which the the AI technology, the surveillance technology,
1238620	1240660	has has really, really transformed it.
1241020	1244980	It is it is again not it is again not.
1246740	1248220	You know, it's not AGI.
1248300	1253140	It is, you know, in many versions, it's it's fairly, you know, barely AI at all.
1253180	1257740	It's just sort of cameras, you know, ways to do big data on this.
1257740	1260220	This is, you know, this is what the Kai-Fu Li book talks about, too,
1260220	1266900	that China will win at AI through these, you know, sort of big data algorithms.
1267020	1270820	It's not about the sort of cutting edge futurists, except the people in Silicon
1270820	1274740	Valley talk about things about like TikTok or what China has done to themselves.
1274740	1278860	And so you can basically, you know, one way to think of the rivalry between
1278860	1283540	the U.S. and China is that it's it's, you know, it's sort of a question,
1283940	1289500	which society will be destroyed faster by the by the somewhat dystopian AI
1289500	1290980	that's being imposed on it.
1291340	1296100	And and and we have sort of a long, long debate about that.
1298100	1302420	So with this is sort of the framing of where AI actually is, where it actually,
1302740	1307860	you know, is being implemented in the most powerful, dramatic ways today.
1309860	1312220	Let's go back to our three questions about AI.
1313300	1317580	You know, first off, is it is it intelligent?
1320060	1324660	You know, I'm not sure whether we should even, I mean, on the first two questions,
1324660	1329300	I'm going to sort of say they're above my pay grade, but it seems to set a low bar
1329300	1330180	for intelligence.
1330860	1336420	And the rhetorical point I would make is that it's often just a filler word when,
1336620	1339300	you know, we're talking about something quite different.
1339980	1345260	There was a 2016 Obama administration study about the transformative importance
1345260	1348980	of AI entitled quote, the title of the whole paper, the National Artificial
1348980	1351380	Intelligence Research and Development Strategic Plan.
1351940	1356780	And and basically, if you went through this paper and if you replaced every use
1356780	1362500	of the word of AI with software or even just computers, the meaning wouldn't
1362500	1363460	change at all.
1364100	1370900	And and I think this is sort of a a tell that, you know, maybe the first
1370900	1375460	approximation when you hear AI, you should just think software or computers.
1375740	1381540	It's, you know, AI is is not, is probably not intelligent.
1381820	1385020	AGI, that's somewhere in the future, don't know.
1386980	1390300	You know, in a similar way, I would say the question of, you know, whether
1390300	1395780	it's conscious is probably hard to say, you know, my my my strong suspicion
1395780	1400100	is that it's it's not, of course, have, you know, the epistemological problems.
1400380	1402380	You know, Thomas Snagle, what does it like to be a bad?
1402420	1406420	You have the Searle's Chinese room problem.
1407460	1409820	And then, of course, and so it's, you know, it's hard to say.
1409820	1410620	And for Mr.
1410620	1415180	Lemoine, who I think is is talking later, it was literally hard to say that AI
1415180	1416140	might be conscious.
1416500	1420500	And so, you know, as a as a contrarian, I'm always a little bit biased to say
1420500	1422460	that things that you're not allowed to say might be true.
1423380	1426260	So I don't want to dismiss the possibility entirely that it's conscious.
1427380	1435180	But but it's probably probably the wrong sort of question on on on some level
1435180	1437940	for us to be asking the question, whether it's intelligent, whether it's
1437940	1442780	conscious or just the wrong questions that the, you know, I always go back
1442820	1447100	to what I don't like even about Descartes, where you think about Cartesian dualism
1447100	1451420	as the the origins of of the problem of consciousness.
1452980	1456900	The way the way consciousness worked for Descartes was that it was meant
1456900	1460980	to be, you know, a smart person in the 17th century was supposed to become
1460980	1463500	a priest and use his brains to think about God.
1464020	1468780	And Descartes came up with this very mysterious different thing called the mind.
1469100	1474220	And it was sort of an attention, redirection, distraction mechanism.
1474220	1481580	And and I always think we should we should remember the 17th century context
1481580	1488340	where consciousness was not something that was mystical or spiritual or dualist
1488340	1491980	in sort of the way we might think of these categories in the 21st century.
1492140	1494540	But it was meant to be anti theological.
1495500	1499340	And that but that, you know, maybe more generally the problems of consciousness
1499340	1502860	or even of intelligence are somehow the wrong question.
1502860	1509060	So let me go to my my third one, you know, is AI evil?
1509820	1516100	And this one seems seems, you know, more straightforwardly answerable.
1516100	1520380	Certainly, I think that the TikTok algorithm is evil.
1520620	1525820	I think what China is doing to itself is is clearly evil.
1526140	1531180	You know, we can, of course, you can talk about evil in all kinds of different
1531180	1534780	versions. There's, you know, there's, of course, the kind of,
1536140	1538940	you know, disembodied brain and C.S.
1538940	1541980	Lutus's book that hideous strength, who turns out to be a demon.
1543420	1547860	You know, I'm not sure it's literally demonic and quite quite that sort of a way.
1547900	1552220	Although, although certainly I don't think that we've had an exorcist at Google
1552220	1554420	to check that out and make that determination.
1554900	1558380	So I think, you know, even that possibility couldn't quite be ruled out here.
1558700	1565140	But but, you know, it's it's evil in the the the the creepy looking woman
1565140	1571460	on the upper right is this is this is this image loa who seems to be
1571460	1575700	sort of a strange attract that comes up in a number of the art projects
1575700	1580780	that Dolly the the AI art program has generated.
1580780	1586740	And it's it's sort of if you do if you ask what is the opposite of Marlon Brando,
1587020	1589380	you get this sort of you get the somewhat abstract painting.
1589380	1591340	And then if you ask what's the double negative of that,
1592500	1596940	this sort of creepy woman loa emerges
1597620	1602860	and and and and what's what's what's sort of an interesting
1602900	1606700	that she emerged on sort of a number of things were sort of this strange attractor.
1606700	1611020	And you can think of it as maybe it is a kind of occult knowledge
1611340	1613980	where we're we're learning something.
1614020	1618220	Did we really need to know that the double negative of Marlon Brando
1618380	1625140	was a witchlike woman and and and and something like that.
1625420	1629980	But but of course, maybe the closest analog to sort of a
1630980	1635900	to sort of a demon is is an idol, a pagan God,
1637140	1639260	where, you know, we worship the God,
1640660	1643100	the God seems to tell us what to do.
1644100	1646860	And it's unclear if it's actually telling us these things
1647100	1650300	or if it's just somehow some kind of psychosocial effect
1650500	1653740	that's creating some kind of mass hallucination
1654100	1658180	and and and that leads leads to this.
1658180	1661460	And this is sort of this is sort of where I've suggested that, you know,
1661460	1666140	maybe you should think of the European Union as a kind of the closest thing
1666140	1669020	we have to functioning AI and government in a way where it's
1669500	1672660	it's the goal is just to prevent human thought.
1673060	1675780	It tells us very basic, simple things that we should do.
1676180	1679700	But, you know, it functions these ways.
1680220	1681220	But.
1683220	1685860	But if we but now let's come back, you know,
1685940	1691060	if we say that there is a lot in AI
1691620	1696540	that is straightforwardly evil, that is merely evil,
1696700	1699020	that is simply about stopping humans
1700060	1704340	from thinking, from using their capacities and things like this.
1704540	1707700	Let me let me use this to come back to the
1709220	1712700	the very big cosmological question about the simulation.
1713700	1717620	And and so now let me give an alternate sort of explanation
1717620	1721580	of why the simulation hypothesis gained so much traction
1722100	1724740	in the 2000s and.
1726060	1727460	Early 2010s.
1729660	1732420	And it's something it's something like this.
1733660	1737940	As we were building AI, as we were building towards AGI,
1738940	1743700	it seemed, you know, it seemed potentially dangerous, you know, AGI
1744700	1749540	in the full utopian sense was going to be this, you know, superhuman mind.
1750140	1753180	Could we really be confident that it was going to be, you know,
1753180	1756220	aligned with human beings, that it was not going to be lying?
1756220	1758340	There seemed to be, you know, a lot of.
1760140	1763740	Of risk in that, you know, the and I think that, you know, a lot of
1764740	1770420	those of us who are skeptical of AI or skeptical of AGI often underestimate
1770620	1777420	how troubling the the alignment arguments are, how, you know,
1777420	1778900	it's not straightforward.
1779100	1782460	If you can have such a thing as friend as AGI, it's not at all
1782460	1784900	straightforward to get the AGI to be friendly.
1784940	1788100	You know, if you have a Darwinian view of the world or just a Machiavellian
1788100	1791460	view of the world, where you'd say the core axiom is that there is no such
1791940	1796340	thing as a selfless being, a purely selfless being, and therefore the alignment
1796340	1798620	problem is fundamentally difficult to solve.
1798620	1800500	So how do you get to friendly AI?
1800900	1803340	Not not super straightforward.
1804980	1810860	But if we say, and maybe maybe AGI ends up being a kind of great
1810860	1816820	filter where, you know, if you get it wrong, it will, it will destroy the world.
1817460	1821300	And this is where, you know, this is where there seemed to be a very
1821300	1825620	big difference between the multiverse and the simulation theories, because in
1825620	1829340	the multiverse, the AGI is simply in the future.
1829700	1835380	And whatever great filter the AGI represents, whatever threat it represents
1835380	1841580	to all of humanity, wipe out all of humanity, it's in the future and seems
1841580	1842460	quite dangerous.
1842780	1847980	Whereas if you have a simulation theory, you also have this cosmic AI or cosmic
1847980	1850660	AGI that created our universe.
1851020	1853860	And in some sense, there was a great filter in the past.
1853900	1864580	And so there is a way that perhaps you could think that the cosmic AI isn't
1864580	1867180	entirely hostile since we're here having this conversation.
1867380	1874740	Perhaps the cosmic AI is guiding the development of the AGI and we can infer
1874740	1882140	that it will, it will be sort of, it will sort of be, be aligned with, I think
1882140	1884940	this is the way you have to think of the simulation theory.
1884940	1891140	It was in the context of a lot of these concerns about friendly versus
1891140	1895740	unfriendly AI, and it shifted the problem from the future where it is a
1895740	1900380	multiverse to the past and seemed to solve it.
1901180	1906780	Now, the thing that is very different from when Kurzweil was writing about this
1906780	1915020	in 2005 is, you know, we have, we've seen some of the progress and, and somehow,
1915740	1918980	somehow this illusion has become very hard to maintain.
1919460	1927540	And, and, you know, we have, you know, the, the, the sort of AI, the thing
1927540	1934540	that perhaps is tracking towards, you know, you know, autonomous weapon system,
1934540	1943620	cyber warfare, you know, a runaway AGI, it doesn't seem very good for humans.
1943620	1948820	And this is, this is both, both in its cutting edge, centralized and decentralized
1948820	1955780	forms. And, and, and, and as a result, I would say we are, we're sort of, we've
1955780	1962980	been sort of inclined to flip, flip the causation that, you know, the emergent AI
1963020	1968820	or emergent AGI is what's telling us something about the AI that built the
1968820	1973500	universe. And if the emergent, if it's the nature of the emergent AI to be
1974460	1980180	fundamentally or merely evil, then perhaps we should not be so, so assured
1980180	1986900	that the, the, the cosmic AI that created the simulation was, was fundamentally,
1987980	1991780	was fundamentally good. And, you know, we should extrapolate from one AI to the
1991780	1996260	other and assume that it's, it's also self-interested, not aligned with humans,
1996380	2004340	not fundamentally beneficial to the human world. And, and, and this is why I
2004380	2011180	think the, the simulation theory, you know, maybe was a fake way to solve this,
2011180	2016500	this problem, but it's not at all working anymore. You know, I think one, one, one
2016500	2020660	way to think about this is there, there's sort of all these, all these kinds
2020660	2027100	of debates about the meaning and nature of AI that map onto these theological
2027100	2032300	controversies from the Middle Ages. And it's, it's always sort of interesting to
2032300	2038300	try to, to try to, to map it onto these, these, these past theological debates,
2038300	2042780	the way to just sort of understand the nature of the argument. And you can think
2042780	2050580	of, you can think of the cosmic AI as sort of analogous to a form of strict
2050580	2057820	monotheism, like Judaism or Islam, where it is the oneness of God. And, and then
2057860	2062620	the problem with extreme monotheism is that you cannot speculate on the
2062620	2067420	attributes of God. You ultimately do not know much about the nature of God. To
2067420	2072020	have a, you know, science of God, you need, you need a plurality. If you have too
2072020	2076980	many gods, of course, they're probably not gods. But, and, and this is sort of
2076980	2083580	where, and then, and if you think of Christ and Trinitarian Christianity as
2083580	2088580	telling us something about the nature of God, you go from the God in history to
2088580	2094420	tell us about the God outside of history. And, and I think there's roughly a
2094420	2100220	similar move that's happened with, you know, the, the emergent AI, which is the
2100220	2107900	sort of, you know, the, the idol, the, the demon idol, whatever you want to call
2108220	2115100	it, that's emerging in history, that, that it is telling us that if, if there was
2115140	2120220	some demiurge or something like that, that, that built the simulation, we should
2120220	2126660	also infer that it's, it's, it's, it's, it's not that well aligned. And so, and
2126660	2132900	so this is sort of where, you know, we are seemingly at, at these, at these dead
2132900	2140260	ends with, with the progress of AI. And, and it's, you know, I'm not going to
2140420	2145260	solve, solve this problem today, but it seems to me that, you know, surrendering
2145260	2152660	control to AI, you know, blindly worshiping AI, the emergent AI, letting it
2152700	2157980	dominate and control our societies leads to, you know, one of two catastrophic
2157980	2164220	outcomes, you know, it's sort of decentralized runaway violence, which is,
2164420	2169260	you know, the derangement of TikTok, America versus America, and then, you
2169260	2178300	know, centralized totalitarian one world state, you know, worse than North
2178300	2185260	Korea or China. And, and that the challenge for us is to find some kind of a
2185260	2192700	third path, you know, where we, we make progress in areas other than AI, we, we
2192700	2198340	find a way to, to get back to the future, you know, the runaway apocalyptic
2198340	2203300	violence, the centralized totalitarian one world state, they are, they are
2203300	2209060	seemingly exclusive possibilities. I don't think they're exhausted. I think
2209060	2212900	there should be a third way in the challenges for us to, to find a way to
2212940	2217300	build it. You know, not a fan of BF Skinner, the behavioralist, psychologist,
2217300	2220940	but the quote I, as I always like to cite is, you know, the real problem is not
2220940	2225980	whether machines think, but whether men do. And we need to get back to thinking
2225980	2230820	ourselves and, and regain control of our future. Thank you very much.
2231580	2237740	Peter, thank you so much for that stimulating and philosophical and
2238260	2249500	prophetic oration. It was really worthy of zero to one. This conference is going
2249500	2260140	to be one of our prime themes is the development of superabundance. And this
2260140	2270140	is really being demonstrated by this new book by Marion Tupi and Gail Pooley
2270180	2279820	about that really documents on the basis of time prices that abundance is
2279980	2288900	steadily increasing at an accelerating pace. And this conflicts and important
2288900	2294900	ways with your vision, Peter, that you have previously expressed that in some
2295260	2306300	way technology is becoming less fruitful and less creative and less responsive
2306300	2315500	to real human needs. So I wondered whether you can record what sort of insights
2315500	2322460	you can have that transcends this apparent conflict, you know, between the idea
2322460	2330860	that science is going stagnant or technology is becoming sterile and
2331660	2342300	demonstration of this magnificent new book that that poverty is being overcome
2342340	2348660	everywhere, that the price of commodities is plummeting, that everything's becoming
2348660	2357500	more abundant, that science actually is offering new bounties every day. That
2357620	2362140	would be my... Well, I, well, I don't, I don't agree with the book on any level.
2362140	2367740	So, so it's okay. But I think, look, I think, I think, I think even something
2367740	2373220	as basic as commodity prices are, you know, at, you know, there was a hundred year
2373220	2377700	decline trend in the 20th century. And then, you know, and if they go much higher
2377700	2383500	than they are now, it's like that we've had a 20 year bull market, and it will go up
2383500	2388860	in a way that suggests the whole decline trend is broken. And so, and then, you know,
2388860	2392900	there are all these different, you know, reasons you can do this. We have, you know,
2392900	2398260	certainly, you know, the macroeconomic version is always to look at, is to look
2398260	2405660	at real inflation, inflation versus real, you know, wages or people's wages going
2405660	2410020	up faster than inflation. And the felt sense is that that's not happening. And
2410020	2413260	then you can make, you know, and then, you know, the super abundance argument is
2413260	2416620	somehow that the government is understating the inflation and there's
2416620	2420180	less inflation than it looks. And, and, you know, I don't think they're
2420420	2425140	overstating it massively, but, but at the margins, I believe the government is, I
2425140	2427900	will, the super abundance argument is that they're overstating inflation. There's
2427900	2432300	less inflation. There's more real growth. My argument would be, you know, at the
2432300	2436060	margins, they're probably understating inflation. So there's more inflation and
2436060	2441980	actually, you know, even, even less productivity growth. But, but, but the
2441980	2447300	way, the way, if I had to sort of reconcile these two views, it would be along
2447300	2451700	the lines of the talk I just gave you, which is, you know, let us say that there
2451700	2457380	are some dimensions where there is, you know, a reasonably rapid amount of
2457380	2460780	progress that there has been, you know, maybe not as much progress in the world
2460780	2464700	of Adams as I would like. There has not been progress on energy or, you know, we
2464700	2468780	don't have nuclear power plants. We don't have, you know, we haven't had less
2468780	2473300	progress in futuristic medicines than I would like. But we had, we've had, you
2473300	2480140	know, a lot of progress around computers and, and the internet, the mobile
2480140	2483780	internet, and then, and then of course, all these things that get loosely
2483780	2489580	categorized under, under AI. And, and then, you know, you have, you have the macro
2489580	2494620	economic question, you know, how much that progress lifts our human society
2494620	2499460	generally, I would say it's less than said, but let's, but, but then I think the
2499460	2503700	other dimension you have to ask is, is it the sort of progress that people think
2503700	2509260	of as, as simply, simply good? And, you know, if the, you know, the, the futuristic
2509260	2514900	AGI was pitched to me in 2005, as you have no idea what sort of, we will be able
2514900	2520100	to cure aging, we'll be able to find all these fantastic medical treatments. We
2520100	2524140	still have not gotten those. If we had gotten those, I would score it as, as
2524140	2529180	at least more positive. What we have gotten, we got TikTok. And yeah, that's,
2529220	2534940	it's valuable for TikTok. It's valuable for the company that sort of does this
2535340	2540300	AGI. But it, you know, I would, I would score it as, as a form of technology
2540300	2544660	that is, that's, you know, and I don't want to sound overly lead, but even if
2544660	2549700	it's rapidly, it's, it's deranging us. It's making us go crazy. Or the
2549700	2553620	surveillance state in China, that is a form of technological progress over a
2553620	2559260	decade ago, but it has, you know, it has, it has really deranged that society. It
2559260	2563500	has disabled the humans. It's, it's a less happy, less functional, less free
2563500	2568180	place than it was 10 years ago. Even if we say that it somehow shows up in the
2568180	2571540	economic statistics, which it doesn't, but even if you can make, even if you can
2571540	2576620	jigger the ACON statistics to make the super abundance show up, it might, we
2576620	2578780	should ask this question, is it evil or is it good?
2579060	2584580	Yep. So Pete, so Peter, before we have some audience questions, I'm going to ask
2584580	2590820	a question too, as I think about some of these contemporary examples, like in the
2590820	2595700	area of science, things like Alpha fold or deep fold, you know, where we've been
2595700	2599300	able to predict and understand the structure of proteins coming out of the
2599300	2603100	deep mind team. Or we, you know, and so we have the productivity, you know,
2603100	2607620	co-pilot that's been recently launched by Microsoft and the claim by, by
2607660	2611500	Satya and Charles Lamont and others. There's a 30% improvement in productivity
2611500	2616580	and software development. Or you take stable diffusion and the creativity that
2616580	2619780	that unlocks, you know, for, for humans. In every one of those cases, of course,
2619780	2624660	humans in the loop in many, many different ways. So no AGI here. And I guess
2624660	2629020	the question is, therefore, can AI both be evil and good?
2632020	2636940	Sure. But, but, you know, all, like all the examples, you know, like there's
2636940	2640620	always, look, there's always a fast line, it's just a technology and it just, it
2640620	2645820	just, you know, it's, and as such, it's, it's, it's pretty neutral and it's up to
2645820	2652820	humans what we, what we do with it. But I think one can, I would still say one
2652820	2657300	should ask the question more, you know, is it, you know, you know, how do we sort
2657300	2660820	of weight these different kinds of applications? You know, the protein
2660820	2665900	folding is, is interesting. It's, it's, it's only valuable, I would say, if it
2665940	2672340	actually leads to new medical interventions, new cures. And when I, when I
2672340	2677380	push the AI people on that, they always, they don't, they don't want to engage
2677380	2684460	in that conversation. And, you know, so, and then, and then, you know, if, if it
2684460	2691780	is saving 30% of coding time, that, that would be very impressive. It, it's, it's
2691820	2696820	not clear that that's, that's showing up in, in any of the, of the stats of the, of
2696820	2701860	the big, big software companies at this point, you know, where, you know, I would
2701860	2707180	say all of Silicon Valley has, has this problem where, where, you know, the, they
2707180	2712460	have to pay the, the coders, the computer programmers more and more. And so, yeah,
2712460	2716060	there's, you know, there's definitely a need for technology to replace the people
2716060	2720220	and would make the businesses more profitable. And I would, I would have scored
2720500	2725460	that as probably a productivity enhancing, generally positive thing. It doesn't
2725460	2733700	show up in the computer science labor market. So, and then, and then I think,
2733700	2739140	look, the, the, the big thing, the biggest, the biggest AI technology that's
2739140	2743540	actually being used is TikTok. And we need to be, we need to be talking about
2743540	2748020	that. And I, you know, I, I, I gave you two, you know, very different ones. One is,
2748060	2751900	one is that it's emergent. It's not like an intentional weapon. It's, it's, it's
2751900	2756060	just emergent. And it's just, you know, we have a tendency to get deranged. And
2756500	2761460	this is, this is what happens even though it seems to derange us. And then, and
2761460	2765780	then, but nobody, nobody's in China to double check the algorithms. The ones
2765780	2768820	they're training on the US are very different from the ones they train on
2768820	2774540	people in China. You don't get, you don't get videos that, that make people, you
2774540	2779180	know, radical, that radically undermine the belief in the society in China like
2779180	2782500	you do in the US. And that, that, that suggests to me that we should at least
2782500	2784540	be asking this intentional question.
2784940	2788020	Very, very fair point. I, I'm certainly not trying to make the argument that
2788020	2793340	TikTok is a productivity helper. And it could well be insidious and evil as
2793340	2793580	well.
2794780	2798500	My intuition is that it's, it's the, it's the biggest thing in AI. So if I had a,
2798500	2802980	if you had to wait them, how big they are, I think TikTok is the biggest thing.
2803980	2808620	We have a microphone here if folks are interested in asking questions to Peter.
2808620	2811980	And if you can look this way towards the cameras, that would be helpful as well.
2812980	2817300	Thank you. And thank you, Peter, for that most precious of gifts, your time. And I
2817300	2822380	for one, I'm willing to pay the time price for it. Given our addiction.
2822380	2825420	Watch it, who are you? Give it, now it's your.
2825940	2826500	I'm nobody.
2827260	2827740	How are you?
2827740	2831060	I'm Stephen's. I went to your first conference.
2831340	2835580	I know, but you've got to give your name when you speak at the microphone.
2835580	2838100	We don't believe in anonymity around here.
2839460	2842740	Very good. And you would go to the core of the question, which is the individual
2842780	2846660	versus the identity. I stand as an individual to ask you this question.
2847420	2852820	Given our addiction to narratives like AI and given our aversion to knowledge in
2852820	2858820	favor of miss mill and all kinds of misinformation and given our emotional
2858860	2863580	overload of the past few years, as a student of René Girard,
2865020	2871660	my question is, will we have a mimetic pandemic where virus is the vengeance
2871660	2873660	and could AI help or hinder it?
2875740	2876740	There's one for you.
2879620	2885940	You know, um, yeah, this is, I mean, this is certainly, um, this is certainly a
2885940	2889540	read on what is very haywire about
2890580	2897620	tech talk that it, you know, it just gives people what they mistakenly think they want.
2898260	2903780	You know, they, they, um, and, um, and, uh, and, um, and then, um, and then, you know,
2903780	2907660	that's, that's often, you know, that's often somehow, somehow a bad thing.
2907660	2912740	I, um, I don't know, you know, I look, I think there is, there are, there are, um,
2912820	2919580	there are ways in which, you know, I wouldn't, um, I wouldn't, um, I'm always,
2919580	2923380	I'm always, I'm hesitant to sort of blame tech for everything that's wrong in our
2923380	2929820	society. Uh, but I do think, I do think there's, there's something about, um, you
2929820	2934060	know, it's, if we, if, and it's, it's, it's wrong to scapegoat and say it's the
2934060	2939780	single thing that causes everything to go haywire, but, uh, but at the margins, you
2939820	2945260	know, is it, is it helping us get, does, does, does, does, um, you know, there's
2945260	2949900	something about, um, about a lot of these sort of short packet content forms that
2949900	2951780	has, you know, has deranged this course.
2951780	2957660	I think, I think TikTok is, is, is by far the worst, um, and, um, and this is sort
2957660	2962180	of a sense in which the, the, you know, Silicon Valley is getting some, some of
2962180	2966020	the blame for, um, uh, for, for all this stuff.
2966420	2969300	And then, you know, I think the alternatives also didn't work because, you
2969300	2972180	know, the alternatives were, you know, a centralized media system, which
2972180	2973140	everything was controlled.
2974100	2974540	Sir.
2974900	2980220	So I, I, I think, I think you have to, if you frame the social problem, it's, um,
2980580	2985780	it's, um, you know, it's too much totalitarian centralization versus
2986060	2987780	deranged decentralization.
2987820	2992060	And you have to, you have to think of both problems and there's, you know, there
2992060	2996740	are instances of both that we had in the COVID epidemic the last two, three years.
2996740	3000420	We had, you know, decentralized conspiracy theories that were not helpful.
3000780	3005260	And we had a centralized narratives that cut off, uh, cut off much needed debate.
3006740	3006980	Hi.
3006980	3007500	Thank you.
3007500	3008540	Uh, I'm Dr.
3008540	3010620	Jeff Garneson from Anchorage, Alaska.
3010700	3011980	This is my second cousin.
3012700	3020500	Um, 60 years ago, CS Lewis wrote an article in the Saturday evening post called,
3020500	3025700	uh, screw tape proposes a toast where he criticized, uh, the dumbing down of
3025700	3029380	American education and I chair an educational foundation.
3029380	3033460	And I'm very concerned about the quality of, uh, U S education.
3033900	3039860	And, uh, and, uh, as you talked about, and what can we do about, uh, educating
3039860	3044860	our youth in, uh, at least in public education, uh, all my colleagues send
3044860	3046500	their kids to private schools now.
3046900	3049460	And so I just wanted to see if you could comment on that.
3051260	3055460	You know, I, I mean, I think there are a lot of people who articulated the,
3055460	3059460	the issues quite, uh, quite, quite strongly.
3059460	3065660	I don't have, you know, I don't have, um, I know, I, I've got much to add to it.
3065700	3073620	Um, I, I do think, um, I do think it's sort of like always a question, you
3073620	3075300	know, what are people doing?
3075300	3076980	What is the teleology of it?
3077060	3083340	And, you know, I think a healthy, you know, primary, secondary, tertiary
3083340	3087740	education system is, you know, it's, it's supposed to, um, make you a
3087740	3093020	well-rounded educated person, become a functioning citizen, our society.
3093420	3099820	Um, uh, you know, if you, if you go into research or academia or certain
3099820	3104820	industries, it is, uh, for you to become be a creative person who sort of
3104820	3111300	pushes the frontiers of, of, of knowledge and, um, and in some sense,
3111540	3117660	you know, it has, it has, um, it has, it has, it has somehow gotten, gotten
3117660	3118260	deranged.
3118260	3122820	And, uh, again, I don't want to blame it on AI or make this the, the
3122820	3127980	single focus, but if you, if you have a narrative out there that, uh, that in
3127980	3133460	the future, AI will do all the thinking for you and you don't need to think for
3133460	3135300	yourself in any way.
3135780	3143220	Um, you know, you know, maybe, maybe, um, maybe, you know, you don't need to
3143860	3144660	learn as much stuff.
3144660	3145780	You don't need to memorize things.
3145780	3148500	You don't, there's sort of all this, these things that are considered
3148500	3153220	education that, that, that seemed to be, um, seemed to be much, uh, much less
3153220	3153860	important.
3153860	3159380	And, um, and I, I, yeah, I, I do wonder that's, that's sort of, that's the
3159380	3162100	broader context in which a lot of these things are operating.
3162180	3166660	That it's sort of like, um, you know, if you, if you say it's not the public
3166660	3169700	school system is not completely deranged, it's just trying to sort of make
3169700	3174340	people these passive cogs in this very large machine.
3174740	3179620	Um, you know, that's, that's sort of the, uh, the way in which it fits into
3179620	3181300	this dystopian AI narrative.
3181300	3184900	And, um, and we need to, yeah, we need to tackle this problem on all these
3184900	3185460	different levels.
3185460	3186340	Hi, Peter.
3186340	3187700	Um, my name is Rick with Mivium.
3187700	3190820	Um, I, I heard you give a talk at Stanford a few months back.
3190820	3194820	And so this is the second time I heard you mention a lack of progress, uh, in, uh,
3194820	3195700	on the atom level.
3196260	3200820	So, uh, coming from the, uh, semiconductor material science field, uh, you
3200820	3203860	know, I, I wanted to see what you mean by that because all we do is manipulate
3203860	3207380	atoms on a day-to-day basis at scale from atomic layer deposition to molecular
3207380	3209460	beam, epitaxial growth to mechanical chemistry.
3209460	3212900	That's the only way we can make these next gen semiconductors, uh, these new
3212900	3216100	materials that can basically replace silicon and free us from any kind of
3216100	3218980	independent dependence from China and raw materials.
3218980	3222740	So for me in my field, it, it's not a lack of progress.
3222740	3226100	It's more the incumbent thought, school of thought out there is all chemistry
3226100	3226500	based.
3226500	3229780	Everybody wants to do with chemistry and toxic chemicals, uh, and things like
3229780	3229940	that.
3229940	3233300	Whereas we're trying to do it different way, but we face a lot of resistance.
3233300	3235620	The barrier of entry and the cost is also very high.
3235620	3237860	So I want to know what you mean by lack of progress.
3238580	3245780	Well, it's, it's, it's, it's, um, it's certainly, um, slowed in, in a lot of
3245780	3246740	fields.
3246740	3250900	Look, I think, I think the, the complicated version I would tell is that we've had,
3250900	3257620	you know, a decent amount of progress in the world of bits the last 40, 50 years.
3258500	3261860	I think semiconductors are sort of the, the in-between thing.
3261860	3266500	And then most other fields have been disappointingly, at least slow.
3266500	3270180	You know, when I was an undergraduate at Stanford, I was class of 89, um,
3271220	3274420	you know, in, in retrospect, it wasn't obvious at the time, but in retrospect,
3274420	3276420	you were supposed to study computer science.
3277300	3279700	All the engineering fields were bad fields to go into.
3279700	3282660	It was a bad idea to go into aero astro.
3282660	3284340	It was a bad idea to go into nuclear engineering.
3284340	3285540	If people knew that, they didn't do that.
3286180	3290500	It was a bad idea, mechanical, chemical, you know, all the sort of, uh, world of
3290500	3291700	atom stuff was bad.
3291700	3296660	I think the one that was still in, in between that, that worked okay, but much less well
3296660	3299380	than computer science was electrical engineering.
3300180	3304020	And, um, and, and so, you know, the, if we, if we look at, you know,
3304020	3308980	how well have the companies done, how well have the people gotten paid, um, you know,
3308980	3314420	EE has done better than other fields, but a lot less well than, um, than computer science.
3314420	3320500	I, I think on some level, it reflects the ways in which the progress is hard.
3320500	3324820	It's, it's, it's, it's slower than it was in the 80s and 90s, but by various measures,
3325700	3327620	it requires enormous scale.
3327620	3329860	So, um, the role for the individual is less.
3329860	3335700	You know, if you have to have a $500 million ASML, you know, machine to do the lithography,
3335700	3341460	you know, um, it's harder to start, you know, it's hard to start a new, um, semiconductor company.
3342420	3346900	And so, yeah, the ecosystem is shifted towards, you know, much bigger businesses dominating it.
3348340	3355220	And, and, you know, certainly as a venture capitalist, um, uh, I, we've done virtually
3355220	3356740	no investing in semiconductors.
3356740	3362020	You know, I, I think, I think I should, but, um, you know, it's, we do so little that we
3362020	3364100	don't know enough about it to do it.
3364100	3367700	And that makes me think that it's, um, you know, there's some things happening,
3367700	3371300	but it's, it's, it's still, uh, it's still a lot slower than it was in the past.
3372340	3374260	This is, this is the, this is the challenge with China.
3374260	3377380	It's not like China is copying the West.
3377380	3379140	They will eventually catch us up.
3379140	3384500	And so if we are progressing slowly, they will eventually be able to copy things and converge.
3384500	3390340	And, uh, I think the sort of rate at which we're doing new things is not so great
3390340	3392180	that China will not be able to converge.
3392820	3392980	Yeah.
3392980	3395300	But China has no access to the ASML machines.
3395300	3397220	Uh, they're still doing a chemical method.
3397220	3401860	They're probably, they're, it's, it's possible that it's, uh, it's still going to be
3402580	3407300	non-trivial for them to catch up, but it's, it's, it's not as though, you know, we have this
3407300	3413300	exponentially growing lead that, uh, you know, um, it's, it's, it's, it's not quite as strong as that.
3413300	3420980	Thank you, Peter, for your provocative remarks that are going to precipitate many discussions
3420980	3422340	for the next two days.
3422900	3432500	I, and, uh, uh, including about, uh, all these issues surrounding technological progress.
3432500	3433540	And thank you.
