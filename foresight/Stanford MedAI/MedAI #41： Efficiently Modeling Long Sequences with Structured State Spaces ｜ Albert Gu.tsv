start	end	text
0	8080	Okay, hi everyone, welcome to our 41st session of the May AI Group Exchange.
8080	13320	This week we have Elba Gu from Stanford here with us to present his research on efficiently
13320	17280	modeling long sequences with structured state spaces.
17280	21200	Elba is a final year PhD candidate in the computer science department here at Stanford
21200	24080	University, advised by Chris Ray.
24080	28920	He's brought the interest in studying structured representations for advanced signal capabilities
28920	34400	of machine learning and deep learning models with focuses on structured linear algebra,
34400	38480	non-euclidean representations, and theory of sequence models.
38480	41040	Thank you so much Elba for joining us today.
41040	45440	Before we start, do you have any preference on how you want to take questions?
45440	49560	Yeah, thank you, thank you for an introduction.
49560	56560	For this talk, I think I'm not sure usually the level of formality, but I'm very happy
56560	63560	to have the casual in terms of the conversation and the questions.
63560	67320	I think there's some time, it's not going to be a full hour talk, so I'm more than happy
67320	71120	to take questions during it and I'll watch the time in case it gets too long.
71120	77320	And then I'll also pause a few times to pause for potential questions during some sections.
77320	79520	Okay, sounds good.
79520	83120	Let's try to make this session as interactive as possible.
83120	86840	How further or do let me hand it over to Albert?
86840	88840	Thank you.
88840	95640	All right, so this talk will be about a new sequence model called S4, or structured state
95640	96640	spaces.
96640	105320	Now, for the purposes of this talk, when I mentioned sequence models, we will think of
105320	111280	them as a black box sequence-to-sequence map composed of primitive layers, where each
111280	119680	layer simply takes an input sequence and returns a sequence of the same shape.
119680	124680	For our purposes right now, we'll think of them as just being a one-dimensional to one-dimensional
124680	132360	map, but this can be easily converted to higher-dimensional features.
132360	135600	Many sequence models have been developed that satisfy this interface, particularly in the
135600	137520	context of deep learning.
137520	140800	These include many classical deep learning models, such as recurrent neural networks
140800	145320	or RNNs and convolutional neural networks or CNNs, as well as many more modern models,
145320	150360	such as transformers or neural ODEs.
150360	153440	And all of these models kind of satisfy the same interface.
153440	157600	They map a sequence to a sequence of the same shape, or meaning the same length and
157600	158800	field dimension.
158800	164760	And then you can incorporate any of these into a deep learning model fairly easily just
164760	173080	by using standard architectures, where you can include normalization layers, other linear
173080	177920	or nonlinear activations, as well as with the dual connections.
177920	183040	And so the core component of all of this is the core sequence model, and that's what
183040	186200	we'll focus on.
186200	191280	And this generic deep neural architecture based on sequence models can be used to solve
191280	198280	many types of problems with many types of sequence data, from medallies such as text
198280	206680	and audio to images and videos to general time series data or biosignals, for example,
206680	211080	which is depicted here.
211080	214120	In this talk, I'm going to draw a very rough distinction between different types of sequence
214120	215120	data.
215120	219560	Now much of modern sequence modeling in the context of machine learning focuses on data
219560	221440	such as text.
221440	225120	And very roughly I'll classify this as being a discrete sequence because the input comes
225120	228560	in the form of discrete tokens.
228560	233240	And other types of data like this includes things like graphs or things like DNA-based
233240	234240	pairs.
234240	240480	In contrast, what this talk will focus on is data that's roughly more continuous, things
240480	244600	such as video or time series or audio.
244600	248520	And what's common to all of these is that there's an underlying notion of time from
248520	252920	which the sort of data is sampled from.
252920	257960	And so I'm going to very broadly call this type of data signal data as opposed to sequence
257960	259080	data.
259080	263000	And roughly speaking, signals can be defined as data that's generated from an underlying
263000	269200	continuous physical process, including all these examples here.
269200	271320	This talk will be composed of two parts.
271320	276480	The first part covers a method called HIPPO, which was the predecessor to S4.
276480	282000	And it's a new conceptual framework for the online memorization of signals and led to
282000	285320	a new method for modeling signals and sequences.
285320	289560	And then the second part will be S4, which built right on top of HIPPO.
289560	293120	And it has a lot of important properties that have been very effective for addressing some
293120	295240	types of sequence modeling problems.
295240	299240	And before I get into the technical stuff, I'll give a quick preview experimental results
299240	304120	to highlight the types of improvements I will see and what it's good at.
304120	307080	And this will kind of illustrate the types of challenges that we'll hope to address with
307080	310200	these new models.
310200	314520	The first challenge overall is just going to be to signal or general temporal data that
314520	315600	I just defined.
315600	317000	And this data is really everywhere.
317000	323760	So some examples include audio waveforms, spatial temporal data like videos, biosignals
323760	330840	like electrocardiograms, which have important applications of medicine, or market and financial
330840	332200	data.
332280	335520	And then there will be multiple time series logs being generated by every major industry
335520	339040	and many other types of scientific modeling problems.
339040	344360	And we'll return to these experiments later with a particular focus on some biosignal
344360	345360	data.
345360	351080	But for now, I will just use one example to illustrate, which is audio.
351080	355320	And audio is actually one of the most common types of data because it's just raw sound.
355320	356320	It's everywhere.
356320	361200	And so to illustrate, machine learning right now is really all about text and so many
361200	365240	headline results recently have been about people scraping together all the raw text
365240	368040	data they can get, creating massive models on them.
368040	373000	And that's led to very impressive results like GPT-3, which I don't know the audience,
373000	377400	but hopefully many of you have heard of this model.
377400	380960	In contrast, audio actually has orders of magnitude more data than text.
380960	384880	For example, a single labeled dataset has more data set than all of the data used to
384880	387080	train those massive language models.
387080	389600	But you don't hear about benchmarks in this domain nearly as much.
389800	395280	And I think part of the reason is just because audio models are, audio is very challenging
395280	399640	and current models seem much worse in comparison to text.
399640	407680	And so here's a concrete example where we consider basically a very general and hard
407680	414400	audio generation setting of generating spoken digits, zero to nine, using a completely
414400	416640	unconditional autoregressive model.
416640	420120	And the gold standard here is a baseline called WaveNet.
420120	423760	And here's what it sounds like trying to say these numbers.
431760	434360	So it's, it's not very good.
434360	438320	And here's results for S4, which was just, these results are just from the past like
438320	440320	two months or so ago.
441320	447800	One, two, three, four.
447800	451600	So that's, that's a pretty concrete example.
451600	455440	And so in this talk, we'll see how models like S4 are kind of designed for signals in
455440	459200	a way and can have significant advantages for this type of data.
459200	464920	And the second example up front, or the second example of a running challenge will be, can
464920	467480	be motivated by examining audio more closely.
467480	471320	And one reason why audio is so hard is because it's sampled at such an extremely high rate
471320	475400	where a single second has 16,000 or more samples.
475400	480800	In contrast, most sequence models can't deal with more than a thousand or so.
480800	484320	And to illustrate, there was a benchmark in the past year called Long Range Arena that
484320	487760	measured the performance of models on a suite of long range tasks.
487760	493000	And the most popular sequence models these days, Transformers, were the main focus.
493000	497200	But despite their many successes, they don't do so well on long context.
497200	501440	And so there were dozens of variants that were tried, and they all get to around the
501440	506920	same performance, which is actually not much above random guessing.
506920	512480	In contrast, S4 we'll see is explicitly designed to be effective on long context, which leads
512480	514960	to a huge improvement on this benchmark.
514960	519360	And it's the first model to ever make progress on some really difficult long sequence tasks.
519360	523320	Can I ask a quick question here, Albert?
523320	524320	Yeah.
524760	529360	So in the previous task, that was a generative process.
529360	536680	And in this, the Long Context Channel challenge, is it a classification or what kind of task
536680	537680	is it?
537680	539080	These are all classification problems.
539080	545200	And they're on data such that includes several data modalities, such as text, images, some
545200	548440	sort of symbolic processing, stuff like that.
548440	549440	I see.
549440	553440	And so you can use S4 both as a generative model to actually generate sequences.
553680	554680	Yeah.
554680	560640	So a lot of sequence models, again, a sequence model I'm defining as a black box interface,
560640	563280	really, that's just a sequence-to-sequence map.
563280	568640	And many of these can be used in many ways, both for classification and generation.
568640	574320	For example, transformers or RNNs are similar things that satisfy the same interface and
574320	576280	can be used in many ways as well.
576280	577280	Gotcha.
577280	578280	Thank you.
579960	580960	Yeah.
581080	583160	OK, so now I'll get into the technical portions.
583160	586360	And the first part will be about Hippo, as I mentioned.
586360	592960	And to motivate what Hippo's goal was, I gave a bunch of examples of data that machine
592960	596640	learning models currently struggle with, particularly things like time series.
596640	600600	And to highlight why this is hard, I'm going to use a running example to illustrate a very
600600	606000	basic capability that's difficult for modern models.
606000	609400	And that's the moving average, which is perhaps the most basic method in modern time series
609400	610640	analysis.
610640	615120	So this figure depicts the exponential moving average or EMA, which is the blue line.
615120	620360	And the way it's used is that it's a fixed non-learnable feature that's often the first
620360	626080	pre-processing step that's performed in any sort of time series analysis pipeline.
626080	630960	Now in the context, in the spirit of machine learning and deep learning, instead of doing
630960	636680	manual processing like creating these features, we really would like to be able to learn these
636680	641120	sort of things automatically from the data.
641120	646600	And so in particular, here's a very simple concrete task is, suppose you have a model
646600	654920	and you're feeding it this black input signal, and you want the model to predict the EMA
654920	657320	or the blue signal as the output.
657320	662760	And fortunately, it turns out that standard sequence models, such as attention and convolutions,
662760	665680	cannot do this at all.
665680	670920	And the reason why is essentially because the EMA has unbounded context.
670920	674600	It's actually just a weighted average of the history of the signal with an exponentially
674600	676440	decaying weight that searches back infinitely.
676440	681520	Whereas in contrast, most modern models such as attention or a convolutions have finite
681520	685000	context in those.
685000	687600	Some people wonder about other things like RNNs.
687600	692800	And the short answer is that RNNs are better than attention convolutions here, but they
692800	700320	still aren't that good due to empirical problems with optimization and other things.
700320	708440	So we'll see that the methods that are introduced in this talk will be very naturally suited
708440	711160	for this and are much stronger versions.
711160	715200	But going back to the EMA, the way that one way to think about it is that it's a very
715200	718720	simple summary of the entire history of your signal.
719160	725600	In other words, it's a state X, which is a single number that summarizes the entire
725600	728720	history of the input U.
728720	732800	And the reason why it's useful is that it's easy to compute because if you get new data,
732800	737360	you can update the EMA in constant time using this weighted average.
737360	741280	And beyond the simple example, though, I think these two properties are actually conceptually
741280	743160	really important.
743160	746520	For example, they're exactly the properties that you need in any sort of real-time decision-making
746520	747520	problem.
747520	751200	And really abstractly, you can even imagine that your brain is a state that's summarizing
751200	755640	the entire context of your life, and it's constantly updating as you acquire new information.
755640	760080	So I think that's actually a pretty general important question, and this was a direct
760080	762840	inspiration for HIPPO.
762840	767280	In the context of machine learning, this question has a lot of direct impact on our models because,
767280	769440	as I mentioned, they struggle with long context.
769440	774280	For example, text models, it's been shown typically have a context range of about 100
774280	776440	to at most a few thousand tokens.
776440	782480	Whereas if you want to deal with data such as speech and audio, a single word in speech
782480	787840	is a sequence of length more than 10,000, and this can really stretch to unbounded length.
787840	793800	And so this is the question that I was trying to, that I was thinking about.
793800	798880	And what I did was I tried to convert this vague goal of long-range memory into a more
798880	801240	formal mathematical question.
801240	806560	The conceptual idea is that if you can compress the past into a smaller state that's accurately
806560	812680	remembering it, then you should be able to reconstruct the past.
812680	815520	And we can then attempt to turn this into a technical problem.
815520	819440	So the idea is that we're going to observe an input signal online and try to maintain
819440	823800	a good representation of it that allows us to reconstruct it.
823800	828000	And so, okay, so first in this section, I'm going to formalize this idea, and then I'll
828000	830080	define HIPPO and visualize it.
830080	833960	And then talk about a couple of generalizations.
833960	837120	So the first thing is that let me formalize this idea that I just mentioned.
837120	842960	And so the idea of HIPPO is that, again, we're trying to observe an input signal online,
842960	847360	and we're going to try to encode it as well as possible given a memory budget.
847360	850640	So concretely, you'd think of it like this.
850640	855520	So suppose at some initial time, T0, we've seen part of the input, and we're going to
855520	856880	try to compress this input.
856880	860720	So what you can do is store the best approximation to what we've seen so far.
860720	865880	For example, we can create the best polynomial approximation and write down the coefficients
865880	867560	of that polynomial.
867560	873120	So now the degree of the polynomial or the number of coefficients is the memory budget.
873120	875400	And we want to do this continuously at all times.
875400	882480	So as we keep seeing more data at some later time T1, we'll have to update our best approximation
882480	885040	and write down the new coefficients.
885040	891600	Now the central question is, first of all, how do you actually find these optimal approximations?
891600	898680	And moreover, how can you update this representation efficiently as you keep seeing more information?
898680	904360	And so this is the main conceptual idea, and it's a little bit of work to formalize a little
904360	905360	more.
905360	908680	And in particular, I've been talking about optimal approximations, but that's actually
908680	910080	not well-defined.
910080	914960	And so what we'll need is to find a measure that specifies the quality of approximation.
914960	918880	For example, we can choose the exponentially decaying measure, which says that we care
918880	925400	about approximating the recent pass of the input more than the far pass.
925400	929120	And this will relate back to the EMA.
929120	934160	But given this, the problem is more or less well-defined.
934160	938600	So basically, we have to pick the measure sort of as a hyperparameter or a prior for
938600	939600	now.
939600	942120	Let's talk about how you can actually learn it.
942600	947200	But for now, we need to pick a measure up front, say the exponential decaying measure.
947200	951040	And then you need to choose a polynomial basis.
951040	955120	And then the problem's completely defined, and you can write down the coefficients in
955120	959000	closed form, and you can figure out how they evolve through time.
959000	965880	So I'm going to skip the details of the derivation, but you end up with a closed form method.
965880	970160	And what I want to emphasize is that the derivation has some technically interesting
970160	971360	new ideas.
971360	974640	But the most interesting and important part of this, I think, is just this simple conceptual
974640	979480	idea of the online compression and reconstruction and how to formalize that mathematically.
979480	981920	So that's the main point.
981920	988320	OK, and now with the definitions out of the way, things will become a lot more clear with
988320	991280	some visualizations of what it does.
991280	996280	So first of all, let me just be really formal about defining what HIPAA is.
996280	999080	So I mentioned the problem was that we are encoding.
999080	1003280	So XFT is going to represent a vector of our coefficients at all times.
1003280	1008240	And the question is, how does this evolve through time as we see more data in the input
1008240	1009240	U?
1009240	1011600	And it turns out that it just satisfies a simple differential equation.
1011600	1015040	By going through the derivation, you can write down this differential equation in closed
1015040	1021320	form and write down closed form formulas for this transition matrix involved here.
1021320	1027880	So to be concrete, the ODE is called the HIPAA operator.
1027880	1031840	And the matrices, the matrix in the operator are called HIPAA matrices, which have closed
1031840	1032840	form formulas.
1032840	1037320	In fact, the actual matrix is this matrix.
1037320	1043400	It's an extremely simple matrix, which is a special type of structure matrix.
1043400	1045440	And yeah, so it's just a simple formula.
1045440	1048800	And then we write down a closed form formula for this differential equation.
1048800	1051920	And that's how our coefficients evolve over time.
1051920	1058960	And now, right, so this equation, again, is called the HIPAA operator or the high order
1058960	1063160	polynomial projection operator, because we're projecting on the high degree polynomial basis
1063160	1064160	functions.
1064160	1068600	Now visually, the way to think about it is like this.
1068600	1072400	The reason I call an operator is because it maps a function to a function.
1072400	1077880	So it's an operator that maps this black input signal U to these sets of coefficients X
1077880	1085120	and blue, where every time X of t compresses the history of the input signal U.
1085120	1091160	And you can compute X online as you see one input at a time.
1091160	1093520	So the black line represents our current time step.
1093520	1095880	We're gradually seeing more of the input.
1095880	1100840	And we are updating our coefficient vector, which is depicted in blue.
1100840	1106040	Here I've, this is visualizing just the lowest order for coefficients of the best polynomial
1106480	1109720	approximation.
1109720	1113360	And now here is what the reconstruction looks like.
1113360	1119760	So as I move along through time and update my coefficients, the coefficients that that
1119760	1124480	polynomial defines, in a sense, is actually just this red line.
1124480	1129320	So it is reconstructing the input just like we wanted.
1129320	1133600	Note that we are using only, so here I've only visualized four coefficients, but I'm
1133600	1137800	actually using 64 coefficients, but the whole function was linked to 10,000.
1137800	1142320	So I'm compressing it a lot.
1142320	1148760	And this, here's a static image that kind of illustrates the effect of the reconstruction.
1148760	1153720	So because I'm compressing it, I can't perfectly reconstruct the input.
1153720	1155840	And so how good is the reconstruction then?
1155840	1157480	Well, it depends on the measure.
1157480	1161920	So the green, the green line in this figure was the exponentially decaying measure that
1161920	1164880	we are basically projecting onto.
1164880	1171760	And so intuitively, you can see that the red reconstruction line is really accurate for
1171760	1176400	the recent past and degrades further out in history, but still maintains some rough information
1176400	1179600	about the whole signal.
1179600	1184080	And so that is, that's hippo.
1184080	1188760	Now, oh, one, oh, okay, a question here.
1189120	1190280	No, you can continue.
1190280	1194320	I just had one clarification, but I can ask after you finish.
1194320	1195320	Here's fine too.
1195320	1196320	Okay.
1196320	1201840	So is it fair to think about X as being the state at each time point?
1201840	1208560	And then essentially the red line is trying to reconstruct the signal given the current
1208560	1211920	state or do you also use all the past states to reconstruct?
1211920	1213280	That's exactly right.
1213280	1218160	So yeah, so the reconstruction is happening using only the coefficient vector at the current
1218160	1219320	black line.
1219320	1223640	So every single time I'm using, I'm for, yeah, the blue line, I'm visualizing the whole
1223640	1227280	thing, but at any given point in time, I'm remembering only the current vector, which
1227280	1229880	has length 64.
1229880	1233400	Here I'm only visualizing four of the components, but it has length 64.
1233400	1237400	And using those 64 numbers, I'm reconstructing what I've seen so far in red.
1237400	1238400	Oh, awesome.
1238400	1239400	Thanks.
1239400	1240400	Right.
1240400	1246760	Now, in that previous figure, if I just take one of the blue lines, actually the lowest
1246760	1250900	order coefficient and overlay over the function, you can see that it actually turns out to
1250900	1252400	exactly be the EMA.
1252400	1260200	And so it turns out that moving averages can be viewed as order zero or low order projections.
1260200	1263360	On the other hand, hippo is essentially a very strong generalization of this that solves
1263360	1271360	a natural mathematical question and gets back things like the EMA for free.
1271360	1276520	So that's what hippo is, and now I'll just talk a little bit about some extensions of
1276520	1277520	it.
1277520	1281760	So first of all, a natural question that may be wondering is that I've been using this
1281760	1284680	example of an exponential measure, but what about other cases?
1284680	1288000	Well, it turns out that hippo can be derived for any measure.
1288000	1292640	For example, here's a case that is pretty natural as well, which is what if I want to
1292640	1294160	reconstruct along a uniform measure?
1294160	1301320	In other words, I only care about remembering the recent past in sliding windows of my function.
1301400	1307880	And this is possible, so you would get a different ODE, and here's a reconstruction in effect.
1307880	1313280	So again, using just 64 numbers in memory, I'm trying to reconstruct the last 2,000 time
1313280	1320080	steps of this function uniformly, and it's doing this quite accurately.
1320080	1324080	Now you can generalize it even further to, for example, when the measure is changing over
1324080	1328760	time instead of just sliding along, and so there's a very general framework here that
1328760	1331600	can do lots of things.
1331600	1334680	A lot of this was in follow-up work to their general hippo paper, and what we showed was
1334680	1343080	that for essentially any measure, there exists a corresponding hippo operator where the hippo
1343080	1348640	matrices A and B depend on the measure, and you can write them down in closed form.
1348640	1352480	And this is important, I think, because it draws an equivalence between measures and
1352640	1359520	these ODE's, where this means that we don't, I mentioned earlier that we had to choose the
1359520	1364480	measure up front as a prior, such as the accidentally decaying case, but actually just by learning
1364480	1371360	these matrices A and B, it's in some sense the same as learning the measure.
1371360	1377880	Okay, so now even better, not only do these operators always exist, but it turns out that
1377880	1379400	the matrices are always structured.
1379800	1385160	Previously, we saw, for the accidentally decaying case, the matrix was actually extremely simple.
1386040	1389880	In general, they're going to be more complicated than that, and it's, they satisfy a structure,
1389880	1394120	which was something that I introduced in much earlier work, but they are all structured in
1394120	1400280	some way, and that means that you can, how do you actually calculate these updates through time?
1400280	1404280	You can actually update the state or the coefficients in nearly optimal time.
1404760	1412680	Okay, so that was the main takeaways from HIPPO, and so just to recap, we were inspired by these
1412680	1417560	very basic, these simple but important properties of trying to maintain a state that's summarizing
1417560	1423000	the entire context, and we formalized this into a mathematical problem, which was pretty
1423000	1427160	intuitive, and we were then able to solve analytically, and this resulted in a nice
1427160	1429640	class of methods for addressing long context and signals.
1429880	1431880	Okay, so I see a question in the chat.
1434920	1439560	So, can these operators be expressed in terms of Z-transforms? I'm not quite sure what you mean here.
1441480	1444760	To my understanding, Z-transforms are like the discrete version of a Laplace transfer,
1444760	1448520	and I'm not sure if that's the one you're referring to, or another notion.
1451000	1454840	Yes, that's what I was thinking about. It seems like as though
1455320	1462120	just as you can express like exponentially, exponential decay in terms of Z-transforms
1462120	1467320	of the functions, that there seems like it's likely to be a link.
1469320	1472760	Yeah, I mean, I think all these things have a tight link, and they're connected to each other.
1472760	1478200	It turns out actually that the way that in the next part, when I talk about S4,
1478200	1481640	there's going to be some difficult computations, and I'm not sure if that's the one you're
1481640	1484280	talking about. In the next part, when I talk about S4, there's going to be some difficult
1484280	1492760	computational issues for computing certain things that I'll introduce, and to actually compute them,
1492760	1496760	I essentially actually go through Laplace space or frequency space. So, essentially,
1496760	1502520	I actually take the Z-transform of this equation and calculate that transform at several values,
1502520	1508040	and then invert it to get the hippo matrices back, or to get a certain thing back.
1508040	1517880	Sounds good. That makes sense. Yeah, great question. And yeah, so I wanted also just
1517880	1521240	to stop around here at the summary for any other questions.
1525880	1530200	And if there's none, that's great because usually this is a pretty complicated
1531960	1536040	framework mathematically, but hopefully the visualizations help explain it a lot.
1538120	1547160	Okay, so I'll move on to the next part where, so one thing I didn't include in this section
1547160	1551560	was any experiments. So the way we evaluated this is kind of just like how good is the
1551560	1556680	reconstruction, and actually using this method in machine learning models did pretty well,
1556680	1560840	just naively, but where it became really effective was when incorporated into a model
1560840	1567160	in a particular way, and so that's what S4 will be. And so to, first I'm just going to define S4,
1568200	1571400	and I'm going to define it through hippo, which was the original motivation,
1572920	1576280	and the motivation here is going to be very simple. So to refresh your memory, this is what
1576280	1580680	hippo does, it maps an input signal, which in our case they're thinking of as 1D,
1580680	1585160	to a higher dimensional signal. Now the problem is that we've blown up the dimension of the input
1585160	1592360	from one dimension to n dimensions, where n was our memory budget or the number of coefficients,
1592360	1602120	and typically this is going to be at least 100 or so. So the motivation for, so I work in deep
1602120	1607640	learning, and I just wanted to incorporate hippo into a deep learning model, but this is a problem
1607640	1611560	because you can't just stack layers of this because you just keep increasing the dimension.
1612120	1617400	And so a very simple motivation to fix this is just, let's just decrease the dimension again.
1618360	1623640	And the way to do this is that you can just take a very simple linear projection. So
1625960	1629400	what we'll do is that we have a state x, which was like a 100 dimensional vector,
1629400	1633960	and we'll just hit it with a dot product that can be learnable to get back a single number,
1633960	1638280	which is essentially taking a linear combination of the blue lines to get the final output,
1638280	1643000	which is the red line. And then we'll add a multiple of the original input, which can be seen as a
1643080	1649160	skip connection. And that is the entire definition of S4. It's finally these two equations where
1649160	1654520	the first one is the hippo equation, which takes the input to a state that's kind of memorizing it.
1654520	1660360	And then the second equation just combines the state linearly into a single output.
1661880	1665640	Now, for those of you with a background engineering, this definition may look really familiar.
1666920	1671000	And this is because this is a well-known model called a state space model or SSM,
1671080	1674520	which is sometimes depicted with this simple control diagram. And they've been around for
1674520	1680840	decades, such as the famous common filter and use in many scientific fields. I think outside of
1680840	1686760	controls and statistics, they're also pretty commonly used in perhaps computational neuroscience
1686760	1693160	and many medical problems as well. Now, what the theme of this part will be is that
1694840	1697640	we'll see that SSMs are a really elegant and natural model,
1698200	1702760	but they haven't been used in deep learning before in this way. And for underlying reasons
1702760	1708200	that we'll see in that S4 address. But for now, just to define S4 in terms of this model,
1709080	1715080	the way that we'll define it is that it's just an instantiation of an SSM, these two equations,
1715960	1723960	where we'll plug in specific values of matrices in. And although it turns out that
1723960	1727800	although this model is simple to define, actually computing with it turns out to be
1727800	1737880	difficult and will require new ideas and algorithms. And so my goal of this in this section is to
1737880	1745080	convince you that this is a really elegant and fundamental model. And so first of all, I will
1745080	1750280	talk about some general properties of SSMs that would have a lot of benefits in machine learning
1750280	1759000	and deep learning that are independent of S4. And then I'll show how those come with associated
1759000	1763640	trade-offs that prevent them from being really good in deep learning. And S4 will solve those
1763640	1769320	problems. And finally, I'll show several real world experiments that show S4's effectiveness
1769320	1777560	in a bunch of settings. So in this first part, I'm actually going to describe three different
1777560	1781720	ways to think about SSMs, which give them a lot of nice properties. And this was theory developed
1781720	1787240	in the predecessor work to S4 that and will have empirical concrete empirical benefits.
1787960	1793480	And so the first way to the first property is that SSMs inherently operate on continuous time
1793480	1800040	signals instead of discrete time sequences. So here's how to think about it. So in machine learning,
1800040	1803320	we usually work with sequence models, which I defined as a parameterized map
1803880	1806280	from an input sequence to an output sequence.
1809720	1815640	What if instead of mapping a sequence to a sequence, I coined this term signal model
1815640	1820120	to denote a parameterized map that maps a function to a function or a signal to a signal.
1821480	1827400	And given one of these maps, you can essentially discretize the inputs and outputs however you
1827400	1833880	want to get back a sequence. So essentially, the upshot is that signal models are in some sense
1833880	1838760	a generalization of sequence models, where they actually map functions and functions,
1838760	1844760	but by discretizing them, you get back a sequence model. And so the first way to think about SSMs
1844760	1849960	is that they are just a simple parameterized signal model, where the parameters were matrices A, B,
1849960	1856600	C, and D, and they map an input function to an output function. That's it. Just in terms of the
1856680	1861640	interface or the API of the model, that this is what it does. The reason that this property is
1861640	1867240	important is because even when we're working in discrete time, the model in some sense understands
1867240	1873080	the underlying continuous domain. So I will show what I mean concretely by this later empirically.
1875800	1880040	All right, so that's the first representation. The next perspective relates back to the original
1880040	1885160	motivation of HIPAA, which was about online computation. So how do we actually compute
1885160	1890520	the output of this SSM? One way to do it is to process the input one at a time, just like HIPAA did
1891240	1896760	in an online setting. And so this is our current computation because each update can be computed
1896760	1902920	efficiently from the previous one. And just to unpack a little why this is non-trivial, imagine
1902920	1908440	we're processing this very long input, and we're at this current time step denoted by the vertical
1908440	1913080	line, and we get just one more data point. So just like a single number for the input,
1913080	1919560	and we want to compute the next output. So this output depends on the entire history of the input,
1919560	1924760	and so you'd expect it, the computation of the next one to scale with the length of the sequence.
1925880	1931960	But actually we can compute it in constant time. And this is a non-trivial property that most
1931960	1937080	sequence models don't have. For example, in a transformer or a convolution, if you were to
1937080	1942200	do this in an online or autoregressive fashion, computing mapping one input to one output,
1942200	1945240	each computation will scale with the entire length of the context window.
1947160	1950520	The reason that SSMs can do this so efficiently is because they're stateful,
1951160	1956360	which is a point that's kept coming up, where in memory we're maintaining a state,
1956360	1960760	which is the blue thing, which is a single vector that summarizing the history, and can be updated
1960760	1970520	very efficiently. This makes them really efficient in any sort of online setting, as we've seen.
1972760	1978760	And yeah, so we'll see again why this matters. But there's one main drawback, which is that if
1978760	1985080	you're not in an online setting, this is slow because it's sequential. And so what if you
1985080	1989720	actually know all the future inputs? Then ideally you wouldn't do this step by step,
1989720	1994840	and you could do something faster and parallelizable. And so that was actually basically the main
1994840	1999640	problem with RNNs and why they've recently fallen out of favor in machine learning,
1999640	2005560	because they're sequential and not parallelizable when you see a lot of data at once.
2007320	2010520	And so that motivates the final representation, which is the convolutional representation,
2010520	2015480	which allows them to be paralyzed. And so the idea is that instead of mapping going from the input
2015480	2019720	to the state to the output, you can actually go straight from the input to the output,
2020440	2025640	bypassing the state, and doing the entire computation in parallel over the sequence length.
2026200	2030200	The reason is that SSMs turn out to be equivalent to convolutions,
2031560	2037880	where computing the map from the input U to the output Y is equivalent to convolving the input
2037880	2044600	by a particular convolution filter, which is depicted in green here. And so to compute this
2044600	2050760	map, you just do it's just Y equals U convolved with K for this convolution kernel. And so this
2050760	2059960	can be done very efficiently using no techniques. So for the practitioner, what one thing I want
2059960	2065240	to emphasize is that I think the most useful way to think about SSMs potentially is as
2065240	2070440	essentially a very fancy CNN, where you're parameterizing the convolution kernel in a
2070440	2076840	different way. And notably, this kernel can be infinitely long, which again points to one reason
2076840	2086680	why this is very good at long range dependencies. So just to call back to this example again,
2086680	2092120	the EMA, the EMA is actually literally just a single convolution where you can involve the input
2092120	2098760	by an accidentally decaying convolution kernel. And as I mentioned, although things like CNNs
2098760	2103880	are also literally convolutions, they can't represent the EMA because CNNs are finite
2103880	2108840	window and the EMA is infinite window. On the other hand, SSMs do represent infinitely long
2108840	2114280	convolutions. And in fact, there's a very, very simple way to write down the EMA as a directly
2114280	2124200	as an SSM. And I think Chris kind of pointed to that earlier. So those were the three properties
2124200	2130200	of SSMs that I wanted to mention. And just to recap, first of all, we're going to think of them as
2130200	2136440	maps that operate on continuous signals, not just sequences. If your model is deployed in a
2136440	2140600	setting where it sees inputs in real time or online, it can compute these efficiently
2143720	2149240	recurrently. And if you see an entire input at once, such as usually during training time,
2149240	2151480	you can compute it even more efficiently and in parallel.
2154440	2159960	I have a quick question here, Albert. This is super cool. I was just wondering if the goal
2159960	2164040	is actually to get a representation of your signal so that you can perform different
2164040	2169000	downstream tasks. Isn't it better to actually have the state space representation rather than
2169000	2174920	directly going to the outputs? In that case, would we have to stick with HIPAA instead of going to S4?
2176440	2180680	Great question. Actually, no one's asked me that, but that's a great question.
2181400	2191560	So the way that I think about this is that what's happening is that essentially we have
2191560	2197560	this nice state, which is very meaningful. And then the second part of the SSM that projects it
2197560	2201800	is kind of like the learnable thing that's figuring out how to extract the right features
2201800	2207000	from this state. Now, I mentioned that everything I've done so far, so that's a learnable part
2207000	2212040	that's actually using the entire state, in a sense. And I mentioned that I'm only considering
2212040	2218120	the one-dimensional case so far with 1B inputs and outputs. But actually, what's going to happen
2218120	2223800	in practice in our actual deep learning models is that we'll have multi-dimensional inputs and
2223800	2229080	outputs, and we'll essentially run an SSM on each one of them. And each one of these will
2229080	2234120	learn how to use the state in a different way. So we'll have essentially many, you can think of
2234200	2239240	as maybe we'll have a single state, but many, many possible outputs that are all learnable,
2239240	2243240	and we'll extract different features from that state. So we are going to get a lot of different
2243240	2250040	features that utilize the state in however they want. I see. Okay. Thank you. But sorry,
2250040	2257960	sorry, Joanne. But isn't it like all these dimensions also have a correlation? So do you also,
2257960	2262680	if you run the space independently, don't you want to also preserve the correlation?
2264840	2267960	So this is something that I think a lot of people working with time series
2269240	2274440	are concerned with. And somehow in deep learning, we don't normally consider that aspect. And we
2274440	2280440	kind of just throw in a really big model and a lot of these independent layers. And kind of,
2280440	2285480	I think in practice, what usually happens at the model learns to, it learns whatever it needs to do
2285480	2290600	for the final prediction task. And this often does involve like, I think it does end up
2290600	2295960	decorrelating things. But it's not super clear exactly the dynamics of what happens. And this is
2295960	2300520	kind of a more broad question for deep learning theory in general. That's not well understood
2300520	2309320	right now. What I can say is that we've used this on many types of like noisy data that usually
2309320	2314280	involve, so I'm going to get to experiments later, but we have tried this on many types of
2314280	2318040	like time series and other noisy data like EEG. But one day, one day, right?
2319560	2324360	It can work on multiple dimensions, which I kind of just pointed to you. And also I'll mention
2324360	2330840	again later how we do that. But yeah, you can just kind of do it do it naively on multiple
2330840	2337720	dimensions. And it just works out of the box. Okay. Okay, so before we get to the experiments,
2337720	2345640	I just have a little bit on kind of the how S4 builds on top of SSMs. And so just to refresh
2345640	2351480	your memory of what S4 is, it's just an SSM where we plug in certain formulas that were
2351480	2354920	based on the theory of memorization. And we have special algorithms to compute it.
2356760	2363000	And so first of all, why are these matrices needed? Well, the most important part of the SSM is the
2363080	2373720	state as Nandita keeps insightfully bringing up. And so what HIPPO did was that it computed a
2373720	2379560	very particular state that was mathematically meaningful and compresses the history of the input
2380760	2385640	in a way that captures long range dependencies. And so basically just by plugging in that formula
2385640	2390360	into this SSM, it learns a more meaningful state that allows the SSM to address long
2390360	2395880	dependencies better. So just to illustrate this empirically, here's a simple experiment on a very
2395880	2401640	standard benchmark for sequence models. The actual task doesn't matter, but it's well studied and
2401640	2406760	standard sequence model based on such as transformers, CNNs and LSTMs all get to around the
2406760	2411480	around the same accuracy of like 60 ish percent. Now, what happens if we use an SSM?
2412600	2415960	If you use it naively by randomly initializing all the parameters, which is
2415960	2419480	what you would typically do in deep learning, it actually does terribly.
2421480	2427880	But what happens if we just plug in this formula? Plugging this in and not even needing to train
2427880	2433960	the matrix gives a massive boost to the SSM and goes from much below the baselines to
2433960	2439000	substantially above the baselines. And actually, I use the very small models for this ablation here,
2439000	2445480	but the full model as far on this data set gets over 90%, which is something like 20 plus points
2445480	2453160	better than all other sequence models. So that kind of illustrates why HIPPO is so useful.
2454520	2462200	Now, quick question in this example. So are both A and B basically just plug in matrices or
2462200	2468840	is A alone basically a measure? A is the more important matrix, but actually, yeah, just plugging
2468840	2477160	in A and B essentially just just they're both fixed matrices, which are the HIPPO operators
2477160	2480680	specifies both of these. I've only illustrated A because it's a more important one. But yeah,
2481640	2485400	this particular experiment froze both of these matrices to specific ones.
2487560	2491640	One question people have is that like, do we always freeze these? And actually,
2491640	2497400	we can train them as well. This was to illustrate just like even freezing them, it does super well.
2498360	2502680	And then, but in practice, we do train them and it makes it do a little bit better.
2505080	2510360	Okay, so that was one thing. And that kind of points to I mentioned that SSMs have not been
2510360	2514920	used in deep learning before in this way. And that's kind of one problem. If you do it naively,
2514920	2520200	it doesn't work. And so you need this new theory. The second reason is actually that they're
2520200	2526600	computationally pretty difficult to work with. And so here's the illustrate.
2528360	2532680	Again, so to remind you, we're thinking of an SSM as a parameterized map from an input signal to
2532680	2537560	an output signal. And I'll suppose that our input had length L. So our input would just
2537560	2543480	give us a sequence of L numbers. Then the output of this whole thing is also a sequence of L numbers.
2543480	2547320	And computing this map ideally takes around O of L time or not too much more.
2549640	2554040	But here's the problem. SSMs map the input to the output through this state.
2554120	2559480	And that state gave them a lot of nice properties, but it's also 100 dimensions higher. And so
2559480	2564520	computing the end to end mapping through the state will take 100 times more computation and memory
2564520	2572040	than what's needed to compute the final answer. And this is actually a real problem. And now,
2572040	2575160	earlier I said that you don't actually have to compute the state. You can compute it using a
2575160	2579720	convolution instead. But what happens is that before computing the convolution, I have to compute
2579720	2584840	the kernel or the convolution filter in green. And computing that is just as slow as computing the
2584840	2591320	state. And this sort of makes sense because it hasn't changed the computational hardness of the
2591320	2597400	problem. So essentially computing it no matter how you do it is going to be slow and memory inefficient.
2599080	2604600	So the main point of S4 was showing that you could substantially reduce this computation
2604600	2608200	when the SSM is structured. And for example, when using the
2609240	2613480	hippo matrix instead of an unstructured matrix, you can save this factor of 100
2614040	2621320	and make S4 overall extremely efficient. So this is done through a particular algorithm,
2621320	2626440	which I'll just flash up. But basically we're trying to work with this SSM, but we only need
2626440	2631800	to work with specific structured cases such as this, such as some particular hippo matrices.
2632520	2638760	And now using some algorithmic ideas, it turns out there is a way to compute the convolution
2638760	2644920	kernel, which was depicted in green before, very efficiently. And then compute the whole thing using
2644920	2651560	a convolution. So I won't go into details here. And I will also mention that recently we've been
2651560	2655720	developing simplifications of the model that allow you to bypass all of this and do things much more
2655720	2662120	simply. So hopefully in a few weeks we'll have some stuff out that's where you don't need to
2662120	2669320	worry about this really complicated algorithm. All right, so that was the technical portion of
2669880	2672840	that I wanted to mention for S4. And I'll stop here for questions as well.
2676440	2684840	So if in any case you want to actually get the state, can S4 actually recover the state?
2685880	2692680	Or is it like, yeah, so like, I don't know if that would be any use case.
2692680	2696600	But like I was saying, like, if there's a case where I actually want the state,
2696600	2701960	can I do that and get the convolution? Yes, you can. And in fact, that will be used in
2702920	2707720	some experiments. I guess I didn't mention explicitly, but you can compute it in either way,
2707720	2713240	either through the convolution or through the state. And where the state or the convolution is
2713240	2718920	useful is during training time for parallelizability. But where the state is useful is at some sort of
2720200	2725000	inference or deployment settings, where perhaps you might be online, and then you would actually
2725000	2728760	be going through the state instead of the convolution and unrolling things one step at a time.
2729720	2735480	Right. So you can do it either way, which is pretty cool. Thanks.
2736360	2743640	I had more of a thought question. Let's say I'm interested in two different measures. Like,
2743640	2749400	I want to see how the exponential average works, but I also want like, so is it,
2750200	2754760	does it basically mean that I just have to create a new measure that combines this efficiently before
2754760	2761960	this? Before I plug it into SSM or can S4 basically kind of,
2762920	2766360	because there are two independent blocks that I can basically...
2768120	2771320	Yeah. So I'm just about to get to the experiments. And actually, I will,
2772440	2777400	I'll get to that slide right now, where, so first of all, the experiments will be on this type of
2777400	2784520	signal data. And what, as I mentioned a couple of times, what we actually do is that I have to
2784520	2790120	find this 1D to 1D map, but I'm actually going to just like, given a multidimensional input,
2790120	2795320	I'm just going to stack a bunch of copies of this. And now as a parallel to that, you can do
2795320	2799640	many things with these copies. So to answer your question, one thing that I've been starting to
2799640	2804520	experiment with is just using different measures or essentially different A and B matrices for
2804520	2809880	every copy. And that can, and so that sort of has interpretation of using multiple measures.
2811000	2816440	I see. Because when Iman actually talked about the correlations between different dimensions,
2817080	2820920	let's say you have an image, like two different pixels are actually correlated.
2820920	2824600	So I was thinking like, you can have a measure that captures this correlation,
2824600	2828360	but you can have another measure that captures it over time.
2829960	2834360	Another thing actually, since you mentioned that, I don't know if you tried that on image
2834360	2839240	space, I would be curious like if this kind of like long convolution actually makes any difference
2839240	2845160	with the image space. Because image usually like, when we do the image analysis theoretically,
2845160	2849640	when we start thinking about it, it seems that like also the local feature as well as of course
2849640	2855320	the global feature is important. But I don't know, like if we are missing any local features by just
2855320	2863160	using this kind of like long representation. That's a good question. I actually, we have started
2863160	2867880	doing more experiments on images, which I didn't include in this talk, but luckily we do find that
2868520	2875080	the local bias of convolutions does seem pretty good. I don't know, it's hard to quantify if
2875080	2880120	we're missing features, but I think there are settings where we're not, we're only on power
2880120	2887240	or not, or maybe a little bit worse than a standard local CNN. It is hard to say. I will
2887240	2894600	mention though that you can forcibly incorporate locality into this just by changing the measure.
2894600	2898920	For example, if you choose a uniform measure that has a short window, that's the same as saying
2898920	2904280	I would just want a local convolution kernel. Because I would imagine like for this particular
2905240	2910120	thing, like the use case where we have to have to work with a very high resolution image data,
2911000	2914920	you know, for example, like imagine like mammogram, right? Like we have to go with like
2914920	2921160	1000 by 1000 minimum dimension. So for this probably would be useful because they are actually,
2921160	2925960	we want to like do the rescaling, but we cannot because we'll lose probably a lot of features
2925960	2929880	in the middle. But this kind of like long convolution could this, this is a perfect
2929880	2934920	problem that I will actually, I wasn't going to, but now I'll mention this in the experiments as
2934920	2941480	well. Okay. It's actually something that we have thought about basically rescaling of convolutions
2941480	2948200	and using. Right. Okay, I'll get to that. Before that, so I want to get the experiments and
2948200	2952040	basically I just wanted to find, I'm only to find the simple linear one either one D map,
2952040	2956360	but you can just do it in parallel across a lot of features and then plug it into a standard neural
2956360	2963480	network to do sequence modeling. So the first type of data I'll see is a biosignal data.
2966840	2974760	So here is a, there's a real world data set of trying to predict vital signs such as heart rate
2974760	2982440	from raw biosignal data such as I wrote EKG and EG here, but I think it's actually EKG and PPG.
2983000	2986920	And so that's visualized here. And this data is pretty challenging for deep learning models
2986920	2992840	because you can see that it's very long. This is a sequence of like 4000. If you zoom in a lot,
2992840	2995880	it would be pretty smooth actually, but if you zoom out, it displays a lot of
2995880	3001560	periodicity and spikes and other things. And so a lot of methods have been tried on this data set,
3001640	3007320	which include kind of standard machine learning techniques like XGBoost as well as many very
3007320	3013320	modern deep learning sequence models. And S4 substantially improves over all of these in
3014600	3018360	I think cutting the root mean squared error by at least two thirds on all of these targets
3020280	3023240	just with that generic deep learning model that deep model that I showed.
3024680	3029240	Actually I've, these were like older numbers and recently I've been rerunning these again and
3029240	3037480	actually you can drop this down even more. One thing I will note is that attention and
3037480	3043080	transformers does really poorly on this type of data. And that's something that I think I found
3043080	3048600	pretty consistently. So there's some sort of bias toward what type of data you have and S4 is really
3048600	3053240	good at signals and attention is not. Conversely, attention is good at some other types of discrete
3053240	3060680	data that S4 is not as good at. Okay, so that's, that was one experiment. The next one is two time
3060680	3067880	series data where we did a forecasting task where you're given a context window and you want to
3067880	3072120	predict future values. Actually, I'm going to go through this kind of fast because I don't want
3072120	3075880	that much time I want to get through some more of the bio applications and the things that you
3075880	3082360	guys brought up. The models here are very complicated. Whereas for S4, we're actually
3082360	3087480	doing an extremely simple setup, which is just a mask prediction. We're just going to give you,
3087480	3091640	we're going to take the entire sequence and mask out the desired forecast range and then just
3093000	3099080	predict what's in the mask by passing it through this generic deep model. So this is really,
3099080	3104520	it's like a very extremely simple application. I won't unpack the numbers too much, but there's a
3104520	3110920	lot of baselines here, including time series models, LSTMs, lots of transformers, and S4
3110920	3114840	does better than all of them on these real time series data sets, including weather and energy
3114840	3122840	data with much less specialization. These models were all designed for time series and we were
3122840	3128280	just using our generic model. And you didn't even like tune the window size, right?
3129640	3134280	We did not for this one. Actually, by tuning the window size, you can get the numbers down even
3134280	3144760	more. Okay. Okay. The next one here points to Amon's question about rescaling. So it's actually,
3144760	3151800	I'm going to display this to audio. But essentially, I've used audio a few times. I'm running example.
3152520	3158840	It's sampled at extremely high rate. And it's extremely long. So this is a data set of classifying
3158840	3165000	one second speech clips, which were length 16,000, into classifying the words. And most
3165000	3170760	sequence models like transformers and RNNs are really bad here. The only thing that works is CNNs,
3171320	3178760	which the red line is pointing to a speech CNN baseline. And these work okay. But what happens
3178760	3183480	if you are resampling the signal at different frequencies? And this happens commonly in audio
3183480	3188760	because your signal can be sampled at any rate and sound more or less the same. So for example,
3188840	3194120	this orange sequence is a sequence of samples, but it's actually the same underlying signal
3194120	3198360	as the original blue sequence of samples, just at a different frequency. And so it's ideal if
3198360	3204360	the same model works on both of them. But standard models like CNNs cannot do this,
3205160	3211160	essentially because of the local bias that was brought up earlier. I won't unpack this here,
3211160	3216360	but if you use like a standard local CNN, it will break at a different frequency. However,
3216360	3221240	by using a signal model such as S4, which is actually understanding the underlying continuous
3221240	3226440	domain or the underlying continuous function, it can work here without modification. So this is
3226440	3231560	all in a zero shot setting where it's trained at one resolution and tested on a different resolution.
3232440	3239080	And this breaks a CNN, but S4 can do it out of the box. And that's because of this first property
3239080	3246200	of being a continuous time model. And now the last two things I'll show are just calling back to
3246200	3253160	the experiments at very beginning. I showed some audio generation clips. And that was an
3253160	3257560	autoregressive setting where we're generating things one sample at a time. And despite having
3257560	3263320	an extremely large context window, which made it do better and more coherent, we could still
3263320	3267960	sample things autoregressively just as fast as other autoregressive models. And that's because
3267960	3273480	of the fast online or autoregressive representation where you're computing the state and updating it
3273480	3279560	every time. And finally, I showed this benchmark of long range modeling where S4 substantially
3279560	3285480	outperforms other models on a range of different tasks. And this benchmark was also used to
3285480	3291960	benchmark the speed of models during training where S4 is just as fast as all of these
3291960	3296600	efficient transformer variants. And that's because of the efficient, paralyzable view
3297240	3303800	along with the new algorithms we introduced. And so all these properties, as I promised,
3303800	3308600	have concrete empirical benefits. Now for, I'm running out of time, so I just want to get to
3308600	3314520	a couple more things. For the last part, I just wanted to, for this audience, I wanted to point to
3314520	3319640	where I hope that this model will be useful, which is as a general tool for deep learning for
3319640	3326360	biosignals. And I've pointed out one, one example of a data set already where we were predicting
3326360	3332280	like heart rate from EKG signals. But this was another one that CE was working on actually and
3333240	3340120	her and another lab mate have been trying to test S4 here, where this is a data set of raw
3340120	3346360	EEG signals that are difficult to process because they're so noisy and long. And the state-of-the-art
3346360	3353560	models are very recent. CE's model from a couple months ago was there they are on one of these
3353560	3358840	EEG data sets, but it was quite involved and involved a lot of domain knowledge, such as even
3358840	3364040	like the placement of the electrodes and a lot of different parts, components of the model.
3364600	3370440	And so where I hope that S4 could be useful is as a generic tool or building block for
3371320	3380520	for addressing these types of signal data without as much domain expertise and how to design the
3380520	3388520	model. And so CE and Collid have been running some preliminary experiments using S4 on this data,
3388520	3394520	where we don't even need to process the, you don't need to pre-process it with FFT features,
3394520	3399640	you don't need to do a lot of these other things and just run it through these, a generic deep
3399640	3405480	model composed of S4 layers. And Collid found some very preliminary results where it is improving
3405480	3413080	over the baselines in some settings. This is still very preliminary, so it's not, there's other
3413080	3417960	settings that we care about, such as incorporating self-supervision and so on, where it's not quite
3417960	3425960	there, but I do think it has a lot of potential in this type of domain. Another example, actually
3426040	3431080	that was published was another recent collaboration with Stanford Medicine that was submitted to a
3431080	3437960	gastroenterology journal on detecting acid reflux from impotence sensor data. And so again,
3437960	3447480	S4 was really good on that type of prediction task. So that is all I was going to talk about
3447960	3455960	for this. So just to review, S4 is a, it's an SSM, which are these two equations,
3455960	3459560	where we plug in certain formulas and have special algorithms to compute the model.
3460600	3466280	And overall, SSMs and in particular, S4 have a number of very nice properties with
3466280	3470920	concrete empirical benefits, as we saw, and I think can become a very effective building block for
3470920	3476440	modeling many types of sequential data in the future. Thanks for listening and thanks for all
3476440	3483000	the collaborators for the hard work. This slide lists a couple of resources, such as blog posts
3483000	3489560	and related papers, as well as the audio results from an ongoing, a paper that's under submission
3489560	3495800	right now. Feel free to reach out if you have questions and thanks. This was my last slide,
3495800	3500680	but because you want to ask how I will, I guess I'm technically out of time, so of course people
3500680	3505960	feel free to leave, but if you want to stay, I can show one thing about the high resolution images
3505960	3514920	that that was brought up. Let me find that slide. Yeah, if people have conflicts, feel free to leave
3514920	3521160	and we will put up the recording of the talk later in our YouTube channel. Otherwise, if you
3521160	3527160	would like to stay, then yeah, I'll be able to share the slide. So yeah, I'll just really quickly go
3527160	3536600	over this where medical, medical imaging is something that we think could be a potential
3537400	3546680	strong use case for S4 because of this high resolution feature where, so this slide was
3546680	3550600	about, I was moving from a different way, but the point I wanted to make was that
3551320	3555320	Can you go to presentation mode? I think we are still seeing your screen nose.
3556120	3564200	Yeah, sorry. Oops. Am I showing my whole screen? No, we are seeing your screen rather than the
3564200	3573400	presentation. Okay, I thought I had it on. You can just swap the view. I thought I had it on the right
3573960	3582840	view. Is this one still show the, oops. Yeah, I can see the high definition view.
3583880	3588600	Okay, great. So yeah, so the point I was making is that normally image data sets are things like
3588600	3592760	ImageNet, which are actually extremely low resolution compared to other data that we might
3592760	3600280	find such as medical imaging where apparently the images can be up to 100,000 by 100,000 pixels.
3600600	3605400	And this is obviously like way too big for current models, which can only operate on small
3606040	3611000	patches at a time. So I don't know how to address this really, but it's something that fascinates
3611000	3616760	me. But just to point out, this is part of a longer drop pack where I pointed some potential
3616760	3622200	future directions. The one that I'll mention here relates to some things that we brought up,
3622200	3627800	which is that just like the speech experiment that I showed, I believe that S4 should work
3627880	3632920	training on images at different resolutions. And so what you can do is essentially try to
3634120	3640440	train on lower dimensional versions of the image, lower resolution versions, and then transfer
3640440	3644920	the same model to work on high dimensions, which is a very similar thing that I showed
3644920	3653320	for the speech example. So yeah, I think that's potentially something that could work. And the
3653320	3657800	point is that a signal model like S4 will work at different resolutions
3660280	3665480	because you can sample at different rates essentially. And yeah, so what you need is a
3665480	3670200	signal model that understands the continuous domain, just like the example I showed. And that
3670200	3674280	points to this property again. So this is something where we haven't tried it and I don't know if it
3674280	3679320	works, but it's some part of me feels like it might be the right way to or one potential
3679960	3683880	good way to approach this type of problem. But I would think in the opposite way,
3685000	3689160	it's not really we want to generate the high resolution from the low resolution,
3689160	3693080	but I would imagine since you have this kind of like state-based representation and
3693080	3697240	finally you're getting this signal, I would imagine like in some case always like we had
3697240	3701800	to deal with this kind of situation that we had a very high resolution image. And before running
3701800	3707320	through the convolution because of the memory computations, like computational complexity,
3707400	3711560	memory complexity and all this kind of thing. So you have to reskill the image into a much lower
3711560	3719080	dimension. Yeah. And we had a chance of losing a lot of features, specifically histopathology,
3719080	3722680	exactly the example that you showed, or the mammogram, those kind of images, you know.
3723480	3728520	Yeah, I see. So I was thinking that perhaps like what you can do is kind of like iteratively
3729720	3733240	increase the resolution and pick up higher and higher resolution features as you go.
3733800	3738920	But the benefit is that perhaps you can pick up the coarser grain things and then as you
3738920	3744440	upsize the image then and you rescale your kernel essentially, then it's already going to be doing
3744440	3748920	as everybody knows the coarse grain features, but then it as you keep training, it only has
3748920	3755000	to learn the higher frequency features as you go. And again, I have no idea if this is,
3755000	3760040	if this makes sense or it's promising, but it sounds pretty interesting. I think,
3760040	3766680	I think Alba's idea is actually very similar to how pathologists analyze our hostile images.
3766680	3771960	So they usually look at. Define zoom. Define zoom. They usually look at low resolution
3772520	3779320	image first and localize like the potential areas where the tumor are and then they zoom
3779320	3785960	into higher resolution. Right. Yeah. I see. Yeah. And so yeah, I don't know if this will be better
3785960	3790920	than CNN or other things, but it definitely has different and interesting properties.
3793400	3800840	Okay. So that was, yeah, I think that's the end of the material I have. I can say around a few
3800840	3805800	more minutes if people still have questions. All right. Great. Thanks so much, Albert.
3805800	3810120	I have just one question. Sure. Yeah. Thank you for the presentation. That was awesome.
3811000	3816440	Did you find any scenario where it's better to use transformer than S4?
3817240	3822760	Yes. Another great question. So let me just share my screen again. I have one slide prepared for
3822760	3831000	that. Basically at the beginning, I drew this distinction between continuous kind of continuous
3831000	3838920	and discrete data. And I think that S4 will be the best or like the ideas involved are potentially
3839320	3844280	going to be the best thing to do for signals. But for kind of higher level concepts or more
3844280	3849240	discrete concepts such as language or some other things, transformers, I think that's
3849240	3859240	where transformers really shine and are probably going to be better. So here's a,
3860200	3865800	I don't know if this is the right screen again. Sorry. Anyways, here's the one slide on language
3865800	3871080	modeling where we took a transformer, which are currently of course the best models for
3871080	3878040	text and NLP. And we replaced the attention with S4 and found that it doesn't do quite as well.
3878840	3882840	But it is still better than all non, it's significantly better than all non-transformer
3882840	3890200	models. And it also has some other benefits. Like you can do language generation much faster
3890200	3895000	because of the fast recurrent view. That was the main point of this. But this also does
3895000	3899720	kind of point to the fact that personally my intuition is that transformers are really good
3899720	3905160	for dense and discrete data. Whereas S4 is really good for more like noisy and raw data.
3906440	3911720	Yeah. And I mean the speed up here I think is very interesting. Do you know what was the window
3911720	3917320	you would take for language modeling? Like how many tokens or words rather did you consider?
3917320	3922280	Yeah. This experiment was done using a pretty standard length of either 512 or 1024 tokens.
3923160	3927560	You actually can keep increasing the window length for S4, which only slows it down a little
3927560	3932440	bit and actually improves the performance a little bit as well. But I found that out after
3932440	3938520	the fact and I didn't feel like retraining this. Okay. Cool. Thanks. So yeah, but the
3938520	3941960	sort of findings of this slide is the speed up, right? It's massive.
3943080	3948920	That was the point that we did this experiment for. But yeah, so there's a lot of the speed up.
3948920	3952680	In terms of the original question though, in terms of the raw performance of modeling the data,
3953320	3957480	transformers are currently doing a little bit better here. Cool. Thank you.
3963160	3964680	All right. Is there any other questions?
3967960	3975000	Let's all give Albert a round of virtual applause. Thank you for the very comprehensive
3975560	3983080	presentation of state-based models. Thanks for having me. Thank you, Albert.
3983080	3987480	Thank you. Thank you, everyone, for joining us. We will put up the recording of the video,
3987480	3992280	the talk, later to our YouTube channel. And yeah, we'll see you at the same time next week.
3993480	3994840	Thank you. See you, guys.
