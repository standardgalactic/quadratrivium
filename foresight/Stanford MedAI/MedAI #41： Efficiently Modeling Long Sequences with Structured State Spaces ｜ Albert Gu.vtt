WEBVTT

00:00.000 --> 00:08.080
Okay, hi everyone, welcome to our 41st session of the May AI Group Exchange.

00:08.080 --> 00:13.320
This week we have Elba Gu from Stanford here with us to present his research on efficiently

00:13.320 --> 00:17.280
modeling long sequences with structured state spaces.

00:17.280 --> 00:21.200
Elba is a final year PhD candidate in the computer science department here at Stanford

00:21.200 --> 00:24.080
University, advised by Chris Ray.

00:24.080 --> 00:28.920
He's brought the interest in studying structured representations for advanced signal capabilities

00:28.920 --> 00:34.400
of machine learning and deep learning models with focuses on structured linear algebra,

00:34.400 --> 00:38.480
non-euclidean representations, and theory of sequence models.

00:38.480 --> 00:41.040
Thank you so much Elba for joining us today.

00:41.040 --> 00:45.440
Before we start, do you have any preference on how you want to take questions?

00:45.440 --> 00:49.560
Yeah, thank you, thank you for an introduction.

00:49.560 --> 00:56.560
For this talk, I think I'm not sure usually the level of formality, but I'm very happy

00:56.560 --> 01:03.560
to have the casual in terms of the conversation and the questions.

01:03.560 --> 01:07.320
I think there's some time, it's not going to be a full hour talk, so I'm more than happy

01:07.320 --> 01:11.120
to take questions during it and I'll watch the time in case it gets too long.

01:11.120 --> 01:17.320
And then I'll also pause a few times to pause for potential questions during some sections.

01:17.320 --> 01:19.520
Okay, sounds good.

01:19.520 --> 01:23.120
Let's try to make this session as interactive as possible.

01:23.120 --> 01:26.840
How further or do let me hand it over to Albert?

01:26.840 --> 01:28.840
Thank you.

01:28.840 --> 01:35.640
All right, so this talk will be about a new sequence model called S4, or structured state

01:35.640 --> 01:36.640
spaces.

01:36.640 --> 01:45.320
Now, for the purposes of this talk, when I mentioned sequence models, we will think of

01:45.320 --> 01:51.280
them as a black box sequence-to-sequence map composed of primitive layers, where each

01:51.280 --> 01:59.680
layer simply takes an input sequence and returns a sequence of the same shape.

01:59.680 --> 02:04.680
For our purposes right now, we'll think of them as just being a one-dimensional to one-dimensional

02:04.680 --> 02:12.360
map, but this can be easily converted to higher-dimensional features.

02:12.360 --> 02:15.600
Many sequence models have been developed that satisfy this interface, particularly in the

02:15.600 --> 02:17.520
context of deep learning.

02:17.520 --> 02:20.800
These include many classical deep learning models, such as recurrent neural networks

02:20.800 --> 02:25.320
or RNNs and convolutional neural networks or CNNs, as well as many more modern models,

02:25.320 --> 02:30.360
such as transformers or neural ODEs.

02:30.360 --> 02:33.440
And all of these models kind of satisfy the same interface.

02:33.440 --> 02:37.600
They map a sequence to a sequence of the same shape, or meaning the same length and

02:37.600 --> 02:38.800
field dimension.

02:38.800 --> 02:44.760
And then you can incorporate any of these into a deep learning model fairly easily just

02:44.760 --> 02:53.080
by using standard architectures, where you can include normalization layers, other linear

02:53.080 --> 02:57.920
or nonlinear activations, as well as with the dual connections.

02:57.920 --> 03:03.040
And so the core component of all of this is the core sequence model, and that's what

03:03.040 --> 03:06.200
we'll focus on.

03:06.200 --> 03:11.280
And this generic deep neural architecture based on sequence models can be used to solve

03:11.280 --> 03:18.280
many types of problems with many types of sequence data, from medallies such as text

03:18.280 --> 03:26.680
and audio to images and videos to general time series data or biosignals, for example,

03:26.680 --> 03:31.080
which is depicted here.

03:31.080 --> 03:34.120
In this talk, I'm going to draw a very rough distinction between different types of sequence

03:34.120 --> 03:35.120
data.

03:35.120 --> 03:39.560
Now much of modern sequence modeling in the context of machine learning focuses on data

03:39.560 --> 03:41.440
such as text.

03:41.440 --> 03:45.120
And very roughly I'll classify this as being a discrete sequence because the input comes

03:45.120 --> 03:48.560
in the form of discrete tokens.

03:48.560 --> 03:53.240
And other types of data like this includes things like graphs or things like DNA-based

03:53.240 --> 03:54.240
pairs.

03:54.240 --> 04:00.480
In contrast, what this talk will focus on is data that's roughly more continuous, things

04:00.480 --> 04:04.600
such as video or time series or audio.

04:04.600 --> 04:08.520
And what's common to all of these is that there's an underlying notion of time from

04:08.520 --> 04:12.920
which the sort of data is sampled from.

04:12.920 --> 04:17.960
And so I'm going to very broadly call this type of data signal data as opposed to sequence

04:17.960 --> 04:19.080
data.

04:19.080 --> 04:23.000
And roughly speaking, signals can be defined as data that's generated from an underlying

04:23.000 --> 04:29.200
continuous physical process, including all these examples here.

04:29.200 --> 04:31.320
This talk will be composed of two parts.

04:31.320 --> 04:36.480
The first part covers a method called HIPPO, which was the predecessor to S4.

04:36.480 --> 04:42.000
And it's a new conceptual framework for the online memorization of signals and led to

04:42.000 --> 04:45.320
a new method for modeling signals and sequences.

04:45.320 --> 04:49.560
And then the second part will be S4, which built right on top of HIPPO.

04:49.560 --> 04:53.120
And it has a lot of important properties that have been very effective for addressing some

04:53.120 --> 04:55.240
types of sequence modeling problems.

04:55.240 --> 04:59.240
And before I get into the technical stuff, I'll give a quick preview experimental results

04:59.240 --> 05:04.120
to highlight the types of improvements I will see and what it's good at.

05:04.120 --> 05:07.080
And this will kind of illustrate the types of challenges that we'll hope to address with

05:07.080 --> 05:10.200
these new models.

05:10.200 --> 05:14.520
The first challenge overall is just going to be to signal or general temporal data that

05:14.520 --> 05:15.600
I just defined.

05:15.600 --> 05:17.000
And this data is really everywhere.

05:17.000 --> 05:23.760
So some examples include audio waveforms, spatial temporal data like videos, biosignals

05:23.760 --> 05:30.840
like electrocardiograms, which have important applications of medicine, or market and financial

05:30.840 --> 05:32.200
data.

05:32.280 --> 05:35.520
And then there will be multiple time series logs being generated by every major industry

05:35.520 --> 05:39.040
and many other types of scientific modeling problems.

05:39.040 --> 05:44.360
And we'll return to these experiments later with a particular focus on some biosignal

05:44.360 --> 05:45.360
data.

05:45.360 --> 05:51.080
But for now, I will just use one example to illustrate, which is audio.

05:51.080 --> 05:55.320
And audio is actually one of the most common types of data because it's just raw sound.

05:55.320 --> 05:56.320
It's everywhere.

05:56.320 --> 06:01.200
And so to illustrate, machine learning right now is really all about text and so many

06:01.200 --> 06:05.240
headline results recently have been about people scraping together all the raw text

06:05.240 --> 06:08.040
data they can get, creating massive models on them.

06:08.040 --> 06:13.000
And that's led to very impressive results like GPT-3, which I don't know the audience,

06:13.000 --> 06:17.400
but hopefully many of you have heard of this model.

06:17.400 --> 06:20.960
In contrast, audio actually has orders of magnitude more data than text.

06:20.960 --> 06:24.880
For example, a single labeled dataset has more data set than all of the data used to

06:24.880 --> 06:27.080
train those massive language models.

06:27.080 --> 06:29.600
But you don't hear about benchmarks in this domain nearly as much.

06:29.800 --> 06:35.280
And I think part of the reason is just because audio models are, audio is very challenging

06:35.280 --> 06:39.640
and current models seem much worse in comparison to text.

06:39.640 --> 06:47.680
And so here's a concrete example where we consider basically a very general and hard

06:47.680 --> 06:54.400
audio generation setting of generating spoken digits, zero to nine, using a completely

06:54.400 --> 06:56.640
unconditional autoregressive model.

06:56.640 --> 07:00.120
And the gold standard here is a baseline called WaveNet.

07:00.120 --> 07:03.760
And here's what it sounds like trying to say these numbers.

07:11.760 --> 07:14.360
So it's, it's not very good.

07:14.360 --> 07:18.320
And here's results for S4, which was just, these results are just from the past like

07:18.320 --> 07:20.320
two months or so ago.

07:21.320 --> 07:27.800
One, two, three, four.

07:27.800 --> 07:31.600
So that's, that's a pretty concrete example.

07:31.600 --> 07:35.440
And so in this talk, we'll see how models like S4 are kind of designed for signals in

07:35.440 --> 07:39.200
a way and can have significant advantages for this type of data.

07:39.200 --> 07:44.920
And the second example up front, or the second example of a running challenge will be, can

07:44.920 --> 07:47.480
be motivated by examining audio more closely.

07:47.480 --> 07:51.320
And one reason why audio is so hard is because it's sampled at such an extremely high rate

07:51.320 --> 07:55.400
where a single second has 16,000 or more samples.

07:55.400 --> 08:00.800
In contrast, most sequence models can't deal with more than a thousand or so.

08:00.800 --> 08:04.320
And to illustrate, there was a benchmark in the past year called Long Range Arena that

08:04.320 --> 08:07.760
measured the performance of models on a suite of long range tasks.

08:07.760 --> 08:13.000
And the most popular sequence models these days, Transformers, were the main focus.

08:13.000 --> 08:17.200
But despite their many successes, they don't do so well on long context.

08:17.200 --> 08:21.440
And so there were dozens of variants that were tried, and they all get to around the

08:21.440 --> 08:26.920
same performance, which is actually not much above random guessing.

08:26.920 --> 08:32.480
In contrast, S4 we'll see is explicitly designed to be effective on long context, which leads

08:32.480 --> 08:34.960
to a huge improvement on this benchmark.

08:34.960 --> 08:39.360
And it's the first model to ever make progress on some really difficult long sequence tasks.

08:39.360 --> 08:43.320
Can I ask a quick question here, Albert?

08:43.320 --> 08:44.320
Yeah.

08:44.760 --> 08:49.360
So in the previous task, that was a generative process.

08:49.360 --> 08:56.680
And in this, the Long Context Channel challenge, is it a classification or what kind of task

08:56.680 --> 08:57.680
is it?

08:57.680 --> 08:59.080
These are all classification problems.

08:59.080 --> 09:05.200
And they're on data such that includes several data modalities, such as text, images, some

09:05.200 --> 09:08.440
sort of symbolic processing, stuff like that.

09:08.440 --> 09:09.440
I see.

09:09.440 --> 09:13.440
And so you can use S4 both as a generative model to actually generate sequences.

09:13.680 --> 09:14.680
Yeah.

09:14.680 --> 09:20.640
So a lot of sequence models, again, a sequence model I'm defining as a black box interface,

09:20.640 --> 09:23.280
really, that's just a sequence-to-sequence map.

09:23.280 --> 09:28.640
And many of these can be used in many ways, both for classification and generation.

09:28.640 --> 09:34.320
For example, transformers or RNNs are similar things that satisfy the same interface and

09:34.320 --> 09:36.280
can be used in many ways as well.

09:36.280 --> 09:37.280
Gotcha.

09:37.280 --> 09:38.280
Thank you.

09:39.960 --> 09:40.960
Yeah.

09:41.080 --> 09:43.160
OK, so now I'll get into the technical portions.

09:43.160 --> 09:46.360
And the first part will be about Hippo, as I mentioned.

09:46.360 --> 09:52.960
And to motivate what Hippo's goal was, I gave a bunch of examples of data that machine

09:52.960 --> 09:56.640
learning models currently struggle with, particularly things like time series.

09:56.640 --> 10:00.600
And to highlight why this is hard, I'm going to use a running example to illustrate a very

10:00.600 --> 10:06.000
basic capability that's difficult for modern models.

10:06.000 --> 10:09.400
And that's the moving average, which is perhaps the most basic method in modern time series

10:09.400 --> 10:10.640
analysis.

10:10.640 --> 10:15.120
So this figure depicts the exponential moving average or EMA, which is the blue line.

10:15.120 --> 10:20.360
And the way it's used is that it's a fixed non-learnable feature that's often the first

10:20.360 --> 10:26.080
pre-processing step that's performed in any sort of time series analysis pipeline.

10:26.080 --> 10:30.960
Now in the context, in the spirit of machine learning and deep learning, instead of doing

10:30.960 --> 10:36.680
manual processing like creating these features, we really would like to be able to learn these

10:36.680 --> 10:41.120
sort of things automatically from the data.

10:41.120 --> 10:46.600
And so in particular, here's a very simple concrete task is, suppose you have a model

10:46.600 --> 10:54.920
and you're feeding it this black input signal, and you want the model to predict the EMA

10:54.920 --> 10:57.320
or the blue signal as the output.

10:57.320 --> 11:02.760
And fortunately, it turns out that standard sequence models, such as attention and convolutions,

11:02.760 --> 11:05.680
cannot do this at all.

11:05.680 --> 11:10.920
And the reason why is essentially because the EMA has unbounded context.

11:10.920 --> 11:14.600
It's actually just a weighted average of the history of the signal with an exponentially

11:14.600 --> 11:16.440
decaying weight that searches back infinitely.

11:16.440 --> 11:21.520
Whereas in contrast, most modern models such as attention or a convolutions have finite

11:21.520 --> 11:25.000
context in those.

11:25.000 --> 11:27.600
Some people wonder about other things like RNNs.

11:27.600 --> 11:32.800
And the short answer is that RNNs are better than attention convolutions here, but they

11:32.800 --> 11:40.320
still aren't that good due to empirical problems with optimization and other things.

11:40.320 --> 11:48.440
So we'll see that the methods that are introduced in this talk will be very naturally suited

11:48.440 --> 11:51.160
for this and are much stronger versions.

11:51.160 --> 11:55.200
But going back to the EMA, the way that one way to think about it is that it's a very

11:55.200 --> 11:58.720
simple summary of the entire history of your signal.

11:59.160 --> 12:05.600
In other words, it's a state X, which is a single number that summarizes the entire

12:05.600 --> 12:08.720
history of the input U.

12:08.720 --> 12:12.800
And the reason why it's useful is that it's easy to compute because if you get new data,

12:12.800 --> 12:17.360
you can update the EMA in constant time using this weighted average.

12:17.360 --> 12:21.280
And beyond the simple example, though, I think these two properties are actually conceptually

12:21.280 --> 12:23.160
really important.

12:23.160 --> 12:26.520
For example, they're exactly the properties that you need in any sort of real-time decision-making

12:26.520 --> 12:27.520
problem.

12:27.520 --> 12:31.200
And really abstractly, you can even imagine that your brain is a state that's summarizing

12:31.200 --> 12:35.640
the entire context of your life, and it's constantly updating as you acquire new information.

12:35.640 --> 12:40.080
So I think that's actually a pretty general important question, and this was a direct

12:40.080 --> 12:42.840
inspiration for HIPPO.

12:42.840 --> 12:47.280
In the context of machine learning, this question has a lot of direct impact on our models because,

12:47.280 --> 12:49.440
as I mentioned, they struggle with long context.

12:49.440 --> 12:54.280
For example, text models, it's been shown typically have a context range of about 100

12:54.280 --> 12:56.440
to at most a few thousand tokens.

12:56.440 --> 13:02.480
Whereas if you want to deal with data such as speech and audio, a single word in speech

13:02.480 --> 13:07.840
is a sequence of length more than 10,000, and this can really stretch to unbounded length.

13:07.840 --> 13:13.800
And so this is the question that I was trying to, that I was thinking about.

13:13.800 --> 13:18.880
And what I did was I tried to convert this vague goal of long-range memory into a more

13:18.880 --> 13:21.240
formal mathematical question.

13:21.240 --> 13:26.560
The conceptual idea is that if you can compress the past into a smaller state that's accurately

13:26.560 --> 13:32.680
remembering it, then you should be able to reconstruct the past.

13:32.680 --> 13:35.520
And we can then attempt to turn this into a technical problem.

13:35.520 --> 13:39.440
So the idea is that we're going to observe an input signal online and try to maintain

13:39.440 --> 13:43.800
a good representation of it that allows us to reconstruct it.

13:43.800 --> 13:48.000
And so, okay, so first in this section, I'm going to formalize this idea, and then I'll

13:48.000 --> 13:50.080
define HIPPO and visualize it.

13:50.080 --> 13:53.960
And then talk about a couple of generalizations.

13:53.960 --> 13:57.120
So the first thing is that let me formalize this idea that I just mentioned.

13:57.120 --> 14:02.960
And so the idea of HIPPO is that, again, we're trying to observe an input signal online,

14:02.960 --> 14:07.360
and we're going to try to encode it as well as possible given a memory budget.

14:07.360 --> 14:10.640
So concretely, you'd think of it like this.

14:10.640 --> 14:15.520
So suppose at some initial time, T0, we've seen part of the input, and we're going to

14:15.520 --> 14:16.880
try to compress this input.

14:16.880 --> 14:20.720
So what you can do is store the best approximation to what we've seen so far.

14:20.720 --> 14:25.880
For example, we can create the best polynomial approximation and write down the coefficients

14:25.880 --> 14:27.560
of that polynomial.

14:27.560 --> 14:33.120
So now the degree of the polynomial or the number of coefficients is the memory budget.

14:33.120 --> 14:35.400
And we want to do this continuously at all times.

14:35.400 --> 14:42.480
So as we keep seeing more data at some later time T1, we'll have to update our best approximation

14:42.480 --> 14:45.040
and write down the new coefficients.

14:45.040 --> 14:51.600
Now the central question is, first of all, how do you actually find these optimal approximations?

14:51.600 --> 14:58.680
And moreover, how can you update this representation efficiently as you keep seeing more information?

14:58.680 --> 15:04.360
And so this is the main conceptual idea, and it's a little bit of work to formalize a little

15:04.360 --> 15:05.360
more.

15:05.360 --> 15:08.680
And in particular, I've been talking about optimal approximations, but that's actually

15:08.680 --> 15:10.080
not well-defined.

15:10.080 --> 15:14.960
And so what we'll need is to find a measure that specifies the quality of approximation.

15:14.960 --> 15:18.880
For example, we can choose the exponentially decaying measure, which says that we care

15:18.880 --> 15:25.400
about approximating the recent pass of the input more than the far pass.

15:25.400 --> 15:29.120
And this will relate back to the EMA.

15:29.120 --> 15:34.160
But given this, the problem is more or less well-defined.

15:34.160 --> 15:38.600
So basically, we have to pick the measure sort of as a hyperparameter or a prior for

15:38.600 --> 15:39.600
now.

15:39.600 --> 15:42.120
Let's talk about how you can actually learn it.

15:42.600 --> 15:47.200
But for now, we need to pick a measure up front, say the exponential decaying measure.

15:47.200 --> 15:51.040
And then you need to choose a polynomial basis.

15:51.040 --> 15:55.120
And then the problem's completely defined, and you can write down the coefficients in

15:55.120 --> 15:59.000
closed form, and you can figure out how they evolve through time.

15:59.000 --> 16:05.880
So I'm going to skip the details of the derivation, but you end up with a closed form method.

16:05.880 --> 16:10.160
And what I want to emphasize is that the derivation has some technically interesting

16:10.160 --> 16:11.360
new ideas.

16:11.360 --> 16:14.640
But the most interesting and important part of this, I think, is just this simple conceptual

16:14.640 --> 16:19.480
idea of the online compression and reconstruction and how to formalize that mathematically.

16:19.480 --> 16:21.920
So that's the main point.

16:21.920 --> 16:28.320
OK, and now with the definitions out of the way, things will become a lot more clear with

16:28.320 --> 16:31.280
some visualizations of what it does.

16:31.280 --> 16:36.280
So first of all, let me just be really formal about defining what HIPAA is.

16:36.280 --> 16:39.080
So I mentioned the problem was that we are encoding.

16:39.080 --> 16:43.280
So XFT is going to represent a vector of our coefficients at all times.

16:43.280 --> 16:48.240
And the question is, how does this evolve through time as we see more data in the input

16:48.240 --> 16:49.240
U?

16:49.240 --> 16:51.600
And it turns out that it just satisfies a simple differential equation.

16:51.600 --> 16:55.040
By going through the derivation, you can write down this differential equation in closed

16:55.040 --> 17:01.320
form and write down closed form formulas for this transition matrix involved here.

17:01.320 --> 17:07.880
So to be concrete, the ODE is called the HIPAA operator.

17:07.880 --> 17:11.840
And the matrices, the matrix in the operator are called HIPAA matrices, which have closed

17:11.840 --> 17:12.840
form formulas.

17:12.840 --> 17:17.320
In fact, the actual matrix is this matrix.

17:17.320 --> 17:23.400
It's an extremely simple matrix, which is a special type of structure matrix.

17:23.400 --> 17:25.440
And yeah, so it's just a simple formula.

17:25.440 --> 17:28.800
And then we write down a closed form formula for this differential equation.

17:28.800 --> 17:31.920
And that's how our coefficients evolve over time.

17:31.920 --> 17:38.960
And now, right, so this equation, again, is called the HIPAA operator or the high order

17:38.960 --> 17:43.160
polynomial projection operator, because we're projecting on the high degree polynomial basis

17:43.160 --> 17:44.160
functions.

17:44.160 --> 17:48.600
Now visually, the way to think about it is like this.

17:48.600 --> 17:52.400
The reason I call an operator is because it maps a function to a function.

17:52.400 --> 17:57.880
So it's an operator that maps this black input signal U to these sets of coefficients X

17:57.880 --> 18:05.120
and blue, where every time X of t compresses the history of the input signal U.

18:05.120 --> 18:11.160
And you can compute X online as you see one input at a time.

18:11.160 --> 18:13.520
So the black line represents our current time step.

18:13.520 --> 18:15.880
We're gradually seeing more of the input.

18:15.880 --> 18:20.840
And we are updating our coefficient vector, which is depicted in blue.

18:20.840 --> 18:26.040
Here I've, this is visualizing just the lowest order for coefficients of the best polynomial

18:26.480 --> 18:29.720
approximation.

18:29.720 --> 18:33.360
And now here is what the reconstruction looks like.

18:33.360 --> 18:39.760
So as I move along through time and update my coefficients, the coefficients that that

18:39.760 --> 18:44.480
polynomial defines, in a sense, is actually just this red line.

18:44.480 --> 18:49.320
So it is reconstructing the input just like we wanted.

18:49.320 --> 18:53.600
Note that we are using only, so here I've only visualized four coefficients, but I'm

18:53.600 --> 18:57.800
actually using 64 coefficients, but the whole function was linked to 10,000.

18:57.800 --> 19:02.320
So I'm compressing it a lot.

19:02.320 --> 19:08.760
And this, here's a static image that kind of illustrates the effect of the reconstruction.

19:08.760 --> 19:13.720
So because I'm compressing it, I can't perfectly reconstruct the input.

19:13.720 --> 19:15.840
And so how good is the reconstruction then?

19:15.840 --> 19:17.480
Well, it depends on the measure.

19:17.480 --> 19:21.920
So the green, the green line in this figure was the exponentially decaying measure that

19:21.920 --> 19:24.880
we are basically projecting onto.

19:24.880 --> 19:31.760
And so intuitively, you can see that the red reconstruction line is really accurate for

19:31.760 --> 19:36.400
the recent past and degrades further out in history, but still maintains some rough information

19:36.400 --> 19:39.600
about the whole signal.

19:39.600 --> 19:44.080
And so that is, that's hippo.

19:44.080 --> 19:48.760
Now, oh, one, oh, okay, a question here.

19:49.120 --> 19:50.280
No, you can continue.

19:50.280 --> 19:54.320
I just had one clarification, but I can ask after you finish.

19:54.320 --> 19:55.320
Here's fine too.

19:55.320 --> 19:56.320
Okay.

19:56.320 --> 20:01.840
So is it fair to think about X as being the state at each time point?

20:01.840 --> 20:08.560
And then essentially the red line is trying to reconstruct the signal given the current

20:08.560 --> 20:11.920
state or do you also use all the past states to reconstruct?

20:11.920 --> 20:13.280
That's exactly right.

20:13.280 --> 20:18.160
So yeah, so the reconstruction is happening using only the coefficient vector at the current

20:18.160 --> 20:19.320
black line.

20:19.320 --> 20:23.640
So every single time I'm using, I'm for, yeah, the blue line, I'm visualizing the whole

20:23.640 --> 20:27.280
thing, but at any given point in time, I'm remembering only the current vector, which

20:27.280 --> 20:29.880
has length 64.

20:29.880 --> 20:33.400
Here I'm only visualizing four of the components, but it has length 64.

20:33.400 --> 20:37.400
And using those 64 numbers, I'm reconstructing what I've seen so far in red.

20:37.400 --> 20:38.400
Oh, awesome.

20:38.400 --> 20:39.400
Thanks.

20:39.400 --> 20:40.400
Right.

20:40.400 --> 20:46.760
Now, in that previous figure, if I just take one of the blue lines, actually the lowest

20:46.760 --> 20:50.900
order coefficient and overlay over the function, you can see that it actually turns out to

20:50.900 --> 20:52.400
exactly be the EMA.

20:52.400 --> 21:00.200
And so it turns out that moving averages can be viewed as order zero or low order projections.

21:00.200 --> 21:03.360
On the other hand, hippo is essentially a very strong generalization of this that solves

21:03.360 --> 21:11.360
a natural mathematical question and gets back things like the EMA for free.

21:11.360 --> 21:16.520
So that's what hippo is, and now I'll just talk a little bit about some extensions of

21:16.520 --> 21:17.520
it.

21:17.520 --> 21:21.760
So first of all, a natural question that may be wondering is that I've been using this

21:21.760 --> 21:24.680
example of an exponential measure, but what about other cases?

21:24.680 --> 21:28.000
Well, it turns out that hippo can be derived for any measure.

21:28.000 --> 21:32.640
For example, here's a case that is pretty natural as well, which is what if I want to

21:32.640 --> 21:34.160
reconstruct along a uniform measure?

21:34.160 --> 21:41.320
In other words, I only care about remembering the recent past in sliding windows of my function.

21:41.400 --> 21:47.880
And this is possible, so you would get a different ODE, and here's a reconstruction in effect.

21:47.880 --> 21:53.280
So again, using just 64 numbers in memory, I'm trying to reconstruct the last 2,000 time

21:53.280 --> 22:00.080
steps of this function uniformly, and it's doing this quite accurately.

22:00.080 --> 22:04.080
Now you can generalize it even further to, for example, when the measure is changing over

22:04.080 --> 22:08.760
time instead of just sliding along, and so there's a very general framework here that

22:08.760 --> 22:11.600
can do lots of things.

22:11.600 --> 22:14.680
A lot of this was in follow-up work to their general hippo paper, and what we showed was

22:14.680 --> 22:23.080
that for essentially any measure, there exists a corresponding hippo operator where the hippo

22:23.080 --> 22:28.640
matrices A and B depend on the measure, and you can write them down in closed form.

22:28.640 --> 22:32.480
And this is important, I think, because it draws an equivalence between measures and

22:32.640 --> 22:39.520
these ODE's, where this means that we don't, I mentioned earlier that we had to choose the

22:39.520 --> 22:44.480
measure up front as a prior, such as the accidentally decaying case, but actually just by learning

22:44.480 --> 22:51.360
these matrices A and B, it's in some sense the same as learning the measure.

22:51.360 --> 22:57.880
Okay, so now even better, not only do these operators always exist, but it turns out that

22:57.880 --> 22:59.400
the matrices are always structured.

22:59.800 --> 23:05.160
Previously, we saw, for the accidentally decaying case, the matrix was actually extremely simple.

23:06.040 --> 23:09.880
In general, they're going to be more complicated than that, and it's, they satisfy a structure,

23:09.880 --> 23:14.120
which was something that I introduced in much earlier work, but they are all structured in

23:14.120 --> 23:20.280
some way, and that means that you can, how do you actually calculate these updates through time?

23:20.280 --> 23:24.280
You can actually update the state or the coefficients in nearly optimal time.

23:24.760 --> 23:32.680
Okay, so that was the main takeaways from HIPPO, and so just to recap, we were inspired by these

23:32.680 --> 23:37.560
very basic, these simple but important properties of trying to maintain a state that's summarizing

23:37.560 --> 23:43.000
the entire context, and we formalized this into a mathematical problem, which was pretty

23:43.000 --> 23:47.160
intuitive, and we were then able to solve analytically, and this resulted in a nice

23:47.160 --> 23:49.640
class of methods for addressing long context and signals.

23:49.880 --> 23:51.880
Okay, so I see a question in the chat.

23:54.920 --> 23:59.560
So, can these operators be expressed in terms of Z-transforms? I'm not quite sure what you mean here.

24:01.480 --> 24:04.760
To my understanding, Z-transforms are like the discrete version of a Laplace transfer,

24:04.760 --> 24:08.520
and I'm not sure if that's the one you're referring to, or another notion.

24:11.000 --> 24:14.840
Yes, that's what I was thinking about. It seems like as though

24:15.320 --> 24:22.120
just as you can express like exponentially, exponential decay in terms of Z-transforms

24:22.120 --> 24:27.320
of the functions, that there seems like it's likely to be a link.

24:29.320 --> 24:32.760
Yeah, I mean, I think all these things have a tight link, and they're connected to each other.

24:32.760 --> 24:38.200
It turns out actually that the way that in the next part, when I talk about S4,

24:38.200 --> 24:41.640
there's going to be some difficult computations, and I'm not sure if that's the one you're

24:41.640 --> 24:44.280
talking about. In the next part, when I talk about S4, there's going to be some difficult

24:44.280 --> 24:52.760
computational issues for computing certain things that I'll introduce, and to actually compute them,

24:52.760 --> 24:56.760
I essentially actually go through Laplace space or frequency space. So, essentially,

24:56.760 --> 25:02.520
I actually take the Z-transform of this equation and calculate that transform at several values,

25:02.520 --> 25:08.040
and then invert it to get the hippo matrices back, or to get a certain thing back.

25:08.040 --> 25:17.880
Sounds good. That makes sense. Yeah, great question. And yeah, so I wanted also just

25:17.880 --> 25:21.240
to stop around here at the summary for any other questions.

25:25.880 --> 25:30.200
And if there's none, that's great because usually this is a pretty complicated

25:31.960 --> 25:36.040
framework mathematically, but hopefully the visualizations help explain it a lot.

25:38.120 --> 25:47.160
Okay, so I'll move on to the next part where, so one thing I didn't include in this section

25:47.160 --> 25:51.560
was any experiments. So the way we evaluated this is kind of just like how good is the

25:51.560 --> 25:56.680
reconstruction, and actually using this method in machine learning models did pretty well,

25:56.680 --> 26:00.840
just naively, but where it became really effective was when incorporated into a model

26:00.840 --> 26:07.160
in a particular way, and so that's what S4 will be. And so to, first I'm just going to define S4,

26:08.200 --> 26:11.400
and I'm going to define it through hippo, which was the original motivation,

26:12.920 --> 26:16.280
and the motivation here is going to be very simple. So to refresh your memory, this is what

26:16.280 --> 26:20.680
hippo does, it maps an input signal, which in our case they're thinking of as 1D,

26:20.680 --> 26:25.160
to a higher dimensional signal. Now the problem is that we've blown up the dimension of the input

26:25.160 --> 26:32.360
from one dimension to n dimensions, where n was our memory budget or the number of coefficients,

26:32.360 --> 26:42.120
and typically this is going to be at least 100 or so. So the motivation for, so I work in deep

26:42.120 --> 26:47.640
learning, and I just wanted to incorporate hippo into a deep learning model, but this is a problem

26:47.640 --> 26:51.560
because you can't just stack layers of this because you just keep increasing the dimension.

26:52.120 --> 26:57.400
And so a very simple motivation to fix this is just, let's just decrease the dimension again.

26:58.360 --> 27:03.640
And the way to do this is that you can just take a very simple linear projection. So

27:05.960 --> 27:09.400
what we'll do is that we have a state x, which was like a 100 dimensional vector,

27:09.400 --> 27:13.960
and we'll just hit it with a dot product that can be learnable to get back a single number,

27:13.960 --> 27:18.280
which is essentially taking a linear combination of the blue lines to get the final output,

27:18.280 --> 27:23.000
which is the red line. And then we'll add a multiple of the original input, which can be seen as a

27:23.080 --> 27:29.160
skip connection. And that is the entire definition of S4. It's finally these two equations where

27:29.160 --> 27:34.520
the first one is the hippo equation, which takes the input to a state that's kind of memorizing it.

27:34.520 --> 27:40.360
And then the second equation just combines the state linearly into a single output.

27:41.880 --> 27:45.640
Now, for those of you with a background engineering, this definition may look really familiar.

27:46.920 --> 27:51.000
And this is because this is a well-known model called a state space model or SSM,

27:51.080 --> 27:54.520
which is sometimes depicted with this simple control diagram. And they've been around for

27:54.520 --> 28:00.840
decades, such as the famous common filter and use in many scientific fields. I think outside of

28:00.840 --> 28:06.760
controls and statistics, they're also pretty commonly used in perhaps computational neuroscience

28:06.760 --> 28:13.160
and many medical problems as well. Now, what the theme of this part will be is that

28:14.840 --> 28:17.640
we'll see that SSMs are a really elegant and natural model,

28:18.200 --> 28:22.760
but they haven't been used in deep learning before in this way. And for underlying reasons

28:22.760 --> 28:28.200
that we'll see in that S4 address. But for now, just to define S4 in terms of this model,

28:29.080 --> 28:35.080
the way that we'll define it is that it's just an instantiation of an SSM, these two equations,

28:35.960 --> 28:43.960
where we'll plug in specific values of matrices in. And although it turns out that

28:43.960 --> 28:47.800
although this model is simple to define, actually computing with it turns out to be

28:47.800 --> 28:57.880
difficult and will require new ideas and algorithms. And so my goal of this in this section is to

28:57.880 --> 29:05.080
convince you that this is a really elegant and fundamental model. And so first of all, I will

29:05.080 --> 29:10.280
talk about some general properties of SSMs that would have a lot of benefits in machine learning

29:10.280 --> 29:19.000
and deep learning that are independent of S4. And then I'll show how those come with associated

29:19.000 --> 29:23.640
trade-offs that prevent them from being really good in deep learning. And S4 will solve those

29:23.640 --> 29:29.320
problems. And finally, I'll show several real world experiments that show S4's effectiveness

29:29.320 --> 29:37.560
in a bunch of settings. So in this first part, I'm actually going to describe three different

29:37.560 --> 29:41.720
ways to think about SSMs, which give them a lot of nice properties. And this was theory developed

29:41.720 --> 29:47.240
in the predecessor work to S4 that and will have empirical concrete empirical benefits.

29:47.960 --> 29:53.480
And so the first way to the first property is that SSMs inherently operate on continuous time

29:53.480 --> 30:00.040
signals instead of discrete time sequences. So here's how to think about it. So in machine learning,

30:00.040 --> 30:03.320
we usually work with sequence models, which I defined as a parameterized map

30:03.880 --> 30:06.280
from an input sequence to an output sequence.

30:09.720 --> 30:15.640
What if instead of mapping a sequence to a sequence, I coined this term signal model

30:15.640 --> 30:20.120
to denote a parameterized map that maps a function to a function or a signal to a signal.

30:21.480 --> 30:27.400
And given one of these maps, you can essentially discretize the inputs and outputs however you

30:27.400 --> 30:33.880
want to get back a sequence. So essentially, the upshot is that signal models are in some sense

30:33.880 --> 30:38.760
a generalization of sequence models, where they actually map functions and functions,

30:38.760 --> 30:44.760
but by discretizing them, you get back a sequence model. And so the first way to think about SSMs

30:44.760 --> 30:49.960
is that they are just a simple parameterized signal model, where the parameters were matrices A, B,

30:49.960 --> 30:56.600
C, and D, and they map an input function to an output function. That's it. Just in terms of the

30:56.680 --> 31:01.640
interface or the API of the model, that this is what it does. The reason that this property is

31:01.640 --> 31:07.240
important is because even when we're working in discrete time, the model in some sense understands

31:07.240 --> 31:13.080
the underlying continuous domain. So I will show what I mean concretely by this later empirically.

31:15.800 --> 31:20.040
All right, so that's the first representation. The next perspective relates back to the original

31:20.040 --> 31:25.160
motivation of HIPAA, which was about online computation. So how do we actually compute

31:25.160 --> 31:30.520
the output of this SSM? One way to do it is to process the input one at a time, just like HIPAA did

31:31.240 --> 31:36.760
in an online setting. And so this is our current computation because each update can be computed

31:36.760 --> 31:42.920
efficiently from the previous one. And just to unpack a little why this is non-trivial, imagine

31:42.920 --> 31:48.440
we're processing this very long input, and we're at this current time step denoted by the vertical

31:48.440 --> 31:53.080
line, and we get just one more data point. So just like a single number for the input,

31:53.080 --> 31:59.560
and we want to compute the next output. So this output depends on the entire history of the input,

31:59.560 --> 32:04.760
and so you'd expect it, the computation of the next one to scale with the length of the sequence.

32:05.880 --> 32:11.960
But actually we can compute it in constant time. And this is a non-trivial property that most

32:11.960 --> 32:17.080
sequence models don't have. For example, in a transformer or a convolution, if you were to

32:17.080 --> 32:22.200
do this in an online or autoregressive fashion, computing mapping one input to one output,

32:22.200 --> 32:25.240
each computation will scale with the entire length of the context window.

32:27.160 --> 32:30.520
The reason that SSMs can do this so efficiently is because they're stateful,

32:31.160 --> 32:36.360
which is a point that's kept coming up, where in memory we're maintaining a state,

32:36.360 --> 32:40.760
which is the blue thing, which is a single vector that summarizing the history, and can be updated

32:40.760 --> 32:50.520
very efficiently. This makes them really efficient in any sort of online setting, as we've seen.

32:52.760 --> 32:58.760
And yeah, so we'll see again why this matters. But there's one main drawback, which is that if

32:58.760 --> 33:05.080
you're not in an online setting, this is slow because it's sequential. And so what if you

33:05.080 --> 33:09.720
actually know all the future inputs? Then ideally you wouldn't do this step by step,

33:09.720 --> 33:14.840
and you could do something faster and parallelizable. And so that was actually basically the main

33:14.840 --> 33:19.640
problem with RNNs and why they've recently fallen out of favor in machine learning,

33:19.640 --> 33:25.560
because they're sequential and not parallelizable when you see a lot of data at once.

33:27.320 --> 33:30.520
And so that motivates the final representation, which is the convolutional representation,

33:30.520 --> 33:35.480
which allows them to be paralyzed. And so the idea is that instead of mapping going from the input

33:35.480 --> 33:39.720
to the state to the output, you can actually go straight from the input to the output,

33:40.440 --> 33:45.640
bypassing the state, and doing the entire computation in parallel over the sequence length.

33:46.200 --> 33:50.200
The reason is that SSMs turn out to be equivalent to convolutions,

33:51.560 --> 33:57.880
where computing the map from the input U to the output Y is equivalent to convolving the input

33:57.880 --> 34:04.600
by a particular convolution filter, which is depicted in green here. And so to compute this

34:04.600 --> 34:10.760
map, you just do it's just Y equals U convolved with K for this convolution kernel. And so this

34:10.760 --> 34:19.960
can be done very efficiently using no techniques. So for the practitioner, what one thing I want

34:19.960 --> 34:25.240
to emphasize is that I think the most useful way to think about SSMs potentially is as

34:25.240 --> 34:30.440
essentially a very fancy CNN, where you're parameterizing the convolution kernel in a

34:30.440 --> 34:36.840
different way. And notably, this kernel can be infinitely long, which again points to one reason

34:36.840 --> 34:46.680
why this is very good at long range dependencies. So just to call back to this example again,

34:46.680 --> 34:52.120
the EMA, the EMA is actually literally just a single convolution where you can involve the input

34:52.120 --> 34:58.760
by an accidentally decaying convolution kernel. And as I mentioned, although things like CNNs

34:58.760 --> 35:03.880
are also literally convolutions, they can't represent the EMA because CNNs are finite

35:03.880 --> 35:08.840
window and the EMA is infinite window. On the other hand, SSMs do represent infinitely long

35:08.840 --> 35:14.280
convolutions. And in fact, there's a very, very simple way to write down the EMA as a directly

35:14.280 --> 35:24.200
as an SSM. And I think Chris kind of pointed to that earlier. So those were the three properties

35:24.200 --> 35:30.200
of SSMs that I wanted to mention. And just to recap, first of all, we're going to think of them as

35:30.200 --> 35:36.440
maps that operate on continuous signals, not just sequences. If your model is deployed in a

35:36.440 --> 35:40.600
setting where it sees inputs in real time or online, it can compute these efficiently

35:43.720 --> 35:49.240
recurrently. And if you see an entire input at once, such as usually during training time,

35:49.240 --> 35:51.480
you can compute it even more efficiently and in parallel.

35:54.440 --> 35:59.960
I have a quick question here, Albert. This is super cool. I was just wondering if the goal

35:59.960 --> 36:04.040
is actually to get a representation of your signal so that you can perform different

36:04.040 --> 36:09.000
downstream tasks. Isn't it better to actually have the state space representation rather than

36:09.000 --> 36:14.920
directly going to the outputs? In that case, would we have to stick with HIPAA instead of going to S4?

36:16.440 --> 36:20.680
Great question. Actually, no one's asked me that, but that's a great question.

36:21.400 --> 36:31.560
So the way that I think about this is that what's happening is that essentially we have

36:31.560 --> 36:37.560
this nice state, which is very meaningful. And then the second part of the SSM that projects it

36:37.560 --> 36:41.800
is kind of like the learnable thing that's figuring out how to extract the right features

36:41.800 --> 36:47.000
from this state. Now, I mentioned that everything I've done so far, so that's a learnable part

36:47.000 --> 36:52.040
that's actually using the entire state, in a sense. And I mentioned that I'm only considering

36:52.040 --> 36:58.120
the one-dimensional case so far with 1B inputs and outputs. But actually, what's going to happen

36:58.120 --> 37:03.800
in practice in our actual deep learning models is that we'll have multi-dimensional inputs and

37:03.800 --> 37:09.080
outputs, and we'll essentially run an SSM on each one of them. And each one of these will

37:09.080 --> 37:14.120
learn how to use the state in a different way. So we'll have essentially many, you can think of

37:14.200 --> 37:19.240
as maybe we'll have a single state, but many, many possible outputs that are all learnable,

37:19.240 --> 37:23.240
and we'll extract different features from that state. So we are going to get a lot of different

37:23.240 --> 37:30.040
features that utilize the state in however they want. I see. Okay. Thank you. But sorry,

37:30.040 --> 37:37.960
sorry, Joanne. But isn't it like all these dimensions also have a correlation? So do you also,

37:37.960 --> 37:42.680
if you run the space independently, don't you want to also preserve the correlation?

37:44.840 --> 37:47.960
So this is something that I think a lot of people working with time series

37:49.240 --> 37:54.440
are concerned with. And somehow in deep learning, we don't normally consider that aspect. And we

37:54.440 --> 38:00.440
kind of just throw in a really big model and a lot of these independent layers. And kind of,

38:00.440 --> 38:05.480
I think in practice, what usually happens at the model learns to, it learns whatever it needs to do

38:05.480 --> 38:10.600
for the final prediction task. And this often does involve like, I think it does end up

38:10.600 --> 38:15.960
decorrelating things. But it's not super clear exactly the dynamics of what happens. And this is

38:15.960 --> 38:20.520
kind of a more broad question for deep learning theory in general. That's not well understood

38:20.520 --> 38:29.320
right now. What I can say is that we've used this on many types of like noisy data that usually

38:29.320 --> 38:34.280
involve, so I'm going to get to experiments later, but we have tried this on many types of

38:34.280 --> 38:38.040
like time series and other noisy data like EEG. But one day, one day, right?

38:39.560 --> 38:44.360
It can work on multiple dimensions, which I kind of just pointed to you. And also I'll mention

38:44.360 --> 38:50.840
again later how we do that. But yeah, you can just kind of do it do it naively on multiple

38:50.840 --> 38:57.720
dimensions. And it just works out of the box. Okay. Okay, so before we get to the experiments,

38:57.720 --> 39:05.640
I just have a little bit on kind of the how S4 builds on top of SSMs. And so just to refresh

39:05.640 --> 39:11.480
your memory of what S4 is, it's just an SSM where we plug in certain formulas that were

39:11.480 --> 39:14.920
based on the theory of memorization. And we have special algorithms to compute it.

39:16.760 --> 39:23.000
And so first of all, why are these matrices needed? Well, the most important part of the SSM is the

39:23.080 --> 39:33.720
state as Nandita keeps insightfully bringing up. And so what HIPPO did was that it computed a

39:33.720 --> 39:39.560
very particular state that was mathematically meaningful and compresses the history of the input

39:40.760 --> 39:45.640
in a way that captures long range dependencies. And so basically just by plugging in that formula

39:45.640 --> 39:50.360
into this SSM, it learns a more meaningful state that allows the SSM to address long

39:50.360 --> 39:55.880
dependencies better. So just to illustrate this empirically, here's a simple experiment on a very

39:55.880 --> 40:01.640
standard benchmark for sequence models. The actual task doesn't matter, but it's well studied and

40:01.640 --> 40:06.760
standard sequence model based on such as transformers, CNNs and LSTMs all get to around the

40:06.760 --> 40:11.480
around the same accuracy of like 60 ish percent. Now, what happens if we use an SSM?

40:12.600 --> 40:15.960
If you use it naively by randomly initializing all the parameters, which is

40:15.960 --> 40:19.480
what you would typically do in deep learning, it actually does terribly.

40:21.480 --> 40:27.880
But what happens if we just plug in this formula? Plugging this in and not even needing to train

40:27.880 --> 40:33.960
the matrix gives a massive boost to the SSM and goes from much below the baselines to

40:33.960 --> 40:39.000
substantially above the baselines. And actually, I use the very small models for this ablation here,

40:39.000 --> 40:45.480
but the full model as far on this data set gets over 90%, which is something like 20 plus points

40:45.480 --> 40:53.160
better than all other sequence models. So that kind of illustrates why HIPPO is so useful.

40:54.520 --> 41:02.200
Now, quick question in this example. So are both A and B basically just plug in matrices or

41:02.200 --> 41:08.840
is A alone basically a measure? A is the more important matrix, but actually, yeah, just plugging

41:08.840 --> 41:17.160
in A and B essentially just just they're both fixed matrices, which are the HIPPO operators

41:17.160 --> 41:20.680
specifies both of these. I've only illustrated A because it's a more important one. But yeah,

41:21.640 --> 41:25.400
this particular experiment froze both of these matrices to specific ones.

41:27.560 --> 41:31.640
One question people have is that like, do we always freeze these? And actually,

41:31.640 --> 41:37.400
we can train them as well. This was to illustrate just like even freezing them, it does super well.

41:38.360 --> 41:42.680
And then, but in practice, we do train them and it makes it do a little bit better.

41:45.080 --> 41:50.360
Okay, so that was one thing. And that kind of points to I mentioned that SSMs have not been

41:50.360 --> 41:54.920
used in deep learning before in this way. And that's kind of one problem. If you do it naively,

41:54.920 --> 42:00.200
it doesn't work. And so you need this new theory. The second reason is actually that they're

42:00.200 --> 42:06.600
computationally pretty difficult to work with. And so here's the illustrate.

42:08.360 --> 42:12.680
Again, so to remind you, we're thinking of an SSM as a parameterized map from an input signal to

42:12.680 --> 42:17.560
an output signal. And I'll suppose that our input had length L. So our input would just

42:17.560 --> 42:23.480
give us a sequence of L numbers. Then the output of this whole thing is also a sequence of L numbers.

42:23.480 --> 42:27.320
And computing this map ideally takes around O of L time or not too much more.

42:29.640 --> 42:34.040
But here's the problem. SSMs map the input to the output through this state.

42:34.120 --> 42:39.480
And that state gave them a lot of nice properties, but it's also 100 dimensions higher. And so

42:39.480 --> 42:44.520
computing the end to end mapping through the state will take 100 times more computation and memory

42:44.520 --> 42:52.040
than what's needed to compute the final answer. And this is actually a real problem. And now,

42:52.040 --> 42:55.160
earlier I said that you don't actually have to compute the state. You can compute it using a

42:55.160 --> 42:59.720
convolution instead. But what happens is that before computing the convolution, I have to compute

42:59.720 --> 43:04.840
the kernel or the convolution filter in green. And computing that is just as slow as computing the

43:04.840 --> 43:11.320
state. And this sort of makes sense because it hasn't changed the computational hardness of the

43:11.320 --> 43:17.400
problem. So essentially computing it no matter how you do it is going to be slow and memory inefficient.

43:19.080 --> 43:24.600
So the main point of S4 was showing that you could substantially reduce this computation

43:24.600 --> 43:28.200
when the SSM is structured. And for example, when using the

43:29.240 --> 43:33.480
hippo matrix instead of an unstructured matrix, you can save this factor of 100

43:34.040 --> 43:41.320
and make S4 overall extremely efficient. So this is done through a particular algorithm,

43:41.320 --> 43:46.440
which I'll just flash up. But basically we're trying to work with this SSM, but we only need

43:46.440 --> 43:51.800
to work with specific structured cases such as this, such as some particular hippo matrices.

43:52.520 --> 43:58.760
And now using some algorithmic ideas, it turns out there is a way to compute the convolution

43:58.760 --> 44:04.920
kernel, which was depicted in green before, very efficiently. And then compute the whole thing using

44:04.920 --> 44:11.560
a convolution. So I won't go into details here. And I will also mention that recently we've been

44:11.560 --> 44:15.720
developing simplifications of the model that allow you to bypass all of this and do things much more

44:15.720 --> 44:22.120
simply. So hopefully in a few weeks we'll have some stuff out that's where you don't need to

44:22.120 --> 44:29.320
worry about this really complicated algorithm. All right, so that was the technical portion of

44:29.880 --> 44:32.840
that I wanted to mention for S4. And I'll stop here for questions as well.

44:36.440 --> 44:44.840
So if in any case you want to actually get the state, can S4 actually recover the state?

44:45.880 --> 44:52.680
Or is it like, yeah, so like, I don't know if that would be any use case.

44:52.680 --> 44:56.600
But like I was saying, like, if there's a case where I actually want the state,

44:56.600 --> 45:01.960
can I do that and get the convolution? Yes, you can. And in fact, that will be used in

45:02.920 --> 45:07.720
some experiments. I guess I didn't mention explicitly, but you can compute it in either way,

45:07.720 --> 45:13.240
either through the convolution or through the state. And where the state or the convolution is

45:13.240 --> 45:18.920
useful is during training time for parallelizability. But where the state is useful is at some sort of

45:20.200 --> 45:25.000
inference or deployment settings, where perhaps you might be online, and then you would actually

45:25.000 --> 45:28.760
be going through the state instead of the convolution and unrolling things one step at a time.

45:29.720 --> 45:35.480
Right. So you can do it either way, which is pretty cool. Thanks.

45:36.360 --> 45:43.640
I had more of a thought question. Let's say I'm interested in two different measures. Like,

45:43.640 --> 45:49.400
I want to see how the exponential average works, but I also want like, so is it,

45:50.200 --> 45:54.760
does it basically mean that I just have to create a new measure that combines this efficiently before

45:54.760 --> 46:01.960
this? Before I plug it into SSM or can S4 basically kind of,

46:02.920 --> 46:06.360
because there are two independent blocks that I can basically...

46:08.120 --> 46:11.320
Yeah. So I'm just about to get to the experiments. And actually, I will,

46:12.440 --> 46:17.400
I'll get to that slide right now, where, so first of all, the experiments will be on this type of

46:17.400 --> 46:24.520
signal data. And what, as I mentioned a couple of times, what we actually do is that I have to

46:24.520 --> 46:30.120
find this 1D to 1D map, but I'm actually going to just like, given a multidimensional input,

46:30.120 --> 46:35.320
I'm just going to stack a bunch of copies of this. And now as a parallel to that, you can do

46:35.320 --> 46:39.640
many things with these copies. So to answer your question, one thing that I've been starting to

46:39.640 --> 46:44.520
experiment with is just using different measures or essentially different A and B matrices for

46:44.520 --> 46:49.880
every copy. And that can, and so that sort of has interpretation of using multiple measures.

46:51.000 --> 46:56.440
I see. Because when Iman actually talked about the correlations between different dimensions,

46:57.080 --> 47:00.920
let's say you have an image, like two different pixels are actually correlated.

47:00.920 --> 47:04.600
So I was thinking like, you can have a measure that captures this correlation,

47:04.600 --> 47:08.360
but you can have another measure that captures it over time.

47:09.960 --> 47:14.360
Another thing actually, since you mentioned that, I don't know if you tried that on image

47:14.360 --> 47:19.240
space, I would be curious like if this kind of like long convolution actually makes any difference

47:19.240 --> 47:25.160
with the image space. Because image usually like, when we do the image analysis theoretically,

47:25.160 --> 47:29.640
when we start thinking about it, it seems that like also the local feature as well as of course

47:29.640 --> 47:35.320
the global feature is important. But I don't know, like if we are missing any local features by just

47:35.320 --> 47:43.160
using this kind of like long representation. That's a good question. I actually, we have started

47:43.160 --> 47:47.880
doing more experiments on images, which I didn't include in this talk, but luckily we do find that

47:48.520 --> 47:55.080
the local bias of convolutions does seem pretty good. I don't know, it's hard to quantify if

47:55.080 --> 48:00.120
we're missing features, but I think there are settings where we're not, we're only on power

48:00.120 --> 48:07.240
or not, or maybe a little bit worse than a standard local CNN. It is hard to say. I will

48:07.240 --> 48:14.600
mention though that you can forcibly incorporate locality into this just by changing the measure.

48:14.600 --> 48:18.920
For example, if you choose a uniform measure that has a short window, that's the same as saying

48:18.920 --> 48:24.280
I would just want a local convolution kernel. Because I would imagine like for this particular

48:25.240 --> 48:30.120
thing, like the use case where we have to have to work with a very high resolution image data,

48:31.000 --> 48:34.920
you know, for example, like imagine like mammogram, right? Like we have to go with like

48:34.920 --> 48:41.160
1000 by 1000 minimum dimension. So for this probably would be useful because they are actually,

48:41.160 --> 48:45.960
we want to like do the rescaling, but we cannot because we'll lose probably a lot of features

48:45.960 --> 48:49.880
in the middle. But this kind of like long convolution could this, this is a perfect

48:49.880 --> 48:54.920
problem that I will actually, I wasn't going to, but now I'll mention this in the experiments as

48:54.920 --> 49:01.480
well. Okay. It's actually something that we have thought about basically rescaling of convolutions

49:01.480 --> 49:08.200
and using. Right. Okay, I'll get to that. Before that, so I want to get the experiments and

49:08.200 --> 49:12.040
basically I just wanted to find, I'm only to find the simple linear one either one D map,

49:12.040 --> 49:16.360
but you can just do it in parallel across a lot of features and then plug it into a standard neural

49:16.360 --> 49:23.480
network to do sequence modeling. So the first type of data I'll see is a biosignal data.

49:26.840 --> 49:34.760
So here is a, there's a real world data set of trying to predict vital signs such as heart rate

49:34.760 --> 49:42.440
from raw biosignal data such as I wrote EKG and EG here, but I think it's actually EKG and PPG.

49:43.000 --> 49:46.920
And so that's visualized here. And this data is pretty challenging for deep learning models

49:46.920 --> 49:52.840
because you can see that it's very long. This is a sequence of like 4000. If you zoom in a lot,

49:52.840 --> 49:55.880
it would be pretty smooth actually, but if you zoom out, it displays a lot of

49:55.880 --> 50:01.560
periodicity and spikes and other things. And so a lot of methods have been tried on this data set,

50:01.640 --> 50:07.320
which include kind of standard machine learning techniques like XGBoost as well as many very

50:07.320 --> 50:13.320
modern deep learning sequence models. And S4 substantially improves over all of these in

50:14.600 --> 50:18.360
I think cutting the root mean squared error by at least two thirds on all of these targets

50:20.280 --> 50:23.240
just with that generic deep learning model that deep model that I showed.

50:24.680 --> 50:29.240
Actually I've, these were like older numbers and recently I've been rerunning these again and

50:29.240 --> 50:37.480
actually you can drop this down even more. One thing I will note is that attention and

50:37.480 --> 50:43.080
transformers does really poorly on this type of data. And that's something that I think I found

50:43.080 --> 50:48.600
pretty consistently. So there's some sort of bias toward what type of data you have and S4 is really

50:48.600 --> 50:53.240
good at signals and attention is not. Conversely, attention is good at some other types of discrete

50:53.240 --> 51:00.680
data that S4 is not as good at. Okay, so that's, that was one experiment. The next one is two time

51:00.680 --> 51:07.880
series data where we did a forecasting task where you're given a context window and you want to

51:07.880 --> 51:12.120
predict future values. Actually, I'm going to go through this kind of fast because I don't want

51:12.120 --> 51:15.880
that much time I want to get through some more of the bio applications and the things that you

51:15.880 --> 51:22.360
guys brought up. The models here are very complicated. Whereas for S4, we're actually

51:22.360 --> 51:27.480
doing an extremely simple setup, which is just a mask prediction. We're just going to give you,

51:27.480 --> 51:31.640
we're going to take the entire sequence and mask out the desired forecast range and then just

51:33.000 --> 51:39.080
predict what's in the mask by passing it through this generic deep model. So this is really,

51:39.080 --> 51:44.520
it's like a very extremely simple application. I won't unpack the numbers too much, but there's a

51:44.520 --> 51:50.920
lot of baselines here, including time series models, LSTMs, lots of transformers, and S4

51:50.920 --> 51:54.840
does better than all of them on these real time series data sets, including weather and energy

51:54.840 --> 52:02.840
data with much less specialization. These models were all designed for time series and we were

52:02.840 --> 52:08.280
just using our generic model. And you didn't even like tune the window size, right?

52:09.640 --> 52:14.280
We did not for this one. Actually, by tuning the window size, you can get the numbers down even

52:14.280 --> 52:24.760
more. Okay. Okay. The next one here points to Amon's question about rescaling. So it's actually,

52:24.760 --> 52:31.800
I'm going to display this to audio. But essentially, I've used audio a few times. I'm running example.

52:32.520 --> 52:38.840
It's sampled at extremely high rate. And it's extremely long. So this is a data set of classifying

52:38.840 --> 52:45.000
one second speech clips, which were length 16,000, into classifying the words. And most

52:45.000 --> 52:50.760
sequence models like transformers and RNNs are really bad here. The only thing that works is CNNs,

52:51.320 --> 52:58.760
which the red line is pointing to a speech CNN baseline. And these work okay. But what happens

52:58.760 --> 53:03.480
if you are resampling the signal at different frequencies? And this happens commonly in audio

53:03.480 --> 53:08.760
because your signal can be sampled at any rate and sound more or less the same. So for example,

53:08.840 --> 53:14.120
this orange sequence is a sequence of samples, but it's actually the same underlying signal

53:14.120 --> 53:18.360
as the original blue sequence of samples, just at a different frequency. And so it's ideal if

53:18.360 --> 53:24.360
the same model works on both of them. But standard models like CNNs cannot do this,

53:25.160 --> 53:31.160
essentially because of the local bias that was brought up earlier. I won't unpack this here,

53:31.160 --> 53:36.360
but if you use like a standard local CNN, it will break at a different frequency. However,

53:36.360 --> 53:41.240
by using a signal model such as S4, which is actually understanding the underlying continuous

53:41.240 --> 53:46.440
domain or the underlying continuous function, it can work here without modification. So this is

53:46.440 --> 53:51.560
all in a zero shot setting where it's trained at one resolution and tested on a different resolution.

53:52.440 --> 53:59.080
And this breaks a CNN, but S4 can do it out of the box. And that's because of this first property

53:59.080 --> 54:06.200
of being a continuous time model. And now the last two things I'll show are just calling back to

54:06.200 --> 54:13.160
the experiments at very beginning. I showed some audio generation clips. And that was an

54:13.160 --> 54:17.560
autoregressive setting where we're generating things one sample at a time. And despite having

54:17.560 --> 54:23.320
an extremely large context window, which made it do better and more coherent, we could still

54:23.320 --> 54:27.960
sample things autoregressively just as fast as other autoregressive models. And that's because

54:27.960 --> 54:33.480
of the fast online or autoregressive representation where you're computing the state and updating it

54:33.480 --> 54:39.560
every time. And finally, I showed this benchmark of long range modeling where S4 substantially

54:39.560 --> 54:45.480
outperforms other models on a range of different tasks. And this benchmark was also used to

54:45.480 --> 54:51.960
benchmark the speed of models during training where S4 is just as fast as all of these

54:51.960 --> 54:56.600
efficient transformer variants. And that's because of the efficient, paralyzable view

54:57.240 --> 55:03.800
along with the new algorithms we introduced. And so all these properties, as I promised,

55:03.800 --> 55:08.600
have concrete empirical benefits. Now for, I'm running out of time, so I just want to get to

55:08.600 --> 55:14.520
a couple more things. For the last part, I just wanted to, for this audience, I wanted to point to

55:14.520 --> 55:19.640
where I hope that this model will be useful, which is as a general tool for deep learning for

55:19.640 --> 55:26.360
biosignals. And I've pointed out one, one example of a data set already where we were predicting

55:26.360 --> 55:32.280
like heart rate from EKG signals. But this was another one that CE was working on actually and

55:33.240 --> 55:40.120
her and another lab mate have been trying to test S4 here, where this is a data set of raw

55:40.120 --> 55:46.360
EEG signals that are difficult to process because they're so noisy and long. And the state-of-the-art

55:46.360 --> 55:53.560
models are very recent. CE's model from a couple months ago was there they are on one of these

55:53.560 --> 55:58.840
EEG data sets, but it was quite involved and involved a lot of domain knowledge, such as even

55:58.840 --> 56:04.040
like the placement of the electrodes and a lot of different parts, components of the model.

56:04.600 --> 56:10.440
And so where I hope that S4 could be useful is as a generic tool or building block for

56:11.320 --> 56:20.520
for addressing these types of signal data without as much domain expertise and how to design the

56:20.520 --> 56:28.520
model. And so CE and Collid have been running some preliminary experiments using S4 on this data,

56:28.520 --> 56:34.520
where we don't even need to process the, you don't need to pre-process it with FFT features,

56:34.520 --> 56:39.640
you don't need to do a lot of these other things and just run it through these, a generic deep

56:39.640 --> 56:45.480
model composed of S4 layers. And Collid found some very preliminary results where it is improving

56:45.480 --> 56:53.080
over the baselines in some settings. This is still very preliminary, so it's not, there's other

56:53.080 --> 56:57.960
settings that we care about, such as incorporating self-supervision and so on, where it's not quite

56:57.960 --> 57:05.960
there, but I do think it has a lot of potential in this type of domain. Another example, actually

57:06.040 --> 57:11.080
that was published was another recent collaboration with Stanford Medicine that was submitted to a

57:11.080 --> 57:17.960
gastroenterology journal on detecting acid reflux from impotence sensor data. And so again,

57:17.960 --> 57:27.480
S4 was really good on that type of prediction task. So that is all I was going to talk about

57:27.960 --> 57:35.960
for this. So just to review, S4 is a, it's an SSM, which are these two equations,

57:35.960 --> 57:39.560
where we plug in certain formulas and have special algorithms to compute the model.

57:40.600 --> 57:46.280
And overall, SSMs and in particular, S4 have a number of very nice properties with

57:46.280 --> 57:50.920
concrete empirical benefits, as we saw, and I think can become a very effective building block for

57:50.920 --> 57:56.440
modeling many types of sequential data in the future. Thanks for listening and thanks for all

57:56.440 --> 58:03.000
the collaborators for the hard work. This slide lists a couple of resources, such as blog posts

58:03.000 --> 58:09.560
and related papers, as well as the audio results from an ongoing, a paper that's under submission

58:09.560 --> 58:15.800
right now. Feel free to reach out if you have questions and thanks. This was my last slide,

58:15.800 --> 58:20.680
but because you want to ask how I will, I guess I'm technically out of time, so of course people

58:20.680 --> 58:25.960
feel free to leave, but if you want to stay, I can show one thing about the high resolution images

58:25.960 --> 58:34.920
that that was brought up. Let me find that slide. Yeah, if people have conflicts, feel free to leave

58:34.920 --> 58:41.160
and we will put up the recording of the talk later in our YouTube channel. Otherwise, if you

58:41.160 --> 58:47.160
would like to stay, then yeah, I'll be able to share the slide. So yeah, I'll just really quickly go

58:47.160 --> 58:56.600
over this where medical, medical imaging is something that we think could be a potential

58:57.400 --> 59:06.680
strong use case for S4 because of this high resolution feature where, so this slide was

59:06.680 --> 59:10.600
about, I was moving from a different way, but the point I wanted to make was that

59:11.320 --> 59:15.320
Can you go to presentation mode? I think we are still seeing your screen nose.

59:16.120 --> 59:24.200
Yeah, sorry. Oops. Am I showing my whole screen? No, we are seeing your screen rather than the

59:24.200 --> 59:33.400
presentation. Okay, I thought I had it on. You can just swap the view. I thought I had it on the right

59:33.960 --> 59:42.840
view. Is this one still show the, oops. Yeah, I can see the high definition view.

59:43.880 --> 59:48.600
Okay, great. So yeah, so the point I was making is that normally image data sets are things like

59:48.600 --> 59:52.760
ImageNet, which are actually extremely low resolution compared to other data that we might

59:52.760 --> 01:00:00.280
find such as medical imaging where apparently the images can be up to 100,000 by 100,000 pixels.

01:00:00.600 --> 01:00:05.400
And this is obviously like way too big for current models, which can only operate on small

01:00:06.040 --> 01:00:11.000
patches at a time. So I don't know how to address this really, but it's something that fascinates

01:00:11.000 --> 01:00:16.760
me. But just to point out, this is part of a longer drop pack where I pointed some potential

01:00:16.760 --> 01:00:22.200
future directions. The one that I'll mention here relates to some things that we brought up,

01:00:22.200 --> 01:00:27.800
which is that just like the speech experiment that I showed, I believe that S4 should work

01:00:27.880 --> 01:00:32.920
training on images at different resolutions. And so what you can do is essentially try to

01:00:34.120 --> 01:00:40.440
train on lower dimensional versions of the image, lower resolution versions, and then transfer

01:00:40.440 --> 01:00:44.920
the same model to work on high dimensions, which is a very similar thing that I showed

01:00:44.920 --> 01:00:53.320
for the speech example. So yeah, I think that's potentially something that could work. And the

01:00:53.320 --> 01:00:57.800
point is that a signal model like S4 will work at different resolutions

01:01:00.280 --> 01:01:05.480
because you can sample at different rates essentially. And yeah, so what you need is a

01:01:05.480 --> 01:01:10.200
signal model that understands the continuous domain, just like the example I showed. And that

01:01:10.200 --> 01:01:14.280
points to this property again. So this is something where we haven't tried it and I don't know if it

01:01:14.280 --> 01:01:19.320
works, but it's some part of me feels like it might be the right way to or one potential

01:01:19.960 --> 01:01:23.880
good way to approach this type of problem. But I would think in the opposite way,

01:01:25.000 --> 01:01:29.160
it's not really we want to generate the high resolution from the low resolution,

01:01:29.160 --> 01:01:33.080
but I would imagine since you have this kind of like state-based representation and

01:01:33.080 --> 01:01:37.240
finally you're getting this signal, I would imagine like in some case always like we had

01:01:37.240 --> 01:01:41.800
to deal with this kind of situation that we had a very high resolution image. And before running

01:01:41.800 --> 01:01:47.320
through the convolution because of the memory computations, like computational complexity,

01:01:47.400 --> 01:01:51.560
memory complexity and all this kind of thing. So you have to reskill the image into a much lower

01:01:51.560 --> 01:01:59.080
dimension. Yeah. And we had a chance of losing a lot of features, specifically histopathology,

01:01:59.080 --> 01:02:02.680
exactly the example that you showed, or the mammogram, those kind of images, you know.

01:02:03.480 --> 01:02:08.520
Yeah, I see. So I was thinking that perhaps like what you can do is kind of like iteratively

01:02:09.720 --> 01:02:13.240
increase the resolution and pick up higher and higher resolution features as you go.

01:02:13.800 --> 01:02:18.920
But the benefit is that perhaps you can pick up the coarser grain things and then as you

01:02:18.920 --> 01:02:24.440
upsize the image then and you rescale your kernel essentially, then it's already going to be doing

01:02:24.440 --> 01:02:28.920
as everybody knows the coarse grain features, but then it as you keep training, it only has

01:02:28.920 --> 01:02:35.000
to learn the higher frequency features as you go. And again, I have no idea if this is,

01:02:35.000 --> 01:02:40.040
if this makes sense or it's promising, but it sounds pretty interesting. I think,

01:02:40.040 --> 01:02:46.680
I think Alba's idea is actually very similar to how pathologists analyze our hostile images.

01:02:46.680 --> 01:02:51.960
So they usually look at. Define zoom. Define zoom. They usually look at low resolution

01:02:52.520 --> 01:02:59.320
image first and localize like the potential areas where the tumor are and then they zoom

01:02:59.320 --> 01:03:05.960
into higher resolution. Right. Yeah. I see. Yeah. And so yeah, I don't know if this will be better

01:03:05.960 --> 01:03:10.920
than CNN or other things, but it definitely has different and interesting properties.

01:03:13.400 --> 01:03:20.840
Okay. So that was, yeah, I think that's the end of the material I have. I can say around a few

01:03:20.840 --> 01:03:25.800
more minutes if people still have questions. All right. Great. Thanks so much, Albert.

01:03:25.800 --> 01:03:30.120
I have just one question. Sure. Yeah. Thank you for the presentation. That was awesome.

01:03:31.000 --> 01:03:36.440
Did you find any scenario where it's better to use transformer than S4?

01:03:37.240 --> 01:03:42.760
Yes. Another great question. So let me just share my screen again. I have one slide prepared for

01:03:42.760 --> 01:03:51.000
that. Basically at the beginning, I drew this distinction between continuous kind of continuous

01:03:51.000 --> 01:03:58.920
and discrete data. And I think that S4 will be the best or like the ideas involved are potentially

01:03:59.320 --> 01:04:04.280
going to be the best thing to do for signals. But for kind of higher level concepts or more

01:04:04.280 --> 01:04:09.240
discrete concepts such as language or some other things, transformers, I think that's

01:04:09.240 --> 01:04:19.240
where transformers really shine and are probably going to be better. So here's a,

01:04:20.200 --> 01:04:25.800
I don't know if this is the right screen again. Sorry. Anyways, here's the one slide on language

01:04:25.800 --> 01:04:31.080
modeling where we took a transformer, which are currently of course the best models for

01:04:31.080 --> 01:04:38.040
text and NLP. And we replaced the attention with S4 and found that it doesn't do quite as well.

01:04:38.840 --> 01:04:42.840
But it is still better than all non, it's significantly better than all non-transformer

01:04:42.840 --> 01:04:50.200
models. And it also has some other benefits. Like you can do language generation much faster

01:04:50.200 --> 01:04:55.000
because of the fast recurrent view. That was the main point of this. But this also does

01:04:55.000 --> 01:04:59.720
kind of point to the fact that personally my intuition is that transformers are really good

01:04:59.720 --> 01:05:05.160
for dense and discrete data. Whereas S4 is really good for more like noisy and raw data.

01:05:06.440 --> 01:05:11.720
Yeah. And I mean the speed up here I think is very interesting. Do you know what was the window

01:05:11.720 --> 01:05:17.320
you would take for language modeling? Like how many tokens or words rather did you consider?

01:05:17.320 --> 01:05:22.280
Yeah. This experiment was done using a pretty standard length of either 512 or 1024 tokens.

01:05:23.160 --> 01:05:27.560
You actually can keep increasing the window length for S4, which only slows it down a little

01:05:27.560 --> 01:05:32.440
bit and actually improves the performance a little bit as well. But I found that out after

01:05:32.440 --> 01:05:38.520
the fact and I didn't feel like retraining this. Okay. Cool. Thanks. So yeah, but the

01:05:38.520 --> 01:05:41.960
sort of findings of this slide is the speed up, right? It's massive.

01:05:43.080 --> 01:05:48.920
That was the point that we did this experiment for. But yeah, so there's a lot of the speed up.

01:05:48.920 --> 01:05:52.680
In terms of the original question though, in terms of the raw performance of modeling the data,

01:05:53.320 --> 01:05:57.480
transformers are currently doing a little bit better here. Cool. Thank you.

01:06:03.160 --> 01:06:04.680
All right. Is there any other questions?

01:06:07.960 --> 01:06:15.000
Let's all give Albert a round of virtual applause. Thank you for the very comprehensive

01:06:15.560 --> 01:06:23.080
presentation of state-based models. Thanks for having me. Thank you, Albert.

01:06:23.080 --> 01:06:27.480
Thank you. Thank you, everyone, for joining us. We will put up the recording of the video,

01:06:27.480 --> 01:06:32.280
the talk, later to our YouTube channel. And yeah, we'll see you at the same time next week.

01:06:33.480 --> 01:06:34.840
Thank you. See you, guys.

