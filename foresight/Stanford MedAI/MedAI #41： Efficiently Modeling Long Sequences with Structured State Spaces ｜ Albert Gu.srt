1
00:00:00,000 --> 00:00:08,080
Okay, hi everyone, welcome to our 41st session of the May AI Group Exchange.

2
00:00:08,080 --> 00:00:13,320
This week we have Elba Gu from Stanford here with us to present his research on efficiently

3
00:00:13,320 --> 00:00:17,280
modeling long sequences with structured state spaces.

4
00:00:17,280 --> 00:00:21,200
Elba is a final year PhD candidate in the computer science department here at Stanford

5
00:00:21,200 --> 00:00:24,080
University, advised by Chris Ray.

6
00:00:24,080 --> 00:00:28,920
He's brought the interest in studying structured representations for advanced signal capabilities

7
00:00:28,920 --> 00:00:34,400
of machine learning and deep learning models with focuses on structured linear algebra,

8
00:00:34,400 --> 00:00:38,480
non-euclidean representations, and theory of sequence models.

9
00:00:38,480 --> 00:00:41,040
Thank you so much Elba for joining us today.

10
00:00:41,040 --> 00:00:45,440
Before we start, do you have any preference on how you want to take questions?

11
00:00:45,440 --> 00:00:49,560
Yeah, thank you, thank you for an introduction.

12
00:00:49,560 --> 00:00:56,560
For this talk, I think I'm not sure usually the level of formality, but I'm very happy

13
00:00:56,560 --> 00:01:03,560
to have the casual in terms of the conversation and the questions.

14
00:01:03,560 --> 00:01:07,320
I think there's some time, it's not going to be a full hour talk, so I'm more than happy

15
00:01:07,320 --> 00:01:11,120
to take questions during it and I'll watch the time in case it gets too long.

16
00:01:11,120 --> 00:01:17,320
And then I'll also pause a few times to pause for potential questions during some sections.

17
00:01:17,320 --> 00:01:19,520
Okay, sounds good.

18
00:01:19,520 --> 00:01:23,120
Let's try to make this session as interactive as possible.

19
00:01:23,120 --> 00:01:26,840
How further or do let me hand it over to Albert?

20
00:01:26,840 --> 00:01:28,840
Thank you.

21
00:01:28,840 --> 00:01:35,640
All right, so this talk will be about a new sequence model called S4, or structured state

22
00:01:35,640 --> 00:01:36,640
spaces.

23
00:01:36,640 --> 00:01:45,320
Now, for the purposes of this talk, when I mentioned sequence models, we will think of

24
00:01:45,320 --> 00:01:51,280
them as a black box sequence-to-sequence map composed of primitive layers, where each

25
00:01:51,280 --> 00:01:59,680
layer simply takes an input sequence and returns a sequence of the same shape.

26
00:01:59,680 --> 00:02:04,680
For our purposes right now, we'll think of them as just being a one-dimensional to one-dimensional

27
00:02:04,680 --> 00:02:12,360
map, but this can be easily converted to higher-dimensional features.

28
00:02:12,360 --> 00:02:15,600
Many sequence models have been developed that satisfy this interface, particularly in the

29
00:02:15,600 --> 00:02:17,520
context of deep learning.

30
00:02:17,520 --> 00:02:20,800
These include many classical deep learning models, such as recurrent neural networks

31
00:02:20,800 --> 00:02:25,320
or RNNs and convolutional neural networks or CNNs, as well as many more modern models,

32
00:02:25,320 --> 00:02:30,360
such as transformers or neural ODEs.

33
00:02:30,360 --> 00:02:33,440
And all of these models kind of satisfy the same interface.

34
00:02:33,440 --> 00:02:37,600
They map a sequence to a sequence of the same shape, or meaning the same length and

35
00:02:37,600 --> 00:02:38,800
field dimension.

36
00:02:38,800 --> 00:02:44,760
And then you can incorporate any of these into a deep learning model fairly easily just

37
00:02:44,760 --> 00:02:53,080
by using standard architectures, where you can include normalization layers, other linear

38
00:02:53,080 --> 00:02:57,920
or nonlinear activations, as well as with the dual connections.

39
00:02:57,920 --> 00:03:03,040
And so the core component of all of this is the core sequence model, and that's what

40
00:03:03,040 --> 00:03:06,200
we'll focus on.

41
00:03:06,200 --> 00:03:11,280
And this generic deep neural architecture based on sequence models can be used to solve

42
00:03:11,280 --> 00:03:18,280
many types of problems with many types of sequence data, from medallies such as text

43
00:03:18,280 --> 00:03:26,680
and audio to images and videos to general time series data or biosignals, for example,

44
00:03:26,680 --> 00:03:31,080
which is depicted here.

45
00:03:31,080 --> 00:03:34,120
In this talk, I'm going to draw a very rough distinction between different types of sequence

46
00:03:34,120 --> 00:03:35,120
data.

47
00:03:35,120 --> 00:03:39,560
Now much of modern sequence modeling in the context of machine learning focuses on data

48
00:03:39,560 --> 00:03:41,440
such as text.

49
00:03:41,440 --> 00:03:45,120
And very roughly I'll classify this as being a discrete sequence because the input comes

50
00:03:45,120 --> 00:03:48,560
in the form of discrete tokens.

51
00:03:48,560 --> 00:03:53,240
And other types of data like this includes things like graphs or things like DNA-based

52
00:03:53,240 --> 00:03:54,240
pairs.

53
00:03:54,240 --> 00:04:00,480
In contrast, what this talk will focus on is data that's roughly more continuous, things

54
00:04:00,480 --> 00:04:04,600
such as video or time series or audio.

55
00:04:04,600 --> 00:04:08,520
And what's common to all of these is that there's an underlying notion of time from

56
00:04:08,520 --> 00:04:12,920
which the sort of data is sampled from.

57
00:04:12,920 --> 00:04:17,960
And so I'm going to very broadly call this type of data signal data as opposed to sequence

58
00:04:17,960 --> 00:04:19,080
data.

59
00:04:19,080 --> 00:04:23,000
And roughly speaking, signals can be defined as data that's generated from an underlying

60
00:04:23,000 --> 00:04:29,200
continuous physical process, including all these examples here.

61
00:04:29,200 --> 00:04:31,320
This talk will be composed of two parts.

62
00:04:31,320 --> 00:04:36,480
The first part covers a method called HIPPO, which was the predecessor to S4.

63
00:04:36,480 --> 00:04:42,000
And it's a new conceptual framework for the online memorization of signals and led to

64
00:04:42,000 --> 00:04:45,320
a new method for modeling signals and sequences.

65
00:04:45,320 --> 00:04:49,560
And then the second part will be S4, which built right on top of HIPPO.

66
00:04:49,560 --> 00:04:53,120
And it has a lot of important properties that have been very effective for addressing some

67
00:04:53,120 --> 00:04:55,240
types of sequence modeling problems.

68
00:04:55,240 --> 00:04:59,240
And before I get into the technical stuff, I'll give a quick preview experimental results

69
00:04:59,240 --> 00:05:04,120
to highlight the types of improvements I will see and what it's good at.

70
00:05:04,120 --> 00:05:07,080
And this will kind of illustrate the types of challenges that we'll hope to address with

71
00:05:07,080 --> 00:05:10,200
these new models.

72
00:05:10,200 --> 00:05:14,520
The first challenge overall is just going to be to signal or general temporal data that

73
00:05:14,520 --> 00:05:15,600
I just defined.

74
00:05:15,600 --> 00:05:17,000
And this data is really everywhere.

75
00:05:17,000 --> 00:05:23,760
So some examples include audio waveforms, spatial temporal data like videos, biosignals

76
00:05:23,760 --> 00:05:30,840
like electrocardiograms, which have important applications of medicine, or market and financial

77
00:05:30,840 --> 00:05:32,200
data.

78
00:05:32,280 --> 00:05:35,520
And then there will be multiple time series logs being generated by every major industry

79
00:05:35,520 --> 00:05:39,040
and many other types of scientific modeling problems.

80
00:05:39,040 --> 00:05:44,360
And we'll return to these experiments later with a particular focus on some biosignal

81
00:05:44,360 --> 00:05:45,360
data.

82
00:05:45,360 --> 00:05:51,080
But for now, I will just use one example to illustrate, which is audio.

83
00:05:51,080 --> 00:05:55,320
And audio is actually one of the most common types of data because it's just raw sound.

84
00:05:55,320 --> 00:05:56,320
It's everywhere.

85
00:05:56,320 --> 00:06:01,200
And so to illustrate, machine learning right now is really all about text and so many

86
00:06:01,200 --> 00:06:05,240
headline results recently have been about people scraping together all the raw text

87
00:06:05,240 --> 00:06:08,040
data they can get, creating massive models on them.

88
00:06:08,040 --> 00:06:13,000
And that's led to very impressive results like GPT-3, which I don't know the audience,

89
00:06:13,000 --> 00:06:17,400
but hopefully many of you have heard of this model.

90
00:06:17,400 --> 00:06:20,960
In contrast, audio actually has orders of magnitude more data than text.

91
00:06:20,960 --> 00:06:24,880
For example, a single labeled dataset has more data set than all of the data used to

92
00:06:24,880 --> 00:06:27,080
train those massive language models.

93
00:06:27,080 --> 00:06:29,600
But you don't hear about benchmarks in this domain nearly as much.

94
00:06:29,800 --> 00:06:35,280
And I think part of the reason is just because audio models are, audio is very challenging

95
00:06:35,280 --> 00:06:39,640
and current models seem much worse in comparison to text.

96
00:06:39,640 --> 00:06:47,680
And so here's a concrete example where we consider basically a very general and hard

97
00:06:47,680 --> 00:06:54,400
audio generation setting of generating spoken digits, zero to nine, using a completely

98
00:06:54,400 --> 00:06:56,640
unconditional autoregressive model.

99
00:06:56,640 --> 00:07:00,120
And the gold standard here is a baseline called WaveNet.

100
00:07:00,120 --> 00:07:03,760
And here's what it sounds like trying to say these numbers.

101
00:07:11,760 --> 00:07:14,360
So it's, it's not very good.

102
00:07:14,360 --> 00:07:18,320
And here's results for S4, which was just, these results are just from the past like

103
00:07:18,320 --> 00:07:20,320
two months or so ago.

104
00:07:21,320 --> 00:07:27,800
One, two, three, four.

105
00:07:27,800 --> 00:07:31,600
So that's, that's a pretty concrete example.

106
00:07:31,600 --> 00:07:35,440
And so in this talk, we'll see how models like S4 are kind of designed for signals in

107
00:07:35,440 --> 00:07:39,200
a way and can have significant advantages for this type of data.

108
00:07:39,200 --> 00:07:44,920
And the second example up front, or the second example of a running challenge will be, can

109
00:07:44,920 --> 00:07:47,480
be motivated by examining audio more closely.

110
00:07:47,480 --> 00:07:51,320
And one reason why audio is so hard is because it's sampled at such an extremely high rate

111
00:07:51,320 --> 00:07:55,400
where a single second has 16,000 or more samples.

112
00:07:55,400 --> 00:08:00,800
In contrast, most sequence models can't deal with more than a thousand or so.

113
00:08:00,800 --> 00:08:04,320
And to illustrate, there was a benchmark in the past year called Long Range Arena that

114
00:08:04,320 --> 00:08:07,760
measured the performance of models on a suite of long range tasks.

115
00:08:07,760 --> 00:08:13,000
And the most popular sequence models these days, Transformers, were the main focus.

116
00:08:13,000 --> 00:08:17,200
But despite their many successes, they don't do so well on long context.

117
00:08:17,200 --> 00:08:21,440
And so there were dozens of variants that were tried, and they all get to around the

118
00:08:21,440 --> 00:08:26,920
same performance, which is actually not much above random guessing.

119
00:08:26,920 --> 00:08:32,480
In contrast, S4 we'll see is explicitly designed to be effective on long context, which leads

120
00:08:32,480 --> 00:08:34,960
to a huge improvement on this benchmark.

121
00:08:34,960 --> 00:08:39,360
And it's the first model to ever make progress on some really difficult long sequence tasks.

122
00:08:39,360 --> 00:08:43,320
Can I ask a quick question here, Albert?

123
00:08:43,320 --> 00:08:44,320
Yeah.

124
00:08:44,760 --> 00:08:49,360
So in the previous task, that was a generative process.

125
00:08:49,360 --> 00:08:56,680
And in this, the Long Context Channel challenge, is it a classification or what kind of task

126
00:08:56,680 --> 00:08:57,680
is it?

127
00:08:57,680 --> 00:08:59,080
These are all classification problems.

128
00:08:59,080 --> 00:09:05,200
And they're on data such that includes several data modalities, such as text, images, some

129
00:09:05,200 --> 00:09:08,440
sort of symbolic processing, stuff like that.

130
00:09:08,440 --> 00:09:09,440
I see.

131
00:09:09,440 --> 00:09:13,440
And so you can use S4 both as a generative model to actually generate sequences.

132
00:09:13,680 --> 00:09:14,680
Yeah.

133
00:09:14,680 --> 00:09:20,640
So a lot of sequence models, again, a sequence model I'm defining as a black box interface,

134
00:09:20,640 --> 00:09:23,280
really, that's just a sequence-to-sequence map.

135
00:09:23,280 --> 00:09:28,640
And many of these can be used in many ways, both for classification and generation.

136
00:09:28,640 --> 00:09:34,320
For example, transformers or RNNs are similar things that satisfy the same interface and

137
00:09:34,320 --> 00:09:36,280
can be used in many ways as well.

138
00:09:36,280 --> 00:09:37,280
Gotcha.

139
00:09:37,280 --> 00:09:38,280
Thank you.

140
00:09:39,960 --> 00:09:40,960
Yeah.

141
00:09:41,080 --> 00:09:43,160
OK, so now I'll get into the technical portions.

142
00:09:43,160 --> 00:09:46,360
And the first part will be about Hippo, as I mentioned.

143
00:09:46,360 --> 00:09:52,960
And to motivate what Hippo's goal was, I gave a bunch of examples of data that machine

144
00:09:52,960 --> 00:09:56,640
learning models currently struggle with, particularly things like time series.

145
00:09:56,640 --> 00:10:00,600
And to highlight why this is hard, I'm going to use a running example to illustrate a very

146
00:10:00,600 --> 00:10:06,000
basic capability that's difficult for modern models.

147
00:10:06,000 --> 00:10:09,400
And that's the moving average, which is perhaps the most basic method in modern time series

148
00:10:09,400 --> 00:10:10,640
analysis.

149
00:10:10,640 --> 00:10:15,120
So this figure depicts the exponential moving average or EMA, which is the blue line.

150
00:10:15,120 --> 00:10:20,360
And the way it's used is that it's a fixed non-learnable feature that's often the first

151
00:10:20,360 --> 00:10:26,080
pre-processing step that's performed in any sort of time series analysis pipeline.

152
00:10:26,080 --> 00:10:30,960
Now in the context, in the spirit of machine learning and deep learning, instead of doing

153
00:10:30,960 --> 00:10:36,680
manual processing like creating these features, we really would like to be able to learn these

154
00:10:36,680 --> 00:10:41,120
sort of things automatically from the data.

155
00:10:41,120 --> 00:10:46,600
And so in particular, here's a very simple concrete task is, suppose you have a model

156
00:10:46,600 --> 00:10:54,920
and you're feeding it this black input signal, and you want the model to predict the EMA

157
00:10:54,920 --> 00:10:57,320
or the blue signal as the output.

158
00:10:57,320 --> 00:11:02,760
And fortunately, it turns out that standard sequence models, such as attention and convolutions,

159
00:11:02,760 --> 00:11:05,680
cannot do this at all.

160
00:11:05,680 --> 00:11:10,920
And the reason why is essentially because the EMA has unbounded context.

161
00:11:10,920 --> 00:11:14,600
It's actually just a weighted average of the history of the signal with an exponentially

162
00:11:14,600 --> 00:11:16,440
decaying weight that searches back infinitely.

163
00:11:16,440 --> 00:11:21,520
Whereas in contrast, most modern models such as attention or a convolutions have finite

164
00:11:21,520 --> 00:11:25,000
context in those.

165
00:11:25,000 --> 00:11:27,600
Some people wonder about other things like RNNs.

166
00:11:27,600 --> 00:11:32,800
And the short answer is that RNNs are better than attention convolutions here, but they

167
00:11:32,800 --> 00:11:40,320
still aren't that good due to empirical problems with optimization and other things.

168
00:11:40,320 --> 00:11:48,440
So we'll see that the methods that are introduced in this talk will be very naturally suited

169
00:11:48,440 --> 00:11:51,160
for this and are much stronger versions.

170
00:11:51,160 --> 00:11:55,200
But going back to the EMA, the way that one way to think about it is that it's a very

171
00:11:55,200 --> 00:11:58,720
simple summary of the entire history of your signal.

172
00:11:59,160 --> 00:12:05,600
In other words, it's a state X, which is a single number that summarizes the entire

173
00:12:05,600 --> 00:12:08,720
history of the input U.

174
00:12:08,720 --> 00:12:12,800
And the reason why it's useful is that it's easy to compute because if you get new data,

175
00:12:12,800 --> 00:12:17,360
you can update the EMA in constant time using this weighted average.

176
00:12:17,360 --> 00:12:21,280
And beyond the simple example, though, I think these two properties are actually conceptually

177
00:12:21,280 --> 00:12:23,160
really important.

178
00:12:23,160 --> 00:12:26,520
For example, they're exactly the properties that you need in any sort of real-time decision-making

179
00:12:26,520 --> 00:12:27,520
problem.

180
00:12:27,520 --> 00:12:31,200
And really abstractly, you can even imagine that your brain is a state that's summarizing

181
00:12:31,200 --> 00:12:35,640
the entire context of your life, and it's constantly updating as you acquire new information.

182
00:12:35,640 --> 00:12:40,080
So I think that's actually a pretty general important question, and this was a direct

183
00:12:40,080 --> 00:12:42,840
inspiration for HIPPO.

184
00:12:42,840 --> 00:12:47,280
In the context of machine learning, this question has a lot of direct impact on our models because,

185
00:12:47,280 --> 00:12:49,440
as I mentioned, they struggle with long context.

186
00:12:49,440 --> 00:12:54,280
For example, text models, it's been shown typically have a context range of about 100

187
00:12:54,280 --> 00:12:56,440
to at most a few thousand tokens.

188
00:12:56,440 --> 00:13:02,480
Whereas if you want to deal with data such as speech and audio, a single word in speech

189
00:13:02,480 --> 00:13:07,840
is a sequence of length more than 10,000, and this can really stretch to unbounded length.

190
00:13:07,840 --> 00:13:13,800
And so this is the question that I was trying to, that I was thinking about.

191
00:13:13,800 --> 00:13:18,880
And what I did was I tried to convert this vague goal of long-range memory into a more

192
00:13:18,880 --> 00:13:21,240
formal mathematical question.

193
00:13:21,240 --> 00:13:26,560
The conceptual idea is that if you can compress the past into a smaller state that's accurately

194
00:13:26,560 --> 00:13:32,680
remembering it, then you should be able to reconstruct the past.

195
00:13:32,680 --> 00:13:35,520
And we can then attempt to turn this into a technical problem.

196
00:13:35,520 --> 00:13:39,440
So the idea is that we're going to observe an input signal online and try to maintain

197
00:13:39,440 --> 00:13:43,800
a good representation of it that allows us to reconstruct it.

198
00:13:43,800 --> 00:13:48,000
And so, okay, so first in this section, I'm going to formalize this idea, and then I'll

199
00:13:48,000 --> 00:13:50,080
define HIPPO and visualize it.

200
00:13:50,080 --> 00:13:53,960
And then talk about a couple of generalizations.

201
00:13:53,960 --> 00:13:57,120
So the first thing is that let me formalize this idea that I just mentioned.

202
00:13:57,120 --> 00:14:02,960
And so the idea of HIPPO is that, again, we're trying to observe an input signal online,

203
00:14:02,960 --> 00:14:07,360
and we're going to try to encode it as well as possible given a memory budget.

204
00:14:07,360 --> 00:14:10,640
So concretely, you'd think of it like this.

205
00:14:10,640 --> 00:14:15,520
So suppose at some initial time, T0, we've seen part of the input, and we're going to

206
00:14:15,520 --> 00:14:16,880
try to compress this input.

207
00:14:16,880 --> 00:14:20,720
So what you can do is store the best approximation to what we've seen so far.

208
00:14:20,720 --> 00:14:25,880
For example, we can create the best polynomial approximation and write down the coefficients

209
00:14:25,880 --> 00:14:27,560
of that polynomial.

210
00:14:27,560 --> 00:14:33,120
So now the degree of the polynomial or the number of coefficients is the memory budget.

211
00:14:33,120 --> 00:14:35,400
And we want to do this continuously at all times.

212
00:14:35,400 --> 00:14:42,480
So as we keep seeing more data at some later time T1, we'll have to update our best approximation

213
00:14:42,480 --> 00:14:45,040
and write down the new coefficients.

214
00:14:45,040 --> 00:14:51,600
Now the central question is, first of all, how do you actually find these optimal approximations?

215
00:14:51,600 --> 00:14:58,680
And moreover, how can you update this representation efficiently as you keep seeing more information?

216
00:14:58,680 --> 00:15:04,360
And so this is the main conceptual idea, and it's a little bit of work to formalize a little

217
00:15:04,360 --> 00:15:05,360
more.

218
00:15:05,360 --> 00:15:08,680
And in particular, I've been talking about optimal approximations, but that's actually

219
00:15:08,680 --> 00:15:10,080
not well-defined.

220
00:15:10,080 --> 00:15:14,960
And so what we'll need is to find a measure that specifies the quality of approximation.

221
00:15:14,960 --> 00:15:18,880
For example, we can choose the exponentially decaying measure, which says that we care

222
00:15:18,880 --> 00:15:25,400
about approximating the recent pass of the input more than the far pass.

223
00:15:25,400 --> 00:15:29,120
And this will relate back to the EMA.

224
00:15:29,120 --> 00:15:34,160
But given this, the problem is more or less well-defined.

225
00:15:34,160 --> 00:15:38,600
So basically, we have to pick the measure sort of as a hyperparameter or a prior for

226
00:15:38,600 --> 00:15:39,600
now.

227
00:15:39,600 --> 00:15:42,120
Let's talk about how you can actually learn it.

228
00:15:42,600 --> 00:15:47,200
But for now, we need to pick a measure up front, say the exponential decaying measure.

229
00:15:47,200 --> 00:15:51,040
And then you need to choose a polynomial basis.

230
00:15:51,040 --> 00:15:55,120
And then the problem's completely defined, and you can write down the coefficients in

231
00:15:55,120 --> 00:15:59,000
closed form, and you can figure out how they evolve through time.

232
00:15:59,000 --> 00:16:05,880
So I'm going to skip the details of the derivation, but you end up with a closed form method.

233
00:16:05,880 --> 00:16:10,160
And what I want to emphasize is that the derivation has some technically interesting

234
00:16:10,160 --> 00:16:11,360
new ideas.

235
00:16:11,360 --> 00:16:14,640
But the most interesting and important part of this, I think, is just this simple conceptual

236
00:16:14,640 --> 00:16:19,480
idea of the online compression and reconstruction and how to formalize that mathematically.

237
00:16:19,480 --> 00:16:21,920
So that's the main point.

238
00:16:21,920 --> 00:16:28,320
OK, and now with the definitions out of the way, things will become a lot more clear with

239
00:16:28,320 --> 00:16:31,280
some visualizations of what it does.

240
00:16:31,280 --> 00:16:36,280
So first of all, let me just be really formal about defining what HIPAA is.

241
00:16:36,280 --> 00:16:39,080
So I mentioned the problem was that we are encoding.

242
00:16:39,080 --> 00:16:43,280
So XFT is going to represent a vector of our coefficients at all times.

243
00:16:43,280 --> 00:16:48,240
And the question is, how does this evolve through time as we see more data in the input

244
00:16:48,240 --> 00:16:49,240
U?

245
00:16:49,240 --> 00:16:51,600
And it turns out that it just satisfies a simple differential equation.

246
00:16:51,600 --> 00:16:55,040
By going through the derivation, you can write down this differential equation in closed

247
00:16:55,040 --> 00:17:01,320
form and write down closed form formulas for this transition matrix involved here.

248
00:17:01,320 --> 00:17:07,880
So to be concrete, the ODE is called the HIPAA operator.

249
00:17:07,880 --> 00:17:11,840
And the matrices, the matrix in the operator are called HIPAA matrices, which have closed

250
00:17:11,840 --> 00:17:12,840
form formulas.

251
00:17:12,840 --> 00:17:17,320
In fact, the actual matrix is this matrix.

252
00:17:17,320 --> 00:17:23,400
It's an extremely simple matrix, which is a special type of structure matrix.

253
00:17:23,400 --> 00:17:25,440
And yeah, so it's just a simple formula.

254
00:17:25,440 --> 00:17:28,800
And then we write down a closed form formula for this differential equation.

255
00:17:28,800 --> 00:17:31,920
And that's how our coefficients evolve over time.

256
00:17:31,920 --> 00:17:38,960
And now, right, so this equation, again, is called the HIPAA operator or the high order

257
00:17:38,960 --> 00:17:43,160
polynomial projection operator, because we're projecting on the high degree polynomial basis

258
00:17:43,160 --> 00:17:44,160
functions.

259
00:17:44,160 --> 00:17:48,600
Now visually, the way to think about it is like this.

260
00:17:48,600 --> 00:17:52,400
The reason I call an operator is because it maps a function to a function.

261
00:17:52,400 --> 00:17:57,880
So it's an operator that maps this black input signal U to these sets of coefficients X

262
00:17:57,880 --> 00:18:05,120
and blue, where every time X of t compresses the history of the input signal U.

263
00:18:05,120 --> 00:18:11,160
And you can compute X online as you see one input at a time.

264
00:18:11,160 --> 00:18:13,520
So the black line represents our current time step.

265
00:18:13,520 --> 00:18:15,880
We're gradually seeing more of the input.

266
00:18:15,880 --> 00:18:20,840
And we are updating our coefficient vector, which is depicted in blue.

267
00:18:20,840 --> 00:18:26,040
Here I've, this is visualizing just the lowest order for coefficients of the best polynomial

268
00:18:26,480 --> 00:18:29,720
approximation.

269
00:18:29,720 --> 00:18:33,360
And now here is what the reconstruction looks like.

270
00:18:33,360 --> 00:18:39,760
So as I move along through time and update my coefficients, the coefficients that that

271
00:18:39,760 --> 00:18:44,480
polynomial defines, in a sense, is actually just this red line.

272
00:18:44,480 --> 00:18:49,320
So it is reconstructing the input just like we wanted.

273
00:18:49,320 --> 00:18:53,600
Note that we are using only, so here I've only visualized four coefficients, but I'm

274
00:18:53,600 --> 00:18:57,800
actually using 64 coefficients, but the whole function was linked to 10,000.

275
00:18:57,800 --> 00:19:02,320
So I'm compressing it a lot.

276
00:19:02,320 --> 00:19:08,760
And this, here's a static image that kind of illustrates the effect of the reconstruction.

277
00:19:08,760 --> 00:19:13,720
So because I'm compressing it, I can't perfectly reconstruct the input.

278
00:19:13,720 --> 00:19:15,840
And so how good is the reconstruction then?

279
00:19:15,840 --> 00:19:17,480
Well, it depends on the measure.

280
00:19:17,480 --> 00:19:21,920
So the green, the green line in this figure was the exponentially decaying measure that

281
00:19:21,920 --> 00:19:24,880
we are basically projecting onto.

282
00:19:24,880 --> 00:19:31,760
And so intuitively, you can see that the red reconstruction line is really accurate for

283
00:19:31,760 --> 00:19:36,400
the recent past and degrades further out in history, but still maintains some rough information

284
00:19:36,400 --> 00:19:39,600
about the whole signal.

285
00:19:39,600 --> 00:19:44,080
And so that is, that's hippo.

286
00:19:44,080 --> 00:19:48,760
Now, oh, one, oh, okay, a question here.

287
00:19:49,120 --> 00:19:50,280
No, you can continue.

288
00:19:50,280 --> 00:19:54,320
I just had one clarification, but I can ask after you finish.

289
00:19:54,320 --> 00:19:55,320
Here's fine too.

290
00:19:55,320 --> 00:19:56,320
Okay.

291
00:19:56,320 --> 00:20:01,840
So is it fair to think about X as being the state at each time point?

292
00:20:01,840 --> 00:20:08,560
And then essentially the red line is trying to reconstruct the signal given the current

293
00:20:08,560 --> 00:20:11,920
state or do you also use all the past states to reconstruct?

294
00:20:11,920 --> 00:20:13,280
That's exactly right.

295
00:20:13,280 --> 00:20:18,160
So yeah, so the reconstruction is happening using only the coefficient vector at the current

296
00:20:18,160 --> 00:20:19,320
black line.

297
00:20:19,320 --> 00:20:23,640
So every single time I'm using, I'm for, yeah, the blue line, I'm visualizing the whole

298
00:20:23,640 --> 00:20:27,280
thing, but at any given point in time, I'm remembering only the current vector, which

299
00:20:27,280 --> 00:20:29,880
has length 64.

300
00:20:29,880 --> 00:20:33,400
Here I'm only visualizing four of the components, but it has length 64.

301
00:20:33,400 --> 00:20:37,400
And using those 64 numbers, I'm reconstructing what I've seen so far in red.

302
00:20:37,400 --> 00:20:38,400
Oh, awesome.

303
00:20:38,400 --> 00:20:39,400
Thanks.

304
00:20:39,400 --> 00:20:40,400
Right.

305
00:20:40,400 --> 00:20:46,760
Now, in that previous figure, if I just take one of the blue lines, actually the lowest

306
00:20:46,760 --> 00:20:50,900
order coefficient and overlay over the function, you can see that it actually turns out to

307
00:20:50,900 --> 00:20:52,400
exactly be the EMA.

308
00:20:52,400 --> 00:21:00,200
And so it turns out that moving averages can be viewed as order zero or low order projections.

309
00:21:00,200 --> 00:21:03,360
On the other hand, hippo is essentially a very strong generalization of this that solves

310
00:21:03,360 --> 00:21:11,360
a natural mathematical question and gets back things like the EMA for free.

311
00:21:11,360 --> 00:21:16,520
So that's what hippo is, and now I'll just talk a little bit about some extensions of

312
00:21:16,520 --> 00:21:17,520
it.

313
00:21:17,520 --> 00:21:21,760
So first of all, a natural question that may be wondering is that I've been using this

314
00:21:21,760 --> 00:21:24,680
example of an exponential measure, but what about other cases?

315
00:21:24,680 --> 00:21:28,000
Well, it turns out that hippo can be derived for any measure.

316
00:21:28,000 --> 00:21:32,640
For example, here's a case that is pretty natural as well, which is what if I want to

317
00:21:32,640 --> 00:21:34,160
reconstruct along a uniform measure?

318
00:21:34,160 --> 00:21:41,320
In other words, I only care about remembering the recent past in sliding windows of my function.

319
00:21:41,400 --> 00:21:47,880
And this is possible, so you would get a different ODE, and here's a reconstruction in effect.

320
00:21:47,880 --> 00:21:53,280
So again, using just 64 numbers in memory, I'm trying to reconstruct the last 2,000 time

321
00:21:53,280 --> 00:22:00,080
steps of this function uniformly, and it's doing this quite accurately.

322
00:22:00,080 --> 00:22:04,080
Now you can generalize it even further to, for example, when the measure is changing over

323
00:22:04,080 --> 00:22:08,760
time instead of just sliding along, and so there's a very general framework here that

324
00:22:08,760 --> 00:22:11,600
can do lots of things.

325
00:22:11,600 --> 00:22:14,680
A lot of this was in follow-up work to their general hippo paper, and what we showed was

326
00:22:14,680 --> 00:22:23,080
that for essentially any measure, there exists a corresponding hippo operator where the hippo

327
00:22:23,080 --> 00:22:28,640
matrices A and B depend on the measure, and you can write them down in closed form.

328
00:22:28,640 --> 00:22:32,480
And this is important, I think, because it draws an equivalence between measures and

329
00:22:32,640 --> 00:22:39,520
these ODE's, where this means that we don't, I mentioned earlier that we had to choose the

330
00:22:39,520 --> 00:22:44,480
measure up front as a prior, such as the accidentally decaying case, but actually just by learning

331
00:22:44,480 --> 00:22:51,360
these matrices A and B, it's in some sense the same as learning the measure.

332
00:22:51,360 --> 00:22:57,880
Okay, so now even better, not only do these operators always exist, but it turns out that

333
00:22:57,880 --> 00:22:59,400
the matrices are always structured.

334
00:22:59,800 --> 00:23:05,160
Previously, we saw, for the accidentally decaying case, the matrix was actually extremely simple.

335
00:23:06,040 --> 00:23:09,880
In general, they're going to be more complicated than that, and it's, they satisfy a structure,

336
00:23:09,880 --> 00:23:14,120
which was something that I introduced in much earlier work, but they are all structured in

337
00:23:14,120 --> 00:23:20,280
some way, and that means that you can, how do you actually calculate these updates through time?

338
00:23:20,280 --> 00:23:24,280
You can actually update the state or the coefficients in nearly optimal time.

339
00:23:24,760 --> 00:23:32,680
Okay, so that was the main takeaways from HIPPO, and so just to recap, we were inspired by these

340
00:23:32,680 --> 00:23:37,560
very basic, these simple but important properties of trying to maintain a state that's summarizing

341
00:23:37,560 --> 00:23:43,000
the entire context, and we formalized this into a mathematical problem, which was pretty

342
00:23:43,000 --> 00:23:47,160
intuitive, and we were then able to solve analytically, and this resulted in a nice

343
00:23:47,160 --> 00:23:49,640
class of methods for addressing long context and signals.

344
00:23:49,880 --> 00:23:51,880
Okay, so I see a question in the chat.

345
00:23:54,920 --> 00:23:59,560
So, can these operators be expressed in terms of Z-transforms? I'm not quite sure what you mean here.

346
00:24:01,480 --> 00:24:04,760
To my understanding, Z-transforms are like the discrete version of a Laplace transfer,

347
00:24:04,760 --> 00:24:08,520
and I'm not sure if that's the one you're referring to, or another notion.

348
00:24:11,000 --> 00:24:14,840
Yes, that's what I was thinking about. It seems like as though

349
00:24:15,320 --> 00:24:22,120
just as you can express like exponentially, exponential decay in terms of Z-transforms

350
00:24:22,120 --> 00:24:27,320
of the functions, that there seems like it's likely to be a link.

351
00:24:29,320 --> 00:24:32,760
Yeah, I mean, I think all these things have a tight link, and they're connected to each other.

352
00:24:32,760 --> 00:24:38,200
It turns out actually that the way that in the next part, when I talk about S4,

353
00:24:38,200 --> 00:24:41,640
there's going to be some difficult computations, and I'm not sure if that's the one you're

354
00:24:41,640 --> 00:24:44,280
talking about. In the next part, when I talk about S4, there's going to be some difficult

355
00:24:44,280 --> 00:24:52,760
computational issues for computing certain things that I'll introduce, and to actually compute them,

356
00:24:52,760 --> 00:24:56,760
I essentially actually go through Laplace space or frequency space. So, essentially,

357
00:24:56,760 --> 00:25:02,520
I actually take the Z-transform of this equation and calculate that transform at several values,

358
00:25:02,520 --> 00:25:08,040
and then invert it to get the hippo matrices back, or to get a certain thing back.

359
00:25:08,040 --> 00:25:17,880
Sounds good. That makes sense. Yeah, great question. And yeah, so I wanted also just

360
00:25:17,880 --> 00:25:21,240
to stop around here at the summary for any other questions.

361
00:25:25,880 --> 00:25:30,200
And if there's none, that's great because usually this is a pretty complicated

362
00:25:31,960 --> 00:25:36,040
framework mathematically, but hopefully the visualizations help explain it a lot.

363
00:25:38,120 --> 00:25:47,160
Okay, so I'll move on to the next part where, so one thing I didn't include in this section

364
00:25:47,160 --> 00:25:51,560
was any experiments. So the way we evaluated this is kind of just like how good is the

365
00:25:51,560 --> 00:25:56,680
reconstruction, and actually using this method in machine learning models did pretty well,

366
00:25:56,680 --> 00:26:00,840
just naively, but where it became really effective was when incorporated into a model

367
00:26:00,840 --> 00:26:07,160
in a particular way, and so that's what S4 will be. And so to, first I'm just going to define S4,

368
00:26:08,200 --> 00:26:11,400
and I'm going to define it through hippo, which was the original motivation,

369
00:26:12,920 --> 00:26:16,280
and the motivation here is going to be very simple. So to refresh your memory, this is what

370
00:26:16,280 --> 00:26:20,680
hippo does, it maps an input signal, which in our case they're thinking of as 1D,

371
00:26:20,680 --> 00:26:25,160
to a higher dimensional signal. Now the problem is that we've blown up the dimension of the input

372
00:26:25,160 --> 00:26:32,360
from one dimension to n dimensions, where n was our memory budget or the number of coefficients,

373
00:26:32,360 --> 00:26:42,120
and typically this is going to be at least 100 or so. So the motivation for, so I work in deep

374
00:26:42,120 --> 00:26:47,640
learning, and I just wanted to incorporate hippo into a deep learning model, but this is a problem

375
00:26:47,640 --> 00:26:51,560
because you can't just stack layers of this because you just keep increasing the dimension.

376
00:26:52,120 --> 00:26:57,400
And so a very simple motivation to fix this is just, let's just decrease the dimension again.

377
00:26:58,360 --> 00:27:03,640
And the way to do this is that you can just take a very simple linear projection. So

378
00:27:05,960 --> 00:27:09,400
what we'll do is that we have a state x, which was like a 100 dimensional vector,

379
00:27:09,400 --> 00:27:13,960
and we'll just hit it with a dot product that can be learnable to get back a single number,

380
00:27:13,960 --> 00:27:18,280
which is essentially taking a linear combination of the blue lines to get the final output,

381
00:27:18,280 --> 00:27:23,000
which is the red line. And then we'll add a multiple of the original input, which can be seen as a

382
00:27:23,080 --> 00:27:29,160
skip connection. And that is the entire definition of S4. It's finally these two equations where

383
00:27:29,160 --> 00:27:34,520
the first one is the hippo equation, which takes the input to a state that's kind of memorizing it.

384
00:27:34,520 --> 00:27:40,360
And then the second equation just combines the state linearly into a single output.

385
00:27:41,880 --> 00:27:45,640
Now, for those of you with a background engineering, this definition may look really familiar.

386
00:27:46,920 --> 00:27:51,000
And this is because this is a well-known model called a state space model or SSM,

387
00:27:51,080 --> 00:27:54,520
which is sometimes depicted with this simple control diagram. And they've been around for

388
00:27:54,520 --> 00:28:00,840
decades, such as the famous common filter and use in many scientific fields. I think outside of

389
00:28:00,840 --> 00:28:06,760
controls and statistics, they're also pretty commonly used in perhaps computational neuroscience

390
00:28:06,760 --> 00:28:13,160
and many medical problems as well. Now, what the theme of this part will be is that

391
00:28:14,840 --> 00:28:17,640
we'll see that SSMs are a really elegant and natural model,

392
00:28:18,200 --> 00:28:22,760
but they haven't been used in deep learning before in this way. And for underlying reasons

393
00:28:22,760 --> 00:28:28,200
that we'll see in that S4 address. But for now, just to define S4 in terms of this model,

394
00:28:29,080 --> 00:28:35,080
the way that we'll define it is that it's just an instantiation of an SSM, these two equations,

395
00:28:35,960 --> 00:28:43,960
where we'll plug in specific values of matrices in. And although it turns out that

396
00:28:43,960 --> 00:28:47,800
although this model is simple to define, actually computing with it turns out to be

397
00:28:47,800 --> 00:28:57,880
difficult and will require new ideas and algorithms. And so my goal of this in this section is to

398
00:28:57,880 --> 00:29:05,080
convince you that this is a really elegant and fundamental model. And so first of all, I will

399
00:29:05,080 --> 00:29:10,280
talk about some general properties of SSMs that would have a lot of benefits in machine learning

400
00:29:10,280 --> 00:29:19,000
and deep learning that are independent of S4. And then I'll show how those come with associated

401
00:29:19,000 --> 00:29:23,640
trade-offs that prevent them from being really good in deep learning. And S4 will solve those

402
00:29:23,640 --> 00:29:29,320
problems. And finally, I'll show several real world experiments that show S4's effectiveness

403
00:29:29,320 --> 00:29:37,560
in a bunch of settings. So in this first part, I'm actually going to describe three different

404
00:29:37,560 --> 00:29:41,720
ways to think about SSMs, which give them a lot of nice properties. And this was theory developed

405
00:29:41,720 --> 00:29:47,240
in the predecessor work to S4 that and will have empirical concrete empirical benefits.

406
00:29:47,960 --> 00:29:53,480
And so the first way to the first property is that SSMs inherently operate on continuous time

407
00:29:53,480 --> 00:30:00,040
signals instead of discrete time sequences. So here's how to think about it. So in machine learning,

408
00:30:00,040 --> 00:30:03,320
we usually work with sequence models, which I defined as a parameterized map

409
00:30:03,880 --> 00:30:06,280
from an input sequence to an output sequence.

410
00:30:09,720 --> 00:30:15,640
What if instead of mapping a sequence to a sequence, I coined this term signal model

411
00:30:15,640 --> 00:30:20,120
to denote a parameterized map that maps a function to a function or a signal to a signal.

412
00:30:21,480 --> 00:30:27,400
And given one of these maps, you can essentially discretize the inputs and outputs however you

413
00:30:27,400 --> 00:30:33,880
want to get back a sequence. So essentially, the upshot is that signal models are in some sense

414
00:30:33,880 --> 00:30:38,760
a generalization of sequence models, where they actually map functions and functions,

415
00:30:38,760 --> 00:30:44,760
but by discretizing them, you get back a sequence model. And so the first way to think about SSMs

416
00:30:44,760 --> 00:30:49,960
is that they are just a simple parameterized signal model, where the parameters were matrices A, B,

417
00:30:49,960 --> 00:30:56,600
C, and D, and they map an input function to an output function. That's it. Just in terms of the

418
00:30:56,680 --> 00:31:01,640
interface or the API of the model, that this is what it does. The reason that this property is

419
00:31:01,640 --> 00:31:07,240
important is because even when we're working in discrete time, the model in some sense understands

420
00:31:07,240 --> 00:31:13,080
the underlying continuous domain. So I will show what I mean concretely by this later empirically.

421
00:31:15,800 --> 00:31:20,040
All right, so that's the first representation. The next perspective relates back to the original

422
00:31:20,040 --> 00:31:25,160
motivation of HIPAA, which was about online computation. So how do we actually compute

423
00:31:25,160 --> 00:31:30,520
the output of this SSM? One way to do it is to process the input one at a time, just like HIPAA did

424
00:31:31,240 --> 00:31:36,760
in an online setting. And so this is our current computation because each update can be computed

425
00:31:36,760 --> 00:31:42,920
efficiently from the previous one. And just to unpack a little why this is non-trivial, imagine

426
00:31:42,920 --> 00:31:48,440
we're processing this very long input, and we're at this current time step denoted by the vertical

427
00:31:48,440 --> 00:31:53,080
line, and we get just one more data point. So just like a single number for the input,

428
00:31:53,080 --> 00:31:59,560
and we want to compute the next output. So this output depends on the entire history of the input,

429
00:31:59,560 --> 00:32:04,760
and so you'd expect it, the computation of the next one to scale with the length of the sequence.

430
00:32:05,880 --> 00:32:11,960
But actually we can compute it in constant time. And this is a non-trivial property that most

431
00:32:11,960 --> 00:32:17,080
sequence models don't have. For example, in a transformer or a convolution, if you were to

432
00:32:17,080 --> 00:32:22,200
do this in an online or autoregressive fashion, computing mapping one input to one output,

433
00:32:22,200 --> 00:32:25,240
each computation will scale with the entire length of the context window.

434
00:32:27,160 --> 00:32:30,520
The reason that SSMs can do this so efficiently is because they're stateful,

435
00:32:31,160 --> 00:32:36,360
which is a point that's kept coming up, where in memory we're maintaining a state,

436
00:32:36,360 --> 00:32:40,760
which is the blue thing, which is a single vector that summarizing the history, and can be updated

437
00:32:40,760 --> 00:32:50,520
very efficiently. This makes them really efficient in any sort of online setting, as we've seen.

438
00:32:52,760 --> 00:32:58,760
And yeah, so we'll see again why this matters. But there's one main drawback, which is that if

439
00:32:58,760 --> 00:33:05,080
you're not in an online setting, this is slow because it's sequential. And so what if you

440
00:33:05,080 --> 00:33:09,720
actually know all the future inputs? Then ideally you wouldn't do this step by step,

441
00:33:09,720 --> 00:33:14,840
and you could do something faster and parallelizable. And so that was actually basically the main

442
00:33:14,840 --> 00:33:19,640
problem with RNNs and why they've recently fallen out of favor in machine learning,

443
00:33:19,640 --> 00:33:25,560
because they're sequential and not parallelizable when you see a lot of data at once.

444
00:33:27,320 --> 00:33:30,520
And so that motivates the final representation, which is the convolutional representation,

445
00:33:30,520 --> 00:33:35,480
which allows them to be paralyzed. And so the idea is that instead of mapping going from the input

446
00:33:35,480 --> 00:33:39,720
to the state to the output, you can actually go straight from the input to the output,

447
00:33:40,440 --> 00:33:45,640
bypassing the state, and doing the entire computation in parallel over the sequence length.

448
00:33:46,200 --> 00:33:50,200
The reason is that SSMs turn out to be equivalent to convolutions,

449
00:33:51,560 --> 00:33:57,880
where computing the map from the input U to the output Y is equivalent to convolving the input

450
00:33:57,880 --> 00:34:04,600
by a particular convolution filter, which is depicted in green here. And so to compute this

451
00:34:04,600 --> 00:34:10,760
map, you just do it's just Y equals U convolved with K for this convolution kernel. And so this

452
00:34:10,760 --> 00:34:19,960
can be done very efficiently using no techniques. So for the practitioner, what one thing I want

453
00:34:19,960 --> 00:34:25,240
to emphasize is that I think the most useful way to think about SSMs potentially is as

454
00:34:25,240 --> 00:34:30,440
essentially a very fancy CNN, where you're parameterizing the convolution kernel in a

455
00:34:30,440 --> 00:34:36,840
different way. And notably, this kernel can be infinitely long, which again points to one reason

456
00:34:36,840 --> 00:34:46,680
why this is very good at long range dependencies. So just to call back to this example again,

457
00:34:46,680 --> 00:34:52,120
the EMA, the EMA is actually literally just a single convolution where you can involve the input

458
00:34:52,120 --> 00:34:58,760
by an accidentally decaying convolution kernel. And as I mentioned, although things like CNNs

459
00:34:58,760 --> 00:35:03,880
are also literally convolutions, they can't represent the EMA because CNNs are finite

460
00:35:03,880 --> 00:35:08,840
window and the EMA is infinite window. On the other hand, SSMs do represent infinitely long

461
00:35:08,840 --> 00:35:14,280
convolutions. And in fact, there's a very, very simple way to write down the EMA as a directly

462
00:35:14,280 --> 00:35:24,200
as an SSM. And I think Chris kind of pointed to that earlier. So those were the three properties

463
00:35:24,200 --> 00:35:30,200
of SSMs that I wanted to mention. And just to recap, first of all, we're going to think of them as

464
00:35:30,200 --> 00:35:36,440
maps that operate on continuous signals, not just sequences. If your model is deployed in a

465
00:35:36,440 --> 00:35:40,600
setting where it sees inputs in real time or online, it can compute these efficiently

466
00:35:43,720 --> 00:35:49,240
recurrently. And if you see an entire input at once, such as usually during training time,

467
00:35:49,240 --> 00:35:51,480
you can compute it even more efficiently and in parallel.

468
00:35:54,440 --> 00:35:59,960
I have a quick question here, Albert. This is super cool. I was just wondering if the goal

469
00:35:59,960 --> 00:36:04,040
is actually to get a representation of your signal so that you can perform different

470
00:36:04,040 --> 00:36:09,000
downstream tasks. Isn't it better to actually have the state space representation rather than

471
00:36:09,000 --> 00:36:14,920
directly going to the outputs? In that case, would we have to stick with HIPAA instead of going to S4?

472
00:36:16,440 --> 00:36:20,680
Great question. Actually, no one's asked me that, but that's a great question.

473
00:36:21,400 --> 00:36:31,560
So the way that I think about this is that what's happening is that essentially we have

474
00:36:31,560 --> 00:36:37,560
this nice state, which is very meaningful. And then the second part of the SSM that projects it

475
00:36:37,560 --> 00:36:41,800
is kind of like the learnable thing that's figuring out how to extract the right features

476
00:36:41,800 --> 00:36:47,000
from this state. Now, I mentioned that everything I've done so far, so that's a learnable part

477
00:36:47,000 --> 00:36:52,040
that's actually using the entire state, in a sense. And I mentioned that I'm only considering

478
00:36:52,040 --> 00:36:58,120
the one-dimensional case so far with 1B inputs and outputs. But actually, what's going to happen

479
00:36:58,120 --> 00:37:03,800
in practice in our actual deep learning models is that we'll have multi-dimensional inputs and

480
00:37:03,800 --> 00:37:09,080
outputs, and we'll essentially run an SSM on each one of them. And each one of these will

481
00:37:09,080 --> 00:37:14,120
learn how to use the state in a different way. So we'll have essentially many, you can think of

482
00:37:14,200 --> 00:37:19,240
as maybe we'll have a single state, but many, many possible outputs that are all learnable,

483
00:37:19,240 --> 00:37:23,240
and we'll extract different features from that state. So we are going to get a lot of different

484
00:37:23,240 --> 00:37:30,040
features that utilize the state in however they want. I see. Okay. Thank you. But sorry,

485
00:37:30,040 --> 00:37:37,960
sorry, Joanne. But isn't it like all these dimensions also have a correlation? So do you also,

486
00:37:37,960 --> 00:37:42,680
if you run the space independently, don't you want to also preserve the correlation?

487
00:37:44,840 --> 00:37:47,960
So this is something that I think a lot of people working with time series

488
00:37:49,240 --> 00:37:54,440
are concerned with. And somehow in deep learning, we don't normally consider that aspect. And we

489
00:37:54,440 --> 00:38:00,440
kind of just throw in a really big model and a lot of these independent layers. And kind of,

490
00:38:00,440 --> 00:38:05,480
I think in practice, what usually happens at the model learns to, it learns whatever it needs to do

491
00:38:05,480 --> 00:38:10,600
for the final prediction task. And this often does involve like, I think it does end up

492
00:38:10,600 --> 00:38:15,960
decorrelating things. But it's not super clear exactly the dynamics of what happens. And this is

493
00:38:15,960 --> 00:38:20,520
kind of a more broad question for deep learning theory in general. That's not well understood

494
00:38:20,520 --> 00:38:29,320
right now. What I can say is that we've used this on many types of like noisy data that usually

495
00:38:29,320 --> 00:38:34,280
involve, so I'm going to get to experiments later, but we have tried this on many types of

496
00:38:34,280 --> 00:38:38,040
like time series and other noisy data like EEG. But one day, one day, right?

497
00:38:39,560 --> 00:38:44,360
It can work on multiple dimensions, which I kind of just pointed to you. And also I'll mention

498
00:38:44,360 --> 00:38:50,840
again later how we do that. But yeah, you can just kind of do it do it naively on multiple

499
00:38:50,840 --> 00:38:57,720
dimensions. And it just works out of the box. Okay. Okay, so before we get to the experiments,

500
00:38:57,720 --> 00:39:05,640
I just have a little bit on kind of the how S4 builds on top of SSMs. And so just to refresh

501
00:39:05,640 --> 00:39:11,480
your memory of what S4 is, it's just an SSM where we plug in certain formulas that were

502
00:39:11,480 --> 00:39:14,920
based on the theory of memorization. And we have special algorithms to compute it.

503
00:39:16,760 --> 00:39:23,000
And so first of all, why are these matrices needed? Well, the most important part of the SSM is the

504
00:39:23,080 --> 00:39:33,720
state as Nandita keeps insightfully bringing up. And so what HIPPO did was that it computed a

505
00:39:33,720 --> 00:39:39,560
very particular state that was mathematically meaningful and compresses the history of the input

506
00:39:40,760 --> 00:39:45,640
in a way that captures long range dependencies. And so basically just by plugging in that formula

507
00:39:45,640 --> 00:39:50,360
into this SSM, it learns a more meaningful state that allows the SSM to address long

508
00:39:50,360 --> 00:39:55,880
dependencies better. So just to illustrate this empirically, here's a simple experiment on a very

509
00:39:55,880 --> 00:40:01,640
standard benchmark for sequence models. The actual task doesn't matter, but it's well studied and

510
00:40:01,640 --> 00:40:06,760
standard sequence model based on such as transformers, CNNs and LSTMs all get to around the

511
00:40:06,760 --> 00:40:11,480
around the same accuracy of like 60 ish percent. Now, what happens if we use an SSM?

512
00:40:12,600 --> 00:40:15,960
If you use it naively by randomly initializing all the parameters, which is

513
00:40:15,960 --> 00:40:19,480
what you would typically do in deep learning, it actually does terribly.

514
00:40:21,480 --> 00:40:27,880
But what happens if we just plug in this formula? Plugging this in and not even needing to train

515
00:40:27,880 --> 00:40:33,960
the matrix gives a massive boost to the SSM and goes from much below the baselines to

516
00:40:33,960 --> 00:40:39,000
substantially above the baselines. And actually, I use the very small models for this ablation here,

517
00:40:39,000 --> 00:40:45,480
but the full model as far on this data set gets over 90%, which is something like 20 plus points

518
00:40:45,480 --> 00:40:53,160
better than all other sequence models. So that kind of illustrates why HIPPO is so useful.

519
00:40:54,520 --> 00:41:02,200
Now, quick question in this example. So are both A and B basically just plug in matrices or

520
00:41:02,200 --> 00:41:08,840
is A alone basically a measure? A is the more important matrix, but actually, yeah, just plugging

521
00:41:08,840 --> 00:41:17,160
in A and B essentially just just they're both fixed matrices, which are the HIPPO operators

522
00:41:17,160 --> 00:41:20,680
specifies both of these. I've only illustrated A because it's a more important one. But yeah,

523
00:41:21,640 --> 00:41:25,400
this particular experiment froze both of these matrices to specific ones.

524
00:41:27,560 --> 00:41:31,640
One question people have is that like, do we always freeze these? And actually,

525
00:41:31,640 --> 00:41:37,400
we can train them as well. This was to illustrate just like even freezing them, it does super well.

526
00:41:38,360 --> 00:41:42,680
And then, but in practice, we do train them and it makes it do a little bit better.

527
00:41:45,080 --> 00:41:50,360
Okay, so that was one thing. And that kind of points to I mentioned that SSMs have not been

528
00:41:50,360 --> 00:41:54,920
used in deep learning before in this way. And that's kind of one problem. If you do it naively,

529
00:41:54,920 --> 00:42:00,200
it doesn't work. And so you need this new theory. The second reason is actually that they're

530
00:42:00,200 --> 00:42:06,600
computationally pretty difficult to work with. And so here's the illustrate.

531
00:42:08,360 --> 00:42:12,680
Again, so to remind you, we're thinking of an SSM as a parameterized map from an input signal to

532
00:42:12,680 --> 00:42:17,560
an output signal. And I'll suppose that our input had length L. So our input would just

533
00:42:17,560 --> 00:42:23,480
give us a sequence of L numbers. Then the output of this whole thing is also a sequence of L numbers.

534
00:42:23,480 --> 00:42:27,320
And computing this map ideally takes around O of L time or not too much more.

535
00:42:29,640 --> 00:42:34,040
But here's the problem. SSMs map the input to the output through this state.

536
00:42:34,120 --> 00:42:39,480
And that state gave them a lot of nice properties, but it's also 100 dimensions higher. And so

537
00:42:39,480 --> 00:42:44,520
computing the end to end mapping through the state will take 100 times more computation and memory

538
00:42:44,520 --> 00:42:52,040
than what's needed to compute the final answer. And this is actually a real problem. And now,

539
00:42:52,040 --> 00:42:55,160
earlier I said that you don't actually have to compute the state. You can compute it using a

540
00:42:55,160 --> 00:42:59,720
convolution instead. But what happens is that before computing the convolution, I have to compute

541
00:42:59,720 --> 00:43:04,840
the kernel or the convolution filter in green. And computing that is just as slow as computing the

542
00:43:04,840 --> 00:43:11,320
state. And this sort of makes sense because it hasn't changed the computational hardness of the

543
00:43:11,320 --> 00:43:17,400
problem. So essentially computing it no matter how you do it is going to be slow and memory inefficient.

544
00:43:19,080 --> 00:43:24,600
So the main point of S4 was showing that you could substantially reduce this computation

545
00:43:24,600 --> 00:43:28,200
when the SSM is structured. And for example, when using the

546
00:43:29,240 --> 00:43:33,480
hippo matrix instead of an unstructured matrix, you can save this factor of 100

547
00:43:34,040 --> 00:43:41,320
and make S4 overall extremely efficient. So this is done through a particular algorithm,

548
00:43:41,320 --> 00:43:46,440
which I'll just flash up. But basically we're trying to work with this SSM, but we only need

549
00:43:46,440 --> 00:43:51,800
to work with specific structured cases such as this, such as some particular hippo matrices.

550
00:43:52,520 --> 00:43:58,760
And now using some algorithmic ideas, it turns out there is a way to compute the convolution

551
00:43:58,760 --> 00:44:04,920
kernel, which was depicted in green before, very efficiently. And then compute the whole thing using

552
00:44:04,920 --> 00:44:11,560
a convolution. So I won't go into details here. And I will also mention that recently we've been

553
00:44:11,560 --> 00:44:15,720
developing simplifications of the model that allow you to bypass all of this and do things much more

554
00:44:15,720 --> 00:44:22,120
simply. So hopefully in a few weeks we'll have some stuff out that's where you don't need to

555
00:44:22,120 --> 00:44:29,320
worry about this really complicated algorithm. All right, so that was the technical portion of

556
00:44:29,880 --> 00:44:32,840
that I wanted to mention for S4. And I'll stop here for questions as well.

557
00:44:36,440 --> 00:44:44,840
So if in any case you want to actually get the state, can S4 actually recover the state?

558
00:44:45,880 --> 00:44:52,680
Or is it like, yeah, so like, I don't know if that would be any use case.

559
00:44:52,680 --> 00:44:56,600
But like I was saying, like, if there's a case where I actually want the state,

560
00:44:56,600 --> 00:45:01,960
can I do that and get the convolution? Yes, you can. And in fact, that will be used in

561
00:45:02,920 --> 00:45:07,720
some experiments. I guess I didn't mention explicitly, but you can compute it in either way,

562
00:45:07,720 --> 00:45:13,240
either through the convolution or through the state. And where the state or the convolution is

563
00:45:13,240 --> 00:45:18,920
useful is during training time for parallelizability. But where the state is useful is at some sort of

564
00:45:20,200 --> 00:45:25,000
inference or deployment settings, where perhaps you might be online, and then you would actually

565
00:45:25,000 --> 00:45:28,760
be going through the state instead of the convolution and unrolling things one step at a time.

566
00:45:29,720 --> 00:45:35,480
Right. So you can do it either way, which is pretty cool. Thanks.

567
00:45:36,360 --> 00:45:43,640
I had more of a thought question. Let's say I'm interested in two different measures. Like,

568
00:45:43,640 --> 00:45:49,400
I want to see how the exponential average works, but I also want like, so is it,

569
00:45:50,200 --> 00:45:54,760
does it basically mean that I just have to create a new measure that combines this efficiently before

570
00:45:54,760 --> 00:46:01,960
this? Before I plug it into SSM or can S4 basically kind of,

571
00:46:02,920 --> 00:46:06,360
because there are two independent blocks that I can basically...

572
00:46:08,120 --> 00:46:11,320
Yeah. So I'm just about to get to the experiments. And actually, I will,

573
00:46:12,440 --> 00:46:17,400
I'll get to that slide right now, where, so first of all, the experiments will be on this type of

574
00:46:17,400 --> 00:46:24,520
signal data. And what, as I mentioned a couple of times, what we actually do is that I have to

575
00:46:24,520 --> 00:46:30,120
find this 1D to 1D map, but I'm actually going to just like, given a multidimensional input,

576
00:46:30,120 --> 00:46:35,320
I'm just going to stack a bunch of copies of this. And now as a parallel to that, you can do

577
00:46:35,320 --> 00:46:39,640
many things with these copies. So to answer your question, one thing that I've been starting to

578
00:46:39,640 --> 00:46:44,520
experiment with is just using different measures or essentially different A and B matrices for

579
00:46:44,520 --> 00:46:49,880
every copy. And that can, and so that sort of has interpretation of using multiple measures.

580
00:46:51,000 --> 00:46:56,440
I see. Because when Iman actually talked about the correlations between different dimensions,

581
00:46:57,080 --> 00:47:00,920
let's say you have an image, like two different pixels are actually correlated.

582
00:47:00,920 --> 00:47:04,600
So I was thinking like, you can have a measure that captures this correlation,

583
00:47:04,600 --> 00:47:08,360
but you can have another measure that captures it over time.

584
00:47:09,960 --> 00:47:14,360
Another thing actually, since you mentioned that, I don't know if you tried that on image

585
00:47:14,360 --> 00:47:19,240
space, I would be curious like if this kind of like long convolution actually makes any difference

586
00:47:19,240 --> 00:47:25,160
with the image space. Because image usually like, when we do the image analysis theoretically,

587
00:47:25,160 --> 00:47:29,640
when we start thinking about it, it seems that like also the local feature as well as of course

588
00:47:29,640 --> 00:47:35,320
the global feature is important. But I don't know, like if we are missing any local features by just

589
00:47:35,320 --> 00:47:43,160
using this kind of like long representation. That's a good question. I actually, we have started

590
00:47:43,160 --> 00:47:47,880
doing more experiments on images, which I didn't include in this talk, but luckily we do find that

591
00:47:48,520 --> 00:47:55,080
the local bias of convolutions does seem pretty good. I don't know, it's hard to quantify if

592
00:47:55,080 --> 00:48:00,120
we're missing features, but I think there are settings where we're not, we're only on power

593
00:48:00,120 --> 00:48:07,240
or not, or maybe a little bit worse than a standard local CNN. It is hard to say. I will

594
00:48:07,240 --> 00:48:14,600
mention though that you can forcibly incorporate locality into this just by changing the measure.

595
00:48:14,600 --> 00:48:18,920
For example, if you choose a uniform measure that has a short window, that's the same as saying

596
00:48:18,920 --> 00:48:24,280
I would just want a local convolution kernel. Because I would imagine like for this particular

597
00:48:25,240 --> 00:48:30,120
thing, like the use case where we have to have to work with a very high resolution image data,

598
00:48:31,000 --> 00:48:34,920
you know, for example, like imagine like mammogram, right? Like we have to go with like

599
00:48:34,920 --> 00:48:41,160
1000 by 1000 minimum dimension. So for this probably would be useful because they are actually,

600
00:48:41,160 --> 00:48:45,960
we want to like do the rescaling, but we cannot because we'll lose probably a lot of features

601
00:48:45,960 --> 00:48:49,880
in the middle. But this kind of like long convolution could this, this is a perfect

602
00:48:49,880 --> 00:48:54,920
problem that I will actually, I wasn't going to, but now I'll mention this in the experiments as

603
00:48:54,920 --> 00:49:01,480
well. Okay. It's actually something that we have thought about basically rescaling of convolutions

604
00:49:01,480 --> 00:49:08,200
and using. Right. Okay, I'll get to that. Before that, so I want to get the experiments and

605
00:49:08,200 --> 00:49:12,040
basically I just wanted to find, I'm only to find the simple linear one either one D map,

606
00:49:12,040 --> 00:49:16,360
but you can just do it in parallel across a lot of features and then plug it into a standard neural

607
00:49:16,360 --> 00:49:23,480
network to do sequence modeling. So the first type of data I'll see is a biosignal data.

608
00:49:26,840 --> 00:49:34,760
So here is a, there's a real world data set of trying to predict vital signs such as heart rate

609
00:49:34,760 --> 00:49:42,440
from raw biosignal data such as I wrote EKG and EG here, but I think it's actually EKG and PPG.

610
00:49:43,000 --> 00:49:46,920
And so that's visualized here. And this data is pretty challenging for deep learning models

611
00:49:46,920 --> 00:49:52,840
because you can see that it's very long. This is a sequence of like 4000. If you zoom in a lot,

612
00:49:52,840 --> 00:49:55,880
it would be pretty smooth actually, but if you zoom out, it displays a lot of

613
00:49:55,880 --> 00:50:01,560
periodicity and spikes and other things. And so a lot of methods have been tried on this data set,

614
00:50:01,640 --> 00:50:07,320
which include kind of standard machine learning techniques like XGBoost as well as many very

615
00:50:07,320 --> 00:50:13,320
modern deep learning sequence models. And S4 substantially improves over all of these in

616
00:50:14,600 --> 00:50:18,360
I think cutting the root mean squared error by at least two thirds on all of these targets

617
00:50:20,280 --> 00:50:23,240
just with that generic deep learning model that deep model that I showed.

618
00:50:24,680 --> 00:50:29,240
Actually I've, these were like older numbers and recently I've been rerunning these again and

619
00:50:29,240 --> 00:50:37,480
actually you can drop this down even more. One thing I will note is that attention and

620
00:50:37,480 --> 00:50:43,080
transformers does really poorly on this type of data. And that's something that I think I found

621
00:50:43,080 --> 00:50:48,600
pretty consistently. So there's some sort of bias toward what type of data you have and S4 is really

622
00:50:48,600 --> 00:50:53,240
good at signals and attention is not. Conversely, attention is good at some other types of discrete

623
00:50:53,240 --> 00:51:00,680
data that S4 is not as good at. Okay, so that's, that was one experiment. The next one is two time

624
00:51:00,680 --> 00:51:07,880
series data where we did a forecasting task where you're given a context window and you want to

625
00:51:07,880 --> 00:51:12,120
predict future values. Actually, I'm going to go through this kind of fast because I don't want

626
00:51:12,120 --> 00:51:15,880
that much time I want to get through some more of the bio applications and the things that you

627
00:51:15,880 --> 00:51:22,360
guys brought up. The models here are very complicated. Whereas for S4, we're actually

628
00:51:22,360 --> 00:51:27,480
doing an extremely simple setup, which is just a mask prediction. We're just going to give you,

629
00:51:27,480 --> 00:51:31,640
we're going to take the entire sequence and mask out the desired forecast range and then just

630
00:51:33,000 --> 00:51:39,080
predict what's in the mask by passing it through this generic deep model. So this is really,

631
00:51:39,080 --> 00:51:44,520
it's like a very extremely simple application. I won't unpack the numbers too much, but there's a

632
00:51:44,520 --> 00:51:50,920
lot of baselines here, including time series models, LSTMs, lots of transformers, and S4

633
00:51:50,920 --> 00:51:54,840
does better than all of them on these real time series data sets, including weather and energy

634
00:51:54,840 --> 00:52:02,840
data with much less specialization. These models were all designed for time series and we were

635
00:52:02,840 --> 00:52:08,280
just using our generic model. And you didn't even like tune the window size, right?

636
00:52:09,640 --> 00:52:14,280
We did not for this one. Actually, by tuning the window size, you can get the numbers down even

637
00:52:14,280 --> 00:52:24,760
more. Okay. Okay. The next one here points to Amon's question about rescaling. So it's actually,

638
00:52:24,760 --> 00:52:31,800
I'm going to display this to audio. But essentially, I've used audio a few times. I'm running example.

639
00:52:32,520 --> 00:52:38,840
It's sampled at extremely high rate. And it's extremely long. So this is a data set of classifying

640
00:52:38,840 --> 00:52:45,000
one second speech clips, which were length 16,000, into classifying the words. And most

641
00:52:45,000 --> 00:52:50,760
sequence models like transformers and RNNs are really bad here. The only thing that works is CNNs,

642
00:52:51,320 --> 00:52:58,760
which the red line is pointing to a speech CNN baseline. And these work okay. But what happens

643
00:52:58,760 --> 00:53:03,480
if you are resampling the signal at different frequencies? And this happens commonly in audio

644
00:53:03,480 --> 00:53:08,760
because your signal can be sampled at any rate and sound more or less the same. So for example,

645
00:53:08,840 --> 00:53:14,120
this orange sequence is a sequence of samples, but it's actually the same underlying signal

646
00:53:14,120 --> 00:53:18,360
as the original blue sequence of samples, just at a different frequency. And so it's ideal if

647
00:53:18,360 --> 00:53:24,360
the same model works on both of them. But standard models like CNNs cannot do this,

648
00:53:25,160 --> 00:53:31,160
essentially because of the local bias that was brought up earlier. I won't unpack this here,

649
00:53:31,160 --> 00:53:36,360
but if you use like a standard local CNN, it will break at a different frequency. However,

650
00:53:36,360 --> 00:53:41,240
by using a signal model such as S4, which is actually understanding the underlying continuous

651
00:53:41,240 --> 00:53:46,440
domain or the underlying continuous function, it can work here without modification. So this is

652
00:53:46,440 --> 00:53:51,560
all in a zero shot setting where it's trained at one resolution and tested on a different resolution.

653
00:53:52,440 --> 00:53:59,080
And this breaks a CNN, but S4 can do it out of the box. And that's because of this first property

654
00:53:59,080 --> 00:54:06,200
of being a continuous time model. And now the last two things I'll show are just calling back to

655
00:54:06,200 --> 00:54:13,160
the experiments at very beginning. I showed some audio generation clips. And that was an

656
00:54:13,160 --> 00:54:17,560
autoregressive setting where we're generating things one sample at a time. And despite having

657
00:54:17,560 --> 00:54:23,320
an extremely large context window, which made it do better and more coherent, we could still

658
00:54:23,320 --> 00:54:27,960
sample things autoregressively just as fast as other autoregressive models. And that's because

659
00:54:27,960 --> 00:54:33,480
of the fast online or autoregressive representation where you're computing the state and updating it

660
00:54:33,480 --> 00:54:39,560
every time. And finally, I showed this benchmark of long range modeling where S4 substantially

661
00:54:39,560 --> 00:54:45,480
outperforms other models on a range of different tasks. And this benchmark was also used to

662
00:54:45,480 --> 00:54:51,960
benchmark the speed of models during training where S4 is just as fast as all of these

663
00:54:51,960 --> 00:54:56,600
efficient transformer variants. And that's because of the efficient, paralyzable view

664
00:54:57,240 --> 00:55:03,800
along with the new algorithms we introduced. And so all these properties, as I promised,

665
00:55:03,800 --> 00:55:08,600
have concrete empirical benefits. Now for, I'm running out of time, so I just want to get to

666
00:55:08,600 --> 00:55:14,520
a couple more things. For the last part, I just wanted to, for this audience, I wanted to point to

667
00:55:14,520 --> 00:55:19,640
where I hope that this model will be useful, which is as a general tool for deep learning for

668
00:55:19,640 --> 00:55:26,360
biosignals. And I've pointed out one, one example of a data set already where we were predicting

669
00:55:26,360 --> 00:55:32,280
like heart rate from EKG signals. But this was another one that CE was working on actually and

670
00:55:33,240 --> 00:55:40,120
her and another lab mate have been trying to test S4 here, where this is a data set of raw

671
00:55:40,120 --> 00:55:46,360
EEG signals that are difficult to process because they're so noisy and long. And the state-of-the-art

672
00:55:46,360 --> 00:55:53,560
models are very recent. CE's model from a couple months ago was there they are on one of these

673
00:55:53,560 --> 00:55:58,840
EEG data sets, but it was quite involved and involved a lot of domain knowledge, such as even

674
00:55:58,840 --> 00:56:04,040
like the placement of the electrodes and a lot of different parts, components of the model.

675
00:56:04,600 --> 00:56:10,440
And so where I hope that S4 could be useful is as a generic tool or building block for

676
00:56:11,320 --> 00:56:20,520
for addressing these types of signal data without as much domain expertise and how to design the

677
00:56:20,520 --> 00:56:28,520
model. And so CE and Collid have been running some preliminary experiments using S4 on this data,

678
00:56:28,520 --> 00:56:34,520
where we don't even need to process the, you don't need to pre-process it with FFT features,

679
00:56:34,520 --> 00:56:39,640
you don't need to do a lot of these other things and just run it through these, a generic deep

680
00:56:39,640 --> 00:56:45,480
model composed of S4 layers. And Collid found some very preliminary results where it is improving

681
00:56:45,480 --> 00:56:53,080
over the baselines in some settings. This is still very preliminary, so it's not, there's other

682
00:56:53,080 --> 00:56:57,960
settings that we care about, such as incorporating self-supervision and so on, where it's not quite

683
00:56:57,960 --> 00:57:05,960
there, but I do think it has a lot of potential in this type of domain. Another example, actually

684
00:57:06,040 --> 00:57:11,080
that was published was another recent collaboration with Stanford Medicine that was submitted to a

685
00:57:11,080 --> 00:57:17,960
gastroenterology journal on detecting acid reflux from impotence sensor data. And so again,

686
00:57:17,960 --> 00:57:27,480
S4 was really good on that type of prediction task. So that is all I was going to talk about

687
00:57:27,960 --> 00:57:35,960
for this. So just to review, S4 is a, it's an SSM, which are these two equations,

688
00:57:35,960 --> 00:57:39,560
where we plug in certain formulas and have special algorithms to compute the model.

689
00:57:40,600 --> 00:57:46,280
And overall, SSMs and in particular, S4 have a number of very nice properties with

690
00:57:46,280 --> 00:57:50,920
concrete empirical benefits, as we saw, and I think can become a very effective building block for

691
00:57:50,920 --> 00:57:56,440
modeling many types of sequential data in the future. Thanks for listening and thanks for all

692
00:57:56,440 --> 00:58:03,000
the collaborators for the hard work. This slide lists a couple of resources, such as blog posts

693
00:58:03,000 --> 00:58:09,560
and related papers, as well as the audio results from an ongoing, a paper that's under submission

694
00:58:09,560 --> 00:58:15,800
right now. Feel free to reach out if you have questions and thanks. This was my last slide,

695
00:58:15,800 --> 00:58:20,680
but because you want to ask how I will, I guess I'm technically out of time, so of course people

696
00:58:20,680 --> 00:58:25,960
feel free to leave, but if you want to stay, I can show one thing about the high resolution images

697
00:58:25,960 --> 00:58:34,920
that that was brought up. Let me find that slide. Yeah, if people have conflicts, feel free to leave

698
00:58:34,920 --> 00:58:41,160
and we will put up the recording of the talk later in our YouTube channel. Otherwise, if you

699
00:58:41,160 --> 00:58:47,160
would like to stay, then yeah, I'll be able to share the slide. So yeah, I'll just really quickly go

700
00:58:47,160 --> 00:58:56,600
over this where medical, medical imaging is something that we think could be a potential

701
00:58:57,400 --> 00:59:06,680
strong use case for S4 because of this high resolution feature where, so this slide was

702
00:59:06,680 --> 00:59:10,600
about, I was moving from a different way, but the point I wanted to make was that

703
00:59:11,320 --> 00:59:15,320
Can you go to presentation mode? I think we are still seeing your screen nose.

704
00:59:16,120 --> 00:59:24,200
Yeah, sorry. Oops. Am I showing my whole screen? No, we are seeing your screen rather than the

705
00:59:24,200 --> 00:59:33,400
presentation. Okay, I thought I had it on. You can just swap the view. I thought I had it on the right

706
00:59:33,960 --> 00:59:42,840
view. Is this one still show the, oops. Yeah, I can see the high definition view.

707
00:59:43,880 --> 00:59:48,600
Okay, great. So yeah, so the point I was making is that normally image data sets are things like

708
00:59:48,600 --> 00:59:52,760
ImageNet, which are actually extremely low resolution compared to other data that we might

709
00:59:52,760 --> 01:00:00,280
find such as medical imaging where apparently the images can be up to 100,000 by 100,000 pixels.

710
01:00:00,600 --> 01:00:05,400
And this is obviously like way too big for current models, which can only operate on small

711
01:00:06,040 --> 01:00:11,000
patches at a time. So I don't know how to address this really, but it's something that fascinates

712
01:00:11,000 --> 01:00:16,760
me. But just to point out, this is part of a longer drop pack where I pointed some potential

713
01:00:16,760 --> 01:00:22,200
future directions. The one that I'll mention here relates to some things that we brought up,

714
01:00:22,200 --> 01:00:27,800
which is that just like the speech experiment that I showed, I believe that S4 should work

715
01:00:27,880 --> 01:00:32,920
training on images at different resolutions. And so what you can do is essentially try to

716
01:00:34,120 --> 01:00:40,440
train on lower dimensional versions of the image, lower resolution versions, and then transfer

717
01:00:40,440 --> 01:00:44,920
the same model to work on high dimensions, which is a very similar thing that I showed

718
01:00:44,920 --> 01:00:53,320
for the speech example. So yeah, I think that's potentially something that could work. And the

719
01:00:53,320 --> 01:00:57,800
point is that a signal model like S4 will work at different resolutions

720
01:01:00,280 --> 01:01:05,480
because you can sample at different rates essentially. And yeah, so what you need is a

721
01:01:05,480 --> 01:01:10,200
signal model that understands the continuous domain, just like the example I showed. And that

722
01:01:10,200 --> 01:01:14,280
points to this property again. So this is something where we haven't tried it and I don't know if it

723
01:01:14,280 --> 01:01:19,320
works, but it's some part of me feels like it might be the right way to or one potential

724
01:01:19,960 --> 01:01:23,880
good way to approach this type of problem. But I would think in the opposite way,

725
01:01:25,000 --> 01:01:29,160
it's not really we want to generate the high resolution from the low resolution,

726
01:01:29,160 --> 01:01:33,080
but I would imagine since you have this kind of like state-based representation and

727
01:01:33,080 --> 01:01:37,240
finally you're getting this signal, I would imagine like in some case always like we had

728
01:01:37,240 --> 01:01:41,800
to deal with this kind of situation that we had a very high resolution image. And before running

729
01:01:41,800 --> 01:01:47,320
through the convolution because of the memory computations, like computational complexity,

730
01:01:47,400 --> 01:01:51,560
memory complexity and all this kind of thing. So you have to reskill the image into a much lower

731
01:01:51,560 --> 01:01:59,080
dimension. Yeah. And we had a chance of losing a lot of features, specifically histopathology,

732
01:01:59,080 --> 01:02:02,680
exactly the example that you showed, or the mammogram, those kind of images, you know.

733
01:02:03,480 --> 01:02:08,520
Yeah, I see. So I was thinking that perhaps like what you can do is kind of like iteratively

734
01:02:09,720 --> 01:02:13,240
increase the resolution and pick up higher and higher resolution features as you go.

735
01:02:13,800 --> 01:02:18,920
But the benefit is that perhaps you can pick up the coarser grain things and then as you

736
01:02:18,920 --> 01:02:24,440
upsize the image then and you rescale your kernel essentially, then it's already going to be doing

737
01:02:24,440 --> 01:02:28,920
as everybody knows the coarse grain features, but then it as you keep training, it only has

738
01:02:28,920 --> 01:02:35,000
to learn the higher frequency features as you go. And again, I have no idea if this is,

739
01:02:35,000 --> 01:02:40,040
if this makes sense or it's promising, but it sounds pretty interesting. I think,

740
01:02:40,040 --> 01:02:46,680
I think Alba's idea is actually very similar to how pathologists analyze our hostile images.

741
01:02:46,680 --> 01:02:51,960
So they usually look at. Define zoom. Define zoom. They usually look at low resolution

742
01:02:52,520 --> 01:02:59,320
image first and localize like the potential areas where the tumor are and then they zoom

743
01:02:59,320 --> 01:03:05,960
into higher resolution. Right. Yeah. I see. Yeah. And so yeah, I don't know if this will be better

744
01:03:05,960 --> 01:03:10,920
than CNN or other things, but it definitely has different and interesting properties.

745
01:03:13,400 --> 01:03:20,840
Okay. So that was, yeah, I think that's the end of the material I have. I can say around a few

746
01:03:20,840 --> 01:03:25,800
more minutes if people still have questions. All right. Great. Thanks so much, Albert.

747
01:03:25,800 --> 01:03:30,120
I have just one question. Sure. Yeah. Thank you for the presentation. That was awesome.

748
01:03:31,000 --> 01:03:36,440
Did you find any scenario where it's better to use transformer than S4?

749
01:03:37,240 --> 01:03:42,760
Yes. Another great question. So let me just share my screen again. I have one slide prepared for

750
01:03:42,760 --> 01:03:51,000
that. Basically at the beginning, I drew this distinction between continuous kind of continuous

751
01:03:51,000 --> 01:03:58,920
and discrete data. And I think that S4 will be the best or like the ideas involved are potentially

752
01:03:59,320 --> 01:04:04,280
going to be the best thing to do for signals. But for kind of higher level concepts or more

753
01:04:04,280 --> 01:04:09,240
discrete concepts such as language or some other things, transformers, I think that's

754
01:04:09,240 --> 01:04:19,240
where transformers really shine and are probably going to be better. So here's a,

755
01:04:20,200 --> 01:04:25,800
I don't know if this is the right screen again. Sorry. Anyways, here's the one slide on language

756
01:04:25,800 --> 01:04:31,080
modeling where we took a transformer, which are currently of course the best models for

757
01:04:31,080 --> 01:04:38,040
text and NLP. And we replaced the attention with S4 and found that it doesn't do quite as well.

758
01:04:38,840 --> 01:04:42,840
But it is still better than all non, it's significantly better than all non-transformer

759
01:04:42,840 --> 01:04:50,200
models. And it also has some other benefits. Like you can do language generation much faster

760
01:04:50,200 --> 01:04:55,000
because of the fast recurrent view. That was the main point of this. But this also does

761
01:04:55,000 --> 01:04:59,720
kind of point to the fact that personally my intuition is that transformers are really good

762
01:04:59,720 --> 01:05:05,160
for dense and discrete data. Whereas S4 is really good for more like noisy and raw data.

763
01:05:06,440 --> 01:05:11,720
Yeah. And I mean the speed up here I think is very interesting. Do you know what was the window

764
01:05:11,720 --> 01:05:17,320
you would take for language modeling? Like how many tokens or words rather did you consider?

765
01:05:17,320 --> 01:05:22,280
Yeah. This experiment was done using a pretty standard length of either 512 or 1024 tokens.

766
01:05:23,160 --> 01:05:27,560
You actually can keep increasing the window length for S4, which only slows it down a little

767
01:05:27,560 --> 01:05:32,440
bit and actually improves the performance a little bit as well. But I found that out after

768
01:05:32,440 --> 01:05:38,520
the fact and I didn't feel like retraining this. Okay. Cool. Thanks. So yeah, but the

769
01:05:38,520 --> 01:05:41,960
sort of findings of this slide is the speed up, right? It's massive.

770
01:05:43,080 --> 01:05:48,920
That was the point that we did this experiment for. But yeah, so there's a lot of the speed up.

771
01:05:48,920 --> 01:05:52,680
In terms of the original question though, in terms of the raw performance of modeling the data,

772
01:05:53,320 --> 01:05:57,480
transformers are currently doing a little bit better here. Cool. Thank you.

773
01:06:03,160 --> 01:06:04,680
All right. Is there any other questions?

774
01:06:07,960 --> 01:06:15,000
Let's all give Albert a round of virtual applause. Thank you for the very comprehensive

775
01:06:15,560 --> 01:06:23,080
presentation of state-based models. Thanks for having me. Thank you, Albert.

776
01:06:23,080 --> 01:06:27,480
Thank you. Thank you, everyone, for joining us. We will put up the recording of the video,

777
01:06:27,480 --> 01:06:32,280
the talk, later to our YouTube channel. And yeah, we'll see you at the same time next week.

778
01:06:33,480 --> 01:06:34,840
Thank you. See you, guys.

