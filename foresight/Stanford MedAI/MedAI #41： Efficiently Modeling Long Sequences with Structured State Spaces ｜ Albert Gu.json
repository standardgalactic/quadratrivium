{"text": " Okay, hi everyone, welcome to our 41st session of the May AI Group Exchange. This week we have Elba Gu from Stanford here with us to present his research on efficiently modeling long sequences with structured state spaces. Elba is a final year PhD candidate in the computer science department here at Stanford University, advised by Chris Ray. He's brought the interest in studying structured representations for advanced signal capabilities of machine learning and deep learning models with focuses on structured linear algebra, non-euclidean representations, and theory of sequence models. Thank you so much Elba for joining us today. Before we start, do you have any preference on how you want to take questions? Yeah, thank you, thank you for an introduction. For this talk, I think I'm not sure usually the level of formality, but I'm very happy to have the casual in terms of the conversation and the questions. I think there's some time, it's not going to be a full hour talk, so I'm more than happy to take questions during it and I'll watch the time in case it gets too long. And then I'll also pause a few times to pause for potential questions during some sections. Okay, sounds good. Let's try to make this session as interactive as possible. How further or do let me hand it over to Albert? Thank you. All right, so this talk will be about a new sequence model called S4, or structured state spaces. Now, for the purposes of this talk, when I mentioned sequence models, we will think of them as a black box sequence-to-sequence map composed of primitive layers, where each layer simply takes an input sequence and returns a sequence of the same shape. For our purposes right now, we'll think of them as just being a one-dimensional to one-dimensional map, but this can be easily converted to higher-dimensional features. Many sequence models have been developed that satisfy this interface, particularly in the context of deep learning. These include many classical deep learning models, such as recurrent neural networks or RNNs and convolutional neural networks or CNNs, as well as many more modern models, such as transformers or neural ODEs. And all of these models kind of satisfy the same interface. They map a sequence to a sequence of the same shape, or meaning the same length and field dimension. And then you can incorporate any of these into a deep learning model fairly easily just by using standard architectures, where you can include normalization layers, other linear or nonlinear activations, as well as with the dual connections. And so the core component of all of this is the core sequence model, and that's what we'll focus on. And this generic deep neural architecture based on sequence models can be used to solve many types of problems with many types of sequence data, from medallies such as text and audio to images and videos to general time series data or biosignals, for example, which is depicted here. In this talk, I'm going to draw a very rough distinction between different types of sequence data. Now much of modern sequence modeling in the context of machine learning focuses on data such as text. And very roughly I'll classify this as being a discrete sequence because the input comes in the form of discrete tokens. And other types of data like this includes things like graphs or things like DNA-based pairs. In contrast, what this talk will focus on is data that's roughly more continuous, things such as video or time series or audio. And what's common to all of these is that there's an underlying notion of time from which the sort of data is sampled from. And so I'm going to very broadly call this type of data signal data as opposed to sequence data. And roughly speaking, signals can be defined as data that's generated from an underlying continuous physical process, including all these examples here. This talk will be composed of two parts. The first part covers a method called HIPPO, which was the predecessor to S4. And it's a new conceptual framework for the online memorization of signals and led to a new method for modeling signals and sequences. And then the second part will be S4, which built right on top of HIPPO. And it has a lot of important properties that have been very effective for addressing some types of sequence modeling problems. And before I get into the technical stuff, I'll give a quick preview experimental results to highlight the types of improvements I will see and what it's good at. And this will kind of illustrate the types of challenges that we'll hope to address with these new models. The first challenge overall is just going to be to signal or general temporal data that I just defined. And this data is really everywhere. So some examples include audio waveforms, spatial temporal data like videos, biosignals like electrocardiograms, which have important applications of medicine, or market and financial data. And then there will be multiple time series logs being generated by every major industry and many other types of scientific modeling problems. And we'll return to these experiments later with a particular focus on some biosignal data. But for now, I will just use one example to illustrate, which is audio. And audio is actually one of the most common types of data because it's just raw sound. It's everywhere. And so to illustrate, machine learning right now is really all about text and so many headline results recently have been about people scraping together all the raw text data they can get, creating massive models on them. And that's led to very impressive results like GPT-3, which I don't know the audience, but hopefully many of you have heard of this model. In contrast, audio actually has orders of magnitude more data than text. For example, a single labeled dataset has more data set than all of the data used to train those massive language models. But you don't hear about benchmarks in this domain nearly as much. And I think part of the reason is just because audio models are, audio is very challenging and current models seem much worse in comparison to text. And so here's a concrete example where we consider basically a very general and hard audio generation setting of generating spoken digits, zero to nine, using a completely unconditional autoregressive model. And the gold standard here is a baseline called WaveNet. And here's what it sounds like trying to say these numbers. So it's, it's not very good. And here's results for S4, which was just, these results are just from the past like two months or so ago. One, two, three, four. So that's, that's a pretty concrete example. And so in this talk, we'll see how models like S4 are kind of designed for signals in a way and can have significant advantages for this type of data. And the second example up front, or the second example of a running challenge will be, can be motivated by examining audio more closely. And one reason why audio is so hard is because it's sampled at such an extremely high rate where a single second has 16,000 or more samples. In contrast, most sequence models can't deal with more than a thousand or so. And to illustrate, there was a benchmark in the past year called Long Range Arena that measured the performance of models on a suite of long range tasks. And the most popular sequence models these days, Transformers, were the main focus. But despite their many successes, they don't do so well on long context. And so there were dozens of variants that were tried, and they all get to around the same performance, which is actually not much above random guessing. In contrast, S4 we'll see is explicitly designed to be effective on long context, which leads to a huge improvement on this benchmark. And it's the first model to ever make progress on some really difficult long sequence tasks. Can I ask a quick question here, Albert? Yeah. So in the previous task, that was a generative process. And in this, the Long Context Channel challenge, is it a classification or what kind of task is it? These are all classification problems. And they're on data such that includes several data modalities, such as text, images, some sort of symbolic processing, stuff like that. I see. And so you can use S4 both as a generative model to actually generate sequences. Yeah. So a lot of sequence models, again, a sequence model I'm defining as a black box interface, really, that's just a sequence-to-sequence map. And many of these can be used in many ways, both for classification and generation. For example, transformers or RNNs are similar things that satisfy the same interface and can be used in many ways as well. Gotcha. Thank you. Yeah. OK, so now I'll get into the technical portions. And the first part will be about Hippo, as I mentioned. And to motivate what Hippo's goal was, I gave a bunch of examples of data that machine learning models currently struggle with, particularly things like time series. And to highlight why this is hard, I'm going to use a running example to illustrate a very basic capability that's difficult for modern models. And that's the moving average, which is perhaps the most basic method in modern time series analysis. So this figure depicts the exponential moving average or EMA, which is the blue line. And the way it's used is that it's a fixed non-learnable feature that's often the first pre-processing step that's performed in any sort of time series analysis pipeline. Now in the context, in the spirit of machine learning and deep learning, instead of doing manual processing like creating these features, we really would like to be able to learn these sort of things automatically from the data. And so in particular, here's a very simple concrete task is, suppose you have a model and you're feeding it this black input signal, and you want the model to predict the EMA or the blue signal as the output. And fortunately, it turns out that standard sequence models, such as attention and convolutions, cannot do this at all. And the reason why is essentially because the EMA has unbounded context. It's actually just a weighted average of the history of the signal with an exponentially decaying weight that searches back infinitely. Whereas in contrast, most modern models such as attention or a convolutions have finite context in those. Some people wonder about other things like RNNs. And the short answer is that RNNs are better than attention convolutions here, but they still aren't that good due to empirical problems with optimization and other things. So we'll see that the methods that are introduced in this talk will be very naturally suited for this and are much stronger versions. But going back to the EMA, the way that one way to think about it is that it's a very simple summary of the entire history of your signal. In other words, it's a state X, which is a single number that summarizes the entire history of the input U. And the reason why it's useful is that it's easy to compute because if you get new data, you can update the EMA in constant time using this weighted average. And beyond the simple example, though, I think these two properties are actually conceptually really important. For example, they're exactly the properties that you need in any sort of real-time decision-making problem. And really abstractly, you can even imagine that your brain is a state that's summarizing the entire context of your life, and it's constantly updating as you acquire new information. So I think that's actually a pretty general important question, and this was a direct inspiration for HIPPO. In the context of machine learning, this question has a lot of direct impact on our models because, as I mentioned, they struggle with long context. For example, text models, it's been shown typically have a context range of about 100 to at most a few thousand tokens. Whereas if you want to deal with data such as speech and audio, a single word in speech is a sequence of length more than 10,000, and this can really stretch to unbounded length. And so this is the question that I was trying to, that I was thinking about. And what I did was I tried to convert this vague goal of long-range memory into a more formal mathematical question. The conceptual idea is that if you can compress the past into a smaller state that's accurately remembering it, then you should be able to reconstruct the past. And we can then attempt to turn this into a technical problem. So the idea is that we're going to observe an input signal online and try to maintain a good representation of it that allows us to reconstruct it. And so, okay, so first in this section, I'm going to formalize this idea, and then I'll define HIPPO and visualize it. And then talk about a couple of generalizations. So the first thing is that let me formalize this idea that I just mentioned. And so the idea of HIPPO is that, again, we're trying to observe an input signal online, and we're going to try to encode it as well as possible given a memory budget. So concretely, you'd think of it like this. So suppose at some initial time, T0, we've seen part of the input, and we're going to try to compress this input. So what you can do is store the best approximation to what we've seen so far. For example, we can create the best polynomial approximation and write down the coefficients of that polynomial. So now the degree of the polynomial or the number of coefficients is the memory budget. And we want to do this continuously at all times. So as we keep seeing more data at some later time T1, we'll have to update our best approximation and write down the new coefficients. Now the central question is, first of all, how do you actually find these optimal approximations? And moreover, how can you update this representation efficiently as you keep seeing more information? And so this is the main conceptual idea, and it's a little bit of work to formalize a little more. And in particular, I've been talking about optimal approximations, but that's actually not well-defined. And so what we'll need is to find a measure that specifies the quality of approximation. For example, we can choose the exponentially decaying measure, which says that we care about approximating the recent pass of the input more than the far pass. And this will relate back to the EMA. But given this, the problem is more or less well-defined. So basically, we have to pick the measure sort of as a hyperparameter or a prior for now. Let's talk about how you can actually learn it. But for now, we need to pick a measure up front, say the exponential decaying measure. And then you need to choose a polynomial basis. And then the problem's completely defined, and you can write down the coefficients in closed form, and you can figure out how they evolve through time. So I'm going to skip the details of the derivation, but you end up with a closed form method. And what I want to emphasize is that the derivation has some technically interesting new ideas. But the most interesting and important part of this, I think, is just this simple conceptual idea of the online compression and reconstruction and how to formalize that mathematically. So that's the main point. OK, and now with the definitions out of the way, things will become a lot more clear with some visualizations of what it does. So first of all, let me just be really formal about defining what HIPAA is. So I mentioned the problem was that we are encoding. So XFT is going to represent a vector of our coefficients at all times. And the question is, how does this evolve through time as we see more data in the input U? And it turns out that it just satisfies a simple differential equation. By going through the derivation, you can write down this differential equation in closed form and write down closed form formulas for this transition matrix involved here. So to be concrete, the ODE is called the HIPAA operator. And the matrices, the matrix in the operator are called HIPAA matrices, which have closed form formulas. In fact, the actual matrix is this matrix. It's an extremely simple matrix, which is a special type of structure matrix. And yeah, so it's just a simple formula. And then we write down a closed form formula for this differential equation. And that's how our coefficients evolve over time. And now, right, so this equation, again, is called the HIPAA operator or the high order polynomial projection operator, because we're projecting on the high degree polynomial basis functions. Now visually, the way to think about it is like this. The reason I call an operator is because it maps a function to a function. So it's an operator that maps this black input signal U to these sets of coefficients X and blue, where every time X of t compresses the history of the input signal U. And you can compute X online as you see one input at a time. So the black line represents our current time step. We're gradually seeing more of the input. And we are updating our coefficient vector, which is depicted in blue. Here I've, this is visualizing just the lowest order for coefficients of the best polynomial approximation. And now here is what the reconstruction looks like. So as I move along through time and update my coefficients, the coefficients that that polynomial defines, in a sense, is actually just this red line. So it is reconstructing the input just like we wanted. Note that we are using only, so here I've only visualized four coefficients, but I'm actually using 64 coefficients, but the whole function was linked to 10,000. So I'm compressing it a lot. And this, here's a static image that kind of illustrates the effect of the reconstruction. So because I'm compressing it, I can't perfectly reconstruct the input. And so how good is the reconstruction then? Well, it depends on the measure. So the green, the green line in this figure was the exponentially decaying measure that we are basically projecting onto. And so intuitively, you can see that the red reconstruction line is really accurate for the recent past and degrades further out in history, but still maintains some rough information about the whole signal. And so that is, that's hippo. Now, oh, one, oh, okay, a question here. No, you can continue. I just had one clarification, but I can ask after you finish. Here's fine too. Okay. So is it fair to think about X as being the state at each time point? And then essentially the red line is trying to reconstruct the signal given the current state or do you also use all the past states to reconstruct? That's exactly right. So yeah, so the reconstruction is happening using only the coefficient vector at the current black line. So every single time I'm using, I'm for, yeah, the blue line, I'm visualizing the whole thing, but at any given point in time, I'm remembering only the current vector, which has length 64. Here I'm only visualizing four of the components, but it has length 64. And using those 64 numbers, I'm reconstructing what I've seen so far in red. Oh, awesome. Thanks. Right. Now, in that previous figure, if I just take one of the blue lines, actually the lowest order coefficient and overlay over the function, you can see that it actually turns out to exactly be the EMA. And so it turns out that moving averages can be viewed as order zero or low order projections. On the other hand, hippo is essentially a very strong generalization of this that solves a natural mathematical question and gets back things like the EMA for free. So that's what hippo is, and now I'll just talk a little bit about some extensions of it. So first of all, a natural question that may be wondering is that I've been using this example of an exponential measure, but what about other cases? Well, it turns out that hippo can be derived for any measure. For example, here's a case that is pretty natural as well, which is what if I want to reconstruct along a uniform measure? In other words, I only care about remembering the recent past in sliding windows of my function. And this is possible, so you would get a different ODE, and here's a reconstruction in effect. So again, using just 64 numbers in memory, I'm trying to reconstruct the last 2,000 time steps of this function uniformly, and it's doing this quite accurately. Now you can generalize it even further to, for example, when the measure is changing over time instead of just sliding along, and so there's a very general framework here that can do lots of things. A lot of this was in follow-up work to their general hippo paper, and what we showed was that for essentially any measure, there exists a corresponding hippo operator where the hippo matrices A and B depend on the measure, and you can write them down in closed form. And this is important, I think, because it draws an equivalence between measures and these ODE's, where this means that we don't, I mentioned earlier that we had to choose the measure up front as a prior, such as the accidentally decaying case, but actually just by learning these matrices A and B, it's in some sense the same as learning the measure. Okay, so now even better, not only do these operators always exist, but it turns out that the matrices are always structured. Previously, we saw, for the accidentally decaying case, the matrix was actually extremely simple. In general, they're going to be more complicated than that, and it's, they satisfy a structure, which was something that I introduced in much earlier work, but they are all structured in some way, and that means that you can, how do you actually calculate these updates through time? You can actually update the state or the coefficients in nearly optimal time. Okay, so that was the main takeaways from HIPPO, and so just to recap, we were inspired by these very basic, these simple but important properties of trying to maintain a state that's summarizing the entire context, and we formalized this into a mathematical problem, which was pretty intuitive, and we were then able to solve analytically, and this resulted in a nice class of methods for addressing long context and signals. Okay, so I see a question in the chat. So, can these operators be expressed in terms of Z-transforms? I'm not quite sure what you mean here. To my understanding, Z-transforms are like the discrete version of a Laplace transfer, and I'm not sure if that's the one you're referring to, or another notion. Yes, that's what I was thinking about. It seems like as though just as you can express like exponentially, exponential decay in terms of Z-transforms of the functions, that there seems like it's likely to be a link. Yeah, I mean, I think all these things have a tight link, and they're connected to each other. It turns out actually that the way that in the next part, when I talk about S4, there's going to be some difficult computations, and I'm not sure if that's the one you're talking about. In the next part, when I talk about S4, there's going to be some difficult computational issues for computing certain things that I'll introduce, and to actually compute them, I essentially actually go through Laplace space or frequency space. So, essentially, I actually take the Z-transform of this equation and calculate that transform at several values, and then invert it to get the hippo matrices back, or to get a certain thing back. Sounds good. That makes sense. Yeah, great question. And yeah, so I wanted also just to stop around here at the summary for any other questions. And if there's none, that's great because usually this is a pretty complicated framework mathematically, but hopefully the visualizations help explain it a lot. Okay, so I'll move on to the next part where, so one thing I didn't include in this section was any experiments. So the way we evaluated this is kind of just like how good is the reconstruction, and actually using this method in machine learning models did pretty well, just naively, but where it became really effective was when incorporated into a model in a particular way, and so that's what S4 will be. And so to, first I'm just going to define S4, and I'm going to define it through hippo, which was the original motivation, and the motivation here is going to be very simple. So to refresh your memory, this is what hippo does, it maps an input signal, which in our case they're thinking of as 1D, to a higher dimensional signal. Now the problem is that we've blown up the dimension of the input from one dimension to n dimensions, where n was our memory budget or the number of coefficients, and typically this is going to be at least 100 or so. So the motivation for, so I work in deep learning, and I just wanted to incorporate hippo into a deep learning model, but this is a problem because you can't just stack layers of this because you just keep increasing the dimension. And so a very simple motivation to fix this is just, let's just decrease the dimension again. And the way to do this is that you can just take a very simple linear projection. So what we'll do is that we have a state x, which was like a 100 dimensional vector, and we'll just hit it with a dot product that can be learnable to get back a single number, which is essentially taking a linear combination of the blue lines to get the final output, which is the red line. And then we'll add a multiple of the original input, which can be seen as a skip connection. And that is the entire definition of S4. It's finally these two equations where the first one is the hippo equation, which takes the input to a state that's kind of memorizing it. And then the second equation just combines the state linearly into a single output. Now, for those of you with a background engineering, this definition may look really familiar. And this is because this is a well-known model called a state space model or SSM, which is sometimes depicted with this simple control diagram. And they've been around for decades, such as the famous common filter and use in many scientific fields. I think outside of controls and statistics, they're also pretty commonly used in perhaps computational neuroscience and many medical problems as well. Now, what the theme of this part will be is that we'll see that SSMs are a really elegant and natural model, but they haven't been used in deep learning before in this way. And for underlying reasons that we'll see in that S4 address. But for now, just to define S4 in terms of this model, the way that we'll define it is that it's just an instantiation of an SSM, these two equations, where we'll plug in specific values of matrices in. And although it turns out that although this model is simple to define, actually computing with it turns out to be difficult and will require new ideas and algorithms. And so my goal of this in this section is to convince you that this is a really elegant and fundamental model. And so first of all, I will talk about some general properties of SSMs that would have a lot of benefits in machine learning and deep learning that are independent of S4. And then I'll show how those come with associated trade-offs that prevent them from being really good in deep learning. And S4 will solve those problems. And finally, I'll show several real world experiments that show S4's effectiveness in a bunch of settings. So in this first part, I'm actually going to describe three different ways to think about SSMs, which give them a lot of nice properties. And this was theory developed in the predecessor work to S4 that and will have empirical concrete empirical benefits. And so the first way to the first property is that SSMs inherently operate on continuous time signals instead of discrete time sequences. So here's how to think about it. So in machine learning, we usually work with sequence models, which I defined as a parameterized map from an input sequence to an output sequence. What if instead of mapping a sequence to a sequence, I coined this term signal model to denote a parameterized map that maps a function to a function or a signal to a signal. And given one of these maps, you can essentially discretize the inputs and outputs however you want to get back a sequence. So essentially, the upshot is that signal models are in some sense a generalization of sequence models, where they actually map functions and functions, but by discretizing them, you get back a sequence model. And so the first way to think about SSMs is that they are just a simple parameterized signal model, where the parameters were matrices A, B, C, and D, and they map an input function to an output function. That's it. Just in terms of the interface or the API of the model, that this is what it does. The reason that this property is important is because even when we're working in discrete time, the model in some sense understands the underlying continuous domain. So I will show what I mean concretely by this later empirically. All right, so that's the first representation. The next perspective relates back to the original motivation of HIPAA, which was about online computation. So how do we actually compute the output of this SSM? One way to do it is to process the input one at a time, just like HIPAA did in an online setting. And so this is our current computation because each update can be computed efficiently from the previous one. And just to unpack a little why this is non-trivial, imagine we're processing this very long input, and we're at this current time step denoted by the vertical line, and we get just one more data point. So just like a single number for the input, and we want to compute the next output. So this output depends on the entire history of the input, and so you'd expect it, the computation of the next one to scale with the length of the sequence. But actually we can compute it in constant time. And this is a non-trivial property that most sequence models don't have. For example, in a transformer or a convolution, if you were to do this in an online or autoregressive fashion, computing mapping one input to one output, each computation will scale with the entire length of the context window. The reason that SSMs can do this so efficiently is because they're stateful, which is a point that's kept coming up, where in memory we're maintaining a state, which is the blue thing, which is a single vector that summarizing the history, and can be updated very efficiently. This makes them really efficient in any sort of online setting, as we've seen. And yeah, so we'll see again why this matters. But there's one main drawback, which is that if you're not in an online setting, this is slow because it's sequential. And so what if you actually know all the future inputs? Then ideally you wouldn't do this step by step, and you could do something faster and parallelizable. And so that was actually basically the main problem with RNNs and why they've recently fallen out of favor in machine learning, because they're sequential and not parallelizable when you see a lot of data at once. And so that motivates the final representation, which is the convolutional representation, which allows them to be paralyzed. And so the idea is that instead of mapping going from the input to the state to the output, you can actually go straight from the input to the output, bypassing the state, and doing the entire computation in parallel over the sequence length. The reason is that SSMs turn out to be equivalent to convolutions, where computing the map from the input U to the output Y is equivalent to convolving the input by a particular convolution filter, which is depicted in green here. And so to compute this map, you just do it's just Y equals U convolved with K for this convolution kernel. And so this can be done very efficiently using no techniques. So for the practitioner, what one thing I want to emphasize is that I think the most useful way to think about SSMs potentially is as essentially a very fancy CNN, where you're parameterizing the convolution kernel in a different way. And notably, this kernel can be infinitely long, which again points to one reason why this is very good at long range dependencies. So just to call back to this example again, the EMA, the EMA is actually literally just a single convolution where you can involve the input by an accidentally decaying convolution kernel. And as I mentioned, although things like CNNs are also literally convolutions, they can't represent the EMA because CNNs are finite window and the EMA is infinite window. On the other hand, SSMs do represent infinitely long convolutions. And in fact, there's a very, very simple way to write down the EMA as a directly as an SSM. And I think Chris kind of pointed to that earlier. So those were the three properties of SSMs that I wanted to mention. And just to recap, first of all, we're going to think of them as maps that operate on continuous signals, not just sequences. If your model is deployed in a setting where it sees inputs in real time or online, it can compute these efficiently recurrently. And if you see an entire input at once, such as usually during training time, you can compute it even more efficiently and in parallel. I have a quick question here, Albert. This is super cool. I was just wondering if the goal is actually to get a representation of your signal so that you can perform different downstream tasks. Isn't it better to actually have the state space representation rather than directly going to the outputs? In that case, would we have to stick with HIPAA instead of going to S4? Great question. Actually, no one's asked me that, but that's a great question. So the way that I think about this is that what's happening is that essentially we have this nice state, which is very meaningful. And then the second part of the SSM that projects it is kind of like the learnable thing that's figuring out how to extract the right features from this state. Now, I mentioned that everything I've done so far, so that's a learnable part that's actually using the entire state, in a sense. And I mentioned that I'm only considering the one-dimensional case so far with 1B inputs and outputs. But actually, what's going to happen in practice in our actual deep learning models is that we'll have multi-dimensional inputs and outputs, and we'll essentially run an SSM on each one of them. And each one of these will learn how to use the state in a different way. So we'll have essentially many, you can think of as maybe we'll have a single state, but many, many possible outputs that are all learnable, and we'll extract different features from that state. So we are going to get a lot of different features that utilize the state in however they want. I see. Okay. Thank you. But sorry, sorry, Joanne. But isn't it like all these dimensions also have a correlation? So do you also, if you run the space independently, don't you want to also preserve the correlation? So this is something that I think a lot of people working with time series are concerned with. And somehow in deep learning, we don't normally consider that aspect. And we kind of just throw in a really big model and a lot of these independent layers. And kind of, I think in practice, what usually happens at the model learns to, it learns whatever it needs to do for the final prediction task. And this often does involve like, I think it does end up decorrelating things. But it's not super clear exactly the dynamics of what happens. And this is kind of a more broad question for deep learning theory in general. That's not well understood right now. What I can say is that we've used this on many types of like noisy data that usually involve, so I'm going to get to experiments later, but we have tried this on many types of like time series and other noisy data like EEG. But one day, one day, right? It can work on multiple dimensions, which I kind of just pointed to you. And also I'll mention again later how we do that. But yeah, you can just kind of do it do it naively on multiple dimensions. And it just works out of the box. Okay. Okay, so before we get to the experiments, I just have a little bit on kind of the how S4 builds on top of SSMs. And so just to refresh your memory of what S4 is, it's just an SSM where we plug in certain formulas that were based on the theory of memorization. And we have special algorithms to compute it. And so first of all, why are these matrices needed? Well, the most important part of the SSM is the state as Nandita keeps insightfully bringing up. And so what HIPPO did was that it computed a very particular state that was mathematically meaningful and compresses the history of the input in a way that captures long range dependencies. And so basically just by plugging in that formula into this SSM, it learns a more meaningful state that allows the SSM to address long dependencies better. So just to illustrate this empirically, here's a simple experiment on a very standard benchmark for sequence models. The actual task doesn't matter, but it's well studied and standard sequence model based on such as transformers, CNNs and LSTMs all get to around the around the same accuracy of like 60 ish percent. Now, what happens if we use an SSM? If you use it naively by randomly initializing all the parameters, which is what you would typically do in deep learning, it actually does terribly. But what happens if we just plug in this formula? Plugging this in and not even needing to train the matrix gives a massive boost to the SSM and goes from much below the baselines to substantially above the baselines. And actually, I use the very small models for this ablation here, but the full model as far on this data set gets over 90%, which is something like 20 plus points better than all other sequence models. So that kind of illustrates why HIPPO is so useful. Now, quick question in this example. So are both A and B basically just plug in matrices or is A alone basically a measure? A is the more important matrix, but actually, yeah, just plugging in A and B essentially just just they're both fixed matrices, which are the HIPPO operators specifies both of these. I've only illustrated A because it's a more important one. But yeah, this particular experiment froze both of these matrices to specific ones. One question people have is that like, do we always freeze these? And actually, we can train them as well. This was to illustrate just like even freezing them, it does super well. And then, but in practice, we do train them and it makes it do a little bit better. Okay, so that was one thing. And that kind of points to I mentioned that SSMs have not been used in deep learning before in this way. And that's kind of one problem. If you do it naively, it doesn't work. And so you need this new theory. The second reason is actually that they're computationally pretty difficult to work with. And so here's the illustrate. Again, so to remind you, we're thinking of an SSM as a parameterized map from an input signal to an output signal. And I'll suppose that our input had length L. So our input would just give us a sequence of L numbers. Then the output of this whole thing is also a sequence of L numbers. And computing this map ideally takes around O of L time or not too much more. But here's the problem. SSMs map the input to the output through this state. And that state gave them a lot of nice properties, but it's also 100 dimensions higher. And so computing the end to end mapping through the state will take 100 times more computation and memory than what's needed to compute the final answer. And this is actually a real problem. And now, earlier I said that you don't actually have to compute the state. You can compute it using a convolution instead. But what happens is that before computing the convolution, I have to compute the kernel or the convolution filter in green. And computing that is just as slow as computing the state. And this sort of makes sense because it hasn't changed the computational hardness of the problem. So essentially computing it no matter how you do it is going to be slow and memory inefficient. So the main point of S4 was showing that you could substantially reduce this computation when the SSM is structured. And for example, when using the hippo matrix instead of an unstructured matrix, you can save this factor of 100 and make S4 overall extremely efficient. So this is done through a particular algorithm, which I'll just flash up. But basically we're trying to work with this SSM, but we only need to work with specific structured cases such as this, such as some particular hippo matrices. And now using some algorithmic ideas, it turns out there is a way to compute the convolution kernel, which was depicted in green before, very efficiently. And then compute the whole thing using a convolution. So I won't go into details here. And I will also mention that recently we've been developing simplifications of the model that allow you to bypass all of this and do things much more simply. So hopefully in a few weeks we'll have some stuff out that's where you don't need to worry about this really complicated algorithm. All right, so that was the technical portion of that I wanted to mention for S4. And I'll stop here for questions as well. So if in any case you want to actually get the state, can S4 actually recover the state? Or is it like, yeah, so like, I don't know if that would be any use case. But like I was saying, like, if there's a case where I actually want the state, can I do that and get the convolution? Yes, you can. And in fact, that will be used in some experiments. I guess I didn't mention explicitly, but you can compute it in either way, either through the convolution or through the state. And where the state or the convolution is useful is during training time for parallelizability. But where the state is useful is at some sort of inference or deployment settings, where perhaps you might be online, and then you would actually be going through the state instead of the convolution and unrolling things one step at a time. Right. So you can do it either way, which is pretty cool. Thanks. I had more of a thought question. Let's say I'm interested in two different measures. Like, I want to see how the exponential average works, but I also want like, so is it, does it basically mean that I just have to create a new measure that combines this efficiently before this? Before I plug it into SSM or can S4 basically kind of, because there are two independent blocks that I can basically... Yeah. So I'm just about to get to the experiments. And actually, I will, I'll get to that slide right now, where, so first of all, the experiments will be on this type of signal data. And what, as I mentioned a couple of times, what we actually do is that I have to find this 1D to 1D map, but I'm actually going to just like, given a multidimensional input, I'm just going to stack a bunch of copies of this. And now as a parallel to that, you can do many things with these copies. So to answer your question, one thing that I've been starting to experiment with is just using different measures or essentially different A and B matrices for every copy. And that can, and so that sort of has interpretation of using multiple measures. I see. Because when Iman actually talked about the correlations between different dimensions, let's say you have an image, like two different pixels are actually correlated. So I was thinking like, you can have a measure that captures this correlation, but you can have another measure that captures it over time. Another thing actually, since you mentioned that, I don't know if you tried that on image space, I would be curious like if this kind of like long convolution actually makes any difference with the image space. Because image usually like, when we do the image analysis theoretically, when we start thinking about it, it seems that like also the local feature as well as of course the global feature is important. But I don't know, like if we are missing any local features by just using this kind of like long representation. That's a good question. I actually, we have started doing more experiments on images, which I didn't include in this talk, but luckily we do find that the local bias of convolutions does seem pretty good. I don't know, it's hard to quantify if we're missing features, but I think there are settings where we're not, we're only on power or not, or maybe a little bit worse than a standard local CNN. It is hard to say. I will mention though that you can forcibly incorporate locality into this just by changing the measure. For example, if you choose a uniform measure that has a short window, that's the same as saying I would just want a local convolution kernel. Because I would imagine like for this particular thing, like the use case where we have to have to work with a very high resolution image data, you know, for example, like imagine like mammogram, right? Like we have to go with like 1000 by 1000 minimum dimension. So for this probably would be useful because they are actually, we want to like do the rescaling, but we cannot because we'll lose probably a lot of features in the middle. But this kind of like long convolution could this, this is a perfect problem that I will actually, I wasn't going to, but now I'll mention this in the experiments as well. Okay. It's actually something that we have thought about basically rescaling of convolutions and using. Right. Okay, I'll get to that. Before that, so I want to get the experiments and basically I just wanted to find, I'm only to find the simple linear one either one D map, but you can just do it in parallel across a lot of features and then plug it into a standard neural network to do sequence modeling. So the first type of data I'll see is a biosignal data. So here is a, there's a real world data set of trying to predict vital signs such as heart rate from raw biosignal data such as I wrote EKG and EG here, but I think it's actually EKG and PPG. And so that's visualized here. And this data is pretty challenging for deep learning models because you can see that it's very long. This is a sequence of like 4000. If you zoom in a lot, it would be pretty smooth actually, but if you zoom out, it displays a lot of periodicity and spikes and other things. And so a lot of methods have been tried on this data set, which include kind of standard machine learning techniques like XGBoost as well as many very modern deep learning sequence models. And S4 substantially improves over all of these in I think cutting the root mean squared error by at least two thirds on all of these targets just with that generic deep learning model that deep model that I showed. Actually I've, these were like older numbers and recently I've been rerunning these again and actually you can drop this down even more. One thing I will note is that attention and transformers does really poorly on this type of data. And that's something that I think I found pretty consistently. So there's some sort of bias toward what type of data you have and S4 is really good at signals and attention is not. Conversely, attention is good at some other types of discrete data that S4 is not as good at. Okay, so that's, that was one experiment. The next one is two time series data where we did a forecasting task where you're given a context window and you want to predict future values. Actually, I'm going to go through this kind of fast because I don't want that much time I want to get through some more of the bio applications and the things that you guys brought up. The models here are very complicated. Whereas for S4, we're actually doing an extremely simple setup, which is just a mask prediction. We're just going to give you, we're going to take the entire sequence and mask out the desired forecast range and then just predict what's in the mask by passing it through this generic deep model. So this is really, it's like a very extremely simple application. I won't unpack the numbers too much, but there's a lot of baselines here, including time series models, LSTMs, lots of transformers, and S4 does better than all of them on these real time series data sets, including weather and energy data with much less specialization. These models were all designed for time series and we were just using our generic model. And you didn't even like tune the window size, right? We did not for this one. Actually, by tuning the window size, you can get the numbers down even more. Okay. Okay. The next one here points to Amon's question about rescaling. So it's actually, I'm going to display this to audio. But essentially, I've used audio a few times. I'm running example. It's sampled at extremely high rate. And it's extremely long. So this is a data set of classifying one second speech clips, which were length 16,000, into classifying the words. And most sequence models like transformers and RNNs are really bad here. The only thing that works is CNNs, which the red line is pointing to a speech CNN baseline. And these work okay. But what happens if you are resampling the signal at different frequencies? And this happens commonly in audio because your signal can be sampled at any rate and sound more or less the same. So for example, this orange sequence is a sequence of samples, but it's actually the same underlying signal as the original blue sequence of samples, just at a different frequency. And so it's ideal if the same model works on both of them. But standard models like CNNs cannot do this, essentially because of the local bias that was brought up earlier. I won't unpack this here, but if you use like a standard local CNN, it will break at a different frequency. However, by using a signal model such as S4, which is actually understanding the underlying continuous domain or the underlying continuous function, it can work here without modification. So this is all in a zero shot setting where it's trained at one resolution and tested on a different resolution. And this breaks a CNN, but S4 can do it out of the box. And that's because of this first property of being a continuous time model. And now the last two things I'll show are just calling back to the experiments at very beginning. I showed some audio generation clips. And that was an autoregressive setting where we're generating things one sample at a time. And despite having an extremely large context window, which made it do better and more coherent, we could still sample things autoregressively just as fast as other autoregressive models. And that's because of the fast online or autoregressive representation where you're computing the state and updating it every time. And finally, I showed this benchmark of long range modeling where S4 substantially outperforms other models on a range of different tasks. And this benchmark was also used to benchmark the speed of models during training where S4 is just as fast as all of these efficient transformer variants. And that's because of the efficient, paralyzable view along with the new algorithms we introduced. And so all these properties, as I promised, have concrete empirical benefits. Now for, I'm running out of time, so I just want to get to a couple more things. For the last part, I just wanted to, for this audience, I wanted to point to where I hope that this model will be useful, which is as a general tool for deep learning for biosignals. And I've pointed out one, one example of a data set already where we were predicting like heart rate from EKG signals. But this was another one that CE was working on actually and her and another lab mate have been trying to test S4 here, where this is a data set of raw EEG signals that are difficult to process because they're so noisy and long. And the state-of-the-art models are very recent. CE's model from a couple months ago was there they are on one of these EEG data sets, but it was quite involved and involved a lot of domain knowledge, such as even like the placement of the electrodes and a lot of different parts, components of the model. And so where I hope that S4 could be useful is as a generic tool or building block for for addressing these types of signal data without as much domain expertise and how to design the model. And so CE and Collid have been running some preliminary experiments using S4 on this data, where we don't even need to process the, you don't need to pre-process it with FFT features, you don't need to do a lot of these other things and just run it through these, a generic deep model composed of S4 layers. And Collid found some very preliminary results where it is improving over the baselines in some settings. This is still very preliminary, so it's not, there's other settings that we care about, such as incorporating self-supervision and so on, where it's not quite there, but I do think it has a lot of potential in this type of domain. Another example, actually that was published was another recent collaboration with Stanford Medicine that was submitted to a gastroenterology journal on detecting acid reflux from impotence sensor data. And so again, S4 was really good on that type of prediction task. So that is all I was going to talk about for this. So just to review, S4 is a, it's an SSM, which are these two equations, where we plug in certain formulas and have special algorithms to compute the model. And overall, SSMs and in particular, S4 have a number of very nice properties with concrete empirical benefits, as we saw, and I think can become a very effective building block for modeling many types of sequential data in the future. Thanks for listening and thanks for all the collaborators for the hard work. This slide lists a couple of resources, such as blog posts and related papers, as well as the audio results from an ongoing, a paper that's under submission right now. Feel free to reach out if you have questions and thanks. This was my last slide, but because you want to ask how I will, I guess I'm technically out of time, so of course people feel free to leave, but if you want to stay, I can show one thing about the high resolution images that that was brought up. Let me find that slide. Yeah, if people have conflicts, feel free to leave and we will put up the recording of the talk later in our YouTube channel. Otherwise, if you would like to stay, then yeah, I'll be able to share the slide. So yeah, I'll just really quickly go over this where medical, medical imaging is something that we think could be a potential strong use case for S4 because of this high resolution feature where, so this slide was about, I was moving from a different way, but the point I wanted to make was that Can you go to presentation mode? I think we are still seeing your screen nose. Yeah, sorry. Oops. Am I showing my whole screen? No, we are seeing your screen rather than the presentation. Okay, I thought I had it on. You can just swap the view. I thought I had it on the right view. Is this one still show the, oops. Yeah, I can see the high definition view. Okay, great. So yeah, so the point I was making is that normally image data sets are things like ImageNet, which are actually extremely low resolution compared to other data that we might find such as medical imaging where apparently the images can be up to 100,000 by 100,000 pixels. And this is obviously like way too big for current models, which can only operate on small patches at a time. So I don't know how to address this really, but it's something that fascinates me. But just to point out, this is part of a longer drop pack where I pointed some potential future directions. The one that I'll mention here relates to some things that we brought up, which is that just like the speech experiment that I showed, I believe that S4 should work training on images at different resolutions. And so what you can do is essentially try to train on lower dimensional versions of the image, lower resolution versions, and then transfer the same model to work on high dimensions, which is a very similar thing that I showed for the speech example. So yeah, I think that's potentially something that could work. And the point is that a signal model like S4 will work at different resolutions because you can sample at different rates essentially. And yeah, so what you need is a signal model that understands the continuous domain, just like the example I showed. And that points to this property again. So this is something where we haven't tried it and I don't know if it works, but it's some part of me feels like it might be the right way to or one potential good way to approach this type of problem. But I would think in the opposite way, it's not really we want to generate the high resolution from the low resolution, but I would imagine since you have this kind of like state-based representation and finally you're getting this signal, I would imagine like in some case always like we had to deal with this kind of situation that we had a very high resolution image. And before running through the convolution because of the memory computations, like computational complexity, memory complexity and all this kind of thing. So you have to reskill the image into a much lower dimension. Yeah. And we had a chance of losing a lot of features, specifically histopathology, exactly the example that you showed, or the mammogram, those kind of images, you know. Yeah, I see. So I was thinking that perhaps like what you can do is kind of like iteratively increase the resolution and pick up higher and higher resolution features as you go. But the benefit is that perhaps you can pick up the coarser grain things and then as you upsize the image then and you rescale your kernel essentially, then it's already going to be doing as everybody knows the coarse grain features, but then it as you keep training, it only has to learn the higher frequency features as you go. And again, I have no idea if this is, if this makes sense or it's promising, but it sounds pretty interesting. I think, I think Alba's idea is actually very similar to how pathologists analyze our hostile images. So they usually look at. Define zoom. Define zoom. They usually look at low resolution image first and localize like the potential areas where the tumor are and then they zoom into higher resolution. Right. Yeah. I see. Yeah. And so yeah, I don't know if this will be better than CNN or other things, but it definitely has different and interesting properties. Okay. So that was, yeah, I think that's the end of the material I have. I can say around a few more minutes if people still have questions. All right. Great. Thanks so much, Albert. I have just one question. Sure. Yeah. Thank you for the presentation. That was awesome. Did you find any scenario where it's better to use transformer than S4? Yes. Another great question. So let me just share my screen again. I have one slide prepared for that. Basically at the beginning, I drew this distinction between continuous kind of continuous and discrete data. And I think that S4 will be the best or like the ideas involved are potentially going to be the best thing to do for signals. But for kind of higher level concepts or more discrete concepts such as language or some other things, transformers, I think that's where transformers really shine and are probably going to be better. So here's a, I don't know if this is the right screen again. Sorry. Anyways, here's the one slide on language modeling where we took a transformer, which are currently of course the best models for text and NLP. And we replaced the attention with S4 and found that it doesn't do quite as well. But it is still better than all non, it's significantly better than all non-transformer models. And it also has some other benefits. Like you can do language generation much faster because of the fast recurrent view. That was the main point of this. But this also does kind of point to the fact that personally my intuition is that transformers are really good for dense and discrete data. Whereas S4 is really good for more like noisy and raw data. Yeah. And I mean the speed up here I think is very interesting. Do you know what was the window you would take for language modeling? Like how many tokens or words rather did you consider? Yeah. This experiment was done using a pretty standard length of either 512 or 1024 tokens. You actually can keep increasing the window length for S4, which only slows it down a little bit and actually improves the performance a little bit as well. But I found that out after the fact and I didn't feel like retraining this. Okay. Cool. Thanks. So yeah, but the sort of findings of this slide is the speed up, right? It's massive. That was the point that we did this experiment for. But yeah, so there's a lot of the speed up. In terms of the original question though, in terms of the raw performance of modeling the data, transformers are currently doing a little bit better here. Cool. Thank you. All right. Is there any other questions? Let's all give Albert a round of virtual applause. Thank you for the very comprehensive presentation of state-based models. Thanks for having me. Thank you, Albert. Thank you. Thank you, everyone, for joining us. We will put up the recording of the video, the talk, later to our YouTube channel. And yeah, we'll see you at the same time next week. Thank you. See you, guys.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 8.08, "text": " Okay, hi everyone, welcome to our 41st session of the May AI Group Exchange.", "tokens": [50364, 1033, 11, 4879, 1518, 11, 2928, 281, 527, 18173, 372, 5481, 295, 264, 1891, 7318, 10500, 31169, 13, 50768], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 1, "seek": 0, "start": 8.08, "end": 13.32, "text": " This week we have Elba Gu from Stanford here with us to present his research on efficiently", "tokens": [50768, 639, 1243, 321, 362, 2699, 4231, 2694, 490, 20374, 510, 365, 505, 281, 1974, 702, 2132, 322, 19621, 51030], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 2, "seek": 0, "start": 13.32, "end": 17.28, "text": " modeling long sequences with structured state spaces.", "tokens": [51030, 15983, 938, 22978, 365, 18519, 1785, 7673, 13, 51228], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 3, "seek": 0, "start": 17.28, "end": 21.2, "text": " Elba is a final year PhD candidate in the computer science department here at Stanford", "tokens": [51228, 2699, 4231, 307, 257, 2572, 1064, 14476, 11532, 294, 264, 3820, 3497, 5882, 510, 412, 20374, 51424], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 4, "seek": 0, "start": 21.2, "end": 24.080000000000002, "text": " University, advised by Chris Ray.", "tokens": [51424, 3535, 11, 26269, 538, 6688, 10883, 13, 51568], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 5, "seek": 0, "start": 24.080000000000002, "end": 28.92, "text": " He's brought the interest in studying structured representations for advanced signal capabilities", "tokens": [51568, 634, 311, 3038, 264, 1179, 294, 7601, 18519, 33358, 337, 7339, 6358, 10862, 51810], "temperature": 0.0, "avg_logprob": -0.27330445228738987, "compression_ratio": 1.5259515570934257, "no_speech_prob": 0.023508252575993538}, {"id": 6, "seek": 2892, "start": 28.92, "end": 34.400000000000006, "text": " of machine learning and deep learning models with focuses on structured linear algebra,", "tokens": [50364, 295, 3479, 2539, 293, 2452, 2539, 5245, 365, 16109, 322, 18519, 8213, 21989, 11, 50638], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 7, "seek": 2892, "start": 34.400000000000006, "end": 38.480000000000004, "text": " non-euclidean representations, and theory of sequence models.", "tokens": [50638, 2107, 12, 68, 1311, 31264, 282, 33358, 11, 293, 5261, 295, 8310, 5245, 13, 50842], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 8, "seek": 2892, "start": 38.480000000000004, "end": 41.040000000000006, "text": " Thank you so much Elba for joining us today.", "tokens": [50842, 1044, 291, 370, 709, 2699, 4231, 337, 5549, 505, 965, 13, 50970], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 9, "seek": 2892, "start": 41.040000000000006, "end": 45.44, "text": " Before we start, do you have any preference on how you want to take questions?", "tokens": [50970, 4546, 321, 722, 11, 360, 291, 362, 604, 17502, 322, 577, 291, 528, 281, 747, 1651, 30, 51190], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 10, "seek": 2892, "start": 45.44, "end": 49.56, "text": " Yeah, thank you, thank you for an introduction.", "tokens": [51190, 865, 11, 1309, 291, 11, 1309, 291, 337, 364, 9339, 13, 51396], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 11, "seek": 2892, "start": 49.56, "end": 56.56, "text": " For this talk, I think I'm not sure usually the level of formality, but I'm very happy", "tokens": [51396, 1171, 341, 751, 11, 286, 519, 286, 478, 406, 988, 2673, 264, 1496, 295, 1254, 1860, 11, 457, 286, 478, 588, 2055, 51746], "temperature": 0.0, "avg_logprob": -0.2506327767973965, "compression_ratio": 1.5632183908045978, "no_speech_prob": 0.0014079498359933496}, {"id": 12, "seek": 5656, "start": 56.56, "end": 63.56, "text": " to have the casual in terms of the conversation and the questions.", "tokens": [50364, 281, 362, 264, 13052, 294, 2115, 295, 264, 3761, 293, 264, 1651, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 13, "seek": 5656, "start": 63.56, "end": 67.32000000000001, "text": " I think there's some time, it's not going to be a full hour talk, so I'm more than happy", "tokens": [50714, 286, 519, 456, 311, 512, 565, 11, 309, 311, 406, 516, 281, 312, 257, 1577, 1773, 751, 11, 370, 286, 478, 544, 813, 2055, 50902], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 14, "seek": 5656, "start": 67.32000000000001, "end": 71.12, "text": " to take questions during it and I'll watch the time in case it gets too long.", "tokens": [50902, 281, 747, 1651, 1830, 309, 293, 286, 603, 1159, 264, 565, 294, 1389, 309, 2170, 886, 938, 13, 51092], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 15, "seek": 5656, "start": 71.12, "end": 77.32000000000001, "text": " And then I'll also pause a few times to pause for potential questions during some sections.", "tokens": [51092, 400, 550, 286, 603, 611, 10465, 257, 1326, 1413, 281, 10465, 337, 3995, 1651, 1830, 512, 10863, 13, 51402], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 16, "seek": 5656, "start": 77.32000000000001, "end": 79.52000000000001, "text": " Okay, sounds good.", "tokens": [51402, 1033, 11, 3263, 665, 13, 51512], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 17, "seek": 5656, "start": 79.52000000000001, "end": 83.12, "text": " Let's try to make this session as interactive as possible.", "tokens": [51512, 961, 311, 853, 281, 652, 341, 5481, 382, 15141, 382, 1944, 13, 51692], "temperature": 0.0, "avg_logprob": -0.22380775671738845, "compression_ratio": 1.6791666666666667, "no_speech_prob": 0.009098902344703674}, {"id": 18, "seek": 8312, "start": 83.12, "end": 86.84, "text": " How further or do let me hand it over to Albert?", "tokens": [50364, 1012, 3052, 420, 360, 718, 385, 1011, 309, 670, 281, 20812, 30, 50550], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 19, "seek": 8312, "start": 86.84, "end": 88.84, "text": " Thank you.", "tokens": [50550, 1044, 291, 13, 50650], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 20, "seek": 8312, "start": 88.84, "end": 95.64, "text": " All right, so this talk will be about a new sequence model called S4, or structured state", "tokens": [50650, 1057, 558, 11, 370, 341, 751, 486, 312, 466, 257, 777, 8310, 2316, 1219, 318, 19, 11, 420, 18519, 1785, 50990], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 21, "seek": 8312, "start": 95.64, "end": 96.64, "text": " spaces.", "tokens": [50990, 7673, 13, 51040], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 22, "seek": 8312, "start": 96.64, "end": 105.32000000000001, "text": " Now, for the purposes of this talk, when I mentioned sequence models, we will think of", "tokens": [51040, 823, 11, 337, 264, 9932, 295, 341, 751, 11, 562, 286, 2835, 8310, 5245, 11, 321, 486, 519, 295, 51474], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 23, "seek": 8312, "start": 105.32000000000001, "end": 111.28, "text": " them as a black box sequence-to-sequence map composed of primitive layers, where each", "tokens": [51474, 552, 382, 257, 2211, 2424, 8310, 12, 1353, 12, 11834, 655, 4471, 18204, 295, 28540, 7914, 11, 689, 1184, 51772], "temperature": 0.0, "avg_logprob": -0.26077178355013386, "compression_ratio": 1.5566037735849056, "no_speech_prob": 0.020915061235427856}, {"id": 24, "seek": 11128, "start": 111.28, "end": 119.68, "text": " layer simply takes an input sequence and returns a sequence of the same shape.", "tokens": [50364, 4583, 2935, 2516, 364, 4846, 8310, 293, 11247, 257, 8310, 295, 264, 912, 3909, 13, 50784], "temperature": 0.0, "avg_logprob": -0.14880878169362138, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0012441023718565702}, {"id": 25, "seek": 11128, "start": 119.68, "end": 124.68, "text": " For our purposes right now, we'll think of them as just being a one-dimensional to one-dimensional", "tokens": [50784, 1171, 527, 9932, 558, 586, 11, 321, 603, 519, 295, 552, 382, 445, 885, 257, 472, 12, 18759, 281, 472, 12, 18759, 51034], "temperature": 0.0, "avg_logprob": -0.14880878169362138, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0012441023718565702}, {"id": 26, "seek": 11128, "start": 124.68, "end": 132.36, "text": " map, but this can be easily converted to higher-dimensional features.", "tokens": [51034, 4471, 11, 457, 341, 393, 312, 3612, 16424, 281, 2946, 12, 18759, 4122, 13, 51418], "temperature": 0.0, "avg_logprob": -0.14880878169362138, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0012441023718565702}, {"id": 27, "seek": 11128, "start": 132.36, "end": 135.6, "text": " Many sequence models have been developed that satisfy this interface, particularly in the", "tokens": [51418, 5126, 8310, 5245, 362, 668, 4743, 300, 19319, 341, 9226, 11, 4098, 294, 264, 51580], "temperature": 0.0, "avg_logprob": -0.14880878169362138, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0012441023718565702}, {"id": 28, "seek": 11128, "start": 135.6, "end": 137.52, "text": " context of deep learning.", "tokens": [51580, 4319, 295, 2452, 2539, 13, 51676], "temperature": 0.0, "avg_logprob": -0.14880878169362138, "compression_ratio": 1.6278026905829597, "no_speech_prob": 0.0012441023718565702}, {"id": 29, "seek": 13752, "start": 137.52, "end": 140.8, "text": " These include many classical deep learning models, such as recurrent neural networks", "tokens": [50364, 1981, 4090, 867, 13735, 2452, 2539, 5245, 11, 1270, 382, 18680, 1753, 18161, 9590, 50528], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 30, "seek": 13752, "start": 140.8, "end": 145.32000000000002, "text": " or RNNs and convolutional neural networks or CNNs, as well as many more modern models,", "tokens": [50528, 420, 45702, 45, 82, 293, 45216, 304, 18161, 9590, 420, 24859, 82, 11, 382, 731, 382, 867, 544, 4363, 5245, 11, 50754], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 31, "seek": 13752, "start": 145.32000000000002, "end": 150.36, "text": " such as transformers or neural ODEs.", "tokens": [50754, 1270, 382, 4088, 433, 420, 18161, 422, 22296, 82, 13, 51006], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 32, "seek": 13752, "start": 150.36, "end": 153.44, "text": " And all of these models kind of satisfy the same interface.", "tokens": [51006, 400, 439, 295, 613, 5245, 733, 295, 19319, 264, 912, 9226, 13, 51160], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 33, "seek": 13752, "start": 153.44, "end": 157.60000000000002, "text": " They map a sequence to a sequence of the same shape, or meaning the same length and", "tokens": [51160, 814, 4471, 257, 8310, 281, 257, 8310, 295, 264, 912, 3909, 11, 420, 3620, 264, 912, 4641, 293, 51368], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 34, "seek": 13752, "start": 157.60000000000002, "end": 158.8, "text": " field dimension.", "tokens": [51368, 2519, 10139, 13, 51428], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 35, "seek": 13752, "start": 158.8, "end": 164.76000000000002, "text": " And then you can incorporate any of these into a deep learning model fairly easily just", "tokens": [51428, 400, 550, 291, 393, 16091, 604, 295, 613, 666, 257, 2452, 2539, 2316, 6457, 3612, 445, 51726], "temperature": 0.0, "avg_logprob": -0.19525860873135653, "compression_ratio": 1.7992125984251968, "no_speech_prob": 0.003646622411906719}, {"id": 36, "seek": 16476, "start": 164.76, "end": 173.07999999999998, "text": " by using standard architectures, where you can include normalization layers, other linear", "tokens": [50364, 538, 1228, 3832, 6331, 1303, 11, 689, 291, 393, 4090, 2710, 2144, 7914, 11, 661, 8213, 50780], "temperature": 0.0, "avg_logprob": -0.18100036809473863, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003024723380804062}, {"id": 37, "seek": 16476, "start": 173.07999999999998, "end": 177.92, "text": " or nonlinear activations, as well as with the dual connections.", "tokens": [50780, 420, 2107, 28263, 2430, 763, 11, 382, 731, 382, 365, 264, 11848, 9271, 13, 51022], "temperature": 0.0, "avg_logprob": -0.18100036809473863, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003024723380804062}, {"id": 38, "seek": 16476, "start": 177.92, "end": 183.04, "text": " And so the core component of all of this is the core sequence model, and that's what", "tokens": [51022, 400, 370, 264, 4965, 6542, 295, 439, 295, 341, 307, 264, 4965, 8310, 2316, 11, 293, 300, 311, 437, 51278], "temperature": 0.0, "avg_logprob": -0.18100036809473863, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003024723380804062}, {"id": 39, "seek": 16476, "start": 183.04, "end": 186.2, "text": " we'll focus on.", "tokens": [51278, 321, 603, 1879, 322, 13, 51436], "temperature": 0.0, "avg_logprob": -0.18100036809473863, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003024723380804062}, {"id": 40, "seek": 16476, "start": 186.2, "end": 191.28, "text": " And this generic deep neural architecture based on sequence models can be used to solve", "tokens": [51436, 400, 341, 19577, 2452, 18161, 9482, 2361, 322, 8310, 5245, 393, 312, 1143, 281, 5039, 51690], "temperature": 0.0, "avg_logprob": -0.18100036809473863, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.003024723380804062}, {"id": 41, "seek": 19128, "start": 191.28, "end": 198.28, "text": " many types of problems with many types of sequence data, from medallies such as text", "tokens": [50364, 867, 3467, 295, 2740, 365, 867, 3467, 295, 8310, 1412, 11, 490, 1205, 336, 530, 1270, 382, 2487, 50714], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 42, "seek": 19128, "start": 198.28, "end": 206.68, "text": " and audio to images and videos to general time series data or biosignals, for example,", "tokens": [50714, 293, 6278, 281, 5267, 293, 2145, 281, 2674, 565, 2638, 1412, 420, 36997, 788, 1124, 11, 337, 1365, 11, 51134], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 43, "seek": 19128, "start": 206.68, "end": 211.08, "text": " which is depicted here.", "tokens": [51134, 597, 307, 30207, 510, 13, 51354], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 44, "seek": 19128, "start": 211.08, "end": 214.12, "text": " In this talk, I'm going to draw a very rough distinction between different types of sequence", "tokens": [51354, 682, 341, 751, 11, 286, 478, 516, 281, 2642, 257, 588, 5903, 16844, 1296, 819, 3467, 295, 8310, 51506], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 45, "seek": 19128, "start": 214.12, "end": 215.12, "text": " data.", "tokens": [51506, 1412, 13, 51556], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 46, "seek": 19128, "start": 215.12, "end": 219.56, "text": " Now much of modern sequence modeling in the context of machine learning focuses on data", "tokens": [51556, 823, 709, 295, 4363, 8310, 15983, 294, 264, 4319, 295, 3479, 2539, 16109, 322, 1412, 51778], "temperature": 0.0, "avg_logprob": -0.13773062988951967, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.0014538849936798215}, {"id": 47, "seek": 21956, "start": 219.56, "end": 221.44, "text": " such as text.", "tokens": [50364, 1270, 382, 2487, 13, 50458], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 48, "seek": 21956, "start": 221.44, "end": 225.12, "text": " And very roughly I'll classify this as being a discrete sequence because the input comes", "tokens": [50458, 400, 588, 9810, 286, 603, 33872, 341, 382, 885, 257, 27706, 8310, 570, 264, 4846, 1487, 50642], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 49, "seek": 21956, "start": 225.12, "end": 228.56, "text": " in the form of discrete tokens.", "tokens": [50642, 294, 264, 1254, 295, 27706, 22667, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 50, "seek": 21956, "start": 228.56, "end": 233.24, "text": " And other types of data like this includes things like graphs or things like DNA-based", "tokens": [50814, 400, 661, 3467, 295, 1412, 411, 341, 5974, 721, 411, 24877, 420, 721, 411, 8272, 12, 6032, 51048], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 51, "seek": 21956, "start": 233.24, "end": 234.24, "text": " pairs.", "tokens": [51048, 15494, 13, 51098], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 52, "seek": 21956, "start": 234.24, "end": 240.48000000000002, "text": " In contrast, what this talk will focus on is data that's roughly more continuous, things", "tokens": [51098, 682, 8712, 11, 437, 341, 751, 486, 1879, 322, 307, 1412, 300, 311, 9810, 544, 10957, 11, 721, 51410], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 53, "seek": 21956, "start": 240.48000000000002, "end": 244.6, "text": " such as video or time series or audio.", "tokens": [51410, 1270, 382, 960, 420, 565, 2638, 420, 6278, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 54, "seek": 21956, "start": 244.6, "end": 248.52, "text": " And what's common to all of these is that there's an underlying notion of time from", "tokens": [51616, 400, 437, 311, 2689, 281, 439, 295, 613, 307, 300, 456, 311, 364, 14217, 10710, 295, 565, 490, 51812], "temperature": 0.0, "avg_logprob": -0.13103597098534261, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.006681289989501238}, {"id": 55, "seek": 24852, "start": 248.52, "end": 252.92000000000002, "text": " which the sort of data is sampled from.", "tokens": [50364, 597, 264, 1333, 295, 1412, 307, 3247, 15551, 490, 13, 50584], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 56, "seek": 24852, "start": 252.92000000000002, "end": 257.96000000000004, "text": " And so I'm going to very broadly call this type of data signal data as opposed to sequence", "tokens": [50584, 400, 370, 286, 478, 516, 281, 588, 19511, 818, 341, 2010, 295, 1412, 6358, 1412, 382, 8851, 281, 8310, 50836], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 57, "seek": 24852, "start": 257.96000000000004, "end": 259.08, "text": " data.", "tokens": [50836, 1412, 13, 50892], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 58, "seek": 24852, "start": 259.08, "end": 263.0, "text": " And roughly speaking, signals can be defined as data that's generated from an underlying", "tokens": [50892, 400, 9810, 4124, 11, 12354, 393, 312, 7642, 382, 1412, 300, 311, 10833, 490, 364, 14217, 51088], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 59, "seek": 24852, "start": 263.0, "end": 269.2, "text": " continuous physical process, including all these examples here.", "tokens": [51088, 10957, 4001, 1399, 11, 3009, 439, 613, 5110, 510, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 60, "seek": 24852, "start": 269.2, "end": 271.32, "text": " This talk will be composed of two parts.", "tokens": [51398, 639, 751, 486, 312, 18204, 295, 732, 3166, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 61, "seek": 24852, "start": 271.32, "end": 276.48, "text": " The first part covers a method called HIPPO, which was the predecessor to S4.", "tokens": [51504, 440, 700, 644, 10538, 257, 3170, 1219, 389, 9139, 34885, 11, 597, 390, 264, 34991, 281, 318, 19, 13, 51762], "temperature": 0.0, "avg_logprob": -0.14438479961735187, "compression_ratio": 1.5752895752895753, "no_speech_prob": 0.00028672211919911206}, {"id": 62, "seek": 27648, "start": 276.48, "end": 282.0, "text": " And it's a new conceptual framework for the online memorization of signals and led to", "tokens": [50364, 400, 309, 311, 257, 777, 24106, 8388, 337, 264, 2950, 10560, 2144, 295, 12354, 293, 4684, 281, 50640], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 63, "seek": 27648, "start": 282.0, "end": 285.32, "text": " a new method for modeling signals and sequences.", "tokens": [50640, 257, 777, 3170, 337, 15983, 12354, 293, 22978, 13, 50806], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 64, "seek": 27648, "start": 285.32, "end": 289.56, "text": " And then the second part will be S4, which built right on top of HIPPO.", "tokens": [50806, 400, 550, 264, 1150, 644, 486, 312, 318, 19, 11, 597, 3094, 558, 322, 1192, 295, 389, 9139, 34885, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 65, "seek": 27648, "start": 289.56, "end": 293.12, "text": " And it has a lot of important properties that have been very effective for addressing some", "tokens": [51018, 400, 309, 575, 257, 688, 295, 1021, 7221, 300, 362, 668, 588, 4942, 337, 14329, 512, 51196], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 66, "seek": 27648, "start": 293.12, "end": 295.24, "text": " types of sequence modeling problems.", "tokens": [51196, 3467, 295, 8310, 15983, 2740, 13, 51302], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 67, "seek": 27648, "start": 295.24, "end": 299.24, "text": " And before I get into the technical stuff, I'll give a quick preview experimental results", "tokens": [51302, 400, 949, 286, 483, 666, 264, 6191, 1507, 11, 286, 603, 976, 257, 1702, 14281, 17069, 3542, 51502], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 68, "seek": 27648, "start": 299.24, "end": 304.12, "text": " to highlight the types of improvements I will see and what it's good at.", "tokens": [51502, 281, 5078, 264, 3467, 295, 13797, 286, 486, 536, 293, 437, 309, 311, 665, 412, 13, 51746], "temperature": 0.0, "avg_logprob": -0.1042284924759824, "compression_ratio": 1.6402640264026402, "no_speech_prob": 0.0011327652027830482}, {"id": 69, "seek": 30412, "start": 304.12, "end": 307.08, "text": " And this will kind of illustrate the types of challenges that we'll hope to address with", "tokens": [50364, 400, 341, 486, 733, 295, 23221, 264, 3467, 295, 4759, 300, 321, 603, 1454, 281, 2985, 365, 50512], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 70, "seek": 30412, "start": 307.08, "end": 310.2, "text": " these new models.", "tokens": [50512, 613, 777, 5245, 13, 50668], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 71, "seek": 30412, "start": 310.2, "end": 314.52, "text": " The first challenge overall is just going to be to signal or general temporal data that", "tokens": [50668, 440, 700, 3430, 4787, 307, 445, 516, 281, 312, 281, 6358, 420, 2674, 30881, 1412, 300, 50884], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 72, "seek": 30412, "start": 314.52, "end": 315.6, "text": " I just defined.", "tokens": [50884, 286, 445, 7642, 13, 50938], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 73, "seek": 30412, "start": 315.6, "end": 317.0, "text": " And this data is really everywhere.", "tokens": [50938, 400, 341, 1412, 307, 534, 5315, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 74, "seek": 30412, "start": 317.0, "end": 323.76, "text": " So some examples include audio waveforms, spatial temporal data like videos, biosignals", "tokens": [51008, 407, 512, 5110, 4090, 6278, 36512, 82, 11, 23598, 30881, 1412, 411, 2145, 11, 36997, 788, 1124, 51346], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 75, "seek": 30412, "start": 323.76, "end": 330.84000000000003, "text": " like electrocardiograms, which have important applications of medicine, or market and financial", "tokens": [51346, 411, 16717, 22259, 72, 12820, 82, 11, 597, 362, 1021, 5821, 295, 7195, 11, 420, 2142, 293, 4669, 51700], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 76, "seek": 30412, "start": 330.84000000000003, "end": 332.2, "text": " data.", "tokens": [51700, 1412, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1854233232516687, "compression_ratio": 1.6577946768060836, "no_speech_prob": 0.0027136134449392557}, {"id": 77, "seek": 33220, "start": 332.28, "end": 335.52, "text": " And then there will be multiple time series logs being generated by every major industry", "tokens": [50368, 400, 550, 456, 486, 312, 3866, 565, 2638, 20820, 885, 10833, 538, 633, 2563, 3518, 50530], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 78, "seek": 33220, "start": 335.52, "end": 339.03999999999996, "text": " and many other types of scientific modeling problems.", "tokens": [50530, 293, 867, 661, 3467, 295, 8134, 15983, 2740, 13, 50706], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 79, "seek": 33220, "start": 339.03999999999996, "end": 344.36, "text": " And we'll return to these experiments later with a particular focus on some biosignal", "tokens": [50706, 400, 321, 603, 2736, 281, 613, 12050, 1780, 365, 257, 1729, 1879, 322, 512, 36997, 788, 304, 50972], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 80, "seek": 33220, "start": 344.36, "end": 345.36, "text": " data.", "tokens": [50972, 1412, 13, 51022], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 81, "seek": 33220, "start": 345.36, "end": 351.08, "text": " But for now, I will just use one example to illustrate, which is audio.", "tokens": [51022, 583, 337, 586, 11, 286, 486, 445, 764, 472, 1365, 281, 23221, 11, 597, 307, 6278, 13, 51308], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 82, "seek": 33220, "start": 351.08, "end": 355.32, "text": " And audio is actually one of the most common types of data because it's just raw sound.", "tokens": [51308, 400, 6278, 307, 767, 472, 295, 264, 881, 2689, 3467, 295, 1412, 570, 309, 311, 445, 8936, 1626, 13, 51520], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 83, "seek": 33220, "start": 355.32, "end": 356.32, "text": " It's everywhere.", "tokens": [51520, 467, 311, 5315, 13, 51570], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 84, "seek": 33220, "start": 356.32, "end": 361.2, "text": " And so to illustrate, machine learning right now is really all about text and so many", "tokens": [51570, 400, 370, 281, 23221, 11, 3479, 2539, 558, 586, 307, 534, 439, 466, 2487, 293, 370, 867, 51814], "temperature": 0.0, "avg_logprob": -0.24147057937363448, "compression_ratio": 1.679054054054054, "no_speech_prob": 0.0013452150160446763}, {"id": 85, "seek": 36120, "start": 361.2, "end": 365.24, "text": " headline results recently have been about people scraping together all the raw text", "tokens": [50364, 28380, 3542, 3938, 362, 668, 466, 561, 43738, 1214, 439, 264, 8936, 2487, 50566], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 86, "seek": 36120, "start": 365.24, "end": 368.03999999999996, "text": " data they can get, creating massive models on them.", "tokens": [50566, 1412, 436, 393, 483, 11, 4084, 5994, 5245, 322, 552, 13, 50706], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 87, "seek": 36120, "start": 368.03999999999996, "end": 373.0, "text": " And that's led to very impressive results like GPT-3, which I don't know the audience,", "tokens": [50706, 400, 300, 311, 4684, 281, 588, 8992, 3542, 411, 26039, 51, 12, 18, 11, 597, 286, 500, 380, 458, 264, 4034, 11, 50954], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 88, "seek": 36120, "start": 373.0, "end": 377.4, "text": " but hopefully many of you have heard of this model.", "tokens": [50954, 457, 4696, 867, 295, 291, 362, 2198, 295, 341, 2316, 13, 51174], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 89, "seek": 36120, "start": 377.4, "end": 380.96, "text": " In contrast, audio actually has orders of magnitude more data than text.", "tokens": [51174, 682, 8712, 11, 6278, 767, 575, 9470, 295, 15668, 544, 1412, 813, 2487, 13, 51352], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 90, "seek": 36120, "start": 380.96, "end": 384.88, "text": " For example, a single labeled dataset has more data set than all of the data used to", "tokens": [51352, 1171, 1365, 11, 257, 2167, 21335, 28872, 575, 544, 1412, 992, 813, 439, 295, 264, 1412, 1143, 281, 51548], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 91, "seek": 36120, "start": 384.88, "end": 387.08, "text": " train those massive language models.", "tokens": [51548, 3847, 729, 5994, 2856, 5245, 13, 51658], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 92, "seek": 36120, "start": 387.08, "end": 389.59999999999997, "text": " But you don't hear about benchmarks in this domain nearly as much.", "tokens": [51658, 583, 291, 500, 380, 1568, 466, 43751, 294, 341, 9274, 6217, 382, 709, 13, 51784], "temperature": 0.0, "avg_logprob": -0.14424406637356976, "compression_ratio": 1.6908517350157728, "no_speech_prob": 0.0009845757158473134}, {"id": 93, "seek": 38960, "start": 389.8, "end": 395.28000000000003, "text": " And I think part of the reason is just because audio models are, audio is very challenging", "tokens": [50374, 400, 286, 519, 644, 295, 264, 1778, 307, 445, 570, 6278, 5245, 366, 11, 6278, 307, 588, 7595, 50648], "temperature": 0.0, "avg_logprob": -0.18895696084710617, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0011325788218528032}, {"id": 94, "seek": 38960, "start": 395.28000000000003, "end": 399.64000000000004, "text": " and current models seem much worse in comparison to text.", "tokens": [50648, 293, 2190, 5245, 1643, 709, 5324, 294, 9660, 281, 2487, 13, 50866], "temperature": 0.0, "avg_logprob": -0.18895696084710617, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0011325788218528032}, {"id": 95, "seek": 38960, "start": 399.64000000000004, "end": 407.68, "text": " And so here's a concrete example where we consider basically a very general and hard", "tokens": [50866, 400, 370, 510, 311, 257, 9859, 1365, 689, 321, 1949, 1936, 257, 588, 2674, 293, 1152, 51268], "temperature": 0.0, "avg_logprob": -0.18895696084710617, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0011325788218528032}, {"id": 96, "seek": 38960, "start": 407.68, "end": 414.40000000000003, "text": " audio generation setting of generating spoken digits, zero to nine, using a completely", "tokens": [51268, 6278, 5125, 3287, 295, 17746, 10759, 27011, 11, 4018, 281, 4949, 11, 1228, 257, 2584, 51604], "temperature": 0.0, "avg_logprob": -0.18895696084710617, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0011325788218528032}, {"id": 97, "seek": 38960, "start": 414.40000000000003, "end": 416.64000000000004, "text": " unconditional autoregressive model.", "tokens": [51604, 47916, 1476, 418, 3091, 488, 2316, 13, 51716], "temperature": 0.0, "avg_logprob": -0.18895696084710617, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.0011325788218528032}, {"id": 98, "seek": 41664, "start": 416.64, "end": 420.12, "text": " And the gold standard here is a baseline called WaveNet.", "tokens": [50364, 400, 264, 3821, 3832, 510, 307, 257, 20518, 1219, 28530, 31890, 13, 50538], "temperature": 0.0, "avg_logprob": -0.26438015454436004, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.013833549804985523}, {"id": 99, "seek": 41664, "start": 420.12, "end": 423.76, "text": " And here's what it sounds like trying to say these numbers.", "tokens": [50538, 400, 510, 311, 437, 309, 3263, 411, 1382, 281, 584, 613, 3547, 13, 50720], "temperature": 0.0, "avg_logprob": -0.26438015454436004, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.013833549804985523}, {"id": 100, "seek": 41664, "start": 431.76, "end": 434.36, "text": " So it's, it's not very good.", "tokens": [51120, 407, 309, 311, 11, 309, 311, 406, 588, 665, 13, 51250], "temperature": 0.0, "avg_logprob": -0.26438015454436004, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.013833549804985523}, {"id": 101, "seek": 41664, "start": 434.36, "end": 438.32, "text": " And here's results for S4, which was just, these results are just from the past like", "tokens": [51250, 400, 510, 311, 3542, 337, 318, 19, 11, 597, 390, 445, 11, 613, 3542, 366, 445, 490, 264, 1791, 411, 51448], "temperature": 0.0, "avg_logprob": -0.26438015454436004, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.013833549804985523}, {"id": 102, "seek": 41664, "start": 438.32, "end": 440.32, "text": " two months or so ago.", "tokens": [51448, 732, 2493, 420, 370, 2057, 13, 51548], "temperature": 0.0, "avg_logprob": -0.26438015454436004, "compression_ratio": 1.4651162790697674, "no_speech_prob": 0.013833549804985523}, {"id": 103, "seek": 44032, "start": 441.32, "end": 447.8, "text": " One, two, three, four.", "tokens": [50414, 1485, 11, 732, 11, 1045, 11, 1451, 13, 50738], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 104, "seek": 44032, "start": 447.8, "end": 451.59999999999997, "text": " So that's, that's a pretty concrete example.", "tokens": [50738, 407, 300, 311, 11, 300, 311, 257, 1238, 9859, 1365, 13, 50928], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 105, "seek": 44032, "start": 451.59999999999997, "end": 455.44, "text": " And so in this talk, we'll see how models like S4 are kind of designed for signals in", "tokens": [50928, 400, 370, 294, 341, 751, 11, 321, 603, 536, 577, 5245, 411, 318, 19, 366, 733, 295, 4761, 337, 12354, 294, 51120], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 106, "seek": 44032, "start": 455.44, "end": 459.2, "text": " a way and can have significant advantages for this type of data.", "tokens": [51120, 257, 636, 293, 393, 362, 4776, 14906, 337, 341, 2010, 295, 1412, 13, 51308], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 107, "seek": 44032, "start": 459.2, "end": 464.92, "text": " And the second example up front, or the second example of a running challenge will be, can", "tokens": [51308, 400, 264, 1150, 1365, 493, 1868, 11, 420, 264, 1150, 1365, 295, 257, 2614, 3430, 486, 312, 11, 393, 51594], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 108, "seek": 44032, "start": 464.92, "end": 467.48, "text": " be motivated by examining audio more closely.", "tokens": [51594, 312, 14515, 538, 34662, 6278, 544, 8185, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1855398543337558, "compression_ratio": 1.5919282511210762, "no_speech_prob": 0.0006665027467533946}, {"id": 109, "seek": 46748, "start": 467.48, "end": 471.32, "text": " And one reason why audio is so hard is because it's sampled at such an extremely high rate", "tokens": [50364, 400, 472, 1778, 983, 6278, 307, 370, 1152, 307, 570, 309, 311, 3247, 15551, 412, 1270, 364, 4664, 1090, 3314, 50556], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 110, "seek": 46748, "start": 471.32, "end": 475.40000000000003, "text": " where a single second has 16,000 or more samples.", "tokens": [50556, 689, 257, 2167, 1150, 575, 3165, 11, 1360, 420, 544, 10938, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 111, "seek": 46748, "start": 475.40000000000003, "end": 480.8, "text": " In contrast, most sequence models can't deal with more than a thousand or so.", "tokens": [50760, 682, 8712, 11, 881, 8310, 5245, 393, 380, 2028, 365, 544, 813, 257, 4714, 420, 370, 13, 51030], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 112, "seek": 46748, "start": 480.8, "end": 484.32, "text": " And to illustrate, there was a benchmark in the past year called Long Range Arena that", "tokens": [51030, 400, 281, 23221, 11, 456, 390, 257, 18927, 294, 264, 1791, 1064, 1219, 8282, 33778, 34290, 300, 51206], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 113, "seek": 46748, "start": 484.32, "end": 487.76, "text": " measured the performance of models on a suite of long range tasks.", "tokens": [51206, 12690, 264, 3389, 295, 5245, 322, 257, 14205, 295, 938, 3613, 9608, 13, 51378], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 114, "seek": 46748, "start": 487.76, "end": 493.0, "text": " And the most popular sequence models these days, Transformers, were the main focus.", "tokens": [51378, 400, 264, 881, 3743, 8310, 5245, 613, 1708, 11, 27938, 433, 11, 645, 264, 2135, 1879, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 115, "seek": 46748, "start": 493.0, "end": 497.20000000000005, "text": " But despite their many successes, they don't do so well on long context.", "tokens": [51640, 583, 7228, 641, 867, 26101, 11, 436, 500, 380, 360, 370, 731, 322, 938, 4319, 13, 51850], "temperature": 0.0, "avg_logprob": -0.1052149161696434, "compression_ratio": 1.6635220125786163, "no_speech_prob": 0.002549484372138977}, {"id": 116, "seek": 49720, "start": 497.2, "end": 501.44, "text": " And so there were dozens of variants that were tried, and they all get to around the", "tokens": [50364, 400, 370, 456, 645, 18431, 295, 21669, 300, 645, 3031, 11, 293, 436, 439, 483, 281, 926, 264, 50576], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 117, "seek": 49720, "start": 501.44, "end": 506.92, "text": " same performance, which is actually not much above random guessing.", "tokens": [50576, 912, 3389, 11, 597, 307, 767, 406, 709, 3673, 4974, 17939, 13, 50850], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 118, "seek": 49720, "start": 506.92, "end": 512.48, "text": " In contrast, S4 we'll see is explicitly designed to be effective on long context, which leads", "tokens": [50850, 682, 8712, 11, 318, 19, 321, 603, 536, 307, 20803, 4761, 281, 312, 4942, 322, 938, 4319, 11, 597, 6689, 51128], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 119, "seek": 49720, "start": 512.48, "end": 514.96, "text": " to a huge improvement on this benchmark.", "tokens": [51128, 281, 257, 2603, 10444, 322, 341, 18927, 13, 51252], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 120, "seek": 49720, "start": 514.96, "end": 519.36, "text": " And it's the first model to ever make progress on some really difficult long sequence tasks.", "tokens": [51252, 400, 309, 311, 264, 700, 2316, 281, 1562, 652, 4205, 322, 512, 534, 2252, 938, 8310, 9608, 13, 51472], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 121, "seek": 49720, "start": 519.36, "end": 523.3199999999999, "text": " Can I ask a quick question here, Albert?", "tokens": [51472, 1664, 286, 1029, 257, 1702, 1168, 510, 11, 20812, 30, 51670], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 122, "seek": 49720, "start": 523.3199999999999, "end": 524.3199999999999, "text": " Yeah.", "tokens": [51670, 865, 13, 51720], "temperature": 0.0, "avg_logprob": -0.16330728164085975, "compression_ratio": 1.5359712230215827, "no_speech_prob": 0.00024529302027076483}, {"id": 123, "seek": 52432, "start": 524.7600000000001, "end": 529.36, "text": " So in the previous task, that was a generative process.", "tokens": [50386, 407, 294, 264, 3894, 5633, 11, 300, 390, 257, 1337, 1166, 1399, 13, 50616], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 124, "seek": 52432, "start": 529.36, "end": 536.6800000000001, "text": " And in this, the Long Context Channel challenge, is it a classification or what kind of task", "tokens": [50616, 400, 294, 341, 11, 264, 8282, 4839, 3828, 13553, 3430, 11, 307, 309, 257, 21538, 420, 437, 733, 295, 5633, 50982], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 125, "seek": 52432, "start": 536.6800000000001, "end": 537.6800000000001, "text": " is it?", "tokens": [50982, 307, 309, 30, 51032], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 126, "seek": 52432, "start": 537.6800000000001, "end": 539.08, "text": " These are all classification problems.", "tokens": [51032, 1981, 366, 439, 21538, 2740, 13, 51102], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 127, "seek": 52432, "start": 539.08, "end": 545.2, "text": " And they're on data such that includes several data modalities, such as text, images, some", "tokens": [51102, 400, 436, 434, 322, 1412, 1270, 300, 5974, 2940, 1412, 1072, 16110, 11, 1270, 382, 2487, 11, 5267, 11, 512, 51408], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 128, "seek": 52432, "start": 545.2, "end": 548.44, "text": " sort of symbolic processing, stuff like that.", "tokens": [51408, 1333, 295, 25755, 9007, 11, 1507, 411, 300, 13, 51570], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 129, "seek": 52432, "start": 548.44, "end": 549.44, "text": " I see.", "tokens": [51570, 286, 536, 13, 51620], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 130, "seek": 52432, "start": 549.44, "end": 553.44, "text": " And so you can use S4 both as a generative model to actually generate sequences.", "tokens": [51620, 400, 370, 291, 393, 764, 318, 19, 1293, 382, 257, 1337, 1166, 2316, 281, 767, 8460, 22978, 13, 51820], "temperature": 0.0, "avg_logprob": -0.2104519930752841, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.000842324981931597}, {"id": 131, "seek": 55344, "start": 553.6800000000001, "end": 554.6800000000001, "text": " Yeah.", "tokens": [50376, 865, 13, 50426], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 132, "seek": 55344, "start": 554.6800000000001, "end": 560.6400000000001, "text": " So a lot of sequence models, again, a sequence model I'm defining as a black box interface,", "tokens": [50426, 407, 257, 688, 295, 8310, 5245, 11, 797, 11, 257, 8310, 2316, 286, 478, 17827, 382, 257, 2211, 2424, 9226, 11, 50724], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 133, "seek": 55344, "start": 560.6400000000001, "end": 563.2800000000001, "text": " really, that's just a sequence-to-sequence map.", "tokens": [50724, 534, 11, 300, 311, 445, 257, 8310, 12, 1353, 12, 11834, 655, 4471, 13, 50856], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 134, "seek": 55344, "start": 563.2800000000001, "end": 568.6400000000001, "text": " And many of these can be used in many ways, both for classification and generation.", "tokens": [50856, 400, 867, 295, 613, 393, 312, 1143, 294, 867, 2098, 11, 1293, 337, 21538, 293, 5125, 13, 51124], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 135, "seek": 55344, "start": 568.6400000000001, "end": 574.32, "text": " For example, transformers or RNNs are similar things that satisfy the same interface and", "tokens": [51124, 1171, 1365, 11, 4088, 433, 420, 45702, 45, 82, 366, 2531, 721, 300, 19319, 264, 912, 9226, 293, 51408], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 136, "seek": 55344, "start": 574.32, "end": 576.2800000000001, "text": " can be used in many ways as well.", "tokens": [51408, 393, 312, 1143, 294, 867, 2098, 382, 731, 13, 51506], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 137, "seek": 55344, "start": 576.2800000000001, "end": 577.2800000000001, "text": " Gotcha.", "tokens": [51506, 42109, 13, 51556], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 138, "seek": 55344, "start": 577.2800000000001, "end": 578.2800000000001, "text": " Thank you.", "tokens": [51556, 1044, 291, 13, 51606], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 139, "seek": 55344, "start": 579.96, "end": 580.96, "text": " Yeah.", "tokens": [51690, 865, 13, 51740], "temperature": 0.0, "avg_logprob": -0.24996792828595196, "compression_ratio": 1.6180257510729614, "no_speech_prob": 0.0005438851658254862}, {"id": 140, "seek": 58096, "start": 581.08, "end": 583.1600000000001, "text": " OK, so now I'll get into the technical portions.", "tokens": [50370, 2264, 11, 370, 586, 286, 603, 483, 666, 264, 6191, 25070, 13, 50474], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 141, "seek": 58096, "start": 583.1600000000001, "end": 586.36, "text": " And the first part will be about Hippo, as I mentioned.", "tokens": [50474, 400, 264, 700, 644, 486, 312, 466, 2421, 27000, 11, 382, 286, 2835, 13, 50634], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 142, "seek": 58096, "start": 586.36, "end": 592.96, "text": " And to motivate what Hippo's goal was, I gave a bunch of examples of data that machine", "tokens": [50634, 400, 281, 28497, 437, 2421, 27000, 311, 3387, 390, 11, 286, 2729, 257, 3840, 295, 5110, 295, 1412, 300, 3479, 50964], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 143, "seek": 58096, "start": 592.96, "end": 596.64, "text": " learning models currently struggle with, particularly things like time series.", "tokens": [50964, 2539, 5245, 4362, 7799, 365, 11, 4098, 721, 411, 565, 2638, 13, 51148], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 144, "seek": 58096, "start": 596.64, "end": 600.6, "text": " And to highlight why this is hard, I'm going to use a running example to illustrate a very", "tokens": [51148, 400, 281, 5078, 983, 341, 307, 1152, 11, 286, 478, 516, 281, 764, 257, 2614, 1365, 281, 23221, 257, 588, 51346], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 145, "seek": 58096, "start": 600.6, "end": 606.0, "text": " basic capability that's difficult for modern models.", "tokens": [51346, 3875, 13759, 300, 311, 2252, 337, 4363, 5245, 13, 51616], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 146, "seek": 58096, "start": 606.0, "end": 609.4000000000001, "text": " And that's the moving average, which is perhaps the most basic method in modern time series", "tokens": [51616, 400, 300, 311, 264, 2684, 4274, 11, 597, 307, 4317, 264, 881, 3875, 3170, 294, 4363, 565, 2638, 51786], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 147, "seek": 58096, "start": 609.4000000000001, "end": 610.64, "text": " analysis.", "tokens": [51786, 5215, 13, 51848], "temperature": 0.0, "avg_logprob": -0.13384658813476563, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.00033001412521116436}, {"id": 148, "seek": 61064, "start": 610.64, "end": 615.12, "text": " So this figure depicts the exponential moving average or EMA, which is the blue line.", "tokens": [50364, 407, 341, 2573, 48949, 264, 21510, 2684, 4274, 420, 462, 9998, 11, 597, 307, 264, 3344, 1622, 13, 50588], "temperature": 0.0, "avg_logprob": -0.12427837735130673, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.310758468927816e-05}, {"id": 149, "seek": 61064, "start": 615.12, "end": 620.36, "text": " And the way it's used is that it's a fixed non-learnable feature that's often the first", "tokens": [50588, 400, 264, 636, 309, 311, 1143, 307, 300, 309, 311, 257, 6806, 2107, 12, 306, 1083, 712, 4111, 300, 311, 2049, 264, 700, 50850], "temperature": 0.0, "avg_logprob": -0.12427837735130673, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.310758468927816e-05}, {"id": 150, "seek": 61064, "start": 620.36, "end": 626.08, "text": " pre-processing step that's performed in any sort of time series analysis pipeline.", "tokens": [50850, 659, 12, 41075, 278, 1823, 300, 311, 10332, 294, 604, 1333, 295, 565, 2638, 5215, 15517, 13, 51136], "temperature": 0.0, "avg_logprob": -0.12427837735130673, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.310758468927816e-05}, {"id": 151, "seek": 61064, "start": 626.08, "end": 630.96, "text": " Now in the context, in the spirit of machine learning and deep learning, instead of doing", "tokens": [51136, 823, 294, 264, 4319, 11, 294, 264, 3797, 295, 3479, 2539, 293, 2452, 2539, 11, 2602, 295, 884, 51380], "temperature": 0.0, "avg_logprob": -0.12427837735130673, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.310758468927816e-05}, {"id": 152, "seek": 61064, "start": 630.96, "end": 636.68, "text": " manual processing like creating these features, we really would like to be able to learn these", "tokens": [51380, 9688, 9007, 411, 4084, 613, 4122, 11, 321, 534, 576, 411, 281, 312, 1075, 281, 1466, 613, 51666], "temperature": 0.0, "avg_logprob": -0.12427837735130673, "compression_ratio": 1.7027027027027026, "no_speech_prob": 9.310758468927816e-05}, {"id": 153, "seek": 63668, "start": 636.68, "end": 641.12, "text": " sort of things automatically from the data.", "tokens": [50364, 1333, 295, 721, 6772, 490, 264, 1412, 13, 50586], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 154, "seek": 63668, "start": 641.12, "end": 646.5999999999999, "text": " And so in particular, here's a very simple concrete task is, suppose you have a model", "tokens": [50586, 400, 370, 294, 1729, 11, 510, 311, 257, 588, 2199, 9859, 5633, 307, 11, 7297, 291, 362, 257, 2316, 50860], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 155, "seek": 63668, "start": 646.5999999999999, "end": 654.92, "text": " and you're feeding it this black input signal, and you want the model to predict the EMA", "tokens": [50860, 293, 291, 434, 12919, 309, 341, 2211, 4846, 6358, 11, 293, 291, 528, 264, 2316, 281, 6069, 264, 462, 9998, 51276], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 156, "seek": 63668, "start": 654.92, "end": 657.3199999999999, "text": " or the blue signal as the output.", "tokens": [51276, 420, 264, 3344, 6358, 382, 264, 5598, 13, 51396], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 157, "seek": 63668, "start": 657.3199999999999, "end": 662.76, "text": " And fortunately, it turns out that standard sequence models, such as attention and convolutions,", "tokens": [51396, 400, 25511, 11, 309, 4523, 484, 300, 3832, 8310, 5245, 11, 1270, 382, 3202, 293, 3754, 15892, 11, 51668], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 158, "seek": 63668, "start": 662.76, "end": 665.68, "text": " cannot do this at all.", "tokens": [51668, 2644, 360, 341, 412, 439, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16799397622385331, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0120391845703125}, {"id": 159, "seek": 66568, "start": 665.68, "end": 670.92, "text": " And the reason why is essentially because the EMA has unbounded context.", "tokens": [50364, 400, 264, 1778, 983, 307, 4476, 570, 264, 462, 9998, 575, 517, 18767, 292, 4319, 13, 50626], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 160, "seek": 66568, "start": 670.92, "end": 674.5999999999999, "text": " It's actually just a weighted average of the history of the signal with an exponentially", "tokens": [50626, 467, 311, 767, 445, 257, 32807, 4274, 295, 264, 2503, 295, 264, 6358, 365, 364, 37330, 50810], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 161, "seek": 66568, "start": 674.5999999999999, "end": 676.4399999999999, "text": " decaying weight that searches back infinitely.", "tokens": [50810, 21039, 278, 3364, 300, 26701, 646, 36227, 13, 50902], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 162, "seek": 66568, "start": 676.4399999999999, "end": 681.52, "text": " Whereas in contrast, most modern models such as attention or a convolutions have finite", "tokens": [50902, 13813, 294, 8712, 11, 881, 4363, 5245, 1270, 382, 3202, 420, 257, 3754, 15892, 362, 19362, 51156], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 163, "seek": 66568, "start": 681.52, "end": 685.0, "text": " context in those.", "tokens": [51156, 4319, 294, 729, 13, 51330], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 164, "seek": 66568, "start": 685.0, "end": 687.5999999999999, "text": " Some people wonder about other things like RNNs.", "tokens": [51330, 2188, 561, 2441, 466, 661, 721, 411, 45702, 45, 82, 13, 51460], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 165, "seek": 66568, "start": 687.5999999999999, "end": 692.8, "text": " And the short answer is that RNNs are better than attention convolutions here, but they", "tokens": [51460, 400, 264, 2099, 1867, 307, 300, 45702, 45, 82, 366, 1101, 813, 3202, 3754, 15892, 510, 11, 457, 436, 51720], "temperature": 0.0, "avg_logprob": -0.15558440730256853, "compression_ratio": 1.6580882352941178, "no_speech_prob": 0.0004171510227024555}, {"id": 166, "seek": 69280, "start": 692.8, "end": 700.3199999999999, "text": " still aren't that good due to empirical problems with optimization and other things.", "tokens": [50364, 920, 3212, 380, 300, 665, 3462, 281, 31886, 2740, 365, 19618, 293, 661, 721, 13, 50740], "temperature": 0.0, "avg_logprob": -0.16643967739371365, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004533023573458195}, {"id": 167, "seek": 69280, "start": 700.3199999999999, "end": 708.4399999999999, "text": " So we'll see that the methods that are introduced in this talk will be very naturally suited", "tokens": [50740, 407, 321, 603, 536, 300, 264, 7150, 300, 366, 7268, 294, 341, 751, 486, 312, 588, 8195, 24736, 51146], "temperature": 0.0, "avg_logprob": -0.16643967739371365, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004533023573458195}, {"id": 168, "seek": 69280, "start": 708.4399999999999, "end": 711.16, "text": " for this and are much stronger versions.", "tokens": [51146, 337, 341, 293, 366, 709, 7249, 9606, 13, 51282], "temperature": 0.0, "avg_logprob": -0.16643967739371365, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004533023573458195}, {"id": 169, "seek": 69280, "start": 711.16, "end": 715.1999999999999, "text": " But going back to the EMA, the way that one way to think about it is that it's a very", "tokens": [51282, 583, 516, 646, 281, 264, 462, 9998, 11, 264, 636, 300, 472, 636, 281, 519, 466, 309, 307, 300, 309, 311, 257, 588, 51484], "temperature": 0.0, "avg_logprob": -0.16643967739371365, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004533023573458195}, {"id": 170, "seek": 69280, "start": 715.1999999999999, "end": 718.7199999999999, "text": " simple summary of the entire history of your signal.", "tokens": [51484, 2199, 12691, 295, 264, 2302, 2503, 295, 428, 6358, 13, 51660], "temperature": 0.0, "avg_logprob": -0.16643967739371365, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.004533023573458195}, {"id": 171, "seek": 71872, "start": 719.1600000000001, "end": 725.6, "text": " In other words, it's a state X, which is a single number that summarizes the entire", "tokens": [50386, 682, 661, 2283, 11, 309, 311, 257, 1785, 1783, 11, 597, 307, 257, 2167, 1230, 300, 14611, 5660, 264, 2302, 50708], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 172, "seek": 71872, "start": 725.6, "end": 728.72, "text": " history of the input U.", "tokens": [50708, 2503, 295, 264, 4846, 624, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 173, "seek": 71872, "start": 728.72, "end": 732.8000000000001, "text": " And the reason why it's useful is that it's easy to compute because if you get new data,", "tokens": [50864, 400, 264, 1778, 983, 309, 311, 4420, 307, 300, 309, 311, 1858, 281, 14722, 570, 498, 291, 483, 777, 1412, 11, 51068], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 174, "seek": 71872, "start": 732.8000000000001, "end": 737.36, "text": " you can update the EMA in constant time using this weighted average.", "tokens": [51068, 291, 393, 5623, 264, 462, 9998, 294, 5754, 565, 1228, 341, 32807, 4274, 13, 51296], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 175, "seek": 71872, "start": 737.36, "end": 741.28, "text": " And beyond the simple example, though, I think these two properties are actually conceptually", "tokens": [51296, 400, 4399, 264, 2199, 1365, 11, 1673, 11, 286, 519, 613, 732, 7221, 366, 767, 3410, 671, 51492], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 176, "seek": 71872, "start": 741.28, "end": 743.1600000000001, "text": " really important.", "tokens": [51492, 534, 1021, 13, 51586], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 177, "seek": 71872, "start": 743.1600000000001, "end": 746.52, "text": " For example, they're exactly the properties that you need in any sort of real-time decision-making", "tokens": [51586, 1171, 1365, 11, 436, 434, 2293, 264, 7221, 300, 291, 643, 294, 604, 1333, 295, 957, 12, 3766, 3537, 12, 12402, 51754], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 178, "seek": 71872, "start": 746.52, "end": 747.52, "text": " problem.", "tokens": [51754, 1154, 13, 51804], "temperature": 0.0, "avg_logprob": -0.1307061930171779, "compression_ratio": 1.6609589041095891, "no_speech_prob": 0.0011691795662045479}, {"id": 179, "seek": 74752, "start": 747.52, "end": 751.1999999999999, "text": " And really abstractly, you can even imagine that your brain is a state that's summarizing", "tokens": [50364, 400, 534, 12649, 356, 11, 291, 393, 754, 3811, 300, 428, 3567, 307, 257, 1785, 300, 311, 14611, 3319, 50548], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 180, "seek": 74752, "start": 751.1999999999999, "end": 755.64, "text": " the entire context of your life, and it's constantly updating as you acquire new information.", "tokens": [50548, 264, 2302, 4319, 295, 428, 993, 11, 293, 309, 311, 6460, 25113, 382, 291, 20001, 777, 1589, 13, 50770], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 181, "seek": 74752, "start": 755.64, "end": 760.0799999999999, "text": " So I think that's actually a pretty general important question, and this was a direct", "tokens": [50770, 407, 286, 519, 300, 311, 767, 257, 1238, 2674, 1021, 1168, 11, 293, 341, 390, 257, 2047, 50992], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 182, "seek": 74752, "start": 760.0799999999999, "end": 762.84, "text": " inspiration for HIPPO.", "tokens": [50992, 10249, 337, 389, 9139, 34885, 13, 51130], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 183, "seek": 74752, "start": 762.84, "end": 767.28, "text": " In the context of machine learning, this question has a lot of direct impact on our models because,", "tokens": [51130, 682, 264, 4319, 295, 3479, 2539, 11, 341, 1168, 575, 257, 688, 295, 2047, 2712, 322, 527, 5245, 570, 11, 51352], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 184, "seek": 74752, "start": 767.28, "end": 769.4399999999999, "text": " as I mentioned, they struggle with long context.", "tokens": [51352, 382, 286, 2835, 11, 436, 7799, 365, 938, 4319, 13, 51460], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 185, "seek": 74752, "start": 769.4399999999999, "end": 774.28, "text": " For example, text models, it's been shown typically have a context range of about 100", "tokens": [51460, 1171, 1365, 11, 2487, 5245, 11, 309, 311, 668, 4898, 5850, 362, 257, 4319, 3613, 295, 466, 2319, 51702], "temperature": 0.0, "avg_logprob": -0.16736422815630514, "compression_ratio": 1.689102564102564, "no_speech_prob": 0.00014420843217521906}, {"id": 186, "seek": 77428, "start": 774.28, "end": 776.4399999999999, "text": " to at most a few thousand tokens.", "tokens": [50364, 281, 412, 881, 257, 1326, 4714, 22667, 13, 50472], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 187, "seek": 77428, "start": 776.4399999999999, "end": 782.48, "text": " Whereas if you want to deal with data such as speech and audio, a single word in speech", "tokens": [50472, 13813, 498, 291, 528, 281, 2028, 365, 1412, 1270, 382, 6218, 293, 6278, 11, 257, 2167, 1349, 294, 6218, 50774], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 188, "seek": 77428, "start": 782.48, "end": 787.8399999999999, "text": " is a sequence of length more than 10,000, and this can really stretch to unbounded length.", "tokens": [50774, 307, 257, 8310, 295, 4641, 544, 813, 1266, 11, 1360, 11, 293, 341, 393, 534, 5985, 281, 517, 18767, 292, 4641, 13, 51042], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 189, "seek": 77428, "start": 787.8399999999999, "end": 793.8, "text": " And so this is the question that I was trying to, that I was thinking about.", "tokens": [51042, 400, 370, 341, 307, 264, 1168, 300, 286, 390, 1382, 281, 11, 300, 286, 390, 1953, 466, 13, 51340], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 190, "seek": 77428, "start": 793.8, "end": 798.88, "text": " And what I did was I tried to convert this vague goal of long-range memory into a more", "tokens": [51340, 400, 437, 286, 630, 390, 286, 3031, 281, 7620, 341, 24247, 3387, 295, 938, 12, 14521, 4675, 666, 257, 544, 51594], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 191, "seek": 77428, "start": 798.88, "end": 801.24, "text": " formal mathematical question.", "tokens": [51594, 9860, 18894, 1168, 13, 51712], "temperature": 0.0, "avg_logprob": -0.14946255456833613, "compression_ratio": 1.6305220883534137, "no_speech_prob": 0.0037037658039480448}, {"id": 192, "seek": 80124, "start": 801.24, "end": 806.5600000000001, "text": " The conceptual idea is that if you can compress the past into a smaller state that's accurately", "tokens": [50364, 440, 24106, 1558, 307, 300, 498, 291, 393, 14778, 264, 1791, 666, 257, 4356, 1785, 300, 311, 20095, 50630], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 193, "seek": 80124, "start": 806.5600000000001, "end": 812.6800000000001, "text": " remembering it, then you should be able to reconstruct the past.", "tokens": [50630, 20719, 309, 11, 550, 291, 820, 312, 1075, 281, 31499, 264, 1791, 13, 50936], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 194, "seek": 80124, "start": 812.6800000000001, "end": 815.52, "text": " And we can then attempt to turn this into a technical problem.", "tokens": [50936, 400, 321, 393, 550, 5217, 281, 1261, 341, 666, 257, 6191, 1154, 13, 51078], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 195, "seek": 80124, "start": 815.52, "end": 819.44, "text": " So the idea is that we're going to observe an input signal online and try to maintain", "tokens": [51078, 407, 264, 1558, 307, 300, 321, 434, 516, 281, 11441, 364, 4846, 6358, 2950, 293, 853, 281, 6909, 51274], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 196, "seek": 80124, "start": 819.44, "end": 823.8, "text": " a good representation of it that allows us to reconstruct it.", "tokens": [51274, 257, 665, 10290, 295, 309, 300, 4045, 505, 281, 31499, 309, 13, 51492], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 197, "seek": 80124, "start": 823.8, "end": 828.0, "text": " And so, okay, so first in this section, I'm going to formalize this idea, and then I'll", "tokens": [51492, 400, 370, 11, 1392, 11, 370, 700, 294, 341, 3541, 11, 286, 478, 516, 281, 9860, 1125, 341, 1558, 11, 293, 550, 286, 603, 51702], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 198, "seek": 80124, "start": 828.0, "end": 830.08, "text": " define HIPPO and visualize it.", "tokens": [51702, 6964, 389, 9139, 34885, 293, 23273, 309, 13, 51806], "temperature": 0.0, "avg_logprob": -0.10760578562001713, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.0006561886402778327}, {"id": 199, "seek": 83008, "start": 830.08, "end": 833.96, "text": " And then talk about a couple of generalizations.", "tokens": [50364, 400, 550, 751, 466, 257, 1916, 295, 2674, 14455, 13, 50558], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 200, "seek": 83008, "start": 833.96, "end": 837.12, "text": " So the first thing is that let me formalize this idea that I just mentioned.", "tokens": [50558, 407, 264, 700, 551, 307, 300, 718, 385, 9860, 1125, 341, 1558, 300, 286, 445, 2835, 13, 50716], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 201, "seek": 83008, "start": 837.12, "end": 842.96, "text": " And so the idea of HIPPO is that, again, we're trying to observe an input signal online,", "tokens": [50716, 400, 370, 264, 1558, 295, 389, 9139, 34885, 307, 300, 11, 797, 11, 321, 434, 1382, 281, 11441, 364, 4846, 6358, 2950, 11, 51008], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 202, "seek": 83008, "start": 842.96, "end": 847.36, "text": " and we're going to try to encode it as well as possible given a memory budget.", "tokens": [51008, 293, 321, 434, 516, 281, 853, 281, 2058, 1429, 309, 382, 731, 382, 1944, 2212, 257, 4675, 4706, 13, 51228], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 203, "seek": 83008, "start": 847.36, "end": 850.64, "text": " So concretely, you'd think of it like this.", "tokens": [51228, 407, 39481, 736, 11, 291, 1116, 519, 295, 309, 411, 341, 13, 51392], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 204, "seek": 83008, "start": 850.64, "end": 855.5200000000001, "text": " So suppose at some initial time, T0, we've seen part of the input, and we're going to", "tokens": [51392, 407, 7297, 412, 512, 5883, 565, 11, 314, 15, 11, 321, 600, 1612, 644, 295, 264, 4846, 11, 293, 321, 434, 516, 281, 51636], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 205, "seek": 83008, "start": 855.5200000000001, "end": 856.88, "text": " try to compress this input.", "tokens": [51636, 853, 281, 14778, 341, 4846, 13, 51704], "temperature": 0.0, "avg_logprob": -0.15178553263346353, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0005881935358047485}, {"id": 206, "seek": 85688, "start": 856.88, "end": 860.72, "text": " So what you can do is store the best approximation to what we've seen so far.", "tokens": [50364, 407, 437, 291, 393, 360, 307, 3531, 264, 1151, 28023, 281, 437, 321, 600, 1612, 370, 1400, 13, 50556], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 207, "seek": 85688, "start": 860.72, "end": 865.88, "text": " For example, we can create the best polynomial approximation and write down the coefficients", "tokens": [50556, 1171, 1365, 11, 321, 393, 1884, 264, 1151, 26110, 28023, 293, 2464, 760, 264, 31994, 50814], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 208, "seek": 85688, "start": 865.88, "end": 867.56, "text": " of that polynomial.", "tokens": [50814, 295, 300, 26110, 13, 50898], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 209, "seek": 85688, "start": 867.56, "end": 873.12, "text": " So now the degree of the polynomial or the number of coefficients is the memory budget.", "tokens": [50898, 407, 586, 264, 4314, 295, 264, 26110, 420, 264, 1230, 295, 31994, 307, 264, 4675, 4706, 13, 51176], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 210, "seek": 85688, "start": 873.12, "end": 875.4, "text": " And we want to do this continuously at all times.", "tokens": [51176, 400, 321, 528, 281, 360, 341, 15684, 412, 439, 1413, 13, 51290], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 211, "seek": 85688, "start": 875.4, "end": 882.48, "text": " So as we keep seeing more data at some later time T1, we'll have to update our best approximation", "tokens": [51290, 407, 382, 321, 1066, 2577, 544, 1412, 412, 512, 1780, 565, 314, 16, 11, 321, 603, 362, 281, 5623, 527, 1151, 28023, 51644], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 212, "seek": 85688, "start": 882.48, "end": 885.04, "text": " and write down the new coefficients.", "tokens": [51644, 293, 2464, 760, 264, 777, 31994, 13, 51772], "temperature": 0.0, "avg_logprob": -0.10372364737770774, "compression_ratio": 1.852, "no_speech_prob": 0.00020983029389753938}, {"id": 213, "seek": 88504, "start": 885.04, "end": 891.5999999999999, "text": " Now the central question is, first of all, how do you actually find these optimal approximations?", "tokens": [50364, 823, 264, 5777, 1168, 307, 11, 700, 295, 439, 11, 577, 360, 291, 767, 915, 613, 16252, 8542, 763, 30, 50692], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 214, "seek": 88504, "start": 891.5999999999999, "end": 898.68, "text": " And moreover, how can you update this representation efficiently as you keep seeing more information?", "tokens": [50692, 400, 544, 3570, 11, 577, 393, 291, 5623, 341, 10290, 19621, 382, 291, 1066, 2577, 544, 1589, 30, 51046], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 215, "seek": 88504, "start": 898.68, "end": 904.36, "text": " And so this is the main conceptual idea, and it's a little bit of work to formalize a little", "tokens": [51046, 400, 370, 341, 307, 264, 2135, 24106, 1558, 11, 293, 309, 311, 257, 707, 857, 295, 589, 281, 9860, 1125, 257, 707, 51330], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 216, "seek": 88504, "start": 904.36, "end": 905.36, "text": " more.", "tokens": [51330, 544, 13, 51380], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 217, "seek": 88504, "start": 905.36, "end": 908.68, "text": " And in particular, I've been talking about optimal approximations, but that's actually", "tokens": [51380, 400, 294, 1729, 11, 286, 600, 668, 1417, 466, 16252, 8542, 763, 11, 457, 300, 311, 767, 51546], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 218, "seek": 88504, "start": 908.68, "end": 910.0799999999999, "text": " not well-defined.", "tokens": [51546, 406, 731, 12, 37716, 13, 51616], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 219, "seek": 88504, "start": 910.0799999999999, "end": 914.9599999999999, "text": " And so what we'll need is to find a measure that specifies the quality of approximation.", "tokens": [51616, 400, 370, 437, 321, 603, 643, 307, 281, 915, 257, 3481, 300, 1608, 11221, 264, 3125, 295, 28023, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11197294507707868, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0002131398068740964}, {"id": 220, "seek": 91496, "start": 914.96, "end": 918.88, "text": " For example, we can choose the exponentially decaying measure, which says that we care", "tokens": [50364, 1171, 1365, 11, 321, 393, 2826, 264, 37330, 21039, 278, 3481, 11, 597, 1619, 300, 321, 1127, 50560], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 221, "seek": 91496, "start": 918.88, "end": 925.4000000000001, "text": " about approximating the recent pass of the input more than the far pass.", "tokens": [50560, 466, 8542, 990, 264, 5162, 1320, 295, 264, 4846, 544, 813, 264, 1400, 1320, 13, 50886], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 222, "seek": 91496, "start": 925.4000000000001, "end": 929.12, "text": " And this will relate back to the EMA.", "tokens": [50886, 400, 341, 486, 10961, 646, 281, 264, 462, 9998, 13, 51072], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 223, "seek": 91496, "start": 929.12, "end": 934.1600000000001, "text": " But given this, the problem is more or less well-defined.", "tokens": [51072, 583, 2212, 341, 11, 264, 1154, 307, 544, 420, 1570, 731, 12, 37716, 13, 51324], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 224, "seek": 91496, "start": 934.1600000000001, "end": 938.6, "text": " So basically, we have to pick the measure sort of as a hyperparameter or a prior for", "tokens": [51324, 407, 1936, 11, 321, 362, 281, 1888, 264, 3481, 1333, 295, 382, 257, 9848, 2181, 335, 2398, 420, 257, 4059, 337, 51546], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 225, "seek": 91496, "start": 938.6, "end": 939.6, "text": " now.", "tokens": [51546, 586, 13, 51596], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 226, "seek": 91496, "start": 939.6, "end": 942.12, "text": " Let's talk about how you can actually learn it.", "tokens": [51596, 961, 311, 751, 466, 577, 291, 393, 767, 1466, 309, 13, 51722], "temperature": 0.0, "avg_logprob": -0.15156736913717017, "compression_ratio": 1.5657370517928286, "no_speech_prob": 0.0002957943070214242}, {"id": 227, "seek": 94212, "start": 942.6, "end": 947.2, "text": " But for now, we need to pick a measure up front, say the exponential decaying measure.", "tokens": [50388, 583, 337, 586, 11, 321, 643, 281, 1888, 257, 3481, 493, 1868, 11, 584, 264, 21510, 21039, 278, 3481, 13, 50618], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 228, "seek": 94212, "start": 947.2, "end": 951.04, "text": " And then you need to choose a polynomial basis.", "tokens": [50618, 400, 550, 291, 643, 281, 2826, 257, 26110, 5143, 13, 50810], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 229, "seek": 94212, "start": 951.04, "end": 955.12, "text": " And then the problem's completely defined, and you can write down the coefficients in", "tokens": [50810, 400, 550, 264, 1154, 311, 2584, 7642, 11, 293, 291, 393, 2464, 760, 264, 31994, 294, 51014], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 230, "seek": 94212, "start": 955.12, "end": 959.0, "text": " closed form, and you can figure out how they evolve through time.", "tokens": [51014, 5395, 1254, 11, 293, 291, 393, 2573, 484, 577, 436, 16693, 807, 565, 13, 51208], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 231, "seek": 94212, "start": 959.0, "end": 965.88, "text": " So I'm going to skip the details of the derivation, but you end up with a closed form method.", "tokens": [51208, 407, 286, 478, 516, 281, 10023, 264, 4365, 295, 264, 10151, 399, 11, 457, 291, 917, 493, 365, 257, 5395, 1254, 3170, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 232, "seek": 94212, "start": 965.88, "end": 970.16, "text": " And what I want to emphasize is that the derivation has some technically interesting", "tokens": [51552, 400, 437, 286, 528, 281, 16078, 307, 300, 264, 10151, 399, 575, 512, 12120, 1880, 51766], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 233, "seek": 94212, "start": 970.16, "end": 971.36, "text": " new ideas.", "tokens": [51766, 777, 3487, 13, 51826], "temperature": 0.0, "avg_logprob": -0.14563602871365017, "compression_ratio": 1.7122302158273381, "no_speech_prob": 0.00047266343608498573}, {"id": 234, "seek": 97136, "start": 971.36, "end": 974.64, "text": " But the most interesting and important part of this, I think, is just this simple conceptual", "tokens": [50364, 583, 264, 881, 1880, 293, 1021, 644, 295, 341, 11, 286, 519, 11, 307, 445, 341, 2199, 24106, 50528], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 235, "seek": 97136, "start": 974.64, "end": 979.48, "text": " idea of the online compression and reconstruction and how to formalize that mathematically.", "tokens": [50528, 1558, 295, 264, 2950, 19355, 293, 31565, 293, 577, 281, 9860, 1125, 300, 44003, 13, 50770], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 236, "seek": 97136, "start": 979.48, "end": 981.92, "text": " So that's the main point.", "tokens": [50770, 407, 300, 311, 264, 2135, 935, 13, 50892], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 237, "seek": 97136, "start": 981.92, "end": 988.32, "text": " OK, and now with the definitions out of the way, things will become a lot more clear with", "tokens": [50892, 2264, 11, 293, 586, 365, 264, 21988, 484, 295, 264, 636, 11, 721, 486, 1813, 257, 688, 544, 1850, 365, 51212], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 238, "seek": 97136, "start": 988.32, "end": 991.28, "text": " some visualizations of what it does.", "tokens": [51212, 512, 5056, 14455, 295, 437, 309, 775, 13, 51360], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 239, "seek": 97136, "start": 991.28, "end": 996.28, "text": " So first of all, let me just be really formal about defining what HIPAA is.", "tokens": [51360, 407, 700, 295, 439, 11, 718, 385, 445, 312, 534, 9860, 466, 17827, 437, 389, 9139, 5265, 307, 13, 51610], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 240, "seek": 97136, "start": 996.28, "end": 999.08, "text": " So I mentioned the problem was that we are encoding.", "tokens": [51610, 407, 286, 2835, 264, 1154, 390, 300, 321, 366, 43430, 13, 51750], "temperature": 0.0, "avg_logprob": -0.17671698854680648, "compression_ratio": 1.6702508960573477, "no_speech_prob": 0.0003249404835514724}, {"id": 241, "seek": 99908, "start": 999.08, "end": 1003.2800000000001, "text": " So XFT is going to represent a vector of our coefficients at all times.", "tokens": [50364, 407, 1783, 25469, 307, 516, 281, 2906, 257, 8062, 295, 527, 31994, 412, 439, 1413, 13, 50574], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 242, "seek": 99908, "start": 1003.2800000000001, "end": 1008.24, "text": " And the question is, how does this evolve through time as we see more data in the input", "tokens": [50574, 400, 264, 1168, 307, 11, 577, 775, 341, 16693, 807, 565, 382, 321, 536, 544, 1412, 294, 264, 4846, 50822], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 243, "seek": 99908, "start": 1008.24, "end": 1009.24, "text": " U?", "tokens": [50822, 624, 30, 50872], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 244, "seek": 99908, "start": 1009.24, "end": 1011.6, "text": " And it turns out that it just satisfies a simple differential equation.", "tokens": [50872, 400, 309, 4523, 484, 300, 309, 445, 44271, 257, 2199, 15756, 5367, 13, 50990], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 245, "seek": 99908, "start": 1011.6, "end": 1015.0400000000001, "text": " By going through the derivation, you can write down this differential equation in closed", "tokens": [50990, 3146, 516, 807, 264, 10151, 399, 11, 291, 393, 2464, 760, 341, 15756, 5367, 294, 5395, 51162], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 246, "seek": 99908, "start": 1015.0400000000001, "end": 1021.32, "text": " form and write down closed form formulas for this transition matrix involved here.", "tokens": [51162, 1254, 293, 2464, 760, 5395, 1254, 30546, 337, 341, 6034, 8141, 3288, 510, 13, 51476], "temperature": 0.0, "avg_logprob": -0.14599232978009163, "compression_ratio": 1.677685950413223, "no_speech_prob": 0.00011772090510930866}, {"id": 247, "seek": 102132, "start": 1021.32, "end": 1027.88, "text": " So to be concrete, the ODE is called the HIPAA operator.", "tokens": [50364, 407, 281, 312, 9859, 11, 264, 422, 22296, 307, 1219, 264, 389, 9139, 5265, 12973, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 248, "seek": 102132, "start": 1027.88, "end": 1031.8400000000001, "text": " And the matrices, the matrix in the operator are called HIPAA matrices, which have closed", "tokens": [50692, 400, 264, 32284, 11, 264, 8141, 294, 264, 12973, 366, 1219, 389, 9139, 5265, 32284, 11, 597, 362, 5395, 50890], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 249, "seek": 102132, "start": 1031.8400000000001, "end": 1032.8400000000001, "text": " form formulas.", "tokens": [50890, 1254, 30546, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 250, "seek": 102132, "start": 1032.8400000000001, "end": 1037.3200000000002, "text": " In fact, the actual matrix is this matrix.", "tokens": [50940, 682, 1186, 11, 264, 3539, 8141, 307, 341, 8141, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 251, "seek": 102132, "start": 1037.3200000000002, "end": 1043.4, "text": " It's an extremely simple matrix, which is a special type of structure matrix.", "tokens": [51164, 467, 311, 364, 4664, 2199, 8141, 11, 597, 307, 257, 2121, 2010, 295, 3877, 8141, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 252, "seek": 102132, "start": 1043.4, "end": 1045.44, "text": " And yeah, so it's just a simple formula.", "tokens": [51468, 400, 1338, 11, 370, 309, 311, 445, 257, 2199, 8513, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 253, "seek": 102132, "start": 1045.44, "end": 1048.8, "text": " And then we write down a closed form formula for this differential equation.", "tokens": [51570, 400, 550, 321, 2464, 760, 257, 5395, 1254, 8513, 337, 341, 15756, 5367, 13, 51738], "temperature": 0.0, "avg_logprob": -0.1460005351475307, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0013454067520797253}, {"id": 254, "seek": 104880, "start": 1048.8, "end": 1051.9199999999998, "text": " And that's how our coefficients evolve over time.", "tokens": [50364, 400, 300, 311, 577, 527, 31994, 16693, 670, 565, 13, 50520], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 255, "seek": 104880, "start": 1051.9199999999998, "end": 1058.96, "text": " And now, right, so this equation, again, is called the HIPAA operator or the high order", "tokens": [50520, 400, 586, 11, 558, 11, 370, 341, 5367, 11, 797, 11, 307, 1219, 264, 389, 9139, 5265, 12973, 420, 264, 1090, 1668, 50872], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 256, "seek": 104880, "start": 1058.96, "end": 1063.1599999999999, "text": " polynomial projection operator, because we're projecting on the high degree polynomial basis", "tokens": [50872, 26110, 22743, 12973, 11, 570, 321, 434, 43001, 322, 264, 1090, 4314, 26110, 5143, 51082], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 257, "seek": 104880, "start": 1063.1599999999999, "end": 1064.1599999999999, "text": " functions.", "tokens": [51082, 6828, 13, 51132], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 258, "seek": 104880, "start": 1064.1599999999999, "end": 1068.6, "text": " Now visually, the way to think about it is like this.", "tokens": [51132, 823, 19622, 11, 264, 636, 281, 519, 466, 309, 307, 411, 341, 13, 51354], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 259, "seek": 104880, "start": 1068.6, "end": 1072.3999999999999, "text": " The reason I call an operator is because it maps a function to a function.", "tokens": [51354, 440, 1778, 286, 818, 364, 12973, 307, 570, 309, 11317, 257, 2445, 281, 257, 2445, 13, 51544], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 260, "seek": 104880, "start": 1072.3999999999999, "end": 1077.8799999999999, "text": " So it's an operator that maps this black input signal U to these sets of coefficients X", "tokens": [51544, 407, 309, 311, 364, 12973, 300, 11317, 341, 2211, 4846, 6358, 624, 281, 613, 6352, 295, 31994, 1783, 51818], "temperature": 0.0, "avg_logprob": -0.15631069148982968, "compression_ratio": 1.728301886792453, "no_speech_prob": 0.003171247197315097}, {"id": 261, "seek": 107788, "start": 1077.88, "end": 1085.1200000000001, "text": " and blue, where every time X of t compresses the history of the input signal U.", "tokens": [50364, 293, 3344, 11, 689, 633, 565, 1783, 295, 256, 14778, 279, 264, 2503, 295, 264, 4846, 6358, 624, 13, 50726], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 262, "seek": 107788, "start": 1085.1200000000001, "end": 1091.16, "text": " And you can compute X online as you see one input at a time.", "tokens": [50726, 400, 291, 393, 14722, 1783, 2950, 382, 291, 536, 472, 4846, 412, 257, 565, 13, 51028], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 263, "seek": 107788, "start": 1091.16, "end": 1093.5200000000002, "text": " So the black line represents our current time step.", "tokens": [51028, 407, 264, 2211, 1622, 8855, 527, 2190, 565, 1823, 13, 51146], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 264, "seek": 107788, "start": 1093.5200000000002, "end": 1095.88, "text": " We're gradually seeing more of the input.", "tokens": [51146, 492, 434, 13145, 2577, 544, 295, 264, 4846, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 265, "seek": 107788, "start": 1095.88, "end": 1100.8400000000001, "text": " And we are updating our coefficient vector, which is depicted in blue.", "tokens": [51264, 400, 321, 366, 25113, 527, 17619, 8062, 11, 597, 307, 30207, 294, 3344, 13, 51512], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 266, "seek": 107788, "start": 1100.8400000000001, "end": 1106.0400000000002, "text": " Here I've, this is visualizing just the lowest order for coefficients of the best polynomial", "tokens": [51512, 1692, 286, 600, 11, 341, 307, 5056, 3319, 445, 264, 12437, 1668, 337, 31994, 295, 264, 1151, 26110, 51772], "temperature": 0.0, "avg_logprob": -0.16951730516221789, "compression_ratio": 1.6378600823045268, "no_speech_prob": 0.004531635902822018}, {"id": 267, "seek": 110604, "start": 1106.48, "end": 1109.72, "text": " approximation.", "tokens": [50386, 28023, 13, 50548], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 268, "seek": 110604, "start": 1109.72, "end": 1113.36, "text": " And now here is what the reconstruction looks like.", "tokens": [50548, 400, 586, 510, 307, 437, 264, 31565, 1542, 411, 13, 50730], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 269, "seek": 110604, "start": 1113.36, "end": 1119.76, "text": " So as I move along through time and update my coefficients, the coefficients that that", "tokens": [50730, 407, 382, 286, 1286, 2051, 807, 565, 293, 5623, 452, 31994, 11, 264, 31994, 300, 300, 51050], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 270, "seek": 110604, "start": 1119.76, "end": 1124.48, "text": " polynomial defines, in a sense, is actually just this red line.", "tokens": [51050, 26110, 23122, 11, 294, 257, 2020, 11, 307, 767, 445, 341, 2182, 1622, 13, 51286], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 271, "seek": 110604, "start": 1124.48, "end": 1129.32, "text": " So it is reconstructing the input just like we wanted.", "tokens": [51286, 407, 309, 307, 31499, 278, 264, 4846, 445, 411, 321, 1415, 13, 51528], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 272, "seek": 110604, "start": 1129.32, "end": 1133.6, "text": " Note that we are using only, so here I've only visualized four coefficients, but I'm", "tokens": [51528, 11633, 300, 321, 366, 1228, 787, 11, 370, 510, 286, 600, 787, 5056, 1602, 1451, 31994, 11, 457, 286, 478, 51742], "temperature": 0.0, "avg_logprob": -0.13636098124764182, "compression_ratio": 1.600896860986547, "no_speech_prob": 0.0029789921827614307}, {"id": 273, "seek": 113360, "start": 1133.6, "end": 1137.8, "text": " actually using 64 coefficients, but the whole function was linked to 10,000.", "tokens": [50364, 767, 1228, 12145, 31994, 11, 457, 264, 1379, 2445, 390, 9408, 281, 1266, 11, 1360, 13, 50574], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 274, "seek": 113360, "start": 1137.8, "end": 1142.32, "text": " So I'm compressing it a lot.", "tokens": [50574, 407, 286, 478, 14778, 278, 309, 257, 688, 13, 50800], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 275, "seek": 113360, "start": 1142.32, "end": 1148.76, "text": " And this, here's a static image that kind of illustrates the effect of the reconstruction.", "tokens": [50800, 400, 341, 11, 510, 311, 257, 13437, 3256, 300, 733, 295, 41718, 264, 1802, 295, 264, 31565, 13, 51122], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 276, "seek": 113360, "start": 1148.76, "end": 1153.7199999999998, "text": " So because I'm compressing it, I can't perfectly reconstruct the input.", "tokens": [51122, 407, 570, 286, 478, 14778, 278, 309, 11, 286, 393, 380, 6239, 31499, 264, 4846, 13, 51370], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 277, "seek": 113360, "start": 1153.7199999999998, "end": 1155.84, "text": " And so how good is the reconstruction then?", "tokens": [51370, 400, 370, 577, 665, 307, 264, 31565, 550, 30, 51476], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 278, "seek": 113360, "start": 1155.84, "end": 1157.48, "text": " Well, it depends on the measure.", "tokens": [51476, 1042, 11, 309, 5946, 322, 264, 3481, 13, 51558], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 279, "seek": 113360, "start": 1157.48, "end": 1161.9199999999998, "text": " So the green, the green line in this figure was the exponentially decaying measure that", "tokens": [51558, 407, 264, 3092, 11, 264, 3092, 1622, 294, 341, 2573, 390, 264, 37330, 21039, 278, 3481, 300, 51780], "temperature": 0.0, "avg_logprob": -0.12477837133845059, "compression_ratio": 1.7250996015936255, "no_speech_prob": 0.00024528970243409276}, {"id": 280, "seek": 116192, "start": 1161.92, "end": 1164.88, "text": " we are basically projecting onto.", "tokens": [50364, 321, 366, 1936, 43001, 3911, 13, 50512], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 281, "seek": 116192, "start": 1164.88, "end": 1171.76, "text": " And so intuitively, you can see that the red reconstruction line is really accurate for", "tokens": [50512, 400, 370, 46506, 11, 291, 393, 536, 300, 264, 2182, 31565, 1622, 307, 534, 8559, 337, 50856], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 282, "seek": 116192, "start": 1171.76, "end": 1176.4, "text": " the recent past and degrades further out in history, but still maintains some rough information", "tokens": [50856, 264, 5162, 1791, 293, 368, 22626, 3052, 484, 294, 2503, 11, 457, 920, 33385, 512, 5903, 1589, 51088], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 283, "seek": 116192, "start": 1176.4, "end": 1179.6000000000001, "text": " about the whole signal.", "tokens": [51088, 466, 264, 1379, 6358, 13, 51248], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 284, "seek": 116192, "start": 1179.6000000000001, "end": 1184.0800000000002, "text": " And so that is, that's hippo.", "tokens": [51248, 400, 370, 300, 307, 11, 300, 311, 27745, 78, 13, 51472], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 285, "seek": 116192, "start": 1184.0800000000002, "end": 1188.76, "text": " Now, oh, one, oh, okay, a question here.", "tokens": [51472, 823, 11, 1954, 11, 472, 11, 1954, 11, 1392, 11, 257, 1168, 510, 13, 51706], "temperature": 0.0, "avg_logprob": -0.28571403317335176, "compression_ratio": 1.5445544554455446, "no_speech_prob": 0.0006984261563047767}, {"id": 286, "seek": 118876, "start": 1189.12, "end": 1190.28, "text": " No, you can continue.", "tokens": [50382, 883, 11, 291, 393, 2354, 13, 50440], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 287, "seek": 118876, "start": 1190.28, "end": 1194.32, "text": " I just had one clarification, but I can ask after you finish.", "tokens": [50440, 286, 445, 632, 472, 34449, 11, 457, 286, 393, 1029, 934, 291, 2413, 13, 50642], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 288, "seek": 118876, "start": 1194.32, "end": 1195.32, "text": " Here's fine too.", "tokens": [50642, 1692, 311, 2489, 886, 13, 50692], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 289, "seek": 118876, "start": 1195.32, "end": 1196.32, "text": " Okay.", "tokens": [50692, 1033, 13, 50742], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 290, "seek": 118876, "start": 1196.32, "end": 1201.84, "text": " So is it fair to think about X as being the state at each time point?", "tokens": [50742, 407, 307, 309, 3143, 281, 519, 466, 1783, 382, 885, 264, 1785, 412, 1184, 565, 935, 30, 51018], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 291, "seek": 118876, "start": 1201.84, "end": 1208.56, "text": " And then essentially the red line is trying to reconstruct the signal given the current", "tokens": [51018, 400, 550, 4476, 264, 2182, 1622, 307, 1382, 281, 31499, 264, 6358, 2212, 264, 2190, 51354], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 292, "seek": 118876, "start": 1208.56, "end": 1211.92, "text": " state or do you also use all the past states to reconstruct?", "tokens": [51354, 1785, 420, 360, 291, 611, 764, 439, 264, 1791, 4368, 281, 31499, 30, 51522], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 293, "seek": 118876, "start": 1211.92, "end": 1213.28, "text": " That's exactly right.", "tokens": [51522, 663, 311, 2293, 558, 13, 51590], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 294, "seek": 118876, "start": 1213.28, "end": 1218.16, "text": " So yeah, so the reconstruction is happening using only the coefficient vector at the current", "tokens": [51590, 407, 1338, 11, 370, 264, 31565, 307, 2737, 1228, 787, 264, 17619, 8062, 412, 264, 2190, 51834], "temperature": 0.0, "avg_logprob": -0.18661115865791794, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0013246197486296296}, {"id": 295, "seek": 121816, "start": 1218.16, "end": 1219.3200000000002, "text": " black line.", "tokens": [50364, 2211, 1622, 13, 50422], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 296, "seek": 121816, "start": 1219.3200000000002, "end": 1223.64, "text": " So every single time I'm using, I'm for, yeah, the blue line, I'm visualizing the whole", "tokens": [50422, 407, 633, 2167, 565, 286, 478, 1228, 11, 286, 478, 337, 11, 1338, 11, 264, 3344, 1622, 11, 286, 478, 5056, 3319, 264, 1379, 50638], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 297, "seek": 121816, "start": 1223.64, "end": 1227.28, "text": " thing, but at any given point in time, I'm remembering only the current vector, which", "tokens": [50638, 551, 11, 457, 412, 604, 2212, 935, 294, 565, 11, 286, 478, 20719, 787, 264, 2190, 8062, 11, 597, 50820], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 298, "seek": 121816, "start": 1227.28, "end": 1229.88, "text": " has length 64.", "tokens": [50820, 575, 4641, 12145, 13, 50950], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 299, "seek": 121816, "start": 1229.88, "end": 1233.4, "text": " Here I'm only visualizing four of the components, but it has length 64.", "tokens": [50950, 1692, 286, 478, 787, 5056, 3319, 1451, 295, 264, 6677, 11, 457, 309, 575, 4641, 12145, 13, 51126], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 300, "seek": 121816, "start": 1233.4, "end": 1237.4, "text": " And using those 64 numbers, I'm reconstructing what I've seen so far in red.", "tokens": [51126, 400, 1228, 729, 12145, 3547, 11, 286, 478, 31499, 278, 437, 286, 600, 1612, 370, 1400, 294, 2182, 13, 51326], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 301, "seek": 121816, "start": 1237.4, "end": 1238.4, "text": " Oh, awesome.", "tokens": [51326, 876, 11, 3476, 13, 51376], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 302, "seek": 121816, "start": 1238.4, "end": 1239.4, "text": " Thanks.", "tokens": [51376, 2561, 13, 51426], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 303, "seek": 121816, "start": 1239.4, "end": 1240.4, "text": " Right.", "tokens": [51426, 1779, 13, 51476], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 304, "seek": 121816, "start": 1240.4, "end": 1246.76, "text": " Now, in that previous figure, if I just take one of the blue lines, actually the lowest", "tokens": [51476, 823, 11, 294, 300, 3894, 2573, 11, 498, 286, 445, 747, 472, 295, 264, 3344, 3876, 11, 767, 264, 12437, 51794], "temperature": 0.0, "avg_logprob": -0.19387184872346766, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0010318721178919077}, {"id": 305, "seek": 124676, "start": 1246.76, "end": 1250.9, "text": " order coefficient and overlay over the function, you can see that it actually turns out to", "tokens": [50364, 1668, 17619, 293, 31741, 670, 264, 2445, 11, 291, 393, 536, 300, 309, 767, 4523, 484, 281, 50571], "temperature": 0.0, "avg_logprob": -0.182618820804289, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.004328378941863775}, {"id": 306, "seek": 124676, "start": 1250.9, "end": 1252.4, "text": " exactly be the EMA.", "tokens": [50571, 2293, 312, 264, 462, 9998, 13, 50646], "temperature": 0.0, "avg_logprob": -0.182618820804289, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.004328378941863775}, {"id": 307, "seek": 124676, "start": 1252.4, "end": 1260.2, "text": " And so it turns out that moving averages can be viewed as order zero or low order projections.", "tokens": [50646, 400, 370, 309, 4523, 484, 300, 2684, 42257, 393, 312, 19174, 382, 1668, 4018, 420, 2295, 1668, 32371, 13, 51036], "temperature": 0.0, "avg_logprob": -0.182618820804289, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.004328378941863775}, {"id": 308, "seek": 124676, "start": 1260.2, "end": 1263.36, "text": " On the other hand, hippo is essentially a very strong generalization of this that solves", "tokens": [51036, 1282, 264, 661, 1011, 11, 27745, 78, 307, 4476, 257, 588, 2068, 2674, 2144, 295, 341, 300, 39890, 51194], "temperature": 0.0, "avg_logprob": -0.182618820804289, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.004328378941863775}, {"id": 309, "seek": 124676, "start": 1263.36, "end": 1271.36, "text": " a natural mathematical question and gets back things like the EMA for free.", "tokens": [51194, 257, 3303, 18894, 1168, 293, 2170, 646, 721, 411, 264, 462, 9998, 337, 1737, 13, 51594], "temperature": 0.0, "avg_logprob": -0.182618820804289, "compression_ratio": 1.6017316017316017, "no_speech_prob": 0.004328378941863775}, {"id": 310, "seek": 127136, "start": 1271.36, "end": 1276.52, "text": " So that's what hippo is, and now I'll just talk a little bit about some extensions of", "tokens": [50364, 407, 300, 311, 437, 27745, 78, 307, 11, 293, 586, 286, 603, 445, 751, 257, 707, 857, 466, 512, 25129, 295, 50622], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 311, "seek": 127136, "start": 1276.52, "end": 1277.52, "text": " it.", "tokens": [50622, 309, 13, 50672], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 312, "seek": 127136, "start": 1277.52, "end": 1281.76, "text": " So first of all, a natural question that may be wondering is that I've been using this", "tokens": [50672, 407, 700, 295, 439, 11, 257, 3303, 1168, 300, 815, 312, 6359, 307, 300, 286, 600, 668, 1228, 341, 50884], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 313, "seek": 127136, "start": 1281.76, "end": 1284.6799999999998, "text": " example of an exponential measure, but what about other cases?", "tokens": [50884, 1365, 295, 364, 21510, 3481, 11, 457, 437, 466, 661, 3331, 30, 51030], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 314, "seek": 127136, "start": 1284.6799999999998, "end": 1288.0, "text": " Well, it turns out that hippo can be derived for any measure.", "tokens": [51030, 1042, 11, 309, 4523, 484, 300, 27745, 78, 393, 312, 18949, 337, 604, 3481, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 315, "seek": 127136, "start": 1288.0, "end": 1292.6399999999999, "text": " For example, here's a case that is pretty natural as well, which is what if I want to", "tokens": [51196, 1171, 1365, 11, 510, 311, 257, 1389, 300, 307, 1238, 3303, 382, 731, 11, 597, 307, 437, 498, 286, 528, 281, 51428], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 316, "seek": 127136, "start": 1292.6399999999999, "end": 1294.1599999999999, "text": " reconstruct along a uniform measure?", "tokens": [51428, 31499, 2051, 257, 9452, 3481, 30, 51504], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 317, "seek": 127136, "start": 1294.1599999999999, "end": 1301.32, "text": " In other words, I only care about remembering the recent past in sliding windows of my function.", "tokens": [51504, 682, 661, 2283, 11, 286, 787, 1127, 466, 20719, 264, 5162, 1791, 294, 21169, 9309, 295, 452, 2445, 13, 51862], "temperature": 0.0, "avg_logprob": -0.1644283954362224, "compression_ratio": 1.67524115755627, "no_speech_prob": 0.0012840027920901775}, {"id": 318, "seek": 130132, "start": 1301.3999999999999, "end": 1307.8799999999999, "text": " And this is possible, so you would get a different ODE, and here's a reconstruction in effect.", "tokens": [50368, 400, 341, 307, 1944, 11, 370, 291, 576, 483, 257, 819, 422, 22296, 11, 293, 510, 311, 257, 31565, 294, 1802, 13, 50692], "temperature": 0.0, "avg_logprob": -0.17795978727794828, "compression_ratio": 1.5615942028985508, "no_speech_prob": 0.0014990903437137604}, {"id": 319, "seek": 130132, "start": 1307.8799999999999, "end": 1313.28, "text": " So again, using just 64 numbers in memory, I'm trying to reconstruct the last 2,000 time", "tokens": [50692, 407, 797, 11, 1228, 445, 12145, 3547, 294, 4675, 11, 286, 478, 1382, 281, 31499, 264, 1036, 568, 11, 1360, 565, 50962], "temperature": 0.0, "avg_logprob": -0.17795978727794828, "compression_ratio": 1.5615942028985508, "no_speech_prob": 0.0014990903437137604}, {"id": 320, "seek": 130132, "start": 1313.28, "end": 1320.08, "text": " steps of this function uniformly, and it's doing this quite accurately.", "tokens": [50962, 4439, 295, 341, 2445, 48806, 11, 293, 309, 311, 884, 341, 1596, 20095, 13, 51302], "temperature": 0.0, "avg_logprob": -0.17795978727794828, "compression_ratio": 1.5615942028985508, "no_speech_prob": 0.0014990903437137604}, {"id": 321, "seek": 130132, "start": 1320.08, "end": 1324.08, "text": " Now you can generalize it even further to, for example, when the measure is changing over", "tokens": [51302, 823, 291, 393, 2674, 1125, 309, 754, 3052, 281, 11, 337, 1365, 11, 562, 264, 3481, 307, 4473, 670, 51502], "temperature": 0.0, "avg_logprob": -0.17795978727794828, "compression_ratio": 1.5615942028985508, "no_speech_prob": 0.0014990903437137604}, {"id": 322, "seek": 130132, "start": 1324.08, "end": 1328.76, "text": " time instead of just sliding along, and so there's a very general framework here that", "tokens": [51502, 565, 2602, 295, 445, 21169, 2051, 11, 293, 370, 456, 311, 257, 588, 2674, 8388, 510, 300, 51736], "temperature": 0.0, "avg_logprob": -0.17795978727794828, "compression_ratio": 1.5615942028985508, "no_speech_prob": 0.0014990903437137604}, {"id": 323, "seek": 132876, "start": 1328.76, "end": 1331.6, "text": " can do lots of things.", "tokens": [50364, 393, 360, 3195, 295, 721, 13, 50506], "temperature": 0.0, "avg_logprob": -0.1748177723218036, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0008556621614843607}, {"id": 324, "seek": 132876, "start": 1331.6, "end": 1334.68, "text": " A lot of this was in follow-up work to their general hippo paper, and what we showed was", "tokens": [50506, 316, 688, 295, 341, 390, 294, 1524, 12, 1010, 589, 281, 641, 2674, 27745, 78, 3035, 11, 293, 437, 321, 4712, 390, 50660], "temperature": 0.0, "avg_logprob": -0.1748177723218036, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0008556621614843607}, {"id": 325, "seek": 132876, "start": 1334.68, "end": 1343.08, "text": " that for essentially any measure, there exists a corresponding hippo operator where the hippo", "tokens": [50660, 300, 337, 4476, 604, 3481, 11, 456, 8198, 257, 11760, 27745, 78, 12973, 689, 264, 27745, 78, 51080], "temperature": 0.0, "avg_logprob": -0.1748177723218036, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0008556621614843607}, {"id": 326, "seek": 132876, "start": 1343.08, "end": 1348.64, "text": " matrices A and B depend on the measure, and you can write them down in closed form.", "tokens": [51080, 32284, 316, 293, 363, 5672, 322, 264, 3481, 11, 293, 291, 393, 2464, 552, 760, 294, 5395, 1254, 13, 51358], "temperature": 0.0, "avg_logprob": -0.1748177723218036, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0008556621614843607}, {"id": 327, "seek": 132876, "start": 1348.64, "end": 1352.48, "text": " And this is important, I think, because it draws an equivalence between measures and", "tokens": [51358, 400, 341, 307, 1021, 11, 286, 519, 11, 570, 309, 20045, 364, 9052, 655, 1296, 8000, 293, 51550], "temperature": 0.0, "avg_logprob": -0.1748177723218036, "compression_ratio": 1.6331877729257642, "no_speech_prob": 0.0008556621614843607}, {"id": 328, "seek": 135248, "start": 1352.64, "end": 1359.52, "text": " these ODE's, where this means that we don't, I mentioned earlier that we had to choose the", "tokens": [50372, 613, 422, 22296, 311, 11, 689, 341, 1355, 300, 321, 500, 380, 11, 286, 2835, 3071, 300, 321, 632, 281, 2826, 264, 50716], "temperature": 0.0, "avg_logprob": -0.18881453300008968, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.001754168188199401}, {"id": 329, "seek": 135248, "start": 1359.52, "end": 1364.48, "text": " measure up front as a prior, such as the accidentally decaying case, but actually just by learning", "tokens": [50716, 3481, 493, 1868, 382, 257, 4059, 11, 1270, 382, 264, 15715, 21039, 278, 1389, 11, 457, 767, 445, 538, 2539, 50964], "temperature": 0.0, "avg_logprob": -0.18881453300008968, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.001754168188199401}, {"id": 330, "seek": 135248, "start": 1364.48, "end": 1371.3600000000001, "text": " these matrices A and B, it's in some sense the same as learning the measure.", "tokens": [50964, 613, 32284, 316, 293, 363, 11, 309, 311, 294, 512, 2020, 264, 912, 382, 2539, 264, 3481, 13, 51308], "temperature": 0.0, "avg_logprob": -0.18881453300008968, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.001754168188199401}, {"id": 331, "seek": 135248, "start": 1371.3600000000001, "end": 1377.88, "text": " Okay, so now even better, not only do these operators always exist, but it turns out that", "tokens": [51308, 1033, 11, 370, 586, 754, 1101, 11, 406, 787, 360, 613, 19077, 1009, 2514, 11, 457, 309, 4523, 484, 300, 51634], "temperature": 0.0, "avg_logprob": -0.18881453300008968, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.001754168188199401}, {"id": 332, "seek": 135248, "start": 1377.88, "end": 1379.4, "text": " the matrices are always structured.", "tokens": [51634, 264, 32284, 366, 1009, 18519, 13, 51710], "temperature": 0.0, "avg_logprob": -0.18881453300008968, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.001754168188199401}, {"id": 333, "seek": 137940, "start": 1379.8000000000002, "end": 1385.16, "text": " Previously, we saw, for the accidentally decaying case, the matrix was actually extremely simple.", "tokens": [50384, 33606, 11, 321, 1866, 11, 337, 264, 15715, 21039, 278, 1389, 11, 264, 8141, 390, 767, 4664, 2199, 13, 50652], "temperature": 0.0, "avg_logprob": -0.16968769293565017, "compression_ratio": 1.7063197026022305, "no_speech_prob": 8.218886068789288e-05}, {"id": 334, "seek": 137940, "start": 1386.0400000000002, "end": 1389.88, "text": " In general, they're going to be more complicated than that, and it's, they satisfy a structure,", "tokens": [50696, 682, 2674, 11, 436, 434, 516, 281, 312, 544, 6179, 813, 300, 11, 293, 309, 311, 11, 436, 19319, 257, 3877, 11, 50888], "temperature": 0.0, "avg_logprob": -0.16968769293565017, "compression_ratio": 1.7063197026022305, "no_speech_prob": 8.218886068789288e-05}, {"id": 335, "seek": 137940, "start": 1389.88, "end": 1394.1200000000001, "text": " which was something that I introduced in much earlier work, but they are all structured in", "tokens": [50888, 597, 390, 746, 300, 286, 7268, 294, 709, 3071, 589, 11, 457, 436, 366, 439, 18519, 294, 51100], "temperature": 0.0, "avg_logprob": -0.16968769293565017, "compression_ratio": 1.7063197026022305, "no_speech_prob": 8.218886068789288e-05}, {"id": 336, "seek": 137940, "start": 1394.1200000000001, "end": 1400.2800000000002, "text": " some way, and that means that you can, how do you actually calculate these updates through time?", "tokens": [51100, 512, 636, 11, 293, 300, 1355, 300, 291, 393, 11, 577, 360, 291, 767, 8873, 613, 9205, 807, 565, 30, 51408], "temperature": 0.0, "avg_logprob": -0.16968769293565017, "compression_ratio": 1.7063197026022305, "no_speech_prob": 8.218886068789288e-05}, {"id": 337, "seek": 137940, "start": 1400.2800000000002, "end": 1404.2800000000002, "text": " You can actually update the state or the coefficients in nearly optimal time.", "tokens": [51408, 509, 393, 767, 5623, 264, 1785, 420, 264, 31994, 294, 6217, 16252, 565, 13, 51608], "temperature": 0.0, "avg_logprob": -0.16968769293565017, "compression_ratio": 1.7063197026022305, "no_speech_prob": 8.218886068789288e-05}, {"id": 338, "seek": 140428, "start": 1404.76, "end": 1412.68, "text": " Okay, so that was the main takeaways from HIPPO, and so just to recap, we were inspired by these", "tokens": [50388, 1033, 11, 370, 300, 390, 264, 2135, 45584, 490, 389, 9139, 34885, 11, 293, 370, 445, 281, 20928, 11, 321, 645, 7547, 538, 613, 50784], "temperature": 0.0, "avg_logprob": -0.22199247340963343, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0002268952812300995}, {"id": 339, "seek": 140428, "start": 1412.68, "end": 1417.56, "text": " very basic, these simple but important properties of trying to maintain a state that's summarizing", "tokens": [50784, 588, 3875, 11, 613, 2199, 457, 1021, 7221, 295, 1382, 281, 6909, 257, 1785, 300, 311, 14611, 3319, 51028], "temperature": 0.0, "avg_logprob": -0.22199247340963343, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0002268952812300995}, {"id": 340, "seek": 140428, "start": 1417.56, "end": 1423.0, "text": " the entire context, and we formalized this into a mathematical problem, which was pretty", "tokens": [51028, 264, 2302, 4319, 11, 293, 321, 9860, 1602, 341, 666, 257, 18894, 1154, 11, 597, 390, 1238, 51300], "temperature": 0.0, "avg_logprob": -0.22199247340963343, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0002268952812300995}, {"id": 341, "seek": 140428, "start": 1423.0, "end": 1427.16, "text": " intuitive, and we were then able to solve analytically, and this resulted in a nice", "tokens": [51300, 21769, 11, 293, 321, 645, 550, 1075, 281, 5039, 10783, 984, 11, 293, 341, 18753, 294, 257, 1481, 51508], "temperature": 0.0, "avg_logprob": -0.22199247340963343, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0002268952812300995}, {"id": 342, "seek": 140428, "start": 1427.16, "end": 1429.6399999999999, "text": " class of methods for addressing long context and signals.", "tokens": [51508, 1508, 295, 7150, 337, 14329, 938, 4319, 293, 12354, 13, 51632], "temperature": 0.0, "avg_logprob": -0.22199247340963343, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.0002268952812300995}, {"id": 343, "seek": 142964, "start": 1429.88, "end": 1431.88, "text": " Okay, so I see a question in the chat.", "tokens": [50376, 1033, 11, 370, 286, 536, 257, 1168, 294, 264, 5081, 13, 50476], "temperature": 0.0, "avg_logprob": -0.2996790247056091, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010002284543588758}, {"id": 344, "seek": 142964, "start": 1434.92, "end": 1439.5600000000002, "text": " So, can these operators be expressed in terms of Z-transforms? I'm not quite sure what you mean here.", "tokens": [50628, 407, 11, 393, 613, 19077, 312, 12675, 294, 2115, 295, 1176, 12, 24999, 837, 82, 30, 286, 478, 406, 1596, 988, 437, 291, 914, 510, 13, 50860], "temperature": 0.0, "avg_logprob": -0.2996790247056091, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010002284543588758}, {"id": 345, "seek": 142964, "start": 1441.48, "end": 1444.76, "text": " To my understanding, Z-transforms are like the discrete version of a Laplace transfer,", "tokens": [50956, 1407, 452, 3701, 11, 1176, 12, 24999, 837, 82, 366, 411, 264, 27706, 3037, 295, 257, 2369, 6742, 5003, 11, 51120], "temperature": 0.0, "avg_logprob": -0.2996790247056091, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010002284543588758}, {"id": 346, "seek": 142964, "start": 1444.76, "end": 1448.5200000000002, "text": " and I'm not sure if that's the one you're referring to, or another notion.", "tokens": [51120, 293, 286, 478, 406, 988, 498, 300, 311, 264, 472, 291, 434, 13761, 281, 11, 420, 1071, 10710, 13, 51308], "temperature": 0.0, "avg_logprob": -0.2996790247056091, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010002284543588758}, {"id": 347, "seek": 142964, "start": 1451.0, "end": 1454.8400000000001, "text": " Yes, that's what I was thinking about. It seems like as though", "tokens": [51432, 1079, 11, 300, 311, 437, 286, 390, 1953, 466, 13, 467, 2544, 411, 382, 1673, 51624], "temperature": 0.0, "avg_logprob": -0.2996790247056091, "compression_ratio": 1.5869565217391304, "no_speech_prob": 0.0010002284543588758}, {"id": 348, "seek": 145484, "start": 1455.32, "end": 1462.12, "text": " just as you can express like exponentially, exponential decay in terms of Z-transforms", "tokens": [50388, 445, 382, 291, 393, 5109, 411, 37330, 11, 21510, 21039, 294, 2115, 295, 1176, 12, 24999, 837, 82, 50728], "temperature": 0.0, "avg_logprob": -0.4020016644452069, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.000487713172333315}, {"id": 349, "seek": 145484, "start": 1462.12, "end": 1467.32, "text": " of the functions, that there seems like it's likely to be a link.", "tokens": [50728, 295, 264, 6828, 11, 300, 456, 2544, 411, 309, 311, 3700, 281, 312, 257, 2113, 13, 50988], "temperature": 0.0, "avg_logprob": -0.4020016644452069, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.000487713172333315}, {"id": 350, "seek": 145484, "start": 1469.32, "end": 1472.76, "text": " Yeah, I mean, I think all these things have a tight link, and they're connected to each other.", "tokens": [51088, 865, 11, 286, 914, 11, 286, 519, 439, 613, 721, 362, 257, 4524, 2113, 11, 293, 436, 434, 4582, 281, 1184, 661, 13, 51260], "temperature": 0.0, "avg_logprob": -0.4020016644452069, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.000487713172333315}, {"id": 351, "seek": 145484, "start": 1472.76, "end": 1478.1999999999998, "text": " It turns out actually that the way that in the next part, when I talk about S4,", "tokens": [51260, 467, 4523, 484, 767, 300, 264, 636, 300, 294, 264, 958, 644, 11, 562, 286, 751, 466, 318, 19, 11, 51532], "temperature": 0.0, "avg_logprob": -0.4020016644452069, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.000487713172333315}, {"id": 352, "seek": 145484, "start": 1478.1999999999998, "end": 1481.6399999999999, "text": " there's going to be some difficult computations, and I'm not sure if that's the one you're", "tokens": [51532, 456, 311, 516, 281, 312, 512, 2252, 2807, 763, 11, 293, 286, 478, 406, 988, 498, 300, 311, 264, 472, 291, 434, 51704], "temperature": 0.0, "avg_logprob": -0.4020016644452069, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.000487713172333315}, {"id": 353, "seek": 148164, "start": 1481.64, "end": 1484.2800000000002, "text": " talking about. In the next part, when I talk about S4, there's going to be some difficult", "tokens": [50364, 1417, 466, 13, 682, 264, 958, 644, 11, 562, 286, 751, 466, 318, 19, 11, 456, 311, 516, 281, 312, 512, 2252, 50496], "temperature": 0.0, "avg_logprob": -0.1843615657878372, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0010319032007828355}, {"id": 354, "seek": 148164, "start": 1484.2800000000002, "end": 1492.76, "text": " computational issues for computing certain things that I'll introduce, and to actually compute them,", "tokens": [50496, 28270, 2663, 337, 15866, 1629, 721, 300, 286, 603, 5366, 11, 293, 281, 767, 14722, 552, 11, 50920], "temperature": 0.0, "avg_logprob": -0.1843615657878372, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0010319032007828355}, {"id": 355, "seek": 148164, "start": 1492.76, "end": 1496.76, "text": " I essentially actually go through Laplace space or frequency space. So, essentially,", "tokens": [50920, 286, 4476, 767, 352, 807, 2369, 6742, 1901, 420, 7893, 1901, 13, 407, 11, 4476, 11, 51120], "temperature": 0.0, "avg_logprob": -0.1843615657878372, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0010319032007828355}, {"id": 356, "seek": 148164, "start": 1496.76, "end": 1502.5200000000002, "text": " I actually take the Z-transform of this equation and calculate that transform at several values,", "tokens": [51120, 286, 767, 747, 264, 1176, 12, 24999, 837, 295, 341, 5367, 293, 8873, 300, 4088, 412, 2940, 4190, 11, 51408], "temperature": 0.0, "avg_logprob": -0.1843615657878372, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0010319032007828355}, {"id": 357, "seek": 148164, "start": 1502.5200000000002, "end": 1508.0400000000002, "text": " and then invert it to get the hippo matrices back, or to get a certain thing back.", "tokens": [51408, 293, 550, 33966, 309, 281, 483, 264, 27745, 78, 32284, 646, 11, 420, 281, 483, 257, 1629, 551, 646, 13, 51684], "temperature": 0.0, "avg_logprob": -0.1843615657878372, "compression_ratio": 1.7366412213740459, "no_speech_prob": 0.0010319032007828355}, {"id": 358, "seek": 150804, "start": 1508.04, "end": 1517.8799999999999, "text": " Sounds good. That makes sense. Yeah, great question. And yeah, so I wanted also just", "tokens": [50364, 14576, 665, 13, 663, 1669, 2020, 13, 865, 11, 869, 1168, 13, 400, 1338, 11, 370, 286, 1415, 611, 445, 50856], "temperature": 0.0, "avg_logprob": -0.18006621615987428, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00018516990530770272}, {"id": 359, "seek": 150804, "start": 1517.8799999999999, "end": 1521.24, "text": " to stop around here at the summary for any other questions.", "tokens": [50856, 281, 1590, 926, 510, 412, 264, 12691, 337, 604, 661, 1651, 13, 51024], "temperature": 0.0, "avg_logprob": -0.18006621615987428, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00018516990530770272}, {"id": 360, "seek": 150804, "start": 1525.8799999999999, "end": 1530.2, "text": " And if there's none, that's great because usually this is a pretty complicated", "tokens": [51256, 400, 498, 456, 311, 6022, 11, 300, 311, 869, 570, 2673, 341, 307, 257, 1238, 6179, 51472], "temperature": 0.0, "avg_logprob": -0.18006621615987428, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00018516990530770272}, {"id": 361, "seek": 150804, "start": 1531.96, "end": 1536.04, "text": " framework mathematically, but hopefully the visualizations help explain it a lot.", "tokens": [51560, 8388, 44003, 11, 457, 4696, 264, 5056, 14455, 854, 2903, 309, 257, 688, 13, 51764], "temperature": 0.0, "avg_logprob": -0.18006621615987428, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.00018516990530770272}, {"id": 362, "seek": 153804, "start": 1538.12, "end": 1547.1599999999999, "text": " Okay, so I'll move on to the next part where, so one thing I didn't include in this section", "tokens": [50368, 1033, 11, 370, 286, 603, 1286, 322, 281, 264, 958, 644, 689, 11, 370, 472, 551, 286, 994, 380, 4090, 294, 341, 3541, 50820], "temperature": 0.0, "avg_logprob": -0.11941897130645482, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.00010389480303274468}, {"id": 363, "seek": 153804, "start": 1547.1599999999999, "end": 1551.56, "text": " was any experiments. So the way we evaluated this is kind of just like how good is the", "tokens": [50820, 390, 604, 12050, 13, 407, 264, 636, 321, 25509, 341, 307, 733, 295, 445, 411, 577, 665, 307, 264, 51040], "temperature": 0.0, "avg_logprob": -0.11941897130645482, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.00010389480303274468}, {"id": 364, "seek": 153804, "start": 1551.56, "end": 1556.68, "text": " reconstruction, and actually using this method in machine learning models did pretty well,", "tokens": [51040, 31565, 11, 293, 767, 1228, 341, 3170, 294, 3479, 2539, 5245, 630, 1238, 731, 11, 51296], "temperature": 0.0, "avg_logprob": -0.11941897130645482, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.00010389480303274468}, {"id": 365, "seek": 153804, "start": 1556.68, "end": 1560.84, "text": " just naively, but where it became really effective was when incorporated into a model", "tokens": [51296, 445, 1667, 3413, 11, 457, 689, 309, 3062, 534, 4942, 390, 562, 21654, 666, 257, 2316, 51504], "temperature": 0.0, "avg_logprob": -0.11941897130645482, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.00010389480303274468}, {"id": 366, "seek": 153804, "start": 1560.84, "end": 1567.1599999999999, "text": " in a particular way, and so that's what S4 will be. And so to, first I'm just going to define S4,", "tokens": [51504, 294, 257, 1729, 636, 11, 293, 370, 300, 311, 437, 318, 19, 486, 312, 13, 400, 370, 281, 11, 700, 286, 478, 445, 516, 281, 6964, 318, 19, 11, 51820], "temperature": 0.0, "avg_logprob": -0.11941897130645482, "compression_ratio": 1.629496402877698, "no_speech_prob": 0.00010389480303274468}, {"id": 367, "seek": 156804, "start": 1568.2, "end": 1571.3999999999999, "text": " and I'm going to define it through hippo, which was the original motivation,", "tokens": [50372, 293, 286, 478, 516, 281, 6964, 309, 807, 27745, 78, 11, 597, 390, 264, 3380, 12335, 11, 50532], "temperature": 0.0, "avg_logprob": -0.10244578448208895, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00011772170546464622}, {"id": 368, "seek": 156804, "start": 1572.92, "end": 1576.28, "text": " and the motivation here is going to be very simple. So to refresh your memory, this is what", "tokens": [50608, 293, 264, 12335, 510, 307, 516, 281, 312, 588, 2199, 13, 407, 281, 15134, 428, 4675, 11, 341, 307, 437, 50776], "temperature": 0.0, "avg_logprob": -0.10244578448208895, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00011772170546464622}, {"id": 369, "seek": 156804, "start": 1576.28, "end": 1580.68, "text": " hippo does, it maps an input signal, which in our case they're thinking of as 1D,", "tokens": [50776, 27745, 78, 775, 11, 309, 11317, 364, 4846, 6358, 11, 597, 294, 527, 1389, 436, 434, 1953, 295, 382, 502, 35, 11, 50996], "temperature": 0.0, "avg_logprob": -0.10244578448208895, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00011772170546464622}, {"id": 370, "seek": 156804, "start": 1580.68, "end": 1585.1599999999999, "text": " to a higher dimensional signal. Now the problem is that we've blown up the dimension of the input", "tokens": [50996, 281, 257, 2946, 18795, 6358, 13, 823, 264, 1154, 307, 300, 321, 600, 16479, 493, 264, 10139, 295, 264, 4846, 51220], "temperature": 0.0, "avg_logprob": -0.10244578448208895, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00011772170546464622}, {"id": 371, "seek": 156804, "start": 1585.1599999999999, "end": 1592.36, "text": " from one dimension to n dimensions, where n was our memory budget or the number of coefficients,", "tokens": [51220, 490, 472, 10139, 281, 297, 12819, 11, 689, 297, 390, 527, 4675, 4706, 420, 264, 1230, 295, 31994, 11, 51580], "temperature": 0.0, "avg_logprob": -0.10244578448208895, "compression_ratio": 1.718146718146718, "no_speech_prob": 0.00011772170546464622}, {"id": 372, "seek": 159236, "start": 1592.36, "end": 1602.12, "text": " and typically this is going to be at least 100 or so. So the motivation for, so I work in deep", "tokens": [50364, 293, 5850, 341, 307, 516, 281, 312, 412, 1935, 2319, 420, 370, 13, 407, 264, 12335, 337, 11, 370, 286, 589, 294, 2452, 50852], "temperature": 0.0, "avg_logprob": -0.09675506969074626, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0001852020068326965}, {"id": 373, "seek": 159236, "start": 1602.12, "end": 1607.6399999999999, "text": " learning, and I just wanted to incorporate hippo into a deep learning model, but this is a problem", "tokens": [50852, 2539, 11, 293, 286, 445, 1415, 281, 16091, 27745, 78, 666, 257, 2452, 2539, 2316, 11, 457, 341, 307, 257, 1154, 51128], "temperature": 0.0, "avg_logprob": -0.09675506969074626, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0001852020068326965}, {"id": 374, "seek": 159236, "start": 1607.6399999999999, "end": 1611.56, "text": " because you can't just stack layers of this because you just keep increasing the dimension.", "tokens": [51128, 570, 291, 393, 380, 445, 8630, 7914, 295, 341, 570, 291, 445, 1066, 5662, 264, 10139, 13, 51324], "temperature": 0.0, "avg_logprob": -0.09675506969074626, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0001852020068326965}, {"id": 375, "seek": 159236, "start": 1612.12, "end": 1617.3999999999999, "text": " And so a very simple motivation to fix this is just, let's just decrease the dimension again.", "tokens": [51352, 400, 370, 257, 588, 2199, 12335, 281, 3191, 341, 307, 445, 11, 718, 311, 445, 11514, 264, 10139, 797, 13, 51616], "temperature": 0.0, "avg_logprob": -0.09675506969074626, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0001852020068326965}, {"id": 376, "seek": 161740, "start": 1618.3600000000001, "end": 1623.64, "text": " And the way to do this is that you can just take a very simple linear projection. So", "tokens": [50412, 400, 264, 636, 281, 360, 341, 307, 300, 291, 393, 445, 747, 257, 588, 2199, 8213, 22743, 13, 407, 50676], "temperature": 0.0, "avg_logprob": -0.126782684995417, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00019109091954305768}, {"id": 377, "seek": 161740, "start": 1625.96, "end": 1629.4, "text": " what we'll do is that we have a state x, which was like a 100 dimensional vector,", "tokens": [50792, 437, 321, 603, 360, 307, 300, 321, 362, 257, 1785, 2031, 11, 597, 390, 411, 257, 2319, 18795, 8062, 11, 50964], "temperature": 0.0, "avg_logprob": -0.126782684995417, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00019109091954305768}, {"id": 378, "seek": 161740, "start": 1629.4, "end": 1633.96, "text": " and we'll just hit it with a dot product that can be learnable to get back a single number,", "tokens": [50964, 293, 321, 603, 445, 2045, 309, 365, 257, 5893, 1674, 300, 393, 312, 1466, 712, 281, 483, 646, 257, 2167, 1230, 11, 51192], "temperature": 0.0, "avg_logprob": -0.126782684995417, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00019109091954305768}, {"id": 379, "seek": 161740, "start": 1633.96, "end": 1638.2800000000002, "text": " which is essentially taking a linear combination of the blue lines to get the final output,", "tokens": [51192, 597, 307, 4476, 1940, 257, 8213, 6562, 295, 264, 3344, 3876, 281, 483, 264, 2572, 5598, 11, 51408], "temperature": 0.0, "avg_logprob": -0.126782684995417, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00019109091954305768}, {"id": 380, "seek": 161740, "start": 1638.2800000000002, "end": 1643.0, "text": " which is the red line. And then we'll add a multiple of the original input, which can be seen as a", "tokens": [51408, 597, 307, 264, 2182, 1622, 13, 400, 550, 321, 603, 909, 257, 3866, 295, 264, 3380, 4846, 11, 597, 393, 312, 1612, 382, 257, 51644], "temperature": 0.0, "avg_logprob": -0.126782684995417, "compression_ratio": 1.7335907335907337, "no_speech_prob": 0.00019109091954305768}, {"id": 381, "seek": 164300, "start": 1643.08, "end": 1649.16, "text": " skip connection. And that is the entire definition of S4. It's finally these two equations where", "tokens": [50368, 10023, 4984, 13, 400, 300, 307, 264, 2302, 7123, 295, 318, 19, 13, 467, 311, 2721, 613, 732, 11787, 689, 50672], "temperature": 0.0, "avg_logprob": -0.12172957954056766, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00039197670412249863}, {"id": 382, "seek": 164300, "start": 1649.16, "end": 1654.52, "text": " the first one is the hippo equation, which takes the input to a state that's kind of memorizing it.", "tokens": [50672, 264, 700, 472, 307, 264, 27745, 78, 5367, 11, 597, 2516, 264, 4846, 281, 257, 1785, 300, 311, 733, 295, 10560, 3319, 309, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12172957954056766, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00039197670412249863}, {"id": 383, "seek": 164300, "start": 1654.52, "end": 1660.36, "text": " And then the second equation just combines the state linearly into a single output.", "tokens": [50940, 400, 550, 264, 1150, 5367, 445, 29520, 264, 1785, 43586, 666, 257, 2167, 5598, 13, 51232], "temperature": 0.0, "avg_logprob": -0.12172957954056766, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00039197670412249863}, {"id": 384, "seek": 164300, "start": 1661.88, "end": 1665.64, "text": " Now, for those of you with a background engineering, this definition may look really familiar.", "tokens": [51308, 823, 11, 337, 729, 295, 291, 365, 257, 3678, 7043, 11, 341, 7123, 815, 574, 534, 4963, 13, 51496], "temperature": 0.0, "avg_logprob": -0.12172957954056766, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00039197670412249863}, {"id": 385, "seek": 164300, "start": 1666.92, "end": 1671.0, "text": " And this is because this is a well-known model called a state space model or SSM,", "tokens": [51560, 400, 341, 307, 570, 341, 307, 257, 731, 12, 6861, 2316, 1219, 257, 1785, 1901, 2316, 420, 12238, 44, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12172957954056766, "compression_ratio": 1.6988847583643123, "no_speech_prob": 0.00039197670412249863}, {"id": 386, "seek": 167100, "start": 1671.08, "end": 1674.52, "text": " which is sometimes depicted with this simple control diagram. And they've been around for", "tokens": [50368, 597, 307, 2171, 30207, 365, 341, 2199, 1969, 10686, 13, 400, 436, 600, 668, 926, 337, 50540], "temperature": 0.0, "avg_logprob": -0.10450859272733648, "compression_ratio": 1.6075471698113208, "no_speech_prob": 0.00018519598233979195}, {"id": 387, "seek": 167100, "start": 1674.52, "end": 1680.84, "text": " decades, such as the famous common filter and use in many scientific fields. I think outside of", "tokens": [50540, 7878, 11, 1270, 382, 264, 4618, 2689, 6608, 293, 764, 294, 867, 8134, 7909, 13, 286, 519, 2380, 295, 50856], "temperature": 0.0, "avg_logprob": -0.10450859272733648, "compression_ratio": 1.6075471698113208, "no_speech_prob": 0.00018519598233979195}, {"id": 388, "seek": 167100, "start": 1680.84, "end": 1686.76, "text": " controls and statistics, they're also pretty commonly used in perhaps computational neuroscience", "tokens": [50856, 9003, 293, 12523, 11, 436, 434, 611, 1238, 12719, 1143, 294, 4317, 28270, 42762, 51152], "temperature": 0.0, "avg_logprob": -0.10450859272733648, "compression_ratio": 1.6075471698113208, "no_speech_prob": 0.00018519598233979195}, {"id": 389, "seek": 167100, "start": 1686.76, "end": 1693.16, "text": " and many medical problems as well. Now, what the theme of this part will be is that", "tokens": [51152, 293, 867, 4625, 2740, 382, 731, 13, 823, 11, 437, 264, 6314, 295, 341, 644, 486, 312, 307, 300, 51472], "temperature": 0.0, "avg_logprob": -0.10450859272733648, "compression_ratio": 1.6075471698113208, "no_speech_prob": 0.00018519598233979195}, {"id": 390, "seek": 167100, "start": 1694.84, "end": 1697.64, "text": " we'll see that SSMs are a really elegant and natural model,", "tokens": [51556, 321, 603, 536, 300, 12238, 26386, 366, 257, 534, 21117, 293, 3303, 2316, 11, 51696], "temperature": 0.0, "avg_logprob": -0.10450859272733648, "compression_ratio": 1.6075471698113208, "no_speech_prob": 0.00018519598233979195}, {"id": 391, "seek": 169764, "start": 1698.2, "end": 1702.76, "text": " but they haven't been used in deep learning before in this way. And for underlying reasons", "tokens": [50392, 457, 436, 2378, 380, 668, 1143, 294, 2452, 2539, 949, 294, 341, 636, 13, 400, 337, 14217, 4112, 50620], "temperature": 0.0, "avg_logprob": -0.08977296504568547, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00027367082657292485}, {"id": 392, "seek": 169764, "start": 1702.76, "end": 1708.2, "text": " that we'll see in that S4 address. But for now, just to define S4 in terms of this model,", "tokens": [50620, 300, 321, 603, 536, 294, 300, 318, 19, 2985, 13, 583, 337, 586, 11, 445, 281, 6964, 318, 19, 294, 2115, 295, 341, 2316, 11, 50892], "temperature": 0.0, "avg_logprob": -0.08977296504568547, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00027367082657292485}, {"id": 393, "seek": 169764, "start": 1709.0800000000002, "end": 1715.0800000000002, "text": " the way that we'll define it is that it's just an instantiation of an SSM, these two equations,", "tokens": [50936, 264, 636, 300, 321, 603, 6964, 309, 307, 300, 309, 311, 445, 364, 9836, 6642, 295, 364, 12238, 44, 11, 613, 732, 11787, 11, 51236], "temperature": 0.0, "avg_logprob": -0.08977296504568547, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00027367082657292485}, {"id": 394, "seek": 169764, "start": 1715.96, "end": 1723.96, "text": " where we'll plug in specific values of matrices in. And although it turns out that", "tokens": [51280, 689, 321, 603, 5452, 294, 2685, 4190, 295, 32284, 294, 13, 400, 4878, 309, 4523, 484, 300, 51680], "temperature": 0.0, "avg_logprob": -0.08977296504568547, "compression_ratio": 1.617117117117117, "no_speech_prob": 0.00027367082657292485}, {"id": 395, "seek": 172396, "start": 1723.96, "end": 1727.8, "text": " although this model is simple to define, actually computing with it turns out to be", "tokens": [50364, 4878, 341, 2316, 307, 2199, 281, 6964, 11, 767, 15866, 365, 309, 4523, 484, 281, 312, 50556], "temperature": 0.0, "avg_logprob": -0.07451298690977551, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0001739796862239018}, {"id": 396, "seek": 172396, "start": 1727.8, "end": 1737.88, "text": " difficult and will require new ideas and algorithms. And so my goal of this in this section is to", "tokens": [50556, 2252, 293, 486, 3651, 777, 3487, 293, 14642, 13, 400, 370, 452, 3387, 295, 341, 294, 341, 3541, 307, 281, 51060], "temperature": 0.0, "avg_logprob": -0.07451298690977551, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0001739796862239018}, {"id": 397, "seek": 172396, "start": 1737.88, "end": 1745.08, "text": " convince you that this is a really elegant and fundamental model. And so first of all, I will", "tokens": [51060, 13447, 291, 300, 341, 307, 257, 534, 21117, 293, 8088, 2316, 13, 400, 370, 700, 295, 439, 11, 286, 486, 51420], "temperature": 0.0, "avg_logprob": -0.07451298690977551, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0001739796862239018}, {"id": 398, "seek": 172396, "start": 1745.08, "end": 1750.28, "text": " talk about some general properties of SSMs that would have a lot of benefits in machine learning", "tokens": [51420, 751, 466, 512, 2674, 7221, 295, 12238, 26386, 300, 576, 362, 257, 688, 295, 5311, 294, 3479, 2539, 51680], "temperature": 0.0, "avg_logprob": -0.07451298690977551, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0001739796862239018}, {"id": 399, "seek": 175028, "start": 1750.28, "end": 1759.0, "text": " and deep learning that are independent of S4. And then I'll show how those come with associated", "tokens": [50364, 293, 2452, 2539, 300, 366, 6695, 295, 318, 19, 13, 400, 550, 286, 603, 855, 577, 729, 808, 365, 6615, 50800], "temperature": 0.0, "avg_logprob": -0.06917359070344405, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0006163918878883123}, {"id": 400, "seek": 175028, "start": 1759.0, "end": 1763.6399999999999, "text": " trade-offs that prevent them from being really good in deep learning. And S4 will solve those", "tokens": [50800, 4923, 12, 19231, 300, 4871, 552, 490, 885, 534, 665, 294, 2452, 2539, 13, 400, 318, 19, 486, 5039, 729, 51032], "temperature": 0.0, "avg_logprob": -0.06917359070344405, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0006163918878883123}, {"id": 401, "seek": 175028, "start": 1763.6399999999999, "end": 1769.32, "text": " problems. And finally, I'll show several real world experiments that show S4's effectiveness", "tokens": [51032, 2740, 13, 400, 2721, 11, 286, 603, 855, 2940, 957, 1002, 12050, 300, 855, 318, 19, 311, 21208, 51316], "temperature": 0.0, "avg_logprob": -0.06917359070344405, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0006163918878883123}, {"id": 402, "seek": 175028, "start": 1769.32, "end": 1777.56, "text": " in a bunch of settings. So in this first part, I'm actually going to describe three different", "tokens": [51316, 294, 257, 3840, 295, 6257, 13, 407, 294, 341, 700, 644, 11, 286, 478, 767, 516, 281, 6786, 1045, 819, 51728], "temperature": 0.0, "avg_logprob": -0.06917359070344405, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0006163918878883123}, {"id": 403, "seek": 177756, "start": 1777.56, "end": 1781.72, "text": " ways to think about SSMs, which give them a lot of nice properties. And this was theory developed", "tokens": [50364, 2098, 281, 519, 466, 12238, 26386, 11, 597, 976, 552, 257, 688, 295, 1481, 7221, 13, 400, 341, 390, 5261, 4743, 50572], "temperature": 0.0, "avg_logprob": -0.09028602340846385, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.00017669804219622165}, {"id": 404, "seek": 177756, "start": 1781.72, "end": 1787.24, "text": " in the predecessor work to S4 that and will have empirical concrete empirical benefits.", "tokens": [50572, 294, 264, 34991, 589, 281, 318, 19, 300, 293, 486, 362, 31886, 9859, 31886, 5311, 13, 50848], "temperature": 0.0, "avg_logprob": -0.09028602340846385, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.00017669804219622165}, {"id": 405, "seek": 177756, "start": 1787.96, "end": 1793.48, "text": " And so the first way to the first property is that SSMs inherently operate on continuous time", "tokens": [50884, 400, 370, 264, 700, 636, 281, 264, 700, 4707, 307, 300, 12238, 26386, 27993, 9651, 322, 10957, 565, 51160], "temperature": 0.0, "avg_logprob": -0.09028602340846385, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.00017669804219622165}, {"id": 406, "seek": 177756, "start": 1793.48, "end": 1800.04, "text": " signals instead of discrete time sequences. So here's how to think about it. So in machine learning,", "tokens": [51160, 12354, 2602, 295, 27706, 565, 22978, 13, 407, 510, 311, 577, 281, 519, 466, 309, 13, 407, 294, 3479, 2539, 11, 51488], "temperature": 0.0, "avg_logprob": -0.09028602340846385, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.00017669804219622165}, {"id": 407, "seek": 177756, "start": 1800.04, "end": 1803.32, "text": " we usually work with sequence models, which I defined as a parameterized map", "tokens": [51488, 321, 2673, 589, 365, 8310, 5245, 11, 597, 286, 7642, 382, 257, 13075, 1602, 4471, 51652], "temperature": 0.0, "avg_logprob": -0.09028602340846385, "compression_ratio": 1.705223880597015, "no_speech_prob": 0.00017669804219622165}, {"id": 408, "seek": 180332, "start": 1803.8799999999999, "end": 1806.28, "text": " from an input sequence to an output sequence.", "tokens": [50392, 490, 364, 4846, 8310, 281, 364, 5598, 8310, 13, 50512], "temperature": 0.0, "avg_logprob": -0.06967035499778954, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0009999866597354412}, {"id": 409, "seek": 180332, "start": 1809.72, "end": 1815.6399999999999, "text": " What if instead of mapping a sequence to a sequence, I coined this term signal model", "tokens": [50684, 708, 498, 2602, 295, 18350, 257, 8310, 281, 257, 8310, 11, 286, 45222, 341, 1433, 6358, 2316, 50980], "temperature": 0.0, "avg_logprob": -0.06967035499778954, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0009999866597354412}, {"id": 410, "seek": 180332, "start": 1815.6399999999999, "end": 1820.12, "text": " to denote a parameterized map that maps a function to a function or a signal to a signal.", "tokens": [50980, 281, 45708, 257, 13075, 1602, 4471, 300, 11317, 257, 2445, 281, 257, 2445, 420, 257, 6358, 281, 257, 6358, 13, 51204], "temperature": 0.0, "avg_logprob": -0.06967035499778954, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0009999866597354412}, {"id": 411, "seek": 180332, "start": 1821.48, "end": 1827.3999999999999, "text": " And given one of these maps, you can essentially discretize the inputs and outputs however you", "tokens": [51272, 400, 2212, 472, 295, 613, 11317, 11, 291, 393, 4476, 25656, 1125, 264, 15743, 293, 23930, 4461, 291, 51568], "temperature": 0.0, "avg_logprob": -0.06967035499778954, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0009999866597354412}, {"id": 412, "seek": 182740, "start": 1827.4, "end": 1833.88, "text": " want to get back a sequence. So essentially, the upshot is that signal models are in some sense", "tokens": [50364, 528, 281, 483, 646, 257, 8310, 13, 407, 4476, 11, 264, 493, 18402, 307, 300, 6358, 5245, 366, 294, 512, 2020, 50688], "temperature": 0.0, "avg_logprob": -0.10051056462475377, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0006665885448455811}, {"id": 413, "seek": 182740, "start": 1833.88, "end": 1838.76, "text": " a generalization of sequence models, where they actually map functions and functions,", "tokens": [50688, 257, 2674, 2144, 295, 8310, 5245, 11, 689, 436, 767, 4471, 6828, 293, 6828, 11, 50932], "temperature": 0.0, "avg_logprob": -0.10051056462475377, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0006665885448455811}, {"id": 414, "seek": 182740, "start": 1838.76, "end": 1844.76, "text": " but by discretizing them, you get back a sequence model. And so the first way to think about SSMs", "tokens": [50932, 457, 538, 25656, 3319, 552, 11, 291, 483, 646, 257, 8310, 2316, 13, 400, 370, 264, 700, 636, 281, 519, 466, 12238, 26386, 51232], "temperature": 0.0, "avg_logprob": -0.10051056462475377, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0006665885448455811}, {"id": 415, "seek": 182740, "start": 1844.76, "end": 1849.96, "text": " is that they are just a simple parameterized signal model, where the parameters were matrices A, B,", "tokens": [51232, 307, 300, 436, 366, 445, 257, 2199, 13075, 1602, 6358, 2316, 11, 689, 264, 9834, 645, 32284, 316, 11, 363, 11, 51492], "temperature": 0.0, "avg_logprob": -0.10051056462475377, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0006665885448455811}, {"id": 416, "seek": 182740, "start": 1849.96, "end": 1856.6000000000001, "text": " C, and D, and they map an input function to an output function. That's it. Just in terms of the", "tokens": [51492, 383, 11, 293, 413, 11, 293, 436, 4471, 364, 4846, 2445, 281, 364, 5598, 2445, 13, 663, 311, 309, 13, 1449, 294, 2115, 295, 264, 51824], "temperature": 0.0, "avg_logprob": -0.10051056462475377, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0006665885448455811}, {"id": 417, "seek": 185660, "start": 1856.6799999999998, "end": 1861.6399999999999, "text": " interface or the API of the model, that this is what it does. The reason that this property is", "tokens": [50368, 9226, 420, 264, 9362, 295, 264, 2316, 11, 300, 341, 307, 437, 309, 775, 13, 440, 1778, 300, 341, 4707, 307, 50616], "temperature": 0.0, "avg_logprob": -0.07367263330477421, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.707759166602045e-05}, {"id": 418, "seek": 185660, "start": 1861.6399999999999, "end": 1867.24, "text": " important is because even when we're working in discrete time, the model in some sense understands", "tokens": [50616, 1021, 307, 570, 754, 562, 321, 434, 1364, 294, 27706, 565, 11, 264, 2316, 294, 512, 2020, 15146, 50896], "temperature": 0.0, "avg_logprob": -0.07367263330477421, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.707759166602045e-05}, {"id": 419, "seek": 185660, "start": 1867.24, "end": 1873.08, "text": " the underlying continuous domain. So I will show what I mean concretely by this later empirically.", "tokens": [50896, 264, 14217, 10957, 9274, 13, 407, 286, 486, 855, 437, 286, 914, 39481, 736, 538, 341, 1780, 25790, 984, 13, 51188], "temperature": 0.0, "avg_logprob": -0.07367263330477421, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.707759166602045e-05}, {"id": 420, "seek": 185660, "start": 1875.8, "end": 1880.04, "text": " All right, so that's the first representation. The next perspective relates back to the original", "tokens": [51324, 1057, 558, 11, 370, 300, 311, 264, 700, 10290, 13, 440, 958, 4585, 16155, 646, 281, 264, 3380, 51536], "temperature": 0.0, "avg_logprob": -0.07367263330477421, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.707759166602045e-05}, {"id": 421, "seek": 185660, "start": 1880.04, "end": 1885.1599999999999, "text": " motivation of HIPAA, which was about online computation. So how do we actually compute", "tokens": [51536, 12335, 295, 389, 9139, 5265, 11, 597, 390, 466, 2950, 24903, 13, 407, 577, 360, 321, 767, 14722, 51792], "temperature": 0.0, "avg_logprob": -0.07367263330477421, "compression_ratio": 1.6585365853658536, "no_speech_prob": 6.707759166602045e-05}, {"id": 422, "seek": 188516, "start": 1885.16, "end": 1890.52, "text": " the output of this SSM? One way to do it is to process the input one at a time, just like HIPAA did", "tokens": [50364, 264, 5598, 295, 341, 12238, 44, 30, 1485, 636, 281, 360, 309, 307, 281, 1399, 264, 4846, 472, 412, 257, 565, 11, 445, 411, 389, 9139, 5265, 630, 50632], "temperature": 0.0, "avg_logprob": -0.07793555494214667, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.00045813218457624316}, {"id": 423, "seek": 188516, "start": 1891.24, "end": 1896.76, "text": " in an online setting. And so this is our current computation because each update can be computed", "tokens": [50668, 294, 364, 2950, 3287, 13, 400, 370, 341, 307, 527, 2190, 24903, 570, 1184, 5623, 393, 312, 40610, 50944], "temperature": 0.0, "avg_logprob": -0.07793555494214667, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.00045813218457624316}, {"id": 424, "seek": 188516, "start": 1896.76, "end": 1902.92, "text": " efficiently from the previous one. And just to unpack a little why this is non-trivial, imagine", "tokens": [50944, 19621, 490, 264, 3894, 472, 13, 400, 445, 281, 26699, 257, 707, 983, 341, 307, 2107, 12, 83, 470, 22640, 11, 3811, 51252], "temperature": 0.0, "avg_logprob": -0.07793555494214667, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.00045813218457624316}, {"id": 425, "seek": 188516, "start": 1902.92, "end": 1908.44, "text": " we're processing this very long input, and we're at this current time step denoted by the vertical", "tokens": [51252, 321, 434, 9007, 341, 588, 938, 4846, 11, 293, 321, 434, 412, 341, 2190, 565, 1823, 1441, 23325, 538, 264, 9429, 51528], "temperature": 0.0, "avg_logprob": -0.07793555494214667, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.00045813218457624316}, {"id": 426, "seek": 188516, "start": 1908.44, "end": 1913.0800000000002, "text": " line, and we get just one more data point. So just like a single number for the input,", "tokens": [51528, 1622, 11, 293, 321, 483, 445, 472, 544, 1412, 935, 13, 407, 445, 411, 257, 2167, 1230, 337, 264, 4846, 11, 51760], "temperature": 0.0, "avg_logprob": -0.07793555494214667, "compression_ratio": 1.6771929824561405, "no_speech_prob": 0.00045813218457624316}, {"id": 427, "seek": 191308, "start": 1913.08, "end": 1919.56, "text": " and we want to compute the next output. So this output depends on the entire history of the input,", "tokens": [50364, 293, 321, 528, 281, 14722, 264, 958, 5598, 13, 407, 341, 5598, 5946, 322, 264, 2302, 2503, 295, 264, 4846, 11, 50688], "temperature": 0.0, "avg_logprob": -0.08489797054192959, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.00012727591092698276}, {"id": 428, "seek": 191308, "start": 1919.56, "end": 1924.76, "text": " and so you'd expect it, the computation of the next one to scale with the length of the sequence.", "tokens": [50688, 293, 370, 291, 1116, 2066, 309, 11, 264, 24903, 295, 264, 958, 472, 281, 4373, 365, 264, 4641, 295, 264, 8310, 13, 50948], "temperature": 0.0, "avg_logprob": -0.08489797054192959, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.00012727591092698276}, {"id": 429, "seek": 191308, "start": 1925.8799999999999, "end": 1931.96, "text": " But actually we can compute it in constant time. And this is a non-trivial property that most", "tokens": [51004, 583, 767, 321, 393, 14722, 309, 294, 5754, 565, 13, 400, 341, 307, 257, 2107, 12, 83, 470, 22640, 4707, 300, 881, 51308], "temperature": 0.0, "avg_logprob": -0.08489797054192959, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.00012727591092698276}, {"id": 430, "seek": 191308, "start": 1931.96, "end": 1937.08, "text": " sequence models don't have. For example, in a transformer or a convolution, if you were to", "tokens": [51308, 8310, 5245, 500, 380, 362, 13, 1171, 1365, 11, 294, 257, 31782, 420, 257, 45216, 11, 498, 291, 645, 281, 51564], "temperature": 0.0, "avg_logprob": -0.08489797054192959, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.00012727591092698276}, {"id": 431, "seek": 191308, "start": 1937.08, "end": 1942.1999999999998, "text": " do this in an online or autoregressive fashion, computing mapping one input to one output,", "tokens": [51564, 360, 341, 294, 364, 2950, 420, 1476, 418, 3091, 488, 6700, 11, 15866, 18350, 472, 4846, 281, 472, 5598, 11, 51820], "temperature": 0.0, "avg_logprob": -0.08489797054192959, "compression_ratio": 1.7226277372262773, "no_speech_prob": 0.00012727591092698276}, {"id": 432, "seek": 194220, "start": 1942.2, "end": 1945.24, "text": " each computation will scale with the entire length of the context window.", "tokens": [50364, 1184, 24903, 486, 4373, 365, 264, 2302, 4641, 295, 264, 4319, 4910, 13, 50516], "temperature": 0.0, "avg_logprob": -0.09024899548823291, "compression_ratio": 1.716, "no_speech_prob": 7.252753857756034e-05}, {"id": 433, "seek": 194220, "start": 1947.16, "end": 1950.52, "text": " The reason that SSMs can do this so efficiently is because they're stateful,", "tokens": [50612, 440, 1778, 300, 12238, 26386, 393, 360, 341, 370, 19621, 307, 570, 436, 434, 1785, 906, 11, 50780], "temperature": 0.0, "avg_logprob": -0.09024899548823291, "compression_ratio": 1.716, "no_speech_prob": 7.252753857756034e-05}, {"id": 434, "seek": 194220, "start": 1951.16, "end": 1956.3600000000001, "text": " which is a point that's kept coming up, where in memory we're maintaining a state,", "tokens": [50812, 597, 307, 257, 935, 300, 311, 4305, 1348, 493, 11, 689, 294, 4675, 321, 434, 14916, 257, 1785, 11, 51072], "temperature": 0.0, "avg_logprob": -0.09024899548823291, "compression_ratio": 1.716, "no_speech_prob": 7.252753857756034e-05}, {"id": 435, "seek": 194220, "start": 1956.3600000000001, "end": 1960.76, "text": " which is the blue thing, which is a single vector that summarizing the history, and can be updated", "tokens": [51072, 597, 307, 264, 3344, 551, 11, 597, 307, 257, 2167, 8062, 300, 14611, 3319, 264, 2503, 11, 293, 393, 312, 10588, 51292], "temperature": 0.0, "avg_logprob": -0.09024899548823291, "compression_ratio": 1.716, "no_speech_prob": 7.252753857756034e-05}, {"id": 436, "seek": 194220, "start": 1960.76, "end": 1970.52, "text": " very efficiently. This makes them really efficient in any sort of online setting, as we've seen.", "tokens": [51292, 588, 19621, 13, 639, 1669, 552, 534, 7148, 294, 604, 1333, 295, 2950, 3287, 11, 382, 321, 600, 1612, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09024899548823291, "compression_ratio": 1.716, "no_speech_prob": 7.252753857756034e-05}, {"id": 437, "seek": 197220, "start": 1972.76, "end": 1978.76, "text": " And yeah, so we'll see again why this matters. But there's one main drawback, which is that if", "tokens": [50392, 400, 1338, 11, 370, 321, 603, 536, 797, 983, 341, 7001, 13, 583, 456, 311, 472, 2135, 2642, 3207, 11, 597, 307, 300, 498, 50692], "temperature": 0.0, "avg_logprob": -0.10553238842938398, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00031011251849122345}, {"id": 438, "seek": 197220, "start": 1978.76, "end": 1985.0800000000002, "text": " you're not in an online setting, this is slow because it's sequential. And so what if you", "tokens": [50692, 291, 434, 406, 294, 364, 2950, 3287, 11, 341, 307, 2964, 570, 309, 311, 42881, 13, 400, 370, 437, 498, 291, 51008], "temperature": 0.0, "avg_logprob": -0.10553238842938398, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00031011251849122345}, {"id": 439, "seek": 197220, "start": 1985.0800000000002, "end": 1989.72, "text": " actually know all the future inputs? Then ideally you wouldn't do this step by step,", "tokens": [51008, 767, 458, 439, 264, 2027, 15743, 30, 1396, 22915, 291, 2759, 380, 360, 341, 1823, 538, 1823, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10553238842938398, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00031011251849122345}, {"id": 440, "seek": 197220, "start": 1989.72, "end": 1994.8400000000001, "text": " and you could do something faster and parallelizable. And so that was actually basically the main", "tokens": [51240, 293, 291, 727, 360, 746, 4663, 293, 8952, 22395, 13, 400, 370, 300, 390, 767, 1936, 264, 2135, 51496], "temperature": 0.0, "avg_logprob": -0.10553238842938398, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00031011251849122345}, {"id": 441, "seek": 197220, "start": 1994.8400000000001, "end": 1999.64, "text": " problem with RNNs and why they've recently fallen out of favor in machine learning,", "tokens": [51496, 1154, 365, 45702, 45, 82, 293, 983, 436, 600, 3938, 11547, 484, 295, 2294, 294, 3479, 2539, 11, 51736], "temperature": 0.0, "avg_logprob": -0.10553238842938398, "compression_ratio": 1.6340579710144927, "no_speech_prob": 0.00031011251849122345}, {"id": 442, "seek": 199964, "start": 1999.64, "end": 2005.5600000000002, "text": " because they're sequential and not parallelizable when you see a lot of data at once.", "tokens": [50364, 570, 436, 434, 42881, 293, 406, 8952, 22395, 562, 291, 536, 257, 688, 295, 1412, 412, 1564, 13, 50660], "temperature": 0.0, "avg_logprob": -0.07100535855434909, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.00011233759869355708}, {"id": 443, "seek": 199964, "start": 2007.3200000000002, "end": 2010.5200000000002, "text": " And so that motivates the final representation, which is the convolutional representation,", "tokens": [50748, 400, 370, 300, 42569, 264, 2572, 10290, 11, 597, 307, 264, 45216, 304, 10290, 11, 50908], "temperature": 0.0, "avg_logprob": -0.07100535855434909, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.00011233759869355708}, {"id": 444, "seek": 199964, "start": 2010.5200000000002, "end": 2015.48, "text": " which allows them to be paralyzed. And so the idea is that instead of mapping going from the input", "tokens": [50908, 597, 4045, 552, 281, 312, 41919, 13, 400, 370, 264, 1558, 307, 300, 2602, 295, 18350, 516, 490, 264, 4846, 51156], "temperature": 0.0, "avg_logprob": -0.07100535855434909, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.00011233759869355708}, {"id": 445, "seek": 199964, "start": 2015.48, "end": 2019.72, "text": " to the state to the output, you can actually go straight from the input to the output,", "tokens": [51156, 281, 264, 1785, 281, 264, 5598, 11, 291, 393, 767, 352, 2997, 490, 264, 4846, 281, 264, 5598, 11, 51368], "temperature": 0.0, "avg_logprob": -0.07100535855434909, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.00011233759869355708}, {"id": 446, "seek": 199964, "start": 2020.44, "end": 2025.64, "text": " bypassing the state, and doing the entire computation in parallel over the sequence length.", "tokens": [51404, 24996, 278, 264, 1785, 11, 293, 884, 264, 2302, 24903, 294, 8952, 670, 264, 8310, 4641, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07100535855434909, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.00011233759869355708}, {"id": 447, "seek": 202564, "start": 2026.2, "end": 2030.2, "text": " The reason is that SSMs turn out to be equivalent to convolutions,", "tokens": [50392, 440, 1778, 307, 300, 12238, 26386, 1261, 484, 281, 312, 10344, 281, 3754, 15892, 11, 50592], "temperature": 0.0, "avg_logprob": -0.1473958880402321, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00016862594929989427}, {"id": 448, "seek": 202564, "start": 2031.5600000000002, "end": 2037.88, "text": " where computing the map from the input U to the output Y is equivalent to convolving the input", "tokens": [50660, 689, 15866, 264, 4471, 490, 264, 4846, 624, 281, 264, 5598, 398, 307, 10344, 281, 3754, 401, 798, 264, 4846, 50976], "temperature": 0.0, "avg_logprob": -0.1473958880402321, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00016862594929989427}, {"id": 449, "seek": 202564, "start": 2037.88, "end": 2044.6000000000001, "text": " by a particular convolution filter, which is depicted in green here. And so to compute this", "tokens": [50976, 538, 257, 1729, 45216, 6608, 11, 597, 307, 30207, 294, 3092, 510, 13, 400, 370, 281, 14722, 341, 51312], "temperature": 0.0, "avg_logprob": -0.1473958880402321, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00016862594929989427}, {"id": 450, "seek": 202564, "start": 2044.6000000000001, "end": 2050.76, "text": " map, you just do it's just Y equals U convolved with K for this convolution kernel. And so this", "tokens": [51312, 4471, 11, 291, 445, 360, 309, 311, 445, 398, 6915, 624, 3754, 29110, 365, 591, 337, 341, 45216, 28256, 13, 400, 370, 341, 51620], "temperature": 0.0, "avg_logprob": -0.1473958880402321, "compression_ratio": 1.7107843137254901, "no_speech_prob": 0.00016862594929989427}, {"id": 451, "seek": 205076, "start": 2050.76, "end": 2059.96, "text": " can be done very efficiently using no techniques. So for the practitioner, what one thing I want", "tokens": [50364, 393, 312, 1096, 588, 19621, 1228, 572, 7512, 13, 407, 337, 264, 32125, 11, 437, 472, 551, 286, 528, 50824], "temperature": 0.0, "avg_logprob": -0.08370733842617128, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.00019713115761987865}, {"id": 452, "seek": 205076, "start": 2059.96, "end": 2065.2400000000002, "text": " to emphasize is that I think the most useful way to think about SSMs potentially is as", "tokens": [50824, 281, 16078, 307, 300, 286, 519, 264, 881, 4420, 636, 281, 519, 466, 12238, 26386, 7263, 307, 382, 51088], "temperature": 0.0, "avg_logprob": -0.08370733842617128, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.00019713115761987865}, {"id": 453, "seek": 205076, "start": 2065.2400000000002, "end": 2070.44, "text": " essentially a very fancy CNN, where you're parameterizing the convolution kernel in a", "tokens": [51088, 4476, 257, 588, 10247, 24859, 11, 689, 291, 434, 13075, 3319, 264, 45216, 28256, 294, 257, 51348], "temperature": 0.0, "avg_logprob": -0.08370733842617128, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.00019713115761987865}, {"id": 454, "seek": 205076, "start": 2070.44, "end": 2076.84, "text": " different way. And notably, this kernel can be infinitely long, which again points to one reason", "tokens": [51348, 819, 636, 13, 400, 31357, 11, 341, 28256, 393, 312, 36227, 938, 11, 597, 797, 2793, 281, 472, 1778, 51668], "temperature": 0.0, "avg_logprob": -0.08370733842617128, "compression_ratio": 1.5378151260504203, "no_speech_prob": 0.00019713115761987865}, {"id": 455, "seek": 207684, "start": 2076.84, "end": 2086.6800000000003, "text": " why this is very good at long range dependencies. So just to call back to this example again,", "tokens": [50364, 983, 341, 307, 588, 665, 412, 938, 3613, 36606, 13, 407, 445, 281, 818, 646, 281, 341, 1365, 797, 11, 50856], "temperature": 0.0, "avg_logprob": -0.09751258894454601, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0002377830387558788}, {"id": 456, "seek": 207684, "start": 2086.6800000000003, "end": 2092.1200000000003, "text": " the EMA, the EMA is actually literally just a single convolution where you can involve the input", "tokens": [50856, 264, 462, 9998, 11, 264, 462, 9998, 307, 767, 3736, 445, 257, 2167, 45216, 689, 291, 393, 9494, 264, 4846, 51128], "temperature": 0.0, "avg_logprob": -0.09751258894454601, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0002377830387558788}, {"id": 457, "seek": 207684, "start": 2092.1200000000003, "end": 2098.76, "text": " by an accidentally decaying convolution kernel. And as I mentioned, although things like CNNs", "tokens": [51128, 538, 364, 15715, 21039, 278, 45216, 28256, 13, 400, 382, 286, 2835, 11, 4878, 721, 411, 24859, 82, 51460], "temperature": 0.0, "avg_logprob": -0.09751258894454601, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0002377830387558788}, {"id": 458, "seek": 207684, "start": 2098.76, "end": 2103.88, "text": " are also literally convolutions, they can't represent the EMA because CNNs are finite", "tokens": [51460, 366, 611, 3736, 3754, 15892, 11, 436, 393, 380, 2906, 264, 462, 9998, 570, 24859, 82, 366, 19362, 51716], "temperature": 0.0, "avg_logprob": -0.09751258894454601, "compression_ratio": 1.6371681415929205, "no_speech_prob": 0.0002377830387558788}, {"id": 459, "seek": 210388, "start": 2103.88, "end": 2108.84, "text": " window and the EMA is infinite window. On the other hand, SSMs do represent infinitely long", "tokens": [50364, 4910, 293, 264, 462, 9998, 307, 13785, 4910, 13, 1282, 264, 661, 1011, 11, 12238, 26386, 360, 2906, 36227, 938, 50612], "temperature": 0.0, "avg_logprob": -0.05822443962097168, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0004581972898449749}, {"id": 460, "seek": 210388, "start": 2108.84, "end": 2114.28, "text": " convolutions. And in fact, there's a very, very simple way to write down the EMA as a directly", "tokens": [50612, 3754, 15892, 13, 400, 294, 1186, 11, 456, 311, 257, 588, 11, 588, 2199, 636, 281, 2464, 760, 264, 462, 9998, 382, 257, 3838, 50884], "temperature": 0.0, "avg_logprob": -0.05822443962097168, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0004581972898449749}, {"id": 461, "seek": 210388, "start": 2114.28, "end": 2124.2000000000003, "text": " as an SSM. And I think Chris kind of pointed to that earlier. So those were the three properties", "tokens": [50884, 382, 364, 12238, 44, 13, 400, 286, 519, 6688, 733, 295, 10932, 281, 300, 3071, 13, 407, 729, 645, 264, 1045, 7221, 51380], "temperature": 0.0, "avg_logprob": -0.05822443962097168, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0004581972898449749}, {"id": 462, "seek": 210388, "start": 2124.2000000000003, "end": 2130.2000000000003, "text": " of SSMs that I wanted to mention. And just to recap, first of all, we're going to think of them as", "tokens": [51380, 295, 12238, 26386, 300, 286, 1415, 281, 2152, 13, 400, 445, 281, 20928, 11, 700, 295, 439, 11, 321, 434, 516, 281, 519, 295, 552, 382, 51680], "temperature": 0.0, "avg_logprob": -0.05822443962097168, "compression_ratio": 1.6050420168067228, "no_speech_prob": 0.0004581972898449749}, {"id": 463, "seek": 213020, "start": 2130.2, "end": 2136.4399999999996, "text": " maps that operate on continuous signals, not just sequences. If your model is deployed in a", "tokens": [50364, 11317, 300, 9651, 322, 10957, 12354, 11, 406, 445, 22978, 13, 759, 428, 2316, 307, 17826, 294, 257, 50676], "temperature": 0.0, "avg_logprob": -0.10480380780769116, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.0012831948697566986}, {"id": 464, "seek": 213020, "start": 2136.4399999999996, "end": 2140.6, "text": " setting where it sees inputs in real time or online, it can compute these efficiently", "tokens": [50676, 3287, 689, 309, 8194, 15743, 294, 957, 565, 420, 2950, 11, 309, 393, 14722, 613, 19621, 50884], "temperature": 0.0, "avg_logprob": -0.10480380780769116, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.0012831948697566986}, {"id": 465, "seek": 213020, "start": 2143.72, "end": 2149.24, "text": " recurrently. And if you see an entire input at once, such as usually during training time,", "tokens": [51040, 18680, 1753, 356, 13, 400, 498, 291, 536, 364, 2302, 4846, 412, 1564, 11, 1270, 382, 2673, 1830, 3097, 565, 11, 51316], "temperature": 0.0, "avg_logprob": -0.10480380780769116, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.0012831948697566986}, {"id": 466, "seek": 213020, "start": 2149.24, "end": 2151.48, "text": " you can compute it even more efficiently and in parallel.", "tokens": [51316, 291, 393, 14722, 309, 754, 544, 19621, 293, 294, 8952, 13, 51428], "temperature": 0.0, "avg_logprob": -0.10480380780769116, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.0012831948697566986}, {"id": 467, "seek": 213020, "start": 2154.4399999999996, "end": 2159.96, "text": " I have a quick question here, Albert. This is super cool. I was just wondering if the goal", "tokens": [51576, 286, 362, 257, 1702, 1168, 510, 11, 20812, 13, 639, 307, 1687, 1627, 13, 286, 390, 445, 6359, 498, 264, 3387, 51852], "temperature": 0.0, "avg_logprob": -0.10480380780769116, "compression_ratio": 1.6482213438735178, "no_speech_prob": 0.0012831948697566986}, {"id": 468, "seek": 215996, "start": 2159.96, "end": 2164.04, "text": " is actually to get a representation of your signal so that you can perform different", "tokens": [50364, 307, 767, 281, 483, 257, 10290, 295, 428, 6358, 370, 300, 291, 393, 2042, 819, 50568], "temperature": 0.0, "avg_logprob": -0.11896044708961664, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0007550973678007722}, {"id": 469, "seek": 215996, "start": 2164.04, "end": 2169.0, "text": " downstream tasks. Isn't it better to actually have the state space representation rather than", "tokens": [50568, 30621, 9608, 13, 6998, 380, 309, 1101, 281, 767, 362, 264, 1785, 1901, 10290, 2831, 813, 50816], "temperature": 0.0, "avg_logprob": -0.11896044708961664, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0007550973678007722}, {"id": 470, "seek": 215996, "start": 2169.0, "end": 2174.92, "text": " directly going to the outputs? In that case, would we have to stick with HIPAA instead of going to S4?", "tokens": [50816, 3838, 516, 281, 264, 23930, 30, 682, 300, 1389, 11, 576, 321, 362, 281, 2897, 365, 389, 9139, 5265, 2602, 295, 516, 281, 318, 19, 30, 51112], "temperature": 0.0, "avg_logprob": -0.11896044708961664, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0007550973678007722}, {"id": 471, "seek": 215996, "start": 2176.44, "end": 2180.68, "text": " Great question. Actually, no one's asked me that, but that's a great question.", "tokens": [51188, 3769, 1168, 13, 5135, 11, 572, 472, 311, 2351, 385, 300, 11, 457, 300, 311, 257, 869, 1168, 13, 51400], "temperature": 0.0, "avg_logprob": -0.11896044708961664, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0007550973678007722}, {"id": 472, "seek": 218068, "start": 2181.3999999999996, "end": 2191.56, "text": " So the way that I think about this is that what's happening is that essentially we have", "tokens": [50400, 407, 264, 636, 300, 286, 519, 466, 341, 307, 300, 437, 311, 2737, 307, 300, 4476, 321, 362, 50908], "temperature": 0.0, "avg_logprob": -0.1415282885233561, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.017978789284825325}, {"id": 473, "seek": 218068, "start": 2191.56, "end": 2197.56, "text": " this nice state, which is very meaningful. And then the second part of the SSM that projects it", "tokens": [50908, 341, 1481, 1785, 11, 597, 307, 588, 10995, 13, 400, 550, 264, 1150, 644, 295, 264, 12238, 44, 300, 4455, 309, 51208], "temperature": 0.0, "avg_logprob": -0.1415282885233561, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.017978789284825325}, {"id": 474, "seek": 218068, "start": 2197.56, "end": 2201.7999999999997, "text": " is kind of like the learnable thing that's figuring out how to extract the right features", "tokens": [51208, 307, 733, 295, 411, 264, 1466, 712, 551, 300, 311, 15213, 484, 577, 281, 8947, 264, 558, 4122, 51420], "temperature": 0.0, "avg_logprob": -0.1415282885233561, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.017978789284825325}, {"id": 475, "seek": 218068, "start": 2201.7999999999997, "end": 2207.0, "text": " from this state. Now, I mentioned that everything I've done so far, so that's a learnable part", "tokens": [51420, 490, 341, 1785, 13, 823, 11, 286, 2835, 300, 1203, 286, 600, 1096, 370, 1400, 11, 370, 300, 311, 257, 1466, 712, 644, 51680], "temperature": 0.0, "avg_logprob": -0.1415282885233561, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.017978789284825325}, {"id": 476, "seek": 220700, "start": 2207.0, "end": 2212.04, "text": " that's actually using the entire state, in a sense. And I mentioned that I'm only considering", "tokens": [50364, 300, 311, 767, 1228, 264, 2302, 1785, 11, 294, 257, 2020, 13, 400, 286, 2835, 300, 286, 478, 787, 8079, 50616], "temperature": 0.0, "avg_logprob": -0.11062262798177785, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0017538605025038123}, {"id": 477, "seek": 220700, "start": 2212.04, "end": 2218.12, "text": " the one-dimensional case so far with 1B inputs and outputs. But actually, what's going to happen", "tokens": [50616, 264, 472, 12, 18759, 1389, 370, 1400, 365, 502, 33, 15743, 293, 23930, 13, 583, 767, 11, 437, 311, 516, 281, 1051, 50920], "temperature": 0.0, "avg_logprob": -0.11062262798177785, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0017538605025038123}, {"id": 478, "seek": 220700, "start": 2218.12, "end": 2223.8, "text": " in practice in our actual deep learning models is that we'll have multi-dimensional inputs and", "tokens": [50920, 294, 3124, 294, 527, 3539, 2452, 2539, 5245, 307, 300, 321, 603, 362, 4825, 12, 18759, 15743, 293, 51204], "temperature": 0.0, "avg_logprob": -0.11062262798177785, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0017538605025038123}, {"id": 479, "seek": 220700, "start": 2223.8, "end": 2229.08, "text": " outputs, and we'll essentially run an SSM on each one of them. And each one of these will", "tokens": [51204, 23930, 11, 293, 321, 603, 4476, 1190, 364, 12238, 44, 322, 1184, 472, 295, 552, 13, 400, 1184, 472, 295, 613, 486, 51468], "temperature": 0.0, "avg_logprob": -0.11062262798177785, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0017538605025038123}, {"id": 480, "seek": 220700, "start": 2229.08, "end": 2234.12, "text": " learn how to use the state in a different way. So we'll have essentially many, you can think of", "tokens": [51468, 1466, 577, 281, 764, 264, 1785, 294, 257, 819, 636, 13, 407, 321, 603, 362, 4476, 867, 11, 291, 393, 519, 295, 51720], "temperature": 0.0, "avg_logprob": -0.11062262798177785, "compression_ratio": 1.7840909090909092, "no_speech_prob": 0.0017538605025038123}, {"id": 481, "seek": 223412, "start": 2234.2, "end": 2239.24, "text": " as maybe we'll have a single state, but many, many possible outputs that are all learnable,", "tokens": [50368, 382, 1310, 321, 603, 362, 257, 2167, 1785, 11, 457, 867, 11, 867, 1944, 23930, 300, 366, 439, 1466, 712, 11, 50620], "temperature": 0.0, "avg_logprob": -0.16626251693320485, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.00048012647312134504}, {"id": 482, "seek": 223412, "start": 2239.24, "end": 2243.24, "text": " and we'll extract different features from that state. So we are going to get a lot of different", "tokens": [50620, 293, 321, 603, 8947, 819, 4122, 490, 300, 1785, 13, 407, 321, 366, 516, 281, 483, 257, 688, 295, 819, 50820], "temperature": 0.0, "avg_logprob": -0.16626251693320485, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.00048012647312134504}, {"id": 483, "seek": 223412, "start": 2243.24, "end": 2250.04, "text": " features that utilize the state in however they want. I see. Okay. Thank you. But sorry,", "tokens": [50820, 4122, 300, 16117, 264, 1785, 294, 4461, 436, 528, 13, 286, 536, 13, 1033, 13, 1044, 291, 13, 583, 2597, 11, 51160], "temperature": 0.0, "avg_logprob": -0.16626251693320485, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.00048012647312134504}, {"id": 484, "seek": 223412, "start": 2250.04, "end": 2257.96, "text": " sorry, Joanne. But isn't it like all these dimensions also have a correlation? So do you also,", "tokens": [51160, 2597, 11, 3139, 12674, 13, 583, 1943, 380, 309, 411, 439, 613, 12819, 611, 362, 257, 20009, 30, 407, 360, 291, 611, 11, 51556], "temperature": 0.0, "avg_logprob": -0.16626251693320485, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.00048012647312134504}, {"id": 485, "seek": 223412, "start": 2257.96, "end": 2262.68, "text": " if you run the space independently, don't you want to also preserve the correlation?", "tokens": [51556, 498, 291, 1190, 264, 1901, 21761, 11, 500, 380, 291, 528, 281, 611, 15665, 264, 20009, 30, 51792], "temperature": 0.0, "avg_logprob": -0.16626251693320485, "compression_ratio": 1.7078651685393258, "no_speech_prob": 0.00048012647312134504}, {"id": 486, "seek": 226412, "start": 2264.8399999999997, "end": 2267.96, "text": " So this is something that I think a lot of people working with time series", "tokens": [50400, 407, 341, 307, 746, 300, 286, 519, 257, 688, 295, 561, 1364, 365, 565, 2638, 50556], "temperature": 0.0, "avg_logprob": -0.10613288702788176, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.00014881504466757178}, {"id": 487, "seek": 226412, "start": 2269.24, "end": 2274.44, "text": " are concerned with. And somehow in deep learning, we don't normally consider that aspect. And we", "tokens": [50620, 366, 5922, 365, 13, 400, 6063, 294, 2452, 2539, 11, 321, 500, 380, 5646, 1949, 300, 4171, 13, 400, 321, 50880], "temperature": 0.0, "avg_logprob": -0.10613288702788176, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.00014881504466757178}, {"id": 488, "seek": 226412, "start": 2274.44, "end": 2280.44, "text": " kind of just throw in a really big model and a lot of these independent layers. And kind of,", "tokens": [50880, 733, 295, 445, 3507, 294, 257, 534, 955, 2316, 293, 257, 688, 295, 613, 6695, 7914, 13, 400, 733, 295, 11, 51180], "temperature": 0.0, "avg_logprob": -0.10613288702788176, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.00014881504466757178}, {"id": 489, "seek": 226412, "start": 2280.44, "end": 2285.48, "text": " I think in practice, what usually happens at the model learns to, it learns whatever it needs to do", "tokens": [51180, 286, 519, 294, 3124, 11, 437, 2673, 2314, 412, 264, 2316, 27152, 281, 11, 309, 27152, 2035, 309, 2203, 281, 360, 51432], "temperature": 0.0, "avg_logprob": -0.10613288702788176, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.00014881504466757178}, {"id": 490, "seek": 226412, "start": 2285.48, "end": 2290.6, "text": " for the final prediction task. And this often does involve like, I think it does end up", "tokens": [51432, 337, 264, 2572, 17630, 5633, 13, 400, 341, 2049, 775, 9494, 411, 11, 286, 519, 309, 775, 917, 493, 51688], "temperature": 0.0, "avg_logprob": -0.10613288702788176, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.00014881504466757178}, {"id": 491, "seek": 229060, "start": 2290.6, "end": 2295.96, "text": " decorrelating things. But it's not super clear exactly the dynamics of what happens. And this is", "tokens": [50364, 979, 284, 4419, 990, 721, 13, 583, 309, 311, 406, 1687, 1850, 2293, 264, 15679, 295, 437, 2314, 13, 400, 341, 307, 50632], "temperature": 0.0, "avg_logprob": -0.09374595970235845, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.003943301737308502}, {"id": 492, "seek": 229060, "start": 2295.96, "end": 2300.52, "text": " kind of a more broad question for deep learning theory in general. That's not well understood", "tokens": [50632, 733, 295, 257, 544, 4152, 1168, 337, 2452, 2539, 5261, 294, 2674, 13, 663, 311, 406, 731, 7320, 50860], "temperature": 0.0, "avg_logprob": -0.09374595970235845, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.003943301737308502}, {"id": 493, "seek": 229060, "start": 2300.52, "end": 2309.3199999999997, "text": " right now. What I can say is that we've used this on many types of like noisy data that usually", "tokens": [50860, 558, 586, 13, 708, 286, 393, 584, 307, 300, 321, 600, 1143, 341, 322, 867, 3467, 295, 411, 24518, 1412, 300, 2673, 51300], "temperature": 0.0, "avg_logprob": -0.09374595970235845, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.003943301737308502}, {"id": 494, "seek": 229060, "start": 2309.3199999999997, "end": 2314.2799999999997, "text": " involve, so I'm going to get to experiments later, but we have tried this on many types of", "tokens": [51300, 9494, 11, 370, 286, 478, 516, 281, 483, 281, 12050, 1780, 11, 457, 321, 362, 3031, 341, 322, 867, 3467, 295, 51548], "temperature": 0.0, "avg_logprob": -0.09374595970235845, "compression_ratio": 1.590717299578059, "no_speech_prob": 0.003943301737308502}, {"id": 495, "seek": 231428, "start": 2314.28, "end": 2318.0400000000004, "text": " like time series and other noisy data like EEG. But one day, one day, right?", "tokens": [50364, 411, 565, 2638, 293, 661, 24518, 1412, 411, 33685, 38, 13, 583, 472, 786, 11, 472, 786, 11, 558, 30, 50552], "temperature": 0.0, "avg_logprob": -0.15479721973851784, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.005462996661663055}, {"id": 496, "seek": 231428, "start": 2319.5600000000004, "end": 2324.36, "text": " It can work on multiple dimensions, which I kind of just pointed to you. And also I'll mention", "tokens": [50628, 467, 393, 589, 322, 3866, 12819, 11, 597, 286, 733, 295, 445, 10932, 281, 291, 13, 400, 611, 286, 603, 2152, 50868], "temperature": 0.0, "avg_logprob": -0.15479721973851784, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.005462996661663055}, {"id": 497, "seek": 231428, "start": 2324.36, "end": 2330.84, "text": " again later how we do that. But yeah, you can just kind of do it do it naively on multiple", "tokens": [50868, 797, 1780, 577, 321, 360, 300, 13, 583, 1338, 11, 291, 393, 445, 733, 295, 360, 309, 360, 309, 1667, 3413, 322, 3866, 51192], "temperature": 0.0, "avg_logprob": -0.15479721973851784, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.005462996661663055}, {"id": 498, "seek": 231428, "start": 2330.84, "end": 2337.7200000000003, "text": " dimensions. And it just works out of the box. Okay. Okay, so before we get to the experiments,", "tokens": [51192, 12819, 13, 400, 309, 445, 1985, 484, 295, 264, 2424, 13, 1033, 13, 1033, 11, 370, 949, 321, 483, 281, 264, 12050, 11, 51536], "temperature": 0.0, "avg_logprob": -0.15479721973851784, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.005462996661663055}, {"id": 499, "seek": 233772, "start": 2337.72, "end": 2345.64, "text": " I just have a little bit on kind of the how S4 builds on top of SSMs. And so just to refresh", "tokens": [50364, 286, 445, 362, 257, 707, 857, 322, 733, 295, 264, 577, 318, 19, 15182, 322, 1192, 295, 12238, 26386, 13, 400, 370, 445, 281, 15134, 50760], "temperature": 0.0, "avg_logprob": -0.1129465103149414, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0006461238372139633}, {"id": 500, "seek": 233772, "start": 2345.64, "end": 2351.48, "text": " your memory of what S4 is, it's just an SSM where we plug in certain formulas that were", "tokens": [50760, 428, 4675, 295, 437, 318, 19, 307, 11, 309, 311, 445, 364, 12238, 44, 689, 321, 5452, 294, 1629, 30546, 300, 645, 51052], "temperature": 0.0, "avg_logprob": -0.1129465103149414, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0006461238372139633}, {"id": 501, "seek": 233772, "start": 2351.48, "end": 2354.9199999999996, "text": " based on the theory of memorization. And we have special algorithms to compute it.", "tokens": [51052, 2361, 322, 264, 5261, 295, 10560, 2144, 13, 400, 321, 362, 2121, 14642, 281, 14722, 309, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1129465103149414, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0006461238372139633}, {"id": 502, "seek": 233772, "start": 2356.7599999999998, "end": 2363.0, "text": " And so first of all, why are these matrices needed? Well, the most important part of the SSM is the", "tokens": [51316, 400, 370, 700, 295, 439, 11, 983, 366, 613, 32284, 2978, 30, 1042, 11, 264, 881, 1021, 644, 295, 264, 12238, 44, 307, 264, 51628], "temperature": 0.0, "avg_logprob": -0.1129465103149414, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0006461238372139633}, {"id": 503, "seek": 236300, "start": 2363.08, "end": 2373.72, "text": " state as Nandita keeps insightfully bringing up. And so what HIPPO did was that it computed a", "tokens": [50368, 1785, 382, 426, 474, 2786, 5965, 11269, 2277, 5062, 493, 13, 400, 370, 437, 389, 9139, 34885, 630, 390, 300, 309, 40610, 257, 50900], "temperature": 0.0, "avg_logprob": -0.12641087700338924, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011691033141687512}, {"id": 504, "seek": 236300, "start": 2373.72, "end": 2379.56, "text": " very particular state that was mathematically meaningful and compresses the history of the input", "tokens": [50900, 588, 1729, 1785, 300, 390, 44003, 10995, 293, 14778, 279, 264, 2503, 295, 264, 4846, 51192], "temperature": 0.0, "avg_logprob": -0.12641087700338924, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011691033141687512}, {"id": 505, "seek": 236300, "start": 2380.76, "end": 2385.64, "text": " in a way that captures long range dependencies. And so basically just by plugging in that formula", "tokens": [51252, 294, 257, 636, 300, 27986, 938, 3613, 36606, 13, 400, 370, 1936, 445, 538, 42975, 294, 300, 8513, 51496], "temperature": 0.0, "avg_logprob": -0.12641087700338924, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011691033141687512}, {"id": 506, "seek": 236300, "start": 2385.64, "end": 2390.36, "text": " into this SSM, it learns a more meaningful state that allows the SSM to address long", "tokens": [51496, 666, 341, 12238, 44, 11, 309, 27152, 257, 544, 10995, 1785, 300, 4045, 264, 12238, 44, 281, 2985, 938, 51732], "temperature": 0.0, "avg_logprob": -0.12641087700338924, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011691033141687512}, {"id": 507, "seek": 239036, "start": 2390.36, "end": 2395.88, "text": " dependencies better. So just to illustrate this empirically, here's a simple experiment on a very", "tokens": [50364, 36606, 1101, 13, 407, 445, 281, 23221, 341, 25790, 984, 11, 510, 311, 257, 2199, 5120, 322, 257, 588, 50640], "temperature": 0.0, "avg_logprob": -0.1438719285737484, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00037400261498987675}, {"id": 508, "seek": 239036, "start": 2395.88, "end": 2401.6400000000003, "text": " standard benchmark for sequence models. The actual task doesn't matter, but it's well studied and", "tokens": [50640, 3832, 18927, 337, 8310, 5245, 13, 440, 3539, 5633, 1177, 380, 1871, 11, 457, 309, 311, 731, 9454, 293, 50928], "temperature": 0.0, "avg_logprob": -0.1438719285737484, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00037400261498987675}, {"id": 509, "seek": 239036, "start": 2401.6400000000003, "end": 2406.76, "text": " standard sequence model based on such as transformers, CNNs and LSTMs all get to around the", "tokens": [50928, 3832, 8310, 2316, 2361, 322, 1270, 382, 4088, 433, 11, 24859, 82, 293, 441, 6840, 26386, 439, 483, 281, 926, 264, 51184], "temperature": 0.0, "avg_logprob": -0.1438719285737484, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00037400261498987675}, {"id": 510, "seek": 239036, "start": 2406.76, "end": 2411.48, "text": " around the same accuracy of like 60 ish percent. Now, what happens if we use an SSM?", "tokens": [51184, 926, 264, 912, 14170, 295, 411, 4060, 307, 71, 3043, 13, 823, 11, 437, 2314, 498, 321, 764, 364, 12238, 44, 30, 51420], "temperature": 0.0, "avg_logprob": -0.1438719285737484, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00037400261498987675}, {"id": 511, "seek": 239036, "start": 2412.6, "end": 2415.96, "text": " If you use it naively by randomly initializing all the parameters, which is", "tokens": [51476, 759, 291, 764, 309, 1667, 3413, 538, 16979, 5883, 3319, 439, 264, 9834, 11, 597, 307, 51644], "temperature": 0.0, "avg_logprob": -0.1438719285737484, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.00037400261498987675}, {"id": 512, "seek": 241596, "start": 2415.96, "end": 2419.48, "text": " what you would typically do in deep learning, it actually does terribly.", "tokens": [50364, 437, 291, 576, 5850, 360, 294, 2452, 2539, 11, 309, 767, 775, 22903, 13, 50540], "temperature": 0.0, "avg_logprob": -0.09771183685020164, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0004441499477252364}, {"id": 513, "seek": 241596, "start": 2421.48, "end": 2427.88, "text": " But what happens if we just plug in this formula? Plugging this in and not even needing to train", "tokens": [50640, 583, 437, 2314, 498, 321, 445, 5452, 294, 341, 8513, 30, 40740, 3249, 341, 294, 293, 406, 754, 18006, 281, 3847, 50960], "temperature": 0.0, "avg_logprob": -0.09771183685020164, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0004441499477252364}, {"id": 514, "seek": 241596, "start": 2427.88, "end": 2433.96, "text": " the matrix gives a massive boost to the SSM and goes from much below the baselines to", "tokens": [50960, 264, 8141, 2709, 257, 5994, 9194, 281, 264, 12238, 44, 293, 1709, 490, 709, 2507, 264, 987, 9173, 281, 51264], "temperature": 0.0, "avg_logprob": -0.09771183685020164, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0004441499477252364}, {"id": 515, "seek": 241596, "start": 2433.96, "end": 2439.0, "text": " substantially above the baselines. And actually, I use the very small models for this ablation here,", "tokens": [51264, 30797, 3673, 264, 987, 9173, 13, 400, 767, 11, 286, 764, 264, 588, 1359, 5245, 337, 341, 410, 24278, 510, 11, 51516], "temperature": 0.0, "avg_logprob": -0.09771183685020164, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0004441499477252364}, {"id": 516, "seek": 241596, "start": 2439.0, "end": 2445.48, "text": " but the full model as far on this data set gets over 90%, which is something like 20 plus points", "tokens": [51516, 457, 264, 1577, 2316, 382, 1400, 322, 341, 1412, 992, 2170, 670, 4289, 8923, 597, 307, 746, 411, 945, 1804, 2793, 51840], "temperature": 0.0, "avg_logprob": -0.09771183685020164, "compression_ratio": 1.612099644128114, "no_speech_prob": 0.0004441499477252364}, {"id": 517, "seek": 244548, "start": 2445.48, "end": 2453.16, "text": " better than all other sequence models. So that kind of illustrates why HIPPO is so useful.", "tokens": [50364, 1101, 813, 439, 661, 8310, 5245, 13, 407, 300, 733, 295, 41718, 983, 389, 9139, 34885, 307, 370, 4420, 13, 50748], "temperature": 0.0, "avg_logprob": -0.2234160796455715, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00021649965492542833}, {"id": 518, "seek": 244548, "start": 2454.52, "end": 2462.2, "text": " Now, quick question in this example. So are both A and B basically just plug in matrices or", "tokens": [50816, 823, 11, 1702, 1168, 294, 341, 1365, 13, 407, 366, 1293, 316, 293, 363, 1936, 445, 5452, 294, 32284, 420, 51200], "temperature": 0.0, "avg_logprob": -0.2234160796455715, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00021649965492542833}, {"id": 519, "seek": 244548, "start": 2462.2, "end": 2468.84, "text": " is A alone basically a measure? A is the more important matrix, but actually, yeah, just plugging", "tokens": [51200, 307, 316, 3312, 1936, 257, 3481, 30, 316, 307, 264, 544, 1021, 8141, 11, 457, 767, 11, 1338, 11, 445, 42975, 51532], "temperature": 0.0, "avg_logprob": -0.2234160796455715, "compression_ratio": 1.435897435897436, "no_speech_prob": 0.00021649965492542833}, {"id": 520, "seek": 246884, "start": 2468.84, "end": 2477.1600000000003, "text": " in A and B essentially just just they're both fixed matrices, which are the HIPPO operators", "tokens": [50364, 294, 316, 293, 363, 4476, 445, 445, 436, 434, 1293, 6806, 32284, 11, 597, 366, 264, 389, 9139, 34885, 19077, 50780], "temperature": 0.0, "avg_logprob": -0.1913671584356399, "compression_ratio": 1.6139705882352942, "no_speech_prob": 0.001987293129786849}, {"id": 521, "seek": 246884, "start": 2477.1600000000003, "end": 2480.6800000000003, "text": " specifies both of these. I've only illustrated A because it's a more important one. But yeah,", "tokens": [50780, 1608, 11221, 1293, 295, 613, 13, 286, 600, 787, 33875, 316, 570, 309, 311, 257, 544, 1021, 472, 13, 583, 1338, 11, 50956], "temperature": 0.0, "avg_logprob": -0.1913671584356399, "compression_ratio": 1.6139705882352942, "no_speech_prob": 0.001987293129786849}, {"id": 522, "seek": 246884, "start": 2481.6400000000003, "end": 2485.4, "text": " this particular experiment froze both of these matrices to specific ones.", "tokens": [51004, 341, 1729, 5120, 46077, 1293, 295, 613, 32284, 281, 2685, 2306, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1913671584356399, "compression_ratio": 1.6139705882352942, "no_speech_prob": 0.001987293129786849}, {"id": 523, "seek": 246884, "start": 2487.56, "end": 2491.6400000000003, "text": " One question people have is that like, do we always freeze these? And actually,", "tokens": [51300, 1485, 1168, 561, 362, 307, 300, 411, 11, 360, 321, 1009, 15959, 613, 30, 400, 767, 11, 51504], "temperature": 0.0, "avg_logprob": -0.1913671584356399, "compression_ratio": 1.6139705882352942, "no_speech_prob": 0.001987293129786849}, {"id": 524, "seek": 246884, "start": 2491.6400000000003, "end": 2497.4, "text": " we can train them as well. This was to illustrate just like even freezing them, it does super well.", "tokens": [51504, 321, 393, 3847, 552, 382, 731, 13, 639, 390, 281, 23221, 445, 411, 754, 20200, 552, 11, 309, 775, 1687, 731, 13, 51792], "temperature": 0.0, "avg_logprob": -0.1913671584356399, "compression_ratio": 1.6139705882352942, "no_speech_prob": 0.001987293129786849}, {"id": 525, "seek": 249740, "start": 2498.36, "end": 2502.6800000000003, "text": " And then, but in practice, we do train them and it makes it do a little bit better.", "tokens": [50412, 400, 550, 11, 457, 294, 3124, 11, 321, 360, 3847, 552, 293, 309, 1669, 309, 360, 257, 707, 857, 1101, 13, 50628], "temperature": 0.0, "avg_logprob": -0.10292806959988778, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.00022688206809107214}, {"id": 526, "seek": 249740, "start": 2505.08, "end": 2510.36, "text": " Okay, so that was one thing. And that kind of points to I mentioned that SSMs have not been", "tokens": [50748, 1033, 11, 370, 300, 390, 472, 551, 13, 400, 300, 733, 295, 2793, 281, 286, 2835, 300, 12238, 26386, 362, 406, 668, 51012], "temperature": 0.0, "avg_logprob": -0.10292806959988778, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.00022688206809107214}, {"id": 527, "seek": 249740, "start": 2510.36, "end": 2514.92, "text": " used in deep learning before in this way. And that's kind of one problem. If you do it naively,", "tokens": [51012, 1143, 294, 2452, 2539, 949, 294, 341, 636, 13, 400, 300, 311, 733, 295, 472, 1154, 13, 759, 291, 360, 309, 1667, 3413, 11, 51240], "temperature": 0.0, "avg_logprob": -0.10292806959988778, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.00022688206809107214}, {"id": 528, "seek": 249740, "start": 2514.92, "end": 2520.2000000000003, "text": " it doesn't work. And so you need this new theory. The second reason is actually that they're", "tokens": [51240, 309, 1177, 380, 589, 13, 400, 370, 291, 643, 341, 777, 5261, 13, 440, 1150, 1778, 307, 767, 300, 436, 434, 51504], "temperature": 0.0, "avg_logprob": -0.10292806959988778, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.00022688206809107214}, {"id": 529, "seek": 249740, "start": 2520.2000000000003, "end": 2526.6, "text": " computationally pretty difficult to work with. And so here's the illustrate.", "tokens": [51504, 24903, 379, 1238, 2252, 281, 589, 365, 13, 400, 370, 510, 311, 264, 23221, 13, 51824], "temperature": 0.0, "avg_logprob": -0.10292806959988778, "compression_ratio": 1.6273062730627306, "no_speech_prob": 0.00022688206809107214}, {"id": 530, "seek": 252740, "start": 2528.36, "end": 2532.6800000000003, "text": " Again, so to remind you, we're thinking of an SSM as a parameterized map from an input signal to", "tokens": [50412, 3764, 11, 370, 281, 4160, 291, 11, 321, 434, 1953, 295, 364, 12238, 44, 382, 257, 13075, 1602, 4471, 490, 364, 4846, 6358, 281, 50628], "temperature": 0.0, "avg_logprob": -0.11804065369723137, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00023776653688400984}, {"id": 531, "seek": 252740, "start": 2532.6800000000003, "end": 2537.56, "text": " an output signal. And I'll suppose that our input had length L. So our input would just", "tokens": [50628, 364, 5598, 6358, 13, 400, 286, 603, 7297, 300, 527, 4846, 632, 4641, 441, 13, 407, 527, 4846, 576, 445, 50872], "temperature": 0.0, "avg_logprob": -0.11804065369723137, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00023776653688400984}, {"id": 532, "seek": 252740, "start": 2537.56, "end": 2543.48, "text": " give us a sequence of L numbers. Then the output of this whole thing is also a sequence of L numbers.", "tokens": [50872, 976, 505, 257, 8310, 295, 441, 3547, 13, 1396, 264, 5598, 295, 341, 1379, 551, 307, 611, 257, 8310, 295, 441, 3547, 13, 51168], "temperature": 0.0, "avg_logprob": -0.11804065369723137, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00023776653688400984}, {"id": 533, "seek": 252740, "start": 2543.48, "end": 2547.32, "text": " And computing this map ideally takes around O of L time or not too much more.", "tokens": [51168, 400, 15866, 341, 4471, 22915, 2516, 926, 422, 295, 441, 565, 420, 406, 886, 709, 544, 13, 51360], "temperature": 0.0, "avg_logprob": -0.11804065369723137, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00023776653688400984}, {"id": 534, "seek": 252740, "start": 2549.64, "end": 2554.04, "text": " But here's the problem. SSMs map the input to the output through this state.", "tokens": [51476, 583, 510, 311, 264, 1154, 13, 12238, 26386, 4471, 264, 4846, 281, 264, 5598, 807, 341, 1785, 13, 51696], "temperature": 0.0, "avg_logprob": -0.11804065369723137, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.00023776653688400984}, {"id": 535, "seek": 255404, "start": 2554.12, "end": 2559.48, "text": " And that state gave them a lot of nice properties, but it's also 100 dimensions higher. And so", "tokens": [50368, 400, 300, 1785, 2729, 552, 257, 688, 295, 1481, 7221, 11, 457, 309, 311, 611, 2319, 12819, 2946, 13, 400, 370, 50636], "temperature": 0.0, "avg_logprob": -0.091637858638057, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.0003919389273505658}, {"id": 536, "seek": 255404, "start": 2559.48, "end": 2564.52, "text": " computing the end to end mapping through the state will take 100 times more computation and memory", "tokens": [50636, 15866, 264, 917, 281, 917, 18350, 807, 264, 1785, 486, 747, 2319, 1413, 544, 24903, 293, 4675, 50888], "temperature": 0.0, "avg_logprob": -0.091637858638057, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.0003919389273505658}, {"id": 537, "seek": 255404, "start": 2564.52, "end": 2572.04, "text": " than what's needed to compute the final answer. And this is actually a real problem. And now,", "tokens": [50888, 813, 437, 311, 2978, 281, 14722, 264, 2572, 1867, 13, 400, 341, 307, 767, 257, 957, 1154, 13, 400, 586, 11, 51264], "temperature": 0.0, "avg_logprob": -0.091637858638057, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.0003919389273505658}, {"id": 538, "seek": 255404, "start": 2572.04, "end": 2575.16, "text": " earlier I said that you don't actually have to compute the state. You can compute it using a", "tokens": [51264, 3071, 286, 848, 300, 291, 500, 380, 767, 362, 281, 14722, 264, 1785, 13, 509, 393, 14722, 309, 1228, 257, 51420], "temperature": 0.0, "avg_logprob": -0.091637858638057, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.0003919389273505658}, {"id": 539, "seek": 255404, "start": 2575.16, "end": 2579.72, "text": " convolution instead. But what happens is that before computing the convolution, I have to compute", "tokens": [51420, 45216, 2602, 13, 583, 437, 2314, 307, 300, 949, 15866, 264, 45216, 11, 286, 362, 281, 14722, 51648], "temperature": 0.0, "avg_logprob": -0.091637858638057, "compression_ratio": 1.8037735849056604, "no_speech_prob": 0.0003919389273505658}, {"id": 540, "seek": 257972, "start": 2579.72, "end": 2584.8399999999997, "text": " the kernel or the convolution filter in green. And computing that is just as slow as computing the", "tokens": [50364, 264, 28256, 420, 264, 45216, 6608, 294, 3092, 13, 400, 15866, 300, 307, 445, 382, 2964, 382, 15866, 264, 50620], "temperature": 0.0, "avg_logprob": -0.07698327586764381, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0007791509851813316}, {"id": 541, "seek": 257972, "start": 2584.8399999999997, "end": 2591.3199999999997, "text": " state. And this sort of makes sense because it hasn't changed the computational hardness of the", "tokens": [50620, 1785, 13, 400, 341, 1333, 295, 1669, 2020, 570, 309, 6132, 380, 3105, 264, 28270, 44019, 295, 264, 50944], "temperature": 0.0, "avg_logprob": -0.07698327586764381, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0007791509851813316}, {"id": 542, "seek": 257972, "start": 2591.3199999999997, "end": 2597.3999999999996, "text": " problem. So essentially computing it no matter how you do it is going to be slow and memory inefficient.", "tokens": [50944, 1154, 13, 407, 4476, 15866, 309, 572, 1871, 577, 291, 360, 309, 307, 516, 281, 312, 2964, 293, 4675, 43495, 13, 51248], "temperature": 0.0, "avg_logprob": -0.07698327586764381, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0007791509851813316}, {"id": 543, "seek": 257972, "start": 2599.08, "end": 2604.6, "text": " So the main point of S4 was showing that you could substantially reduce this computation", "tokens": [51332, 407, 264, 2135, 935, 295, 318, 19, 390, 4099, 300, 291, 727, 30797, 5407, 341, 24903, 51608], "temperature": 0.0, "avg_logprob": -0.07698327586764381, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.0007791509851813316}, {"id": 544, "seek": 260460, "start": 2604.6, "end": 2608.2, "text": " when the SSM is structured. And for example, when using the", "tokens": [50364, 562, 264, 12238, 44, 307, 18519, 13, 400, 337, 1365, 11, 562, 1228, 264, 50544], "temperature": 0.0, "avg_logprob": -0.09790734961481377, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00046540723997168243}, {"id": 545, "seek": 260460, "start": 2609.24, "end": 2613.48, "text": " hippo matrix instead of an unstructured matrix, you can save this factor of 100", "tokens": [50596, 27745, 78, 8141, 2602, 295, 364, 18799, 46847, 8141, 11, 291, 393, 3155, 341, 5952, 295, 2319, 50808], "temperature": 0.0, "avg_logprob": -0.09790734961481377, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00046540723997168243}, {"id": 546, "seek": 260460, "start": 2614.04, "end": 2621.3199999999997, "text": " and make S4 overall extremely efficient. So this is done through a particular algorithm,", "tokens": [50836, 293, 652, 318, 19, 4787, 4664, 7148, 13, 407, 341, 307, 1096, 807, 257, 1729, 9284, 11, 51200], "temperature": 0.0, "avg_logprob": -0.09790734961481377, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00046540723997168243}, {"id": 547, "seek": 260460, "start": 2621.3199999999997, "end": 2626.44, "text": " which I'll just flash up. But basically we're trying to work with this SSM, but we only need", "tokens": [51200, 597, 286, 603, 445, 7319, 493, 13, 583, 1936, 321, 434, 1382, 281, 589, 365, 341, 12238, 44, 11, 457, 321, 787, 643, 51456], "temperature": 0.0, "avg_logprob": -0.09790734961481377, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00046540723997168243}, {"id": 548, "seek": 260460, "start": 2626.44, "end": 2631.7999999999997, "text": " to work with specific structured cases such as this, such as some particular hippo matrices.", "tokens": [51456, 281, 589, 365, 2685, 18519, 3331, 1270, 382, 341, 11, 1270, 382, 512, 1729, 27745, 78, 32284, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09790734961481377, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.00046540723997168243}, {"id": 549, "seek": 263180, "start": 2632.52, "end": 2638.76, "text": " And now using some algorithmic ideas, it turns out there is a way to compute the convolution", "tokens": [50400, 400, 586, 1228, 512, 9284, 299, 3487, 11, 309, 4523, 484, 456, 307, 257, 636, 281, 14722, 264, 45216, 50712], "temperature": 0.0, "avg_logprob": -0.07005768143728877, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.00030057510593906045}, {"id": 550, "seek": 263180, "start": 2638.76, "end": 2644.92, "text": " kernel, which was depicted in green before, very efficiently. And then compute the whole thing using", "tokens": [50712, 28256, 11, 597, 390, 30207, 294, 3092, 949, 11, 588, 19621, 13, 400, 550, 14722, 264, 1379, 551, 1228, 51020], "temperature": 0.0, "avg_logprob": -0.07005768143728877, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.00030057510593906045}, {"id": 551, "seek": 263180, "start": 2644.92, "end": 2651.5600000000004, "text": " a convolution. So I won't go into details here. And I will also mention that recently we've been", "tokens": [51020, 257, 45216, 13, 407, 286, 1582, 380, 352, 666, 4365, 510, 13, 400, 286, 486, 611, 2152, 300, 3938, 321, 600, 668, 51352], "temperature": 0.0, "avg_logprob": -0.07005768143728877, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.00030057510593906045}, {"id": 552, "seek": 263180, "start": 2651.5600000000004, "end": 2655.7200000000003, "text": " developing simplifications of the model that allow you to bypass all of this and do things much more", "tokens": [51352, 6416, 6883, 7833, 295, 264, 2316, 300, 2089, 291, 281, 24996, 439, 295, 341, 293, 360, 721, 709, 544, 51560], "temperature": 0.0, "avg_logprob": -0.07005768143728877, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.00030057510593906045}, {"id": 553, "seek": 265572, "start": 2655.72, "end": 2662.12, "text": " simply. So hopefully in a few weeks we'll have some stuff out that's where you don't need to", "tokens": [50364, 2935, 13, 407, 4696, 294, 257, 1326, 3259, 321, 603, 362, 512, 1507, 484, 300, 311, 689, 291, 500, 380, 643, 281, 50684], "temperature": 0.0, "avg_logprob": -0.12287281860004771, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0006162881618365645}, {"id": 554, "seek": 265572, "start": 2662.12, "end": 2669.3199999999997, "text": " worry about this really complicated algorithm. All right, so that was the technical portion of", "tokens": [50684, 3292, 466, 341, 534, 6179, 9284, 13, 1057, 558, 11, 370, 300, 390, 264, 6191, 8044, 295, 51044], "temperature": 0.0, "avg_logprob": -0.12287281860004771, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0006162881618365645}, {"id": 555, "seek": 265572, "start": 2669.8799999999997, "end": 2672.8399999999997, "text": " that I wanted to mention for S4. And I'll stop here for questions as well.", "tokens": [51072, 300, 286, 1415, 281, 2152, 337, 318, 19, 13, 400, 286, 603, 1590, 510, 337, 1651, 382, 731, 13, 51220], "temperature": 0.0, "avg_logprob": -0.12287281860004771, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0006162881618365645}, {"id": 556, "seek": 265572, "start": 2676.4399999999996, "end": 2684.8399999999997, "text": " So if in any case you want to actually get the state, can S4 actually recover the state?", "tokens": [51400, 407, 498, 294, 604, 1389, 291, 528, 281, 767, 483, 264, 1785, 11, 393, 318, 19, 767, 8114, 264, 1785, 30, 51820], "temperature": 0.0, "avg_logprob": -0.12287281860004771, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.0006162881618365645}, {"id": 557, "seek": 268572, "start": 2685.8799999999997, "end": 2692.68, "text": " Or is it like, yeah, so like, I don't know if that would be any use case.", "tokens": [50372, 1610, 307, 309, 411, 11, 1338, 11, 370, 411, 11, 286, 500, 380, 458, 498, 300, 576, 312, 604, 764, 1389, 13, 50712], "temperature": 0.0, "avg_logprob": -0.21837545696057772, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.0013869571266695857}, {"id": 558, "seek": 268572, "start": 2692.68, "end": 2696.6, "text": " But like I was saying, like, if there's a case where I actually want the state,", "tokens": [50712, 583, 411, 286, 390, 1566, 11, 411, 11, 498, 456, 311, 257, 1389, 689, 286, 767, 528, 264, 1785, 11, 50908], "temperature": 0.0, "avg_logprob": -0.21837545696057772, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.0013869571266695857}, {"id": 559, "seek": 268572, "start": 2696.6, "end": 2701.9599999999996, "text": " can I do that and get the convolution? Yes, you can. And in fact, that will be used in", "tokens": [50908, 393, 286, 360, 300, 293, 483, 264, 45216, 30, 1079, 11, 291, 393, 13, 400, 294, 1186, 11, 300, 486, 312, 1143, 294, 51176], "temperature": 0.0, "avg_logprob": -0.21837545696057772, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.0013869571266695857}, {"id": 560, "seek": 268572, "start": 2702.9199999999996, "end": 2707.72, "text": " some experiments. I guess I didn't mention explicitly, but you can compute it in either way,", "tokens": [51224, 512, 12050, 13, 286, 2041, 286, 994, 380, 2152, 20803, 11, 457, 291, 393, 14722, 309, 294, 2139, 636, 11, 51464], "temperature": 0.0, "avg_logprob": -0.21837545696057772, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.0013869571266695857}, {"id": 561, "seek": 268572, "start": 2707.72, "end": 2713.24, "text": " either through the convolution or through the state. And where the state or the convolution is", "tokens": [51464, 2139, 807, 264, 45216, 420, 807, 264, 1785, 13, 400, 689, 264, 1785, 420, 264, 45216, 307, 51740], "temperature": 0.0, "avg_logprob": -0.21837545696057772, "compression_ratio": 1.8059071729957805, "no_speech_prob": 0.0013869571266695857}, {"id": 562, "seek": 271324, "start": 2713.24, "end": 2718.9199999999996, "text": " useful is during training time for parallelizability. But where the state is useful is at some sort of", "tokens": [50364, 4420, 307, 1830, 3097, 565, 337, 8952, 590, 2310, 13, 583, 689, 264, 1785, 307, 4420, 307, 412, 512, 1333, 295, 50648], "temperature": 0.0, "avg_logprob": -0.12486571357363746, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0005882200202904642}, {"id": 563, "seek": 271324, "start": 2720.2, "end": 2725.0, "text": " inference or deployment settings, where perhaps you might be online, and then you would actually", "tokens": [50712, 38253, 420, 19317, 6257, 11, 689, 4317, 291, 1062, 312, 2950, 11, 293, 550, 291, 576, 767, 50952], "temperature": 0.0, "avg_logprob": -0.12486571357363746, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0005882200202904642}, {"id": 564, "seek": 271324, "start": 2725.0, "end": 2728.7599999999998, "text": " be going through the state instead of the convolution and unrolling things one step at a time.", "tokens": [50952, 312, 516, 807, 264, 1785, 2602, 295, 264, 45216, 293, 517, 18688, 721, 472, 1823, 412, 257, 565, 13, 51140], "temperature": 0.0, "avg_logprob": -0.12486571357363746, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0005882200202904642}, {"id": 565, "seek": 271324, "start": 2729.72, "end": 2735.4799999999996, "text": " Right. So you can do it either way, which is pretty cool. Thanks.", "tokens": [51188, 1779, 13, 407, 291, 393, 360, 309, 2139, 636, 11, 597, 307, 1238, 1627, 13, 2561, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12486571357363746, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0005882200202904642}, {"id": 566, "seek": 273548, "start": 2736.36, "end": 2743.64, "text": " I had more of a thought question. Let's say I'm interested in two different measures. Like,", "tokens": [50408, 286, 632, 544, 295, 257, 1194, 1168, 13, 961, 311, 584, 286, 478, 3102, 294, 732, 819, 8000, 13, 1743, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1673639541448549, "compression_ratio": 1.5158371040723981, "no_speech_prob": 0.00178144546225667}, {"id": 567, "seek": 273548, "start": 2743.64, "end": 2749.4, "text": " I want to see how the exponential average works, but I also want like, so is it,", "tokens": [50772, 286, 528, 281, 536, 577, 264, 21510, 4274, 1985, 11, 457, 286, 611, 528, 411, 11, 370, 307, 309, 11, 51060], "temperature": 0.0, "avg_logprob": -0.1673639541448549, "compression_ratio": 1.5158371040723981, "no_speech_prob": 0.00178144546225667}, {"id": 568, "seek": 273548, "start": 2750.2, "end": 2754.76, "text": " does it basically mean that I just have to create a new measure that combines this efficiently before", "tokens": [51100, 775, 309, 1936, 914, 300, 286, 445, 362, 281, 1884, 257, 777, 3481, 300, 29520, 341, 19621, 949, 51328], "temperature": 0.0, "avg_logprob": -0.1673639541448549, "compression_ratio": 1.5158371040723981, "no_speech_prob": 0.00178144546225667}, {"id": 569, "seek": 273548, "start": 2754.76, "end": 2761.96, "text": " this? Before I plug it into SSM or can S4 basically kind of,", "tokens": [51328, 341, 30, 4546, 286, 5452, 309, 666, 12238, 44, 420, 393, 318, 19, 1936, 733, 295, 11, 51688], "temperature": 0.0, "avg_logprob": -0.1673639541448549, "compression_ratio": 1.5158371040723981, "no_speech_prob": 0.00178144546225667}, {"id": 570, "seek": 276196, "start": 2762.92, "end": 2766.36, "text": " because there are two independent blocks that I can basically...", "tokens": [50412, 570, 456, 366, 732, 6695, 8474, 300, 286, 393, 1936, 485, 50584], "temperature": 0.0, "avg_logprob": -0.1662673452626104, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.00042383975232951343}, {"id": 571, "seek": 276196, "start": 2768.12, "end": 2771.32, "text": " Yeah. So I'm just about to get to the experiments. And actually, I will,", "tokens": [50672, 865, 13, 407, 286, 478, 445, 466, 281, 483, 281, 264, 12050, 13, 400, 767, 11, 286, 486, 11, 50832], "temperature": 0.0, "avg_logprob": -0.1662673452626104, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.00042383975232951343}, {"id": 572, "seek": 276196, "start": 2772.44, "end": 2777.4, "text": " I'll get to that slide right now, where, so first of all, the experiments will be on this type of", "tokens": [50888, 286, 603, 483, 281, 300, 4137, 558, 586, 11, 689, 11, 370, 700, 295, 439, 11, 264, 12050, 486, 312, 322, 341, 2010, 295, 51136], "temperature": 0.0, "avg_logprob": -0.1662673452626104, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.00042383975232951343}, {"id": 573, "seek": 276196, "start": 2777.4, "end": 2784.52, "text": " signal data. And what, as I mentioned a couple of times, what we actually do is that I have to", "tokens": [51136, 6358, 1412, 13, 400, 437, 11, 382, 286, 2835, 257, 1916, 295, 1413, 11, 437, 321, 767, 360, 307, 300, 286, 362, 281, 51492], "temperature": 0.0, "avg_logprob": -0.1662673452626104, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.00042383975232951343}, {"id": 574, "seek": 276196, "start": 2784.52, "end": 2790.12, "text": " find this 1D to 1D map, but I'm actually going to just like, given a multidimensional input,", "tokens": [51492, 915, 341, 502, 35, 281, 502, 35, 4471, 11, 457, 286, 478, 767, 516, 281, 445, 411, 11, 2212, 257, 2120, 327, 332, 11075, 4846, 11, 51772], "temperature": 0.0, "avg_logprob": -0.1662673452626104, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.00042383975232951343}, {"id": 575, "seek": 279012, "start": 2790.12, "end": 2795.3199999999997, "text": " I'm just going to stack a bunch of copies of this. And now as a parallel to that, you can do", "tokens": [50364, 286, 478, 445, 516, 281, 8630, 257, 3840, 295, 14341, 295, 341, 13, 400, 586, 382, 257, 8952, 281, 300, 11, 291, 393, 360, 50624], "temperature": 0.0, "avg_logprob": -0.0987705159410138, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0006562406197190285}, {"id": 576, "seek": 279012, "start": 2795.3199999999997, "end": 2799.64, "text": " many things with these copies. So to answer your question, one thing that I've been starting to", "tokens": [50624, 867, 721, 365, 613, 14341, 13, 407, 281, 1867, 428, 1168, 11, 472, 551, 300, 286, 600, 668, 2891, 281, 50840], "temperature": 0.0, "avg_logprob": -0.0987705159410138, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0006562406197190285}, {"id": 577, "seek": 279012, "start": 2799.64, "end": 2804.52, "text": " experiment with is just using different measures or essentially different A and B matrices for", "tokens": [50840, 5120, 365, 307, 445, 1228, 819, 8000, 420, 4476, 819, 316, 293, 363, 32284, 337, 51084], "temperature": 0.0, "avg_logprob": -0.0987705159410138, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0006562406197190285}, {"id": 578, "seek": 279012, "start": 2804.52, "end": 2809.88, "text": " every copy. And that can, and so that sort of has interpretation of using multiple measures.", "tokens": [51084, 633, 5055, 13, 400, 300, 393, 11, 293, 370, 300, 1333, 295, 575, 14174, 295, 1228, 3866, 8000, 13, 51352], "temperature": 0.0, "avg_logprob": -0.0987705159410138, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0006562406197190285}, {"id": 579, "seek": 279012, "start": 2811.0, "end": 2816.44, "text": " I see. Because when Iman actually talked about the correlations between different dimensions,", "tokens": [51408, 286, 536, 13, 1436, 562, 286, 1601, 767, 2825, 466, 264, 13983, 763, 1296, 819, 12819, 11, 51680], "temperature": 0.0, "avg_logprob": -0.0987705159410138, "compression_ratio": 1.7028985507246377, "no_speech_prob": 0.0006562406197190285}, {"id": 580, "seek": 281644, "start": 2817.08, "end": 2820.92, "text": " let's say you have an image, like two different pixels are actually correlated.", "tokens": [50396, 718, 311, 584, 291, 362, 364, 3256, 11, 411, 732, 819, 18668, 366, 767, 38574, 13, 50588], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 581, "seek": 281644, "start": 2820.92, "end": 2824.6, "text": " So I was thinking like, you can have a measure that captures this correlation,", "tokens": [50588, 407, 286, 390, 1953, 411, 11, 291, 393, 362, 257, 3481, 300, 27986, 341, 20009, 11, 50772], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 582, "seek": 281644, "start": 2824.6, "end": 2828.36, "text": " but you can have another measure that captures it over time.", "tokens": [50772, 457, 291, 393, 362, 1071, 3481, 300, 27986, 309, 670, 565, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 583, "seek": 281644, "start": 2829.96, "end": 2834.36, "text": " Another thing actually, since you mentioned that, I don't know if you tried that on image", "tokens": [51040, 3996, 551, 767, 11, 1670, 291, 2835, 300, 11, 286, 500, 380, 458, 498, 291, 3031, 300, 322, 3256, 51260], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 584, "seek": 281644, "start": 2834.36, "end": 2839.2400000000002, "text": " space, I would be curious like if this kind of like long convolution actually makes any difference", "tokens": [51260, 1901, 11, 286, 576, 312, 6369, 411, 498, 341, 733, 295, 411, 938, 45216, 767, 1669, 604, 2649, 51504], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 585, "seek": 281644, "start": 2839.2400000000002, "end": 2845.16, "text": " with the image space. Because image usually like, when we do the image analysis theoretically,", "tokens": [51504, 365, 264, 3256, 1901, 13, 1436, 3256, 2673, 411, 11, 562, 321, 360, 264, 3256, 5215, 29400, 11, 51800], "temperature": 0.0, "avg_logprob": -0.1601573640266351, "compression_ratio": 1.8698884758364311, "no_speech_prob": 0.0008290052064694464}, {"id": 586, "seek": 284516, "start": 2845.16, "end": 2849.64, "text": " when we start thinking about it, it seems that like also the local feature as well as of course", "tokens": [50364, 562, 321, 722, 1953, 466, 309, 11, 309, 2544, 300, 411, 611, 264, 2654, 4111, 382, 731, 382, 295, 1164, 50588], "temperature": 0.0, "avg_logprob": -0.1499671423307029, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.00043712148908525705}, {"id": 587, "seek": 284516, "start": 2849.64, "end": 2855.3199999999997, "text": " the global feature is important. But I don't know, like if we are missing any local features by just", "tokens": [50588, 264, 4338, 4111, 307, 1021, 13, 583, 286, 500, 380, 458, 11, 411, 498, 321, 366, 5361, 604, 2654, 4122, 538, 445, 50872], "temperature": 0.0, "avg_logprob": -0.1499671423307029, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.00043712148908525705}, {"id": 588, "seek": 284516, "start": 2855.3199999999997, "end": 2863.16, "text": " using this kind of like long representation. That's a good question. I actually, we have started", "tokens": [50872, 1228, 341, 733, 295, 411, 938, 10290, 13, 663, 311, 257, 665, 1168, 13, 286, 767, 11, 321, 362, 1409, 51264], "temperature": 0.0, "avg_logprob": -0.1499671423307029, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.00043712148908525705}, {"id": 589, "seek": 284516, "start": 2863.16, "end": 2867.8799999999997, "text": " doing more experiments on images, which I didn't include in this talk, but luckily we do find that", "tokens": [51264, 884, 544, 12050, 322, 5267, 11, 597, 286, 994, 380, 4090, 294, 341, 751, 11, 457, 22880, 321, 360, 915, 300, 51500], "temperature": 0.0, "avg_logprob": -0.1499671423307029, "compression_ratio": 1.6131687242798354, "no_speech_prob": 0.00043712148908525705}, {"id": 590, "seek": 286788, "start": 2868.52, "end": 2875.08, "text": " the local bias of convolutions does seem pretty good. I don't know, it's hard to quantify if", "tokens": [50396, 264, 2654, 12577, 295, 3754, 15892, 775, 1643, 1238, 665, 13, 286, 500, 380, 458, 11, 309, 311, 1152, 281, 40421, 498, 50724], "temperature": 0.0, "avg_logprob": -0.10892677307128906, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.010647998191416264}, {"id": 591, "seek": 286788, "start": 2875.08, "end": 2880.12, "text": " we're missing features, but I think there are settings where we're not, we're only on power", "tokens": [50724, 321, 434, 5361, 4122, 11, 457, 286, 519, 456, 366, 6257, 689, 321, 434, 406, 11, 321, 434, 787, 322, 1347, 50976], "temperature": 0.0, "avg_logprob": -0.10892677307128906, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.010647998191416264}, {"id": 592, "seek": 286788, "start": 2880.12, "end": 2887.2400000000002, "text": " or not, or maybe a little bit worse than a standard local CNN. It is hard to say. I will", "tokens": [50976, 420, 406, 11, 420, 1310, 257, 707, 857, 5324, 813, 257, 3832, 2654, 24859, 13, 467, 307, 1152, 281, 584, 13, 286, 486, 51332], "temperature": 0.0, "avg_logprob": -0.10892677307128906, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.010647998191416264}, {"id": 593, "seek": 286788, "start": 2887.2400000000002, "end": 2894.6, "text": " mention though that you can forcibly incorporate locality into this just by changing the measure.", "tokens": [51332, 2152, 1673, 300, 291, 393, 337, 537, 25021, 16091, 1628, 1860, 666, 341, 445, 538, 4473, 264, 3481, 13, 51700], "temperature": 0.0, "avg_logprob": -0.10892677307128906, "compression_ratio": 1.5720338983050848, "no_speech_prob": 0.010647998191416264}, {"id": 594, "seek": 289460, "start": 2894.6, "end": 2898.92, "text": " For example, if you choose a uniform measure that has a short window, that's the same as saying", "tokens": [50364, 1171, 1365, 11, 498, 291, 2826, 257, 9452, 3481, 300, 575, 257, 2099, 4910, 11, 300, 311, 264, 912, 382, 1566, 50580], "temperature": 0.0, "avg_logprob": -0.1386529575694691, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.0009848158806562424}, {"id": 595, "seek": 289460, "start": 2898.92, "end": 2904.2799999999997, "text": " I would just want a local convolution kernel. Because I would imagine like for this particular", "tokens": [50580, 286, 576, 445, 528, 257, 2654, 45216, 28256, 13, 1436, 286, 576, 3811, 411, 337, 341, 1729, 50848], "temperature": 0.0, "avg_logprob": -0.1386529575694691, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.0009848158806562424}, {"id": 596, "seek": 289460, "start": 2905.24, "end": 2910.12, "text": " thing, like the use case where we have to have to work with a very high resolution image data,", "tokens": [50896, 551, 11, 411, 264, 764, 1389, 689, 321, 362, 281, 362, 281, 589, 365, 257, 588, 1090, 8669, 3256, 1412, 11, 51140], "temperature": 0.0, "avg_logprob": -0.1386529575694691, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.0009848158806562424}, {"id": 597, "seek": 289460, "start": 2911.0, "end": 2914.92, "text": " you know, for example, like imagine like mammogram, right? Like we have to go with like", "tokens": [51184, 291, 458, 11, 337, 1365, 11, 411, 3811, 411, 19033, 12820, 11, 558, 30, 1743, 321, 362, 281, 352, 365, 411, 51380], "temperature": 0.0, "avg_logprob": -0.1386529575694691, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.0009848158806562424}, {"id": 598, "seek": 289460, "start": 2914.92, "end": 2921.16, "text": " 1000 by 1000 minimum dimension. So for this probably would be useful because they are actually,", "tokens": [51380, 9714, 538, 9714, 7285, 10139, 13, 407, 337, 341, 1391, 576, 312, 4420, 570, 436, 366, 767, 11, 51692], "temperature": 0.0, "avg_logprob": -0.1386529575694691, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.0009848158806562424}, {"id": 599, "seek": 292116, "start": 2921.16, "end": 2925.96, "text": " we want to like do the rescaling, but we cannot because we'll lose probably a lot of features", "tokens": [50364, 321, 528, 281, 411, 360, 264, 9610, 4270, 11, 457, 321, 2644, 570, 321, 603, 3624, 1391, 257, 688, 295, 4122, 50604], "temperature": 0.0, "avg_logprob": -0.15706496403135103, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.001366646378301084}, {"id": 600, "seek": 292116, "start": 2925.96, "end": 2929.8799999999997, "text": " in the middle. But this kind of like long convolution could this, this is a perfect", "tokens": [50604, 294, 264, 2808, 13, 583, 341, 733, 295, 411, 938, 45216, 727, 341, 11, 341, 307, 257, 2176, 50800], "temperature": 0.0, "avg_logprob": -0.15706496403135103, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.001366646378301084}, {"id": 601, "seek": 292116, "start": 2929.8799999999997, "end": 2934.92, "text": " problem that I will actually, I wasn't going to, but now I'll mention this in the experiments as", "tokens": [50800, 1154, 300, 286, 486, 767, 11, 286, 2067, 380, 516, 281, 11, 457, 586, 286, 603, 2152, 341, 294, 264, 12050, 382, 51052], "temperature": 0.0, "avg_logprob": -0.15706496403135103, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.001366646378301084}, {"id": 602, "seek": 292116, "start": 2934.92, "end": 2941.48, "text": " well. Okay. It's actually something that we have thought about basically rescaling of convolutions", "tokens": [51052, 731, 13, 1033, 13, 467, 311, 767, 746, 300, 321, 362, 1194, 466, 1936, 9610, 4270, 295, 3754, 15892, 51380], "temperature": 0.0, "avg_logprob": -0.15706496403135103, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.001366646378301084}, {"id": 603, "seek": 292116, "start": 2941.48, "end": 2948.2, "text": " and using. Right. Okay, I'll get to that. Before that, so I want to get the experiments and", "tokens": [51380, 293, 1228, 13, 1779, 13, 1033, 11, 286, 603, 483, 281, 300, 13, 4546, 300, 11, 370, 286, 528, 281, 483, 264, 12050, 293, 51716], "temperature": 0.0, "avg_logprob": -0.15706496403135103, "compression_ratio": 1.7350746268656716, "no_speech_prob": 0.001366646378301084}, {"id": 604, "seek": 294820, "start": 2948.2, "end": 2952.04, "text": " basically I just wanted to find, I'm only to find the simple linear one either one D map,", "tokens": [50364, 1936, 286, 445, 1415, 281, 915, 11, 286, 478, 787, 281, 915, 264, 2199, 8213, 472, 2139, 472, 413, 4471, 11, 50556], "temperature": 0.0, "avg_logprob": -0.14391289154688516, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004801963223144412}, {"id": 605, "seek": 294820, "start": 2952.04, "end": 2956.3599999999997, "text": " but you can just do it in parallel across a lot of features and then plug it into a standard neural", "tokens": [50556, 457, 291, 393, 445, 360, 309, 294, 8952, 2108, 257, 688, 295, 4122, 293, 550, 5452, 309, 666, 257, 3832, 18161, 50772], "temperature": 0.0, "avg_logprob": -0.14391289154688516, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004801963223144412}, {"id": 606, "seek": 294820, "start": 2956.3599999999997, "end": 2963.48, "text": " network to do sequence modeling. So the first type of data I'll see is a biosignal data.", "tokens": [50772, 3209, 281, 360, 8310, 15983, 13, 407, 264, 700, 2010, 295, 1412, 286, 603, 536, 307, 257, 36997, 788, 304, 1412, 13, 51128], "temperature": 0.0, "avg_logprob": -0.14391289154688516, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004801963223144412}, {"id": 607, "seek": 294820, "start": 2966.8399999999997, "end": 2974.7599999999998, "text": " So here is a, there's a real world data set of trying to predict vital signs such as heart rate", "tokens": [51296, 407, 510, 307, 257, 11, 456, 311, 257, 957, 1002, 1412, 992, 295, 1382, 281, 6069, 11707, 7880, 1270, 382, 1917, 3314, 51692], "temperature": 0.0, "avg_logprob": -0.14391289154688516, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0004801963223144412}, {"id": 608, "seek": 297476, "start": 2974.76, "end": 2982.44, "text": " from raw biosignal data such as I wrote EKG and EG here, but I think it's actually EKG and PPG.", "tokens": [50364, 490, 8936, 36997, 788, 304, 1412, 1270, 382, 286, 4114, 46078, 38, 293, 462, 38, 510, 11, 457, 286, 519, 309, 311, 767, 46078, 38, 293, 37369, 38, 13, 50748], "temperature": 0.0, "avg_logprob": -0.11049709781523674, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0001971247693290934}, {"id": 609, "seek": 297476, "start": 2983.0, "end": 2986.92, "text": " And so that's visualized here. And this data is pretty challenging for deep learning models", "tokens": [50776, 400, 370, 300, 311, 5056, 1602, 510, 13, 400, 341, 1412, 307, 1238, 7595, 337, 2452, 2539, 5245, 50972], "temperature": 0.0, "avg_logprob": -0.11049709781523674, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0001971247693290934}, {"id": 610, "seek": 297476, "start": 2986.92, "end": 2992.84, "text": " because you can see that it's very long. This is a sequence of like 4000. If you zoom in a lot,", "tokens": [50972, 570, 291, 393, 536, 300, 309, 311, 588, 938, 13, 639, 307, 257, 8310, 295, 411, 31104, 13, 759, 291, 8863, 294, 257, 688, 11, 51268], "temperature": 0.0, "avg_logprob": -0.11049709781523674, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0001971247693290934}, {"id": 611, "seek": 297476, "start": 2992.84, "end": 2995.88, "text": " it would be pretty smooth actually, but if you zoom out, it displays a lot of", "tokens": [51268, 309, 576, 312, 1238, 5508, 767, 11, 457, 498, 291, 8863, 484, 11, 309, 20119, 257, 688, 295, 51420], "temperature": 0.0, "avg_logprob": -0.11049709781523674, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0001971247693290934}, {"id": 612, "seek": 297476, "start": 2995.88, "end": 3001.5600000000004, "text": " periodicity and spikes and other things. And so a lot of methods have been tried on this data set,", "tokens": [51420, 27790, 507, 293, 28997, 293, 661, 721, 13, 400, 370, 257, 688, 295, 7150, 362, 668, 3031, 322, 341, 1412, 992, 11, 51704], "temperature": 0.0, "avg_logprob": -0.11049709781523674, "compression_ratio": 1.6487455197132617, "no_speech_prob": 0.0001971247693290934}, {"id": 613, "seek": 300156, "start": 3001.64, "end": 3007.32, "text": " which include kind of standard machine learning techniques like XGBoost as well as many very", "tokens": [50368, 597, 4090, 733, 295, 3832, 3479, 2539, 7512, 411, 1783, 8769, 78, 555, 382, 731, 382, 867, 588, 50652], "temperature": 0.0, "avg_logprob": -0.1411024958817, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00020660499285440892}, {"id": 614, "seek": 300156, "start": 3007.32, "end": 3013.32, "text": " modern deep learning sequence models. And S4 substantially improves over all of these in", "tokens": [50652, 4363, 2452, 2539, 8310, 5245, 13, 400, 318, 19, 30797, 24771, 670, 439, 295, 613, 294, 50952], "temperature": 0.0, "avg_logprob": -0.1411024958817, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00020660499285440892}, {"id": 615, "seek": 300156, "start": 3014.6, "end": 3018.36, "text": " I think cutting the root mean squared error by at least two thirds on all of these targets", "tokens": [51016, 286, 519, 6492, 264, 5593, 914, 8889, 6713, 538, 412, 1935, 732, 34552, 322, 439, 295, 613, 12911, 51204], "temperature": 0.0, "avg_logprob": -0.1411024958817, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00020660499285440892}, {"id": 616, "seek": 300156, "start": 3020.2799999999997, "end": 3023.24, "text": " just with that generic deep learning model that deep model that I showed.", "tokens": [51300, 445, 365, 300, 19577, 2452, 2539, 2316, 300, 2452, 2316, 300, 286, 4712, 13, 51448], "temperature": 0.0, "avg_logprob": -0.1411024958817, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00020660499285440892}, {"id": 617, "seek": 300156, "start": 3024.68, "end": 3029.24, "text": " Actually I've, these were like older numbers and recently I've been rerunning these again and", "tokens": [51520, 5135, 286, 600, 11, 613, 645, 411, 4906, 3547, 293, 3938, 286, 600, 668, 43819, 25589, 613, 797, 293, 51748], "temperature": 0.0, "avg_logprob": -0.1411024958817, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.00020660499285440892}, {"id": 618, "seek": 302924, "start": 3029.24, "end": 3037.4799999999996, "text": " actually you can drop this down even more. One thing I will note is that attention and", "tokens": [50364, 767, 291, 393, 3270, 341, 760, 754, 544, 13, 1485, 551, 286, 486, 3637, 307, 300, 3202, 293, 50776], "temperature": 0.0, "avg_logprob": -0.07007368935479058, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0001397980231558904}, {"id": 619, "seek": 302924, "start": 3037.4799999999996, "end": 3043.08, "text": " transformers does really poorly on this type of data. And that's something that I think I found", "tokens": [50776, 4088, 433, 775, 534, 22271, 322, 341, 2010, 295, 1412, 13, 400, 300, 311, 746, 300, 286, 519, 286, 1352, 51056], "temperature": 0.0, "avg_logprob": -0.07007368935479058, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0001397980231558904}, {"id": 620, "seek": 302924, "start": 3043.08, "end": 3048.6, "text": " pretty consistently. So there's some sort of bias toward what type of data you have and S4 is really", "tokens": [51056, 1238, 14961, 13, 407, 456, 311, 512, 1333, 295, 12577, 7361, 437, 2010, 295, 1412, 291, 362, 293, 318, 19, 307, 534, 51332], "temperature": 0.0, "avg_logprob": -0.07007368935479058, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0001397980231558904}, {"id": 621, "seek": 302924, "start": 3048.6, "end": 3053.24, "text": " good at signals and attention is not. Conversely, attention is good at some other types of discrete", "tokens": [51332, 665, 412, 12354, 293, 3202, 307, 406, 13, 33247, 736, 11, 3202, 307, 665, 412, 512, 661, 3467, 295, 27706, 51564], "temperature": 0.0, "avg_logprob": -0.07007368935479058, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0001397980231558904}, {"id": 622, "seek": 305324, "start": 3053.24, "end": 3060.68, "text": " data that S4 is not as good at. Okay, so that's, that was one experiment. The next one is two time", "tokens": [50364, 1412, 300, 318, 19, 307, 406, 382, 665, 412, 13, 1033, 11, 370, 300, 311, 11, 300, 390, 472, 5120, 13, 440, 958, 472, 307, 732, 565, 50736], "temperature": 0.0, "avg_logprob": -0.12011995071019882, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.001206377288326621}, {"id": 623, "seek": 305324, "start": 3060.68, "end": 3067.8799999999997, "text": " series data where we did a forecasting task where you're given a context window and you want to", "tokens": [50736, 2638, 1412, 689, 321, 630, 257, 44331, 5633, 689, 291, 434, 2212, 257, 4319, 4910, 293, 291, 528, 281, 51096], "temperature": 0.0, "avg_logprob": -0.12011995071019882, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.001206377288326621}, {"id": 624, "seek": 305324, "start": 3067.8799999999997, "end": 3072.12, "text": " predict future values. Actually, I'm going to go through this kind of fast because I don't want", "tokens": [51096, 6069, 2027, 4190, 13, 5135, 11, 286, 478, 516, 281, 352, 807, 341, 733, 295, 2370, 570, 286, 500, 380, 528, 51308], "temperature": 0.0, "avg_logprob": -0.12011995071019882, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.001206377288326621}, {"id": 625, "seek": 305324, "start": 3072.12, "end": 3075.8799999999997, "text": " that much time I want to get through some more of the bio applications and the things that you", "tokens": [51308, 300, 709, 565, 286, 528, 281, 483, 807, 512, 544, 295, 264, 12198, 5821, 293, 264, 721, 300, 291, 51496], "temperature": 0.0, "avg_logprob": -0.12011995071019882, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.001206377288326621}, {"id": 626, "seek": 305324, "start": 3075.8799999999997, "end": 3082.3599999999997, "text": " guys brought up. The models here are very complicated. Whereas for S4, we're actually", "tokens": [51496, 1074, 3038, 493, 13, 440, 5245, 510, 366, 588, 6179, 13, 13813, 337, 318, 19, 11, 321, 434, 767, 51820], "temperature": 0.0, "avg_logprob": -0.12011995071019882, "compression_ratio": 1.658450704225352, "no_speech_prob": 0.001206377288326621}, {"id": 627, "seek": 308236, "start": 3082.36, "end": 3087.48, "text": " doing an extremely simple setup, which is just a mask prediction. We're just going to give you,", "tokens": [50364, 884, 364, 4664, 2199, 8657, 11, 597, 307, 445, 257, 6094, 17630, 13, 492, 434, 445, 516, 281, 976, 291, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09371048828651166, "compression_ratio": 1.675, "no_speech_prob": 0.0007318698917515576}, {"id": 628, "seek": 308236, "start": 3087.48, "end": 3091.6400000000003, "text": " we're going to take the entire sequence and mask out the desired forecast range and then just", "tokens": [50620, 321, 434, 516, 281, 747, 264, 2302, 8310, 293, 6094, 484, 264, 14721, 14330, 3613, 293, 550, 445, 50828], "temperature": 0.0, "avg_logprob": -0.09371048828651166, "compression_ratio": 1.675, "no_speech_prob": 0.0007318698917515576}, {"id": 629, "seek": 308236, "start": 3093.0, "end": 3099.08, "text": " predict what's in the mask by passing it through this generic deep model. So this is really,", "tokens": [50896, 6069, 437, 311, 294, 264, 6094, 538, 8437, 309, 807, 341, 19577, 2452, 2316, 13, 407, 341, 307, 534, 11, 51200], "temperature": 0.0, "avg_logprob": -0.09371048828651166, "compression_ratio": 1.675, "no_speech_prob": 0.0007318698917515576}, {"id": 630, "seek": 308236, "start": 3099.08, "end": 3104.52, "text": " it's like a very extremely simple application. I won't unpack the numbers too much, but there's a", "tokens": [51200, 309, 311, 411, 257, 588, 4664, 2199, 3861, 13, 286, 1582, 380, 26699, 264, 3547, 886, 709, 11, 457, 456, 311, 257, 51472], "temperature": 0.0, "avg_logprob": -0.09371048828651166, "compression_ratio": 1.675, "no_speech_prob": 0.0007318698917515576}, {"id": 631, "seek": 308236, "start": 3104.52, "end": 3110.92, "text": " lot of baselines here, including time series models, LSTMs, lots of transformers, and S4", "tokens": [51472, 688, 295, 987, 9173, 510, 11, 3009, 565, 2638, 5245, 11, 441, 6840, 26386, 11, 3195, 295, 4088, 433, 11, 293, 318, 19, 51792], "temperature": 0.0, "avg_logprob": -0.09371048828651166, "compression_ratio": 1.675, "no_speech_prob": 0.0007318698917515576}, {"id": 632, "seek": 311092, "start": 3110.92, "end": 3114.84, "text": " does better than all of them on these real time series data sets, including weather and energy", "tokens": [50364, 775, 1101, 813, 439, 295, 552, 322, 613, 957, 565, 2638, 1412, 6352, 11, 3009, 5503, 293, 2281, 50560], "temperature": 0.0, "avg_logprob": -0.09616635859697714, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00036251090932637453}, {"id": 633, "seek": 311092, "start": 3114.84, "end": 3122.84, "text": " data with much less specialization. These models were all designed for time series and we were", "tokens": [50560, 1412, 365, 709, 1570, 2121, 2144, 13, 1981, 5245, 645, 439, 4761, 337, 565, 2638, 293, 321, 645, 50960], "temperature": 0.0, "avg_logprob": -0.09616635859697714, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00036251090932637453}, {"id": 634, "seek": 311092, "start": 3122.84, "end": 3128.28, "text": " just using our generic model. And you didn't even like tune the window size, right?", "tokens": [50960, 445, 1228, 527, 19577, 2316, 13, 400, 291, 994, 380, 754, 411, 10864, 264, 4910, 2744, 11, 558, 30, 51232], "temperature": 0.0, "avg_logprob": -0.09616635859697714, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00036251090932637453}, {"id": 635, "seek": 311092, "start": 3129.64, "end": 3134.28, "text": " We did not for this one. Actually, by tuning the window size, you can get the numbers down even", "tokens": [51300, 492, 630, 406, 337, 341, 472, 13, 5135, 11, 538, 15164, 264, 4910, 2744, 11, 291, 393, 483, 264, 3547, 760, 754, 51532], "temperature": 0.0, "avg_logprob": -0.09616635859697714, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00036251090932637453}, {"id": 636, "seek": 313428, "start": 3134.28, "end": 3144.76, "text": " more. Okay. Okay. The next one here points to Amon's question about rescaling. So it's actually,", "tokens": [50364, 544, 13, 1033, 13, 1033, 13, 440, 958, 472, 510, 2793, 281, 2012, 266, 311, 1168, 466, 9610, 4270, 13, 407, 309, 311, 767, 11, 50888], "temperature": 0.0, "avg_logprob": -0.19192485111515697, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.005299283191561699}, {"id": 637, "seek": 313428, "start": 3144.76, "end": 3151.8, "text": " I'm going to display this to audio. But essentially, I've used audio a few times. I'm running example.", "tokens": [50888, 286, 478, 516, 281, 4674, 341, 281, 6278, 13, 583, 4476, 11, 286, 600, 1143, 6278, 257, 1326, 1413, 13, 286, 478, 2614, 1365, 13, 51240], "temperature": 0.0, "avg_logprob": -0.19192485111515697, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.005299283191561699}, {"id": 638, "seek": 313428, "start": 3152.52, "end": 3158.84, "text": " It's sampled at extremely high rate. And it's extremely long. So this is a data set of classifying", "tokens": [51276, 467, 311, 3247, 15551, 412, 4664, 1090, 3314, 13, 400, 309, 311, 4664, 938, 13, 407, 341, 307, 257, 1412, 992, 295, 1508, 5489, 51592], "temperature": 0.0, "avg_logprob": -0.19192485111515697, "compression_ratio": 1.4974874371859297, "no_speech_prob": 0.005299283191561699}, {"id": 639, "seek": 315884, "start": 3158.84, "end": 3165.0, "text": " one second speech clips, which were length 16,000, into classifying the words. And most", "tokens": [50364, 472, 1150, 6218, 13117, 11, 597, 645, 4641, 3165, 11, 1360, 11, 666, 1508, 5489, 264, 2283, 13, 400, 881, 50672], "temperature": 0.0, "avg_logprob": -0.10860953952955163, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0018093488179147243}, {"id": 640, "seek": 315884, "start": 3165.0, "end": 3170.76, "text": " sequence models like transformers and RNNs are really bad here. The only thing that works is CNNs,", "tokens": [50672, 8310, 5245, 411, 4088, 433, 293, 45702, 45, 82, 366, 534, 1578, 510, 13, 440, 787, 551, 300, 1985, 307, 24859, 82, 11, 50960], "temperature": 0.0, "avg_logprob": -0.10860953952955163, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0018093488179147243}, {"id": 641, "seek": 315884, "start": 3171.32, "end": 3178.76, "text": " which the red line is pointing to a speech CNN baseline. And these work okay. But what happens", "tokens": [50988, 597, 264, 2182, 1622, 307, 12166, 281, 257, 6218, 24859, 20518, 13, 400, 613, 589, 1392, 13, 583, 437, 2314, 51360], "temperature": 0.0, "avg_logprob": -0.10860953952955163, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0018093488179147243}, {"id": 642, "seek": 315884, "start": 3178.76, "end": 3183.48, "text": " if you are resampling the signal at different frequencies? And this happens commonly in audio", "tokens": [51360, 498, 291, 366, 725, 335, 11970, 264, 6358, 412, 819, 20250, 30, 400, 341, 2314, 12719, 294, 6278, 51596], "temperature": 0.0, "avg_logprob": -0.10860953952955163, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0018093488179147243}, {"id": 643, "seek": 315884, "start": 3183.48, "end": 3188.76, "text": " because your signal can be sampled at any rate and sound more or less the same. So for example,", "tokens": [51596, 570, 428, 6358, 393, 312, 3247, 15551, 412, 604, 3314, 293, 1626, 544, 420, 1570, 264, 912, 13, 407, 337, 1365, 11, 51860], "temperature": 0.0, "avg_logprob": -0.10860953952955163, "compression_ratio": 1.6643109540636043, "no_speech_prob": 0.0018093488179147243}, {"id": 644, "seek": 318876, "start": 3188.84, "end": 3194.1200000000003, "text": " this orange sequence is a sequence of samples, but it's actually the same underlying signal", "tokens": [50368, 341, 7671, 8310, 307, 257, 8310, 295, 10938, 11, 457, 309, 311, 767, 264, 912, 14217, 6358, 50632], "temperature": 0.0, "avg_logprob": -0.07075594073144074, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0003797754761762917}, {"id": 645, "seek": 318876, "start": 3194.1200000000003, "end": 3198.36, "text": " as the original blue sequence of samples, just at a different frequency. And so it's ideal if", "tokens": [50632, 382, 264, 3380, 3344, 8310, 295, 10938, 11, 445, 412, 257, 819, 7893, 13, 400, 370, 309, 311, 7157, 498, 50844], "temperature": 0.0, "avg_logprob": -0.07075594073144074, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0003797754761762917}, {"id": 646, "seek": 318876, "start": 3198.36, "end": 3204.36, "text": " the same model works on both of them. But standard models like CNNs cannot do this,", "tokens": [50844, 264, 912, 2316, 1985, 322, 1293, 295, 552, 13, 583, 3832, 5245, 411, 24859, 82, 2644, 360, 341, 11, 51144], "temperature": 0.0, "avg_logprob": -0.07075594073144074, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0003797754761762917}, {"id": 647, "seek": 318876, "start": 3205.1600000000003, "end": 3211.1600000000003, "text": " essentially because of the local bias that was brought up earlier. I won't unpack this here,", "tokens": [51184, 4476, 570, 295, 264, 2654, 12577, 300, 390, 3038, 493, 3071, 13, 286, 1582, 380, 26699, 341, 510, 11, 51484], "temperature": 0.0, "avg_logprob": -0.07075594073144074, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0003797754761762917}, {"id": 648, "seek": 318876, "start": 3211.1600000000003, "end": 3216.36, "text": " but if you use like a standard local CNN, it will break at a different frequency. However,", "tokens": [51484, 457, 498, 291, 764, 411, 257, 3832, 2654, 24859, 11, 309, 486, 1821, 412, 257, 819, 7893, 13, 2908, 11, 51744], "temperature": 0.0, "avg_logprob": -0.07075594073144074, "compression_ratio": 1.776470588235294, "no_speech_prob": 0.0003797754761762917}, {"id": 649, "seek": 321636, "start": 3216.36, "end": 3221.2400000000002, "text": " by using a signal model such as S4, which is actually understanding the underlying continuous", "tokens": [50364, 538, 1228, 257, 6358, 2316, 1270, 382, 318, 19, 11, 597, 307, 767, 3701, 264, 14217, 10957, 50608], "temperature": 0.0, "avg_logprob": -0.06666076183319092, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.00040434172842651606}, {"id": 650, "seek": 321636, "start": 3221.2400000000002, "end": 3226.44, "text": " domain or the underlying continuous function, it can work here without modification. So this is", "tokens": [50608, 9274, 420, 264, 14217, 10957, 2445, 11, 309, 393, 589, 510, 1553, 26747, 13, 407, 341, 307, 50868], "temperature": 0.0, "avg_logprob": -0.06666076183319092, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.00040434172842651606}, {"id": 651, "seek": 321636, "start": 3226.44, "end": 3231.56, "text": " all in a zero shot setting where it's trained at one resolution and tested on a different resolution.", "tokens": [50868, 439, 294, 257, 4018, 3347, 3287, 689, 309, 311, 8895, 412, 472, 8669, 293, 8246, 322, 257, 819, 8669, 13, 51124], "temperature": 0.0, "avg_logprob": -0.06666076183319092, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.00040434172842651606}, {"id": 652, "seek": 321636, "start": 3232.44, "end": 3239.08, "text": " And this breaks a CNN, but S4 can do it out of the box. And that's because of this first property", "tokens": [51168, 400, 341, 9857, 257, 24859, 11, 457, 318, 19, 393, 360, 309, 484, 295, 264, 2424, 13, 400, 300, 311, 570, 295, 341, 700, 4707, 51500], "temperature": 0.0, "avg_logprob": -0.06666076183319092, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.00040434172842651606}, {"id": 653, "seek": 321636, "start": 3239.08, "end": 3246.2000000000003, "text": " of being a continuous time model. And now the last two things I'll show are just calling back to", "tokens": [51500, 295, 885, 257, 10957, 565, 2316, 13, 400, 586, 264, 1036, 732, 721, 286, 603, 855, 366, 445, 5141, 646, 281, 51856], "temperature": 0.0, "avg_logprob": -0.06666076183319092, "compression_ratio": 1.7234042553191489, "no_speech_prob": 0.00040434172842651606}, {"id": 654, "seek": 324620, "start": 3246.2, "end": 3253.16, "text": " the experiments at very beginning. I showed some audio generation clips. And that was an", "tokens": [50364, 264, 12050, 412, 588, 2863, 13, 286, 4712, 512, 6278, 5125, 13117, 13, 400, 300, 390, 364, 50712], "temperature": 0.0, "avg_logprob": -0.08020768075619104, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00034593857708387077}, {"id": 655, "seek": 324620, "start": 3253.16, "end": 3257.56, "text": " autoregressive setting where we're generating things one sample at a time. And despite having", "tokens": [50712, 1476, 418, 3091, 488, 3287, 689, 321, 434, 17746, 721, 472, 6889, 412, 257, 565, 13, 400, 7228, 1419, 50932], "temperature": 0.0, "avg_logprob": -0.08020768075619104, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00034593857708387077}, {"id": 656, "seek": 324620, "start": 3257.56, "end": 3263.3199999999997, "text": " an extremely large context window, which made it do better and more coherent, we could still", "tokens": [50932, 364, 4664, 2416, 4319, 4910, 11, 597, 1027, 309, 360, 1101, 293, 544, 36239, 11, 321, 727, 920, 51220], "temperature": 0.0, "avg_logprob": -0.08020768075619104, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00034593857708387077}, {"id": 657, "seek": 324620, "start": 3263.3199999999997, "end": 3267.96, "text": " sample things autoregressively just as fast as other autoregressive models. And that's because", "tokens": [51220, 6889, 721, 1476, 418, 3091, 3413, 445, 382, 2370, 382, 661, 1476, 418, 3091, 488, 5245, 13, 400, 300, 311, 570, 51452], "temperature": 0.0, "avg_logprob": -0.08020768075619104, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00034593857708387077}, {"id": 658, "seek": 324620, "start": 3267.96, "end": 3273.48, "text": " of the fast online or autoregressive representation where you're computing the state and updating it", "tokens": [51452, 295, 264, 2370, 2950, 420, 1476, 418, 3091, 488, 10290, 689, 291, 434, 15866, 264, 1785, 293, 25113, 309, 51728], "temperature": 0.0, "avg_logprob": -0.08020768075619104, "compression_ratio": 1.797709923664122, "no_speech_prob": 0.00034593857708387077}, {"id": 659, "seek": 327348, "start": 3273.48, "end": 3279.56, "text": " every time. And finally, I showed this benchmark of long range modeling where S4 substantially", "tokens": [50364, 633, 565, 13, 400, 2721, 11, 286, 4712, 341, 18927, 295, 938, 3613, 15983, 689, 318, 19, 30797, 50668], "temperature": 0.0, "avg_logprob": -0.09762304800528067, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00024530477821826935}, {"id": 660, "seek": 327348, "start": 3279.56, "end": 3285.48, "text": " outperforms other models on a range of different tasks. And this benchmark was also used to", "tokens": [50668, 484, 26765, 82, 661, 5245, 322, 257, 3613, 295, 819, 9608, 13, 400, 341, 18927, 390, 611, 1143, 281, 50964], "temperature": 0.0, "avg_logprob": -0.09762304800528067, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00024530477821826935}, {"id": 661, "seek": 327348, "start": 3285.48, "end": 3291.96, "text": " benchmark the speed of models during training where S4 is just as fast as all of these", "tokens": [50964, 18927, 264, 3073, 295, 5245, 1830, 3097, 689, 318, 19, 307, 445, 382, 2370, 382, 439, 295, 613, 51288], "temperature": 0.0, "avg_logprob": -0.09762304800528067, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00024530477821826935}, {"id": 662, "seek": 327348, "start": 3291.96, "end": 3296.6, "text": " efficient transformer variants. And that's because of the efficient, paralyzable view", "tokens": [51288, 7148, 31782, 21669, 13, 400, 300, 311, 570, 295, 264, 7148, 11, 32645, 89, 712, 1910, 51520], "temperature": 0.0, "avg_logprob": -0.09762304800528067, "compression_ratio": 1.6933962264150944, "no_speech_prob": 0.00024530477821826935}, {"id": 663, "seek": 329660, "start": 3297.24, "end": 3303.7999999999997, "text": " along with the new algorithms we introduced. And so all these properties, as I promised,", "tokens": [50396, 2051, 365, 264, 777, 14642, 321, 7268, 13, 400, 370, 439, 613, 7221, 11, 382, 286, 10768, 11, 50724], "temperature": 0.0, "avg_logprob": -0.11306120486969644, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.007229926064610481}, {"id": 664, "seek": 329660, "start": 3303.7999999999997, "end": 3308.6, "text": " have concrete empirical benefits. Now for, I'm running out of time, so I just want to get to", "tokens": [50724, 362, 9859, 31886, 5311, 13, 823, 337, 11, 286, 478, 2614, 484, 295, 565, 11, 370, 286, 445, 528, 281, 483, 281, 50964], "temperature": 0.0, "avg_logprob": -0.11306120486969644, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.007229926064610481}, {"id": 665, "seek": 329660, "start": 3308.6, "end": 3314.52, "text": " a couple more things. For the last part, I just wanted to, for this audience, I wanted to point to", "tokens": [50964, 257, 1916, 544, 721, 13, 1171, 264, 1036, 644, 11, 286, 445, 1415, 281, 11, 337, 341, 4034, 11, 286, 1415, 281, 935, 281, 51260], "temperature": 0.0, "avg_logprob": -0.11306120486969644, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.007229926064610481}, {"id": 666, "seek": 329660, "start": 3314.52, "end": 3319.64, "text": " where I hope that this model will be useful, which is as a general tool for deep learning for", "tokens": [51260, 689, 286, 1454, 300, 341, 2316, 486, 312, 4420, 11, 597, 307, 382, 257, 2674, 2290, 337, 2452, 2539, 337, 51516], "temperature": 0.0, "avg_logprob": -0.11306120486969644, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.007229926064610481}, {"id": 667, "seek": 331964, "start": 3319.64, "end": 3326.3599999999997, "text": " biosignals. And I've pointed out one, one example of a data set already where we were predicting", "tokens": [50364, 36997, 788, 1124, 13, 400, 286, 600, 10932, 484, 472, 11, 472, 1365, 295, 257, 1412, 992, 1217, 689, 321, 645, 32884, 50700], "temperature": 0.0, "avg_logprob": -0.1458167259139244, "compression_ratio": 1.6, "no_speech_prob": 0.017427710816264153}, {"id": 668, "seek": 331964, "start": 3326.3599999999997, "end": 3332.2799999999997, "text": " like heart rate from EKG signals. But this was another one that CE was working on actually and", "tokens": [50700, 411, 1917, 3314, 490, 46078, 38, 12354, 13, 583, 341, 390, 1071, 472, 300, 28109, 390, 1364, 322, 767, 293, 50996], "temperature": 0.0, "avg_logprob": -0.1458167259139244, "compression_ratio": 1.6, "no_speech_prob": 0.017427710816264153}, {"id": 669, "seek": 331964, "start": 3333.24, "end": 3340.12, "text": " her and another lab mate have been trying to test S4 here, where this is a data set of raw", "tokens": [51044, 720, 293, 1071, 2715, 11709, 362, 668, 1382, 281, 1500, 318, 19, 510, 11, 689, 341, 307, 257, 1412, 992, 295, 8936, 51388], "temperature": 0.0, "avg_logprob": -0.1458167259139244, "compression_ratio": 1.6, "no_speech_prob": 0.017427710816264153}, {"id": 670, "seek": 331964, "start": 3340.12, "end": 3346.3599999999997, "text": " EEG signals that are difficult to process because they're so noisy and long. And the state-of-the-art", "tokens": [51388, 33685, 38, 12354, 300, 366, 2252, 281, 1399, 570, 436, 434, 370, 24518, 293, 938, 13, 400, 264, 1785, 12, 2670, 12, 3322, 12, 446, 51700], "temperature": 0.0, "avg_logprob": -0.1458167259139244, "compression_ratio": 1.6, "no_speech_prob": 0.017427710816264153}, {"id": 671, "seek": 334636, "start": 3346.36, "end": 3353.56, "text": " models are very recent. CE's model from a couple months ago was there they are on one of these", "tokens": [50364, 5245, 366, 588, 5162, 13, 28109, 311, 2316, 490, 257, 1916, 2493, 2057, 390, 456, 436, 366, 322, 472, 295, 613, 50724], "temperature": 0.0, "avg_logprob": -0.1395506540934245, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0005032388144172728}, {"id": 672, "seek": 334636, "start": 3353.56, "end": 3358.84, "text": " EEG data sets, but it was quite involved and involved a lot of domain knowledge, such as even", "tokens": [50724, 33685, 38, 1412, 6352, 11, 457, 309, 390, 1596, 3288, 293, 3288, 257, 688, 295, 9274, 3601, 11, 1270, 382, 754, 50988], "temperature": 0.0, "avg_logprob": -0.1395506540934245, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0005032388144172728}, {"id": 673, "seek": 334636, "start": 3358.84, "end": 3364.04, "text": " like the placement of the electrodes and a lot of different parts, components of the model.", "tokens": [50988, 411, 264, 17257, 295, 264, 47824, 293, 257, 688, 295, 819, 3166, 11, 6677, 295, 264, 2316, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1395506540934245, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0005032388144172728}, {"id": 674, "seek": 334636, "start": 3364.6, "end": 3370.44, "text": " And so where I hope that S4 could be useful is as a generic tool or building block for", "tokens": [51276, 400, 370, 689, 286, 1454, 300, 318, 19, 727, 312, 4420, 307, 382, 257, 19577, 2290, 420, 2390, 3461, 337, 51568], "temperature": 0.0, "avg_logprob": -0.1395506540934245, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0005032388144172728}, {"id": 675, "seek": 337044, "start": 3371.32, "end": 3380.52, "text": " for addressing these types of signal data without as much domain expertise and how to design the", "tokens": [50408, 337, 14329, 613, 3467, 295, 6358, 1412, 1553, 382, 709, 9274, 11769, 293, 577, 281, 1715, 264, 50868], "temperature": 0.0, "avg_logprob": -0.16038567969139586, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0003981366171501577}, {"id": 676, "seek": 337044, "start": 3380.52, "end": 3388.52, "text": " model. And so CE and Collid have been running some preliminary experiments using S4 on this data,", "tokens": [50868, 2316, 13, 400, 370, 28109, 293, 4586, 327, 362, 668, 2614, 512, 28817, 12050, 1228, 318, 19, 322, 341, 1412, 11, 51268], "temperature": 0.0, "avg_logprob": -0.16038567969139586, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0003981366171501577}, {"id": 677, "seek": 337044, "start": 3388.52, "end": 3394.52, "text": " where we don't even need to process the, you don't need to pre-process it with FFT features,", "tokens": [51268, 689, 321, 500, 380, 754, 643, 281, 1399, 264, 11, 291, 500, 380, 643, 281, 659, 12, 41075, 309, 365, 479, 25469, 4122, 11, 51568], "temperature": 0.0, "avg_logprob": -0.16038567969139586, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0003981366171501577}, {"id": 678, "seek": 337044, "start": 3394.52, "end": 3399.64, "text": " you don't need to do a lot of these other things and just run it through these, a generic deep", "tokens": [51568, 291, 500, 380, 643, 281, 360, 257, 688, 295, 613, 661, 721, 293, 445, 1190, 309, 807, 613, 11, 257, 19577, 2452, 51824], "temperature": 0.0, "avg_logprob": -0.16038567969139586, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0003981366171501577}, {"id": 679, "seek": 339964, "start": 3399.64, "end": 3405.48, "text": " model composed of S4 layers. And Collid found some very preliminary results where it is improving", "tokens": [50364, 2316, 18204, 295, 318, 19, 7914, 13, 400, 4586, 327, 1352, 512, 588, 28817, 3542, 689, 309, 307, 11470, 50656], "temperature": 0.0, "avg_logprob": -0.09777932418020148, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004953280440531671}, {"id": 680, "seek": 339964, "start": 3405.48, "end": 3413.08, "text": " over the baselines in some settings. This is still very preliminary, so it's not, there's other", "tokens": [50656, 670, 264, 987, 9173, 294, 512, 6257, 13, 639, 307, 920, 588, 28817, 11, 370, 309, 311, 406, 11, 456, 311, 661, 51036], "temperature": 0.0, "avg_logprob": -0.09777932418020148, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004953280440531671}, {"id": 681, "seek": 339964, "start": 3413.08, "end": 3417.96, "text": " settings that we care about, such as incorporating self-supervision and so on, where it's not quite", "tokens": [51036, 6257, 300, 321, 1127, 466, 11, 1270, 382, 33613, 2698, 12, 48172, 6763, 293, 370, 322, 11, 689, 309, 311, 406, 1596, 51280], "temperature": 0.0, "avg_logprob": -0.09777932418020148, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004953280440531671}, {"id": 682, "seek": 339964, "start": 3417.96, "end": 3425.96, "text": " there, but I do think it has a lot of potential in this type of domain. Another example, actually", "tokens": [51280, 456, 11, 457, 286, 360, 519, 309, 575, 257, 688, 295, 3995, 294, 341, 2010, 295, 9274, 13, 3996, 1365, 11, 767, 51680], "temperature": 0.0, "avg_logprob": -0.09777932418020148, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.0004953280440531671}, {"id": 683, "seek": 342596, "start": 3426.04, "end": 3431.08, "text": " that was published was another recent collaboration with Stanford Medicine that was submitted to a", "tokens": [50368, 300, 390, 6572, 390, 1071, 5162, 9363, 365, 20374, 20338, 300, 390, 14405, 281, 257, 50620], "temperature": 0.0, "avg_logprob": -0.1407744706566654, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.000503229268360883}, {"id": 684, "seek": 342596, "start": 3431.08, "end": 3437.96, "text": " gastroenterology journal on detecting acid reflux from impotence sensor data. And so again,", "tokens": [50620, 17898, 340, 14278, 1793, 6708, 322, 40237, 8258, 1895, 75, 2449, 490, 704, 310, 655, 10200, 1412, 13, 400, 370, 797, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1407744706566654, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.000503229268360883}, {"id": 685, "seek": 342596, "start": 3437.96, "end": 3447.48, "text": " S4 was really good on that type of prediction task. So that is all I was going to talk about", "tokens": [50964, 318, 19, 390, 534, 665, 322, 300, 2010, 295, 17630, 5633, 13, 407, 300, 307, 439, 286, 390, 516, 281, 751, 466, 51440], "temperature": 0.0, "avg_logprob": -0.1407744706566654, "compression_ratio": 1.4663212435233162, "no_speech_prob": 0.000503229268360883}, {"id": 686, "seek": 344748, "start": 3447.96, "end": 3455.96, "text": " for this. So just to review, S4 is a, it's an SSM, which are these two equations,", "tokens": [50388, 337, 341, 13, 407, 445, 281, 3131, 11, 318, 19, 307, 257, 11, 309, 311, 364, 12238, 44, 11, 597, 366, 613, 732, 11787, 11, 50788], "temperature": 0.0, "avg_logprob": -0.11896129857713932, "compression_ratio": 1.575, "no_speech_prob": 0.0067949253134429455}, {"id": 687, "seek": 344748, "start": 3455.96, "end": 3459.56, "text": " where we plug in certain formulas and have special algorithms to compute the model.", "tokens": [50788, 689, 321, 5452, 294, 1629, 30546, 293, 362, 2121, 14642, 281, 14722, 264, 2316, 13, 50968], "temperature": 0.0, "avg_logprob": -0.11896129857713932, "compression_ratio": 1.575, "no_speech_prob": 0.0067949253134429455}, {"id": 688, "seek": 344748, "start": 3460.6, "end": 3466.28, "text": " And overall, SSMs and in particular, S4 have a number of very nice properties with", "tokens": [51020, 400, 4787, 11, 12238, 26386, 293, 294, 1729, 11, 318, 19, 362, 257, 1230, 295, 588, 1481, 7221, 365, 51304], "temperature": 0.0, "avg_logprob": -0.11896129857713932, "compression_ratio": 1.575, "no_speech_prob": 0.0067949253134429455}, {"id": 689, "seek": 344748, "start": 3466.28, "end": 3470.92, "text": " concrete empirical benefits, as we saw, and I think can become a very effective building block for", "tokens": [51304, 9859, 31886, 5311, 11, 382, 321, 1866, 11, 293, 286, 519, 393, 1813, 257, 588, 4942, 2390, 3461, 337, 51536], "temperature": 0.0, "avg_logprob": -0.11896129857713932, "compression_ratio": 1.575, "no_speech_prob": 0.0067949253134429455}, {"id": 690, "seek": 344748, "start": 3470.92, "end": 3476.44, "text": " modeling many types of sequential data in the future. Thanks for listening and thanks for all", "tokens": [51536, 15983, 867, 3467, 295, 42881, 1412, 294, 264, 2027, 13, 2561, 337, 4764, 293, 3231, 337, 439, 51812], "temperature": 0.0, "avg_logprob": -0.11896129857713932, "compression_ratio": 1.575, "no_speech_prob": 0.0067949253134429455}, {"id": 691, "seek": 347644, "start": 3476.44, "end": 3483.0, "text": " the collaborators for the hard work. This slide lists a couple of resources, such as blog posts", "tokens": [50364, 264, 39789, 337, 264, 1152, 589, 13, 639, 4137, 14511, 257, 1916, 295, 3593, 11, 1270, 382, 6968, 12300, 50692], "temperature": 0.0, "avg_logprob": -0.09471190688956498, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.0012442286824807525}, {"id": 692, "seek": 347644, "start": 3483.0, "end": 3489.56, "text": " and related papers, as well as the audio results from an ongoing, a paper that's under submission", "tokens": [50692, 293, 4077, 10577, 11, 382, 731, 382, 264, 6278, 3542, 490, 364, 10452, 11, 257, 3035, 300, 311, 833, 23689, 51020], "temperature": 0.0, "avg_logprob": -0.09471190688956498, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.0012442286824807525}, {"id": 693, "seek": 347644, "start": 3489.56, "end": 3495.8, "text": " right now. Feel free to reach out if you have questions and thanks. This was my last slide,", "tokens": [51020, 558, 586, 13, 14113, 1737, 281, 2524, 484, 498, 291, 362, 1651, 293, 3231, 13, 639, 390, 452, 1036, 4137, 11, 51332], "temperature": 0.0, "avg_logprob": -0.09471190688956498, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.0012442286824807525}, {"id": 694, "seek": 347644, "start": 3495.8, "end": 3500.68, "text": " but because you want to ask how I will, I guess I'm technically out of time, so of course people", "tokens": [51332, 457, 570, 291, 528, 281, 1029, 577, 286, 486, 11, 286, 2041, 286, 478, 12120, 484, 295, 565, 11, 370, 295, 1164, 561, 51576], "temperature": 0.0, "avg_logprob": -0.09471190688956498, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.0012442286824807525}, {"id": 695, "seek": 347644, "start": 3500.68, "end": 3505.96, "text": " feel free to leave, but if you want to stay, I can show one thing about the high resolution images", "tokens": [51576, 841, 1737, 281, 1856, 11, 457, 498, 291, 528, 281, 1754, 11, 286, 393, 855, 472, 551, 466, 264, 1090, 8669, 5267, 51840], "temperature": 0.0, "avg_logprob": -0.09471190688956498, "compression_ratio": 1.67595818815331, "no_speech_prob": 0.0012442286824807525}, {"id": 696, "seek": 350596, "start": 3505.96, "end": 3514.92, "text": " that that was brought up. Let me find that slide. Yeah, if people have conflicts, feel free to leave", "tokens": [50364, 300, 300, 390, 3038, 493, 13, 961, 385, 915, 300, 4137, 13, 865, 11, 498, 561, 362, 19807, 11, 841, 1737, 281, 1856, 50812], "temperature": 0.0, "avg_logprob": -0.17858476762647751, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0011857675854116678}, {"id": 697, "seek": 350596, "start": 3514.92, "end": 3521.16, "text": " and we will put up the recording of the talk later in our YouTube channel. Otherwise, if you", "tokens": [50812, 293, 321, 486, 829, 493, 264, 6613, 295, 264, 751, 1780, 294, 527, 3088, 2269, 13, 10328, 11, 498, 291, 51124], "temperature": 0.0, "avg_logprob": -0.17858476762647751, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0011857675854116678}, {"id": 698, "seek": 350596, "start": 3521.16, "end": 3527.16, "text": " would like to stay, then yeah, I'll be able to share the slide. So yeah, I'll just really quickly go", "tokens": [51124, 576, 411, 281, 1754, 11, 550, 1338, 11, 286, 603, 312, 1075, 281, 2073, 264, 4137, 13, 407, 1338, 11, 286, 603, 445, 534, 2661, 352, 51424], "temperature": 0.0, "avg_logprob": -0.17858476762647751, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.0011857675854116678}, {"id": 699, "seek": 352716, "start": 3527.16, "end": 3536.6, "text": " over this where medical, medical imaging is something that we think could be a potential", "tokens": [50364, 670, 341, 689, 4625, 11, 4625, 25036, 307, 746, 300, 321, 519, 727, 312, 257, 3995, 50836], "temperature": 0.0, "avg_logprob": -0.19277921270151607, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.043950244784355164}, {"id": 700, "seek": 352716, "start": 3537.3999999999996, "end": 3546.68, "text": " strong use case for S4 because of this high resolution feature where, so this slide was", "tokens": [50876, 2068, 764, 1389, 337, 318, 19, 570, 295, 341, 1090, 8669, 4111, 689, 11, 370, 341, 4137, 390, 51340], "temperature": 0.0, "avg_logprob": -0.19277921270151607, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.043950244784355164}, {"id": 701, "seek": 352716, "start": 3546.68, "end": 3550.6, "text": " about, I was moving from a different way, but the point I wanted to make was that", "tokens": [51340, 466, 11, 286, 390, 2684, 490, 257, 819, 636, 11, 457, 264, 935, 286, 1415, 281, 652, 390, 300, 51536], "temperature": 0.0, "avg_logprob": -0.19277921270151607, "compression_ratio": 1.5087719298245614, "no_speech_prob": 0.043950244784355164}, {"id": 702, "seek": 355060, "start": 3551.3199999999997, "end": 3555.3199999999997, "text": " Can you go to presentation mode? I think we are still seeing your screen nose.", "tokens": [50400, 1664, 291, 352, 281, 5860, 4391, 30, 286, 519, 321, 366, 920, 2577, 428, 2568, 6690, 13, 50600], "temperature": 0.0, "avg_logprob": -0.2666525196384739, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.03399787098169327}, {"id": 703, "seek": 355060, "start": 3556.12, "end": 3564.2, "text": " Yeah, sorry. Oops. Am I showing my whole screen? No, we are seeing your screen rather than the", "tokens": [50640, 865, 11, 2597, 13, 21726, 13, 2012, 286, 4099, 452, 1379, 2568, 30, 883, 11, 321, 366, 2577, 428, 2568, 2831, 813, 264, 51044], "temperature": 0.0, "avg_logprob": -0.2666525196384739, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.03399787098169327}, {"id": 704, "seek": 355060, "start": 3564.2, "end": 3573.4, "text": " presentation. Okay, I thought I had it on. You can just swap the view. I thought I had it on the right", "tokens": [51044, 5860, 13, 1033, 11, 286, 1194, 286, 632, 309, 322, 13, 509, 393, 445, 18135, 264, 1910, 13, 286, 1194, 286, 632, 309, 322, 264, 558, 51504], "temperature": 0.0, "avg_logprob": -0.2666525196384739, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.03399787098169327}, {"id": 705, "seek": 357340, "start": 3573.96, "end": 3582.84, "text": " view. Is this one still show the, oops. Yeah, I can see the high definition view.", "tokens": [50392, 1910, 13, 1119, 341, 472, 920, 855, 264, 11, 34166, 13, 865, 11, 286, 393, 536, 264, 1090, 7123, 1910, 13, 50836], "temperature": 0.0, "avg_logprob": -0.19168183043762876, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.006585803348571062}, {"id": 706, "seek": 357340, "start": 3583.88, "end": 3588.6, "text": " Okay, great. So yeah, so the point I was making is that normally image data sets are things like", "tokens": [50888, 1033, 11, 869, 13, 407, 1338, 11, 370, 264, 935, 286, 390, 1455, 307, 300, 5646, 3256, 1412, 6352, 366, 721, 411, 51124], "temperature": 0.0, "avg_logprob": -0.19168183043762876, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.006585803348571062}, {"id": 707, "seek": 357340, "start": 3588.6, "end": 3592.76, "text": " ImageNet, which are actually extremely low resolution compared to other data that we might", "tokens": [51124, 29903, 31890, 11, 597, 366, 767, 4664, 2295, 8669, 5347, 281, 661, 1412, 300, 321, 1062, 51332], "temperature": 0.0, "avg_logprob": -0.19168183043762876, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.006585803348571062}, {"id": 708, "seek": 357340, "start": 3592.76, "end": 3600.28, "text": " find such as medical imaging where apparently the images can be up to 100,000 by 100,000 pixels.", "tokens": [51332, 915, 1270, 382, 4625, 25036, 689, 7970, 264, 5267, 393, 312, 493, 281, 2319, 11, 1360, 538, 2319, 11, 1360, 18668, 13, 51708], "temperature": 0.0, "avg_logprob": -0.19168183043762876, "compression_ratio": 1.564102564102564, "no_speech_prob": 0.006585803348571062}, {"id": 709, "seek": 360028, "start": 3600.6000000000004, "end": 3605.4, "text": " And this is obviously like way too big for current models, which can only operate on small", "tokens": [50380, 400, 341, 307, 2745, 411, 636, 886, 955, 337, 2190, 5245, 11, 597, 393, 787, 9651, 322, 1359, 50620], "temperature": 0.0, "avg_logprob": -0.14863026036625415, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0011689590755850077}, {"id": 710, "seek": 360028, "start": 3606.0400000000004, "end": 3611.0, "text": " patches at a time. So I don't know how to address this really, but it's something that fascinates", "tokens": [50652, 26531, 412, 257, 565, 13, 407, 286, 500, 380, 458, 577, 281, 2985, 341, 534, 11, 457, 309, 311, 746, 300, 7184, 259, 1024, 50900], "temperature": 0.0, "avg_logprob": -0.14863026036625415, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0011689590755850077}, {"id": 711, "seek": 360028, "start": 3611.0, "end": 3616.76, "text": " me. But just to point out, this is part of a longer drop pack where I pointed some potential", "tokens": [50900, 385, 13, 583, 445, 281, 935, 484, 11, 341, 307, 644, 295, 257, 2854, 3270, 2844, 689, 286, 10932, 512, 3995, 51188], "temperature": 0.0, "avg_logprob": -0.14863026036625415, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0011689590755850077}, {"id": 712, "seek": 360028, "start": 3616.76, "end": 3622.2000000000003, "text": " future directions. The one that I'll mention here relates to some things that we brought up,", "tokens": [51188, 2027, 11095, 13, 440, 472, 300, 286, 603, 2152, 510, 16155, 281, 512, 721, 300, 321, 3038, 493, 11, 51460], "temperature": 0.0, "avg_logprob": -0.14863026036625415, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0011689590755850077}, {"id": 713, "seek": 360028, "start": 3622.2000000000003, "end": 3627.8, "text": " which is that just like the speech experiment that I showed, I believe that S4 should work", "tokens": [51460, 597, 307, 300, 445, 411, 264, 6218, 5120, 300, 286, 4712, 11, 286, 1697, 300, 318, 19, 820, 589, 51740], "temperature": 0.0, "avg_logprob": -0.14863026036625415, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0011689590755850077}, {"id": 714, "seek": 362780, "start": 3627.88, "end": 3632.92, "text": " training on images at different resolutions. And so what you can do is essentially try to", "tokens": [50368, 3097, 322, 5267, 412, 819, 32179, 13, 400, 370, 437, 291, 393, 360, 307, 4476, 853, 281, 50620], "temperature": 0.0, "avg_logprob": -0.08203336927625868, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005190725205466151}, {"id": 715, "seek": 362780, "start": 3634.1200000000003, "end": 3640.44, "text": " train on lower dimensional versions of the image, lower resolution versions, and then transfer", "tokens": [50680, 3847, 322, 3126, 18795, 9606, 295, 264, 3256, 11, 3126, 8669, 9606, 11, 293, 550, 5003, 50996], "temperature": 0.0, "avg_logprob": -0.08203336927625868, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005190725205466151}, {"id": 716, "seek": 362780, "start": 3640.44, "end": 3644.92, "text": " the same model to work on high dimensions, which is a very similar thing that I showed", "tokens": [50996, 264, 912, 2316, 281, 589, 322, 1090, 12819, 11, 597, 307, 257, 588, 2531, 551, 300, 286, 4712, 51220], "temperature": 0.0, "avg_logprob": -0.08203336927625868, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005190725205466151}, {"id": 717, "seek": 362780, "start": 3644.92, "end": 3653.32, "text": " for the speech example. So yeah, I think that's potentially something that could work. And the", "tokens": [51220, 337, 264, 6218, 1365, 13, 407, 1338, 11, 286, 519, 300, 311, 7263, 746, 300, 727, 589, 13, 400, 264, 51640], "temperature": 0.0, "avg_logprob": -0.08203336927625868, "compression_ratio": 1.7023255813953488, "no_speech_prob": 0.0005190725205466151}, {"id": 718, "seek": 365332, "start": 3653.32, "end": 3657.8, "text": " point is that a signal model like S4 will work at different resolutions", "tokens": [50364, 935, 307, 300, 257, 6358, 2316, 411, 318, 19, 486, 589, 412, 819, 32179, 50588], "temperature": 0.0, "avg_logprob": -0.12954207829066686, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00046536256559193134}, {"id": 719, "seek": 365332, "start": 3660.28, "end": 3665.48, "text": " because you can sample at different rates essentially. And yeah, so what you need is a", "tokens": [50712, 570, 291, 393, 6889, 412, 819, 6846, 4476, 13, 400, 1338, 11, 370, 437, 291, 643, 307, 257, 50972], "temperature": 0.0, "avg_logprob": -0.12954207829066686, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00046536256559193134}, {"id": 720, "seek": 365332, "start": 3665.48, "end": 3670.2000000000003, "text": " signal model that understands the continuous domain, just like the example I showed. And that", "tokens": [50972, 6358, 2316, 300, 15146, 264, 10957, 9274, 11, 445, 411, 264, 1365, 286, 4712, 13, 400, 300, 51208], "temperature": 0.0, "avg_logprob": -0.12954207829066686, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00046536256559193134}, {"id": 721, "seek": 365332, "start": 3670.2000000000003, "end": 3674.28, "text": " points to this property again. So this is something where we haven't tried it and I don't know if it", "tokens": [51208, 2793, 281, 341, 4707, 797, 13, 407, 341, 307, 746, 689, 321, 2378, 380, 3031, 309, 293, 286, 500, 380, 458, 498, 309, 51412], "temperature": 0.0, "avg_logprob": -0.12954207829066686, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00046536256559193134}, {"id": 722, "seek": 365332, "start": 3674.28, "end": 3679.32, "text": " works, but it's some part of me feels like it might be the right way to or one potential", "tokens": [51412, 1985, 11, 457, 309, 311, 512, 644, 295, 385, 3417, 411, 309, 1062, 312, 264, 558, 636, 281, 420, 472, 3995, 51664], "temperature": 0.0, "avg_logprob": -0.12954207829066686, "compression_ratio": 1.6870229007633588, "no_speech_prob": 0.00046536256559193134}, {"id": 723, "seek": 367932, "start": 3679.96, "end": 3683.88, "text": " good way to approach this type of problem. But I would think in the opposite way,", "tokens": [50396, 665, 636, 281, 3109, 341, 2010, 295, 1154, 13, 583, 286, 576, 519, 294, 264, 6182, 636, 11, 50592], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 724, "seek": 367932, "start": 3685.0, "end": 3689.1600000000003, "text": " it's not really we want to generate the high resolution from the low resolution,", "tokens": [50648, 309, 311, 406, 534, 321, 528, 281, 8460, 264, 1090, 8669, 490, 264, 2295, 8669, 11, 50856], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 725, "seek": 367932, "start": 3689.1600000000003, "end": 3693.0800000000004, "text": " but I would imagine since you have this kind of like state-based representation and", "tokens": [50856, 457, 286, 576, 3811, 1670, 291, 362, 341, 733, 295, 411, 1785, 12, 6032, 10290, 293, 51052], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 726, "seek": 367932, "start": 3693.0800000000004, "end": 3697.2400000000002, "text": " finally you're getting this signal, I would imagine like in some case always like we had", "tokens": [51052, 2721, 291, 434, 1242, 341, 6358, 11, 286, 576, 3811, 411, 294, 512, 1389, 1009, 411, 321, 632, 51260], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 727, "seek": 367932, "start": 3697.2400000000002, "end": 3701.8, "text": " to deal with this kind of situation that we had a very high resolution image. And before running", "tokens": [51260, 281, 2028, 365, 341, 733, 295, 2590, 300, 321, 632, 257, 588, 1090, 8669, 3256, 13, 400, 949, 2614, 51488], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 728, "seek": 367932, "start": 3701.8, "end": 3707.32, "text": " through the convolution because of the memory computations, like computational complexity,", "tokens": [51488, 807, 264, 45216, 570, 295, 264, 4675, 2807, 763, 11, 411, 28270, 14024, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12669406559156335, "compression_ratio": 1.8480565371024735, "no_speech_prob": 0.0015964644262567163}, {"id": 729, "seek": 370732, "start": 3707.4, "end": 3711.56, "text": " memory complexity and all this kind of thing. So you have to reskill the image into a much lower", "tokens": [50368, 4675, 14024, 293, 439, 341, 733, 295, 551, 13, 407, 291, 362, 281, 725, 34213, 264, 3256, 666, 257, 709, 3126, 50576], "temperature": 0.0, "avg_logprob": -0.12929671519511454, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0005701386835426092}, {"id": 730, "seek": 370732, "start": 3711.56, "end": 3719.0800000000004, "text": " dimension. Yeah. And we had a chance of losing a lot of features, specifically histopathology,", "tokens": [50576, 10139, 13, 865, 13, 400, 321, 632, 257, 2931, 295, 7027, 257, 688, 295, 4122, 11, 4682, 1758, 27212, 1793, 11, 50952], "temperature": 0.0, "avg_logprob": -0.12929671519511454, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0005701386835426092}, {"id": 731, "seek": 370732, "start": 3719.0800000000004, "end": 3722.6800000000003, "text": " exactly the example that you showed, or the mammogram, those kind of images, you know.", "tokens": [50952, 2293, 264, 1365, 300, 291, 4712, 11, 420, 264, 19033, 12820, 11, 729, 733, 295, 5267, 11, 291, 458, 13, 51132], "temperature": 0.0, "avg_logprob": -0.12929671519511454, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0005701386835426092}, {"id": 732, "seek": 370732, "start": 3723.48, "end": 3728.52, "text": " Yeah, I see. So I was thinking that perhaps like what you can do is kind of like iteratively", "tokens": [51172, 865, 11, 286, 536, 13, 407, 286, 390, 1953, 300, 4317, 411, 437, 291, 393, 360, 307, 733, 295, 411, 17138, 19020, 51424], "temperature": 0.0, "avg_logprob": -0.12929671519511454, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0005701386835426092}, {"id": 733, "seek": 370732, "start": 3729.7200000000003, "end": 3733.2400000000002, "text": " increase the resolution and pick up higher and higher resolution features as you go.", "tokens": [51484, 3488, 264, 8669, 293, 1888, 493, 2946, 293, 2946, 8669, 4122, 382, 291, 352, 13, 51660], "temperature": 0.0, "avg_logprob": -0.12929671519511454, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.0005701386835426092}, {"id": 734, "seek": 373324, "start": 3733.7999999999997, "end": 3738.9199999999996, "text": " But the benefit is that perhaps you can pick up the coarser grain things and then as you", "tokens": [50392, 583, 264, 5121, 307, 300, 4317, 291, 393, 1888, 493, 264, 598, 685, 260, 12837, 721, 293, 550, 382, 291, 50648], "temperature": 0.0, "avg_logprob": -0.17092778661229588, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.003269733628258109}, {"id": 735, "seek": 373324, "start": 3738.9199999999996, "end": 3744.4399999999996, "text": " upsize the image then and you rescale your kernel essentially, then it's already going to be doing", "tokens": [50648, 15497, 1125, 264, 3256, 550, 293, 291, 9610, 1220, 428, 28256, 4476, 11, 550, 309, 311, 1217, 516, 281, 312, 884, 50924], "temperature": 0.0, "avg_logprob": -0.17092778661229588, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.003269733628258109}, {"id": 736, "seek": 373324, "start": 3744.4399999999996, "end": 3748.9199999999996, "text": " as everybody knows the coarse grain features, but then it as you keep training, it only has", "tokens": [50924, 382, 2201, 3255, 264, 39312, 12837, 4122, 11, 457, 550, 309, 382, 291, 1066, 3097, 11, 309, 787, 575, 51148], "temperature": 0.0, "avg_logprob": -0.17092778661229588, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.003269733628258109}, {"id": 737, "seek": 373324, "start": 3748.9199999999996, "end": 3755.0, "text": " to learn the higher frequency features as you go. And again, I have no idea if this is,", "tokens": [51148, 281, 1466, 264, 2946, 7893, 4122, 382, 291, 352, 13, 400, 797, 11, 286, 362, 572, 1558, 498, 341, 307, 11, 51452], "temperature": 0.0, "avg_logprob": -0.17092778661229588, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.003269733628258109}, {"id": 738, "seek": 373324, "start": 3755.0, "end": 3760.04, "text": " if this makes sense or it's promising, but it sounds pretty interesting. I think,", "tokens": [51452, 498, 341, 1669, 2020, 420, 309, 311, 20257, 11, 457, 309, 3263, 1238, 1880, 13, 286, 519, 11, 51704], "temperature": 0.0, "avg_logprob": -0.17092778661229588, "compression_ratio": 1.726923076923077, "no_speech_prob": 0.003269733628258109}, {"id": 739, "seek": 376004, "start": 3760.04, "end": 3766.68, "text": " I think Alba's idea is actually very similar to how pathologists analyze our hostile images.", "tokens": [50364, 286, 519, 967, 4231, 311, 1558, 307, 767, 588, 2531, 281, 577, 3100, 12256, 12477, 527, 27312, 5267, 13, 50696], "temperature": 0.0, "avg_logprob": -0.22282662598983102, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0008944032597355545}, {"id": 740, "seek": 376004, "start": 3766.68, "end": 3771.96, "text": " So they usually look at. Define zoom. Define zoom. They usually look at low resolution", "tokens": [50696, 407, 436, 2673, 574, 412, 13, 9548, 533, 8863, 13, 9548, 533, 8863, 13, 814, 2673, 574, 412, 2295, 8669, 50960], "temperature": 0.0, "avg_logprob": -0.22282662598983102, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0008944032597355545}, {"id": 741, "seek": 376004, "start": 3772.52, "end": 3779.32, "text": " image first and localize like the potential areas where the tumor are and then they zoom", "tokens": [50988, 3256, 700, 293, 2654, 1125, 411, 264, 3995, 3179, 689, 264, 22512, 366, 293, 550, 436, 8863, 51328], "temperature": 0.0, "avg_logprob": -0.22282662598983102, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0008944032597355545}, {"id": 742, "seek": 376004, "start": 3779.32, "end": 3785.96, "text": " into higher resolution. Right. Yeah. I see. Yeah. And so yeah, I don't know if this will be better", "tokens": [51328, 666, 2946, 8669, 13, 1779, 13, 865, 13, 286, 536, 13, 865, 13, 400, 370, 1338, 11, 286, 500, 380, 458, 498, 341, 486, 312, 1101, 51660], "temperature": 0.0, "avg_logprob": -0.22282662598983102, "compression_ratio": 1.6383928571428572, "no_speech_prob": 0.0008944032597355545}, {"id": 743, "seek": 378596, "start": 3785.96, "end": 3790.92, "text": " than CNN or other things, but it definitely has different and interesting properties.", "tokens": [50364, 813, 24859, 420, 661, 721, 11, 457, 309, 2138, 575, 819, 293, 1880, 7221, 13, 50612], "temperature": 0.0, "avg_logprob": -0.15154603322347004, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0021788899321109056}, {"id": 744, "seek": 378596, "start": 3793.4, "end": 3800.84, "text": " Okay. So that was, yeah, I think that's the end of the material I have. I can say around a few", "tokens": [50736, 1033, 13, 407, 300, 390, 11, 1338, 11, 286, 519, 300, 311, 264, 917, 295, 264, 2527, 286, 362, 13, 286, 393, 584, 926, 257, 1326, 51108], "temperature": 0.0, "avg_logprob": -0.15154603322347004, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0021788899321109056}, {"id": 745, "seek": 378596, "start": 3800.84, "end": 3805.8, "text": " more minutes if people still have questions. All right. Great. Thanks so much, Albert.", "tokens": [51108, 544, 2077, 498, 561, 920, 362, 1651, 13, 1057, 558, 13, 3769, 13, 2561, 370, 709, 11, 20812, 13, 51356], "temperature": 0.0, "avg_logprob": -0.15154603322347004, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0021788899321109056}, {"id": 746, "seek": 378596, "start": 3805.8, "end": 3810.12, "text": " I have just one question. Sure. Yeah. Thank you for the presentation. That was awesome.", "tokens": [51356, 286, 362, 445, 472, 1168, 13, 4894, 13, 865, 13, 1044, 291, 337, 264, 5860, 13, 663, 390, 3476, 13, 51572], "temperature": 0.0, "avg_logprob": -0.15154603322347004, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0021788899321109056}, {"id": 747, "seek": 381012, "start": 3811.0, "end": 3816.44, "text": " Did you find any scenario where it's better to use transformer than S4?", "tokens": [50408, 2589, 291, 915, 604, 9005, 689, 309, 311, 1101, 281, 764, 31782, 813, 318, 19, 30, 50680], "temperature": 0.0, "avg_logprob": -0.10945699328467959, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0009395772940479219}, {"id": 748, "seek": 381012, "start": 3817.24, "end": 3822.7599999999998, "text": " Yes. Another great question. So let me just share my screen again. I have one slide prepared for", "tokens": [50720, 1079, 13, 3996, 869, 1168, 13, 407, 718, 385, 445, 2073, 452, 2568, 797, 13, 286, 362, 472, 4137, 4927, 337, 50996], "temperature": 0.0, "avg_logprob": -0.10945699328467959, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0009395772940479219}, {"id": 749, "seek": 381012, "start": 3822.7599999999998, "end": 3831.0, "text": " that. Basically at the beginning, I drew this distinction between continuous kind of continuous", "tokens": [50996, 300, 13, 8537, 412, 264, 2863, 11, 286, 12804, 341, 16844, 1296, 10957, 733, 295, 10957, 51408], "temperature": 0.0, "avg_logprob": -0.10945699328467959, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0009395772940479219}, {"id": 750, "seek": 381012, "start": 3831.0, "end": 3838.92, "text": " and discrete data. And I think that S4 will be the best or like the ideas involved are potentially", "tokens": [51408, 293, 27706, 1412, 13, 400, 286, 519, 300, 318, 19, 486, 312, 264, 1151, 420, 411, 264, 3487, 3288, 366, 7263, 51804], "temperature": 0.0, "avg_logprob": -0.10945699328467959, "compression_ratio": 1.55793991416309, "no_speech_prob": 0.0009395772940479219}, {"id": 751, "seek": 383892, "start": 3839.32, "end": 3844.28, "text": " going to be the best thing to do for signals. But for kind of higher level concepts or more", "tokens": [50384, 516, 281, 312, 264, 1151, 551, 281, 360, 337, 12354, 13, 583, 337, 733, 295, 2946, 1496, 10392, 420, 544, 50632], "temperature": 0.0, "avg_logprob": -0.07424909418279474, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.0006460531149059534}, {"id": 752, "seek": 383892, "start": 3844.28, "end": 3849.2400000000002, "text": " discrete concepts such as language or some other things, transformers, I think that's", "tokens": [50632, 27706, 10392, 1270, 382, 2856, 420, 512, 661, 721, 11, 4088, 433, 11, 286, 519, 300, 311, 50880], "temperature": 0.0, "avg_logprob": -0.07424909418279474, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.0006460531149059534}, {"id": 753, "seek": 383892, "start": 3849.2400000000002, "end": 3859.2400000000002, "text": " where transformers really shine and are probably going to be better. So here's a,", "tokens": [50880, 689, 4088, 433, 534, 12207, 293, 366, 1391, 516, 281, 312, 1101, 13, 407, 510, 311, 257, 11, 51380], "temperature": 0.0, "avg_logprob": -0.07424909418279474, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.0006460531149059534}, {"id": 754, "seek": 383892, "start": 3860.2000000000003, "end": 3865.8, "text": " I don't know if this is the right screen again. Sorry. Anyways, here's the one slide on language", "tokens": [51428, 286, 500, 380, 458, 498, 341, 307, 264, 558, 2568, 797, 13, 4919, 13, 15585, 11, 510, 311, 264, 472, 4137, 322, 2856, 51708], "temperature": 0.0, "avg_logprob": -0.07424909418279474, "compression_ratio": 1.655813953488372, "no_speech_prob": 0.0006460531149059534}, {"id": 755, "seek": 386580, "start": 3865.8, "end": 3871.0800000000004, "text": " modeling where we took a transformer, which are currently of course the best models for", "tokens": [50364, 15983, 689, 321, 1890, 257, 31782, 11, 597, 366, 4362, 295, 1164, 264, 1151, 5245, 337, 50628], "temperature": 0.0, "avg_logprob": -0.11570345271717418, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.002322013024240732}, {"id": 756, "seek": 386580, "start": 3871.0800000000004, "end": 3878.04, "text": " text and NLP. And we replaced the attention with S4 and found that it doesn't do quite as well.", "tokens": [50628, 2487, 293, 426, 45196, 13, 400, 321, 10772, 264, 3202, 365, 318, 19, 293, 1352, 300, 309, 1177, 380, 360, 1596, 382, 731, 13, 50976], "temperature": 0.0, "avg_logprob": -0.11570345271717418, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.002322013024240732}, {"id": 757, "seek": 386580, "start": 3878.84, "end": 3882.84, "text": " But it is still better than all non, it's significantly better than all non-transformer", "tokens": [51016, 583, 309, 307, 920, 1101, 813, 439, 2107, 11, 309, 311, 10591, 1101, 813, 439, 2107, 12, 24999, 837, 260, 51216], "temperature": 0.0, "avg_logprob": -0.11570345271717418, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.002322013024240732}, {"id": 758, "seek": 386580, "start": 3882.84, "end": 3890.2000000000003, "text": " models. And it also has some other benefits. Like you can do language generation much faster", "tokens": [51216, 5245, 13, 400, 309, 611, 575, 512, 661, 5311, 13, 1743, 291, 393, 360, 2856, 5125, 709, 4663, 51584], "temperature": 0.0, "avg_logprob": -0.11570345271717418, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.002322013024240732}, {"id": 759, "seek": 386580, "start": 3890.2000000000003, "end": 3895.0, "text": " because of the fast recurrent view. That was the main point of this. But this also does", "tokens": [51584, 570, 295, 264, 2370, 18680, 1753, 1910, 13, 663, 390, 264, 2135, 935, 295, 341, 13, 583, 341, 611, 775, 51824], "temperature": 0.0, "avg_logprob": -0.11570345271717418, "compression_ratio": 1.674074074074074, "no_speech_prob": 0.002322013024240732}, {"id": 760, "seek": 389500, "start": 3895.0, "end": 3899.72, "text": " kind of point to the fact that personally my intuition is that transformers are really good", "tokens": [50364, 733, 295, 935, 281, 264, 1186, 300, 5665, 452, 24002, 307, 300, 4088, 433, 366, 534, 665, 50600], "temperature": 0.0, "avg_logprob": -0.12073690081955096, "compression_ratio": 1.6118881118881119, "no_speech_prob": 0.0003198581107426435}, {"id": 761, "seek": 389500, "start": 3899.72, "end": 3905.16, "text": " for dense and discrete data. Whereas S4 is really good for more like noisy and raw data.", "tokens": [50600, 337, 18011, 293, 27706, 1412, 13, 13813, 318, 19, 307, 534, 665, 337, 544, 411, 24518, 293, 8936, 1412, 13, 50872], "temperature": 0.0, "avg_logprob": -0.12073690081955096, "compression_ratio": 1.6118881118881119, "no_speech_prob": 0.0003198581107426435}, {"id": 762, "seek": 389500, "start": 3906.44, "end": 3911.72, "text": " Yeah. And I mean the speed up here I think is very interesting. Do you know what was the window", "tokens": [50936, 865, 13, 400, 286, 914, 264, 3073, 493, 510, 286, 519, 307, 588, 1880, 13, 1144, 291, 458, 437, 390, 264, 4910, 51200], "temperature": 0.0, "avg_logprob": -0.12073690081955096, "compression_ratio": 1.6118881118881119, "no_speech_prob": 0.0003198581107426435}, {"id": 763, "seek": 389500, "start": 3911.72, "end": 3917.32, "text": " you would take for language modeling? Like how many tokens or words rather did you consider?", "tokens": [51200, 291, 576, 747, 337, 2856, 15983, 30, 1743, 577, 867, 22667, 420, 2283, 2831, 630, 291, 1949, 30, 51480], "temperature": 0.0, "avg_logprob": -0.12073690081955096, "compression_ratio": 1.6118881118881119, "no_speech_prob": 0.0003198581107426435}, {"id": 764, "seek": 389500, "start": 3917.32, "end": 3922.28, "text": " Yeah. This experiment was done using a pretty standard length of either 512 or 1024 tokens.", "tokens": [51480, 865, 13, 639, 5120, 390, 1096, 1228, 257, 1238, 3832, 4641, 295, 2139, 1025, 4762, 420, 1266, 7911, 22667, 13, 51728], "temperature": 0.0, "avg_logprob": -0.12073690081955096, "compression_ratio": 1.6118881118881119, "no_speech_prob": 0.0003198581107426435}, {"id": 765, "seek": 392228, "start": 3923.1600000000003, "end": 3927.5600000000004, "text": " You actually can keep increasing the window length for S4, which only slows it down a little", "tokens": [50408, 509, 767, 393, 1066, 5662, 264, 4910, 4641, 337, 318, 19, 11, 597, 787, 35789, 309, 760, 257, 707, 50628], "temperature": 0.0, "avg_logprob": -0.15329337331046045, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0006658564670942724}, {"id": 766, "seek": 392228, "start": 3927.5600000000004, "end": 3932.44, "text": " bit and actually improves the performance a little bit as well. But I found that out after", "tokens": [50628, 857, 293, 767, 24771, 264, 3389, 257, 707, 857, 382, 731, 13, 583, 286, 1352, 300, 484, 934, 50872], "temperature": 0.0, "avg_logprob": -0.15329337331046045, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0006658564670942724}, {"id": 767, "seek": 392228, "start": 3932.44, "end": 3938.52, "text": " the fact and I didn't feel like retraining this. Okay. Cool. Thanks. So yeah, but the", "tokens": [50872, 264, 1186, 293, 286, 994, 380, 841, 411, 49356, 1760, 341, 13, 1033, 13, 8561, 13, 2561, 13, 407, 1338, 11, 457, 264, 51176], "temperature": 0.0, "avg_logprob": -0.15329337331046045, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0006658564670942724}, {"id": 768, "seek": 392228, "start": 3938.52, "end": 3941.96, "text": " sort of findings of this slide is the speed up, right? It's massive.", "tokens": [51176, 1333, 295, 16483, 295, 341, 4137, 307, 264, 3073, 493, 11, 558, 30, 467, 311, 5994, 13, 51348], "temperature": 0.0, "avg_logprob": -0.15329337331046045, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0006658564670942724}, {"id": 769, "seek": 392228, "start": 3943.0800000000004, "end": 3948.92, "text": " That was the point that we did this experiment for. But yeah, so there's a lot of the speed up.", "tokens": [51404, 663, 390, 264, 935, 300, 321, 630, 341, 5120, 337, 13, 583, 1338, 11, 370, 456, 311, 257, 688, 295, 264, 3073, 493, 13, 51696], "temperature": 0.0, "avg_logprob": -0.15329337331046045, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.0006658564670942724}, {"id": 770, "seek": 394892, "start": 3948.92, "end": 3952.6800000000003, "text": " In terms of the original question though, in terms of the raw performance of modeling the data,", "tokens": [50364, 682, 2115, 295, 264, 3380, 1168, 1673, 11, 294, 2115, 295, 264, 8936, 3389, 295, 15983, 264, 1412, 11, 50552], "temperature": 0.0, "avg_logprob": -0.1424401108647736, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0006761779659427702}, {"id": 771, "seek": 394892, "start": 3953.32, "end": 3957.48, "text": " transformers are currently doing a little bit better here. Cool. Thank you.", "tokens": [50584, 4088, 433, 366, 4362, 884, 257, 707, 857, 1101, 510, 13, 8561, 13, 1044, 291, 13, 50792], "temperature": 0.0, "avg_logprob": -0.1424401108647736, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0006761779659427702}, {"id": 772, "seek": 394892, "start": 3963.16, "end": 3964.6800000000003, "text": " All right. Is there any other questions?", "tokens": [51076, 1057, 558, 13, 1119, 456, 604, 661, 1651, 30, 51152], "temperature": 0.0, "avg_logprob": -0.1424401108647736, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0006761779659427702}, {"id": 773, "seek": 394892, "start": 3967.96, "end": 3975.0, "text": " Let's all give Albert a round of virtual applause. Thank you for the very comprehensive", "tokens": [51316, 961, 311, 439, 976, 20812, 257, 3098, 295, 6374, 9969, 13, 1044, 291, 337, 264, 588, 13914, 51668], "temperature": 0.0, "avg_logprob": -0.1424401108647736, "compression_ratio": 1.5075376884422111, "no_speech_prob": 0.0006761779659427702}, {"id": 774, "seek": 397500, "start": 3975.56, "end": 3983.08, "text": " presentation of state-based models. Thanks for having me. Thank you, Albert.", "tokens": [50392, 5860, 295, 1785, 12, 6032, 5245, 13, 2561, 337, 1419, 385, 13, 1044, 291, 11, 20812, 13, 50768], "temperature": 0.0, "avg_logprob": -0.24809195120123367, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.006866684649139643}, {"id": 775, "seek": 397500, "start": 3983.08, "end": 3987.48, "text": " Thank you. Thank you, everyone, for joining us. We will put up the recording of the video,", "tokens": [50768, 1044, 291, 13, 1044, 291, 11, 1518, 11, 337, 5549, 505, 13, 492, 486, 829, 493, 264, 6613, 295, 264, 960, 11, 50988], "temperature": 0.0, "avg_logprob": -0.24809195120123367, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.006866684649139643}, {"id": 776, "seek": 397500, "start": 3987.48, "end": 3992.28, "text": " the talk, later to our YouTube channel. And yeah, we'll see you at the same time next week.", "tokens": [50988, 264, 751, 11, 1780, 281, 527, 3088, 2269, 13, 400, 1338, 11, 321, 603, 536, 291, 412, 264, 912, 565, 958, 1243, 13, 51228], "temperature": 0.0, "avg_logprob": -0.24809195120123367, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.006866684649139643}, {"id": 777, "seek": 397500, "start": 3993.48, "end": 3994.84, "text": " Thank you. See you, guys.", "tokens": [51288, 1044, 291, 13, 3008, 291, 11, 1074, 13, 51356], "temperature": 0.0, "avg_logprob": -0.24809195120123367, "compression_ratio": 1.5405405405405406, "no_speech_prob": 0.006866684649139643}], "language": "en"}