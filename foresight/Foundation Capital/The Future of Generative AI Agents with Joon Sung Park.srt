1
00:00:00,000 --> 00:00:04,280
The number of users who use ChatGPT, that's incredible.

2
00:00:04,280 --> 00:00:07,920
But I think it's sort of worth asking ourselves,

3
00:00:07,920 --> 00:00:11,080
is that sort of quote unquote the killer applications

4
00:00:11,080 --> 00:00:12,520
that we were waiting for?

5
00:00:12,520 --> 00:00:16,280
ChatGPT does feel like a fairly simple wrapper

6
00:00:16,280 --> 00:00:19,320
around our sling model, because that's what it is.

7
00:00:19,320 --> 00:00:22,160
OpenAI has done fantastic things to make it safer

8
00:00:22,160 --> 00:00:23,880
and make it more useful by tuning,

9
00:00:23,880 --> 00:00:25,560
I think, what's really great.

10
00:00:25,560 --> 00:00:28,200
But I think it's worth asking if that is actually

11
00:00:28,200 --> 00:00:32,480
the killer application, why is it a killer application?

12
00:00:32,480 --> 00:00:33,960
And the answer might actually come out

13
00:00:33,960 --> 00:00:36,960
that maybe it actually isn't the killer application

14
00:00:36,960 --> 00:00:39,880
that we were waiting for, in which case,

15
00:00:39,880 --> 00:00:42,160
what is going to be the killer application?

16
00:00:42,160 --> 00:00:44,080
That's really going to add value

17
00:00:44,080 --> 00:00:45,800
in a much more generalizable way.

18
00:00:48,360 --> 00:00:50,720
Welcome to AI in the real world.

19
00:00:50,720 --> 00:00:51,920
I'm your host.

20
00:00:51,920 --> 00:00:53,280
My name is Joanne Chen,

21
00:00:53,280 --> 00:00:55,760
and I am a general partner at Foundation Capital.

22
00:00:55,760 --> 00:00:56,920
I work closely with startups

23
00:00:56,920 --> 00:00:59,160
that are reshaping business with AI.

24
00:00:59,160 --> 00:01:02,000
In this series, I'll be holding in-depth discussions

25
00:01:02,000 --> 00:01:03,800
with leading AI researchers.

26
00:01:03,800 --> 00:01:05,680
We'll explore how state-of-the-art AI models

27
00:01:05,680 --> 00:01:08,960
are being applied in real enterprises today.

28
00:01:08,960 --> 00:01:12,200
To kick things off, I'm excited to speak with June Zempark,

29
00:01:12,200 --> 00:01:15,120
a PhD student in computer science at Stanford.

30
00:01:15,120 --> 00:01:18,280
June works at the intersection of human-computer interaction

31
00:01:18,280 --> 00:01:20,080
and natural language processing.

32
00:01:20,080 --> 00:01:23,800
He is best known for his research on AI agents.

33
00:01:23,800 --> 00:01:27,040
We break down how AI is transforming agent design,

34
00:01:27,040 --> 00:01:29,520
share advice for builders working with these models,

35
00:01:29,520 --> 00:01:31,000
and unpack why we haven't yet found

36
00:01:31,000 --> 00:01:34,440
the perfect killer app for AI agents.

37
00:01:34,440 --> 00:01:35,800
Here's our conversation.

38
00:01:35,800 --> 00:01:37,280
How are you?

39
00:01:37,280 --> 00:01:38,120
Good, what about you?

40
00:01:38,120 --> 00:01:38,960
Great seeing you again.

41
00:01:38,960 --> 00:01:39,800
Good to see you again.

42
00:01:39,800 --> 00:01:40,720
It's been a while.

43
00:01:40,720 --> 00:01:43,040
Because the on-conference was last,

44
00:01:43,040 --> 00:01:44,640
I want to say, June.

45
00:01:44,640 --> 00:01:45,960
June?

46
00:01:45,960 --> 00:01:46,800
May or June?

47
00:01:46,800 --> 00:01:47,800
Yeah.

48
00:01:47,800 --> 00:01:50,120
Last June, May or June.

49
00:01:50,120 --> 00:01:51,640
Wow, time-wise.

50
00:01:51,640 --> 00:01:53,200
And the world has changed.

51
00:01:53,240 --> 00:01:57,240
I think that agents have finally made its way

52
00:01:57,240 --> 00:02:01,080
into real enterprises with real use cases.

53
00:02:01,080 --> 00:02:04,760
And it was not, back then, it was a lot of,

54
00:02:04,760 --> 00:02:07,920
like, what could this be, right?

55
00:02:07,920 --> 00:02:09,680
Thanks to you as some of your work,

56
00:02:09,680 --> 00:02:11,240
which is why I'm super excited

57
00:02:11,240 --> 00:02:14,320
to have this conversation together.

58
00:02:15,240 --> 00:02:16,240
Especially right now,

59
00:02:16,240 --> 00:02:21,240
since enterprises are sinking in a real way

60
00:02:22,240 --> 00:02:24,400
to adopt.

61
00:02:24,400 --> 00:02:28,040
So I thought, who can we chat with

62
00:02:28,040 --> 00:02:30,200
that would have a really interesting perspective?

63
00:02:30,200 --> 00:02:32,160
And that's why we reached out back to you.

64
00:02:32,160 --> 00:02:34,760
So I really appreciate the time.

65
00:02:34,760 --> 00:02:35,600
Of course.

66
00:02:35,600 --> 00:02:37,360
Thanks for having me.

67
00:02:37,360 --> 00:02:39,200
Do you mind maybe just to start

68
00:02:39,200 --> 00:02:43,520
giving us a quick rundown of what's happened,

69
00:02:43,520 --> 00:02:46,040
maybe some of the background that you have

70
00:02:46,040 --> 00:02:47,280
building this technology?

71
00:02:48,440 --> 00:02:49,640
Yeah.

72
00:02:49,640 --> 00:02:51,600
So let's see.

73
00:02:51,600 --> 00:02:54,360
So do you want me to just sort of speak about

74
00:02:54,360 --> 00:02:56,120
sort of what has happened in the past six months,

75
00:02:56,120 --> 00:02:59,080
or sort of what would be interesting for you?

76
00:02:59,080 --> 00:03:01,560
Just a brief overview of what you worked on

77
00:03:01,560 --> 00:03:04,440
and also what's happened in the last six months

78
00:03:04,440 --> 00:03:06,880
to a year in terms of evolution.

79
00:03:06,880 --> 00:03:07,720
Yeah.

80
00:03:07,720 --> 00:03:08,840
All right, that sounds good.

81
00:03:08,840 --> 00:03:09,680
Right.

82
00:03:09,680 --> 00:03:12,200
So I guess I'll do a quick intro.

83
00:03:12,200 --> 00:03:15,200
So I'm sort of, I'm a PhD student here,

84
00:03:15,200 --> 00:03:18,480
sort of working in the area of HCI and NLP.

85
00:03:18,520 --> 00:03:21,560
So as you know, sort of the work that we've done,

86
00:03:21,560 --> 00:03:23,560
I think the one that I'm sort of mainly known for

87
00:03:23,560 --> 00:03:25,520
is this paper called Generative Agents.

88
00:03:26,480 --> 00:03:29,440
And Generative Agents in particular was a project

89
00:03:29,440 --> 00:03:31,000
that I tried to ask,

90
00:03:31,000 --> 00:03:34,680
can we use our language models to create general agents

91
00:03:34,680 --> 00:03:37,040
that can populate a simulation world?

92
00:03:37,040 --> 00:03:37,880
Right?

93
00:03:37,880 --> 00:03:40,480
So if you play something like SimCity or Sims,

94
00:03:40,480 --> 00:03:42,760
can we actually create these NPC-like characters

95
00:03:42,760 --> 00:03:44,720
that would actually flood into the city

96
00:03:44,720 --> 00:03:46,800
and actually live like humans?

97
00:03:46,800 --> 00:03:49,080
And by definition, it is sort of everything

98
00:03:49,080 --> 00:03:52,200
from how they would wake up in the morning,

99
00:03:52,200 --> 00:03:55,360
talk to each other, form routines and relationships,

100
00:03:55,360 --> 00:03:58,320
all the way to creating basically communities

101
00:03:58,320 --> 00:04:00,280
and emerging social dynamics.

102
00:04:01,440 --> 00:04:03,480
And sort of my interest in this area

103
00:04:03,480 --> 00:04:05,400
really stems from this idea.

104
00:04:05,400 --> 00:04:07,600
So this is sort of what people at the intersection

105
00:04:07,600 --> 00:04:09,840
of human-computer interaction and natural language processing

106
00:04:09,840 --> 00:04:11,600
in which learning like to ask,

107
00:04:11,600 --> 00:04:14,640
which is we now have these really amazing models

108
00:04:14,680 --> 00:04:17,160
like our language models and foundation models,

109
00:04:17,160 --> 00:04:18,840
the question really becomes,

110
00:04:18,840 --> 00:04:20,400
what are you going to do with them?

111
00:04:20,400 --> 00:04:21,240
Right?

112
00:04:21,240 --> 00:04:22,960
These models are new and they're great

113
00:04:22,960 --> 00:04:25,200
and we think they have great capacity,

114
00:04:25,200 --> 00:04:27,840
but are they really going to enable us

115
00:04:27,840 --> 00:04:31,360
to do something that's quite new and unique?

116
00:04:31,360 --> 00:04:33,120
And that has been sort of the focal point

117
00:04:33,120 --> 00:04:35,320
for a lot of the research that I do.

118
00:04:35,320 --> 00:04:37,600
And ultimately the conversation that we got down

119
00:04:37,600 --> 00:04:39,640
towards this idea of,

120
00:04:39,640 --> 00:04:41,760
well, these models are trained and brought data

121
00:04:41,760 --> 00:04:45,680
like the web, Wikipedia and so forth.

122
00:04:45,680 --> 00:04:47,600
So they can actually be used to generate

123
00:04:47,600 --> 00:04:49,760
a lot of believable human behavior

124
00:04:49,760 --> 00:04:52,000
when you're given a very micro context.

125
00:04:52,000 --> 00:04:53,600
So can we actually piece this together

126
00:04:53,600 --> 00:04:55,200
to create human-like agents,

127
00:04:55,200 --> 00:04:58,000
which is something that AI more broadly

128
00:04:58,000 --> 00:05:00,120
has envisioned since its founding days.

129
00:05:01,160 --> 00:05:03,600
And we decided that this is the time to do that.

130
00:05:03,600 --> 00:05:06,800
And so that's how we got to where we are.

131
00:05:06,800 --> 00:05:08,800
So that's the generative agents.

132
00:05:08,800 --> 00:05:12,160
And this is the paper that was published in April last year.

133
00:05:12,160 --> 00:05:14,000
We put it on archive in April

134
00:05:14,000 --> 00:05:16,640
and was officially published November.

135
00:05:16,640 --> 00:05:21,640
Which is crazy how much the world has developed.

136
00:05:22,240 --> 00:05:26,520
I'm curious what initially motivated this topic for you?

137
00:05:26,520 --> 00:05:28,440
I'm sure you had lots of different options

138
00:05:28,440 --> 00:05:30,840
in terms of what to research and study.

139
00:05:30,840 --> 00:05:33,080
Why did you decide to focus on this?

140
00:05:34,640 --> 00:05:35,480
Yeah.

141
00:05:35,480 --> 00:05:39,160
So ultimately, it really was the question of

142
00:05:39,160 --> 00:05:41,120
what will large language models,

143
00:05:41,120 --> 00:05:44,280
these new models that are being trained,

144
00:05:44,280 --> 00:05:45,800
really going to enable us to do.

145
00:05:45,800 --> 00:05:48,840
And when I started my PhD was around like 2020.

146
00:05:48,840 --> 00:05:51,680
And that was when GPT-3 was just about to come out.

147
00:05:51,680 --> 00:05:54,200
During my first year, we wrote a paper called

148
00:05:54,200 --> 00:05:57,880
Foundation Models, which sort of made this observation

149
00:05:57,880 --> 00:06:00,280
that there's going to be this new wave of models

150
00:06:00,280 --> 00:06:01,480
that's going to come out,

151
00:06:01,480 --> 00:06:04,000
where we're not going to be training these models

152
00:06:04,000 --> 00:06:05,600
for a specific task,

153
00:06:05,600 --> 00:06:08,000
but rather we'll be training for a modality.

154
00:06:08,000 --> 00:06:10,280
We're going to be training this language model

155
00:06:10,280 --> 00:06:12,560
that can process language and so forth.

156
00:06:13,520 --> 00:06:17,600
And we thought that was going to be a big opportunity there

157
00:06:17,600 --> 00:06:19,320
in terms of what we can do with them.

158
00:06:19,320 --> 00:06:21,120
But the question of what are we going to do with them

159
00:06:21,120 --> 00:06:22,720
was incredibly unclear.

160
00:06:22,720 --> 00:06:26,640
So really our first instinct as sort of researchers

161
00:06:26,640 --> 00:06:29,600
and more machine learning in the NLP community,

162
00:06:29,600 --> 00:06:32,000
where we sort of were drawn to was this idea

163
00:06:32,000 --> 00:06:33,440
of can we do classifications?

164
00:06:33,440 --> 00:06:35,480
Or generations with these models?

165
00:06:35,480 --> 00:06:38,400
And seeing that these models could do that was really exciting

166
00:06:38,400 --> 00:06:40,120
because we didn't train these models to do that,

167
00:06:40,120 --> 00:06:41,640
but they could.

168
00:06:41,640 --> 00:06:44,240
But more from the interaction perspective,

169
00:06:44,240 --> 00:06:46,720
doing classification and simple generation

170
00:06:46,720 --> 00:06:48,600
was something that we already knew how to do.

171
00:06:48,600 --> 00:06:51,600
So that did them feel fundamentally new.

172
00:06:51,600 --> 00:06:53,920
So really the question again became

173
00:06:53,920 --> 00:06:56,840
what are we going to do that's going to be truly new

174
00:06:56,840 --> 00:06:59,880
and transformative in the sense of interaction?

175
00:07:00,880 --> 00:07:05,960
So that's what really drew us to look for these kind of ideas.

176
00:07:05,960 --> 00:07:07,880
And again, that's we thought

177
00:07:07,880 --> 00:07:11,880
simulating human behavior in general computational agents,

178
00:07:11,880 --> 00:07:14,200
that felt like a big problem because in part

179
00:07:14,200 --> 00:07:16,280
because it's something that, again,

180
00:07:16,280 --> 00:07:19,440
our community had wanted for many decades.

181
00:07:19,440 --> 00:07:22,480
It was sort of the idea that people in more

182
00:07:22,480 --> 00:07:25,560
the cognitive science field that really inspired

183
00:07:25,560 --> 00:07:29,040
the early AI research, like Alan Newell and Albert Sass.

184
00:07:29,200 --> 00:07:31,520
Simon, these folks were asking.

185
00:07:31,520 --> 00:07:35,720
And we were certainly inspired by those ideas.

186
00:07:35,720 --> 00:07:37,520
And of course, we thought it would be a lot of fun

187
00:07:37,520 --> 00:07:40,080
because we sort of grew up with sims, Pokemon

188
00:07:40,080 --> 00:07:44,400
and these kinds of games in the 90s and early 2000s.

189
00:07:44,400 --> 00:07:46,760
And we were certainly inspired by those games as well.

190
00:07:46,760 --> 00:07:48,520
I love those games as well.

191
00:07:48,520 --> 00:07:53,480
And it's nice to see some of that play out in the real world.

192
00:07:53,480 --> 00:07:56,440
I agree. I think games are fun in the sense that,

193
00:07:56,440 --> 00:07:58,760
you know, I think they are inspirational in many ways

194
00:07:58,760 --> 00:08:02,240
because they are very forward-looking in many ways, right?

195
00:08:02,240 --> 00:08:04,400
Because you can be a little bit more playful.

196
00:08:04,400 --> 00:08:07,080
And I think research can be in many ways playful,

197
00:08:07,080 --> 00:08:10,240
especially when you're trying to do really forward-looking research.

198
00:08:10,240 --> 00:08:12,680
So it certainly is a big inspiration.

199
00:08:12,680 --> 00:08:16,280
And I was just going to sort of end that comment by saying

200
00:08:16,280 --> 00:08:19,960
that I think it's worth asking for us as a community

201
00:08:19,960 --> 00:08:23,160
what's going to be the new sort of quote unquote

202
00:08:23,160 --> 00:08:26,880
care application of these models.

203
00:08:26,920 --> 00:08:29,480
In the sense that when we had personal computers

204
00:08:29,480 --> 00:08:32,960
in the early 80s and so forth,

205
00:08:32,960 --> 00:08:35,600
the computers were very cool.

206
00:08:35,600 --> 00:08:40,320
But what really made them into household applications

207
00:08:40,320 --> 00:08:41,840
were the existence of this,

208
00:08:41,840 --> 00:08:46,280
what we would now consider as killer application of PCs,

209
00:08:46,280 --> 00:08:47,680
like Microsoft Excel,

210
00:08:47,680 --> 00:08:52,400
that really made tabular information usable and scalable.

211
00:08:52,400 --> 00:08:54,400
I think we, Luris Language Model Community,

212
00:08:54,400 --> 00:08:57,280
or should also be looking for those kind of ideas as well,

213
00:08:57,280 --> 00:08:58,960
because that's going to be ultimately

214
00:08:58,960 --> 00:09:01,680
what's going to really transform the user experience

215
00:09:01,680 --> 00:09:02,760
around these models.

216
00:09:02,760 --> 00:09:06,920
And I think we're seeing some great usage of these models,

217
00:09:06,920 --> 00:09:10,200
but I think there's a lot more to do going forward.

218
00:09:10,200 --> 00:09:11,680
Makes a lot of sense.

219
00:09:11,680 --> 00:09:15,480
When you look at what's happened since April, right,

220
00:09:15,480 --> 00:09:17,160
a lot of things have changed.

221
00:09:17,160 --> 00:09:20,120
We have new LLM capabilities.

222
00:09:20,120 --> 00:09:24,760
We have a whole flurry of startups building in this space.

223
00:09:24,760 --> 00:09:28,400
Could you maybe summarize what you've seen?

224
00:09:28,400 --> 00:09:30,080
Right.

225
00:09:30,080 --> 00:09:31,200
So, right.

226
00:09:31,200 --> 00:09:33,640
So, Agent Sotini has been a big thing,

227
00:09:33,640 --> 00:09:38,200
especially first the latter half of 2023.

228
00:09:38,200 --> 00:09:40,480
This is how I'm seeing it.

229
00:09:40,480 --> 00:09:43,600
Agent Community, it's sort of the way I view it,

230
00:09:43,600 --> 00:09:47,200
has split into two communities, I would argue now.

231
00:09:47,200 --> 00:09:50,840
So, maybe it might actually make a little more sense

232
00:09:50,840 --> 00:09:53,280
to really talk about the history of agents,

233
00:09:53,280 --> 00:09:55,200
because agent became a big thing last year,

234
00:09:55,200 --> 00:09:58,640
but this is not a new idea in and out of itself.

235
00:09:58,640 --> 00:10:01,480
Right, even in the commercial space,

236
00:10:01,480 --> 00:10:04,200
we actually had agents like Microsoft Clippy,

237
00:10:04,200 --> 00:10:06,560
I'm not sure how many of us will actually remember that,

238
00:10:06,560 --> 00:10:10,960
but there used to be these agents in sort of our industry

239
00:10:10,960 --> 00:10:12,520
and in research.

240
00:10:12,520 --> 00:10:15,440
So, this is certainly not a new idea.

241
00:10:15,520 --> 00:10:18,360
So, if you go all the way back,

242
00:10:18,360 --> 00:10:21,080
so we had agents like Clippy,

243
00:10:21,080 --> 00:10:23,360
and in many ways these agents,

244
00:10:23,360 --> 00:10:24,920
especially in the reinforcement learning

245
00:10:24,920 --> 00:10:26,000
and machine learning community,

246
00:10:26,000 --> 00:10:29,720
agents were these elements that basically

247
00:10:29,720 --> 00:10:31,040
could simulate human behavior.

248
00:10:31,040 --> 00:10:34,240
I think that is ultimately sort of underlying thesis,

249
00:10:34,240 --> 00:10:37,320
but many of the agents were given tools

250
00:10:37,320 --> 00:10:39,360
to automate certain tasks.

251
00:10:39,360 --> 00:10:41,720
And the task it were meant to automate

252
00:10:41,720 --> 00:10:44,000
were tasks that are not simple, right?

253
00:10:44,000 --> 00:10:46,440
It's not something like you're running a for loop

254
00:10:46,440 --> 00:10:47,520
with your Python code,

255
00:10:47,520 --> 00:10:49,200
but it's a little bit more complex than that, right?

256
00:10:49,200 --> 00:10:52,520
It operates in much more embodied spaces

257
00:10:52,520 --> 00:10:56,080
or in spaces that we often operate in, right?

258
00:10:56,080 --> 00:10:58,520
The web, right?

259
00:10:58,520 --> 00:11:00,480
Can it, the simplest example

260
00:11:00,480 --> 00:11:02,280
with these kind of tool-based agents

261
00:11:02,280 --> 00:11:04,200
are can it order me pizza?

262
00:11:04,200 --> 00:11:06,520
Can it buy plane tickets?

263
00:11:06,520 --> 00:11:08,200
And those might sound simple,

264
00:11:08,200 --> 00:11:10,480
but we know from our experiences

265
00:11:10,480 --> 00:11:11,920
that even ordering pizza actually

266
00:11:11,920 --> 00:11:14,240
does require multiple steps, right?

267
00:11:14,240 --> 00:11:15,880
We need to travel to certain websites,

268
00:11:15,880 --> 00:11:17,520
we need to look through the menus,

269
00:11:17,520 --> 00:11:19,200
actually make the payment,

270
00:11:19,200 --> 00:11:22,800
and deal with sort of entering your address and so forth.

271
00:11:22,800 --> 00:11:24,920
So that was one genre of agents

272
00:11:24,920 --> 00:11:27,400
that already sort of existed for a long time,

273
00:11:27,400 --> 00:11:30,120
or I would say all genres of agents sort of existed,

274
00:11:30,120 --> 00:11:33,560
but that was one genre that was highlighted in the past.

275
00:11:33,560 --> 00:11:37,080
So you see things like Clippy is also in that genre as well.

276
00:11:37,080 --> 00:11:39,520
You're a Microsoft Office user,

277
00:11:39,520 --> 00:11:41,440
Clippy would try to automate some tasks for you

278
00:11:41,440 --> 00:11:44,720
based on your prior interaction with the software.

279
00:11:44,720 --> 00:11:49,720
Another set of agents was this idea of simulation agents,

280
00:11:50,880 --> 00:11:52,320
or agents that were created-

281
00:11:52,320 --> 00:11:54,080
To clarify on that point,

282
00:11:54,080 --> 00:11:57,720
those agents are single agents, correct?

283
00:11:57,720 --> 00:11:58,840
They can be single agents,

284
00:11:58,840 --> 00:12:03,400
they were often implemented as single agents, that's right.

285
00:12:03,400 --> 00:12:04,600
I don't think by definition

286
00:12:04,600 --> 00:12:06,280
they actually had to be single agents.

287
00:12:06,280 --> 00:12:08,600
So you'd actually try, you're now seeing,

288
00:12:08,600 --> 00:12:10,600
at least in the research,

289
00:12:10,600 --> 00:12:12,640
you're starting to see glimpse of people

290
00:12:12,640 --> 00:12:15,480
is trying to imagine what would it look like

291
00:12:15,480 --> 00:12:18,240
for these agents to be in a multi-agent setting.

292
00:12:18,240 --> 00:12:20,640
So research paper that I remember coming out

293
00:12:20,640 --> 00:12:23,920
after Generative Agents was basically,

294
00:12:23,920 --> 00:12:27,560
what if you have a company of agents, right?

295
00:12:27,560 --> 00:12:28,920
There's going to be a CEO,

296
00:12:28,920 --> 00:12:30,920
but there's also going to be a designer agent

297
00:12:30,920 --> 00:12:33,560
who works in some other aspects,

298
00:12:33,560 --> 00:12:36,560
there is going to be editor in this company,

299
00:12:36,560 --> 00:12:39,720
and those are still much within the literature

300
00:12:39,720 --> 00:12:42,680
of what I would call tool-based agents, right?

301
00:12:42,680 --> 00:12:47,360
They're trying to automate some complex tasks for the users.

302
00:12:47,360 --> 00:12:48,520
And I think there's going to be a lot of

303
00:12:48,520 --> 00:12:50,640
sort of really big opportunities in this space,

304
00:12:50,640 --> 00:12:52,080
that's something that people have been working on

305
00:12:52,080 --> 00:12:55,400
for a long time, for all the right reasons.

306
00:12:55,400 --> 00:13:00,400
Now, another community that has formed,

307
00:13:00,600 --> 00:13:04,520
but to some extent actually has a slightly different route,

308
00:13:04,520 --> 00:13:07,960
is agents that were created for simulations.

309
00:13:09,960 --> 00:13:12,920
And these agents were certainly a part of games, right?

310
00:13:12,920 --> 00:13:14,760
In the past we had Sims,

311
00:13:14,760 --> 00:13:17,440
but we also had these NPC characters

312
00:13:17,440 --> 00:13:19,200
that we could interact with.

313
00:13:19,200 --> 00:13:23,000
Now, those NPCs and agents back then were very much,

314
00:13:23,000 --> 00:13:26,400
it was simpler agents that were either rule-based,

315
00:13:26,400 --> 00:13:28,080
there were some reinforcement learning agents

316
00:13:28,080 --> 00:13:29,680
back then as well in that space.

317
00:13:31,040 --> 00:13:33,480
But another one that we could usually think about

318
00:13:33,480 --> 00:13:37,680
were agents that were used basically in social science,

319
00:13:37,720 --> 00:13:40,440
economic agents, or agents that would simulate

320
00:13:40,440 --> 00:13:42,760
our policy decision-making and so forth.

321
00:13:43,760 --> 00:13:46,960
And those agents were also a part of this literature.

322
00:13:48,040 --> 00:13:50,520
And what we're seeing today is,

323
00:13:50,520 --> 00:13:53,080
we're one recognizing that Lawrence Lynch model

324
00:13:53,080 --> 00:13:54,840
is simulating human behavior.

325
00:13:54,840 --> 00:13:56,520
So it touches on all these agents,

326
00:13:56,520 --> 00:13:59,800
that it can be a foundational sort of architectural layer

327
00:13:59,800 --> 00:14:02,240
for creating all these different sorts of agents.

328
00:14:02,240 --> 00:14:05,080
But in terms of our initial application spaces,

329
00:14:05,080 --> 00:14:06,280
we're seeing this split,

330
00:14:06,280 --> 00:14:07,360
where there's one community

331
00:14:07,360 --> 00:14:10,680
who's now deeply interested in agents using tools,

332
00:14:10,680 --> 00:14:13,280
but another community that is deeply interested

333
00:14:13,280 --> 00:14:15,800
in this idea of can we simulate?

334
00:14:15,800 --> 00:14:19,120
And this is where I would say like multi-agents

335
00:14:19,120 --> 00:14:20,600
and as well as personalization

336
00:14:20,600 --> 00:14:24,080
is really starting to be highlighted in the simulation space

337
00:14:24,080 --> 00:14:26,960
because it's a little bit more directly incorporated

338
00:14:26,960 --> 00:14:28,880
to the idea of simulations.

339
00:14:28,880 --> 00:14:30,280
Who are we simulating for?

340
00:14:30,280 --> 00:14:31,560
What are we simulating?

341
00:14:31,560 --> 00:14:33,200
Who are we simulating?

342
00:14:33,200 --> 00:14:35,360
And by definition, simulations often happen

343
00:14:35,440 --> 00:14:36,920
in this multi-agent space.

344
00:14:36,920 --> 00:14:40,560
So those are the two communities that you're starting to see.

345
00:14:40,560 --> 00:14:43,520
So generative agent certainly stands on the far end

346
00:14:43,520 --> 00:14:45,480
of the simulation-based agents,

347
00:14:45,480 --> 00:14:46,680
whereas some other projects

348
00:14:46,680 --> 00:14:48,520
that were also really cool last year,

349
00:14:48,520 --> 00:14:51,800
I think a lot of sort of open AI, GPTs, I would say,

350
00:14:51,800 --> 00:14:53,840
are another end of the simulation agents

351
00:14:53,840 --> 00:14:56,440
or another end of tool-based agents.

352
00:14:56,440 --> 00:14:59,760
So those are the axes that you're sort of seeing right now.

353
00:14:59,760 --> 00:15:00,840
And sort of end by saying,

354
00:15:00,840 --> 00:15:02,560
my hunch actually is again,

355
00:15:02,560 --> 00:15:05,760
because they all start from the same technical thesis

356
00:15:05,760 --> 00:15:07,720
that we can simulate human behavior,

357
00:15:07,720 --> 00:15:08,880
they will merge in the end.

358
00:15:08,880 --> 00:15:11,720
I don't think they will be completely separate thesis

359
00:15:11,720 --> 00:15:13,960
like five to 10 years down the line.

360
00:15:13,960 --> 00:15:15,680
It's more going to be the question of

361
00:15:15,680 --> 00:15:18,920
where are we going to make our short-term bets

362
00:15:18,920 --> 00:15:20,840
and what's going to be an interesting

363
00:15:20,840 --> 00:15:24,720
and meaningful application space in the next two to five years.

364
00:15:24,720 --> 00:15:26,120
So that's the field that I'm seeing

365
00:15:26,120 --> 00:15:28,680
and how it's developing right now.

366
00:15:28,680 --> 00:15:30,040
Before we maybe go into that,

367
00:15:30,040 --> 00:15:34,520
could you maybe describe how LLM specifically

368
00:15:34,520 --> 00:15:39,520
has affected the, especially the latter cohort, right?

369
00:15:39,520 --> 00:15:41,800
What is the before and what is the after?

370
00:15:41,800 --> 00:15:44,600
And what is the magnitude of improvement

371
00:15:44,600 --> 00:15:49,600
because of this technology that's now cheap enough to use?

372
00:15:49,600 --> 00:15:50,480
Right.

373
00:15:50,480 --> 00:15:54,480
So Lawrence-Lenge Motor is really what made this possible.

374
00:15:54,480 --> 00:15:58,080
That is really the fundamental fact that we needed.

375
00:15:58,080 --> 00:16:00,480
In the past, when you wanted to create,

376
00:16:00,480 --> 00:16:02,720
and this goes for both types of agents,

377
00:16:02,720 --> 00:16:04,520
tool-based and simulations,

378
00:16:04,520 --> 00:16:06,680
what you really needed was,

379
00:16:08,080 --> 00:16:10,080
you basically needed rule-based agents.

380
00:16:10,080 --> 00:16:11,520
That was the most common.

381
00:16:11,520 --> 00:16:14,320
And rule-based agents are sort of a more sophisticated way

382
00:16:14,320 --> 00:16:17,360
of saying we're scripting all the behaviors.

383
00:16:17,360 --> 00:16:21,280
So imagine you're building an NPC for a game.

384
00:16:21,280 --> 00:16:24,760
A human author would actually write every sentence

385
00:16:24,760 --> 00:16:27,640
that the agent would say to the user, for instance.

386
00:16:28,480 --> 00:16:32,960
Human author would actually describe in either code or language,

387
00:16:32,960 --> 00:16:34,920
if this happens, you do this.

388
00:16:34,920 --> 00:16:37,800
So you basically design all the possible behaviors.

389
00:16:38,880 --> 00:16:41,800
Now, that is expensive and not scalable, right?

390
00:16:41,800 --> 00:16:45,840
And that was the fundamental block that we had.

391
00:16:45,840 --> 00:16:48,360
Now, tool-based agents had similar issues

392
00:16:48,360 --> 00:16:51,720
that in many of the contexts it had to operate,

393
00:16:51,720 --> 00:16:53,800
it's not a very generalizable tool.

394
00:16:53,800 --> 00:16:56,920
So if you sort of see how clippy

395
00:16:56,920 --> 00:17:00,240
or even some of the agents that we're using today,

396
00:17:00,240 --> 00:17:01,920
very simple types of agents actually

397
00:17:01,920 --> 00:17:04,480
are already embedded into our daily usage.

398
00:17:04,480 --> 00:17:07,600
So you may have used Google spreadsheet or Google doc.

399
00:17:07,600 --> 00:17:11,960
It would auto-complete in some very rudimentary way

400
00:17:11,960 --> 00:17:13,880
that actually could be considered in some ways

401
00:17:13,880 --> 00:17:17,200
an agent in this direction of tool-based agents.

402
00:17:17,200 --> 00:17:20,280
And the rules they were using so far were very simple.

403
00:17:20,280 --> 00:17:21,800
It's not exactly rule-based,

404
00:17:21,800 --> 00:17:24,080
but it is something that was very much hard-coded

405
00:17:24,080 --> 00:17:25,680
into the agent's behavior.

406
00:17:25,680 --> 00:17:28,120
And there was some learning going on,

407
00:17:28,120 --> 00:17:32,640
but there were very strict or simple statistics

408
00:17:32,640 --> 00:17:34,200
that we were using.

409
00:17:34,200 --> 00:17:36,760
What large language model changes

410
00:17:36,760 --> 00:17:39,200
is large language model gives us a single ingredient,

411
00:17:39,200 --> 00:17:43,520
which is given a micro-context, micromoment,

412
00:17:43,520 --> 00:17:48,520
let's say I'm sitting in this room talking to Joanne

413
00:17:48,840 --> 00:17:51,200
and about let's say generative agents

414
00:17:51,200 --> 00:17:53,120
or simulations and so forth.

415
00:17:53,120 --> 00:17:56,320
Given that micromoment description,

416
00:17:56,320 --> 00:17:58,480
a language model is extremely good

417
00:17:58,480 --> 00:18:01,880
at predicting the next moment, right?

418
00:18:01,880 --> 00:18:05,240
So what are the reasonable set of things

419
00:18:05,240 --> 00:18:09,120
that June might say in this particular conversation

420
00:18:09,120 --> 00:18:10,840
given what he knows?

421
00:18:10,840 --> 00:18:12,240
It's very good at doing that.

422
00:18:13,840 --> 00:18:17,680
That on its own is not a perfect agent

423
00:18:17,680 --> 00:18:19,560
or it's not the complete ingredient

424
00:18:19,560 --> 00:18:22,080
that you need to create these agents

425
00:18:22,080 --> 00:18:25,400
that are meant to live for many, many years or decades.

426
00:18:25,400 --> 00:18:28,320
But they are the right ingredient

427
00:18:28,320 --> 00:18:30,000
or building block that we need it

428
00:18:30,000 --> 00:18:32,080
because that can be used to replace

429
00:18:32,080 --> 00:18:35,240
what was in the past, manual authoring.

430
00:18:36,240 --> 00:18:38,120
In the past, we had to manually author

431
00:18:38,120 --> 00:18:42,320
all the possible sequences given any micromoment,

432
00:18:42,320 --> 00:18:44,360
but large language model can come in.

433
00:18:45,360 --> 00:18:49,400
So given that ingredient, what we really could do

434
00:18:49,400 --> 00:18:53,160
is bake in long-term memory and some reflection module

435
00:18:53,160 --> 00:18:55,400
on top of it and planning module.

436
00:18:55,400 --> 00:18:57,840
So given the micro-ingredient

437
00:18:58,760 --> 00:19:02,800
plus an agent architecture that we give it on top of it,

438
00:19:02,800 --> 00:19:05,400
these agents can basically now start to function

439
00:19:05,400 --> 00:19:07,960
as something that can operate in that

440
00:19:07,960 --> 00:19:10,720
in a world that's much like ours

441
00:19:10,760 --> 00:19:14,720
with a fairly decent degree of long-term coherence.

442
00:19:14,720 --> 00:19:15,800
So that's where we are

443
00:19:15,800 --> 00:19:17,640
and that's really the difference it made.

444
00:19:17,640 --> 00:19:20,480
And I'd say this is sort of a zero to one difference,

445
00:19:20,480 --> 00:19:21,520
not a degree difference

446
00:19:21,520 --> 00:19:24,480
because before large language model, this was not possible.

447
00:19:25,840 --> 00:19:30,600
What else is, so we, large language models gave memory,

448
00:19:30,600 --> 00:19:34,840
gave context, gave interactions to these agents.

449
00:19:34,840 --> 00:19:39,240
What else in a perfect world would these agents have

450
00:19:39,240 --> 00:19:41,120
in order to better mimic the real world?

451
00:19:41,120 --> 00:19:44,040
Like what's maybe in the next stop, just out of curiosity?

452
00:19:45,160 --> 00:19:47,080
Right, so to clarify, large language model

453
00:19:47,080 --> 00:19:48,360
doesn't actually have,

454
00:19:48,360 --> 00:19:50,880
so large language model provides one element.

455
00:19:50,880 --> 00:19:53,320
It's the micro sort of a module

456
00:19:53,320 --> 00:19:55,440
for predicting the next sequence.

457
00:19:57,080 --> 00:19:58,560
It is the agent architecture

458
00:19:58,560 --> 00:20:02,600
that actually ends up giving the memory and planning ability,

459
00:20:02,600 --> 00:20:05,560
but those two pair becomes a fantastic combination.

460
00:20:06,320 --> 00:20:09,040
Now, going forward,

461
00:20:09,040 --> 00:20:12,120
what I do think is going to be interesting are,

462
00:20:12,120 --> 00:20:13,960
so right now we're using large language model,

463
00:20:13,960 --> 00:20:17,600
but we may have all noticed that things like chatGPT

464
00:20:17,600 --> 00:20:21,600
can now not only do it just language,

465
00:20:21,600 --> 00:20:24,480
but also other modality like image.

466
00:20:25,800 --> 00:20:29,000
I think that's going to be really interesting, right?

467
00:20:29,000 --> 00:20:32,160
So right now, let's say if you,

468
00:20:32,160 --> 00:20:34,240
and this is sort of based on my prior work

469
00:20:34,280 --> 00:20:35,480
called Generative Agents,

470
00:20:35,480 --> 00:20:37,520
and we had this game world like sim ad

471
00:20:37,520 --> 00:20:39,240
that we call a smallville,

472
00:20:39,240 --> 00:20:44,240
the way these agents perceived and operated in their world

473
00:20:44,400 --> 00:20:47,760
was basically by translating,

474
00:20:47,760 --> 00:20:49,760
and like our system translating

475
00:20:49,760 --> 00:20:52,440
the visual world into natural language.

476
00:20:52,440 --> 00:20:54,360
So we would tell the agent,

477
00:20:54,360 --> 00:20:56,120
you are in your apartment,

478
00:20:56,120 --> 00:21:00,400
or you are in the kitchen talking to someone.

479
00:21:00,400 --> 00:21:02,720
So we would actually take the visual world

480
00:21:03,720 --> 00:21:07,000
and use our system to translate the visual world

481
00:21:07,000 --> 00:21:08,440
into natural language,

482
00:21:08,440 --> 00:21:10,880
and then feeding it to the agent architecture

483
00:21:10,880 --> 00:21:13,760
that would use our language model to process this.

484
00:21:13,760 --> 00:21:17,200
But now with these models being able to deal

485
00:21:17,200 --> 00:21:20,360
with multi-modal aspect,

486
00:21:20,360 --> 00:21:23,480
we might actually be able to bypass that phase

487
00:21:23,480 --> 00:21:26,000
and go straight from here is the visual world

488
00:21:26,000 --> 00:21:28,720
or space that you're seeing right now.

489
00:21:28,720 --> 00:21:31,200
That is your memory, now act on it.

490
00:21:32,160 --> 00:21:34,760
I think that's going to be potentially very powerful

491
00:21:34,760 --> 00:21:39,480
because in part, image is much richer to some,

492
00:21:39,480 --> 00:21:40,600
it conveys a lot more.

493
00:21:40,600 --> 00:21:44,480
I do come from natural language processing background,

494
00:21:44,480 --> 00:21:48,560
at least that's my other half of my sort of academic background.

495
00:21:48,560 --> 00:21:51,200
So I have bias towards believing

496
00:21:51,200 --> 00:21:52,840
the natural language is profound,

497
00:21:52,840 --> 00:21:54,760
and I think that we're going to be,

498
00:21:54,760 --> 00:21:57,080
that will be the case going forward as well.

499
00:21:57,080 --> 00:21:59,000
But image does offer something

500
00:21:59,000 --> 00:22:01,920
that just language alone does not.

501
00:22:01,920 --> 00:22:04,000
So image is going to be a big thing.

502
00:22:04,000 --> 00:22:07,640
Now imagine the future video is going to be a big thing as well,

503
00:22:07,640 --> 00:22:10,680
then gradually the more these agents

504
00:22:10,680 --> 00:22:12,840
will basically increasingly get more powerful

505
00:22:12,840 --> 00:22:15,400
as this new modality gets piled on.

506
00:22:15,400 --> 00:22:18,320
So that's something that we should be looking forward to.

507
00:22:18,320 --> 00:22:19,560
That's great.

508
00:22:19,560 --> 00:22:21,000
What are some on the downside,

509
00:22:21,000 --> 00:22:22,920
what are some of the limitations

510
00:22:23,880 --> 00:22:27,560
that you're seeing in terms of these agents,

511
00:22:27,560 --> 00:22:29,080
especially generative agents?

512
00:22:30,240 --> 00:22:31,640
Right.

513
00:22:31,640 --> 00:22:36,640
So there are limitations that I can mention

514
00:22:36,640 --> 00:22:40,200
just about sort of in the context of our work.

515
00:22:40,200 --> 00:22:43,120
And then I think there are going to be interesting limitations

516
00:22:43,120 --> 00:22:46,280
that are much more application specific.

517
00:22:46,280 --> 00:22:48,520
So for generative agents today,

518
00:22:48,520 --> 00:22:50,960
certainly the technical limitation right now

519
00:22:50,960 --> 00:22:53,320
might have to do with things like,

520
00:22:53,320 --> 00:22:55,560
so you're using whether it's an open source.

521
00:22:55,560 --> 00:22:58,480
So right now we use OpenAI's model.

522
00:22:58,480 --> 00:23:01,120
OpenAI has actually done a lot of work

523
00:23:01,120 --> 00:23:03,040
to make the model safer.

524
00:23:03,040 --> 00:23:06,400
And I think OpenAI, I think that was the right approach

525
00:23:06,400 --> 00:23:08,400
in the sense that what they really wanted to create

526
00:23:08,400 --> 00:23:12,560
was these chatbots or agents or chatGPT

527
00:23:12,560 --> 00:23:16,080
that are a safe tool to use for most people.

528
00:23:17,000 --> 00:23:19,120
Now, if you want to run a simulation

529
00:23:19,120 --> 00:23:22,080
or create truly accurate and believable agents

530
00:23:22,080 --> 00:23:24,480
with something like chatGPT, however,

531
00:23:24,480 --> 00:23:26,920
that could become a limitation

532
00:23:26,920 --> 00:23:32,040
because what we really experience as humans

533
00:23:32,040 --> 00:23:34,280
is we fight, we sometimes have conflicts,

534
00:23:34,280 --> 00:23:36,320
we disagree with each other.

535
00:23:36,320 --> 00:23:38,320
And that might not be something that's something

536
00:23:38,320 --> 00:23:40,640
like chatGPT that's been fine-tuned

537
00:23:40,640 --> 00:23:43,280
to not behave that way to remain safe.

538
00:23:43,280 --> 00:23:45,160
It's something, it might not be something

539
00:23:45,160 --> 00:23:47,760
that these models would try to surface.

540
00:23:47,760 --> 00:23:51,800
And that could be a potential block

541
00:23:51,800 --> 00:23:55,600
in creating more accurate, more believable simulations

542
00:23:55,600 --> 00:23:57,760
or agents for that matter.

543
00:23:57,760 --> 00:23:59,880
So that's something that is one limitation

544
00:23:59,880 --> 00:24:01,360
right now that we're seeing.

545
00:24:02,880 --> 00:24:06,040
An interesting way to tackle this, I think, going forward

546
00:24:06,040 --> 00:24:08,840
is to use open source models or other models

547
00:24:08,840 --> 00:24:11,520
that have less of these fine-tuned nature.

548
00:24:13,240 --> 00:24:15,360
But it's going to be highly dependent

549
00:24:15,360 --> 00:24:17,600
on the models that we'll be using for this.

550
00:24:17,600 --> 00:24:20,480
So I think that's one thing to look forward to.

551
00:24:20,480 --> 00:24:24,120
Got it, got it, that's super helpful.

552
00:24:24,120 --> 00:24:27,480
And maybe one last question on the research side.

553
00:24:28,720 --> 00:24:31,120
When you think about future areas to explore

554
00:24:31,120 --> 00:24:35,320
for you specifically, what are some of the

555
00:24:38,160 --> 00:24:42,360
more narrow topics that you're hoping to dive deeper in?

556
00:24:43,440 --> 00:24:44,320
Given the world.

557
00:24:45,320 --> 00:24:50,160
So ultimately, I think making the agents more accurate

558
00:24:52,040 --> 00:24:55,320
for these agents to be more accurate reflection of who we are,

559
00:24:55,320 --> 00:24:57,640
I think it's going to be a really interesting research

560
00:24:57,640 --> 00:25:01,760
and I think it's going to, that's going to be an area

561
00:25:01,760 --> 00:25:04,600
that's going to have more of a research and broader impact.

562
00:25:05,960 --> 00:25:10,400
So right now, you may have seen the sort of simulation demo,

563
00:25:10,640 --> 00:25:14,640
the agents that live in that simulation are fictional,

564
00:25:14,640 --> 00:25:18,560
that we just, for instance, we have an agent named Isabella,

565
00:25:18,560 --> 00:25:21,480
we told Isabella that she is a cafe owner

566
00:25:21,480 --> 00:25:24,680
and large language model basically makes up

567
00:25:24,680 --> 00:25:29,320
what a persona that is reasonable given that description.

568
00:25:29,320 --> 00:25:31,720
But I think it's going to be far more interesting

569
00:25:31,720 --> 00:25:36,280
if we can make these simulations actually closely model

570
00:25:36,280 --> 00:25:38,840
our actual human communities.

571
00:25:38,840 --> 00:25:41,960
So it's not just fictional, but actually has groundings.

572
00:25:41,960 --> 00:25:44,640
That's going to open up from our perspective

573
00:25:44,640 --> 00:25:47,560
an entirely new set of application spaces

574
00:25:47,560 --> 00:25:49,120
as well as research impact.

575
00:25:50,320 --> 00:25:51,800
This can be used, for instance,

576
00:25:51,800 --> 00:25:54,760
to actually model or predict markets

577
00:25:54,760 --> 00:25:59,760
or it's going to be able to use two more closely personalized

578
00:26:00,200 --> 00:26:03,120
many of these agents for individual use cases.

579
00:26:03,120 --> 00:26:04,600
So that's something that we're looking forward to

580
00:26:04,600 --> 00:26:06,120
in terms of sort of a particular topic

581
00:26:06,120 --> 00:26:08,240
that we're diving into.

582
00:26:08,240 --> 00:26:10,320
That plug, of course, scaling up the agents.

583
00:26:10,320 --> 00:26:13,160
I think that's another big one, but those two.

584
00:26:13,160 --> 00:26:14,480
That makes sense.

585
00:26:14,480 --> 00:26:16,480
Now, one of the things that's missing

586
00:26:16,480 --> 00:26:21,480
in most AI technologies is like really the

587
00:26:22,440 --> 00:26:25,520
emotional part of how humans feel, right?

588
00:26:25,520 --> 00:26:29,240
Like all of that data is largely not captured

589
00:26:29,240 --> 00:26:32,640
and therefore not part of any kind of models today.

590
00:26:33,560 --> 00:26:36,720
Language is one small output of what we have.

591
00:26:36,720 --> 00:26:39,840
It's a very important output for sure,

592
00:26:39,840 --> 00:26:41,040
but it's still one small output.

593
00:26:41,040 --> 00:26:44,720
So I wonder how we might be able to incorporate

594
00:26:44,720 --> 00:26:47,220
some of the data around our emotions.

595
00:26:48,280 --> 00:26:49,200
I agree.

596
00:26:49,200 --> 00:26:50,580
Some thoughts in the future.

597
00:26:52,200 --> 00:26:54,720
Maybe let's move on to the applications today

598
00:26:54,720 --> 00:26:58,140
since you talked about some of the challenges for agents.

599
00:26:59,120 --> 00:27:02,480
Many organizations are thinking about

600
00:27:02,480 --> 00:27:04,200
how to use large language models today, right?

601
00:27:04,200 --> 00:27:06,960
There's a huge amount of aspirations.

602
00:27:06,960 --> 00:27:09,520
A subset of them are also thinking about

603
00:27:09,520 --> 00:27:12,360
what are some of the agent technology applications

604
00:27:12,360 --> 00:27:15,880
that are viable within an enterprise,

605
00:27:15,880 --> 00:27:18,520
which has limitations around infrastructure,

606
00:27:18,520 --> 00:27:22,240
around data silos, security, and all that stuff.

607
00:27:23,200 --> 00:27:25,640
Any particular areas where you've seen

608
00:27:26,920 --> 00:27:29,880
companies be successful at using these technologies

609
00:27:29,880 --> 00:27:30,860
in production?

610
00:27:31,800 --> 00:27:32,960
Right.

611
00:27:32,960 --> 00:27:34,560
So I think this is going to be incredibly

612
00:27:34,560 --> 00:27:38,360
like case-by-case answer.

613
00:27:38,360 --> 00:27:40,440
So let me think.

614
00:27:42,320 --> 00:27:45,040
Or if not, like any hypothesis as to

615
00:27:46,240 --> 00:27:48,600
where you might see the first

616
00:27:48,600 --> 00:27:50,680
commercial deployments at scale.

617
00:27:51,720 --> 00:27:52,560
Right.

618
00:27:55,240 --> 00:27:59,080
So there is something that I have.

619
00:27:59,080 --> 00:28:01,600
This is, there's a message that I have

620
00:28:01,600 --> 00:28:04,320
in trying to communicate, I think, in different settings.

621
00:28:04,320 --> 00:28:06,440
So this is not something I'm sort of

622
00:28:06,440 --> 00:28:07,840
conveying for the first time.

623
00:28:08,960 --> 00:28:12,280
And my opinion has been getting updated,

624
00:28:12,280 --> 00:28:14,400
but I think fundamentally I think this is right.

625
00:28:14,400 --> 00:28:17,040
So the way I've been describing it is

626
00:28:17,040 --> 00:28:22,040
in human computer interaction or in most task settings,

627
00:28:22,640 --> 00:28:24,200
there are two types of problems

628
00:28:24,200 --> 00:28:27,140
that we deploy our machines or agents in.

629
00:28:27,140 --> 00:28:30,480
One has very hard-edge problem spaces.

630
00:28:30,520 --> 00:28:33,000
These are things like, hey, order me pizza

631
00:28:33,000 --> 00:28:34,720
or buy me a plane ticket.

632
00:28:34,720 --> 00:28:38,360
These are tasks where there's a very concrete outcome

633
00:28:38,360 --> 00:28:41,160
and there often is a right or wrong answer

634
00:28:41,160 --> 00:28:43,640
that the agent has to achieve.

635
00:28:43,640 --> 00:28:45,680
At least from the user's perspective,

636
00:28:45,680 --> 00:28:48,960
there is something that would absolutely be yes or no.

637
00:28:50,240 --> 00:28:51,880
And then there are problem spaces

638
00:28:51,880 --> 00:28:54,800
where we have soft-edge problems.

639
00:28:54,800 --> 00:28:57,840
These are problems where we can increasingly

640
00:28:57,840 --> 00:29:00,520
co-climb towards sort of being better,

641
00:29:00,520 --> 00:29:03,800
but at certain level it starts to actually become useful.

642
00:29:03,800 --> 00:29:05,760
So to make this intuition a little bit,

643
00:29:05,760 --> 00:29:08,080
to make this a little bit more intuitive here,

644
00:29:08,080 --> 00:29:11,680
for instance, if I guess the worst possible case scenario

645
00:29:11,680 --> 00:29:13,920
is I asked the agent to buy me a plane ticket

646
00:29:13,920 --> 00:29:15,880
and it bought me the wrong ticket

647
00:29:15,880 --> 00:29:17,760
that just goes to a different place,

648
00:29:17,760 --> 00:29:19,600
then that's like a heavy no.

649
00:29:21,200 --> 00:29:25,280
Whereas let's say I asked the agent to sort of

650
00:29:26,280 --> 00:29:29,080
simulate a behavior that is sort of fun

651
00:29:29,080 --> 00:29:30,600
so that when I'm in a game,

652
00:29:30,600 --> 00:29:33,080
this is sort of entertaining and interesting.

653
00:29:33,080 --> 00:29:35,080
That might be something that the agent

654
00:29:35,080 --> 00:29:36,720
doesn't need to be quite perfect in,

655
00:29:36,720 --> 00:29:39,120
but it can still get there quite quickly.

656
00:29:39,120 --> 00:29:41,240
And then we can gradually improve.

657
00:29:41,240 --> 00:29:45,000
Those two are the spaces that we can sort of,

658
00:29:45,000 --> 00:29:47,520
in terms of when we consider where to deploy

659
00:29:47,520 --> 00:29:50,080
or how these will actually make its first impact,

660
00:29:50,080 --> 00:29:53,080
we might actually be looking at those problem spaces first.

661
00:29:53,080 --> 00:29:55,240
And these are the classes that I'm seeing.

662
00:29:55,240 --> 00:29:57,240
If I were to make my bet,

663
00:29:57,240 --> 00:30:00,200
agents will likely succeed first

664
00:30:00,200 --> 00:30:02,800
in the soft-edge problem spaces

665
00:30:02,800 --> 00:30:05,400
and will gradually inch into making it work

666
00:30:05,400 --> 00:30:07,480
in the hardest problem space.

667
00:30:07,480 --> 00:30:09,560
This has been sort of an intuition

668
00:30:09,560 --> 00:30:13,040
with agent research community for some time.

669
00:30:13,040 --> 00:30:15,440
So when Clippy, for instance, failed.

670
00:30:16,520 --> 00:30:19,680
Our intuition there, at least from research perspective,

671
00:30:19,680 --> 00:30:22,120
wasn't that these agents failed

672
00:30:22,120 --> 00:30:25,080
because we didn't have the technology there.

673
00:30:25,080 --> 00:30:26,320
Certainly these were deployed

674
00:30:26,320 --> 00:30:28,840
and there were some confidence around the technology.

675
00:30:28,840 --> 00:30:32,400
But the problem there actually was with interaction,

676
00:30:32,400 --> 00:30:36,520
that when agents are deployed in hard-edge problem spaces,

677
00:30:36,520 --> 00:30:39,040
it's often deployed in states where

678
00:30:39,960 --> 00:30:43,280
it actually has to have a fairly long chain of steps

679
00:30:43,280 --> 00:30:47,200
and fairly high, the risk were fairly high.

680
00:30:47,200 --> 00:30:49,920
When it fails, the cost of correcting its error

681
00:30:49,920 --> 00:30:50,840
is actually quite high,

682
00:30:50,840 --> 00:30:53,440
cost of auditing its error is actually quite high.

683
00:30:54,440 --> 00:30:56,040
So when these agents are deployed

684
00:30:56,040 --> 00:30:57,680
in hardest problem spaces,

685
00:30:57,680 --> 00:30:59,160
it has to reckon with the fact

686
00:30:59,160 --> 00:31:02,280
that it will undoubtedly make mistakes.

687
00:31:02,280 --> 00:31:04,000
And when it does make a mistake,

688
00:31:04,000 --> 00:31:05,920
it has to be increasingly auditable

689
00:31:05,920 --> 00:31:08,520
and controllable by the users

690
00:31:08,520 --> 00:31:11,600
so that the cost of correcting its error

691
00:31:11,600 --> 00:31:15,160
is not high enough that from the user's perspective,

692
00:31:15,160 --> 00:31:18,960
the cost-benefit analysis basically has to make sense.

693
00:31:18,960 --> 00:31:22,120
And that's been a fundamental challenge with agents.

694
00:31:22,240 --> 00:31:24,560
That's why in every era,

695
00:31:24,560 --> 00:31:28,240
we see the interest around agents spike for a while

696
00:31:28,240 --> 00:31:29,600
and then it quickly subsides

697
00:31:29,600 --> 00:31:32,320
after like maybe a half a year or two a year.

698
00:31:32,320 --> 00:31:35,440
Now there's a real issue now though,

699
00:31:35,440 --> 00:31:37,720
that given the large language model in the progress we saw

700
00:31:37,720 --> 00:31:39,720
that this might not be the case this time

701
00:31:39,720 --> 00:31:42,560
or at some point we might be able to make this work.

702
00:31:42,560 --> 00:31:46,240
But for now, so I'm closely monitoring this

703
00:31:46,240 --> 00:31:47,920
as well and I think we all should.

704
00:31:47,920 --> 00:31:49,400
I don't think we should just go and say,

705
00:31:49,400 --> 00:31:50,560
because it didn't work before,

706
00:31:50,560 --> 00:31:52,680
it's not going to work this time.

707
00:31:52,680 --> 00:31:57,360
But my hunch is that we will likely see

708
00:31:57,360 --> 00:31:59,640
very similar pattern arise,

709
00:31:59,640 --> 00:32:01,800
at least for the first of the future.

710
00:32:01,800 --> 00:32:05,000
And we haven't quite dealt with the interaction problems

711
00:32:05,000 --> 00:32:07,160
with those types of agents.

712
00:32:07,160 --> 00:32:10,280
So I think it's much safer to assume

713
00:32:10,280 --> 00:32:12,840
that it is going to be in the soft edge problems basis.

714
00:32:12,840 --> 00:32:15,920
And that's why in some areas, in many of the aspects,

715
00:32:15,920 --> 00:32:18,040
that's why our team was also interested

716
00:32:18,040 --> 00:32:19,640
in this idea of simulation.

717
00:32:19,640 --> 00:32:22,800
Because simulation is sort of the prime example

718
00:32:22,800 --> 00:32:24,800
of soft edge problem spaces,

719
00:32:24,800 --> 00:32:27,000
where the simulation has to be good enough

720
00:32:27,000 --> 00:32:29,040
for it to start being useful.

721
00:32:29,040 --> 00:32:32,480
That's also why I think a lot of really early promising AI

722
00:32:32,480 --> 00:32:35,360
startups that's going to go in the agent space

723
00:32:35,360 --> 00:32:39,080
are places that does NPCs for games.

724
00:32:39,080 --> 00:32:42,080
Because those are very safe soft edge problem spaces

725
00:32:42,080 --> 00:32:44,360
where the agents can fail, but that's okay.

726
00:32:45,320 --> 00:32:47,620
And gradually we'll sort of go to the other area as well.

727
00:32:47,620 --> 00:32:50,260
But I think that's where the impact is going to start

728
00:32:50,260 --> 00:32:51,660
in the next couple of years.

729
00:32:53,100 --> 00:32:58,100
I also think just seeing the startups in this space,

730
00:32:58,740 --> 00:33:03,740
in sectors and functions that allow for failure,

731
00:33:04,380 --> 00:33:08,420
like you said, include things like marketing.

732
00:33:09,260 --> 00:33:12,500
Where if you market incorrectly,

733
00:33:12,500 --> 00:33:14,780
it's not that big of a deal.

734
00:33:14,780 --> 00:33:19,260
If you write the wrong code,

735
00:33:19,260 --> 00:33:21,580
that's probably going to be a bigger deal.

736
00:33:21,580 --> 00:33:26,300
If you pick the wrong security features, that's a huge deal.

737
00:33:26,300 --> 00:33:28,260
If you pick the wrong things for healthcare,

738
00:33:28,260 --> 00:33:29,900
that's an even bigger deal.

739
00:33:29,900 --> 00:33:33,900
So there are degrees of fault tolerance

740
00:33:33,900 --> 00:33:35,940
within the enterprise.

741
00:33:35,940 --> 00:33:36,940
That's one thing.

742
00:33:36,940 --> 00:33:39,460
And the second on the consumer side,

743
00:33:39,460 --> 00:33:43,340
especially if the agents are just assisting consumers

744
00:33:43,340 --> 00:33:46,400
by not executing on anything.

745
00:33:47,660 --> 00:33:49,460
That could probably also work.

746
00:33:49,460 --> 00:33:52,140
For example, there's a company called Rewind,

747
00:33:52,140 --> 00:33:55,740
which is using some of the agent technologies, I believe.

748
00:33:55,740 --> 00:33:58,780
And they're getting a bunch of consumer demand,

749
00:33:58,780 --> 00:34:02,980
but what consumers are doing is just searching

750
00:34:02,980 --> 00:34:06,160
for a behavior that they have had before.

751
00:34:07,380 --> 00:34:11,420
And this product is helping them do that

752
00:34:11,460 --> 00:34:15,780
versus do anything real world, really.

753
00:34:15,780 --> 00:34:16,620
Interesting.

754
00:34:18,260 --> 00:34:19,420
But that's super.

755
00:34:19,420 --> 00:34:23,660
The way that you frame it is very useful.

756
00:34:24,500 --> 00:34:29,060
What about just from an architecture standpoint,

757
00:34:29,060 --> 00:34:33,740
large language models enabled by transformer architectures?

758
00:34:34,740 --> 00:34:36,740
This is a whole different direction.

759
00:34:36,740 --> 00:34:39,540
We're already seeing companies that are saying,

760
00:34:39,540 --> 00:34:42,180
hey, transformers are not the most efficient.

761
00:34:43,300 --> 00:34:44,900
Inference costs are very high.

762
00:34:46,020 --> 00:34:48,540
Let's look at the next thing.

763
00:34:48,540 --> 00:34:51,660
Have you spent much time thinking about that?

764
00:34:51,660 --> 00:34:55,260
And if so, any impact to the work that you're doing?

765
00:34:56,460 --> 00:34:57,300
Right.

766
00:34:57,300 --> 00:35:01,220
So certainly a next sort of model

767
00:35:01,220 --> 00:35:02,580
that we're going to be banking on,

768
00:35:02,580 --> 00:35:05,180
I think that always is an important topic.

769
00:35:05,180 --> 00:35:07,500
And that's something that I think we as a community

770
00:35:07,500 --> 00:35:09,100
always has to sort of monitor,

771
00:35:09,100 --> 00:35:10,700
because I think you're right,

772
00:35:10,700 --> 00:35:13,140
that transformer is not going to be the end model

773
00:35:13,140 --> 00:35:16,380
that will be hopefully, I mean, not on what.

774
00:35:16,380 --> 00:35:18,620
The hope here is that we wouldn't be using transformer

775
00:35:18,620 --> 00:35:19,700
10 years down the line.

776
00:35:21,180 --> 00:35:24,500
But one way that we do view this is,

777
00:35:24,500 --> 00:35:27,220
this is very much like a programmer's way

778
00:35:27,220 --> 00:35:28,060
of looking at this.

779
00:35:28,060 --> 00:35:30,380
We view this in abstractions, right?

780
00:35:30,380 --> 00:35:33,500
So what transformer has gotten us right now

781
00:35:33,500 --> 00:35:36,300
is this amazing capacity for reasoning

782
00:35:36,300 --> 00:35:39,540
and processing information and generating information.

783
00:35:40,380 --> 00:35:43,620
So it might be the case that in the future,

784
00:35:43,620 --> 00:35:46,580
that task will be done by even better models.

785
00:35:46,580 --> 00:35:48,940
And hopefully that's going to be the case.

786
00:35:48,940 --> 00:35:51,820
But for the sake of building applications,

787
00:35:51,820 --> 00:35:53,780
it is true that we can sort of view this

788
00:35:53,780 --> 00:35:55,620
as a layer of abstraction.

789
00:35:55,620 --> 00:35:57,820
That there might be some other technology

790
00:35:57,820 --> 00:36:00,620
that's going to be powering it in the future,

791
00:36:00,620 --> 00:36:02,140
but really what we're focusing on

792
00:36:02,140 --> 00:36:04,820
is the capacity and the modality.

793
00:36:04,820 --> 00:36:07,700
What kind of reasoning, using what modality,

794
00:36:07,700 --> 00:36:10,300
can these technologies that exist today do?

795
00:36:10,300 --> 00:36:12,460
And we're going to be building on top of it.

796
00:36:12,460 --> 00:36:16,100
So I think that's sort of our way of looking at it

797
00:36:16,100 --> 00:36:20,780
in sort of a medium term, again, the next three to five years.

798
00:36:20,780 --> 00:36:23,700
Now, if you're looking, because right now,

799
00:36:23,700 --> 00:36:25,540
there are some promising architectures

800
00:36:25,540 --> 00:36:29,620
that's sort of been created at the forefront of,

801
00:36:29,620 --> 00:36:31,340
I would say more in the machine learning

802
00:36:31,340 --> 00:36:33,660
and natural language processing communities

803
00:36:33,660 --> 00:36:37,580
that I'm personally getting a little bit excited about.

804
00:36:37,580 --> 00:36:40,340
But at the moment, those are still

805
00:36:40,340 --> 00:36:43,300
in the very much in the research phase.

806
00:36:43,300 --> 00:36:45,820
And can you share some examples of that?

807
00:36:45,820 --> 00:36:48,860
Yeah, so I think there's one model that recently came out,

808
00:36:48,860 --> 00:36:50,940
like Mamba by some folks.

809
00:36:50,940 --> 00:36:53,260
Those are from Stanford folks.

810
00:36:53,260 --> 00:36:57,020
So I think the authors now at CMU and Princeton

811
00:36:57,020 --> 00:36:59,900
sort of all within sort of this community.

812
00:36:59,900 --> 00:37:02,260
So that's one example of sort of a potentially promising

813
00:37:02,260 --> 00:37:03,180
or interesting model.

814
00:37:03,180 --> 00:37:04,940
And that's the one that I recently heard about

815
00:37:04,940 --> 00:37:08,940
that I think is interesting to be looking at.

816
00:37:08,940 --> 00:37:12,860
But these models, for them to be deployed at scale

817
00:37:12,860 --> 00:37:17,100
in a commercial way, if we decide to basically go

818
00:37:17,100 --> 00:37:19,820
with certain model that's getting created today,

819
00:37:19,820 --> 00:37:24,740
it will give us maybe two to five year timeline

820
00:37:24,740 --> 00:37:26,580
before they can really take off.

821
00:37:26,580 --> 00:37:32,660
Because Transformer is not, it is a relatively modern model,

822
00:37:32,660 --> 00:37:35,300
but it really is how you look at sort of the timeline.

823
00:37:35,300 --> 00:37:36,100
There's Transformer.

824
00:37:36,100 --> 00:37:37,220
Seven-ish years?

825
00:37:37,220 --> 00:37:39,140
Seven-ish years.

826
00:37:39,140 --> 00:37:42,020
So I think hopefully if we find something like this time,

827
00:37:42,020 --> 00:37:43,500
maybe it's going to go much faster.

828
00:37:43,500 --> 00:37:46,500
But it still took about seven-ish years for Chatchapiti

829
00:37:46,500 --> 00:37:47,740
to really come out.

830
00:37:47,740 --> 00:37:49,580
So it's not immediate.

831
00:37:49,580 --> 00:37:51,700
Whereas a lot of interactions that we can build,

832
00:37:51,700 --> 00:37:53,780
I think there's a lot that we can do like today

833
00:37:53,780 --> 00:37:55,940
to create really cool experiences.

834
00:37:55,940 --> 00:37:57,500
So I think that's how we're looking at this.

835
00:37:57,500 --> 00:37:58,780
There is a medium term.

836
00:37:58,780 --> 00:38:01,620
This is where we are focused on the level of extraction,

837
00:38:01,620 --> 00:38:04,020
that this is the capacity that we'll have.

838
00:38:04,020 --> 00:38:07,100
And then maybe in the down the line five to 10 year term,

839
00:38:07,100 --> 00:38:09,420
we can really be looking forward to some new models that's

840
00:38:09,420 --> 00:38:11,500
going to make an impact.

841
00:38:11,500 --> 00:38:12,020
That's great.

842
00:38:12,020 --> 00:38:13,980
That's great.

843
00:38:13,980 --> 00:38:14,460
Cool.

844
00:38:14,460 --> 00:38:16,140
Very cool.

845
00:38:16,140 --> 00:38:20,980
Maybe more generally, if you just zoom out for a moment,

846
00:38:20,980 --> 00:38:24,700
when you look at the ecosystem today,

847
00:38:24,700 --> 00:38:27,180
what are some of the problems that you want to see solve?

848
00:38:27,180 --> 00:38:28,860
We talked about multimodal a little bit.

849
00:38:28,860 --> 00:38:33,020
We talked about new models right after Transformers

850
00:38:33,020 --> 00:38:34,060
that might come out.

851
00:38:34,060 --> 00:38:37,420
What are some of the problems that you are most excited

852
00:38:37,420 --> 00:38:39,260
about someone solving?

853
00:38:39,260 --> 00:38:43,540
Not necessarily you personally, but someone solving.

854
00:38:43,540 --> 00:38:46,100
I sort of have two in mind.

855
00:38:46,100 --> 00:38:50,380
And it's a little bit less of a, this is a specific problem

856
00:38:50,380 --> 00:38:52,180
that I want to solve.

857
00:38:52,180 --> 00:38:54,980
But it's more sort of questions that I have

858
00:38:54,980 --> 00:38:58,740
that I think more of us should be thinking about.

859
00:38:58,740 --> 00:39:02,020
And to some extent, and this happens a lot with sort of the way

860
00:39:02,020 --> 00:39:04,900
I do my research as well, where I get

861
00:39:04,900 --> 00:39:08,820
inspired by big problems or foundational problems

862
00:39:08,820 --> 00:39:11,380
that we had in previous decades.

863
00:39:11,380 --> 00:39:14,020
Because oftentimes, there's a lot of insights

864
00:39:14,020 --> 00:39:18,700
that we can learn from the past as we build on the future.

865
00:39:18,700 --> 00:39:25,180
One, certainly I'm embedded into this agent space.

866
00:39:25,180 --> 00:39:31,540
One is, in the past, agents had its hype cycles, basically.

867
00:39:31,540 --> 00:39:34,260
But it failed.

868
00:39:34,260 --> 00:39:36,780
That the hype cycle lasted for a couple of years,

869
00:39:36,780 --> 00:39:39,100
and then people very quickly lost interest.

870
00:39:39,100 --> 00:39:41,340
Basically because it didn't quite deliver

871
00:39:41,340 --> 00:39:44,860
on the promises that it had.

872
00:39:44,860 --> 00:39:49,540
I think it's worth asking ourselves why that was the case.

873
00:39:49,540 --> 00:39:52,820
I think the opportunity this time is real.

874
00:39:52,820 --> 00:39:54,820
But I also think the opportunity in the past

875
00:39:54,860 --> 00:39:57,900
was also real to some aspect as well.

876
00:39:57,900 --> 00:40:00,100
So just because the opportunity is real

877
00:40:00,100 --> 00:40:02,340
and language model is really cool doesn't necessarily

878
00:40:02,340 --> 00:40:04,500
guarantee us, at least from my perspective,

879
00:40:04,500 --> 00:40:06,820
that we're going to, that agent will finally

880
00:40:06,820 --> 00:40:10,140
be a thing that everyone will use.

881
00:40:10,140 --> 00:40:13,580
I think there is a future where that will happen at some point.

882
00:40:13,580 --> 00:40:16,180
I think it might even happen this cycle.

883
00:40:16,180 --> 00:40:19,940
But I think it's really worth asking, as a community,

884
00:40:19,940 --> 00:40:23,220
why did it fail in the past so that we don't repeat those mistakes?

885
00:40:23,220 --> 00:40:25,660
One sort of main thing I'm sort of curious about

886
00:40:25,660 --> 00:40:27,980
that I don't think a lot of us are thinking about

887
00:40:27,980 --> 00:40:32,300
is actually not the technology part, but the interaction.

888
00:40:32,300 --> 00:40:36,020
How are these agents going to be used in what way?

889
00:40:36,020 --> 00:40:38,900
Because ultimately that's where it really delivers value

890
00:40:38,900 --> 00:40:40,660
to the end users.

891
00:40:40,660 --> 00:40:43,940
And that's where agents in the past have failed.

892
00:40:43,940 --> 00:40:45,300
That it was really cool technology,

893
00:40:45,300 --> 00:40:47,660
but we didn't seriously ask ourselves,

894
00:40:47,660 --> 00:40:49,620
is this something that people really need?

895
00:40:49,620 --> 00:40:51,580
And does the cost-benefit analysis

896
00:40:51,580 --> 00:40:54,620
of using these agents and learning how to use them well

897
00:40:54,620 --> 00:40:57,620
really make sense for the broader user base?

898
00:40:57,620 --> 00:40:59,340
So that's one.

899
00:40:59,340 --> 00:41:04,500
And other one is sort of my, it's a little bit of a hot take,

900
00:41:04,500 --> 00:41:06,380
but it's also a shorter take, which

901
00:41:06,380 --> 00:41:08,300
is we have large language models,

902
00:41:08,300 --> 00:41:12,020
and I think these have made a huge impact already.

903
00:41:12,020 --> 00:41:16,260
The number of users who use chat.gbt, that's incredible.

904
00:41:16,260 --> 00:41:19,940
But I think it's sort of worth asking ourselves,

905
00:41:19,940 --> 00:41:23,060
is that sort of quote unquote the killer applications

906
00:41:23,060 --> 00:41:25,140
that we were waiting for?

907
00:41:25,140 --> 00:41:29,980
Because in many ways, chat.gbt, or maybe it is.

908
00:41:29,980 --> 00:41:34,140
And I think if it is, I think somebody should articulate this.

909
00:41:34,140 --> 00:41:37,980
But chat.gbt does feel like a fairly simple wrapper

910
00:41:37,980 --> 00:41:41,020
around our language model, because that's what it is.

911
00:41:41,020 --> 00:41:44,700
And OpenAI has done fantastic things to make it safer

912
00:41:44,700 --> 00:41:46,700
and make it more useful by tuning, I think,

913
00:41:46,700 --> 00:41:48,140
what's really great.

914
00:41:48,180 --> 00:41:50,780
But I think it's worth asking, if that is actually

915
00:41:50,780 --> 00:41:55,020
the killer application, why is it a killer application?

916
00:41:55,020 --> 00:41:56,500
And the answer might actually come out

917
00:41:56,500 --> 00:41:59,540
that maybe it actually isn't the killer application

918
00:41:59,540 --> 00:42:02,780
that we were waiting for, in which case, what

919
00:42:02,780 --> 00:42:04,740
is going to be the killer application?

920
00:42:04,740 --> 00:42:07,140
That's really going to add value in a much more

921
00:42:07,140 --> 00:42:10,340
generalizable way.

922
00:42:10,340 --> 00:42:12,820
That's a very abstract question.

923
00:42:12,820 --> 00:42:14,900
For now, it's for me, it's just a hunch

924
00:42:14,900 --> 00:42:17,860
that I think there's something to be asked about there.

925
00:42:17,900 --> 00:42:20,140
And if I'm wrong, I would also love to hear again

926
00:42:20,140 --> 00:42:23,580
somebody really say, we already have this killer application,

927
00:42:23,580 --> 00:42:26,820
maybe it's co-pilot, chat.gbt, and here's why.

928
00:42:26,820 --> 00:42:31,820
But for now, this is a question that I'm still asking myself.

929
00:42:31,820 --> 00:42:33,060
That makes sense.

930
00:42:33,060 --> 00:42:35,060
And thank you for sharing that.

931
00:42:35,060 --> 00:42:40,660
What are some of your favorite AI apps today that you use?

932
00:42:40,660 --> 00:42:41,900
I love chat.gbt.

933
00:42:41,900 --> 00:42:43,300
I use it every day.

934
00:42:43,300 --> 00:42:46,700
Chat.gbt did make a difference in my workflow.

935
00:42:47,580 --> 00:42:51,220
So as a researcher, one of the main things I do

936
00:42:51,220 --> 00:42:54,220
is I program every day, or at least most days,

937
00:42:54,220 --> 00:42:56,340
or I write or write papers.

938
00:42:56,340 --> 00:42:58,300
So I do one of those two things.

939
00:42:58,300 --> 00:43:00,740
Chat.gbt is fantastic at both.

940
00:43:00,740 --> 00:43:05,580
So as all programmers sort of know,

941
00:43:05,580 --> 00:43:07,180
we sometimes don't bother remembering

942
00:43:07,180 --> 00:43:10,860
all the different functions or documentation.

943
00:43:10,860 --> 00:43:12,820
It's very good at generating a lot of the code

944
00:43:12,820 --> 00:43:14,500
when I have an idea.

945
00:43:14,500 --> 00:43:16,060
Really impressive.

946
00:43:16,060 --> 00:43:17,780
It's also quite a good editor.

947
00:43:17,780 --> 00:43:21,460
So if I make grammar error in my sort of sentences,

948
00:43:21,460 --> 00:43:24,140
chat.gbt will usually catch them for me.

949
00:43:24,140 --> 00:43:27,140
It's simple and easy thing, but it's good enough now

950
00:43:27,140 --> 00:43:30,980
that it's actually making a difference in the workflow.

951
00:43:30,980 --> 00:43:34,300
So as a chat.gbt, for sure, by extension, I think co-pilot

952
00:43:34,300 --> 00:43:36,980
will make a difference.

953
00:43:36,980 --> 00:43:38,860
So it's sort of worth asking, maybe going back

954
00:43:38,860 --> 00:43:41,180
to the question around killer application.

955
00:43:41,180 --> 00:43:43,180
What is the definition of killer application?

956
00:43:43,180 --> 00:43:45,780
I think it does some people define it

957
00:43:45,780 --> 00:43:48,340
as application that has more users.

958
00:43:48,340 --> 00:43:50,700
And the fact of that, I think, always has to be the case,

959
00:43:50,700 --> 00:43:52,940
that no killer application has no user.

960
00:43:52,940 --> 00:43:56,180
Killer application, by default, means the application

961
00:43:56,180 --> 00:43:59,740
that will have the most number of users.

962
00:43:59,740 --> 00:44:02,460
But I think there is more theoretical definition

963
00:44:02,460 --> 00:44:04,780
to what a killer application is.

964
00:44:04,780 --> 00:44:07,980
That implies a lot of users who are the most number of users.

965
00:44:07,980 --> 00:44:12,100
But for instance, if we look back to the prior era of PC,

966
00:44:12,100 --> 00:44:13,780
my killer application that I mentioned

967
00:44:13,780 --> 00:44:18,060
was something like Microsoft's Excel or this tabular data

968
00:44:18,060 --> 00:44:21,580
format, the thing that would let us manipulate the tabular

969
00:44:21,580 --> 00:44:22,900
data.

970
00:44:22,900 --> 00:44:25,740
So really, the definition, in a more theoretical sense

971
00:44:25,740 --> 00:44:27,820
of killer application here, is there

972
00:44:27,820 --> 00:44:30,140
is new technology stack that is being developed.

973
00:44:30,140 --> 00:44:33,060
There is a new file type that is getting generated.

974
00:44:33,060 --> 00:44:36,460
Then the killer application is the one that would let us

975
00:44:36,460 --> 00:44:39,100
manipulate the file application, file type.

976
00:44:39,100 --> 00:44:40,700
That's one theoretical definition

977
00:44:40,700 --> 00:44:43,020
that one could give, at least that's

978
00:44:43,020 --> 00:44:44,980
sort of the definition that I've been towing with.

979
00:44:44,980 --> 00:44:46,380
I think it's an interesting one.

980
00:44:46,380 --> 00:44:47,900
I don't think it's the only one.

981
00:44:47,900 --> 00:44:50,700
But those are sort of ways that I'm looking at this.

982
00:44:50,700 --> 00:44:52,060
That makes a lot of sense.

983
00:44:52,060 --> 00:44:54,620
I also use chatGPT every single day.

984
00:44:54,620 --> 00:44:56,580
It's been very helpful.

985
00:44:56,580 --> 00:44:59,500
Everything from coming up with menu names

986
00:44:59,500 --> 00:45:05,740
to rewriting emails that don't sound as nice.

987
00:45:05,740 --> 00:45:09,660
And I've tried a little bit to give it files and images.

988
00:45:09,660 --> 00:45:12,460
So I actually helped my mother create a background.

989
00:45:12,460 --> 00:45:14,260
She's a dancer, so she was performing

990
00:45:14,260 --> 00:45:16,380
on one of the very specific background for her dance.

991
00:45:16,380 --> 00:45:19,140
And I created that for her using chatGPT.

992
00:45:19,140 --> 00:45:21,420
So all sorts of utility there.

993
00:45:21,420 --> 00:45:24,220
But I love the way that you framed

994
00:45:24,220 --> 00:45:26,140
the last potential application.

995
00:45:26,140 --> 00:45:29,140
Maybe just one last question from my end.

996
00:45:29,140 --> 00:45:39,300
Any resources or books that you love that is on this topic?

997
00:45:40,300 --> 00:45:41,300
Right.

998
00:45:45,300 --> 00:45:48,580
I do think, and this is often the case

999
00:45:48,580 --> 00:45:51,100
with many of the cutting edge spaces.

1000
00:45:51,100 --> 00:45:53,340
I think a lot of the papers that are coming out

1001
00:45:53,340 --> 00:45:55,140
that are gaining a lot of attention,

1002
00:45:55,140 --> 00:45:56,740
I think those are sort of worth checking out

1003
00:45:56,740 --> 00:45:57,940
as sort of the resources.

1004
00:45:57,940 --> 00:46:01,540
It's not exactly like here's one book that we can all look at.

1005
00:46:01,540 --> 00:46:04,940
But things are moving fast enough that I think

1006
00:46:04,940 --> 00:46:07,140
those are sort of interesting resources

1007
00:46:07,140 --> 00:46:09,740
or just things that are getting created today

1008
00:46:09,740 --> 00:46:11,540
and their documentations.

1009
00:46:11,540 --> 00:46:15,140
So those are sort of mentioned as sort of a generic answer.

1010
00:46:15,140 --> 00:46:18,540
I do think, I think this has been a sort of running thing

1011
00:46:18,540 --> 00:46:20,540
in some of the things I mentioned today,

1012
00:46:20,540 --> 00:46:28,540
I get inspired by insights that basically had an impact

1013
00:46:28,540 --> 00:46:31,140
that stood the test of time.

1014
00:46:31,140 --> 00:46:33,540
And the reason why that is the case

1015
00:46:33,540 --> 00:46:36,540
is because I personally think all the great ideas

1016
00:46:36,540 --> 00:46:38,540
are sort of timeless.

1017
00:46:38,540 --> 00:46:41,540
It's because current time cycle is over,

1018
00:46:41,540 --> 00:46:43,540
it doesn't mean they're less interesting or less meaningful.

1019
00:46:43,540 --> 00:46:44,540
For sure.

1020
00:46:44,540 --> 00:46:48,540
That foundational ideas that will continue to have impact.

1021
00:46:48,540 --> 00:46:50,540
So when I look for resources,

1022
00:46:50,540 --> 00:46:55,540
I actually look back to books from truly the prior generations.

1023
00:46:55,540 --> 00:46:58,540
So some of the works that I often go back to

1024
00:46:58,540 --> 00:47:01,540
are works by Herbert Simon, Alan G.

1025
00:47:01,540 --> 00:47:04,540
Those are founders of AI and many of these fields

1026
00:47:04,540 --> 00:47:06,540
who would later go into a turning award

1027
00:47:06,540 --> 00:47:08,540
and Nobel Prize and so forth.

1028
00:47:08,540 --> 00:47:11,540
And those works, early cognitive psychologists

1029
00:47:11,540 --> 00:47:16,540
and scientists inspired my work a lot and their textbook.

1030
00:47:16,540 --> 00:47:18,540
Those people actually have written books

1031
00:47:18,540 --> 00:47:20,540
because they were much more established

1032
00:47:20,540 --> 00:47:22,540
than sort of the cutting edge spaces today.

1033
00:47:22,540 --> 00:47:26,540
So I go back to those as sort of my personal resources

1034
00:47:26,540 --> 00:47:28,540
for getting ideas.

1035
00:47:28,540 --> 00:47:29,540
That's great.

1036
00:47:29,540 --> 00:47:30,540
Thank you so much.

1037
00:47:30,540 --> 00:47:32,540
This was super, super helpful to me personally

1038
00:47:32,540 --> 00:47:34,540
because what we do as investors

1039
00:47:34,540 --> 00:47:39,540
is we try to understand the impacts of technology

1040
00:47:39,540 --> 00:47:44,540
and start to invest in companies when it becomes,

1041
00:47:44,540 --> 00:47:47,540
at the beginning of when it becomes commercially viable.

1042
00:47:47,540 --> 00:47:50,540
So to your point around what are the problem spaces,

1043
00:47:50,540 --> 00:47:53,540
what are the applications in which this can be applied

1044
00:47:53,540 --> 00:47:56,540
in a cost-effective and secure way

1045
00:47:56,540 --> 00:48:01,540
that where the end user is willing to interact

1046
00:48:01,540 --> 00:48:03,540
and get value,

1047
00:48:03,540 --> 00:48:07,540
that's when we start to come in and invest in these companies

1048
00:48:07,540 --> 00:48:11,540
which will hopefully be much bigger companies in the future.

1049
00:48:11,540 --> 00:48:13,540
So really appreciate this chat.

1050
00:48:13,540 --> 00:48:15,540
Yeah, it was fun.

