start	end	text
0	4280	The number of users who use ChatGPT, that's incredible.
4280	7920	But I think it's sort of worth asking ourselves,
7920	11080	is that sort of quote unquote the killer applications
11080	12520	that we were waiting for?
12520	16280	ChatGPT does feel like a fairly simple wrapper
16280	19320	around our sling model, because that's what it is.
19320	22160	OpenAI has done fantastic things to make it safer
22160	23880	and make it more useful by tuning,
23880	25560	I think, what's really great.
25560	28200	But I think it's worth asking if that is actually
28200	32480	the killer application, why is it a killer application?
32480	33960	And the answer might actually come out
33960	36960	that maybe it actually isn't the killer application
36960	39880	that we were waiting for, in which case,
39880	42160	what is going to be the killer application?
42160	44080	That's really going to add value
44080	45800	in a much more generalizable way.
48360	50720	Welcome to AI in the real world.
50720	51920	I'm your host.
51920	53280	My name is Joanne Chen,
53280	55760	and I am a general partner at Foundation Capital.
55760	56920	I work closely with startups
56920	59160	that are reshaping business with AI.
59160	62000	In this series, I'll be holding in-depth discussions
62000	63800	with leading AI researchers.
63800	65680	We'll explore how state-of-the-art AI models
65680	68960	are being applied in real enterprises today.
68960	72200	To kick things off, I'm excited to speak with June Zempark,
72200	75120	a PhD student in computer science at Stanford.
75120	78280	June works at the intersection of human-computer interaction
78280	80080	and natural language processing.
80080	83800	He is best known for his research on AI agents.
83800	87040	We break down how AI is transforming agent design,
87040	89520	share advice for builders working with these models,
89520	91000	and unpack why we haven't yet found
91000	94440	the perfect killer app for AI agents.
94440	95800	Here's our conversation.
95800	97280	How are you?
97280	98120	Good, what about you?
98120	98960	Great seeing you again.
98960	99800	Good to see you again.
99800	100720	It's been a while.
100720	103040	Because the on-conference was last,
103040	104640	I want to say, June.
104640	105960	June?
105960	106800	May or June?
106800	107800	Yeah.
107800	110120	Last June, May or June.
110120	111640	Wow, time-wise.
111640	113200	And the world has changed.
113240	117240	I think that agents have finally made its way
117240	121080	into real enterprises with real use cases.
121080	124760	And it was not, back then, it was a lot of,
124760	127920	like, what could this be, right?
127920	129680	Thanks to you as some of your work,
129680	131240	which is why I'm super excited
131240	134320	to have this conversation together.
135240	136240	Especially right now,
136240	141240	since enterprises are sinking in a real way
142240	144400	to adopt.
144400	148040	So I thought, who can we chat with
148040	150200	that would have a really interesting perspective?
150200	152160	And that's why we reached out back to you.
152160	154760	So I really appreciate the time.
154760	155600	Of course.
155600	157360	Thanks for having me.
157360	159200	Do you mind maybe just to start
159200	163520	giving us a quick rundown of what's happened,
163520	166040	maybe some of the background that you have
166040	167280	building this technology?
168440	169640	Yeah.
169640	171600	So let's see.
171600	174360	So do you want me to just sort of speak about
174360	176120	sort of what has happened in the past six months,
176120	179080	or sort of what would be interesting for you?
179080	181560	Just a brief overview of what you worked on
181560	184440	and also what's happened in the last six months
184440	186880	to a year in terms of evolution.
186880	187720	Yeah.
187720	188840	All right, that sounds good.
188840	189680	Right.
189680	192200	So I guess I'll do a quick intro.
192200	195200	So I'm sort of, I'm a PhD student here,
195200	198480	sort of working in the area of HCI and NLP.
198520	201560	So as you know, sort of the work that we've done,
201560	203560	I think the one that I'm sort of mainly known for
203560	205520	is this paper called Generative Agents.
206480	209440	And Generative Agents in particular was a project
209440	211000	that I tried to ask,
211000	214680	can we use our language models to create general agents
214680	217040	that can populate a simulation world?
217040	217880	Right?
217880	220480	So if you play something like SimCity or Sims,
220480	222760	can we actually create these NPC-like characters
222760	224720	that would actually flood into the city
224720	226800	and actually live like humans?
226800	229080	And by definition, it is sort of everything
229080	232200	from how they would wake up in the morning,
232200	235360	talk to each other, form routines and relationships,
235360	238320	all the way to creating basically communities
238320	240280	and emerging social dynamics.
241440	243480	And sort of my interest in this area
243480	245400	really stems from this idea.
245400	247600	So this is sort of what people at the intersection
247600	249840	of human-computer interaction and natural language processing
249840	251600	in which learning like to ask,
251600	254640	which is we now have these really amazing models
254680	257160	like our language models and foundation models,
257160	258840	the question really becomes,
258840	260400	what are you going to do with them?
260400	261240	Right?
261240	262960	These models are new and they're great
262960	265200	and we think they have great capacity,
265200	267840	but are they really going to enable us
267840	271360	to do something that's quite new and unique?
271360	273120	And that has been sort of the focal point
273120	275320	for a lot of the research that I do.
275320	277600	And ultimately the conversation that we got down
277600	279640	towards this idea of,
279640	281760	well, these models are trained and brought data
281760	285680	like the web, Wikipedia and so forth.
285680	287600	So they can actually be used to generate
287600	289760	a lot of believable human behavior
289760	292000	when you're given a very micro context.
292000	293600	So can we actually piece this together
293600	295200	to create human-like agents,
295200	298000	which is something that AI more broadly
298000	300120	has envisioned since its founding days.
301160	303600	And we decided that this is the time to do that.
303600	306800	And so that's how we got to where we are.
306800	308800	So that's the generative agents.
308800	312160	And this is the paper that was published in April last year.
312160	314000	We put it on archive in April
314000	316640	and was officially published November.
316640	321640	Which is crazy how much the world has developed.
322240	326520	I'm curious what initially motivated this topic for you?
326520	328440	I'm sure you had lots of different options
328440	330840	in terms of what to research and study.
330840	333080	Why did you decide to focus on this?
334640	335480	Yeah.
335480	339160	So ultimately, it really was the question of
339160	341120	what will large language models,
341120	344280	these new models that are being trained,
344280	345800	really going to enable us to do.
345800	348840	And when I started my PhD was around like 2020.
348840	351680	And that was when GPT-3 was just about to come out.
351680	354200	During my first year, we wrote a paper called
354200	357880	Foundation Models, which sort of made this observation
357880	360280	that there's going to be this new wave of models
360280	361480	that's going to come out,
361480	364000	where we're not going to be training these models
364000	365600	for a specific task,
365600	368000	but rather we'll be training for a modality.
368000	370280	We're going to be training this language model
370280	372560	that can process language and so forth.
373520	377600	And we thought that was going to be a big opportunity there
377600	379320	in terms of what we can do with them.
379320	381120	But the question of what are we going to do with them
381120	382720	was incredibly unclear.
382720	386640	So really our first instinct as sort of researchers
386640	389600	and more machine learning in the NLP community,
389600	392000	where we sort of were drawn to was this idea
392000	393440	of can we do classifications?
393440	395480	Or generations with these models?
395480	398400	And seeing that these models could do that was really exciting
398400	400120	because we didn't train these models to do that,
400120	401640	but they could.
401640	404240	But more from the interaction perspective,
404240	406720	doing classification and simple generation
406720	408600	was something that we already knew how to do.
408600	411600	So that did them feel fundamentally new.
411600	413920	So really the question again became
413920	416840	what are we going to do that's going to be truly new
416840	419880	and transformative in the sense of interaction?
420880	425960	So that's what really drew us to look for these kind of ideas.
425960	427880	And again, that's we thought
427880	431880	simulating human behavior in general computational agents,
431880	434200	that felt like a big problem because in part
434200	436280	because it's something that, again,
436280	439440	our community had wanted for many decades.
439440	442480	It was sort of the idea that people in more
442480	445560	the cognitive science field that really inspired
445560	449040	the early AI research, like Alan Newell and Albert Sass.
449200	451520	Simon, these folks were asking.
451520	455720	And we were certainly inspired by those ideas.
455720	457520	And of course, we thought it would be a lot of fun
457520	460080	because we sort of grew up with sims, Pokemon
460080	464400	and these kinds of games in the 90s and early 2000s.
464400	466760	And we were certainly inspired by those games as well.
466760	468520	I love those games as well.
468520	473480	And it's nice to see some of that play out in the real world.
473480	476440	I agree. I think games are fun in the sense that,
476440	478760	you know, I think they are inspirational in many ways
478760	482240	because they are very forward-looking in many ways, right?
482240	484400	Because you can be a little bit more playful.
484400	487080	And I think research can be in many ways playful,
487080	490240	especially when you're trying to do really forward-looking research.
490240	492680	So it certainly is a big inspiration.
492680	496280	And I was just going to sort of end that comment by saying
496280	499960	that I think it's worth asking for us as a community
499960	503160	what's going to be the new sort of quote unquote
503160	506880	care application of these models.
506920	509480	In the sense that when we had personal computers
509480	512960	in the early 80s and so forth,
512960	515600	the computers were very cool.
515600	520320	But what really made them into household applications
520320	521840	were the existence of this,
521840	526280	what we would now consider as killer application of PCs,
526280	527680	like Microsoft Excel,
527680	532400	that really made tabular information usable and scalable.
532400	534400	I think we, Luris Language Model Community,
534400	537280	or should also be looking for those kind of ideas as well,
537280	538960	because that's going to be ultimately
538960	541680	what's going to really transform the user experience
541680	542760	around these models.
542760	546920	And I think we're seeing some great usage of these models,
546920	550200	but I think there's a lot more to do going forward.
550200	551680	Makes a lot of sense.
551680	555480	When you look at what's happened since April, right,
555480	557160	a lot of things have changed.
557160	560120	We have new LLM capabilities.
560120	564760	We have a whole flurry of startups building in this space.
564760	568400	Could you maybe summarize what you've seen?
568400	570080	Right.
570080	571200	So, right.
571200	573640	So, Agent Sotini has been a big thing,
573640	578200	especially first the latter half of 2023.
578200	580480	This is how I'm seeing it.
580480	583600	Agent Community, it's sort of the way I view it,
583600	587200	has split into two communities, I would argue now.
587200	590840	So, maybe it might actually make a little more sense
590840	593280	to really talk about the history of agents,
593280	595200	because agent became a big thing last year,
595200	598640	but this is not a new idea in and out of itself.
598640	601480	Right, even in the commercial space,
601480	604200	we actually had agents like Microsoft Clippy,
604200	606560	I'm not sure how many of us will actually remember that,
606560	610960	but there used to be these agents in sort of our industry
610960	612520	and in research.
612520	615440	So, this is certainly not a new idea.
615520	618360	So, if you go all the way back,
618360	621080	so we had agents like Clippy,
621080	623360	and in many ways these agents,
623360	624920	especially in the reinforcement learning
624920	626000	and machine learning community,
626000	629720	agents were these elements that basically
629720	631040	could simulate human behavior.
631040	634240	I think that is ultimately sort of underlying thesis,
634240	637320	but many of the agents were given tools
637320	639360	to automate certain tasks.
639360	641720	And the task it were meant to automate
641720	644000	were tasks that are not simple, right?
644000	646440	It's not something like you're running a for loop
646440	647520	with your Python code,
647520	649200	but it's a little bit more complex than that, right?
649200	652520	It operates in much more embodied spaces
652520	656080	or in spaces that we often operate in, right?
656080	658520	The web, right?
658520	660480	Can it, the simplest example
660480	662280	with these kind of tool-based agents
662280	664200	are can it order me pizza?
664200	666520	Can it buy plane tickets?
666520	668200	And those might sound simple,
668200	670480	but we know from our experiences
670480	671920	that even ordering pizza actually
671920	674240	does require multiple steps, right?
674240	675880	We need to travel to certain websites,
675880	677520	we need to look through the menus,
677520	679200	actually make the payment,
679200	682800	and deal with sort of entering your address and so forth.
682800	684920	So that was one genre of agents
684920	687400	that already sort of existed for a long time,
687400	690120	or I would say all genres of agents sort of existed,
690120	693560	but that was one genre that was highlighted in the past.
693560	697080	So you see things like Clippy is also in that genre as well.
697080	699520	You're a Microsoft Office user,
699520	701440	Clippy would try to automate some tasks for you
701440	704720	based on your prior interaction with the software.
704720	709720	Another set of agents was this idea of simulation agents,
710880	712320	or agents that were created-
712320	714080	To clarify on that point,
714080	717720	those agents are single agents, correct?
717720	718840	They can be single agents,
718840	723400	they were often implemented as single agents, that's right.
723400	724600	I don't think by definition
724600	726280	they actually had to be single agents.
726280	728600	So you'd actually try, you're now seeing,
728600	730600	at least in the research,
730600	732640	you're starting to see glimpse of people
732640	735480	is trying to imagine what would it look like
735480	738240	for these agents to be in a multi-agent setting.
738240	740640	So research paper that I remember coming out
740640	743920	after Generative Agents was basically,
743920	747560	what if you have a company of agents, right?
747560	748920	There's going to be a CEO,
748920	750920	but there's also going to be a designer agent
750920	753560	who works in some other aspects,
753560	756560	there is going to be editor in this company,
756560	759720	and those are still much within the literature
759720	762680	of what I would call tool-based agents, right?
762680	767360	They're trying to automate some complex tasks for the users.
767360	768520	And I think there's going to be a lot of
768520	770640	sort of really big opportunities in this space,
770640	772080	that's something that people have been working on
772080	775400	for a long time, for all the right reasons.
775400	780400	Now, another community that has formed,
780600	784520	but to some extent actually has a slightly different route,
784520	787960	is agents that were created for simulations.
789960	792920	And these agents were certainly a part of games, right?
792920	794760	In the past we had Sims,
794760	797440	but we also had these NPC characters
797440	799200	that we could interact with.
799200	803000	Now, those NPCs and agents back then were very much,
803000	806400	it was simpler agents that were either rule-based,
806400	808080	there were some reinforcement learning agents
808080	809680	back then as well in that space.
811040	813480	But another one that we could usually think about
813480	817680	were agents that were used basically in social science,
817720	820440	economic agents, or agents that would simulate
820440	822760	our policy decision-making and so forth.
823760	826960	And those agents were also a part of this literature.
828040	830520	And what we're seeing today is,
830520	833080	we're one recognizing that Lawrence Lynch model
833080	834840	is simulating human behavior.
834840	836520	So it touches on all these agents,
836520	839800	that it can be a foundational sort of architectural layer
839800	842240	for creating all these different sorts of agents.
842240	845080	But in terms of our initial application spaces,
845080	846280	we're seeing this split,
846280	847360	where there's one community
847360	850680	who's now deeply interested in agents using tools,
850680	853280	but another community that is deeply interested
853280	855800	in this idea of can we simulate?
855800	859120	And this is where I would say like multi-agents
859120	860600	and as well as personalization
860600	864080	is really starting to be highlighted in the simulation space
864080	866960	because it's a little bit more directly incorporated
866960	868880	to the idea of simulations.
868880	870280	Who are we simulating for?
870280	871560	What are we simulating?
871560	873200	Who are we simulating?
873200	875360	And by definition, simulations often happen
875440	876920	in this multi-agent space.
876920	880560	So those are the two communities that you're starting to see.
880560	883520	So generative agent certainly stands on the far end
883520	885480	of the simulation-based agents,
885480	886680	whereas some other projects
886680	888520	that were also really cool last year,
888520	891800	I think a lot of sort of open AI, GPTs, I would say,
891800	893840	are another end of the simulation agents
893840	896440	or another end of tool-based agents.
896440	899760	So those are the axes that you're sort of seeing right now.
899760	900840	And sort of end by saying,
900840	902560	my hunch actually is again,
902560	905760	because they all start from the same technical thesis
905760	907720	that we can simulate human behavior,
907720	908880	they will merge in the end.
908880	911720	I don't think they will be completely separate thesis
911720	913960	like five to 10 years down the line.
913960	915680	It's more going to be the question of
915680	918920	where are we going to make our short-term bets
918920	920840	and what's going to be an interesting
920840	924720	and meaningful application space in the next two to five years.
924720	926120	So that's the field that I'm seeing
926120	928680	and how it's developing right now.
928680	930040	Before we maybe go into that,
930040	934520	could you maybe describe how LLM specifically
934520	939520	has affected the, especially the latter cohort, right?
939520	941800	What is the before and what is the after?
941800	944600	And what is the magnitude of improvement
944600	949600	because of this technology that's now cheap enough to use?
949600	950480	Right.
950480	954480	So Lawrence-Lenge Motor is really what made this possible.
954480	958080	That is really the fundamental fact that we needed.
958080	960480	In the past, when you wanted to create,
960480	962720	and this goes for both types of agents,
962720	964520	tool-based and simulations,
964520	966680	what you really needed was,
968080	970080	you basically needed rule-based agents.
970080	971520	That was the most common.
971520	974320	And rule-based agents are sort of a more sophisticated way
974320	977360	of saying we're scripting all the behaviors.
977360	981280	So imagine you're building an NPC for a game.
981280	984760	A human author would actually write every sentence
984760	987640	that the agent would say to the user, for instance.
988480	992960	Human author would actually describe in either code or language,
992960	994920	if this happens, you do this.
994920	997800	So you basically design all the possible behaviors.
998880	1001800	Now, that is expensive and not scalable, right?
1001800	1005840	And that was the fundamental block that we had.
1005840	1008360	Now, tool-based agents had similar issues
1008360	1011720	that in many of the contexts it had to operate,
1011720	1013800	it's not a very generalizable tool.
1013800	1016920	So if you sort of see how clippy
1016920	1020240	or even some of the agents that we're using today,
1020240	1021920	very simple types of agents actually
1021920	1024480	are already embedded into our daily usage.
1024480	1027600	So you may have used Google spreadsheet or Google doc.
1027600	1031960	It would auto-complete in some very rudimentary way
1031960	1033880	that actually could be considered in some ways
1033880	1037200	an agent in this direction of tool-based agents.
1037200	1040280	And the rules they were using so far were very simple.
1040280	1041800	It's not exactly rule-based,
1041800	1044080	but it is something that was very much hard-coded
1044080	1045680	into the agent's behavior.
1045680	1048120	And there was some learning going on,
1048120	1052640	but there were very strict or simple statistics
1052640	1054200	that we were using.
1054200	1056760	What large language model changes
1056760	1059200	is large language model gives us a single ingredient,
1059200	1063520	which is given a micro-context, micromoment,
1063520	1068520	let's say I'm sitting in this room talking to Joanne
1068840	1071200	and about let's say generative agents
1071200	1073120	or simulations and so forth.
1073120	1076320	Given that micromoment description,
1076320	1078480	a language model is extremely good
1078480	1081880	at predicting the next moment, right?
1081880	1085240	So what are the reasonable set of things
1085240	1089120	that June might say in this particular conversation
1089120	1090840	given what he knows?
1090840	1092240	It's very good at doing that.
1093840	1097680	That on its own is not a perfect agent
1097680	1099560	or it's not the complete ingredient
1099560	1102080	that you need to create these agents
1102080	1105400	that are meant to live for many, many years or decades.
1105400	1108320	But they are the right ingredient
1108320	1110000	or building block that we need it
1110000	1112080	because that can be used to replace
1112080	1115240	what was in the past, manual authoring.
1116240	1118120	In the past, we had to manually author
1118120	1122320	all the possible sequences given any micromoment,
1122320	1124360	but large language model can come in.
1125360	1129400	So given that ingredient, what we really could do
1129400	1133160	is bake in long-term memory and some reflection module
1133160	1135400	on top of it and planning module.
1135400	1137840	So given the micro-ingredient
1138760	1142800	plus an agent architecture that we give it on top of it,
1142800	1145400	these agents can basically now start to function
1145400	1147960	as something that can operate in that
1147960	1150720	in a world that's much like ours
1150760	1154720	with a fairly decent degree of long-term coherence.
1154720	1155800	So that's where we are
1155800	1157640	and that's really the difference it made.
1157640	1160480	And I'd say this is sort of a zero to one difference,
1160480	1161520	not a degree difference
1161520	1164480	because before large language model, this was not possible.
1165840	1170600	What else is, so we, large language models gave memory,
1170600	1174840	gave context, gave interactions to these agents.
1174840	1179240	What else in a perfect world would these agents have
1179240	1181120	in order to better mimic the real world?
1181120	1184040	Like what's maybe in the next stop, just out of curiosity?
1185160	1187080	Right, so to clarify, large language model
1187080	1188360	doesn't actually have,
1188360	1190880	so large language model provides one element.
1190880	1193320	It's the micro sort of a module
1193320	1195440	for predicting the next sequence.
1197080	1198560	It is the agent architecture
1198560	1202600	that actually ends up giving the memory and planning ability,
1202600	1205560	but those two pair becomes a fantastic combination.
1206320	1209040	Now, going forward,
1209040	1212120	what I do think is going to be interesting are,
1212120	1213960	so right now we're using large language model,
1213960	1217600	but we may have all noticed that things like chatGPT
1217600	1221600	can now not only do it just language,
1221600	1224480	but also other modality like image.
1225800	1229000	I think that's going to be really interesting, right?
1229000	1232160	So right now, let's say if you,
1232160	1234240	and this is sort of based on my prior work
1234280	1235480	called Generative Agents,
1235480	1237520	and we had this game world like sim ad
1237520	1239240	that we call a smallville,
1239240	1244240	the way these agents perceived and operated in their world
1244400	1247760	was basically by translating,
1247760	1249760	and like our system translating
1249760	1252440	the visual world into natural language.
1252440	1254360	So we would tell the agent,
1254360	1256120	you are in your apartment,
1256120	1260400	or you are in the kitchen talking to someone.
1260400	1262720	So we would actually take the visual world
1263720	1267000	and use our system to translate the visual world
1267000	1268440	into natural language,
1268440	1270880	and then feeding it to the agent architecture
1270880	1273760	that would use our language model to process this.
1273760	1277200	But now with these models being able to deal
1277200	1280360	with multi-modal aspect,
1280360	1283480	we might actually be able to bypass that phase
1283480	1286000	and go straight from here is the visual world
1286000	1288720	or space that you're seeing right now.
1288720	1291200	That is your memory, now act on it.
1292160	1294760	I think that's going to be potentially very powerful
1294760	1299480	because in part, image is much richer to some,
1299480	1300600	it conveys a lot more.
1300600	1304480	I do come from natural language processing background,
1304480	1308560	at least that's my other half of my sort of academic background.
1308560	1311200	So I have bias towards believing
1311200	1312840	the natural language is profound,
1312840	1314760	and I think that we're going to be,
1314760	1317080	that will be the case going forward as well.
1317080	1319000	But image does offer something
1319000	1321920	that just language alone does not.
1321920	1324000	So image is going to be a big thing.
1324000	1327640	Now imagine the future video is going to be a big thing as well,
1327640	1330680	then gradually the more these agents
1330680	1332840	will basically increasingly get more powerful
1332840	1335400	as this new modality gets piled on.
1335400	1338320	So that's something that we should be looking forward to.
1338320	1339560	That's great.
1339560	1341000	What are some on the downside,
1341000	1342920	what are some of the limitations
1343880	1347560	that you're seeing in terms of these agents,
1347560	1349080	especially generative agents?
1350240	1351640	Right.
1351640	1356640	So there are limitations that I can mention
1356640	1360200	just about sort of in the context of our work.
1360200	1363120	And then I think there are going to be interesting limitations
1363120	1366280	that are much more application specific.
1366280	1368520	So for generative agents today,
1368520	1370960	certainly the technical limitation right now
1370960	1373320	might have to do with things like,
1373320	1375560	so you're using whether it's an open source.
1375560	1378480	So right now we use OpenAI's model.
1378480	1381120	OpenAI has actually done a lot of work
1381120	1383040	to make the model safer.
1383040	1386400	And I think OpenAI, I think that was the right approach
1386400	1388400	in the sense that what they really wanted to create
1388400	1392560	was these chatbots or agents or chatGPT
1392560	1396080	that are a safe tool to use for most people.
1397000	1399120	Now, if you want to run a simulation
1399120	1402080	or create truly accurate and believable agents
1402080	1404480	with something like chatGPT, however,
1404480	1406920	that could become a limitation
1406920	1412040	because what we really experience as humans
1412040	1414280	is we fight, we sometimes have conflicts,
1414280	1416320	we disagree with each other.
1416320	1418320	And that might not be something that's something
1418320	1420640	like chatGPT that's been fine-tuned
1420640	1423280	to not behave that way to remain safe.
1423280	1425160	It's something, it might not be something
1425160	1427760	that these models would try to surface.
1427760	1431800	And that could be a potential block
1431800	1435600	in creating more accurate, more believable simulations
1435600	1437760	or agents for that matter.
1437760	1439880	So that's something that is one limitation
1439880	1441360	right now that we're seeing.
1442880	1446040	An interesting way to tackle this, I think, going forward
1446040	1448840	is to use open source models or other models
1448840	1451520	that have less of these fine-tuned nature.
1453240	1455360	But it's going to be highly dependent
1455360	1457600	on the models that we'll be using for this.
1457600	1460480	So I think that's one thing to look forward to.
1460480	1464120	Got it, got it, that's super helpful.
1464120	1467480	And maybe one last question on the research side.
1468720	1471120	When you think about future areas to explore
1471120	1475320	for you specifically, what are some of the
1478160	1482360	more narrow topics that you're hoping to dive deeper in?
1483440	1484320	Given the world.
1485320	1490160	So ultimately, I think making the agents more accurate
1492040	1495320	for these agents to be more accurate reflection of who we are,
1495320	1497640	I think it's going to be a really interesting research
1497640	1501760	and I think it's going to, that's going to be an area
1501760	1504600	that's going to have more of a research and broader impact.
1505960	1510400	So right now, you may have seen the sort of simulation demo,
1510640	1514640	the agents that live in that simulation are fictional,
1514640	1518560	that we just, for instance, we have an agent named Isabella,
1518560	1521480	we told Isabella that she is a cafe owner
1521480	1524680	and large language model basically makes up
1524680	1529320	what a persona that is reasonable given that description.
1529320	1531720	But I think it's going to be far more interesting
1531720	1536280	if we can make these simulations actually closely model
1536280	1538840	our actual human communities.
1538840	1541960	So it's not just fictional, but actually has groundings.
1541960	1544640	That's going to open up from our perspective
1544640	1547560	an entirely new set of application spaces
1547560	1549120	as well as research impact.
1550320	1551800	This can be used, for instance,
1551800	1554760	to actually model or predict markets
1554760	1559760	or it's going to be able to use two more closely personalized
1560200	1563120	many of these agents for individual use cases.
1563120	1564600	So that's something that we're looking forward to
1564600	1566120	in terms of sort of a particular topic
1566120	1568240	that we're diving into.
1568240	1570320	That plug, of course, scaling up the agents.
1570320	1573160	I think that's another big one, but those two.
1573160	1574480	That makes sense.
1574480	1576480	Now, one of the things that's missing
1576480	1581480	in most AI technologies is like really the
1582440	1585520	emotional part of how humans feel, right?
1585520	1589240	Like all of that data is largely not captured
1589240	1592640	and therefore not part of any kind of models today.
1593560	1596720	Language is one small output of what we have.
1596720	1599840	It's a very important output for sure,
1599840	1601040	but it's still one small output.
1601040	1604720	So I wonder how we might be able to incorporate
1604720	1607220	some of the data around our emotions.
1608280	1609200	I agree.
1609200	1610580	Some thoughts in the future.
1612200	1614720	Maybe let's move on to the applications today
1614720	1618140	since you talked about some of the challenges for agents.
1619120	1622480	Many organizations are thinking about
1622480	1624200	how to use large language models today, right?
1624200	1626960	There's a huge amount of aspirations.
1626960	1629520	A subset of them are also thinking about
1629520	1632360	what are some of the agent technology applications
1632360	1635880	that are viable within an enterprise,
1635880	1638520	which has limitations around infrastructure,
1638520	1642240	around data silos, security, and all that stuff.
1643200	1645640	Any particular areas where you've seen
1646920	1649880	companies be successful at using these technologies
1649880	1650860	in production?
1651800	1652960	Right.
1652960	1654560	So I think this is going to be incredibly
1654560	1658360	like case-by-case answer.
1658360	1660440	So let me think.
1662320	1665040	Or if not, like any hypothesis as to
1666240	1668600	where you might see the first
1668600	1670680	commercial deployments at scale.
1671720	1672560	Right.
1675240	1679080	So there is something that I have.
1679080	1681600	This is, there's a message that I have
1681600	1684320	in trying to communicate, I think, in different settings.
1684320	1686440	So this is not something I'm sort of
1686440	1687840	conveying for the first time.
1688960	1692280	And my opinion has been getting updated,
1692280	1694400	but I think fundamentally I think this is right.
1694400	1697040	So the way I've been describing it is
1697040	1702040	in human computer interaction or in most task settings,
1702640	1704200	there are two types of problems
1704200	1707140	that we deploy our machines or agents in.
1707140	1710480	One has very hard-edge problem spaces.
1710520	1713000	These are things like, hey, order me pizza
1713000	1714720	or buy me a plane ticket.
1714720	1718360	These are tasks where there's a very concrete outcome
1718360	1721160	and there often is a right or wrong answer
1721160	1723640	that the agent has to achieve.
1723640	1725680	At least from the user's perspective,
1725680	1728960	there is something that would absolutely be yes or no.
1730240	1731880	And then there are problem spaces
1731880	1734800	where we have soft-edge problems.
1734800	1737840	These are problems where we can increasingly
1737840	1740520	co-climb towards sort of being better,
1740520	1743800	but at certain level it starts to actually become useful.
1743800	1745760	So to make this intuition a little bit,
1745760	1748080	to make this a little bit more intuitive here,
1748080	1751680	for instance, if I guess the worst possible case scenario
1751680	1753920	is I asked the agent to buy me a plane ticket
1753920	1755880	and it bought me the wrong ticket
1755880	1757760	that just goes to a different place,
1757760	1759600	then that's like a heavy no.
1761200	1765280	Whereas let's say I asked the agent to sort of
1766280	1769080	simulate a behavior that is sort of fun
1769080	1770600	so that when I'm in a game,
1770600	1773080	this is sort of entertaining and interesting.
1773080	1775080	That might be something that the agent
1775080	1776720	doesn't need to be quite perfect in,
1776720	1779120	but it can still get there quite quickly.
1779120	1781240	And then we can gradually improve.
1781240	1785000	Those two are the spaces that we can sort of,
1785000	1787520	in terms of when we consider where to deploy
1787520	1790080	or how these will actually make its first impact,
1790080	1793080	we might actually be looking at those problem spaces first.
1793080	1795240	And these are the classes that I'm seeing.
1795240	1797240	If I were to make my bet,
1797240	1800200	agents will likely succeed first
1800200	1802800	in the soft-edge problem spaces
1802800	1805400	and will gradually inch into making it work
1805400	1807480	in the hardest problem space.
1807480	1809560	This has been sort of an intuition
1809560	1813040	with agent research community for some time.
1813040	1815440	So when Clippy, for instance, failed.
1816520	1819680	Our intuition there, at least from research perspective,
1819680	1822120	wasn't that these agents failed
1822120	1825080	because we didn't have the technology there.
1825080	1826320	Certainly these were deployed
1826320	1828840	and there were some confidence around the technology.
1828840	1832400	But the problem there actually was with interaction,
1832400	1836520	that when agents are deployed in hard-edge problem spaces,
1836520	1839040	it's often deployed in states where
1839960	1843280	it actually has to have a fairly long chain of steps
1843280	1847200	and fairly high, the risk were fairly high.
1847200	1849920	When it fails, the cost of correcting its error
1849920	1850840	is actually quite high,
1850840	1853440	cost of auditing its error is actually quite high.
1854440	1856040	So when these agents are deployed
1856040	1857680	in hardest problem spaces,
1857680	1859160	it has to reckon with the fact
1859160	1862280	that it will undoubtedly make mistakes.
1862280	1864000	And when it does make a mistake,
1864000	1865920	it has to be increasingly auditable
1865920	1868520	and controllable by the users
1868520	1871600	so that the cost of correcting its error
1871600	1875160	is not high enough that from the user's perspective,
1875160	1878960	the cost-benefit analysis basically has to make sense.
1878960	1882120	And that's been a fundamental challenge with agents.
1882240	1884560	That's why in every era,
1884560	1888240	we see the interest around agents spike for a while
1888240	1889600	and then it quickly subsides
1889600	1892320	after like maybe a half a year or two a year.
1892320	1895440	Now there's a real issue now though,
1895440	1897720	that given the large language model in the progress we saw
1897720	1899720	that this might not be the case this time
1899720	1902560	or at some point we might be able to make this work.
1902560	1906240	But for now, so I'm closely monitoring this
1906240	1907920	as well and I think we all should.
1907920	1909400	I don't think we should just go and say,
1909400	1910560	because it didn't work before,
1910560	1912680	it's not going to work this time.
1912680	1917360	But my hunch is that we will likely see
1917360	1919640	very similar pattern arise,
1919640	1921800	at least for the first of the future.
1921800	1925000	And we haven't quite dealt with the interaction problems
1925000	1927160	with those types of agents.
1927160	1930280	So I think it's much safer to assume
1930280	1932840	that it is going to be in the soft edge problems basis.
1932840	1935920	And that's why in some areas, in many of the aspects,
1935920	1938040	that's why our team was also interested
1938040	1939640	in this idea of simulation.
1939640	1942800	Because simulation is sort of the prime example
1942800	1944800	of soft edge problem spaces,
1944800	1947000	where the simulation has to be good enough
1947000	1949040	for it to start being useful.
1949040	1952480	That's also why I think a lot of really early promising AI
1952480	1955360	startups that's going to go in the agent space
1955360	1959080	are places that does NPCs for games.
1959080	1962080	Because those are very safe soft edge problem spaces
1962080	1964360	where the agents can fail, but that's okay.
1965320	1967620	And gradually we'll sort of go to the other area as well.
1967620	1970260	But I think that's where the impact is going to start
1970260	1971660	in the next couple of years.
1973100	1978100	I also think just seeing the startups in this space,
1978740	1983740	in sectors and functions that allow for failure,
1984380	1988420	like you said, include things like marketing.
1989260	1992500	Where if you market incorrectly,
1992500	1994780	it's not that big of a deal.
1994780	1999260	If you write the wrong code,
1999260	2001580	that's probably going to be a bigger deal.
2001580	2006300	If you pick the wrong security features, that's a huge deal.
2006300	2008260	If you pick the wrong things for healthcare,
2008260	2009900	that's an even bigger deal.
2009900	2013900	So there are degrees of fault tolerance
2013900	2015940	within the enterprise.
2015940	2016940	That's one thing.
2016940	2019460	And the second on the consumer side,
2019460	2023340	especially if the agents are just assisting consumers
2023340	2026400	by not executing on anything.
2027660	2029460	That could probably also work.
2029460	2032140	For example, there's a company called Rewind,
2032140	2035740	which is using some of the agent technologies, I believe.
2035740	2038780	And they're getting a bunch of consumer demand,
2038780	2042980	but what consumers are doing is just searching
2042980	2046160	for a behavior that they have had before.
2047380	2051420	And this product is helping them do that
2051460	2055780	versus do anything real world, really.
2055780	2056620	Interesting.
2058260	2059420	But that's super.
2059420	2063660	The way that you frame it is very useful.
2064500	2069060	What about just from an architecture standpoint,
2069060	2073740	large language models enabled by transformer architectures?
2074740	2076740	This is a whole different direction.
2076740	2079540	We're already seeing companies that are saying,
2079540	2082180	hey, transformers are not the most efficient.
2083300	2084900	Inference costs are very high.
2086020	2088540	Let's look at the next thing.
2088540	2091660	Have you spent much time thinking about that?
2091660	2095260	And if so, any impact to the work that you're doing?
2096460	2097300	Right.
2097300	2101220	So certainly a next sort of model
2101220	2102580	that we're going to be banking on,
2102580	2105180	I think that always is an important topic.
2105180	2107500	And that's something that I think we as a community
2107500	2109100	always has to sort of monitor,
2109100	2110700	because I think you're right,
2110700	2113140	that transformer is not going to be the end model
2113140	2116380	that will be hopefully, I mean, not on what.
2116380	2118620	The hope here is that we wouldn't be using transformer
2118620	2119700	10 years down the line.
2121180	2124500	But one way that we do view this is,
2124500	2127220	this is very much like a programmer's way
2127220	2128060	of looking at this.
2128060	2130380	We view this in abstractions, right?
2130380	2133500	So what transformer has gotten us right now
2133500	2136300	is this amazing capacity for reasoning
2136300	2139540	and processing information and generating information.
2140380	2143620	So it might be the case that in the future,
2143620	2146580	that task will be done by even better models.
2146580	2148940	And hopefully that's going to be the case.
2148940	2151820	But for the sake of building applications,
2151820	2153780	it is true that we can sort of view this
2153780	2155620	as a layer of abstraction.
2155620	2157820	That there might be some other technology
2157820	2160620	that's going to be powering it in the future,
2160620	2162140	but really what we're focusing on
2162140	2164820	is the capacity and the modality.
2164820	2167700	What kind of reasoning, using what modality,
2167700	2170300	can these technologies that exist today do?
2170300	2172460	And we're going to be building on top of it.
2172460	2176100	So I think that's sort of our way of looking at it
2176100	2180780	in sort of a medium term, again, the next three to five years.
2180780	2183700	Now, if you're looking, because right now,
2183700	2185540	there are some promising architectures
2185540	2189620	that's sort of been created at the forefront of,
2189620	2191340	I would say more in the machine learning
2191340	2193660	and natural language processing communities
2193660	2197580	that I'm personally getting a little bit excited about.
2197580	2200340	But at the moment, those are still
2200340	2203300	in the very much in the research phase.
2203300	2205820	And can you share some examples of that?
2205820	2208860	Yeah, so I think there's one model that recently came out,
2208860	2210940	like Mamba by some folks.
2210940	2213260	Those are from Stanford folks.
2213260	2217020	So I think the authors now at CMU and Princeton
2217020	2219900	sort of all within sort of this community.
2219900	2222260	So that's one example of sort of a potentially promising
2222260	2223180	or interesting model.
2223180	2224940	And that's the one that I recently heard about
2224940	2228940	that I think is interesting to be looking at.
2228940	2232860	But these models, for them to be deployed at scale
2232860	2237100	in a commercial way, if we decide to basically go
2237100	2239820	with certain model that's getting created today,
2239820	2244740	it will give us maybe two to five year timeline
2244740	2246580	before they can really take off.
2246580	2252660	Because Transformer is not, it is a relatively modern model,
2252660	2255300	but it really is how you look at sort of the timeline.
2255300	2256100	There's Transformer.
2256100	2257220	Seven-ish years?
2257220	2259140	Seven-ish years.
2259140	2262020	So I think hopefully if we find something like this time,
2262020	2263500	maybe it's going to go much faster.
2263500	2266500	But it still took about seven-ish years for Chatchapiti
2266500	2267740	to really come out.
2267740	2269580	So it's not immediate.
2269580	2271700	Whereas a lot of interactions that we can build,
2271700	2273780	I think there's a lot that we can do like today
2273780	2275940	to create really cool experiences.
2275940	2277500	So I think that's how we're looking at this.
2277500	2278780	There is a medium term.
2278780	2281620	This is where we are focused on the level of extraction,
2281620	2284020	that this is the capacity that we'll have.
2284020	2287100	And then maybe in the down the line five to 10 year term,
2287100	2289420	we can really be looking forward to some new models that's
2289420	2291500	going to make an impact.
2291500	2292020	That's great.
2292020	2293980	That's great.
2293980	2294460	Cool.
2294460	2296140	Very cool.
2296140	2300980	Maybe more generally, if you just zoom out for a moment,
2300980	2304700	when you look at the ecosystem today,
2304700	2307180	what are some of the problems that you want to see solve?
2307180	2308860	We talked about multimodal a little bit.
2308860	2313020	We talked about new models right after Transformers
2313020	2314060	that might come out.
2314060	2317420	What are some of the problems that you are most excited
2317420	2319260	about someone solving?
2319260	2323540	Not necessarily you personally, but someone solving.
2323540	2326100	I sort of have two in mind.
2326100	2330380	And it's a little bit less of a, this is a specific problem
2330380	2332180	that I want to solve.
2332180	2334980	But it's more sort of questions that I have
2334980	2338740	that I think more of us should be thinking about.
2338740	2342020	And to some extent, and this happens a lot with sort of the way
2342020	2344900	I do my research as well, where I get
2344900	2348820	inspired by big problems or foundational problems
2348820	2351380	that we had in previous decades.
2351380	2354020	Because oftentimes, there's a lot of insights
2354020	2358700	that we can learn from the past as we build on the future.
2358700	2365180	One, certainly I'm embedded into this agent space.
2365180	2371540	One is, in the past, agents had its hype cycles, basically.
2371540	2374260	But it failed.
2374260	2376780	That the hype cycle lasted for a couple of years,
2376780	2379100	and then people very quickly lost interest.
2379100	2381340	Basically because it didn't quite deliver
2381340	2384860	on the promises that it had.
2384860	2389540	I think it's worth asking ourselves why that was the case.
2389540	2392820	I think the opportunity this time is real.
2392820	2394820	But I also think the opportunity in the past
2394860	2397900	was also real to some aspect as well.
2397900	2400100	So just because the opportunity is real
2400100	2402340	and language model is really cool doesn't necessarily
2402340	2404500	guarantee us, at least from my perspective,
2404500	2406820	that we're going to, that agent will finally
2406820	2410140	be a thing that everyone will use.
2410140	2413580	I think there is a future where that will happen at some point.
2413580	2416180	I think it might even happen this cycle.
2416180	2419940	But I think it's really worth asking, as a community,
2419940	2423220	why did it fail in the past so that we don't repeat those mistakes?
2423220	2425660	One sort of main thing I'm sort of curious about
2425660	2427980	that I don't think a lot of us are thinking about
2427980	2432300	is actually not the technology part, but the interaction.
2432300	2436020	How are these agents going to be used in what way?
2436020	2438900	Because ultimately that's where it really delivers value
2438900	2440660	to the end users.
2440660	2443940	And that's where agents in the past have failed.
2443940	2445300	That it was really cool technology,
2445300	2447660	but we didn't seriously ask ourselves,
2447660	2449620	is this something that people really need?
2449620	2451580	And does the cost-benefit analysis
2451580	2454620	of using these agents and learning how to use them well
2454620	2457620	really make sense for the broader user base?
2457620	2459340	So that's one.
2459340	2464500	And other one is sort of my, it's a little bit of a hot take,
2464500	2466380	but it's also a shorter take, which
2466380	2468300	is we have large language models,
2468300	2472020	and I think these have made a huge impact already.
2472020	2476260	The number of users who use chat.gbt, that's incredible.
2476260	2479940	But I think it's sort of worth asking ourselves,
2479940	2483060	is that sort of quote unquote the killer applications
2483060	2485140	that we were waiting for?
2485140	2489980	Because in many ways, chat.gbt, or maybe it is.
2489980	2494140	And I think if it is, I think somebody should articulate this.
2494140	2497980	But chat.gbt does feel like a fairly simple wrapper
2497980	2501020	around our language model, because that's what it is.
2501020	2504700	And OpenAI has done fantastic things to make it safer
2504700	2506700	and make it more useful by tuning, I think,
2506700	2508140	what's really great.
2508180	2510780	But I think it's worth asking, if that is actually
2510780	2515020	the killer application, why is it a killer application?
2515020	2516500	And the answer might actually come out
2516500	2519540	that maybe it actually isn't the killer application
2519540	2522780	that we were waiting for, in which case, what
2522780	2524740	is going to be the killer application?
2524740	2527140	That's really going to add value in a much more
2527140	2530340	generalizable way.
2530340	2532820	That's a very abstract question.
2532820	2534900	For now, it's for me, it's just a hunch
2534900	2537860	that I think there's something to be asked about there.
2537900	2540140	And if I'm wrong, I would also love to hear again
2540140	2543580	somebody really say, we already have this killer application,
2543580	2546820	maybe it's co-pilot, chat.gbt, and here's why.
2546820	2551820	But for now, this is a question that I'm still asking myself.
2551820	2553060	That makes sense.
2553060	2555060	And thank you for sharing that.
2555060	2560660	What are some of your favorite AI apps today that you use?
2560660	2561900	I love chat.gbt.
2561900	2563300	I use it every day.
2563300	2566700	Chat.gbt did make a difference in my workflow.
2567580	2571220	So as a researcher, one of the main things I do
2571220	2574220	is I program every day, or at least most days,
2574220	2576340	or I write or write papers.
2576340	2578300	So I do one of those two things.
2578300	2580740	Chat.gbt is fantastic at both.
2580740	2585580	So as all programmers sort of know,
2585580	2587180	we sometimes don't bother remembering
2587180	2590860	all the different functions or documentation.
2590860	2592820	It's very good at generating a lot of the code
2592820	2594500	when I have an idea.
2594500	2596060	Really impressive.
2596060	2597780	It's also quite a good editor.
2597780	2601460	So if I make grammar error in my sort of sentences,
2601460	2604140	chat.gbt will usually catch them for me.
2604140	2607140	It's simple and easy thing, but it's good enough now
2607140	2610980	that it's actually making a difference in the workflow.
2610980	2614300	So as a chat.gbt, for sure, by extension, I think co-pilot
2614300	2616980	will make a difference.
2616980	2618860	So it's sort of worth asking, maybe going back
2618860	2621180	to the question around killer application.
2621180	2623180	What is the definition of killer application?
2623180	2625780	I think it does some people define it
2625780	2628340	as application that has more users.
2628340	2630700	And the fact of that, I think, always has to be the case,
2630700	2632940	that no killer application has no user.
2632940	2636180	Killer application, by default, means the application
2636180	2639740	that will have the most number of users.
2639740	2642460	But I think there is more theoretical definition
2642460	2644780	to what a killer application is.
2644780	2647980	That implies a lot of users who are the most number of users.
2647980	2652100	But for instance, if we look back to the prior era of PC,
2652100	2653780	my killer application that I mentioned
2653780	2658060	was something like Microsoft's Excel or this tabular data
2658060	2661580	format, the thing that would let us manipulate the tabular
2661580	2662900	data.
2662900	2665740	So really, the definition, in a more theoretical sense
2665740	2667820	of killer application here, is there
2667820	2670140	is new technology stack that is being developed.
2670140	2673060	There is a new file type that is getting generated.
2673060	2676460	Then the killer application is the one that would let us
2676460	2679100	manipulate the file application, file type.
2679100	2680700	That's one theoretical definition
2680700	2683020	that one could give, at least that's
2683020	2684980	sort of the definition that I've been towing with.
2684980	2686380	I think it's an interesting one.
2686380	2687900	I don't think it's the only one.
2687900	2690700	But those are sort of ways that I'm looking at this.
2690700	2692060	That makes a lot of sense.
2692060	2694620	I also use chatGPT every single day.
2694620	2696580	It's been very helpful.
2696580	2699500	Everything from coming up with menu names
2699500	2705740	to rewriting emails that don't sound as nice.
2705740	2709660	And I've tried a little bit to give it files and images.
2709660	2712460	So I actually helped my mother create a background.
2712460	2714260	She's a dancer, so she was performing
2714260	2716380	on one of the very specific background for her dance.
2716380	2719140	And I created that for her using chatGPT.
2719140	2721420	So all sorts of utility there.
2721420	2724220	But I love the way that you framed
2724220	2726140	the last potential application.
2726140	2729140	Maybe just one last question from my end.
2729140	2739300	Any resources or books that you love that is on this topic?
2740300	2741300	Right.
2745300	2748580	I do think, and this is often the case
2748580	2751100	with many of the cutting edge spaces.
2751100	2753340	I think a lot of the papers that are coming out
2753340	2755140	that are gaining a lot of attention,
2755140	2756740	I think those are sort of worth checking out
2756740	2757940	as sort of the resources.
2757940	2761540	It's not exactly like here's one book that we can all look at.
2761540	2764940	But things are moving fast enough that I think
2764940	2767140	those are sort of interesting resources
2767140	2769740	or just things that are getting created today
2769740	2771540	and their documentations.
2771540	2775140	So those are sort of mentioned as sort of a generic answer.
2775140	2778540	I do think, I think this has been a sort of running thing
2778540	2780540	in some of the things I mentioned today,
2780540	2788540	I get inspired by insights that basically had an impact
2788540	2791140	that stood the test of time.
2791140	2793540	And the reason why that is the case
2793540	2796540	is because I personally think all the great ideas
2796540	2798540	are sort of timeless.
2798540	2801540	It's because current time cycle is over,
2801540	2803540	it doesn't mean they're less interesting or less meaningful.
2803540	2804540	For sure.
2804540	2808540	That foundational ideas that will continue to have impact.
2808540	2810540	So when I look for resources,
2810540	2815540	I actually look back to books from truly the prior generations.
2815540	2818540	So some of the works that I often go back to
2818540	2821540	are works by Herbert Simon, Alan G.
2821540	2824540	Those are founders of AI and many of these fields
2824540	2826540	who would later go into a turning award
2826540	2828540	and Nobel Prize and so forth.
2828540	2831540	And those works, early cognitive psychologists
2831540	2836540	and scientists inspired my work a lot and their textbook.
2836540	2838540	Those people actually have written books
2838540	2840540	because they were much more established
2840540	2842540	than sort of the cutting edge spaces today.
2842540	2846540	So I go back to those as sort of my personal resources
2846540	2848540	for getting ideas.
2848540	2849540	That's great.
2849540	2850540	Thank you so much.
2850540	2852540	This was super, super helpful to me personally
2852540	2854540	because what we do as investors
2854540	2859540	is we try to understand the impacts of technology
2859540	2864540	and start to invest in companies when it becomes,
2864540	2867540	at the beginning of when it becomes commercially viable.
2867540	2870540	So to your point around what are the problem spaces,
2870540	2873540	what are the applications in which this can be applied
2873540	2876540	in a cost-effective and secure way
2876540	2881540	that where the end user is willing to interact
2881540	2883540	and get value,
2883540	2887540	that's when we start to come in and invest in these companies
2887540	2891540	which will hopefully be much bigger companies in the future.
2891540	2893540	So really appreciate this chat.
2893540	2895540	Yeah, it was fun.
