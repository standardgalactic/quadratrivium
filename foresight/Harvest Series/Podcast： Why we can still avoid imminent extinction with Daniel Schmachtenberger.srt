1
00:00:00,000 --> 00:00:10,480
Daniel Schmartenberger, thank you for being with us.

2
00:00:10,480 --> 00:00:11,480
Thanks for having me.

3
00:00:11,480 --> 00:00:15,440
So, you're a philosopher, a founding member of the Consilience Project.

4
00:00:15,440 --> 00:00:21,200
The goal of this conversation today is to analyze the direction our civilization is

5
00:00:21,200 --> 00:00:28,160
taking in half an hour, because you've been doing like so many great podcasts about the

6
00:00:28,160 --> 00:00:34,120
metacrisis during lasting three to four hours, and I suggest that people go and watch them

7
00:00:34,120 --> 00:00:35,120
on the Internet.

8
00:00:35,120 --> 00:00:36,120
They're very good.

9
00:00:36,120 --> 00:00:37,960
We'll start with the harvest of the day.

10
00:00:37,960 --> 00:00:44,280
The question I'm asking to all the guests here for Harvard Podcasts, if something easy

11
00:00:44,280 --> 00:00:49,760
or simple could be done and would make the world a better place, what would it be for

12
00:00:49,760 --> 00:00:51,000
you, Daniel?

13
00:00:51,000 --> 00:00:54,800
When I saw the note that you sent me, that that would be a last question.

14
00:00:54,800 --> 00:00:58,080
Is there a simple or easy thing that everyone could do that make the world a better place?

15
00:00:58,080 --> 00:00:59,080
That kind of cringed?

16
00:00:59,080 --> 00:01:05,120
Because I usually really am not a fan of that question, because the world needs so many

17
00:01:05,120 --> 00:01:10,120
different kinds of things done that require different skills and capacities and orientations

18
00:01:10,120 --> 00:01:14,920
and to try to reduce it to some thing that would be true for everybody.

19
00:01:14,920 --> 00:01:18,880
You get a platitude like be kind or loving or something like that, or you get something

20
00:01:18,880 --> 00:01:24,480
like recycle or pick up trash or try to use less carbon or something that doesn't map

21
00:01:24,480 --> 00:01:26,680
to the whole set of things that the world needs.

22
00:01:26,680 --> 00:01:35,440
I think there's a process where movements have been associated with political processes

23
00:01:35,440 --> 00:01:40,680
and markets in a way that it's like, here's this great catastrophe that'll happen if the

24
00:01:40,680 --> 00:01:45,360
other side gets elected, so everybody needs to get out and vote for so and so.

25
00:01:45,360 --> 00:01:50,480
That's like everybody can do a simple thing because we're relating to everybody as voters

26
00:01:50,480 --> 00:01:56,120
or everybody donate to this cause or boycott this thing, but the complexity of the world's

27
00:01:56,120 --> 00:02:02,600
issues from climate issues to AI risk to supply chain issues to electrical grid issues, like

28
00:02:02,600 --> 00:02:05,040
there's no action like that.

29
00:02:05,040 --> 00:02:09,360
There's no somebody to vote for or not to vote for thing to donate to that addresses

30
00:02:09,360 --> 00:02:10,360
it.

31
00:02:10,360 --> 00:02:16,240
One thing that is not necessarily easy, but is relatively simple, that would be great

32
00:02:16,240 --> 00:02:22,440
if everybody in the world could do more, is to seek to try to understand other people's

33
00:02:22,440 --> 00:02:27,680
perspectives much more deeply, particularly those that are most different than their own.

34
00:02:27,680 --> 00:02:38,400
If you can try to take the opposite perspective on abortion, on gun control, on climate change,

35
00:02:38,400 --> 00:02:44,320
on the Ukraine-Russia war, on the Chinese versus Western system, on any of those things,

36
00:02:44,320 --> 00:02:46,000
on the Israel-Palestine issue.

37
00:02:46,000 --> 00:02:51,960
If you can try to earnestly be able to make the argument that the person on the other

38
00:02:51,960 --> 00:02:56,640
side would make as well enough that they don't have anything to add to it, and not

39
00:02:56,640 --> 00:03:02,120
just as a rhetorical process, but connect to the values that they care about and what

40
00:03:02,120 --> 00:03:05,840
it feels like to be them and see the world through their eyes, realizing that there might

41
00:03:05,840 --> 00:03:06,840
be distortion.

42
00:03:06,840 --> 00:03:10,920
There might be a lot of things missing, but there's not zero truth or zero value to it.

43
00:03:10,920 --> 00:03:20,160
That process, if everyone did that, would actually result in addressing the metacrisis

44
00:03:20,320 --> 00:03:26,080
in all of its complexity, the issues in synthetic biology risk, and pandemics, and escalation

45
00:03:26,080 --> 00:03:30,560
pathways to warfare, and economic issues, and geopolitical issues, and all of them.

46
00:03:30,560 --> 00:03:37,320
You can say that they either come down to conflict or externalities, like we cause harm

47
00:03:37,320 --> 00:03:43,320
directly, intentionally, which a war is a great example of, or harm gets caused that

48
00:03:43,320 --> 00:03:46,160
we didn't intend to cause.

49
00:03:46,160 --> 00:03:48,040
No one intended to cause climate change.

50
00:03:48,040 --> 00:03:53,720
We just wanted to have transportation and energy, and the secondary byproduct of that

51
00:03:53,720 --> 00:03:55,800
was climate change.

52
00:03:55,800 --> 00:03:59,760
All of the environmental issues, no one intentionally had a conflict with the environment that was

53
00:03:59,760 --> 00:04:00,760
causing it.

54
00:04:00,760 --> 00:04:05,880
It was the externality of optimizing something and causing harm somewhere else.

55
00:04:05,880 --> 00:04:09,240
There are problems that we intentionally cause, and there are problems that we accidentally

56
00:04:09,240 --> 00:04:10,240
cause.

57
00:04:10,240 --> 00:04:13,640
Both of them would be corrected by seeking to understand all the perspectives more,

58
00:04:13,640 --> 00:04:19,440
because if you sought to understand the perspectives well enough, conflict theory would evaporate.

59
00:04:19,440 --> 00:04:23,560
Most of the mistakes, when you're trying to optimize for one thing and you end up causing

60
00:04:23,560 --> 00:04:27,560
externalities to something else, somebody else saw that and knew that, and if you were

61
00:04:27,560 --> 00:04:33,040
in wide enough conversations, then the thing that you're trying to optimize for that's

62
00:04:33,040 --> 00:04:36,400
going to cause harm somewhere else someone else would have mentioned and said, actually,

63
00:04:36,400 --> 00:04:40,040
let's improve your design or your strategy by factoring this.

64
00:04:40,040 --> 00:04:45,240
Both the unintentional externalities and the intentional conflict would be resolved through

65
00:04:45,240 --> 00:04:51,960
active perspective seeking and then perspectives and this is wonderful.

66
00:04:51,960 --> 00:04:56,880
When you look at the history, as you said, humans seem to have a talent for innovation

67
00:04:56,880 --> 00:05:03,200
and progress, but also a natural tendency for war and chaos.

68
00:05:03,200 --> 00:05:08,760
These two tendencies fit each other and make things bigger and bigger, so greatest but

69
00:05:08,760 --> 00:05:12,720
out of control technologies can cause a huge damage.

70
00:05:12,720 --> 00:05:21,800
What do you think should be done about technologies and do they represent innovation or danger

71
00:05:21,800 --> 00:05:23,560
for you?

72
00:05:23,560 --> 00:05:27,560
First thing about technology is that even if we're not talking about a military technology,

73
00:05:27,560 --> 00:05:31,560
we're talking about a technology for some other purpose, even if we develop a technology

74
00:05:31,560 --> 00:05:35,540
for some non-military purpose, it will have a military application or some kind of conflict

75
00:05:35,540 --> 00:05:40,900
oriented application, basically saying all technologies dual use.

76
00:05:40,900 --> 00:05:46,220
Maybe we're doing the synthetic biology gene editing for trying to cure cancer, but as

77
00:05:46,220 --> 00:05:50,660
we get better at making tools to do gene editing, can that be used for bio weapons?

78
00:05:50,660 --> 00:05:51,660
Totally.

79
00:05:51,660 --> 00:05:56,560
Maybe we're making the AI to try to do drug discovery, but can that same AI do autonomous

80
00:05:56,560 --> 00:05:58,060
drones?

81
00:05:58,060 --> 00:05:59,180
Of course it can.

82
00:05:59,180 --> 00:06:04,940
Whatever purpose we're developing technology for, we're also making that technology cheaper

83
00:06:04,940 --> 00:06:11,660
and easier for all other types of purposes simultaneously, and that's a huge thing we

84
00:06:11,660 --> 00:06:13,220
have to factor.

85
00:06:13,220 --> 00:06:19,500
From a conflict point of view, obviously people with stone age technology can't cause a war

86
00:06:19,500 --> 00:06:23,740
that blows the world up, and people with bronze age technology can't cause a war that blows

87
00:06:23,740 --> 00:06:24,740
the world up.

88
00:06:24,740 --> 00:06:28,820
The harm is proportional to the amount of tech, so as we move into exponentially more

89
00:06:28,820 --> 00:06:36,260
powerful tech, we can't continue to use it with the types of conflict orientation and

90
00:06:36,260 --> 00:06:38,420
irresponsibility we used previous tech.

91
00:06:38,420 --> 00:06:43,180
The other thing is that even when we're not using tech for intentionally conflict oriented

92
00:06:43,180 --> 00:06:49,220
purposes, all of the tech we use does externalize harm in different ways.

93
00:06:49,220 --> 00:06:54,580
So whether we're talking about agricultural technology where the nitrogen fertilizer fed

94
00:06:54,580 --> 00:06:59,420
a lot of people but also causes all the dead zones in the ocean and soil erosion and biodiversity

95
00:06:59,420 --> 00:07:05,340
loss, exponentially more technology also means exponentially more externalities.

96
00:07:05,340 --> 00:07:09,660
And so we can't handle exponential war and we can't handle exponential externalities.

97
00:07:09,660 --> 00:07:15,220
So we have to change our relationship with technology really fundamentally and say no

98
00:07:15,220 --> 00:07:20,500
other animal have the ability to destroy the biosphere that it depends upon.

99
00:07:20,500 --> 00:07:21,500
We now do.

100
00:07:21,500 --> 00:07:25,840
We did not for all of human history, so we didn't have to really wrestle with that power.

101
00:07:25,840 --> 00:07:31,820
We did kill and enslave and genocide and every previous civilization doesn't still exist

102
00:07:31,820 --> 00:07:37,160
because they all ended up collapsing mostly for reasons that were largely self-induced.

103
00:07:37,160 --> 00:07:41,660
Even when wars happened, oftentimes a war that overtook a civilization was from an enemy

104
00:07:41,660 --> 00:07:46,700
that was less powerful than ones that they had vanquished in their prime.

105
00:07:46,700 --> 00:07:50,260
They had already went through some internal institutional decay from infighting and things

106
00:07:50,260 --> 00:07:51,420
like that.

107
00:07:51,420 --> 00:07:54,540
Many early civilizations died from environmentally induced causes.

108
00:07:54,540 --> 00:07:56,220
They cut down all the trees.

109
00:07:56,220 --> 00:07:59,200
They over stripped the soil of nutrients.

110
00:07:59,200 --> 00:08:03,540
So civilizational breakdown is actually the norm.

111
00:08:03,540 --> 00:08:05,260
It's just never been at a global level.

112
00:08:05,260 --> 00:08:07,980
Now we don't live in the United States or China.

113
00:08:07,980 --> 00:08:13,740
We live in a place where the cell phone that we're watching this on or the computer we're

114
00:08:13,740 --> 00:08:19,780
watching it on took six continent supply chains to make communicating via satellites

115
00:08:20,300 --> 00:08:24,580
so we live in a kind of global civilization where none of the countries are actually

116
00:08:24,580 --> 00:08:27,180
autonomous for fundamental things that they need.

117
00:08:27,180 --> 00:08:34,860
Now that we do have the ability to destroy the biosphere either very rapidly through

118
00:08:34,860 --> 00:08:41,300
exponential technology like synthetic biology or AI or warfare or kind of slowly through

119
00:08:41,300 --> 00:08:45,100
the limits of growth and environmental issues but that's not all that slow.

120
00:08:45,100 --> 00:08:51,180
If you have the power to destroy the nature that you depend upon you have to consciously

121
00:08:51,180 --> 00:08:53,900
steward it or you'll self-terminate.

122
00:08:53,900 --> 00:09:01,300
So the gist is we don't have evolutionary capacities.

123
00:09:01,300 --> 00:09:04,020
We have trans evolutionary capacities meaning-

124
00:09:04,020 --> 00:09:05,900
What's the difference here?

125
00:09:05,900 --> 00:09:06,900
Yeah.

126
00:09:06,900 --> 00:09:10,100
So and I'm meaning evolution in a biologic evolution sense.

127
00:09:10,100 --> 00:09:17,460
So another animal has the capacities that it has corporeally built into its body based

128
00:09:17,460 --> 00:09:18,460
on its genes.

129
00:09:18,460 --> 00:09:22,580
So a predator can't become radically more predatory quickly.

130
00:09:22,580 --> 00:09:27,700
It is only through genetic mutation that maybe it becomes slightly faster or has slightly

131
00:09:27,700 --> 00:09:32,420
bigger teeth and then it's going to be a relatively small change and then there will be co-selection.

132
00:09:32,420 --> 00:09:36,340
The slightly more effective predator will eat the slightly slower preys which means

133
00:09:36,340 --> 00:09:41,780
that the faster prey genes and breed and you get this kind of co-selective process.

134
00:09:41,780 --> 00:09:47,360
We threw our ability to build tools and then tools on tools, recursive abstraction.

135
00:09:47,360 --> 00:09:53,860
If you look at a true apex predator, you look at an orca in the ocean, an orca maybe can

136
00:09:53,860 --> 00:09:59,500
catch one fish at a time, one tuna at a time, then you look at a trawling boat that has

137
00:09:59,500 --> 00:10:03,340
a mile long drift net that can pull up 100,000 fish at once.

138
00:10:03,340 --> 00:10:05,340
They're not apex predators, right?

139
00:10:05,340 --> 00:10:07,620
Like it's wrong to think of us as apex predators.

140
00:10:07,620 --> 00:10:14,580
We have power that is not encoded in our bodies, extra corporeal technological capacity.

141
00:10:14,580 --> 00:10:17,980
You look at a nuclear bomb explosion versus a pissed off polar bear.

142
00:10:17,980 --> 00:10:21,580
They're not similar levels of destructive capacity.

143
00:10:21,580 --> 00:10:26,860
So since we have beyond evolutionary capacity, we actually have to have beyond evolutionary

144
00:10:26,860 --> 00:10:30,140
motive to guide that capacity.

145
00:10:30,140 --> 00:10:35,220
And if you want to say that mythopoetically, it's if you have the power of gods and by

146
00:10:35,220 --> 00:10:37,660
gods here, like I mean little G, right?

147
00:10:37,660 --> 00:10:42,580
I mean it mythopoetically meaning you can make species extinct.

148
00:10:42,580 --> 00:10:44,100
You can destroy ecosystems.

149
00:10:44,100 --> 00:10:48,940
You can create an Anthropocene where the largest effect on the geology of the planet is human

150
00:10:48,940 --> 00:10:49,940
activity.

151
00:10:49,940 --> 00:10:51,980
You can genetically engineer new species, right?

152
00:10:51,980 --> 00:10:55,940
That's much closer to the power of gods than it is the power of an apex predator.

153
00:10:55,940 --> 00:10:59,740
If you don't also have the love and wisdom of gods and prudence of gods to guide it,

154
00:10:59,740 --> 00:11:01,300
it doesn't go well.

155
00:11:01,300 --> 00:11:09,580
And so, you know, that is just another way of saying if you have recursive abstraction

156
00:11:09,580 --> 00:11:17,420
on tools that gives us and tools and coordination that give us the radically more than evolutionary

157
00:11:17,420 --> 00:11:22,860
capacity to affect the world, we have to move into trans evolutionary motive, which means

158
00:11:22,860 --> 00:11:26,180
the same recursive abstraction that we're doing right now saying, oh yeah, I guess it

159
00:11:26,180 --> 00:11:33,020
makes sense that we can't run an exponential financial system that's attached to a linear

160
00:11:33,020 --> 00:11:37,220
materials economy that takes stuff out of nature faster than it can be replenished and

161
00:11:37,220 --> 00:11:40,740
turns it into trash and pollution in nature faster than it can be processed.

162
00:11:40,740 --> 00:11:44,500
You can't do that exponentially forever on a finite planet, so we have to do something

163
00:11:44,500 --> 00:11:48,740
fundamentally different, which means you can't orient towards continued, maximized growth

164
00:11:48,740 --> 00:11:52,020
and maximized conflict orientation forever.

165
00:11:52,020 --> 00:11:54,460
So that's what I mean by a trans evolutionary motive.

166
00:11:54,860 --> 00:12:02,020
Is it naive to think that we need a global government and we can make a global governance?

167
00:12:02,020 --> 00:12:09,820
When you look at the problem of countries having competitive dynamics with each other

168
00:12:09,820 --> 00:12:16,620
where nobody wants to price carbon properly, because if they do, their own economy will

169
00:12:16,620 --> 00:12:20,900
be so damaged relative to whoever doesn't that the radically decreased geopolitical

170
00:12:20,900 --> 00:12:25,500
power will express itself as less military power, less trade power, and particularly

171
00:12:25,500 --> 00:12:29,020
with whoever is at the leading edge of guiding the world system.

172
00:12:29,020 --> 00:12:36,660
This classic, the US isn't going to if China doesn't and vice versa, so then everyone is

173
00:12:36,660 --> 00:12:47,020
mostly actually just in an economics race that is also bound to an actual arms race.

174
00:12:47,020 --> 00:12:49,500
And that's true for pricing carbon and climate change.

175
00:12:49,500 --> 00:12:54,780
It's also true for fishing of the oceans and aerosols and on and on and on.

176
00:12:54,780 --> 00:13:00,460
So if you have an issue like the atmosphere, aerosols and the atmosphere and ozone layer,

177
00:13:00,460 --> 00:13:05,540
or you have an issue like the oceans or climate change, no country can solve that problem.

178
00:13:05,540 --> 00:13:09,700
And any country that does the thing that is doing its share that is economically disadvantaged

179
00:13:09,700 --> 00:13:13,940
in the short term by doing it, it just isn't going to do that if everyone else doesn't

180
00:13:13,940 --> 00:13:16,020
because they are caught in the competitive dynamics.

181
00:13:17,020 --> 00:13:21,460
So when you look at that, you're like, all right, well, we need global government because

182
00:13:21,460 --> 00:13:22,500
we have global issues.

183
00:13:22,500 --> 00:13:25,700
We don't just have national issues and you have to have governance at the level that

184
00:13:25,700 --> 00:13:27,980
you have issues.

185
00:13:27,980 --> 00:13:32,260
But then of course, most thinking people aren't really a big fan of the idea of global government

186
00:13:32,260 --> 00:13:37,380
because it's not a great idea to have unchecked power, though we don't have a good history

187
00:13:37,380 --> 00:13:40,780
of being good stewards of unchecked power.

188
00:13:40,780 --> 00:13:46,900
And so in many modern governments in the United States, it was kind of like the foundation

189
00:13:46,900 --> 00:13:51,820
of the whole idea was let's separate church and state, let's separate the judicial branch

190
00:13:51,820 --> 00:13:55,020
and the legislative branch and the executive branch.

191
00:13:55,020 --> 00:13:59,020
Let's even separate the legislative branches in the separate houses.

192
00:13:59,020 --> 00:14:02,340
Let's try to create as much check and balance on power as possible.

193
00:14:02,340 --> 00:14:07,580
So if you had a one world government that had enough power to be able to price carbon

194
00:14:07,580 --> 00:14:12,740
properly and enforce fishing laws and et cetera, how do you prevent it from becoming

195
00:14:12,740 --> 00:14:14,980
corrupted or captured?

196
00:14:14,980 --> 00:14:20,500
And so we need global government and we don't want global government and so that this is

197
00:14:20,500 --> 00:14:24,980
this like you have catastrophes on one hand that need to be avoided and that typically

198
00:14:24,980 --> 00:14:29,860
looks like more control mechanisms of things that if you don't control will lead to catastrophe

199
00:14:29,860 --> 00:14:34,620
and the control mechanisms typically lead to dystopias.

200
00:14:35,060 --> 00:14:37,340
So we want something that is not catastrophes or dystopias.

201
00:14:37,340 --> 00:14:41,460
We kind of call this the third attractor and that means you have to have control mechanisms

202
00:14:41,460 --> 00:14:44,860
that prevent catastrophes, but you have to have checks and balances on the power within

203
00:14:44,860 --> 00:14:45,860
those.

204
00:14:45,860 --> 00:14:46,860
How do you do that?

205
00:14:46,860 --> 00:14:50,620
So global governance and global government are not the same thing, right?

206
00:14:50,620 --> 00:14:54,300
Global government, the idea that there's some centralized global monopoly of violence,

207
00:14:54,300 --> 00:14:55,700
the bad idea.

208
00:14:55,700 --> 00:15:00,100
The idea that there is some more effective process of global coordination, even whether

209
00:15:00,140 --> 00:15:06,660
it's a more effective process of nations engaging in multilateral agreements that can

210
00:15:06,660 --> 00:15:12,300
be facilitated by technology that can make the participation or violation of those agreements

211
00:15:12,300 --> 00:15:20,300
more transparent or there is some process of global governance that has to occur where

212
00:15:20,300 --> 00:15:24,460
there's both effective power for enforcement.

213
00:15:24,460 --> 00:15:29,100
This is why we can solve those types of coordination problems to some degree, those race to the

214
00:15:29,100 --> 00:15:35,060
bottom within a country where you have a monopoly of violence because the law on monopoly

215
00:15:35,060 --> 00:15:38,260
of violence just basically says, no, you're not allowed to cut down any of those trees.

216
00:15:38,260 --> 00:15:42,220
That's a national park and if you try, the police will stop you and they have more capacity

217
00:15:42,220 --> 00:15:43,940
for violence than you do.

218
00:15:43,940 --> 00:15:47,660
With international issues where you don't actually have international enforcement, it's

219
00:15:47,660 --> 00:15:48,660
really, really tricky.

220
00:15:48,660 --> 00:15:54,140
So for all of the really global issues, and that looks like it's in each nation's interests

221
00:15:54,140 --> 00:15:58,340
to burn the coal as fast as it can and the oil, it's in each nation's interests to win

222
00:15:58,460 --> 00:16:01,900
the AI arms race, even though that increases the likelihood that we all die from it in

223
00:16:01,900 --> 00:16:03,220
the long term.

224
00:16:03,220 --> 00:16:10,340
So global governance that has appropriate checks and balances is a tricky topic, but

225
00:16:10,340 --> 00:16:12,300
it's a necessary topic.

226
00:16:12,300 --> 00:16:13,780
What gives you hope today?

227
00:16:13,780 --> 00:16:16,780
A lot of things can be hoped.

228
00:16:16,780 --> 00:16:27,900
I have noticed in my own work, people in top positions of power and major institutions

229
00:16:27,980 --> 00:16:37,820
that affect the world being radically more aware of things that are fundamentally unviable

230
00:16:37,820 --> 00:16:45,020
about this world system and interested in deeper changes and actually starting to try

231
00:16:45,020 --> 00:16:48,860
to implement some things just even in the last couple of years than I had ever experienced

232
00:16:48,860 --> 00:16:50,300
previously.

233
00:16:50,300 --> 00:16:57,380
So the idea that, you know, the kind of behavior that individuals can do on their own matters

234
00:16:57,460 --> 00:17:01,820
and the kind of stuff we can do locally like, you know, prototyping new types of communities

235
00:17:01,820 --> 00:17:09,140
and new types of cities, you don't solve climate change in time and you don't solve planetary

236
00:17:09,140 --> 00:17:12,460
boundaries in time and you don't solve AI risk that way, right?

237
00:17:12,460 --> 00:17:16,300
That requires kind of agreement from existing top-down organizations.

238
00:17:16,300 --> 00:17:18,420
They can't actually innovate a new world.

239
00:17:18,420 --> 00:17:22,380
They can just stop bad things from happening with the right kinds of agreements to innovate

240
00:17:22,380 --> 00:17:29,660
a new world actually does require local and more participatory activity.

241
00:17:29,660 --> 00:17:39,940
But the fact that after COVID and after the extreme political polarization that has happened

242
00:17:39,940 --> 00:17:47,540
and after how much of Australia burned and then flooded and, you know, now with the war

243
00:17:47,540 --> 00:17:54,380
on Ukraine and I think there was a situation where previously people who were thinking

244
00:17:54,380 --> 00:18:00,740
about it and who were prescient realized this world system is destabilizing and is fundamentally

245
00:18:00,740 --> 00:18:02,180
not sustainable.

246
00:18:02,180 --> 00:18:04,580
Most of the people who were administrating it didn't think that.

247
00:18:04,580 --> 00:18:09,460
Now almost everybody thinks that and that's actually something that gives me hope.

248
00:18:09,460 --> 00:18:10,700
Great.

249
00:18:10,700 --> 00:18:15,220
How much time do we have to react to avoid extinction?

250
00:18:15,980 --> 00:18:19,780
Some species go extinct every day as a result of human activity.

251
00:18:19,780 --> 00:18:26,700
So for them, we're already past existential risk, you know, Kiev was an incredibly progressive

252
00:18:26,700 --> 00:18:28,420
place not very long ago.

253
00:18:28,420 --> 00:18:33,940
It wouldn't have seemed like a place where eminent catastrophic risk was coming for many

254
00:18:33,940 --> 00:18:43,820
people and, you know, that's even true of Syria not that long ago and you see the pictures

255
00:18:43,820 --> 00:18:47,740
of what culture was like in 1968 in Iran.

256
00:18:47,740 --> 00:18:50,900
So it's not like how long do we have before catastrophe hits.

257
00:18:50,900 --> 00:18:55,180
We're already in a rolling global catastrophe.

258
00:18:55,180 --> 00:19:00,940
Like how long does Australia have before it burns that already happened, you know, and

259
00:19:00,940 --> 00:19:04,580
from extreme weather events that are a result of human induced activity poor environmental

260
00:19:04,580 --> 00:19:11,340
management and problems with utility companies and overuse of groundwater and climate change.

261
00:19:11,340 --> 00:19:16,940
And how quickly does war escalate as a result of what's happening in Ukraine at larger scale

262
00:19:16,940 --> 00:19:23,700
and already what we see in regarding Taiwan and Azerbaijan and Armenia and Iran and so

263
00:19:23,700 --> 00:19:25,940
many places.

264
00:19:25,940 --> 00:19:32,140
These things could move very fast or more slowly in ways that are chaotic and totally

265
00:19:32,140 --> 00:19:34,780
unpredictable.

266
00:19:34,780 --> 00:19:40,020
When you look at things like the planetary boundaries, how long until we pass certain

267
00:19:40,100 --> 00:19:47,140
planetary boundaries, you'll hear people talk about this thing happens in 2050 and this

268
00:19:47,140 --> 00:19:50,780
thing happens and by the end of the century or whatever with climate change.

269
00:19:50,780 --> 00:19:55,780
But we've already passed some of the planetary boundaries, you know, there's a study just

270
00:19:55,780 --> 00:20:01,900
published in the American Chemical Society Journal saying that certain toxic chemicals

271
00:20:01,900 --> 00:20:06,140
in rainwater kind of ubiquitous around the world are past the EPA thresholds for human

272
00:20:06,140 --> 00:20:07,700
health.

273
00:20:07,700 --> 00:20:12,980
And this was particularly the the fluorinated surfactants which don't break down, right?

274
00:20:12,980 --> 00:20:14,900
So they come forever chemicals.

275
00:20:14,900 --> 00:20:19,980
But the idea that things that are carcinogens and cause birth defects and are endocrine

276
00:20:19,980 --> 00:20:25,580
disruptors in rainwater all around the world are past the levels of human health tolerability

277
00:20:25,580 --> 00:20:26,580
is a huge deal.

278
00:20:26,580 --> 00:20:33,180
It means even if you go get off grid as can be and try to live off the land, you can't.

279
00:20:33,180 --> 00:20:37,540
And how quickly we're producing those chemicals, not only is there a cumulative effect of them

280
00:20:37,540 --> 00:20:43,220
because they're persistent, but we're also increasing our production of them exponentially.

281
00:20:43,220 --> 00:20:44,860
And so how long do we have?

282
00:20:44,860 --> 00:20:51,780
We're already in a situation of a breakdown of a world system.

283
00:20:51,780 --> 00:20:54,300
It's already existential for many species.

284
00:20:54,300 --> 00:20:58,140
It's already catastrophic for people in many areas of the world.

285
00:20:58,140 --> 00:21:06,820
And so I would reorient the question to be more like, is there anything that we can do

286
00:21:06,820 --> 00:21:09,860
to have it not be totalizing?

287
00:21:09,860 --> 00:21:11,620
And the answer is yes.

288
00:21:11,620 --> 00:21:19,260
And the answer time wise on that is the full life attention of everyone as best as possible

289
00:21:19,260 --> 00:21:23,100
directed at better understanding the issues and participating in the solutions is what's

290
00:21:23,100 --> 00:21:24,100
required.

291
00:21:24,100 --> 00:21:30,420
In individual level, we've become a bit lazy maybe because we think like there is always

292
00:21:30,420 --> 00:21:33,900
a solution and we don't really need to act.

293
00:21:33,900 --> 00:21:38,780
How to wake up and also how do you get the news because you have like so many news in

294
00:21:38,780 --> 00:21:39,780
different directions.

295
00:21:39,780 --> 00:21:44,180
Like we don't know who to believe and we don't know like we're not sure we need to act because

296
00:21:44,180 --> 00:21:47,140
things always have a solution by themselves.

297
00:21:47,140 --> 00:21:53,460
There's a really interesting book called The Politics of the Invisible written after Chernobyl

298
00:21:53,460 --> 00:22:00,380
because after the Chernobyl explosion, the uranium is invisible, right?

299
00:22:00,380 --> 00:22:02,540
We can't see it with the human eye.

300
00:22:02,540 --> 00:22:06,620
Obviously now COVID that's invisible and yet totally lethal.

301
00:22:06,620 --> 00:22:15,220
And what he was exploring in Politics of the Invisible is because of modern technology and

302
00:22:15,220 --> 00:22:20,300
chemistry, we can make things that are totally lethal that we can't see that require people

303
00:22:20,300 --> 00:22:26,420
with Geiger counters and the ability to do physics that not everyone can do to be able

304
00:22:26,420 --> 00:22:27,820
to determine safety levels.

305
00:22:27,820 --> 00:22:31,420
How does that work with democracy when most people don't have the capacity to do that?

306
00:22:31,420 --> 00:22:36,740
So you'll see currently a lot of people doubting climate change science, but nobody can actually,

307
00:22:36,740 --> 00:22:42,540
the average citizen can't run the IPCC's mathematical models to say they work or they don't work

308
00:22:42,540 --> 00:22:44,140
or they...

309
00:22:44,140 --> 00:22:53,140
And so people are largely kind of left to faith and you then end up getting politics driving

310
00:22:53,140 --> 00:22:57,740
people to either be kind of pro-institutional or anti-institutional.

311
00:22:57,740 --> 00:23:01,860
And the institutions get things wrong, so it's easy to be anti-institutional and neither

312
00:23:01,860 --> 00:23:06,420
of the positions are actually viable and there is something other than truth, which is the

313
00:23:06,420 --> 00:23:08,820
movement to power motivated in both of them.

314
00:23:08,820 --> 00:23:13,140
I see that when people think someone else will come up with a solution, they feel kind

315
00:23:13,140 --> 00:23:18,980
of unmotivated, but also when people think there is no solution, they feel unmotivated.

316
00:23:18,980 --> 00:23:26,060
And this is also something I find really interesting is when I talk to someone who has a really

317
00:23:26,060 --> 00:23:34,540
fervent adamant view about whatever it is, that whether it's vaccines or masks or what

318
00:23:34,540 --> 00:23:40,340
should happen in Russia, Ukraine or abortion law, whatever, they go from complete certainty

319
00:23:40,340 --> 00:23:43,540
without understanding the position of the other side or all the complexity or nuance

320
00:23:43,540 --> 00:23:45,140
well.

321
00:23:45,140 --> 00:23:49,460
And if I challenge it and not, regardless of which side it is, and show them the increased

322
00:23:49,460 --> 00:23:53,580
complexity, okay, well if we price carbon that way and China doesn't, then autocracy

323
00:23:53,620 --> 00:24:00,260
ends up running the world, so you're voting democracy out and whatever it is, then for

324
00:24:00,260 --> 00:24:04,780
so many people, the first response when you increase, show them the way they're thinking

325
00:24:04,780 --> 00:24:08,980
about it doesn't actually map to the complexity of the problem, they go from utter certainty

326
00:24:08,980 --> 00:24:10,580
to nihilism in one step.

327
00:24:10,580 --> 00:24:13,700
They're like, oh fuck it, it's too hard, it's too complex, I give up.

328
00:24:13,700 --> 00:24:20,540
And to move from certainty to nihilism in one step is so damn lazy, like cognitively,

329
00:24:20,540 --> 00:24:22,820
emotionally, epistemically lazy.

330
00:24:22,820 --> 00:24:27,260
And so I want people to go from certainty to like, actually I don't understand this

331
00:24:27,260 --> 00:24:28,260
all that well.

332
00:24:28,260 --> 00:24:33,660
Actually, climate change or global science or policy on this thing is pretty complex.

333
00:24:33,660 --> 00:24:36,780
There are experts who spend their whole life working on it who disagree.

334
00:24:36,780 --> 00:24:41,860
That doesn't mean there aren't solutions, but the one that was fed to me that everybody

335
00:24:41,860 --> 00:24:45,700
on my political side agrees with and everyone on the other side disagrees with is probably

336
00:24:45,700 --> 00:24:49,100
not a fair version of the whole truth.

337
00:24:49,100 --> 00:24:52,140
So I'm not going to give up because I don't know.

338
00:24:52,140 --> 00:24:54,220
I'm not going to hold the certainty that I know because I don't.

339
00:24:54,220 --> 00:24:58,980
I'm going to work to try to understand competently while recognizing I don't yet.

340
00:24:58,980 --> 00:25:04,420
And then even once I get to much deeper understanding, I'll still recognize how much stuff I don't

341
00:25:04,420 --> 00:25:07,180
know that's relevant and new information that might come in.

342
00:25:07,180 --> 00:25:13,980
So I want people to be much more epistemically rigorous and epistemically humble at the same

343
00:25:13,980 --> 00:25:17,980
time, epistemology meaning how we go about knowing things.

344
00:25:17,980 --> 00:25:24,020
So I want them to work much harder at trying to come to understand while having much less

345
00:25:24,020 --> 00:25:26,340
certainty about their current level of understanding.

346
00:25:26,340 --> 00:25:31,140
So when you ask what sources should people go to for news or whatever, the ones they don't

347
00:25:31,140 --> 00:25:34,740
currently go to is the first answer.

348
00:25:34,740 --> 00:25:36,900
And then, of course, progressively better sources.

349
00:25:36,900 --> 00:25:41,140
Not all the sources independent of political spinner are equally good.

350
00:25:41,140 --> 00:25:47,740
But when you can see where do the various earnest experts on the topic disagree and

351
00:25:47,740 --> 00:25:52,780
you at least understand those positions pretty well, then you start to have a sense of the

352
00:25:52,780 --> 00:25:53,780
topic.

353
00:25:53,780 --> 00:25:59,300
As a philosopher and because you spoke about politics, do you want to stay away from politics

354
00:25:59,300 --> 00:26:02,500
or are you into politics?

355
00:26:02,500 --> 00:26:08,700
Politics meaning how people organize and how they coordinate and how they make sense of

356
00:26:08,700 --> 00:26:10,980
the world together so they can make choices together.

357
00:26:10,980 --> 00:26:14,500
No, I'm totally focused on that.

358
00:26:14,500 --> 00:26:23,260
The current political system and the United States does not do a very good job of helping

359
00:26:23,260 --> 00:26:27,900
people collectively make sense of issues well, collectively identify all the values that

360
00:26:27,900 --> 00:26:33,420
matter that are shared values and then collectively make good choices in the presence of the shared

361
00:26:33,420 --> 00:26:36,260
sense making and shared values generation.

362
00:26:36,260 --> 00:26:42,820
So it's not that I think there is never a time to engage in voting for a particular

363
00:26:42,820 --> 00:26:47,860
candidate or on a particular proposition, but how to engage in metapolitics, meaning

364
00:26:47,860 --> 00:26:53,500
how to evolve this political system and economic system, how to evolve the political economy

365
00:26:53,500 --> 00:26:58,060
along with evolving the infrastructure and tech stack and the culture and value system

366
00:26:58,060 --> 00:27:01,700
simultaneously because all three of those inter-effect each other.

367
00:27:01,700 --> 00:27:07,020
The culture, the political economy and the infrastructure and technology, they all inter-effect

368
00:27:07,020 --> 00:27:08,020
each other.

369
00:27:08,020 --> 00:27:12,580
So you can't change any of them without changing all of them to think through what has to happen

370
00:27:12,580 --> 00:27:17,020
in all of those for a viable world to come about.

371
00:27:17,020 --> 00:27:19,540
I'm very interested in that.

372
00:27:19,540 --> 00:27:20,540
Okay.

373
00:27:20,540 --> 00:27:24,780
Let's speak a bit about you, something very interesting I found.

374
00:27:24,780 --> 00:27:28,620
You mentioned you were homeschooled by your parents.

375
00:27:28,620 --> 00:27:36,420
Which qualities did your parents manage to let flourish in you that might not have been

376
00:27:36,420 --> 00:27:42,380
so important also in the traditional education when you're changing teachers every year?

377
00:27:42,380 --> 00:27:49,260
I was homeschooled, I did go to school, both private and public schools for little bits

378
00:27:49,260 --> 00:27:50,260
throughout my life.

379
00:27:50,260 --> 00:27:53,900
So I have some experience of it, but most of my childhood was homeschooled, but it was

380
00:27:53,900 --> 00:27:59,820
not traditional homeschooling, meaning I didn't have the state curriculum and just do it at

381
00:27:59,820 --> 00:28:00,820
home.

382
00:28:00,820 --> 00:28:06,900
My parents were kind of interested in running an educational experiment that is a little

383
00:28:06,900 --> 00:28:10,780
bit closer to what people call unschooling today, but there was just no curriculum.

384
00:28:11,340 --> 00:28:12,340
Okay.

385
00:28:12,340 --> 00:28:15,020
What they felt you need to learn.

386
00:28:15,020 --> 00:28:16,500
It's not what they felt.

387
00:28:16,500 --> 00:28:25,420
Their hypothesis was expose the kids to all the different topics, see which ones they're

388
00:28:25,420 --> 00:28:29,620
interested in, facilitate their interest, and kind of trust them.

389
00:28:29,620 --> 00:28:35,980
So it's aligned with some of the ideas of Montessori and Dewey and Constructivism.

390
00:28:35,980 --> 00:28:39,780
But you know, radical had no curriculum at all.

391
00:28:39,780 --> 00:28:47,020
But and I'm not saying that is what I would advise, but there's a lot good in that.

392
00:28:47,020 --> 00:28:57,820
And what qualities that facilitated in me that most educational systems don't is all

393
00:28:57,820 --> 00:29:00,540
of my studies were things I was interested in.

394
00:29:00,540 --> 00:29:05,260
And so my interest in learning was actually growing all the time, right?

395
00:29:05,260 --> 00:29:08,860
There was never a place where I wanted to get out of school and go play or do something

396
00:29:08,860 --> 00:29:15,540
else where learning felt like a burden or where I ended up not having any negative association

397
00:29:15,540 --> 00:29:16,540
with study.

398
00:29:16,540 --> 00:29:20,020
And I had only positive association because I was studying things I wanted to study.

399
00:29:20,020 --> 00:29:26,620
So I find that people tend to become good at things they really enjoy.

400
00:29:26,620 --> 00:29:33,020
And so facilitating, like even if you have a curriculum, really paying attention to where

401
00:29:33,020 --> 00:29:36,500
a student's interests are and where their passions are.

402
00:29:36,500 --> 00:29:41,260
And if there's a topic that isn't appealing to them, trying to find a way that actually

403
00:29:41,260 --> 00:29:44,980
has it really appeal as opposed to just forcing them to do the thing makes a huge difference

404
00:29:44,980 --> 00:29:48,700
not to their learning of that topic, but to their relationship to learning itself.

405
00:29:48,700 --> 00:29:56,100
What a special event that put you on this path of trying to see the truth, what's happening

406
00:29:56,100 --> 00:29:59,700
and observe the complexity of the world.

407
00:29:59,700 --> 00:30:00,700
Lots of events.

408
00:30:00,700 --> 00:30:11,460
I mean, some people have a near death experience turning point that is really kind of singular.

409
00:30:11,460 --> 00:30:15,820
I think most people's life path unfolds from lots of things.

410
00:30:15,820 --> 00:30:21,300
So as I'm mentioning being homeschooled by parents who are obviously kind of interested

411
00:30:21,300 --> 00:30:26,540
in childhood development and the books my parents read to me as bedtime stories were

412
00:30:26,540 --> 00:30:32,620
Buckminster Fuller's Design Science and Fritschof Capra Systems Theory and Eastern

413
00:30:32,620 --> 00:30:34,380
Vedic Philosophy and things like that.

414
00:30:34,380 --> 00:30:40,140
So there were people who were thinking about what is the world?

415
00:30:40,140 --> 00:30:41,260
How does the world work?

416
00:30:41,260 --> 00:30:46,140
How do we integrate across the various philosophic and scientific traditions?

417
00:30:46,140 --> 00:30:47,540
How do we improve civilization?

418
00:30:47,540 --> 00:30:50,500
Those were kind of like just the core thoughts.

419
00:30:50,500 --> 00:30:54,780
And so I didn't really have to get on that path.

420
00:30:54,780 --> 00:31:02,420
And then a big part of my study as a kid was not just studying various areas of philosophy

421
00:31:02,420 --> 00:31:06,020
or science or whatever, but also being actively engaged in activism.

422
00:31:06,020 --> 00:31:13,420
My mom was particularly into kind of hands-on activism with whether it was helping the local

423
00:31:13,420 --> 00:31:19,780
animal shelter or larger kind of factory farm animal rights issues or environmental issues.

424
00:31:19,780 --> 00:31:26,060
So being engaged in activism and then seeing what the problems in the world were and then

425
00:31:26,060 --> 00:31:30,380
similarly having a system science and kind of design science background to look at it

426
00:31:30,380 --> 00:31:32,700
and say, how are these problems interconnected?

427
00:31:32,700 --> 00:31:34,860
What do they have in common as generative dynamics?

428
00:31:34,860 --> 00:31:41,260
What would it take to address them more comprehensively because it's not that hard to see that whether

429
00:31:41,260 --> 00:31:45,020
we're talking about issues in healthcare or issues in war, issues in politics or issues

430
00:31:45,020 --> 00:31:49,740
in the environment, things like perverse economic incentive or one of the drivers of all of

431
00:31:49,740 --> 00:31:50,740
them.

432
00:31:50,740 --> 00:31:53,500
So it's like, well, how do we think through an economic system that doesn't have perverse

433
00:31:53,500 --> 00:31:54,500
incentive?

434
00:31:54,500 --> 00:31:58,860
Yeah, I would say it was working across many different areas of activism, seeing how they

435
00:31:58,860 --> 00:32:03,700
related to seeing why the solutions that we were working on weren't adequate because they

436
00:32:03,700 --> 00:32:07,820
didn't address the deeper dynamics.

437
00:32:07,820 --> 00:32:10,780
Those were kind of key early things for me.

438
00:32:10,780 --> 00:32:11,780
Thank you very much, Daniel.

439
00:32:11,780 --> 00:32:12,780
Yeah.

440
00:32:12,780 --> 00:32:13,780
Thank you.

441
00:32:13,780 --> 00:32:14,780
Thank you.

442
00:32:14,780 --> 00:32:15,780
Bye.

443
00:32:15,780 --> 00:32:16,780
Bye.

444
00:32:16,780 --> 00:32:17,780
Bye.

445
00:32:17,780 --> 00:32:18,780
Bye.

446
00:32:18,780 --> 00:32:19,780
Bye.

447
00:32:19,780 --> 00:32:20,780
Bye.

448
00:32:20,780 --> 00:32:21,780
Bye.

449
00:32:21,780 --> 00:32:22,780
Bye.

