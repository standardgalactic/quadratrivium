start	end	text
0	10480	Daniel Schmartenberger, thank you for being with us.
10480	11480	Thanks for having me.
11480	15440	So, you're a philosopher, a founding member of the Consilience Project.
15440	21200	The goal of this conversation today is to analyze the direction our civilization is
21200	28160	taking in half an hour, because you've been doing like so many great podcasts about the
28160	34120	metacrisis during lasting three to four hours, and I suggest that people go and watch them
34120	35120	on the Internet.
35120	36120	They're very good.
36120	37960	We'll start with the harvest of the day.
37960	44280	The question I'm asking to all the guests here for Harvard Podcasts, if something easy
44280	49760	or simple could be done and would make the world a better place, what would it be for
49760	51000	you, Daniel?
51000	54800	When I saw the note that you sent me, that that would be a last question.
54800	58080	Is there a simple or easy thing that everyone could do that make the world a better place?
58080	59080	That kind of cringed?
59080	65120	Because I usually really am not a fan of that question, because the world needs so many
65120	70120	different kinds of things done that require different skills and capacities and orientations
70120	74920	and to try to reduce it to some thing that would be true for everybody.
74920	78880	You get a platitude like be kind or loving or something like that, or you get something
78880	84480	like recycle or pick up trash or try to use less carbon or something that doesn't map
84480	86680	to the whole set of things that the world needs.
86680	95440	I think there's a process where movements have been associated with political processes
95440	100680	and markets in a way that it's like, here's this great catastrophe that'll happen if the
100680	105360	other side gets elected, so everybody needs to get out and vote for so and so.
105360	110480	That's like everybody can do a simple thing because we're relating to everybody as voters
110480	116120	or everybody donate to this cause or boycott this thing, but the complexity of the world's
116120	122600	issues from climate issues to AI risk to supply chain issues to electrical grid issues, like
122600	125040	there's no action like that.
125040	129360	There's no somebody to vote for or not to vote for thing to donate to that addresses
129360	130360	it.
130360	136240	One thing that is not necessarily easy, but is relatively simple, that would be great
136240	142440	if everybody in the world could do more, is to seek to try to understand other people's
142440	147680	perspectives much more deeply, particularly those that are most different than their own.
147680	158400	If you can try to take the opposite perspective on abortion, on gun control, on climate change,
158400	164320	on the Ukraine-Russia war, on the Chinese versus Western system, on any of those things,
164320	166000	on the Israel-Palestine issue.
166000	171960	If you can try to earnestly be able to make the argument that the person on the other
171960	176640	side would make as well enough that they don't have anything to add to it, and not
176640	182120	just as a rhetorical process, but connect to the values that they care about and what
182120	185840	it feels like to be them and see the world through their eyes, realizing that there might
185840	186840	be distortion.
186840	190920	There might be a lot of things missing, but there's not zero truth or zero value to it.
190920	200160	That process, if everyone did that, would actually result in addressing the metacrisis
200320	206080	in all of its complexity, the issues in synthetic biology risk, and pandemics, and escalation
206080	210560	pathways to warfare, and economic issues, and geopolitical issues, and all of them.
210560	217320	You can say that they either come down to conflict or externalities, like we cause harm
217320	223320	directly, intentionally, which a war is a great example of, or harm gets caused that
223320	226160	we didn't intend to cause.
226160	228040	No one intended to cause climate change.
228040	233720	We just wanted to have transportation and energy, and the secondary byproduct of that
233720	235800	was climate change.
235800	239760	All of the environmental issues, no one intentionally had a conflict with the environment that was
239760	240760	causing it.
240760	245880	It was the externality of optimizing something and causing harm somewhere else.
245880	249240	There are problems that we intentionally cause, and there are problems that we accidentally
249240	250240	cause.
250240	253640	Both of them would be corrected by seeking to understand all the perspectives more,
253640	259440	because if you sought to understand the perspectives well enough, conflict theory would evaporate.
259440	263560	Most of the mistakes, when you're trying to optimize for one thing and you end up causing
263560	267560	externalities to something else, somebody else saw that and knew that, and if you were
267560	273040	in wide enough conversations, then the thing that you're trying to optimize for that's
273040	276400	going to cause harm somewhere else someone else would have mentioned and said, actually,
276400	280040	let's improve your design or your strategy by factoring this.
280040	285240	Both the unintentional externalities and the intentional conflict would be resolved through
285240	291960	active perspective seeking and then perspectives and this is wonderful.
291960	296880	When you look at the history, as you said, humans seem to have a talent for innovation
296880	303200	and progress, but also a natural tendency for war and chaos.
303200	308760	These two tendencies fit each other and make things bigger and bigger, so greatest but
308760	312720	out of control technologies can cause a huge damage.
312720	321800	What do you think should be done about technologies and do they represent innovation or danger
321800	323560	for you?
323560	327560	First thing about technology is that even if we're not talking about a military technology,
327560	331560	we're talking about a technology for some other purpose, even if we develop a technology
331560	335540	for some non-military purpose, it will have a military application or some kind of conflict
335540	340900	oriented application, basically saying all technologies dual use.
340900	346220	Maybe we're doing the synthetic biology gene editing for trying to cure cancer, but as
346220	350660	we get better at making tools to do gene editing, can that be used for bio weapons?
350660	351660	Totally.
351660	356560	Maybe we're making the AI to try to do drug discovery, but can that same AI do autonomous
356560	358060	drones?
358060	359180	Of course it can.
359180	364940	Whatever purpose we're developing technology for, we're also making that technology cheaper
364940	371660	and easier for all other types of purposes simultaneously, and that's a huge thing we
371660	373220	have to factor.
373220	379500	From a conflict point of view, obviously people with stone age technology can't cause a war
379500	383740	that blows the world up, and people with bronze age technology can't cause a war that blows
383740	384740	the world up.
384740	388820	The harm is proportional to the amount of tech, so as we move into exponentially more
388820	396260	powerful tech, we can't continue to use it with the types of conflict orientation and
396260	398420	irresponsibility we used previous tech.
398420	403180	The other thing is that even when we're not using tech for intentionally conflict oriented
403180	409220	purposes, all of the tech we use does externalize harm in different ways.
409220	414580	So whether we're talking about agricultural technology where the nitrogen fertilizer fed
414580	419420	a lot of people but also causes all the dead zones in the ocean and soil erosion and biodiversity
419420	425340	loss, exponentially more technology also means exponentially more externalities.
425340	429660	And so we can't handle exponential war and we can't handle exponential externalities.
429660	435220	So we have to change our relationship with technology really fundamentally and say no
435220	440500	other animal have the ability to destroy the biosphere that it depends upon.
440500	441500	We now do.
441500	445840	We did not for all of human history, so we didn't have to really wrestle with that power.
445840	451820	We did kill and enslave and genocide and every previous civilization doesn't still exist
451820	457160	because they all ended up collapsing mostly for reasons that were largely self-induced.
457160	461660	Even when wars happened, oftentimes a war that overtook a civilization was from an enemy
461660	466700	that was less powerful than ones that they had vanquished in their prime.
466700	470260	They had already went through some internal institutional decay from infighting and things
470260	471420	like that.
471420	474540	Many early civilizations died from environmentally induced causes.
474540	476220	They cut down all the trees.
476220	479200	They over stripped the soil of nutrients.
479200	483540	So civilizational breakdown is actually the norm.
483540	485260	It's just never been at a global level.
485260	487980	Now we don't live in the United States or China.
487980	493740	We live in a place where the cell phone that we're watching this on or the computer we're
493740	499780	watching it on took six continent supply chains to make communicating via satellites
500300	504580	so we live in a kind of global civilization where none of the countries are actually
504580	507180	autonomous for fundamental things that they need.
507180	514860	Now that we do have the ability to destroy the biosphere either very rapidly through
514860	521300	exponential technology like synthetic biology or AI or warfare or kind of slowly through
521300	525100	the limits of growth and environmental issues but that's not all that slow.
525100	531180	If you have the power to destroy the nature that you depend upon you have to consciously
531180	533900	steward it or you'll self-terminate.
533900	541300	So the gist is we don't have evolutionary capacities.
541300	544020	We have trans evolutionary capacities meaning-
544020	545900	What's the difference here?
545900	546900	Yeah.
546900	550100	So and I'm meaning evolution in a biologic evolution sense.
550100	557460	So another animal has the capacities that it has corporeally built into its body based
557460	558460	on its genes.
558460	562580	So a predator can't become radically more predatory quickly.
562580	567700	It is only through genetic mutation that maybe it becomes slightly faster or has slightly
567700	572420	bigger teeth and then it's going to be a relatively small change and then there will be co-selection.
572420	576340	The slightly more effective predator will eat the slightly slower preys which means
576340	581780	that the faster prey genes and breed and you get this kind of co-selective process.
581780	587360	We threw our ability to build tools and then tools on tools, recursive abstraction.
587360	593860	If you look at a true apex predator, you look at an orca in the ocean, an orca maybe can
593860	599500	catch one fish at a time, one tuna at a time, then you look at a trawling boat that has
599500	603340	a mile long drift net that can pull up 100,000 fish at once.
603340	605340	They're not apex predators, right?
605340	607620	Like it's wrong to think of us as apex predators.
607620	614580	We have power that is not encoded in our bodies, extra corporeal technological capacity.
614580	617980	You look at a nuclear bomb explosion versus a pissed off polar bear.
617980	621580	They're not similar levels of destructive capacity.
621580	626860	So since we have beyond evolutionary capacity, we actually have to have beyond evolutionary
626860	630140	motive to guide that capacity.
630140	635220	And if you want to say that mythopoetically, it's if you have the power of gods and by
635220	637660	gods here, like I mean little G, right?
637660	642580	I mean it mythopoetically meaning you can make species extinct.
642580	644100	You can destroy ecosystems.
644100	648940	You can create an Anthropocene where the largest effect on the geology of the planet is human
648940	649940	activity.
649940	651980	You can genetically engineer new species, right?
651980	655940	That's much closer to the power of gods than it is the power of an apex predator.
655940	659740	If you don't also have the love and wisdom of gods and prudence of gods to guide it,
659740	661300	it doesn't go well.
661300	669580	And so, you know, that is just another way of saying if you have recursive abstraction
669580	677420	on tools that gives us and tools and coordination that give us the radically more than evolutionary
677420	682860	capacity to affect the world, we have to move into trans evolutionary motive, which means
682860	686180	the same recursive abstraction that we're doing right now saying, oh yeah, I guess it
686180	693020	makes sense that we can't run an exponential financial system that's attached to a linear
693020	697220	materials economy that takes stuff out of nature faster than it can be replenished and
697220	700740	turns it into trash and pollution in nature faster than it can be processed.
700740	704500	You can't do that exponentially forever on a finite planet, so we have to do something
704500	708740	fundamentally different, which means you can't orient towards continued, maximized growth
708740	712020	and maximized conflict orientation forever.
712020	714460	So that's what I mean by a trans evolutionary motive.
714860	722020	Is it naive to think that we need a global government and we can make a global governance?
722020	729820	When you look at the problem of countries having competitive dynamics with each other
729820	736620	where nobody wants to price carbon properly, because if they do, their own economy will
736620	740900	be so damaged relative to whoever doesn't that the radically decreased geopolitical
740900	745500	power will express itself as less military power, less trade power, and particularly
745500	749020	with whoever is at the leading edge of guiding the world system.
749020	756660	This classic, the US isn't going to if China doesn't and vice versa, so then everyone is
756660	767020	mostly actually just in an economics race that is also bound to an actual arms race.
767020	769500	And that's true for pricing carbon and climate change.
769500	774780	It's also true for fishing of the oceans and aerosols and on and on and on.
774780	780460	So if you have an issue like the atmosphere, aerosols and the atmosphere and ozone layer,
780460	785540	or you have an issue like the oceans or climate change, no country can solve that problem.
785540	789700	And any country that does the thing that is doing its share that is economically disadvantaged
789700	793940	in the short term by doing it, it just isn't going to do that if everyone else doesn't
793940	796020	because they are caught in the competitive dynamics.
797020	801460	So when you look at that, you're like, all right, well, we need global government because
801460	802500	we have global issues.
802500	805700	We don't just have national issues and you have to have governance at the level that
805700	807980	you have issues.
807980	812260	But then of course, most thinking people aren't really a big fan of the idea of global government
812260	817380	because it's not a great idea to have unchecked power, though we don't have a good history
817380	820780	of being good stewards of unchecked power.
820780	826900	And so in many modern governments in the United States, it was kind of like the foundation
826900	831820	of the whole idea was let's separate church and state, let's separate the judicial branch
831820	835020	and the legislative branch and the executive branch.
835020	839020	Let's even separate the legislative branches in the separate houses.
839020	842340	Let's try to create as much check and balance on power as possible.
842340	847580	So if you had a one world government that had enough power to be able to price carbon
847580	852740	properly and enforce fishing laws and et cetera, how do you prevent it from becoming
852740	854980	corrupted or captured?
854980	860500	And so we need global government and we don't want global government and so that this is
860500	864980	this like you have catastrophes on one hand that need to be avoided and that typically
864980	869860	looks like more control mechanisms of things that if you don't control will lead to catastrophe
869860	874620	and the control mechanisms typically lead to dystopias.
875060	877340	So we want something that is not catastrophes or dystopias.
877340	881460	We kind of call this the third attractor and that means you have to have control mechanisms
881460	884860	that prevent catastrophes, but you have to have checks and balances on the power within
884860	885860	those.
885860	886860	How do you do that?
886860	890620	So global governance and global government are not the same thing, right?
890620	894300	Global government, the idea that there's some centralized global monopoly of violence,
894300	895700	the bad idea.
895700	900100	The idea that there is some more effective process of global coordination, even whether
900140	906660	it's a more effective process of nations engaging in multilateral agreements that can
906660	912300	be facilitated by technology that can make the participation or violation of those agreements
912300	920300	more transparent or there is some process of global governance that has to occur where
920300	924460	there's both effective power for enforcement.
924460	929100	This is why we can solve those types of coordination problems to some degree, those race to the
929100	935060	bottom within a country where you have a monopoly of violence because the law on monopoly
935060	938260	of violence just basically says, no, you're not allowed to cut down any of those trees.
938260	942220	That's a national park and if you try, the police will stop you and they have more capacity
942220	943940	for violence than you do.
943940	947660	With international issues where you don't actually have international enforcement, it's
947660	948660	really, really tricky.
948660	954140	So for all of the really global issues, and that looks like it's in each nation's interests
954140	958340	to burn the coal as fast as it can and the oil, it's in each nation's interests to win
958460	961900	the AI arms race, even though that increases the likelihood that we all die from it in
961900	963220	the long term.
963220	970340	So global governance that has appropriate checks and balances is a tricky topic, but
970340	972300	it's a necessary topic.
972300	973780	What gives you hope today?
973780	976780	A lot of things can be hoped.
976780	987900	I have noticed in my own work, people in top positions of power and major institutions
987980	997820	that affect the world being radically more aware of things that are fundamentally unviable
997820	1005020	about this world system and interested in deeper changes and actually starting to try
1005020	1008860	to implement some things just even in the last couple of years than I had ever experienced
1008860	1010300	previously.
1010300	1017380	So the idea that, you know, the kind of behavior that individuals can do on their own matters
1017460	1021820	and the kind of stuff we can do locally like, you know, prototyping new types of communities
1021820	1029140	and new types of cities, you don't solve climate change in time and you don't solve planetary
1029140	1032460	boundaries in time and you don't solve AI risk that way, right?
1032460	1036300	That requires kind of agreement from existing top-down organizations.
1036300	1038420	They can't actually innovate a new world.
1038420	1042380	They can just stop bad things from happening with the right kinds of agreements to innovate
1042380	1049660	a new world actually does require local and more participatory activity.
1049660	1059940	But the fact that after COVID and after the extreme political polarization that has happened
1059940	1067540	and after how much of Australia burned and then flooded and, you know, now with the war
1067540	1074380	on Ukraine and I think there was a situation where previously people who were thinking
1074380	1080740	about it and who were prescient realized this world system is destabilizing and is fundamentally
1080740	1082180	not sustainable.
1082180	1084580	Most of the people who were administrating it didn't think that.
1084580	1089460	Now almost everybody thinks that and that's actually something that gives me hope.
1089460	1090700	Great.
1090700	1095220	How much time do we have to react to avoid extinction?
1095980	1099780	Some species go extinct every day as a result of human activity.
1099780	1106700	So for them, we're already past existential risk, you know, Kiev was an incredibly progressive
1106700	1108420	place not very long ago.
1108420	1113940	It wouldn't have seemed like a place where eminent catastrophic risk was coming for many
1113940	1123820	people and, you know, that's even true of Syria not that long ago and you see the pictures
1123820	1127740	of what culture was like in 1968 in Iran.
1127740	1130900	So it's not like how long do we have before catastrophe hits.
1130900	1135180	We're already in a rolling global catastrophe.
1135180	1140940	Like how long does Australia have before it burns that already happened, you know, and
1140940	1144580	from extreme weather events that are a result of human induced activity poor environmental
1144580	1151340	management and problems with utility companies and overuse of groundwater and climate change.
1151340	1156940	And how quickly does war escalate as a result of what's happening in Ukraine at larger scale
1156940	1163700	and already what we see in regarding Taiwan and Azerbaijan and Armenia and Iran and so
1163700	1165940	many places.
1165940	1172140	These things could move very fast or more slowly in ways that are chaotic and totally
1172140	1174780	unpredictable.
1174780	1180020	When you look at things like the planetary boundaries, how long until we pass certain
1180100	1187140	planetary boundaries, you'll hear people talk about this thing happens in 2050 and this
1187140	1190780	thing happens and by the end of the century or whatever with climate change.
1190780	1195780	But we've already passed some of the planetary boundaries, you know, there's a study just
1195780	1201900	published in the American Chemical Society Journal saying that certain toxic chemicals
1201900	1206140	in rainwater kind of ubiquitous around the world are past the EPA thresholds for human
1206140	1207700	health.
1207700	1212980	And this was particularly the the fluorinated surfactants which don't break down, right?
1212980	1214900	So they come forever chemicals.
1214900	1219980	But the idea that things that are carcinogens and cause birth defects and are endocrine
1219980	1225580	disruptors in rainwater all around the world are past the levels of human health tolerability
1225580	1226580	is a huge deal.
1226580	1233180	It means even if you go get off grid as can be and try to live off the land, you can't.
1233180	1237540	And how quickly we're producing those chemicals, not only is there a cumulative effect of them
1237540	1243220	because they're persistent, but we're also increasing our production of them exponentially.
1243220	1244860	And so how long do we have?
1244860	1251780	We're already in a situation of a breakdown of a world system.
1251780	1254300	It's already existential for many species.
1254300	1258140	It's already catastrophic for people in many areas of the world.
1258140	1266820	And so I would reorient the question to be more like, is there anything that we can do
1266820	1269860	to have it not be totalizing?
1269860	1271620	And the answer is yes.
1271620	1279260	And the answer time wise on that is the full life attention of everyone as best as possible
1279260	1283100	directed at better understanding the issues and participating in the solutions is what's
1283100	1284100	required.
1284100	1290420	In individual level, we've become a bit lazy maybe because we think like there is always
1290420	1293900	a solution and we don't really need to act.
1293900	1298780	How to wake up and also how do you get the news because you have like so many news in
1298780	1299780	different directions.
1299780	1304180	Like we don't know who to believe and we don't know like we're not sure we need to act because
1304180	1307140	things always have a solution by themselves.
1307140	1313460	There's a really interesting book called The Politics of the Invisible written after Chernobyl
1313460	1320380	because after the Chernobyl explosion, the uranium is invisible, right?
1320380	1322540	We can't see it with the human eye.
1322540	1326620	Obviously now COVID that's invisible and yet totally lethal.
1326620	1335220	And what he was exploring in Politics of the Invisible is because of modern technology and
1335220	1340300	chemistry, we can make things that are totally lethal that we can't see that require people
1340300	1346420	with Geiger counters and the ability to do physics that not everyone can do to be able
1346420	1347820	to determine safety levels.
1347820	1351420	How does that work with democracy when most people don't have the capacity to do that?
1351420	1356740	So you'll see currently a lot of people doubting climate change science, but nobody can actually,
1356740	1362540	the average citizen can't run the IPCC's mathematical models to say they work or they don't work
1362540	1364140	or they...
1364140	1373140	And so people are largely kind of left to faith and you then end up getting politics driving
1373140	1377740	people to either be kind of pro-institutional or anti-institutional.
1377740	1381860	And the institutions get things wrong, so it's easy to be anti-institutional and neither
1381860	1386420	of the positions are actually viable and there is something other than truth, which is the
1386420	1388820	movement to power motivated in both of them.
1388820	1393140	I see that when people think someone else will come up with a solution, they feel kind
1393140	1398980	of unmotivated, but also when people think there is no solution, they feel unmotivated.
1398980	1406060	And this is also something I find really interesting is when I talk to someone who has a really
1406060	1414540	fervent adamant view about whatever it is, that whether it's vaccines or masks or what
1414540	1420340	should happen in Russia, Ukraine or abortion law, whatever, they go from complete certainty
1420340	1423540	without understanding the position of the other side or all the complexity or nuance
1423540	1425140	well.
1425140	1429460	And if I challenge it and not, regardless of which side it is, and show them the increased
1429460	1433580	complexity, okay, well if we price carbon that way and China doesn't, then autocracy
1433620	1440260	ends up running the world, so you're voting democracy out and whatever it is, then for
1440260	1444780	so many people, the first response when you increase, show them the way they're thinking
1444780	1448980	about it doesn't actually map to the complexity of the problem, they go from utter certainty
1448980	1450580	to nihilism in one step.
1450580	1453700	They're like, oh fuck it, it's too hard, it's too complex, I give up.
1453700	1460540	And to move from certainty to nihilism in one step is so damn lazy, like cognitively,
1460540	1462820	emotionally, epistemically lazy.
1462820	1467260	And so I want people to go from certainty to like, actually I don't understand this
1467260	1468260	all that well.
1468260	1473660	Actually, climate change or global science or policy on this thing is pretty complex.
1473660	1476780	There are experts who spend their whole life working on it who disagree.
1476780	1481860	That doesn't mean there aren't solutions, but the one that was fed to me that everybody
1481860	1485700	on my political side agrees with and everyone on the other side disagrees with is probably
1485700	1489100	not a fair version of the whole truth.
1489100	1492140	So I'm not going to give up because I don't know.
1492140	1494220	I'm not going to hold the certainty that I know because I don't.
1494220	1498980	I'm going to work to try to understand competently while recognizing I don't yet.
1498980	1504420	And then even once I get to much deeper understanding, I'll still recognize how much stuff I don't
1504420	1507180	know that's relevant and new information that might come in.
1507180	1513980	So I want people to be much more epistemically rigorous and epistemically humble at the same
1513980	1517980	time, epistemology meaning how we go about knowing things.
1517980	1524020	So I want them to work much harder at trying to come to understand while having much less
1524020	1526340	certainty about their current level of understanding.
1526340	1531140	So when you ask what sources should people go to for news or whatever, the ones they don't
1531140	1534740	currently go to is the first answer.
1534740	1536900	And then, of course, progressively better sources.
1536900	1541140	Not all the sources independent of political spinner are equally good.
1541140	1547740	But when you can see where do the various earnest experts on the topic disagree and
1547740	1552780	you at least understand those positions pretty well, then you start to have a sense of the
1552780	1553780	topic.
1553780	1559300	As a philosopher and because you spoke about politics, do you want to stay away from politics
1559300	1562500	or are you into politics?
1562500	1568700	Politics meaning how people organize and how they coordinate and how they make sense of
1568700	1570980	the world together so they can make choices together.
1570980	1574500	No, I'm totally focused on that.
1574500	1583260	The current political system and the United States does not do a very good job of helping
1583260	1587900	people collectively make sense of issues well, collectively identify all the values that
1587900	1593420	matter that are shared values and then collectively make good choices in the presence of the shared
1593420	1596260	sense making and shared values generation.
1596260	1602820	So it's not that I think there is never a time to engage in voting for a particular
1602820	1607860	candidate or on a particular proposition, but how to engage in metapolitics, meaning
1607860	1613500	how to evolve this political system and economic system, how to evolve the political economy
1613500	1618060	along with evolving the infrastructure and tech stack and the culture and value system
1618060	1621700	simultaneously because all three of those inter-effect each other.
1621700	1627020	The culture, the political economy and the infrastructure and technology, they all inter-effect
1627020	1628020	each other.
1628020	1632580	So you can't change any of them without changing all of them to think through what has to happen
1632580	1637020	in all of those for a viable world to come about.
1637020	1639540	I'm very interested in that.
1639540	1640540	Okay.
1640540	1644780	Let's speak a bit about you, something very interesting I found.
1644780	1648620	You mentioned you were homeschooled by your parents.
1648620	1656420	Which qualities did your parents manage to let flourish in you that might not have been
1656420	1662380	so important also in the traditional education when you're changing teachers every year?
1662380	1669260	I was homeschooled, I did go to school, both private and public schools for little bits
1669260	1670260	throughout my life.
1670260	1673900	So I have some experience of it, but most of my childhood was homeschooled, but it was
1673900	1679820	not traditional homeschooling, meaning I didn't have the state curriculum and just do it at
1679820	1680820	home.
1680820	1686900	My parents were kind of interested in running an educational experiment that is a little
1686900	1690780	bit closer to what people call unschooling today, but there was just no curriculum.
1691340	1692340	Okay.
1692340	1695020	What they felt you need to learn.
1695020	1696500	It's not what they felt.
1696500	1705420	Their hypothesis was expose the kids to all the different topics, see which ones they're
1705420	1709620	interested in, facilitate their interest, and kind of trust them.
1709620	1715980	So it's aligned with some of the ideas of Montessori and Dewey and Constructivism.
1715980	1719780	But you know, radical had no curriculum at all.
1719780	1727020	But and I'm not saying that is what I would advise, but there's a lot good in that.
1727020	1737820	And what qualities that facilitated in me that most educational systems don't is all
1737820	1740540	of my studies were things I was interested in.
1740540	1745260	And so my interest in learning was actually growing all the time, right?
1745260	1748860	There was never a place where I wanted to get out of school and go play or do something
1748860	1755540	else where learning felt like a burden or where I ended up not having any negative association
1755540	1756540	with study.
1756540	1760020	And I had only positive association because I was studying things I wanted to study.
1760020	1766620	So I find that people tend to become good at things they really enjoy.
1766620	1773020	And so facilitating, like even if you have a curriculum, really paying attention to where
1773020	1776500	a student's interests are and where their passions are.
1776500	1781260	And if there's a topic that isn't appealing to them, trying to find a way that actually
1781260	1784980	has it really appeal as opposed to just forcing them to do the thing makes a huge difference
1784980	1788700	not to their learning of that topic, but to their relationship to learning itself.
1788700	1796100	What a special event that put you on this path of trying to see the truth, what's happening
1796100	1799700	and observe the complexity of the world.
1799700	1800700	Lots of events.
1800700	1811460	I mean, some people have a near death experience turning point that is really kind of singular.
1811460	1815820	I think most people's life path unfolds from lots of things.
1815820	1821300	So as I'm mentioning being homeschooled by parents who are obviously kind of interested
1821300	1826540	in childhood development and the books my parents read to me as bedtime stories were
1826540	1832620	Buckminster Fuller's Design Science and Fritschof Capra Systems Theory and Eastern
1832620	1834380	Vedic Philosophy and things like that.
1834380	1840140	So there were people who were thinking about what is the world?
1840140	1841260	How does the world work?
1841260	1846140	How do we integrate across the various philosophic and scientific traditions?
1846140	1847540	How do we improve civilization?
1847540	1850500	Those were kind of like just the core thoughts.
1850500	1854780	And so I didn't really have to get on that path.
1854780	1862420	And then a big part of my study as a kid was not just studying various areas of philosophy
1862420	1866020	or science or whatever, but also being actively engaged in activism.
1866020	1873420	My mom was particularly into kind of hands-on activism with whether it was helping the local
1873420	1879780	animal shelter or larger kind of factory farm animal rights issues or environmental issues.
1879780	1886060	So being engaged in activism and then seeing what the problems in the world were and then
1886060	1890380	similarly having a system science and kind of design science background to look at it
1890380	1892700	and say, how are these problems interconnected?
1892700	1894860	What do they have in common as generative dynamics?
1894860	1901260	What would it take to address them more comprehensively because it's not that hard to see that whether
1901260	1905020	we're talking about issues in healthcare or issues in war, issues in politics or issues
1905020	1909740	in the environment, things like perverse economic incentive or one of the drivers of all of
1909740	1910740	them.
1910740	1913500	So it's like, well, how do we think through an economic system that doesn't have perverse
1913500	1914500	incentive?
1914500	1918860	Yeah, I would say it was working across many different areas of activism, seeing how they
1918860	1923700	related to seeing why the solutions that we were working on weren't adequate because they
1923700	1927820	didn't address the deeper dynamics.
1927820	1930780	Those were kind of key early things for me.
1930780	1931780	Thank you very much, Daniel.
1931780	1932780	Yeah.
1932780	1933780	Thank you.
1933780	1934780	Thank you.
1934780	1935780	Bye.
1935780	1936780	Bye.
1936780	1937780	Bye.
1937780	1938780	Bye.
1938780	1939780	Bye.
1939780	1940780	Bye.
1940780	1941780	Bye.
1941780	1942780	Bye.
