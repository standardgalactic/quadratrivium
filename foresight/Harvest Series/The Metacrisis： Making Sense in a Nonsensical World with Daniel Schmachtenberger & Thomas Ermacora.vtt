WEBVTT

00:00.000 --> 00:19.040
I just want to say thank you for this welcome again from the team at Harvest.

00:19.040 --> 00:24.480
It's great for me to have been able to suggest Daniel as a speaker here.

00:24.480 --> 00:31.480
I'm very fond of him as a human, as a fellow futurist and as a thinker for our time.

00:31.480 --> 00:36.480
I think it probably needs a little introduction. Why is Daniel here?

00:36.480 --> 00:41.480
Daniel is not a prophet of doom, absolutely not.

00:41.480 --> 00:48.480
He's trying to equip us with the understanding that will help us navigate some of the crises that are ahead of us.

00:48.480 --> 00:55.480
It's something that some people call the metacrisis and he's one of the main people in the world now who can explain it very clearly.

00:55.480 --> 01:00.480
I'll try and break it up a little bit so that we can go over some of the points again.

01:00.480 --> 01:10.480
It's difficult to say what Consilience Project is in a few words because it has evolved over time to confront the reality of what the world is ready to accept.

01:10.480 --> 01:20.480
However, it's certainly one of, in my opinion, the best attempts and very tangible attempts to help people navigate what's right now in front of us.

01:20.480 --> 01:28.480
The news cycle is certainly not helping us understand. It's sort of distracting our minds from the real things that are going on.

01:28.480 --> 01:42.480
So Daniel is one of these people out there who's working really to formalize the way that we can anticipate, connect, collaborate and work more effectively to solve problems that we are facing.

01:42.480 --> 01:54.480
So Daniel, I think there's something that we should know about your past and I'm going to be provocative.

01:54.480 --> 02:08.480
If you didn't actually start out with the same objective, you sort of thought that maybe we were doomed and that we had to accelerate doom in order to get somewhere.

02:08.480 --> 02:14.480
I'm going to be very provocative so that you can explain your journey from a young man to where you are now.

02:14.480 --> 02:24.480
I was homeschooled and when I was nine, I was at a gas station and a factory farming cattle truck pulled up and so I went and looked inside the holes.

02:24.480 --> 02:36.480
The cow nearest to me was missing its eye and bleeding. All the cows were in terrible condition and I grew up loving animals, eating meat from factory farms, not putting the two together.

02:36.480 --> 02:46.480
I was just kind of shocked and horrified by that. I asked my parents about it. They said that's where animals come from so I asked to go to a factory farm.

02:46.480 --> 02:54.480
I became a vegetarian that day. I started working with PETA in Greenpeace and studying animal rights work.

02:54.480 --> 03:08.480
That took me from the issue of factory farming to the issue of species extinction, whaling, overfishing the oceans, which took me into the issues of the environment and that took me into global poverty and just kind of the whole set of issues as a young person.

03:08.480 --> 03:19.480
I was fortunate being homeschooled that I got to just have that be my curriculum. I was actually an educational experiment. My parents ran right and didn't have any fixed curriculum.

03:19.480 --> 03:28.480
I just got a study that I was interested in so this became the context of most of my life study and the next really important part happened.

03:28.480 --> 03:43.480
I was 13. I was working on a project that Greenpeace and World Wildlife Federation were leading to protect elephant poaching in a particular elephant preserve in Kenya where the poachers had come in over the preserve and hunted a bunch of elephants.

03:43.480 --> 03:56.480
I saw the videos of the elephant slaughter and was moved in the same way as the factory farms and the thing I observed though was over the two years of the people on the ground working extremely hard to protect the elephants there,

03:56.480 --> 04:05.480
the strategy was get bigger fences around the preserve so the poachers couldn't get in and do legislation to get harsher sentencing for poachers in the area.

04:05.480 --> 04:14.480
All the activists had their life threatened but they finally succeeded. It was a huge win. Those elephants didn't get hunted but they didn't address the poverty of the people that were doing the poaching.

04:14.480 --> 04:22.480
They didn't address a macro economy that creates poverty at scale. They didn't address the views towards animals or identity regarding them or black markets on animal parts.

04:22.480 --> 04:29.480
The same poaching groups moved to hunt other things which happened to be the white rhino and the mountain gorilla both of which were more endangered than the elephant.

04:29.480 --> 04:36.480
I was working with enough groups that I got to see those other issues get worse as a direct result of the success of that project.

04:36.480 --> 04:45.480
And then that was like the second existential hit. The first existential hit with the factory farms was, I'm saying this to relate,

04:45.480 --> 04:52.480
that was the first time I had like a suicidal ideation around not feeling okay being complicit with my species.

04:52.480 --> 05:02.480
My species was somehow fundamentally fucked up and as a kid it wasn't that hard to realize if I kill myself all those cows are still in the factory farms.

05:02.480 --> 05:15.480
I didn't help them. That's unethical. But then I'm like, if I stay alive to try to help it, can my life be a success while sentient beings are experiencing that much suffering?

05:15.480 --> 05:21.480
What does it say about me that I can be totally stoked that my life is a success when other beings are in that kind of suffering?

05:21.480 --> 05:29.480
I'm like, there's no answer for me that doesn't address that. So I remember thinking to myself then I'm like, alright, well if I die and factory farms still exist,

05:29.480 --> 05:40.480
I failed like there is no definition of success for me that isn't a sociopath's definition of success that has to empathetically separate too intensely.

05:40.480 --> 05:47.480
But at least the hope came that like activism might work. And so then when this thing happened where I saw that the activism for one thing made other things worse,

05:47.480 --> 05:54.480
that was the next kind of devastation. And that led to me starting to look at how many other places where we were doing activism,

05:54.480 --> 06:05.480
where the world was doing activism, did that kind of issue. And I saw that there were projects to solve global poverty by creating hydroelectric dams to bring electricity to areas,

06:05.480 --> 06:20.480
and hydroelectric dams, of course, drowned whole ecosystems and extinct species, and on and on. And kind of like it seemed like the world was caught in these trade-offs where when they would focus on solving one problem,

06:20.480 --> 06:30.480
it was too narrowly defined, the result of solving that problem would have externalities related with larger systems. I started to do kind of historical analysis on that,

06:30.480 --> 06:39.480
and that became kind of a defining trend of all of the issues. We created the automobile to solve a transportation issue, which was that horses were a very limited means of transportation.

06:39.480 --> 06:50.480
Transportation, horses in the cities caused a lot of horseshit in places like London. Literally the excessive horseshit in the cities of London was one of the kind of like significant impetuses to make a horseless carriage.

06:50.480 --> 06:59.480
And in solving that problem and creating transportation that increased mobility and comfort for the whole world, climate change, like the venusification of the entire planet,

06:59.480 --> 07:07.480
and oil spills, and wars over oil, and the US petrodollar, like all of that were the side effects of solving a transportation issue, you know, a couple hundred years later.

07:07.480 --> 07:19.480
And I got to see that there was this underlying deep issue that statecraft, that is thinking about the well-being of its own citizens, the rest of the world,

07:19.480 --> 07:25.480
so it's not just exporting its harm somewhere else and of the planet. And there's a lot of countries where they're genicoefficient,

07:25.480 --> 07:33.480
meaning their measure of wealth inequality is pretty good within their country, but they import a bunch of stuff to make their country function from countries that have the worst genicoefficient.

07:33.480 --> 07:40.480
So you can't actually do genicoefficient at the level of a country, it ends up just being a way to kind of whitewash the reality of global supply chains.

07:40.480 --> 07:47.480
Oh, so the conscious statecraft thing. So how do you do what is good for your own country, what's good for other countries,

07:47.480 --> 07:52.480
and what's good for the planet when there are fundamental trade-offs between them and you're stuck in those trade-offs?

07:52.480 --> 08:00.480
That becomes really the deep challenging question. If we look at climate change today, we can see that for the planet and for all of us in the future,

08:00.480 --> 08:09.480
we should not just decarbonize but degrowth. We can also see that any nation that tries to lead that will do worse GDP-wise in the near term,

08:09.480 --> 08:15.480
which means worse geopolitically and actually so much worse that, particularly for the leading countries like U.S. and China,

08:15.480 --> 08:24.480
change the control systems of the world. So so many of the people in the U.S. that are anti-climate change aren't actually anti-climate change

08:24.480 --> 08:30.480
because they did a good analysis of the IPCC science. It's that the solution of increasing the price of carbon for us,

08:30.480 --> 08:36.480
which will decrease our GDP relative to China's GDP in a great power game, which will increase planetary autocracy,

08:36.480 --> 08:42.480
seems like such a bad solution that they don't want the climate change assessment because they don't want the solution that goes with it.

08:43.480 --> 08:52.480
So a part of the thing that I focus on is how do we understand the interconnectivity and the generators of all the issues deeply enough

08:52.480 --> 09:00.480
that we can come up with solutions that don't cause other problems. But one point was a little while after this I had come to,

09:00.480 --> 09:07.480
I started doing forecasting on how many people have read Limits of Growth, the MIT Club of Rome book.

09:07.480 --> 09:14.480
So I read that as a teenager and then started looking at a lot of other analyses on species extinction and biodiversity loss

09:14.480 --> 09:21.480
and dead zones and oceans and all those things. And then I looked at the, you know, Stephen Pinker kind of stuff of everything's getting better.

09:21.480 --> 09:28.480
A lot of people might have that question of like, you hear Stephen Pinker, Hans Rosling Gates and like everything's getting better,

09:28.480 --> 09:33.480
and then you hear all the environmental statistics and like everything's getting worse. They're both true.

09:33.480 --> 09:42.480
You can cherry pick your stats, but they're not equally true. The things that are getting worse are leading to the unviability of human habitation

09:42.480 --> 09:49.480
ongoingly in a way that the things that are getting better don't converge on solving automatically. So there's a real,

09:49.480 --> 09:55.480
and the things that we're making better are causing the things that are getting worse towards self extinction points, right?

09:55.480 --> 10:03.480
Like grow global GDP where we all have like nice stuff because of that in a way that breaks the biosphere's capacity to continue to make life possible.

10:03.480 --> 10:11.480
So we have to do something deeper than saying it's getting better and worse so we just choose. It's like the way that we make things get better

10:11.480 --> 10:18.480
is a major part of what's making them get worse towards tipping points that are unique now to any point in previous civilization.

10:18.480 --> 10:25.480
So when I was at 15, I came to the assessment that there was no chance that humanity would actually make it in time,

10:25.480 --> 10:32.480
and that if we had radically less population, we might have a chance, and I started studying viruses and depopulation strategies,

10:32.480 --> 10:42.480
which is a thing that an angsty but earnest caring teenager might do. I had a spiritual experience that took me off that path while I was working on it,

10:42.480 --> 10:55.480
but I was seriously studying vector delivery of novel pathogens, and the spiritual experience was something Bucky Fuller used to say,

10:55.480 --> 11:05.480
and I had heard it a lot but it hadn't hit me in the same way that the chicken developing inside of a shell is totally unsustainable, right?

11:05.480 --> 11:15.480
When it's eating the unrenewable resource, the egg white, it's creating metabolic pollution in its environment, it doesn't even know that it's inside of a shell,

11:15.480 --> 11:22.480
but it's in a developmental phase, and of course it doesn't have the beak or the gastrointestinal tract to eat seeds yet,

11:22.480 --> 11:28.480
the whites are exactly kind of what it developmentally needs to go through that embryonic phase, and right as it runs out of whites,

11:28.480 --> 11:36.480
as soon as its GI tract and beak and all the things are developed to be able to crack through the shell and emerge into a world where now it's part of a new phase,

11:36.480 --> 11:48.480
and I started thinking about humanity as being unrenewable because of being in a developmental phase or a discrete phase shift that isn't the result of just a continuity of the previous lines,

11:48.480 --> 11:57.480
and I thought about all the examples of caterpillar to chrysalis to butterfly, and that in the chrysalis there's a fundamental dissolution of all the organs

11:57.480 --> 12:08.480
and a restructuring with a different genetic code that if a fetus went more than 40 weeks in the mother's uterus, it would kill itself and kill the mom,

12:08.480 --> 12:15.480
but it has to go through that developmental period, and then the birth is a discrete kind of difficult process, and then the umbilical cords cut,

12:15.480 --> 12:26.480
now there's a new developmental time, and so I had this kind of profound experience that developmental phases, whether inside of an egg or in a chrysalis or in a womb,

12:26.480 --> 12:35.480
are always unsustainable. There is some discrete nonlinear phase shift that is different than the curve in the developmental phase,

12:35.480 --> 12:43.480
and will be different afterwards, and I started thinking about if humanity as a species was in a developmental phase,

12:43.480 --> 12:52.480
and there was a discrete phase shift that was different than the curves, what that might look like, but that got me off of the track that Thomas was bringing up,

12:52.480 --> 13:04.480
but what's really interesting is a lot of my work now involves the way that exponential technology equals exponential decentralized catastrophe weapons for everyone.

13:04.480 --> 13:14.480
Whenever we talk about the positives of exponential tech, AI, biotechnology, nanotechnology, cyber technology,

13:14.480 --> 13:21.480
we talk about in positive terms usually like the democratization of this great technological power of creation,

13:21.480 --> 13:29.480
but the democratization of catastrophe weapons is actually not a great idea, right? Like keeping nukes to the G8,

13:29.480 --> 13:35.480
but you can do because it's really hard to make nukes, and you can see who's making nukes, and there's not that many places that have uranium,

13:35.480 --> 13:41.480
and enriching uranium is hard, and you can see it from outer space, but it's pretty easy to make drone weapons,

13:41.480 --> 13:50.480
and it's increasingly becoming easy, and the ability to use those for infrastructure targets, and specifically it's becoming pretty easy to make pathogens.

13:50.480 --> 14:00.480
In the advancing of synthetic biology, we're only a few years away from the ability to synthesize novel pathogens on a desktop for $1,000 anywhere,

14:00.480 --> 14:06.480
and so when I was a kid, that was not true, it was actually like a hard thing to do, it's becoming an increasingly easy thing to do,

14:06.480 --> 14:18.480
while increasingly more people are feeling kind of concerned and disenfranchised, so as the group of people that would be motivated to change this world system in harmful ways,

14:18.480 --> 14:25.480
and the group of people that are capable is converging and increasing, there's a lot of risk associated with that, how do we deal with that?

14:25.480 --> 14:29.480
That's an interesting part of the topic.

14:29.480 --> 14:33.480
I'm really excited that you didn't turn out to become a terrorist.

14:33.480 --> 14:41.480
It wasn't motivated by not liking people, it was motivated by seeing self-induced human extinction as inevitable, and as being the only way out,

14:41.480 --> 14:51.480
and that's actually like most terrorists are well motivated, they're motivated in service of something they care about that they feel is being harmed and they don't know other solutions.

14:51.480 --> 14:56.480
We could agree, but I'll pause on that one.

14:56.480 --> 15:05.480
I think what a lot of people here are probably expecting if you have heard anything about Daniel is you present a little bit about the meta crisis,

15:05.480 --> 15:13.480
so there's a framework of thinking around how we can put dots in between a lot of the crises that we're facing collectively,

15:13.480 --> 15:23.480
and instead of addressing one and creating externalities in another problem set, perhaps understanding the total problem set at the same time,

15:23.480 --> 15:32.480
and before you act actually have a more enlightened perspective gives you more edge, so that's an assumption and a hopeful one,

15:32.480 --> 15:39.480
and some people might argue that we need to act very fast on certain issues, so they don't really like this sort of temporization of it,

15:39.480 --> 15:47.480
but I'm of the belief like you that basically having better tools to understand the world we live in is essential for us to make critical decisions,

15:47.480 --> 15:55.480
and I would love for you to spend a moment unfolding what you call, or what we call, the meta crisis,

15:55.480 --> 16:03.480
and why it's unique to this moment in history, and why humanity has to put some effort towards solving that concretely,

16:03.480 --> 16:07.480
or we will probably end up in a pretty bad place very quickly.

16:07.480 --> 16:14.480
So it could easily seem like the stuff that I'm talking about is just the collection of all the worst news in one place.

16:14.480 --> 16:19.480
I would bother sharing it because I think it is true.

16:19.480 --> 16:28.480
The risks that we're talking about are true, it's not determined that we definitely fail at them or that we definitely succeed at them,

16:28.480 --> 16:36.480
so what we do actually matters in determining it, and there's no chance that we can solve it if we don't more competently understand it,

16:36.480 --> 16:46.480
so more people competently understanding and seriously working to apply themselves to the unique needs of this particular time in the world

16:46.480 --> 16:56.480
is something that I'm hopeful for, and I guess that's why I'm speaking here, and said yes to Thomas inviting me here.

16:56.480 --> 17:07.480
So the meta crisis frame might actually help deal with some catastrophe fatigue because rather than see the issue of drones and autonomous weapons

17:07.480 --> 17:17.480
and the issue of exponential tech-empowered terrorism and this planetary boundary issue and this pollinator issue and this forever chemical issue as separate issues,

17:17.480 --> 17:26.480
I look at them all as expressions of an interconnected set of generative dynamics that we call the meta crisis,

17:26.480 --> 17:34.480
where what it takes to solve any of them is the same, actually, and if you try to solve them without factoring these deep underlying generative dynamics,

17:34.480 --> 17:40.480
you will at bet, you probably won't solve it, and if you do, you'll displace problems somewhere else and actually kind of mess the whole thing up,

17:40.480 --> 17:47.480
so when you understand that all those problems are connected at first, it makes it seem more overwhelming because you're like fuck,

17:47.480 --> 17:53.480
to think about climate change, I also have to think about geopolitics and fundamental changes to finance and all these other issues,

17:53.480 --> 18:03.480
and that seems like a lot of complexity, but it actually takes it from too many problems to tractably manage all of which have solutions that end up externalizing harm elsewhere,

18:03.480 --> 18:11.480
which means impossible, too hard but tractable, so hopefully in terms of being able to see it all as one interconnected set of issues,

18:11.480 --> 18:18.480
you're like alright, there's a lot more learning that I need to do to be able to competently engage, but there is actually a way through,

18:18.480 --> 18:24.480
there's actually kind of a tractable analysis, so that's what I hope to share.

18:24.480 --> 18:32.480
So, if you can move towards what people call the third attractor or that you like to call the third attractor,

18:32.480 --> 18:40.480
so that we sort of gravitate towards a landscape of defining what is the solution environment that we want to find ourselves in,

18:40.480 --> 18:50.480
because obviously there are a lot of people who are very competent, who are deploying intelligence and capital and being very innovative about what they do,

18:50.480 --> 18:57.480
but they sometimes miss the fact that they are creating really negative externalities in other, let's say problem sets,

18:57.480 --> 19:04.480
so the third attractor to me seems like a very easy thing to understand for people, even though if we don't have the answer to what the third attractor is,

19:04.480 --> 19:11.480
but it's at least theoretically a framework for understanding why the metacrisis may find a solution set through a third attractor.

19:11.480 --> 19:16.480
Yeah, let me give an example though of how we solve problems and make worse problems that are current and really relevant.

19:16.480 --> 19:26.480
We're working to try to change currently US federal government program and have had some really good success with it for pandemic prevention,

19:26.480 --> 19:31.480
for preventing whatever the next pandemic from animal sources do not expel over would be.

19:31.480 --> 19:36.480
And the federal government, and it's not only the US, lots of countries employ this approach.

19:36.480 --> 19:42.480
The US has been leading the way in what seems like great science and technology and innovation towards preemptive problem solving,

19:42.480 --> 19:51.480
which all seems like the right thing, but the approach to preventing zoonotic spillover involves viral hunting,

19:51.480 --> 19:59.480
so going out and finding tens of thousands of new viruses in bat caves that have mammalian viruses that have never been exposed to humans before,

19:59.480 --> 20:06.480
bringing them back to labs, doing gain of function research on them to figure out how they might mutate into things that are more virulent or transmissible,

20:06.480 --> 20:12.480
and then publishing all those genome sequences to an open source database so everyone who wants to work on vaccines has access to the knowledge.

20:12.480 --> 20:20.480
It seems like a decentralizing information, democratizing, multi-state coordination, science, anticipatory, good thing,

20:20.480 --> 20:26.480
and it's maybe one of the worst things happening in the world, and so we're lucky that we've been able to shift it.

20:27.480 --> 20:36.480
If you open source publish all of the pandemic grade viral gene sequences in an age where gene synthesis is becoming extremely cheap,

20:36.480 --> 20:47.480
that we're about three years out from tabletop gene drives and CRISPR and like that, the bioterrorism potential of that is just unimaginable,

20:47.480 --> 20:52.480
and even just the accidental kind of lab leak dynamics that when you have enough labs working with enough things,

20:52.480 --> 20:56.480
the probability that none of them happens drops towards zero over enough period of time.

20:56.480 --> 21:02.480
There was a lab doing gain of function research that figured out how to make an extremely virulent version of H1N1,

21:02.480 --> 21:08.480
like an R0 of 18, H1N1 is like a 60% fatality rate, and it did that in a biosecurity level 2 lab.

21:08.480 --> 21:13.480
So lab leaks happen, right, like this is an example of trying to do the right thing,

21:13.480 --> 21:19.480
but not understanding the problem space well enough and doing something totally that's the wrong thing,

21:19.480 --> 21:26.480
and what I remember the first time I was engaged in the UN network and it was a project with World Food Program when I was 20,

21:26.480 --> 21:32.480
the solution to world hunger involved bringing conventional NPK based agriculture to the developing world,

21:32.480 --> 21:40.480
so we didn't have to send food over there and the answer was way more nitrogen and phosphorus effluent into the rivers that would cause faster dead zones in the ocean,

21:40.480 --> 21:46.480
and so I talked to the guy about it and I said, you realize you'll speed up the rate of dead zones in the ocean catastrophically if you do this,

21:46.480 --> 21:52.480
and he said I hadn't thought of that, but those aren't the metrics I'm tasked with, and those aren't the metrics I'm tasked with,

21:52.480 --> 22:01.480
so I'm going to, for a few years, decrease hunger while working to extinct the planet because that's what my accountability is like.

22:01.480 --> 22:05.480
It just became very clear that that problem-solving approach was ubiquitous,

22:05.480 --> 22:14.480
and so another great example is you look at the advancement of something like gene editing, CRISPR,

22:14.480 --> 22:22.480
it's being advanced for purposes we all want, like immuno-oncology, how do you change 15,000 genes at once to be able to, you know,

22:22.480 --> 22:29.480
cure and prevent cancers that we're genetically predisposed to, but all technologies are dual purpose or multi-purpose,

22:29.480 --> 22:37.480
meaning every technology that you can make for some positive purpose has a military or otherwise weaponized or kind of externalizing application,

22:37.480 --> 22:46.480
so the research that's being done on how to do that type of gene editing is making it then really cheap and easy.

22:46.480 --> 22:52.480
Once we figured out how to do it, it takes major universities that have ethical review boards to do it for that purpose

22:52.480 --> 22:58.480
to develop the technologies that then drop the price by orders of magnitude to do it for any purpose and it's open publishing,

22:58.480 --> 23:07.480
so to give a little bit of history because some people might think, well, people have been predicting rapture since the 1600s,

23:07.480 --> 23:12.480
there's always some kind of like Mayan 2012, whatever, and this is just new catastrophism.

23:12.480 --> 23:20.480
I would really like people to think more deeply about what is discontinuous and novel in this time relative to other times,

23:20.480 --> 23:22.480
so I want to argue that real quick.

23:22.480 --> 23:31.480
The first technology we had that was powerful enough for humans to actually make the planet meaningfully uninhabitable was the nuclear bomb in World War II.

23:31.480 --> 23:38.480
That was the first truly existential technology. It was not an exponential technology, meaning nukes don't automatically make better nukes

23:38.480 --> 23:44.480
in the ways that computers automatically make better computers. Computation allows us to design better computer chips recursively,

23:44.480 --> 23:52.480
you get Moore's Law, but it was the first existential technology and that was really a break from the history of the entire world of tech

23:52.480 --> 24:00.480
up until that point because up until that point, every new military tech that we had, there was an absolute arms race to deploy it as quickly as we could

24:00.480 --> 24:07.480
and to win more battles and territory based on deploying it. This was the first one where we actually had to make an entire world system

24:07.480 --> 24:12.480
to ensure we would never deploy it because nobody would win, mutually assured destruction.

24:12.480 --> 24:18.480
All of a sudden, you're like, whoa, we're so big that we can't actually deploy our tech without destroying everything.

24:18.480 --> 24:21.480
We can't actually do the us versus them effectively anymore at that level.

24:21.480 --> 24:30.480
So post World War II, and we haven't had another world war that is kinetic between superpowers yet and we're at the brink of it right now.

24:31.480 --> 24:43.480
Post World War II, because of that tech, we rebuilt the entire global world system to deal with preventing kinetic World War III

24:43.480 --> 24:50.480
and that kind of Bretton Woods world system, IGO world system has been effective at preventing World War III,

24:50.480 --> 24:58.480
but that world system is almost totally broken down now and it drove all of the catastrophic risks we're facing currently.

24:58.480 --> 25:08.480
Specifically, one part of the post World War II system was a international monetary system that created, that had exponential growth of GDP.

25:08.480 --> 25:13.480
Why is exponential growth of GDP important is because the wars are based on everybody wanting more stuff

25:13.480 --> 25:17.480
and if you don't have exponential growth of GDP, the best way to get more stuff is to take somebody else's stuff.

25:17.480 --> 25:22.480
If you have exponential growth of stuff, everybody can have more stuff without taking other stuff roughly

25:22.480 --> 25:28.480
or at least the major nations don't have to take each other stuff, they can do it through colonialism or vassal nations,

25:28.480 --> 25:33.480
but exponential growth of GDP is comprehensively bad for the environment.

25:33.480 --> 25:47.480
All of that growth of GDP is an important thing called the Garrett relation that shows a one for one correlation between energy used and global GDP.

25:48.480 --> 25:59.480
It's a 99% correlation actually, meaning that the increases in efficiency of energy generation only give you about 1% change of more dollars per joule per year,

25:59.480 --> 26:04.480
but for the most part, exponential growth of GDP equals exponential energy demand.

26:04.480 --> 26:12.480
So climate change and exponential GDP are exactly correlated and all of that money gets used in a materials economy, a supply chain,

26:12.480 --> 26:18.480
it's a linear materials economy that through mining, logging, fishing, et cetera, is unrenewably taking stuff from the earth on one side,

26:18.480 --> 26:23.480
turning it into shit we use for a little while and making it into trash and pollution on the other side.

26:23.480 --> 26:31.480
You cannot run, this is like so obvious, but you cannot run an exponential financial system on a linear materials economy

26:31.480 --> 26:36.480
that has to be coupled to it on a finite planet forever, so you start to hit planetary boundaries, right?

26:36.480 --> 26:44.480
So what decreased us having likelihood for war moved us towards planetary boundaries on all the planetary boundaries.

26:44.480 --> 26:50.480
We said we can all have more stuff without taking each other's stuff by taking all the shit from nature as quickly as we can.

26:50.480 --> 26:59.480
So this was obviously not that smart for our own long term and now we're there where we're actually bypassing some of the planetary boundaries critical tipping points already.

26:59.480 --> 27:10.480
There was a paper published a couple months ago in the American Chemical Society Journal that said the planetary tipping point on certain environmental pollutants,

27:10.480 --> 27:22.480
particularly floral surfactants, had already been passed, meaning rainwater all around the world in very remote areas contained these PFOS forever chemicals beyond EPA safe levels.

27:22.480 --> 27:31.480
That means that if you're gathering rainwater in the middle of nowhere for your off-grid sustainable thing it has beyond EPA levels of carcinogen,

27:31.480 --> 27:37.480
neurotoxin, endocrine disrupting chemicals everywhere, no matter where you are on the planet because we've put that many of them into the environment already.

27:37.480 --> 27:43.480
And an exponential financial system means an exponential amount of pollution, mining, etc.

27:43.480 --> 27:56.480
And so the metacrisis, what happened for me was like, all right, well if we have to address factory farms but then shit we also have to address overfishing,

27:56.480 --> 28:01.480
we have to address what is the problem set? What is the actual problem set that we have to face?

28:01.480 --> 28:04.480
How do we make sure that when we're addressing it we don't cause other worst problems?

28:04.480 --> 28:06.480
So how do we understand the interconnectedness?

28:06.480 --> 28:18.480
I was always asking if we were to actually try to rebuild the world from scratch with 21st century problems and technologies and capacities that actually worked with the biosphere and human nature,

28:18.480 --> 28:25.480
how would we do it and then what does enactment look like to get there given all the vested interests and issues in the current world system?

28:25.480 --> 28:33.480
So you were mentioning third attractor. Third attractor roughly is there are two futures that we want to avoid.

28:33.480 --> 28:38.480
Well, mention the first and the second first so that people can understand the sort of negative image of it.

28:38.480 --> 28:50.480
But maybe just before you go there, just maybe explain what you really think, if there's a takeaway, what's absolutely different about this moment in history for mankind?

28:50.480 --> 28:57.480
You've kind of said it in a roundabout way, which is explaining the sort of crumbling of the Bretton Woods world system

28:58.480 --> 29:06.480
and that a lot of the solutions that we're trying to put forward actually generate bigger problems than we can possibly face.

29:06.480 --> 29:15.480
And in many instances, and you can talk about in the situation of the arms race between China and the US with AI or biotech, for example,

29:15.480 --> 29:22.480
without even talking about road actors that are in garages, but I think if you can sort of map this a little bit,

29:22.480 --> 29:27.480
because I think that it's helpful for people to see their place in history.

29:27.480 --> 29:33.480
Okay, so I was saying that the first existential tech was the bomb. We built a world system to deal with that.

29:33.480 --> 29:42.480
So one of the answers to that world system was the global financial system, which has driven us to all of the planetary boundaries that we currently face.

29:42.480 --> 29:45.480
Planetary boundary is a good way to put all the environmental issues in one place.

29:45.480 --> 29:54.480
So when we're talking about dead zones and oceans, overfishing, species extinction, loss of pollinators, climate change, ozone,

29:54.480 --> 30:01.480
all of those are basically places where the human social sphere, techno sphere complex is incompatible with the biosphere,

30:01.480 --> 30:06.480
but that means we're debasing the substrate that we depend upon. That means it's a system that's self-terminating.

30:06.480 --> 30:09.480
You cannot debase your own substrate forever.

30:09.480 --> 30:17.480
One of the other parts of the post-World War II system was globalization, and these radically interconnected six continent global supply chains

30:17.480 --> 30:21.480
that are necessary to make this microphone or these speakers or anything.

30:21.480 --> 30:26.480
One of the downsides of that we've seen during COVID is the radically interconnected supply chain.

30:26.480 --> 30:32.480
The benefit was you're less likely to bomb somebody who you depend upon for fundamental supply chain purposes.

30:32.480 --> 30:35.480
This is one of the big benefits of globalism.

30:35.480 --> 30:41.480
So the localism movement, if you were really successful at localism, there's actually less investedness in the other guy over there

30:41.480 --> 30:43.480
when I don't actually depend upon them.

30:43.480 --> 30:47.480
So these are some of the tensions we have to factor of, okay, do I want to make everything local,

30:47.480 --> 30:49.480
or do we want to actually have interdependence on them?

30:49.480 --> 30:54.480
But if we have it all global, then we get these cascading fragilities of where you can have an issue in Wuhan

30:54.480 --> 31:00.480
and get supply chain shutting down all around the world, which then means you don't get the movement of pesticides and fertilizers

31:00.480 --> 31:07.480
for the agricultural system in Iran and Northern Africa that probably put more people into radical food insecurity

31:07.480 --> 31:09.480
than we're totally at risk from COVID.

31:09.480 --> 31:18.480
So you can see how those post-World War II situations gave us this world of high interconnectedness

31:18.480 --> 31:22.480
but very high fragility and then high planetary fragility.

31:22.480 --> 31:30.480
And then also the exponential financial system meant the growth of exponential technology, you know, speeding up commerce.

31:30.480 --> 31:38.480
And the key thing to understand about that is that we don't have one technological weapon of mass destruction.

31:38.480 --> 31:39.480
Now we have many.

31:39.480 --> 31:47.480
And in the World War II system, the mutually assured destruction system, you had one catastrophe weapon and two actors that had it.

31:47.480 --> 31:52.480
So you could create a system of mutually assured destruction where neither one could utilize it.

31:52.480 --> 31:56.480
We currently have a world where you have dozens of catastrophe weapons.

31:56.480 --> 32:02.480
If we include all of the types of not only weapons of mass destruction but the ability to take out critical infrastructure

32:02.480 --> 32:10.480
and in a highly connected supply chain system, we have dozens of catastrophe weapons with not just many state actors

32:10.480 --> 32:12.480
but non-state actors having access to them.

32:12.480 --> 32:16.480
So you can't put some mutually assured destruction system on it.

32:16.480 --> 32:24.480
How do we make it through this much distributed technological power with the current incentive systems?

32:24.480 --> 32:33.480
So if you want to look at what is unique to this period of time, humans have been here for roughly 200,000 years,

32:33.480 --> 32:39.480
biologically identical, you know, 2 million years of hominids with tools.

32:39.480 --> 32:43.480
We didn't reach the first billion people till 1815, right?

32:43.480 --> 32:47.480
We were less than a half a billion people for that entire history.

32:47.480 --> 32:59.480
And then with the Industrial Revolution and liquid nitrogen fertilizer, we went from half a billion people to 8 billion people almost overnight.

32:59.480 --> 33:06.480
We simultaneously increased our energy consumption per person and our total resource consumption per person exponentially.

33:06.480 --> 33:12.480
So we exponentially increased the number of people and the consumption of resource per person.

33:12.480 --> 33:24.480
And this does bring us, so for the whole history of the world, we did not have the technological power to quickly destroy everything

33:24.480 --> 33:28.480
like nukes or even to pose a threat to the biosphere as a whole.

33:28.480 --> 33:36.480
That is only the result of post-industrial and now particularly post-late industrial technological capacity.

33:36.480 --> 33:46.480
A really important thing to understand is, you know, as a species, we, because of tool creating, were able to move from early environments

33:46.480 --> 33:51.480
to other environments in a way no other animal could and become the apex predator in every environment.

33:51.480 --> 33:59.480
So when we over-hunted an environment, a lion can't increase its predatory capacity radically faster than the gazelles can

33:59.480 --> 34:05.480
because predatory capacity only comes through genetic evolution, which is very slow over time and there's co-selective pressures.

34:05.480 --> 34:12.480
With tool making, we were able to increase our predative capacity through a different process that wasn't genetic evolution radically faster

34:12.480 --> 34:16.480
than anything else could increase its resilience to our predative capacity.

34:16.480 --> 34:21.480
So we could over-hunt an environment then rather than have our population fall back, move to a new environment.

34:21.480 --> 34:26.480
And so we've actually been on the beginning of a self-terminating path for a very long time.

34:26.480 --> 34:29.480
It's just an exponential curve that looks like this.

34:29.480 --> 34:36.480
And the first major bump was agriculture and then the next major bump was the industrial revolution and then it's been verticalizing.

34:36.480 --> 34:49.480
And so regarding earlier civilizations, though, we didn't have the technological capacity, even in aggregate, to mess up the biosphere.

34:49.480 --> 34:57.480
But we did have the technological capacity to mess up our own local environments and that's one of the main reasons early civilizations died.

34:57.480 --> 35:03.480
If you actually read The Collapse of Complex Societies by Joseph Tainter where you read Jared Diamond's book on it,

35:03.480 --> 35:10.480
many early civilizations actually died because they created topsoil erosion from bad agricultural practices,

35:10.480 --> 35:13.480
cut down too many trees and stopped being able to feed their people.

35:13.480 --> 35:20.480
So civilizational collapse from overuse of the environment is actually a multi-thousand year reality.

35:21.480 --> 35:25.480
And if you think about early civilizations, one of the first insights you'll have,

35:25.480 --> 35:32.480
whether we're thinking about the Ottoman Empire, the Egyptian Empire, the Roman Empire, is that none of them still exist.

35:32.480 --> 35:37.480
So it's actually the precedent of civilizations to have a life cycle and to fall.

35:37.480 --> 35:45.480
But most of them fall from internal self-terminating causes, either environmental ones or even if they lose a war to someone else,

35:45.480 --> 35:49.480
very often they lose a war to a smaller foe than they had defeated during their peak

35:49.480 --> 35:54.480
because internal decay and infighting happens from generational institutional decay.

35:54.480 --> 35:58.480
So self-induced purposes make civilizations break down.

35:58.480 --> 36:03.480
So civilizational collapse is actually the norm, right?

36:03.480 --> 36:06.480
That's the first thing that's important to understand.

36:06.480 --> 36:08.480
It's just it was always a local phenomena.

36:08.480 --> 36:15.480
This is the first time that we really have a global civilization in terms of the supply chains that we depend upon to meet our fundamental needs.

36:15.480 --> 36:19.480
And so we're in the process of a breakdown of this civilization.

36:19.480 --> 36:26.480
But what that portends in scale and what it portends to be having the biosphere, the ecological effects,

36:26.480 --> 36:29.480
but at a biosphere level is totally unprecedented.

36:29.480 --> 36:33.480
And what I would also say is that our solutions to the previous problems,

36:33.480 --> 36:36.480
because there's this nice narrative that there have always been problems,

36:36.480 --> 36:39.480
but we always come up with solutions and necessities the mother of invention

36:39.480 --> 36:43.480
and we'll figure our way out of this and then we get to live to solve new problems.

36:43.480 --> 36:44.480
And that's kind of true.

36:44.480 --> 36:52.480
But it is also true that the problem-solving process we have employed actually drives larger problems.

36:52.480 --> 36:58.480
And you get to a place where those problems are actually beyond the scale of what the biosphere can handle in human capacity.

36:58.480 --> 37:01.480
And so you actually have to have a different problem-solving process.

37:01.480 --> 37:07.480
So if you think about making a technology to solve a problem or a business or a law to solve a problem,

37:07.480 --> 37:13.480
you define the problem in a narrow way, like this viral issue or like a transportation issue or whatever it is.

37:13.480 --> 37:15.480
You define it in a narrow way.

37:15.480 --> 37:18.480
There's one or some small number of metrics you're trying to change.

37:18.480 --> 37:23.480
And you create a technology or a law or a business or a non-profit to produce a first-order effect,

37:23.480 --> 37:25.480
meaning a direct effect to solve that problem.

37:25.480 --> 37:31.480
But it interacts with ecologies and societies and psychologies which are complex

37:31.480 --> 37:37.480
and it has second and third and fourth-order effects on a whole bunch of metrics that aren't even identified.

37:37.480 --> 37:40.480
And that's where the harm ends up being externalized.

37:40.480 --> 37:49.480
So the metacrisis that we face currently, we can talk about very specifically what the generator functions are.

37:49.480 --> 37:56.480
But if we look at all of the planetary boundaries from species extinction to biodiversity loss writ large

37:56.480 --> 37:59.480
to nitrogen and phosphorus cycles to climate change and ozone,

37:59.480 --> 38:08.480
all of those are the result of an exponential financial system coupled to a linear materials economy hitting planetary boundaries.

38:08.480 --> 38:12.480
So you have to fundamentally make the materials economy go closed loop

38:12.480 --> 38:17.480
and the financial system has to stop being exponential, which means a post-growth financial system.

38:17.480 --> 38:22.480
That's a really, really huge lift to get from here to there.

38:22.480 --> 38:31.480
And then when we look at all of the issues associated with externalization from exponential tech,

38:31.480 --> 38:38.480
how do we steward the power that exponential technology gives us safely

38:38.480 --> 38:44.480
does require a totally different level of consideration of like, yes, I'm making this immuno,

38:44.480 --> 38:49.480
I'm doing this genetic modification science for cancer purposes,

38:49.480 --> 38:54.480
but as soon as I've done it, it now becomes cheap and easy for designer babies and every other kind of purpose.

38:54.480 --> 39:04.480
So how do we kind of mythopoetically say if no other animal had the ability to extinct species at scale

39:04.480 --> 39:07.480
or destroy ecosystems or genetically engineer new species?

39:07.480 --> 39:09.480
So this is not the power of apex predators.

39:09.480 --> 39:12.480
This is the power of nature, the power of gods.

39:12.480 --> 39:17.480
If we have the power of gods and not the love and wisdom of gods to steward it, we don't make it.

39:17.480 --> 39:24.480
So to make it through this technological kind of adolescence, what is the infrastructure that can make it

39:24.480 --> 39:31.480
and what is the social structure and culture that are required to be able to guide that are core interesting questions?

39:31.480 --> 39:39.480
So you've very eloquently defined how good we've been for a long time at being self-terminating civilizations

39:39.480 --> 39:41.480
and we've scaled it to the planet now.

39:41.480 --> 39:50.480
So I think to go back to my question around the first and second attractors and then the third,

39:50.480 --> 39:55.480
maybe browse very quickly on the first and the second because there's a whole theory around that.

39:55.480 --> 40:02.480
But there's a theory of change which you're proposing through the third attractor and I think that's where there's hope.

40:02.480 --> 40:09.480
And not only hope, it's sort of a story of collective ingenuity that has to unfold

40:09.480 --> 40:15.480
and unfortunately you have to go through a little bit of a difficult phase to appreciate the complexity of the problem

40:15.480 --> 40:18.480
with a new set of eyes before you can do so effectively.

40:18.480 --> 40:26.480
So that's how the lens of the metacrisis is useful because you can't get to that third attractor before you've understood that.

40:26.480 --> 40:36.480
So could you maybe just go quickly over the two first attractors and then the third one that will give us a little bit of a glimpse of hope?

40:36.480 --> 40:47.480
If we just think about, if you can build some gene synthesis in your basement cheaply with no exotic materials,

40:47.480 --> 40:59.480
if anybody can build gene synthesis in their basement, how does the world make it through that technology being a distributed capacity?

40:59.480 --> 41:06.480
If you think about it for a while, the answer almost everybody comes up with is it can't.

41:06.480 --> 41:09.480
So that can't become a distributed capacity.

41:09.480 --> 41:15.480
Okay, so we have to make it to where the companies that make the gene synthesizers are regulated.

41:15.480 --> 41:18.480
What about the DIY version that makes it on the internet?

41:18.480 --> 41:25.480
Well, now either we have to control information on the internet and some kind of, so who does that that can radically control the information

41:25.480 --> 41:30.480
or we have to know what people are doing in their basement, some kind of ubiquitous surveillance.

41:30.480 --> 41:39.480
This is some of the thinking behind the IoT and sesame credit system in China is actually not stupid thinking.

41:39.480 --> 41:43.480
It's forward thinking to distributed exponential tech and how do we deal with that?

41:43.480 --> 41:51.480
So you can see that the solution to preventing a catastrophe can be a control mechanism that can look like a dystopia.

41:51.480 --> 41:59.480
The two attractors that I say we want to avoid are catastrophes on one hand and solutions to catastrophes that involve being able to keep those from happening,

41:59.480 --> 42:05.480
which requires both optics of what's happening and the ability to prevent it, which sound like control mechanisms, which become dystopic.

42:05.480 --> 42:16.480
And so right now, one thing I'll say about the catastrophes is you have to look at the cascades between all of them together to make good sense of it.

42:16.480 --> 42:23.480
If you look at exponential tech as one category and environment as another and war as another and supply chain and electrical grid separately,

42:23.480 --> 42:25.480
you'll miss the way it actually happens.

42:25.480 --> 42:29.480
So when does climate change become a catastrophic risk?

42:29.480 --> 42:36.480
Well, you're talking about like the venousification of the planet or the drowning of coastal cities or something like that.

42:36.480 --> 42:43.480
Long time before that happens, extreme weather events that start to hit high population density areas and cause human migration

42:43.480 --> 42:47.480
can cause escalation to World War III, right?

42:47.480 --> 42:50.480
So you think about the extreme weather events in Australia.

42:50.480 --> 42:55.480
It was just very fortunate that that was low population density, low total population area.

42:55.480 --> 43:00.480
You look at the droughts in Syria that caused population movement and did cause a war.

43:00.480 --> 43:11.480
What if you look at the temperatures that Pakistan and Bangladesh and Northern India have been starting to hit during the summer

43:11.480 --> 43:18.480
and just say some time in the next few summers you get past the temperature where the crops fail

43:18.480 --> 43:20.480
and they don't actually have stored crop.

43:20.480 --> 43:22.480
A lot of it was destroyed during COVID.

43:22.480 --> 43:25.480
They don't have groundwater and when you're in a 50 Celsius heat wave,

43:25.480 --> 43:30.480
but you're talking about an area that has 100 million people now as opposed to a very small number of people.

43:30.480 --> 43:32.480
What happens?

43:32.480 --> 43:35.480
Does the resource war fall along Muslim Hindu lines?

43:35.480 --> 43:37.480
Does that lead to an India-Pakistan war?

43:37.480 --> 43:43.480
So with climate change, you're not looking at climate change just as a problem itself,

43:43.480 --> 43:47.480
but it is a force amplifier of the other problems and you have to look at all of them that way.

43:47.480 --> 43:52.480
So there's a lot of different entryways to catastrophic collapse and we're seeing some of them right now.

43:52.480 --> 43:58.480
And so one attractor is increasing catastrophes.

43:58.480 --> 44:01.480
The other meaning a likely path the world goes.

44:01.480 --> 44:05.480
Another attractor is the world recognizing that and saying,

44:05.480 --> 44:07.480
should we have to keep that from happening?

44:07.480 --> 44:13.480
So we have to actually deal with climate change through pricing carbon properly and degrowth.

44:13.480 --> 44:20.480
But that is really bad for a lot of people because as soon as you make carbon more expensive,

44:20.480 --> 44:26.480
you make all the commodities more expensive, which hurts the poor people the fastest and on and on.

44:26.480 --> 44:29.480
And everyone who doesn't want that, do you use violence against them?

44:29.480 --> 44:37.480
And so to prevent certain things ends up meaning control if currently human behavior is doing that.

44:37.480 --> 44:45.480
And so how do you prevent increasing dystopias, which a lot of people think the Chinese state is in the direction of.

44:45.480 --> 44:53.480
So a desirable future is a future that doesn't self-terminate and it doesn't have unchecked centralized power structures.

44:53.480 --> 44:58.480
The question, one of the causes of the increase in nationalism is the distrust in globalism.

44:58.480 --> 45:05.480
And one of the major reasons of the distrust of globalism is the idea that unchecked power doesn't have a good history.

45:05.480 --> 45:13.480
And so the idea that we at least keep power checked in a multilateral way is preferable for a lot of people.

45:13.480 --> 45:20.480
But if you have many different nations that are in economic competition with each other, nobody wants to price carbon properly.

45:20.480 --> 45:24.480
Because if the US does, it'll be radically disadvantaged relative to China.

45:24.480 --> 45:31.480
As a result, China's Belt and Road will geopolitically dominate the world and crowd out democracy and those types of things.

45:31.480 --> 45:38.480
And so if you have nation states in competition with each other, they just can't deal with global issues well.

45:38.480 --> 45:44.480
If you have global governance, who's creating a check on that power system to where it can't be captured or corrupted.

45:44.480 --> 45:51.480
So I would say thinking in terms of the design criteria we need has to be able to do global governance.

45:51.480 --> 45:56.480
It has to be able to deal with things like decentralized catastrophe weapons and basements.

45:56.480 --> 46:05.480
But it also simultaneously has to have checks and balances on every power system or every control system that are there that are adjudicatable.

46:05.480 --> 46:10.480
It would take me a long time to describe what I think the best processes for how to do that are.

46:10.480 --> 46:20.480
I will say that there are technological systems that could be enacted that meet these criteria that align with human nature.

46:20.480 --> 46:26.480
So I am optimistic about that. The enactment to get there takes a lot of work.

46:26.480 --> 46:29.480
So just briefly mentioned the third attractor.

46:29.480 --> 46:34.480
I'll move afterwards towards some hopeful scenarios that we might encounter.

46:34.480 --> 46:41.480
So that people don't leave this room with a sense of fear and catastrophe as being dominant.

46:41.480 --> 46:49.480
But in order for us to create a third attractor, we have to put some energy towards developing one.

46:49.480 --> 46:52.480
And that's not a simple thing to do.

46:52.480 --> 46:57.480
But first we have to understand what it is and how they could drive us away from those two attractors.

46:57.480 --> 47:05.480
Decentralized catastrophic capability and centralized capacity to control which is kind of dystopia.

47:10.480 --> 47:17.480
There is not a term. There is not like a term for a type of political economy or system that makes that third attractor.

47:17.480 --> 47:19.480
And we actually don't know.

47:19.480 --> 47:27.480
So specifically what I mean by the third attractor is literally something that can prevent all the catastrophic possibilities

47:27.480 --> 47:39.480
where the control mechanisms required to do so have the types of checks and balances that they prevent centralized power issues.

47:39.480 --> 47:42.480
So we are actually defining in terms of what it isn't.

47:42.480 --> 47:48.480
It's not catastrophes and it's not that the solution of the catastrophes involves dystopias.

47:48.480 --> 47:55.480
Exactly what that looks like, there's both how would we design it from scratch which is a nice question

47:55.480 --> 47:59.480
but ends up being kind of a nonsense question because we don't get to design it from scratch.

47:59.480 --> 48:06.480
There's this enactment issue of with all of the vested interests that the world currently has in play how do we actually get there.

48:06.480 --> 48:12.480
So I can give you a few examples of things that give the sense of what can make a third attractor possible.

48:12.480 --> 48:16.480
So I'm just going to mention one which I think you'll probably refer to.

48:16.480 --> 48:18.480
We have a common friend in Will Marshall.

48:18.480 --> 48:20.480
Were you going to mention Planet Labs?

48:20.480 --> 48:21.480
I wasn't but we can.

48:21.480 --> 48:22.480
Okay.

48:22.480 --> 48:29.480
Well I mean so Planet Labs, I'll let you talk about it but effectively I think it's interesting to see also that some of those third attractors

48:29.480 --> 48:36.480
reside in the domain of intelligence plus or human intelligence plus technology applied to a new level of

48:36.480 --> 48:40.480
what you've actually yourself called force of transparency.

48:40.480 --> 48:48.480
And that probably is not, it doesn't define exactly what the third attractor is but it's a sort of hopeful way of looking at technology

48:48.480 --> 48:55.480
that can constitute an underlying, let's say, conversation between different actors whether they're state or civic actors

48:55.480 --> 49:00.480
or technology bodies that can start to formulate more of those.

49:00.480 --> 49:08.480
So I think we both agree that there needs to be a proliferation of these examples and force transparency is a really important tool

49:08.480 --> 49:11.480
because we've got very weak international governance and law.

49:11.480 --> 49:12.480
Yeah.

49:12.480 --> 49:16.480
So maybe you can address the third attractor by giving some examples.

49:16.480 --> 49:23.480
Yeah so market forces like incentive by itself doesn't solve all the problems.

49:23.480 --> 49:27.480
So you end up having to use both incentive and deterrent.

49:27.480 --> 49:31.480
If we didn't have law protecting national forests, we wouldn't have national forests.

49:31.480 --> 49:36.480
You'd have market forces that continue to turn everything into commodities.

49:36.480 --> 49:45.480
But as far as international law, it's tricky because law mostly exists where you have a monopoly of violence that can enact the law.

49:45.480 --> 49:54.480
A police state inside of a nation state so where we have global issues like the oceans or the atmosphere, this is where we have a really tricky time

49:54.480 --> 50:05.480
because how do you, if you're going to make a law, it requires multinational agreement and then the ability to see if it's being violated

50:05.480 --> 50:13.480
and then the ability to enforce some enactment and the ability to enforce it where it's not more expensive to do so than the benefit you get, right?

50:13.480 --> 50:21.480
So let's say that we have an agreement about oceans and China's violating it and so we say, okay, we're going to sanction you for that.

50:21.480 --> 50:26.480
But the sanction is on supply chains that we depend upon and if you escalate, they also have nukes.

50:26.480 --> 50:30.480
So it's like there's a tricky thing with all that.

50:30.480 --> 50:41.480
But one of the places where a lot of environmental issues, global environmental issues don't get legal support is where we just don't even know what's happening.

50:41.480 --> 50:43.480
It's hard to know what's happening in the middle of the Amazon.

50:43.480 --> 50:47.480
It's hard to know what's happening in the middle of the open oceans.

50:47.480 --> 50:52.480
So this particular example is where technology can be repurposed in a positive way.

50:52.480 --> 50:56.480
There's a satellite imagery of the Earth is a pretty amazing technology.

50:56.480 --> 51:02.480
There's a friend of ours who runs a company called Planet Labs and they image the entire surface of the Earth every day.

51:02.480 --> 51:10.480
30 terabytes of compressed data but they're increasing the spectral and temporal resolution of that and spatial resolution

51:10.480 --> 51:16.480
such that it'll be pretty much real time human level video capture of the surface of the Earth in about three years.

51:16.480 --> 51:22.480
Which is amazing and one of the things it means is the ability to see where logging is happening

51:22.480 --> 51:27.480
and where mining is happening and where dumping is happening and where legal fishing is happening

51:27.480 --> 51:33.480
and even to be able to see in a dead zone in the ocean the effluent, how much of it came from which source,

51:33.480 --> 51:36.480
how much came from which port and all those types of things.

51:36.480 --> 51:41.480
The ability to see all that and use machine learning to process it means that there's a whole bunch of international law

51:41.480 --> 51:45.480
that we've never even bothered to create because there'd be no way to know if it was being violated or enforced.

51:45.480 --> 51:48.480
Now we'd have the ability to create international law that says,

51:48.480 --> 51:51.480
no we actually don't have plausible deniability anymore.

51:51.480 --> 51:57.480
We know exactly how much of the trash or the nitrogen effluent came from there because we can see the whole thing.

51:57.480 --> 52:04.480
It also has the ability to do spectral analysis that can see an invasive species entering an area

52:04.480 --> 52:13.480
or soil microbes in an area to be able to actually support the environment when critical issues are starting to happen.

52:13.480 --> 52:19.480
But this is itself very concerning because probably many of you, even as I'm describing this, are like,

52:19.480 --> 52:24.480
wow, that's really hopeful for the environment to be able to have that level of transparency that could create law

52:24.480 --> 52:25.480
so we could support the environment.

52:25.480 --> 52:31.480
But fuck, who gets to have access to video level data of the entire surface of the Earth all of the time?

52:31.480 --> 52:34.480
That sounds like pretty massive surveillance capability, which it is.

52:34.480 --> 52:41.480
So that can prevent certain catastrophes but can totally create dystopias depending upon how it's managed.

52:41.480 --> 52:48.480
So how do we create the governance of that information such that it doesn't get used for nefarious purposes

52:48.480 --> 52:51.480
and that people get to know what it's being used for?

52:51.480 --> 52:53.480
This is not trivial, right?

52:53.480 --> 52:56.480
Because it's easy to deploy the technology to solve those problems.

52:56.480 --> 53:01.480
It can be quite hard to create the governance to ensure that it's used properly.

53:01.480 --> 53:08.480
Well, the official version that Will gives me is that it's 50 centimeter resolution so you can't see a face.

53:08.480 --> 53:16.480
It's easier to count trees and cars and tanks in Ukraine when the Russians pretend that they're leaving a certain battle area.

53:16.480 --> 53:21.480
But let's agree that, you know, that power is something that needs to be at least checked.

53:21.480 --> 53:29.480
So what I guess I was trying to get at is there is some hope in terms of how we can leverage technology

53:29.480 --> 53:35.480
in order for us to sort of monitor and then have some checks and balances

53:35.480 --> 53:46.480
and also create the international agreements and legal frameworks to enforce some form of limits, if you want to call it that.

53:46.480 --> 53:53.480
So something I'd like to say is there is a naive techno-optimism that I think is super dangerous

53:53.480 --> 54:01.480
that just tech will solve all the problems and the very worst version of it is we're only a few years from generalized AI

54:01.480 --> 54:03.480
and then that'll be able to solve all the problems.

54:03.480 --> 54:10.480
If you study the alignment issue of how do we ensure that truly generalized AI is aligned with our interests,

54:10.480 --> 54:13.480
it's a really, really tricky problem.

54:13.480 --> 54:18.480
There's also a naive anti-tech, a kind of naive Luddite perspective that's like,

54:18.480 --> 54:24.480
man, all these problems are because of excessive tech, quality of life is actually better at a lower level of technology.

54:24.480 --> 54:27.480
Let's get back to the land and permaculture and that kind of thing.

54:27.480 --> 54:35.480
But if you study the history of the world, any time there is an intersection of a less technologically advanced society

54:35.480 --> 54:39.480
with a more technologically advanced society, it doesn't go well for the less technologically advanced.

54:40.480 --> 54:49.480
So the China-Tibet type interaction always happened and so whatever this group of people are saying,

54:49.480 --> 54:54.480
we're going to do the less technologically advanced will not actually influence the direction of the way the future goes

54:54.480 --> 54:57.480
because the technology is power which does mean influence.

54:57.480 --> 55:04.480
So the future will be high tech but it has to be also high nature and high connectivity or it will self-terminate.

55:04.480 --> 55:09.480
So you have to say, we don't get to put the Pandora's box of tech closed,

55:09.480 --> 55:12.480
but we have to actually become wise stewards of it.

55:12.480 --> 55:16.480
So what does a high tech, high nature, high connectivity future actually look like?

55:16.480 --> 55:22.480
And if we don't have the technological capacity for outsized influence over the current systems,

55:22.480 --> 55:26.480
the current systems will be what dominates.

55:26.480 --> 55:30.480
And so I am a techno-optimist but not a naive techno-optimist,

55:30.480 --> 55:33.480
meaning I know that all the existential risks we couldn't do without tech.

55:33.480 --> 55:35.480
Stone Age people cannot destroy the planet.

55:35.480 --> 55:43.480
So I'm acutely aware that all the catastrophic risks are results of human activity extended through technological levers.

55:43.480 --> 55:49.480
But I'm also aware that the solutions to those things can't avoid technological elements,

55:49.480 --> 55:51.480
but the technology alone is not sufficient.

55:51.480 --> 55:56.480
So one way we think about this, there's an anthropologist named Marvin Harris,

55:56.480 --> 56:01.480
and he said you can think about civilizations as these triples of what he called the infrastructure,

56:01.480 --> 56:04.480
the social structure, and the superstructure.

56:04.480 --> 56:09.480
The infrastructure is the tech stack that the civilization depends upon and meets all of its needs with.

56:09.480 --> 56:15.480
The social structure is the collective agreement field, so economics, governance, law, and the institutions.

56:15.480 --> 56:21.480
And the superstructure is basically the culture, the values, the identity, the definition of what the good life,

56:21.480 --> 56:23.480
what we're motivated by are.

56:23.480 --> 56:31.480
He particularly argued that they are interconnected, but the tech changes in tech drive the other ones.

56:31.480 --> 56:35.480
And he gave a heap of examples from the plow to the wheel to on and on,

56:35.480 --> 56:41.480
where a change in tech meant that whoever used that tech, now they're behaving differently, right?

56:41.480 --> 56:44.480
Driving a plow is a different set of behaviors than hunting.

56:45.480 --> 56:52.480
But no tech catches on if it doesn't provide adaptive advantage.

56:52.480 --> 56:56.480
If it does provide adaptive advantage, it changes pattern of human behavior by using it.

56:56.480 --> 57:00.480
By changing human behavior, you also change human values,

57:00.480 --> 57:06.480
and then everybody else has to adopt it, or they lose in war to whoever has that increased adaptive advantage.

57:06.480 --> 57:13.480
So he basically said cultures and political systems change because tech changes.

57:13.480 --> 57:18.480
There are other deep anthropologists and sociologists who say,

57:18.480 --> 57:23.480
no, actually, and give a heap of examples of how cultural changes make us innovate in different ways,

57:23.480 --> 57:28.480
aligned with our values, or have us bind our technology, like the Sabbath, or things like that,

57:28.480 --> 57:31.480
and say that cultural changes are the deepest.

57:31.480 --> 57:33.480
And then there are plenty of others who say,

57:33.480 --> 57:35.480
no, ultimately the economy and law is the deepest thing,

57:35.480 --> 57:41.480
because ultimately whatever you incentivize financially is the technology that will be developed.

57:41.480 --> 57:46.480
If you change the subsidies and the taxes and the incentives, the tech stack would evolve differently.

57:46.480 --> 57:50.480
I would say that all three of these, the infrastructure, the social systems,

57:50.480 --> 57:54.480
and the superstructure or culture, are equally fundamental and inner affecting,

57:54.480 --> 57:57.480
and you have to think about changes to all three simultaneously.

57:57.480 --> 58:01.480
So if you dismiss any of them out of hand, like,

58:01.480 --> 58:05.480
ah, cultural changes don't matter that much, or technological changes, or government doesn't,

58:05.480 --> 58:08.480
you're definitely not thinking comprehensively.

58:08.480 --> 58:12.480
The favorite one, like, we can just do culture change and everything else will automatically happen.

58:12.480 --> 58:14.480
That's also not thinking comprehensively.

58:14.480 --> 58:20.480
They're all necessary and only thinking about them together and the way they inter-effect each other is sufficient.

58:20.480 --> 58:24.480
So we could think about, if we want a future that avoids all these catastrophes,

58:24.480 --> 58:27.480
what does the infrastructure have to look like?

58:27.480 --> 58:32.480
Pretty quickly we can say we can't keep using nature unrenewably and turning it into pollution and waste unrenewably,

58:32.480 --> 58:35.480
so it has to look closed loop and it has to look post-growth,

58:35.480 --> 58:37.480
because you can't grow it exponentially on a finite planet.

58:37.480 --> 58:40.480
So what types of technologies would mediate that?

58:40.480 --> 58:46.480
And what things should be global and what things should be local has a lot to do with the social structure interaction of

58:46.480 --> 58:52.480
there are things that you want to be global in so long as you're wanting to bind the well-being of those people to each other

58:52.480 --> 58:56.480
through supply chains and interdependence and that kind of thing.

58:56.480 --> 59:01.480
What would the social systems of a future look like and what would the culture?

59:01.480 --> 59:09.480
There have been conversations today around in-group, out-group and identity systems that's culture questions, right?

59:09.480 --> 59:17.480
I think there's a lot of probably focus on the well-being picture here at Harvest and its virtual picture of planetary identity.

59:17.480 --> 59:22.480
And obviously the planetary identity has to be not just humans but all life forms,

59:22.480 --> 59:28.480
because you can advance all humans at the expense of the biosphere for a little while, but then it goes pretty badly for the humans.

59:29.480 --> 59:37.480
So when we think about Third Attractor, we have to think about what is the culture of it, what must it be?

59:37.480 --> 59:43.480
What must the coordination systems and the distribution and allocation of resources,

59:43.480 --> 59:47.480
and you start to get into things like, well, man, doesn't interest by itself,

59:47.480 --> 59:52.480
even if we don't think about central bank policy or interest rates or fractional reserve banking or anything,

59:52.480 --> 59:58.480
doesn't interest itself compounding interest force exponential growth of finance?

59:58.480 --> 01:00:00.480
Yeah, it does.

01:00:00.480 --> 01:00:05.480
And then to not debase the currency, doesn't that mean you have to have an exponential growth of goods and services?

01:00:05.480 --> 01:00:09.480
Yeah, it does. Doesn't that mean you basically have an exponential materials economy on a finite planet?

01:00:09.480 --> 01:00:11.480
Yeah, so interest has to go.

01:00:11.480 --> 01:00:15.480
Well, that's really fundamental. We don't know how to make that world.

01:00:16.480 --> 01:00:26.480
And then as long as most access to resources based on private property, doesn't that mean rival risk interest

01:00:26.480 --> 01:00:31.480
where I can do better at the expense of the environment and others based on private property?

01:00:31.480 --> 01:00:34.480
Probably a lot of stuff has to be rethought around property law.

01:00:34.480 --> 01:00:39.480
And then even when you get to, I can appreciate the atmosphere.

01:00:39.480 --> 01:00:42.480
In fact, my life depends upon it, but I don't have to pay for it.

01:00:42.480 --> 01:00:48.480
And so if I cut a tree down, I get immediate benefit from the timber.

01:00:48.480 --> 01:00:53.480
And the little tiny damage it causes to the atmosphere, I don't really notice.

01:00:53.480 --> 01:00:55.480
But when everybody thinks that way, it does have that effect.

01:00:55.480 --> 01:00:59.480
But locally, I have way more incentive to cut it down than to leave it up,

01:00:59.480 --> 01:01:06.480
because the extraction value that I get from turning it into lumber gives me game theoretic value.

01:01:06.480 --> 01:01:13.480
Whereas if I put my resources towards planting more trees that I don't have economic interest in,

01:01:13.480 --> 01:01:19.480
I do less well in the economic system, this means that there is a fundamental rethinking of the value equation.

01:01:19.480 --> 01:01:26.480
Because whoever ends up valuing extractable, exchangeable wealth ends up doing game theoretically much better than those who don't,

01:01:26.480 --> 01:01:28.480
which means they influence the world and culture more.

01:01:28.480 --> 01:01:33.480
Those who pay more attention to common wealth have less influence over the whole thing.

01:01:33.480 --> 01:01:43.480
So the changes that we're talking about at the level of economics are things like interest, private property, fungible currency,

01:01:43.480 --> 01:01:46.480
even deeper than whether we have nation states or not.

01:01:46.480 --> 01:01:51.480
And the same in terms of thinking through what is the future of the tech stack look like.

01:01:51.480 --> 01:01:54.480
So we share a lot of views, and thank you for laying it out.

01:01:54.480 --> 01:02:01.480
I hope everyone could follow with these, you know, this way of presenting where we're at.

01:02:01.480 --> 01:02:11.480
A lot of people are quite naturally fearful of where we're going in terms of the labor force because of AI, for example.

01:02:11.480 --> 01:02:18.480
And so in one of your talks, I don't know if it was very recent, and we share this view about this by the way,

01:02:18.480 --> 01:02:22.480
you talked about education or educators and nurses.

01:02:22.480 --> 01:02:30.480
I think it's a good way to present a hopeful opportunity for us to do something where humans are uniquely designed to do something different than machines.

01:02:30.480 --> 01:02:35.480
And where efficiency is not what matters. It's more about qualitative than the quantitative.

01:02:35.480 --> 01:02:39.480
And our computational capability is not challenged by that.

01:02:39.480 --> 01:02:46.480
So I thought maybe I'd switch gears a little bit, but it's giving a little bit of hope again in terms of how we can address concretely

01:02:46.480 --> 01:02:51.480
some of these challenges that we're facing, whether we're techno-optimists or techno-scepticists.

01:02:51.480 --> 01:03:03.480
So I worked with the G7 on a scheme to try and infuse into national security in the G8 countries a concept of benevolent AI.

01:03:03.480 --> 01:03:09.480
And we were just studying with 80 scientists from all over the world how we could potentially put that into motion

01:03:09.480 --> 01:03:13.480
and start to educate government bodies and leaders.

01:03:13.480 --> 01:03:18.480
And the main failure point was purely geopolitical.

01:03:18.480 --> 01:03:23.480
So the arms race and AI just made it so that it made every single suggestion stalemate.

01:03:23.480 --> 01:03:32.480
So we have to address it on a population level and also in terms of how, in terms of society, we adapt to the change that's coming towards us.

01:03:32.480 --> 01:03:37.480
So we've adapted to bringing cars into our life, arguably imperfectly.

01:03:37.480 --> 01:03:45.480
We've adapted to many changes in our society in terms of healthcare and pandemics and how we travel and etc. etc.

01:03:45.480 --> 01:03:50.480
Give us a little, you know, I share this view with you, by the way.

01:03:50.480 --> 01:04:01.480
How you think educators and nurses could become a little bit of a new orientation for mankind as AI steps in.

01:04:01.480 --> 01:04:10.480
So the topic of technological automation creating jobs issue, there's a couple perspectives.

01:04:10.480 --> 01:04:18.480
One primary perspective is technological automation will obsolete certain jobs, but it will create new jobs.

01:04:18.480 --> 01:04:23.480
It's always been the case we don't have elevator operators anymore, but there's plenty of new jobs.

01:04:23.480 --> 01:04:25.480
And then there's the other perspective.

01:04:25.480 --> 01:04:31.480
No, actually advanced robotics and AI are different in kind in some of the earlier industrial technologies.

01:04:31.480 --> 01:04:38.480
And they're different in speed that as new jobs are created they will still be able to beat humans at doing it for market purposes,

01:04:38.480 --> 01:04:42.480
which I think is much more true.

01:04:42.480 --> 01:04:52.480
So if you take an AI like Google's AI, you take AlphaGo, you can train it on chess and how to beat everybody in chess.

01:04:52.480 --> 01:04:54.480
You can train it on Go and how to beat everybody at Go.

01:04:54.480 --> 01:04:57.480
You can train it on StarCraft and how to beat everybody at StarCraft.

01:04:57.480 --> 01:05:02.480
It can gain the capacity very quickly to be humans at any finitely definable game.

01:05:02.480 --> 01:05:08.480
And so AI is different in kind than other previous technologies.

01:05:08.480 --> 01:05:15.480
It's more like the difference between us and all the other animals with our ability to innovate, you know, creating recursive technology on technology

01:05:15.480 --> 01:05:22.480
that allowed us to become apex predators in every environment and then more than that, the AI is kind of like that jump again.

01:05:22.480 --> 01:05:35.480
And so it does portend a break of capitalism and market structures as we know it and that most of, not just labor,

01:05:35.480 --> 01:05:43.480
but most of the jobs that we currently have and even the new jobs that emerge in the niches that it creates, it's better at than we are.

01:05:43.480 --> 01:05:49.480
That sounds pretty terrible if you keep the existing political economy where people need the jobs,

01:05:49.480 --> 01:05:55.480
but one of the main reason we created a system where the people need the jobs is because the jobs needed the people to do them.

01:05:55.480 --> 01:06:04.480
And if you, to make a civilization run well, if you had to get the people to do the jobs and a market seemed like a better answer than the state forcing the people to do it.

01:06:04.480 --> 01:06:08.480
So let's let the market force people to do it and they can kind of self-organize.

01:06:08.480 --> 01:06:16.480
But as soon as the jobs don't need the people to do them anymore, you can also start thinking about economies where the people don't need the jobs,

01:06:16.480 --> 01:06:22.480
which is why now a lot of people are thinking about universal basic income and like different ways of thinking about that.

01:06:22.480 --> 01:06:26.480
There's early naive thoughts on universal basic income.

01:06:26.480 --> 01:06:28.480
Of course, it's the beginning of thinking about it.

01:06:28.480 --> 01:06:39.480
But there's a really deep question which is, what is the role of humans in a world of advanced robotics and AI?

01:06:39.480 --> 01:06:44.480
Because the advanced robotics and AI will be better than us at most of the things that we're used to being good at.

01:06:44.480 --> 01:06:46.480
So what is the role of humans in that world?

01:06:46.480 --> 01:06:49.480
What are we still uniquely good at?

01:06:49.480 --> 01:06:52.480
And then what is also intrinsically fulfilling and meaningful?

01:06:52.480 --> 01:07:04.480
And there becomes a steep question of what does education become in a post-technological automation world where you're not preparing people for the workforce in the same way?

01:07:04.480 --> 01:07:07.480
What is the role of education and human development?

01:07:07.480 --> 01:07:13.480
But obviously to answer that, you have to say not just education, but and obviously there's the economics component.

01:07:13.480 --> 01:07:16.480
How do we do resource allocation and access in that world?

01:07:16.480 --> 01:07:20.480
But there's a really deep civilizational question of what is the role of human life?

01:07:20.480 --> 01:07:30.480
And if we for a moment avoid the topic of sentient AI, which is a whole theoretically difficult question, and we just talk about functional AI,

01:07:30.480 --> 01:07:35.480
meaning AI that we're not saying there's something that it is like to be it.

01:07:35.480 --> 01:07:39.480
We're simply saying it's very good at figuring out how to do stuff.

01:07:39.480 --> 01:07:46.480
Then right away the key, what is uniquely different about humans and it is sentience.

01:07:46.480 --> 01:07:52.480
Is the capacity to actually have there be something that it is like to be you at all?

01:07:52.480 --> 01:07:57.480
Is subjective experience and then inner subjective, connective capacity?

01:07:57.480 --> 01:08:04.480
And what's interesting is things related to sentience are what is where our intrinsic motive,

01:08:04.480 --> 01:08:12.480
if we're not being behaviorally controlled by extrinsic motive, i.e. being paid to do a thing or external deterrence.

01:08:12.480 --> 01:08:17.480
And so mostly you have to pay people to do shit they don't want to do, right?

01:08:17.480 --> 01:08:22.480
And if the shit that people don't want to do is increasingly getting automated,

01:08:22.480 --> 01:08:32.480
can you then have a world where largely what humans are spending time doing also more deeply coincides with what they have the deepest intrinsic motive to do,

01:08:32.480 --> 01:08:42.480
particularly if you then have a developmental system and developmental society oriented to find out what the unique human motivations and capacities of each person are and develop them in light of that.

01:08:42.480 --> 01:08:49.480
And so not only does AI portend something in terms of the obsolescence of humans for lots of labor roles and repetitive things,

01:08:49.480 --> 01:09:00.480
one of the things Tomas was probably referencing was the topic of AI in human tutoring is pretty amazing.

01:09:00.480 --> 01:09:05.480
It's also scary as fuck because again, you have to get this thing right.

01:09:05.480 --> 01:09:16.480
If you have an AI that has so many orders of magnitude, more information processing in terms of being able to model your micro expressions to see how you're learning,

01:09:16.480 --> 01:09:25.480
you can also have undue influence in a way that no cult leader has dreamed of, right, in terms of asymmetries of power of influence.

01:09:25.480 --> 01:09:28.480
So who controls that and how do they control that?

01:09:28.480 --> 01:09:33.480
These actually become the cultural questions, the theoretical philosophic questions that are so fundamental.

01:09:33.480 --> 01:09:42.480
If you can genetically engineer humans and have designer babies, don't we all want to be like tall and beautiful and thin and is that actually the right idea?

01:09:42.480 --> 01:09:49.480
As soon as you have the ability to actually design intelligent machines and design self-replicating machines and change biology,

01:09:49.480 --> 01:09:57.480
the philosophic questions of what is good and desirable become so fundamental because so much becomes possible, right?

01:09:57.480 --> 01:10:07.480
But if you do it right, imagine, everybody's heard about deepfakes, the ability to train an AI.

01:10:07.480 --> 01:10:13.480
You can make basically pictures that aren't real, faces that aren't real, from AI images that look totally real.

01:10:13.480 --> 01:10:20.480
You can also make a video of me speaking where it looks exactly like me, sounds like me, but it's totally AI generated, it's not real.

01:10:20.480 --> 01:10:23.480
That technology already exists, it's not that good.

01:10:23.480 --> 01:10:34.480
Some people have made deepfakes of me that are audio that sound like me, but the deepfake videos are currently about three years away from being Turing test passing,

01:10:34.480 --> 01:10:42.480
you can watch a video of Obama or Bernie Sanders or whoever say something and have no idea if it was real or not,

01:10:42.480 --> 01:10:46.480
and you won't have the human capacity to tell if it's real or fake.

01:10:46.480 --> 01:10:50.480
You can imagine what that does to our ability for collective sense-making.

01:10:50.480 --> 01:10:58.480
But that same deepfake capacity can be positively purposed because what that means is it's trained on the semantic patterns and the vocal patterns

01:10:58.480 --> 01:11:05.480
enough to be able to generate novel answers like a chatbop, but where you can't tell that it's not real.

01:11:05.480 --> 01:11:11.480
So now imagine an educational environment where you train that same AI, this is large language models as the type of AI,

01:11:11.480 --> 01:11:19.480
say you train it on all the writings of Thomas Jefferson or all the writings of Socrates through Plato or whatever and writings about them,

01:11:19.480 --> 01:11:31.480
such that I can go into a metaverse environment and say I want to pull up Einstein and von Neumann and Kurt Girdel and be able to have a talk with them about formal logic,

01:11:31.480 --> 01:11:41.480
and not only can it seem like I'm actually having a conversation with them where they're sharing differing views based on their actual views, writing, etc.

01:11:42.480 --> 01:11:51.480
But I could also just have an avatar that is like the voice of chemistry, that is the holistic knowledge of all chemistry that no human could be,

01:11:51.480 --> 01:12:02.480
and yet all of them are the AI's best attempt to model what that person would do in terms of semantic coherency with what they did and said in the past.

01:12:02.480 --> 01:12:10.480
So now imagine a future where every kid has access to be able to study with Einstein and Gandhi and Socrates and von Neumann and whatever directly,

01:12:10.480 --> 01:12:18.480
where those AI's can model our theory of mind and titrate the learning directly to us associated with our learning dynamics,

01:12:18.480 --> 01:12:28.480
but then also because the jobs have largely been automated, what humans spend way more of their time with is things like being educators and being nurses and being musicians

01:12:28.480 --> 01:12:36.480
and the things that have kind of high connectivity value because those are the things that the machines don't do as well as the actual sense of shared interiority.

01:12:36.480 --> 01:12:45.480
So now imagine that we have way more teachers per capita that are way more well trained, so all the teachers are PhD level trained,

01:12:45.480 --> 01:12:56.480
there's one of them per 10 kids as opposed to one per 30, and now I get out of my AI environment where I was just studying physics with Einstein

01:12:56.480 --> 01:13:06.480
and my tutor asks me a question like, what do you think was different about what the AI Einstein said than what an actual Einstein alive today might have said?

01:13:06.480 --> 01:13:13.480
So then helping us to try to understand the difference between human intelligence and artificial intelligence and what it means to be sentient,

01:13:13.480 --> 01:13:19.480
and what effect does consciousness have on intelligence, so not only are they getting that level of educational access,

01:13:20.480 --> 01:13:26.480
but the AI can do, but the differential of what is unique about human intelligence and artificial intelligence.

01:13:26.480 --> 01:13:33.480
So this is one of a million examples we can give of how those technologies could do mind-bendingly amazing things.

01:13:33.480 --> 01:13:42.480
I'll say one quick thing about this is there is a study done on super genius of the past or polymaths,

01:13:42.480 --> 01:13:52.480
who advanced many different fields beyond what the specialists in those fields did, and was there anything that the great polymaths had in common,

01:13:52.480 --> 01:13:57.480
and the single thing that stood out the most was that they were all the result of aristocratic tutoring.

01:13:57.480 --> 01:14:04.480
And this was actually a very taboo topic because when we ended feudalism and tried to create democratic states,

01:14:05.480 --> 01:14:14.480
the idea, like if you think of meditations by Marcus Aurelius, Marcus Aurelius spends the entire first chapter just thanking all of his tutors,

01:14:14.480 --> 01:14:21.480
but when you're being raised to be the emperor of Rome, the best mathematician, the best poet, the best grammarian, the best historian,

01:14:21.480 --> 01:14:29.480
literally in all of Rome are your private tutors, and you can't learn to think like a mathematical genius from someone who wasn't a mathematical genius,

01:14:29.480 --> 01:14:34.480
so your average math teacher cannot teach you to think like that because they don't think like that.

01:14:34.480 --> 01:14:38.480
And yet, how do you make that accessible to everyone? You don't.

01:14:38.480 --> 01:14:45.480
So the aristocratic tutoring in the past was a nice patronage job for the smartest people, but it was also so radically unequal,

01:14:45.480 --> 01:14:51.480
but it's what created the best minds. So could that possibly be made popular?

01:14:51.480 --> 01:15:02.480
Well, we can see in a third attractor kind of world the application of these technologies where you could actually have better tutoring than Marcus Aurelius had for everybody,

01:15:02.480 --> 01:15:09.480
you know, et cetera, et cetera. Now, for each of these wonderful scenarios are like a million ways it goes wrong,

01:15:09.480 --> 01:15:17.480
and so how we steward that properly is the key defining thing over the next while.

01:15:17.480 --> 01:15:19.480
Thank you, Daniel.

01:15:19.480 --> 01:15:21.480
Thank you.

