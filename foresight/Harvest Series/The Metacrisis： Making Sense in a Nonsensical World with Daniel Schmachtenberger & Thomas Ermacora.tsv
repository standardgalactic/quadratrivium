start	end	text
0	19040	I just want to say thank you for this welcome again from the team at Harvest.
19040	24480	It's great for me to have been able to suggest Daniel as a speaker here.
24480	31480	I'm very fond of him as a human, as a fellow futurist and as a thinker for our time.
31480	36480	I think it probably needs a little introduction. Why is Daniel here?
36480	41480	Daniel is not a prophet of doom, absolutely not.
41480	48480	He's trying to equip us with the understanding that will help us navigate some of the crises that are ahead of us.
48480	55480	It's something that some people call the metacrisis and he's one of the main people in the world now who can explain it very clearly.
55480	60480	I'll try and break it up a little bit so that we can go over some of the points again.
60480	70480	It's difficult to say what Consilience Project is in a few words because it has evolved over time to confront the reality of what the world is ready to accept.
70480	80480	However, it's certainly one of, in my opinion, the best attempts and very tangible attempts to help people navigate what's right now in front of us.
80480	88480	The news cycle is certainly not helping us understand. It's sort of distracting our minds from the real things that are going on.
88480	102480	So Daniel is one of these people out there who's working really to formalize the way that we can anticipate, connect, collaborate and work more effectively to solve problems that we are facing.
102480	114480	So Daniel, I think there's something that we should know about your past and I'm going to be provocative.
114480	128480	If you didn't actually start out with the same objective, you sort of thought that maybe we were doomed and that we had to accelerate doom in order to get somewhere.
128480	134480	I'm going to be very provocative so that you can explain your journey from a young man to where you are now.
134480	144480	I was homeschooled and when I was nine, I was at a gas station and a factory farming cattle truck pulled up and so I went and looked inside the holes.
144480	156480	The cow nearest to me was missing its eye and bleeding. All the cows were in terrible condition and I grew up loving animals, eating meat from factory farms, not putting the two together.
156480	166480	I was just kind of shocked and horrified by that. I asked my parents about it. They said that's where animals come from so I asked to go to a factory farm.
166480	174480	I became a vegetarian that day. I started working with PETA in Greenpeace and studying animal rights work.
174480	188480	That took me from the issue of factory farming to the issue of species extinction, whaling, overfishing the oceans, which took me into the issues of the environment and that took me into global poverty and just kind of the whole set of issues as a young person.
188480	199480	I was fortunate being homeschooled that I got to just have that be my curriculum. I was actually an educational experiment. My parents ran right and didn't have any fixed curriculum.
199480	208480	I just got a study that I was interested in so this became the context of most of my life study and the next really important part happened.
208480	223480	I was 13. I was working on a project that Greenpeace and World Wildlife Federation were leading to protect elephant poaching in a particular elephant preserve in Kenya where the poachers had come in over the preserve and hunted a bunch of elephants.
223480	236480	I saw the videos of the elephant slaughter and was moved in the same way as the factory farms and the thing I observed though was over the two years of the people on the ground working extremely hard to protect the elephants there,
236480	245480	the strategy was get bigger fences around the preserve so the poachers couldn't get in and do legislation to get harsher sentencing for poachers in the area.
245480	254480	All the activists had their life threatened but they finally succeeded. It was a huge win. Those elephants didn't get hunted but they didn't address the poverty of the people that were doing the poaching.
254480	262480	They didn't address a macro economy that creates poverty at scale. They didn't address the views towards animals or identity regarding them or black markets on animal parts.
262480	269480	The same poaching groups moved to hunt other things which happened to be the white rhino and the mountain gorilla both of which were more endangered than the elephant.
269480	276480	I was working with enough groups that I got to see those other issues get worse as a direct result of the success of that project.
276480	285480	And then that was like the second existential hit. The first existential hit with the factory farms was, I'm saying this to relate,
285480	292480	that was the first time I had like a suicidal ideation around not feeling okay being complicit with my species.
292480	302480	My species was somehow fundamentally fucked up and as a kid it wasn't that hard to realize if I kill myself all those cows are still in the factory farms.
302480	315480	I didn't help them. That's unethical. But then I'm like, if I stay alive to try to help it, can my life be a success while sentient beings are experiencing that much suffering?
315480	321480	What does it say about me that I can be totally stoked that my life is a success when other beings are in that kind of suffering?
321480	329480	I'm like, there's no answer for me that doesn't address that. So I remember thinking to myself then I'm like, alright, well if I die and factory farms still exist,
329480	340480	I failed like there is no definition of success for me that isn't a sociopath's definition of success that has to empathetically separate too intensely.
340480	347480	But at least the hope came that like activism might work. And so then when this thing happened where I saw that the activism for one thing made other things worse,
347480	354480	that was the next kind of devastation. And that led to me starting to look at how many other places where we were doing activism,
354480	365480	where the world was doing activism, did that kind of issue. And I saw that there were projects to solve global poverty by creating hydroelectric dams to bring electricity to areas,
365480	380480	and hydroelectric dams, of course, drowned whole ecosystems and extinct species, and on and on. And kind of like it seemed like the world was caught in these trade-offs where when they would focus on solving one problem,
380480	390480	it was too narrowly defined, the result of solving that problem would have externalities related with larger systems. I started to do kind of historical analysis on that,
390480	399480	and that became kind of a defining trend of all of the issues. We created the automobile to solve a transportation issue, which was that horses were a very limited means of transportation.
399480	410480	Transportation, horses in the cities caused a lot of horseshit in places like London. Literally the excessive horseshit in the cities of London was one of the kind of like significant impetuses to make a horseless carriage.
410480	419480	And in solving that problem and creating transportation that increased mobility and comfort for the whole world, climate change, like the venusification of the entire planet,
419480	427480	and oil spills, and wars over oil, and the US petrodollar, like all of that were the side effects of solving a transportation issue, you know, a couple hundred years later.
427480	439480	And I got to see that there was this underlying deep issue that statecraft, that is thinking about the well-being of its own citizens, the rest of the world,
439480	445480	so it's not just exporting its harm somewhere else and of the planet. And there's a lot of countries where they're genicoefficient,
445480	453480	meaning their measure of wealth inequality is pretty good within their country, but they import a bunch of stuff to make their country function from countries that have the worst genicoefficient.
453480	460480	So you can't actually do genicoefficient at the level of a country, it ends up just being a way to kind of whitewash the reality of global supply chains.
460480	467480	Oh, so the conscious statecraft thing. So how do you do what is good for your own country, what's good for other countries,
467480	472480	and what's good for the planet when there are fundamental trade-offs between them and you're stuck in those trade-offs?
472480	480480	That becomes really the deep challenging question. If we look at climate change today, we can see that for the planet and for all of us in the future,
480480	489480	we should not just decarbonize but degrowth. We can also see that any nation that tries to lead that will do worse GDP-wise in the near term,
489480	495480	which means worse geopolitically and actually so much worse that, particularly for the leading countries like U.S. and China,
495480	504480	change the control systems of the world. So so many of the people in the U.S. that are anti-climate change aren't actually anti-climate change
504480	510480	because they did a good analysis of the IPCC science. It's that the solution of increasing the price of carbon for us,
510480	516480	which will decrease our GDP relative to China's GDP in a great power game, which will increase planetary autocracy,
516480	522480	seems like such a bad solution that they don't want the climate change assessment because they don't want the solution that goes with it.
523480	532480	So a part of the thing that I focus on is how do we understand the interconnectivity and the generators of all the issues deeply enough
532480	540480	that we can come up with solutions that don't cause other problems. But one point was a little while after this I had come to,
540480	547480	I started doing forecasting on how many people have read Limits of Growth, the MIT Club of Rome book.
547480	554480	So I read that as a teenager and then started looking at a lot of other analyses on species extinction and biodiversity loss
554480	561480	and dead zones and oceans and all those things. And then I looked at the, you know, Stephen Pinker kind of stuff of everything's getting better.
561480	568480	A lot of people might have that question of like, you hear Stephen Pinker, Hans Rosling Gates and like everything's getting better,
568480	573480	and then you hear all the environmental statistics and like everything's getting worse. They're both true.
573480	582480	You can cherry pick your stats, but they're not equally true. The things that are getting worse are leading to the unviability of human habitation
582480	589480	ongoingly in a way that the things that are getting better don't converge on solving automatically. So there's a real,
589480	595480	and the things that we're making better are causing the things that are getting worse towards self extinction points, right?
595480	603480	Like grow global GDP where we all have like nice stuff because of that in a way that breaks the biosphere's capacity to continue to make life possible.
603480	611480	So we have to do something deeper than saying it's getting better and worse so we just choose. It's like the way that we make things get better
611480	618480	is a major part of what's making them get worse towards tipping points that are unique now to any point in previous civilization.
618480	625480	So when I was at 15, I came to the assessment that there was no chance that humanity would actually make it in time,
625480	632480	and that if we had radically less population, we might have a chance, and I started studying viruses and depopulation strategies,
632480	642480	which is a thing that an angsty but earnest caring teenager might do. I had a spiritual experience that took me off that path while I was working on it,
642480	655480	but I was seriously studying vector delivery of novel pathogens, and the spiritual experience was something Bucky Fuller used to say,
655480	665480	and I had heard it a lot but it hadn't hit me in the same way that the chicken developing inside of a shell is totally unsustainable, right?
665480	675480	When it's eating the unrenewable resource, the egg white, it's creating metabolic pollution in its environment, it doesn't even know that it's inside of a shell,
675480	682480	but it's in a developmental phase, and of course it doesn't have the beak or the gastrointestinal tract to eat seeds yet,
682480	688480	the whites are exactly kind of what it developmentally needs to go through that embryonic phase, and right as it runs out of whites,
688480	696480	as soon as its GI tract and beak and all the things are developed to be able to crack through the shell and emerge into a world where now it's part of a new phase,
696480	708480	and I started thinking about humanity as being unrenewable because of being in a developmental phase or a discrete phase shift that isn't the result of just a continuity of the previous lines,
708480	717480	and I thought about all the examples of caterpillar to chrysalis to butterfly, and that in the chrysalis there's a fundamental dissolution of all the organs
717480	728480	and a restructuring with a different genetic code that if a fetus went more than 40 weeks in the mother's uterus, it would kill itself and kill the mom,
728480	735480	but it has to go through that developmental period, and then the birth is a discrete kind of difficult process, and then the umbilical cords cut,
735480	746480	now there's a new developmental time, and so I had this kind of profound experience that developmental phases, whether inside of an egg or in a chrysalis or in a womb,
746480	755480	are always unsustainable. There is some discrete nonlinear phase shift that is different than the curve in the developmental phase,
755480	763480	and will be different afterwards, and I started thinking about if humanity as a species was in a developmental phase,
763480	772480	and there was a discrete phase shift that was different than the curves, what that might look like, but that got me off of the track that Thomas was bringing up,
772480	784480	but what's really interesting is a lot of my work now involves the way that exponential technology equals exponential decentralized catastrophe weapons for everyone.
784480	794480	Whenever we talk about the positives of exponential tech, AI, biotechnology, nanotechnology, cyber technology,
794480	801480	we talk about in positive terms usually like the democratization of this great technological power of creation,
801480	809480	but the democratization of catastrophe weapons is actually not a great idea, right? Like keeping nukes to the G8,
809480	815480	but you can do because it's really hard to make nukes, and you can see who's making nukes, and there's not that many places that have uranium,
815480	821480	and enriching uranium is hard, and you can see it from outer space, but it's pretty easy to make drone weapons,
821480	830480	and it's increasingly becoming easy, and the ability to use those for infrastructure targets, and specifically it's becoming pretty easy to make pathogens.
830480	840480	In the advancing of synthetic biology, we're only a few years away from the ability to synthesize novel pathogens on a desktop for $1,000 anywhere,
840480	846480	and so when I was a kid, that was not true, it was actually like a hard thing to do, it's becoming an increasingly easy thing to do,
846480	858480	while increasingly more people are feeling kind of concerned and disenfranchised, so as the group of people that would be motivated to change this world system in harmful ways,
858480	865480	and the group of people that are capable is converging and increasing, there's a lot of risk associated with that, how do we deal with that?
865480	869480	That's an interesting part of the topic.
869480	873480	I'm really excited that you didn't turn out to become a terrorist.
873480	881480	It wasn't motivated by not liking people, it was motivated by seeing self-induced human extinction as inevitable, and as being the only way out,
881480	891480	and that's actually like most terrorists are well motivated, they're motivated in service of something they care about that they feel is being harmed and they don't know other solutions.
891480	896480	We could agree, but I'll pause on that one.
896480	905480	I think what a lot of people here are probably expecting if you have heard anything about Daniel is you present a little bit about the meta crisis,
905480	913480	so there's a framework of thinking around how we can put dots in between a lot of the crises that we're facing collectively,
913480	923480	and instead of addressing one and creating externalities in another problem set, perhaps understanding the total problem set at the same time,
923480	932480	and before you act actually have a more enlightened perspective gives you more edge, so that's an assumption and a hopeful one,
932480	939480	and some people might argue that we need to act very fast on certain issues, so they don't really like this sort of temporization of it,
939480	947480	but I'm of the belief like you that basically having better tools to understand the world we live in is essential for us to make critical decisions,
947480	955480	and I would love for you to spend a moment unfolding what you call, or what we call, the meta crisis,
955480	963480	and why it's unique to this moment in history, and why humanity has to put some effort towards solving that concretely,
963480	967480	or we will probably end up in a pretty bad place very quickly.
967480	974480	So it could easily seem like the stuff that I'm talking about is just the collection of all the worst news in one place.
974480	979480	I would bother sharing it because I think it is true.
979480	988480	The risks that we're talking about are true, it's not determined that we definitely fail at them or that we definitely succeed at them,
988480	996480	so what we do actually matters in determining it, and there's no chance that we can solve it if we don't more competently understand it,
996480	1006480	so more people competently understanding and seriously working to apply themselves to the unique needs of this particular time in the world
1006480	1016480	is something that I'm hopeful for, and I guess that's why I'm speaking here, and said yes to Thomas inviting me here.
1016480	1027480	So the meta crisis frame might actually help deal with some catastrophe fatigue because rather than see the issue of drones and autonomous weapons
1027480	1037480	and the issue of exponential tech-empowered terrorism and this planetary boundary issue and this pollinator issue and this forever chemical issue as separate issues,
1037480	1046480	I look at them all as expressions of an interconnected set of generative dynamics that we call the meta crisis,
1046480	1054480	where what it takes to solve any of them is the same, actually, and if you try to solve them without factoring these deep underlying generative dynamics,
1054480	1060480	you will at bet, you probably won't solve it, and if you do, you'll displace problems somewhere else and actually kind of mess the whole thing up,
1060480	1067480	so when you understand that all those problems are connected at first, it makes it seem more overwhelming because you're like fuck,
1067480	1073480	to think about climate change, I also have to think about geopolitics and fundamental changes to finance and all these other issues,
1073480	1083480	and that seems like a lot of complexity, but it actually takes it from too many problems to tractably manage all of which have solutions that end up externalizing harm elsewhere,
1083480	1091480	which means impossible, too hard but tractable, so hopefully in terms of being able to see it all as one interconnected set of issues,
1091480	1098480	you're like alright, there's a lot more learning that I need to do to be able to competently engage, but there is actually a way through,
1098480	1104480	there's actually kind of a tractable analysis, so that's what I hope to share.
1104480	1112480	So, if you can move towards what people call the third attractor or that you like to call the third attractor,
1112480	1120480	so that we sort of gravitate towards a landscape of defining what is the solution environment that we want to find ourselves in,
1120480	1130480	because obviously there are a lot of people who are very competent, who are deploying intelligence and capital and being very innovative about what they do,
1130480	1137480	but they sometimes miss the fact that they are creating really negative externalities in other, let's say problem sets,
1137480	1144480	so the third attractor to me seems like a very easy thing to understand for people, even though if we don't have the answer to what the third attractor is,
1144480	1151480	but it's at least theoretically a framework for understanding why the metacrisis may find a solution set through a third attractor.
1151480	1156480	Yeah, let me give an example though of how we solve problems and make worse problems that are current and really relevant.
1156480	1166480	We're working to try to change currently US federal government program and have had some really good success with it for pandemic prevention,
1166480	1171480	for preventing whatever the next pandemic from animal sources do not expel over would be.
1171480	1176480	And the federal government, and it's not only the US, lots of countries employ this approach.
1176480	1182480	The US has been leading the way in what seems like great science and technology and innovation towards preemptive problem solving,
1182480	1191480	which all seems like the right thing, but the approach to preventing zoonotic spillover involves viral hunting,
1191480	1199480	so going out and finding tens of thousands of new viruses in bat caves that have mammalian viruses that have never been exposed to humans before,
1199480	1206480	bringing them back to labs, doing gain of function research on them to figure out how they might mutate into things that are more virulent or transmissible,
1206480	1212480	and then publishing all those genome sequences to an open source database so everyone who wants to work on vaccines has access to the knowledge.
1212480	1220480	It seems like a decentralizing information, democratizing, multi-state coordination, science, anticipatory, good thing,
1220480	1226480	and it's maybe one of the worst things happening in the world, and so we're lucky that we've been able to shift it.
1227480	1236480	If you open source publish all of the pandemic grade viral gene sequences in an age where gene synthesis is becoming extremely cheap,
1236480	1247480	that we're about three years out from tabletop gene drives and CRISPR and like that, the bioterrorism potential of that is just unimaginable,
1247480	1252480	and even just the accidental kind of lab leak dynamics that when you have enough labs working with enough things,
1252480	1256480	the probability that none of them happens drops towards zero over enough period of time.
1256480	1262480	There was a lab doing gain of function research that figured out how to make an extremely virulent version of H1N1,
1262480	1268480	like an R0 of 18, H1N1 is like a 60% fatality rate, and it did that in a biosecurity level 2 lab.
1268480	1273480	So lab leaks happen, right, like this is an example of trying to do the right thing,
1273480	1279480	but not understanding the problem space well enough and doing something totally that's the wrong thing,
1279480	1286480	and what I remember the first time I was engaged in the UN network and it was a project with World Food Program when I was 20,
1286480	1292480	the solution to world hunger involved bringing conventional NPK based agriculture to the developing world,
1292480	1300480	so we didn't have to send food over there and the answer was way more nitrogen and phosphorus effluent into the rivers that would cause faster dead zones in the ocean,
1300480	1306480	and so I talked to the guy about it and I said, you realize you'll speed up the rate of dead zones in the ocean catastrophically if you do this,
1306480	1312480	and he said I hadn't thought of that, but those aren't the metrics I'm tasked with, and those aren't the metrics I'm tasked with,
1312480	1321480	so I'm going to, for a few years, decrease hunger while working to extinct the planet because that's what my accountability is like.
1321480	1325480	It just became very clear that that problem-solving approach was ubiquitous,
1325480	1334480	and so another great example is you look at the advancement of something like gene editing, CRISPR,
1334480	1342480	it's being advanced for purposes we all want, like immuno-oncology, how do you change 15,000 genes at once to be able to, you know,
1342480	1349480	cure and prevent cancers that we're genetically predisposed to, but all technologies are dual purpose or multi-purpose,
1349480	1357480	meaning every technology that you can make for some positive purpose has a military or otherwise weaponized or kind of externalizing application,
1357480	1366480	so the research that's being done on how to do that type of gene editing is making it then really cheap and easy.
1366480	1372480	Once we figured out how to do it, it takes major universities that have ethical review boards to do it for that purpose
1372480	1378480	to develop the technologies that then drop the price by orders of magnitude to do it for any purpose and it's open publishing,
1378480	1387480	so to give a little bit of history because some people might think, well, people have been predicting rapture since the 1600s,
1387480	1392480	there's always some kind of like Mayan 2012, whatever, and this is just new catastrophism.
1392480	1400480	I would really like people to think more deeply about what is discontinuous and novel in this time relative to other times,
1400480	1402480	so I want to argue that real quick.
1402480	1411480	The first technology we had that was powerful enough for humans to actually make the planet meaningfully uninhabitable was the nuclear bomb in World War II.
1411480	1418480	That was the first truly existential technology. It was not an exponential technology, meaning nukes don't automatically make better nukes
1418480	1424480	in the ways that computers automatically make better computers. Computation allows us to design better computer chips recursively,
1424480	1432480	you get Moore's Law, but it was the first existential technology and that was really a break from the history of the entire world of tech
1432480	1440480	up until that point because up until that point, every new military tech that we had, there was an absolute arms race to deploy it as quickly as we could
1440480	1447480	and to win more battles and territory based on deploying it. This was the first one where we actually had to make an entire world system
1447480	1452480	to ensure we would never deploy it because nobody would win, mutually assured destruction.
1452480	1458480	All of a sudden, you're like, whoa, we're so big that we can't actually deploy our tech without destroying everything.
1458480	1461480	We can't actually do the us versus them effectively anymore at that level.
1461480	1470480	So post World War II, and we haven't had another world war that is kinetic between superpowers yet and we're at the brink of it right now.
1471480	1483480	Post World War II, because of that tech, we rebuilt the entire global world system to deal with preventing kinetic World War III
1483480	1490480	and that kind of Bretton Woods world system, IGO world system has been effective at preventing World War III,
1490480	1498480	but that world system is almost totally broken down now and it drove all of the catastrophic risks we're facing currently.
1498480	1508480	Specifically, one part of the post World War II system was a international monetary system that created, that had exponential growth of GDP.
1508480	1513480	Why is exponential growth of GDP important is because the wars are based on everybody wanting more stuff
1513480	1517480	and if you don't have exponential growth of GDP, the best way to get more stuff is to take somebody else's stuff.
1517480	1522480	If you have exponential growth of stuff, everybody can have more stuff without taking other stuff roughly
1522480	1528480	or at least the major nations don't have to take each other stuff, they can do it through colonialism or vassal nations,
1528480	1533480	but exponential growth of GDP is comprehensively bad for the environment.
1533480	1547480	All of that growth of GDP is an important thing called the Garrett relation that shows a one for one correlation between energy used and global GDP.
1548480	1559480	It's a 99% correlation actually, meaning that the increases in efficiency of energy generation only give you about 1% change of more dollars per joule per year,
1559480	1564480	but for the most part, exponential growth of GDP equals exponential energy demand.
1564480	1572480	So climate change and exponential GDP are exactly correlated and all of that money gets used in a materials economy, a supply chain,
1572480	1578480	it's a linear materials economy that through mining, logging, fishing, et cetera, is unrenewably taking stuff from the earth on one side,
1578480	1583480	turning it into shit we use for a little while and making it into trash and pollution on the other side.
1583480	1591480	You cannot run, this is like so obvious, but you cannot run an exponential financial system on a linear materials economy
1591480	1596480	that has to be coupled to it on a finite planet forever, so you start to hit planetary boundaries, right?
1596480	1604480	So what decreased us having likelihood for war moved us towards planetary boundaries on all the planetary boundaries.
1604480	1610480	We said we can all have more stuff without taking each other's stuff by taking all the shit from nature as quickly as we can.
1610480	1619480	So this was obviously not that smart for our own long term and now we're there where we're actually bypassing some of the planetary boundaries critical tipping points already.
1619480	1630480	There was a paper published a couple months ago in the American Chemical Society Journal that said the planetary tipping point on certain environmental pollutants,
1630480	1642480	particularly floral surfactants, had already been passed, meaning rainwater all around the world in very remote areas contained these PFOS forever chemicals beyond EPA safe levels.
1642480	1651480	That means that if you're gathering rainwater in the middle of nowhere for your off-grid sustainable thing it has beyond EPA levels of carcinogen,
1651480	1657480	neurotoxin, endocrine disrupting chemicals everywhere, no matter where you are on the planet because we've put that many of them into the environment already.
1657480	1663480	And an exponential financial system means an exponential amount of pollution, mining, etc.
1663480	1676480	And so the metacrisis, what happened for me was like, all right, well if we have to address factory farms but then shit we also have to address overfishing,
1676480	1681480	we have to address what is the problem set? What is the actual problem set that we have to face?
1681480	1684480	How do we make sure that when we're addressing it we don't cause other worst problems?
1684480	1686480	So how do we understand the interconnectedness?
1686480	1698480	I was always asking if we were to actually try to rebuild the world from scratch with 21st century problems and technologies and capacities that actually worked with the biosphere and human nature,
1698480	1705480	how would we do it and then what does enactment look like to get there given all the vested interests and issues in the current world system?
1705480	1713480	So you were mentioning third attractor. Third attractor roughly is there are two futures that we want to avoid.
1713480	1718480	Well, mention the first and the second first so that people can understand the sort of negative image of it.
1718480	1730480	But maybe just before you go there, just maybe explain what you really think, if there's a takeaway, what's absolutely different about this moment in history for mankind?
1730480	1737480	You've kind of said it in a roundabout way, which is explaining the sort of crumbling of the Bretton Woods world system
1738480	1746480	and that a lot of the solutions that we're trying to put forward actually generate bigger problems than we can possibly face.
1746480	1755480	And in many instances, and you can talk about in the situation of the arms race between China and the US with AI or biotech, for example,
1755480	1762480	without even talking about road actors that are in garages, but I think if you can sort of map this a little bit,
1762480	1767480	because I think that it's helpful for people to see their place in history.
1767480	1773480	Okay, so I was saying that the first existential tech was the bomb. We built a world system to deal with that.
1773480	1782480	So one of the answers to that world system was the global financial system, which has driven us to all of the planetary boundaries that we currently face.
1782480	1785480	Planetary boundary is a good way to put all the environmental issues in one place.
1785480	1794480	So when we're talking about dead zones and oceans, overfishing, species extinction, loss of pollinators, climate change, ozone,
1794480	1801480	all of those are basically places where the human social sphere, techno sphere complex is incompatible with the biosphere,
1801480	1806480	but that means we're debasing the substrate that we depend upon. That means it's a system that's self-terminating.
1806480	1809480	You cannot debase your own substrate forever.
1809480	1817480	One of the other parts of the post-World War II system was globalization, and these radically interconnected six continent global supply chains
1817480	1821480	that are necessary to make this microphone or these speakers or anything.
1821480	1826480	One of the downsides of that we've seen during COVID is the radically interconnected supply chain.
1826480	1832480	The benefit was you're less likely to bomb somebody who you depend upon for fundamental supply chain purposes.
1832480	1835480	This is one of the big benefits of globalism.
1835480	1841480	So the localism movement, if you were really successful at localism, there's actually less investedness in the other guy over there
1841480	1843480	when I don't actually depend upon them.
1843480	1847480	So these are some of the tensions we have to factor of, okay, do I want to make everything local,
1847480	1849480	or do we want to actually have interdependence on them?
1849480	1854480	But if we have it all global, then we get these cascading fragilities of where you can have an issue in Wuhan
1854480	1860480	and get supply chain shutting down all around the world, which then means you don't get the movement of pesticides and fertilizers
1860480	1867480	for the agricultural system in Iran and Northern Africa that probably put more people into radical food insecurity
1867480	1869480	than we're totally at risk from COVID.
1869480	1878480	So you can see how those post-World War II situations gave us this world of high interconnectedness
1878480	1882480	but very high fragility and then high planetary fragility.
1882480	1890480	And then also the exponential financial system meant the growth of exponential technology, you know, speeding up commerce.
1890480	1898480	And the key thing to understand about that is that we don't have one technological weapon of mass destruction.
1898480	1899480	Now we have many.
1899480	1907480	And in the World War II system, the mutually assured destruction system, you had one catastrophe weapon and two actors that had it.
1907480	1912480	So you could create a system of mutually assured destruction where neither one could utilize it.
1912480	1916480	We currently have a world where you have dozens of catastrophe weapons.
1916480	1922480	If we include all of the types of not only weapons of mass destruction but the ability to take out critical infrastructure
1922480	1930480	and in a highly connected supply chain system, we have dozens of catastrophe weapons with not just many state actors
1930480	1932480	but non-state actors having access to them.
1932480	1936480	So you can't put some mutually assured destruction system on it.
1936480	1944480	How do we make it through this much distributed technological power with the current incentive systems?
1944480	1953480	So if you want to look at what is unique to this period of time, humans have been here for roughly 200,000 years,
1953480	1959480	biologically identical, you know, 2 million years of hominids with tools.
1959480	1963480	We didn't reach the first billion people till 1815, right?
1963480	1967480	We were less than a half a billion people for that entire history.
1967480	1979480	And then with the Industrial Revolution and liquid nitrogen fertilizer, we went from half a billion people to 8 billion people almost overnight.
1979480	1986480	We simultaneously increased our energy consumption per person and our total resource consumption per person exponentially.
1986480	1992480	So we exponentially increased the number of people and the consumption of resource per person.
1992480	2004480	And this does bring us, so for the whole history of the world, we did not have the technological power to quickly destroy everything
2004480	2008480	like nukes or even to pose a threat to the biosphere as a whole.
2008480	2016480	That is only the result of post-industrial and now particularly post-late industrial technological capacity.
2016480	2026480	A really important thing to understand is, you know, as a species, we, because of tool creating, were able to move from early environments
2026480	2031480	to other environments in a way no other animal could and become the apex predator in every environment.
2031480	2039480	So when we over-hunted an environment, a lion can't increase its predatory capacity radically faster than the gazelles can
2039480	2045480	because predatory capacity only comes through genetic evolution, which is very slow over time and there's co-selective pressures.
2045480	2052480	With tool making, we were able to increase our predative capacity through a different process that wasn't genetic evolution radically faster
2052480	2056480	than anything else could increase its resilience to our predative capacity.
2056480	2061480	So we could over-hunt an environment then rather than have our population fall back, move to a new environment.
2061480	2066480	And so we've actually been on the beginning of a self-terminating path for a very long time.
2066480	2069480	It's just an exponential curve that looks like this.
2069480	2076480	And the first major bump was agriculture and then the next major bump was the industrial revolution and then it's been verticalizing.
2076480	2089480	And so regarding earlier civilizations, though, we didn't have the technological capacity, even in aggregate, to mess up the biosphere.
2089480	2097480	But we did have the technological capacity to mess up our own local environments and that's one of the main reasons early civilizations died.
2097480	2103480	If you actually read The Collapse of Complex Societies by Joseph Tainter where you read Jared Diamond's book on it,
2103480	2110480	many early civilizations actually died because they created topsoil erosion from bad agricultural practices,
2110480	2113480	cut down too many trees and stopped being able to feed their people.
2113480	2120480	So civilizational collapse from overuse of the environment is actually a multi-thousand year reality.
2121480	2125480	And if you think about early civilizations, one of the first insights you'll have,
2125480	2132480	whether we're thinking about the Ottoman Empire, the Egyptian Empire, the Roman Empire, is that none of them still exist.
2132480	2137480	So it's actually the precedent of civilizations to have a life cycle and to fall.
2137480	2145480	But most of them fall from internal self-terminating causes, either environmental ones or even if they lose a war to someone else,
2145480	2149480	very often they lose a war to a smaller foe than they had defeated during their peak
2149480	2154480	because internal decay and infighting happens from generational institutional decay.
2154480	2158480	So self-induced purposes make civilizations break down.
2158480	2163480	So civilizational collapse is actually the norm, right?
2163480	2166480	That's the first thing that's important to understand.
2166480	2168480	It's just it was always a local phenomena.
2168480	2175480	This is the first time that we really have a global civilization in terms of the supply chains that we depend upon to meet our fundamental needs.
2175480	2179480	And so we're in the process of a breakdown of this civilization.
2179480	2186480	But what that portends in scale and what it portends to be having the biosphere, the ecological effects,
2186480	2189480	but at a biosphere level is totally unprecedented.
2189480	2193480	And what I would also say is that our solutions to the previous problems,
2193480	2196480	because there's this nice narrative that there have always been problems,
2196480	2199480	but we always come up with solutions and necessities the mother of invention
2199480	2203480	and we'll figure our way out of this and then we get to live to solve new problems.
2203480	2204480	And that's kind of true.
2204480	2212480	But it is also true that the problem-solving process we have employed actually drives larger problems.
2212480	2218480	And you get to a place where those problems are actually beyond the scale of what the biosphere can handle in human capacity.
2218480	2221480	And so you actually have to have a different problem-solving process.
2221480	2227480	So if you think about making a technology to solve a problem or a business or a law to solve a problem,
2227480	2233480	you define the problem in a narrow way, like this viral issue or like a transportation issue or whatever it is.
2233480	2235480	You define it in a narrow way.
2235480	2238480	There's one or some small number of metrics you're trying to change.
2238480	2243480	And you create a technology or a law or a business or a non-profit to produce a first-order effect,
2243480	2245480	meaning a direct effect to solve that problem.
2245480	2251480	But it interacts with ecologies and societies and psychologies which are complex
2251480	2257480	and it has second and third and fourth-order effects on a whole bunch of metrics that aren't even identified.
2257480	2260480	And that's where the harm ends up being externalized.
2260480	2269480	So the metacrisis that we face currently, we can talk about very specifically what the generator functions are.
2269480	2276480	But if we look at all of the planetary boundaries from species extinction to biodiversity loss writ large
2276480	2279480	to nitrogen and phosphorus cycles to climate change and ozone,
2279480	2288480	all of those are the result of an exponential financial system coupled to a linear materials economy hitting planetary boundaries.
2288480	2292480	So you have to fundamentally make the materials economy go closed loop
2292480	2297480	and the financial system has to stop being exponential, which means a post-growth financial system.
2297480	2302480	That's a really, really huge lift to get from here to there.
2302480	2311480	And then when we look at all of the issues associated with externalization from exponential tech,
2311480	2318480	how do we steward the power that exponential technology gives us safely
2318480	2324480	does require a totally different level of consideration of like, yes, I'm making this immuno,
2324480	2329480	I'm doing this genetic modification science for cancer purposes,
2329480	2334480	but as soon as I've done it, it now becomes cheap and easy for designer babies and every other kind of purpose.
2334480	2344480	So how do we kind of mythopoetically say if no other animal had the ability to extinct species at scale
2344480	2347480	or destroy ecosystems or genetically engineer new species?
2347480	2349480	So this is not the power of apex predators.
2349480	2352480	This is the power of nature, the power of gods.
2352480	2357480	If we have the power of gods and not the love and wisdom of gods to steward it, we don't make it.
2357480	2364480	So to make it through this technological kind of adolescence, what is the infrastructure that can make it
2364480	2371480	and what is the social structure and culture that are required to be able to guide that are core interesting questions?
2371480	2379480	So you've very eloquently defined how good we've been for a long time at being self-terminating civilizations
2379480	2381480	and we've scaled it to the planet now.
2381480	2390480	So I think to go back to my question around the first and second attractors and then the third,
2390480	2395480	maybe browse very quickly on the first and the second because there's a whole theory around that.
2395480	2402480	But there's a theory of change which you're proposing through the third attractor and I think that's where there's hope.
2402480	2409480	And not only hope, it's sort of a story of collective ingenuity that has to unfold
2409480	2415480	and unfortunately you have to go through a little bit of a difficult phase to appreciate the complexity of the problem
2415480	2418480	with a new set of eyes before you can do so effectively.
2418480	2426480	So that's how the lens of the metacrisis is useful because you can't get to that third attractor before you've understood that.
2426480	2436480	So could you maybe just go quickly over the two first attractors and then the third one that will give us a little bit of a glimpse of hope?
2436480	2447480	If we just think about, if you can build some gene synthesis in your basement cheaply with no exotic materials,
2447480	2459480	if anybody can build gene synthesis in their basement, how does the world make it through that technology being a distributed capacity?
2459480	2466480	If you think about it for a while, the answer almost everybody comes up with is it can't.
2466480	2469480	So that can't become a distributed capacity.
2469480	2475480	Okay, so we have to make it to where the companies that make the gene synthesizers are regulated.
2475480	2478480	What about the DIY version that makes it on the internet?
2478480	2485480	Well, now either we have to control information on the internet and some kind of, so who does that that can radically control the information
2485480	2490480	or we have to know what people are doing in their basement, some kind of ubiquitous surveillance.
2490480	2499480	This is some of the thinking behind the IoT and sesame credit system in China is actually not stupid thinking.
2499480	2503480	It's forward thinking to distributed exponential tech and how do we deal with that?
2503480	2511480	So you can see that the solution to preventing a catastrophe can be a control mechanism that can look like a dystopia.
2511480	2519480	The two attractors that I say we want to avoid are catastrophes on one hand and solutions to catastrophes that involve being able to keep those from happening,
2519480	2525480	which requires both optics of what's happening and the ability to prevent it, which sound like control mechanisms, which become dystopic.
2525480	2536480	And so right now, one thing I'll say about the catastrophes is you have to look at the cascades between all of them together to make good sense of it.
2536480	2543480	If you look at exponential tech as one category and environment as another and war as another and supply chain and electrical grid separately,
2543480	2545480	you'll miss the way it actually happens.
2545480	2549480	So when does climate change become a catastrophic risk?
2549480	2556480	Well, you're talking about like the venousification of the planet or the drowning of coastal cities or something like that.
2556480	2563480	Long time before that happens, extreme weather events that start to hit high population density areas and cause human migration
2563480	2567480	can cause escalation to World War III, right?
2567480	2570480	So you think about the extreme weather events in Australia.
2570480	2575480	It was just very fortunate that that was low population density, low total population area.
2575480	2580480	You look at the droughts in Syria that caused population movement and did cause a war.
2580480	2591480	What if you look at the temperatures that Pakistan and Bangladesh and Northern India have been starting to hit during the summer
2591480	2598480	and just say some time in the next few summers you get past the temperature where the crops fail
2598480	2600480	and they don't actually have stored crop.
2600480	2602480	A lot of it was destroyed during COVID.
2602480	2605480	They don't have groundwater and when you're in a 50 Celsius heat wave,
2605480	2610480	but you're talking about an area that has 100 million people now as opposed to a very small number of people.
2610480	2612480	What happens?
2612480	2615480	Does the resource war fall along Muslim Hindu lines?
2615480	2617480	Does that lead to an India-Pakistan war?
2617480	2623480	So with climate change, you're not looking at climate change just as a problem itself,
2623480	2627480	but it is a force amplifier of the other problems and you have to look at all of them that way.
2627480	2632480	So there's a lot of different entryways to catastrophic collapse and we're seeing some of them right now.
2632480	2638480	And so one attractor is increasing catastrophes.
2638480	2641480	The other meaning a likely path the world goes.
2641480	2645480	Another attractor is the world recognizing that and saying,
2645480	2647480	should we have to keep that from happening?
2647480	2653480	So we have to actually deal with climate change through pricing carbon properly and degrowth.
2653480	2660480	But that is really bad for a lot of people because as soon as you make carbon more expensive,
2660480	2666480	you make all the commodities more expensive, which hurts the poor people the fastest and on and on.
2666480	2669480	And everyone who doesn't want that, do you use violence against them?
2669480	2677480	And so to prevent certain things ends up meaning control if currently human behavior is doing that.
2677480	2685480	And so how do you prevent increasing dystopias, which a lot of people think the Chinese state is in the direction of.
2685480	2693480	So a desirable future is a future that doesn't self-terminate and it doesn't have unchecked centralized power structures.
2693480	2698480	The question, one of the causes of the increase in nationalism is the distrust in globalism.
2698480	2705480	And one of the major reasons of the distrust of globalism is the idea that unchecked power doesn't have a good history.
2705480	2713480	And so the idea that we at least keep power checked in a multilateral way is preferable for a lot of people.
2713480	2720480	But if you have many different nations that are in economic competition with each other, nobody wants to price carbon properly.
2720480	2724480	Because if the US does, it'll be radically disadvantaged relative to China.
2724480	2731480	As a result, China's Belt and Road will geopolitically dominate the world and crowd out democracy and those types of things.
2731480	2738480	And so if you have nation states in competition with each other, they just can't deal with global issues well.
2738480	2744480	If you have global governance, who's creating a check on that power system to where it can't be captured or corrupted.
2744480	2751480	So I would say thinking in terms of the design criteria we need has to be able to do global governance.
2751480	2756480	It has to be able to deal with things like decentralized catastrophe weapons and basements.
2756480	2765480	But it also simultaneously has to have checks and balances on every power system or every control system that are there that are adjudicatable.
2765480	2770480	It would take me a long time to describe what I think the best processes for how to do that are.
2770480	2780480	I will say that there are technological systems that could be enacted that meet these criteria that align with human nature.
2780480	2786480	So I am optimistic about that. The enactment to get there takes a lot of work.
2786480	2789480	So just briefly mentioned the third attractor.
2789480	2794480	I'll move afterwards towards some hopeful scenarios that we might encounter.
2794480	2801480	So that people don't leave this room with a sense of fear and catastrophe as being dominant.
2801480	2809480	But in order for us to create a third attractor, we have to put some energy towards developing one.
2809480	2812480	And that's not a simple thing to do.
2812480	2817480	But first we have to understand what it is and how they could drive us away from those two attractors.
2817480	2825480	Decentralized catastrophic capability and centralized capacity to control which is kind of dystopia.
2830480	2837480	There is not a term. There is not like a term for a type of political economy or system that makes that third attractor.
2837480	2839480	And we actually don't know.
2839480	2847480	So specifically what I mean by the third attractor is literally something that can prevent all the catastrophic possibilities
2847480	2859480	where the control mechanisms required to do so have the types of checks and balances that they prevent centralized power issues.
2859480	2862480	So we are actually defining in terms of what it isn't.
2862480	2868480	It's not catastrophes and it's not that the solution of the catastrophes involves dystopias.
2868480	2875480	Exactly what that looks like, there's both how would we design it from scratch which is a nice question
2875480	2879480	but ends up being kind of a nonsense question because we don't get to design it from scratch.
2879480	2886480	There's this enactment issue of with all of the vested interests that the world currently has in play how do we actually get there.
2886480	2892480	So I can give you a few examples of things that give the sense of what can make a third attractor possible.
2892480	2896480	So I'm just going to mention one which I think you'll probably refer to.
2896480	2898480	We have a common friend in Will Marshall.
2898480	2900480	Were you going to mention Planet Labs?
2900480	2901480	I wasn't but we can.
2901480	2902480	Okay.
2902480	2909480	Well I mean so Planet Labs, I'll let you talk about it but effectively I think it's interesting to see also that some of those third attractors
2909480	2916480	reside in the domain of intelligence plus or human intelligence plus technology applied to a new level of
2916480	2920480	what you've actually yourself called force of transparency.
2920480	2928480	And that probably is not, it doesn't define exactly what the third attractor is but it's a sort of hopeful way of looking at technology
2928480	2935480	that can constitute an underlying, let's say, conversation between different actors whether they're state or civic actors
2935480	2940480	or technology bodies that can start to formulate more of those.
2940480	2948480	So I think we both agree that there needs to be a proliferation of these examples and force transparency is a really important tool
2948480	2951480	because we've got very weak international governance and law.
2951480	2952480	Yeah.
2952480	2956480	So maybe you can address the third attractor by giving some examples.
2956480	2963480	Yeah so market forces like incentive by itself doesn't solve all the problems.
2963480	2967480	So you end up having to use both incentive and deterrent.
2967480	2971480	If we didn't have law protecting national forests, we wouldn't have national forests.
2971480	2976480	You'd have market forces that continue to turn everything into commodities.
2976480	2985480	But as far as international law, it's tricky because law mostly exists where you have a monopoly of violence that can enact the law.
2985480	2994480	A police state inside of a nation state so where we have global issues like the oceans or the atmosphere, this is where we have a really tricky time
2994480	3005480	because how do you, if you're going to make a law, it requires multinational agreement and then the ability to see if it's being violated
3005480	3013480	and then the ability to enforce some enactment and the ability to enforce it where it's not more expensive to do so than the benefit you get, right?
3013480	3021480	So let's say that we have an agreement about oceans and China's violating it and so we say, okay, we're going to sanction you for that.
3021480	3026480	But the sanction is on supply chains that we depend upon and if you escalate, they also have nukes.
3026480	3030480	So it's like there's a tricky thing with all that.
3030480	3041480	But one of the places where a lot of environmental issues, global environmental issues don't get legal support is where we just don't even know what's happening.
3041480	3043480	It's hard to know what's happening in the middle of the Amazon.
3043480	3047480	It's hard to know what's happening in the middle of the open oceans.
3047480	3052480	So this particular example is where technology can be repurposed in a positive way.
3052480	3056480	There's a satellite imagery of the Earth is a pretty amazing technology.
3056480	3062480	There's a friend of ours who runs a company called Planet Labs and they image the entire surface of the Earth every day.
3062480	3070480	30 terabytes of compressed data but they're increasing the spectral and temporal resolution of that and spatial resolution
3070480	3076480	such that it'll be pretty much real time human level video capture of the surface of the Earth in about three years.
3076480	3082480	Which is amazing and one of the things it means is the ability to see where logging is happening
3082480	3087480	and where mining is happening and where dumping is happening and where legal fishing is happening
3087480	3093480	and even to be able to see in a dead zone in the ocean the effluent, how much of it came from which source,
3093480	3096480	how much came from which port and all those types of things.
3096480	3101480	The ability to see all that and use machine learning to process it means that there's a whole bunch of international law
3101480	3105480	that we've never even bothered to create because there'd be no way to know if it was being violated or enforced.
3105480	3108480	Now we'd have the ability to create international law that says,
3108480	3111480	no we actually don't have plausible deniability anymore.
3111480	3117480	We know exactly how much of the trash or the nitrogen effluent came from there because we can see the whole thing.
3117480	3124480	It also has the ability to do spectral analysis that can see an invasive species entering an area
3124480	3133480	or soil microbes in an area to be able to actually support the environment when critical issues are starting to happen.
3133480	3139480	But this is itself very concerning because probably many of you, even as I'm describing this, are like,
3139480	3144480	wow, that's really hopeful for the environment to be able to have that level of transparency that could create law
3144480	3145480	so we could support the environment.
3145480	3151480	But fuck, who gets to have access to video level data of the entire surface of the Earth all of the time?
3151480	3154480	That sounds like pretty massive surveillance capability, which it is.
3154480	3161480	So that can prevent certain catastrophes but can totally create dystopias depending upon how it's managed.
3161480	3168480	So how do we create the governance of that information such that it doesn't get used for nefarious purposes
3168480	3171480	and that people get to know what it's being used for?
3171480	3173480	This is not trivial, right?
3173480	3176480	Because it's easy to deploy the technology to solve those problems.
3176480	3181480	It can be quite hard to create the governance to ensure that it's used properly.
3181480	3188480	Well, the official version that Will gives me is that it's 50 centimeter resolution so you can't see a face.
3188480	3196480	It's easier to count trees and cars and tanks in Ukraine when the Russians pretend that they're leaving a certain battle area.
3196480	3201480	But let's agree that, you know, that power is something that needs to be at least checked.
3201480	3209480	So what I guess I was trying to get at is there is some hope in terms of how we can leverage technology
3209480	3215480	in order for us to sort of monitor and then have some checks and balances
3215480	3226480	and also create the international agreements and legal frameworks to enforce some form of limits, if you want to call it that.
3226480	3233480	So something I'd like to say is there is a naive techno-optimism that I think is super dangerous
3233480	3241480	that just tech will solve all the problems and the very worst version of it is we're only a few years from generalized AI
3241480	3243480	and then that'll be able to solve all the problems.
3243480	3250480	If you study the alignment issue of how do we ensure that truly generalized AI is aligned with our interests,
3250480	3253480	it's a really, really tricky problem.
3253480	3258480	There's also a naive anti-tech, a kind of naive Luddite perspective that's like,
3258480	3264480	man, all these problems are because of excessive tech, quality of life is actually better at a lower level of technology.
3264480	3267480	Let's get back to the land and permaculture and that kind of thing.
3267480	3275480	But if you study the history of the world, any time there is an intersection of a less technologically advanced society
3275480	3279480	with a more technologically advanced society, it doesn't go well for the less technologically advanced.
3280480	3289480	So the China-Tibet type interaction always happened and so whatever this group of people are saying,
3289480	3294480	we're going to do the less technologically advanced will not actually influence the direction of the way the future goes
3294480	3297480	because the technology is power which does mean influence.
3297480	3304480	So the future will be high tech but it has to be also high nature and high connectivity or it will self-terminate.
3304480	3309480	So you have to say, we don't get to put the Pandora's box of tech closed,
3309480	3312480	but we have to actually become wise stewards of it.
3312480	3316480	So what does a high tech, high nature, high connectivity future actually look like?
3316480	3322480	And if we don't have the technological capacity for outsized influence over the current systems,
3322480	3326480	the current systems will be what dominates.
3326480	3330480	And so I am a techno-optimist but not a naive techno-optimist,
3330480	3333480	meaning I know that all the existential risks we couldn't do without tech.
3333480	3335480	Stone Age people cannot destroy the planet.
3335480	3343480	So I'm acutely aware that all the catastrophic risks are results of human activity extended through technological levers.
3343480	3349480	But I'm also aware that the solutions to those things can't avoid technological elements,
3349480	3351480	but the technology alone is not sufficient.
3351480	3356480	So one way we think about this, there's an anthropologist named Marvin Harris,
3356480	3361480	and he said you can think about civilizations as these triples of what he called the infrastructure,
3361480	3364480	the social structure, and the superstructure.
3364480	3369480	The infrastructure is the tech stack that the civilization depends upon and meets all of its needs with.
3369480	3375480	The social structure is the collective agreement field, so economics, governance, law, and the institutions.
3375480	3381480	And the superstructure is basically the culture, the values, the identity, the definition of what the good life,
3381480	3383480	what we're motivated by are.
3383480	3391480	He particularly argued that they are interconnected, but the tech changes in tech drive the other ones.
3391480	3395480	And he gave a heap of examples from the plow to the wheel to on and on,
3395480	3401480	where a change in tech meant that whoever used that tech, now they're behaving differently, right?
3401480	3404480	Driving a plow is a different set of behaviors than hunting.
3405480	3412480	But no tech catches on if it doesn't provide adaptive advantage.
3412480	3416480	If it does provide adaptive advantage, it changes pattern of human behavior by using it.
3416480	3420480	By changing human behavior, you also change human values,
3420480	3426480	and then everybody else has to adopt it, or they lose in war to whoever has that increased adaptive advantage.
3426480	3433480	So he basically said cultures and political systems change because tech changes.
3433480	3438480	There are other deep anthropologists and sociologists who say,
3438480	3443480	no, actually, and give a heap of examples of how cultural changes make us innovate in different ways,
3443480	3448480	aligned with our values, or have us bind our technology, like the Sabbath, or things like that,
3448480	3451480	and say that cultural changes are the deepest.
3451480	3453480	And then there are plenty of others who say,
3453480	3455480	no, ultimately the economy and law is the deepest thing,
3455480	3461480	because ultimately whatever you incentivize financially is the technology that will be developed.
3461480	3466480	If you change the subsidies and the taxes and the incentives, the tech stack would evolve differently.
3466480	3470480	I would say that all three of these, the infrastructure, the social systems,
3470480	3474480	and the superstructure or culture, are equally fundamental and inner affecting,
3474480	3477480	and you have to think about changes to all three simultaneously.
3477480	3481480	So if you dismiss any of them out of hand, like,
3481480	3485480	ah, cultural changes don't matter that much, or technological changes, or government doesn't,
3485480	3488480	you're definitely not thinking comprehensively.
3488480	3492480	The favorite one, like, we can just do culture change and everything else will automatically happen.
3492480	3494480	That's also not thinking comprehensively.
3494480	3500480	They're all necessary and only thinking about them together and the way they inter-effect each other is sufficient.
3500480	3504480	So we could think about, if we want a future that avoids all these catastrophes,
3504480	3507480	what does the infrastructure have to look like?
3507480	3512480	Pretty quickly we can say we can't keep using nature unrenewably and turning it into pollution and waste unrenewably,
3512480	3515480	so it has to look closed loop and it has to look post-growth,
3515480	3517480	because you can't grow it exponentially on a finite planet.
3517480	3520480	So what types of technologies would mediate that?
3520480	3526480	And what things should be global and what things should be local has a lot to do with the social structure interaction of
3526480	3532480	there are things that you want to be global in so long as you're wanting to bind the well-being of those people to each other
3532480	3536480	through supply chains and interdependence and that kind of thing.
3536480	3541480	What would the social systems of a future look like and what would the culture?
3541480	3549480	There have been conversations today around in-group, out-group and identity systems that's culture questions, right?
3549480	3557480	I think there's a lot of probably focus on the well-being picture here at Harvest and its virtual picture of planetary identity.
3557480	3562480	And obviously the planetary identity has to be not just humans but all life forms,
3562480	3568480	because you can advance all humans at the expense of the biosphere for a little while, but then it goes pretty badly for the humans.
3569480	3577480	So when we think about Third Attractor, we have to think about what is the culture of it, what must it be?
3577480	3583480	What must the coordination systems and the distribution and allocation of resources,
3583480	3587480	and you start to get into things like, well, man, doesn't interest by itself,
3587480	3592480	even if we don't think about central bank policy or interest rates or fractional reserve banking or anything,
3592480	3598480	doesn't interest itself compounding interest force exponential growth of finance?
3598480	3600480	Yeah, it does.
3600480	3605480	And then to not debase the currency, doesn't that mean you have to have an exponential growth of goods and services?
3605480	3609480	Yeah, it does. Doesn't that mean you basically have an exponential materials economy on a finite planet?
3609480	3611480	Yeah, so interest has to go.
3611480	3615480	Well, that's really fundamental. We don't know how to make that world.
3616480	3626480	And then as long as most access to resources based on private property, doesn't that mean rival risk interest
3626480	3631480	where I can do better at the expense of the environment and others based on private property?
3631480	3634480	Probably a lot of stuff has to be rethought around property law.
3634480	3639480	And then even when you get to, I can appreciate the atmosphere.
3639480	3642480	In fact, my life depends upon it, but I don't have to pay for it.
3642480	3648480	And so if I cut a tree down, I get immediate benefit from the timber.
3648480	3653480	And the little tiny damage it causes to the atmosphere, I don't really notice.
3653480	3655480	But when everybody thinks that way, it does have that effect.
3655480	3659480	But locally, I have way more incentive to cut it down than to leave it up,
3659480	3666480	because the extraction value that I get from turning it into lumber gives me game theoretic value.
3666480	3673480	Whereas if I put my resources towards planting more trees that I don't have economic interest in,
3673480	3679480	I do less well in the economic system, this means that there is a fundamental rethinking of the value equation.
3679480	3686480	Because whoever ends up valuing extractable, exchangeable wealth ends up doing game theoretically much better than those who don't,
3686480	3688480	which means they influence the world and culture more.
3688480	3693480	Those who pay more attention to common wealth have less influence over the whole thing.
3693480	3703480	So the changes that we're talking about at the level of economics are things like interest, private property, fungible currency,
3703480	3706480	even deeper than whether we have nation states or not.
3706480	3711480	And the same in terms of thinking through what is the future of the tech stack look like.
3711480	3714480	So we share a lot of views, and thank you for laying it out.
3714480	3721480	I hope everyone could follow with these, you know, this way of presenting where we're at.
3721480	3731480	A lot of people are quite naturally fearful of where we're going in terms of the labor force because of AI, for example.
3731480	3738480	And so in one of your talks, I don't know if it was very recent, and we share this view about this by the way,
3738480	3742480	you talked about education or educators and nurses.
3742480	3750480	I think it's a good way to present a hopeful opportunity for us to do something where humans are uniquely designed to do something different than machines.
3750480	3755480	And where efficiency is not what matters. It's more about qualitative than the quantitative.
3755480	3759480	And our computational capability is not challenged by that.
3759480	3766480	So I thought maybe I'd switch gears a little bit, but it's giving a little bit of hope again in terms of how we can address concretely
3766480	3771480	some of these challenges that we're facing, whether we're techno-optimists or techno-scepticists.
3771480	3783480	So I worked with the G7 on a scheme to try and infuse into national security in the G8 countries a concept of benevolent AI.
3783480	3789480	And we were just studying with 80 scientists from all over the world how we could potentially put that into motion
3789480	3793480	and start to educate government bodies and leaders.
3793480	3798480	And the main failure point was purely geopolitical.
3798480	3803480	So the arms race and AI just made it so that it made every single suggestion stalemate.
3803480	3812480	So we have to address it on a population level and also in terms of how, in terms of society, we adapt to the change that's coming towards us.
3812480	3817480	So we've adapted to bringing cars into our life, arguably imperfectly.
3817480	3825480	We've adapted to many changes in our society in terms of healthcare and pandemics and how we travel and etc. etc.
3825480	3830480	Give us a little, you know, I share this view with you, by the way.
3830480	3841480	How you think educators and nurses could become a little bit of a new orientation for mankind as AI steps in.
3841480	3850480	So the topic of technological automation creating jobs issue, there's a couple perspectives.
3850480	3858480	One primary perspective is technological automation will obsolete certain jobs, but it will create new jobs.
3858480	3863480	It's always been the case we don't have elevator operators anymore, but there's plenty of new jobs.
3863480	3865480	And then there's the other perspective.
3865480	3871480	No, actually advanced robotics and AI are different in kind in some of the earlier industrial technologies.
3871480	3878480	And they're different in speed that as new jobs are created they will still be able to beat humans at doing it for market purposes,
3878480	3882480	which I think is much more true.
3882480	3892480	So if you take an AI like Google's AI, you take AlphaGo, you can train it on chess and how to beat everybody in chess.
3892480	3894480	You can train it on Go and how to beat everybody at Go.
3894480	3897480	You can train it on StarCraft and how to beat everybody at StarCraft.
3897480	3902480	It can gain the capacity very quickly to be humans at any finitely definable game.
3902480	3908480	And so AI is different in kind than other previous technologies.
3908480	3915480	It's more like the difference between us and all the other animals with our ability to innovate, you know, creating recursive technology on technology
3915480	3922480	that allowed us to become apex predators in every environment and then more than that, the AI is kind of like that jump again.
3922480	3935480	And so it does portend a break of capitalism and market structures as we know it and that most of, not just labor,
3935480	3943480	but most of the jobs that we currently have and even the new jobs that emerge in the niches that it creates, it's better at than we are.
3943480	3949480	That sounds pretty terrible if you keep the existing political economy where people need the jobs,
3949480	3955480	but one of the main reason we created a system where the people need the jobs is because the jobs needed the people to do them.
3955480	3964480	And if you, to make a civilization run well, if you had to get the people to do the jobs and a market seemed like a better answer than the state forcing the people to do it.
3964480	3968480	So let's let the market force people to do it and they can kind of self-organize.
3968480	3976480	But as soon as the jobs don't need the people to do them anymore, you can also start thinking about economies where the people don't need the jobs,
3976480	3982480	which is why now a lot of people are thinking about universal basic income and like different ways of thinking about that.
3982480	3986480	There's early naive thoughts on universal basic income.
3986480	3988480	Of course, it's the beginning of thinking about it.
3988480	3999480	But there's a really deep question which is, what is the role of humans in a world of advanced robotics and AI?
3999480	4004480	Because the advanced robotics and AI will be better than us at most of the things that we're used to being good at.
4004480	4006480	So what is the role of humans in that world?
4006480	4009480	What are we still uniquely good at?
4009480	4012480	And then what is also intrinsically fulfilling and meaningful?
4012480	4024480	And there becomes a steep question of what does education become in a post-technological automation world where you're not preparing people for the workforce in the same way?
4024480	4027480	What is the role of education and human development?
4027480	4033480	But obviously to answer that, you have to say not just education, but and obviously there's the economics component.
4033480	4036480	How do we do resource allocation and access in that world?
4036480	4040480	But there's a really deep civilizational question of what is the role of human life?
4040480	4050480	And if we for a moment avoid the topic of sentient AI, which is a whole theoretically difficult question, and we just talk about functional AI,
4050480	4055480	meaning AI that we're not saying there's something that it is like to be it.
4055480	4059480	We're simply saying it's very good at figuring out how to do stuff.
4059480	4066480	Then right away the key, what is uniquely different about humans and it is sentience.
4066480	4072480	Is the capacity to actually have there be something that it is like to be you at all?
4072480	4077480	Is subjective experience and then inner subjective, connective capacity?
4077480	4084480	And what's interesting is things related to sentience are what is where our intrinsic motive,
4084480	4092480	if we're not being behaviorally controlled by extrinsic motive, i.e. being paid to do a thing or external deterrence.
4092480	4097480	And so mostly you have to pay people to do shit they don't want to do, right?
4097480	4102480	And if the shit that people don't want to do is increasingly getting automated,
4102480	4112480	can you then have a world where largely what humans are spending time doing also more deeply coincides with what they have the deepest intrinsic motive to do,
4112480	4122480	particularly if you then have a developmental system and developmental society oriented to find out what the unique human motivations and capacities of each person are and develop them in light of that.
4122480	4129480	And so not only does AI portend something in terms of the obsolescence of humans for lots of labor roles and repetitive things,
4129480	4140480	one of the things Tomas was probably referencing was the topic of AI in human tutoring is pretty amazing.
4140480	4145480	It's also scary as fuck because again, you have to get this thing right.
4145480	4156480	If you have an AI that has so many orders of magnitude, more information processing in terms of being able to model your micro expressions to see how you're learning,
4156480	4165480	you can also have undue influence in a way that no cult leader has dreamed of, right, in terms of asymmetries of power of influence.
4165480	4168480	So who controls that and how do they control that?
4168480	4173480	These actually become the cultural questions, the theoretical philosophic questions that are so fundamental.
4173480	4182480	If you can genetically engineer humans and have designer babies, don't we all want to be like tall and beautiful and thin and is that actually the right idea?
4182480	4189480	As soon as you have the ability to actually design intelligent machines and design self-replicating machines and change biology,
4189480	4197480	the philosophic questions of what is good and desirable become so fundamental because so much becomes possible, right?
4197480	4207480	But if you do it right, imagine, everybody's heard about deepfakes, the ability to train an AI.
4207480	4213480	You can make basically pictures that aren't real, faces that aren't real, from AI images that look totally real.
4213480	4220480	You can also make a video of me speaking where it looks exactly like me, sounds like me, but it's totally AI generated, it's not real.
4220480	4223480	That technology already exists, it's not that good.
4223480	4234480	Some people have made deepfakes of me that are audio that sound like me, but the deepfake videos are currently about three years away from being Turing test passing,
4234480	4242480	you can watch a video of Obama or Bernie Sanders or whoever say something and have no idea if it was real or not,
4242480	4246480	and you won't have the human capacity to tell if it's real or fake.
4246480	4250480	You can imagine what that does to our ability for collective sense-making.
4250480	4258480	But that same deepfake capacity can be positively purposed because what that means is it's trained on the semantic patterns and the vocal patterns
4258480	4265480	enough to be able to generate novel answers like a chatbop, but where you can't tell that it's not real.
4265480	4271480	So now imagine an educational environment where you train that same AI, this is large language models as the type of AI,
4271480	4279480	say you train it on all the writings of Thomas Jefferson or all the writings of Socrates through Plato or whatever and writings about them,
4279480	4291480	such that I can go into a metaverse environment and say I want to pull up Einstein and von Neumann and Kurt Girdel and be able to have a talk with them about formal logic,
4291480	4301480	and not only can it seem like I'm actually having a conversation with them where they're sharing differing views based on their actual views, writing, etc.
4302480	4311480	But I could also just have an avatar that is like the voice of chemistry, that is the holistic knowledge of all chemistry that no human could be,
4311480	4322480	and yet all of them are the AI's best attempt to model what that person would do in terms of semantic coherency with what they did and said in the past.
4322480	4330480	So now imagine a future where every kid has access to be able to study with Einstein and Gandhi and Socrates and von Neumann and whatever directly,
4330480	4338480	where those AI's can model our theory of mind and titrate the learning directly to us associated with our learning dynamics,
4338480	4348480	but then also because the jobs have largely been automated, what humans spend way more of their time with is things like being educators and being nurses and being musicians
4348480	4356480	and the things that have kind of high connectivity value because those are the things that the machines don't do as well as the actual sense of shared interiority.
4356480	4365480	So now imagine that we have way more teachers per capita that are way more well trained, so all the teachers are PhD level trained,
4365480	4376480	there's one of them per 10 kids as opposed to one per 30, and now I get out of my AI environment where I was just studying physics with Einstein
4376480	4386480	and my tutor asks me a question like, what do you think was different about what the AI Einstein said than what an actual Einstein alive today might have said?
4386480	4393480	So then helping us to try to understand the difference between human intelligence and artificial intelligence and what it means to be sentient,
4393480	4399480	and what effect does consciousness have on intelligence, so not only are they getting that level of educational access,
4400480	4406480	but the AI can do, but the differential of what is unique about human intelligence and artificial intelligence.
4406480	4413480	So this is one of a million examples we can give of how those technologies could do mind-bendingly amazing things.
4413480	4422480	I'll say one quick thing about this is there is a study done on super genius of the past or polymaths,
4422480	4432480	who advanced many different fields beyond what the specialists in those fields did, and was there anything that the great polymaths had in common,
4432480	4437480	and the single thing that stood out the most was that they were all the result of aristocratic tutoring.
4437480	4444480	And this was actually a very taboo topic because when we ended feudalism and tried to create democratic states,
4445480	4454480	the idea, like if you think of meditations by Marcus Aurelius, Marcus Aurelius spends the entire first chapter just thanking all of his tutors,
4454480	4461480	but when you're being raised to be the emperor of Rome, the best mathematician, the best poet, the best grammarian, the best historian,
4461480	4469480	literally in all of Rome are your private tutors, and you can't learn to think like a mathematical genius from someone who wasn't a mathematical genius,
4469480	4474480	so your average math teacher cannot teach you to think like that because they don't think like that.
4474480	4478480	And yet, how do you make that accessible to everyone? You don't.
4478480	4485480	So the aristocratic tutoring in the past was a nice patronage job for the smartest people, but it was also so radically unequal,
4485480	4491480	but it's what created the best minds. So could that possibly be made popular?
4491480	4502480	Well, we can see in a third attractor kind of world the application of these technologies where you could actually have better tutoring than Marcus Aurelius had for everybody,
4502480	4509480	you know, et cetera, et cetera. Now, for each of these wonderful scenarios are like a million ways it goes wrong,
4509480	4517480	and so how we steward that properly is the key defining thing over the next while.
4517480	4519480	Thank you, Daniel.
4519480	4521480	Thank you.
