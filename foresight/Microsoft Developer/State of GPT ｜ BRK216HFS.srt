1
00:00:00,000 --> 00:00:17,440
Please welcome AI researcher and founding member of OpenAI, Andre Karpathy.

2
00:00:17,440 --> 00:00:23,640
Hi, everyone.

3
00:00:23,640 --> 00:00:28,880
I'm happy to be here to tell you about the state of GPT and more generally about the

4
00:00:28,880 --> 00:00:32,160
rapidly growing ecosystem of large language models.

5
00:00:32,160 --> 00:00:35,400
So I would like to partition the talk into two parts.

6
00:00:35,400 --> 00:00:39,600
In the first part, I would like to tell you about how we train GPT assistants.

7
00:00:39,600 --> 00:00:43,760
And then in the second part, we are going to take a look at how we can use these assistants

8
00:00:43,760 --> 00:00:46,760
effectively for your applications.

9
00:00:46,760 --> 00:00:50,520
So first, let's take a look at the emerging recipe for how to train these assistants and

10
00:00:50,520 --> 00:00:53,200
keep in mind that this is all very new and still rapidly evolving.

11
00:00:53,200 --> 00:00:55,400
But so far, the recipe looks something like this.

12
00:00:55,880 --> 00:01:00,000
Now, this is kind of a complicated slide, so I'm going to go through it piece by piece.

13
00:01:00,000 --> 00:01:03,400
But roughly speaking, we have four major stages.

14
00:01:03,400 --> 00:01:07,800
Pre-training, supervised fine-tuning, reward modeling, reinforcement learning, and they

15
00:01:07,800 --> 00:01:10,000
follow each other serially.

16
00:01:10,000 --> 00:01:14,840
Now, in each stage, we have a data set that powers that stage.

17
00:01:14,840 --> 00:01:21,480
We have an algorithm that, for our purposes, will be an objective for training a neural

18
00:01:21,480 --> 00:01:22,280
network.

19
00:01:22,280 --> 00:01:26,000
And then we have a resulting model, and then there's some nodes on the bottom.

20
00:01:26,000 --> 00:01:28,800
So the first stage we're going to start with is the pre-training stage.

21
00:01:28,800 --> 00:01:33,640
Now, this stage is kind of special in this diagram, and this diagram is not to scale,

22
00:01:33,640 --> 00:01:36,480
because this stage is where all of the computational work basically happens.

23
00:01:36,480 --> 00:01:42,040
This is 99% of the training compute time and also flops.

24
00:01:42,040 --> 00:01:46,680
And so this is where we are dealing with internet-scale data sets with thousands of

25
00:01:46,680 --> 00:01:51,440
GPUs in the supercomputer and also months of training, potentially.

26
00:01:51,440 --> 00:01:56,160
The other three stages are fine-tuning stages that are much more along the lines of small

27
00:01:56,160 --> 00:01:59,440
few number of GPUs and hours or days.

28
00:01:59,440 --> 00:02:04,240
So let's take a look at the pre-training stage to achieve a base model.

29
00:02:04,240 --> 00:02:07,880
First we're going to gather a large amount of data.

30
00:02:07,880 --> 00:02:12,840
Here's an example of what we call a data mixture that comes from this paper that was

31
00:02:12,840 --> 00:02:16,400
released by Meta, where they released this Lama-based model.

32
00:02:16,400 --> 00:02:20,600
Now, you can see roughly the kinds of data sets that enter into these collections.

33
00:02:20,600 --> 00:02:25,520
So we have common crawl, which is just a web scrape, C4, which is also a common crawl,

34
00:02:25,520 --> 00:02:27,400
and then some high-quality data sets as well.

35
00:02:27,400 --> 00:02:31,640
So for example, GitHub, Wikipedia, Books, Archive, Stack Exchange, and so on.

36
00:02:31,640 --> 00:02:36,720
These are all mixed up together, and then they are sampled according to some given proportions,

37
00:02:36,720 --> 00:02:41,160
and that forms the training set for the neural net for the GPT.

38
00:02:41,160 --> 00:02:45,080
Now before we can actually train on this data, we need to go through one more pre-processing

39
00:02:45,080 --> 00:02:46,960
step, and that is tokenization.

40
00:02:46,960 --> 00:02:51,560
And this is basically a translation of the raw text that we scrape from the internet into

41
00:02:51,560 --> 00:02:57,640
sequences of integers, because that's the native representation over which GPTs function.

42
00:02:57,640 --> 00:03:03,880
Now this is a lossless kind of translation between pieces of text and tokens and integers,

43
00:03:03,880 --> 00:03:05,880
and there are a number of algorithms for this stage.

44
00:03:05,880 --> 00:03:09,480
Typically, for example, you could use something like byte bearing coding, which iteratively

45
00:03:09,480 --> 00:03:14,000
merges little text chunks and groups them into tokens.

46
00:03:14,000 --> 00:03:18,480
And so here I'm showing some example chunks of these tokens, and then this is the raw integer

47
00:03:18,480 --> 00:03:22,160
sequence that will actually feed into a transformer.

48
00:03:22,160 --> 00:03:28,280
Now here I'm showing two sort of like examples for hyper-primers that govern this stage.

49
00:03:28,280 --> 00:03:32,400
So GPT-4, we did not release too much information about how it was trained and so on, so I'm

50
00:03:32,400 --> 00:03:37,040
using GPT-3's numbers, but GPT-3 is of course a little bit old by now, about three years

51
00:03:37,040 --> 00:03:38,040
ago.

52
00:03:38,040 --> 00:03:40,840
But Lama is a fairly recent model from Meta.

53
00:03:40,840 --> 00:03:43,840
So these are roughly the orders of magnitude that we're dealing with when we're doing

54
00:03:43,840 --> 00:03:45,280
pre-training.

55
00:03:45,280 --> 00:03:49,840
The vocabulary size is usually a couple 10,000 tokens, the context length is usually something

56
00:03:49,840 --> 00:03:56,360
like 2,000, 4,000, or nowadays even 100,000, and this governs the maximum number of integers

57
00:03:56,360 --> 00:04:02,320
that the GPT will look at when it's trying to predict the next integer in a sequence.

58
00:04:02,320 --> 00:04:06,480
You can see that roughly the number of parameters is, say, 65 billion for Lama.

59
00:04:06,480 --> 00:04:11,640
Now even though Lama has only 65B parameters compared to GPT-3's 175 billion parameters,

60
00:04:11,640 --> 00:04:16,320
Lama is a significantly more powerful model, and intuitively that's because the model is

61
00:04:16,320 --> 00:04:20,760
trained for significantly longer, in this case 1.4 trillion tokens instead of just 300

62
00:04:20,760 --> 00:04:21,840
billion tokens.

63
00:04:21,840 --> 00:04:25,120
So you shouldn't judge the power of a model just by the number of parameters that it

64
00:04:25,120 --> 00:04:27,120
contains.

65
00:04:27,120 --> 00:04:33,080
Below I'm showing some tables of rough hyper-parameters that typically go into specifying the transformer

66
00:04:33,080 --> 00:04:37,920
neural network, so the number of heads, the dimension size, number of layers, and so on.

67
00:04:37,920 --> 00:04:41,640
And on the bottom I'm showing some training hyper-parameters.

68
00:04:41,640 --> 00:04:50,040
So for example, to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training,

69
00:04:50,040 --> 00:04:54,240
and roughly several million dollars, and so that's the rough orders of magnitude that

70
00:04:54,240 --> 00:04:59,120
you should have in mind for the pre-training stage.

71
00:04:59,120 --> 00:05:02,320
Now when we're actually pre-training what happens, roughly speaking, we are going to

72
00:05:02,320 --> 00:05:06,160
take our tokens and we're going to lay them out into data batches.

73
00:05:06,160 --> 00:05:10,920
So we have these arrays that will feed into the transformer, and these arrays are B, the

74
00:05:10,920 --> 00:05:16,680
batch size, and these are all independent examples stacked up in rows, and B by T, T being the

75
00:05:16,680 --> 00:05:18,120
maximum context length.

76
00:05:18,120 --> 00:05:23,160
So in my picture I only have 10 of the context lengths, so this could be 2,000, 4,000, etc.

77
00:05:23,160 --> 00:05:25,120
So these are extremely long rows.

78
00:05:25,120 --> 00:05:29,120
And what we do is we take these documents and we pack them into rows, and we delimit them

79
00:05:29,120 --> 00:05:33,760
with these special end-of-text tokens, basically telling the transformer where a new document

80
00:05:33,760 --> 00:05:35,280
begins.

81
00:05:35,280 --> 00:05:40,760
And so here I have a few examples of documents, and then I've stretched them out into this

82
00:05:40,760 --> 00:05:41,760
input.

83
00:05:41,760 --> 00:05:48,280
Now, we're going to feed all of these numbers into Transformer, and let me just focus on

84
00:05:48,280 --> 00:05:52,720
a single particular cell, but the same thing will happen at every cell in this diagram.

85
00:05:52,720 --> 00:05:54,720
So let's look at the green cell.

86
00:05:54,720 --> 00:05:59,160
The green cell is going to take a look at all of the tokens before it, so all of the

87
00:05:59,160 --> 00:06:04,340
tokens in yellow, and we're going to feed that entire context into the transformer neural

88
00:06:04,340 --> 00:06:09,040
network, and the transformer is going to try to predict the next token in a sequence,

89
00:06:09,040 --> 00:06:10,640
in this case in red.

90
00:06:10,640 --> 00:06:13,840
Now the transformer, I don't have too much time to unfortunately go into the full details

91
00:06:13,840 --> 00:06:19,120
of this neural network architecture, is just a large blob of neural net stuff for our purposes,

92
00:06:19,120 --> 00:06:22,640
and it's got several 10 billion parameters typically, or something like that.

93
00:06:22,640 --> 00:06:25,840
And of course, as you tune these parameters, you're getting slightly different predicted

94
00:06:25,840 --> 00:06:29,280
distributions for every single one of these cells.

95
00:06:29,280 --> 00:06:35,040
And so, for example, if our vocabulary size is 50,257 tokens, then we're going to have

96
00:06:35,040 --> 00:06:39,600
that many numbers because we need to specify a probability distribution for what comes

97
00:06:39,600 --> 00:06:40,600
next.

98
00:06:40,600 --> 00:06:43,160
So basically we have a probability for whatever may follow.

99
00:06:43,160 --> 00:06:48,120
Now in this specific example for this specific cell, 513 will come next, and so we can use

100
00:06:48,120 --> 00:06:51,720
this as a source of supervision to update our transformer's weights.

101
00:06:51,760 --> 00:06:55,920
And so we're applying this basically on every single cell in parallel, and we keep swapping

102
00:06:55,920 --> 00:06:59,960
batches, and we're trying to get the transformer to make the correct predictions over what

103
00:06:59,960 --> 00:07:02,440
token comes next in a sequence.

104
00:07:02,440 --> 00:07:05,800
So let me show you more concretely what this looks like when you train one of these models.

105
00:07:05,800 --> 00:07:11,160
This is actually coming from New York Times, and they trained a small GPT on Shakespeare,

106
00:07:11,160 --> 00:07:14,720
and so here's a small snippet of Shakespeare, and they trained a GPT on it.

107
00:07:14,720 --> 00:07:19,360
Now in the beginning at initialization, the GPT starts with completely random weights,

108
00:07:19,400 --> 00:07:22,600
so you're just getting completely random outputs as well.

109
00:07:22,600 --> 00:07:27,520
But over time, as you train the GPT longer and longer, you are getting more and more

110
00:07:27,520 --> 00:07:31,720
coherent and consistent sort of samples from the model.

111
00:07:31,720 --> 00:07:35,960
And the way you sample from it, of course, is you predict what comes next, you sample

112
00:07:35,960 --> 00:07:40,600
from that distribution, and you keep feeding that back into the process, and you can basically

113
00:07:40,600 --> 00:07:42,240
sample large sequences.

114
00:07:42,240 --> 00:07:45,880
And so by the end, you see that the transformer has learned about words and where to put spaces

115
00:07:45,880 --> 00:07:48,400
and where to put commas and so on.

116
00:07:48,400 --> 00:07:51,800
And so we're making more and more consistent predictions over time.

117
00:07:51,800 --> 00:07:54,840
These are the kinds of plots that you're looking at when you're doing model pre-training.

118
00:07:54,840 --> 00:07:59,880
Effectively, we're looking at the loss function over time as you train, and low loss means

119
00:07:59,880 --> 00:08:04,360
that our transformer is predicting the correct, is giving a higher probability to get the

120
00:08:04,360 --> 00:08:07,160
correct next integer in a sequence.

121
00:08:07,160 --> 00:08:11,120
Now what are we going to do with this model once we've trained it after a month?

122
00:08:11,120 --> 00:08:16,760
Well the first thing that we noticed, we, the field, is that these models basically

123
00:08:16,760 --> 00:08:21,400
in the process of language modeling learn very powerful, general representations, and

124
00:08:21,400 --> 00:08:25,360
it's possible to very efficiently fine tune them for any arbitrary downstream task you

125
00:08:25,360 --> 00:08:26,640
might be interested in.

126
00:08:26,640 --> 00:08:30,920
So as an example, if you're interested in sentiment classification, the approach used

127
00:08:30,920 --> 00:08:34,280
to be that you collect a bunch of positives and negatives, and then you train some kind

128
00:08:34,280 --> 00:08:40,680
of an NLP model for that, but the new approach is ignore sentiment classification, go off

129
00:08:40,680 --> 00:08:45,480
and do large language model pre-training, train a large transformer, and then you can

130
00:08:45,480 --> 00:08:49,560
only, you may only have a few examples, and you can very efficiently fine tune your model

131
00:08:49,560 --> 00:08:51,360
for that task.

132
00:08:51,360 --> 00:08:53,760
And so this works very well in practice.

133
00:08:53,760 --> 00:08:57,800
And the reason for this is that basically the transformer is forced to multitask a huge

134
00:08:57,800 --> 00:09:02,560
amount of tasks in the language modeling task, because just, just in terms of predicting

135
00:09:02,560 --> 00:09:06,960
the next token, it's forced to understand a lot about the structure of the, of the text

136
00:09:06,960 --> 00:09:10,320
and all the different concepts therein.

137
00:09:10,320 --> 00:09:11,320
So that was GPT-1.

138
00:09:11,320 --> 00:09:15,600
Now around the time of GPT-2, people noticed that actually even better than fine tuning,

139
00:09:15,600 --> 00:09:17,880
you can actually prompt these models very effectively.

140
00:09:17,880 --> 00:09:20,560
So these are language models, and they want to complete documents.

141
00:09:20,560 --> 00:09:25,800
So you can actually trick them into performing tasks just by arranging these fake documents.

142
00:09:25,800 --> 00:09:29,920
So in this example, for, for example, we have some passage, and then we sort of like do

143
00:09:29,920 --> 00:09:34,920
QA, QA, QA, this is called a few-shot prompt, and then we do Q, and then as the transformer

144
00:09:34,920 --> 00:09:37,960
is trying to complete the document, it's actually answering our question.

145
00:09:37,960 --> 00:09:41,880
And so this is an example of prompt engineering a base model, making the belief that it's

146
00:09:41,880 --> 00:09:45,920
sort of imitating a document and getting it to perform a task.

147
00:09:45,920 --> 00:09:50,400
And so this kicked off, I think, the era of, I would say, prompting over fine tuning and

148
00:09:50,400 --> 00:09:54,200
seeing that this actually can work extremely well on a lot of problems, even without training

149
00:09:54,200 --> 00:09:57,600
any neural networks, fine tuning, or so on.

150
00:09:57,600 --> 00:10:03,280
Now since then, we've seen an entire evolutionary tree of base models that everyone has trained.

151
00:10:03,280 --> 00:10:05,520
Not all of these models are available.

152
00:10:05,520 --> 00:10:08,400
For example, the GPT-4 base model was never released.

153
00:10:08,400 --> 00:10:12,400
The GPT-4 model that you might be interacting with over API is not a base model, it's an

154
00:10:12,400 --> 00:10:16,360
assistant model, and we're going to cover how to get those in a bit.

155
00:10:16,360 --> 00:10:21,960
GPT-3 base model is available via the API under the name DaVinci, and GPT-2 base model is

156
00:10:21,960 --> 00:10:24,960
available even as weights on our GitHub repo.

157
00:10:24,960 --> 00:10:30,280
But currently the best available base model probably is the Lama series from Meta, although

158
00:10:30,280 --> 00:10:34,000
it is not commercially licensed.

159
00:10:34,000 --> 00:10:36,880
Now one thing to point out is base models are not assistants.

160
00:10:36,880 --> 00:10:41,400
They don't want to make answers to your questions.

161
00:10:41,400 --> 00:10:43,320
They just want to complete documents.

162
00:10:43,320 --> 00:10:48,440
So if you tell them write a poem about the bread and cheese, it will answer questions

163
00:10:48,440 --> 00:10:49,440
with more questions.

164
00:10:49,440 --> 00:10:51,520
It's just completing what it thinks is a document.

165
00:10:51,520 --> 00:10:57,400
However, you can prompt them in a specific way for base models that is more likely to

166
00:10:57,400 --> 00:10:58,400
work.

167
00:10:58,400 --> 00:11:01,280
So as an example, here's a poem about bread and cheese, and in that case it will autocomplete

168
00:11:01,280 --> 00:11:03,040
correctly.

169
00:11:03,040 --> 00:11:07,320
You can even trick base models into being assistants, and the way you would do this is

170
00:11:07,320 --> 00:11:11,160
you would create like a specific few-shot prompt that makes it look like there's some

171
00:11:11,160 --> 00:11:16,520
kind of a document between a human and assistant, and they're exchanging sort of information.

172
00:11:16,520 --> 00:11:21,120
And then at the bottom you sort of put your query at the end, and the base model will

173
00:11:21,120 --> 00:11:26,960
sort of like condition itself into being like a helpful assistant and kind of answer.

174
00:11:26,960 --> 00:11:29,880
But this is not very reliable and doesn't work super well in practice, although it can

175
00:11:29,880 --> 00:11:31,240
be done.

176
00:11:31,280 --> 00:11:35,520
So instead we have a different path to make actual GPT assistants not just base model

177
00:11:35,520 --> 00:11:37,360
document completers.

178
00:11:37,360 --> 00:11:39,920
And so that takes us into supervised fine-tuning.

179
00:11:39,920 --> 00:11:44,640
So in the supervised fine-tuning stage, we are going to collect small but high-quality

180
00:11:44,640 --> 00:11:45,640
data sets.

181
00:11:45,640 --> 00:11:51,400
And in this case we're going to ask human contractors to gather data of the form prompt

182
00:11:51,400 --> 00:11:55,320
and ideal response, and we're going to collect lots of these, typically tens of thousands

183
00:11:55,320 --> 00:11:57,000
or something like that.

184
00:11:57,000 --> 00:12:00,360
And then we're going to still do language modeling on this data, so nothing changed

185
00:12:00,360 --> 00:12:01,360
algorithmically.

186
00:12:01,360 --> 00:12:03,000
We're just swapping out a training set.

187
00:12:03,000 --> 00:12:08,600
So it used to be internet documents, which is a high-quantity, low-quantity for basically

188
00:12:08,600 --> 00:12:14,360
QA prompt response kind of data, and that is low-quantity, high-quality.

189
00:12:14,360 --> 00:12:18,600
So we still do language modeling, and then after training we get an SFD model.

190
00:12:18,600 --> 00:12:21,880
And you can actually deploy these models, and they are actual assistants, and they work

191
00:12:21,880 --> 00:12:23,200
to some extent.

192
00:12:23,200 --> 00:12:25,960
Let me show you what an example demonstration might look like.

193
00:12:25,960 --> 00:12:28,480
So here's something that a human contractor might come up with.

194
00:12:28,480 --> 00:12:29,840
Here's some random prompt.

195
00:12:29,840 --> 00:12:33,720
When you write a short introduction about the relevance of the term monopsony or something

196
00:12:33,720 --> 00:12:37,680
like that, and then the contractor also writes out an ideal response.

197
00:12:37,680 --> 00:12:41,680
And when they write out these responses, they are following extensive labeling documentations,

198
00:12:41,680 --> 00:12:45,800
and they are being asked to be helpful, truthful, and harmless.

199
00:12:45,800 --> 00:12:48,600
And this is labeling instructions here.

200
00:12:48,600 --> 00:12:52,720
You probably can't read it, neither can I, but they're long, and this is just people

201
00:12:52,720 --> 00:12:56,360
following instructions and trying to complete these prompts.

202
00:12:56,360 --> 00:12:58,920
So that's what the data set looks like, and you can train these models, and this works

203
00:12:58,920 --> 00:13:00,440
to some extent.

204
00:13:00,440 --> 00:13:05,680
Now you can actually continue the pipeline from here on and go into RLHF, Reinforcement

205
00:13:05,680 --> 00:13:09,840
Learning from Human Feedback, that consists of both reward modeling and reinforcement

206
00:13:09,840 --> 00:13:10,840
learning.

207
00:13:10,840 --> 00:13:13,680
So let me cover that, and then I'll come back to why you may want to go through the extra

208
00:13:13,680 --> 00:13:16,760
steps and how that compares to just SFD models.

209
00:13:16,760 --> 00:13:20,240
So in the reward modeling step, what we're going to do is we're now going to shift our

210
00:13:20,240 --> 00:13:23,280
data collection to be of the form of comparisons.

211
00:13:23,280 --> 00:13:25,800
So here's an example of what our data set will look like.

212
00:13:25,800 --> 00:13:30,760
I have the same prompt, identical prompt on the top, which is asking the assistant to

213
00:13:30,760 --> 00:13:35,720
write a program or a function that checks if a given string is a palindrome.

214
00:13:35,720 --> 00:13:40,080
And then what we do is we take the SFD model, which we've already trained, and we create

215
00:13:40,080 --> 00:13:41,240
multiple completions.

216
00:13:41,240 --> 00:13:44,360
So in this case, we have three completions that the model has created.

217
00:13:44,360 --> 00:13:47,160
And then we ask people to rank these completions.

218
00:13:47,160 --> 00:13:50,680
So if you stare at this for a while, and by the way, these are very difficult things

219
00:13:50,680 --> 00:13:54,840
to do to compare some of these predictions, and this can take people even hours for a

220
00:13:54,840 --> 00:13:58,520
single prompt completion pairs.

221
00:13:58,520 --> 00:14:02,040
But let's say we decided that one of these is much better than the others, and so on.

222
00:14:02,040 --> 00:14:03,640
So we rank them.

223
00:14:03,640 --> 00:14:07,360
Then we can follow that with something that looks very much like a binary classification

224
00:14:07,360 --> 00:14:10,520
on all the possible pairs between these completions.

225
00:14:10,520 --> 00:14:15,000
So what we do now is we lay out our prompt in rows, and the prompts is identical across

226
00:14:15,000 --> 00:14:16,360
all three rows here.

227
00:14:16,360 --> 00:14:20,640
So it's all the same prompt, but the completion is very, and so the yellow tokens are coming

228
00:14:20,640 --> 00:14:22,320
from the SFD model.

229
00:14:22,320 --> 00:14:28,320
Then what we do is we append another special reward readout token at the end, and we basically

230
00:14:28,320 --> 00:14:32,040
only supervise the transformer at this single green token.

231
00:14:32,040 --> 00:14:38,560
And the transformer will predict some reward for how good that completion is for that prompt.

232
00:14:38,560 --> 00:14:42,960
And so basically it makes a guess about the quality of each completion, and then once

233
00:14:42,960 --> 00:14:46,560
it makes a guess for every one of them, we also have the ground truth, which is telling

234
00:14:46,560 --> 00:14:48,160
us the ranking of them.

235
00:14:48,160 --> 00:14:51,800
And so we can actually enforce that some of these numbers should be much higher than others,

236
00:14:51,800 --> 00:14:52,800
and so on.

237
00:14:52,800 --> 00:14:56,440
We formulate this into a loss function, and we train our model to make reward predictions

238
00:14:56,440 --> 00:14:59,840
that are consistent with the ground truth coming from the comparisons from all these

239
00:14:59,840 --> 00:15:00,840
contractors.

240
00:15:00,840 --> 00:15:03,040
So that's how we train our reward model.

241
00:15:03,040 --> 00:15:07,680
And that allows us to score how good a completion is for a prompt.

242
00:15:07,680 --> 00:15:12,400
Once we have a reward model, we can't deploy this because this is not very useful as an

243
00:15:12,400 --> 00:15:16,440
assistant by itself, but it's very useful for the reinforcement learning stage that follows

244
00:15:16,440 --> 00:15:17,440
now.

245
00:15:17,440 --> 00:15:21,640
Because we have a reward model, we can score the quality of any arbitrary completion for

246
00:15:21,640 --> 00:15:23,200
any given prompt.

247
00:15:23,200 --> 00:15:26,720
So what we do during reinforcement learning is we basically get, again, a large collection

248
00:15:26,720 --> 00:15:30,960
of prompts, and now we do reinforcement learning with respect to the reward model.

249
00:15:30,960 --> 00:15:33,240
So here's what that looks like.

250
00:15:33,240 --> 00:15:37,880
We take a single prompt, we lay it out and rose, and now we use the SFD model.

251
00:15:37,880 --> 00:15:41,960
We use basically the model we'd like to train, which is initialized at SFD model, to create

252
00:15:41,960 --> 00:15:43,920
some completions in yellow.

253
00:15:43,920 --> 00:15:48,400
And then we append the reward token again, and we read off the reward according to the

254
00:15:48,400 --> 00:15:50,600
reward model, which is now kept fixed.

255
00:15:50,600 --> 00:15:52,400
It doesn't change anymore.

256
00:15:52,400 --> 00:15:57,400
And now the reward model tells us the quality of every single completion for these prompts.

257
00:15:57,400 --> 00:16:00,800
And so what we can do is we can now just basically apply the same language modeling

258
00:16:00,800 --> 00:16:07,040
loss function, but we're currently training on the yellow tokens, and we are weighing

259
00:16:07,040 --> 00:16:11,440
the language modeling objective by the rewards indicated by the reward model.

260
00:16:11,440 --> 00:16:16,440
So as an example, in the first row, the reward model said that this is a fairly high-scoring

261
00:16:16,440 --> 00:16:17,440
completion.

262
00:16:17,440 --> 00:16:21,400
And so all of the tokens that we happened to sample on the first row are going to get

263
00:16:21,400 --> 00:16:25,080
reinforced, and they're going to get higher probabilities for the future.

264
00:16:25,080 --> 00:16:29,000
Conversely, on the second row, the reward model really did not like this completion,

265
00:16:29,000 --> 00:16:30,080
negative 1.2.

266
00:16:30,080 --> 00:16:34,040
And so therefore, every single token that we sampled in that second row is going to get

267
00:16:34,040 --> 00:16:36,440
a slightly higher probability for the future.

268
00:16:36,440 --> 00:16:41,200
And we do this over and over on many prompts, on many batches, and basically we get a policy

269
00:16:41,200 --> 00:16:47,280
which creates yellow tokens here, and basically all of the completions here will score high

270
00:16:47,280 --> 00:16:51,640
according to the reward model that we trained in the previous stage.

271
00:16:51,640 --> 00:16:55,240
So that's how we train, that's what the RLHF pipeline is.

272
00:16:55,240 --> 00:17:00,160
Now, and then at the end, you get a model that you could deploy, and so as an example,

273
00:17:00,160 --> 00:17:04,760
chatGPT is an RLHF model, but some other models that you might come across, like for

274
00:17:04,760 --> 00:17:08,380
example, the Kuna 13B and so on, these are SFD models.

275
00:17:08,380 --> 00:17:13,560
So we have base models, SFD models, and RLHF models, and that's kind of like the state

276
00:17:13,560 --> 00:17:15,040
of things there.

277
00:17:15,040 --> 00:17:17,180
Now why would you want to do RLHF?

278
00:17:17,180 --> 00:17:21,100
So one answer that is kind of not that exciting is that it just works better.

279
00:17:21,100 --> 00:17:23,300
So this comes from the InstructGPT paper.

280
00:17:23,300 --> 00:17:28,460
According to these experiments a while ago now, these PPO models are RLHF, and we see

281
00:17:28,460 --> 00:17:33,860
that they are basically just preferred in a lot of comparisons when we give them to humans.

282
00:17:33,860 --> 00:17:39,700
So humans just prefer basically tokens that come from RLHF models compared to SFD models,

283
00:17:39,700 --> 00:17:44,100
compared to base model that is prompted to be an assistant, and so it just works better.

284
00:17:44,380 --> 00:17:46,260
You might ask why?

285
00:17:46,260 --> 00:17:47,420
Why does it work better?

286
00:17:47,420 --> 00:17:51,220
And I don't think that there's a single amazing answer that the community has really agreed

287
00:17:51,220 --> 00:17:57,660
on, but I will just offer one reason potentially, and it has to do with the asymmetry between

288
00:17:57,660 --> 00:18:02,460
how easy computationally it is to compare versus generate.

289
00:18:02,460 --> 00:18:05,260
So let's take an example of generating a haiku.

290
00:18:05,260 --> 00:18:08,260
Suppose I ask a model to write a haiku about paper clips.

291
00:18:08,260 --> 00:18:12,540
If you're a contractor trying to give train data, then imagine being a contractor collecting

292
00:18:12,540 --> 00:18:16,300
basically data for the SFD stage, how are you supposed to create a nice haiku for a

293
00:18:16,300 --> 00:18:17,300
paper clip?

294
00:18:17,300 --> 00:18:20,860
You might just not be very good at that, but if I give you a few examples of haikus,

295
00:18:20,860 --> 00:18:24,380
you might be able to appreciate some of these haikus a lot more than others.

296
00:18:24,380 --> 00:18:27,660
And so judging which one of these is good is a much easier task.

297
00:18:27,660 --> 00:18:33,420
And so basically this asymmetry makes it so that comparisons are a better way to potentially

298
00:18:33,420 --> 00:18:38,220
leverage yourself as a human and your judgment to create a slightly better model.

299
00:18:38,220 --> 00:18:43,860
Now RLHF models are not strictly an improvement on the base models in some cases.

300
00:18:43,860 --> 00:18:47,100
So in particular, we've noticed, for example, that they lose some entropy.

301
00:18:47,100 --> 00:18:49,900
So that means that they give more PT results.

302
00:18:49,900 --> 00:18:56,220
They can output samples with lower variation than the base model.

303
00:18:56,220 --> 00:19:00,340
So base model has lots of entropy and will give lots of diverse outputs.

304
00:19:00,340 --> 00:19:06,140
So for example, one kind of place where I still prefer to use a base model is in a setup

305
00:19:06,140 --> 00:19:13,900
where you basically have n things and you want to generate more things like it.

306
00:19:13,900 --> 00:19:16,460
And so here is an example that I just cooked up.

307
00:19:16,460 --> 00:19:18,900
I want to generate cool Pokemon names.

308
00:19:18,900 --> 00:19:22,900
I gave it seven Pokemon names and I asked the base model to complete the document and

309
00:19:22,900 --> 00:19:24,980
it gave me a lot more Pokemon names.

310
00:19:24,980 --> 00:19:25,980
These are fictitious.

311
00:19:25,980 --> 00:19:27,060
I tried to look them up.

312
00:19:27,060 --> 00:19:29,260
I don't believe they're actual Pokemons.

313
00:19:29,260 --> 00:19:32,340
And this is the kind of task that I think base model would be good at because it still

314
00:19:32,340 --> 00:19:36,660
has lots of entropy and will give you lots of diverse, cool, kind of more things that

315
00:19:36,660 --> 00:19:40,680
look like whatever you give it before.

316
00:19:40,680 --> 00:19:44,220
So this is what, this is the number, having said all that, these are kind of like the

317
00:19:44,220 --> 00:19:47,740
assistant models that are probably available to you at this point.

318
00:19:47,740 --> 00:19:51,780
There's a team at Berkeley that ranked a lot of the available assistant models and

319
00:19:51,780 --> 00:19:53,540
gave them basically ELO ratings.

320
00:19:53,540 --> 00:19:57,740
So currently some of the best models, of course, are GPT-4 by far, I would say, followed by

321
00:19:57,740 --> 00:20:02,420
Claude, GPT-3.5, and then a number of models, some of these might be available as weights

322
00:20:02,420 --> 00:20:05,100
like the Kuna, Koala, et cetera.

323
00:20:05,100 --> 00:20:10,660
And the first three rows here are all, they are all RLHF models and all of the other models

324
00:20:10,660 --> 00:20:15,100
to my knowledge are SFD models, I believe.

325
00:20:15,100 --> 00:20:19,540
Okay, so that's how we train these models on a high level.

326
00:20:19,540 --> 00:20:25,020
Now I'm going to switch gears and let's look at how we can best apply a GPT-assistant model

327
00:20:25,020 --> 00:20:26,460
to your problems.

328
00:20:26,460 --> 00:20:30,100
Now I would like to work in a setting of a concrete example.

329
00:20:30,100 --> 00:20:33,340
So let's, let's work with a concrete example here.

330
00:20:33,340 --> 00:20:36,900
Let's say that you are working on an article or a blog post and you're going to write this

331
00:20:36,900 --> 00:20:41,300
sentence at the end, California's population is 53 times that of Alaska.

332
00:20:41,300 --> 00:20:45,460
So for some reason you want to compare the populations of these two states.

333
00:20:45,460 --> 00:20:50,500
Think about the rich internal monologue and tool use and how much work actually goes computationally

334
00:20:50,500 --> 00:20:53,500
in your brain to generate this one final sentence.

335
00:20:53,500 --> 00:20:55,580
So here's maybe what that could look like in your brain.

336
00:20:55,780 --> 00:21:00,940
Okay, for this next step, let me blog, or my blog, let me compare these two populations.

337
00:21:00,940 --> 00:21:05,380
Okay, first I'm going to obviously need to get both of these populations.

338
00:21:05,380 --> 00:21:09,220
Now I know that I probably don't know these populations off the top of my head.

339
00:21:09,220 --> 00:21:13,420
So I'm kind of like aware of what I know, what I don't know of my self-knowledge, right?

340
00:21:13,420 --> 00:21:18,420
So I go, I do some tool use and I go to Wikipedia and I look up California's population and

341
00:21:18,420 --> 00:21:20,340
Alaska's population.

342
00:21:20,340 --> 00:21:22,420
Now I know that I should divide the two.

343
00:21:22,420 --> 00:21:27,140
But again, I know that dividing 39.2 by 0.74 is very unlikely to succeed.

344
00:21:27,140 --> 00:21:29,820
That's not the kind of thing that I can do in my head.

345
00:21:29,820 --> 00:21:32,580
And so therefore I'm going to rely on a calculator.

346
00:21:32,580 --> 00:21:37,420
So I'm going to use a calculator, punch it in, and see that the output is roughly 53.

347
00:21:37,420 --> 00:21:40,900
And then maybe I do some reflection and sanity checks in my brain.

348
00:21:40,900 --> 00:21:42,540
So does 53 make sense?

349
00:21:42,540 --> 00:21:46,460
Well, that's quite a large fraction, but then California has the most popular state, so

350
00:21:46,460 --> 00:21:48,260
maybe that looks okay.

351
00:21:48,260 --> 00:21:51,540
So then I have all the information I might need, and now I get to the sort of creative

352
00:21:51,540 --> 00:21:52,980
portion of writing.

353
00:21:52,980 --> 00:21:57,460
So I might start to write something like, California has 53x times greater, and then

354
00:21:57,460 --> 00:22:00,420
I think to myself, that's actually like really awkward phrasing.

355
00:22:00,420 --> 00:22:03,660
So let me actually delete that, and let me try again.

356
00:22:03,660 --> 00:22:08,380
And so as I'm writing, I have this separate process almost inspecting what I'm writing

357
00:22:08,380 --> 00:22:11,220
and judging whether it looks good or not.

358
00:22:11,220 --> 00:22:14,940
And then maybe I delete, and maybe I reframe it, and then maybe I'm happy with what comes

359
00:22:14,940 --> 00:22:15,940
out.

360
00:22:15,940 --> 00:22:20,220
So basically long story short, a ton happens under the hood in terms of your internal monologue

361
00:22:20,220 --> 00:22:22,220
when you create sentences like this.

362
00:22:22,220 --> 00:22:27,580
But what does a sentence like this look like when we are training a GPT on it?

363
00:22:27,580 --> 00:22:30,820
From GPT's perspective, this is just a sequence of tokens.

364
00:22:30,820 --> 00:22:35,660
So a GPT when it's reading or generating these tokens, it just goes chunk, chunk, chunk,

365
00:22:35,660 --> 00:22:36,660
chunk, chunk.

366
00:22:36,660 --> 00:22:40,700
And each chunk is roughly the same amount of computational work for each token.

367
00:22:40,700 --> 00:22:43,420
And these transformers are not very shallow networks.

368
00:22:43,420 --> 00:22:47,780
They have about 80 layers of reasoning, but 80 is still not like too much.

369
00:22:47,780 --> 00:22:51,660
And so this transformer is going to do its best to imitate.

370
00:22:51,660 --> 00:22:56,780
But of course, the process here looks very, very different from the process that you took.

371
00:22:56,780 --> 00:23:00,900
So in particular, in our final artifacts, in the data sets that we create and then eventually

372
00:23:00,900 --> 00:23:06,300
feed to LLMs, all of that internal dialogue is completely stripped.

373
00:23:06,300 --> 00:23:10,460
And unlike you, the GPT will look at every single token and spend the same amount of

374
00:23:10,460 --> 00:23:12,340
compute on every one of them.

375
00:23:12,340 --> 00:23:17,540
And so you can't expect it to actually like, well, you can't expect it to sort of do too

376
00:23:17,540 --> 00:23:19,540
much work per token.

377
00:23:19,540 --> 00:23:23,980
So and also in particular, basically these transformers are just like token simulators.

378
00:23:23,980 --> 00:23:28,180
So they don't know what they don't know, like they just imitate the next token.

379
00:23:28,180 --> 00:23:29,860
They don't know what they're good at or not good at.

380
00:23:29,860 --> 00:23:32,500
They just try their best to imitate the next token.

381
00:23:32,500 --> 00:23:34,260
They don't reflect in the loop.

382
00:23:34,260 --> 00:23:35,620
They don't sanity check anything.

383
00:23:35,620 --> 00:23:38,100
They don't correct their mistakes along the way by default.

384
00:23:38,100 --> 00:23:41,060
They just sample token sequences.

385
00:23:41,060 --> 00:23:43,900
They don't have separate inner monologue streams in their head, right?

386
00:23:43,900 --> 00:23:45,740
They're evaluating what's happening.

387
00:23:45,740 --> 00:23:49,580
Now they do have some sort of cognitive advantages, I would say.

388
00:23:49,580 --> 00:23:53,740
And that is that they do actually have very large fact-based knowledge across a vast number

389
00:23:53,740 --> 00:23:57,140
of areas because they have, say, several 10 billion parameters.

390
00:23:57,140 --> 00:24:00,260
So that's a lot of storage for a lot of facts.

391
00:24:00,260 --> 00:24:04,860
But and they also, I think, have a relatively large and perfect working memory.

392
00:24:04,860 --> 00:24:10,260
So whatever fits into the context window is immediately available to the transformer

393
00:24:10,260 --> 00:24:12,700
through its internal self-attention mechanism.

394
00:24:12,700 --> 00:24:16,660
And so it's kind of like perfect memory, but it's got a finite size.

395
00:24:16,660 --> 00:24:18,780
But the transformer has a very direct access to it.

396
00:24:18,780 --> 00:24:23,420
And so it can, like, losslessly remember anything that is inside its context window.

397
00:24:23,420 --> 00:24:26,100
So that's kind of how I would compare those two.

398
00:24:26,100 --> 00:24:30,580
And the reason I bring all of this up is because I think to a large extent, prompting is just

399
00:24:30,580 --> 00:24:37,460
making up for this sort of cognitive difference between these two kind of architectures, like

400
00:24:37,460 --> 00:24:39,700
our brains here and LLM brains.

401
00:24:39,700 --> 00:24:42,260
You can look at it that way almost.

402
00:24:42,260 --> 00:24:45,940
So here's one thing that people found, for example, works pretty well in practice.

403
00:24:45,940 --> 00:24:50,580
Especially if your tasks require reasoning, you can't expect the transformer to do too

404
00:24:50,580 --> 00:24:52,540
much reasoning per token.

405
00:24:52,540 --> 00:24:56,220
And so you have to really spread out the reasoning across more and more tokens.

406
00:24:56,220 --> 00:24:59,500
So for example, you can't give the transformer a very complicated question and expect it

407
00:24:59,500 --> 00:25:00,900
to get the answer in a single token.

408
00:25:00,900 --> 00:25:03,020
There's just not enough time for it.

409
00:25:03,020 --> 00:25:07,140
These transformers need tokens to think, quote, unquote, I like to say sometimes.

410
00:25:07,140 --> 00:25:09,140
And so this is some of the things that work well.

411
00:25:09,140 --> 00:25:12,420
You may, for example, have a few-shot prompt that shows the transformer that it should

412
00:25:12,420 --> 00:25:16,100
like show its work when it's answering the question.

413
00:25:16,100 --> 00:25:20,780
And if you give a few examples, the transformer will imitate that template, and it will just

414
00:25:20,780 --> 00:25:24,380
end up working out better in terms of its evaluation.

415
00:25:24,380 --> 00:25:28,260
Additionally, you can elicit this kind of behavior from the transformer by saying, let's

416
00:25:28,260 --> 00:25:32,900
think step by step, because this conditioned the transformer into sort of like showing

417
00:25:32,900 --> 00:25:33,900
its work.

418
00:25:33,900 --> 00:25:38,020
And because it kind of snaps into a mode of showing its work, it's going to do less

419
00:25:38,020 --> 00:25:40,260
computational work per token.

420
00:25:40,260 --> 00:25:45,100
And so it's more likely to succeed as a result, because it's making slower reasoning over

421
00:25:45,100 --> 00:25:46,740
time.

422
00:25:46,740 --> 00:25:47,740
Here's another example.

423
00:25:47,740 --> 00:25:50,060
This one is called self-consistency.

424
00:25:50,060 --> 00:25:52,940
We saw that we had the ability to start writing.

425
00:25:52,940 --> 00:25:55,180
And then if it didn't work out, I can try again.

426
00:25:55,180 --> 00:26:00,740
And I can try multiple times and maybe select the one that worked best.

427
00:26:00,740 --> 00:26:04,700
So in these kinds of approaches, you may sample not just once, but you may sample multiple

428
00:26:04,700 --> 00:26:09,380
times, and then have some process for finding the ones that are good and then keeping just

429
00:26:09,380 --> 00:26:12,180
those samples or doing a majority vote or something like that.

430
00:26:12,180 --> 00:26:16,380
So basically, these transformers in the process, as they predict the next token, just like

431
00:26:16,380 --> 00:26:18,140
you, they can get unlucky.

432
00:26:18,140 --> 00:26:22,140
And they could sample a not a very good token, and they can go down sort of like a blind

433
00:26:22,140 --> 00:26:24,100
alley in terms of reasoning.

434
00:26:24,100 --> 00:26:27,340
And so unlike you, they cannot recover from that.

435
00:26:27,340 --> 00:26:31,020
They are stuck with every single token they sample, and so they will continue the sequence

436
00:26:31,020 --> 00:26:34,340
even if they even know that this sequence is not going to work out.

437
00:26:34,380 --> 00:26:41,500
So give them the ability to look back, inspect, or try to basically sample around it.

438
00:26:41,500 --> 00:26:43,860
Here's one technique also.

439
00:26:43,860 --> 00:26:47,620
It turns out that actually LLMs, like they know when they've screwed up.

440
00:26:47,620 --> 00:26:53,700
So as an example, say you ask the model to generate a poem that does not rhyme.

441
00:26:53,700 --> 00:26:56,220
And it might give you a poem, but it actually rhymes.

442
00:26:56,220 --> 00:26:59,940
But it turns out that especially for the bigger models like GPT-4, you can just ask it, did

443
00:26:59,940 --> 00:27:01,300
you meet the assignment?

444
00:27:01,340 --> 00:27:05,060
And actually GPT-4 knows very well that it did not meet the assignment.

445
00:27:05,060 --> 00:27:07,460
It just kind of got unlucky in its sampling.

446
00:27:07,460 --> 00:27:09,620
And so it will tell you, no, I didn't actually meet the assignment.

447
00:27:09,620 --> 00:27:11,260
Here's, let me try again.

448
00:27:11,260 --> 00:27:18,180
But without you prompting it, it doesn't even, like it doesn't know to revisit and so on.

449
00:27:18,180 --> 00:27:20,220
So you have to make up for that in your prompts.

450
00:27:20,220 --> 00:27:22,180
You have to get it to check.

451
00:27:22,180 --> 00:27:25,740
If you don't ask it to check, it's not going to check by itself, it's just a token simulator.

452
00:27:26,740 --> 00:27:33,500
I think more generally, a lot of these techniques fall into the bucket of what I would say recreating

453
00:27:33,500 --> 00:27:34,820
our system two.

454
00:27:34,820 --> 00:27:38,260
So you might be familiar with the system one, system two thinking for humans.

455
00:27:38,260 --> 00:27:40,300
System one is a fast automatic process.

456
00:27:40,300 --> 00:27:44,020
And I think kind of corresponds to like an LLM just sampling tokens.

457
00:27:44,020 --> 00:27:49,500
And system two is the slower, deliberate planning sort of part of your brain.

458
00:27:49,500 --> 00:27:53,380
And so this is a paper actually from just last week, because this space is pretty quickly

459
00:27:53,380 --> 00:27:54,380
evolving.

460
00:27:55,380 --> 00:28:00,780
And in tree of thought, the authors of this paper proposed maintaining multiple completions

461
00:28:00,780 --> 00:28:02,620
for any given prompt.

462
00:28:02,620 --> 00:28:06,500
And then they are also scoring them along the way and keeping the ones that are going

463
00:28:06,500 --> 00:28:08,340
well, if that makes sense.

464
00:28:08,340 --> 00:28:14,940
And so a lot of people are like really playing around with kind of prompt engineering to

465
00:28:14,940 --> 00:28:19,980
basically bring back some of these abilities that we sort of have in our brain for LLMs.

466
00:28:19,980 --> 00:28:23,020
Now one thing I would like to note here is that this is not just a prompt.

467
00:28:23,020 --> 00:28:28,060
This is actually prompts that are together used with some Python glue code because you

468
00:28:28,060 --> 00:28:31,180
don't, you actually have to maintain multiple prompts and you also have to do some tree

469
00:28:31,180 --> 00:28:35,620
search algorithm here to like figure out which prompts to expand, et cetera.

470
00:28:35,620 --> 00:28:40,260
So it's a symbiosis of Python glue code and individual prompts that are called in a Y

471
00:28:40,260 --> 00:28:42,540
loop or in a bigger algorithm.

472
00:28:42,540 --> 00:28:45,220
I also think there's a really cool parallel here to AlphaGo.

473
00:28:45,220 --> 00:28:49,740
AlphaGo has a policy for placing the next stone when it plays go and this policy was

474
00:28:49,740 --> 00:28:52,740
trained originally by imitating humans.

475
00:28:52,740 --> 00:28:57,300
But in addition to this policy, it also does multi-carreler tree search and basically it

476
00:28:57,300 --> 00:29:00,740
will play out a number of possibilities in its head and evaluate all of them and only

477
00:29:00,740 --> 00:29:02,060
keep the ones that work well.

478
00:29:02,060 --> 00:29:09,020
And so I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense.

479
00:29:09,020 --> 00:29:12,580
So just like tree of thought, I think more generally people are starting to like really

480
00:29:12,580 --> 00:29:17,740
explore more general techniques of not just the simple question and answer prompts, but

481
00:29:17,740 --> 00:29:22,220
something that looks a lot more like Python glue code stringing together many prompts.

482
00:29:22,220 --> 00:29:27,620
So on the right, I have an example from this paper called React where they structure the

483
00:29:27,620 --> 00:29:34,260
answer to a prompt as a sequence of thought, action, observation, thought, action, observation

484
00:29:34,260 --> 00:29:38,660
and it's a full rollout, kind of a thinking process to answer the query.

485
00:29:38,660 --> 00:29:42,540
And in these actions, the model is also allowed to tool use.

486
00:29:42,540 --> 00:29:48,380
On the left, I have an example of auto-GPT and now auto-GPT, by the way, is a project

487
00:29:48,380 --> 00:29:53,780
that I think got a lot of hype recently and I think, but I think I still find it kind

488
00:29:53,780 --> 00:29:56,220
of inspirationally interesting.

489
00:29:56,220 --> 00:30:01,180
It's a project that allows an LLM to keep task list and continue to recursively break

490
00:30:01,180 --> 00:30:05,220
down tasks and I don't think this currently works very well and I would not advise people

491
00:30:05,220 --> 00:30:07,220
to use it in practical applications.

492
00:30:07,220 --> 00:30:09,980
I just think it's something to generally take inspiration from in terms of where this

493
00:30:10,060 --> 00:30:13,260
is going, I think, over time.

494
00:30:13,260 --> 00:30:16,500
So that's kind of like giving our model system to thinking.

495
00:30:16,500 --> 00:30:21,020
The next thing that I find kind of interesting is this following sort of, I would say, almost

496
00:30:21,020 --> 00:30:26,780
psychological quirk of LLMs is that LLMs don't want to succeed.

497
00:30:26,780 --> 00:30:28,700
They want to imitate.

498
00:30:28,700 --> 00:30:31,420
You want to succeed and you should ask for it.

499
00:30:31,420 --> 00:30:37,820
So what I mean by that is when transformers are trained, they have training sets and there

500
00:30:37,820 --> 00:30:41,620
can be an entire spectrum of performance qualities in their training data.

501
00:30:41,620 --> 00:30:44,900
So for example, there could be some kind of a prompt for some physics question or something

502
00:30:44,900 --> 00:30:48,500
like that and there could be a student solution that is completely wrong, but there can also

503
00:30:48,500 --> 00:30:51,260
be an expert answer that is extremely right.

504
00:30:51,260 --> 00:30:56,340
And transformers can't tell the difference between, like, they know about low-quality

505
00:30:56,340 --> 00:31:00,180
solutions and high-quality solutions, but by default, they want to imitate all of it

506
00:31:00,180 --> 00:31:02,540
because they're just trained on language modeling.

507
00:31:02,540 --> 00:31:06,380
And so at test time, you actually have to ask for a good performance.

508
00:31:06,380 --> 00:31:12,180
So in this example in this paper, they tried various prompts and let's think step-by-step

509
00:31:12,180 --> 00:31:16,020
was very powerful because it sort of spread out the reasoning over many tokens, but what

510
00:31:16,020 --> 00:31:19,980
worked even better is let's work this out in a step-by-step way to be sure we have the

511
00:31:19,980 --> 00:31:21,100
right answer.

512
00:31:21,100 --> 00:31:24,620
And so it's kind of like conditioning on getting a right answer and this actually makes the

513
00:31:24,620 --> 00:31:29,780
transformer work better because the transformer doesn't have to now hedge its probability mass

514
00:31:29,780 --> 00:31:33,060
on low-quality solutions as ridiculous as that sounds.

515
00:31:33,060 --> 00:31:38,620
And so basically, feel free to ask for a strong solution, say something like, you are a leading

516
00:31:38,620 --> 00:31:42,300
expert on this topic, pretend you have IQ 120, et cetera.

517
00:31:42,300 --> 00:31:46,740
But don't try to ask for too much IQ because if you ask for an IQ of, like, 400, you might

518
00:31:46,740 --> 00:31:51,420
be out of data distribution or, even worse, you could be in data distribution for some,

519
00:31:51,420 --> 00:31:56,260
like, sci-fi stuff and it will start to, like, take on some sci-fi role-playing or something

520
00:31:56,260 --> 00:31:57,260
like that.

521
00:31:57,260 --> 00:32:01,620
So you have to find, like, the right amount of IQ, I think, it's got some U-shaped curve

522
00:32:01,620 --> 00:32:03,460
there.

523
00:32:03,460 --> 00:32:09,180
Next up, as we saw, when we are trying to solve problems, we know what we are good at and what

524
00:32:09,180 --> 00:32:12,260
we're not good at and we lean on tools computationally.

525
00:32:12,260 --> 00:32:15,300
You want to do the same potentially with your LLMs.

526
00:32:15,300 --> 00:32:22,260
So in particular, we may want to give them calculators, code interpreters, and so on,

527
00:32:22,260 --> 00:32:27,540
the ability to do search and there's a lot of techniques for doing that.

528
00:32:27,540 --> 00:32:31,580
One thing to keep in mind, again, is that these transformers by default may not know

529
00:32:31,580 --> 00:32:33,100
what they don't know.

530
00:32:33,100 --> 00:32:36,820
So you may even want to tell the transformer in a prompt, you are not very good at mental

531
00:32:36,820 --> 00:32:37,820
arithmetic.

532
00:32:37,820 --> 00:32:41,420
Whenever you need to do very large number addition, multiplication or whatever, instead

533
00:32:41,420 --> 00:32:42,420
use this calculator.

534
00:32:42,420 --> 00:32:46,660
Here's how you use the calculator, use this token, combination, et cetera, et cetera.

535
00:32:46,660 --> 00:32:49,740
So you have to actually, like, spell it out because the model by default doesn't know

536
00:32:49,740 --> 00:32:55,900
what it's good at or not good at necessarily, just like you and I might be.

537
00:32:56,020 --> 00:33:00,540
Next up, I think something that is very interesting is we went from a world that was retrieval

538
00:33:00,540 --> 00:33:06,020
only all the way the pendulum swung to the other extreme where it's memory only in LLMs.

539
00:33:06,020 --> 00:33:10,740
But actually, there's this entire space in between of these retrieval augmented models

540
00:33:10,740 --> 00:33:13,420
and this works extremely well in practice.

541
00:33:13,420 --> 00:33:17,300
As I mentioned, the context window of a transformer is its working memory.

542
00:33:17,300 --> 00:33:21,500
If you can load the working memory with any information that is relevant to the task,

543
00:33:21,500 --> 00:33:26,580
the model will work extremely well because it can immediately access all that memory.

544
00:33:26,580 --> 00:33:32,060
And so I think a lot of people are really interested in basically retrieval augmented

545
00:33:32,060 --> 00:33:33,060
generation.

546
00:33:33,060 --> 00:33:36,740
And on the bottom, I have, like, an example of Lama index, which is one sort of data

547
00:33:36,740 --> 00:33:41,820
connector to lots of different types of data, and you can make it, you can index all of

548
00:33:41,820 --> 00:33:44,500
that data and you can make it accessible to LLMs.

549
00:33:44,500 --> 00:33:48,420
And the emerging recipe there is you take relevant documents, you split them up into

550
00:33:48,420 --> 00:33:52,820
chunks, you embed all of them, and you basically get embedding vectors that represent that

551
00:33:52,820 --> 00:33:57,020
data, you store that in the vector store, and then at test time, you make some kind

552
00:33:57,020 --> 00:34:01,780
of a query to your vector store, and you fetch chunks that might be relevant to your task

553
00:34:01,780 --> 00:34:04,300
and you stuff them into the prompt and then you generate.

554
00:34:04,300 --> 00:34:06,460
So this can work quite well in practice.

555
00:34:06,460 --> 00:34:10,420
So this is, I think, similar to when you and I solve problems, you can do everything from

556
00:34:10,420 --> 00:34:14,780
your memory and transformers have very large and extensive memory, but also it really helps

557
00:34:14,780 --> 00:34:17,820
to reference some primary documents.

558
00:34:17,820 --> 00:34:21,420
So whenever you find yourself going back to a textbook to find something, or whenever

559
00:34:21,420 --> 00:34:26,220
you find yourself going back to documentation of a library to look something up, the transformers

560
00:34:26,220 --> 00:34:28,020
definitely want to do that, too.

561
00:34:28,020 --> 00:34:32,580
You have some memory over how some documentation of a library works, but it's much better to

562
00:34:32,580 --> 00:34:33,580
look it up.

563
00:34:33,580 --> 00:34:36,020
So the same applies here.

564
00:34:36,020 --> 00:34:39,700
Next, I wanted to briefly talk about constraint prompting.

565
00:34:39,700 --> 00:34:42,300
I also find this very interesting.

566
00:34:42,300 --> 00:34:50,380
This is basically techniques for forcing a certain template in the outputs of LLMs.

567
00:34:50,380 --> 00:34:55,420
So guidance is one example from Microsoft, actually, and here we are enforcing that the

568
00:34:55,420 --> 00:35:00,580
output from the LLM will be JSON, and this will actually guarantee that the output will

569
00:35:00,580 --> 00:35:04,060
take on this form because they go in and they mess with the probabilities of all the different

570
00:35:04,060 --> 00:35:07,700
tokens that come out of the transformer and they clamp those tokens.

571
00:35:07,700 --> 00:35:10,780
And then the transformer is only filling in the blanks here, and then you can enforce

572
00:35:10,820 --> 00:35:13,620
additional restrictions on what could go into those blanks.

573
00:35:13,620 --> 00:35:16,940
So this might be really helpful, and I think this kind of constraint sampling is also extremely

574
00:35:16,940 --> 00:35:17,940
interesting.

575
00:35:17,940 --> 00:35:22,660
I also wanted to say a few words about fine-tuning.

576
00:35:22,660 --> 00:35:27,380
It is the case that you can get really far with prompt engineering, but it's also possible

577
00:35:27,380 --> 00:35:29,820
to think about fine-tuning your models.

578
00:35:29,820 --> 00:35:33,340
Now fine-tuning models means that you are actually going to change the weights of the

579
00:35:33,340 --> 00:35:34,500
model.

580
00:35:34,500 --> 00:35:38,900
It is becoming a lot more accessible to do this in practice, and that's because of a

581
00:35:38,900 --> 00:35:43,340
number of techniques that have been developed and have libraries for very recently.

582
00:35:43,340 --> 00:35:47,940
So for example, parameter-efficient fine-tuning techniques like LORA make sure that you're

583
00:35:47,940 --> 00:35:51,220
only training small sparse pieces of your model.

584
00:35:51,220 --> 00:35:55,340
So most of the model is kept clamped at the base model, and some pieces of it are allowed

585
00:35:55,340 --> 00:35:59,340
to change, and this still works pretty well empirically, and makes it much cheaper to

586
00:35:59,340 --> 00:36:03,300
sort of tune only small pieces of your model.

587
00:36:03,300 --> 00:36:07,180
It also means that because most of your model is clamped, you can use very low-precision

588
00:36:07,180 --> 00:36:11,140
inference for computing those parts, because they are not going to be updated by gradient

589
00:36:11,140 --> 00:36:14,500
ascent, and so that makes everything a lot more efficient as well.

590
00:36:14,500 --> 00:36:17,500
And in addition, we have a number of open-sourced, high-quality base models.

591
00:36:17,500 --> 00:36:21,580
Currently, as I mentioned, I think LAMA is quite nice, although it is not commercially

592
00:36:21,580 --> 00:36:24,620
licensed, I believe right now.

593
00:36:24,620 --> 00:36:29,780
Something to keep in mind is that basically fine-tuning is a lot more technically involved.

594
00:36:29,780 --> 00:36:32,900
It requires a lot more, I think, technical expertise to do right.

595
00:36:32,900 --> 00:36:37,260
It requires human data contractors for datasets and or synthetic data pipelines that can be

596
00:36:37,260 --> 00:36:38,860
pretty complicated.

597
00:36:38,860 --> 00:36:43,420
This will definitely slow down your iteration cycle by a lot, and I would say on a high-level

598
00:36:43,420 --> 00:36:48,460
SFD is achievable, because it is just you're continuing the language modeling task.

599
00:36:48,460 --> 00:36:53,540
It's relatively straightforward, but RLHF, I would say, is very much research territory,

600
00:36:53,540 --> 00:36:58,100
and is even much harder to get to work, and so I would probably not advise that someone

601
00:36:58,100 --> 00:37:00,860
just tries to roll their own RLHF implementation.

602
00:37:00,860 --> 00:37:04,780
These things are pretty unstable, very difficult to train, not something that is, I think,

603
00:37:04,780 --> 00:37:09,860
very beginner-friendly right now, and is also potentially likely also to change pretty rapidly

604
00:37:09,860 --> 00:37:12,380
still.

605
00:37:12,380 --> 00:37:15,780
So I think these are my sort of default recommendations right now.

606
00:37:15,780 --> 00:37:18,540
I would break up your task into two major parts.

607
00:37:18,540 --> 00:37:22,820
Number one, achieve your top performance, and number two, optimize your performance in

608
00:37:22,820 --> 00:37:24,580
that order.

609
00:37:24,580 --> 00:37:27,740
Number one, the best performance will currently come from G504 model.

610
00:37:27,740 --> 00:37:30,220
It is the most capable model by far.

611
00:37:30,220 --> 00:37:35,020
These prompts that are very detailed, they have lots of task contents, relevant information

612
00:37:35,020 --> 00:37:36,780
and instructions.

613
00:37:36,780 --> 00:37:40,100
Think along the lines of what would you tell a task contractor if they can't email you

614
00:37:40,100 --> 00:37:44,300
back, but then also keep in mind that a task contractor is a human, and they have inner

615
00:37:44,300 --> 00:37:49,020
monologue and they're very clever, et cetera, LLMs do not possess those qualities, so make

616
00:37:49,020 --> 00:37:55,860
sure to think through the psychology of the LLM almost, and cater prompts to that.

617
00:37:55,860 --> 00:38:00,900
You can even add any relevant context and information to these prompts.

618
00:38:00,900 --> 00:38:03,220
Basically refer to a lot of the prompt engineering techniques.

619
00:38:03,220 --> 00:38:07,540
Some of them I've highlighted in the slides above, but also this is a very large space,

620
00:38:07,540 --> 00:38:11,900
and I would just advise you to look for prompt engineering techniques online.

621
00:38:11,900 --> 00:38:14,220
There's a lot to cover there.

622
00:38:14,220 --> 00:38:15,940
Experiment with few short examples.

623
00:38:15,940 --> 00:38:20,060
What this refers to is you don't just want to tell, you want to show whenever it's possible,

624
00:38:20,060 --> 00:38:24,100
so give it examples of everything that helps it really understand what you mean if you

625
00:38:24,100 --> 00:38:25,100
can.

626
00:38:25,780 --> 00:38:31,260
Experiment with tools and plugins to offload a task that are difficult for LLMs natively,

627
00:38:31,260 --> 00:38:35,100
and then think about not just a single prompt and answer, think about potential chains and

628
00:38:35,100 --> 00:38:38,440
reflection and how you glue them together and how you could potentially make multiple

629
00:38:38,440 --> 00:38:40,300
samples and so on.

630
00:38:40,300 --> 00:38:44,300
Finally, if you think you've squeezed out prompt engineering, which I think you should

631
00:38:44,300 --> 00:38:51,820
stick with for a while, look at some potentially fine-tuning a model to your application, but

632
00:38:51,820 --> 00:38:54,300
expect this to be a lot more slower and involved.

633
00:38:54,300 --> 00:38:58,580
And then there's an expert fragile research zone here, and I would say that is RLHF, which

634
00:38:58,580 --> 00:39:03,220
currently does work a bit better than SFD if you can get it to work, but again, this

635
00:39:03,220 --> 00:39:05,500
is pretty involved, I would say.

636
00:39:05,500 --> 00:39:10,620
And to optimize your costs, try to explore lower capacity models or shorter prompts and

637
00:39:10,620 --> 00:39:11,620
so on.

638
00:39:11,620 --> 00:39:17,380
I also wanted to say a few words about the use cases in which I think LLMs are currently

639
00:39:17,380 --> 00:39:18,860
well suited for.

640
00:39:18,860 --> 00:39:23,780
So in particular, note that there's a large number of limitations to LLMs today, and

641
00:39:23,780 --> 00:39:26,660
so I would keep that definitely in mind for all your applications.

642
00:39:26,660 --> 00:39:29,540
Models, and this, by the way, could be an entire talk, so I don't have time to cover

643
00:39:29,540 --> 00:39:30,940
it in full detail.

644
00:39:30,940 --> 00:39:35,900
Models may be biased, they may fabricate hallucinate information, they may have reasoning errors,

645
00:39:35,900 --> 00:39:40,580
they may struggle in entire classes of applications, they have knowledge cutoffs, so they might

646
00:39:40,580 --> 00:39:44,100
not know any information above, say, September 2021.

647
00:39:44,100 --> 00:39:48,260
They are susceptible to a large range of attacks, which are sort of like coming out on Twitter

648
00:39:48,260 --> 00:39:53,220
daily, including prompt injection, jailbreak attacks, data poisoning attacks, and so on.

649
00:39:53,220 --> 00:39:58,540
So my recommendation right now is use LLMs in low stakes applications, combine them with

650
00:39:58,540 --> 00:40:03,340
always with human oversight, use them as a source of inspiration and suggestions, and

651
00:40:03,340 --> 00:40:07,220
think co-pilots instead of completely autonomous agents that are just like performing a task

652
00:40:07,220 --> 00:40:08,220
somewhere.

653
00:40:08,220 --> 00:40:12,620
It's just not clear that the models are there right now.

654
00:40:12,620 --> 00:40:15,180
So I wanted to close by saying that GPT-4 is an amazing artifact.

655
00:40:15,180 --> 00:40:19,460
I'm very thankful that it exists, and it's beautiful, it has a ton of knowledge across

656
00:40:19,460 --> 00:40:23,380
so many areas, it can do math, code, and so on.

657
00:40:23,380 --> 00:40:27,020
And in addition, there's this thriving ecosystem of everything else that is being built and

658
00:40:27,020 --> 00:40:32,340
incorporated into the ecosystem, some of these things I've talked about.

659
00:40:32,340 --> 00:40:36,060
And all of this power is accessible at your fingertips.

660
00:40:36,060 --> 00:40:40,860
So here's everything that's needed in terms of code to ask GPT-4 a question, to prompt

661
00:40:40,860 --> 00:40:42,700
it and get a response.

662
00:40:42,700 --> 00:40:46,780
In this case, I said, can you say something to inspire the audience of Microsoft Build

663
00:40:46,780 --> 00:40:48,260
2023?

664
00:40:48,260 --> 00:40:54,340
And I just punched this into Python and verbatim, GPT-4 said the following.

665
00:40:54,340 --> 00:40:58,020
And by the way, I did not know that they used this trick in the keynote, so I thought I

666
00:40:58,020 --> 00:41:04,660
was being clever, but it is really good at this, it says, ladies and gentlemen, innovators

667
00:41:04,660 --> 00:41:09,020
and trail bearers of Microsoft Build 2023, welcome to the gathering of brilliant minds

668
00:41:09,020 --> 00:41:10,260
like no other.

669
00:41:10,260 --> 00:41:14,140
You are the architects of the future, the visionaries molding the digital realm in which

670
00:41:14,180 --> 00:41:19,020
humanity thrives, embrace the limitless possibilities of technologies and let your ideas soar as

671
00:41:19,020 --> 00:41:20,540
high as your imagination.

672
00:41:20,540 --> 00:41:24,540
Together, let's create a more connected, remarkable and inclusive world for generations

673
00:41:24,540 --> 00:41:29,620
to come, get ready to unleash your creativity, canvass the unknown, and turn dreams into

674
00:41:29,620 --> 00:41:30,620
reality.

675
00:41:30,620 --> 00:41:32,260
Your journey begins today.

676
00:41:44,140 --> 00:41:45,140
Thank you.

677
00:41:45,140 --> 00:41:46,140
Thank you.

678
00:41:46,140 --> 00:41:47,140
Thank you.

679
00:41:47,140 --> 00:41:48,140
Thank you.

680
00:41:48,140 --> 00:41:49,140
Thank you.

681
00:41:49,140 --> 00:41:50,140
Thank you.

682
00:41:50,140 --> 00:41:51,140
Thank you.

683
00:41:51,140 --> 00:41:52,140
Thank you.

684
00:41:52,140 --> 00:41:53,140
Thank you.

685
00:41:53,140 --> 00:41:54,140
Thank you.

686
00:41:54,140 --> 00:41:55,140
Thank you.

687
00:41:55,140 --> 00:41:56,140
Thank you.

688
00:41:56,140 --> 00:41:57,140
Thank you.

689
00:41:57,140 --> 00:41:58,140
Thank you.

690
00:41:58,140 --> 00:41:59,140
Thank you.

691
00:41:59,140 --> 00:42:00,140
Thank you.

692
00:42:00,140 --> 00:42:01,140
Thank you.

693
00:42:01,140 --> 00:42:02,140
Thank you.

694
00:42:02,140 --> 00:42:03,140
Thank you.

695
00:42:03,140 --> 00:42:04,140
Thank you.

696
00:42:04,140 --> 00:42:05,140
Thank you.

697
00:42:05,140 --> 00:42:06,140
Thank you.

698
00:42:06,140 --> 00:42:07,140
Thank you.

699
00:42:07,140 --> 00:42:08,140
Thank you.

700
00:42:09,140 --> 00:42:10,140
Thank you.

701
00:42:10,140 --> 00:42:11,140
Thank you.

702
00:42:11,140 --> 00:42:12,140
Thank you.

703
00:42:12,140 --> 00:42:13,140
Thank you.

704
00:42:13,140 --> 00:42:14,140
Thank you.

705
00:42:14,140 --> 00:42:15,140
Thank you.

706
00:42:15,140 --> 00:42:16,140
Thank you.

707
00:42:16,140 --> 00:42:17,140
Thank you.

708
00:42:17,140 --> 00:42:18,140
Thank you.

709
00:42:18,140 --> 00:42:19,140
Thank you.

710
00:42:19,140 --> 00:42:20,140
Thank you.

711
00:42:20,140 --> 00:42:21,140
Thank you.

712
00:42:21,140 --> 00:42:22,140
Thank you.

713
00:42:22,140 --> 00:42:23,140
Thank you.

714
00:42:23,140 --> 00:42:24,140
Thank you.

715
00:42:24,140 --> 00:42:25,140
Thank you.

716
00:42:25,140 --> 00:42:26,140
Thank you.

717
00:42:26,140 --> 00:42:27,140
Thank you.

718
00:42:27,140 --> 00:42:28,140
Thank you.

719
00:42:28,140 --> 00:42:29,140
Thank you.

720
00:42:29,140 --> 00:42:30,140
Thank you.

721
00:42:30,140 --> 00:42:31,140
Thank you.

722
00:42:31,140 --> 00:42:32,140
Thank you.

723
00:42:32,140 --> 00:42:33,140
Thank you.

724
00:42:33,140 --> 00:42:34,140
Thank you.

725
00:42:34,140 --> 00:42:35,140
Thank you.

726
00:42:35,140 --> 00:42:36,140
Thank you.

727
00:42:36,140 --> 00:42:37,140
Thank you.

