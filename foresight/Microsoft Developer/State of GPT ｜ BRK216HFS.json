{"text": " Please welcome AI researcher and founding member of OpenAI, Andre Karpathy. Hi, everyone. I'm happy to be here to tell you about the state of GPT and more generally about the rapidly growing ecosystem of large language models. So I would like to partition the talk into two parts. In the first part, I would like to tell you about how we train GPT assistants. And then in the second part, we are going to take a look at how we can use these assistants effectively for your applications. So first, let's take a look at the emerging recipe for how to train these assistants and keep in mind that this is all very new and still rapidly evolving. But so far, the recipe looks something like this. Now, this is kind of a complicated slide, so I'm going to go through it piece by piece. But roughly speaking, we have four major stages. Pre-training, supervised fine-tuning, reward modeling, reinforcement learning, and they follow each other serially. Now, in each stage, we have a data set that powers that stage. We have an algorithm that, for our purposes, will be an objective for training a neural network. And then we have a resulting model, and then there's some nodes on the bottom. So the first stage we're going to start with is the pre-training stage. Now, this stage is kind of special in this diagram, and this diagram is not to scale, because this stage is where all of the computational work basically happens. This is 99% of the training compute time and also flops. And so this is where we are dealing with internet-scale data sets with thousands of GPUs in the supercomputer and also months of training, potentially. The other three stages are fine-tuning stages that are much more along the lines of small few number of GPUs and hours or days. So let's take a look at the pre-training stage to achieve a base model. First we're going to gather a large amount of data. Here's an example of what we call a data mixture that comes from this paper that was released by Meta, where they released this Lama-based model. Now, you can see roughly the kinds of data sets that enter into these collections. So we have common crawl, which is just a web scrape, C4, which is also a common crawl, and then some high-quality data sets as well. So for example, GitHub, Wikipedia, Books, Archive, Stack Exchange, and so on. These are all mixed up together, and then they are sampled according to some given proportions, and that forms the training set for the neural net for the GPT. Now before we can actually train on this data, we need to go through one more pre-processing step, and that is tokenization. And this is basically a translation of the raw text that we scrape from the internet into sequences of integers, because that's the native representation over which GPTs function. Now this is a lossless kind of translation between pieces of text and tokens and integers, and there are a number of algorithms for this stage. Typically, for example, you could use something like byte bearing coding, which iteratively merges little text chunks and groups them into tokens. And so here I'm showing some example chunks of these tokens, and then this is the raw integer sequence that will actually feed into a transformer. Now here I'm showing two sort of like examples for hyper-primers that govern this stage. So GPT-4, we did not release too much information about how it was trained and so on, so I'm using GPT-3's numbers, but GPT-3 is of course a little bit old by now, about three years ago. But Lama is a fairly recent model from Meta. So these are roughly the orders of magnitude that we're dealing with when we're doing pre-training. The vocabulary size is usually a couple 10,000 tokens, the context length is usually something like 2,000, 4,000, or nowadays even 100,000, and this governs the maximum number of integers that the GPT will look at when it's trying to predict the next integer in a sequence. You can see that roughly the number of parameters is, say, 65 billion for Lama. Now even though Lama has only 65B parameters compared to GPT-3's 175 billion parameters, Lama is a significantly more powerful model, and intuitively that's because the model is trained for significantly longer, in this case 1.4 trillion tokens instead of just 300 billion tokens. So you shouldn't judge the power of a model just by the number of parameters that it contains. Below I'm showing some tables of rough hyper-parameters that typically go into specifying the transformer neural network, so the number of heads, the dimension size, number of layers, and so on. And on the bottom I'm showing some training hyper-parameters. So for example, to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training, and roughly several million dollars, and so that's the rough orders of magnitude that you should have in mind for the pre-training stage. Now when we're actually pre-training what happens, roughly speaking, we are going to take our tokens and we're going to lay them out into data batches. So we have these arrays that will feed into the transformer, and these arrays are B, the batch size, and these are all independent examples stacked up in rows, and B by T, T being the maximum context length. So in my picture I only have 10 of the context lengths, so this could be 2,000, 4,000, etc. So these are extremely long rows. And what we do is we take these documents and we pack them into rows, and we delimit them with these special end-of-text tokens, basically telling the transformer where a new document begins. And so here I have a few examples of documents, and then I've stretched them out into this input. Now, we're going to feed all of these numbers into Transformer, and let me just focus on a single particular cell, but the same thing will happen at every cell in this diagram. So let's look at the green cell. The green cell is going to take a look at all of the tokens before it, so all of the tokens in yellow, and we're going to feed that entire context into the transformer neural network, and the transformer is going to try to predict the next token in a sequence, in this case in red. Now the transformer, I don't have too much time to unfortunately go into the full details of this neural network architecture, is just a large blob of neural net stuff for our purposes, and it's got several 10 billion parameters typically, or something like that. And of course, as you tune these parameters, you're getting slightly different predicted distributions for every single one of these cells. And so, for example, if our vocabulary size is 50,257 tokens, then we're going to have that many numbers because we need to specify a probability distribution for what comes next. So basically we have a probability for whatever may follow. Now in this specific example for this specific cell, 513 will come next, and so we can use this as a source of supervision to update our transformer's weights. And so we're applying this basically on every single cell in parallel, and we keep swapping batches, and we're trying to get the transformer to make the correct predictions over what token comes next in a sequence. So let me show you more concretely what this looks like when you train one of these models. This is actually coming from New York Times, and they trained a small GPT on Shakespeare, and so here's a small snippet of Shakespeare, and they trained a GPT on it. Now in the beginning at initialization, the GPT starts with completely random weights, so you're just getting completely random outputs as well. But over time, as you train the GPT longer and longer, you are getting more and more coherent and consistent sort of samples from the model. And the way you sample from it, of course, is you predict what comes next, you sample from that distribution, and you keep feeding that back into the process, and you can basically sample large sequences. And so by the end, you see that the transformer has learned about words and where to put spaces and where to put commas and so on. And so we're making more and more consistent predictions over time. These are the kinds of plots that you're looking at when you're doing model pre-training. Effectively, we're looking at the loss function over time as you train, and low loss means that our transformer is predicting the correct, is giving a higher probability to get the correct next integer in a sequence. Now what are we going to do with this model once we've trained it after a month? Well the first thing that we noticed, we, the field, is that these models basically in the process of language modeling learn very powerful, general representations, and it's possible to very efficiently fine tune them for any arbitrary downstream task you might be interested in. So as an example, if you're interested in sentiment classification, the approach used to be that you collect a bunch of positives and negatives, and then you train some kind of an NLP model for that, but the new approach is ignore sentiment classification, go off and do large language model pre-training, train a large transformer, and then you can only, you may only have a few examples, and you can very efficiently fine tune your model for that task. And so this works very well in practice. And the reason for this is that basically the transformer is forced to multitask a huge amount of tasks in the language modeling task, because just, just in terms of predicting the next token, it's forced to understand a lot about the structure of the, of the text and all the different concepts therein. So that was GPT-1. Now around the time of GPT-2, people noticed that actually even better than fine tuning, you can actually prompt these models very effectively. So these are language models, and they want to complete documents. So you can actually trick them into performing tasks just by arranging these fake documents. So in this example, for, for example, we have some passage, and then we sort of like do QA, QA, QA, this is called a few-shot prompt, and then we do Q, and then as the transformer is trying to complete the document, it's actually answering our question. And so this is an example of prompt engineering a base model, making the belief that it's sort of imitating a document and getting it to perform a task. And so this kicked off, I think, the era of, I would say, prompting over fine tuning and seeing that this actually can work extremely well on a lot of problems, even without training any neural networks, fine tuning, or so on. Now since then, we've seen an entire evolutionary tree of base models that everyone has trained. Not all of these models are available. For example, the GPT-4 base model was never released. The GPT-4 model that you might be interacting with over API is not a base model, it's an assistant model, and we're going to cover how to get those in a bit. GPT-3 base model is available via the API under the name DaVinci, and GPT-2 base model is available even as weights on our GitHub repo. But currently the best available base model probably is the Lama series from Meta, although it is not commercially licensed. Now one thing to point out is base models are not assistants. They don't want to make answers to your questions. They just want to complete documents. So if you tell them write a poem about the bread and cheese, it will answer questions with more questions. It's just completing what it thinks is a document. However, you can prompt them in a specific way for base models that is more likely to work. So as an example, here's a poem about bread and cheese, and in that case it will autocomplete correctly. You can even trick base models into being assistants, and the way you would do this is you would create like a specific few-shot prompt that makes it look like there's some kind of a document between a human and assistant, and they're exchanging sort of information. And then at the bottom you sort of put your query at the end, and the base model will sort of like condition itself into being like a helpful assistant and kind of answer. But this is not very reliable and doesn't work super well in practice, although it can be done. So instead we have a different path to make actual GPT assistants not just base model document completers. And so that takes us into supervised fine-tuning. So in the supervised fine-tuning stage, we are going to collect small but high-quality data sets. And in this case we're going to ask human contractors to gather data of the form prompt and ideal response, and we're going to collect lots of these, typically tens of thousands or something like that. And then we're going to still do language modeling on this data, so nothing changed algorithmically. We're just swapping out a training set. So it used to be internet documents, which is a high-quantity, low-quantity for basically QA prompt response kind of data, and that is low-quantity, high-quality. So we still do language modeling, and then after training we get an SFD model. And you can actually deploy these models, and they are actual assistants, and they work to some extent. Let me show you what an example demonstration might look like. So here's something that a human contractor might come up with. Here's some random prompt. When you write a short introduction about the relevance of the term monopsony or something like that, and then the contractor also writes out an ideal response. And when they write out these responses, they are following extensive labeling documentations, and they are being asked to be helpful, truthful, and harmless. And this is labeling instructions here. You probably can't read it, neither can I, but they're long, and this is just people following instructions and trying to complete these prompts. So that's what the data set looks like, and you can train these models, and this works to some extent. Now you can actually continue the pipeline from here on and go into RLHF, Reinforcement Learning from Human Feedback, that consists of both reward modeling and reinforcement learning. So let me cover that, and then I'll come back to why you may want to go through the extra steps and how that compares to just SFD models. So in the reward modeling step, what we're going to do is we're now going to shift our data collection to be of the form of comparisons. So here's an example of what our data set will look like. I have the same prompt, identical prompt on the top, which is asking the assistant to write a program or a function that checks if a given string is a palindrome. And then what we do is we take the SFD model, which we've already trained, and we create multiple completions. So in this case, we have three completions that the model has created. And then we ask people to rank these completions. So if you stare at this for a while, and by the way, these are very difficult things to do to compare some of these predictions, and this can take people even hours for a single prompt completion pairs. But let's say we decided that one of these is much better than the others, and so on. So we rank them. Then we can follow that with something that looks very much like a binary classification on all the possible pairs between these completions. So what we do now is we lay out our prompt in rows, and the prompts is identical across all three rows here. So it's all the same prompt, but the completion is very, and so the yellow tokens are coming from the SFD model. Then what we do is we append another special reward readout token at the end, and we basically only supervise the transformer at this single green token. And the transformer will predict some reward for how good that completion is for that prompt. And so basically it makes a guess about the quality of each completion, and then once it makes a guess for every one of them, we also have the ground truth, which is telling us the ranking of them. And so we can actually enforce that some of these numbers should be much higher than others, and so on. We formulate this into a loss function, and we train our model to make reward predictions that are consistent with the ground truth coming from the comparisons from all these contractors. So that's how we train our reward model. And that allows us to score how good a completion is for a prompt. Once we have a reward model, we can't deploy this because this is not very useful as an assistant by itself, but it's very useful for the reinforcement learning stage that follows now. Because we have a reward model, we can score the quality of any arbitrary completion for any given prompt. So what we do during reinforcement learning is we basically get, again, a large collection of prompts, and now we do reinforcement learning with respect to the reward model. So here's what that looks like. We take a single prompt, we lay it out and rose, and now we use the SFD model. We use basically the model we'd like to train, which is initialized at SFD model, to create some completions in yellow. And then we append the reward token again, and we read off the reward according to the reward model, which is now kept fixed. It doesn't change anymore. And now the reward model tells us the quality of every single completion for these prompts. And so what we can do is we can now just basically apply the same language modeling loss function, but we're currently training on the yellow tokens, and we are weighing the language modeling objective by the rewards indicated by the reward model. So as an example, in the first row, the reward model said that this is a fairly high-scoring completion. And so all of the tokens that we happened to sample on the first row are going to get reinforced, and they're going to get higher probabilities for the future. Conversely, on the second row, the reward model really did not like this completion, negative 1.2. And so therefore, every single token that we sampled in that second row is going to get a slightly higher probability for the future. And we do this over and over on many prompts, on many batches, and basically we get a policy which creates yellow tokens here, and basically all of the completions here will score high according to the reward model that we trained in the previous stage. So that's how we train, that's what the RLHF pipeline is. Now, and then at the end, you get a model that you could deploy, and so as an example, chatGPT is an RLHF model, but some other models that you might come across, like for example, the Kuna 13B and so on, these are SFD models. So we have base models, SFD models, and RLHF models, and that's kind of like the state of things there. Now why would you want to do RLHF? So one answer that is kind of not that exciting is that it just works better. So this comes from the InstructGPT paper. According to these experiments a while ago now, these PPO models are RLHF, and we see that they are basically just preferred in a lot of comparisons when we give them to humans. So humans just prefer basically tokens that come from RLHF models compared to SFD models, compared to base model that is prompted to be an assistant, and so it just works better. You might ask why? Why does it work better? And I don't think that there's a single amazing answer that the community has really agreed on, but I will just offer one reason potentially, and it has to do with the asymmetry between how easy computationally it is to compare versus generate. So let's take an example of generating a haiku. Suppose I ask a model to write a haiku about paper clips. If you're a contractor trying to give train data, then imagine being a contractor collecting basically data for the SFD stage, how are you supposed to create a nice haiku for a paper clip? You might just not be very good at that, but if I give you a few examples of haikus, you might be able to appreciate some of these haikus a lot more than others. And so judging which one of these is good is a much easier task. And so basically this asymmetry makes it so that comparisons are a better way to potentially leverage yourself as a human and your judgment to create a slightly better model. Now RLHF models are not strictly an improvement on the base models in some cases. So in particular, we've noticed, for example, that they lose some entropy. So that means that they give more PT results. They can output samples with lower variation than the base model. So base model has lots of entropy and will give lots of diverse outputs. So for example, one kind of place where I still prefer to use a base model is in a setup where you basically have n things and you want to generate more things like it. And so here is an example that I just cooked up. I want to generate cool Pokemon names. I gave it seven Pokemon names and I asked the base model to complete the document and it gave me a lot more Pokemon names. These are fictitious. I tried to look them up. I don't believe they're actual Pokemons. And this is the kind of task that I think base model would be good at because it still has lots of entropy and will give you lots of diverse, cool, kind of more things that look like whatever you give it before. So this is what, this is the number, having said all that, these are kind of like the assistant models that are probably available to you at this point. There's a team at Berkeley that ranked a lot of the available assistant models and gave them basically ELO ratings. So currently some of the best models, of course, are GPT-4 by far, I would say, followed by Claude, GPT-3.5, and then a number of models, some of these might be available as weights like the Kuna, Koala, et cetera. And the first three rows here are all, they are all RLHF models and all of the other models to my knowledge are SFD models, I believe. Okay, so that's how we train these models on a high level. Now I'm going to switch gears and let's look at how we can best apply a GPT-assistant model to your problems. Now I would like to work in a setting of a concrete example. So let's, let's work with a concrete example here. Let's say that you are working on an article or a blog post and you're going to write this sentence at the end, California's population is 53 times that of Alaska. So for some reason you want to compare the populations of these two states. Think about the rich internal monologue and tool use and how much work actually goes computationally in your brain to generate this one final sentence. So here's maybe what that could look like in your brain. Okay, for this next step, let me blog, or my blog, let me compare these two populations. Okay, first I'm going to obviously need to get both of these populations. Now I know that I probably don't know these populations off the top of my head. So I'm kind of like aware of what I know, what I don't know of my self-knowledge, right? So I go, I do some tool use and I go to Wikipedia and I look up California's population and Alaska's population. Now I know that I should divide the two. But again, I know that dividing 39.2 by 0.74 is very unlikely to succeed. That's not the kind of thing that I can do in my head. And so therefore I'm going to rely on a calculator. So I'm going to use a calculator, punch it in, and see that the output is roughly 53. And then maybe I do some reflection and sanity checks in my brain. So does 53 make sense? Well, that's quite a large fraction, but then California has the most popular state, so maybe that looks okay. So then I have all the information I might need, and now I get to the sort of creative portion of writing. So I might start to write something like, California has 53x times greater, and then I think to myself, that's actually like really awkward phrasing. So let me actually delete that, and let me try again. And so as I'm writing, I have this separate process almost inspecting what I'm writing and judging whether it looks good or not. And then maybe I delete, and maybe I reframe it, and then maybe I'm happy with what comes out. So basically long story short, a ton happens under the hood in terms of your internal monologue when you create sentences like this. But what does a sentence like this look like when we are training a GPT on it? From GPT's perspective, this is just a sequence of tokens. So a GPT when it's reading or generating these tokens, it just goes chunk, chunk, chunk, chunk, chunk. And each chunk is roughly the same amount of computational work for each token. And these transformers are not very shallow networks. They have about 80 layers of reasoning, but 80 is still not like too much. And so this transformer is going to do its best to imitate. But of course, the process here looks very, very different from the process that you took. So in particular, in our final artifacts, in the data sets that we create and then eventually feed to LLMs, all of that internal dialogue is completely stripped. And unlike you, the GPT will look at every single token and spend the same amount of compute on every one of them. And so you can't expect it to actually like, well, you can't expect it to sort of do too much work per token. So and also in particular, basically these transformers are just like token simulators. So they don't know what they don't know, like they just imitate the next token. They don't know what they're good at or not good at. They just try their best to imitate the next token. They don't reflect in the loop. They don't sanity check anything. They don't correct their mistakes along the way by default. They just sample token sequences. They don't have separate inner monologue streams in their head, right? They're evaluating what's happening. Now they do have some sort of cognitive advantages, I would say. And that is that they do actually have very large fact-based knowledge across a vast number of areas because they have, say, several 10 billion parameters. So that's a lot of storage for a lot of facts. But and they also, I think, have a relatively large and perfect working memory. So whatever fits into the context window is immediately available to the transformer through its internal self-attention mechanism. And so it's kind of like perfect memory, but it's got a finite size. But the transformer has a very direct access to it. And so it can, like, losslessly remember anything that is inside its context window. So that's kind of how I would compare those two. And the reason I bring all of this up is because I think to a large extent, prompting is just making up for this sort of cognitive difference between these two kind of architectures, like our brains here and LLM brains. You can look at it that way almost. So here's one thing that people found, for example, works pretty well in practice. Especially if your tasks require reasoning, you can't expect the transformer to do too much reasoning per token. And so you have to really spread out the reasoning across more and more tokens. So for example, you can't give the transformer a very complicated question and expect it to get the answer in a single token. There's just not enough time for it. These transformers need tokens to think, quote, unquote, I like to say sometimes. And so this is some of the things that work well. You may, for example, have a few-shot prompt that shows the transformer that it should like show its work when it's answering the question. And if you give a few examples, the transformer will imitate that template, and it will just end up working out better in terms of its evaluation. Additionally, you can elicit this kind of behavior from the transformer by saying, let's think step by step, because this conditioned the transformer into sort of like showing its work. And because it kind of snaps into a mode of showing its work, it's going to do less computational work per token. And so it's more likely to succeed as a result, because it's making slower reasoning over time. Here's another example. This one is called self-consistency. We saw that we had the ability to start writing. And then if it didn't work out, I can try again. And I can try multiple times and maybe select the one that worked best. So in these kinds of approaches, you may sample not just once, but you may sample multiple times, and then have some process for finding the ones that are good and then keeping just those samples or doing a majority vote or something like that. So basically, these transformers in the process, as they predict the next token, just like you, they can get unlucky. And they could sample a not a very good token, and they can go down sort of like a blind alley in terms of reasoning. And so unlike you, they cannot recover from that. They are stuck with every single token they sample, and so they will continue the sequence even if they even know that this sequence is not going to work out. So give them the ability to look back, inspect, or try to basically sample around it. Here's one technique also. It turns out that actually LLMs, like they know when they've screwed up. So as an example, say you ask the model to generate a poem that does not rhyme. And it might give you a poem, but it actually rhymes. But it turns out that especially for the bigger models like GPT-4, you can just ask it, did you meet the assignment? And actually GPT-4 knows very well that it did not meet the assignment. It just kind of got unlucky in its sampling. And so it will tell you, no, I didn't actually meet the assignment. Here's, let me try again. But without you prompting it, it doesn't even, like it doesn't know to revisit and so on. So you have to make up for that in your prompts. You have to get it to check. If you don't ask it to check, it's not going to check by itself, it's just a token simulator. I think more generally, a lot of these techniques fall into the bucket of what I would say recreating our system two. So you might be familiar with the system one, system two thinking for humans. System one is a fast automatic process. And I think kind of corresponds to like an LLM just sampling tokens. And system two is the slower, deliberate planning sort of part of your brain. And so this is a paper actually from just last week, because this space is pretty quickly evolving. And in tree of thought, the authors of this paper proposed maintaining multiple completions for any given prompt. And then they are also scoring them along the way and keeping the ones that are going well, if that makes sense. And so a lot of people are like really playing around with kind of prompt engineering to basically bring back some of these abilities that we sort of have in our brain for LLMs. Now one thing I would like to note here is that this is not just a prompt. This is actually prompts that are together used with some Python glue code because you don't, you actually have to maintain multiple prompts and you also have to do some tree search algorithm here to like figure out which prompts to expand, et cetera. So it's a symbiosis of Python glue code and individual prompts that are called in a Y loop or in a bigger algorithm. I also think there's a really cool parallel here to AlphaGo. AlphaGo has a policy for placing the next stone when it plays go and this policy was trained originally by imitating humans. But in addition to this policy, it also does multi-carreler tree search and basically it will play out a number of possibilities in its head and evaluate all of them and only keep the ones that work well. And so I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense. So just like tree of thought, I think more generally people are starting to like really explore more general techniques of not just the simple question and answer prompts, but something that looks a lot more like Python glue code stringing together many prompts. So on the right, I have an example from this paper called React where they structure the answer to a prompt as a sequence of thought, action, observation, thought, action, observation and it's a full rollout, kind of a thinking process to answer the query. And in these actions, the model is also allowed to tool use. On the left, I have an example of auto-GPT and now auto-GPT, by the way, is a project that I think got a lot of hype recently and I think, but I think I still find it kind of inspirationally interesting. It's a project that allows an LLM to keep task list and continue to recursively break down tasks and I don't think this currently works very well and I would not advise people to use it in practical applications. I just think it's something to generally take inspiration from in terms of where this is going, I think, over time. So that's kind of like giving our model system to thinking. The next thing that I find kind of interesting is this following sort of, I would say, almost psychological quirk of LLMs is that LLMs don't want to succeed. They want to imitate. You want to succeed and you should ask for it. So what I mean by that is when transformers are trained, they have training sets and there can be an entire spectrum of performance qualities in their training data. So for example, there could be some kind of a prompt for some physics question or something like that and there could be a student solution that is completely wrong, but there can also be an expert answer that is extremely right. And transformers can't tell the difference between, like, they know about low-quality solutions and high-quality solutions, but by default, they want to imitate all of it because they're just trained on language modeling. And so at test time, you actually have to ask for a good performance. So in this example in this paper, they tried various prompts and let's think step-by-step was very powerful because it sort of spread out the reasoning over many tokens, but what worked even better is let's work this out in a step-by-step way to be sure we have the right answer. And so it's kind of like conditioning on getting a right answer and this actually makes the transformer work better because the transformer doesn't have to now hedge its probability mass on low-quality solutions as ridiculous as that sounds. And so basically, feel free to ask for a strong solution, say something like, you are a leading expert on this topic, pretend you have IQ 120, et cetera. But don't try to ask for too much IQ because if you ask for an IQ of, like, 400, you might be out of data distribution or, even worse, you could be in data distribution for some, like, sci-fi stuff and it will start to, like, take on some sci-fi role-playing or something like that. So you have to find, like, the right amount of IQ, I think, it's got some U-shaped curve there. Next up, as we saw, when we are trying to solve problems, we know what we are good at and what we're not good at and we lean on tools computationally. You want to do the same potentially with your LLMs. So in particular, we may want to give them calculators, code interpreters, and so on, the ability to do search and there's a lot of techniques for doing that. One thing to keep in mind, again, is that these transformers by default may not know what they don't know. So you may even want to tell the transformer in a prompt, you are not very good at mental arithmetic. Whenever you need to do very large number addition, multiplication or whatever, instead use this calculator. Here's how you use the calculator, use this token, combination, et cetera, et cetera. So you have to actually, like, spell it out because the model by default doesn't know what it's good at or not good at necessarily, just like you and I might be. Next up, I think something that is very interesting is we went from a world that was retrieval only all the way the pendulum swung to the other extreme where it's memory only in LLMs. But actually, there's this entire space in between of these retrieval augmented models and this works extremely well in practice. As I mentioned, the context window of a transformer is its working memory. If you can load the working memory with any information that is relevant to the task, the model will work extremely well because it can immediately access all that memory. And so I think a lot of people are really interested in basically retrieval augmented generation. And on the bottom, I have, like, an example of Lama index, which is one sort of data connector to lots of different types of data, and you can make it, you can index all of that data and you can make it accessible to LLMs. And the emerging recipe there is you take relevant documents, you split them up into chunks, you embed all of them, and you basically get embedding vectors that represent that data, you store that in the vector store, and then at test time, you make some kind of a query to your vector store, and you fetch chunks that might be relevant to your task and you stuff them into the prompt and then you generate. So this can work quite well in practice. So this is, I think, similar to when you and I solve problems, you can do everything from your memory and transformers have very large and extensive memory, but also it really helps to reference some primary documents. So whenever you find yourself going back to a textbook to find something, or whenever you find yourself going back to documentation of a library to look something up, the transformers definitely want to do that, too. You have some memory over how some documentation of a library works, but it's much better to look it up. So the same applies here. Next, I wanted to briefly talk about constraint prompting. I also find this very interesting. This is basically techniques for forcing a certain template in the outputs of LLMs. So guidance is one example from Microsoft, actually, and here we are enforcing that the output from the LLM will be JSON, and this will actually guarantee that the output will take on this form because they go in and they mess with the probabilities of all the different tokens that come out of the transformer and they clamp those tokens. And then the transformer is only filling in the blanks here, and then you can enforce additional restrictions on what could go into those blanks. So this might be really helpful, and I think this kind of constraint sampling is also extremely interesting. I also wanted to say a few words about fine-tuning. It is the case that you can get really far with prompt engineering, but it's also possible to think about fine-tuning your models. Now fine-tuning models means that you are actually going to change the weights of the model. It is becoming a lot more accessible to do this in practice, and that's because of a number of techniques that have been developed and have libraries for very recently. So for example, parameter-efficient fine-tuning techniques like LORA make sure that you're only training small sparse pieces of your model. So most of the model is kept clamped at the base model, and some pieces of it are allowed to change, and this still works pretty well empirically, and makes it much cheaper to sort of tune only small pieces of your model. It also means that because most of your model is clamped, you can use very low-precision inference for computing those parts, because they are not going to be updated by gradient ascent, and so that makes everything a lot more efficient as well. And in addition, we have a number of open-sourced, high-quality base models. Currently, as I mentioned, I think LAMA is quite nice, although it is not commercially licensed, I believe right now. Something to keep in mind is that basically fine-tuning is a lot more technically involved. It requires a lot more, I think, technical expertise to do right. It requires human data contractors for datasets and or synthetic data pipelines that can be pretty complicated. This will definitely slow down your iteration cycle by a lot, and I would say on a high-level SFD is achievable, because it is just you're continuing the language modeling task. It's relatively straightforward, but RLHF, I would say, is very much research territory, and is even much harder to get to work, and so I would probably not advise that someone just tries to roll their own RLHF implementation. These things are pretty unstable, very difficult to train, not something that is, I think, very beginner-friendly right now, and is also potentially likely also to change pretty rapidly still. So I think these are my sort of default recommendations right now. I would break up your task into two major parts. Number one, achieve your top performance, and number two, optimize your performance in that order. Number one, the best performance will currently come from G504 model. It is the most capable model by far. These prompts that are very detailed, they have lots of task contents, relevant information and instructions. Think along the lines of what would you tell a task contractor if they can't email you back, but then also keep in mind that a task contractor is a human, and they have inner monologue and they're very clever, et cetera, LLMs do not possess those qualities, so make sure to think through the psychology of the LLM almost, and cater prompts to that. You can even add any relevant context and information to these prompts. Basically refer to a lot of the prompt engineering techniques. Some of them I've highlighted in the slides above, but also this is a very large space, and I would just advise you to look for prompt engineering techniques online. There's a lot to cover there. Experiment with few short examples. What this refers to is you don't just want to tell, you want to show whenever it's possible, so give it examples of everything that helps it really understand what you mean if you can. Experiment with tools and plugins to offload a task that are difficult for LLMs natively, and then think about not just a single prompt and answer, think about potential chains and reflection and how you glue them together and how you could potentially make multiple samples and so on. Finally, if you think you've squeezed out prompt engineering, which I think you should stick with for a while, look at some potentially fine-tuning a model to your application, but expect this to be a lot more slower and involved. And then there's an expert fragile research zone here, and I would say that is RLHF, which currently does work a bit better than SFD if you can get it to work, but again, this is pretty involved, I would say. And to optimize your costs, try to explore lower capacity models or shorter prompts and so on. I also wanted to say a few words about the use cases in which I think LLMs are currently well suited for. So in particular, note that there's a large number of limitations to LLMs today, and so I would keep that definitely in mind for all your applications. Models, and this, by the way, could be an entire talk, so I don't have time to cover it in full detail. Models may be biased, they may fabricate hallucinate information, they may have reasoning errors, they may struggle in entire classes of applications, they have knowledge cutoffs, so they might not know any information above, say, September 2021. They are susceptible to a large range of attacks, which are sort of like coming out on Twitter daily, including prompt injection, jailbreak attacks, data poisoning attacks, and so on. So my recommendation right now is use LLMs in low stakes applications, combine them with always with human oversight, use them as a source of inspiration and suggestions, and think co-pilots instead of completely autonomous agents that are just like performing a task somewhere. It's just not clear that the models are there right now. So I wanted to close by saying that GPT-4 is an amazing artifact. I'm very thankful that it exists, and it's beautiful, it has a ton of knowledge across so many areas, it can do math, code, and so on. And in addition, there's this thriving ecosystem of everything else that is being built and incorporated into the ecosystem, some of these things I've talked about. And all of this power is accessible at your fingertips. So here's everything that's needed in terms of code to ask GPT-4 a question, to prompt it and get a response. In this case, I said, can you say something to inspire the audience of Microsoft Build 2023? And I just punched this into Python and verbatim, GPT-4 said the following. And by the way, I did not know that they used this trick in the keynote, so I thought I was being clever, but it is really good at this, it says, ladies and gentlemen, innovators and trail bearers of Microsoft Build 2023, welcome to the gathering of brilliant minds like no other. You are the architects of the future, the visionaries molding the digital realm in which humanity thrives, embrace the limitless possibilities of technologies and let your ideas soar as high as your imagination. Together, let's create a more connected, remarkable and inclusive world for generations to come, get ready to unleash your creativity, canvass the unknown, and turn dreams into reality. Your journey begins today. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.44, "text": " Please welcome AI researcher and founding member of OpenAI, Andre Karpathy.", "tokens": [50364, 2555, 2928, 7318, 21751, 293, 22223, 4006, 295, 7238, 48698, 11, 20667, 591, 6529, 9527, 13, 51236], "temperature": 0.0, "avg_logprob": -0.33521556854248047, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.008958077989518642}, {"id": 1, "seek": 0, "start": 17.44, "end": 23.64, "text": " Hi, everyone.", "tokens": [51236, 2421, 11, 1518, 13, 51546], "temperature": 0.0, "avg_logprob": -0.33521556854248047, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.008958077989518642}, {"id": 2, "seek": 0, "start": 23.64, "end": 28.88, "text": " I'm happy to be here to tell you about the state of GPT and more generally about the", "tokens": [51546, 286, 478, 2055, 281, 312, 510, 281, 980, 291, 466, 264, 1785, 295, 26039, 51, 293, 544, 5101, 466, 264, 51808], "temperature": 0.0, "avg_logprob": -0.33521556854248047, "compression_ratio": 1.27007299270073, "no_speech_prob": 0.008958077989518642}, {"id": 3, "seek": 2888, "start": 28.88, "end": 32.16, "text": " rapidly growing ecosystem of large language models.", "tokens": [50364, 12910, 4194, 11311, 295, 2416, 2856, 5245, 13, 50528], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 4, "seek": 2888, "start": 32.16, "end": 35.4, "text": " So I would like to partition the talk into two parts.", "tokens": [50528, 407, 286, 576, 411, 281, 24808, 264, 751, 666, 732, 3166, 13, 50690], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 5, "seek": 2888, "start": 35.4, "end": 39.6, "text": " In the first part, I would like to tell you about how we train GPT assistants.", "tokens": [50690, 682, 264, 700, 644, 11, 286, 576, 411, 281, 980, 291, 466, 577, 321, 3847, 26039, 51, 34949, 13, 50900], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 6, "seek": 2888, "start": 39.6, "end": 43.76, "text": " And then in the second part, we are going to take a look at how we can use these assistants", "tokens": [50900, 400, 550, 294, 264, 1150, 644, 11, 321, 366, 516, 281, 747, 257, 574, 412, 577, 321, 393, 764, 613, 34949, 51108], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 7, "seek": 2888, "start": 43.76, "end": 46.76, "text": " effectively for your applications.", "tokens": [51108, 8659, 337, 428, 5821, 13, 51258], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 8, "seek": 2888, "start": 46.76, "end": 50.519999999999996, "text": " So first, let's take a look at the emerging recipe for how to train these assistants and", "tokens": [51258, 407, 700, 11, 718, 311, 747, 257, 574, 412, 264, 14989, 6782, 337, 577, 281, 3847, 613, 34949, 293, 51446], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 9, "seek": 2888, "start": 50.519999999999996, "end": 53.2, "text": " keep in mind that this is all very new and still rapidly evolving.", "tokens": [51446, 1066, 294, 1575, 300, 341, 307, 439, 588, 777, 293, 920, 12910, 21085, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 10, "seek": 2888, "start": 53.2, "end": 55.4, "text": " But so far, the recipe looks something like this.", "tokens": [51580, 583, 370, 1400, 11, 264, 6782, 1542, 746, 411, 341, 13, 51690], "temperature": 0.0, "avg_logprob": -0.14981322401151884, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.09064796566963196}, {"id": 11, "seek": 5540, "start": 55.879999999999995, "end": 60.0, "text": " Now, this is kind of a complicated slide, so I'm going to go through it piece by piece.", "tokens": [50388, 823, 11, 341, 307, 733, 295, 257, 6179, 4137, 11, 370, 286, 478, 516, 281, 352, 807, 309, 2522, 538, 2522, 13, 50594], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 12, "seek": 5540, "start": 60.0, "end": 63.4, "text": " But roughly speaking, we have four major stages.", "tokens": [50594, 583, 9810, 4124, 11, 321, 362, 1451, 2563, 10232, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 13, "seek": 5540, "start": 63.4, "end": 67.8, "text": " Pre-training, supervised fine-tuning, reward modeling, reinforcement learning, and they", "tokens": [50764, 6001, 12, 17227, 1760, 11, 46533, 2489, 12, 83, 37726, 11, 7782, 15983, 11, 29280, 2539, 11, 293, 436, 50984], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 14, "seek": 5540, "start": 67.8, "end": 70.0, "text": " follow each other serially.", "tokens": [50984, 1524, 1184, 661, 816, 2270, 13, 51094], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 15, "seek": 5540, "start": 70.0, "end": 74.84, "text": " Now, in each stage, we have a data set that powers that stage.", "tokens": [51094, 823, 11, 294, 1184, 3233, 11, 321, 362, 257, 1412, 992, 300, 8674, 300, 3233, 13, 51336], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 16, "seek": 5540, "start": 74.84, "end": 81.48, "text": " We have an algorithm that, for our purposes, will be an objective for training a neural", "tokens": [51336, 492, 362, 364, 9284, 300, 11, 337, 527, 9932, 11, 486, 312, 364, 10024, 337, 3097, 257, 18161, 51668], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 17, "seek": 5540, "start": 81.48, "end": 82.28, "text": " network.", "tokens": [51668, 3209, 13, 51708], "temperature": 0.0, "avg_logprob": -0.16231813343293075, "compression_ratio": 1.6220472440944882, "no_speech_prob": 0.004254218190908432}, {"id": 18, "seek": 8228, "start": 82.28, "end": 86.0, "text": " And then we have a resulting model, and then there's some nodes on the bottom.", "tokens": [50364, 400, 550, 321, 362, 257, 16505, 2316, 11, 293, 550, 456, 311, 512, 13891, 322, 264, 2767, 13, 50550], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 19, "seek": 8228, "start": 86.0, "end": 88.8, "text": " So the first stage we're going to start with is the pre-training stage.", "tokens": [50550, 407, 264, 700, 3233, 321, 434, 516, 281, 722, 365, 307, 264, 659, 12, 17227, 1760, 3233, 13, 50690], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 20, "seek": 8228, "start": 88.8, "end": 93.64, "text": " Now, this stage is kind of special in this diagram, and this diagram is not to scale,", "tokens": [50690, 823, 11, 341, 3233, 307, 733, 295, 2121, 294, 341, 10686, 11, 293, 341, 10686, 307, 406, 281, 4373, 11, 50932], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 21, "seek": 8228, "start": 93.64, "end": 96.48, "text": " because this stage is where all of the computational work basically happens.", "tokens": [50932, 570, 341, 3233, 307, 689, 439, 295, 264, 28270, 589, 1936, 2314, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 22, "seek": 8228, "start": 96.48, "end": 102.04, "text": " This is 99% of the training compute time and also flops.", "tokens": [51074, 639, 307, 11803, 4, 295, 264, 3097, 14722, 565, 293, 611, 932, 3370, 13, 51352], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 23, "seek": 8228, "start": 102.04, "end": 106.68, "text": " And so this is where we are dealing with internet-scale data sets with thousands of", "tokens": [51352, 400, 370, 341, 307, 689, 321, 366, 6260, 365, 4705, 12, 20033, 1412, 6352, 365, 5383, 295, 51584], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 24, "seek": 8228, "start": 106.68, "end": 111.44, "text": " GPUs in the supercomputer and also months of training, potentially.", "tokens": [51584, 18407, 82, 294, 264, 36708, 293, 611, 2493, 295, 3097, 11, 7263, 13, 51822], "temperature": 0.0, "avg_logprob": -0.1400653927825218, "compression_ratio": 1.7938144329896908, "no_speech_prob": 0.001955878222361207}, {"id": 25, "seek": 11144, "start": 111.44, "end": 116.16, "text": " The other three stages are fine-tuning stages that are much more along the lines of small", "tokens": [50364, 440, 661, 1045, 10232, 366, 2489, 12, 83, 37726, 10232, 300, 366, 709, 544, 2051, 264, 3876, 295, 1359, 50600], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 26, "seek": 11144, "start": 116.16, "end": 119.44, "text": " few number of GPUs and hours or days.", "tokens": [50600, 1326, 1230, 295, 18407, 82, 293, 2496, 420, 1708, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 27, "seek": 11144, "start": 119.44, "end": 124.24, "text": " So let's take a look at the pre-training stage to achieve a base model.", "tokens": [50764, 407, 718, 311, 747, 257, 574, 412, 264, 659, 12, 17227, 1760, 3233, 281, 4584, 257, 3096, 2316, 13, 51004], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 28, "seek": 11144, "start": 124.24, "end": 127.88, "text": " First we're going to gather a large amount of data.", "tokens": [51004, 2386, 321, 434, 516, 281, 5448, 257, 2416, 2372, 295, 1412, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 29, "seek": 11144, "start": 127.88, "end": 132.84, "text": " Here's an example of what we call a data mixture that comes from this paper that was", "tokens": [51186, 1692, 311, 364, 1365, 295, 437, 321, 818, 257, 1412, 9925, 300, 1487, 490, 341, 3035, 300, 390, 51434], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 30, "seek": 11144, "start": 132.84, "end": 136.4, "text": " released by Meta, where they released this Lama-based model.", "tokens": [51434, 4736, 538, 6377, 64, 11, 689, 436, 4736, 341, 441, 2404, 12, 6032, 2316, 13, 51612], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 31, "seek": 11144, "start": 136.4, "end": 140.6, "text": " Now, you can see roughly the kinds of data sets that enter into these collections.", "tokens": [51612, 823, 11, 291, 393, 536, 9810, 264, 3685, 295, 1412, 6352, 300, 3242, 666, 613, 16641, 13, 51822], "temperature": 0.0, "avg_logprob": -0.1260585179404607, "compression_ratio": 1.6494845360824741, "no_speech_prob": 0.003323635319247842}, {"id": 32, "seek": 14060, "start": 140.6, "end": 145.51999999999998, "text": " So we have common crawl, which is just a web scrape, C4, which is also a common crawl,", "tokens": [50364, 407, 321, 362, 2689, 24767, 11, 597, 307, 445, 257, 3670, 32827, 11, 383, 19, 11, 597, 307, 611, 257, 2689, 24767, 11, 50610], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 33, "seek": 14060, "start": 145.51999999999998, "end": 147.4, "text": " and then some high-quality data sets as well.", "tokens": [50610, 293, 550, 512, 1090, 12, 11286, 1412, 6352, 382, 731, 13, 50704], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 34, "seek": 14060, "start": 147.4, "end": 151.64, "text": " So for example, GitHub, Wikipedia, Books, Archive, Stack Exchange, and so on.", "tokens": [50704, 407, 337, 1365, 11, 23331, 11, 28999, 11, 33843, 11, 10984, 488, 11, 37649, 31169, 11, 293, 370, 322, 13, 50916], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 35, "seek": 14060, "start": 151.64, "end": 156.72, "text": " These are all mixed up together, and then they are sampled according to some given proportions,", "tokens": [50916, 1981, 366, 439, 7467, 493, 1214, 11, 293, 550, 436, 366, 3247, 15551, 4650, 281, 512, 2212, 32482, 11, 51170], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 36, "seek": 14060, "start": 156.72, "end": 161.16, "text": " and that forms the training set for the neural net for the GPT.", "tokens": [51170, 293, 300, 6422, 264, 3097, 992, 337, 264, 18161, 2533, 337, 264, 26039, 51, 13, 51392], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 37, "seek": 14060, "start": 161.16, "end": 165.07999999999998, "text": " Now before we can actually train on this data, we need to go through one more pre-processing", "tokens": [51392, 823, 949, 321, 393, 767, 3847, 322, 341, 1412, 11, 321, 643, 281, 352, 807, 472, 544, 659, 12, 41075, 278, 51588], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 38, "seek": 14060, "start": 165.07999999999998, "end": 166.95999999999998, "text": " step, and that is tokenization.", "tokens": [51588, 1823, 11, 293, 300, 307, 14862, 2144, 13, 51682], "temperature": 0.0, "avg_logprob": -0.13850068687496328, "compression_ratio": 1.6390728476821192, "no_speech_prob": 0.005058707669377327}, {"id": 39, "seek": 16696, "start": 166.96, "end": 171.56, "text": " And this is basically a translation of the raw text that we scrape from the internet into", "tokens": [50364, 400, 341, 307, 1936, 257, 12853, 295, 264, 8936, 2487, 300, 321, 32827, 490, 264, 4705, 666, 50594], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 40, "seek": 16696, "start": 171.56, "end": 177.64000000000001, "text": " sequences of integers, because that's the native representation over which GPTs function.", "tokens": [50594, 22978, 295, 41674, 11, 570, 300, 311, 264, 8470, 10290, 670, 597, 26039, 33424, 2445, 13, 50898], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 41, "seek": 16696, "start": 177.64000000000001, "end": 183.88, "text": " Now this is a lossless kind of translation between pieces of text and tokens and integers,", "tokens": [50898, 823, 341, 307, 257, 4470, 1832, 733, 295, 12853, 1296, 3755, 295, 2487, 293, 22667, 293, 41674, 11, 51210], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 42, "seek": 16696, "start": 183.88, "end": 185.88, "text": " and there are a number of algorithms for this stage.", "tokens": [51210, 293, 456, 366, 257, 1230, 295, 14642, 337, 341, 3233, 13, 51310], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 43, "seek": 16696, "start": 185.88, "end": 189.48000000000002, "text": " Typically, for example, you could use something like byte bearing coding, which iteratively", "tokens": [51310, 23129, 11, 337, 1365, 11, 291, 727, 764, 746, 411, 40846, 17350, 17720, 11, 597, 17138, 19020, 51490], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 44, "seek": 16696, "start": 189.48000000000002, "end": 194.0, "text": " merges little text chunks and groups them into tokens.", "tokens": [51490, 3551, 2880, 707, 2487, 24004, 293, 3935, 552, 666, 22667, 13, 51716], "temperature": 0.0, "avg_logprob": -0.13407360590421236, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.001225303509272635}, {"id": 45, "seek": 19400, "start": 194.0, "end": 198.48, "text": " And so here I'm showing some example chunks of these tokens, and then this is the raw integer", "tokens": [50364, 400, 370, 510, 286, 478, 4099, 512, 1365, 24004, 295, 613, 22667, 11, 293, 550, 341, 307, 264, 8936, 24922, 50588], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 46, "seek": 19400, "start": 198.48, "end": 202.16, "text": " sequence that will actually feed into a transformer.", "tokens": [50588, 8310, 300, 486, 767, 3154, 666, 257, 31782, 13, 50772], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 47, "seek": 19400, "start": 202.16, "end": 208.28, "text": " Now here I'm showing two sort of like examples for hyper-primers that govern this stage.", "tokens": [50772, 823, 510, 286, 478, 4099, 732, 1333, 295, 411, 5110, 337, 9848, 12, 1424, 332, 433, 300, 1980, 341, 3233, 13, 51078], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 48, "seek": 19400, "start": 208.28, "end": 212.4, "text": " So GPT-4, we did not release too much information about how it was trained and so on, so I'm", "tokens": [51078, 407, 26039, 51, 12, 19, 11, 321, 630, 406, 4374, 886, 709, 1589, 466, 577, 309, 390, 8895, 293, 370, 322, 11, 370, 286, 478, 51284], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 49, "seek": 19400, "start": 212.4, "end": 217.04, "text": " using GPT-3's numbers, but GPT-3 is of course a little bit old by now, about three years", "tokens": [51284, 1228, 26039, 51, 12, 18, 311, 3547, 11, 457, 26039, 51, 12, 18, 307, 295, 1164, 257, 707, 857, 1331, 538, 586, 11, 466, 1045, 924, 51516], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 50, "seek": 19400, "start": 217.04, "end": 218.04, "text": " ago.", "tokens": [51516, 2057, 13, 51566], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 51, "seek": 19400, "start": 218.04, "end": 220.84, "text": " But Lama is a fairly recent model from Meta.", "tokens": [51566, 583, 441, 2404, 307, 257, 6457, 5162, 2316, 490, 6377, 64, 13, 51706], "temperature": 0.0, "avg_logprob": -0.13934966808057014, "compression_ratio": 1.583050847457627, "no_speech_prob": 0.0005613889661617577}, {"id": 52, "seek": 22084, "start": 220.84, "end": 223.84, "text": " So these are roughly the orders of magnitude that we're dealing with when we're doing", "tokens": [50364, 407, 613, 366, 9810, 264, 9470, 295, 15668, 300, 321, 434, 6260, 365, 562, 321, 434, 884, 50514], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 53, "seek": 22084, "start": 223.84, "end": 225.28, "text": " pre-training.", "tokens": [50514, 659, 12, 17227, 1760, 13, 50586], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 54, "seek": 22084, "start": 225.28, "end": 229.84, "text": " The vocabulary size is usually a couple 10,000 tokens, the context length is usually something", "tokens": [50586, 440, 19864, 2744, 307, 2673, 257, 1916, 1266, 11, 1360, 22667, 11, 264, 4319, 4641, 307, 2673, 746, 50814], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 55, "seek": 22084, "start": 229.84, "end": 236.36, "text": " like 2,000, 4,000, or nowadays even 100,000, and this governs the maximum number of integers", "tokens": [50814, 411, 568, 11, 1360, 11, 1017, 11, 1360, 11, 420, 13434, 754, 2319, 11, 1360, 11, 293, 341, 1980, 82, 264, 6674, 1230, 295, 41674, 51140], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 56, "seek": 22084, "start": 236.36, "end": 242.32, "text": " that the GPT will look at when it's trying to predict the next integer in a sequence.", "tokens": [51140, 300, 264, 26039, 51, 486, 574, 412, 562, 309, 311, 1382, 281, 6069, 264, 958, 24922, 294, 257, 8310, 13, 51438], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 57, "seek": 22084, "start": 242.32, "end": 246.48000000000002, "text": " You can see that roughly the number of parameters is, say, 65 billion for Lama.", "tokens": [51438, 509, 393, 536, 300, 9810, 264, 1230, 295, 9834, 307, 11, 584, 11, 11624, 5218, 337, 441, 2404, 13, 51646], "temperature": 0.0, "avg_logprob": -0.12102312152668582, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.00023047073045745492}, {"id": 58, "seek": 24648, "start": 246.48, "end": 251.64, "text": " Now even though Lama has only 65B parameters compared to GPT-3's 175 billion parameters,", "tokens": [50364, 823, 754, 1673, 441, 2404, 575, 787, 11624, 33, 9834, 5347, 281, 26039, 51, 12, 18, 311, 41165, 5218, 9834, 11, 50622], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 59, "seek": 24648, "start": 251.64, "end": 256.32, "text": " Lama is a significantly more powerful model, and intuitively that's because the model is", "tokens": [50622, 441, 2404, 307, 257, 10591, 544, 4005, 2316, 11, 293, 46506, 300, 311, 570, 264, 2316, 307, 50856], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 60, "seek": 24648, "start": 256.32, "end": 260.76, "text": " trained for significantly longer, in this case 1.4 trillion tokens instead of just 300", "tokens": [50856, 8895, 337, 10591, 2854, 11, 294, 341, 1389, 502, 13, 19, 18723, 22667, 2602, 295, 445, 6641, 51078], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 61, "seek": 24648, "start": 260.76, "end": 261.84, "text": " billion tokens.", "tokens": [51078, 5218, 22667, 13, 51132], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 62, "seek": 24648, "start": 261.84, "end": 265.12, "text": " So you shouldn't judge the power of a model just by the number of parameters that it", "tokens": [51132, 407, 291, 4659, 380, 6995, 264, 1347, 295, 257, 2316, 445, 538, 264, 1230, 295, 9834, 300, 309, 51296], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 63, "seek": 24648, "start": 265.12, "end": 267.12, "text": " contains.", "tokens": [51296, 8306, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 64, "seek": 24648, "start": 267.12, "end": 273.08, "text": " Below I'm showing some tables of rough hyper-parameters that typically go into specifying the transformer", "tokens": [51396, 36261, 286, 478, 4099, 512, 8020, 295, 5903, 9848, 12, 2181, 335, 6202, 300, 5850, 352, 666, 1608, 5489, 264, 31782, 51694], "temperature": 0.0, "avg_logprob": -0.14776337665060293, "compression_ratio": 1.6586206896551725, "no_speech_prob": 0.00031992781441658735}, {"id": 65, "seek": 27308, "start": 273.08, "end": 277.91999999999996, "text": " neural network, so the number of heads, the dimension size, number of layers, and so on.", "tokens": [50364, 18161, 3209, 11, 370, 264, 1230, 295, 8050, 11, 264, 10139, 2744, 11, 1230, 295, 7914, 11, 293, 370, 322, 13, 50606], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 66, "seek": 27308, "start": 277.91999999999996, "end": 281.64, "text": " And on the bottom I'm showing some training hyper-parameters.", "tokens": [50606, 400, 322, 264, 2767, 286, 478, 4099, 512, 3097, 9848, 12, 2181, 335, 6202, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 67, "seek": 27308, "start": 281.64, "end": 290.03999999999996, "text": " So for example, to train the 65B model, Meta used 2,000 GPUs, roughly 21 days of training,", "tokens": [50792, 407, 337, 1365, 11, 281, 3847, 264, 11624, 33, 2316, 11, 6377, 64, 1143, 568, 11, 1360, 18407, 82, 11, 9810, 5080, 1708, 295, 3097, 11, 51212], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 68, "seek": 27308, "start": 290.03999999999996, "end": 294.24, "text": " and roughly several million dollars, and so that's the rough orders of magnitude that", "tokens": [51212, 293, 9810, 2940, 2459, 3808, 11, 293, 370, 300, 311, 264, 5903, 9470, 295, 15668, 300, 51422], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 69, "seek": 27308, "start": 294.24, "end": 299.12, "text": " you should have in mind for the pre-training stage.", "tokens": [51422, 291, 820, 362, 294, 1575, 337, 264, 659, 12, 17227, 1760, 3233, 13, 51666], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 70, "seek": 27308, "start": 299.12, "end": 302.32, "text": " Now when we're actually pre-training what happens, roughly speaking, we are going to", "tokens": [51666, 823, 562, 321, 434, 767, 659, 12, 17227, 1760, 437, 2314, 11, 9810, 4124, 11, 321, 366, 516, 281, 51826], "temperature": 0.0, "avg_logprob": -0.14523983001708984, "compression_ratio": 1.6395759717314489, "no_speech_prob": 0.008843614719808102}, {"id": 71, "seek": 30232, "start": 302.32, "end": 306.15999999999997, "text": " take our tokens and we're going to lay them out into data batches.", "tokens": [50364, 747, 527, 22667, 293, 321, 434, 516, 281, 2360, 552, 484, 666, 1412, 15245, 279, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 72, "seek": 30232, "start": 306.15999999999997, "end": 310.92, "text": " So we have these arrays that will feed into the transformer, and these arrays are B, the", "tokens": [50556, 407, 321, 362, 613, 41011, 300, 486, 3154, 666, 264, 31782, 11, 293, 613, 41011, 366, 363, 11, 264, 50794], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 73, "seek": 30232, "start": 310.92, "end": 316.68, "text": " batch size, and these are all independent examples stacked up in rows, and B by T, T being the", "tokens": [50794, 15245, 2744, 11, 293, 613, 366, 439, 6695, 5110, 28867, 493, 294, 13241, 11, 293, 363, 538, 314, 11, 314, 885, 264, 51082], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 74, "seek": 30232, "start": 316.68, "end": 318.12, "text": " maximum context length.", "tokens": [51082, 6674, 4319, 4641, 13, 51154], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 75, "seek": 30232, "start": 318.12, "end": 323.15999999999997, "text": " So in my picture I only have 10 of the context lengths, so this could be 2,000, 4,000, etc.", "tokens": [51154, 407, 294, 452, 3036, 286, 787, 362, 1266, 295, 264, 4319, 26329, 11, 370, 341, 727, 312, 568, 11, 1360, 11, 1017, 11, 1360, 11, 5183, 13, 51406], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 76, "seek": 30232, "start": 323.15999999999997, "end": 325.12, "text": " So these are extremely long rows.", "tokens": [51406, 407, 613, 366, 4664, 938, 13241, 13, 51504], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 77, "seek": 30232, "start": 325.12, "end": 329.12, "text": " And what we do is we take these documents and we pack them into rows, and we delimit them", "tokens": [51504, 400, 437, 321, 360, 307, 321, 747, 613, 8512, 293, 321, 2844, 552, 666, 13241, 11, 293, 321, 1103, 332, 270, 552, 51704], "temperature": 0.0, "avg_logprob": -0.12582621180025258, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0037045644130557775}, {"id": 78, "seek": 32912, "start": 329.12, "end": 333.76, "text": " with these special end-of-text tokens, basically telling the transformer where a new document", "tokens": [50364, 365, 613, 2121, 917, 12, 2670, 12, 25111, 22667, 11, 1936, 3585, 264, 31782, 689, 257, 777, 4166, 50596], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 79, "seek": 32912, "start": 333.76, "end": 335.28000000000003, "text": " begins.", "tokens": [50596, 7338, 13, 50672], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 80, "seek": 32912, "start": 335.28000000000003, "end": 340.76, "text": " And so here I have a few examples of documents, and then I've stretched them out into this", "tokens": [50672, 400, 370, 510, 286, 362, 257, 1326, 5110, 295, 8512, 11, 293, 550, 286, 600, 23563, 552, 484, 666, 341, 50946], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 81, "seek": 32912, "start": 340.76, "end": 341.76, "text": " input.", "tokens": [50946, 4846, 13, 50996], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 82, "seek": 32912, "start": 341.76, "end": 348.28000000000003, "text": " Now, we're going to feed all of these numbers into Transformer, and let me just focus on", "tokens": [50996, 823, 11, 321, 434, 516, 281, 3154, 439, 295, 613, 3547, 666, 27938, 260, 11, 293, 718, 385, 445, 1879, 322, 51322], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 83, "seek": 32912, "start": 348.28000000000003, "end": 352.72, "text": " a single particular cell, but the same thing will happen at every cell in this diagram.", "tokens": [51322, 257, 2167, 1729, 2815, 11, 457, 264, 912, 551, 486, 1051, 412, 633, 2815, 294, 341, 10686, 13, 51544], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 84, "seek": 32912, "start": 352.72, "end": 354.72, "text": " So let's look at the green cell.", "tokens": [51544, 407, 718, 311, 574, 412, 264, 3092, 2815, 13, 51644], "temperature": 0.0, "avg_logprob": -0.13255184101608564, "compression_ratio": 1.5914396887159532, "no_speech_prob": 0.0008293803548440337}, {"id": 85, "seek": 35472, "start": 354.72, "end": 359.16, "text": " The green cell is going to take a look at all of the tokens before it, so all of the", "tokens": [50364, 440, 3092, 2815, 307, 516, 281, 747, 257, 574, 412, 439, 295, 264, 22667, 949, 309, 11, 370, 439, 295, 264, 50586], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 86, "seek": 35472, "start": 359.16, "end": 364.34000000000003, "text": " tokens in yellow, and we're going to feed that entire context into the transformer neural", "tokens": [50586, 22667, 294, 5566, 11, 293, 321, 434, 516, 281, 3154, 300, 2302, 4319, 666, 264, 31782, 18161, 50845], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 87, "seek": 35472, "start": 364.34000000000003, "end": 369.04, "text": " network, and the transformer is going to try to predict the next token in a sequence,", "tokens": [50845, 3209, 11, 293, 264, 31782, 307, 516, 281, 853, 281, 6069, 264, 958, 14862, 294, 257, 8310, 11, 51080], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 88, "seek": 35472, "start": 369.04, "end": 370.64000000000004, "text": " in this case in red.", "tokens": [51080, 294, 341, 1389, 294, 2182, 13, 51160], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 89, "seek": 35472, "start": 370.64000000000004, "end": 373.84000000000003, "text": " Now the transformer, I don't have too much time to unfortunately go into the full details", "tokens": [51160, 823, 264, 31782, 11, 286, 500, 380, 362, 886, 709, 565, 281, 7015, 352, 666, 264, 1577, 4365, 51320], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 90, "seek": 35472, "start": 373.84000000000003, "end": 379.12, "text": " of this neural network architecture, is just a large blob of neural net stuff for our purposes,", "tokens": [51320, 295, 341, 18161, 3209, 9482, 11, 307, 445, 257, 2416, 46115, 295, 18161, 2533, 1507, 337, 527, 9932, 11, 51584], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 91, "seek": 35472, "start": 379.12, "end": 382.64000000000004, "text": " and it's got several 10 billion parameters typically, or something like that.", "tokens": [51584, 293, 309, 311, 658, 2940, 1266, 5218, 9834, 5850, 11, 420, 746, 411, 300, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11564675111037034, "compression_ratio": 1.835016835016835, "no_speech_prob": 0.00032499476219527423}, {"id": 92, "seek": 38264, "start": 382.64, "end": 385.84, "text": " And of course, as you tune these parameters, you're getting slightly different predicted", "tokens": [50364, 400, 295, 1164, 11, 382, 291, 10864, 613, 9834, 11, 291, 434, 1242, 4748, 819, 19147, 50524], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 93, "seek": 38264, "start": 385.84, "end": 389.28, "text": " distributions for every single one of these cells.", "tokens": [50524, 37870, 337, 633, 2167, 472, 295, 613, 5438, 13, 50696], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 94, "seek": 38264, "start": 389.28, "end": 395.03999999999996, "text": " And so, for example, if our vocabulary size is 50,257 tokens, then we're going to have", "tokens": [50696, 400, 370, 11, 337, 1365, 11, 498, 527, 19864, 2744, 307, 2625, 11, 6074, 22, 22667, 11, 550, 321, 434, 516, 281, 362, 50984], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 95, "seek": 38264, "start": 395.03999999999996, "end": 399.59999999999997, "text": " that many numbers because we need to specify a probability distribution for what comes", "tokens": [50984, 300, 867, 3547, 570, 321, 643, 281, 16500, 257, 8482, 7316, 337, 437, 1487, 51212], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 96, "seek": 38264, "start": 399.59999999999997, "end": 400.59999999999997, "text": " next.", "tokens": [51212, 958, 13, 51262], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 97, "seek": 38264, "start": 400.59999999999997, "end": 403.15999999999997, "text": " So basically we have a probability for whatever may follow.", "tokens": [51262, 407, 1936, 321, 362, 257, 8482, 337, 2035, 815, 1524, 13, 51390], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 98, "seek": 38264, "start": 403.15999999999997, "end": 408.12, "text": " Now in this specific example for this specific cell, 513 will come next, and so we can use", "tokens": [51390, 823, 294, 341, 2685, 1365, 337, 341, 2685, 2815, 11, 1025, 7668, 486, 808, 958, 11, 293, 370, 321, 393, 764, 51638], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 99, "seek": 38264, "start": 408.12, "end": 411.71999999999997, "text": " this as a source of supervision to update our transformer's weights.", "tokens": [51638, 341, 382, 257, 4009, 295, 32675, 281, 5623, 527, 31782, 311, 17443, 13, 51818], "temperature": 0.0, "avg_logprob": -0.13475050137737604, "compression_ratio": 1.7003154574132493, "no_speech_prob": 0.0002868149022106081}, {"id": 100, "seek": 41172, "start": 411.76000000000005, "end": 415.92, "text": " And so we're applying this basically on every single cell in parallel, and we keep swapping", "tokens": [50366, 400, 370, 321, 434, 9275, 341, 1936, 322, 633, 2167, 2815, 294, 8952, 11, 293, 321, 1066, 1693, 10534, 50574], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 101, "seek": 41172, "start": 415.92, "end": 419.96000000000004, "text": " batches, and we're trying to get the transformer to make the correct predictions over what", "tokens": [50574, 15245, 279, 11, 293, 321, 434, 1382, 281, 483, 264, 31782, 281, 652, 264, 3006, 21264, 670, 437, 50776], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 102, "seek": 41172, "start": 419.96000000000004, "end": 422.44000000000005, "text": " token comes next in a sequence.", "tokens": [50776, 14862, 1487, 958, 294, 257, 8310, 13, 50900], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 103, "seek": 41172, "start": 422.44000000000005, "end": 425.8, "text": " So let me show you more concretely what this looks like when you train one of these models.", "tokens": [50900, 407, 718, 385, 855, 291, 544, 39481, 736, 437, 341, 1542, 411, 562, 291, 3847, 472, 295, 613, 5245, 13, 51068], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 104, "seek": 41172, "start": 425.8, "end": 431.16, "text": " This is actually coming from New York Times, and they trained a small GPT on Shakespeare,", "tokens": [51068, 639, 307, 767, 1348, 490, 1873, 3609, 11366, 11, 293, 436, 8895, 257, 1359, 26039, 51, 322, 22825, 11, 51336], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 105, "seek": 41172, "start": 431.16, "end": 434.72, "text": " and so here's a small snippet of Shakespeare, and they trained a GPT on it.", "tokens": [51336, 293, 370, 510, 311, 257, 1359, 35623, 302, 295, 22825, 11, 293, 436, 8895, 257, 26039, 51, 322, 309, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 106, "seek": 41172, "start": 434.72, "end": 439.36, "text": " Now in the beginning at initialization, the GPT starts with completely random weights,", "tokens": [51514, 823, 294, 264, 2863, 412, 5883, 2144, 11, 264, 26039, 51, 3719, 365, 2584, 4974, 17443, 11, 51746], "temperature": 0.0, "avg_logprob": -0.09692842820111443, "compression_ratio": 1.709480122324159, "no_speech_prob": 0.0005613945540972054}, {"id": 107, "seek": 43936, "start": 439.40000000000003, "end": 442.6, "text": " so you're just getting completely random outputs as well.", "tokens": [50366, 370, 291, 434, 445, 1242, 2584, 4974, 23930, 382, 731, 13, 50526], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 108, "seek": 43936, "start": 442.6, "end": 447.52000000000004, "text": " But over time, as you train the GPT longer and longer, you are getting more and more", "tokens": [50526, 583, 670, 565, 11, 382, 291, 3847, 264, 26039, 51, 2854, 293, 2854, 11, 291, 366, 1242, 544, 293, 544, 50772], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 109, "seek": 43936, "start": 447.52000000000004, "end": 451.72, "text": " coherent and consistent sort of samples from the model.", "tokens": [50772, 36239, 293, 8398, 1333, 295, 10938, 490, 264, 2316, 13, 50982], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 110, "seek": 43936, "start": 451.72, "end": 455.96000000000004, "text": " And the way you sample from it, of course, is you predict what comes next, you sample", "tokens": [50982, 400, 264, 636, 291, 6889, 490, 309, 11, 295, 1164, 11, 307, 291, 6069, 437, 1487, 958, 11, 291, 6889, 51194], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 111, "seek": 43936, "start": 455.96000000000004, "end": 460.6, "text": " from that distribution, and you keep feeding that back into the process, and you can basically", "tokens": [51194, 490, 300, 7316, 11, 293, 291, 1066, 12919, 300, 646, 666, 264, 1399, 11, 293, 291, 393, 1936, 51426], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 112, "seek": 43936, "start": 460.6, "end": 462.24, "text": " sample large sequences.", "tokens": [51426, 6889, 2416, 22978, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 113, "seek": 43936, "start": 462.24, "end": 465.88, "text": " And so by the end, you see that the transformer has learned about words and where to put spaces", "tokens": [51508, 400, 370, 538, 264, 917, 11, 291, 536, 300, 264, 31782, 575, 3264, 466, 2283, 293, 689, 281, 829, 7673, 51690], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 114, "seek": 43936, "start": 465.88, "end": 468.40000000000003, "text": " and where to put commas and so on.", "tokens": [51690, 293, 689, 281, 829, 800, 296, 293, 370, 322, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11180959221060949, "compression_ratio": 1.804054054054054, "no_speech_prob": 9.314106137026101e-05}, {"id": 115, "seek": 46840, "start": 468.4, "end": 471.79999999999995, "text": " And so we're making more and more consistent predictions over time.", "tokens": [50364, 400, 370, 321, 434, 1455, 544, 293, 544, 8398, 21264, 670, 565, 13, 50534], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 116, "seek": 46840, "start": 471.79999999999995, "end": 474.84, "text": " These are the kinds of plots that you're looking at when you're doing model pre-training.", "tokens": [50534, 1981, 366, 264, 3685, 295, 28609, 300, 291, 434, 1237, 412, 562, 291, 434, 884, 2316, 659, 12, 17227, 1760, 13, 50686], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 117, "seek": 46840, "start": 474.84, "end": 479.88, "text": " Effectively, we're looking at the loss function over time as you train, and low loss means", "tokens": [50686, 17764, 3413, 11, 321, 434, 1237, 412, 264, 4470, 2445, 670, 565, 382, 291, 3847, 11, 293, 2295, 4470, 1355, 50938], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 118, "seek": 46840, "start": 479.88, "end": 484.35999999999996, "text": " that our transformer is predicting the correct, is giving a higher probability to get the", "tokens": [50938, 300, 527, 31782, 307, 32884, 264, 3006, 11, 307, 2902, 257, 2946, 8482, 281, 483, 264, 51162], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 119, "seek": 46840, "start": 484.35999999999996, "end": 487.15999999999997, "text": " correct next integer in a sequence.", "tokens": [51162, 3006, 958, 24922, 294, 257, 8310, 13, 51302], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 120, "seek": 46840, "start": 487.15999999999997, "end": 491.12, "text": " Now what are we going to do with this model once we've trained it after a month?", "tokens": [51302, 823, 437, 366, 321, 516, 281, 360, 365, 341, 2316, 1564, 321, 600, 8895, 309, 934, 257, 1618, 30, 51500], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 121, "seek": 46840, "start": 491.12, "end": 496.76, "text": " Well the first thing that we noticed, we, the field, is that these models basically", "tokens": [51500, 1042, 264, 700, 551, 300, 321, 5694, 11, 321, 11, 264, 2519, 11, 307, 300, 613, 5245, 1936, 51782], "temperature": 0.0, "avg_logprob": -0.134112182030311, "compression_ratio": 1.7966666666666666, "no_speech_prob": 4.757314673042856e-05}, {"id": 122, "seek": 49676, "start": 496.76, "end": 501.4, "text": " in the process of language modeling learn very powerful, general representations, and", "tokens": [50364, 294, 264, 1399, 295, 2856, 15983, 1466, 588, 4005, 11, 2674, 33358, 11, 293, 50596], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 123, "seek": 49676, "start": 501.4, "end": 505.36, "text": " it's possible to very efficiently fine tune them for any arbitrary downstream task you", "tokens": [50596, 309, 311, 1944, 281, 588, 19621, 2489, 10864, 552, 337, 604, 23211, 30621, 5633, 291, 50794], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 124, "seek": 49676, "start": 505.36, "end": 506.64, "text": " might be interested in.", "tokens": [50794, 1062, 312, 3102, 294, 13, 50858], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 125, "seek": 49676, "start": 506.64, "end": 510.92, "text": " So as an example, if you're interested in sentiment classification, the approach used", "tokens": [50858, 407, 382, 364, 1365, 11, 498, 291, 434, 3102, 294, 16149, 21538, 11, 264, 3109, 1143, 51072], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 126, "seek": 49676, "start": 510.92, "end": 514.28, "text": " to be that you collect a bunch of positives and negatives, and then you train some kind", "tokens": [51072, 281, 312, 300, 291, 2500, 257, 3840, 295, 35127, 293, 40019, 11, 293, 550, 291, 3847, 512, 733, 51240], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 127, "seek": 49676, "start": 514.28, "end": 520.68, "text": " of an NLP model for that, but the new approach is ignore sentiment classification, go off", "tokens": [51240, 295, 364, 426, 45196, 2316, 337, 300, 11, 457, 264, 777, 3109, 307, 11200, 16149, 21538, 11, 352, 766, 51560], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 128, "seek": 49676, "start": 520.68, "end": 525.48, "text": " and do large language model pre-training, train a large transformer, and then you can", "tokens": [51560, 293, 360, 2416, 2856, 2316, 659, 12, 17227, 1760, 11, 3847, 257, 2416, 31782, 11, 293, 550, 291, 393, 51800], "temperature": 0.0, "avg_logprob": -0.11996256718870069, "compression_ratio": 1.82, "no_speech_prob": 0.027149079367518425}, {"id": 129, "seek": 52548, "start": 525.48, "end": 529.5600000000001, "text": " only, you may only have a few examples, and you can very efficiently fine tune your model", "tokens": [50364, 787, 11, 291, 815, 787, 362, 257, 1326, 5110, 11, 293, 291, 393, 588, 19621, 2489, 10864, 428, 2316, 50568], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 130, "seek": 52548, "start": 529.5600000000001, "end": 531.36, "text": " for that task.", "tokens": [50568, 337, 300, 5633, 13, 50658], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 131, "seek": 52548, "start": 531.36, "end": 533.76, "text": " And so this works very well in practice.", "tokens": [50658, 400, 370, 341, 1985, 588, 731, 294, 3124, 13, 50778], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 132, "seek": 52548, "start": 533.76, "end": 537.8000000000001, "text": " And the reason for this is that basically the transformer is forced to multitask a huge", "tokens": [50778, 400, 264, 1778, 337, 341, 307, 300, 1936, 264, 31782, 307, 7579, 281, 42338, 3863, 257, 2603, 50980], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 133, "seek": 52548, "start": 537.8000000000001, "end": 542.5600000000001, "text": " amount of tasks in the language modeling task, because just, just in terms of predicting", "tokens": [50980, 2372, 295, 9608, 294, 264, 2856, 15983, 5633, 11, 570, 445, 11, 445, 294, 2115, 295, 32884, 51218], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 134, "seek": 52548, "start": 542.5600000000001, "end": 546.96, "text": " the next token, it's forced to understand a lot about the structure of the, of the text", "tokens": [51218, 264, 958, 14862, 11, 309, 311, 7579, 281, 1223, 257, 688, 466, 264, 3877, 295, 264, 11, 295, 264, 2487, 51438], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 135, "seek": 52548, "start": 546.96, "end": 550.32, "text": " and all the different concepts therein.", "tokens": [51438, 293, 439, 264, 819, 10392, 456, 259, 13, 51606], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 136, "seek": 52548, "start": 550.32, "end": 551.32, "text": " So that was GPT-1.", "tokens": [51606, 407, 300, 390, 26039, 51, 12, 16, 13, 51656], "temperature": 0.0, "avg_logprob": -0.12774486541748048, "compression_ratio": 1.6398601398601398, "no_speech_prob": 0.00019715660891961306}, {"id": 137, "seek": 55132, "start": 551.32, "end": 555.6, "text": " Now around the time of GPT-2, people noticed that actually even better than fine tuning,", "tokens": [50364, 823, 926, 264, 565, 295, 26039, 51, 12, 17, 11, 561, 5694, 300, 767, 754, 1101, 813, 2489, 15164, 11, 50578], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 138, "seek": 55132, "start": 555.6, "end": 557.88, "text": " you can actually prompt these models very effectively.", "tokens": [50578, 291, 393, 767, 12391, 613, 5245, 588, 8659, 13, 50692], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 139, "seek": 55132, "start": 557.88, "end": 560.5600000000001, "text": " So these are language models, and they want to complete documents.", "tokens": [50692, 407, 613, 366, 2856, 5245, 11, 293, 436, 528, 281, 3566, 8512, 13, 50826], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 140, "seek": 55132, "start": 560.5600000000001, "end": 565.8000000000001, "text": " So you can actually trick them into performing tasks just by arranging these fake documents.", "tokens": [50826, 407, 291, 393, 767, 4282, 552, 666, 10205, 9608, 445, 538, 5539, 9741, 613, 7592, 8512, 13, 51088], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 141, "seek": 55132, "start": 565.8000000000001, "end": 569.9200000000001, "text": " So in this example, for, for example, we have some passage, and then we sort of like do", "tokens": [51088, 407, 294, 341, 1365, 11, 337, 11, 337, 1365, 11, 321, 362, 512, 11497, 11, 293, 550, 321, 1333, 295, 411, 360, 51294], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 142, "seek": 55132, "start": 569.9200000000001, "end": 574.9200000000001, "text": " QA, QA, QA, this is called a few-shot prompt, and then we do Q, and then as the transformer", "tokens": [51294, 1249, 32, 11, 1249, 32, 11, 1249, 32, 11, 341, 307, 1219, 257, 1326, 12, 18402, 12391, 11, 293, 550, 321, 360, 1249, 11, 293, 550, 382, 264, 31782, 51544], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 143, "seek": 55132, "start": 574.9200000000001, "end": 577.96, "text": " is trying to complete the document, it's actually answering our question.", "tokens": [51544, 307, 1382, 281, 3566, 264, 4166, 11, 309, 311, 767, 13430, 527, 1168, 13, 51696], "temperature": 0.0, "avg_logprob": -0.13005317960466659, "compression_ratio": 1.8566666666666667, "no_speech_prob": 0.0008557880064472556}, {"id": 144, "seek": 57796, "start": 577.96, "end": 581.88, "text": " And so this is an example of prompt engineering a base model, making the belief that it's", "tokens": [50364, 400, 370, 341, 307, 364, 1365, 295, 12391, 7043, 257, 3096, 2316, 11, 1455, 264, 7107, 300, 309, 311, 50560], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 145, "seek": 57796, "start": 581.88, "end": 585.9200000000001, "text": " sort of imitating a document and getting it to perform a task.", "tokens": [50560, 1333, 295, 566, 16350, 257, 4166, 293, 1242, 309, 281, 2042, 257, 5633, 13, 50762], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 146, "seek": 57796, "start": 585.9200000000001, "end": 590.4000000000001, "text": " And so this kicked off, I think, the era of, I would say, prompting over fine tuning and", "tokens": [50762, 400, 370, 341, 14609, 766, 11, 286, 519, 11, 264, 4249, 295, 11, 286, 576, 584, 11, 12391, 278, 670, 2489, 15164, 293, 50986], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 147, "seek": 57796, "start": 590.4000000000001, "end": 594.2, "text": " seeing that this actually can work extremely well on a lot of problems, even without training", "tokens": [50986, 2577, 300, 341, 767, 393, 589, 4664, 731, 322, 257, 688, 295, 2740, 11, 754, 1553, 3097, 51176], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 148, "seek": 57796, "start": 594.2, "end": 597.6, "text": " any neural networks, fine tuning, or so on.", "tokens": [51176, 604, 18161, 9590, 11, 2489, 15164, 11, 420, 370, 322, 13, 51346], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 149, "seek": 57796, "start": 597.6, "end": 603.2800000000001, "text": " Now since then, we've seen an entire evolutionary tree of base models that everyone has trained.", "tokens": [51346, 823, 1670, 550, 11, 321, 600, 1612, 364, 2302, 27567, 4230, 295, 3096, 5245, 300, 1518, 575, 8895, 13, 51630], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 150, "seek": 57796, "start": 603.2800000000001, "end": 605.52, "text": " Not all of these models are available.", "tokens": [51630, 1726, 439, 295, 613, 5245, 366, 2435, 13, 51742], "temperature": 0.0, "avg_logprob": -0.13115439827986589, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0003053185937460512}, {"id": 151, "seek": 60552, "start": 605.52, "end": 608.4, "text": " For example, the GPT-4 base model was never released.", "tokens": [50364, 1171, 1365, 11, 264, 26039, 51, 12, 19, 3096, 2316, 390, 1128, 4736, 13, 50508], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 152, "seek": 60552, "start": 608.4, "end": 612.4, "text": " The GPT-4 model that you might be interacting with over API is not a base model, it's an", "tokens": [50508, 440, 26039, 51, 12, 19, 2316, 300, 291, 1062, 312, 18017, 365, 670, 9362, 307, 406, 257, 3096, 2316, 11, 309, 311, 364, 50708], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 153, "seek": 60552, "start": 612.4, "end": 616.36, "text": " assistant model, and we're going to cover how to get those in a bit.", "tokens": [50708, 10994, 2316, 11, 293, 321, 434, 516, 281, 2060, 577, 281, 483, 729, 294, 257, 857, 13, 50906], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 154, "seek": 60552, "start": 616.36, "end": 621.96, "text": " GPT-3 base model is available via the API under the name DaVinci, and GPT-2 base model is", "tokens": [50906, 26039, 51, 12, 18, 3096, 2316, 307, 2435, 5766, 264, 9362, 833, 264, 1315, 3933, 53, 21961, 11, 293, 26039, 51, 12, 17, 3096, 2316, 307, 51186], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 155, "seek": 60552, "start": 621.96, "end": 624.96, "text": " available even as weights on our GitHub repo.", "tokens": [51186, 2435, 754, 382, 17443, 322, 527, 23331, 49040, 13, 51336], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 156, "seek": 60552, "start": 624.96, "end": 630.28, "text": " But currently the best available base model probably is the Lama series from Meta, although", "tokens": [51336, 583, 4362, 264, 1151, 2435, 3096, 2316, 1391, 307, 264, 441, 2404, 2638, 490, 6377, 64, 11, 4878, 51602], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 157, "seek": 60552, "start": 630.28, "end": 634.0, "text": " it is not commercially licensed.", "tokens": [51602, 309, 307, 406, 41751, 25225, 13, 51788], "temperature": 0.0, "avg_logprob": -0.13480937573336815, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0021149558015167713}, {"id": 158, "seek": 63400, "start": 634.0, "end": 636.88, "text": " Now one thing to point out is base models are not assistants.", "tokens": [50364, 823, 472, 551, 281, 935, 484, 307, 3096, 5245, 366, 406, 34949, 13, 50508], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 159, "seek": 63400, "start": 636.88, "end": 641.4, "text": " They don't want to make answers to your questions.", "tokens": [50508, 814, 500, 380, 528, 281, 652, 6338, 281, 428, 1651, 13, 50734], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 160, "seek": 63400, "start": 641.4, "end": 643.32, "text": " They just want to complete documents.", "tokens": [50734, 814, 445, 528, 281, 3566, 8512, 13, 50830], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 161, "seek": 63400, "start": 643.32, "end": 648.44, "text": " So if you tell them write a poem about the bread and cheese, it will answer questions", "tokens": [50830, 407, 498, 291, 980, 552, 2464, 257, 13065, 466, 264, 5961, 293, 5399, 11, 309, 486, 1867, 1651, 51086], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 162, "seek": 63400, "start": 648.44, "end": 649.44, "text": " with more questions.", "tokens": [51086, 365, 544, 1651, 13, 51136], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 163, "seek": 63400, "start": 649.44, "end": 651.52, "text": " It's just completing what it thinks is a document.", "tokens": [51136, 467, 311, 445, 19472, 437, 309, 7309, 307, 257, 4166, 13, 51240], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 164, "seek": 63400, "start": 651.52, "end": 657.4, "text": " However, you can prompt them in a specific way for base models that is more likely to", "tokens": [51240, 2908, 11, 291, 393, 12391, 552, 294, 257, 2685, 636, 337, 3096, 5245, 300, 307, 544, 3700, 281, 51534], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 165, "seek": 63400, "start": 657.4, "end": 658.4, "text": " work.", "tokens": [51534, 589, 13, 51584], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 166, "seek": 63400, "start": 658.4, "end": 661.28, "text": " So as an example, here's a poem about bread and cheese, and in that case it will autocomplete", "tokens": [51584, 407, 382, 364, 1365, 11, 510, 311, 257, 13065, 466, 5961, 293, 5399, 11, 293, 294, 300, 1389, 309, 486, 45833, 298, 17220, 51728], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 167, "seek": 63400, "start": 661.28, "end": 663.04, "text": " correctly.", "tokens": [51728, 8944, 13, 51816], "temperature": 0.0, "avg_logprob": -0.15297343712726622, "compression_ratio": 1.84981684981685, "no_speech_prob": 0.07576783746480942}, {"id": 168, "seek": 66304, "start": 663.04, "end": 667.3199999999999, "text": " You can even trick base models into being assistants, and the way you would do this is", "tokens": [50364, 509, 393, 754, 4282, 3096, 5245, 666, 885, 34949, 11, 293, 264, 636, 291, 576, 360, 341, 307, 50578], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 169, "seek": 66304, "start": 667.3199999999999, "end": 671.16, "text": " you would create like a specific few-shot prompt that makes it look like there's some", "tokens": [50578, 291, 576, 1884, 411, 257, 2685, 1326, 12, 18402, 12391, 300, 1669, 309, 574, 411, 456, 311, 512, 50770], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 170, "seek": 66304, "start": 671.16, "end": 676.52, "text": " kind of a document between a human and assistant, and they're exchanging sort of information.", "tokens": [50770, 733, 295, 257, 4166, 1296, 257, 1952, 293, 10994, 11, 293, 436, 434, 6210, 9741, 1333, 295, 1589, 13, 51038], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 171, "seek": 66304, "start": 676.52, "end": 681.12, "text": " And then at the bottom you sort of put your query at the end, and the base model will", "tokens": [51038, 400, 550, 412, 264, 2767, 291, 1333, 295, 829, 428, 14581, 412, 264, 917, 11, 293, 264, 3096, 2316, 486, 51268], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 172, "seek": 66304, "start": 681.12, "end": 686.9599999999999, "text": " sort of like condition itself into being like a helpful assistant and kind of answer.", "tokens": [51268, 1333, 295, 411, 4188, 2564, 666, 885, 411, 257, 4961, 10994, 293, 733, 295, 1867, 13, 51560], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 173, "seek": 66304, "start": 686.9599999999999, "end": 689.88, "text": " But this is not very reliable and doesn't work super well in practice, although it can", "tokens": [51560, 583, 341, 307, 406, 588, 12924, 293, 1177, 380, 589, 1687, 731, 294, 3124, 11, 4878, 309, 393, 51706], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 174, "seek": 66304, "start": 689.88, "end": 691.24, "text": " be done.", "tokens": [51706, 312, 1096, 13, 51774], "temperature": 0.0, "avg_logprob": -0.11557472497224808, "compression_ratio": 1.797979797979798, "no_speech_prob": 0.005999771412461996}, {"id": 175, "seek": 69124, "start": 691.28, "end": 695.52, "text": " So instead we have a different path to make actual GPT assistants not just base model", "tokens": [50366, 407, 2602, 321, 362, 257, 819, 3100, 281, 652, 3539, 26039, 51, 34949, 406, 445, 3096, 2316, 50578], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 176, "seek": 69124, "start": 695.52, "end": 697.36, "text": " document completers.", "tokens": [50578, 4166, 1557, 433, 13, 50670], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 177, "seek": 69124, "start": 697.36, "end": 699.92, "text": " And so that takes us into supervised fine-tuning.", "tokens": [50670, 400, 370, 300, 2516, 505, 666, 46533, 2489, 12, 83, 37726, 13, 50798], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 178, "seek": 69124, "start": 699.92, "end": 704.64, "text": " So in the supervised fine-tuning stage, we are going to collect small but high-quality", "tokens": [50798, 407, 294, 264, 46533, 2489, 12, 83, 37726, 3233, 11, 321, 366, 516, 281, 2500, 1359, 457, 1090, 12, 11286, 51034], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 179, "seek": 69124, "start": 704.64, "end": 705.64, "text": " data sets.", "tokens": [51034, 1412, 6352, 13, 51084], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 180, "seek": 69124, "start": 705.64, "end": 711.4, "text": " And in this case we're going to ask human contractors to gather data of the form prompt", "tokens": [51084, 400, 294, 341, 1389, 321, 434, 516, 281, 1029, 1952, 28377, 281, 5448, 1412, 295, 264, 1254, 12391, 51372], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 181, "seek": 69124, "start": 711.4, "end": 715.32, "text": " and ideal response, and we're going to collect lots of these, typically tens of thousands", "tokens": [51372, 293, 7157, 4134, 11, 293, 321, 434, 516, 281, 2500, 3195, 295, 613, 11, 5850, 10688, 295, 5383, 51568], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 182, "seek": 69124, "start": 715.32, "end": 717.0, "text": " or something like that.", "tokens": [51568, 420, 746, 411, 300, 13, 51652], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 183, "seek": 69124, "start": 717.0, "end": 720.36, "text": " And then we're going to still do language modeling on this data, so nothing changed", "tokens": [51652, 400, 550, 321, 434, 516, 281, 920, 360, 2856, 15983, 322, 341, 1412, 11, 370, 1825, 3105, 51820], "temperature": 0.0, "avg_logprob": -0.13297192018423507, "compression_ratio": 1.7880794701986755, "no_speech_prob": 0.0004237988614477217}, {"id": 184, "seek": 72036, "start": 720.36, "end": 721.36, "text": " algorithmically.", "tokens": [50364, 9284, 984, 13, 50414], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 185, "seek": 72036, "start": 721.36, "end": 723.0, "text": " We're just swapping out a training set.", "tokens": [50414, 492, 434, 445, 1693, 10534, 484, 257, 3097, 992, 13, 50496], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 186, "seek": 72036, "start": 723.0, "end": 728.6, "text": " So it used to be internet documents, which is a high-quantity, low-quantity for basically", "tokens": [50496, 407, 309, 1143, 281, 312, 4705, 8512, 11, 597, 307, 257, 1090, 12, 358, 394, 507, 11, 2295, 12, 358, 394, 507, 337, 1936, 50776], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 187, "seek": 72036, "start": 728.6, "end": 734.36, "text": " QA prompt response kind of data, and that is low-quantity, high-quality.", "tokens": [50776, 1249, 32, 12391, 4134, 733, 295, 1412, 11, 293, 300, 307, 2295, 12, 358, 394, 507, 11, 1090, 12, 11286, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 188, "seek": 72036, "start": 734.36, "end": 738.6, "text": " So we still do language modeling, and then after training we get an SFD model.", "tokens": [51064, 407, 321, 920, 360, 2856, 15983, 11, 293, 550, 934, 3097, 321, 483, 364, 31095, 35, 2316, 13, 51276], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 189, "seek": 72036, "start": 738.6, "end": 741.88, "text": " And you can actually deploy these models, and they are actual assistants, and they work", "tokens": [51276, 400, 291, 393, 767, 7274, 613, 5245, 11, 293, 436, 366, 3539, 34949, 11, 293, 436, 589, 51440], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 190, "seek": 72036, "start": 741.88, "end": 743.2, "text": " to some extent.", "tokens": [51440, 281, 512, 8396, 13, 51506], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 191, "seek": 72036, "start": 743.2, "end": 745.96, "text": " Let me show you what an example demonstration might look like.", "tokens": [51506, 961, 385, 855, 291, 437, 364, 1365, 16520, 1062, 574, 411, 13, 51644], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 192, "seek": 72036, "start": 745.96, "end": 748.48, "text": " So here's something that a human contractor might come up with.", "tokens": [51644, 407, 510, 311, 746, 300, 257, 1952, 26463, 1062, 808, 493, 365, 13, 51770], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 193, "seek": 72036, "start": 748.48, "end": 749.84, "text": " Here's some random prompt.", "tokens": [51770, 1692, 311, 512, 4974, 12391, 13, 51838], "temperature": 0.0, "avg_logprob": -0.15630531311035156, "compression_ratio": 1.7320872274143302, "no_speech_prob": 0.0010645481524989009}, {"id": 194, "seek": 74984, "start": 749.84, "end": 753.72, "text": " When you write a short introduction about the relevance of the term monopsony or something", "tokens": [50364, 1133, 291, 2464, 257, 2099, 9339, 466, 264, 32684, 295, 264, 1433, 1108, 3370, 2526, 420, 746, 50558], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 195, "seek": 74984, "start": 753.72, "end": 757.6800000000001, "text": " like that, and then the contractor also writes out an ideal response.", "tokens": [50558, 411, 300, 11, 293, 550, 264, 26463, 611, 13657, 484, 364, 7157, 4134, 13, 50756], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 196, "seek": 74984, "start": 757.6800000000001, "end": 761.6800000000001, "text": " And when they write out these responses, they are following extensive labeling documentations,", "tokens": [50756, 400, 562, 436, 2464, 484, 613, 13019, 11, 436, 366, 3480, 13246, 40244, 4166, 763, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 197, "seek": 74984, "start": 761.6800000000001, "end": 765.8000000000001, "text": " and they are being asked to be helpful, truthful, and harmless.", "tokens": [50956, 293, 436, 366, 885, 2351, 281, 312, 4961, 11, 44669, 11, 293, 40160, 13, 51162], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 198, "seek": 74984, "start": 765.8000000000001, "end": 768.6, "text": " And this is labeling instructions here.", "tokens": [51162, 400, 341, 307, 40244, 9415, 510, 13, 51302], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 199, "seek": 74984, "start": 768.6, "end": 772.72, "text": " You probably can't read it, neither can I, but they're long, and this is just people", "tokens": [51302, 509, 1391, 393, 380, 1401, 309, 11, 9662, 393, 286, 11, 457, 436, 434, 938, 11, 293, 341, 307, 445, 561, 51508], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 200, "seek": 74984, "start": 772.72, "end": 776.36, "text": " following instructions and trying to complete these prompts.", "tokens": [51508, 3480, 9415, 293, 1382, 281, 3566, 613, 41095, 13, 51690], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 201, "seek": 74984, "start": 776.36, "end": 778.9200000000001, "text": " So that's what the data set looks like, and you can train these models, and this works", "tokens": [51690, 407, 300, 311, 437, 264, 1412, 992, 1542, 411, 11, 293, 291, 393, 3847, 613, 5245, 11, 293, 341, 1985, 51818], "temperature": 0.0, "avg_logprob": -0.11620411452125101, "compression_ratio": 1.8215384615384616, "no_speech_prob": 0.0006262193783186376}, {"id": 202, "seek": 77892, "start": 778.92, "end": 780.4399999999999, "text": " to some extent.", "tokens": [50364, 281, 512, 8396, 13, 50440], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 203, "seek": 77892, "start": 780.4399999999999, "end": 785.68, "text": " Now you can actually continue the pipeline from here on and go into RLHF, Reinforcement", "tokens": [50440, 823, 291, 393, 767, 2354, 264, 15517, 490, 510, 322, 293, 352, 666, 497, 43, 39, 37, 11, 42116, 9382, 50702], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 204, "seek": 77892, "start": 785.68, "end": 789.8399999999999, "text": " Learning from Human Feedback, that consists of both reward modeling and reinforcement", "tokens": [50702, 15205, 490, 10294, 33720, 3207, 11, 300, 14689, 295, 1293, 7782, 15983, 293, 29280, 50910], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 205, "seek": 77892, "start": 789.8399999999999, "end": 790.8399999999999, "text": " learning.", "tokens": [50910, 2539, 13, 50960], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 206, "seek": 77892, "start": 790.8399999999999, "end": 793.68, "text": " So let me cover that, and then I'll come back to why you may want to go through the extra", "tokens": [50960, 407, 718, 385, 2060, 300, 11, 293, 550, 286, 603, 808, 646, 281, 983, 291, 815, 528, 281, 352, 807, 264, 2857, 51102], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 207, "seek": 77892, "start": 793.68, "end": 796.76, "text": " steps and how that compares to just SFD models.", "tokens": [51102, 4439, 293, 577, 300, 38334, 281, 445, 31095, 35, 5245, 13, 51256], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 208, "seek": 77892, "start": 796.76, "end": 800.24, "text": " So in the reward modeling step, what we're going to do is we're now going to shift our", "tokens": [51256, 407, 294, 264, 7782, 15983, 1823, 11, 437, 321, 434, 516, 281, 360, 307, 321, 434, 586, 516, 281, 5513, 527, 51430], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 209, "seek": 77892, "start": 800.24, "end": 803.28, "text": " data collection to be of the form of comparisons.", "tokens": [51430, 1412, 5765, 281, 312, 295, 264, 1254, 295, 33157, 13, 51582], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 210, "seek": 77892, "start": 803.28, "end": 805.8, "text": " So here's an example of what our data set will look like.", "tokens": [51582, 407, 510, 311, 364, 1365, 295, 437, 527, 1412, 992, 486, 574, 411, 13, 51708], "temperature": 0.0, "avg_logprob": -0.0966463641843934, "compression_ratio": 1.7161290322580645, "no_speech_prob": 0.0019564630929380655}, {"id": 211, "seek": 80580, "start": 805.8, "end": 810.76, "text": " I have the same prompt, identical prompt on the top, which is asking the assistant to", "tokens": [50364, 286, 362, 264, 912, 12391, 11, 14800, 12391, 322, 264, 1192, 11, 597, 307, 3365, 264, 10994, 281, 50612], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 212, "seek": 80580, "start": 810.76, "end": 815.7199999999999, "text": " write a program or a function that checks if a given string is a palindrome.", "tokens": [50612, 2464, 257, 1461, 420, 257, 2445, 300, 13834, 498, 257, 2212, 6798, 307, 257, 3984, 471, 11505, 13, 50860], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 213, "seek": 80580, "start": 815.7199999999999, "end": 820.0799999999999, "text": " And then what we do is we take the SFD model, which we've already trained, and we create", "tokens": [50860, 400, 550, 437, 321, 360, 307, 321, 747, 264, 31095, 35, 2316, 11, 597, 321, 600, 1217, 8895, 11, 293, 321, 1884, 51078], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 214, "seek": 80580, "start": 820.0799999999999, "end": 821.24, "text": " multiple completions.", "tokens": [51078, 3866, 1557, 626, 13, 51136], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 215, "seek": 80580, "start": 821.24, "end": 824.3599999999999, "text": " So in this case, we have three completions that the model has created.", "tokens": [51136, 407, 294, 341, 1389, 11, 321, 362, 1045, 1557, 626, 300, 264, 2316, 575, 2942, 13, 51292], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 216, "seek": 80580, "start": 824.3599999999999, "end": 827.16, "text": " And then we ask people to rank these completions.", "tokens": [51292, 400, 550, 321, 1029, 561, 281, 6181, 613, 1557, 626, 13, 51432], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 217, "seek": 80580, "start": 827.16, "end": 830.68, "text": " So if you stare at this for a while, and by the way, these are very difficult things", "tokens": [51432, 407, 498, 291, 22432, 412, 341, 337, 257, 1339, 11, 293, 538, 264, 636, 11, 613, 366, 588, 2252, 721, 51608], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 218, "seek": 80580, "start": 830.68, "end": 834.8399999999999, "text": " to do to compare some of these predictions, and this can take people even hours for a", "tokens": [51608, 281, 360, 281, 6794, 512, 295, 613, 21264, 11, 293, 341, 393, 747, 561, 754, 2496, 337, 257, 51816], "temperature": 0.0, "avg_logprob": -0.0761437251650054, "compression_ratio": 1.8585526315789473, "no_speech_prob": 0.00073203508509323}, {"id": 219, "seek": 83484, "start": 834.84, "end": 838.52, "text": " single prompt completion pairs.", "tokens": [50364, 2167, 12391, 19372, 15494, 13, 50548], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 220, "seek": 83484, "start": 838.52, "end": 842.0400000000001, "text": " But let's say we decided that one of these is much better than the others, and so on.", "tokens": [50548, 583, 718, 311, 584, 321, 3047, 300, 472, 295, 613, 307, 709, 1101, 813, 264, 2357, 11, 293, 370, 322, 13, 50724], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 221, "seek": 83484, "start": 842.0400000000001, "end": 843.64, "text": " So we rank them.", "tokens": [50724, 407, 321, 6181, 552, 13, 50804], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 222, "seek": 83484, "start": 843.64, "end": 847.36, "text": " Then we can follow that with something that looks very much like a binary classification", "tokens": [50804, 1396, 321, 393, 1524, 300, 365, 746, 300, 1542, 588, 709, 411, 257, 17434, 21538, 50990], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 223, "seek": 83484, "start": 847.36, "end": 850.52, "text": " on all the possible pairs between these completions.", "tokens": [50990, 322, 439, 264, 1944, 15494, 1296, 613, 1557, 626, 13, 51148], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 224, "seek": 83484, "start": 850.52, "end": 855.0, "text": " So what we do now is we lay out our prompt in rows, and the prompts is identical across", "tokens": [51148, 407, 437, 321, 360, 586, 307, 321, 2360, 484, 527, 12391, 294, 13241, 11, 293, 264, 41095, 307, 14800, 2108, 51372], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 225, "seek": 83484, "start": 855.0, "end": 856.36, "text": " all three rows here.", "tokens": [51372, 439, 1045, 13241, 510, 13, 51440], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 226, "seek": 83484, "start": 856.36, "end": 860.64, "text": " So it's all the same prompt, but the completion is very, and so the yellow tokens are coming", "tokens": [51440, 407, 309, 311, 439, 264, 912, 12391, 11, 457, 264, 19372, 307, 588, 11, 293, 370, 264, 5566, 22667, 366, 1348, 51654], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 227, "seek": 83484, "start": 860.64, "end": 862.32, "text": " from the SFD model.", "tokens": [51654, 490, 264, 31095, 35, 2316, 13, 51738], "temperature": 0.0, "avg_logprob": -0.12427912652492523, "compression_ratio": 1.7978339350180506, "no_speech_prob": 0.0005032638437114656}, {"id": 228, "seek": 86232, "start": 862.32, "end": 868.32, "text": " Then what we do is we append another special reward readout token at the end, and we basically", "tokens": [50364, 1396, 437, 321, 360, 307, 321, 34116, 1071, 2121, 7782, 1401, 346, 14862, 412, 264, 917, 11, 293, 321, 1936, 50664], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 229, "seek": 86232, "start": 868.32, "end": 872.0400000000001, "text": " only supervise the transformer at this single green token.", "tokens": [50664, 787, 37971, 908, 264, 31782, 412, 341, 2167, 3092, 14862, 13, 50850], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 230, "seek": 86232, "start": 872.0400000000001, "end": 878.5600000000001, "text": " And the transformer will predict some reward for how good that completion is for that prompt.", "tokens": [50850, 400, 264, 31782, 486, 6069, 512, 7782, 337, 577, 665, 300, 19372, 307, 337, 300, 12391, 13, 51176], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 231, "seek": 86232, "start": 878.5600000000001, "end": 882.96, "text": " And so basically it makes a guess about the quality of each completion, and then once", "tokens": [51176, 400, 370, 1936, 309, 1669, 257, 2041, 466, 264, 3125, 295, 1184, 19372, 11, 293, 550, 1564, 51396], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 232, "seek": 86232, "start": 882.96, "end": 886.5600000000001, "text": " it makes a guess for every one of them, we also have the ground truth, which is telling", "tokens": [51396, 309, 1669, 257, 2041, 337, 633, 472, 295, 552, 11, 321, 611, 362, 264, 2727, 3494, 11, 597, 307, 3585, 51576], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 233, "seek": 86232, "start": 886.5600000000001, "end": 888.1600000000001, "text": " us the ranking of them.", "tokens": [51576, 505, 264, 17833, 295, 552, 13, 51656], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 234, "seek": 86232, "start": 888.1600000000001, "end": 891.8000000000001, "text": " And so we can actually enforce that some of these numbers should be much higher than others,", "tokens": [51656, 400, 370, 321, 393, 767, 24825, 300, 512, 295, 613, 3547, 820, 312, 709, 2946, 813, 2357, 11, 51838], "temperature": 0.0, "avg_logprob": -0.09385136413574219, "compression_ratio": 1.8424657534246576, "no_speech_prob": 0.005466895177960396}, {"id": 235, "seek": 89180, "start": 891.8, "end": 892.8, "text": " and so on.", "tokens": [50364, 293, 370, 322, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 236, "seek": 89180, "start": 892.8, "end": 896.4399999999999, "text": " We formulate this into a loss function, and we train our model to make reward predictions", "tokens": [50414, 492, 47881, 341, 666, 257, 4470, 2445, 11, 293, 321, 3847, 527, 2316, 281, 652, 7782, 21264, 50596], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 237, "seek": 89180, "start": 896.4399999999999, "end": 899.8399999999999, "text": " that are consistent with the ground truth coming from the comparisons from all these", "tokens": [50596, 300, 366, 8398, 365, 264, 2727, 3494, 1348, 490, 264, 33157, 490, 439, 613, 50766], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 238, "seek": 89180, "start": 899.8399999999999, "end": 900.8399999999999, "text": " contractors.", "tokens": [50766, 28377, 13, 50816], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 239, "seek": 89180, "start": 900.8399999999999, "end": 903.04, "text": " So that's how we train our reward model.", "tokens": [50816, 407, 300, 311, 577, 321, 3847, 527, 7782, 2316, 13, 50926], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 240, "seek": 89180, "start": 903.04, "end": 907.68, "text": " And that allows us to score how good a completion is for a prompt.", "tokens": [50926, 400, 300, 4045, 505, 281, 6175, 577, 665, 257, 19372, 307, 337, 257, 12391, 13, 51158], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 241, "seek": 89180, "start": 907.68, "end": 912.4, "text": " Once we have a reward model, we can't deploy this because this is not very useful as an", "tokens": [51158, 3443, 321, 362, 257, 7782, 2316, 11, 321, 393, 380, 7274, 341, 570, 341, 307, 406, 588, 4420, 382, 364, 51394], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 242, "seek": 89180, "start": 912.4, "end": 916.4399999999999, "text": " assistant by itself, but it's very useful for the reinforcement learning stage that follows", "tokens": [51394, 10994, 538, 2564, 11, 457, 309, 311, 588, 4420, 337, 264, 29280, 2539, 3233, 300, 10002, 51596], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 243, "seek": 89180, "start": 916.4399999999999, "end": 917.4399999999999, "text": " now.", "tokens": [51596, 586, 13, 51646], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 244, "seek": 89180, "start": 917.4399999999999, "end": 921.64, "text": " Because we have a reward model, we can score the quality of any arbitrary completion for", "tokens": [51646, 1436, 321, 362, 257, 7782, 2316, 11, 321, 393, 6175, 264, 3125, 295, 604, 23211, 19372, 337, 51856], "temperature": 0.0, "avg_logprob": -0.12432014684882953, "compression_ratio": 1.8296529968454258, "no_speech_prob": 0.00019408682419452816}, {"id": 245, "seek": 92164, "start": 921.64, "end": 923.1999999999999, "text": " any given prompt.", "tokens": [50364, 604, 2212, 12391, 13, 50442], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 246, "seek": 92164, "start": 923.1999999999999, "end": 926.72, "text": " So what we do during reinforcement learning is we basically get, again, a large collection", "tokens": [50442, 407, 437, 321, 360, 1830, 29280, 2539, 307, 321, 1936, 483, 11, 797, 11, 257, 2416, 5765, 50618], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 247, "seek": 92164, "start": 926.72, "end": 930.96, "text": " of prompts, and now we do reinforcement learning with respect to the reward model.", "tokens": [50618, 295, 41095, 11, 293, 586, 321, 360, 29280, 2539, 365, 3104, 281, 264, 7782, 2316, 13, 50830], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 248, "seek": 92164, "start": 930.96, "end": 933.24, "text": " So here's what that looks like.", "tokens": [50830, 407, 510, 311, 437, 300, 1542, 411, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 249, "seek": 92164, "start": 933.24, "end": 937.88, "text": " We take a single prompt, we lay it out and rose, and now we use the SFD model.", "tokens": [50944, 492, 747, 257, 2167, 12391, 11, 321, 2360, 309, 484, 293, 10895, 11, 293, 586, 321, 764, 264, 31095, 35, 2316, 13, 51176], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 250, "seek": 92164, "start": 937.88, "end": 941.96, "text": " We use basically the model we'd like to train, which is initialized at SFD model, to create", "tokens": [51176, 492, 764, 1936, 264, 2316, 321, 1116, 411, 281, 3847, 11, 597, 307, 5883, 1602, 412, 31095, 35, 2316, 11, 281, 1884, 51380], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 251, "seek": 92164, "start": 941.96, "end": 943.92, "text": " some completions in yellow.", "tokens": [51380, 512, 1557, 626, 294, 5566, 13, 51478], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 252, "seek": 92164, "start": 943.92, "end": 948.4, "text": " And then we append the reward token again, and we read off the reward according to the", "tokens": [51478, 400, 550, 321, 34116, 264, 7782, 14862, 797, 11, 293, 321, 1401, 766, 264, 7782, 4650, 281, 264, 51702], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 253, "seek": 92164, "start": 948.4, "end": 950.6, "text": " reward model, which is now kept fixed.", "tokens": [51702, 7782, 2316, 11, 597, 307, 586, 4305, 6806, 13, 51812], "temperature": 0.0, "avg_logprob": -0.12631592280428175, "compression_ratio": 1.889655172413793, "no_speech_prob": 0.0033756319899111986}, {"id": 254, "seek": 95060, "start": 950.6, "end": 952.4, "text": " It doesn't change anymore.", "tokens": [50364, 467, 1177, 380, 1319, 3602, 13, 50454], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 255, "seek": 95060, "start": 952.4, "end": 957.4, "text": " And now the reward model tells us the quality of every single completion for these prompts.", "tokens": [50454, 400, 586, 264, 7782, 2316, 5112, 505, 264, 3125, 295, 633, 2167, 19372, 337, 613, 41095, 13, 50704], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 256, "seek": 95060, "start": 957.4, "end": 960.8000000000001, "text": " And so what we can do is we can now just basically apply the same language modeling", "tokens": [50704, 400, 370, 437, 321, 393, 360, 307, 321, 393, 586, 445, 1936, 3079, 264, 912, 2856, 15983, 50874], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 257, "seek": 95060, "start": 960.8000000000001, "end": 967.0400000000001, "text": " loss function, but we're currently training on the yellow tokens, and we are weighing", "tokens": [50874, 4470, 2445, 11, 457, 321, 434, 4362, 3097, 322, 264, 5566, 22667, 11, 293, 321, 366, 31986, 51186], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 258, "seek": 95060, "start": 967.0400000000001, "end": 971.44, "text": " the language modeling objective by the rewards indicated by the reward model.", "tokens": [51186, 264, 2856, 15983, 10024, 538, 264, 17203, 16176, 538, 264, 7782, 2316, 13, 51406], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 259, "seek": 95060, "start": 971.44, "end": 976.44, "text": " So as an example, in the first row, the reward model said that this is a fairly high-scoring", "tokens": [51406, 407, 382, 364, 1365, 11, 294, 264, 700, 5386, 11, 264, 7782, 2316, 848, 300, 341, 307, 257, 6457, 1090, 12, 4417, 3662, 51656], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 260, "seek": 95060, "start": 976.44, "end": 977.44, "text": " completion.", "tokens": [51656, 19372, 13, 51706], "temperature": 0.0, "avg_logprob": -0.11496793901598132, "compression_ratio": 1.7444444444444445, "no_speech_prob": 0.0006165383965708315}, {"id": 261, "seek": 97744, "start": 977.44, "end": 981.4000000000001, "text": " And so all of the tokens that we happened to sample on the first row are going to get", "tokens": [50364, 400, 370, 439, 295, 264, 22667, 300, 321, 2011, 281, 6889, 322, 264, 700, 5386, 366, 516, 281, 483, 50562], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 262, "seek": 97744, "start": 981.4000000000001, "end": 985.08, "text": " reinforced, and they're going to get higher probabilities for the future.", "tokens": [50562, 31365, 11, 293, 436, 434, 516, 281, 483, 2946, 33783, 337, 264, 2027, 13, 50746], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 263, "seek": 97744, "start": 985.08, "end": 989.0, "text": " Conversely, on the second row, the reward model really did not like this completion,", "tokens": [50746, 33247, 736, 11, 322, 264, 1150, 5386, 11, 264, 7782, 2316, 534, 630, 406, 411, 341, 19372, 11, 50942], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 264, "seek": 97744, "start": 989.0, "end": 990.08, "text": " negative 1.2.", "tokens": [50942, 3671, 502, 13, 17, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 265, "seek": 97744, "start": 990.08, "end": 994.0400000000001, "text": " And so therefore, every single token that we sampled in that second row is going to get", "tokens": [50996, 400, 370, 4412, 11, 633, 2167, 14862, 300, 321, 3247, 15551, 294, 300, 1150, 5386, 307, 516, 281, 483, 51194], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 266, "seek": 97744, "start": 994.0400000000001, "end": 996.44, "text": " a slightly higher probability for the future.", "tokens": [51194, 257, 4748, 2946, 8482, 337, 264, 2027, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 267, "seek": 97744, "start": 996.44, "end": 1001.2, "text": " And we do this over and over on many prompts, on many batches, and basically we get a policy", "tokens": [51314, 400, 321, 360, 341, 670, 293, 670, 322, 867, 41095, 11, 322, 867, 15245, 279, 11, 293, 1936, 321, 483, 257, 3897, 51552], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 268, "seek": 97744, "start": 1001.2, "end": 1007.2800000000001, "text": " which creates yellow tokens here, and basically all of the completions here will score high", "tokens": [51552, 597, 7829, 5566, 22667, 510, 11, 293, 1936, 439, 295, 264, 1557, 626, 510, 486, 6175, 1090, 51856], "temperature": 0.0, "avg_logprob": -0.12588397434779575, "compression_ratio": 1.9625850340136055, "no_speech_prob": 0.000698493851814419}, {"id": 269, "seek": 100728, "start": 1007.28, "end": 1011.64, "text": " according to the reward model that we trained in the previous stage.", "tokens": [50364, 4650, 281, 264, 7782, 2316, 300, 321, 8895, 294, 264, 3894, 3233, 13, 50582], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 270, "seek": 100728, "start": 1011.64, "end": 1015.24, "text": " So that's how we train, that's what the RLHF pipeline is.", "tokens": [50582, 407, 300, 311, 577, 321, 3847, 11, 300, 311, 437, 264, 497, 43, 39, 37, 15517, 307, 13, 50762], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 271, "seek": 100728, "start": 1015.24, "end": 1020.16, "text": " Now, and then at the end, you get a model that you could deploy, and so as an example,", "tokens": [50762, 823, 11, 293, 550, 412, 264, 917, 11, 291, 483, 257, 2316, 300, 291, 727, 7274, 11, 293, 370, 382, 364, 1365, 11, 51008], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 272, "seek": 100728, "start": 1020.16, "end": 1024.76, "text": " chatGPT is an RLHF model, but some other models that you might come across, like for", "tokens": [51008, 5081, 38, 47, 51, 307, 364, 497, 43, 39, 37, 2316, 11, 457, 512, 661, 5245, 300, 291, 1062, 808, 2108, 11, 411, 337, 51238], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 273, "seek": 100728, "start": 1024.76, "end": 1028.3799999999999, "text": " example, the Kuna 13B and so on, these are SFD models.", "tokens": [51238, 1365, 11, 264, 591, 5051, 3705, 33, 293, 370, 322, 11, 613, 366, 31095, 35, 5245, 13, 51419], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 274, "seek": 100728, "start": 1028.3799999999999, "end": 1033.56, "text": " So we have base models, SFD models, and RLHF models, and that's kind of like the state", "tokens": [51419, 407, 321, 362, 3096, 5245, 11, 31095, 35, 5245, 11, 293, 497, 43, 39, 37, 5245, 11, 293, 300, 311, 733, 295, 411, 264, 1785, 51678], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 275, "seek": 100728, "start": 1033.56, "end": 1035.04, "text": " of things there.", "tokens": [51678, 295, 721, 456, 13, 51752], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 276, "seek": 100728, "start": 1035.04, "end": 1037.18, "text": " Now why would you want to do RLHF?", "tokens": [51752, 823, 983, 576, 291, 528, 281, 360, 497, 43, 39, 37, 30, 51859], "temperature": 0.0, "avg_logprob": -0.18472825087510147, "compression_ratio": 1.7697841726618706, "no_speech_prob": 0.0028886860236525536}, {"id": 277, "seek": 103718, "start": 1037.18, "end": 1041.1000000000001, "text": " So one answer that is kind of not that exciting is that it just works better.", "tokens": [50364, 407, 472, 1867, 300, 307, 733, 295, 406, 300, 4670, 307, 300, 309, 445, 1985, 1101, 13, 50560], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 278, "seek": 103718, "start": 1041.1000000000001, "end": 1043.3, "text": " So this comes from the InstructGPT paper.", "tokens": [50560, 407, 341, 1487, 490, 264, 2730, 1757, 38, 47, 51, 3035, 13, 50670], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 279, "seek": 103718, "start": 1043.3, "end": 1048.46, "text": " According to these experiments a while ago now, these PPO models are RLHF, and we see", "tokens": [50670, 7328, 281, 613, 12050, 257, 1339, 2057, 586, 11, 613, 430, 34885, 5245, 366, 497, 43, 39, 37, 11, 293, 321, 536, 50928], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 280, "seek": 103718, "start": 1048.46, "end": 1053.8600000000001, "text": " that they are basically just preferred in a lot of comparisons when we give them to humans.", "tokens": [50928, 300, 436, 366, 1936, 445, 16494, 294, 257, 688, 295, 33157, 562, 321, 976, 552, 281, 6255, 13, 51198], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 281, "seek": 103718, "start": 1053.8600000000001, "end": 1059.7, "text": " So humans just prefer basically tokens that come from RLHF models compared to SFD models,", "tokens": [51198, 407, 6255, 445, 4382, 1936, 22667, 300, 808, 490, 497, 43, 39, 37, 5245, 5347, 281, 31095, 35, 5245, 11, 51490], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 282, "seek": 103718, "start": 1059.7, "end": 1064.1000000000001, "text": " compared to base model that is prompted to be an assistant, and so it just works better.", "tokens": [51490, 5347, 281, 3096, 2316, 300, 307, 31042, 281, 312, 364, 10994, 11, 293, 370, 309, 445, 1985, 1101, 13, 51710], "temperature": 0.0, "avg_logprob": -0.10222973589037286, "compression_ratio": 1.8098859315589353, "no_speech_prob": 0.0001910550199681893}, {"id": 283, "seek": 106410, "start": 1064.3799999999999, "end": 1066.26, "text": " You might ask why?", "tokens": [50378, 509, 1062, 1029, 983, 30, 50472], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 284, "seek": 106410, "start": 1066.26, "end": 1067.4199999999998, "text": " Why does it work better?", "tokens": [50472, 1545, 775, 309, 589, 1101, 30, 50530], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 285, "seek": 106410, "start": 1067.4199999999998, "end": 1071.2199999999998, "text": " And I don't think that there's a single amazing answer that the community has really agreed", "tokens": [50530, 400, 286, 500, 380, 519, 300, 456, 311, 257, 2167, 2243, 1867, 300, 264, 1768, 575, 534, 9166, 50720], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 286, "seek": 106410, "start": 1071.2199999999998, "end": 1077.6599999999999, "text": " on, but I will just offer one reason potentially, and it has to do with the asymmetry between", "tokens": [50720, 322, 11, 457, 286, 486, 445, 2626, 472, 1778, 7263, 11, 293, 309, 575, 281, 360, 365, 264, 37277, 9889, 1296, 51042], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 287, "seek": 106410, "start": 1077.6599999999999, "end": 1082.4599999999998, "text": " how easy computationally it is to compare versus generate.", "tokens": [51042, 577, 1858, 24903, 379, 309, 307, 281, 6794, 5717, 8460, 13, 51282], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 288, "seek": 106410, "start": 1082.4599999999998, "end": 1085.26, "text": " So let's take an example of generating a haiku.", "tokens": [51282, 407, 718, 311, 747, 364, 1365, 295, 17746, 257, 324, 24320, 13, 51422], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 289, "seek": 106410, "start": 1085.26, "end": 1088.26, "text": " Suppose I ask a model to write a haiku about paper clips.", "tokens": [51422, 21360, 286, 1029, 257, 2316, 281, 2464, 257, 324, 24320, 466, 3035, 13117, 13, 51572], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 290, "seek": 106410, "start": 1088.26, "end": 1092.54, "text": " If you're a contractor trying to give train data, then imagine being a contractor collecting", "tokens": [51572, 759, 291, 434, 257, 26463, 1382, 281, 976, 3847, 1412, 11, 550, 3811, 885, 257, 26463, 12510, 51786], "temperature": 0.0, "avg_logprob": -0.13722042959244524, "compression_ratio": 1.62876254180602, "no_speech_prob": 0.001648239209316671}, {"id": 291, "seek": 109254, "start": 1092.54, "end": 1096.3, "text": " basically data for the SFD stage, how are you supposed to create a nice haiku for a", "tokens": [50364, 1936, 1412, 337, 264, 31095, 35, 3233, 11, 577, 366, 291, 3442, 281, 1884, 257, 1481, 324, 24320, 337, 257, 50552], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 292, "seek": 109254, "start": 1096.3, "end": 1097.3, "text": " paper clip?", "tokens": [50552, 3035, 7353, 30, 50602], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 293, "seek": 109254, "start": 1097.3, "end": 1100.86, "text": " You might just not be very good at that, but if I give you a few examples of haikus,", "tokens": [50602, 509, 1062, 445, 406, 312, 588, 665, 412, 300, 11, 457, 498, 286, 976, 291, 257, 1326, 5110, 295, 324, 1035, 301, 11, 50780], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 294, "seek": 109254, "start": 1100.86, "end": 1104.3799999999999, "text": " you might be able to appreciate some of these haikus a lot more than others.", "tokens": [50780, 291, 1062, 312, 1075, 281, 4449, 512, 295, 613, 324, 1035, 301, 257, 688, 544, 813, 2357, 13, 50956], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 295, "seek": 109254, "start": 1104.3799999999999, "end": 1107.6599999999999, "text": " And so judging which one of these is good is a much easier task.", "tokens": [50956, 400, 370, 23587, 597, 472, 295, 613, 307, 665, 307, 257, 709, 3571, 5633, 13, 51120], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 296, "seek": 109254, "start": 1107.6599999999999, "end": 1113.42, "text": " And so basically this asymmetry makes it so that comparisons are a better way to potentially", "tokens": [51120, 400, 370, 1936, 341, 37277, 9889, 1669, 309, 370, 300, 33157, 366, 257, 1101, 636, 281, 7263, 51408], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 297, "seek": 109254, "start": 1113.42, "end": 1118.22, "text": " leverage yourself as a human and your judgment to create a slightly better model.", "tokens": [51408, 13982, 1803, 382, 257, 1952, 293, 428, 12216, 281, 1884, 257, 4748, 1101, 2316, 13, 51648], "temperature": 0.0, "avg_logprob": -0.09843212037574588, "compression_ratio": 1.6847457627118645, "no_speech_prob": 0.0005440830136649311}, {"id": 298, "seek": 111822, "start": 1118.22, "end": 1123.8600000000001, "text": " Now RLHF models are not strictly an improvement on the base models in some cases.", "tokens": [50364, 823, 497, 43, 39, 37, 5245, 366, 406, 20792, 364, 10444, 322, 264, 3096, 5245, 294, 512, 3331, 13, 50646], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 299, "seek": 111822, "start": 1123.8600000000001, "end": 1127.1000000000001, "text": " So in particular, we've noticed, for example, that they lose some entropy.", "tokens": [50646, 407, 294, 1729, 11, 321, 600, 5694, 11, 337, 1365, 11, 300, 436, 3624, 512, 30867, 13, 50808], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 300, "seek": 111822, "start": 1127.1000000000001, "end": 1129.9, "text": " So that means that they give more PT results.", "tokens": [50808, 407, 300, 1355, 300, 436, 976, 544, 35460, 3542, 13, 50948], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 301, "seek": 111822, "start": 1129.9, "end": 1136.22, "text": " They can output samples with lower variation than the base model.", "tokens": [50948, 814, 393, 5598, 10938, 365, 3126, 12990, 813, 264, 3096, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 302, "seek": 111822, "start": 1136.22, "end": 1140.34, "text": " So base model has lots of entropy and will give lots of diverse outputs.", "tokens": [51264, 407, 3096, 2316, 575, 3195, 295, 30867, 293, 486, 976, 3195, 295, 9521, 23930, 13, 51470], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 303, "seek": 111822, "start": 1140.34, "end": 1146.14, "text": " So for example, one kind of place where I still prefer to use a base model is in a setup", "tokens": [51470, 407, 337, 1365, 11, 472, 733, 295, 1081, 689, 286, 920, 4382, 281, 764, 257, 3096, 2316, 307, 294, 257, 8657, 51760], "temperature": 0.0, "avg_logprob": -0.17323730610035085, "compression_ratio": 1.7622950819672132, "no_speech_prob": 0.0013666007434949279}, {"id": 304, "seek": 114614, "start": 1146.14, "end": 1153.9, "text": " where you basically have n things and you want to generate more things like it.", "tokens": [50364, 689, 291, 1936, 362, 297, 721, 293, 291, 528, 281, 8460, 544, 721, 411, 309, 13, 50752], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 305, "seek": 114614, "start": 1153.9, "end": 1156.46, "text": " And so here is an example that I just cooked up.", "tokens": [50752, 400, 370, 510, 307, 364, 1365, 300, 286, 445, 9267, 493, 13, 50880], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 306, "seek": 114614, "start": 1156.46, "end": 1158.9, "text": " I want to generate cool Pokemon names.", "tokens": [50880, 286, 528, 281, 8460, 1627, 13796, 5288, 13, 51002], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 307, "seek": 114614, "start": 1158.9, "end": 1162.9, "text": " I gave it seven Pokemon names and I asked the base model to complete the document and", "tokens": [51002, 286, 2729, 309, 3407, 13796, 5288, 293, 286, 2351, 264, 3096, 2316, 281, 3566, 264, 4166, 293, 51202], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 308, "seek": 114614, "start": 1162.9, "end": 1164.98, "text": " it gave me a lot more Pokemon names.", "tokens": [51202, 309, 2729, 385, 257, 688, 544, 13796, 5288, 13, 51306], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 309, "seek": 114614, "start": 1164.98, "end": 1165.98, "text": " These are fictitious.", "tokens": [51306, 1981, 366, 283, 985, 16401, 13, 51356], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 310, "seek": 114614, "start": 1165.98, "end": 1167.0600000000002, "text": " I tried to look them up.", "tokens": [51356, 286, 3031, 281, 574, 552, 493, 13, 51410], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 311, "seek": 114614, "start": 1167.0600000000002, "end": 1169.26, "text": " I don't believe they're actual Pokemons.", "tokens": [51410, 286, 500, 380, 1697, 436, 434, 3539, 12645, 27229, 13, 51520], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 312, "seek": 114614, "start": 1169.26, "end": 1172.3400000000001, "text": " And this is the kind of task that I think base model would be good at because it still", "tokens": [51520, 400, 341, 307, 264, 733, 295, 5633, 300, 286, 519, 3096, 2316, 576, 312, 665, 412, 570, 309, 920, 51674], "temperature": 0.0, "avg_logprob": -0.13324260711669922, "compression_ratio": 1.8525896414342629, "no_speech_prob": 0.01114725973457098}, {"id": 313, "seek": 117234, "start": 1172.34, "end": 1176.6599999999999, "text": " has lots of entropy and will give you lots of diverse, cool, kind of more things that", "tokens": [50364, 575, 3195, 295, 30867, 293, 486, 976, 291, 3195, 295, 9521, 11, 1627, 11, 733, 295, 544, 721, 300, 50580], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 314, "seek": 117234, "start": 1176.6599999999999, "end": 1180.6799999999998, "text": " look like whatever you give it before.", "tokens": [50580, 574, 411, 2035, 291, 976, 309, 949, 13, 50781], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 315, "seek": 117234, "start": 1180.6799999999998, "end": 1184.22, "text": " So this is what, this is the number, having said all that, these are kind of like the", "tokens": [50781, 407, 341, 307, 437, 11, 341, 307, 264, 1230, 11, 1419, 848, 439, 300, 11, 613, 366, 733, 295, 411, 264, 50958], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 316, "seek": 117234, "start": 1184.22, "end": 1187.74, "text": " assistant models that are probably available to you at this point.", "tokens": [50958, 10994, 5245, 300, 366, 1391, 2435, 281, 291, 412, 341, 935, 13, 51134], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 317, "seek": 117234, "start": 1187.74, "end": 1191.78, "text": " There's a team at Berkeley that ranked a lot of the available assistant models and", "tokens": [51134, 821, 311, 257, 1469, 412, 23684, 300, 20197, 257, 688, 295, 264, 2435, 10994, 5245, 293, 51336], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 318, "seek": 117234, "start": 1191.78, "end": 1193.54, "text": " gave them basically ELO ratings.", "tokens": [51336, 2729, 552, 1936, 14426, 46, 24603, 13, 51424], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 319, "seek": 117234, "start": 1193.54, "end": 1197.74, "text": " So currently some of the best models, of course, are GPT-4 by far, I would say, followed by", "tokens": [51424, 407, 4362, 512, 295, 264, 1151, 5245, 11, 295, 1164, 11, 366, 26039, 51, 12, 19, 538, 1400, 11, 286, 576, 584, 11, 6263, 538, 51634], "temperature": 0.0, "avg_logprob": -0.14992034050726122, "compression_ratio": 1.7017543859649122, "no_speech_prob": 0.002980222227051854}, {"id": 320, "seek": 119774, "start": 1197.74, "end": 1202.42, "text": " Claude, GPT-3.5, and then a number of models, some of these might be available as weights", "tokens": [50364, 12947, 2303, 11, 26039, 51, 12, 18, 13, 20, 11, 293, 550, 257, 1230, 295, 5245, 11, 512, 295, 613, 1062, 312, 2435, 382, 17443, 50598], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 321, "seek": 119774, "start": 1202.42, "end": 1205.1, "text": " like the Kuna, Koala, et cetera.", "tokens": [50598, 411, 264, 591, 5051, 11, 10509, 5159, 11, 1030, 11458, 13, 50732], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 322, "seek": 119774, "start": 1205.1, "end": 1210.66, "text": " And the first three rows here are all, they are all RLHF models and all of the other models", "tokens": [50732, 400, 264, 700, 1045, 13241, 510, 366, 439, 11, 436, 366, 439, 497, 43, 39, 37, 5245, 293, 439, 295, 264, 661, 5245, 51010], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 323, "seek": 119774, "start": 1210.66, "end": 1215.1, "text": " to my knowledge are SFD models, I believe.", "tokens": [51010, 281, 452, 3601, 366, 31095, 35, 5245, 11, 286, 1697, 13, 51232], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 324, "seek": 119774, "start": 1215.1, "end": 1219.54, "text": " Okay, so that's how we train these models on a high level.", "tokens": [51232, 1033, 11, 370, 300, 311, 577, 321, 3847, 613, 5245, 322, 257, 1090, 1496, 13, 51454], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 325, "seek": 119774, "start": 1219.54, "end": 1225.02, "text": " Now I'm going to switch gears and let's look at how we can best apply a GPT-assistant model", "tokens": [51454, 823, 286, 478, 516, 281, 3679, 20915, 293, 718, 311, 574, 412, 577, 321, 393, 1151, 3079, 257, 26039, 51, 12, 640, 10329, 2316, 51728], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 326, "seek": 119774, "start": 1225.02, "end": 1226.46, "text": " to your problems.", "tokens": [51728, 281, 428, 2740, 13, 51800], "temperature": 0.0, "avg_logprob": -0.18832999236823977, "compression_ratio": 1.595505617977528, "no_speech_prob": 0.0006461517186835408}, {"id": 327, "seek": 122646, "start": 1226.46, "end": 1230.1000000000001, "text": " Now I would like to work in a setting of a concrete example.", "tokens": [50364, 823, 286, 576, 411, 281, 589, 294, 257, 3287, 295, 257, 9859, 1365, 13, 50546], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 328, "seek": 122646, "start": 1230.1000000000001, "end": 1233.3400000000001, "text": " So let's, let's work with a concrete example here.", "tokens": [50546, 407, 718, 311, 11, 718, 311, 589, 365, 257, 9859, 1365, 510, 13, 50708], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 329, "seek": 122646, "start": 1233.3400000000001, "end": 1236.9, "text": " Let's say that you are working on an article or a blog post and you're going to write this", "tokens": [50708, 961, 311, 584, 300, 291, 366, 1364, 322, 364, 7222, 420, 257, 6968, 2183, 293, 291, 434, 516, 281, 2464, 341, 50886], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 330, "seek": 122646, "start": 1236.9, "end": 1241.3, "text": " sentence at the end, California's population is 53 times that of Alaska.", "tokens": [50886, 8174, 412, 264, 917, 11, 5384, 311, 4415, 307, 21860, 1413, 300, 295, 19553, 13, 51106], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 331, "seek": 122646, "start": 1241.3, "end": 1245.46, "text": " So for some reason you want to compare the populations of these two states.", "tokens": [51106, 407, 337, 512, 1778, 291, 528, 281, 6794, 264, 12822, 295, 613, 732, 4368, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 332, "seek": 122646, "start": 1245.46, "end": 1250.5, "text": " Think about the rich internal monologue and tool use and how much work actually goes computationally", "tokens": [51314, 6557, 466, 264, 4593, 6920, 1108, 42298, 293, 2290, 764, 293, 577, 709, 589, 767, 1709, 24903, 379, 51566], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 333, "seek": 122646, "start": 1250.5, "end": 1253.5, "text": " in your brain to generate this one final sentence.", "tokens": [51566, 294, 428, 3567, 281, 8460, 341, 472, 2572, 8174, 13, 51716], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 334, "seek": 122646, "start": 1253.5, "end": 1255.58, "text": " So here's maybe what that could look like in your brain.", "tokens": [51716, 407, 510, 311, 1310, 437, 300, 727, 574, 411, 294, 428, 3567, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12256911897311246, "compression_ratio": 1.7445482866043613, "no_speech_prob": 0.0001233848015544936}, {"id": 335, "seek": 125558, "start": 1255.78, "end": 1260.9399999999998, "text": " Okay, for this next step, let me blog, or my blog, let me compare these two populations.", "tokens": [50374, 1033, 11, 337, 341, 958, 1823, 11, 718, 385, 6968, 11, 420, 452, 6968, 11, 718, 385, 6794, 613, 732, 12822, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 336, "seek": 125558, "start": 1260.9399999999998, "end": 1265.3799999999999, "text": " Okay, first I'm going to obviously need to get both of these populations.", "tokens": [50632, 1033, 11, 700, 286, 478, 516, 281, 2745, 643, 281, 483, 1293, 295, 613, 12822, 13, 50854], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 337, "seek": 125558, "start": 1265.3799999999999, "end": 1269.22, "text": " Now I know that I probably don't know these populations off the top of my head.", "tokens": [50854, 823, 286, 458, 300, 286, 1391, 500, 380, 458, 613, 12822, 766, 264, 1192, 295, 452, 1378, 13, 51046], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 338, "seek": 125558, "start": 1269.22, "end": 1273.4199999999998, "text": " So I'm kind of like aware of what I know, what I don't know of my self-knowledge, right?", "tokens": [51046, 407, 286, 478, 733, 295, 411, 3650, 295, 437, 286, 458, 11, 437, 286, 500, 380, 458, 295, 452, 2698, 12, 15869, 3042, 11, 558, 30, 51256], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 339, "seek": 125558, "start": 1273.4199999999998, "end": 1278.4199999999998, "text": " So I go, I do some tool use and I go to Wikipedia and I look up California's population and", "tokens": [51256, 407, 286, 352, 11, 286, 360, 512, 2290, 764, 293, 286, 352, 281, 28999, 293, 286, 574, 493, 5384, 311, 4415, 293, 51506], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 340, "seek": 125558, "start": 1278.4199999999998, "end": 1280.34, "text": " Alaska's population.", "tokens": [51506, 19553, 311, 4415, 13, 51602], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 341, "seek": 125558, "start": 1280.34, "end": 1282.4199999999998, "text": " Now I know that I should divide the two.", "tokens": [51602, 823, 286, 458, 300, 286, 820, 9845, 264, 732, 13, 51706], "temperature": 0.0, "avg_logprob": -0.1627244095304119, "compression_ratio": 1.8233082706766917, "no_speech_prob": 0.0005882428959012032}, {"id": 342, "seek": 128242, "start": 1282.42, "end": 1287.14, "text": " But again, I know that dividing 39.2 by 0.74 is very unlikely to succeed.", "tokens": [50364, 583, 797, 11, 286, 458, 300, 26764, 15238, 13, 17, 538, 1958, 13, 34026, 307, 588, 17518, 281, 7754, 13, 50600], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 343, "seek": 128242, "start": 1287.14, "end": 1289.8200000000002, "text": " That's not the kind of thing that I can do in my head.", "tokens": [50600, 663, 311, 406, 264, 733, 295, 551, 300, 286, 393, 360, 294, 452, 1378, 13, 50734], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 344, "seek": 128242, "start": 1289.8200000000002, "end": 1292.5800000000002, "text": " And so therefore I'm going to rely on a calculator.", "tokens": [50734, 400, 370, 4412, 286, 478, 516, 281, 10687, 322, 257, 24993, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 345, "seek": 128242, "start": 1292.5800000000002, "end": 1297.42, "text": " So I'm going to use a calculator, punch it in, and see that the output is roughly 53.", "tokens": [50872, 407, 286, 478, 516, 281, 764, 257, 24993, 11, 8135, 309, 294, 11, 293, 536, 300, 264, 5598, 307, 9810, 21860, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 346, "seek": 128242, "start": 1297.42, "end": 1300.9, "text": " And then maybe I do some reflection and sanity checks in my brain.", "tokens": [51114, 400, 550, 1310, 286, 360, 512, 12914, 293, 47892, 13834, 294, 452, 3567, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 347, "seek": 128242, "start": 1300.9, "end": 1302.54, "text": " So does 53 make sense?", "tokens": [51288, 407, 775, 21860, 652, 2020, 30, 51370], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 348, "seek": 128242, "start": 1302.54, "end": 1306.46, "text": " Well, that's quite a large fraction, but then California has the most popular state, so", "tokens": [51370, 1042, 11, 300, 311, 1596, 257, 2416, 14135, 11, 457, 550, 5384, 575, 264, 881, 3743, 1785, 11, 370, 51566], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 349, "seek": 128242, "start": 1306.46, "end": 1308.26, "text": " maybe that looks okay.", "tokens": [51566, 1310, 300, 1542, 1392, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 350, "seek": 128242, "start": 1308.26, "end": 1311.54, "text": " So then I have all the information I might need, and now I get to the sort of creative", "tokens": [51656, 407, 550, 286, 362, 439, 264, 1589, 286, 1062, 643, 11, 293, 586, 286, 483, 281, 264, 1333, 295, 5880, 51820], "temperature": 0.0, "avg_logprob": -0.1257414848975886, "compression_ratio": 1.6636636636636637, "no_speech_prob": 0.00026115658693015575}, {"id": 351, "seek": 131154, "start": 1311.54, "end": 1312.98, "text": " portion of writing.", "tokens": [50364, 8044, 295, 3579, 13, 50436], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 352, "seek": 131154, "start": 1312.98, "end": 1317.46, "text": " So I might start to write something like, California has 53x times greater, and then", "tokens": [50436, 407, 286, 1062, 722, 281, 2464, 746, 411, 11, 5384, 575, 21860, 87, 1413, 5044, 11, 293, 550, 50660], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 353, "seek": 131154, "start": 1317.46, "end": 1320.42, "text": " I think to myself, that's actually like really awkward phrasing.", "tokens": [50660, 286, 519, 281, 2059, 11, 300, 311, 767, 411, 534, 11411, 7636, 3349, 13, 50808], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 354, "seek": 131154, "start": 1320.42, "end": 1323.6599999999999, "text": " So let me actually delete that, and let me try again.", "tokens": [50808, 407, 718, 385, 767, 12097, 300, 11, 293, 718, 385, 853, 797, 13, 50970], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 355, "seek": 131154, "start": 1323.6599999999999, "end": 1328.3799999999999, "text": " And so as I'm writing, I have this separate process almost inspecting what I'm writing", "tokens": [50970, 400, 370, 382, 286, 478, 3579, 11, 286, 362, 341, 4994, 1399, 1920, 15018, 278, 437, 286, 478, 3579, 51206], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 356, "seek": 131154, "start": 1328.3799999999999, "end": 1331.22, "text": " and judging whether it looks good or not.", "tokens": [51206, 293, 23587, 1968, 309, 1542, 665, 420, 406, 13, 51348], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 357, "seek": 131154, "start": 1331.22, "end": 1334.94, "text": " And then maybe I delete, and maybe I reframe it, and then maybe I'm happy with what comes", "tokens": [51348, 400, 550, 1310, 286, 12097, 11, 293, 1310, 286, 13334, 529, 309, 11, 293, 550, 1310, 286, 478, 2055, 365, 437, 1487, 51534], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 358, "seek": 131154, "start": 1334.94, "end": 1335.94, "text": " out.", "tokens": [51534, 484, 13, 51584], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 359, "seek": 131154, "start": 1335.94, "end": 1340.22, "text": " So basically long story short, a ton happens under the hood in terms of your internal monologue", "tokens": [51584, 407, 1936, 938, 1657, 2099, 11, 257, 2952, 2314, 833, 264, 13376, 294, 2115, 295, 428, 6920, 1108, 42298, 51798], "temperature": 0.0, "avg_logprob": -0.11730192729404994, "compression_ratio": 1.7403846153846154, "no_speech_prob": 0.0011512032942846417}, {"id": 360, "seek": 134022, "start": 1340.22, "end": 1342.22, "text": " when you create sentences like this.", "tokens": [50364, 562, 291, 1884, 16579, 411, 341, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 361, "seek": 134022, "start": 1342.22, "end": 1347.58, "text": " But what does a sentence like this look like when we are training a GPT on it?", "tokens": [50464, 583, 437, 775, 257, 8174, 411, 341, 574, 411, 562, 321, 366, 3097, 257, 26039, 51, 322, 309, 30, 50732], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 362, "seek": 134022, "start": 1347.58, "end": 1350.82, "text": " From GPT's perspective, this is just a sequence of tokens.", "tokens": [50732, 3358, 26039, 51, 311, 4585, 11, 341, 307, 445, 257, 8310, 295, 22667, 13, 50894], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 363, "seek": 134022, "start": 1350.82, "end": 1355.66, "text": " So a GPT when it's reading or generating these tokens, it just goes chunk, chunk, chunk,", "tokens": [50894, 407, 257, 26039, 51, 562, 309, 311, 3760, 420, 17746, 613, 22667, 11, 309, 445, 1709, 16635, 11, 16635, 11, 16635, 11, 51136], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 364, "seek": 134022, "start": 1355.66, "end": 1356.66, "text": " chunk, chunk.", "tokens": [51136, 16635, 11, 16635, 13, 51186], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 365, "seek": 134022, "start": 1356.66, "end": 1360.7, "text": " And each chunk is roughly the same amount of computational work for each token.", "tokens": [51186, 400, 1184, 16635, 307, 9810, 264, 912, 2372, 295, 28270, 589, 337, 1184, 14862, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 366, "seek": 134022, "start": 1360.7, "end": 1363.42, "text": " And these transformers are not very shallow networks.", "tokens": [51388, 400, 613, 4088, 433, 366, 406, 588, 20488, 9590, 13, 51524], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 367, "seek": 134022, "start": 1363.42, "end": 1367.78, "text": " They have about 80 layers of reasoning, but 80 is still not like too much.", "tokens": [51524, 814, 362, 466, 4688, 7914, 295, 21577, 11, 457, 4688, 307, 920, 406, 411, 886, 709, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1143688474382673, "compression_ratio": 1.7295373665480427, "no_speech_prob": 0.012812069617211819}, {"id": 368, "seek": 136778, "start": 1367.78, "end": 1371.66, "text": " And so this transformer is going to do its best to imitate.", "tokens": [50364, 400, 370, 341, 31782, 307, 516, 281, 360, 1080, 1151, 281, 35556, 13, 50558], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 369, "seek": 136778, "start": 1371.66, "end": 1376.78, "text": " But of course, the process here looks very, very different from the process that you took.", "tokens": [50558, 583, 295, 1164, 11, 264, 1399, 510, 1542, 588, 11, 588, 819, 490, 264, 1399, 300, 291, 1890, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 370, "seek": 136778, "start": 1376.78, "end": 1380.8999999999999, "text": " So in particular, in our final artifacts, in the data sets that we create and then eventually", "tokens": [50814, 407, 294, 1729, 11, 294, 527, 2572, 24617, 11, 294, 264, 1412, 6352, 300, 321, 1884, 293, 550, 4728, 51020], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 371, "seek": 136778, "start": 1380.8999999999999, "end": 1386.3, "text": " feed to LLMs, all of that internal dialogue is completely stripped.", "tokens": [51020, 3154, 281, 441, 43, 26386, 11, 439, 295, 300, 6920, 10221, 307, 2584, 33221, 13, 51290], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 372, "seek": 136778, "start": 1386.3, "end": 1390.46, "text": " And unlike you, the GPT will look at every single token and spend the same amount of", "tokens": [51290, 400, 8343, 291, 11, 264, 26039, 51, 486, 574, 412, 633, 2167, 14862, 293, 3496, 264, 912, 2372, 295, 51498], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 373, "seek": 136778, "start": 1390.46, "end": 1392.34, "text": " compute on every one of them.", "tokens": [51498, 14722, 322, 633, 472, 295, 552, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11367605317313716, "compression_ratio": 1.6113207547169812, "no_speech_prob": 0.00011772683501476422}, {"id": 374, "seek": 139234, "start": 1392.34, "end": 1397.54, "text": " And so you can't expect it to actually like, well, you can't expect it to sort of do too", "tokens": [50364, 400, 370, 291, 393, 380, 2066, 309, 281, 767, 411, 11, 731, 11, 291, 393, 380, 2066, 309, 281, 1333, 295, 360, 886, 50624], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 375, "seek": 139234, "start": 1397.54, "end": 1399.54, "text": " much work per token.", "tokens": [50624, 709, 589, 680, 14862, 13, 50724], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 376, "seek": 139234, "start": 1399.54, "end": 1403.98, "text": " So and also in particular, basically these transformers are just like token simulators.", "tokens": [50724, 407, 293, 611, 294, 1729, 11, 1936, 613, 4088, 433, 366, 445, 411, 14862, 1034, 39265, 13, 50946], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 377, "seek": 139234, "start": 1403.98, "end": 1408.1799999999998, "text": " So they don't know what they don't know, like they just imitate the next token.", "tokens": [50946, 407, 436, 500, 380, 458, 437, 436, 500, 380, 458, 11, 411, 436, 445, 35556, 264, 958, 14862, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 378, "seek": 139234, "start": 1408.1799999999998, "end": 1409.86, "text": " They don't know what they're good at or not good at.", "tokens": [51156, 814, 500, 380, 458, 437, 436, 434, 665, 412, 420, 406, 665, 412, 13, 51240], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 379, "seek": 139234, "start": 1409.86, "end": 1412.5, "text": " They just try their best to imitate the next token.", "tokens": [51240, 814, 445, 853, 641, 1151, 281, 35556, 264, 958, 14862, 13, 51372], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 380, "seek": 139234, "start": 1412.5, "end": 1414.26, "text": " They don't reflect in the loop.", "tokens": [51372, 814, 500, 380, 5031, 294, 264, 6367, 13, 51460], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 381, "seek": 139234, "start": 1414.26, "end": 1415.62, "text": " They don't sanity check anything.", "tokens": [51460, 814, 500, 380, 47892, 1520, 1340, 13, 51528], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 382, "seek": 139234, "start": 1415.62, "end": 1418.1, "text": " They don't correct their mistakes along the way by default.", "tokens": [51528, 814, 500, 380, 3006, 641, 8038, 2051, 264, 636, 538, 7576, 13, 51652], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 383, "seek": 139234, "start": 1418.1, "end": 1421.06, "text": " They just sample token sequences.", "tokens": [51652, 814, 445, 6889, 14862, 22978, 13, 51800], "temperature": 0.0, "avg_logprob": -0.11973754564921062, "compression_ratio": 2.0608365019011408, "no_speech_prob": 0.0010482938960194588}, {"id": 384, "seek": 142106, "start": 1421.06, "end": 1423.8999999999999, "text": " They don't have separate inner monologue streams in their head, right?", "tokens": [50364, 814, 500, 380, 362, 4994, 7284, 1108, 42298, 15842, 294, 641, 1378, 11, 558, 30, 50506], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 385, "seek": 142106, "start": 1423.8999999999999, "end": 1425.74, "text": " They're evaluating what's happening.", "tokens": [50506, 814, 434, 27479, 437, 311, 2737, 13, 50598], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 386, "seek": 142106, "start": 1425.74, "end": 1429.58, "text": " Now they do have some sort of cognitive advantages, I would say.", "tokens": [50598, 823, 436, 360, 362, 512, 1333, 295, 15605, 14906, 11, 286, 576, 584, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 387, "seek": 142106, "start": 1429.58, "end": 1433.74, "text": " And that is that they do actually have very large fact-based knowledge across a vast number", "tokens": [50790, 400, 300, 307, 300, 436, 360, 767, 362, 588, 2416, 1186, 12, 6032, 3601, 2108, 257, 8369, 1230, 50998], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 388, "seek": 142106, "start": 1433.74, "end": 1437.1399999999999, "text": " of areas because they have, say, several 10 billion parameters.", "tokens": [50998, 295, 3179, 570, 436, 362, 11, 584, 11, 2940, 1266, 5218, 9834, 13, 51168], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 389, "seek": 142106, "start": 1437.1399999999999, "end": 1440.26, "text": " So that's a lot of storage for a lot of facts.", "tokens": [51168, 407, 300, 311, 257, 688, 295, 6725, 337, 257, 688, 295, 9130, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 390, "seek": 142106, "start": 1440.26, "end": 1444.86, "text": " But and they also, I think, have a relatively large and perfect working memory.", "tokens": [51324, 583, 293, 436, 611, 11, 286, 519, 11, 362, 257, 7226, 2416, 293, 2176, 1364, 4675, 13, 51554], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 391, "seek": 142106, "start": 1444.86, "end": 1450.26, "text": " So whatever fits into the context window is immediately available to the transformer", "tokens": [51554, 407, 2035, 9001, 666, 264, 4319, 4910, 307, 4258, 2435, 281, 264, 31782, 51824], "temperature": 0.0, "avg_logprob": -0.12951761484146118, "compression_ratio": 1.646341463414634, "no_speech_prob": 0.004466644022613764}, {"id": 392, "seek": 145026, "start": 1450.26, "end": 1452.7, "text": " through its internal self-attention mechanism.", "tokens": [50364, 807, 1080, 6920, 2698, 12, 1591, 1251, 7513, 13, 50486], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 393, "seek": 145026, "start": 1452.7, "end": 1456.66, "text": " And so it's kind of like perfect memory, but it's got a finite size.", "tokens": [50486, 400, 370, 309, 311, 733, 295, 411, 2176, 4675, 11, 457, 309, 311, 658, 257, 19362, 2744, 13, 50684], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 394, "seek": 145026, "start": 1456.66, "end": 1458.78, "text": " But the transformer has a very direct access to it.", "tokens": [50684, 583, 264, 31782, 575, 257, 588, 2047, 2105, 281, 309, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 395, "seek": 145026, "start": 1458.78, "end": 1463.42, "text": " And so it can, like, losslessly remember anything that is inside its context window.", "tokens": [50790, 400, 370, 309, 393, 11, 411, 11, 4470, 12048, 1604, 1340, 300, 307, 1854, 1080, 4319, 4910, 13, 51022], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 396, "seek": 145026, "start": 1463.42, "end": 1466.1, "text": " So that's kind of how I would compare those two.", "tokens": [51022, 407, 300, 311, 733, 295, 577, 286, 576, 6794, 729, 732, 13, 51156], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 397, "seek": 145026, "start": 1466.1, "end": 1470.58, "text": " And the reason I bring all of this up is because I think to a large extent, prompting is just", "tokens": [51156, 400, 264, 1778, 286, 1565, 439, 295, 341, 493, 307, 570, 286, 519, 281, 257, 2416, 8396, 11, 12391, 278, 307, 445, 51380], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 398, "seek": 145026, "start": 1470.58, "end": 1477.46, "text": " making up for this sort of cognitive difference between these two kind of architectures, like", "tokens": [51380, 1455, 493, 337, 341, 1333, 295, 15605, 2649, 1296, 613, 732, 733, 295, 6331, 1303, 11, 411, 51724], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 399, "seek": 145026, "start": 1477.46, "end": 1479.7, "text": " our brains here and LLM brains.", "tokens": [51724, 527, 15442, 510, 293, 441, 43, 44, 15442, 13, 51836], "temperature": 0.0, "avg_logprob": -0.12688485899967933, "compression_ratio": 1.6645367412140575, "no_speech_prob": 0.00073203444480896}, {"id": 400, "seek": 147970, "start": 1479.7, "end": 1482.26, "text": " You can look at it that way almost.", "tokens": [50364, 509, 393, 574, 412, 309, 300, 636, 1920, 13, 50492], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 401, "seek": 147970, "start": 1482.26, "end": 1485.94, "text": " So here's one thing that people found, for example, works pretty well in practice.", "tokens": [50492, 407, 510, 311, 472, 551, 300, 561, 1352, 11, 337, 1365, 11, 1985, 1238, 731, 294, 3124, 13, 50676], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 402, "seek": 147970, "start": 1485.94, "end": 1490.5800000000002, "text": " Especially if your tasks require reasoning, you can't expect the transformer to do too", "tokens": [50676, 8545, 498, 428, 9608, 3651, 21577, 11, 291, 393, 380, 2066, 264, 31782, 281, 360, 886, 50908], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 403, "seek": 147970, "start": 1490.5800000000002, "end": 1492.54, "text": " much reasoning per token.", "tokens": [50908, 709, 21577, 680, 14862, 13, 51006], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 404, "seek": 147970, "start": 1492.54, "end": 1496.22, "text": " And so you have to really spread out the reasoning across more and more tokens.", "tokens": [51006, 400, 370, 291, 362, 281, 534, 3974, 484, 264, 21577, 2108, 544, 293, 544, 22667, 13, 51190], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 405, "seek": 147970, "start": 1496.22, "end": 1499.5, "text": " So for example, you can't give the transformer a very complicated question and expect it", "tokens": [51190, 407, 337, 1365, 11, 291, 393, 380, 976, 264, 31782, 257, 588, 6179, 1168, 293, 2066, 309, 51354], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 406, "seek": 147970, "start": 1499.5, "end": 1500.9, "text": " to get the answer in a single token.", "tokens": [51354, 281, 483, 264, 1867, 294, 257, 2167, 14862, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 407, "seek": 147970, "start": 1500.9, "end": 1503.02, "text": " There's just not enough time for it.", "tokens": [51424, 821, 311, 445, 406, 1547, 565, 337, 309, 13, 51530], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 408, "seek": 147970, "start": 1503.02, "end": 1507.14, "text": " These transformers need tokens to think, quote, unquote, I like to say sometimes.", "tokens": [51530, 1981, 4088, 433, 643, 22667, 281, 519, 11, 6513, 11, 37557, 11, 286, 411, 281, 584, 2171, 13, 51736], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 409, "seek": 147970, "start": 1507.14, "end": 1509.14, "text": " And so this is some of the things that work well.", "tokens": [51736, 400, 370, 341, 307, 512, 295, 264, 721, 300, 589, 731, 13, 51836], "temperature": 0.0, "avg_logprob": -0.1080106141551441, "compression_ratio": 1.8475609756097562, "no_speech_prob": 0.0025496636517345905}, {"id": 410, "seek": 150914, "start": 1509.14, "end": 1512.42, "text": " You may, for example, have a few-shot prompt that shows the transformer that it should", "tokens": [50364, 509, 815, 11, 337, 1365, 11, 362, 257, 1326, 12, 18402, 12391, 300, 3110, 264, 31782, 300, 309, 820, 50528], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 411, "seek": 150914, "start": 1512.42, "end": 1516.1000000000001, "text": " like show its work when it's answering the question.", "tokens": [50528, 411, 855, 1080, 589, 562, 309, 311, 13430, 264, 1168, 13, 50712], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 412, "seek": 150914, "start": 1516.1000000000001, "end": 1520.7800000000002, "text": " And if you give a few examples, the transformer will imitate that template, and it will just", "tokens": [50712, 400, 498, 291, 976, 257, 1326, 5110, 11, 264, 31782, 486, 35556, 300, 12379, 11, 293, 309, 486, 445, 50946], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 413, "seek": 150914, "start": 1520.7800000000002, "end": 1524.38, "text": " end up working out better in terms of its evaluation.", "tokens": [50946, 917, 493, 1364, 484, 1101, 294, 2115, 295, 1080, 13344, 13, 51126], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 414, "seek": 150914, "start": 1524.38, "end": 1528.26, "text": " Additionally, you can elicit this kind of behavior from the transformer by saying, let's", "tokens": [51126, 19927, 11, 291, 393, 806, 8876, 341, 733, 295, 5223, 490, 264, 31782, 538, 1566, 11, 718, 311, 51320], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 415, "seek": 150914, "start": 1528.26, "end": 1532.9, "text": " think step by step, because this conditioned the transformer into sort of like showing", "tokens": [51320, 519, 1823, 538, 1823, 11, 570, 341, 35833, 264, 31782, 666, 1333, 295, 411, 4099, 51552], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 416, "seek": 150914, "start": 1532.9, "end": 1533.9, "text": " its work.", "tokens": [51552, 1080, 589, 13, 51602], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 417, "seek": 150914, "start": 1533.9, "end": 1538.0200000000002, "text": " And because it kind of snaps into a mode of showing its work, it's going to do less", "tokens": [51602, 400, 570, 309, 733, 295, 19206, 666, 257, 4391, 295, 4099, 1080, 589, 11, 309, 311, 516, 281, 360, 1570, 51808], "temperature": 0.0, "avg_logprob": -0.12261138745208285, "compression_ratio": 1.9372822299651569, "no_speech_prob": 0.0006460978765971959}, {"id": 418, "seek": 153802, "start": 1538.02, "end": 1540.26, "text": " computational work per token.", "tokens": [50364, 28270, 589, 680, 14862, 13, 50476], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 419, "seek": 153802, "start": 1540.26, "end": 1545.1, "text": " And so it's more likely to succeed as a result, because it's making slower reasoning over", "tokens": [50476, 400, 370, 309, 311, 544, 3700, 281, 7754, 382, 257, 1874, 11, 570, 309, 311, 1455, 14009, 21577, 670, 50718], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 420, "seek": 153802, "start": 1545.1, "end": 1546.74, "text": " time.", "tokens": [50718, 565, 13, 50800], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 421, "seek": 153802, "start": 1546.74, "end": 1547.74, "text": " Here's another example.", "tokens": [50800, 1692, 311, 1071, 1365, 13, 50850], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 422, "seek": 153802, "start": 1547.74, "end": 1550.06, "text": " This one is called self-consistency.", "tokens": [50850, 639, 472, 307, 1219, 2698, 12, 21190, 468, 3020, 13, 50966], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 423, "seek": 153802, "start": 1550.06, "end": 1552.94, "text": " We saw that we had the ability to start writing.", "tokens": [50966, 492, 1866, 300, 321, 632, 264, 3485, 281, 722, 3579, 13, 51110], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 424, "seek": 153802, "start": 1552.94, "end": 1555.18, "text": " And then if it didn't work out, I can try again.", "tokens": [51110, 400, 550, 498, 309, 994, 380, 589, 484, 11, 286, 393, 853, 797, 13, 51222], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 425, "seek": 153802, "start": 1555.18, "end": 1560.74, "text": " And I can try multiple times and maybe select the one that worked best.", "tokens": [51222, 400, 286, 393, 853, 3866, 1413, 293, 1310, 3048, 264, 472, 300, 2732, 1151, 13, 51500], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 426, "seek": 153802, "start": 1560.74, "end": 1564.7, "text": " So in these kinds of approaches, you may sample not just once, but you may sample multiple", "tokens": [51500, 407, 294, 613, 3685, 295, 11587, 11, 291, 815, 6889, 406, 445, 1564, 11, 457, 291, 815, 6889, 3866, 51698], "temperature": 0.0, "avg_logprob": -0.12064385414123535, "compression_ratio": 1.6494464944649447, "no_speech_prob": 0.0014548548497259617}, {"id": 427, "seek": 156470, "start": 1564.7, "end": 1569.38, "text": " times, and then have some process for finding the ones that are good and then keeping just", "tokens": [50364, 1413, 11, 293, 550, 362, 512, 1399, 337, 5006, 264, 2306, 300, 366, 665, 293, 550, 5145, 445, 50598], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 428, "seek": 156470, "start": 1569.38, "end": 1572.18, "text": " those samples or doing a majority vote or something like that.", "tokens": [50598, 729, 10938, 420, 884, 257, 6286, 4740, 420, 746, 411, 300, 13, 50738], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 429, "seek": 156470, "start": 1572.18, "end": 1576.38, "text": " So basically, these transformers in the process, as they predict the next token, just like", "tokens": [50738, 407, 1936, 11, 613, 4088, 433, 294, 264, 1399, 11, 382, 436, 6069, 264, 958, 14862, 11, 445, 411, 50948], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 430, "seek": 156470, "start": 1576.38, "end": 1578.14, "text": " you, they can get unlucky.", "tokens": [50948, 291, 11, 436, 393, 483, 38838, 13, 51036], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 431, "seek": 156470, "start": 1578.14, "end": 1582.14, "text": " And they could sample a not a very good token, and they can go down sort of like a blind", "tokens": [51036, 400, 436, 727, 6889, 257, 406, 257, 588, 665, 14862, 11, 293, 436, 393, 352, 760, 1333, 295, 411, 257, 6865, 51236], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 432, "seek": 156470, "start": 1582.14, "end": 1584.1000000000001, "text": " alley in terms of reasoning.", "tokens": [51236, 26660, 294, 2115, 295, 21577, 13, 51334], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 433, "seek": 156470, "start": 1584.1000000000001, "end": 1587.3400000000001, "text": " And so unlike you, they cannot recover from that.", "tokens": [51334, 400, 370, 8343, 291, 11, 436, 2644, 8114, 490, 300, 13, 51496], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 434, "seek": 156470, "start": 1587.3400000000001, "end": 1591.02, "text": " They are stuck with every single token they sample, and so they will continue the sequence", "tokens": [51496, 814, 366, 5541, 365, 633, 2167, 14862, 436, 6889, 11, 293, 370, 436, 486, 2354, 264, 8310, 51680], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 435, "seek": 156470, "start": 1591.02, "end": 1594.3400000000001, "text": " even if they even know that this sequence is not going to work out.", "tokens": [51680, 754, 498, 436, 754, 458, 300, 341, 8310, 307, 406, 516, 281, 589, 484, 13, 51846], "temperature": 0.0, "avg_logprob": -0.12548565537962195, "compression_ratio": 1.8746081504702194, "no_speech_prob": 0.0023964045103639364}, {"id": 436, "seek": 159434, "start": 1594.3799999999999, "end": 1601.5, "text": " So give them the ability to look back, inspect, or try to basically sample around it.", "tokens": [50366, 407, 976, 552, 264, 3485, 281, 574, 646, 11, 15018, 11, 420, 853, 281, 1936, 6889, 926, 309, 13, 50722], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 437, "seek": 159434, "start": 1601.5, "end": 1603.86, "text": " Here's one technique also.", "tokens": [50722, 1692, 311, 472, 6532, 611, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 438, "seek": 159434, "start": 1603.86, "end": 1607.62, "text": " It turns out that actually LLMs, like they know when they've screwed up.", "tokens": [50840, 467, 4523, 484, 300, 767, 441, 43, 26386, 11, 411, 436, 458, 562, 436, 600, 20331, 493, 13, 51028], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 439, "seek": 159434, "start": 1607.62, "end": 1613.6999999999998, "text": " So as an example, say you ask the model to generate a poem that does not rhyme.", "tokens": [51028, 407, 382, 364, 1365, 11, 584, 291, 1029, 264, 2316, 281, 8460, 257, 13065, 300, 775, 406, 34753, 13, 51332], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 440, "seek": 159434, "start": 1613.6999999999998, "end": 1616.22, "text": " And it might give you a poem, but it actually rhymes.", "tokens": [51332, 400, 309, 1062, 976, 291, 257, 13065, 11, 457, 309, 767, 47917, 13, 51458], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 441, "seek": 159434, "start": 1616.22, "end": 1619.9399999999998, "text": " But it turns out that especially for the bigger models like GPT-4, you can just ask it, did", "tokens": [51458, 583, 309, 4523, 484, 300, 2318, 337, 264, 3801, 5245, 411, 26039, 51, 12, 19, 11, 291, 393, 445, 1029, 309, 11, 630, 51644], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 442, "seek": 159434, "start": 1619.9399999999998, "end": 1621.3, "text": " you meet the assignment?", "tokens": [51644, 291, 1677, 264, 15187, 30, 51712], "temperature": 0.0, "avg_logprob": -0.14002575593836167, "compression_ratio": 1.6088560885608856, "no_speech_prob": 0.0013455934822559357}, {"id": 443, "seek": 162130, "start": 1621.34, "end": 1625.06, "text": " And actually GPT-4 knows very well that it did not meet the assignment.", "tokens": [50366, 400, 767, 26039, 51, 12, 19, 3255, 588, 731, 300, 309, 630, 406, 1677, 264, 15187, 13, 50552], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 444, "seek": 162130, "start": 1625.06, "end": 1627.46, "text": " It just kind of got unlucky in its sampling.", "tokens": [50552, 467, 445, 733, 295, 658, 38838, 294, 1080, 21179, 13, 50672], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 445, "seek": 162130, "start": 1627.46, "end": 1629.62, "text": " And so it will tell you, no, I didn't actually meet the assignment.", "tokens": [50672, 400, 370, 309, 486, 980, 291, 11, 572, 11, 286, 994, 380, 767, 1677, 264, 15187, 13, 50780], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 446, "seek": 162130, "start": 1629.62, "end": 1631.26, "text": " Here's, let me try again.", "tokens": [50780, 1692, 311, 11, 718, 385, 853, 797, 13, 50862], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 447, "seek": 162130, "start": 1631.26, "end": 1638.18, "text": " But without you prompting it, it doesn't even, like it doesn't know to revisit and so on.", "tokens": [50862, 583, 1553, 291, 12391, 278, 309, 11, 309, 1177, 380, 754, 11, 411, 309, 1177, 380, 458, 281, 32676, 293, 370, 322, 13, 51208], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 448, "seek": 162130, "start": 1638.18, "end": 1640.22, "text": " So you have to make up for that in your prompts.", "tokens": [51208, 407, 291, 362, 281, 652, 493, 337, 300, 294, 428, 41095, 13, 51310], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 449, "seek": 162130, "start": 1640.22, "end": 1642.18, "text": " You have to get it to check.", "tokens": [51310, 509, 362, 281, 483, 309, 281, 1520, 13, 51408], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 450, "seek": 162130, "start": 1642.18, "end": 1645.74, "text": " If you don't ask it to check, it's not going to check by itself, it's just a token simulator.", "tokens": [51408, 759, 291, 500, 380, 1029, 309, 281, 1520, 11, 309, 311, 406, 516, 281, 1520, 538, 2564, 11, 309, 311, 445, 257, 14862, 32974, 13, 51586], "temperature": 0.0, "avg_logprob": -0.14568782198256341, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.00040445549529977143}, {"id": 451, "seek": 164574, "start": 1646.74, "end": 1653.5, "text": " I think more generally, a lot of these techniques fall into the bucket of what I would say recreating", "tokens": [50414, 286, 519, 544, 5101, 11, 257, 688, 295, 613, 7512, 2100, 666, 264, 13058, 295, 437, 286, 576, 584, 850, 44613, 50752], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 452, "seek": 164574, "start": 1653.5, "end": 1654.82, "text": " our system two.", "tokens": [50752, 527, 1185, 732, 13, 50818], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 453, "seek": 164574, "start": 1654.82, "end": 1658.26, "text": " So you might be familiar with the system one, system two thinking for humans.", "tokens": [50818, 407, 291, 1062, 312, 4963, 365, 264, 1185, 472, 11, 1185, 732, 1953, 337, 6255, 13, 50990], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 454, "seek": 164574, "start": 1658.26, "end": 1660.3, "text": " System one is a fast automatic process.", "tokens": [50990, 8910, 472, 307, 257, 2370, 12509, 1399, 13, 51092], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 455, "seek": 164574, "start": 1660.3, "end": 1664.02, "text": " And I think kind of corresponds to like an LLM just sampling tokens.", "tokens": [51092, 400, 286, 519, 733, 295, 23249, 281, 411, 364, 441, 43, 44, 445, 21179, 22667, 13, 51278], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 456, "seek": 164574, "start": 1664.02, "end": 1669.5, "text": " And system two is the slower, deliberate planning sort of part of your brain.", "tokens": [51278, 400, 1185, 732, 307, 264, 14009, 11, 30515, 5038, 1333, 295, 644, 295, 428, 3567, 13, 51552], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 457, "seek": 164574, "start": 1669.5, "end": 1673.38, "text": " And so this is a paper actually from just last week, because this space is pretty quickly", "tokens": [51552, 400, 370, 341, 307, 257, 3035, 767, 490, 445, 1036, 1243, 11, 570, 341, 1901, 307, 1238, 2661, 51746], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 458, "seek": 164574, "start": 1673.38, "end": 1674.38, "text": " evolving.", "tokens": [51746, 21085, 13, 51796], "temperature": 0.0, "avg_logprob": -0.16522002019802062, "compression_ratio": 1.6912280701754385, "no_speech_prob": 0.00026942865224555135}, {"id": 459, "seek": 167438, "start": 1675.38, "end": 1680.7800000000002, "text": " And in tree of thought, the authors of this paper proposed maintaining multiple completions", "tokens": [50414, 400, 294, 4230, 295, 1194, 11, 264, 16552, 295, 341, 3035, 10348, 14916, 3866, 1557, 626, 50684], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 460, "seek": 167438, "start": 1680.7800000000002, "end": 1682.6200000000001, "text": " for any given prompt.", "tokens": [50684, 337, 604, 2212, 12391, 13, 50776], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 461, "seek": 167438, "start": 1682.6200000000001, "end": 1686.5, "text": " And then they are also scoring them along the way and keeping the ones that are going", "tokens": [50776, 400, 550, 436, 366, 611, 22358, 552, 2051, 264, 636, 293, 5145, 264, 2306, 300, 366, 516, 50970], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 462, "seek": 167438, "start": 1686.5, "end": 1688.3400000000001, "text": " well, if that makes sense.", "tokens": [50970, 731, 11, 498, 300, 1669, 2020, 13, 51062], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 463, "seek": 167438, "start": 1688.3400000000001, "end": 1694.94, "text": " And so a lot of people are like really playing around with kind of prompt engineering to", "tokens": [51062, 400, 370, 257, 688, 295, 561, 366, 411, 534, 2433, 926, 365, 733, 295, 12391, 7043, 281, 51392], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 464, "seek": 167438, "start": 1694.94, "end": 1699.98, "text": " basically bring back some of these abilities that we sort of have in our brain for LLMs.", "tokens": [51392, 1936, 1565, 646, 512, 295, 613, 11582, 300, 321, 1333, 295, 362, 294, 527, 3567, 337, 441, 43, 26386, 13, 51644], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 465, "seek": 167438, "start": 1699.98, "end": 1703.0200000000002, "text": " Now one thing I would like to note here is that this is not just a prompt.", "tokens": [51644, 823, 472, 551, 286, 576, 411, 281, 3637, 510, 307, 300, 341, 307, 406, 445, 257, 12391, 13, 51796], "temperature": 0.0, "avg_logprob": -0.15464046083647628, "compression_ratio": 1.7168458781362008, "no_speech_prob": 0.000791506958194077}, {"id": 466, "seek": 170302, "start": 1703.02, "end": 1708.06, "text": " This is actually prompts that are together used with some Python glue code because you", "tokens": [50364, 639, 307, 767, 41095, 300, 366, 1214, 1143, 365, 512, 15329, 8998, 3089, 570, 291, 50616], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 467, "seek": 170302, "start": 1708.06, "end": 1711.18, "text": " don't, you actually have to maintain multiple prompts and you also have to do some tree", "tokens": [50616, 500, 380, 11, 291, 767, 362, 281, 6909, 3866, 41095, 293, 291, 611, 362, 281, 360, 512, 4230, 50772], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 468, "seek": 170302, "start": 1711.18, "end": 1715.62, "text": " search algorithm here to like figure out which prompts to expand, et cetera.", "tokens": [50772, 3164, 9284, 510, 281, 411, 2573, 484, 597, 41095, 281, 5268, 11, 1030, 11458, 13, 50994], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 469, "seek": 170302, "start": 1715.62, "end": 1720.26, "text": " So it's a symbiosis of Python glue code and individual prompts that are called in a Y", "tokens": [50994, 407, 309, 311, 257, 43700, 48783, 295, 15329, 8998, 3089, 293, 2609, 41095, 300, 366, 1219, 294, 257, 398, 51226], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 470, "seek": 170302, "start": 1720.26, "end": 1722.54, "text": " loop or in a bigger algorithm.", "tokens": [51226, 6367, 420, 294, 257, 3801, 9284, 13, 51340], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 471, "seek": 170302, "start": 1722.54, "end": 1725.22, "text": " I also think there's a really cool parallel here to AlphaGo.", "tokens": [51340, 286, 611, 519, 456, 311, 257, 534, 1627, 8952, 510, 281, 20588, 12104, 13, 51474], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 472, "seek": 170302, "start": 1725.22, "end": 1729.74, "text": " AlphaGo has a policy for placing the next stone when it plays go and this policy was", "tokens": [51474, 20588, 12104, 575, 257, 3897, 337, 17221, 264, 958, 7581, 562, 309, 5749, 352, 293, 341, 3897, 390, 51700], "temperature": 0.0, "avg_logprob": -0.17971360878866227, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.00043050170643255115}, {"id": 473, "seek": 172974, "start": 1729.74, "end": 1732.74, "text": " trained originally by imitating humans.", "tokens": [50364, 8895, 7993, 538, 566, 16350, 6255, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 474, "seek": 172974, "start": 1732.74, "end": 1737.3, "text": " But in addition to this policy, it also does multi-carreler tree search and basically it", "tokens": [50514, 583, 294, 4500, 281, 341, 3897, 11, 309, 611, 775, 4825, 12, 6166, 265, 1918, 4230, 3164, 293, 1936, 309, 50742], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 475, "seek": 172974, "start": 1737.3, "end": 1740.74, "text": " will play out a number of possibilities in its head and evaluate all of them and only", "tokens": [50742, 486, 862, 484, 257, 1230, 295, 12178, 294, 1080, 1378, 293, 13059, 439, 295, 552, 293, 787, 50914], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 476, "seek": 172974, "start": 1740.74, "end": 1742.06, "text": " keep the ones that work well.", "tokens": [50914, 1066, 264, 2306, 300, 589, 731, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 477, "seek": 172974, "start": 1742.06, "end": 1749.02, "text": " And so I think this is kind of an equivalent of AlphaGo, but for text, if that makes sense.", "tokens": [50980, 400, 370, 286, 519, 341, 307, 733, 295, 364, 10344, 295, 20588, 12104, 11, 457, 337, 2487, 11, 498, 300, 1669, 2020, 13, 51328], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 478, "seek": 172974, "start": 1749.02, "end": 1752.58, "text": " So just like tree of thought, I think more generally people are starting to like really", "tokens": [51328, 407, 445, 411, 4230, 295, 1194, 11, 286, 519, 544, 5101, 561, 366, 2891, 281, 411, 534, 51506], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 479, "seek": 172974, "start": 1752.58, "end": 1757.74, "text": " explore more general techniques of not just the simple question and answer prompts, but", "tokens": [51506, 6839, 544, 2674, 7512, 295, 406, 445, 264, 2199, 1168, 293, 1867, 41095, 11, 457, 51764], "temperature": 0.0, "avg_logprob": -0.1528682865080286, "compression_ratio": 1.6623376623376624, "no_speech_prob": 0.0005613631219603121}, {"id": 480, "seek": 175774, "start": 1757.74, "end": 1762.22, "text": " something that looks a lot more like Python glue code stringing together many prompts.", "tokens": [50364, 746, 300, 1542, 257, 688, 544, 411, 15329, 8998, 3089, 6798, 278, 1214, 867, 41095, 13, 50588], "temperature": 0.0, "avg_logprob": -0.14639659763611468, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004903410095721483}, {"id": 481, "seek": 175774, "start": 1762.22, "end": 1767.6200000000001, "text": " So on the right, I have an example from this paper called React where they structure the", "tokens": [50588, 407, 322, 264, 558, 11, 286, 362, 364, 1365, 490, 341, 3035, 1219, 30644, 689, 436, 3877, 264, 50858], "temperature": 0.0, "avg_logprob": -0.14639659763611468, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004903410095721483}, {"id": 482, "seek": 175774, "start": 1767.6200000000001, "end": 1774.26, "text": " answer to a prompt as a sequence of thought, action, observation, thought, action, observation", "tokens": [50858, 1867, 281, 257, 12391, 382, 257, 8310, 295, 1194, 11, 3069, 11, 14816, 11, 1194, 11, 3069, 11, 14816, 51190], "temperature": 0.0, "avg_logprob": -0.14639659763611468, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004903410095721483}, {"id": 483, "seek": 175774, "start": 1774.26, "end": 1778.66, "text": " and it's a full rollout, kind of a thinking process to answer the query.", "tokens": [51190, 293, 309, 311, 257, 1577, 3373, 346, 11, 733, 295, 257, 1953, 1399, 281, 1867, 264, 14581, 13, 51410], "temperature": 0.0, "avg_logprob": -0.14639659763611468, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004903410095721483}, {"id": 484, "seek": 175774, "start": 1778.66, "end": 1782.54, "text": " And in these actions, the model is also allowed to tool use.", "tokens": [51410, 400, 294, 613, 5909, 11, 264, 2316, 307, 611, 4350, 281, 2290, 764, 13, 51604], "temperature": 0.0, "avg_logprob": -0.14639659763611468, "compression_ratio": 1.6833333333333333, "no_speech_prob": 0.004903410095721483}, {"id": 485, "seek": 178254, "start": 1782.54, "end": 1788.3799999999999, "text": " On the left, I have an example of auto-GPT and now auto-GPT, by the way, is a project", "tokens": [50364, 1282, 264, 1411, 11, 286, 362, 364, 1365, 295, 8399, 12, 38, 47, 51, 293, 586, 8399, 12, 38, 47, 51, 11, 538, 264, 636, 11, 307, 257, 1716, 50656], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 486, "seek": 178254, "start": 1788.3799999999999, "end": 1793.78, "text": " that I think got a lot of hype recently and I think, but I think I still find it kind", "tokens": [50656, 300, 286, 519, 658, 257, 688, 295, 24144, 3938, 293, 286, 519, 11, 457, 286, 519, 286, 920, 915, 309, 733, 50926], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 487, "seek": 178254, "start": 1793.78, "end": 1796.22, "text": " of inspirationally interesting.", "tokens": [50926, 295, 10249, 379, 1880, 13, 51048], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 488, "seek": 178254, "start": 1796.22, "end": 1801.18, "text": " It's a project that allows an LLM to keep task list and continue to recursively break", "tokens": [51048, 467, 311, 257, 1716, 300, 4045, 364, 441, 43, 44, 281, 1066, 5633, 1329, 293, 2354, 281, 20560, 3413, 1821, 51296], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 489, "seek": 178254, "start": 1801.18, "end": 1805.22, "text": " down tasks and I don't think this currently works very well and I would not advise people", "tokens": [51296, 760, 9608, 293, 286, 500, 380, 519, 341, 4362, 1985, 588, 731, 293, 286, 576, 406, 18312, 561, 51498], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 490, "seek": 178254, "start": 1805.22, "end": 1807.22, "text": " to use it in practical applications.", "tokens": [51498, 281, 764, 309, 294, 8496, 5821, 13, 51598], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 491, "seek": 178254, "start": 1807.22, "end": 1809.98, "text": " I just think it's something to generally take inspiration from in terms of where this", "tokens": [51598, 286, 445, 519, 309, 311, 746, 281, 5101, 747, 10249, 490, 294, 2115, 295, 689, 341, 51736], "temperature": 0.0, "avg_logprob": -0.19435223666104404, "compression_ratio": 1.7074829931972788, "no_speech_prob": 0.011154009029269218}, {"id": 492, "seek": 180998, "start": 1810.06, "end": 1813.26, "text": " is going, I think, over time.", "tokens": [50368, 307, 516, 11, 286, 519, 11, 670, 565, 13, 50528], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 493, "seek": 180998, "start": 1813.26, "end": 1816.5, "text": " So that's kind of like giving our model system to thinking.", "tokens": [50528, 407, 300, 311, 733, 295, 411, 2902, 527, 2316, 1185, 281, 1953, 13, 50690], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 494, "seek": 180998, "start": 1816.5, "end": 1821.02, "text": " The next thing that I find kind of interesting is this following sort of, I would say, almost", "tokens": [50690, 440, 958, 551, 300, 286, 915, 733, 295, 1880, 307, 341, 3480, 1333, 295, 11, 286, 576, 584, 11, 1920, 50916], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 495, "seek": 180998, "start": 1821.02, "end": 1826.78, "text": " psychological quirk of LLMs is that LLMs don't want to succeed.", "tokens": [50916, 14346, 421, 18610, 295, 441, 43, 26386, 307, 300, 441, 43, 26386, 500, 380, 528, 281, 7754, 13, 51204], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 496, "seek": 180998, "start": 1826.78, "end": 1828.7, "text": " They want to imitate.", "tokens": [51204, 814, 528, 281, 35556, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 497, "seek": 180998, "start": 1828.7, "end": 1831.42, "text": " You want to succeed and you should ask for it.", "tokens": [51300, 509, 528, 281, 7754, 293, 291, 820, 1029, 337, 309, 13, 51436], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 498, "seek": 180998, "start": 1831.42, "end": 1837.82, "text": " So what I mean by that is when transformers are trained, they have training sets and there", "tokens": [51436, 407, 437, 286, 914, 538, 300, 307, 562, 4088, 433, 366, 8895, 11, 436, 362, 3097, 6352, 293, 456, 51756], "temperature": 0.0, "avg_logprob": -0.14204433372428826, "compression_ratio": 1.6748971193415638, "no_speech_prob": 0.0021818496752530336}, {"id": 499, "seek": 183782, "start": 1837.82, "end": 1841.62, "text": " can be an entire spectrum of performance qualities in their training data.", "tokens": [50364, 393, 312, 364, 2302, 11143, 295, 3389, 16477, 294, 641, 3097, 1412, 13, 50554], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 500, "seek": 183782, "start": 1841.62, "end": 1844.8999999999999, "text": " So for example, there could be some kind of a prompt for some physics question or something", "tokens": [50554, 407, 337, 1365, 11, 456, 727, 312, 512, 733, 295, 257, 12391, 337, 512, 10649, 1168, 420, 746, 50718], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 501, "seek": 183782, "start": 1844.8999999999999, "end": 1848.5, "text": " like that and there could be a student solution that is completely wrong, but there can also", "tokens": [50718, 411, 300, 293, 456, 727, 312, 257, 3107, 3827, 300, 307, 2584, 2085, 11, 457, 456, 393, 611, 50898], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 502, "seek": 183782, "start": 1848.5, "end": 1851.26, "text": " be an expert answer that is extremely right.", "tokens": [50898, 312, 364, 5844, 1867, 300, 307, 4664, 558, 13, 51036], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 503, "seek": 183782, "start": 1851.26, "end": 1856.34, "text": " And transformers can't tell the difference between, like, they know about low-quality", "tokens": [51036, 400, 4088, 433, 393, 380, 980, 264, 2649, 1296, 11, 411, 11, 436, 458, 466, 2295, 12, 11286, 51290], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 504, "seek": 183782, "start": 1856.34, "end": 1860.1799999999998, "text": " solutions and high-quality solutions, but by default, they want to imitate all of it", "tokens": [51290, 6547, 293, 1090, 12, 11286, 6547, 11, 457, 538, 7576, 11, 436, 528, 281, 35556, 439, 295, 309, 51482], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 505, "seek": 183782, "start": 1860.1799999999998, "end": 1862.54, "text": " because they're just trained on language modeling.", "tokens": [51482, 570, 436, 434, 445, 8895, 322, 2856, 15983, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 506, "seek": 183782, "start": 1862.54, "end": 1866.3799999999999, "text": " And so at test time, you actually have to ask for a good performance.", "tokens": [51600, 400, 370, 412, 1500, 565, 11, 291, 767, 362, 281, 1029, 337, 257, 665, 3389, 13, 51792], "temperature": 0.0, "avg_logprob": -0.13458967556918625, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0008826380944810808}, {"id": 507, "seek": 186638, "start": 1866.38, "end": 1872.18, "text": " So in this example in this paper, they tried various prompts and let's think step-by-step", "tokens": [50364, 407, 294, 341, 1365, 294, 341, 3035, 11, 436, 3031, 3683, 41095, 293, 718, 311, 519, 1823, 12, 2322, 12, 16792, 50654], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 508, "seek": 186638, "start": 1872.18, "end": 1876.0200000000002, "text": " was very powerful because it sort of spread out the reasoning over many tokens, but what", "tokens": [50654, 390, 588, 4005, 570, 309, 1333, 295, 3974, 484, 264, 21577, 670, 867, 22667, 11, 457, 437, 50846], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 509, "seek": 186638, "start": 1876.0200000000002, "end": 1879.98, "text": " worked even better is let's work this out in a step-by-step way to be sure we have the", "tokens": [50846, 2732, 754, 1101, 307, 718, 311, 589, 341, 484, 294, 257, 1823, 12, 2322, 12, 16792, 636, 281, 312, 988, 321, 362, 264, 51044], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 510, "seek": 186638, "start": 1879.98, "end": 1881.1000000000001, "text": " right answer.", "tokens": [51044, 558, 1867, 13, 51100], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 511, "seek": 186638, "start": 1881.1000000000001, "end": 1884.6200000000001, "text": " And so it's kind of like conditioning on getting a right answer and this actually makes the", "tokens": [51100, 400, 370, 309, 311, 733, 295, 411, 21901, 322, 1242, 257, 558, 1867, 293, 341, 767, 1669, 264, 51276], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 512, "seek": 186638, "start": 1884.6200000000001, "end": 1889.7800000000002, "text": " transformer work better because the transformer doesn't have to now hedge its probability mass", "tokens": [51276, 31782, 589, 1101, 570, 264, 31782, 1177, 380, 362, 281, 586, 25304, 1080, 8482, 2758, 51534], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 513, "seek": 186638, "start": 1889.7800000000002, "end": 1893.0600000000002, "text": " on low-quality solutions as ridiculous as that sounds.", "tokens": [51534, 322, 2295, 12, 11286, 6547, 382, 11083, 382, 300, 3263, 13, 51698], "temperature": 0.0, "avg_logprob": -0.12908443327872984, "compression_ratio": 1.7781569965870307, "no_speech_prob": 0.003322931472212076}, {"id": 514, "seek": 189306, "start": 1893.06, "end": 1898.62, "text": " And so basically, feel free to ask for a strong solution, say something like, you are a leading", "tokens": [50364, 400, 370, 1936, 11, 841, 1737, 281, 1029, 337, 257, 2068, 3827, 11, 584, 746, 411, 11, 291, 366, 257, 5775, 50642], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 515, "seek": 189306, "start": 1898.62, "end": 1902.3, "text": " expert on this topic, pretend you have IQ 120, et cetera.", "tokens": [50642, 5844, 322, 341, 4829, 11, 11865, 291, 362, 28921, 10411, 11, 1030, 11458, 13, 50826], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 516, "seek": 189306, "start": 1902.3, "end": 1906.74, "text": " But don't try to ask for too much IQ because if you ask for an IQ of, like, 400, you might", "tokens": [50826, 583, 500, 380, 853, 281, 1029, 337, 886, 709, 28921, 570, 498, 291, 1029, 337, 364, 28921, 295, 11, 411, 11, 8423, 11, 291, 1062, 51048], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 517, "seek": 189306, "start": 1906.74, "end": 1911.4199999999998, "text": " be out of data distribution or, even worse, you could be in data distribution for some,", "tokens": [51048, 312, 484, 295, 1412, 7316, 420, 11, 754, 5324, 11, 291, 727, 312, 294, 1412, 7316, 337, 512, 11, 51282], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 518, "seek": 189306, "start": 1911.4199999999998, "end": 1916.26, "text": " like, sci-fi stuff and it will start to, like, take on some sci-fi role-playing or something", "tokens": [51282, 411, 11, 2180, 12, 13325, 1507, 293, 309, 486, 722, 281, 11, 411, 11, 747, 322, 512, 2180, 12, 13325, 3090, 12, 32944, 420, 746, 51524], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 519, "seek": 189306, "start": 1916.26, "end": 1917.26, "text": " like that.", "tokens": [51524, 411, 300, 13, 51574], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 520, "seek": 189306, "start": 1917.26, "end": 1921.62, "text": " So you have to find, like, the right amount of IQ, I think, it's got some U-shaped curve", "tokens": [51574, 407, 291, 362, 281, 915, 11, 411, 11, 264, 558, 2372, 295, 28921, 11, 286, 519, 11, 309, 311, 658, 512, 624, 12, 23103, 7605, 51792], "temperature": 0.0, "avg_logprob": -0.16494843766495987, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.005553892347961664}, {"id": 521, "seek": 192162, "start": 1921.62, "end": 1923.4599999999998, "text": " there.", "tokens": [50364, 456, 13, 50456], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 522, "seek": 192162, "start": 1923.4599999999998, "end": 1929.1799999999998, "text": " Next up, as we saw, when we are trying to solve problems, we know what we are good at and what", "tokens": [50456, 3087, 493, 11, 382, 321, 1866, 11, 562, 321, 366, 1382, 281, 5039, 2740, 11, 321, 458, 437, 321, 366, 665, 412, 293, 437, 50742], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 523, "seek": 192162, "start": 1929.1799999999998, "end": 1932.26, "text": " we're not good at and we lean on tools computationally.", "tokens": [50742, 321, 434, 406, 665, 412, 293, 321, 11659, 322, 3873, 24903, 379, 13, 50896], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 524, "seek": 192162, "start": 1932.26, "end": 1935.3, "text": " You want to do the same potentially with your LLMs.", "tokens": [50896, 509, 528, 281, 360, 264, 912, 7263, 365, 428, 441, 43, 26386, 13, 51048], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 525, "seek": 192162, "start": 1935.3, "end": 1942.26, "text": " So in particular, we may want to give them calculators, code interpreters, and so on,", "tokens": [51048, 407, 294, 1729, 11, 321, 815, 528, 281, 976, 552, 4322, 3391, 11, 3089, 17489, 1559, 11, 293, 370, 322, 11, 51396], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 526, "seek": 192162, "start": 1942.26, "end": 1947.54, "text": " the ability to do search and there's a lot of techniques for doing that.", "tokens": [51396, 264, 3485, 281, 360, 3164, 293, 456, 311, 257, 688, 295, 7512, 337, 884, 300, 13, 51660], "temperature": 0.0, "avg_logprob": -0.14718512192513178, "compression_ratio": 1.593073593073593, "no_speech_prob": 0.004197433590888977}, {"id": 527, "seek": 194754, "start": 1947.54, "end": 1951.58, "text": " One thing to keep in mind, again, is that these transformers by default may not know", "tokens": [50364, 1485, 551, 281, 1066, 294, 1575, 11, 797, 11, 307, 300, 613, 4088, 433, 538, 7576, 815, 406, 458, 50566], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 528, "seek": 194754, "start": 1951.58, "end": 1953.1, "text": " what they don't know.", "tokens": [50566, 437, 436, 500, 380, 458, 13, 50642], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 529, "seek": 194754, "start": 1953.1, "end": 1956.82, "text": " So you may even want to tell the transformer in a prompt, you are not very good at mental", "tokens": [50642, 407, 291, 815, 754, 528, 281, 980, 264, 31782, 294, 257, 12391, 11, 291, 366, 406, 588, 665, 412, 4973, 50828], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 530, "seek": 194754, "start": 1956.82, "end": 1957.82, "text": " arithmetic.", "tokens": [50828, 42973, 13, 50878], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 531, "seek": 194754, "start": 1957.82, "end": 1961.42, "text": " Whenever you need to do very large number addition, multiplication or whatever, instead", "tokens": [50878, 14159, 291, 643, 281, 360, 588, 2416, 1230, 4500, 11, 27290, 420, 2035, 11, 2602, 51058], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 532, "seek": 194754, "start": 1961.42, "end": 1962.42, "text": " use this calculator.", "tokens": [51058, 764, 341, 24993, 13, 51108], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 533, "seek": 194754, "start": 1962.42, "end": 1966.6599999999999, "text": " Here's how you use the calculator, use this token, combination, et cetera, et cetera.", "tokens": [51108, 1692, 311, 577, 291, 764, 264, 24993, 11, 764, 341, 14862, 11, 6562, 11, 1030, 11458, 11, 1030, 11458, 13, 51320], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 534, "seek": 194754, "start": 1966.6599999999999, "end": 1969.74, "text": " So you have to actually, like, spell it out because the model by default doesn't know", "tokens": [51320, 407, 291, 362, 281, 767, 11, 411, 11, 9827, 309, 484, 570, 264, 2316, 538, 7576, 1177, 380, 458, 51474], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 535, "seek": 194754, "start": 1969.74, "end": 1975.8999999999999, "text": " what it's good at or not good at necessarily, just like you and I might be.", "tokens": [51474, 437, 309, 311, 665, 412, 420, 406, 665, 412, 4725, 11, 445, 411, 291, 293, 286, 1062, 312, 13, 51782], "temperature": 0.0, "avg_logprob": -0.17719466156429714, "compression_ratio": 1.8167202572347267, "no_speech_prob": 0.2171681523323059}, {"id": 536, "seek": 197590, "start": 1976.02, "end": 1980.5400000000002, "text": " Next up, I think something that is very interesting is we went from a world that was retrieval", "tokens": [50370, 3087, 493, 11, 286, 519, 746, 300, 307, 588, 1880, 307, 321, 1437, 490, 257, 1002, 300, 390, 19817, 3337, 50596], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 537, "seek": 197590, "start": 1980.5400000000002, "end": 1986.02, "text": " only all the way the pendulum swung to the other extreme where it's memory only in LLMs.", "tokens": [50596, 787, 439, 264, 636, 264, 44103, 1693, 1063, 281, 264, 661, 8084, 689, 309, 311, 4675, 787, 294, 441, 43, 26386, 13, 50870], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 538, "seek": 197590, "start": 1986.02, "end": 1990.74, "text": " But actually, there's this entire space in between of these retrieval augmented models", "tokens": [50870, 583, 767, 11, 456, 311, 341, 2302, 1901, 294, 1296, 295, 613, 19817, 3337, 36155, 5245, 51106], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 539, "seek": 197590, "start": 1990.74, "end": 1993.42, "text": " and this works extremely well in practice.", "tokens": [51106, 293, 341, 1985, 4664, 731, 294, 3124, 13, 51240], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 540, "seek": 197590, "start": 1993.42, "end": 1997.3000000000002, "text": " As I mentioned, the context window of a transformer is its working memory.", "tokens": [51240, 1018, 286, 2835, 11, 264, 4319, 4910, 295, 257, 31782, 307, 1080, 1364, 4675, 13, 51434], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 541, "seek": 197590, "start": 1997.3000000000002, "end": 2001.5, "text": " If you can load the working memory with any information that is relevant to the task,", "tokens": [51434, 759, 291, 393, 3677, 264, 1364, 4675, 365, 604, 1589, 300, 307, 7340, 281, 264, 5633, 11, 51644], "temperature": 0.0, "avg_logprob": -0.18436672006334578, "compression_ratio": 1.6928571428571428, "no_speech_prob": 0.06937555968761444}, {"id": 542, "seek": 200150, "start": 2001.5, "end": 2006.58, "text": " the model will work extremely well because it can immediately access all that memory.", "tokens": [50364, 264, 2316, 486, 589, 4664, 731, 570, 309, 393, 4258, 2105, 439, 300, 4675, 13, 50618], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 543, "seek": 200150, "start": 2006.58, "end": 2012.06, "text": " And so I think a lot of people are really interested in basically retrieval augmented", "tokens": [50618, 400, 370, 286, 519, 257, 688, 295, 561, 366, 534, 3102, 294, 1936, 19817, 3337, 36155, 50892], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 544, "seek": 200150, "start": 2012.06, "end": 2013.06, "text": " generation.", "tokens": [50892, 5125, 13, 50942], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 545, "seek": 200150, "start": 2013.06, "end": 2016.74, "text": " And on the bottom, I have, like, an example of Lama index, which is one sort of data", "tokens": [50942, 400, 322, 264, 2767, 11, 286, 362, 11, 411, 11, 364, 1365, 295, 441, 2404, 8186, 11, 597, 307, 472, 1333, 295, 1412, 51126], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 546, "seek": 200150, "start": 2016.74, "end": 2021.82, "text": " connector to lots of different types of data, and you can make it, you can index all of", "tokens": [51126, 19127, 281, 3195, 295, 819, 3467, 295, 1412, 11, 293, 291, 393, 652, 309, 11, 291, 393, 8186, 439, 295, 51380], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 547, "seek": 200150, "start": 2021.82, "end": 2024.5, "text": " that data and you can make it accessible to LLMs.", "tokens": [51380, 300, 1412, 293, 291, 393, 652, 309, 9515, 281, 441, 43, 26386, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 548, "seek": 200150, "start": 2024.5, "end": 2028.42, "text": " And the emerging recipe there is you take relevant documents, you split them up into", "tokens": [51514, 400, 264, 14989, 6782, 456, 307, 291, 747, 7340, 8512, 11, 291, 7472, 552, 493, 666, 51710], "temperature": 0.0, "avg_logprob": -0.16046962265140755, "compression_ratio": 1.7048611111111112, "no_speech_prob": 0.0010814700508490205}, {"id": 549, "seek": 202842, "start": 2028.42, "end": 2032.8200000000002, "text": " chunks, you embed all of them, and you basically get embedding vectors that represent that", "tokens": [50364, 24004, 11, 291, 12240, 439, 295, 552, 11, 293, 291, 1936, 483, 12240, 3584, 18875, 300, 2906, 300, 50584], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 550, "seek": 202842, "start": 2032.8200000000002, "end": 2037.02, "text": " data, you store that in the vector store, and then at test time, you make some kind", "tokens": [50584, 1412, 11, 291, 3531, 300, 294, 264, 8062, 3531, 11, 293, 550, 412, 1500, 565, 11, 291, 652, 512, 733, 50794], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 551, "seek": 202842, "start": 2037.02, "end": 2041.78, "text": " of a query to your vector store, and you fetch chunks that might be relevant to your task", "tokens": [50794, 295, 257, 14581, 281, 428, 8062, 3531, 11, 293, 291, 23673, 24004, 300, 1062, 312, 7340, 281, 428, 5633, 51032], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 552, "seek": 202842, "start": 2041.78, "end": 2044.3000000000002, "text": " and you stuff them into the prompt and then you generate.", "tokens": [51032, 293, 291, 1507, 552, 666, 264, 12391, 293, 550, 291, 8460, 13, 51158], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 553, "seek": 202842, "start": 2044.3000000000002, "end": 2046.46, "text": " So this can work quite well in practice.", "tokens": [51158, 407, 341, 393, 589, 1596, 731, 294, 3124, 13, 51266], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 554, "seek": 202842, "start": 2046.46, "end": 2050.42, "text": " So this is, I think, similar to when you and I solve problems, you can do everything from", "tokens": [51266, 407, 341, 307, 11, 286, 519, 11, 2531, 281, 562, 291, 293, 286, 5039, 2740, 11, 291, 393, 360, 1203, 490, 51464], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 555, "seek": 202842, "start": 2050.42, "end": 2054.78, "text": " your memory and transformers have very large and extensive memory, but also it really helps", "tokens": [51464, 428, 4675, 293, 4088, 433, 362, 588, 2416, 293, 13246, 4675, 11, 457, 611, 309, 534, 3665, 51682], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 556, "seek": 202842, "start": 2054.78, "end": 2057.82, "text": " to reference some primary documents.", "tokens": [51682, 281, 6408, 512, 6194, 8512, 13, 51834], "temperature": 0.0, "avg_logprob": -0.10318762234279087, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0009696141351014376}, {"id": 557, "seek": 205782, "start": 2057.82, "end": 2061.42, "text": " So whenever you find yourself going back to a textbook to find something, or whenever", "tokens": [50364, 407, 5699, 291, 915, 1803, 516, 646, 281, 257, 25591, 281, 915, 746, 11, 420, 5699, 50544], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 558, "seek": 205782, "start": 2061.42, "end": 2066.2200000000003, "text": " you find yourself going back to documentation of a library to look something up, the transformers", "tokens": [50544, 291, 915, 1803, 516, 646, 281, 14333, 295, 257, 6405, 281, 574, 746, 493, 11, 264, 4088, 433, 50784], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 559, "seek": 205782, "start": 2066.2200000000003, "end": 2068.02, "text": " definitely want to do that, too.", "tokens": [50784, 2138, 528, 281, 360, 300, 11, 886, 13, 50874], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 560, "seek": 205782, "start": 2068.02, "end": 2072.5800000000004, "text": " You have some memory over how some documentation of a library works, but it's much better to", "tokens": [50874, 509, 362, 512, 4675, 670, 577, 512, 14333, 295, 257, 6405, 1985, 11, 457, 309, 311, 709, 1101, 281, 51102], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 561, "seek": 205782, "start": 2072.5800000000004, "end": 2073.5800000000004, "text": " look it up.", "tokens": [51102, 574, 309, 493, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 562, "seek": 205782, "start": 2073.5800000000004, "end": 2076.02, "text": " So the same applies here.", "tokens": [51152, 407, 264, 912, 13165, 510, 13, 51274], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 563, "seek": 205782, "start": 2076.02, "end": 2079.7000000000003, "text": " Next, I wanted to briefly talk about constraint prompting.", "tokens": [51274, 3087, 11, 286, 1415, 281, 10515, 751, 466, 25534, 12391, 278, 13, 51458], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 564, "seek": 205782, "start": 2079.7000000000003, "end": 2082.3, "text": " I also find this very interesting.", "tokens": [51458, 286, 611, 915, 341, 588, 1880, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1352408903616446, "compression_ratio": 1.822314049586777, "no_speech_prob": 0.00037993027945049107}, {"id": 565, "seek": 208230, "start": 2082.3, "end": 2090.38, "text": " This is basically techniques for forcing a certain template in the outputs of LLMs.", "tokens": [50364, 639, 307, 1936, 7512, 337, 19030, 257, 1629, 12379, 294, 264, 23930, 295, 441, 43, 26386, 13, 50768], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 566, "seek": 208230, "start": 2090.38, "end": 2095.42, "text": " So guidance is one example from Microsoft, actually, and here we are enforcing that the", "tokens": [50768, 407, 10056, 307, 472, 1365, 490, 8116, 11, 767, 11, 293, 510, 321, 366, 25495, 2175, 300, 264, 51020], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 567, "seek": 208230, "start": 2095.42, "end": 2100.5800000000004, "text": " output from the LLM will be JSON, and this will actually guarantee that the output will", "tokens": [51020, 5598, 490, 264, 441, 43, 44, 486, 312, 31828, 11, 293, 341, 486, 767, 10815, 300, 264, 5598, 486, 51278], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 568, "seek": 208230, "start": 2100.5800000000004, "end": 2104.0600000000004, "text": " take on this form because they go in and they mess with the probabilities of all the different", "tokens": [51278, 747, 322, 341, 1254, 570, 436, 352, 294, 293, 436, 2082, 365, 264, 33783, 295, 439, 264, 819, 51452], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 569, "seek": 208230, "start": 2104.0600000000004, "end": 2107.7000000000003, "text": " tokens that come out of the transformer and they clamp those tokens.", "tokens": [51452, 22667, 300, 808, 484, 295, 264, 31782, 293, 436, 17690, 729, 22667, 13, 51634], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 570, "seek": 208230, "start": 2107.7000000000003, "end": 2110.78, "text": " And then the transformer is only filling in the blanks here, and then you can enforce", "tokens": [51634, 400, 550, 264, 31782, 307, 787, 10623, 294, 264, 8247, 82, 510, 11, 293, 550, 291, 393, 24825, 51788], "temperature": 0.0, "avg_logprob": -0.09588802370250735, "compression_ratio": 1.8442028985507246, "no_speech_prob": 0.0005526281311176717}, {"id": 571, "seek": 211078, "start": 2110.82, "end": 2113.6200000000003, "text": " additional restrictions on what could go into those blanks.", "tokens": [50366, 4497, 14191, 322, 437, 727, 352, 666, 729, 8247, 82, 13, 50506], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 572, "seek": 211078, "start": 2113.6200000000003, "end": 2116.94, "text": " So this might be really helpful, and I think this kind of constraint sampling is also extremely", "tokens": [50506, 407, 341, 1062, 312, 534, 4961, 11, 293, 286, 519, 341, 733, 295, 25534, 21179, 307, 611, 4664, 50672], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 573, "seek": 211078, "start": 2116.94, "end": 2117.94, "text": " interesting.", "tokens": [50672, 1880, 13, 50722], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 574, "seek": 211078, "start": 2117.94, "end": 2122.6600000000003, "text": " I also wanted to say a few words about fine-tuning.", "tokens": [50722, 286, 611, 1415, 281, 584, 257, 1326, 2283, 466, 2489, 12, 83, 37726, 13, 50958], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 575, "seek": 211078, "start": 2122.6600000000003, "end": 2127.38, "text": " It is the case that you can get really far with prompt engineering, but it's also possible", "tokens": [50958, 467, 307, 264, 1389, 300, 291, 393, 483, 534, 1400, 365, 12391, 7043, 11, 457, 309, 311, 611, 1944, 51194], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 576, "seek": 211078, "start": 2127.38, "end": 2129.82, "text": " to think about fine-tuning your models.", "tokens": [51194, 281, 519, 466, 2489, 12, 83, 37726, 428, 5245, 13, 51316], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 577, "seek": 211078, "start": 2129.82, "end": 2133.34, "text": " Now fine-tuning models means that you are actually going to change the weights of the", "tokens": [51316, 823, 2489, 12, 83, 37726, 5245, 1355, 300, 291, 366, 767, 516, 281, 1319, 264, 17443, 295, 264, 51492], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 578, "seek": 211078, "start": 2133.34, "end": 2134.5, "text": " model.", "tokens": [51492, 2316, 13, 51550], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 579, "seek": 211078, "start": 2134.5, "end": 2138.9, "text": " It is becoming a lot more accessible to do this in practice, and that's because of a", "tokens": [51550, 467, 307, 5617, 257, 688, 544, 9515, 281, 360, 341, 294, 3124, 11, 293, 300, 311, 570, 295, 257, 51770], "temperature": 0.0, "avg_logprob": -0.13384358685715755, "compression_ratio": 1.7993197278911566, "no_speech_prob": 0.00035692393430508673}, {"id": 580, "seek": 213890, "start": 2138.9, "end": 2143.34, "text": " number of techniques that have been developed and have libraries for very recently.", "tokens": [50364, 1230, 295, 7512, 300, 362, 668, 4743, 293, 362, 15148, 337, 588, 3938, 13, 50586], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 581, "seek": 213890, "start": 2143.34, "end": 2147.94, "text": " So for example, parameter-efficient fine-tuning techniques like LORA make sure that you're", "tokens": [50586, 407, 337, 1365, 11, 13075, 12, 68, 7816, 2489, 12, 83, 37726, 7512, 411, 441, 36860, 652, 988, 300, 291, 434, 50816], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 582, "seek": 213890, "start": 2147.94, "end": 2151.2200000000003, "text": " only training small sparse pieces of your model.", "tokens": [50816, 787, 3097, 1359, 637, 11668, 3755, 295, 428, 2316, 13, 50980], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 583, "seek": 213890, "start": 2151.2200000000003, "end": 2155.34, "text": " So most of the model is kept clamped at the base model, and some pieces of it are allowed", "tokens": [50980, 407, 881, 295, 264, 2316, 307, 4305, 17690, 292, 412, 264, 3096, 2316, 11, 293, 512, 3755, 295, 309, 366, 4350, 51186], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 584, "seek": 213890, "start": 2155.34, "end": 2159.34, "text": " to change, and this still works pretty well empirically, and makes it much cheaper to", "tokens": [51186, 281, 1319, 11, 293, 341, 920, 1985, 1238, 731, 25790, 984, 11, 293, 1669, 309, 709, 12284, 281, 51386], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 585, "seek": 213890, "start": 2159.34, "end": 2163.3, "text": " sort of tune only small pieces of your model.", "tokens": [51386, 1333, 295, 10864, 787, 1359, 3755, 295, 428, 2316, 13, 51584], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 586, "seek": 213890, "start": 2163.3, "end": 2167.1800000000003, "text": " It also means that because most of your model is clamped, you can use very low-precision", "tokens": [51584, 467, 611, 1355, 300, 570, 881, 295, 428, 2316, 307, 17690, 292, 11, 291, 393, 764, 588, 2295, 12, 3712, 40832, 51778], "temperature": 0.0, "avg_logprob": -0.13253305158542314, "compression_ratio": 1.7919463087248322, "no_speech_prob": 0.0006460662116296589}, {"id": 587, "seek": 216718, "start": 2167.18, "end": 2171.14, "text": " inference for computing those parts, because they are not going to be updated by gradient", "tokens": [50364, 38253, 337, 15866, 729, 3166, 11, 570, 436, 366, 406, 516, 281, 312, 10588, 538, 16235, 50562], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 588, "seek": 216718, "start": 2171.14, "end": 2174.5, "text": " ascent, and so that makes everything a lot more efficient as well.", "tokens": [50562, 382, 2207, 11, 293, 370, 300, 1669, 1203, 257, 688, 544, 7148, 382, 731, 13, 50730], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 589, "seek": 216718, "start": 2174.5, "end": 2177.5, "text": " And in addition, we have a number of open-sourced, high-quality base models.", "tokens": [50730, 400, 294, 4500, 11, 321, 362, 257, 1230, 295, 1269, 12, 82, 396, 1232, 11, 1090, 12, 11286, 3096, 5245, 13, 50880], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 590, "seek": 216718, "start": 2177.5, "end": 2181.58, "text": " Currently, as I mentioned, I think LAMA is quite nice, although it is not commercially", "tokens": [50880, 19964, 11, 382, 286, 2835, 11, 286, 519, 441, 38136, 307, 1596, 1481, 11, 4878, 309, 307, 406, 41751, 51084], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 591, "seek": 216718, "start": 2181.58, "end": 2184.62, "text": " licensed, I believe right now.", "tokens": [51084, 25225, 11, 286, 1697, 558, 586, 13, 51236], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 592, "seek": 216718, "start": 2184.62, "end": 2189.7799999999997, "text": " Something to keep in mind is that basically fine-tuning is a lot more technically involved.", "tokens": [51236, 6595, 281, 1066, 294, 1575, 307, 300, 1936, 2489, 12, 83, 37726, 307, 257, 688, 544, 12120, 3288, 13, 51494], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 593, "seek": 216718, "start": 2189.7799999999997, "end": 2192.8999999999996, "text": " It requires a lot more, I think, technical expertise to do right.", "tokens": [51494, 467, 7029, 257, 688, 544, 11, 286, 519, 11, 6191, 11769, 281, 360, 558, 13, 51650], "temperature": 0.0, "avg_logprob": -0.14873845875263214, "compression_ratio": 1.6633986928104576, "no_speech_prob": 0.0006769855390302837}, {"id": 594, "seek": 219290, "start": 2192.9, "end": 2197.26, "text": " It requires human data contractors for datasets and or synthetic data pipelines that can be", "tokens": [50364, 467, 7029, 1952, 1412, 28377, 337, 42856, 293, 420, 23420, 1412, 40168, 300, 393, 312, 50582], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 595, "seek": 219290, "start": 2197.26, "end": 2198.86, "text": " pretty complicated.", "tokens": [50582, 1238, 6179, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 596, "seek": 219290, "start": 2198.86, "end": 2203.42, "text": " This will definitely slow down your iteration cycle by a lot, and I would say on a high-level", "tokens": [50662, 639, 486, 2138, 2964, 760, 428, 24784, 6586, 538, 257, 688, 11, 293, 286, 576, 584, 322, 257, 1090, 12, 12418, 50890], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 597, "seek": 219290, "start": 2203.42, "end": 2208.46, "text": " SFD is achievable, because it is just you're continuing the language modeling task.", "tokens": [50890, 31095, 35, 307, 3538, 17915, 11, 570, 309, 307, 445, 291, 434, 9289, 264, 2856, 15983, 5633, 13, 51142], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 598, "seek": 219290, "start": 2208.46, "end": 2213.54, "text": " It's relatively straightforward, but RLHF, I would say, is very much research territory,", "tokens": [51142, 467, 311, 7226, 15325, 11, 457, 497, 43, 39, 37, 11, 286, 576, 584, 11, 307, 588, 709, 2132, 11360, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 599, "seek": 219290, "start": 2213.54, "end": 2218.1, "text": " and is even much harder to get to work, and so I would probably not advise that someone", "tokens": [51396, 293, 307, 754, 709, 6081, 281, 483, 281, 589, 11, 293, 370, 286, 576, 1391, 406, 18312, 300, 1580, 51624], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 600, "seek": 219290, "start": 2218.1, "end": 2220.86, "text": " just tries to roll their own RLHF implementation.", "tokens": [51624, 445, 9898, 281, 3373, 641, 1065, 497, 43, 39, 37, 11420, 13, 51762], "temperature": 0.0, "avg_logprob": -0.1393053741455078, "compression_ratio": 1.6074766355140186, "no_speech_prob": 0.0032215614337474108}, {"id": 601, "seek": 222086, "start": 2220.86, "end": 2224.78, "text": " These things are pretty unstable, very difficult to train, not something that is, I think,", "tokens": [50364, 1981, 721, 366, 1238, 23742, 11, 588, 2252, 281, 3847, 11, 406, 746, 300, 307, 11, 286, 519, 11, 50560], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 602, "seek": 222086, "start": 2224.78, "end": 2229.86, "text": " very beginner-friendly right now, and is also potentially likely also to change pretty rapidly", "tokens": [50560, 588, 22080, 12, 22864, 558, 586, 11, 293, 307, 611, 7263, 3700, 611, 281, 1319, 1238, 12910, 50814], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 603, "seek": 222086, "start": 2229.86, "end": 2232.38, "text": " still.", "tokens": [50814, 920, 13, 50940], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 604, "seek": 222086, "start": 2232.38, "end": 2235.78, "text": " So I think these are my sort of default recommendations right now.", "tokens": [50940, 407, 286, 519, 613, 366, 452, 1333, 295, 7576, 10434, 558, 586, 13, 51110], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 605, "seek": 222086, "start": 2235.78, "end": 2238.54, "text": " I would break up your task into two major parts.", "tokens": [51110, 286, 576, 1821, 493, 428, 5633, 666, 732, 2563, 3166, 13, 51248], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 606, "seek": 222086, "start": 2238.54, "end": 2242.82, "text": " Number one, achieve your top performance, and number two, optimize your performance in", "tokens": [51248, 5118, 472, 11, 4584, 428, 1192, 3389, 11, 293, 1230, 732, 11, 19719, 428, 3389, 294, 51462], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 607, "seek": 222086, "start": 2242.82, "end": 2244.58, "text": " that order.", "tokens": [51462, 300, 1668, 13, 51550], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 608, "seek": 222086, "start": 2244.58, "end": 2247.7400000000002, "text": " Number one, the best performance will currently come from G504 model.", "tokens": [51550, 5118, 472, 11, 264, 1151, 3389, 486, 4362, 808, 490, 460, 2803, 19, 2316, 13, 51708], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 609, "seek": 222086, "start": 2247.7400000000002, "end": 2250.2200000000003, "text": " It is the most capable model by far.", "tokens": [51708, 467, 307, 264, 881, 8189, 2316, 538, 1400, 13, 51832], "temperature": 0.0, "avg_logprob": -0.14701690673828124, "compression_ratio": 1.7076411960132891, "no_speech_prob": 0.0021151676774024963}, {"id": 610, "seek": 225022, "start": 2250.22, "end": 2255.02, "text": " These prompts that are very detailed, they have lots of task contents, relevant information", "tokens": [50364, 1981, 41095, 300, 366, 588, 9942, 11, 436, 362, 3195, 295, 5633, 15768, 11, 7340, 1589, 50604], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 611, "seek": 225022, "start": 2255.02, "end": 2256.7799999999997, "text": " and instructions.", "tokens": [50604, 293, 9415, 13, 50692], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 612, "seek": 225022, "start": 2256.7799999999997, "end": 2260.1, "text": " Think along the lines of what would you tell a task contractor if they can't email you", "tokens": [50692, 6557, 2051, 264, 3876, 295, 437, 576, 291, 980, 257, 5633, 26463, 498, 436, 393, 380, 3796, 291, 50858], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 613, "seek": 225022, "start": 2260.1, "end": 2264.2999999999997, "text": " back, but then also keep in mind that a task contractor is a human, and they have inner", "tokens": [50858, 646, 11, 457, 550, 611, 1066, 294, 1575, 300, 257, 5633, 26463, 307, 257, 1952, 11, 293, 436, 362, 7284, 51068], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 614, "seek": 225022, "start": 2264.2999999999997, "end": 2269.02, "text": " monologue and they're very clever, et cetera, LLMs do not possess those qualities, so make", "tokens": [51068, 1108, 42298, 293, 436, 434, 588, 13494, 11, 1030, 11458, 11, 441, 43, 26386, 360, 406, 17490, 729, 16477, 11, 370, 652, 51304], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 615, "seek": 225022, "start": 2269.02, "end": 2275.8599999999997, "text": " sure to think through the psychology of the LLM almost, and cater prompts to that.", "tokens": [51304, 988, 281, 519, 807, 264, 15105, 295, 264, 441, 43, 44, 1920, 11, 293, 21557, 41095, 281, 300, 13, 51646], "temperature": 0.0, "avg_logprob": -0.14206951005118235, "compression_ratio": 1.7218045112781954, "no_speech_prob": 0.0015008804621174932}, {"id": 616, "seek": 227586, "start": 2275.86, "end": 2280.9, "text": " You can even add any relevant context and information to these prompts.", "tokens": [50364, 509, 393, 754, 909, 604, 7340, 4319, 293, 1589, 281, 613, 41095, 13, 50616], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 617, "seek": 227586, "start": 2280.9, "end": 2283.2200000000003, "text": " Basically refer to a lot of the prompt engineering techniques.", "tokens": [50616, 8537, 2864, 281, 257, 688, 295, 264, 12391, 7043, 7512, 13, 50732], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 618, "seek": 227586, "start": 2283.2200000000003, "end": 2287.54, "text": " Some of them I've highlighted in the slides above, but also this is a very large space,", "tokens": [50732, 2188, 295, 552, 286, 600, 17173, 294, 264, 9788, 3673, 11, 457, 611, 341, 307, 257, 588, 2416, 1901, 11, 50948], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 619, "seek": 227586, "start": 2287.54, "end": 2291.9, "text": " and I would just advise you to look for prompt engineering techniques online.", "tokens": [50948, 293, 286, 576, 445, 18312, 291, 281, 574, 337, 12391, 7043, 7512, 2950, 13, 51166], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 620, "seek": 227586, "start": 2291.9, "end": 2294.2200000000003, "text": " There's a lot to cover there.", "tokens": [51166, 821, 311, 257, 688, 281, 2060, 456, 13, 51282], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 621, "seek": 227586, "start": 2294.2200000000003, "end": 2295.94, "text": " Experiment with few short examples.", "tokens": [51282, 37933, 365, 1326, 2099, 5110, 13, 51368], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 622, "seek": 227586, "start": 2295.94, "end": 2300.06, "text": " What this refers to is you don't just want to tell, you want to show whenever it's possible,", "tokens": [51368, 708, 341, 14942, 281, 307, 291, 500, 380, 445, 528, 281, 980, 11, 291, 528, 281, 855, 5699, 309, 311, 1944, 11, 51574], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 623, "seek": 227586, "start": 2300.06, "end": 2304.1, "text": " so give it examples of everything that helps it really understand what you mean if you", "tokens": [51574, 370, 976, 309, 5110, 295, 1203, 300, 3665, 309, 534, 1223, 437, 291, 914, 498, 291, 51776], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 624, "seek": 227586, "start": 2304.1, "end": 2305.1, "text": " can.", "tokens": [51776, 393, 13, 51826], "temperature": 0.0, "avg_logprob": -0.15876868276885062, "compression_ratio": 1.7111801242236024, "no_speech_prob": 0.07364340126514435}, {"id": 625, "seek": 230510, "start": 2305.7799999999997, "end": 2311.2599999999998, "text": " Experiment with tools and plugins to offload a task that are difficult for LLMs natively,", "tokens": [50398, 37933, 365, 3873, 293, 33759, 281, 766, 2907, 257, 5633, 300, 366, 2252, 337, 441, 43, 26386, 8470, 356, 11, 50672], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 626, "seek": 230510, "start": 2311.2599999999998, "end": 2315.1, "text": " and then think about not just a single prompt and answer, think about potential chains and", "tokens": [50672, 293, 550, 519, 466, 406, 445, 257, 2167, 12391, 293, 1867, 11, 519, 466, 3995, 12626, 293, 50864], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 627, "seek": 230510, "start": 2315.1, "end": 2318.44, "text": " reflection and how you glue them together and how you could potentially make multiple", "tokens": [50864, 12914, 293, 577, 291, 8998, 552, 1214, 293, 577, 291, 727, 7263, 652, 3866, 51031], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 628, "seek": 230510, "start": 2318.44, "end": 2320.2999999999997, "text": " samples and so on.", "tokens": [51031, 10938, 293, 370, 322, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 629, "seek": 230510, "start": 2320.2999999999997, "end": 2324.2999999999997, "text": " Finally, if you think you've squeezed out prompt engineering, which I think you should", "tokens": [51124, 6288, 11, 498, 291, 519, 291, 600, 39470, 484, 12391, 7043, 11, 597, 286, 519, 291, 820, 51324], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 630, "seek": 230510, "start": 2324.2999999999997, "end": 2331.8199999999997, "text": " stick with for a while, look at some potentially fine-tuning a model to your application, but", "tokens": [51324, 2897, 365, 337, 257, 1339, 11, 574, 412, 512, 7263, 2489, 12, 83, 37726, 257, 2316, 281, 428, 3861, 11, 457, 51700], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 631, "seek": 230510, "start": 2331.8199999999997, "end": 2334.2999999999997, "text": " expect this to be a lot more slower and involved.", "tokens": [51700, 2066, 341, 281, 312, 257, 688, 544, 14009, 293, 3288, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1625087674984262, "compression_ratio": 1.702970297029703, "no_speech_prob": 7.030697452137247e-05}, {"id": 632, "seek": 233430, "start": 2334.3, "end": 2338.5800000000004, "text": " And then there's an expert fragile research zone here, and I would say that is RLHF, which", "tokens": [50364, 400, 550, 456, 311, 364, 5844, 23847, 2132, 6668, 510, 11, 293, 286, 576, 584, 300, 307, 497, 43, 39, 37, 11, 597, 50578], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 633, "seek": 233430, "start": 2338.5800000000004, "end": 2343.2200000000003, "text": " currently does work a bit better than SFD if you can get it to work, but again, this", "tokens": [50578, 4362, 775, 589, 257, 857, 1101, 813, 31095, 35, 498, 291, 393, 483, 309, 281, 589, 11, 457, 797, 11, 341, 50810], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 634, "seek": 233430, "start": 2343.2200000000003, "end": 2345.5, "text": " is pretty involved, I would say.", "tokens": [50810, 307, 1238, 3288, 11, 286, 576, 584, 13, 50924], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 635, "seek": 233430, "start": 2345.5, "end": 2350.6200000000003, "text": " And to optimize your costs, try to explore lower capacity models or shorter prompts and", "tokens": [50924, 400, 281, 19719, 428, 5497, 11, 853, 281, 6839, 3126, 6042, 5245, 420, 11639, 41095, 293, 51180], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 636, "seek": 233430, "start": 2350.6200000000003, "end": 2351.6200000000003, "text": " so on.", "tokens": [51180, 370, 322, 13, 51230], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 637, "seek": 233430, "start": 2351.6200000000003, "end": 2357.38, "text": " I also wanted to say a few words about the use cases in which I think LLMs are currently", "tokens": [51230, 286, 611, 1415, 281, 584, 257, 1326, 2283, 466, 264, 764, 3331, 294, 597, 286, 519, 441, 43, 26386, 366, 4362, 51518], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 638, "seek": 233430, "start": 2357.38, "end": 2358.86, "text": " well suited for.", "tokens": [51518, 731, 24736, 337, 13, 51592], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 639, "seek": 233430, "start": 2358.86, "end": 2363.78, "text": " So in particular, note that there's a large number of limitations to LLMs today, and", "tokens": [51592, 407, 294, 1729, 11, 3637, 300, 456, 311, 257, 2416, 1230, 295, 15705, 281, 441, 43, 26386, 965, 11, 293, 51838], "temperature": 0.0, "avg_logprob": -0.1500232611129533, "compression_ratio": 1.6091205211726385, "no_speech_prob": 0.00020338199101388454}, {"id": 640, "seek": 236378, "start": 2363.78, "end": 2366.6600000000003, "text": " so I would keep that definitely in mind for all your applications.", "tokens": [50364, 370, 286, 576, 1066, 300, 2138, 294, 1575, 337, 439, 428, 5821, 13, 50508], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 641, "seek": 236378, "start": 2366.6600000000003, "end": 2369.5400000000004, "text": " Models, and this, by the way, could be an entire talk, so I don't have time to cover", "tokens": [50508, 6583, 1625, 11, 293, 341, 11, 538, 264, 636, 11, 727, 312, 364, 2302, 751, 11, 370, 286, 500, 380, 362, 565, 281, 2060, 50652], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 642, "seek": 236378, "start": 2369.5400000000004, "end": 2370.94, "text": " it in full detail.", "tokens": [50652, 309, 294, 1577, 2607, 13, 50722], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 643, "seek": 236378, "start": 2370.94, "end": 2375.9, "text": " Models may be biased, they may fabricate hallucinate information, they may have reasoning errors,", "tokens": [50722, 6583, 1625, 815, 312, 28035, 11, 436, 815, 7253, 473, 35212, 13923, 1589, 11, 436, 815, 362, 21577, 13603, 11, 50970], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 644, "seek": 236378, "start": 2375.9, "end": 2380.5800000000004, "text": " they may struggle in entire classes of applications, they have knowledge cutoffs, so they might", "tokens": [50970, 436, 815, 7799, 294, 2302, 5359, 295, 5821, 11, 436, 362, 3601, 1723, 19231, 11, 370, 436, 1062, 51204], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 645, "seek": 236378, "start": 2380.5800000000004, "end": 2384.1000000000004, "text": " not know any information above, say, September 2021.", "tokens": [51204, 406, 458, 604, 1589, 3673, 11, 584, 11, 7216, 7201, 13, 51380], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 646, "seek": 236378, "start": 2384.1000000000004, "end": 2388.26, "text": " They are susceptible to a large range of attacks, which are sort of like coming out on Twitter", "tokens": [51380, 814, 366, 31249, 281, 257, 2416, 3613, 295, 8122, 11, 597, 366, 1333, 295, 411, 1348, 484, 322, 5794, 51588], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 647, "seek": 236378, "start": 2388.26, "end": 2393.2200000000003, "text": " daily, including prompt injection, jailbreak attacks, data poisoning attacks, and so on.", "tokens": [51588, 5212, 11, 3009, 12391, 22873, 11, 10511, 13225, 8122, 11, 1412, 36778, 8122, 11, 293, 370, 322, 13, 51836], "temperature": 0.0, "avg_logprob": -0.14715169880488146, "compression_ratio": 1.7728613569321534, "no_speech_prob": 0.0029339012689888477}, {"id": 648, "seek": 239322, "start": 2393.22, "end": 2398.54, "text": " So my recommendation right now is use LLMs in low stakes applications, combine them with", "tokens": [50364, 407, 452, 11879, 558, 586, 307, 764, 441, 43, 26386, 294, 2295, 28429, 5821, 11, 10432, 552, 365, 50630], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 649, "seek": 239322, "start": 2398.54, "end": 2403.3399999999997, "text": " always with human oversight, use them as a source of inspiration and suggestions, and", "tokens": [50630, 1009, 365, 1952, 29146, 11, 764, 552, 382, 257, 4009, 295, 10249, 293, 13396, 11, 293, 50870], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 650, "seek": 239322, "start": 2403.3399999999997, "end": 2407.22, "text": " think co-pilots instead of completely autonomous agents that are just like performing a task", "tokens": [50870, 519, 598, 12, 79, 388, 1971, 2602, 295, 2584, 23797, 12554, 300, 366, 445, 411, 10205, 257, 5633, 51064], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 651, "seek": 239322, "start": 2407.22, "end": 2408.22, "text": " somewhere.", "tokens": [51064, 4079, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 652, "seek": 239322, "start": 2408.22, "end": 2412.62, "text": " It's just not clear that the models are there right now.", "tokens": [51114, 467, 311, 445, 406, 1850, 300, 264, 5245, 366, 456, 558, 586, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 653, "seek": 239322, "start": 2412.62, "end": 2415.18, "text": " So I wanted to close by saying that GPT-4 is an amazing artifact.", "tokens": [51334, 407, 286, 1415, 281, 1998, 538, 1566, 300, 26039, 51, 12, 19, 307, 364, 2243, 34806, 13, 51462], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 654, "seek": 239322, "start": 2415.18, "end": 2419.4599999999996, "text": " I'm very thankful that it exists, and it's beautiful, it has a ton of knowledge across", "tokens": [51462, 286, 478, 588, 13611, 300, 309, 8198, 11, 293, 309, 311, 2238, 11, 309, 575, 257, 2952, 295, 3601, 2108, 51676], "temperature": 0.0, "avg_logprob": -0.1347385088602702, "compression_ratio": 1.6105610561056105, "no_speech_prob": 0.001000216812826693}, {"id": 655, "seek": 241946, "start": 2419.46, "end": 2423.38, "text": " so many areas, it can do math, code, and so on.", "tokens": [50364, 370, 867, 3179, 11, 309, 393, 360, 5221, 11, 3089, 11, 293, 370, 322, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 656, "seek": 241946, "start": 2423.38, "end": 2427.02, "text": " And in addition, there's this thriving ecosystem of everything else that is being built and", "tokens": [50560, 400, 294, 4500, 11, 456, 311, 341, 30643, 11311, 295, 1203, 1646, 300, 307, 885, 3094, 293, 50742], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 657, "seek": 241946, "start": 2427.02, "end": 2432.34, "text": " incorporated into the ecosystem, some of these things I've talked about.", "tokens": [50742, 21654, 666, 264, 11311, 11, 512, 295, 613, 721, 286, 600, 2825, 466, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 658, "seek": 241946, "start": 2432.34, "end": 2436.06, "text": " And all of this power is accessible at your fingertips.", "tokens": [51008, 400, 439, 295, 341, 1347, 307, 9515, 412, 428, 27715, 13, 51194], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 659, "seek": 241946, "start": 2436.06, "end": 2440.86, "text": " So here's everything that's needed in terms of code to ask GPT-4 a question, to prompt", "tokens": [51194, 407, 510, 311, 1203, 300, 311, 2978, 294, 2115, 295, 3089, 281, 1029, 26039, 51, 12, 19, 257, 1168, 11, 281, 12391, 51434], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 660, "seek": 241946, "start": 2440.86, "end": 2442.7, "text": " it and get a response.", "tokens": [51434, 309, 293, 483, 257, 4134, 13, 51526], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 661, "seek": 241946, "start": 2442.7, "end": 2446.78, "text": " In this case, I said, can you say something to inspire the audience of Microsoft Build", "tokens": [51526, 682, 341, 1389, 11, 286, 848, 11, 393, 291, 584, 746, 281, 15638, 264, 4034, 295, 8116, 11875, 51730], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 662, "seek": 241946, "start": 2446.78, "end": 2448.26, "text": " 2023?", "tokens": [51730, 44377, 30, 51804], "temperature": 0.0, "avg_logprob": -0.1307878106590209, "compression_ratio": 1.6241379310344828, "no_speech_prob": 0.01261613517999649}, {"id": 663, "seek": 244826, "start": 2448.26, "end": 2454.34, "text": " And I just punched this into Python and verbatim, GPT-4 said the following.", "tokens": [50364, 400, 286, 445, 37842, 341, 666, 15329, 293, 9595, 267, 332, 11, 26039, 51, 12, 19, 848, 264, 3480, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 664, "seek": 244826, "start": 2454.34, "end": 2458.0200000000004, "text": " And by the way, I did not know that they used this trick in the keynote, so I thought I", "tokens": [50668, 400, 538, 264, 636, 11, 286, 630, 406, 458, 300, 436, 1143, 341, 4282, 294, 264, 33896, 11, 370, 286, 1194, 286, 50852], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 665, "seek": 244826, "start": 2458.0200000000004, "end": 2464.6600000000003, "text": " was being clever, but it is really good at this, it says, ladies and gentlemen, innovators", "tokens": [50852, 390, 885, 13494, 11, 457, 309, 307, 534, 665, 412, 341, 11, 309, 1619, 11, 9974, 293, 11669, 11, 5083, 3391, 51184], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 666, "seek": 244826, "start": 2464.6600000000003, "end": 2469.0200000000004, "text": " and trail bearers of Microsoft Build 2023, welcome to the gathering of brilliant minds", "tokens": [51184, 293, 9924, 6155, 433, 295, 8116, 11875, 44377, 11, 2928, 281, 264, 13519, 295, 10248, 9634, 51402], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 667, "seek": 244826, "start": 2469.0200000000004, "end": 2470.26, "text": " like no other.", "tokens": [51402, 411, 572, 661, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 668, "seek": 244826, "start": 2470.26, "end": 2474.1400000000003, "text": " You are the architects of the future, the visionaries molding the digital realm in which", "tokens": [51464, 509, 366, 264, 30491, 295, 264, 2027, 11, 264, 5201, 4889, 11102, 278, 264, 4562, 15355, 294, 597, 51658], "temperature": 0.0, "avg_logprob": -0.14910578520401666, "compression_ratio": 1.5505226480836236, "no_speech_prob": 0.012796347960829735}, {"id": 669, "seek": 247414, "start": 2474.18, "end": 2479.02, "text": " humanity thrives, embrace the limitless possibilities of technologies and let your ideas soar as", "tokens": [50366, 10243, 23949, 977, 11, 14038, 264, 4948, 1832, 12178, 295, 7943, 293, 718, 428, 3487, 370, 289, 382, 50608], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 670, "seek": 247414, "start": 2479.02, "end": 2480.54, "text": " high as your imagination.", "tokens": [50608, 1090, 382, 428, 12938, 13, 50684], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 671, "seek": 247414, "start": 2480.54, "end": 2484.54, "text": " Together, let's create a more connected, remarkable and inclusive world for generations", "tokens": [50684, 15911, 11, 718, 311, 1884, 257, 544, 4582, 11, 12802, 293, 13429, 1002, 337, 10593, 50884], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 672, "seek": 247414, "start": 2484.54, "end": 2489.62, "text": " to come, get ready to unleash your creativity, canvass the unknown, and turn dreams into", "tokens": [50884, 281, 808, 11, 483, 1919, 281, 49814, 428, 12915, 11, 393, 85, 640, 264, 9841, 11, 293, 1261, 7505, 666, 51138], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 673, "seek": 247414, "start": 2489.62, "end": 2490.62, "text": " reality.", "tokens": [51138, 4103, 13, 51188], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 674, "seek": 247414, "start": 2490.62, "end": 2492.2599999999998, "text": " Your journey begins today.", "tokens": [51188, 2260, 4671, 7338, 965, 13, 51270], "temperature": 0.0, "avg_logprob": -0.20450184895442083, "compression_ratio": 1.5509259259259258, "no_speech_prob": 0.1431678980588913}, {"id": 675, "seek": 250414, "start": 2504.14, "end": 2505.14, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 676, "seek": 250414, "start": 2505.14, "end": 2506.14, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 677, "seek": 250414, "start": 2506.14, "end": 2507.14, "text": " Thank you.", "tokens": [50464, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 678, "seek": 250414, "start": 2507.14, "end": 2508.14, "text": " Thank you.", "tokens": [50514, 1044, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 679, "seek": 250414, "start": 2508.14, "end": 2509.14, "text": " Thank you.", "tokens": [50564, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 680, "seek": 250414, "start": 2509.14, "end": 2510.14, "text": " Thank you.", "tokens": [50614, 1044, 291, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 681, "seek": 250414, "start": 2510.14, "end": 2511.14, "text": " Thank you.", "tokens": [50664, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 682, "seek": 250414, "start": 2511.14, "end": 2512.14, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 683, "seek": 250414, "start": 2512.14, "end": 2513.14, "text": " Thank you.", "tokens": [50764, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 684, "seek": 250414, "start": 2513.14, "end": 2514.14, "text": " Thank you.", "tokens": [50814, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 685, "seek": 250414, "start": 2514.14, "end": 2515.14, "text": " Thank you.", "tokens": [50864, 1044, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 686, "seek": 250414, "start": 2515.14, "end": 2516.14, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 687, "seek": 250414, "start": 2516.14, "end": 2517.14, "text": " Thank you.", "tokens": [50964, 1044, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 688, "seek": 250414, "start": 2517.14, "end": 2518.14, "text": " Thank you.", "tokens": [51014, 1044, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 689, "seek": 250414, "start": 2518.14, "end": 2519.14, "text": " Thank you.", "tokens": [51064, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 690, "seek": 250414, "start": 2519.14, "end": 2520.14, "text": " Thank you.", "tokens": [51114, 1044, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 691, "seek": 250414, "start": 2520.14, "end": 2521.14, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 692, "seek": 250414, "start": 2521.14, "end": 2522.14, "text": " Thank you.", "tokens": [51214, 1044, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 693, "seek": 250414, "start": 2522.14, "end": 2523.14, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 694, "seek": 250414, "start": 2523.14, "end": 2524.14, "text": " Thank you.", "tokens": [51314, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 695, "seek": 250414, "start": 2524.14, "end": 2525.14, "text": " Thank you.", "tokens": [51364, 1044, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 696, "seek": 250414, "start": 2525.14, "end": 2526.14, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 697, "seek": 250414, "start": 2526.14, "end": 2527.14, "text": " Thank you.", "tokens": [51464, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 698, "seek": 250414, "start": 2527.14, "end": 2528.14, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1677552363911613, "compression_ratio": 11.954545454545455, "no_speech_prob": 0.9977989792823792}, {"id": 699, "seek": 252814, "start": 2529.14, "end": 2530.14, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 700, "seek": 252814, "start": 2530.14, "end": 2531.14, "text": " Thank you.", "tokens": [50464, 1044, 291, 13, 50514], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 701, "seek": 252814, "start": 2531.14, "end": 2532.14, "text": " Thank you.", "tokens": [50514, 1044, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 702, "seek": 252814, "start": 2532.14, "end": 2533.14, "text": " Thank you.", "tokens": [50564, 1044, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 703, "seek": 252814, "start": 2533.14, "end": 2534.14, "text": " Thank you.", "tokens": [50614, 1044, 291, 13, 50664], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 704, "seek": 252814, "start": 2534.14, "end": 2535.14, "text": " Thank you.", "tokens": [50664, 1044, 291, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 705, "seek": 252814, "start": 2535.14, "end": 2536.14, "text": " Thank you.", "tokens": [50714, 1044, 291, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 706, "seek": 252814, "start": 2536.14, "end": 2537.14, "text": " Thank you.", "tokens": [50764, 1044, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 707, "seek": 252814, "start": 2537.14, "end": 2538.14, "text": " Thank you.", "tokens": [50814, 1044, 291, 13, 50864], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 708, "seek": 252814, "start": 2538.14, "end": 2539.14, "text": " Thank you.", "tokens": [50864, 1044, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 709, "seek": 252814, "start": 2539.14, "end": 2540.14, "text": " Thank you.", "tokens": [50914, 1044, 291, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 710, "seek": 252814, "start": 2540.14, "end": 2541.14, "text": " Thank you.", "tokens": [50964, 1044, 291, 13, 51014], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 711, "seek": 252814, "start": 2541.14, "end": 2542.14, "text": " Thank you.", "tokens": [51014, 1044, 291, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 712, "seek": 252814, "start": 2542.14, "end": 2543.14, "text": " Thank you.", "tokens": [51064, 1044, 291, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 713, "seek": 252814, "start": 2543.14, "end": 2544.14, "text": " Thank you.", "tokens": [51114, 1044, 291, 13, 51164], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 714, "seek": 252814, "start": 2544.14, "end": 2545.14, "text": " Thank you.", "tokens": [51164, 1044, 291, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 715, "seek": 252814, "start": 2545.14, "end": 2546.14, "text": " Thank you.", "tokens": [51214, 1044, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 716, "seek": 252814, "start": 2546.14, "end": 2547.14, "text": " Thank you.", "tokens": [51264, 1044, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 717, "seek": 252814, "start": 2547.14, "end": 2548.14, "text": " Thank you.", "tokens": [51314, 1044, 291, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 718, "seek": 252814, "start": 2548.14, "end": 2549.14, "text": " Thank you.", "tokens": [51364, 1044, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 719, "seek": 252814, "start": 2549.14, "end": 2550.14, "text": " Thank you.", "tokens": [51414, 1044, 291, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 720, "seek": 252814, "start": 2550.14, "end": 2551.14, "text": " Thank you.", "tokens": [51464, 1044, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 721, "seek": 252814, "start": 2551.14, "end": 2552.14, "text": " Thank you.", "tokens": [51514, 1044, 291, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 722, "seek": 252814, "start": 2552.14, "end": 2553.14, "text": " Thank you.", "tokens": [51564, 1044, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 723, "seek": 252814, "start": 2553.14, "end": 2554.14, "text": " Thank you.", "tokens": [51614, 1044, 291, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 724, "seek": 252814, "start": 2554.14, "end": 2555.14, "text": " Thank you.", "tokens": [51664, 1044, 291, 13, 51714], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 725, "seek": 252814, "start": 2555.14, "end": 2556.14, "text": " Thank you.", "tokens": [51714, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0544138163545706, "compression_ratio": 12.333333333333334, "no_speech_prob": 0.9913371801376343}, {"id": 726, "seek": 255614, "start": 2556.14, "end": 2557.14, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.16686381896336874, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9251894950866699}], "language": "en"}